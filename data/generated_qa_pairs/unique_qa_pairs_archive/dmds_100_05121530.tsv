qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	What are the new presidential term limits in Kazakhstan?	Kazakhstan has implemented a new limit of one seven-year term for presidents, replacing the previous system of maximum two five-year terms. Re-election to the highest state post is now prohibited. This change was recommended by President Tokayev to prevent the monopolization of power.	['This week the Embassy of the Republic of Kazakhstan to the Republic of South Africa, together with the Embassy of the State of Qatar in Pretoria Launched the second edition of the book by Abu Nasr Al-Farabi entitled, “Treatise on the views of excellent societies and their opposites: (The need to strive for an excellent state).\nThe book of Al-Farabi was published in Pretoria at the initiative of the Embassy of Kazakhstan to the Republic of South Africa in cooperation with the Embassy of the State of Qatar to RSA (a qualitatively new Kazakh and English translation was made by the diplomatic mission of the Land of Great Steppe).\nThe event coincided with the celebration of Kazakhstan National Day in Pretoria on 25 October 2022 and bidding farewell to a Diplomat Par Excellence, His Excellency Ambassador Kanat Tumysh, who is due to complete his tenure in South Africa.\nThe event was attended by representatives of the Department of International Relations and Cooperation and other South African governmental institutions, the diplomatic corps, led by its Dean Ambassador Salih Omar Abdou of the State of Eritrea, prominent public figures, inter alia, Professor Ndileka Mandela, the Founder and CEO of the Thembekile Mandela Foundation, representatives of business circles, academia, mass media and clergy, led by the Maulana Abdul Khaleq Allie, headman of the country’s Muslim Community, President of the countrywide United Ulama (Muslim Theologians) Council, First Vice President of the Muslim Judicial Council.\nIn his openning remarks Ambassador Kanat Tumysh of Kazakhstan made a briefing on the establishment of the Second Republic in his country and gradual building a New Fair Kazakhstan.\nAmbassador Tumysh familiarized the audience with a solid package of political, social, and economic reforms undertaken by the President of Kazakhstan Kassym-Jomart Tokayev in line with His Excellency’s State of the Nation Addresses of 16 March and 1 September 2022; as well as amendments made to the Kazakh Constitution and the domestic legislation at the nationwide referendum held on 5 June 2022 to achieve the great goals set up by the Head of State. The changes included following innovations:\n•A ban on the membership of the President in political parties for the period of his presidency. The membership ban also affects the chairpersons and members of the Central Election Commission, the Accounts Committee and the Constitutional Court.\n•A ban at the constitutional level for close relatives of the President on holding managerial positions in state and quasi-state structures.\n•The Majilis (lower chamber of Parliament) would be fully formed under a mixed electoral system based on party lists and single-member territorial districts. Introduction of a mixed proportional-majority system of forming the Majilis in the ratio of 70/30. This measure would increase the degree of direct participation of citizens in the formation of the Majilis. A recall mechanism was also introduced. It would also allow our citizens to recall MPs elected under the majoritarian system who have not justified the faith of the voters.\n•The Constitution of Kazakhstan explicitly abolished the death penalty.\nKazakh diplomat acquainted public with the upcoming snap elections of the President of Kazakhstan to be held on 20 November 2022, as well as the snap parliamentary and local elections to be arranged in the first half of 2023.\nThe President Tokayev recommended limiting the president’s term to one term of seven years (instead of maximum two five-year terms) in order to prevent the monopolization of power.\nRe-election to the highest state post is prohibited. New electoral cycle designed to restart the political system because of the enshrined constitutional and legislative novelties. This will lead to more efficient and smart state governance.\nAmbassador Tumysh said the milestone achievements and results by different benchmarks, obtained recently under the leadership of President Tokayev in the socio-economic sphere. Among them are the growth of the national economy by 3-4% this year and proficient surplus of the Kazakhstani budget (to be at the level of USD 7-10 billion at the end of 2022)\nThe main results and other information on the high-level international events held recently in Astana, inter alia, Seventh Congress of Leaders of World and Traditional Religions (14-15 September 2022), as well as the Sixth Summit of the Conference on Interaction and Confidence-Building Measures in Asia\n(CICA) (13 October 2022) were also communicated to the South African and international guests. They proved the relevance of peace-loving, pragmatic, predictable, reliable and balanced multi-vector foreign policy of President Tokayev. Yet, such a course is very important in modern geopolitical realities. At the CICA summit the process of its transformation from multilateral forum into a full-fledged international organisation was launched. The CICA summit also appointed incumbent Executive Director of the CICA Secretariat H.E. Ambassador Kairat Sarybay as the First Secretary-General of CICA.\nIn conclusion, Ambassador K.Tumysh spoke about Abu Nasr Al-Farabi and his book. Ambassador emphasised that this event is a second large-scale gathering since Covid-19 time, when Kazakhstan and the international community celebrated the 1150th birth Anniversary of the outstanding Oriental scientist, philosopher and educator of the Islamic Golden Age Abu Nasr Al-Farabi (870 AD – 950 AD), who known in the West as Alpharabius. He was born in the ancient city of Farab (Otrar) on the Jaxartes (Syr Darya) river in Central Asia (territory of the modern Kazakhstan). Al-Farabi was a renowned philosopher, jurist and theologian, who wrote in the fields of political philosophy, metaphysics, ethics and logic. He was also a scientist, cosmologist, mathematician and music scholar. In the Islamic philosophical tradition, he was often called the Second Teacher of the world, following Aristotle who was known as the First Teacher of the world. Through his works and especially wise philosophical treatises Al-Farabi became well-known in the West as well as the East.\nIn general, the participants noted the great potential of cooperation between Kazakhstan and South Africa, in particular, in the areas of bilateral trade, investments, tourism, and cultural exchange. Some of them noted that Kazakhstan and South Africa, who this year celebrate the 30th Anniversary of their diplomatic relations, may be geographically distant from each other, but distances matter a little, when nations build cultural bridges.']	['<urn:uuid:b79b4601-2056-4bf6-8693-3b58e3de06d5>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	9	43	1016
2	I'm curious about the intersection of biology and technology - what are the medical applications of microfluidic labs-on-a-chip, and what security concerns come with storing patient data on these devices?	Microfluidic labs-on-a-chip are used for handling and analyzing biological samples, and they can be integrated with biomedical sensor implants. However, when it comes to storing patient data on such devices, there are significant security concerns since wireless communication is prone to hacking, and the data can be intercepted, modified, or stolen. Additionally, these devices often have low processing power to handle encryption, making them vulnerable to security breaches.	['Joint Professor of Electrical Engineering and Bioengineering\nOffice: EEB 253I and Fluke Hall 215\nMy lab uses microfabrication and nanotechnology to build microfluidic labs-on-a-chip and implantable sensors.\nIn our lab, we work on many aspects of micro and nano electro mechanical systems (MEMS / NEMS). This research tends to be very interdisciplinary, and we have collaborations in biology, chemical engineering, bioengineering, computer science, and other departments. We have built, for example, self-assembling microstructures, biomedical implants, systems for docking of picosatellites, and walking microrobots.\nThere are two major research themes in our work:\nControlling surfaces and interfacial forces at the micro and nano scale: this includes systems for controlled self-assembly of microcomponents, “programmable” surfaces whose local properties (for example, hydrophobicity) can be changed on demand, and MEMS actuator arrays and microrobots for moving tiny objects.\nJoining MEMS and biology: this includes integrating new biomaterials into MEMS processes and devices, biomedical sensor implants, and microfluidic chips for handling and analyzing biological samples.\nPhD Cornell University, 1997\nMS Cornell University, 1993\nDipl.-Inform., Technical University Karlsruhe, Germany, 1990\nUniversity of California at Berkeley, 1996-1998\n2011 IEEE Fellow\n2010 John M. Fluke Distinguished Chair of Engineering, University of Washington\n2004-2005 Japan Society for the Promotion of Science (JSPS) Invitational Fellowship for Research in Japan\n2004 IEEE Robotics and Automation Society Academic Early Career Award\n1999 National Science Foundation CAREER Award\n1997 National Science Foundation Postdoctoral Associateship\nBowen Cheng, Dirk De Bruyker, Chris Chua, Kunal Sahasrabuddhe, Ivan Shubin, John E. Cunningham, Ying Luo, Karl F. Böhringer, Ashok V. Krishnamoorthy, Eugene M. Chow, “Microspring characterization and flip-chip assembly reliability.” IEEE Transactions on Components, Packaging and Manufacturing Technology 3(2):187-196, February 2013.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Optimization of Angular Alignment in Self-assembly of Thin Parts at an Air-water Interface.” ASME/IEEE Journal of Microelectromechanical Systems (letter) 22(1):13-15, February 2013.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Parallel heterogeneous integration of chip-scale parts by self-assembly.” ASME/IEEE Journal of Microelectromechanical Systems (letter) 21(6):1273-5, December 2012.\nChang-Ching Tu, Ji-Hao Hoo, Karl F. Böhringer, Lih Y. Lin, Guozhong Cao, “Surface passivation dependent photoluminescence from silicon quantum dot phosphors.” Optics Letters 37(22):4771-4, 15 November 2012.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Orientation-controlled parallel assembly at air-water interface”, Journal of Micromechanics and Microengineering 22(10):105028, October 2012.\nTodd A. Duncombe, James F. Parsons, Karl F. Böhringer, „Directed Drop Transport Rectified from Orthogonal Vibrations via a Flat Wetting Barrier Ratchet”, Langmuir 28(38):13765-13770, 30 August 2012.\nShaghayegh Abbasi, Sathana Kitayaporn, Michael J. Siedlik, Daniel T. Schwartz, Karl F. Böhringer, “Electrodeposition modeling and optimization to improve thin film patterning with orchestrated structure evolution”, IOP Nanotechnology 23(30):305301, 3 August 2012.\nTodd A. Duncombe, E. Yegân Erdem, Ashutosh Shastry, Rajashree Baskaran, Karl F. Böhringer, “Controlling Liquid Drops with Texture Ratchets.” Advanced Materials 24(12):1545-1550, 22 March 2012.\nAnupama V. Govindarajan, Sujatha Ramachandran, Genevieve D. Vigil, Paul Yager, Karl F. Böhringer, “A low cost point-of-care viscous sample preparation device for molecular diagnosis in the developing world; an example of microfluidic origami.” Lab-on-a-Chip 12(1):174-181, 2012.', 'By Mohammad Bajwa, PhD, CPHIMS, CPHI, RHIA, MCSE\nWhile it has only just emerged this century, the use of mobile health technology (mHealth) in the delivery and management of healthcare is gaining traction due to the global implementation of health information technology. This is exemplified mostly by electronic health record (EHR) systems, wherein patients’ health information resides on computers and travels on computer networks. Normally, the information stored on computers is accessed through wires and is limited to places where such infrastructure exists. With the advent of wireless technology, electronic health information can be accessed by anybody, anytime from anywhere and on any mobile device as long as wireless connectivity is available. Despite this advantage, the security of health information stored on and accessed through mobile devices is cause for public concern. Wireless communication is prone to hacking, and mobile devices, given their size and value, increase the chance they will be lost or stolen.\nmHealth is defined as the use of mobile devices (mDevices) in the practice of medicine. These include mobile/cellphones, iPads, tablets, personal and notebook (laptop) computers, personal digital assistants (PDAs), and similar other devices that use wireless technology to access health information networks. Such devices often use radio waves for communication either through central access points (hot spots) or satellites. Of these, mobile phones have emerged as the greatest mobile technology gadgets.\nSince wireless signals travel through the atmosphere, diffuse in all directions, and can pass through most physical barriers, they are liable be intercepted. The intent of this article is to review wireless technology, its use in healthcare, its security issues, and best practices to make safe use of this novel technology for the delivery and management of healthcare.\nTypes of mHealth Security\nBefore discussing the security of mHealth, it is appropriate to enumerate four types of information security requirements: physical, network, application, and user security.\nPhysical security implies the security of computers, network hardware, and storage media against intruders and natural disasters. The most common mitigative security measures for this purpose includes using ID badges and biometric authentication for entry to premises, locking computers with desks, and regular backups for data restoration in case of any human-made or natural disasters.\nNetwork security denotes the protection of data and information residing on computers and in transit over the network/internet through techniques like encryption (encodes data, rendering it unreadable by unauthorized persons) and technologies such as firewalls (controls both inbound and outbound data based on the nature of traffic and source); virtual private networks (makes private data communication channel in the public network such as internet); network intrusion detection (notifies the network administrator of any intrusion by email, text message, alert); and intrusion detection and prevention systems (blocks intruder and reports to network administrator).\nApplication security requires validation of services accessing the information, level of information access and use permissions, software patches and updates, software logs, and input validation techniques (best defense against malware/spyware injection).\nUser security is a three-step process that entails user identification through user ID, authentication through password, biometric or two-factor authentication (verifies user), and authorization through permissions (what the user can access and do).\nData Transmission Media\nData and information on the network travels either on wires (wireline transmission media)—the most common being the copper cables (network and coaxial or TV cables) and fiber optic (FO) cables—or wirelessly through the atmosphere. For the discussion of data security, it is pertinent to examine how data travels on these two types of transmission media and their security vulnerabilities.\nWireline Transmission Media\nThe two types of wireline transmission media are copper cables and fiber optic (FO) cables. Copper cables transmit data through voltages (electrical signals), called electromagnetic (EM) waves, as pulses (5 volts representing 1) or no pulses (representing 0). The data from these cables can be stolen by placing a device close to the cable, which can sense pulses and covert them back to 1s and 0s. To avoid this, some copper cables are shielded (covered with aluminum foil) to stop pulses from being detected externally. One typical example of this type of cable is TV (coaxial) cable that not only shields data being braided but can also transmit it at multiple voltage levels, thus catering to the needs of TV, internet, and digital phone requirements simultaneously. Contrary to the copper cables, FO cables are made of glass or highly transparent plastic and transmit data as light pulses. Its twofold benefits include data transmission at almost the speed of light (several times faster than copper cables) and data security, as light pulses cannot be detected externally. Besides being fast and secure, FO cables transmit data in multiple frequencies, enabling several data channels in one cable (video, internet, phone). Verizon FiOS is one such example.\nWireless signals are also EM waves, but unlike the wireline media, they travel through the air or atmosphere at different frequencies (amplitudes). They are advantageous in areas where wired networks cannot be set up (old and historical buildings) or are required temporarily (training or classroom setup). Because the wireless signals radiate in all directions and can pass most physical barriers, they are very valuable in hospital environments or healthcare organizations with mobile workforces. However, they are the least secure, as they can easily be intercepted by monitoring devices, and data can be interrupted, interjected, modified, stolen, rendered unusable, or even destroyed.\nMobile Technology (mTechnology)\nmTechnology is based on wireless technology that moves with the user. It is a portable two-way communication network and includes internet-enabled devices like smartphones, tablets, notebook computers, and navigation devices. mTechnology touches all aspects of our daily lives from starting the day with the alarm clock in your phone; managing the day through your calendar, reminders, and notes; communicating through emails, texts, social media; and accessing online information like maps, books, magazines, files, photos, and videos. The common four types of mTechnologies are:\n- Cellular or radio networks: Use distributed cell towers that enable mobile devices (cellphones) to switch frequencies automatically and communicate without interruption across large geographic areas.\n- Packet switching technology (5G network): Organizes data into packets for transmission and reassembles packets into information at destination.\n- Wi-Fi: Radio waves that connect devices to internet through localized routers.\n- Bluetooth: Connects devices over short distances using short-wavelength radio waves.\nMobile Health (mHealth)\nmHealth is the use of mTechnology in healthcare. The World Health organization (WHO) defines mHealth as the “use of mobile and wireless technologies to support the achievement of health objectives,”1 while the National Institutes of Health (NIH) describes it as “use of mobile and wireless devices to improve health outcomes, healthcare services, and healthcare research.”2\nThus, mHealth is the practice of medicine and healthcare over mobile devices, like smartphones, tablets, and iPads, such as eVisit (only mode of remote healthcare delivery service during COVID-19 pandemic). Although traditionally used for wellness management, their use for healthcare both by patients and providers has increased exponentially.3\nCurrently, mHealth technologies are being used for patient monitoring, patient-provider communication, telehealth, and e-health. Although the lines between telehealth (also referred to as telemedicine) and e-health are blurred, they are two different technologies. E-health is the electronic communication of information for improving patient’s health, while telehealth uses video, smartphones, or any wireless tools or telecommunications technology (network/internet) for specific healthcare delivery and is named after the type of healthcare service: telenursing, telepharmacy, telerehabilitation, teleradiology, teletrauma care, telepsychiatry, telepathology, and teledermatology. Telehealth requires license and an infrastructure to practice, while e-health has no such requirements. Both became routine during COVID-19 pandemic.\nThe global health data breaches in 2020 increased to 24.5 percent against financial services breaches (7.3 percent), once being the reverse.4 Thus, it is pertinent to elaborate on some mHealth security issues to understand their specific security vulnerabilities.\nMobile devices use air interface to communicate and transmit data through the atmosphere. The wireless medium is broadcast (signals diffuse in all directions), and signals can pass through physical barriers (e.g., walls, roofs, and concrete). They are vulnerable to active (through injecting, deleting, altering the message) or passive (redirecting the information) attacks. The signals traveling through the atmosphere can be intercepted, modified, destroyed, hacked, and rendered unusable through ransomware. Being portable, mobile devices are small and lucrative and can be lost or stolen along with data, have low processing power to handle encryption, and can use public unsecure Wi-Fi. Bring your own device (BYOD) policies poses additional risks of healthcare cybersecurity, as healthcare workers can use their own devices to access organizational network, access patient data, and enter medical orders. BYOD policies are advantageous because of higher productivity, as the providers can access health information from anywhere and anytime; they increase job satisfaction by supporting flexible work arrangements; and they increase effectiveness due to more comfort and speed with the use of people’s own devices. However, they suffer from several disadvantages, like data breaches due to lost or stolen personal devices, lack of firewall and anti-virus software, dearth of encryption power, and increased IT costs for supporting personal devices.\nHIPAA and mHealth\nHIPAA requires security of the protected health information (PHI) through its Security Rule that mandates the security of health information through a three-pronged approach: administrative (policy-based), physical (physical access to facility, workstation, and storage devices), and technical (technological requirements to control access to information).5 mHealth devices have potential HIPAA Security Rule compliance issues such as physical loss or theft of devices, data transmission over unsecured Wi-Fi, unencrypted text messages/emails, inadequate or lack of authentication protocols, and poor adherence to BYOD policies and procedures.\nBeside the potential to have HIPAA security compliance issues, other specific mHealth device issues include interference with implanted medical devices (pacemakers) and their use in monitoring vital signs and supporting life-threatening and critical care situations that use a part of electromagnetic spectrum, like mobile devices. They also suffer from diagnostic interpretations due to small screen size, which may conceal some crucial details in EKG, ultrasound, MRI, and X-rays.\nBest Security Practices for Securing mHealth Devices\nAlthough no technique or technology can provide fool-proof security for the data contained in mHealth devices or accessed through them, some measures can minimize the security risks. These include implementation of strong user authentication (biometric or two-factor authentication), automatic lock after excessive number of incorrect logins, remote wiping when a device is lost or stolen, employing encryption for conversations (emails, text messages), developing an application policy for BOYD devices, regular updating to keep vulnerabilities mitigated, and installing security programs/antivirus, as being networked they are apt to all sorts of malware attacks.\nInternet of Things in Healthcare (IoHT)\nYet another breed of mHealth devices are Internet of Healthcare Things (IoHT) or Internet of Medical Things (IoMT) devices, which is the use of Internet of Things (IoT) in healthcare. IoHT devices are sensor-based interconnected devices used for tracking assets and resources. In hospitals, they can be used for locating patients, medical staff, and visitors (called smart hospitals), and collection and integration of health data (generation, collection, and communication of health data through wearable devices).6 Other benefits of IoHT devices include savings in medical cost, reduction in medical errors, improved patient experience, manageability of medical drugs and adherence, better control over wastage in healthcare sector, and better outcomes of medical treatments.\nSecurity Mitigation of IoHT Devices\nMitigation of security for IoHT devices include both technological and administrative measures. The technological mitigation measures comprises use of blockchain technology to protect data in IoHT devices, enable IoHT devices to use authentication to validate user identity and access privileges, enable IoHT devices to use encryption for all health-related communication, enable integrity on IoHT devices to verify devices to ensure that they are unaltered and uninterrupted, and ensure IoHT devices are patched and updated to avert any vulnerability.\nThe administrative mitigation measures comprise using the principle of least privilege for required actions and types of communication, logging all user activities and events and monitoring them regularly for unusual activities (duplicate device ID or elevation in privileges), reviewing device data regularly to identify unusual trends or patterns, implementing security best practices associated with protecting and securing sensitive data, and conducting formal education, training workshops, certifications, and participation in mobile security conferences.\nFuture of IoHT\nIoHT devices have a great future in healthcare applications, especially through the integration of artificial intelligence with IoHT in the form of ingestible sensors (ePills, which are pill-sized devices to monitor internal physiology and act as diagnostic devices sending medical information and images to outside connected devices); nano-devices to monitor human physiology and deliver drugs to targeted areas like cancer cells; connected lenses that would determine tear glucose and eyes diseases; and blood clot monitoring sensors to avert heart attacks. Their potential healthcare implications include smart hospitals, virtual clinics, microsurgery, vital sign monitoring systems (which can analyze real-time data inputs from critically ill intensive care unit patients), activity trackers for heart patients to collect lifestyle information, and fitness wearables, to name a few.\nIn view of the significance of mTechnology in healthcare, the Journal of Mobile Technology in Medicine is published to disseminate the mHealth research results.\n- World Health Organization. https://www.who.int/goe/publications/goe_mhealth_web.pdf\n- National Institutes of Health. https://grants.nih.gov/grants/guide/pa-files/PAR-14-028.html\n- Phaneuf, A. “How mHealth apps are providing solutions to the healthcare market’s problems” (2019). Retrieved form What Is MHealth? Apps, Examples & Mobile Health Industry Trends (businessinsider.com).\n- US Department of Health & Human Services. “A Retrospective Look at Healthcare Cybersecurity.” 2020. https://www.hhs.gov/sites/default/files/2020-hph-cybersecurty-retrospective-tlpwhite.pdf\n- US Department of Health & Human Services. Summary of the HIPAA Security Rule Guidance Portal. https://www.hhs.gov/guidance/document/summary-hipaa-security-rule-1\n- Bajwa, M. “Opportunities and Challenges for Internet of Things in Healthcare (IoHT).” International Journal of General Medicine and Pharmacy (IJGMP): Vol. 9 (2020): 1-4.\nMohammad Bajwa ([email protected]) is a professor of health informatics at the University of Maryland-Global Campus.\nLeave a commentSyndicated from https://journal.ahima.org/mobile-health-mhealth-security-matters-and-mitigation/']	['<urn:uuid:6fb06661-b03c-4bda-ada4-ab151e1b6440>', '<urn:uuid:556cfcd0-022c-4723-a4e5-a1f05c8b063e>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	30	68	2796
3	I'm curious about the leadership structure in the US and India - how do their top leaders get power, and what kind of authority do they have?	In India, the Prime Minister becomes head of government by getting majority votes in elections and works with advice from the council of ministers. In the US, the President becomes both head of state and head of government through the electoral college system. While the Indian PM is responsible to Parliament and can be removed through a no-confidence motion, the US President is more powerful - they can select their own cabinet independently and can only be removed through impeachment, which is a lengthy process.	"['Article authored by Komal Bhati First year student from Army Institute of Law\n“Constitution is not an instrument for the government to restrain the people, it is an instrument for the people to restrain the government.” – Patrick Henry\nA document which is supreme and legally binds the citizens of a country is called constitution. Constitution of India is one of the “lengthiest constitutions in the world”. It is considered as the “supreme law” which binds the entire nation. It has taken inspiration from written documents of various countries like fundamental rights from USA, Parliamentary form of government from UK, etc. The base or the original structure is based on the “Government of India act, 1935”. The provisions of amendment and the unchanging nature of the basic structure makes our constitution flexible and rigid respectively.\nOriginally, it has eight parts, twenty-two schedules and three hundred ninety-five articles. It has provision for all sections of our country which include schedule caste, schedule tribe, women, children, disable and all the other weaker section of our country. Our constitution ensures that the citizens of our country can live, express, practice, work, learn and do many others things freely which do not harm the other citizens and also do not disturb the integrity of our nation. Salient features of constitution include “Parliamentary form of government, fundamental rights, single citizenship, directive principles, secular state, independent judiciary, fundamental duties, adult suffrage and judicial review”. Indian constitution is unique in itself which makes it suitable for diverse country like India.\nEstablishment of Sovereign, Secular, Democratic and Republic.\nThese words are incorporated into the preamble of the constitution which enlighten everyone about the basic structure of the constitution. It says that this constitution is made to protect people from injustice and give them freedom to grow and develop to grow in best environment.\n“India is a Sovereign country” which means that India is independent and all powerful to rule its own land, no other country can interference into the matters of our country until it permits to do so. Sovereignty is important as it gives power to the state to rule its people independently. This status makes fixed territory, population and established government as important elements of the state to do justice with the term sovereignty or to get all the tools to function as a sovereign state. It gives state the supreme authority within as well as outside the boundaries of the state.\nFor example, Prime Minister performing taking all the decision of the government with the advice of the council of ministers is the example for within the state and the decision of India to step out of the RCEP group is the sovereign decision outside the boundaries of the state.\n‘Secular state’ means in the matters of the religion it will neither support nor negate any religion but respect the sentiments of all the religion.\nIndia is a country where people of different religion stay together, to maintain the peace among them, the secular nature of the state is very important. It promotes the unity among the different religious communities and also give right to every community to practice and promote their religious practice without raising any seeds of communal violence.\nIn the case of S.R Bommai v. Union of India, it was held that secularism is the part of the basic structure of the constitution and also state cannot interfere in the matters of the religion.\nIn the case of the Indira Nehru Gandhi v. Shri Raj Narain and another, it was held that state cannot discriminate any citizens on the grounds of religion.\nConstitution has established democratic form of government that gets its authority or the ruling powers from the people’s will. It gives ‘Supreme power to the people’ through the right of universal adult franchise which gives voting rights to the citizens above the age of eighteen years.\nPrime minister gets its place as the head of government after he had got the majority of the votes in the election, this is how democracy works.\nDemocratic and Republic are almost same. It also says that the government must be elected by the people’s will but this adds on more significance and meaning to the democratic form of government which says that elected government will work according to the rule of the law i.e., Constitution. Like if there is only democratic form of government then the leader which is elected by the majority’s will can work according to his own will but if the government is both democratic and republic it has to abide by the rule of law.\nParliamentary v. Presidential system\nIn Parliamentary form of government, Prime minister is the head of the government and the real power of the executive lies in the hands of Prime minister. He works on the support and advice of the council of ministers (given in the article 74 and 75 of the constitution).\nThe PM and council of ministers are responsible for both Lok Sabha and Rajya Sabha of the Parliament which makes them responsible for the legislature. And the President is just the nominal head of the executive. This form of government is not very stable because the government can be dissolved by the no confidence motion. There is no separation of powers as the members of the executive are among the members of the legislature.\nIn Presidential form of government, President is both the head of the state and also the head of the government. He selects his own cabinets who are highly skilled and specialized in their respective fields. He is not bound with the advice of the cabinet minister which makes him as very powerful head of the government. President is not the part of legislature and is not even responsible towards it for his policies and acts. This is a more stable form of government because it cannot be dissolved with the no confidence motion. The impeachment of the president is only way to get the president down from its position which is also very lengthy process.\nThe judicial review is the part of the basic structure of the constitution. The judiciary is called as the “Guardian of the Constitution” which makes it responsible for the action of the legislature and executive which violates the provisions of the constitution. Judicial review is a power which authorizes the court to invalidate any act that is violative of the basic structure of the constitution.\nIn the case of Kesavananda Bharati v. State of Kerala, it was held by the court that parliament is having the power to amend the constitution but cannot break the basic structure of the constitutional including the feature of judicial review.\nIn the case of A.K. Gopalan v. the state of Madras, it was held that the preventive detention is subject to the provision of the judicial review.\nConstitution of India has lots of salient features which makes it unique and suitable for diverse country like India. All the provisions, articles and rights of the constitution gives free environment to the citizens in which they can enjoy their liberty.\n AIR 1994 SC 1918.\n AIR 1975 SC 2299.\n AIR 1973 SC 1461.\n AIR 1950 SC 27.', 'When the U.S. Constitution was being drafted, its writers had the British Parliamentary system to draw on. The British system was the system they were used to and had learnt since childhood. However, because the monarchy was one of the chief things that the former colonists had rebelled against, any vestiges of monarchy and most forms of concentrated power were avoided.\nBoth the U.S. and British political systems have a head of state, a court system and an upper and lower house. Both have constitutions that lay out the rules for government and the rights of the people. Both systems are democratic in nature in that governments are put in place and removed from power by the will of the people and both have systems of checks and balances to limit the power of any one branch.\nHead of State\nIn the U.S. political system, the president is the official head of state. The president is elected under the electoral college system. In the U.K., although the prime minister usually has the spotlight on political matters and is the official head of government, the queen or king is the official head of state. The queen officially signs off on acts of parliament and, just as the U.S. president delivers the State of the Union Address every year, the queen reads the ""Speech from the Throne,"" which is written by the prime minister. In modern government, the monarch is more of a ceremonial figurehead and it is unusual for any member of the royal family to directly interfere with the political process.\nThe Upper House\nThe United States has a Senate as the upper house of the legislative branch and the U.K. has the House of Lords. Under the U.S. system, each state, regardless of size, has two senators. Originally, senators were appointed by the governor of the state they represented but they are now elected to serve six-year terms. The House of Lords is very different. Members of the House of Lords are not elected. The 792 members of the House of Lords are members by inheritance, appointment or their rank in the Church of England; they are not elected and cannot be removed by popular vote. Otherwise they serve the same purpose as the U.S. Senate. They discuss, debate and vote on legislation passed by the lower house of the legislative branch.\nThe Lower House\nThe U.S. House of Representatives and the British House of Commons have a great deal in common. Each house is made up of representatives elected by the people. In both systems control of the lower house goes to the party that has the most seats. Under the U.K. system, the leader of the party with the most seats becomes the Prime Minister and the official head of the government. Under the U.S. system this person would be the Speaker of the House. One other key difference is elections. Under the parliamentary system, the prime minister can go to the crown at any point and ask to dissolve Parliament. If this is done an election is called. An election can also be called if the prime minister loses ""the confidence of the house."" This means that the prime minister lost a vote in Parliament on a matter of confidence. Matters of confidence are usually, but not always, over budgetary matters. If the prime minister loses a vote of confidence, the end result is an election.']"	['<urn:uuid:e3777bac-ebcd-436b-b764-8bac6b14ecf5>', '<urn:uuid:c2059a35-54d9-4af3-a441-b8049689fffe>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	27	85	1768
4	What happens in a mini stroke versus a regular stroke?	In a mini stroke (transient ischemic attack), the blood clot temporarily stops blood flow to the brain and the neurological symptoms completely withdraw. The symptoms are the same as a regular stroke but are temporary and come and go. However, in a regular stroke, brain cells die due to lack of oxygen when blood flow is cut off, leading to permanent damage and potentially long-term disabilities, affecting functions like memory and muscle control.	['A stroke is a “brain attack. It can certainly take place with anyone most of the time. It happens when ever blood flow to an area of human brain is cut-off. When an attack happens, brain cells are short of oxygen and then they begin to die out. Whenever human brain cells die throughout a heart stroke, capabilities controlled by that area of the human brain like memories and muscular control are lost.\nThe way an individual will be suspected by their stroke depends on where the stroke takes place in the human brain as well as how much of the human brain is affected (damaged). For example; an individual who experienced a small heart stroke will probably end up with significantly less serious issues like temporary body weakness of an arm or maybe leg. Those who have larger strokes could possibly be completely paralyzed in one part of their entire body or possibly lose their capability to speak. Most people recover completely from strokes, although over 2/3 of survivors will encounter some kind of disability.\nA stroke is an unexpected phenomenon that happens whenever the supply of blood to a part of the human brain is disrupted or when a blood vessel in the brain bursts, spilling the blood into the gaps surrounding human brain cells. Endangering factors involve things such as having high blood pressure levels, the smoking habit, and having a family history of heart stroke. Signs and Symptoms of a Stroke typically take place suddenly and can include things like confusion, nausea, extreme headache, and numbness or weakness in the arms and legs or face. Proper treatment can include medications, surgical operation, and rehabilitation.\nA stroke is a sudden encounter that might affect awareness, sensation, and movement, which results from a clogging or even rupture of the blood vessel in the human brain. It can cause signs or symptoms that survive for at least twenty-four hours. A heart stroke (known medically as a cerebrovascular accident or CVA) will occur when the blood supply to section of the human brain is unexpectedly disrupted or when a blood vessel in the brain bursts, spilling blood into the areas encircling brain cells. Brain cells stop working as they do not get oxygen and nutrients from the blood or there is sudden bleeding into or around the brain.\nTypes of Stroke\nThere are two main types of stroke – ischemic and hemorrhagic.\n1. Ischemic: Ischemic represents a reduced blood circulation along with oxygen supply to a portion of the human body. It is brought on by blood clot in an artery, which obstructs the access of blood. This happens in nearly 7 in 10 cases.\nThe blood clot normally forms within the artery by itself. This generally occurs over an area of fatty component called atheroma. Atheroma is normally known as furring or hardening of the arteries. Small patches of atheroma forms within arteries for most elder people. when a patch of atheroma becomes thick, it will prompt the blood to clot.\nIn some cases, the blood clot takes shape in another part of the body thereafter transmit in the bloodstream – it is termed as an embolus. The most frequently encountered illustration is a blood clot which forms in a heart chamber due to uneven turbulent flow of blood. This might happen in a condition known as atrial fibrillation. The blood clot is then passed in the bloodstream until it becomes jammed in an artery in the human brain. There are many rare factors that cause ischemic stroke.\n2. Hemorrhagic: An intracerebral hemorrhage can take place when the blood vessel bursts inside the human brain. The blood in the vessel then splatters into the adjoining brain tissues. This can result in the damaged brain cells to eliminate their supply of oxygen. They get damaged or else die. Things like this occur in about 1 in 10 strokes. A subarachnoid hemorrhage happens if a blood vessel bursts in the subarachnoid area. This is the narrow area between the human brain and the skull. This space is filled with a fluid called the cerebrospinal fluid. Approximately 1 stroke in 20 is because of a subarachnoid hemorrhage.\nEarly Warning Signs and Symptoms of Stroke\n- Suddenly experiencing weakness and numbness of the face, leg or arm, commonly on one side of the body.\n- Sudden misunderstandings, trouble in making decisions, interpreting or talking.\n- Blurred vision of eyes.\n- Trouble in walking, loss of balance or control.\n- Severe headache.\nExperts have suggested a key word to remember signs of strokes such as “BEFAST”, which expands into\n- B- Balance: A sudden loss in co-ordination of legs, which will not allow you to walk straight.\n- E- Eyes: Double vision, or blurred vision and blindness in one eye.\n- F- Face dropping: Numbness on one side of face or uneven smile.\n- A- Arm weakness: Weakness in one arm, unable to move hands.\n- S- Speech difficulty: Difficulty in speaking and understanding; slurred speech.\n- T- Time to call doctor: If you experience any of the above symptoms then it’s time for you to seek immediate medical expertise.\nOne should not sit at home, if one experiences the above early signs of stroke. So just remember BEFAST whenever you are trying to determine if you or someone else is having a mild or full stroke.\nWhat is Mild Stroke?\nA mild (Mini) stroke is an ordinary type of heart stroke and thus its neurological symptoms and signs usually do not last long so they totally withdraw. Despite this, even a mild stroke needs to be taken very seriously since it can represent formation to a much more serious situation, an actual stroke. In a mild stroke, a blood clot merely temporary stops the flow of blood to a certain part of the human brain. Once the blood flow is restored all the damaged features are restored. Medical name for mild stroke is transitory ischemic attack. The symptoms and signs of mild stroke usually depend upon the part of the brain that is damaged and most of them may even stay unacknowledged. The most significant symptoms and signs occur when areas of the human brain we commonly and regularly use are damaged.\nThis medical condition is associated with various aspects for example age, family medical record, high blood pressure levels, cardiovascular disease, elevated levels of cholesterol, sickle cell anemia etc. Smoking, a diet rich in fatty foods as well as being overweight are also risk factors for mild stroke.\nSymptoms and Signs of Mild Stroke\nFollowing are the most common symptoms of mild stroke:\n- Pain in chest\n- Blurred vision\n- Loss of balance and co-ordination\n- Slurred speech\n- Weakness in one hand and leg\n- Numbness on the face\n- Uneven smile\n- Muscle stiffness\n- Memory loss\n- Difficulty in swallowing\nCall your doctor if you experience the above symptoms and signs of mild stroke.\nStrokes Symptoms and Signs in Men\nFor both women and men, stroke is identified by an inability to speak and recognize speech, a hassled expression, numbness on face and arms or legs and sudden confusion. An individual who’s experiencing a stroke can also have trouble in speaking or interpreting conversation. There are no stroke signs and symptoms unique to men. Most common signs and symptoms are BEFAST that we discussed above.\nSigns and Symptoms of Strokes in Women\nWomen are less likely to receive a Tissue Plasminogen Activator (TPA) meant to break up blood clots. This is an important development because the most common type of stroke is caused a blood clot reaching the brain. In short, women tend to have “vaguer” stroke symptoms. In turn, they are not always treated as a sign of stroke which leads doctors to avoid the use of a tissue plasminogen activator.\nEven though men are more inclined to possess a stroke, women hold a higher life time threat of stroke. Women are more prone to die from a stroke. Women encounter a greater possibility of getting a stroke for several reasons. Women live for a longer time than men. Most women have more stress than men have. Women will probably possess high blood pressure (hypertension) as compared to men. Pregnancy as well as birth control additionally increase a woman’s threat of stroke. The stroke signs and symptoms for women involve:\n- Coordination problems\n- Shortness of Breath\n- Pain (arms, legs, face)\n- Chest Pain\nAs these signs and symptoms are unique to women, it might be hard to immediately link them with stroke. It will delay the treatment, leading to severe injuries, long-term disability, or loss of life.\nMore Info on Strokes Signs and Symptoms\nheart stroke will cause damage to the human brain. An acutely common factor that because stroke is a blood clot that develops in a human brain blood vessel (artery). Immediate treatment includes a clot-busting medicine to melt the blood clot. Other remedies involve medicine to help in reducing risk elements for further strokes. Rehabilitation is an important portion of treatment method. Disability following a heart stroke depends on aspects like the part of the human brain affected, precisely how rapidly treatment was handed along with the scope of the damage to the brain.\nThe blood supply to the human brain arrives mainly from four blood vessels (arteries) – the right and the left carotid arteries together with the right and the left vertebrobasilar arteries. They branch into various small arteries which supply the blood to almost all the areas of the human brain. The part of brain damaged as well as the level of the damage depends upon which blood vessel is injured.\nTake for example, in case you cut the supply of blood from a primary carotid artery then a vast part of your brain is damaged; this may result in critical signs and symptoms or loss of life. On the other hand, if a small branch of artery is affected so only a small part of brain is affected that may cause considerably slight signs and symptoms of Strokes.\nRelated Other Posts:', 'A CVA (cerebrovascular accident) is the medical name for a stroke, which is something that happens when the oxygen supply to parts of the brain is interrupted or cut off completely. Strokes can happen for a variety of reasons and are considered to be a medical emergency. However, being aware of the signs and symptoms of CVA can help ensure that your loved one receives prompt medical treatment, which can reduce the level of damage and lessen the risk of serious complications.\nWhat are the common signs and symptoms of CVA?\nA classic sign of a stroke is when the person experiences sudden paralysis or numbness down one side of their body or face. You may notice one side of their face begin to droop, or one arm might stop working. If in doubt, ask the person to lift both arms above their head—if one of their arms starts to drop, it could be because they are having a CVA.\nDepending on which part of the brain is affected, the person may have trouble speaking or understanding things said to them. They might try to speak, but slur their words or find it hard to form a coherent sentence to explain what is happening. The medical term for this is “aphasia” and it can be a symptom of a stroke.\nOther signs and symptoms of CVA\nIf the part of the brain responsible for coordination and motor skills is affected by the stroke, the person may stumble or fall over because of dizziness or a sudden loss of balance and coordination. Any sudden loss of motor skills is cause for concern as it could be the result of a stroke.\nStrokes can often affect vision and if the person says their eyesight is suddenly blurred, or they are seeing two of everything, they might be having a stroke. Vision can also turn black.\nA sudden and very severe headache can be the result of a stroke. If severe head pain is accompanied by vomiting, the person is delirious or in and out of consciousness, or is complaining of dizziness, consider that they could be having a stroke.\nThe left side of the brain controls the right hand side of the body, so if the right side of the face droops or the person becomes numb or paralyzed down the right side of their body, it could be because there is a problem with the blood supply to the LHS of their brain.\nSigns and symptoms of TIA\nA TIA is a transient ischemic attack, also known as a mini-stroke. TIAs can often be a precursor to a major stroke, but unlike a major CVA, the symptoms of a TIA are only temporary. A person experiencing a TIA will suffer the same symptoms as a person having a stroke, but these symptoms will typically come and go. However, unless treatment is sought, there is every chance that the patient will go on to have a major stroke, with potentially catastrophic results, so it is very important that a course of treatment is started as soon as possible.']	['<urn:uuid:49838351-e448-4c7e-9b1d-1739ac37cff4>', '<urn:uuid:f6d0f04e-1c18-4417-bd4e-eec8b71a7c26>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	10	73	2199
5	I'm building a house. What makes a promise legally binding, and what happens if it's broken?	A promise becomes legally binding when there is consideration - meaning both parties give up something in exchange. For example, in a construction contract, the contractor gives the service of building while the owner gives payment. If a party fails to perform their obligations (breaches the contract), there are several possible remedies: compensatory damages to cover actual losses, punitive damages in cases of malicious behavior, specific performance requiring completion of the work, or restitution requiring payment of agreed amounts. The seriousness of the breach matters - a material breach that irreparably breaks the contract (like non-payment) is treated more seriously than a minor breach (like a slight delay).	"['What is a contract?\n2. Society could not function without reliable promises, law of contracts, promises must be enforced, whole fundamental principal\n3. Not all contracts are written. They can be oral and written. Just remember an oral contract is not worth the paper it is written on.\n4. Certain contracts have to be in writing. These contracts were derived from the ancient English statute which was called the statute of frauds. The following contracts must be in writing to be enforceable:\nA. Contract which cannot be performed within a year\nB. Contract which cannot be performed within a lifetime of the promising party\nC. Promise to answer for debt or default of another(a performance bond)\nD. An agreement in exchange for marriage\nE. Contract for the sale of goods over $ 500\nF. Contract to sell real property or lease it for more than 1 year\nG. Contract employing a real estate agent to sell real estate for a commission\nH. Contract which modifies a written agreement\nI. An arbitration agreement\nJ. Certain consumer contracts that must also be in writing to be enforceable by the contractor\n5. No other contracts must be in writing but there must be mutual assent to the terms. I promise to pay the painter $15000 and he agrees to paint my house for that sum, sign a written contract and there is mutual assent. It could also occur if you tell the painter to paint my house and he does so.\n6. An agreement to agree is not enforceable. Suppose two fathers, one is a contractor, are watching their kids play soccer. The homeowner father says that he is thinking about converting his garage into a playroom. The contractor says that he could do the job for $10000. The fathers then agree that they will get together the next day at noon and sign a contract. There is “an agreement to agree” it is too uncertain to be enforceable–there is no way for a court to tell the terms of the agreement and make it enforceable.\n7. Conduct of the parties is sufficient to form a contract. Without words, you take your shirts to the laundry, leave them on the counter and walk out without speaking to the owner. The conduct is sufficient to create a contract between you and the laundry. You are in effect saying if you wash these shirts I will pay you.\nThe same is true for construction. A prime contractor can call one of its regular subs to go out on a job without mentioning the price and not signing a contract. P calls S out to pour a driveway. No writing and no price is agreed upon. P and S have done business in the past and always on the same price formula. There is no gift of the work by S, so the law implies the missing price and payment by prior conduct. P cannot escape paying.\nHere are several legal terms\nExpress contract: A contract created by the words of the parties-an express contract can be either written or oral\nImplied contract: A contract implied by actions or circumstances\nPromisor: A person who makes a promise\nPromisee: A person who receives a promise from a promisor\nOfferor: One who makes an offer\nOfferee: One who receives an offer\n8. Privity of contract. This rule has many exceptions states that a contract may be enforced only by those who are parties to the contract.\n9. Mutuality. Sometimes there are contracts in construction where there is a reservation granted to either the owner or the prime contractor giving them an unrestricted right to cancel the contract at any time. This clause may render the contract unenforceable due to lack of mutuality.\n10. Manifestation of mutual assent. This is rather a complex concept if there is an outward expression of agreement for example construction contracts usually require that the contractor perform in a workmanlike manner. The parties may honestly disagree as to what qualify of work is workmanlike. The owner may have visions that beguile the imagination. This may be quite different than the contractor envisioned. Yet such a failure to achieve a true meeting of the minds does not prevent a contract from coming into existence. The law will look to the parties’ objective conduct. The contract is the sum total of the obligations which the parties, by their words and conduct, objectively assume.\nA meeting of the minds is something the parties to a contract should always try to achieve. However, if they fail to achieve a complete meeting of the minds, a contract may nevertheless be formed if, by words and actions, the parties have manifested an intention to exchange enforceable promises.\n11. Uncertainty–sometimes a contract cannot be enforced due to uncertainty despite the parties unequivocally manifesting their intention to enter into a contract. If a promise is too uncertain to be understood. If P agrees to build a building in accordance with preliminary plans and outline specifications and agreed that the construction would be in accordance with final plans and specifications to be prepared by P in the future and thereafter approved in writing by the tenant. There could not be unreasonable withholding of approval of the plans by T.\nThe T refused to approve the plans and specs and P sued alleging that the refusal to approve was unreasonable. The court held that the contract was too uncertain to be enforced. It was merely an agreement to agree in the future, and such an agreement is unenforceable, especially if the matter left to future agreement is an essential terms such as the contents of plans specifications.\n12. Offer and counter offer\nMutual assent is often manifested by an offer and acceptance.\nSuppose an offeree rather than accept the work makes a counter-offer. If painter offers to paint owners house for $5000. Supposed painter says I accept your offer to paint my house and to paint the garage floor and driveway for the same price included. Is this conditional offer sufficient to form a contract?\nIs there a manifestation of mutual assent? Since the acceptance was conditioned the assent was incomplete. Both parties agreed to the price but not what was to be painted. However, the counter-offer can ripen into a contract if P and O agree that the $5000 will be paid for painting the house and the garage leaving out the driveway.\nSuppose K announces that in the trade papers it is accepting plumbing bids on an apartment house job. S sends in a written proposal to do the job for $25000 and a written contract will follow. K prepares a written contract, signs it and mails it to S. S crosses out a clause in the contract and initials the change, signs the contract and returns it. K notices the change, puts the contract in its file, does not initial the change, and notifies S that he should start work. S installs the groundwork and send a bill to K. K refuses to pay on the ground there is no contract.\nIs there a contract?\nA newspaper announcement states that K is accepting bids on a job, S submits a written proposal. There is an offer but no acceptance. Therefore, there is no contract. The invitation to bid was a solicitation for an offer. S’s proposal is an offer than can ripen into a contract if it is accepted.\nK calls s on the telephone and tell S that its bid is satisfactory and that a written contract will follow. There is a tentative manifestation of assent. It is not sufficiently complete to form a contract. K has indicated that it does not intend to be bound until a written contract is signed.\nS receives a signed contract from K. K draws a line through part of it and initials the change, signs the form and returns a signed copy to K. Still no K. When K presented a signed form to S, it was in effect an offer to enter into a contract. But instead of accepting the offer unconditionally, S drew a line through part of the form and signed it as changed. Conditional acceptance amounts to a counter-offer. A counter-offer cannot ripen into a contract until it is accepted.\nNext K without initialing the change -places a signed copy of the contract in its file, and sends S a notice to start work. S installs the ground work.\nNow there is a contract–but the questions is what contract? K by conduct has indicated an intention to be bound to the subcontract,–but K’s action in ordering s to go work is still ambiguous, that action is not specifically referable either the contract originally drawn up by K or to the contract as modified by S.\nIn order to find out whether the contract includes the modification the court will look for some conduct of the parties that is specifically referable to the modification. That is some conduct to show that the parties agreed to the modification or that they did not.\nSuppose the original contract provided s would be paid 30 percent of the contract price when the groundwork was installed. S cross out 30 percent and wrote in 40 percent. And substituted 10 percent on final completion instead of 20 percent. S then sent a bill for 40 percent on completion of the groundwork and K paid the bill. The action of K in paying the bill would be consistent only with the theory that K had accepted the offer as modified by S.\nHowever if K did not pay the bill on the ground that there was no agreement to change the percentages. Then there would be no contract. But that does not mean that S would not be paid for its work–to prevent K from being unjustly enriched, the court would require to have K pay the reasonable value, as opposed to the bid price, of the work performed.\n13. Reading the contract. Sometimes a person tries to avoid liability under a contract by failing or stating that he or she failed to read the contract. Liability is not avoided. If you have the capacity to read and understand the contract, in the absence of fraud or imposition, you are bound by the terms of the contents and are estopped to claim that the explicit provisions are contrary to intention or understanding.\n14. Consideration–we know a contract is a promise that the law will enforce. But the law will not enforce all promises. For example, I promise to take my son fishing sometime or make a gift of my watch–they are not legally enforceable promises because there is no consideration. A promisee is not entitled to insist that a promisor perform its promise unless the promise has given something up in return. This thing that is given up is known as consideration.\nFor example, if I offer to paint you house for $5000 and you accept my office a contract arises. The consideration passing from me to you is the painting of the house and the consideration from you to me is the payment for $5000.\nThere must be consideration on both sides.\nConsider the situation: P has finished a building for O. O says he can’t pay the balance and asks K to wait six months for payment and P says he agrees. Instead of keeping his promise P sues O. Is there a breach of contract by P? No P’s promise to wait six months was not supported by consideration from O. O gave nothing more up for the forbearance–not a watch for security not additional interest. O gave up nothing. Hence no consideration and no enforceable forbearance contract.\nA contract must be legal to be enforced. P was to be paid money to paint O’s house. P forgot to renew his license for $25 halfway through the painting of the house. It was illegal to act as a contractor without a valid license. On P’s part. Therefore, any monies paid by O were recoverable if he paid them and O would not have to pay the balance of the monies due if he didn’t pay P. This is because for P to require payment was illegal due to the fact the statute required P to have a valid license. P didn’t pay the required $25 to renew his license. Tough luck and love.\nAn improperly written contract can cost you. Have your contract reviewed by filling in the form below.', ""Breach of Contract and Non-Performance in Arizona\nA contract is a legally binding agreement between two parties, where one party offers to do something—for example, build a new house—and the other party accepts the offer and promises to do something in exchange—for example, pay money.\nIf one party fails to perform their obligations under an agreement without a reasonable excuse, the other party may be able to sue them for breach of contract.\nTo do this, the contract must be valid—it must be legal and both parties must have had the capacity to enter into it. A contract is also usually written, although a breach of contract can apply to an oral agreement in certain situations.\nWhere a party sues for breach of contract, the party that fails to perform their obligations is called the defendant and the wronged party is called the plaintiff.\nTypes of Breaches of Contract in Arizona\nNot all breaches are alike, and as such, not all breaches may be resolved through litigation. The law categorizes breaches of contracts based on their seriousness and timing.\nMaterial vs Minor Breach\nMaterial breaches occur when a defendant fails to perform an important obligation under the contract and that causes the contract to be “irreparably broken.” A material breach makes it impossible for the other party to perform their obligations under the contract.\nSome contracts explicitly define a material breach of the agreement. If the contract is silent on the issue, a court looks at factors like whether or not the defendant acted in bad faith and whether the plaintiff can be compensated for their loss. An example of a material breach is when a buyer fails to pay for a product they received.\nIn comparison, a minor breach does not materially disadvantage the other party and the contract can still be substantially performed. For example, the delivery of a product one day late does not go to the core of the agreement and is likely a minor breach.\nActual vs Anticipatory Breach\nAn actual breach occurs when one party fails to perform a term under the contract.\nAnticipatory breaches, on the other hand, occur when one party notifies the other that they will not comply with a term of the contract. This allows the other party to claim a breach of contract, even though it has not yet happened.\nRemedies in Arizona for Breaches of Contract\nRemedies exist to resolve contract breaches. What those remedies might be will depend on the facts and circumstances. More than one remedy may apply in your unique case. Potential remedies for a breach of contract include but are not necessarily limited to:\n- Compensatory damages, which are the most common type of remedy for breach of contract, covering the actual loss caused to the non-breaching party – these damages can be viewed in terms of (1) expectation damages where the non-breaching party recovers monetary damages based on the contract itself or the market value; and (2) consequential damages where the non-breaching party recovers indirect damages for reasonably foreseeable harm resulting from the breach\n- Punitive damages are the least common type of remedy for breach of contract but when awarded, are meant to punish the breaching party who acted fraudulently, maliciously, or willfully when breaching the contract and to prevent the same type of breaches in the future\n- Specific performance, which is the result of a court order requiring a defendant to perform an obligation under the contract\n- An injunction, which is a court order requiring a defendant not to do something, either temporarily or permanently\n- Rescission, which occurs when the court cancels the parties' obligations under the contract so that, theoretically at least, they are in the same position as before the contract was made\n- Restitution, which is a court order requiring the defendant to pay the plaintiff the amount agreed to in the contract – often involving the return of money or property\nAgain, the remedies available in your case depend on the jurisdiction, type of breach, specific terms of the contract, and circumstances in which the contract was made.\nPossible Defenses to Breaches of Contract\nIt is important to know the possible defenses a business or consumer may have when they breach a contract. If you are the party to breach the contract, make sure to speak with an attorney about how any one of these or other defenses may apply in your case.\nIf you are the non-breaching party, knowing these defenses can help you understand how the other party may argue their case. This knowledge will provide insight on how to move forward to uphold the terms and conditions of the contract or to come to another resolution, even if that means litigation.\nDefenses available to a party accused of breaching a contract include but are not limited to:\n- Fraud, where the plaintiff intentionally misrepresented material facts about the contract, making it fraudulent and therefore invalid\n- Invalid contract, where the contract does not meet the criteria of a valid contract\n- Duress, where the breaching party was forced to enter into the contract against their will\n- Incapacity, where the breaching party did not have the necessary capacity to enter into the contract in the first place – for example, because they were a minor at the time\n- Unclean hands, where both parties are at fault for the breach of contract\n- Mutual mistake, where the contract contains a mistake, different from what both parties intended\n- Illegality, where the contract was an agreement to do something illegal – for example, commit a murder\nRegardless of whether you are the breaching or non-breaching party, you should speak to an experienced contract attorney to see what remedies or defenses may apply to your unique situation. Call us at 480-771-3986 to schedule a Consultation.""]"	['<urn:uuid:63faf00f-3a93-44b5-9bef-86edd0b00af4>', '<urn:uuid:45566779-157f-457e-aa93-e4437d7d325b>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	16	108	3059
6	Hi, I'm researching workplace accidents. Which incident resulted in a longer absence from work - the MTI welder's finger injury or the Swissport worker's spinal injury from falling off the truck?	The MTI welder was off work for three weeks after losing two fingers, while the Swissport worker who suffered spinal injuries was unable to work for eight weeks after falling from the flatbed truck.	['A Dudley welding machine manufacturer has been prosecuted after a catalogue of safety failings led to a worker losing two fingers.\nSelf-employed electrical contractor Ian Mowbray was working at MTI Welding Technologies Ltd when the incident happened on 23 August 2013.\nDudley Magistrates’ Court heard that the 49-year-old from Wolverhampton was trying to rectify a loading problem on a high-friction welding machine when he pressed an incorrect button, closing the powerful hydraulic holding fixture on to his left hand.\nHis middle and ring fingers were so badly crushed they had to be amputated in hospital. He was off work for three weeks and has since returned to the company.\nThe court was told MTI Technologies failed to report the incident to the Health and Safety Executive (HSE). It came to light four weeks later when HSE received an anonymous complaint which led to an unannounced inspection.\nThe investigation found the company had acquired the machine from another firm that had modified it from a safe, automatically-loaded one. The modifications had defeated safety interlocks and introduced manual controls so it could be loaded by an operator within the safety enclosure rather than the robot and automation provided.\nOver-riding the interlocks meant the door to the safety enclosure could be opened without stopping the machine, giving workers access while machinery was operating. The manual buttons were located dangerously close to the fixture, allowing operators to initiate powered machine movements while still within the danger zone.\nNo secondary guarding had been fitted to protect the operator during manual operations and no emergency stops were provided at the manual operating position.\nMTI Welding Technologies continued to use it in its dangerous manual mode without assessing the risks.\nDuring the HSE visit a radial arm drill was running without a suitably maintained safety switch, leaving operators inadequately protected. A Prohibition Notice was issued banning its use with immediate effect until the fault was repaired.\nMTI Welding Technologies Ltd, of First Avenue, Pensnett Estate, Kingswinford, Dudley, was today (2 July) fined £53,000 and ordered to pay costs of £3,100 after pleading guilty to breaching the Health and Safety at Work etc Act 1974, the Reporting of Injuries, Diseases and Dangerous Occurrences Regulations 1995 and the Provision and Use of Work Equipment Regulations 1998.\nAfter the hearing, HSE inspector John Glynn said:\n“The company builds and sells its welding machines to major manufacturing companies worldwide. As original manufacturers they are fully aware of the legal requirements to supply machines with all the required safety measures. They were therefore grossly negligent to allow the use of this machine within their own premises in its modified state.\n“MTI Welding Technologies Ltd entirely failed to consider the risk to workers while engaged in manual operations. Had they done so a man would not have suffered a serious, painful injury.”\nNotes to Editors\n1. The Health and Safety Executive is Britain’s national regulator for workplace health and safety. It aims to reduce work-related death, injury and ill health. It does so through research, information and advice; promoting training; new or revised regulations and codes of practice; and working with local authority partners by inspection, investigation and enforcement. www.hse.gov.uk\n2. Section 3(1) of the Health and Safety at Work etc. Act 1974 states: “It shall be the duty of every employer to conduct his undertaking in such a way as to ensure, so far as is reasonably practicable, that persons not in his employment who may be affected thereby are not thereby exposed to risks to their health or safety.”\n3. Regulation 11 of the Provision and Use of Work Equipment Regulations 1998 states: “Every employer shall ensure that measures are which are effective to prevent access to any dangerous part of machinery or to any rotating stock-bar; or to stop the movement of any dangerous part of machinery or rotating stock-bar before any part of a person enters a danger zone.”\n4. Regulation 3(2) of the Reporting of Injuries, Diseases and Dangerous Occurrences Regulations 1995 states: “Where a person at work is incapacitated for work of a kind which he might reasonably be expected to do, either under his contract of employment, or, if there is no such contract, in the normal course of his work, for more than three consecutive days (excluding the day of the accident but including any days which would not have been working days) because of an injury resulting from an accident arising out of or in connection with work (other than one reportable under paragraph (1)), the responsible person shall as soon as practicable and, in any event, within 10 days of the accident send a report thereof to the relevant enforcing authority on a form approved for the purposes of this regulation, unless within that period he makes a report thereof to the Executive by some other means so approved.”', 'Working at height – airport firm’s £500,000 fine\nA company has been fined after previously being served with improvement notices after multiple accidents involving transport and work at height. What could it have done to avoid this situation?\nIn June 2015 employees of Swissport GB Ltd (S) were tasked with unloading baggage from a commercial aircraft. The bags were stacked on to the back of a flatbed truck. Once they had finished unloading, the team leader told the truck driver to take the load to the terminal building.\nAs the driver prepared to depart he checked his mirrors but due to his view being obstructed, he was unable to see the team leader who was still standing on the back of the truck. When the vehicle moved forward the worker fell to the ground causing spinal injuries which left him unable to work for eight weeks.\nFollowing the incident, an investigation carried out by the HSE discovered that the workers had not followed company procedure. They had obstructed the driver’s view by stacking the luggage above a marked height restriction and this was a direct cause of the accident. To comply with safety procedures a second vehicle would have been required.\nIn both 2007 and 2010 the HSE had served S with two improvement notices following a couple of incidents which had also involved falls from flatbed trucks.\nIn September 2015, a night worker was undergoing training on unloading containers from a cargo aircraft. While climbing an access ladder he slipped on the wet metal rungs and fell approximately two metres, suffering an injury to his right foot.\nS had not properly planned the work at height and the worker had not used the correct access route.\nFurther investigation discovered this method of work was routinely carried out and the ladder could be extended to a height of nine metres. Therefore, the injury had the potential to be much worse.\nS pleaded guilty to breaching the Health and Safety at Work etc. Act 1974 for the worker falling from the flatbed truck and the Work at Height Regulations 2005 for the incident involving the ladder. The total fine was £502,000.\nFor both accidents the HSE felt that safety procedures were ignored due to time pressures.\nTip 1.In an environment where there are high penalties for missed deadlines, staff will always seek shortcuts. If you’re in this situation, put additional investment into practical safety solutions which workers cannot, or are less tempted to, circumvent.\nTip 2.Use supervision to enforce safety procedures which may be ignored.\nTip 3.Training in your rules and procedures should not only target staff but also managers and supervisors. They must be made aware of the potential consequences of non-compliance.\nAccidents involving falls from trucks and ladders occurred when safety procedures were not followed. If your workers are pressured to meet deadlines, anticipate that they will look for shortcuts. In these environments find control measures which cannot be side-stepped and use a high level of supervision to enforce rules.']	['<urn:uuid:f5ea755a-75e4-47c9-8925-211db67fda2b>', '<urn:uuid:9af4c65d-5436-4f24-a8e2-d13b27709f5b>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	31	34	1307
7	ballet historian wondering how long dancer must train before starting pointe shoes	A dancer must have trained for about six years before beginning pointe work, as they need time to achieve the proper form, strength and alignment needed to make a successful transition into pointe work without risk of injury.	"['Pointe shoes for black, Asian and minority ethnic ballet dancers are being made in the UK for the first time, in what is being hailed as a landmark event in British ballet. London-based Ballet Black,\nDance History and Facts. Famous Ballet/Pointe Dancers. ~Anna Pavlova: Russian Ballet Dancer and created the modern pointe shoe. ~Mikhail Baryshnikov: Russian Ballet Dancer and has had roles in film. ~Rudolf Nureyev: Regarded to as the greatest ballet dancer of the 1960s and 1970s.\nBut some dancers have found another purpose for the formula—it also doubles as paint for their pink pointe shoes. Ingrid Silva. a black soloist for the Royal Ballet, made headlines when he posted.\n""Pointe shoes make ballet dancing look magical and even daring. They create an illusion. No one knows for sure who invented the first modern pointe shoes.\nSummary Of Poem The Tyger By William Blake Summary of ‘The Lamb’. Little Lamb, God bless thee! Little Lamb, God bless thee!’ This second stanza supplies the answer proposed in the first stanza. Blake describes the Lord Jesus Christ as the creator of the lamb. Since Jesus is often called the ‘Lamb of God,’ the symbolism of the animal chosen in the poem\nThe principals for Scottish Ballet’s production of Cinderella will use a new pair of custom-made pointe shoes every night during their tour. Kelly Apter explores the relationship between the dancers.\nYou cannot begin a ballet career in pointe shoes. In order to be able to dance on pointe, a dancer must have had time to achieve the form, strength and alignment needed to make a successful transition into pointe work. Proper technique is required to be able to properly rise on the toes without risk of injury.\nThere’s still much progress to be made on diversity in the classical-dance realm — a world that’s historically been so white that dance-shoe manufacturers are only just now beginning to make pointe.\nWho Is Nobel Laureate The word ""Laureate"" refers to being signified by the ""Laurel Wreath"". In Greek mythology, god Apollo wears a laurel wreath on his head. A laurel wreath is a. The laureates receive them at an elegant ceremony in Stockholm on Dec. 10 the anniversary of Nobel’s death in 1896 together. Oct 9, 2012. For most of\nIn the early 19th Century, a big change came to ballet because of the ballet pump , which was the first type of ballet shoe – before that people used hard shoes.\nNov 13, 2018. There\'s a new tool that lets amputee ballet dancers perform on pointe. His invention isn\'t the only new option recently developed for dancers. They are expected to go from sneakers, to pointe shoes, to character shoes,\nRadio City Music Hall Rockettes Seating Chart And while the 2018 Video Music Awards, held Monday night at Radio City Music Hall, didn’t have any Internet. for rocketing Travis Scott to No. 1 on the charts, beating her own new record, plenty of. Six-time Grammy Awards winner and Country Music Hall of Fame. party and VIP seating on Friday and Saturday night.\nOct 31, 2017 · So you’ve been approved for pointe work and have just bought your first pair of pointe shoes. Congratulations! You’re living every tiny ballerina’s dream, except for one problem… they don’t stay on your feet! You quickly discovered that pointe shoes are pretty much useless without the ribbons and elastics attached.\nBallet : Charles Didelot and pointe shoes By the time of the French Revolution (1789-1799) ballet dancers exchanged heeled shoes and heavy costumes for lighter, flat-soled slippers, pointe shoes, and flesh-colored tights.\nKelly Ripa : talk show host, mother — and ballet dancer! On Tuesday, the Live with Kelly and Ryan host shared a very impressive photo of herself wearing some satin pointe shoes and balancing on her.\nOct 04, 2011 · How Ballet Pointe Shoes are made and unmade You probably know by now that I love tutus; long, short, bell shaped, pancake or not, you name it, but a classical ballerina is not re Designer tutus on the catwalk for auction at English National Ballet’s Fundraising Summer Party\n* Prep your pointe shoes with pre-packaged ribbon and elastic * Wide, single faced satin ribbon and elastic cut to length needed for pointe shoes * Two yards of 7/8 inch ribbon per set * 18 inches of ¾ inch elastic per set * One set per package * Made in USA\nOct 1, 2012. And A Ballet Shoe Inventor. When did you have time to invent a new type of ballet shoe?. The toe shoe has not changed in centuries.\nBetween hours of class, pointe shoes, and stamping it out in tap. The band also comes with a separate mesh bag, so it’s easy to keep in your dance bag without getting tangled up in all your dance.\nJul 01, 2019 · 10 odd facts about pointe shoes July 1, 2019 November 17, 2014 by admin Back in the late 18th century, the ballet world decided that female dancers should look ethereal, sylph-like, as.\nJan 4, 2019. The ballet is a premier of Justin Peck\'s \'The Times Are Racing\'; a ballet. \'Of course [pointe shoes] were invented by a man as a torture device.\nif you wanted to know what feet look like inside pointe shoes, well here they are. Ballerina Feet – Life with pointe shoes. Dancers Feet, Ballet Feet, Ballet.\nThe blocked pointe shoe with a shanked sole did not evolve until much later. August Bournonville invented the ""Bournonville slipper"" for male dancer which is.\nSix days a week, the willowy 78-year-old Dallas ballet instructor laces up her pointe shoes and teaches classical ballet. ""I put on my tunic, I made my hair neat; ""I learned how to stand and turn.\nA possible answer is, pointe shoes were custom made by a cobbler according to the wishes of the dancer. It does not seem that pointe shoes were ""invented"" by a manufacturer and bought by dancers in retail, but rather an experiment of trial and error, continually refined and reworked, as the art form was being created. MARIA TAGLIONI. Pointe.\nOct 26, 2018. Dancers from minority ethnic backgrounds can now get pointe shoes in both bronze and brown instead of traditional pink to match their skin.\nWho Is Author Edward Klein Meet Dr. Klein Learn from Edward A. Klein how advanced surgical techniques in total hip replacement are helping patients recover more quickly than before. Established in 1970, Hawthorn Medical is the largest multi-specialty medical group practice in greater New Bedford. According to what a friend of Princess Beatrice’s told journalist, Edward Klein in 2011, the\nAug 25, 2017. Onstage, ballet has captivated audiences for centuries; it should come as no. and her friend Victor (Nat Wolff), who dreams of being an inventor. dramatic emphasis as dancers lacing up their pointe shoes, the film rolls.\nThe whole point of training ballet is to get pointe shoes. So, before you even attempt pointe, you probably should know what a pointe shoe is… This isn’t the history of pointe… This is the anatomy of a pointe shoe 🙂 Anatomy of a Pointe Shoe. A pointe shoe has to be designed for the masses, and, as a result, the pointe shoe industry has.\nSix days a week, the willowy 78-year-old Dallas ballet instructor laces up her pointe shoes and teaches classical ballet. “I put on my tunic, I made my hair neat; “I learned how to stand and turn.\nshe made a decision. From dance, she redirected her creative talents to design and teaching. She opened her own ballet studio, then created her own line of dancewear. Now, her latest venture is.\nSurprising to say, it was a woman who invented. 20th-century pointe shoes. Pavlova was so self-conscious about her wide toe boxes that she’d have her shoes airbrushed in publicity shots to give the.\nJul 13, 2019. Pointe shoes were invented to create the look of a dancer being ethereal. Take regular ballet classes for a good six years before you even.\n(Phil Hossack / Winnipeg Free Press) La Bayadère is also an all-pointe-shoes-on-deck affair. As Horsman has pointed out, when the ballet first made its première in 1877, India had been influenced.\nAug 05, 2018 · The satin “pointe shoes” worn by performers today have become synonymous with the art of classical dance, even though ballet was established some 200 years before them. Join us in a brief history of the evolution of pointe shoes below:\nHistory [change | change source] When Marie Taglioni first danced La Sylphide en pointe, her shoes were nothing more than modified satin slippers. Because the shoes of this period offered no support, dancers would pad their toes for comfort and rely on the strength of their feet and ankles for support.\nWho invented pointe shoes? Though ""toe dancing"" was popular in London as early as the 1820s, it is believed that the first ballet dancer to dance en pointe with.\nRichmond Ballet’s Maggie Small is set to reprise her role as the. was meant to — the wear and tear on her own body has forced her out of her tutu and pointe shoes. But Small’s not planning to go.\nMar 11, 2016. The flesh-coloured ballet shoes we see now with satin and ribbons date back to the 1820s when pointe shoes were created, says Anna.\n""You immediately fall in love with them"" – Katharina Unger, Ballet Student, Germany. The Pavlova pointe shoes by Siberian Swan is a great fit for dancers with. of the brand, you know that these pointe shoes were invented by dancers for.\nWhile being fit for my first pair of pointe shoes, what can I do to ensure that it is an easy. While wearing pre-pointe shoes, the dancer can take ballet class in them without. A foot stretcher is a clever device that was invented to help with the.\nHow a Pointe Shoe Works Pointe shoes enable the dancer to balance, spin, hop, pounce, slide, and linger on the tips of her toes. Before the advent of the modern reinforced pointe shoe, around 1900, ballerinas wore soft slippers and could not perform the steps, turns, and sustained balances on pointe that we expect of today’s dancers.\nA pointe shoe is a type of shoe worn by ballet dancers when performing pointe. The first dancers to rise up on their toes did so with the help of an invention by.\nThe hope is that the progress made by these two companies will lead to larger change in the dance world, as other pointe shoe makers will realize there is a market for dancers of color. Currently,\nDancers would darn the toe of their satin slippers with strong tough thread, because block toed shoes had not been invented yet. However, dancers only stood on pointe for a moment or two. Women dominated the Romantic Period.\nNo one really knows who invented the first pointe shoes. A possible answer is, pointe shoes were custom made by a cobbler according to the wishes of the dancer. It does not seem that pointe shoes were ""invented"" by a manufacturer and bought by dancers in retail, but rather an experiment of trial and error, continually refined and reworked, as the art form was being created. MARIA TAGLIONI\nNov 10, 2018. For just as long, the tights and pointe shoes that have given ballet. As The New York Times reports, pointe shoes were invented around 1820,\nOct 20, 2013 · Stronger shoes Ballet shoemakers started to make harder shoes for pointe work. Harder shanks were introduced with reinforced toe boxes to make the platform bigger. Marius Petipa made good use of the new shoes in his complex ‘pouncing‘ routines. 16. Michel Fokine (1880 – 1942) The first choreographer of the Ballet Russes.\nShort Funny Poems About Women Young poets are on Tumblr and Twitter, composing affecting and funny verse as short as a hundred and forty. Of this new genre of poets, Warsan Shire, a twenty-six-year-old Somali-British woman, is. Whether you’re having a Valentine’s Day get-together tonight or another day this week, you may need some quotes or little poems for messages.\nTo become world record holders, Ripa, co-host Ryan Seacrest, and more than 200 others gathered on set at Live with Kelly and Ryan and held ballet en pointe for a full minute. Though they may have made.\n4, Milwaukee Ballet Day as 2,500 yards of ribbon and 100 pointe shoes were displayed around the city. The Baumgartner Center for Dance was named after Donna and Donald Baumgartner, who made the.\nPointe Shoes. Several ballerinas also began experimenting with dancing en pointe, bringing pointe shoes to ballet in the early 1820s. In 1832, La Sylphide was created to showcase Marie Taglioni’s talents dancing en pointe. This was significant because up until then, pointe shoes were seen as a type of ungraceful stunt.\nBallet Pointe Shoes are a key part of a ballet dancer’s journey and a moment every young ballerina hopes to reach. It takes practice, patience and persistence to dance effortlessly on pointe, but it.\nMar 11, 2015. I\'ve been dancing in pointe shoes for just over a year now. Whenever I tell people that I take ballet–and, more specifically, that I dance in pointe.\nJul 02, 2009 · The fascination with pointe shoes and dreaming of the day you’ll actually get your first pair is surely part and parcel of growing up in ballet. Every young ballerina looks forward to this rite of passage. I remember when I was fitted for my first pair of Capezio pointe shoes at Kinney Dancewear in Indianapolis, Indiana–by Frank Kinney himself, no less!\nThe world\'s leading ballerina, Anna Pavlova, purchased Capezio pointe shoes for herself and her entire company during her first tour of the United States.\nThe production’s tagline “Think ballet is all pointe shoes and tutus? Think again” seemed to assume. Teenage ballet dancer Haruo Niyama made headlines across Japanese news programs when he won the.\n(Pointe shoes differ from regular ballet shoes because they include a block in front. But because those shoes are made of elastomeric plastic rather than the traditional paper-and-glue combo, not.\nPointe ballet shoes in colours that match Asian and black skin tones are being made for the first time in the UK. For women of colour, practising ballet used to mean having to paint pointe shoes to.']"	['<urn:uuid:ecfa87ef-bb0c-427d-9723-b98c74fb79d7>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	12	38	2396
8	I read that Argentina and other countries work together on space research - what kind of partnership do they have with Europe for space exploration?	Argentina and ESA (European Space Agency) have maintained a strong space partnership since 1997, when they first signed an agreement for Argentina to use data from European Earth observation missions. In 2002, they signed a broader cooperation agreement valid until 2023, under which they organize courses and grants for Argentine students and workshops on Earth observation applications. As part of their collaboration, when ESA built its deep-space station in Malargüe in 2012, they agreed that 10% of the station's time would be available to Argentine scientists and engineers for radio spectrum research, providing unique opportunities for regional research teams.	['ESA boosting its Argentine link with deep space\nThanks to some high-tech improvements, ESA’s radio dish in Argentina will be ready to receive the rising torrent of scientific data beamed back by future missions exploring deep in our Solar System.\nSince 2012, ESA’s deep-space tracking station at Malargüe, about 1200 km west of Buenos Aires, Argentina, has provided critical links to some of Europe’s most important missions, including ExoMars, Mars Express, Gaia and Rosetta.\nThe data-gathering capabilities of upcoming exploration missions is steadily increasing, however, and this means a 10-fold growth, each decade, in the amount of science data that must be downlinked from Mercury, the surface of Mars or the enigmatic moons circling Jupiter.\nTo cater for this need, ESA is investing in a series of significant upgrades for its Malargüe station, underscoring the Agency’s long and productive partnership with Argentina and that country’s strong involvement in space science.\nBoosting bitrates from deep space\nThe upgrades will be spread over two years and include a new main signal-processing system and the addition of a 26 GHz downlink that will enable high-speed data receipt from space.\n“This means our station at Malargüe will be able to download data from ESA’s future Euclid mission, for example, at 75 Mbit/s, many times faster than today,” says Michel Dugast, ESA’s station engineer and project manager for the upgrade.\n“It will also support cornerstone ESA missions like ExoMars 2020, BepiColombo and Juice, as well as partner missions from Russia, the US and Japan, among others.”\nThe 18 months of work, valued at about €4 million, will start in May.\nSignals from Saturn\nIn addition to its routine communication tasks, ESA is using the Malargüe dish this year to catch signals from the international Cassini spacecraft operated by NASA at Saturn, more than 1.4 billion km away, some of the most-distant radio link-ups ever achieved by European stations.\nMalargüe station is also playing a critical role in this year’s ‘aerobraking’ by the ExoMars Trace Gas Orbiter at Mars. Aerobraking uses the Red Planet’s faint, wispy upper atmosphere to slow the orbiter, allowing it to attain its final science orbit using very little fuel.\nExtending global ties\nThe investment was announced today during a visit to Malargüe station by Agustín Campero, Argentina’s Secretary of Scientific and Technological Articulation, who was briefed by senior ESA managers including the Director of Operations, Rolf Densing.\n“Argentina is not only the host country for our tracking station, it is an important and valued partner in European and South American space development,” said Mr Densing.\n“I am especially pleased to see this new investment, which cements the country’s role as a crucial partner in ESA’s future exploration missions, starting in the 20th anniversary year since the inception of the formal cooperation agreement between ESA and Argentina.”\nTwenty years of Argentina and ESA in space\nIn 1997, ESA and Argentina signed an agreement for Argentine use of data from the Agency’s ERS-1 and ERS-2 Earth observation missions. This ensured access to a vast repository of information about Earth’s land, water, ice and atmosphere until 2011.\nIn 2002, a more general cooperation agreement was signed, in effect until 2023.\nUnder this, ESA and Argentina’s National Commission for Space Activities (CONAE) have organised courses and grants for Argentine students as well as workshops in South America studying applications for Earth observation data, particularly in hydrology, natural disaster monitoring and radar applications.\nIn 2012, after considering several dozen possible sites, ESA selected Malargüe to build its third deep-space station, which joined sites in Australia and Spain to provide the global coverage that helps ensure Europe’s independent capability to explore space.\nAs part of the implementing arrangement with Argentina, 10% of the station’s time is available to Argentine scientists and engineers conducting research using the radio spectrum. This provides a unique opportunity to regional research teams.\nWorld heritage location\nCommunicating across billions of kilometres means that the Malargüe station has very stringent technical, geographic and environmental requirements.\nThese include a stable terrain, an arid climate and the absence of radio interference – which can be caused by mobile phone networks – as well as access to power, data and logistics links.\nThe station is ideally situated in the Parque Científico Provincial de Observación del Espacio Malargüe (Malargüe Provincial Scientific Park for Space Observation), a largely unspoilt, world-class scientific, geologic and natural region. The park is part of a larger area – la Payunia – that is under consideration for designation as a UNESCO World Heritage site.']	['<urn:uuid:7b0338fa-4889-4983-8754-f485928cd8ce>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	25	99	750
9	What methods are used to reach agreement among experts about health guidelines, and how is shoulder load being assessed in mobility device research?	Expert consensus is reached through iterative processes like the Delphi technique, which combines expert opinions through anonymous rounds until group agreement is achieved. For shoulder load assessment, researchers use measurements in motion laboratories to compare different mobility devices - for example, studies have shown that handcycling produces much lower shoulder compression forces compared to wheelchair propulsion, as demonstrated through glenohumeral contact force measurements.	[') survey software, and then exported into Microsoft Excel for analysis (Version 16.0.10325.20082, Microsoft Office Professional 2016; Redmond, WA). Design The Delphi technique is an iterative process, designed to combine expert opinion, in order to arrive at a group consensus ( Hsu & Sandford, 2007\nRichard J. Keegan, Lisa M. Barnett, Dean A. Dudley, Richard D. Telford, David R. Lubans, Anna S. Bryant, William M. Roberts, Philip J. Morgan, Natasha K. Schranz, Juanita R. Weissensteiner, Stewart A. Vella, Jo Salmon, Jenny Ziviani, Anthony D. Okely, Nalda Wainwright and John R. Evans\nMichael Kellmann, Maurizio Bertollo, Laurent Bosquet, Michel Brink, Aaron J. Coutts, Rob Duffield, Daniel Erlacher, Shona L. Halson, Anne Hecksteden, Jahan Heidari, K. Wolfgang Kallus, Romain Meeusen, Iñigo Mujika, Claudio Robazza, Sabrina Skorski, Ranel Venter and Jürgen Beckmann\n: 133 – 145 . 7. Meeusen R , Duclos M , Foster C , et al . Prevention, diagnosis, and treatment of the overtraining syndrome: joint consensus statement of the European College of Sport Science and the American College of Sports Medicine . Med Sci Sports Exerc . 2013 ; 45 : 186 – 205 . PubMed\nTricia Majewski-Schrage, Todd A. Evans and Brian Ragan\nDespite widespread acceptance, there is currently no consensus on the definition, components, and the specific techniques most appropriate to measure and quantify core stability.\nTo develop a comprehensive core-stability model addressing its definition, components, and assessment techniques.\n15 content experts from United States and Canada, representing a variety of disciplines.\nMain Outcome Measure:\nThe authors distributed an open-ended questionnaire pertaining to a core-stability definition, components, and assessment techniques specific to each expert. They collected data over 2 rounds of telephone interviews. They concluded data collection once a consensus was achieved that equated with 51% agreement among respondents.\nThe authors developed a working definition of core stability as the ability to achieve and sustain control of the trunk region at rest and during precise movement. Eighty-three percent of the experts considered the definition satisfactory. Therefore, the definition was accepted. Furthermore, the experts agreed that muscles (14/15 = 93.3%) and neuromuscular control (8/12 = 66.7%) were components of core stability. Assessment techniques were identified and inconsistencies were highlighted; however, no consensus was established.\nA consensus core-stability definition was created and 2 components were identified. However, of the initial definitions provided by the experts, no 2 were identical, which revealed the inconsistencies among experts and the importance of this study. Nonetheless, the goal of obtaining a consensus definition was obtained. Although a consensus for the assessment techniques of core stability could not be reached, it was a beneficial starting point to identify the inconsistencies that were discovered among the content experts.\nNicholas Gilson, Wendy J. Brown, Guy Faulkner, Jim McKenna, Marie Murphy, Andy Pringle, Karin Proper, Anna Puig-Ribera and Aphroditi Stathi\nThis paper aimed to use the Delphi technique to develop a consensus framework for a multinational, workplace walking intervention.\nIdeas were gathered and ranked from eight recognized and emerging experts in the fields of physical activity and health, from universities in Australia, Canada, England, the Netherlands, Northern Ireland, and Spain. Members of the panel were asked to consider the key characteristics of a successful campus walking intervention. Consensus was reached by an inductive, content analytic approach, conducted through an anonymous, three-round, e-mail process.\nThe resulting framework consisted of three interlinking themes defined as “design, implementation, and evaluation.” Top-ranked subitems in these themes included the need to generate research capacity (design), to respond to group needs through different walking approaches (implementation), and to undertake physical activity assessment (evaluation). Themes were set within an underpinning domain, referred to as the “institution” and sites are currently engaging with subitems in this domain, to provide sustainable interventions that refect the practicalities of local contexts and needs.\nFindings provide a unique framework for designing, implementing, and evaluating walking projects in universities and highlight the value of adopting the Delphi technique for planning international, multisite health initiatives.\nErik A. Wikstrom, Cole Mueller and Mary Spencer Cain\nLAS. However, there are numerous published expert opinion papers on RTS within the literature. Establishing agreement among such opinions could help to further clinical practice and research into this important area. Objective To determine if there was consensus among published expert opinions that\nJames F. Sallis and Kevin Patrick\nThe International Consensus Conference on Physical Activity Guidelines for Adolescents convened to review the effects of physical activity on the health of adolescents, to establish age-appropriate physical activity guidelines, and to consider how these guidelines might be implemented in primary health care settings. Thirty-four invited experts and representatives of scientific, medical, and governmental organizations established two main guidelines. First, all adolescents should be physically active daily or nearly every day as part of their lifestyles. Second, adolescents should engage in three or more sessions per week of activities that last 20 min or more and that require moderate to vigorous levels of exertion. Available data suggest that the vast majority of U.S. adolescents meet the first guideline, but only about two thirds of boys and one half of girls meet the second guideline. Physical activity has important effects on the health of adolescents, and the promotion of regular physical activity should be a priority for physicians and other health professionals.\nPitre C. Bourdon, Marco Cardinale, Andrew Murray, Paul Gastin, Michael Kellmann, Matthew C. Varley, Tim J. Gabbett, Aaron J. Coutts, Darren J. Burgess, Warren Gregson and N. Timothy Cable\nMonitoring the load placed on athletes in both training and competition has become a very hot topic in sport science. Both scientists and coaches routinely monitor training loads using multidisciplinary approaches, and the pursuit of the best methodologies to capture and interpret data has produced an exponential increase in empirical and applied research. Indeed, the field has developed with such speed in recent years that it has given rise to industries aimed at developing new and novel paradigms to allow us to precisely quantify the internal and external loads placed on athletes and to help protect them from injury and ill health. In February 2016, a conference on “Monitoring Athlete Training Loads—The Hows and the Whys” was convened in Doha, Qatar, which brought together experts from around the world to share their applied research and contemporary practices in this rapidly growing field and also to investigate where it may branch to in the future. This consensus statement brings together the key findings and recommendations from this conference in a shared conceptual framework for use by coaches, sport-science and -medicine staff, and other related professionals who have an interest in monitoring athlete training loads and serves to provide an outline on what athlete-load monitoring is and how it is being applied in research and practice, why load monitoring is important and what the underlying rationale and prospective goals of monitoring are, and where athlete-load monitoring is heading in the future.\nClaire-Marie Roberts and Jacky Forsyth\nhow to leverage charity challenges (e.g., race for life) as a gateway to more regular exercise ( McVinnie, Plateau, & Stevinson, 2018 ). Extended abstracts, providing a greater level of detail, are featured in this issue of the Women in Sport and Physical Activity Journal . Next Steps The consensus\nRonald J. Maughan, Louise M. Burke, Jiri Dvorak, D. Enette Larson-Meyer, Peter Peeling, Stuart M. Phillips, Eric S. Rawson, Neil P. Walsh, Ina Garthe, Hans Geyer, Romain Meeusen, Luc van Loon, Susan M. Shirreffs, Lawrence L. Spriet, Mark Stuart, Alan Vernec, Kevin Currell, Vidya M. Ali, Richard G.M. Budgett, Arne Ljungqvist, Margo Mountjoy, Yannis Pitsiladis, Torbjørn Soligard, Uğur Erdener and Lars Engebretsen\nbody systems ( Hossein-nezhad et al., 2013 ); many athletes are at risk of insufficiency at various times throughout the year ( Larson-Meyer & Willis, 2010 ) No consensus over the serum 25-hydroxyvitamin D concentration (the marker of vitamin D status) that defines deficiency, insufficiency\nNick Cavill, Stuart Biddle and James F. Sallis\nAn expert consensus development process was initiated to make public health recommendations regarding young people (5–18 years) and physical activity. Eight commissioned review papers were discussed at a meeting of over 50 academics and experts from a range of disciplines from the UK and overseas. Participants agreed on a consensus statement that summarized the research evidence and made two core recommendations. First, to optimize current and future health, all young people should participate in physical activity of at least moderate intensity for 1 hour per day. Young people who currently do little activity should participate in physical activity of at least moderate intensity for at least half an hour per day. The subsidiary recommendation is that, at least twice a week, some of these activities should help to enhance and maintain muscular strength and flexibility and bone health. A second aspect of the consensus process, which was based on extensive consultation, outlined the practical ways in which key organizations can work together to implement these recommendations. The resultant consensus statement provides a strong basis for the planning of future policies and programs to enhance young people’s participation in health-enhancing physical activity', 'Shoulder Health and Mobility\nDue to their upper extremity dependency in daily life, the mechanical loading on the shoulder in individuals with spinal cord injury (SCI) is frequent and high. This increases the risk of tissue overload and shoulder complaints, thereby affecting functioning, participation and quality of life. This is confirmed by results from the Swiss Spinal Cord Injury Cohort Study (SwiSCI).\nDue to its anatomical structure, the human shoulder enables a large range of motion while joint stability is mainly controlled by muscles, rendering the upper extremity vulnerable to complaints.\nIn order to diminish the shoulder complaints, we aim our research at:\n- studying underlying mechanisms of shoulder load, in our movement laboratory,\n- quantifying and relating load, capacity and tissue change,\n- development of methods in monitoring shoulder load in daily conditions.\nBased on increased insight in the relation of load, capacity and tissue change, injury prevention programs can be optimized, in an open and bilateral discussion with clinical experts.\nWe currently have a strong national and international collaboration with the Swiss Paraplegic Centre, the Vrije University in Amsterdam, University Medical Center in Groningen and are continuously expanding this collaboration with other research groups, clinical disciplines and the SCI population for their valuable input.\nWith these combined efforts we work towards a quality management system for upper extremity functioning, aiming at long-term preservation of shoulder health in SCI individuals.\nExamples of projects in relation to shoulder health:\n- Handcycling versus handrim wheelchair (completed project)\n- Muscle fatigue induced by wheelchair propulsion (current project)\n- Wearables & big data (future project)\nHandcycling versus handrim wheelchair propulsion\nSeveral factors are involved in the development of shoulder pain, but the repetitive load of wheelchair propulsion is seen as one of the major risk factors. However, mobility devices such as the wheelchair are important since they support people with spinal cord injury (SCI) to live an independent life. Therefore, alternative or additional mobility devices to the manual handrim wheelchair should be considered. Since upper extremity mobility is usually the only means of physical exercise for SCI individuals, a manual device is preferable to maintain regular physical activity.\nThe handbike is one of these alternative mobility devices. It is increasingly used for commuting, recreation and sports. Compared to handrim wheelchair propulsion, the usage of a handbike is physiologically (heart rate, breathing, energy expenditure) more efficient and less straining. Whether also the mechanical load at the shoulder is lower than during wheelchair propulsion has not been investigated.\nThe focus of this project was on the biophysical benefits of handcycling in order to investigate the general assumption that the handcycle is indeed a good device for the prevention of shoulder problems. Additionally, we studied the handcycle-user interface. This will allow for practical advice on how to best adjust the handcycle to its user in order to further reduce the load on the shoulder joint.\nThe figure below shows the difference in shoulder load between handcycling and wheelchair propulsion. Glenohumeral contact force is the force with which the shoulder joint is compressed during the propulsion. The compression is a result of the exerted force by the hand and of the contractions of all muscles around the shoulder to perform the required movement and to stabilize the shoulder joint while doing so. (For example, 10 Newton (N) is the force needed to hold a mass of 1 kg against gravity).\nFrom the graphs it can be concluded that the load on the shoulder is much lower for handcycling.\nMuscle fatigue caused by wheelchair propulsion\n30% to 70% of individuals with spinal cord injury (SCI) suffer from shoulder pain. This has an enormous impact on functioning, independence and quality of life. The treatment is often unsatisfying and the pain problem remains.\nEarly diagnosis and early intervention of shoulder problems can markedly reduce the impact of shoulder pain. Although the exact mechanism remains unclear, several known factors contribute to the development of shoulder pain.\nOne of these factors is the mechanical load on the shoulder in relation to its capacity, for instance: how much force is needed to propel a wheelchair, and what is the maximum propelling force an individual can deliver? The lower the maximum, the sooner muscles will be fatigued. Improving the capacity to a sufficient level will have a positive effect on the prevention of shoulder pain.\nIt is also known that movement patterns can change due to fatigue, by a change in the activation or coordination of muscles. A less optimal movement pattern during wheelchair propulsion, a weight relief lift or a transfer can increase the risk of shoulder pain. Movement patterns are measured in our motion lab.\nFinally, prolonged activities can also lead to tissue change, for instance thickness and appearance of biceps tendon. Such changes in tendon thickness can be observed and quantified by Ultrasound imaging, not directly during wheelchair propulsion, but shortly after a fatiguing exercise, or during a weight relief lift.\nHow these mentioned changes, due to fatigue, are related to wheelchair propulsion and mechanical load at the shoulder is not known. It is, however, required information for the further development of training and prevention programs.\nQuality management system of upper extremity functioningWearables & Big Data\nFundamental research is required to gain insight in working mechanisms of upper extremity function, for which controlled conditions in motion labs are essential.\nIn addition questionnaires are used to gather information on lived experience from larger groups of participants.\nBoth approaches aim at extracting information from real life, and after analysis and discussion, generalize conclusions back to real life. Both approaches are, however, also just snapshots of what happens in real-life.\nWith the rapid technological development in the past two decades a third approach comes into reach. A broad variety of wearable sensors and smartphones might enable the desired longitudinal monitoring of upper extremity functioning in real life, to obtain an overview of real life demands and clarify dose-response relations among:\n- physical activities,\n- biomechanical load on the upper extremity, up to the level of musculoskeletal modelling,\n- long-term effect on tissue change,\n- performance of function.\nBesides, these non-intervening, discrete and wearable devices can potentially also be used as feedback devices, during rehabilitation or after discharge in real life in order to:\n- optimize or maintain optimal wheelchair propulsion style,\n- indicate over- or underuse with respect to individual capacity,\n- support or coach in adherence to «healthy» performance.\nTo enable the development of such an upper extremity quality management system, we focus our research on:\n- development of methods and algorithms (e.g. to estimate biomechanical load in real life),\n- testing usability of equipment (e.g. Smart Watches, Internet of Things modules) to enable large scale measurement in real life,\n- creation of safe and adequate infrastructure (where to store and process data, who can access data).']	['<urn:uuid:59541b25-b7a8-4270-8555-d38e6a3df227>', '<urn:uuid:9075a208-866c-4118-a3af-98c04d46c992>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	23	63	2635
10	What was Bessie Coleman's role in integration and her relationship with segregation?	Bessie Coleman fought against segregation by refusing to perform at venues with segregated entrances, though she eventually compromised to allow segregated seating. Additionally, she was part of a broader movement of Black aviators who faced discrimination - the Tuskegee Airmen also fought against racism both at home and overseas, ultimately contributing to the military's racial integration through Executive Order 9981 in 1948.	"['This article appears courtesy of the National Women’s History Museum and was originally published on their website here.\nBessie Coleman soared across the sky as the first African American, and the first Native American woman pilot. Known for performing flying tricks, Coleman’s nicknames were; “Brave Bessie,” “Queen Bess,” and “The Only Race Aviatrix in the World.” Her goal was to encourage women and African Americans to reach their dreams. Unfortunately, her career ended with a tragic plane crash, but her life continues to inspire people around the world.\nBorn in Atlanta, Texas on January 26, 1892, Bessie Coleman had twelve brothers and sisters. Her mother, Susan Coleman, was an African American maid, and her father George Coleman was a sharecroppper of mixed Native American and African American descent. In 1901, her father decided to move back to Oklahoma to try to escape discrimination. Bessie’s mother decided not to go with him. Instead, the rest of the family stayed in Waxahachie, Texas. Bessie grew up helping her mother pick cotton and wash laundry to earn extra money. By the time she was eighteen, she saved enough money to attend the Colored Agricultural and Normal University (now Langston University) in Langston, Oklahoma. She dropped out of college after only one semester because she could not afford to attend.\nAt age 23, Coleman went to live with her brothers in Chicago. She went to the Burnham School of Beauty Culture in 1915 and became a manicurist in a local barbershop. Meanwhile, her brothers served in the military during World War I and came home with stories from their time in France. Her brother John teased her because French women were allowed to learn how to fly airplanes and Bessie could not. This made Bessie want to become a pilot. She applied to many flight schools across the country, but no school would take her because she was both African American and a woman. Famous African American newspaper publisher, Robert Abbott told her to move to France where she could learn how to fly. She began taking French classes at night because her application to flight schools needed to be written in French.\nFinally, Coleman was accepted at the Caudron Brothers’ School of Aviation in Le Crotoy, France. She received her international pilot’s license on June 15, 1921 from the Fédération Aéronautique Internationale. Coleman’s dream was to own a plane and to open her own flight school. She gave speeches and showed films of her air tricks in churches, theaters, and schools to earn money. She refused to speak anywhere that was segregated or discriminated against African Americans. In 1922, she performed the first public flight by an African American woman. She was famous for doing “loop-the-loops” and making the shape of an “8” in an airplane. People were fascinated by her performances and she became more popular both in the United States and in Europe. She toured the country giving flight lessons, performing in flight shows, and she encouraged African Americans and women to learn how to fly.\nOnly two years into her flight career, Coleman survived her first major airplane accident. In February of 1923, her airplane engine suddenly stopped working and she crashed. She was badly hurt in the accident and suffered a broken leg, a few cracked ribs, and cuts on her face. Thankfully, Coleman was able to fully heal from her injuries. This accident did not stop her from flying. She went back to performing dangerous air tricks in 1925. Her hard work helped her to save up enough money to purchase her own plane, a Jenny – JN-4 with an OX-5 engine. Soon she returned to her hometown in Texas to perform for a large crowd. Because Texas was still segregated, the managers planned to create two separate entrances for African Americans and white people to get into the stadium. Coleman refused to perform unless there was only one gate for everyone to use. After many meetings, the managers agreed to have one gate, but people would still have to sit in segregated sections of the stadium. She agreed to perform and became famous for standing up for her beliefs.\nOn April 30, 1926, Bessie Coleman took a test flight with a mechanic named William Wills. Wills was piloting the plane, as Coleman sat in the passenger seat. At about 3,000 feet in the air, a loose wrench got stuck in the engine of the aircraft. Wills was no longer able to control the steering wheel and the plane flipped over. Unfortunately, Coleman was not wearing a seatbelt. Airplanes at the time did not have a roof or any protection. Coleman immediately fell out of the open plane and died. Wills crashed the aircraft a few feet away from Coleman’s body and also died. Her death was heartbreaking for thousands of people. Famous activist Ida B. Wells-Barnett performed the funeral service to honor Coleman in Chicago. In 1931, the Challenger Pilots’ Association of Chicago started a tradition of flying over Coleman’s grave every year. By 1977, African American women pilots formed the Bessie Coleman Aviators Club. In 1995, the “Bessie Coleman Stamp” was made to remember all of her accomplishments.\nHere in Ellis County, we celebrate Bessie Coleman Day on January 26th, and we celebrate this native’s accomplishments here and abroad! There will be a proclamation held at the Historic Courthouse in downtown Waxahachie at 2pm honoring Bessie. The Ellis County African American Hall of Fame, Museum & Library will also be selling mugs honoring her to raise funds for continued education for Black History.\nTo read more fascinating articles about women, check out the National Women’s History Museum website here.', 'Black Americans, both men and women, have left their legacy in the skies over the United States and around the world. From commercial enterprises to war veterans, these brave men and women opened the aviation frontier to all that would follow.\nAfrican Americans have played a significant role in U.S. military\nhistory. They were denied military leadership\nroles and skilled training because many believed they lacked\nqualifications for combat duty.\nThis exhibit is the beginning of the epic story of The Evolution of Black Americans in Aviation and Space. From people such as General Benjamin Davis Jr., to the Tuskegee Airmen, these individuals made significant and everlasting contributions to the United States itself and specifically to aviation.\nOakland Aviation Museum takes pride in being able to exhibit artifacts and information relevant to the struggle and ultimate success of all Black Americans as they ventured into aviation and space.\nGeneral Benjamin O. Davis Jr\nBenjamin O. Davis Jr., a pioneering military\nofficer was the leader of the fabled Tuskegee Airmen during World War\nII. He was the first African American to become a General in the Air Force. His\ncombat record and that of the unit he led have been credited with playing\na major role in prompting the integration of the armed services after World\nIn a career that began in the days of segregation,\nGeneral Davis, who was born in Washington and lived there for much of his\nlife, compiled a long history of achievements and accomplishments.\nAfter retiring from the Air Force in 1970, General Davis supervised the Federal Sky Marshal Program - designed to quell\na rash of airliner hijackings. He was named an Assistant Secretary\nof Transportation in 1971.\nHe left the Air Force as a Lieutenant\nGeneral, wearing three stars and was the senior black officer in the armed\nforces. President Bill Clinton awarded General Davis his fourth\nstar in 1998, advancing him to full general.\nPrior to 1940, African Americans were barred from flying for the U.S.\nmilitary. Civil rights organizations coupled with the black press exerted\npressure that resulted in the formation of an all African-American aircraft\npursuit squadron based in Tuskegee, Alabama - in 1941. They became known\nas the Tuskegee Airmen.\nTuskegee Airmen were dedicated, determined young men who enlisted to become America\'s first black military airmen. Four hundred and fifty (450) of the pilots who were trained, served\noverseas in either the 99th Pursuit Squadron (later the 99th Fighter\nSquadron) or the 332nd Fighter Group.\nThe outstanding record of black aviators in World War II was accomplished\nby men whose names will forever live in hallowed memory. Each one\naccepted the challenge, proudly displayed his skill and determination\nwhile suppressing internal rage from humiliation and indignation caused\nby frequent experiences of racism and bigotry - both at home and overseas.\nThese airmen fought two wars - one against a military force overseas\nand the other against racism at home and abroad. After the war ended in 1945, black airmen returned to the\nUnited States and faced continued racism and bigotry despite their\noutstanding war record. Large numbers of black airmen elected to remain in the service but because of segregation their assignments were limited.\nExecutive Order Number 9981\nwas signed by President Harry Truman in 1948, which directed equality of treatment and opportunity in all of the\nUnited States Armed Forces. This order, over time, led to the end of\nracial segregation in the military forces. It was also the first step\ntoward racial integration in the United States of America.\npositive experience, outstanding record of accomplishment and the\nsuperb behavior of black airmen during World War II, and after, were\nimportant factors in the initiation of the historic social change to\nachieve racial equality in America.\nElizabeth ""Bessie"" Coleman\nBessie Coleman, born in 1892, was\nthe first female African-American aviator. Unable to gain\nadmission to US flight schools because of her race and gender, she moved to\nFrance where, in 1921, she became the first African-American woman to earn an\ninternational aviation license. She also became the first American of any\ngender or ethnicity to do so. In Europe, Coleman learned how to fly and\nparachute out of a plane and then trained to become a stunt pilot.\nreturning to the U.S, Coleman became a star on the barnstorming circuit, where\nshe performed aerial stunts. Part of Coleman\'s aviation career brought her to\nOakland, Calif., where in 1923, she landed a job publicizing the Coast Tire and\nRubber Company of Oakland and appeared in the company\'s newspaper ads. In 1926,\nwhile her mechanic was piloting her plane, it spun out of control, resulting in\nboth the mechanic\'s and Coleman\'s deaths.\nBessie Coleman standing next to a L.F.G. Roland\nbiplane with instructor Robert Thelen, who taught her advanced aerobatics in\nBerlin, Germany in 1921. (Photo courtesy of the Port of Oakland Archives)']"	['<urn:uuid:3cd722a4-9fd9-4f85-8e02-d875c32212ef>', '<urn:uuid:f203c8e5-3f45-4f03-9de0-8454a2469459>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	12	62	1755
11	How much time does it take to send an image using the Robot 36 SSTV mode that is used in satellite downlinks?	The Robot 36 SSTV mode takes 36 seconds to transmit an image.	['Additional Experiments with SSTV, with some ideas….\nPreviously, I had written an encoder for the Robot 36 SSTV mode. I chose this for a simple reason: it appears to be the most common mode used in downlinks from satellites, such as the ARISSat-1. It’s not a bad choice, and presents reasonable quality in just 36 seconds.\nToday, I decided that I should probably go ahead and implement another of the “Robot” modes, specifically Robot 72. It transmits images with the same resolution (320×240) as Robot 36, but with a bit better quality, and I suspect a bit better fidelity. Both modes transform the RGB colors of the original into a different color space with a luminance channel (usually labeled Y for Ylluminance) and the color encoded in a R-Y and a B-Y channel. To speed transmission, Robot 36 downsamples the last two channels into half resolution images in both dimensions (it really only sends a 160×120 image in those channels). Robot 72 does a similar thing, but only downsamples in the horizontal direction, sending R-Y and B-Y in 160×240.\nIt wasn’t too hard to modify my Robot 36 code to transmit Robot 72. For fun, I set it up and tested it. It works! Sending the resulting file to my Macbook and decoding with Multiscan 3B, I got:\n(The image has been expanded by 2, to 640×480, which makes it look a bit soft)\nSo, anyway, I was thinking about where to take this idea a bit further. I want to create a project that others can duplicate and expand upon, and that maybe promote the SSTV in a way that is amusing and fun. I wanted to build upon the work I’ve done so far, but take it further, and make it into a project that others might want to duplicate.\nWhat I envision is a small box, consisting of a Raspberry Pi, a Raspberry Pi Camera, and a PiTFT display, together with a USB sound card like this one. (You need a USB sound card because while the Pi does have sound output, it doesn’t have sound input.) Add a microphone and a speaker. This collection will be interfaced with a radio: let’s assume for the moment an amateur radio like a the little Baofeng BF-888S radio I’ve been playing with. Add some buttons for interface.\nHere’s what I’m imagining as the use case: it’s an interface to your HT. You could talk, and have it relayed to the radio. You could listen to the radio through the speaker. But you can also click a different button, and it will capture and send an image via SSTV. And if it hears an SSTV image, it will decode it and display it on the TFT display. I’ll probably initially support some of the low resolution black and white modes as well as the Robot 36 and Robot72 modes. I can also imagine a keyboard interface that will allow you to add text to your live images and send it out as well. The fastest, lowest resolution BW modes are just 160×120, and transmit in just 8 seconds. With an 8×8 character matrix, you can send the equivalent of a tweet (about 120 characters) in one image.\nTo make this work, I’ll have to work on a demodulator. So that’s the next step. Stay tuned.']	['<urn:uuid:c1d38d95-8432-4d5b-866c-70315bbf2540>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	22	12	555
12	Could you explain the basic steps of fermenting tomato seeds before saving them for next season? I've heard it's important but don't know how to do it.	The basic fermenting process for tomato seeds involves first washing and slicing a ripe tomato in half, then scooping out the seeds and gel. Place these in a container and add enough water to cover the seeds. Cover the container with cheesecloth, paper towel, or plastic wrap, ensuring there's room for air circulation. Place it in a warm location away from direct sunlight. Stir the mixture daily and wait for several days until you see a film form on top and the seeds sink to the bottom. You'll know fermentation is complete when the seeds separate and sink, white foamy mold forms on top, and bubbles appear. Then remove the mold and floating seeds, rinse the good seeds that sunk, and dry them thoroughly before storage.	['Other than buying tomato seeds or seedlings for your garden, there is a more economical and money-saving technique, which is to be harvesting tomato seeds from the bounty of your tomato fruits.\nIt is not complicated to save your own tomato seeds. If you have fully ripe, disease-free tomatoes, then you have the perfect setup to start your own seed collecting.\nWhy Should You Save Tomato Seeds?\nIt saves you money\nIf you collect the seeds from your favorite tomato variety, you can grow them again next season.\nIt makes you self-sufficient. You can grow tomatoes independently year after year without buying seeds or seedlings from a vendor, plus you know how the seeds performed and were cultivated.\nYou can exchange seeds\nAfter saving your own tomato seeds, it’s possible to use the seeds to trade with other gardeners. If you have extras, you can also share them with other family members and friends and help them to get their tomato planting started.\nAllows you to grow lots of crops\nIf you have your own seeds, you can grow as many tomatoes as you like.\nWhen you use the seeds from your best tomato plants you are ensuring that next year’s crop will look the same as this season.\nHarvesting seeds and saving them for the next growing season is easy as long as you know how. It does require waiting time and a few steps, but it’s not difficult. Let’s take a look:\nChoose The Seeds\nBefore you start saving tomato seeds for next year, you should consider which plants to use to cultivate your seeds for the next season.\nWhen choosing seeds, make sure you choose seeds from a tomato plant that are open pollinated varieties. The plants that have been open-pollinated will produce seeds that remain true to type to their parent plant vs seeds from hybrid plants were produced by companies that have combined varieties to make a new one and may not breed properly for you from seed.\nIf you don’t have any open-pollinated tomatoes open-pollinated tomatoes in your garden, you can use those from heirloom tomatoes you can get at the local farmer’s market.\nCollecting Tomato Seeds Preparation\nTo prepare the seeds for saving you’ll need to take them through a process of fermentation, in order to do this, make sure you have the following:\nGlass jar or bowl\nEnvelopes or paper bags for storage\nCheesecloth, paper towel, or plastic wrap\nFermenting Tomato Seeds\nThe fermenting process is easy:\n1. Wash all the dirt and dust off of the tomato before slicing to keep any disease or fungus from being transferred inside the tomato. Using a knife, slice the ripe tomato in half.\n2. Scoop out the insides, including the seeds and the gel surrounding the seeds, with a spoon. The gel sac that surrounds the seed keeps it from germinating, so this process will allow the seeds to germinate.\n3. Place the contents in a clean cup or container.\n4. Add water to the container. The amount of water should be enough to cover the seeds.\n5. Cover the container with cheesecloth, a paper towel, or plastic wrap. Before securing, you should ensure that there is enough room for air circulation around the seeds. If you are using plastic wrap, make sure to poke a few holes in the top.\n6. Place the container in a warm location, but away from direct sunlight.\n7. At least once a day, remove the cover from the container and stir the seed mixture.\n8. Allow the seeds to sit for several days until you see a film form on top of the water and the seeds start to sink to the bottom. The viable seeds are those that sink to the bottom, in most varieties. You know that the seeds are fermented if:\n- The seeds start to separate and they sink to the bottom of the container.\n- There’s a white, foamy mold that has formed on top of the water.\n- Bubbles have started to form at the top of the container.\n9. Using a spoon, remove the moldy film as well as any seeds that float on top of the water, as they aren’t viable, then you’ll want to discard all of that.\n10. Clean the container well and fill it again with fresh, room temperature water.\n11. Wash the seeds by swishing them gently around in the clean water.\n12. Put a cover on top of the container (you can use cheesecloth) and pour off the water.\n13. Place the seeds in a strainer and then rinse them under running room temperature water.\n14. Spread the seeds out on a paper plate, coffee filter, wax paper, or paper towels (warning – the tomato seeds can stick to the paper towels as they dry and be hard to take off). If you’re saving different varieties, label each plate with the seed variety.\n15. Allow the seeds to dry out in direct sunlight completely. Shake or stir them periodically. The drying time may take up to 1 to 2 weeks.\n16. Once the seeds are completely dry, place them in an envelope or paper bag. Don’t forget to label them with the name of the seed variety and the date they were stored.\n17. Place the container in a cool, dark place like your pantry or at the back of your fridge, don’t freeze them. They need to remain in a cool, dry area. If they are subjected to humidity or moisture it can cause them to rot.\nHow Will You Know If The Seeds Are Completely Dry?\n1. Allow the seeds to dry out for at least one to three weeks.\n2. The seeds start to be very hard.\n3. They are difficult to smash or to bite.\n4. The seeds make a faint snapping noise when you try and break them.\nThings To Keep In Mind When Saving Tomato Seeds\nNever use plastic or ceramic plates when drying your tomato seeds. When drying, any water needs to be wicked away from the seeds to allow for proper drying out. Plastic or ceramic plates can retain water and inhibit the process.\nProper storing and drying of the seeds means that they can be kept and will remain viable for up to 6 years.\nYou can store the dry seeds in an envelope or paper bag. Just be sure to label and date them and keep them sealed well.\nIf you store the seeds in the refrigerator, allow the container to reach room temperature before opening it to use for planting. Opening the container before allowing it to reach room temperature will introduce moisture to the seeds which can cause them to rot and fail or harbor fungus or disease.\nBe careful when storing the seeds in a plastic packet. If some seeds still contain moisture, it will transfer to all the seeds in the packet. The moisture will encourage mildew and will cause the seeds to rot.\nDo not try to speed up the process of drying out the seeds by using heat. Heat can destroy the seeds.\nDuring the drying process, shake the plate or stir the seeds to avoid clumping and allowing equal and even drying. Spread the seeds in a single layer. Once the seeds are exposed to air, they can dry out faster.\nSaving your tomato seeds will guarantee that you can grow the best (or your favorite) variety. It also saves you money.\nBeing able to save your own seeds and grow crops from them year after year is very satisfying and allows you to be self-sufficient.\nGood luck with saving your tomato seeds!']	['<urn:uuid:6688255e-a425-4cc9-960b-2e639a4f5477>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	27	126	1276
13	I've been studying bone fracture mechanics and would like to understand what is the fundamental failure mode that initiates bone fractures at the microscopic level?	Bone fractures begin when the organic matrix of the bone, or 'glue' holding mineralized collagen fibrils together, fails causing crack propagation.	"['Reference Point Indentation\nReference Point Indentation without a Reference Probe\nUsing the Osteoprobe.\nThe Osteoprobe is a novel, hand-held reference point indentation (RPI), instrument that is designed for clinical measurements of bone material properties in living patients. This instrument differs from previous RPI instruments in that it requires neither a reference probe nor removal of the periosteum that covers the bone, thus significantly simplifying its use in patient testing.\nReference Point Indentation with a Reference Probe\nFigure 1 The current iteration of the Reference Point Indentation instrument.\nOur current research is now devoted to the development of the Reference Point Indentation (RPI) instrument (formerly known as the Bone Diagnostic Instrument, BDI, and the Tissue Diagnostic Instrument, TDI). The RPI is being developed with the long-term goal of providing a way for researchers and clinicians to measure material properties of human bone in vivo. Such measurements could contribute to the overall assessment of bone fragility in the future. The portability, ease of use, and minimal training make this instrument suitable to measure bone material properties in a clinical setting.\nThe Reference Point Indentation instrument will measure microscopic materials properties of the bones of living patients with accuracy and ease. Our hypothesis is, and our preliminary results are in agreement, that a measure of microscopic fracture resistance with the Reference Point Indentation instrument in an individual\'s bone correlates with their resistance to macroscopic bone fractures, or, stated another way, their overall bone fracture risk. Indentation testing is already well-established as a powerful tool in characterizing mechanical properties of materials.\nFundamental research has revealed that bone fractures begin when the organic matrix of the bone, or ""glue"" holding mineralized collagen fibrils together, fails causing crack propagation. Preliminary results with the Reference Point Indentation instrument have shown that an individual\'s susceptibility to this fundamental failure mode can be measured by indentation tests in which bone is forced, on a microscopic scale, into the same types of failure - separation of mineralized collagen fibrils - that is the root event of bone fractures. Preliminary results with the Reference Point Indentation instrument have also shown that the necessary measurement can be performed on bone that is still covered with soft tissue.\nFigure 2 Basic operation of the Reference Point Indentation instrument.\nThe heart of the Reference Point Indentation instrument is a probe assembly that can be inserted through the skin of a living patient to measure the fracture properties of bone by creating microscopic fractures in an indentation on the surface of the bone (Figure 2). At this stage of development, the indentation distance increase (IDI) appears to best distinguish bone that is more easily fractured from bone that is less easily fractured. The IDI is a measure of the difference in indentation after repetitive cycling to a fixed force. These indented fractured areas are very small: of order of one-thousandth of a cubic millimeter, but give important information about how easy or difficult it is to fracture the bone of an individual.\nFigure 3 The normalized Indentation Distance Increase (IDI) of a 17 year old female and a 79 year old female.\nFor more detailed information please refer to our two papers available in the publications section.']"	['<urn:uuid:d4d8ddb7-e650-4bea-9200-6b9788eeb34f>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	25	21	532
14	looking build cabin mountains what old faithful inn yellowstone modern style mashup characteristics	The Old Faithful Inn, built in 1904, pioneered the parkitecture style with its massive gable roof, 80-foot high vaulted ceiling, and 40-foot tall fireplace constructed of logs and stones. Today's modern mountain designs blend this traditional parkitecture approach with contemporary elements. Key characteristics include using both modern materials (concrete, glass, steel) and traditional ones (timber, stone), incorporating large windows that serve as natural artwork, and maintaining simplicity in finishes. The design should harmonize with the natural environment while including modern amenities and clean lines.	"['Collisions and mountains aren’t strangers. Mountain ranges form as great plates of land collide, yield and uplift. Swirling clouds collide with the mountains and weather brings the sculpting forces of wind, snow, rain, and water. In the design community, modern architecture and contemporary interior design are in a collision course with the mountain lifestyle and traditional mountain building practices, birthing the new design movement of Mountain Modern.\n“Architecture is the will of an epoch translated into space,” asserted Ludwig Mies van der Rohe, a pioneering master of modern architecture along with Walter Gropius and Le Corbusier. Modernism emerged at the beginning of the century with influences from Art Nouveau, the early work of Frank Lloyd Wright, and industrial design, according to architectural historian,Spiro Kostov. Drivers were new materialism, standardization, prefabrication and an available material palette of concrete, steel and glass thanks to preceding industrial advances.\nPhilosophically, the modernist movement pursued freedom from the past, similar to the political movements in Europe immediately after the First World War. The new emphasis was on function, honest expression of structure and materials, and a rational approach to design. This included suppressing all ornamentation and historic allusions and style. In keeping, the forced symmetry of classical architecture was abandoned in favor of balanced asymmetry. The pioneers of modernism adopted a fitness of purpose expressed in a desire to simplify and eliminate the superfluous. Less is more became a repeated saying of Mies van der Rohe.\nAs exemplified by the International Style, modern urban architecture turned inward. The buildings were anchored to the site, but did not extend to define the spaces around them or to create interaction with the streetscapes. While designers aimed at efficiencies in places to live and work, the buildings did not connect with their inhabitants emotionally or by tradition. Mies van der Rohe reported he searched and distilled until he reached an architecture that said “almost nothing.”\nAs with most movements, modernism continued to evolve as new designers pressed it forward in fresh directions and some of the early practitioners extended their thinking. The U.S. also honed its own version of modernism after World War II. Especially in California, modern residential architecture became more conscious of the sites as rooms articulated to terraces and patios for outdoor living, and some architects began to specify outdoor spaces and landscaping.\nWhile there have been other important subsequent architectural trends and movements, a minority of practitioners have continued undeterred in the modernist tradition. However, in the last decade, there has been a considerable resurgence of public interest in mid-century modernism and the current modernists’ work.\nUntil this time, mountain architecture and interior design has largely followed in the traditions of climate-tested designs like cabins, lodges, and chalets that use the abundant local materials of stone, logs and timber. These are designs that are immersed in custom and tied to comfort.\nNow, mountain meets modern. It is an exciting time for new creations using the modern materials of concrete, glass and steel juxtaposed with traditional mountain timber and stone. The simplicity of the finishes and interiors draw the occupants’ eyes to the outdoors where windows function like a room’s art.\nIt is fresh, new, daring design filled with individual innovations and creativity from the talented architects, designers, and builders of the area, complemented by the artwork. Let’s see their works and what they are saying about the collision as mountain meets modern. ~ Story By Ann Zimmerman', 'Check Out This Blend of Late 19th Century and Rustic Architectural Styles for the Modern Home\nIt all started with the Old Faithful Inn in Yellowstone Park, Wyoming.\nDesigned by architect Robert Reamer and located near the Old Faithful geyser within Yellowstone, the Inn – which opened in 1904 – was an immediate sensation. Constructed of logs and stones, it featured a massive gable roof, a majestic lobby with a vaulted ceiling almost 80 feet high, and a 40-foot tall fireplace. Described as “rustic and luxurious, breathtaking yet casual, a monumental icon that fit perfectly into its surroundings,” the Old Faithful Inn inspired a novel architectural style: the National Park Service Rustic, or Parkitecture, which created buildings that fit naturally into the amazing landscapes and scenery of the parks.\nTop:The Old Faithful Inn (in 1914) launched the rustic parkitecture style in the United States. One of the largest log-style buildings in the world and a National Historic Landmark, the Inn provides a great view of the Old Faithful geyser (photo credit: Old Faithful Inn, Northeast Aspect, by Yellowstone NP, USA, under license CC BY 2.0). Bottom: Be prepared for the breathtaking lobby with its 80-foot vaulted ceiling and a stone fireplace that measures 40 feet in height (photo credit: Public Domain).\nWhat exactly is Parkitecture?\nInspired by the Arts and Crafts Movement and Frank Lloyd Wright’s Prairie School, parkitecture is the signature rustic style of the national parks in the U.S. When it first emerged in the 19th century, it blended several architectural styles – like the Queen Anne, shingle, and mountain designs – within the park structures. It became a more cohesive and defined design when the National Park Service was established in 1916.\nLike the masters of the Arts and Crafts and Prairie style homes, famous parkitecture designers, landscape artists, and professionals such as Reamer, Gilbert Stanley Underwood (architect of Yosemite’s Ahwahnee Hotel), Mary Jane Coulter (Hermit’s Rest-Grand Canyon), and others advocated simplicity, use of natural materials, and a connection to the surroundings. They ensured that structures throughout the entire Park system – gates, entrances, roads and bridges, visitor centers, shelters, hotels, inns, and staff facilities – were made with local wood and stone, were visually appealing, and “harmonized with their natural environment.”\nTop: Nestled in a grassy meadow in Yosemite National Park’s east end of the valley, the Ahwahnee Hotel is surrounded by oak and fir trees. Instead of competing with the magnificent scenery, architect Gilbert Stanley Underwood designed a building that provided wonderful views of the park from every window. The asymmetrical layout features three different 3-story wings that radiate out from a 6-story central tower. An interesting color scheme adds to the exterior visual appeal of the Ahwahnee Hotel. Look closely at the slate hip roofs and windows; and the massive and tall granite boulder piers and chimneys that match the color of the nearby cliffs (photo credit: Ahwahnee Hotel Half Dome by Mark James Miller under license CC BY-SA 3.0). Bottom: The dining room in Ahwahnee displays the exquisite rustic design sensibilities in the establishment with the vaulted ceiling and exposed natural timber trusswork (Ahwahnee Dining Room by Mark James Miller under license CC BY-SA 3.0).\nPrinciples of Parkitecture\nThrough the early and mid-1900s, park architecture evolved into a more cohesive and refined style centered on the rustic design. In addition to the emphasis on natural materials, there are a number of principles that define parkitecture.\n2. Buildings should blend with each other. For example, similar materials should be used in design; roof types and slopes should be about the same.\n3. Horizontal rather than vertical lines should dominate.\n4. Rigid, straight lines should be avoided.\n5. Stone, log, and heavy timber work should be in scale to have a well-balanced design.\n6. In some cases, it may be necessary to make stone and log work a little oversize so that the cliffs, rocks, and natural forest do not dwarf the buildings.\nHow is Parkitecture Incorporated in Modern Designs?\nA classic architecture that has withstood the test of time, Parkitecture is a major influencing factor in the way architects design and plan buildings as well as today’s contemporary homes.\nYou don’t have to live within the boundaries of a national park to achieve the feel and ambiance of the rustic aesthetic. From various architectural styles – including Arts and Crafts, Craftsman, Prairie, Ranch, Farmhouse, Country, Midwestern, Modern, and the obvious log cabin and mountain lodge – to exterior and interior home features and fixtures, you can have your own parkitecture rustic designs.\nThis 4-bedroom, 4.5-bath Luxury home shows the insfluence of parkitecture in its Rustic style: exposed timbers. stpme sodomg. ahip rooflines all at the same slope (Plan #193-1094).\nOne of the exciting challenges that architects face today is how to bring parkitecture house plans to their clients. And what do they tell clients who come to them about design elements of the inns and lodges they’ve seen at Yosemite, Yellowstone, the Grand Canyon, Glacier Park, or Crater Lake?\nIt’s not surprising that architects and designers are equal to this task and very willing to work with their clients to give them that parkitecture vibe. Architect Ellis Nunn of Jackson Hole, Wyoming, says he likes to “incorporate dormer windows and a large log and stone porte-cochere into the design” to give the home “a great lodge look.” Other designers include cedar posts and large windows to bring the outdoors into the home.\nTop: It may not be the Ahwahnee Hotel in Yosemite, but this 2,815-sq.-ft. Ranch style home with a loft and unfinished basement features gable rooflines, large windows, and a gorgeous exterior facade of natural stone and dark wood. To complete the stunning exterior, there are large rocks that form a hardscape around the covered vaulted porch supported by wooden posts on stone pedestals (Plan #161-1101). Bottom: The vaulted timber roof over this rear patio, along with the stone column bases, wood and stone siding, and relaxed furniture put you in a Parkitecture frame of mind (Plan #161-1094).\nIf you\'re searching for a plan or building new in a style that embraces at least some elements of the rustic Parkitecture style, you\'ll likely want your interior to echo the style as well. An open floor plan is essential, along with modern furniture and natural materials – and some of the tips below.\nIf you have have an existing home and like the style but you’re not ready for a major interior renovation to transform your home decor into parkitecture rustic, don’t fret. Think of what you can do and all the ways to create an interior that exudes the warmth, comfort, simplicity, and beauty of the design.\nHere are a few elements of the rustic style that can be included in a 21st century home:\n1. Let’s begin with the natural color palette. While beige, cream, brown, gray, and neutral hues rule the day, you can also go with bold touches by mixing shades of blue, green, and even red. Just make sure the tones are soothing, calming, and inviting.\nYou can’t get a more neutral palette than the one in the spectacular Great Room of a luxurious 2-story, 4,412-sq.-ft. Arts and Crafts style mountain lodge. Soft shades of green in the L-shape sofa, area rug, heavy wooden door – with splashes of color in the yellow throw pillow, reddish brown and dark brown wood beams, and stone fireplace – create a cohesive and appealing design (Plan #202-1017).\n2.Wood and stone are the essential materials that give the home its natural look and feel. So designers suggest using them in furnishings, ceilings, floors, and walls.\nThis impressive kitchen in a 2,478-sq.-ft., 1-story, 2-bedroom Transitional Craftsman home uses a lot of wood and stone in its design. There are dark wood floors that match the cabinets, drawers and trim. Then you have a stone arch surrounding the stove, a stone backsplash and countertops, and stainless-steel appliances (Plan #161-1073).\n3. Large wood beams for the ceiling will always remind you of those park inns and lodges, and of course, the great outdoors.\nWhile not as high as the vaulted ceiling of the Old Faithful Inn in Yellowstone Park, this ceiling (top) in the Great Room of a sprawling 2-bedroom, 3,623-sq.-ft. Ranch home (bottom) – with its huge wood beams – is magnificent in its own right. Wide glass doors and clerestory windows provide terrific views of the surroundings (Plan #161-1094).\n4. You’re going for the natural look here. Sochoosethe right accessories, materials, and fabrics to add to the atmosphere. Floors only need area rugs, and to capture the feel of a rustic home, go for hand-crafted accents like woven baskets, quilts, throws, and cushions. Make sure that you use sturdy fabrics like jute, burlap, and canvas.\nTake in the natural color palette of this bedroom in an attractive 2-story European-Rustic style home with a total of 4719 sq. ft. of finished space. An area rug made of a coarse fabric is the main accent of the dark wood floors. Accessories on the bunk beds include neutral-tone pillows with a slight splash of color and coverlets of a sturdy material (Plan #193-1095).\n5. Furniture is usually oversized and designed to be cozy and comfortable. Heavy wood pieces are common – like side and coffee tables and built-ins. Usually the dark tone of the wood is balanced by lighter accents and the fabrics used for accent chairs and sofas.\nTop: Feel at home in this fabulous Great Room of a Ranch style home. From the high vaulted ceiling to the large windows, huge stone fireplace, wood accessories, and muted fabrics and accents, the Great Room, which is designed in a modern-rustic aesthetic, exudes warmth and comfort. Center: The delightful 3,162-sq.-ft., 2-bedroom Ranch home, which houses the Great Room shown, sports rustic accents like stone and exposed timbers.Bottom: The timber-frame entrance reinforces the Parkitecture feel of the house (Plan #161-1100).\nAnother take on the rustic design is animal head wall decor as seen in this Great Room of a 2,478-sq.-ft., 1-story, 2-bedroom Transitional Craftsman style home. In addition to the wall accent, there is a corner table made of dark wood and a fireplace with a stone surround (Plan #161-1073).\n6. How about a fireplace for a campfire atmosphere? Whatever style your home is, a fireplace adds an inviting and welcoming feeling. In a rustic-inspired home, a fireplace is almost a must. So start imagining a very tall one with a rock or brick surround and a thick wood mantel as the focus of your family or living room.\nThe towering fireplace featuring a stone surround and a thick wood mantel (top) is the “stop-in-your-tracks” attraction in this Great Room of a rustic Ranch style home (bottom) with 2815 sq. ft. of living space. Look closely at the pieces made of wood – built-in cabinets, shelves for picture frames and collectibles, side table, and staircase against a stone wall (Plan #161-1101).\nThis fireplace in a 4-bedroom Rustic style home shares center stage with the dazzling vaulted ceiling. On the left of the fireplace is a wooden staircase leading to the upper level; on the right, a large glass window gives a glimpse of the brick and stone exterior of the home (Plan #161-1108).\n7. Don’t forget the large windows to bring the outdoors into the home. One of the principal elements of Parkitecture rustic – inspired by the Arts and Crafts movement – is a home’s (building’s) harmony with nature. You can bring this home by installing large glass windows and doors that provide abundant natural light, air, and great views of the surrounding landscape.\nOne view from two perspectives: Large picture windows (top) in the Great Room of a rustic Ranch style home provide a view of the surroundings and homes across the lake. Another band of huge windows (bottom) in the dining room/breakfast nook allow a more visible view of the lake (Plan #161-1101).\nRoom by Room Rustic\nHere are some rustic looks for different areas of the home to fuel your creativity and imagination.\nFrom the foyer of a European-Rustic style home (Plan #193-1095), guests step into its magnificent Great Room, which includes the spectacular modern-rustic kitchen above. With its wooden built-ins, a colorful backsplash, animal art decor, and interesting wood pieces in the dining area wall, the kitchen complements the rustic decor of the rest of the house.\nThe vaulted kitchen (top) in a two-story, four-bedroom Rustic Contemporary style home (Plan #161-1108) features a double island counter. A step away is part of the casual dining area, shown just below the photo of the kitchen.\nThough not a large contributor to your home\'s overall style, as it\'s not exactly a ""public space,"" being a room only close family and friends are ever likely to see, the master bedroom may still a place where you want to maintain the continuity of the home decor. You can do so by staying tradtional in your design choices and imagining yourself in a National Park hotel room.\nIn a five-bedroom European Rustic home (Plan #193-1095), the master suite shown above comes with several dark and distressed wood pieces, a unique wall accent, and a colorful coverlet to meld with the National Park aesthetic.\nThe master suite above in a charming three-bedroom Bungalow style home (Plan #153-2083) has a huge bed nestled in a thick wood frame. It is covered with a thick patterned quilt with shades of yellow and blue-green. The tray ceiling gives a feeling of height and the neutral gray walls complement the rest of the decor.\nRustic balconies are a fixture in many National Park structures, allowing visitors to take in the views and surroundings from unique viewpoints. If you\'re lucky enough to have a balcony or elevated deck or porch in your home, outfitting it in the right finishes will help bring the park atmosphere to your yard, especially if you have wooded, water, or mountain/hill views.\nWood floors and ceiling, stone walls, and wood fixtures and furnishings are the finishing touches of the spacious balcony shown above in a five-bedroom European Rustic home. The 4,719-square-foot home features a second-floor living room that opens onto the balcony (Plan #193-1095),\nPowder Room Accents\nAn innovative vanity (shown above) in the powder room of a Transitional Craftsman style home (Plan #161-1073) will definitely get the attention of guests. It does what you\'re after when working with rustic Parkitecture style in your home: harks back to the past with details that look like they may have been used in the early 20th century while being modren and functional in today\'s home. Note the vessel sink that resembles an old wash basin on a wash stand (the vanity) with backsplash and shelving and overhead light fixture, which calls to mind a gas or oil lamp.\nOf course, you want bathrooms to have all of the modern conveniences, but in a Parkitecture decor it can be difficult to marry that with the detailing necessary to create a rustic atmosphere.\nModern meets rustic in the amazing master bathroom shown above, which comes from a two-story, 4,531-square-foot Rustic Country style home with three bedrooms, two full baths, and two half-baths (Plan #161-1076). The natural wood vanities, cabinet, door, and window frames go a long way in creating a rustic theme, as do the stone, the rubbed bronze faucets (especially the bath faucet, which looks as though it could be the spout from a well riser), the heavy light fixtures, and the wall sconce.\nParkitecture’s Back Story\nIn 1872, President Ulysses S. Grant declared Yellowstone the first national park in the United States. Little did he realize then that this would spark the emergence of a novel architectural design within the country’s national wparks. As Sequoia, Yosemite, Rocky Mountain, Mount Rainier, Glacier, and other parks gained national recognition, there came a need for shelters and housing facilities for visitors. By 1904, architect Robert Reamer designed Old Faithful Inn, a lodge within Yellowstone Park near the famous Old Faithful geyser.\nConstructed of logs and stones, it became the first lodge of the Western U.S. – and the inspiration for the National Park District’s rustic design or parkitecture.\nVery soon, other famous architects designed and planned spectacular inns, shelters, museums, and other structures within the national parks. Among them:\nEl Tovar in the Grand Canyon (1905 - Charles Whittlesley)\nThe Stanley Hotel – Rocky Mountain National Park (1909 – Freelan Oscar Stanley\nMany Glacier Hotel - Glacier National Park (1915)\nCrater Lake Lodge - Oregon’s Crater Lake National Park (1915)\nLake Crescent Lodge - Washington’s Olympic National Park (1916)\nParadise Inn - Washington’s Mount Rainier National Park (1917)\nThe Ahwahnee Hotel (formerly the Majestic Yosemite Hotel) – California’s Yosemite National Park – Designed by Gilbert Stanley Underwood, the amazing 123-room with large granite chimneys opened in 1927.\nAs the Park Service became more organized, the Parkitecture rustic aesthetic firmly took hold. Soon this design filtered to the state parks – and the American home.\nThis 4412-sq.-ft. Luxury Rustic Cottage style home with 5 bedrooms and 5.5 baths (Plan #202-1017) has many of the hallmarks of Parkitecture style, and one could easily see it – on a grander scale for sure – as a lodge on the grounds of a National Park.\nYou don’t have to travel to Yellowstone or Yosemite to get the Parkitecture rustic feel. Just step out of the box – and get ready to transform your home into the rustic haven you’ve always imagined!\nFootnote: The lead image in this article is a picturesque 1-story, 3-bedroom Traditional style home with Arts and Crafts influences. For more details on this stunning home, go to: (Plan #190-1074)']"	['<urn:uuid:941c886f-a90c-442f-9b73-57cbc27d5a6b>', '<urn:uuid:d325d01a-30fc-4223-b01d-5d08820c977f>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	13	84	3517
15	What are the main ways to teach geology in a classroom setting, and how have digital technologies changed the approach to geological studies?	Traditional classroom geology teaching involves using a systematic approach through laboratory manuals that cover topics like sedimentary rocks, geologic time, and plate tectonics. Students work with physical materials including diagrams, maps, and photographs to solve various geological problems. The modern digital approach has introduced virtual and augmented reality experiences through platforms like Google Expeditions, which allow students to explore geological concepts using mobile devices. Teachers can guide these virtual experiences by selecting points of interest, creating custom markers throughout the classroom, and leading small group expeditions, making geological education more interactive and accessible without requiring expensive equipment.	"['This new and revised edition of Historical Geology: Interpretations and Applications has been written as a guide to the laboratory study of historical geology and can be used in conjunction with most of the current textbooks in the field. This manual helps students understand the fundamental concepts of historical geology by providing outcrop and realistic situations to which they can apply geologic concepts leading to an interpretation of the available data. The direct application of abstract principles to concrete situations and practical problems reinforces the learning process while instilling a strong sense of the purpose for geologic study. This method is of particular value to students for whom a course in historical geology will be their last academic encounter with the physical sciences. COMPREHENSIVENESS The manual is divided into six chapters. The first chapter covers the great group of sedimentary rocks that are vital to the interpretation of most geologic structures and sequence of events, and that act as the preservation medium for the majority of fossils. Chapters 2 and 3 provide discussions, examples, and exercises in such basic principles of historical geology as the concepts of geologic time, the use of sedimentary rock layering and stratification during the ordering of geologic events, and the interpretation of environmental and sea level changes. Chapter 4 presents a systemic overview of the three groups of life forms that are commonly preserved as fossils: the invertebrates, the vertebrates, and plants. Chapter 5 covers the development and reading of geologic maps. Chapter 6 deals with plate tectonics, an important topic for both physical and historical geology. Within these chapters, exercise sets are available that require students to use the principles they have learned to solve problems involving both physical and historical geological concepts and methodologies. USE OF THE SCIENTIFIC METHOD Historical geology is best learned through fieldwork supervised by a professional. When field studies are not feasible, diagrams, maps, photographs, and slides of fossils, landforms, and sedimentary structures can be used effectively to model a field experience. In this respect, the manual permits students to visualize how geologic data are collected, tabulated, synthesized, interpreted, and applied to solutions of various geologic problems. NEW ORGANIZATION As suggested by several reviewers and users of the manual, a comprehensive change has been made to this edition. A greatly expanded section concerning the development, classification, and interpretation of common depositional attributes of sedimentary rocks has been added as a new Chapter 1. Selected problems from the previous Chapter 7 have been dispersed within the revised first four chapters. Extensive new photographs and new problems have been added, and many problems have been reorganized and moved to be near their explanatory text material. SCHEDULING FLEXIBILITY In most cases, this manual will be used during a one-semester or one-quarter term. It provides students with numerous exercises and problems. Some of these, such as the exercise questions that conclude each chapter, will require only a few minutes to complete. Others can be worked in the laboratory or at home, when more time is available. Variations in the length and content of exercises give instructors considerable latitude in choosing topics they wish to stress. This manual is written and illustrated with outcrop or rock feature photographs with the aim of helping students readily understand the principles of historical geology. These illustrations should help build student awareness that the geologic problems presented in the manual are based on real data, often in contexts that could typically affect the nongeologist in his or her professional life. This manual is not, therefore, just an academic exercise; it is designed to demonstrate that geology is a profession that relates directly to the world outside of college and that basic principles of geology can be used to understand professional situations as well as postcollege travel experiences. A glossary is also included. INSTRUCTOR AIDS AND INTERNET SITES One of this manual\'s principal teaching strengths is that it does not depend on extensive supplemental teaching aids; its coverage of the basic principles of historical geology can be thought of as self-contained. A complimentary Instructor\'s Guide accompanies the manual and is available from the publisher on request. To obtain a copy contact your local Prentice Hall sales representative. The guide provides lists of a few additional items that may be necessary for a historical geology laboratory. It also offers suggested answers to the manual\'s exercises and problems. Instructors should also encourage students to use the Internet as a geologic resource. Many Web sites change through time, new sites are rapidly being added, and topical links are continually being developed. Instructors can provide key words, associated topics, and names of major museums and research organizations for students to expand in-depth analysis of topics provided in the classroom or in a laboratory. Jon M. Poort Roseann J. Carlson\nBack to top\nRent Historical Geology 6th edition today, or search our site for other textbooks by Jon M. Poort. Every textbook comes with a 21-day ""Any Reason"" guarantee. Published by Prentice Hall.\nNeed help ASAP? We have you covered with 24/7 instant online tutoring. Connect with one of our Geology tutors now.', 'Google Expeditions – Beyond Virtual Field Trips\nFrom the bottom of the sea to the moon and beyond: virtual and augmented reality now enable us to take students virtually anywhere. The best part is that you only need a mobile device, no expensive virtual reality headsets, to take full advantage of these virtual field trips. Google Expeditions create engaging experiences for students that transcend time and space.\nGetting Started with Google Expeditions\nFind an Expedition\nSearch the List of Available expeditions\nThis convenient Google Sheet makes it easy to find an expedition for any age or subject area. Search by topic, keyword, location, etc. to find an expedition related to your curriculum.\nYou can browse expeditions directly within the app. While it’s less efficient, it has led me to discover some interesting and new material. Look for icons that indicate whether an expedition is available in virtual reality or augmented reality.\nVirtual Reality vs. Augmented Reality\nVirtual Reality (VR) creates a simulated environment (i.e. it puts you among dinosaurs on a prehistoric island).\nAugmented Reality (AR) provides an enhanced version of reality (i.e. a dinosaur appears to be standing on a desk in your classroom).\nNo Fancy Equipment Required\nOriginally designed for viewing with a virtual reality headset (or Google Cardboard and a phone), expeditions now work on any mobile device (iPad, tablet, smartphone); just click “View Full Screen” when you open the expedition.\nDownload your Expedition\nTeachers and students can explore expeditions on their own, but it’s hard to beat a teacher-led guided expedition; either way, you first have to download the expedition. Simply access your chosen expedition by browsing or searching in the app, and tap to download it. Downloaded expeditions will display a checkmark on them, and can later be found by tapping “Saved” at the bottom of the screen.\nPlan Your Expedition\nLesson Plans to Accompany Google Expeditions\nYou’ll find a large number of free lesson plans under #GoogleExpeditions on tes.com.\nLesson Planning Tips\nCheck out this site for great lesson planning tips to get the most out of expeditions as an instructional tool. It’s broken into ideas for pre-expedition prep, what do to before the expedition, during, and after.\nPrepare Before The Expedition\nLead The Expedition\nGuiding VR Expeditions\nIn the Expeditions app select “Guide.”\nStart the expedition by tapping “Play”\nSelect points of interest from the suggestions provided or touch and hold a location on your screen to create your own point of interest.\nAsk students to tap “Follow” then select you as their guide. They will follow the arrows on the screen to view the points of interest that their teacher selects.\nYou can pause (locking student screens) at any time.\nYou can also draw on the screen using the scribble icon at the top of your screen. Students will see your drawings.\nSwipe horizontally to change scenes.\nGuiding AR Expeditions\nAR expeditions are work similar to VR expeditions, but require the use of markers:\nTeachers place up to seven markers throughout the room.\nStudents are divided into groups around the markers, and point their cameras at their group markers to see the image that the guide has chosen.\nNote: the same image is displayed simultaneously across all seven markers.\nSpend some time exploring the expedition ahead of time.\nLimit Guided expeditions to small groups (5-10), otherwise they tend to not listen to what you’re sharing as a guide.\nGive students time to explore before guiding the expedition.\nDon’t forget that you can pause a scene.']"	['<urn:uuid:99ad86f9-6c7f-4e19-a457-d9d3919a4f82>', '<urn:uuid:1e3f6644-49f9-4cf4-9b30-d23bf915fda2>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	23	97	1432
16	Can you tell me what career path Father Calum MacLellan followed after completing his time in the military service?	After completing his National Service, Calum was readmitted to seminary and spent six years in the Pontifical Scots College in Rome, studying philosophy and theology at the Gregorian University. He was ordained a priest in Rome on 5th July 1953 and returned to Scotland to minister in the Diocese of Argyll and the Isles.	"[""- Catechist Training\n- News from Bishop Joseph\n- Around the Diocese\n- The Magpie's Nest\n- Diocesan Youth Events\n- Pastoral Letters\n- Gaelic Prayers\n- Justice and Peace\n- Fr. Colin in Ecuador\n- Our Heritage\n- Religious Education\n- Pastoral Music\n- Scottish Catholic Safeguarding Service\n- Cathedral Lights\n- Episcopal Ordination of Bishop Joseph Toal\nLife of Fr. Calum MacLellan\nFr Calum MacLellan, retired Parish Priest of St. Michael’s, Eriskay, died in Raigmore Hospital, Inverness, in the early hours of Saturday July 14th aged 86. Although Fr Calum had been in hospital for a number of weeks his death was somewhat unexpected and a cause of much sorrow to his family, his fellow clergy, and the many people throughout the Diocese of Argyll and the Isles, Scotland and beyond who loved and respected him.\nFr Calum was born in Glasgow on 6th June 1926, the son of Allan and Flora MacLellan, and was baptised in St Margaret’s, Kinning Park. In his early childhood the family returned home to Eriskay, where Calum grew up and went to school. Having expressed a desire to become a priest he went to St Mary’s College, Blairs, in 1940, where he studied throughout the Second World War. In 1945 Calum was called up for National Service and spent three years in the Army, including a number of postings to places, like North Africa and Greece, devastated by the War. Having completed his National Service Calum was readmitted to seminary and spent the next six years in the Pontifical Scots College in Rome, studying philosophy and theology at the Gregorian University. He was ordained a priest in Rome on 5th July 1953 and returned to Scotland to minister in the Diocese of Argyll and the Isles.\nFr Calum spent the whole of his priesthood in parish ministry in the Diocese, first as a curate in Oban between 1954 and 1956, in Dunoon from 1956 to 1962, and in Daliburgh, South Uist, for a short time in 1963. In that year he was appointed Parish Priest of the neighbouring Parish, St. Mary’s Bornish, and then successively of St. Barr’s, Northbay, Barra (1966 – 72), St. Mary’s, Benbecula (1973-80), Our Lady Star of the Sea, Castlebay (1980-91), and St. Michael’s, Eriskay (1991-2001). The bare statistics give an indication of Fr Calum’s extensive priestly ministry in the Western Isles, but cannot show the work he did in each parish and the service he offered in local government, for development agencies, in cultural and sporting activities, and especially in promoting Gaelic. He loved being involved in everything that was happening, and was a most effective voice in the public bodies on which he served. It was a very special honour for him to be made a Freeman of the Western Isles in recognition of his contribution to Comhairle nan Eilean and other public bodies.\nIn 2001, having reached the age of 75, Fr Calum retired as Parish Priest, but chose to stay on in his beloved Eriskay. Through these last 11 years he offered great support to his fellow priests, by the many supplies he did both in the island and mainland parishes. His spiritual and human contribution was much loved by the people for whom he celebrated Mass and the sacraments through these later years. He was never averse to talking about his own experiences, particularly his achievements, but always with a touch of humour and humility. He had indeed gained a lot of knowledge and wisdom through his priestly life, and this shone through in the interviews he gave last year for the television series, Island Parish, his contribution to which many may remember.\nWe trust Fr Calum is at peace with the Lord – hopefully there is still a room for smokers in heaven! We thank God for his life, his priesthood, and all that he did in service of the community. We extend our sympathy to his brother Donald and his family, his sisters Morag and Margaret, his fellow priests, the people of Eriskay, and all his many friends who mourn his passing. May he rest in peace.\nFr Calum’s Funeral Mass and Burial will be on Eriskay on Friday July 20th at 11am. A Requiem Mass will also be celebrated in St. Columba’s Cathedral, Oban on Thursday July 19th at 1pm.\n+ Joseph Toal\nclick on the link below to leave a comment""]"	['<urn:uuid:c91fee17-e9b4-4667-8c7d-45046caeed3f>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	19	54	732
17	How do inland waterways and oceans handle pollution?	Inland waterways and oceans have different systems for managing pollution. For inland waterways, there is a network of reception stations in every country that collect oily and greasy waste from vessels, with ships paying a disposal fee of €7.50 per 1000 liters of gas oil bunkered to fund this system. For oceans, the International Convention for the Prevention of Pollution from Ships (MARPOL) regulates pollution by establishing caps on air pollutants in ships' exhaust gases, including nitrous oxides and sulfur oxides, and prohibits the release of ozone-depleting gases. MARPOL's regulations apply to approximately 80,000 ships that carry the majority of global freight transport.	['Part A of the Implementing Regulation specifies rules for handling oily and greasy waste arising from operating the vessel. Wastes occurring in the engine room are specifically addressed: i.e. used oil, bilge water, used grease, used cloths, used filters and similar wastes. Such wastes are generally the same for all inland vessels. The amount of the wastes occurring depends on vessel use: a greater amount occurs on vessels used frequently than on those used less often or little. In the engine room, the technical equipment and the condition of the machinery are also factors affecting the amount of waste. Consequently, inland vessels vary in terms of how frequently such wastes have to be deposited, yet every ship inevitably has to dispose of them several times a year.\nA network of reception stations is established in every country in order to provide the shipping industry with the best possible support. The system consists mostly of oil separator vessels that dock onto vessels during passage, whereas onshore reception stations do exist in isolated cases. The network is more developed in waterway regions with heavy shipping traffic than in remote areas navigated by few vessels. In certain cases, wastes can be deposited on request at a reception station not specifically designed for the inland shipping industry (e.g. tank trucks). It is necessary to consult with the national institution responsible.\nThe deposit of wastes is subject to certain rules. For example, not every kind of bilge water or used oil can be deposited, nor can just any amount of waste. The most important rule is that the substances must be of a sort that is produced during normal activities of the operation. This means that waste quantities arising from a shipwreck cannot be accepted unconditionally, for instance. Wastes that are mixed with chemicals cannot be deposited free of charge either, since treating such wastes incurs substantially greater expense compared with “normal” bilge water. Waste in such cases can not be considered as oily waste but may be deposit against payment of a fee.\nThe deposit facilities obviously require funding as well. This is provided based on the “polluter-pays” principle. The party causing (or potentially causing) waste in this case is the shipping industry. While the relevant types of waste occur on all ships, both the shipping industry and the environment stand to benefit from a well functioning waste collection system and easily accessible facilities. The associated costs are consequently charged to the shipping industry. Specifically, as of 1 January 2011 every ship pays a disposal fee of €7.50 per 1000 liters of (tax-free) gas oil bunkered. The fee is the same in every contracting country and for every vessel and entitles ships to deposit wastes free of charge.\nThe SPE-CDNI is a software solution the purpose of which, as an electronic payment system , is to handle Part A of the Convention on the collection, deposit and reception of waste produced during navigation on the Rhine and inland waterways (CDNI), which came into force on 1 January 2011. The SPE-CDNI is in active use in six countries (France, Germany, Netherlands, Belgium, Switzerland, Luxembourg). It covers the entire inland navigation fleet within the scope of the CDNI: approx. 16,000 vessels, 70% of the European fleet, 500,000 transactions per annum.\nCONVENTION ON THE COLLECTION, DEPOSIT AND RECEPTION OF WASTE GENERATED DURING NAVIGATION ON THE RHINE AND OTHER INLAND WATERWAYS\nSecrétariat de la CDNI, PALAIS DU RHIN\nPalais du Rhin 2, place de la République F-67082 Strasbourg cedex\nTél. 03 88 52 52 72\nFax. 03 88 32 10 72', 'Ryan B. Stoa is a Fellow in Water Law and Policy at Florida International University College of Law and Deputy Director of the Global Water for Sustainability Program.\nClimate change presents the international community with a monumental regulatory problem that transcends generations, sectors, and political boundaries. Yet comprehensive climate change legislation on the international and national level seems a long way off, as countries appear unwilling to alter the course of their economic development without reciprocal commitments from the rest of the international community. In the absence of such comprehensive legislation, legal mechanisms that indirectly regulate climate change have emerged as viable, albeit interim, options. Among these mechanisms, international freshwater and ocean agreements are unappreciated sources of indirect climate change regulation.\nThe Ramsar Convention on Wetlands of International Importance Especially as Waterfowl Habitat, for example, aims to reverse the loss of wetlands through the adoption of “wise use” or sustainable use principles. The Ramsar Convention requires 168 contracting states to designate at least one area as a wetland of “international importance” in which the wise use of the wetland must be promoted in order to maintain its ecological character. With wetlands covering more than six percent of the Earth’s surface and playing a key role as sinks for carbon emissions, the convention’s ability to mobilize international support for wetlands conservation and wise utilization is a critical—and often neglected—component of the community’s mitigation and adaptation approach to climate change. To date 2,188 sites have been listed as internationally important wetlands, covering a total area of over 805,440 square miles.\nJust as the Ramsar Convention represents an important international effort to protect wetlands, the 1994 United Nations Convention to Combat Desertification (UNCCD) aims to foster international cooperation to combat desertification and mitigate the effects of drought. The UNCCD explicitly recognizes the contribution “that combating desertification can make to achieving the objectives of the UN Framework Convention on Climate Change,” presumably because the challenges of combating desertification and mitigating the effects of drought are so intricately linked with climate change. Not only does climate change exacerbate desertification by making precipitation patterns more irregular, more direct forms of desertification—such as unsustainable agricultural practices and deforestation—eliminate another barrier ecosystem capable of absorbing atmospheric carbon dioxide. Thus, the UNCCD’s ability to mobilize support for combatting desertification has a significant impact on climate change mitigation and adaptation, while the treaty’s unique integration with the UNFCCC provides a model for future international environmental agreements to fit their objectives into a climate change framework.\nTreaties regulating the world’s oceans have even greater potential to indirectly regulate climate change. The oceans are the Earth’s primary heat sink, and therefore its first line of defense against climate change. The Intergovernmental Panel on Climate Change reported in 2013 that ninety percent of the energy increase in the earth’s climate system from 1971-2010 was absorbed in the ocean, including sixty percent that was absorbed by the upper ocean (0-700 meters). Polar ice melts, rising sea levels, and ocean acidification are all a result of the climate change-induced alterations in oceanic processes.\nAmong the United Nations Convention on the Law of the Sea’s hundreds of provisions, two stand out for their potential to mitigate climate change or induce much-needed adaptation. First, the Convention reinforces the “common heritage of mankind” principle, which implicates collective action on matters or areas of collective concern. The principle promotes international stewardship of marine resources, while invoking notions of generational equity that pervade climate change regulation discourse.\nSecond, the Convention provides an incentive for states to prevent further increases in sea level rise. All states are entitled to certain privileges that extend outward from their nautical baseline. However, the baseline is defined as the low-water line along the coast, and sea level rise would obviously submerge lands that are unlikely to be productive underwater while diminishing the reach of each affected state’s various jurisdictional zones. Even a withdrawal of a few feet can have tremendous consequences for a state’s legal rights, especially in areas where jurisdictional zones are contiguous or overlapping.\nThe International Convention for the Prevention of Pollution from Ships (MARPOL) offers another, more direct, contribution to climate change regulation efforts. Annex VI of MARPOL establishes caps on the type and quantity of air pollutants contained in ships’ exhaust gasses, including nitrous oxides and sulfur oxides. Annex VI also prohibits the deliberate release of ozone depleting gases, and it establishes a mechanism for creating control areas in which emissions are further restricted. While regulations on pollution from ships may not seem terribly significant to global climate change processes, consider that in 2012 alone the world’s fleet included 80,000 ships that carried an overwhelming majority of global freight transport. In this context, regulating air pollutant emissions from merchant vessels has the potential to prompt significant reductions in global greenhouse gas emissions.\nFurther amendments to MARPOL’s Annex VI may yield even more promising climate change mitigation measures, as recently adopted energy efficiency standards for ships have been touted as the first legally binding international treaty since the Kyoto Protocol to directly address climate change. With international shipping estimated to have contributed 2.7% of global emissions in 2007, MARPOL’s emissions reduction schemes and widespread political support make the treaty a promising source of climate change mitigation.\nUltimately, because of the water cycle’s crucial role in climatic processes, international agreements regulating various aspects of the water cycle are uniquely positioned to indirectly contribute to climate change mitigation and adaptation. While they are not known as “climate” treaties, Ramsar, UNCCD, MARPOL, and the Law of the Sea Convention are making unique contributions to the climate change challenge. Given the political sensitivity of direct climate change regulation, international environmental agreements with climate linkages represent a significant source of untapped regulatory potential.']	['<urn:uuid:3ad2ccad-555c-4dba-8e13-c7798be01ea2>', '<urn:uuid:69c523d3-b249-4cc6-9715-7c0a75f90ba0>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T15:30:12.136021	8	103	1549
18	Which ripening indicators are measured in dates versus perilla studies?	The studies use different ripening indicators for dates versus perilla. For dates, the focus is on measuring chemical composition changes, including ash levels, fiber content, and alcohol-insoluble solids. For perilla, multiple physiological indicators are measured including SPAD value (chlorophyll content), adjusted normalized difference vegetation index (NDVI), and maximal quantum yield of PSII photochemistry (Fv/Fm). Additionally, antioxidant activity measurements are used for perilla, tracking changes in specific compounds like rosmarinic acid and caffeic acid concentrations during development.	"['2 edition of Biochemical changes during ripening of some Sudanese date varieties. found in the catalog.\nBiochemical changes during ripening of some Sudanese date varieties.\nAziza Babiker Mustafa\nWritten in English\nThesis (M. Sc.)--The Queen""s University of Belfast, 1984.\n|The Physical Object|\nNotably, the process of ripening in tomato is also an important factor in determining the phytochemical content, as significant physiological, biochemical, and structural changes occur during ripening including degradation of chlorophylls, synthesis of carotenoids (mainly lycopene and β-carotene) and other phytochemicals, which in turn result. Peel firmness. The peel firmness of mango cultivars varied from initial level of – N to – N during ripening period of 10 days (Fig. 1).Early harvested mangoes of cv. CP and NT and mid harvested mangoes of cv. BA and KM didn’t show much variation in peel firmness with ripening period, where percent decline in peel firmness from 1 st to 10 th day of storage was .\nThe biochemical changes which occur during ripening involve the conversion of lactic acid, citric acid, milkfat and protein to a very wide range of flavour compounds. Mature cheese contains many hundred flavour compounds and the unique characters of different varieties are caused not by unique chemical compounds but rather by the same compounds. Five date palm (Phoenix dactylifera L.) varieties, commonly grown in the UAE, were studied for their chemical composition, physical, mechanical and rheological measurements were made at the ‘tamr’ stage, the final stage of fruit ripeness. Samples were analysed for sugar, moisture, protein, ash and mineral element contents.\nThe process of fruit ripening is normally viewed distinctly in climacteric and non-climacteric fruits. But, many fruits such as guava, melon, Japanese plum, Asian pear and pepper show climacteric as well as non-climacteric behaviour depending on the cultivar or genotype. Investigations on in planta levels of CO2 and ethylene at various stages of fruits during ripening . The study reports the changes in pigmentation and fruit composition during growth and ripening of spanish cherry, Mimusops elengi. Chlorophyll pigments decreased and carotenoids and anthocyanins increased. Sugar, starch and proteins concentration increased, while amino acids and phenolic contents decreased during successive growth stages.\nWhat you should know.\nThe vvoefull crie of Rome\nThe sonnets of Shakespeare\nConsolidation of fine grained dredged material after hydraulic deposition\nEarly life history of fish\nSome heretical thoughts on economic development\nReport of the National Home Economics Workshop on the Place of Home Economics Education in the Development of the Nigerian Child, held at Hill Station Hotel, Jos, 29th January-2nd February 1980\nprogressive diary firm\nHistory of Oro schools, 1836 to 1966.\nRoyal Ulster Academy 1881-1981\nFood Chemistry 49 () Biochemical changes in some tropical fruits during ripening M. EI-Zoghbi Food Science Department, Faculty of Agriculture, Zagazig University, Zagazig, Egypt (Received 7 October ; revised version received and accepted 5 March ) Changes during ripening in alcohol-insoluble solids (AIS) and dietary fibres of mango, guava, date Cited by: A.B.\nMustafa, D.B. Harper, D.E. JohnstonBiochemical changes during ripening of some Sudanese date varieties Journal of the Science of Food and Agriculture, 37 (), pp. Google ScholarCited by: Biochemical changes during ripening of some sudanese date varieties.\nJournal of the Science of Food and Agriculture37 (1), DOI: /jsfa N. Prentice, J. Refsguard. Enzymic Hydrolysis of Cited by: During the growth and ripening processes of mango fruit, changes in chemical composition occur (Figure 2), including decrease in ash level with some rise when nearing maturity, fiber remaining more or less constant, increase in the content of alcohol-insoluble solids due to starch accumulation, change of structural polysaccharides, and Cited by: 8.\nThere are various biochemical changes taking place during the phase of ripening of fruit such as changes in respiration, aroma, flavor, ethylene. Changes in the chemical composition of three popular Sudanese varieties of date (Jawa, Bentamoda and Mishrig Wad Laggai) during ripening are described and are related to changes in texture and in.\nChanges in fruit weight of five date palm cultivars during development and ripening. Data are the mean of and season.\nLSD at level P = for cultivar and stage effects are andrespectively. Download: Download full-size image; Fig. Changes in vitamin C concentration of five date palm cultivars during development and.\nCheese ripening gets completed in several steps where numerous biochemical and biophysical changes occur, all done by enzymes. In the first step, tasteless curd is developed from raw milk using rennets. Then, specific taste, aroma and flavor for the designated cheese is generated by the partial and gradual breakdown of carbohydrates, lipids, and proteins by specific enzymes during ripening.\nIn the area surrounding Oman, some 12 varieties of dates are commonly pro- duced (Ahmed et al., ). These date varieties can vary significantly in their chemical composition, especially the amounts of reducing, non-reducing sugars, as well as the amount and composition of dietary fiber (Ahmed et al., ; Mustafa et al., ).\nTechnical conference on the improvement of date production and processing. ripening period. ln: Punjab Horticultural Journal,12 (l), pp. et al., Biochemical changes during. Changes in cellulose content and activity of cellulase enzyme were evaluated during fruit ripening in three Galia F1 muskmelon cultivars.\nRespiration rate progressively increased in a climacteric pattern. Fruit flesh firmness steadily declined and rind color increased during ripening. Cellulose content progressively decreased with fruit ripening, and a positive. ZUBAIDY, AL- A., AL-TAI, W., SARKIS, J.\n() Some nutritive value indications of by-products of factories using dates as raw material and their change during storage. Palm and Dates Research Centre, Baghdad, Iraq. Effects of waxing, microperforated polyethylene bag, 1-methylcyclopropene and nitric oxide on firmness and shrivel and weight loss of ‘Manila’ mango fruit during ripening.\nPostharvest Biology and Technology, Key word: Biochemical indicators, some point defects, varieties of Palms Introduction The date palm (Phoenix dactylifera L.) is among the most important species in the Palm family (Arecaceae), which encompasses about genera and more than 2, species (Jain et al., ).\nIt is a long-lived monocotyledonous species and one of the tallest. Suitability of some Sudanese mango varieties for jam making Abdelazim A.\nNour 1, Khalid S. Khalid 1 and Gammaa A. Osman 2 1 Dept. of. The calcium content of different date varieties was within a range of – mg/ g and was significantly (P date varieties. These findings were within the wide range – mg/ g reported for some Iranian dates at the Tamr stage (Rastegar et al.\nDavid B. Harper\'s 83 research works with 3, citations and 3, reads, including: Seasonal changes in chlorine and methoxyl content of leaves of.\nIn many sugarcane growing areas of the world, sugar productivityper se is low due to physio-biochemical constraints and adverse climatic conditions during growth and ripening.\nThe cane growth and sucrose accumulation are severely impeded due to many factors such as poor sprouting and low tillering, climatic impediments during cane maturation, poor sprouting of.\nBioactive compounds contents, antioxidant and antimicrobial activities during ripening of Prunus persica L. varieties from the North West. Volatile compositions and some fruit characteristics of table apricot varieties from Turkey.\nChemistry of Natural Compounds43 (3) Volatile flavour substances of apricot and their changes during ripening. Food / Nahrung33 (5) Biochemical changes in some fruits at different ripening stages.\nFood Chemistry31 (1. Abstract. Grapevine (Vitis vinifera) is a model for the investigation of physiological and biochemical changes during the formation and ripening of nonclimacteric fleshy r, the order and complexity of the molecular events during fruit development remain poorly understood.\nTo identify the key molecular events controlling berry formation and ripening, we .This title contains up-to-date reviews of the literature on the chemical, biochemical, microbiological and physico-chemical aspects of cheese in general.\nCheese: Chemistry, Physics, and Microbiology Two-Volume Set, 3E is available for purchase as a set, and as well, so are the volumes individually.Changes in carbohydrate content of fruits during growth and maturation depend to a certain extent upon the type of fruit.\nThere are, for example, those fruits which, during ripening, accumulate a so-called “starch reserve.” Examples of this type are the apple (Pyrus malus L.), pear (Pyrus communis L.), and banana (Musa sapientum L.).', 'Influence of fertilizer and salicylic acid treatments on growth, physiological, and antioxidant characteristics in green and red Perilla frutescens varieties\nPerilla is herbaceous plant, functional food, and nutraceutical product with antioxidant properties. The objective of this study was to investigate the growth, reflectance indices, and antioxidant properties of P. frutescens species in response to fertilizer and salicylic acid (SA) applications. Two independent experiments were carried out in an environmentally controlled greenhouse: (1) pots of red-leaf and green-leaf cultivars divided into four groups treated with 10-30-20 (10N-13.1P-16.6K), 15-10-30 (15N-4.4P-24.9K), 20-20-20 (20N-8.7P-16.6K), and 30-10-10 (30N-4.4P-8.3K) fertilizers for periods of 10 weeks, and (2) pots of red and green Perilla cultivars divided into five groups treated with 0 (control), 125, 250, 500, and 1,000 µM of SA for periods of 7 weeks. Wide variations occurred in the agronomic performance, soil-plant analysis development (SPAD) value, adjusted normalized difference vegetation index (NDVI), maximal quantum yield of PSII photochemistry (Fv/Fm), and antioxidant activity of the two Perilla varieties. All the measured traits were higher in green than in red Perilla under identical fertilizing, and all agronomic traits in green and red Perilla plants subjected to 125 and 500 μM SA were better than in controls. The SPAD and NDVI values of all plants increased as N% increased, the lowest Fv/Fm values of all plants were observed under 15-10-30 fertilizer treatment, the lowest NDVI values were detected in controls, and the Fv/Fm values of all plants decreased under 1,000 μM SA treatment. These indices can be used as indicators to characterize the physiology of these plants and are suitable for evaluating their growth and development under specific fertilizer and SA treatments. Green Perilla leaf extract (PLE) contained higher rosmarinic acid (RA) concentration in each fertilizer treatment, and higher total phenolic (TP) and RA concentration in each SA treatment. However, red PLE contained higher caffeic acid (CA) concentration than green PLE in each fertilizer and SA treatment, implying that their two genotypes exhibited different abilities and specificities of photosynthetic metabolites, and that different varieties may prepare for 2,2-diphenyl-1-picrylhydrazyl (DPPH) radical scavenging activity by up-regulating TP, RA, and CA concentration differently. Thus, Perilla plants can be used as health foods due to high TP, RA, and CA concentration. To produce Perilla efficiently in industrial applications, we undertook to determine the optimum N-P-K fertilizer ratio and SA application for maximizing the growth and accumulation of TP, RA, and CA in Perilla plants.\nBallester C, Zarco-Tejada PJ, Nicola E, Alarco JJ, Fereres E, Intrigliolo D, Gonzalez-Dugo V (2018). Evaluating the performance of xanthophyll, chlorophyll, and structure-sensitive spectral indices to detect water stress in five fruit tree species. Precision Agriculture 19:178-193. https://doi.org/10.1007/s11119-017-9512-y\nBajwa S, Mishra A, Norman R (2010). Canopy reflectance response to plant nitrogen accumulation in rice. Precision Agriculture 11:488-506. https://doi.org/10.1007%2Fs11119-009-9142-0\nBeta T, Nam S, Dexter J, Sapirstein H (2017). Phenolic content and antioxidant activity of pearled wheat and roller-milled fractions. LWT-Food Science Technology 78:151-159. https://doi.org/10.1094/CC-82-0390\nBoatwright JL, Karolina PM (2013). Salicylic acid: An old hormone up to new tricks. Molecular Plant Pathology 14:623-634. https://doi.org/10.1111/mpp.12035\nBonneville M, Fyles J (2006). Assessing variations in SPAD-502 Chlorophyll meter measurements and their relationships with nutrient content of trembling aspen foliage. Communications in Soil Science and Plant Analysis 37:525-539. https://doi.org/10.1080/00103620500449385\nCaleja C, Barros L, Antonio A, Oliveira M, Ferreira I (2017). A comparative study between natural and synthetic antioxidants: Evaluation of their performance after incorporation into biscuits. Food Chemistry 216:342-346. https://doi.org/10.1016/j.foodchem.2016.08.075\nCamejo D, Rodriguez P, Morales M, Dell’Amico J, Torrecillas A, Alarcon J (2005). High temperature effects on photosynthetic activity of two tomato cultivars with different heat susceptibility. Journal of Plant Physiology 162:281-289. https://doi.org/10.1016/j.jplph.2004.07.014\nChao PY, Lin SY, Lin KH, Liu YF, Hsu JI, Yang CM, Lai JY (2014). Antioxidant activity in extracts of 27 indigenous Taiwanese vegetables. Nutrients 6:2115-2130. https://doi.org/10.3390/nu6052115\nChauhan NK, Singh S, Haider S, Lohani H, Kushwaha B (2013). Compositional variability in volatiles from different plant organs of Perilla frutescens L. cultivated in Uttarakhand (India). Journal of Pharmacy Research 6:361-363. https://doi.org/10.1016/j.jopr.2013.03.004\nChen Y, Wang E, Wei Z, Zheng Y, Yan R, Ma X (2019). Phytochemical analysis, cellular antioxidant, and α-glucosidase inhibitory activities of various herb plant organs. Industrial Crops and Products 141:111771. https://doi.org/10.1016/j.indcrop.2019.111771\nColica C, Di Renzo L, Aiello V, De Lorenzo A, Abenavoli L (2018). Rosmarinic acid as potential anti-inflammatory agent. Reviews on Recent Clinical Trials 13:240-242. https://doi.org/10.2174/157488711304180911095818\nD’Ambrosio N, Arena C, De Santo AV (2006). Temperature response of photosynthesis, excitation energy dissipation and alternative electron sinks to carbon assimilation in Beta vulgaris L. Environmental and Experimental Botany 55:248-257. https://doi.org/10.1016/j.envexpbot.2004.11.006\nDemmig-Adams B, Adams W, Barker DH, Logan BA, Bowlong DR, Verhoeven AS (1996). Using chlorophyll fluorescence to assess the fraction of absorbed light allocated to thermal dissipation of excess excitation. Physiologia Plantarum 98:253-264. https://doi.org/10.1034/j.1399-3054.1996.980206.x\nDevitt DA, Morris RL, Fenstermaker LK (2005). Foliar damage, spectral reflectance, and tissue ion concentrations of trees sprinkle irrigated with waters of similar salinity but different chemical composition. HortScience 40:819-826. https://doi.org/10.21273/HORTSCI.40.3.819\nDhyani A, Chopra R, Garg M (2019). A review on nutritional value, functional properties, and pharmacological application of perilla (Perilla frutescens L.). Biomedical and Pharmacology Journal 12:649-660. https://doi.org/10.13128/ahs-23828\nDiao M, Ma L, Wang J, Cui J, Fu A, Liu HY (2014). Selenium promotes the growth and photosynthesis of tomato seedlings under salt stress by enhancing chloroplast antioxidant defense system. Journal Plant Growth Regulation 33:671-682. https://doi.org/10.1007/s00344-014-9416-2\nGhane M, Mohammadi M, Pirdashti H (2019). Yield and physiological response of Perilla (Perilla frutescens) under different soil fertility treatments. Advances in Horticultural Science 33:205-214. https://doi.org/10.13128/ahs-23828\nGhanta S, Datta R, Bhattacharyya D, Sinha R, Kumar D, Hazra S, Mazumdar AB, Chattopadhyay S (2014). Multistep involvement of glutathione with salicylic acid and ethylene to combat environmental stress. Journal of Plant Physiology 171:940-950. https://doi.org/10.1016/j.jplph.2014.03.002\nGhimire BK, Yoo JH, Yu CY, Kim SH, Chung IM (2019). Profiling volatile and phenolic compound composition and characterization of the morphological and biological activities of Perilla frutescence Britton var. japonica accessions. Acta Physiologiae Plantarum 41:108. https://doi.org/10.1007/s11738-019-2890-1\nGhimire BK, Yoo H, Yu CY, Chung IM (2017). GC-MS analysis of volatile compounds of Perilla frutescens Britton var. Japonica accessions: Morphological and seasonal variability. Asian Pacific Journal of Tropical Medicine 10(7):643-651. https://doi.org/10.1016/j.apjtm.2017.07.004\nHa TJ, Lee JH, Lee MH, Lee BW, Kwon HS, Park CH, … Jang DS (2012). Isolation and identification of phenolic compounds from the seeds of Perilla frutescens (L.) and their inhibitory activities against a-glucosidase and aldose refuctase. Food Chemistry 135:1397-1403. https://doi.org/10.1016/j.foodchem.2012.05.104\nHabibi G (2018). Effects of mild and severe drought stress on the biomass, phenolic compounds production and photochemical activity of Aloe vera (L.) Burm.f. Acta Agriculturae Slovenica 2018:463-476. https://doi.org/10.14720/aas.2018.111.2.19\nHikosaka S, Iwamoto N, Goto E, Ching HC (2017). Effects of supplemental lighting on growth and medicinal compounds of Japanese Honysuckle Lonicera japonica Thunb. Environmental Control in Biology 55: 71-76. https://doi.org/10.2525/ecb.55.71\nHong E, Park KH, Kim GH (2011). Phenolic‐enriched fractions from Perilla frutescens var. acuta: Determinating rosmarinic acid and antioxidant activity. Journal of Food Biochemistry 35:1637-1645. https://doi.org/10.1111/j.1745-4514.2010.00481.x\nHuang C, Zhao S, Wang L, Anjum S, Chen M, Zou C (2013). Alteration in chlorophyll fluorescence, lipid peroxidation and antioxidant enzymes activities in hybrid ramie (Boehmeria nivea L.) under drought stress. Australian Journal of Crop Science 7:594-599. http://www.cropj.com/huang3003_7_5_2013_594_599.pdf\nJanda M, Ruellan E (2015). Magical mystery tour: Salicylic acid signalling. Environmental and Experimental Botany 114:117-128. https://doi.org/10.1016/j.envexpbot.2014.07.003\nKagawa N, Iguchi H, Henzan M, Hanaoka M (2019). Drying the leaves of Perilla frutescens increases their content of anticancer nutraceuticals. Food Science and Nutrition 7:1494-1501 https://doi.org/10.1002/fsn3.993\nKheirizadeh-Arough Y, Seyed-Sharif R (2016). Biofertilizers and zinc effects on some physiological parameters of triticale under water-limitation condition. Journal of Plant Interactions 11:167-177. https://doi.org/10.1080/17429145.2016.1262914\nKiazolu JB, Intisar A, Zhang L, Wang Y, Zhang R, Wu Z (2016). Phytochemical screening and chemical variability in volatile oils of aerial parts of Morinda morindoides. Natural Product Research 30:2249-2252. https://doi.org/10.1080/14786419.2016.1154058\nKim SJ, Bok KJ, Lam VP, Park JS (2017). Response of nutrient solution and photosynthetic photon flux density for growth and accumulation of antioxidant in Agastache rugosa under hydroponic culture systems. Protected Horticulture and Plant Factory 26:249-257. https://doi.org/10.12791/KSBEC.2017.26.4.249\nKim YB, Kim JK, Uddin MR, Xu HH, Park WT, Tuan PA, … Park SU (2013). Metabolomics analysis and biosynthesis of rosmarinic acid in Agastache rugosa Kuntze treated with methyl jasmonate. PLoS One 8:e64199. https://doi.org/10.1371/journal.pone.0064199\nKozai T, Niu G, Takagaki M (2015). Plant factory: An indoor vertical farming system for efficient quality food production. Amsterdam: Academic Press.\nLee YH, Kim B, Kim S, Kim MS, Kim H, Hwang SR, Lee JH (2017). Characterization of metabolite profiles from the leaves of green perilla (Perilla frutescens) by ultra-high-performance liquid chromatography coupled with electrospray ionization quadrupole time-of-flight mass spectrometry and screening for their antioxidant properties. Journal of Food and Drug Analysis 25:776-788. https://doi.org/10.1016/j.jfda.2016.09.003\nLevizou E, Drilias P, Psaras GK, Maneta Y (2005). Nondestructive assessment of leaf chemistry and physiology through spectral reflectance measurements may be misleading when changes in trichome density cooccur. New Phytologist 165:463-472. https://doi.org/10.1111/j.1469-8137.2004.01250.x\nLin T, Zhu X, Zhang F (2012). The interaction effect of cadmium and nitrogen on Populus yunnanensis. Journal of Agricultural Science 4:125-134.\nLin KH, Huang SB, Wu CW, Chang YS (2019). Effects of salicylic acid and calcium chloride on heat tolerance of Poinsettia. HortScience 54:499-504. https://doi.org/10.21273/HORTSCI13566-18\nLiu J, Wan Y, Zhao Z, Chen H (2013). Determination of the content of rosmarinic acid by HPLC and analytical comparison of volatile constituents by GC-MS in different parts of Perilla frutescens (L.) Britt. Chemistry Central Journal 7:61. https://doi.org/10.1186/1752-153X-7-61\nLu N, Takagaki M, Yamori W, Kagawa N (2018). Flavonoid productivity optimized for green and red forms of Perilla frutescens via environmental control technologies in plant factory. Journal of Food Quality 2018:4270279. https://doi.org/10.1155/2018/4270279\nLu N, Bernardo EL, Tippayadarapanich C, Takagaki M, Kagawa N, Yamori W (2017). Growth and accumulation of secondary metabolites in perilla as affected by photosynthetic photon flux density and electrical conductivity of the nutrient solution. Frontiers in Plant Science 8:708. https://doi.org/10.3389/fpls.2017.00708\nMarin A, Rubio JS, Martinez V, Gil MI (2009). Antioxidant compounds in green and red peppers as affected by irrigation frequency, salinity and nutrient solution composition. Journal of the Science of Food and Agriculture 89:1352-1359. https://doi.org/10.1002/jsfa.3594\nMartinetti L, Ferrante A, Bassoli A, Borgonovo G, Tosca A, Spoleto P (2012). Characterization of some qualitative traits in different perilla cultivars. Acta Horticulturae 939:301-308. https://doi.org/10.17660/ActaHortic.2012.939.39\nNaczk M, Shahidi F (2004). Extraction and analysis of phenolic in food. Journal of Chromatography A 1054:95-111.\nOgawa E, Hikosaka S, Goto E (2018). Effects of nutrient solution temperature on the concentration of major bioactive compounds in red perilla. Journal of Agricultural Meteorology 74:71-78. https://doi.org/10.2480/agrmet.D-17-00037\nOsakabe N, Yasuda A, Natsume M, Sanbongi C, Kato Y, Osawa T, Yoshikawa T (2002). Rosmarinic acid, a major polyphenolic component of Perilla frutescens, reduces lipopolysaccharide (LPS)-induced liver injury in D-galactosamine (D-GalN)- sensitized mice. Free Radical Biology and Medicine 33:798-806. https://doi.org/10.1016/s0891-5849(02)00970-x\nPark SY, Oh SB, Kim SM, Cho YY, Oh MM (2016). Evaluating the effects of a newly developed nutrient solution on growth, antioxidants, and chicoric acid contents in Crepidiastrum denticulatum. Horticulture, Environment, and Biotechnology 57:478-486. https://doi.org/10.1007%2Fs13580-016-1060-2\nPintha K, Tantipaiboonwong P, Yodkeeree S, Chaiwangyen W, Chumphukam O, Khantamat O, … Suttajit M (2018). Thai perilla (Perilla frutescens) leaf extract inhibits human breast cancer invasion and migration. Maejo International Journal of Science and Technology 12:112-123.\nSaeb K, Gholamrezaee S (2012). Variation of essential oil composition of Melissa officinalis L. leaves during different stages of plant growth. Asian Pacific Journal of Tropical Biomedicine 2:547-549.\nSakamoto M, Suzuki T (2015). Effect of root-zone temperature on growth and quality of hydroponically grown red leaf lettuce (Lactuca sativa L. cv. Red Wave). American Journal of Plant Sciences 6:2350-2360. https://doi.org/10.4236/ajps.2015.614238\nSelmar D, Kleinwächter M (2013). Stress enhances the synthesis of secondary plant products: the impact of the stress-related over-reduction on the accumulation of natural products. Plant and Cell Physiology 54:817-826. https://doi.org/10.1093/pcp/pct054\nSkowyra M, Falguera V, Azman NA, Segovia F, Almajano MP (2014). The effect of Perilla frutescens extract on the oxidative stability of model food emulsions. Antioxidants3:38-54. https://doi.org/10.3390/antiox3010038\nShen H, B Zhao, J Xu, X Zheng, W Huang (2016). Effects of salicylic acid and calcium chloride on heat tolerance in Rhododendron ‘Fen Zhen Zhu’. Journal of the American Society for Horticultural Science 141:363-372. https://doi.org/10.21273/JASHS.141.4.363\nSivanandhan G, Arun M, Mayavan S, Rajesh M, Mariashibu TS, Manickavasagam M, … Ganapathi A (2012). Chitosan enhances withanolides production in adventitious root cultures of Withania somnifera (L.). Dunal. Industry Crops and Products 37:124-129. https://doi.org/10.1016/j.indcrop.2011.11.022\nSwamy MK, Sinniah UR, Ghasemzadeh A (2018). Anticancer potential of rosmarinic acid and its improved production through biotechnological interventions and functional genomics. Applied Microbiology Biotechnology 102:7775-7793. https://doi.org/10.1007/s00253-018-9223-y\nTaga MS, Miller EE, Pratt DE (1984). Chia seeds as a source of natural lipid antioxidants. Journal American Oil Chemistry Society 61:928-931. https://doi.org/10.1007/BF02542169\nUmakanta S, Oba S (2018). Response of nutrients, minerals, antioxidant leaf pigments, vitamins, polyphenol, flavonoid and antioxidant activity in selected vegetable amaranth under four soil water content. Food Chemistry 252:72-83. https://doi.org/10.1016/j.foodchem.2018.01.097\nWeng JH, Jhaung LH, Lin RJ, Chen HY (2010). Relationship between photochemical efficiency of photosystem II and the photochemical reflectance index of mango tree: Merging data from different illuminations, seasons, and leaf colors. Tree Physiology 30:469-478. https://doi.org/10.1093/treephys/tpq007\nWhitehead D, Boelman NT, Turnbull MH, Griffin KL, Tissue DY, Barbour MM, … Peltzer DA (2005). Photosynthesis and reflectance indices for rainforest species in ecosystems undergoing progression and retrogression along a soil fertility chronosequence in New Zealand. Oecologia 144:233-244.\nWu CW, Lin KH, Lee MC, Peng YL, Chou TY, Chang YS (2015). Using chlorophyll fluorescence and vegetation indices to predict the timing of nitrogen demand in Pentas lanceolata. Korean Journal Horticulture Science Technology 33:845-853.\nYamori W (2016). Photosynthetic response to fluctuating environments and photoprotective strategies under abiotic stress. Journal Plant Research 129:379-395. https://doi.org/10.1007/s10265-016-0816-1\nYoshiki Y, Kahhara T, Sakabe S, Yamasaki T (2001). Superoxide and DPPH radical-scavenging activities of soyasponin beta bg related to gallic acid. Bioscience Biotechnology Biochemistry 65:2162-2165. https://doi.org/10.1271/bbb.65.2162\nYoshimatsu K (2012). Innovative cultivation: Hydroponics of medicinal plants in the closed-type cultivation facilities. Journal of Traditional Medicines 29:30-34. https://doi.org/10.11339/jtm.29.30\nZhao Y, Kong H, Zhang X, Hu X, Wang M (2019). The effect of Perilla (Perilla frutescens) leaf extracts on the quality of surimi fish balls. Food Science and Nutrition 7:2083-2090. https://doi.org/10.1002/fsn3.1049\nZhou X, Yan L, Yin P, Shi L, Zhang L, Liu L, Ma C (2014). Structural characterisation and antioxidant activity evaluation of phenolic compounds from cold-pressed Perilla frutescens var. arguta seed flour. Food Chemistry 164:150-157. https://doi.org/10.1016/j.foodchem.2014.05.062\nZou J (2011). Proteomics of rice in response to heat stress and advances in genetic engineering for heat tolerance in rice. Plant Cell Report 30:2155-2165. https://doi.org/10.1007/s00299-011-1122-y\nCopyright (c) 2021 Notulae Botanicae Horti Agrobotanici Cluj-Napoca\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nOpen Access Journal:\nThe journal allows the author(s) to retain publishing rights without restriction. Users are allowed to read, download, copy, distribute, print, search, or link to the full texts of the articles, or use them for any other lawful purpose, without asking prior permission from the publisher or the author.']"	['<urn:uuid:9f5fe0c8-7490-45b1-9ab1-f28e6b254888>', '<urn:uuid:a5637c5a-f8e8-40d8-b2c1-ba2ce8918d31>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T15:30:12.136021	10	76	3796
19	taxonomist studying species organization need to know how taxa arranged within catalog orders families species	Orders within a class follow a perceived phylogenetic sequence. At the level of family and below, all taxa are listed alphabetically: families within orders, genera within families, and species within genera. For infraspecific taxa, when there is only one rank within a species, trinomials are arranged alphabetically.	"[""The Indian Ocean, for the purposes of this catalogue, is defined as follows (see map). It is bounded on the west by the eastern shore of Africa as far south and west as Cape Agulhas. It is bounded on the east by the western shore of the Malay Peninsula and the southern coasts of Sumatra, Java, and the Lesser Sundas. The Strait of Malacca and Singapore are included. From the easternmost point of Timor, the boundary extends to Cape Londonderry, the northernmost point of Western Australia, and westward and southward to West Cape Howe, the southernmost point of Western Australia. Although the Indian Ocean, as sometimes defined, continues around the southern coast of Australia, we have excluded this region because its rich algal flora has been the subject of numerous scholarly monographs in which previous records are assessed and assigned to appropriate species, thus obviating the need for a catalogue in that region. In the southern part of the Indian Ocean we have included Amsterdam and St. Paul islands, but have excluded Iles Kerguelen and other subantarctic islands. Because the algae of the Red Sea have been the subject of a previous catalogue (Papenfuss, 1968c), we have excluded that area.\nWe have compiled from the literature all Indian Ocean records of benthic marine algae that have been identified to species, including records in papers that are principally concerned with biochemistry, physiology, ecology, or mariculture. Previous compilations (such as De Toni's Sylloge algarum) are included. Although in general we have restricted the content of this catalogue to published opinions and records, when faced with problems involving South African algae we have consulted and cited the unpublished observations that G.F. Papenfuss assembled during 45 years of study. He tried to verify all published records of South African algae by examining the pertinent specimens housed in various European herbaria. He left notebooks, manuscripts, and annotations that convey his taxonomic opinion concerning these specimens. In the body of the catalogue we refer to this material as ``Papenfuss (notes)''.\nSequence of Taxa\nAbove the level of family, the sequence of taxa follows an assumed phylogenetic scheme, with class being the highest rank employed. While divisions (phyla) are important in general biological classifications and thus are of immediate concern to phylogeneticists, teachers, and textbook writers, we consider them irrelevant to the field phycologist for whom the catalogue was designed. Were we to include them, we would need to decide which names to use. The International Code of Botanical Nomenclature (ICBN) permits names of taxa above the rank of family to be taken either from distinctive characters, resulting in descriptive names, or from the name of an included genus, resulting in typified names (Arts. 16 and 17). The ICBN recommends (Rec. 16A.1) that the name of a division (phylum) end in -phyta for all plants except fungi, for which the ending -mycota is recommended. For algae, the ending -phycophyta is sometimes used (Papenfuss, 1946), but we prefer -phycota, which retains the link with algae but is shorter and parallels the fungal ending -mycota. Thus, if we were to include divisions in this catalogue, we would use the descriptive names Cyanophycota (with the single class Cyanophyceae), Rhodophycota (with the single class Rhodophyceae), Chromophycota (including Phaeophyceae and Xanthophyceae), and Chlorophycota (with the single class Chlorophyceae). We have used descriptive names for the five classes recognized in this catalogue, with the ending -phyceae as recommended by Rec. 16A.3.\nWe have considered the advantages and disadvantages of arranging the orders within a class alphabetically rather than phylogenetically. When forced by frustration to consult the index, we are inclined toward an alphabetical sequence, but keeping in mind the deeply engrained habit of looking for the Ceramiales at the end of the reds and the Ectocarpales at the beginning of the browns, we decided on a perceived phylogenetic sequence. At the level of family and below, all taxa are listed alphabetically: families within orders, genera within families, and species within genera. The sequence of infraspecific taxa, which is of greatest concern in Sargassum and Caulerpa, is less straightforward. When there is only one infraspecific rank within a species, the trinomials are arranged alphabetically. When both varieties and formae are involved, the varieties are listed alphabetically (except that the nomenclaturally typical variety is listed before other varieties), with formae listed alphabetically within a variety.\nFor each specific or infraspecific entry, the accepted name or its basionym is listed first, followed in chronological order by combinations or replacements of the basionym (i.e., homotypic or nomenclatural synonyms). Next, taxonomic (heterotypic) synonyms are listed in chronological order, again with the basionym listed first, followed in chronological order by combinations or replacements of the basionym. Lastly, misapplied names are listed according to the chronology of their misapplication.\nFor each name (except misapplied names) the author and place of publication are given. If a name was not formed according to the rules of the ICBN when it was published, we have made the appropriate corrections and appended the original spelling in single quotation marks within parentheses. The type locality of each accepted name and taxonomic synonym is indicated. When the orthography of the type locality as cited in the protologue differs from modern orthography, or when historical names differ from modern names, the modern equivalent is given in brackets. Centered headings of names with Indian Ocean types and basionyms of taxonomic synonyms with Indian Ocean types are preceded by an asterisk. All combinations of a basionym with an Indian Ocean type are listed on the assumption that the combining author was making a taxonomic statement about the type. For a basionym with a non--Indian Ocean type, a combination is listed only if it is the accepted name or if Indian Ocean records have been published under that name. For a basionym or combination with a non--Indian Ocean type, the phrase ``including Indian Ocean record'' has been added when the author simultaneously cited at least one Indian Ocean collection. Following each name is a chronological list of references to Indian Ocean records under that name. Usually the heading ``Misapplied name'' includes a reference to the authority for the misapplication, but in some instances this information is given in the accompanying note. The authority for taxonomic synonymy is always given in the accompanying note.\nThe reported distribution of species is indicated by an alphabetical listing of countries (e.g., Mozambique, Sri Lanka, Tanzania) or, where it seemed more informative, by geographic unit irrespective of political affiliation (e.g., Andaman Islands, Christmas Island). For Indonesian records deriving from only a few islands, the islands have been named, but when numerous islands are involved, Indonesia alone is listed. It should be kept in mind that Australia, Indonesia, South Africa, and Thailand all have extensive non--Indian Ocean as well as Indian Ocean coasts.\nA note has been appended to many entries. These notes are primarily used to give the authority for taxonomic synonymy and less often the authority for misapplications. They also provide reasons for treating a name as invalidly published or illegitimate, explanations of unusual, complicated, or controversial taxonomic and nomenclatural situations, and discussions of alternative taxonomic opinions. Particularly long discourses have been removed to an appendix called Taxonomic and Nomenclatural Notes.\nThroughout the notes there is frequent reference to the International Code of Botanical Nomenclature (Greuter et al. (eds.), 1994). Articles, recommendations, examples, and notes are cited from the current edition (the Tokyo Code) unless otherwise stated. Because the Tokyo Code incorporates substantial changes and a renumbering of articles and other components, the user of this catalogue is urged to obtain access to this current edition. The location of particular specimens (mostly types) is indicated by citation of herbarium abbreviations assigned by the International Association for Plant Taxonomy (P. Holmgren, N. Holmgren, & Barnett, 1990). References to pages of this catalogue are placed within square brackets while those in other publications are placed within parentheses. Usually, however, the distinction is made clear by the context.\nAn orphaned infraspecific taxon is a taxon that does not fit neatly into our chosen taxonomic and nomenclatural scheme. To make it fit would require an investigation leading to its disposal as a synonym or to its acceptance under a new combination, an investigation beyond the scope and means of this catalogue. Each such taxon is appended to the accepted taxon deemed most closely related. An example is the treatment of Caulerpa clavifera forma remota Svedelius, which follows the last entry of C. racemosa var. racemosa. Caulerpa clavifera (Turner) C. Agardh was placed in the synonymy of C. racemosa var. racemosa by Papenfuss & Egerod (1977), but not all infraspecific taxa within C. clavifera have been transferred to C. racemosa. It may be assumed that an investigation of the type of C. clavifera forma remota would result in an opinion regarding the taxonomic value and placement of this taxon. Approximately 70 orphans are included in this catalogue.\nVarious records have been excluded from the systematic part of this catalogue because they are under names of uncertain application or under names of taxa excluded from the Indian Ocean as defined herein. These excluded records are presented in an appendix in which the entries are listed alphabetically in one series, regardless of class.""]"	['<urn:uuid:676a5f02-c9ba-42a0-8a63-bfbe8ef16084>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	15	47	1532
20	How do modern Greeks preserve culture and longevity?	Modern Greeks preserve their culture through celebrations featuring traditional food, music, dancing, and sharing immigrant stories, as exemplified by St. Nicholas Greek Orthodox Church's events. Additionally, their traditional Mediterranean diet, which includes foods like olive oil, fish, and legumes, has been scientifically linked to longer telomeres - DNA components that protect genetic information and are associated with increased lifespan.	"['Guests to delve into Greek culture at Gala\nBy Kristin Burns\nThe Greek Gala, hosted by the St. Nicholas Greek Orthodox Church, will give guests the opportunity to immerse themselves in Greek culture through food, music and dancing.\nDana Telep, a co-chair of the Greek Gala and a member of the church, said the event will allow guests to experience Greek culture and food at its finest.\nThe gala will be at 6 p.m. Saturday at the Phoenix Ballroom at 401 S. Third St. in Waco.\nTickets are $35 and can be ordered at stnickwaco.org. Tickets for the event are selling fast, Telep said, and will not be available at the door.\nAppetizers and music will be from 6 to 7 p.m. in the ballroom, and the dinner buffet will open at 7 p.m. Dessert and dancing will follow.\nGuests are invited to learn the traditional Greek dances with the dancers and explore the bookstore that will be at the venue.\n“The Greek Gala is open to Wacoans and Baylor’s family, certainly this year with Parents Weekend going on,” Telep said. “This would be a fun thing to do after the football game.”\nFor the past 25 years, St. Nicholas has hosted the annual Greek Food Festival. This year, the Greek Gala will be replacing the festival as a three-course, sit-down dinner with samples of authentic Greek food. Members of St. Nicholas are preparing the food based on family recipes.\n“We have recipes going back through families and generations,” Telep said. “It’s going to be the best you can find in Waco.”\nThe appetizers of the evening will be spanakopita (spinach pastries), tiropita (cheese pastries), keftedes (Greek meatballs) and hummus and pita.\nThe main course will include chicken or beef souvlaki, rice pilaf, Greek village salad, pastitsio (pasta dish) and dolmades (stuffed grape leaves).\nThe dessert menu is baklava, vanilla ice cream and coffee.\n“What we have is a tour of the typical kinds of food that we associate with the Greek world,” said Dr. Daniel Nodes, chair of the classics department at Baylor and a member of St. Nicholas.\nNodes said the Gala is a celebration of both ethnic and religious originations.\n“This is a celebration of a particular nation, part of the world, cuisine, music, dance forms and other forms of arts,” Nodes said. “It has a way of celebrating Greek contributions to American civilization.”\nThe Greek Gala is an opportunity for the members of St. Nicholas to educate the people of Waco about their heritage.\nMany of the members of the church are descendants from immigrants of Greece.\n“The immigrants never wanted to lose their roots,” Nodes said. “How do you show that? You celebrate. You celebrate life. It really is a rich experience.”\nAt the Gala, guests will experience live music played on Greek instruments during their appetizer and main course.\nA troop of dancers will attend the event as an example of the ancient tradition of regional dance.\nThe historical displays at the event will focus on the personal stories of Greek immigrants who settled in Waco and how they impacted the community with their arrival.\n“Many restaurants are still owned by Greek immigrants,” Telep said. Some restaurants started by Greek immigrants and members of St. Nicholas are The Elite Café and D’s Mediterranean Grill.\nThe Greek Gala will continue the annual tradition of the Greek Food Festival in the future.\n“The Greek Gala is a way for St. Nicholas Greek Orthodox Church to participate with the Waco community and invite them into our culture,” Telep said.', ""Mediterranean Diet Again Linked To Longer Life Span\nIf you’re still looking for a reason to switch to a Mediterranean diet, the latest research on how the healthy eating plan could potentially help you live longer might be the final piece of proof you need.\nFollowed by those living in Greece, Southern Italy and Spain, this diet takes advantage of the region’s bounty, and is rich in fruits, vegetables, nuts and seeds, olive oil, legumes and fish – as well as a regular glass of antioxidant-rich wine.\nAnd according to new research that appeared earlier this month in the British Medical Journal, the diet could lead to a longer life span, based on one marker of aging.\nTelomeres tell the story\nA team of researchers from Harvard Medical School, Brigham and Women’s Hospital, the Harvard School of Public Health and the University of Washington in Seattle, among others, found that women who followed a Mediterranean diet had longer telomeres than those who did not.\nTelomeres are essentially bits of DNA that protect the ends of chromosomes along with our genetic information. They have been compared to a bomb fuse, because every time a cell divides, the telomeres shorten, until eventually, the cell can no longer divide, and it becomes either inactive and dies, a process that has been linked to several severe health conditions. (Ref. 1)\nThis new study, however, reveals that the Mediterranean diet – rich in antioxidants that may help resist free radical damage caused by stress and pollution - can slow the shortening of telomeres, reducing the associated risks.\nThe secret to cellular youth\nAs part of the study – the largest to address the relationship between diet and telomeres - researchers studied 4,676 women who were enrolled in the Nurses’ Health Study, an ongoing trial that has tracked the health and habits of more than 120,000 registered nurses in the U.S. since 1976.\nLed by Immaculata De Vivo, associate professor of medicine at Harvard Medical School and Brigham and Women’s Hospital, the team of esteemed researchers found that women who ate a Mediterranean diet had longer telomeres than those who ate more red meat and dairy products. (Dairy is included in the Mediterranean diet – think feta cheese and Greek yogurt – but amounts are somewhat limited.)\nEssentially, their cells appeared to be younger.\nThat makes choosing delicious, garlicky hummus (see recipe at the end) over a mayo-based ranch dip a pretty easy choice, doesn’t it?\n“To our knowledge, this is the largest population-based study specifically addressing the association between Mediterranean diet adherence and telomere length in healthy, middle-aged women,” De Vivo said in a statement. “Our results further support the benefits of adherence to this diet to promote health and longevity.” (Ref. 2)\nWhy the Mediterranean diet is so powerful\nAccording to researchers, oxidative stress and chronic inflammation (due to the immune system’s inability to effectively manage the inflammation process in the body) both are believed to accelerate the shortening of telomeres, which are particularly sensitive to damage from oxidation.\nBecause the Mediterranean diet is so rich in nutrients that support healthy inflammation management and help the body address oxidative stress – including omega-3 fatty acids from fish, nuts and olive oil – it may help reduce the impact of these two brutal enemies of our youth. (Ref. 3)\nIf you’re eating a healthy diet, is that protection enough? Maybe, researchers said, but the Mediterranean diet offered the most age defying benefits.\n“Our findings showed that healthy eating, overall, was associated with longer telomeres,” said study co-author Marta Crous-Bou, a postdoctoral fellow in the Channing Division of Network Medicine at Brigham and Women’s. “However, the strongest association was observed among women who adhered to the Mediterranean diet.” (Ref. 4)\nAre you too old to defy aging?\nThe best part about the benefits of a Mediterranean diet is that you’re probably never too old to enjoy it.\nAdherence to the Mediterranean diet may reduce the risk of developing health conditions linked to junk food irrespective of age, sex, race or culture. “This diet has a beneficial effect, even in high risk groups, and speaks to the fact that it is never too late to start eating a healthy diet,” said Demosthenes Panagiotakos, Ph.D., a professor at Harokopio University, in Athens, Greece, who headed the analysis. (Ref. 5)\nMediterranean diet options\nIf you’re looking for fresh ways to up your health factor by revamping your diet and going Mediterranean, include more broiled fish, make salads the focus of some meals and consider this delicious hummus recipe as an afternoon snack.\n2 cans chickpeas, drained, rinsed and peels removed\n3 cloves garlic\n¼ cup tahini\n¼ cup olive oil\n1 large roasted red pepper\nJuice of one lemon\n1 teaspoon ground cumin\nSalt to taste\nIn a food processor, mix the chickpeas, garlic, tahini, olive oil, roasted red pepper, lemon juice and cumin until the mixture is smooth and well blended.\nAdd extra olive oil or a few tablespoons of water if your hummus is too thick.\nNote: Dried chickpeas are also excellent for making hummus, if you have more time. Soak the chickpeas overnight and rinse them thoroughly. Put them in a large pot and cover with cold water. Add 2 teaspoons salt and ½ teaspoon baking soda and simmer until the chickpeas are soft and easily mashed, about 1 hour. Drain and blend. (The baking soda softens the skins so they do not need to be removed. You can skip that step if using canned chickpeas, but the texture will not be as smooth.)\nServe your hummus with carrot and celery sticks, sesame sticks or warm whole-grain pita bread.\nYou may also like...\nTotal Women's Health\nSeptember 2021 by, Xtend-Life Team\nRight now, our health, resilience and immunity is being challenged like never before, meaning foundational health - taking...Read More\nTotal Balance Reformulation\nSeptember 2021 by, Xtend-Life Team\nThe new formulation of Total Balance means our customers can continue to take advantage of the efficacy, quality...Read More\nHealth Matters Newsletter - September 2021\nSeptember 2021 by, Warren MatthewsRead More""]"	['<urn:uuid:2c5b18be-bff3-4263-9f06-7a4d93bc02e7>', '<urn:uuid:b50b9928-abb4-491e-985d-e63ea7ace6a4>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	8	59	1605
21	global disability statistics wearable exoskeleton applications medical conditions benefits patient mobility improvement	According to WHO and World Bank data, over 1 billion people globally suffer from some form of disability, including 95 million children. Specifically, 250,000 to 500,000 people suffer from spinal cord injuries. Wearable exoskeletons have demonstrated effectiveness in helping patients with various conditions, including MS, spinal cord injuries, and stroke patients. Research has shown that these devices can reduce metabolic expenditure during walking, decrease muscle activity in lower extremities, and enable more efficient mobility. For MS patients in particular, exoskeleton-assisted walking has proven to facilitate daily activities and improve quality of life through reduced physical strain and enhanced walking capability.	"[""Analysis of Wearable Exoskeletons market covering 30 + countries including analysis of US, Canada, UK, Germany, France, Nordics, GCC countries, Japan, Korea and many more\nThe global wearable exoskeleton market is poised to generate highly lucrative gains and surge ahead at an astonishing CAGR of over 30% from 2022 to 2032, with growth largely reliant on future technological innovations and product developments. As muscle fatigue and disorder prevalence rates rise, key industries are demanding more physical comfort-enhancing solutions, prompting manufacturers to introduce new product lines.\nMarket Size Value in 2022\nUS$ 282.88 Mn\nProjected Market Forecast Value by 2032\nUS$ 3,991 Mn\nGlobal Growth Rate (2022 to 2032)\nGrowth Rate of the U.S\nKey Companies Profiled\nSignificant breakthroughs are expected with regard to soft wearable exoskeletons. Traditional hard wearable exoskeletons, although quite effective, have limited applicability as comfort levels are not that high. Key players are incorporating smart carbon fibers to manufacture these exoskeletons, a breakthrough that is likely to have significant ramifications on the adoption of wearable exoskeletons and eventually market growth.\nDon't Need a Global Report?\nSave 40% on Country & Region specific reports\nAccording to Fact.MR, the global wearable exoskeleton market was valued at over US$ 100 Mn in 2021, registering an incremental CAGR of 25% from 2017 to 2021. Realizing the application potential, key manufacturers have been introducing a slew of innovative product lines, offering highly customized solutions based on industry-specific requirements.\nIn spite of a COVID-19-induced deceleration, the market performed significantly well throughout 2020, attributed to extensive uptake across the healthcare industry. With key stakeholders, including doctors, nurses, administrative staff, and clinicians experiencing elongated work hours, exoskeleton deployment helped reduce physical stress levels to a very large extent.\nAs the intensity of the pandemic gradually lifted in 2021, growth prospects experienced an uptick, as key end-use industries such as military & defense, industrial manufacturing, and others experience a resurgence. Moreover, planned technological innovations and greater integration of AI and machine learning are further expected to enhance growth prospects, with the market projected to expand at a stellar CAGR of over 30.3% through 2031.\nAccording to the World Health Organization and World Bank’s joint study, over a billion people (15% of the global population) suffer from some of the other forms of disability. Moreover, the Global Burden of Disease Study estimates childhood disability prevalence to be 95 million (5.1%) children, with 13 million (0.7%) having an acute disability.\nFurthermore, the growing incidence of strokes, injury-induced trauma, as well as rising number of road hazards is leading to an increase in the number of physically disabled patients, globally. Amongst all of those, spinal cord injuries are the maximum, with almost 250,000 to 500,000 people suffering from them. The majority of them are due to preventable causes such as falls, violence, or road accidents.\nAttributed to these trends, demand for wearable exoskeletons is expected to remain sustained throughout the forecast period. Since almost nine out of ten injuries arise from traumatic wounds, prominent manufacturers are designing appropriate wearable exoskeletons that help reduce stress on the spinal cord. In 2018, researchers at the Universidad Miguel Hernandez in Elche, Spain, developed a robotic smart exoskeleton, which, when combined with a robotic wheelchair, helps disabled people to eat, drink, or wash.\nMore Insights, Lesser Cost (-50% off)\nInsights on import/export production,\npricing analysis, and more – Only @ Fact.MR\nSince 2019, a multitude of manufacturers are striving to introduce lightweight wearable exoskeletons, including personalized wearable exoskeletons, a trend that is expected to majorly determine the future growth trajectory of the market. A prototype lower-body device was developed via a collaboration between The City University of New York, City College (CCNY), and The University of Texas Health Science Center at Houston (UTHealth).\nThe exoskeleton is intended for use by seniors who possess some degree of mobility but find it difficult to walk for extended periods. According to a Journal of Public Health report, over 30% of people aged 65 or older find it challenging to walk the equivalent of three city blocks. Hence, this exoskeleton provides assistance in this regard by utilizing integrated motors providing an electric boost to the wearer’s gait, and is quite light in weight.\nIn December 2020, Italy-based Comau announced the launch of MATE-XT, a new version of its Muscular Aiding Tech Exoskeleton which features a slim, lightweight carbon fiber structure and an intuitive regulation system, permitting easy adjustment of the system’s water-, dust-, ultraviolet light-, and temperature-resistant design. The exoskeleton is deployed in agriculture, carpentry, construction, assembly, logistics, and other industries.\nAs wearable exoskeletons are a relatively new technological advancement, the landscape is interspersed with the emergence of countless start-up companies, besides existing established players. With key industries looking to integrate flexible solutions, demand for wearable exoskeletons is only poised to incline. Hence, governments are making the investment climate more favorable.\nSuitX is amongst the most prominent wearable exoskeleton manufacturers, offering an exoskeleton that attaches to the user’s legs. This exoskeleton is lightweight, adjustable, and does not limit the tools carried on a worker’s belt. The solution is highly effective in preventing knee injuries and reducing the load on the quadriceps.\nAdditionally, U.S.-based Levitate Technologies designs and develops upper extremity musculoskeletal health wearables for professionals and skilled trade workers. Titled AIRFRAME, this wearable exoskeleton helps workers perform repetitive tasks with their hands extended above their heads for prolonged periods. The mechanical support offered reduces muscle stress and fatigue.\nKnow thy Competitors\nCompetitive landscape highlights only certain players\nComplete list available upon request\nThe U.S. is expected to yield credible gains to the global smart exoskeleton market, especially across the military sector. The U.S. army has been exploring commercial exoskeleton technologies for potential military applications, mostly to reduce strain injuries and build soldiers’ endurance. A CAGR of around 25% is projected for the U.S market.\nWearable exoskeletons have proven to be capable of enhancing the physical capabilities of a warfighter, permitting them to run faster and lift much heavier objects without straining any of their joints. The U.S. Army Natick Soldier Research and Engineering Center (NSRDEC/Soldier Center) initiated a 48-month long Other Transaction Agreement (OTA) worth US$ 6.9 million for developing exoskeleton systems in 2019.\nAdditionally, Lockheed Martin Corporation developed the OnyxTM Missiles and Fire Controls lower-body exoskeleton suit in 2018. The project is also a beneficiary of the Soldier Center’s grant initiative. The system combines mechanical knee actuators with multiple sensors and artificial intelligence software to improve strength and endurance. Also, Dephy’s Exo-boot uses the FlexSEA technology to simplify the electronics behind wearable robotics designs.\nExoskeletons have been acquiring rapid popularity across the U.K., falling under the Great British Technology area of Robotics and Autonomous Systems. Since many years, exoskeleton deployment has been the focus of significant public spending. Innovate the UK, the country’s innovation agency, has funded £22 million, while other research organizations have contributed over £200 million pounds for this purpose.\nOne such company funded through government initiatives includes Elumotion, which has developed a fully functional robotic hand commercially available, owned, and marketed as the Schunk Servo-electric-5-Finger Gripping Hand. The Elu2-Hand was a human-scale anthropomorphic robot hand able to approximate real hand movements at humanlike speeds, making it ideal for interacting with people. A CAGR of around 20% is projected for the market in the U.K.\nThe German wearable exoskeleton industry is riding the high wave, with demand surging for healthcare and industrial wearable exoskeletons. As part of the Horizon Europe initiative, Germany aims to intensify R&D activities to bring about a revolution in the regional healthcare market. A major expected breakthrough is the increased incorporation of robotics and automation.\nWhile exoskeleton manufacturers have been collaborating with healthcare providers for a long, significant headway has been achieved in the very recent past. In March 2020, for instance, ReWalk Robotics, a developer of lower limb disability exoskeleton, reported significant progress with a group of German Statutory Health Insurers (SHI) to allow any eligible beneficiary with a spinal cord injury to seek procurement for ReWalk 6.0 for home and community use.\nSimilarly, German Bionic’s Cray-X Robot-as-a-Service (RaaS) was unveiled in February 2020, designed to support and enhance the wearer’s movements, and reduce the risk of accidents and excessive strain for employees in sectors that involve repetitive yet unpredictable heavy lifting. As per Fact.MR, the German market is forecast to surge at a CAGR of 22% throughout the 2022-2032 assessment period.\nBy product, while rigid smart exoskeletons remain significantly popular. Demand for soft smart exoskeletons is expected to register high growth, owing to the rising need for incorporating flexible wearable technologies with a view to reducing discomfort. Soft exosuits, as they are also widely known, offer more conformal, unobtrusive, and compliant means to interface with the human body.\nThe Harvard Biodesign Lab is largely credited with introducing soft wearable exoskeletons, which incorporate structured functional textiles that are inspired by an understanding of human biomechanics and anatomy. These wearable garments provide means to transmit assistive torques to a wearer’s joints without the use of rigid external structures. Moreover, they are lightweight, offering efficient actuation.\n“Robotic wearable exoskeletons to surge in demand for lower extremities“\nIn terms of extremity, lower extremity wearable exoskeletons are garnering significant traction, characterized by rapid technological advancements in powered exoskeletons for the lower limb. With a rising proportion of lower limb neuromuscular impairments, clinicians are demanding more robust mobility solutions, thus fueling demand.\nIncreasing numbers of neurological diseases, such as spinal cord injuries (SCIs), and strokes, tend to have a negative impact on quality of life. Thus, regaining the ability to walk is one of the key objectives of rehabilitation. The segment is likely to capture 3/5th of the global market revenue.\nGrowing incidence of muscle strain and subsequent onset of joint stiffness is laying bare the ground for enhanced sales of rehabilitation wearable exoskeletons through 2022 and beyond. Fact.MR expects this segment to surge at a CAGR of 20%.\nA highly effective alternative to conventional wheelchairs, rehabilitation exoskeletons are going the distance in terms of enhancing physical mobility. Players such as Ekso Bionics, Rex Bionics, and ReWalk Bionics are swiftly penetrating the global wearable exoskeleton space, offering a diverse range of rehabilitation mobility solutions.\nIncreasing product functionality is a key factor for the growing demand for smart exoskeletons. Therefore, market players are increasing their R&D spending in order to launch new products in the market. Furthermore, both acquisitions & mergers, collaborations, and regulatory approvals are also crucial components of effective growth.\n- Premium Report Details -\n- Let's Connect -\n- Quick Contact -\n- Get Started -\nGet insights that lead to new growth opportunitiesBuy Now\nGet A Special pricing for start-ups and universitiesEnquiry Before Buying\n- Humble, Yet Honored -\nThe global wearable exoskeletons market size is likely to be worth US$ 282.88 Bn in 2022\nFrom 2017 to 2021, the wearable exoskeletons industry flourished at a CAGR of 25%\nThe wearable exoskeletons market is expected to reach nearly US$ 3,991 Mn by 2032\nIncreasing demand for exoskeletons across multiple industries, investment from major industry players, and the establishment of numerous start-ups, along with the need to increase individual efficiency and reduce fatigue.\nThe rising number of strokes and spinal cord injuries are driving the global wearable exoskeletons market.\nMajor market players include EksoBionics, ReWalkRobotics, Bionik, Inc., Cyberdyne Inc., and Rex Bionics Ltd.\nU.S., UK, Germany, France, and China are the top 5 countries driving demand for wearable exoskeletons in the market.\nWearable exoskeletons demand is expected to rise at a 25% CAGR in the U.S through 2032\nNeed An Exclusive Report For Your Unique Requirement?"", ""Wearable robotic exoskeletons may soon help people with advanced multiple sclerosis (MS) walk more efficiently and confidently again. Exoskeleton technology reduces the amount of energy and muscle exertion needed to initiate and control the process of walking, according to research presented last week at the Association for Academic Physiatrists (AAP) Annual Meeting in Sacramento, California. The AAP is the only academic association in the world dedicated to the specialty of physical medicine and rehabilitation (PM&R) — a community of physicians, researchers, in-training physiatrists, and others involved or interested in PM&R.\nMS, afflicting an estimated 2.3 million people worldwide, is a chronic, currently incurable, and potentially disabling disease with an array of symptoms that often include leg numbness and weakness, tremors, diminished coordination, unsteady walking and, eventually, paralysis.\nOnce considered the stuff of science fiction, wearable exoskeletons are now poised to enter mainstream healthcare as rehabilitation tools that can enhance physical function and quality of life, and are designed to facilitate walking for people with neurological/neuromuscular disorders. However, few studies have been conducted to evaluate the degree of assistance exoskeletons can provide.\nThe study presented at the AAP meeting, “Metabolic Expenditures during Exoskeleton-Assisted Walking in Person with Multiple Sclerosis,” was co-authored by researchers representing The University of Texas Health Science Center at Houston (UTHealth), TIRR Memorial Hermann, and Texas Women’s University, and examined the feasibility and effectiveness of exoskeletons to assist persons with MS in walking as part of a study funded by the National Multiple Sclerosis Society.\nThe team notes that wearable exoskeletons are designed to be utilized to facilitate walking for individuals with neurological conditions such as spinal cord injury, but the amount of assistance provided during exoskeleton-assisted walking had not been quantified scientifically.\nResearchers studied one subject, a 33-year-old woman with relapsing-remitting MS (RRMS) with an expanded disability status scale (EDSS) score of 6.0. She received 12 training sessions in exoskeleton-assisted walking using an EKSO wearable exoskeleton made by Ekso Bionics of Richmond, California. Outcome measures were taken after training with the device. Metabolic expenditures during a timed up-and-go test (TUG), a 25-foot walk test (25FTWT, self-selected and fast speed) and a 6-minute walk distance test (6MWT) were measured for exoskeleton-assisted walking and compared with measures taken during unassisted walking. Muscle activation profiles were also recorded and analyzed in major muscles of the upper and lower extremities using surface electromyography (sEMG).\nThe team reported that preliminary outcomes data showed maximal oxygen consumption (VO2max) and heart rate (HR) during the 6MWT (approximately 7.5 percent and 6 percent, respectively) were lower in exoskeleton-assisted walking than in unassisted walking. But in 25FTWT at fast speed, the VO2max and HR were higher (approximately 3 percent and 10 percent, respectively) in exoskeleton-assisted walking than in unassisted walking. For sEMG, all muscles of the lower extremities except the semitendinosus showed lesser activity during the 25FTWT at self-selected speed and the 6MWT in exoskeleton-assisted walking than in unassisted walking. Similar to the metabolic pattern observed, muscle activities were greater during the 25FTWT at fast speed in exoskeleton-assisted walking than in unassisted walking. The study subject also found it difficult to operate the exoskeleton when attempting the TUG test.\nOverall, the investigators concluded that exoskeleton assisted walking may enable MS patients to walk more efficiently by reducing metabolic expenditures and muscle activity, especially over longer distances, and that wearable exoskeletons could also provide useful assistance to persons with MS in performing daily activities, leading to a better quality of life.\n“Our study participant told me when she was in the study, ‘I am looking at the quality of the rest of my life, not quantity of the days I can live,'” the study’s lead investigator Shuo-Hsiu ‘James’ Chang, an assistant professor at UTHealth’s Neurorecovery Research Center at TIRR Memorial Hermann in Houston, Texas, said in a news release. “It is a strong message telling us that rehabilitation plays a critical role in helping people with MS to live better quality lives. I believe exoskeleton-assisted training and walking is one of the most effective strategies that can promote quality of life and community reintegration in this population.”\nDr. Chang, a physical therapist from Taiwan, holds an MS and PhD in Human Movement Science with a specialty in motor control and learning from The University of North Carolina at Chapel Hill. His research focus has been on neuromuscular plasticity and its underlying mechanisms, and the involvement of cortical and spinal circuitry in dynamic balance control, especially in maintaining balance or regaining balance from ‘unexpected perturbation.’ Dr. Chang is interested in developing effective interventions that can exercise paralyzed or weak muscle, and facilitate neuroplasticity at the spinal or cortical level. He has investigated the effects of lower limb exoskeleton locomotion training on gait in patients with spinal cord injuries, and collaborated with researchers from TWU and UTHealth in several motor control studies.\nThe NeuroRecovery Research Center, a new facility on the campus of TIRR Memorial Hermann, opened in early February 2016. The center, a collaborative project among clinicians, scientists and engineers, has the goal of finding breakthroughs that will take rehabilitation services to the next level. It is part of a more than 42,000 square-foot building devoted to new, innovative ways to treat and rehabilitate patients with a range of disabilities and conditions, such as brain and spinal cord injury, stroke, multiple trauma, amputation, and neurodegenerative diseases, and with capacity to house multiple labs conducting different studies and trials at the same time.\n“The amount of collaboration that will go on among our teams is something that just can’t be beat,” Dr. Gerard E. Francisco, chief medical officer at TIRR Memorial Hermann, director of the NeuroRecovery Research Center and professor and chairman of Physical Medicine and Rehabilitation Department (PM&R) at McGovern Medical School at UTHealth, said in a UTHealth release. “We do research to answer real world clinical questions so we can do better in our clinical care.””This is an exciting time at TIRR as we take the research component of our rehabilitation services to the next level,” added Carl Josehart, TIRR Memorial Hermann’s senior vice president and CEO. “This state-of-the-art facility will provide researchers with the tools to pioneer new advances in rehabilitative medicine and get our patients back to the life they love.”\n“To promote walking for people with MS, we need to be creative and develop novel strategies. Wearable exoskeletons offer the feasibility of assisted overground walking and may be effective assistance devices to promote efficient walking, and better quality of life,” said Dr. Chang, who notes that the wearable exoskeleton is one of the advanced technologies that could promote the effectiveness of rehabilitation strategies. “We need to conduct more studies, such as larger trials with larger populations … to perform a systematic investigation and analysis that studies effectiveness of exoskeleton-assisted rehabilitation in MS. In our research center, we have another ongoing exoskeleton study that is designed to develop an algorithmic-based evaluation and treatment approach for wearable exoskeleton-assisted rehabilitation that focuses on physiological changes and gait in patients with stroke, spinal cord injuries and MS. We expect the results will help us to understand better how we can utilize exoskeletons and design effective exoskeleton-assisted rehabilitation programs.”\nAnother, larger clinical trial led by Dr. Chang is currently underway and recruiting MS patients with an EDSS score between 6 and 7.5, inclusive, and sponsored by The University of Texas Health Science Center, Houston, in collaboration with The National Multiple Sclerosis Society and Texas Woman’s University. The goal of the study, titled “Wearable Lower Extremity Exoskeleton to Promote Walking in Persons With Multiple Sclerosis” (ClinicalTrials.gov identifier: NCT02519244), is to investigate the potential of a wearable robotic exoskeleton — again the Ekso wearable lower limb exoskeleton — to help people with MS to walk more naturally again.\nAssociation of Academic Physiatrists (AAP) Via Newswise and Science Daily\nTIRR Memorial Hermann Health System\nThe NeuroRecovery Research Center\nThe University of Texas Health Science Center at Houston (UTHealth)""]"	['<urn:uuid:db7cafd5-fcd6-4b37-964b-9a49a1d49553>', '<urn:uuid:7303d60b-5bee-44a6-bb91-6023bbcbd7a8>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T15:30:12.136021	12	100	3240
22	what gases are used for cooking and heating	Compressed natural gas (CNG) and liquefied petroleum gas (LPG) are used for cooking and heating. CNG is primarily comprised of methane, while LPG is a mixture of propane and butane.	"[""Fuel dispensing equipment dispenses and monitors liquid or gaseous fuel. Fuel dispensers are used to pump liquid fuels such as gasoline, diesel fuel, oil, or kerosene into a vehicle, aircraft, storage tank, or portable container. Gaseous fuel dispensers may refuel hydrogen- or syngas-powered vehicles or machinery, or simply be used to move gases from one location to another.\nLiquid fuel dispensing equipment is widely distributed in developed countries in the form of automotive gas pumps (petrol pumps in Commonwealth areas).\nFuel dispenser design depends on the intended fuel type. A fuel type's volatility, flammability, boiling point, and other characteristics factor into a dispenser's design and construction.\nCommon liquid fuel groups include:\n- Petroleum fuels include gasoline (petrol), diesel, and kerosene. Liquid petroleum fuels do not directly ignite. Instead, fumes from the fuel ignite when exposed to an ignition source and vaporize the remaining liquid, causing the liquid to burn as fumes itself. Gasoline and diesel are common automotive fuels, while kerosene is common as fuel for heaters, a component of many jet and rocket fuels, and an additive to diesel to prevent cold-temperature effects.\n- Compressed natural gas (CNG) and liquefied petroleum gas (LPG) are related compressible fuel types. CNG is primarily comprised of methane, while LPG is a mixture of propane and butane. Both are relatively clean-burning but must be stored under pressure due to their low boiling points. CNG and LPG are used for cooking, heating, and increasingly as motorized vehicle fuel.\n- Alcohols include ethanol, butanol, and methanol. They are employed as vehicle fuels or fuel additives in combination with gasoline.\n- Specialized or uncommon liquid fuels include liquid hydrogen, ammonia, synthetic fuels, and biodiesel.\nSome fuels, commonly termed fuel gases, are ordinarily gaseous. Fuel gases are used in gas burners, heaters, stoves, and occasionally to power vehicles. Before the widespread use of electric street lighting, gas-lit streetlamps burned fuel gas.\nFuel gas can be broken into two groups:\n- Natural fuel gas is produced during the petroleum refining process and includes propane, butane, and regasified LPG. Natural gas, the uncompressed version of CNG, is naturally occurring in gas field deposits.\n- Manufactured fuel gas is produced through artificial processes, most commonly by gasification. Manufactured gases include coal gas, water gas, syngas, wood gas, and biogas. Manufactured gas was used for lighting and cooking until the mid-20th century and is now found as gas turbine or combustion engine fuels.\nDesign and Operation\nTypical fuel dispensers include several sections, such as hydraulic, metering, and hose/nozzle portions. The hydraulic section contains a rotary pump for drawing fuel from the storage tank and a solenoid/pilot valve to ensure that fuel flows only toward the dispenser and not back into the pump. Commercial dispensers such as those found at gas/petrol stations may house multiple units connected to different storage tanks for the purpose of supplying multiple fuels with different compositions or octane ratings.\nAn operational fuel dispenser contains a continuously running electric motor between the storage tank and the hydraulics portion of the unit for the purpose of maintaining a partial vacuum at the rotary pump inlet. When the nozzle opens, the storage tank draws suction pressure from the inlet, causing fuel to flow toward the pumping unit. An intermediary filter removes air bubbles or suspended solids from the fuel.\nFuel then flows through the pump and valve and into the metering unit. This portion includes mechanical gearboxes, as in older pumps, or piston meters and encoders to measure and distribute a specified amount of fuel or track fuel output. After the metering section, the fuel progresses through a flexible hose and into a nozzle that dispenses it into a vehicle or storage tank.\nFuel dispensers range from large commercial gas pumps to simple dispenser pumps for use with portable storage tanks.\n- Nozzle size: Diesel dispensers use larger nozzles that are incompatible with standard vehicle fillpipes to avoid misfueling. However, this sizing is not a mandated requirement, and most diesel nozzles are color-coded.\n- Pressure-sensitive nozzles restrict flow until the fuel dispensing system is pressurized.\n- Breakaway valves allow the nozzle to break away from the hose and stop fuel flow if a vehicle begins moving with the nozzle still in the fillpipe.\n- Dual-plane nozzles include a swivel joint to allow fueling from many different angles.\n- Quick-release nozzles have a single, coaxial coupling that eliminates the need for separate filling and venting points\nProducts and Equipment\nManufacturers may supply complete dispensing systems or the discrete components that comprise a system. These products include fuel meters, nozzles, hydraulics systems, and individual valves or pumps.\nFuels and fuel gases are flammable, and dispensing them improperly or with faulty equipment can lead to fire or explosions. For this reason fuel dispensing equipment often conforms to specifications and recommendations contained in published standards like those listed below.\n- NFPA 30A—Code for motor fuel dispensing facilities and repair garages\n- UL 87A—Investigation of power-operated gasoline and ethanol dispensers\n- FAA AC 150/5230-4—Aircraft fuel storage, handling, and dispensing at airports""]"	['<urn:uuid:100c4b4b-7983-4be2-84b1-cce6a3a60922>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	8	30	836
23	sps coral habitat temperature requirements bleaching	Small-polyp stony (SPS) corals require specific temperature conditions between 72 and 78 F to survive. When water temperature rises due to global warming, it can cause coral polyps to expel their symbiotic algae, leading to coral bleaching, which renders the corals colorless and can endanger thousands of species dependent on coral reefs.	['Small-polyp stony (SPS) corals are among the most diverse and well-known types of corals found in the world’s oceans – they include many specific kinds of corals from the genera Pocillopora, Montipora and Stylophora. Their skeleton is slowly secreted by the epidermis found at the base of each polyp, and as in the case of LPS (large polyp stony) corals, some varieties can grow much faster than others. The beauty and diversity of SPS corals is maintained in stable habitats, where factors such as temperature, alkalinity and water PH are kept between specific values.\nLarge-Polyp Stony Corals vs. Small-Polyp Corals – The “Great” Divide\nAlthough there is a difference between small-polyp stony corals and LPS corals, there isn’t an actual, concrete distinction between the two. Rather, the transition is smooth, as there are many SPS coral species that have larger polyps, and can often be compared with LPS species. The general difference, however, has to do with the greater fragility and more specific habitat conditions that small-polyp corals may require in order to survive and propagate. In contrast with large-polyp stony corals, they cannot be found at depths where sunlight can’t reach them too easily, nor in waters that are too polluted, murky or cold.\nThe Beauty and Fragility of Small-Polyp Stony Corals\nThere are many beautiful SPS species that are commonly sought out by divers in tropical regions. These include anything from stunning “flowery” patterns like the ones found on Sunset Montipora, and the vibrant purple stem-shaped polyps of the Booberry Acropora, to incredibly exotic, intricate shapes and multicolored textures, such as in the case of the Bisma Worm Rock. Despite their beauty, SPS corals are quite fragile, being commonly attacked by creatures such as nudibranchs and various insects or flatworms that mainly attack Acropora corals. Also, they require specific habitat conditions, such as a temperature between 72 and 78 F, and moderate to intense sunlight.\nThe Preferred Habitats of these Stony Corals\nAside from stable temperature and lighting conditions that can usually only be found in the tropics, small-polyp corals also require several other important factors to be just right, before they can survive. The water alkalinity, for example, has to be between 8-12 dKH, while PH fluctuations of only 0.3 (ideal ranges are somewhere between 8.1 and 8.4) can be tolerated, before the corals begin to find it difficult to maintain their life cycles in the long run. Preferred habitats exist in areas like the islands of the Indian and Pacific Oceans, including places like Fiji, the Maldives, Hawaii or Papua New Guinea.\nPopular Small-Polyp Stony Coral Varieties\nThe most well-known varieties of SPS corals include primarily members of Acroporidae, including the Montipora and Acropora genera. These corals are most fascinating, not only due to their remarkable natural beauty and diversity, but also because they are the dominant reef building hard coral species of the world’s oceans. Although their diversity makes them extremely prevalent, like most small-polyp stony corals, these species still require specific conditions in order to thrive and survive, which is part of the reason why they are rarely able to live in deeper waters.', 'The beauty of coral reefs has eluded sea lovers for centuries. If you are a sea lover and love to go to trips in the ocean occasionally, you may have heard the term “coral,” maybe you’ve even seen them in person. But what exactly is a coral? Let’s dive deep into the sea and find out.\nSome of us think that corals are rocks. Even I used to think the same when I was younger. Actually, they’re not “dead.” Corals are living beings and they are classified as “animals.”\nCorals are sessile animals, meaning, unlike most other animals, they cannot move. Since corals are immobile, many people mistake them for plants. There are hundreds of coral species out there including Black coral, Pillar coral etc. Corals belong to the cnidaria phylum. They are composed of thousands of tiny creatures known as “polyps.” Polyps are soft-bodied animals. Polyps have a tentacular mouth and a sac-like body.\nSince they are animals and cannot synthesize their own food, they have to depend on others for energy. Polyps can feed in two ways. Some use their tentacles to capture foods such as small fish or plankton from the seawater while most others depend on a symbiotic relationship with algae for food.\nZooxanthellae is the algae species that live inside the tissue of coral polyps. They utilize the wastes produced by the polyps and convert them into energy sufficient enough to support both the animal and the algae itself. Polyps benefit from algae’s in many ways. They remove metabolic wastes, produce oxygen for the polyps, and they provide energy. In return, the algae get carbon dioxide and a safe environment inside the body of the polyp, which prevents them from being devoured by fish and other animals. Corals are generally colorless; it is the algae that gives them vivid color.\nCorals reproduce diversely. Some corals form a colony that produces a particular gamete, male or female, while another colony nearby produces its counterparts. Some corals produce sperm and eggs at the same time.\nCorals live for thousands of years. According to nature, some species of corals can live for more than four thousand years.\nNot all coral species contribute to reef-building. The corals that can make rift are known as Stony corals. They appear in many shapes, some look like flattened branches while some others create branches. Stony corals secrets calcium carbonate from the lower portion of the body which produces a structure right beneath them. This structure is called “calyx.” Polyps sit on the calyx, and as they secrets more calcium carbonate, partitions of limestone build-up. This lays the foundation of coral reefs. As years go by, generations of corals reproduce and die, repeating the cycle slowly bringing coral reefs to life.\nThere are mainly four categories of coral reefs which are:\n- Fringing reefs\n- Barrier reefs\n- Patch reefs\nFringing reefs are often found near coastlines while barrier reefs are found in a little deeper part of the sea.\nCoral reefs are not only heavenly beautiful but also incredibly diverse ecosystems. They provide food, shelter, and breeding space for as much as 25 percent of all known marine species while they cover less than 1 percent of the ocean floor.\nCorals are called the “rainforest of the sea” because of their incredible biodiversity. More than 4000 species of fish and thousands of other plants are dependent on corals reefs directly.\nGlobal warming and the future of Coral reefs\nWater pollution such as oil and gas leak, sewage discharge, and rise of water temperature due to global warming can cause coral polyps to expel the algae living inside rendering the corals colorless. This process is known as coral bleaching. Due to some bad practices while fishing and many other reasons, including global warming, coral bleaching have become an alarming issue in recent years.\nIf coral bleaching continues, it can endanger thousands of species of animals and plants that are dependent on corals for survival as well as destroying coral reefs. This can severely impact the marine biosphere. Considering the severity of this situation, several organizations have stood up and taken steps to raise awareness about coral bleaching and its effect on the marine ecosystem.\n- Coral. Retrieved from https://www.britannica.com/animal/coral on 05/01/2020\n- Coral reef ecosystems. Retrieved from https://www.noaa.gov/education/resource-collections/marine-life/coral-reef-ecosystems on 05/01/2020\n- What Are Coral Reefs? Retrieved from https://www.livescience.com/40276-coral-reefs.html on 05/01/2020\n- Coral Reefs. Retrieved from https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/coral-reefs on 05/01/2020\n- Coral bleaching. Retrieved from https://www.britannica.com/science/coral-bleaching on 05/01/2020\n- Coral bleaching. Retrieved from https://www.marineconservation.org.au/coral-bleaching/ on 05/01/2020\nSubscribe to our newsletter to stay updated']	['<urn:uuid:0188cd88-89b3-4ca7-996c-d2270ed15854>', '<urn:uuid:b6eefac1-7ec2-40e7-ba7e-0b611ceb9d35>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	6	52	1278
24	As a weather scientist, I'm wondering how do dynamic pricing and AI forecasting help manage unpredictable events?	Dynamic pricing and AI forecasting both help manage unpredictability, but in different ways. In baseball, dynamic ticket pricing uses machine learning to adjust prices based on variables like weather and team performance, helping teams optimize revenue despite uncertain demand - the San Francisco Giants generated an additional $500,000 in 2009 using this approach. Similarly in weather prediction, Google DeepMind's GraphCast AI can forecast weather patterns with unprecedented accuracy up to 10 days in advance, processing hundreds of data points in under a minute and predicting extreme weather events earlier than traditional methods. Both technologies ultimately help organizations better prepare for and respond to uncertain conditions.	['In a highly competitive world, the pressure is on for businesses to capture the eyeballs and wallets of their customers. Baseball teams are no exception to this phenomenon. With a 162-game season, almost double the length of other major sports, ticket revenue is an enormously significant part of the business of baseball compared to sports like the NFL which rely more heavily on TV contracts. In this context, how can baseball teams maximize their revenue?\nWell, step one is to sell more tickets. But another option which is increasingly becoming relevant is to adjust pricing based on customer demand, the same way that airlines have priced tickets for years. Dynamic (or Variable) Ticket Pricing is a machine learning process which refers to periodically adjusting prices based on the factors affecting individual events.1 With secondary ticket marketplaces like StubHub leveraging pricing discrepancies based on customer demand as their core business model, it only makes sense that content providers (in this case, baseball teams) compete for fan dollars using the same strategy as that of secondary ticket brokers in order to maximize their own revenue.\nTicketing Ecosystem Map (7)\nWhy does a baseball fan choose to attend one game versus another? In 2009, the San Francisco Giants were the first team to use dynamic ticket pricing (DTP). They recognized that fan demand for each game depends on a number of inputs including team/ player performance, day of the week, opposing team, weather and more. In contrast to the traditional practice of setting prices before the season and not adjusting them during, the Giants started using machine learning to dynamically adjust prices during the season to account for fluctuations in demand. As a result, they were able to generate an additional $500,000 in incremental revenue in 2009 and had an overall revenue increase of 7% in 2010.2\nIt is important to recognize that since DTP is a relatively new phenomenon, baseball’s short term and long term goals differ in their implementation of this pricing strategy. This issue is complex because pricing decisions are not made in a vacuum. First, a fan’s perception of a team is impacted by the prices the team sets. The team-fan relationship is critical to maintain since sustained strong team performance on the field is rare. Therefore, teams need to keep fans happy so that they continue to show up whether the team plays well or plays poorly. Having a reputation for “high” ticket prices does not lead to strong team-fan relationships. Second, fan demand is inherently uncertain. There is a fixed, finite amount of inventory with a number of variables that can cause major swings in consumer demand.3\nGiven this complexity, teams are applying machine learning to dynamic ticket pricing in a conservative manner in the short term. For example, the LA Dodgers recognize that season ticket holders have relatively higher importance than individual ticket buyers. There would be significant brand damage if they were to sell tickets to a game to season ticket holders at a higher price than to individual ticket buyers due to a drop in demand.4 The Dodgers are using the recommendations of the machine learning algorithms as a starting point to their ticket pricing, but ultimately maintain human control over final prices to ensure that they are never jeopardizing their most important fan relationships with pricing practices which could be perceived as “unfair.”\nBy contrast, in the long term once teams have more experience with dynamic ticket pricing and specific data on their fanbase’s reaction to it, teams plan to incorporate simulation-based algorithms to minimize the need for human intervention in their pricing. One such algorithm is the “Learning Curve Simulator” which imitates real market conditions by matching randomized game variables with a group of buyers. Each buyer is comparing the ticket sales price with his or her individual willingness to pay and is making a buying decision accordingly.5 By running these simulations, teams can increase the accuracy of DTP.\nLearning Curve Simulator (5)\nRegarding other steps, baseball teams should look to other industries and use what works for them while still recognizing the unique intricacies of sports which require differentiated strategies. For example, Taylor Swift is using higher priced face value tickets in combination with a “Verified Fan” program to ensure tickets aren’t being scooped up by scalpers and resold to her fans for profit. On the positive side, these steps helped her Reputation tour generate 15% more revenue than her previous tour.6 However, while these shows created more revenue, there were fewer sellouts. Sellouts in sports can be more significant to fan experience than sellouts in music which is why implementing this strategy in baseball would be risky.\nT Swift Reputation Tour (8)\nWhat impact would your favorite team or artist using dynamic pricing have on your loyalty to them? Do you see secondary markets as a friend or foe to teams/ artists?\n(Word Count: 799)\n- Rascher, Daniel A.; McEvoy, Chad D.; Nagel, Mark S.; and Brown, Matthew T. (2007), “Variable Ticket Pricing in Major League Baseball”. Kinesiology (Formerly Exercise and Sport Science). Paper 6. http://repository.usfca.edu/ess/6\n- Shapiro, Steven L, and Joris Drayer, (2012). “A New Age of Demand-Based Pricing: An Examination of Dynamic Ticket Pricing and Secondary Market Prices in Major League Baseball.” Journal of Sports Management, pdfs.semanticscholar.org/9ebf/87dadc294b14ae68615f50f168faf57b6b66.pdf.\n- den Boer, Arnoud V (2013). Dynamic Pricing and Learning: Historical Origins, Current Research, and New Directions. Universite of Twente.\n- Parris, D. L., Drayer, J., & Shapiro, S. L. (2012). Developing a pricing strategy for the los angeles dodgers.Sport Marketing Quarterly, 21(4), 256-264. Retrieved from http://search.proquest.com.ezp-prod1.hul.harvard.edu/docview/1324536397?accountid=11311\n- Morris Dimico, Joan, et al (2003). Learning Curve: A Simulation-Based Approach to Dynamic Pricing. Kluwer Academic Publishers.\n- Steele, Anne. “Why Empty Seats at Taylor Swift’s Concerts Are Good for Business.” The Wall Street Journal, Dow Jones & Company, 15 May 2018, www.wsj.com/articles/why-empty-seats-at-taylor-swifts-concerts-are-good-for-business-1526385600.\n- Fang, Jane; Quinn Matt. (2017), “Evolution of the Ticket: An Ecosystem Analysis for Gametime United”. http://launchingtechventures.blogspot.com/2015/04/evolution-of-ticket-ecosystem-analysis.html\n- [Image] http://www.soundslikenashville.com/culture/the-five-coolest-things-we-saw-at-taylor-swifts-reputation-tour-stop-in-nashville/', 'This yr was a nonstop parade of maximum climate occasions. Unprecedented warmth swept the globe. This summer season was the Earth’s hottest since 1880. From flash floods in California and ice storms in Texas to devastating wildfires in Maui and Canada, weather-related occasions deeply affected lives and communities.\nEach second counts on the subject of predicting these occasions. AI may assist.\nThis week, Google DeepMind launched an AI that delivers 10-day climate forecasts with unprecedented accuracy and velocity. Referred to as GraphCast, the mannequin can churn by means of a whole bunch of weather-related datapoints for a given location and generate predictions in underneath a minute. When challenged with over a thousand potential climate patterns, the AI beat state-of-the-art techniques roughly 90 p.c of the time.\nHowever GraphCast isn’t nearly constructing a extra correct climate app for choosing wardrobes.\nThough not explicitly educated to detect excessive climate patterns, the AI picked up a number of atmospheric occasions linked to those patterns. In comparison with earlier strategies, it extra precisely tracked cyclone trajectories and detected atmospheric rivers—sinewy areas within the ambiance related to flooding.\nGraphCast additionally predicted the onset of maximum temperatures properly prematurely of present strategies. With 2024 set to be even hotter and excessive climate occasions on the rise, the AI’s predictions may give communities helpful time to arrange and doubtlessly save lives.\n“GraphCast is now essentially the most correct 10-day international climate forecasting system on this planet, and may predict excessive climate occasions additional into the long run than was beforehand potential,” the authors wrote in a DeepMind weblog put up.\nPredicting climate patterns, even only a week forward, is an previous however extraordinarily difficult drawback. We base many choices on these forecasts. Some are embedded in our on a regular basis lives: Ought to I seize my umbrella at this time? Different selections are life-or-death, like when to situation orders to evacuate or shelter in place.\nOur present forecasting software program is essentially primarily based on bodily fashions of the Earth’s ambiance. By analyzing the physics of climate techniques, scientists have written quite a lot of equations from a long time of knowledge, that are then fed into supercomputers to generate predictions.\nA outstanding instance is the Built-in Forecasting System on the European Middle for Medium-Vary Climate Forecasts. The system makes use of refined calculations primarily based on our present understanding of climate patterns to churn out predictions each six hours, offering the world with a few of the most correct climate forecasts accessible.\nThis technique “and trendy climate forecasting extra typically, are triumphs of science and engineering,” wrote the DeepMind staff.\nOver time, physics-based strategies have quickly improved in accuracy, partially due to extra highly effective computer systems. However they continue to be time consuming and expensive.\nThis isn’t stunning. Climate is one essentially the most advanced bodily techniques on Earth. You might need heard of the butterfly impact: A butterfly flaps its wings, and this tiny change within the ambiance alters the trajectory of a twister. Whereas only a metaphor, it captures the complexity of climate prediction.\nGraphCast took a distinct strategy. Neglect physics, let’s discover patterns in previous climate information alone.\nAn AI Meteorologist\nGraphCast builds on a sort of neural community that’s beforehand been used to foretell different physics-based techniques, akin to fluid dynamics.\nIt has three components. First, the encoder maps related data—say, temperature and altitude at a sure location—onto an intricate graph. Consider this as an summary infographic that machines can simply perceive.\nThe second half is the processor which learns to investigate and cross data to the ultimate half, the decoder. The decoder then interprets the outcomes right into a real-world weather-prediction map. Altogether, GraphCast can predict climate patterns for the subsequent six hours.\nHowever six hours isn’t 10 days. Right here’s the kicker. The AI can study from its personal forecasts. GraphCast’s predictions are fed again into itself as enter, permitting it to progressively predict climate additional out in time. It’s a technique that’s additionally utilized in conventional climate prediction techniques, the staff wrote.\nGraphCast was educated on almost 4 a long time of historic climate information. Taking a divide-and-conquer technique, the staff break up the planet into small patches, roughly 17 by 17 miles on the equator. This resulted in additional than 1,000,000 “factors” masking the globe.\nFor every level, the AI was educated with information collected at two occasions—one present, the opposite six hours in the past—and included dozens of variables from the Earth’s floor and ambiance—like temperature, humidity, and wind velocity and course at many alternative altitudes\nThe coaching was computationally intensive and took a month to finish.\nAs soon as educated, nevertheless, the AI itself is very environment friendly. It may produce a 10-day forecast with a single TPU in underneath a minute. Conventional strategies utilizing supercomputers take hours of computation, defined the staff.\nRay of Gentle\nTo check its talents, the staff pitted GraphCast towards the present gold commonplace for climate prediction.\nThe AI was extra correct almost 90 p.c of the time. It particularly excelled when relying solely on information from the troposphere—the layer of ambiance closest to the Earth and demanding for climate forecasting—beating the competitors 99.7 p.c of the time. GraphCast additionally outperformed Pangu-Climate, a prime competing climate mannequin that makes use of machine studying.\nThe staff subsequent examined GraphCast in a number of harmful climate situations: monitoring tropical cyclones, detecting atmospheric rivers, and predicting excessive warmth and chilly. Though not educated on particular “warning indicators,” the AI raised the alarm sooner than conventional fashions.\nThe mannequin additionally had assist from basic meteorology. For instance, the staff added present cyclone monitoring software program to GraphCast’s forecasts. The mixture paid off. In September, the AI efficiently predicted the trajectory of Hurricane Lee because it swept up the East Coast in direction of Nova Scotia. The system precisely predicted the storm’s landfall 9 days prematurely—three valuable days quicker than conventional forecasting strategies.\nGraphCast received’t substitute conventional physics-based fashions. Somewhat, DeepMind hopes it may possibly bolster them. The European Middle for Medium-Vary Climate Forecasts is already experimenting with the mannequin to see the way it may very well be built-in into their predictions. DeepMind can also be working to enhance the AI’s capability to deal with uncertainty—a crucial want given the climate’s more and more unpredictable habits.\nGraphCast isn’t the one AI weatherman. DeepMind and Google researchers beforehand constructed two regional fashions that may precisely forecast short-term climate 90 minutes or 24 hours forward. Nevertheless, GraphCast can look additional forward. When used with commonplace climate software program, the mix may affect selections on climate emergencies or information local weather insurance policies. As a minimum, we’d really feel extra assured concerning the resolution to convey that umbrella to work.\n“We consider this marks a turning level in climate forecasting,” the authors wrote.\nPicture Credit score: Google DeepMind']	['<urn:uuid:cdcc2c86-b66e-47a5-83bd-a243abdedb66>', '<urn:uuid:aefe2df6-2a2b-480f-9cda-074a0b185b03>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T15:30:12.136021	17	105	2132
25	environmental impact differences nuclear wind power	Both nuclear and wind power have distinct environmental impacts. Nuclear power, while producing no direct emissions during electricity generation, has environmental impacts through uranium mining (releasing arsenic and radon) and thermal pollution of water bodies used for cooling. It also produces radioactive waste that remains hazardous for thousands of years. Wind turbines, on the other hand, have different environmental considerations - they require specific siting conditions and create some noise pollution, though typically less than a washing machine. Wind turbines don't produce any hazardous waste or emissions, but they require significantly more land area than nuclear facilities, which can impact local environments through land use changes. Another consideration is that nuclear power plants contribute to protecting air quality and produce 50% of the nation's emissions-free electricity, while wind power's contribution to clean energy is dependent on consistent wind conditions.	['Pros & cons of nuclear energy: is it safe?\nPeople often associate nuclear energy with nuclear bombs, or tragedies like Chernobyl or Fukushima. If this is your impression or image of nuclear power, you will be surprised to learn that nuclear energy is actually considered one of the most environmentally friendly forms of power generation. If you compare nuclear energy with other forms of energy generation, it is much more efficient than any other form and is actually considered one of the most environmentally friendly means of energy production available.\nThere are of course, many challenges associated with producing nuclear energy including initial cost, waste, security, safety, and more. Here are the main pros and cons associated with nuclear energy.\nWhat is Nuclear Energy?\nBefore going forward, let’s quickly define nuclear energy and how it’s created. Nuclear energy is created by splitting uranium atoms. When atoms are split they become 2 smaller, lighter atoms and because energy doesn’t just disappear, the “lost” mass is converted into heat. This heat is then used to produce electricity. Nuclear power plants create a controlled environment for this process.\nPros of nuclear energy\n- Nuclear energy protects air quality\nNuclear energy is the largest source of clean energy in the world. It produces about 800 billion kilowatt hours of electricity each year and produces 50% of the nation’s emissions-free electricity. This avoids over 470 million metric tons of carbon every year, which is like taking 100 million cars off the road.\n- Nuclear energy’s operating costs are low\nOnce a nuclear plant is built, the cost of producing electricity is much less than gas, coal or oil unless those fossil fuels are located very close to their power plant. Nuclear energy also has a comparatively low risk of price inflation. This is because less factors affect the price, as opposed to fossil fuels which are affected by a multitude of factors and their prices vary wildly.\n- Nuclear energy uses considerably less land\nNuclear power is incredibly efficient at producing electricity and because of this, it requires significantly less space than any other clean source.\nA standard 1,000 megawatt nuclear facility in the United States needs slightly more than 1 square mile to operate. Per the Nuclear Energy Institute (NEI) wind farms require about 360 times more land to produce the same amount of electricity and solar plants require 75 times more space.\n- Nuclear energy is the most reliable energy source in America\nNuclear power plants were able to run at full capacity more than 92% of the time and generated 20% of U.S. electricity last year. To put this in perspective nuclear operated at full capacity 336 out of 365 days last year (the 29 days off were for standard maintenance). In contrast, hydroelectric systems were running 38.2% of the time (138 days a year), wind turbines produced 34.5% of the time (127 days a year), solar arrays only generated electricity 25.1% of the time (92 days a year), and fossil fuels were active about 50% of the year. Nuclear is the clear winner on reliability.\n- Nuclear power puts less radiation into the environment than any other major source\nMost people don’t know that non-nuclear energy sources release radiation into the environment. The worst offender is actually coal which contains radioactive elements thorium and uranium. When coal is burned, its organic materials gasifies and its mineral components concentrate into waste. This waste is called fly ash. Since so much coal is burned all over the world, which produces so much fly ash, coal is actually the major source of radioactive releases into the environment.\n- Disposing nuclear waste is no longer a technological problem\nThe disposal of nuclear waste is a political problem in the U.S., not a technological one. Over 90% of the spent fuel in the U.S. could be recycled and used to extend nuclear power production for hundreds of years. Instead of doing this, it sits safely in impenetrable concrete and steel dry casks on the grounds of active reactors, slowly losing its radiation.\nIn New Mexico, the U.S. Waste Isolation Pilot Plant (WIPP) stores transuranic military waste and it could store commercial nuclear waste in its 2-kilometer thick bed of crystalline salt. The salt formation extends from southern New Mexico all the way to southwest Kansas. This could easily hold the entire world’s nuclear waste for 1000 years.\nTo put this in perspective, we could take all of the fuel used by the U.S. over the last 60 years and fit it in a football field about 10 yards deep.\nCons of nuclear energy\n- Nuclear plants are extremely expensive to build\nEven though they are relatively cheap to operate, nuclear plants are very, very expensive to build and are getting more expensive. The cost to build a nuclear power plant has risen from $2-$4 billion to $9 billion and construction often exceeds estimates. Nuclear plants must also set aside funds to build structures to protect and secure nuclear waste produced.\n- Risk of nuclear accidents\nEven though nuclear power plants have extremely strict measures in place to prevent accidents, they obviously do occur. Such accidents can cause radiation to leak which can lead to serious health complications or death and serious environmental damage.\nHowever, as technology advances, the negative effects are minimized and studies show that even if you account for catastrophes like Chernobyl fossil fuels are substantially more deadly.\n- Environmental Impact\nThough the actual production of electricity doesn’t produce emissions, the process of mining the uranium needed isn’t clean. The mining of uranium is known to release arsenic and radon to surrounding areas.\nNuclear power plants also cause thermal pollution. Most nuclear plants are located on a lake or ocean as they need water to cool their reactors. The body of water is used to condense steam back into water.\nThis process causes the water to heat up to around 100 degrees Fahrenheit. If this hot water is immediately released back into the body of water it can significantly change the chemistry and make it inhabitable to most aquatic life.\n- Radioactive waste\nThe waste produced by nuclear plants is radioactive and remains so for thousands of years. This waste needs to be safely stored and ideally recycled. There is also the potential for a leak which could cause damage to surrounding areas.\n- Nuclear is not currently renewable\nIf you account for all known uranium deposits and current consumption we have about 200 years of uranium left. If you account for recycling used fuel that number would increase by hundreds of years.\nThe future of nuclear energy\nThere are clear arguments that support both sides of the nuclear debate. But as technology advances nuclear energy could be a key factor in helping us achieve 100% clean energy in the future.\nAnother thing to keep in mind is if nuclear fusion can be achieved, it would supply nearly limitless clean energy.', 'This guide can help you decide if a residential scale wind turbine is right for you.\nIs Wind Right for you? Wind turbines are an effective source of renewable energy in many areas across the world. The American Wind Energy Association (AWEA) estimates there are as many as 13 million homes in the United States with a viable wind resource.\nTo make sure a small wind system is right for you, there things to consider:\n- You have at least 6.0 MPH (2.7 m/s) average wind speed for the Polaris 50, 100, 500 and 1000kW turbines. Best results at 12 MPH (5.4 m/s) or more.\n- The ideal location for a wind turbine is 20’ (6.1m) above any surrounding object (trees, roof tops, hills, tall buildings) within a 250 foot (76.2m) radius. This generally means your property should be at least ½ acre (2,023m2)larger in size.\n- The local zoning allows a structure that is at least 42 feet (12.8m) tall (see “Getting Connected Consumer Guide”)\n- Your local utility has an existing interconnection agreement (see “Getting Connected Consumer Guide”)\nOnce you have determined you have the right conditions for a wind system, you will need to consider the best place on your property.Proper siting is critical to the performance and longevity of your investment. It can make the difference between years of trouble free operation or continuous frustration.\nThis guide is a precursor to investing in a small residential scale wind turbine. Whether your reason for investing is because of your concern for the environment, helping reduce foreign fuel imports or pure economics, we at Polaris America want to make sure you get the most from your wind turbine. Before making any investment in small wind, be sure to review this guide carefully, ask lots of questions and once you feel confident, call your dealer.\nDETERMINING YOUR WIND RESOUCE\n“Do I have enough wind?” is often the first and most asked question. Generally, if you feel like you have a very windy site, you most likely have a good place for a wind system. It is always a good idea to do your homework just to make sure. Wind resources can vary from placed to place. There are several places you can go to assist you.\nThe U.S. Department of Energy recently developed a series of high resolution wind maps for much of the United States that will give you a good idea what your wind resource is at your location. Keep in mind, these maps are about 80 percent accurate and estimate the wind resource at about 150 feet (46 m) off the ground.\nWinds decrease the closer they are to the ground. You want to make sure the location of your property on the map is in a Class III wind or better. These maps have a resolution of +/- 1 km (0.7 mile). Click here for our wind maps page.\nYou may also obtain wind data from your local weather station, university or even television station. You may also contact the closest weather, TV or radio station and ask them for an idea of the average wind in your area. Some local station web sites may have this information. Wind patterns can change over short distances, so try to obtain information as close to your potential wind turbine as possible. Airport data is not recommended. Airports are generally located in lower wind areas (valleys) and their measurement techniques do not produce good data.\nIf wind maps or local data doesn’t work, another method is looking around and observing of the deformation of vegetation and trees on and around your property. The Griggs-Puttnam Index was a scientific study that looked at how wind deformed vegetation to determine the wind resource at a particular site. Compare the drawing below with your own property to determine your wind resource.\nThe Griggs-Putnam Index of Deformity is an additional useful tool to help determine the potential of a wind site. The idea is to observe the area’s vegetation. A trees shape, especially conifers or evergreens, in often influenced by winds. Strong winds can permanently deform the trees. This dormity in trees is known as “flagging.” Flagging is usually more pronounced for single, isolated trees with some height.\nThe Griggs-Putnam diagram, like the Wind Resource Maps, can offer a rough estimate of the wind in your area. The more information that you can obtain from the various sources listed above, the greater degree of accuracy you will have in determining your wind speed and your potential power output.\nThe last resource is conducting an actual wind resource assessment. This is by far the most accurate way of doing a site assessment but it is also the most expensive and time consuming. It involves the installation of a small tower on your property with 1-2 anemometers and a data collecting logger. Companies offer such hardware and then charge a monthly fee for consolidating the data and generating a report.\nKeep in mind that many Polaris America turbine dealers also perform site assessments.\nWhat is the “best” site?\nWind resource is not the only consideration when siting. Components such as taller towers and running large amounts of wire–more than 500 feet (152 m) from your site–can add significant costs to your wind system. You will need to balance energy achieved in the best wind site over the cost of installation to determine the best overall placement of your system.\nA-typical siting Considerations\nWinds may be very turbulent running off a cliff causing wind shears. It is important to site the generator far enough from the cliff to avoid turbulent wind.\nWind compresses as it blows over the top of a hill, increasing the wind speed. With proper placement, you may be able to use a shorter tower. We never recommend anything shorter than 33 feet (10 m). It is important to follow the general rule; that is the tower must be at least 20 feet (6 m) above any surrounding object.\nCoastal or Lakeside\nVery strong prevailing winds typically blow from the ocean. If this is the case, it is very important to install your wind generator as close to the coastline as possible. Trees and taller structures can be down-wind from the wind generator.\nA wind turbine must have a clear shot at the wind to perform efficiently. Turbulence which both reduces performance and “works” the turbine harder than smooth air is highest close to the ground and diminishes with height. Also, wind speed increases with height above the ground.\nAs a general rule of thumb, you should install a wind turbine on a tower such that it is at least 20 ft above any obstacles within 250 ft. Smaller turbines typically go on shorter towers than larger turbines. A 1 kW turbine is often, for example, installed on a 30-50 ft tower, while a 10 kW turbine will usually need a tower of 60-100 ft. We do not recommend mounting wind turbines to small buildings that people live in because of the inherent problems of turbulence, noise, and vibration.\nThe least expensive tower type is the guyed-lattice tower, such as those commonly used for ham radio antennas or cell phone towers. Smaller guyed towers are sometimes constructed with tubular sections or pipe.\nSelf-supporting towers, either lattice or tubular in construction, take up less room and are more attractive but they are also more expensive. Telephone poles can be used for smaller wind turbines.\nTowers, particularly guyed towers, can be hinged at their base and suitably equipped to allow them to be tilted up or down using a winch or vehicle. This allows all work to be done at ground level.\nSome towers and turbines can be easily erected by the purchaser, while others are best left to trained professionals. Anti-fall devices, consisting of a wire with a latching runner, are available and are highly recommended for any tower that will be climbed.\nAluminum towers should be avoided because they are prone to developing cracks. Towers are usually offered by wind turbine manufacturers and purchasing one from them is the best way to ensure proper compatibility.\nSmall wind turbines do make some noise, but not enough to be found objectionable by most people. A typical residential wind system makes less noise than the average washing machine. Small wind turbines do not interfere with TV reception.\nMost residential-sized wind generators are direct-drive devices with few moving parts. Unlike the utility-scale turbines used in wind farms, they do not have high-speed transmissions. Thus, most of the sound that comes from a residential sized wind turbine is aerodynamic noise caused by the blades passing through the air. The noise level of most modern residential turbines measures close to the ambient noise levels under average wind conditions. It is audible, if you are out of doors and listening for it, but no noisier than your average refrigerator.\nSmall Wind Systems and Public Safety\nSome of the public safety questions asked about small wind turbines concern the structural integrity of the tower, what happens during power outages or icing conditions [cold climates only], and the question of whether the tower represents an “attractive nuisance.”\nHow reliable is the structure?\nThe U.S. has about 15-MW of nameplate capacity of small wind turbines – over 2,000 installations – with an excellent track record for safety. Following manufacturer specifications for the installation will ensure that the footings are adequate to anchor the structure. A building permit is also required, and the structure must meet local building and safety requirements.\nWhat happens if there is a power outage?\nIn the event of a power outage, small wind systems connected to the utility grid are designed to detect the outage and automatically shut down, so that there is no danger to anyone trying to repair utility lines. Stand-alone systems are connected only to the home’s electrical service, posing no risk to anyone trying to repair utility lines in the event of a power outage.\nWhat about icing?\nUnlike tree branches, wind turbine blades are designed to withstand a heavy load of ice. And when ice builds up on the blades, they simply don’t “fly” any more – they loose their aerodynamic properties. In fact, an ice-covered rotor will turn so slowly (at only several revolutions per minute) that there is no danger of ice being “thrown” off. Typically, the ice melts and falls – straight down – to the base of the tower.\nCan people get hurt climbing the tower?\nWindmill towers pose no more of an “attractive nuisance” than any other climbable pole or tower in the neighborhood, including water towers, cell phone towers, etc. Some wind turbine tower designs have no available hand- or footholds so they simply can’t be climbed. Towers which need to be climbed for repair work can be equipped with devices to prevent falls, and some towers are designed to be lowered so that any maintenance or repair work can be done on the ground.\nDo small turbines interfere with electronic devices?\nModern windmill blades are made of wood, fiberglass, or other composite materials which do not interfere with electronic devices.\nDo small wind turbines need to be lit?\nThe FAA doesn’t require lights on structures less than 200 feet tall, and prohibits both commercial and small private aircraft pilots from flying lower than 1,000 feet. Pilots cannot drop lower than 500 feet when approaching a runway. The only time a small wind turbine would be affected by FAA regulations is if it were proposed to be sited adjacent to an airport.\nMany common structures that are taller than small wind turbines – transmission line poles, for example – are not required to be lit.\nHowever, turbine owners may need to alert crop duster pilots to their installations in certain regions.\nInstallation or Mounting\nA general rule of thumb is to install a small wind turbine on a tower with the bottom of the rotor blades at least 20 feet (6 m) above any obstacle that is within 250 feet (76 m) of the tower, 25 feet (7.6 m) to the hub.\nMounting wind turbines on top of homes is not recommended. Larger residential home wind turbines vibrate and transmit noise to the structure on which they are mounted. This vibration can lead to noise and structural problems with the building. Mounting on the rooftop can also expose the generator to excessive turbulence that shortens its life.']	['<urn:uuid:2d7ca715-9852-4971-9a32-a9c57cdb9a9a>', '<urn:uuid:011837e5-3f02-4e9b-abe1-47af6bbfb8fe>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T15:30:12.136021	6	139	3227
26	What's special about FSC certified wood products?	Forest Stewardship Council (FSC) certification is given preference as it ensures sustainable forest management practices. FSC certified forests must meet criteria including forest regeneration, protection of soil and water quality, conservation of biological diversity, and respect for local communities and indigenous people.	['Procter & Gamble Wood Pulp Procurement Policy\nP&G purchases wood pulp for tissue and absorbent hygiene products. Though we do not own or manage forests, we have a responsibility through our procurement practices to ensure sustainability of the world’s forest resources. As such, we are committed to understanding the sources of our pulp fiber, transparency in sourcing, ensuring that sustainable forest management practices are used, avoiding unwanted sources of wood, and working with stakeholders on stepwise increases in preferred certification schemes. Independent third party verification systems are used to ensure sustainable forest management and wood traceability, while we work with global multi- stakeholder organizations to develop tools and scientific methods to protect values and services that forests provide such as biodiversity, watershed protection and climate moderation. For transparency, P&G does track and report annually the amount of wood pulp it purchases from suppliers certified under each of the forest certification schemes.\nSustainable Forest Management\nP&G purchases wood pulp from suppliers that:\n• Ensure the safety of forestry and manufacturing operations for employees and the environment.\n• Document that wood is legally harvested and that other legal requirements are met. P&G will not knowingly use illegally sourced wood fiber in our products.\n• Practice principles of sustainable forest management and continuous improvement in their own operations and in sourcing of wood, as verified by independent forest and chain-of-custody certification.\n• Do not obtain wood from High Conservation Value Forests. P&G supports multi-stakeholder efforts to develop information sources and tools that will help suppliers identify these areas on their own forestlands and in their procurement of wood raw materials from third-parties (e.g., https://www.hcvnetwork.org/)\n• Do not obtain the wood from genetically modified tree sources.\n• Reflect our social values and support of universal human rights through work with local governments and communities to improve the educational, cultural, economic and social well-being of those communities.\n• Do not source wood from conflict timber.\nWe ensure compliance with this policy through site visits, comprehensive surveys and research, required documentation and independent third-party certification of sustainable forest management practices. P&G requires pulp suppliers to independently certify their operations for sustainable forest management. P&G also requires suppliers to provide auditable assurance that wood from sources not directly owned and managed by them is legally sourced and that harvest is conducted via sustainable practices.\nWe give preference to pulp from Forest Stewardship Council (FSC) certified forests when it is available, meets product performance requirements and competitive market conditions. We will utilize FSC controlled wood in a stepwise approach to progressively increase the amount of pulp from FSC certified sources. P&G will actively participate in ongoing discussions concerning FSC standards, including becoming an FSC member. As a minimum standard, P&G requires that any wood pulp supplier ensure that their Forest certification system are consistent with the following generally accepted criteria for sustainable forest management:\n• A commitment to forest regeneration and reforestation.\n• Protection of soil quality, riparian zones and water quality.\n• Protection of ecologically and culturally unique forest areas.\n• Maintenance and conservation of biological diversity.\n• Participation of interested and affected stakeholders.\n• Continuous improvement in forestry practices.\n• Compliance with legislation, agreements and accords.\n• Support of universal human rights and respect for ownership and use rights of local communities and indigenous people.\n• Avoidance of unwanted wood sources.\n• Multi-stakeholder governance and transparency in decisionmaking and reporting.\n• Independent third-party verification of forestry practices and chain of custody for wood fiber.\nP&G may use wood fiber harvested from recently converted forest lands only if the supplier can demonstrate that they have protected HCV forests using standards and tools developed through multi-stakeholder efforts (e.g., https://www.hcvnetwork.org/) and that the forestry practices meet the minimum criteria previously discussed.\nEfficient Use of Resources\nP&G is committed to reducing demand on the world’s forests and other resources. P&G believes that we should invest our resources where we can make the greatest sustainability improvements and will continue to:\n• Focus on source reduction in the long\nterm or use of less fiber through development of innovative technologies that provide maximum product performance using minimal fiber.\n• Evaluate the use of non-forest derived sources of fiber, recognizing that alternatives must also meet principles of sustainable management.\n• Explore and implement energy and water conservation opportunities in our paper making operations.\n• Use recovered fiber when it has no negative impact on product performance, manufacturing efficiency, resource and energy usage and waste generation. We will continue to invest in research to identify the technical breakthroughs needed to allow us to use recovered fiber in our premium products without an impact on product performance, manufacturing efficiency, resource and energy usage and waste generation.\nDefinitions of Terms\nA source that falls within one or more of the following categories:\n• The source forest is known or suspected of containing high conservation values, except where:\n• The forest is certified or in progress to certification under a credible certification system, or\n• The forest manager can otherwise demonstrate that the forest and/or surrounding landscape is managed to ensure those values are maintained.\n• The source forest is being actively converted from natural forest to a plantation or other land use, unless the conversion is justified on grounds of net social and environmental gain, including the enhancement of high conservation values in the surrounding landscape.\n• The timber was illegally harvested or traded.\n• The timber is conflict timber (i.e., it was traded in a way that drives violent armed conflict or threatens national or regional stability).\n• The harvesting or processing entity, or a related political or military regime, is violating human rights.\n• The timber is from genetically-modified trees.\n• The source forest is unknown.\nHigh Conservation Value Forests\nHigh Conservation Value areas are critical areas in a landscape which need to be appropriately managed in order to maintain or enhance High Conservation Values. There are six main types of HCV values:\n• HCV1. Forest areas containing globally, regionally or nationally significant concentrations of biodiversity values (e.g. endemism, endangered species).\n• HCV2. Forest areas containing globally, regionally or nationally significant large landscape level forests, contained within, or containing the management unit, where viable populations of most if not all naturally occurring species exist in natural patterns of distribution and abundance.\n• HCV3. Forest areas that are in or contain rare, threatened or endangered ecosystems.\n• HCV4. Forest areas that provide basic services of nature in critical situations (e.g. watershed protection, erosion control).\n• HCV5. Forest areas fundamental to meeting basic needs of local communities (e.g. subsistence, health).\n• HCV6. Forest areas critical to local communities’ traditional cultural identity (areas of cultural, ecological, economic or religious significance identified in cooperation with such local communities).\nIllegal Sourced Wood\nIllegal logging occurs when timber is harvested, transported, processed, bought or sold in violation or circumvention of national or sub-national laws. Illegal activities often have a devastating impact on biodiversity as well human communities.\nTimber that was traded in a way that drives violent armed conflict or threatens national or region stability.']	['<urn:uuid:1ed49734-3ae2-4051-9560-ea5cfb203def>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	7	42	1175
27	What were the social changes and population impacts during Victorian times?	The Industrial Revolution narrowed class divisions as the middle class earned more wealth and began hiring servants like the upper class. The population of England nearly doubled due to improved health and no major epidemics or famines, leading to approximately 15 million people emigrating primarily to the United States, Australia, and Canada.	"[""Fun Facts About the Victorian Era\n25 JUN 2018\nWhen you think of the Victorian Era, you might picture a repressed English society. Women couldn't show their ankles due to societal norms set by class and gender rules of the time. The time period known as the Victorian Era included the years 1837 until 1901. Queen Victoria reigned during this period. While social norms were restrictive during that time, England went through enormous transformations during the Victorian Era, politically and economically. There were many new inventions that happened during then along with many social changes that began to equalize the social classes and narrow the gap between poverty and the upper class.\nThe Victorian Era was filled with the development of new inventions. In 1876 the telephone was invented by Alexander Graham Bell and the radio was invented in 1895 by Guglielmo Marconi. The camera, toilet, sewing machine, vacuum, train and stamp were all created during the Victorian Era. Factories began to be powered by steam, and the police force was created during this period. The introduction of these inventions helped to streamline day-to-day processes and create new working options. Social and health aspects of daily life improved with the advent of these inventions as people could now travel by train, utilize indoor toilets, clean with vacuums and communicate with the telephone. The intention of the radio brought news and entertainment to families in a way that had never existed before.\n2 Morals and Conduct\nDuring the Victorian Era, there were extremely strict codes of morals and conduct. Children were not allowed to be loud and did not spend much time with their parents. Children from upper-class families had less interactions with their parents because they were often raised by governesses or nannies, indicating their family's wealth level. Parent-child interaction during the Victorian Era was limited in reflection of gender and societal norms of the times. The gender-assigned parent roles also came into play with children learning the roles through their parents' behavior and their place in the class structure. For example, a lady did not wear a dress that would show her ankles and men did not call a single woman by her first name unless they were engaged. Maintaining a high level of societal, gender and economic roles dictated class interactions as well.\n3 Social Structure\nThe Victorian Era included the upper class, the middle class, and the working class. The households that had the most money had servants during this era. The Industrial Revolution occurred during the Victorian Era, which made the division of classes not as large. In particular, the middle class earned much more income after the Industrial Revolution and were considered wealthy by many. This greater earning potential started to narrow the wealth gap. Middle class Britains began hiring servants and governesses which echoed the societal behavior of the wealthier upper class.\n4 Hospital, Surgeries and Health\nTuberculosis caused the most health-related deaths during the Victorian Era. People who became ill during this time were sent away to “workhouses.” When a person needed to have surgery, there was no painkiller or anesthesia provided meaning that operations were hours of unbearable pain and agony for patients. Odd beliefs about health and wellness persisted in the Victorian Era. One example is that people often dined in their basements because they thoughts food would digest better in the dark. During the Victorian Era, growth meant that England's population nearly doubled. This was due in part to there being no catastrophic epidemic or famine in England during the 19th century."", 'Victorian Era dangers that people faced varied. That was because the Victorian Era — the period of Queen Victoria’s reign from June 20, 1837 until her death on January 22, 1901 — was also a time of peace, prosperity, and refined sensibilities. Fertility rates increased, no catastrophic epidemics or famines occurred, and health standards and nutrition rose. Nitrous oxide became a common anesthetic in 1846, chloroform was introduced a year later, and antiseptics were discovered by Joseph Lister in 1867. In fact, things were so good during the Victorian Era the populations of England and Wales doubled and approximately 15 million people emigrated to other countries, primarily the United States, Australia, and Canada.\nA year after the famous wax sculptress Madame Tussaud died, Victorians attended the first World’s Fair, known as The Great Exhibition of the Works of Industry of all Nations of 1851, which showcased the greatest and latest discoveries, including innovations from bookbinding to firearms and from carriages to musical instruments. It was a time for cultural fulfillment, lavish entertainment, and modern sports. Literature created by Charlotte Bronte, Charles Dickens, and William Makepeace Thackeray was available for all Victorians to read. There were also traveling circuses, museums, and seaside activities, and, when it came to sports, there was cricket, skating, bicycling, croquet, and tennis. Yet, not everything was wonderful during the Victorian Era because there were many dangers Victorians faced some of which included ship travel, crinoline, boracic acid, hydrophobia, and the air.\nOne of the more notable Victorian Era dangers was ship travel for poor emigrants who were primarily English, Irish, or Scottish. To transport them, emigrants traveled in “steerage,” which in the early Victorian Era was a windowless and ventless lower deck compartment near the rudder that was about 5 feet high:\n“[Steerage is] cold, dark, and — at the very outset of the voyage — foul-smelling. It extends nearly the whole length of the vessel beneath the saloon deck, and is divided into gloomy compartments.”\nPeople were crammed into these compartments with four tiers of berths or bunks, two on each side:\n“[They were] roughly driven hither and thither, and urged into their places by much hard swearing and abuse. Neither officers nor men considered them worthy of the least respect, and treated them as a drove of cattle.”\nEmigrants were also expected to provide and cook their own food, and often they either brought an insufficient quantity or no provisions at all. To cook the food, several hundred people used “two small ‘galleys,’ about five feet wide and four feet deep, each supplied with a grate.”\nThousands of emigrants died before reaching their destination because nearly 20 out of every 100 passengers died at sea either from fever or starvation. After the Irish potato famine of 1846, “out of about ninety-eight thousand laborers sent from Ireland to Canada … nearly twenty-five thousand perished in consequence of the poor rations and defective ventilation of the ships.”\nShip travel was only one of the Victorian Era dangers as crinoline was another. Crinoline was a stiffened petticoat or rigid shape designed to support and form a woman’s dress into a bell shape. It was described in this sense in 1863:\n“We use the word ‘crinoline’ in its conventional meaning — that is, an extravagantly large hoop, or congeries of hoops, whether made of steel, whalebone, or other material.”\nBut crinoline was more than just a fashion statement. It was dangerous and blamed for numerous deaths, including the death of an 18-year-old named Sarah Padley. According to Thomas Spencer, about 11:20 in the morning he heard screams and found Padley with her dress on fire. He grabbed a carpet, rolled Padley in it, and succeeded in extinguishing the flames but not before every particle Padley was wearing, except for her boots, was completely consumed by fire. Not only was Padley burnt but also two rooms were set on fire. After the accident it was learned that Padley was wearing a muslin dress extended by crinoline, and, apparently, when she crossed the room, the skirt of her dress touched the fire and immediately ignited. The Middlesex coroner, a Dr. Lankester, noted:\n“[It was] one of those numerously distressing casualties from the use of the dangerous crinolines … [and] if every fatal crinoline accident were reported the pubic would know of them, and … crinoline would soon be abandoned.”\nIn fact, Lankester claimed that between 1861 and 1864 “as many females have met their deaths by fire as were burned to death in Santiago!” Of these 2,500 deaths, he claimed “the great majority … resulted from the use of crinoline!”\nVictorians also had hardly a food or drink that did not have an additive in it making it another of the Victorian Era dangers. Liquor, bread, cheese, pickled vegetables, and sugary confections were just some of the items poisoned with such things as copper, sulphuric acid, or lead. To stop these poisonings, numerous acts were passed between 1860 and 1879 that helped to create safer foods, but it did not completely solve the problem.\nMilk dealers regularly added a boracic acid (sometimes called boric acid) preparation to milk to retard its coagulation and did so into the 1890s. Boracic acid was considered an antiseptic agent and a great food preservative because “when applied to milk warm from the cow, it kept it sweet and lasted twice as long as milk not treated with it.” A Professor Caldwell popularized boracic acid’s use and drank milk with boracic acid in it. He claimed, “No injury occurs to the milk in using 1 part boracic acid for 1000 of milk.” Additionally, proponents of the acid asserted that it was “as permissible to add boracic acid to a food as it is to add salt.”\nCritics disagreed. They argued there was no comparison as salt was a food and boracic acid was a drug. In 1892, Dr. J. Förster, who had conducted several experiments related to boracic acid, maintained that even the “addition of boracic acid to articles of food in far smaller proportions than is customary is injurious to health.” The writer of an article titled Food, noted. the “long continued use of the acid is not favourable to good health and … its addition to milk should be prohibited.”\nOne thing that caused great fear in Victorians was hydrophobia or canine madness (one of the two historic names for rabies). In fact, people of 1800s were so scared of rabies when someone was bitten, sometimes the person committed suicide or other people killed the person because of an irrational fear about what would happen and how the bitten person might behave. The fear was not diminished by Victorian papers either. They were constantly reporting on deaths caused by rabid dogs. For instance, in 1880, in Belfast, the Portsmouth Evening News reported:\n“Great excitement was occasioned in Belfast yesterday afternoon by a mad dog running through the principal thoroughfare and biting several persons.”\nFortunately, the animal was caught and destroyed, but not until after it had bitten six individuals, three of who the paper reported were in critical condition.\nThere were also numerous incidences of hydrophobia in 1881. One particular incident noted that a 5-year-old Manchester girl was bitten when a dog ran into a shop. As the dog was being driven out, it jumped up and bit her on the cheek and lip. She ran home, her mother took her to a chemist, who cauterized the wound, and she seemed fine until suddenly she manifested symptoms of hydrophobia and then died on Christmas Day.\nMany other hydrophobia cases happened exactly the same way, and almost all cases were fatal until Louis Pasteur developed a vaccine — harvested from infected rabbits — in 1885. Ten years later, The Living Age noted that prior to Pasteur’s vaccine the outcome was dismal:\n“[O]ut of a hundred persons bitten, nineteen or twenty [died because of hydrophobia and that] … ‘the mortality amongst cases treated at the Pasteur Institute … [fell] to less than one-half per cent,’ … and of 1673 patients treated by Pasteur’s method only thirteen died.”\nBesides ship travel, crinoline, adulterated milk, and hydrophobia, probably one of the most Victorian Era dangers people had to deal with was the air. Throughout most of the Victorian Era, horses were the primary source of transportation. This meant Victorians were constantly surrounded and dealing with piles of manure, which caused two major problems: It smelled bad and when dried easily converted into a powder that swirled in the air, coating and covering everything wherever and whenever it landed.\nAdd to the floating manure was coal. It was the main energy source for Victorian businesses and homes. In foggy weather, houses spewed out sooty smoke:\n“Steamboats on the Thames … [were] accustomed to vomit out huge volumes of smoke … [and] factory chimneys in the eastern districts of London … filled the air with blackness.”\nWhen this smoke was combined with London’s thick fog, it created smog, and London was soon nicknamed the “Big Smoke.” Yet, London was not alone in creating dangerous sooty air, all major cities did. If smoking houses, boats, and industries were not bad enough, there were numerous noxious and offensive businesses that created pungent odors daily. Tanners, hatters, “fellmongers, soap-makers, fat-melters, bone-boilers, glue-makers, etc. together with all sorts of waste from furriers, wool-spinners, hairdressers, etc.; putrid fish, putrid flesh, and the offal of markets and slaughter-houses … treated in vats or boilers with sulphuric acid and steam [regularly created offensive vapors].” These were mixed and mingled with manure and smoke already thick and blanketing the cities.\n-  The Century, Volume 14, 1877, p. 580.\n-  Ibid., p. 579.\n-  Ibid.\n-  Ibid.\n-  Crinoline. 1863, p. 2.\n-  Punch, Vol. 42, 1862, p. 79.\n-  “Crinoline and Fire,” Louth and North Lincolnshire Advertiser, 26 March 1864, p. 4.\n-  Ibid.\n-  Workshop Receipts, Vol. 2, 1890, p. 450.\n-  Workshop Receipts, Vol. 5, 1883, p. 273.\n-  Ibid., p. 272.\n-  Ibid. p. 271.\n-  “Hydrophobia,” in Portsmouth Evening News, 22 July 1880, p. 2.\n-  The Living Age, Vol. 207, 1895, p. 458.\n-  Transactions of the Seventh International Congress of Hygiene and Demography, Volumes 5-6, 1892, p. 28.\n-  The American Chemist, Vol. 5, 1875, p. 380.']"	['<urn:uuid:44f5aa81-516a-4e99-bdad-df50e8d68d56>', '<urn:uuid:c809cf59-d176-4c9c-b3ab-6cc6191bef5d>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T15:30:12.136021	11	52	2308
28	what signs doctor check diagnose appendicitis during examination	During examination, doctors check for specific signs including the Blumberg sign (pain when pushing deep and releasing the abdominal wall) and the Psoas sign (increased pain when raising the right leg while palpating the iliac fossa). They also check for pain in the right iliac fossa that radiates to the right leg through palpation.	['Appendicitis, Acute appendicitis is the most common abdominal surgical emergency. This condition commonly occurs between the ages of 10 and 30 years, but its occurrence is possible at any age, are known cases of particular events in elderly patients (pseudotumor forms or pseudoocluziva).\nAcute appendicitis is a disease in which there is an inflammation of the appendix. Once on the inflammatory process can not be stopped medication, treatment of this condition is surgical. As surgical intervention is faster, the patient’s pain is lower and more easily realizable surgery complications as reduced frequency and severity. I\nn If surgery is delayed, the disease may be complicated: the catarrhal stage (early), the state phlegmon, gangrenous later (necrosis body) with its perforation and peritonitis initially localized and then generalized and can cause the patient’s death.\nVermiform appendix is a tubular structure attached to the first portion of the large intestine (colon), section called check. Appendix base is implanted in the cecum, the most common medial and inferior to the union of the three tapeworms (smooth muscle strips), the most common being mezocecala position.\nBut also may have atypical positions (subhepatic, pelvic, etc.). The abdominal wall, it is projected on the right side of the abdomen in the lower, right iliac fossa region called.\nIs lymphoid structure (hollow organ abundant lymphatic tissue), but its function is not well known and is considered primarily a vestige embryo. Removal to the appendectomy (surgery that is extracted when it is inflamed appendix) does not alter digestive function.\nBecause this disease can be, in many cases, obstruction of the lumen, the body cavity or due to bacterial proliferation stimulated lymphatic or small foreign body (stone fruits, seeds).\nThis blockage of the lumen causes an increase in intraluminal pressure (inside the body), a disorder of blood circulation and inflammation in the body walls it can evolve without treatment until gangrene and perforation (rupture).\nCan be discussed and a genetic predisposition to obstruction causing appendicitis appendicular lumen, starting from the observation that there are families with more subjects apendicectomizati (surgery for appendicitis) than others.\nThese are varied but have certain characteristics:\n– Pain – is initially located in the epigastrium (the chest) and around the navel, and later to modify based right iliac fossa, often radiating to the right lower limb, pain intensity is different and variable from one stage to another; if initially looks like a discomfort, then it can become intense and even defensive muscle (abdomen becomes hard to the touch)\n– Inappetence (lack of appetite), nausea and vomiting, constipation or diarrhea are other symptoms that may occur\n– Subfebrilitatea is possible – not very high fever.\nIn some cases, symptoms can develop very quickly without representative, therefore presenting to an emergency doctor, these cases are the immunosuppressive therapies used in patients with organ transplants and certain diseases, patients with HIV, patients with diabetes mellitus, neoplastic disease treated with chemotherapy obese patients.\nIf pelvic appendix to these symptoms can be associated rectal tenesmus (cramps) with diarrhea.\nAlso pregnant women, young children and the elderly may have particular forms of manifestation of this disease. Women during pregnancy often have symptoms such as pain, nausea and vomiting, but when they have special shapes and intensities, the patient must be present to emergency surgeon for examination.\nAppearance of the disease raises special concerns because of their inability to communicate pain doctor. They may have atypical symptoms – just vomiting, drowsiness, difficulty in feeding, constipation, etc.. Also, specialized medical consultation should be done as quickly, especially if small children are often quick evolution, often even without intermediate phases.\nManifestations may differ, diagnosis is more difficult. May take the form pseudoocluzive – with symptoms like intestinal obstruction and pseudo – Ceca tumor-like symptoms. To them, a feature represents comorbidities (diseases associated with old age Typical): ischemic heart disease, hypertension, type 2 diabetes, kidney failure and so on, conditions that worsen the prognosis of the disease and that can and they decompensate during disease evolution .\nIn the diagnosis of this disease, an important role is occupied by history, medical history and physical examination.\nPhysician should carefully formulate questions, questions of which to deduce the nature of symptoms, when they occur and time evolution, their location, severity of symptoms. Also the patient will be asked by the existence of other diseases in history (personal or family), any ongoing treatments, allergies to medications or substances (elements to be taken into consideration in drug therapy during hospitalization), alcohol consumption, smoking and possible drug use must be mentioned, which is important information.\nThis involves inspection and can uncover a possible rash, swollen lymph (lymph increase), other skin lesions, respiratory movements mobility with the wall (peritonitis important for diagnosis, stage the patient shows a rigid abdomen movements without mobility Respiratory – abs “wood”).\nPalpation can detect right iliac fossa pain that radiates frequently right leg, pain intensity is variable from discomfort to push deep muscle pain with defense. Entire abdomen should be palpated.\nThere are certain signs with more specificity:\n– Sign Blumberg, push deep and then immediate release of the abdominal wall sign followed by a short-lasting pain at this level, which is evidence of peritoneal irritation, acute abdominal pain or implicitly a Heightened\n– Psoas sign – palpation of the iliac fossa and the patient’s recommendation to raise the right leg stretched, noticing an increase in pain at this level.\nPercussion and auscultation are not highly specific in diagnosing this disease. Percussion can detect an area of dullness in the iliac fossa where a plastron timpanism up or if CECA stasis in various stages of the disease. Auscultation may reveal one abdominal silence in appendicular peritonitis established.\nIt will measure the patient’s temperature, heart rate (pulse), respiratory rate and blood pressure.\nSpecific for acute appendicitis are:\n– Moderate leukocytosis – increase in blood leukocytes (white blood cells), usually reaching values 10.000/mmc but can get to 20-30.000/mmc in severe cases\n– Can be highlighted changes in ionogramei in cases of vomiting and dehydration\n– Urinalysis – laboratory examination is common for differential diagnosis of a urinary tract disorder\n– Test task for young women presenting these symptoms.\nCommon imaging tests are:\n– Simple abdominal radiography or dye (Radiography or gastrointestinal radiography)\n– Abdominal ultrasound\nThese tests are not specific for acute appendicitis.\nSimple abdominal radiography may show a certain level of aerocolie (distended loops in the abdominal flank – the region check) in early stages and in cases with appendicular peritonitis due to bowel distension loops around the abdomen may reveal levels hidroaerice – mark functional occlusion.\nRadiography is contraindicated in acute appendicitis, appendiceal perforation can trigger. It is only useful for differentiating elderly cecal neoplasm.\nAbdominal ultrasound also is not very specific exploration, but due to low costs and lack of harm is common practice especially for the differential diagnosis of acute appendicitis with other types of diseases: the female genital (ovarian cyst, ectopic pregnancy, uterine fibroids with / without necrobiosis, Annex Acute right, and so on – which is why you should be consulted sick and gynecological), diseases of the urinary tract (renal colic right ureteral calculus, cystitis, etc..), other diseases of small pelvis or intestinal (mesenteric lymphadenitis common in children, Meckel diverticulum), etc..\nCT scan is not usual in diagnosing acute appendicitis, being reserved for cases with uncertain diagnosis.\nAs was mentioned above, treatment of appendicitis is surgical and consists of surgery called appendectomy. This is performed in adults in most cases under spinal anesthesia, sometimes under general anesthesia, general anesthesia in children usually is.\nIncision in usual cases, uncomplicated right iliac fossa is small, – 1.5 to 3 cm and can be extended if intraoperative difficulties (atypical positioning, associated pathology – ovarian cysts, adhesions, and so on).\nThere are cases when appendectomy can be achieved through an incision of only 1 cm (if then will practice and intradermal suturing, the scar will be very aesthetic, resulting appreciated especially young patients).\nCan practice surgery and laparoscopy, which involves three small incisions.\nSurgical treatment should be associated and appropriate medical treatment: broad-spectrum antibiotic, analgesic, sometimes gastric antisecretory (patients complaining of postoperative pain in the epigastrium which is due to ligation meso appendicular) administered by infusion during the first 24 hours and then orally.\nAt 12-24 hours after surgery the patient can mobilize can begin to feed on clinical and typically 48-72 hours at the hospital is considering releasing surgeon recommendations: overall diet, avoid exercise 4-6 weeks, return the patient to control and remove wires from 1 week after surgery in general.\nTreatment for complications\nIn more severe cases or complicated (gangrenous acute appendicitis with abscess periapendicular, appendicular peritonitis) incision can be enlarged or median incision can be practiced even subombilicala. Antibiotic treatment will be much stronger and for a longer period and also the recovery will take longer. The surgeon will decide intraoperatively and need drainage peritoneal cavity lavage and frequently practicing the Douglas bag bottom drainage (region in which fluid accumulates collections) with drainage tubes, which will be suppressed from 48-72 hours, depending on the evolution patient.\nComplications of the disease:\n– Appendiceal perforation with abscess\n– Localized and then generalized peritonitis.\nThe evolution of this complication in children is more rapid. This is an extremely serious complication of the disease that can lead to death within a short time without emergency surgery.\nEarly postoperative complications:\n– Parietal suppuration (common complication in complicated cases)\n– Residual abscess (occurs after generalized peritonitis is a serious complication and requires reintervention)\n– Postrahianestezie headache often associated with nausea and vomiting, etc..\nLate postoperative complications:\n– Eventration postapendicectomie (in obese patients do not follow doctor’s recommendations or treadmill exercise after surgery and consists of a parietal defect that can be solved through surgery to restore it)\n– Peritoneal adhesions (after severe cases) that can lead to opportunities volvulari with intestinal occlusion.\n– Appendix is a small tubular structure attached to the first portion of the colon\n– Appendicitis is inflammatory disease of the body and is considered a surgical emergency\n– Symptoms are abdominal pain (usually on the right, but may initially be located in “the chest”), inappetence (lack of appetite), nausea and varstaturi, fever (but not too high) but have retained , not always acute appendicitis has all the symptoms and also may present with atypical symptoms\n– Diagnosis is blamed on personal history, clinical examination (performed by surgeon), laboratory tests and imaging laboratory investigations\n– Disease treatment is surgical and consists of appendectomy\n– Is a disease whose complications (appendicular perforation with generalized peritonitis) are of extreme gravity and may become sick.']	['<urn:uuid:4e73427d-db29-4eb7-a19a-1ce4ef260fa3>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	8	54	1756
29	Can all risks in healthcare be completely avoided?	No, people often think that all risks can be avoided, but this is unrealistic. Risk is constantly present in medical practice. However, risks can be minimised by planning ahead and following risk-management procedures.	['Risk is a natural part of all healthcare. There is always the potential that an unwanted or unexpected outcome will occur whenever any action is taken. Clinical risk management is therefore essential for improving patient safety, and all practices are expected to undertake it as part of their clinical governance procedures.\nHistorically, risk management has tended to be used reactively, in response to an adverse incident, but there is increasing demand for it to be used proactively. Remember the five Ps: perfect planning prevents poor performance.\nBe aware of the possible risks associated with a proposed treatment and the balance of benefit versus harm, and consider the range of alternative management strategies. This is essential if high-quality care is to be provided and patient safety is to be maintained. This planning can avoid serious consequences to both the patient and the healthcare professional.\nPeople often think that all risks can be avoided, but this is unrealistic. It is important to remember that risk is constantly present in medical practice.\nHowever, risks can be minimised by planning ahead, and following a four-step risk-management procedure.\nThe first step is to identify the risk. A common method used for this is critical incident analysis or significant event review. This identifies risks after an event has occurred. It involves reflecting on past events that have caused harm or have been identified as having the potential to cause actual harm.\nThe importance of near-misses, in which potentially harmful events are considered, is that these events usually identify underlying problems with the provision of healthcare, such as sloppy procedures or insufficient training. Prompt remedial action at this stage can prevent future tragic events.\nA more proactive approach is to consider the possible risks before any treatment is given or procedure is performed. For example, before prescribing penicillin it is essential to consider the risk of an allergic reaction but it is also important to ascertain any problems in swallowing the medication or remembering to take the prescribed dose.\nOnce a risk has been identified, it should be analysed to determine what action needs to be taken to eliminate or reduce it. The potential likelihood of harm has to be considered.\nDifficult decisions often have to be made about a risk that is rare, but potentially very serious, compared with a risk that is very common but has a low probability of causing harm. An example is the risk of a flare-up of asthma when NSAIDs are used.\nIn some circumstances it may be possible to avoid the risk by understanding the causes of it and taking appropriate actions.\nIt will not always be possible to eliminate risks entirely but risk-control measures can be introduced that reduce the likelihood of an adverse outcome. These include policies, clinical guidelines and patient care pathways.\nThese are also important for contingency or disaster planning, in which possible worst-case scenarios are considered. An example is the use of protocols when using methotrexate.\nRisks can be minimised using various control strategies which have been developed to limit the potential consequences of a given risk without accepting or avoiding it. This is the main approach to clinical risk management and is achieved through training of healthcare workers, patient education, policies and procedures.\nIn cases where the risk of managing a problem in primary care is too high, then risk transfer may be appropriate. This means moving the at-risk patient to another context, such as secondary care, before treatment.\nFinally, the effectiveness of the various approaches used to identify, analyse and treat a risk should come under scrutiny.\nIt is useful to have a multi-professional approach with the adoption of a ‘blame-free’ culture so that people can honestly give their opinion.\nDr Sandars is a senior lecturer in Community Based Education in the Medical Education Unit at the University of Leeds, West Yorkshire\nHow to reduce the risks of treatment in primary care\n- Be proactive about identifying and controlling clinical risk. Remember that risk is constantly present in medical practice.\n- Reflect on past events that have caused harm and on near-misses to identify any underlying problems that could be solved.\n- Consider the risks of treatment in advance.\n- Analyse risks to work out those which can be avoided and those which can be minimised using risk-control strategies.\n- In cases where the risk of treatment is too high for primary care, transfer the patient to secondary care.\n- Evaluate risk management strategies regularly.\n- Lilley R, Lambden P. Making Sense of Risk Management (2nd edition) Radcliffe Publishing: Abingdon, 2005.\n- Haynes K, Thomas M. Clinical Risk Management in Primary Care. Radcliffe Medical Press: Oxford, 2005.\n- Online postgraduate certificate in Patient Safety and Clinical Risk Management at the University of Leeds.']	['<urn:uuid:cafd5889-9992-4359-a711-302a91a14d0f>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	8	33	790
30	Looking for eco-friendly homes. What green features does this place have?	The building harvests water, features double glazing, contains bulk insulation in large cavities between external and internal cladding, has a central courtyard for cooling and cross ventilation, and uses efficient artificial lighting.	['Klein Bottle HouseEdit profile\nThis holiday home is situated in heavily treed sand dunes directly behind 16th beach at Rye on the Mornington Peninsula, only two hours from Melbourne. There was in this home to be the sense of arriving at a natural and built environment which was unique and in complete contrast to the owner’s city home. We began with the idea of the spiral, an acknowledgment of the end of the weekend journey to the beach. An Australian icon, McIntyre’s 1950’s Snellman House came to mind. As the project developed we carefully considered where the building should be located, its relationship to the prevailing winds, and possible points of entry, topography and vegetation. Subjected to numerous distortions, the topology of the spiral gave way to the topological classic The Klein Bottle. We were keen to remain topologically pure, to distort the shape as need dictates but not to appear to sever this form. We were attracted to the idea of the origami version of the Klein bottle, not just because it was able to be approximated in cement sheet (the resonance with the ubiquitous 1950s fibro shack was compelling) but both because of its beauty and the perversity of an origami Klein Bottle. To accommodate ‘rooms’ within the bottle we thought of them like objects inserted (the ship) within the bottle. The mathematician’s complex topological surfaces like the Klein bottle are appealing to us and many architects. They look like the new architecture of the computer age, and hold the promise of new form and spatial sequence. Radically they merge the floor, wall and ceiling, inside and outside. The fact that there are so few examples is evidence that they are actually almost impossible to achieve in reality. In this project, the Klein bottle was the perfect fit to the constraints of the site. Once adopted, the shape of this building had something of a life of its own, the genie was out and she was difficult to temper. The development was intense, the serious pursuit of joyful nonsense. The result we think is a unique shape and internal space, an unexpected entry sequence and series of new relationships between the traditional components of the home. The process was also a reminder that architecture does not spring naturally from place, and yet it is the beautiful abstract idea, that when carefully selected and developed can suddenly seem as natural as the tee tree that surrounds it. The building required extensive use of 3d software for both its development and eventual execution. The building is largely steel framed, the complexity of which required our office to develop representations for use in the shop drawing and production process. The building is a lightweight structure largely clad in cement and metal sheet, incombustible and lightweight these materials meet stringent fire overlays. The building harvests water, is double glazed and the large cavities between the external and internal cladding allow packing with bulk insulation. The central courtyard adds cooling and cross ventilation, Windows are hardwood, Flooring bamboo and artificial lighting selected for its efficiency.']	['<urn:uuid:0808ed3c-5ce2-4dc7-84c5-dccb431dc54c>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	11	32	512
31	maximum height reached each rocket stage	The booster reached 924 feet, the second stage reached 1342 feet, and the sustainer reached 1652 feet. The rocket was expected to reach about 3000 feet of altitude, but fell short due to inability to properly simulate the interstages and external conduits, added nose weight on site, and two motor failures - one K1050 in the booster split and dumped its contents, and one H124 in the second stage lost its nozzle.	"['LAUNCH OF THE N1\nIn the photo above, you can see part of the frontage of the site. The large vehicle assembly building is behind the green awning and the Ryder truck can be seen to the right of it. And, most spectacularly, is the N1 painting done by Mare McMurray for the launch! (See a close up.)\nAll of the rocket building had been done by the time we got out to the launch site (although some of it just barely).\nAbove you can see the inside of the assembly building (shelter) as things were unpacked. David Wilkins and just arrived and we were getting ready to start rocket preparations.\nHere is another view of the shelter, after we\'d started to work on prepping the rocket. Chet is starting to wire the sustainer motors.\nAnd you can see the active flight electronics. We flew 9 G-Wiz units in total:\nWe also flew a Missile Works WRC² R/C unit for sustainer ignition and back-up sustainer deployment.\nAbove you can see Larry hooking up the ejection charges for one of the recovery systems. We had six parachute cannon in the rocket:\n(Each parachute was a 10\' SPECTRAchute hemispherical parachute with red and gold bands.) Since each parachute cannon had redundant deployment, we had two charges in each cannon. The single parachute cannons and 5g charges and the double cannon had 10g charges for a total of 70g of black powder.\nAs motor guy, Chet had the responsibility for installing all 42 motors. In the pictures above, you can see him installing booster motors, sustainer motors and with motor installation done.\nTom Cloud (""Bubba von Braun"") hung out with us and was a huge help with the rocket. In the picture above, you can see he and Larry starting to work on the booster.\nBob made up two plaques for the rocket. The first was a take-off on the Surveyor plaque and the second had all the team members\' names in Cyrillic characters (sort of).\nThe rocket was far too large to assemble and measure the C.G. as a whole. Instead, we weighted and measured the C.G. of each stage separately and them worked out the C.G. of each flying unit based on weighted averages. In the picture above, you can see us balancing the booster to find the C.G.\nOn Friday night, we had the C.G. and update the RockSim file with the final weights and C.G.s for all stages. We decided to add another 25# to the nose to give us a larger margin of stability when the sustainer was flying alone. (You can download our RockSim 5 file to see for yourself.) In the picture above, John is drilling holes in the wooden escape tower for lead shot.\nRAISING THE ROCKET\nWe reconvened on Saturday morning at 6:00am, after two hours of sleep, to set the rocket up for the promised 8:00am launch time. Big projects are always late, but we had taken every precaution to prepare the rocket to go and we intended to make the time. (Because of the lack of fins and three stages, we had to launch before the wind came up.)\nHere you can see us out at the launch site (1500\' from the flight line) with the launcher base and the rocket parts laid out and ready for assembly.\nThe rocket was quickly assembled and raised to an upright position on the launcher base. Note that because the launch rod was a bit too flexible, we had to stabilize the sustainer with ropes at the second interstage and the escape tower.\nIn this picture, you can see the ropes stabilizing the sustainer and the four perimeter blocks. These were added at the last minute and Tom Cloud sacrificed one of his beautiful hand-made fiberglass tubes to make them. (That\'s a 12\' ladder to the left of the rocket.)\nChet was the lucky one who got to arm the stages once the rocket was upright. Here you can see him arming the sustainer while the rest of us mill around.\nEach stage had arming for the recovery electronics and shunts for the second stage and sustainer ignition. We used no switches. Instead, a wire came out from each unit to the outside. The leads were stripped and twisted together, then pushed back into the vent tubes. This system was simple and reliable and all units armed properly.\nThere we were with a rocket with 42 motors and 70g of black powder armed and ready to go. Not to mention with a height of 22\' and weighing over 200#. In particular, there was no way that Chet could escape since he was on top of a 12\' ladder right next to the rocket. His bravery and dedication definitely merit the Order of Lenin!\nAbove you can see Chet and Ed hooking up the igniters for the booster motors. The six central K1050s were launched using one system and the 24 perimeter G38s were lit with another. That\'s a lot of igniters!\nOur goal was to launch at 8:00am on Saturday and at 8:15 we were ready. The rocket was ready ahead of time and assembling and arming the rocket went smoothly. We made this announcement before the launch:\n""We have a model of the Soviet moon rocket, the N1. During the Apollo program in the 1960s, the Soviets also had their own moon program. The vehicle for this mission was the N1, mostly designed by Korolev. They flew four N1 vehicles, all of which had engine or structural failures. The program was abandoned in 1974 and the remaining airframe pieces became storage sheds.\n""The N1 stood 350 feet tall and was 55 feet in diameter at the base. It had 42 engines burning kerosene in liquid oxygen. The lift-off weight was 2860 tons. The total impulse of the first three stages was 7.4 billion Newton-seconds. (That would 723,000 full M motors.)\n""That was the 1960s. Now, for the new millennium, AERO-PAC conceived a group project: ""Moon Race 2001."" This was to be a 1/16 scale Saturn V (the U.S. moon vehicle) vs. an N1. We would drag race them at LDRS. Well, the American team never got their rocket built so the N1 wins by default. Be sure to check out the web site at www.moonrace2001.org!\n""Our N1 stands 22 feet tall and is more than 3½ feet in diameter at the base. Our model has the prototypical number of motors (for the 3 stages). The first stage has 24 G38s in a ring around the perimeter and 6 K1050s in the center. The second stage has 8 H124s in a ring and the third has 4 H124s. The rocket will recover in 4 pieces: the booster, the 2nd stage, and the sustainer recovers in two sections.\n""We want to thank Peter Alway for his wonderful book Rockets of the World. Also, we\'d like to thank Loc/Precision and FibreGlast for filling many last-minute orders. OK, let\'s go! We have the Soviet anthem and a countdown in Russian for you.""\nThe soundtrack can be downloaded here. First, the Soviet anthem starts. Then, 30 seconds into it, the countdown begins in Russian. (Countdown spoken by Vladimir Abdilov.)\nThe photo sequence below (by Ed Hackett) is worth a thousand words.\nThe first picture shows the smoke charges which ignited 20 seconds before the motors, during the anthem. The other three shots show the actual liftoff. (Click on the pictures for larger versions.)\nThe sequence above was taken by Chris Atteberry. (Again, click on the pictures for larger versions.)\nThis picture, showing the crowd and the smoke trail, was taken by John Bohner.\nThe photo on the right was taken by Jeff Stai and really shows the power of all those motors with the flames glowing through the smoke.\nThe flight was perfect, straight up and steady. The second stage staged properly at booster burnout and the third stage was ignited manually very shortly thereafter (Chet\'s magic again). The booster reached 924 feet, the second stage reached 1342 feet and the sustainer reached 1652 feet. The entire flight with all the events was easily visible from the ground. Truly spectacular!\nThe many parachutes (3 for the booster and 2 each for the second stage and sustainer) all deployed, but did not inflate properly. Partly because the shroud lines were taped and partly because there was not enough descent time.\nBecause the \'chutes never completely opened, all pieces of the rocket hit hard. The booster even bounced about three feet into the air! The parachutes were 10\' hemishperical shapes from SPECTRAchutes. They were made with alternating red and gold stripes which would have looked wonderful fully inflated. (The parachutes didn\'t inflate fully because we taped the shroud lines for packing and there was not enough distance to fall to break all the tape.)\nBecause the rocket didn\'t go very high (1600\'), the parts were all recovered very close to the launch pad. In the photo above, you can see the booster back on the ground.\nAll those blackjack motors make quite a mess as you can see in the picture above of the aft end of the second stage.\nThe booster was too large to fit into any of the cars, so we took the Ryder truck out to pick it up. You can see some of the landing damage (one of the outer nozzles broken off).\nIn the picture on the right, you can see the booster and second stage stacked on top of each other back in camp (without the interstage). Note the black marks from the second and third stage blackjack motors.\nOn the ground to the right of the booster, you can see the forward end plate. This plate jarred loose when the booster hit. (The three large holes are for the parachute cannon.)\nThe rocket survived amazingly well for such a large rocket and such a hard landing. Other than some cracks in the fiberglass, one of the booster conduit details broke off, and the breaking off of the top plate, the booster is in pretty good shape. Most impressively, the large base plate survived intact with just one nozzle detail broken off.\nThe second stage is in even better shape with no major damage.\nThe sustainer faired the worst. The wooden escape tower was driven back into the rest of the rocket when the sustainer landed, pretty well destroying the forward half of the sustainer. The aft half was fine.\nIt seems that the rocket faired so well because of its light weight and because of the Styrofoam. The foam didn\'t add much weight, but provided good support for the skin and the plates. In fact, the booster bounced when it hit, instead of shattering.\nThe numbers: the booster flew to 924\', the second stage to 1342\' and the sustainer to 1651\'. We had expected about 3000\' of altitude, but weren\'t able to simulate the interstages or external conduits properly. Also, we added nose weight on site to a rocket that was heavier than predicted.\nWe also had two motors fail. One of the booster K1050s split the fiberglass tube and dumped out its contents shortly after ignition. (This lost us 1/7th of our booster thrust.) Also, one of the second stage H124s blew off the nozzle.\nAbove you can see the summary screen from one of the two G-Wiz MCs we flew.\nThe MC is a new recording unit coming soon from G-Wiz partners.\nClick on the tabs ""Altitude Graph"" and ""Acceration Graph""\nto see some of the other screens.\nA couple of friends from work came down to see the launch. Above you can see Andy Mazur saluting in his Soviet uniform. There also extra pictures and videos for those who want to see more.\nThe rocket could be repaired and flown again, but the LDRS flight was its mission and that was accomplished spectacularly. The booster rests in the Rocketeer Media museum and the other parts are currently in the warehouse in which it was built. Thank you N1, you did us proud!']"	['<urn:uuid:b8a90dee-7663-4f7d-9249-aa561e617ba1>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	6	72	2012
32	cold war intelligence gathering transformation differences	Intelligence gathering methods transformed significantly from the early Cold War to modern times. During the Cold War's onset in 1947, Britain's Labour government established the Information Research Department (IRD) focusing on anti-communist propaganda and information distribution through traditional channels like media, MPs, and trade unions. The department gathered intelligence from diplomatic missions and secret service sources. In contrast, modern intelligence operations, as described in current Russian and American approaches, rely heavily on technological means and long-term human intelligence. The US now emphasizes technical collection through the NSA's interception of communications and computer network penetration, while Russia maintains a mix of traditional human intelligence with modern methods like computer infiltration, as demonstrated by their operatives attempting to deploy software for potential cyber intrusion.	"['The Origins of the Cold War\nby Ernie Trory\nTHERE MAY BE doubts in the minds of many as to when to so-called ""cold war"" was actually declared. Did it begin with the accession of Truman to the Presidency of the United States after the death of Roosevelt on the 12 April, 1945; or did it begin with Churchill\'s notorious ""iron curtain"" speech at Fulton, Missouri on 5th March, 1946? Or did it begin much earlier?\nBut whatever the doubts as to its origins there can be no doubt that the cold war as such was finally institutionalised in Britain, albeit secretly, by the Labour government during 1947, when it set up a secret Foreign Office department, known as the Information Research Department (IRD) ""to distribute anti-communist propaganda in Britain and abroad.""\nDetails of this operation, successfully concealed from the public for 30 years, were not officially released until 1978, when they were pursued, first by Guardian and Observer reporters, but later more relentlessly by Lyn Smith, then a tutor at the Centre for Continuing Education at the University of Sussex and a research student with the Open University.\nThe original idea was the brain-child of Christopher Mayhew, Foreign Office Junior Minister in 1947. In a 17 October confidential paper to Ernest Bevin in that year, he proposed a covert ""propaganda counter-offensive"" against the Russians by means of a Foreign Office department to be formed specifically for that purpose.\nForeign Secretary Bevin did not require much convincing and nor, for that matter, did Prime Minister Attlee who, having also received a copy of the paper, invited Mayhew down to Chequers for discussions.\nAccording to Lyn Smith: ""A meeting was held on 18 November 1947, to discuss Mayhew\'s ideas in greater detail. This was attended by Sir Orme Sargent, the Foreign Office Permanent Under Secretary of State; Yvone Kirkpatrick, Assistant Under Secretary in charge of Foreign Office Information; and Christopher Warner, another Under Secretary in the Foreign Office.\n""Mayhew put forward his ideas: the campaign should be as positive as possible laying stress on the merits of social democracy but, he pointed out, ""we shouldn\'t appear as defenders of the status quo but should attack capitalism and imperialism along with Russian communism.""\nIn fact, at this early stage the idea was more of a ""Third Force"" propaganda attacking capitalism as well as communism. This, however, was not to last for, as later documents reveal, anti-communism soon came to the fore.\n""Mayhew also explained that stress should be laid on the weaknesses of communism rather than on its strengths. In particular Russia should be portrayed as a backward country, a land of poverty for the masses and privilege for the few. He was doubtful about making civil liberty the main platform of the counter-propaganda since this appealed largely to those already converted, namely the intelligentsia.\n""Rather the aim should be to win the confidence of the workers and peasants and the best way of doing this was to show that there was more social justice and better living conditions for ordinary people under social democracy than under communism.""\nAnd so it was that in the mid-term of a Labour government, elected on a manifesto professing peace and friendship with the Soviet Union, the Labour Prime Minister and his Foreign Secretary were secretly engaged in setting up a propaganda machine. It was designed not only to keep voters in Britain safely insulated from communist ideas, but to win over the ""workers and peasants"" of the Soviet Union and of those countries in central and eastern Europe that had taken the path to socialism.\n""Yvone Kirkpatrick was entrusted with the task of setting up IRD and recruiting the right personnel, many of whom, as Lyn Smith pointed out, were ""emigrants from ""iron curtain"" countries, often journalists and writers."" The staff of the Soviet section numbered 10 at the start but steadily rose, reaching more than 60 in the mid 1950s after Churchill had once more become Prime Minister.\nThroughout the period of the Labour government, IRD gathered information from secret service sources as well as from diplomats in overseas missions. This Information, after processing, was distributed to British Ministers, MPs, trade union leaders, UN delegates and, of course, the British media - including the BBC World Service.\nOn 22 January 1948 during a debate on foreign affairs in the House of Commons, so we are informed by Lyn Smith: ""Bevin made a long speech, decidedly more aggressive than anything hitherto and incorporating all the main IRD themes.""\nBevin was not the only MP to use the ""notes"" provided by IRD. As Mayhew admitted in an interview with Lyn Smith: ""If some anti-Stalinist MP wanted information or briefing on some subject, then we were only too happy to send him the facts.""\nIRD had close links with an anti-communist organisation called ""Freedom First,"" run by one Herbert Tracey, Publicity Officer of the TUC until his retirement from full-time service in 1949. IRD publicity material was used in a newsletter published by ""Freedom First"" and distributed to several hundred key trade union organisers. There was also an international edition of this newsletter.\nLyn Smith tells us that: ""Indirect financial aid was given by an arrangement whereby IRD purchased newsletters at a price and on a scale sufficient to guarantee its financial soundness."" The ""Freedom First"" operation, however, came to an end when accusations were made that some of the money had been used dishonestly and a junior minister was forced to resign.\nThereafter, IRD concentrated on international affairs, being responsible in October 1948, for the initiation of an attack on ""Stalinist tyranny and labour camps."" That and similar ""information"" was supplied to journalists and broadcasters in OHMS envelopes, so them was no doubt as to its source. But the recipients were told that the documents were not statements of official policy and should be destroyed when no longer needed.\nIRD also had relations with the External Services of the BBC.\nAccording to Sir Hugh Greene, head of the Eastern European Services in 1949-50, IRD was known as ""an anti-communist department for propaganda."" In 1949 a regional information office was set up in Singapore.\nIn a letter from the head of IRD in London to the British Embassy in Singapore it was stated that: ""The Commissioner General attaches considerable importance to the project, which has become even more necessary now that the communists look like becoming masters of at least most of China.""\nThere is a copy of this letter in the US archives. ""Although the Americans were not involved in any way with IRD\'s founding,"" says Lyn Smith, ""they very soon, according to Mayhew, ""started their own show"" with the formation of the CIA and the US Information Agency.""\nAfter the fall of the Labour government in 1951, IRD expanded rapidly, moving from modest offices in Carlton House Terrace to a 12-storey block in Millbank but here we are only concerned with the extent of its operations under a Labour Foreign Secretary.\nHad anything been known about these covert operations there would most certainly have been objections that the non-attributable information being circulated was calculated to deceive the public and, in some cases, ""journalists and other opinion formers.""\nIn an interview with Lyn Smith, sometime in the 1970s, Mayhew defended the need for secrecy as follows: ""There was a large Labour majority in the House of Commons and quite a number of ""fellow travellers"" and people with Stalinist views inside the Labour Party. I think that had they known about IRD they would have attacked it as being contrary to Labour Party policies. So in a sense we had to keep it confidential for political reasons.\n""It would not be politically dangerous now [in the 1970s] but in those days so may people made excuses for Stalin that what we were doing would have looked to them [had they known] to be contrary to the interests of peace and friendship with the Soviet Union. That was the political reason for secrecy.\n""But it is difficult to make out that there was anything sinister about it. We were ahead of our time in fighting Stalinism; we were certainly taking great political risks, and quite right too. It was not underhand, unless it is underhand to brief up anti-Stalinist writers, broadcasters and trade unionists. I don\'t consider that underhand: it was confidential, but it wasn\'t underhand.""\nThe department was curtailed in 1976 by Anthony Crosland, then Foreign Secretary, and finally closed down m May 1977 by his successor, Dr David Owen. Its activities were no longer considered appropriate in the ""balmy atmosphere"" of detente that was then developing on the eve of impending changes in the Soviet Union.\nStudents of the cold war, and of the role of social democracy in general, owe a debt of gratitude to Lyn Smith for her meticulous work in unearthing the facts of this sordid story, and to the editor of Millenium: Journal of International Studies, for publishing Lyn Smith\'s definitive article on the subject.', ""Russian Spies and Strategic Intelligence\nBy George Friedman\nThe United States has captured a group of Russian spies and exchanged them for four individuals held by the Russians on espionage charges. The ways the media have reported on the issue fall into three groups:\n- That the Cold War is back,\n- That, given that the Cold War is over, the point of such outmoded intelligence operations is questionable,\n- And that the Russian spy ring was spending its time aimlessly nosing around in think tanks and open meetings in an archaic and incompetent effort.\nIt is said that the world is global and interdependent. This makes it vital for a given nation to know three things about all of the nations with which it interacts.\nFirst, it needs to know what other nations are capable of doing. Whether militarily, economically or politically, knowing what other nations are capable of narrows down those nations' possible actions, eliminating fantasies and rhetoric from the spectrum of possible moves. Second, the nation needs to know what other nations intend to do. This is important in the short run, especially when intentions and capabilities match up. And third, the nation needs to know what will happen in other nations that those nations' governments didn't anticipate.\nThe more powerful a nation is, the more important it is to understand what it is doing. The United States is the most powerful country in the world. It therefore follows that it is one of the prime focuses of every country in the world. Knowing what the United States will do, and shifting policy based on that, can save countries from difficulties and even disaster. This need is not confined, of course, to the United States. Each country in the world has a list of nations that it is interdependent with, and it keeps an eye on those nations. These can be enemies, friends or just acquaintances. It is impossible for nations not to keep their eyes on other nations, corporations not to keep their eyes on other corporations and individuals not to keep their eyes on other people. How they do so varies; that they do so is a permanent part of the human condition. The shock at learning that the Russians really do want to know what is going on in the United States is, to say the least, overdone.\nRussian Tradecraft Examined\nLet's consider whether the Russian spies were amateurish. During the 1920s and 1930s, the Soviets developed a unique model of espionage. They would certainly recruit government officials or steal documents. What they excelled at, however, was placing undetectable operatives in key positions. Soviet talent scouts would range around left-wing meetings to discover potential recruits. These would be young people with impeccable backgrounds and only limited contact with the left. They would be recruited based on ideology, and less often via money, sex or blackmail. They would never again be in contact with communists or fellow travelers. They would apply for jobs in their countries' intelligence services, foreign or defense ministries, and so on. Given their family and academic backgrounds, they would be hired. They would then be left in place for 20 or 30 years while they rose in the ranks — and, on occasion, aided with bits of information from the Soviet side to move their careers ahead.\nThe Soviets understood that a recruited employee might be a double agent. But stealing information on an ad hoc basis was also risky, as the provenance of such material was always murky. Recruiting people who were not yet agents, creating psychological and material bonds over long years of management and allowing them to mature into senior intelligence or ministry officials allowed ample time for testing loyalty and positioning. The Soviets not only got more reliable information this way but also the ability to influence the other country's decision-making. Recruiting a young man in the 1930s, having him work with the OSS and later the CIA, and having him rise to the top levels of the CIA — had that ever happened — would thus give the Soviets information and control.\nThese operations took decades, and Soviet handlers would spend their entire careers managing one career. There were four phases:\n- Identifying likely candidates,\n- Evaluating and recruiting them,\n- Placing them and managing their rise in the organization,\n- And exploiting them.\nThe longer the third phase took, the more effective the fourth phase would be.\nIt is difficult to know what the Russian team was up to in the United States from news reports, but there are two things we know about the Russians: They are not stupid, and they are extremely patient. If we were to guess — and we are guessing — this was a team of talent scouts. They were not going to meetings at the think tanks because they were interested in listening to the papers; rather, they were searching for recruits. These were people between the ages of 22 and 30, doing internships or entry-level jobs, with family and academic backgrounds that would make employment in classified areas of the U.S. government easy — and who in 20 to 30 years would provide intelligence and control to Moscow.\nIn our view, the media may have conflated two of Moscow's missions.\nTwin Goals and the Espionage Challenge\nOne of the Russian operatives, Don Heathfield, once approached a STRATFOR employee in a series of five meetings. There appeared to be no goal of recruitment; rather, the Russian operative tried to get the STRATFOR employee to try out software he said his company had developed. We suspect that had this been done, our servers would be outputting to Moscow. We did not know at the time who he was. (We have since reported the incident to the FBI, but these folks were everywhere, and we were one among many.)\nThus, the group apparently included a man using software sales as cover — or as we suspect, as a way to intrude on computers. As discussed, the group also included talent scouts. We would guess that Anna Chapman was brought in as part of the recruitment phase of talent scouting. No one at STRATFOR ever had a chance to meet her, having apparently failed the first screening.\nEach of the phases of the operatives' tasks required a tremendous amount of time, patience and, above all, cover. The operatives had to blend in (in this case, they didn't do so well enough). Russians have always had a tremendous advantage over Americans in this regard. A Russian long-term deployment took you to the United States, for example. Were the Americans to try the same thing, they would have to convince people to spend years learning Russian to near-native perfection and then to spend 20-30 years of their lives in Russia. Some would be willing to do so, but not nearly as many as there are Russians prepared to spend that amount of time in the United States or Western Europe.\nThe United States can thus recruit sources (and sometimes it gets genuine ones). It can buy documents. But the extremely patient, long-term deployments are very difficult for it. It doesn't fit with U.S. career patterns or family expectations.\nThe United States has substituted technical intelligence for this process. Thus, the most important U.S. intelligence-collection agency is not the CIA; it is the National Security Agency (NSA). The NSA focuses on intercepting communications, penetrating computer networks, encryption and the like. (We will assume that they are successful at this.) So whereas the Russians seek to control the career of a recruit through retirement, the NSA seeks access to everything that is recorded electronically. The goal here is understanding capabilities and intentions. To the extent that the target is unaware of the NSA's capabilities, the NSA does well. In many ways, this provides better and faster intelligence than the placement of agents, except that this does not provide influence.\nThe Intelligence Assumption\nIn the end, both the U.S. and Russian models — indeed most intelligence models — are built on the core assumption that the more senior the individual, the more knowledge he and his staff have. To put it more starkly, it assumes that what senior (and other) individuals say, write or even think reveals the most important things about the country in question. Thus, controlling a senior government official or listening to his phone conversations or e-mails makes one privy to the actions that country will take — thus allowing one to tell the future.\nLet's consider two cases: Iran in 1979 and the Soviet Union from 1989 to 1991. The fall of the shah of Iran and the collapse of the Soviet empire were events of towering importance for the United States. Assume that the United States knew everything the shah's senior officials and their staffs knew, wrote, or said in the period leading up to the Iranian Revolution. Or assume that the shah's prime minister or a member of the Soviet Union's Politburo was a long-term mole.\nEither of those scenarios would not have made any difference to how events played out. This is because, in the end, the respective senior leadership didn't know how events were going to play out. Partly this is because they were in denial, but mostly this is because they didn't have the facts and they didn't interpret the facts they did have properly. At these critical turning points in history, the most thorough penetration using either American or Russian techniques would have failed to provide warning of the change ahead. This is because the basic premise of the intelligence operation was wrong. The people being spied on and penetrated simply didn't understand their own capabilities — i.e., the reality on the ground in their respective countries — and therefore their intentions about what to do were irrelevant and actually misleading.\nIn saying this, we must be very cautious, since obviously there are many instances in which targets of intelligence agencies do have valuable information and their decisions do actually represent what will happen. But if we regard anticipating systemic changes as one of the most important categories of intelligence, then these are cases where the targets of intelligence may well know the least and know it last. The Japanese knew they were going to hit Pearl Harbor, and having intelligence on that fact was enormously important. But that the British would collapse at Singapore was a fact not known to the British, so there would have been no way to obtain that information in advance from the British.\nWe started with three classes of intelligence: capabilities, intentions and what will actually happen. The first is an objective measure that can sometimes be seen directly but more frequently is obtained through data held by someone in the target country. The most important issue is not what this data says but how accurate it is. Intentions, by contrast, represent the subjective plans of decision-makers. History is filled with intentions that were never implemented, or that, when implemented, had wildly different outcomes than the decision-maker expected. From our point of view, the most important aspect of this category is the potential for unintended consequences. For example, George W. Bush did not intend to get bogged down in a guerrilla war in Iraq. What he intended and what happened were two different things because his view of American and Iraqi capabilities were not tied to reality.\nAmerican and Russian intelligence is source-based. There is value in sources, but they need to be taken with many grains of salt, not because they necessarily lie but because the highest-placed source may simply be wrong — and at times, an entire government can be wrong. If the purpose of intelligence is to predict what will happen, and it is source-based, then that assumes that the sources know what is going on and how it will play out. But often they don't.\nRussian and American intelligence agencies are both source-obsessed. On the surface, this is reasonable and essential. But it assumes something about sources that is frequently true, but not always — and in fact is true only with great infrequency on the most important issues. From our point of view, the purpose of intelligence is obvious: It is to collect as much information as possible, and surely from the most highly placed sources. But in the end, the most important question to ask is whether the most highly placed source has any clue as to what is going to happen.\nKnowledge of what is being thought is essential. But gaming out how the objective and impersonal forces will interact and play out it is the most important thing of all. The focus on sources allows the universe of intelligence to be populated by the thoughts of the target. Sometimes that is of enormous value. But sometimes the most highly placed source has no idea what is about to happen. Sometimes it is necessary to listen to the tape of Gorbachev or Bush planning the future and recognize that what they think will happen and what is about to happen are very different things.\nThe events of the past few weeks show intelligence doing the necessary work of recruiting and rescuing agents. The measure of all of this activity is not whether one has penetrated the other side, but in the end, whether your intelligence organization knew what was going to happen and told you regardless of what well-placed sources believed. Sometimes sources are indispensable. Sometimes they are misleading. And sometimes they are the way an intelligence organization justifies being wrong.""]"	['<urn:uuid:11b57903-1b51-4be5-a74f-c277d1d4957d>', '<urn:uuid:024058de-9c4e-46f2-bbcf-2c7bddff63ba>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T15:30:12.136021	6	122	3770
33	gluten free diet benefits risks weight gain	A gluten-free diet can be healthy if focused on whole foods like vegetables, fruits, lean proteins, and alternative grains like quinoa. However, it poses significant risks for weight gain and poor nutrition. Gluten-free processed foods often contain more calories, sugar, and salt while having less fiber and protein than regular versions. People tend to overeat these foods thinking they're healthier, leading to excess calorie intake. Additionally, gluten-free products can cost up to 200% more than regular versions while being less nutritious. Unless you have celiac disease or gluten sensitivity, you may be better off eating whole grain products in moderation.	"[""Watch Out For The Hidden Pitfalls Of These Popular Diets\nHere's what you should know.\nThe products and services mentioned below were selected independent of sales and advertising. However, Simplemost may receive a small commission from the purchase of any products or services through an affiliate link to the retailer's website.\nIf you’ve resolved to eat healthier at one point or another, you may have been enticed by popular nutrition trends, like organic or gluten-free eating, or even vegetarianism.\nAlthough these seemingly healthy eating styles may be tempting to try, if you’re not careful with what or how much you eat, you could end up sabotaging your efforts and gaining weight.\n“People get caught up on going gluten-free, organic, low-sugar or whatever but then totally disregard portions, which can lead to significant weight gain over time,” said Lisa Young, a registered dietitian nutritionist and author of “Finally Full, Finally Slim: 30 Days to Permanent Weight Loss One Portion at a Time.”\nA diet based on health claims can harm weight loss efforts in two sneaky ways, according to Young. First, if you think something is healthy, you often give yourself permission to eat more of it. Second, foods with health claims can have more calories than you may realize.\nAnd there are other diet pitfalls to be wary of, particular to specific diets.\nThe problem with eating foods based on health claims is that you think you’re being virtuous. “You think, ‘I’m going gluten-free; I can then eat French fries, gluten-free cookies and pasta on top of that. Or I can have three pieces of gluten-free bread as opposed to one or two,’ ” Young said. But this can lead to excess calories and weight gain over time.\nThe same holds true for desserts. “Just because you have gluten-free cake, that doesn’t mean you can eat the whole cake,” Young said.\n“If you do not have celiac disease or non-celiac gluten sensitivity, you’re better off having a piece of whole-wheat bread than having a piece of gluten-free potato bread with sugar, salt and all sorts of other junk,” Young said.\nIngredients may be added to breads with tapioca or rice flour in order to make those foods taste more like the original version and as well as to mimic consistency. “Those breads often have more sugar or more salt, and gluten-free pretzels may have added fat,” Young said. “The product as a whole ends up being less nutritious.”\n2. Low-Fat, Reduced-Fat, Fat-Free\nAs with gluten-free foods, eating low-fat foods can cause you to consume more calories than if you stuck with the full-fat versions.\n“If someone eats one cup of regular yogurt, they consume 150 calories; by eating one cup of low-fat yogurt, they can reduce the calories to 100. Well, many people then eat two yogurts or even three, because ‘it’s healthier,’ without realizing that the calories are adding up,” said Sandra J. Arevalo, a registered dietitian nutritionist and spokeswoman for the Academy of Nutrition and Dietetics.\nFoods that are lower in fat may also have more sugar to maintain flavor, an ingredient that not only adds calories but can be of special concern for people with diabetes. Arevalo had a patient who switched from salami to turkey — a seemingly innocent move until she realized that his sugars were going up. Then she realized that one slice of salami has more calories and fat than turkey, but the sugar in salami was less than half the amount in the turkey. “Obviously, the more turkey he was eating, the more sugars he was getting,” Arevalo explained.\nYoung had a client with high cholesterol who ate a ton of sugary licorice. “I said to him, ‘what is it that makes you think you can have it?’ He said, ‘It’s fat-free.’ And I said, ‘but the calories aren’t free.'”\nPeople may know that fat-free licorice still has calories and isn’t exactly a healthy food. But subconsciously, it moves into another realm and gives them permission to eat more, according to Young.\nIn fact, one study found that low-fat foods can actually contribute to obesity, as people buy and consume more junk food when it is labeled “low-fat.”\n“If you are eating more of the food, you’re eating more calories,” Young said. “And ultimately, you could be gaining weight.”\nMany foods with low-fat or similar claims may have not only more sugar but more artificial ingredients, preservatives, salt and calories than the full-fat versions, Young explained. For example, low-fat salad dressing may seem like an appealing choice, but it may have more sodium, sugar and calories than the full-fat version.\nChoosing to eat certain organic fruits and vegetables can minimize your exposure to pesticides, and it offers environmental benefits. But seeing “organic” on a food label says nothing about its calories, because organic refers to the way the food is grown, not how it’s processed, Arevalo explained.\n“If something is plastered on a label, people think, ‘it’s gotta be good for me or better for me.’ But organic junk food is still junk food,” Young said.\nTerms like “organic” are calorie distracters and can give us permission to overlook the amount of calories that sweets can pack.\nFor example, an organic chocolate chip cookie delivers 150 calories, 40 more than a Toll House chocolate chip cookie.\n“People forget these things have calories, and so they eat more. But the calories will be comparable [to original versions of these foods], or they might have a little bit more,” Young said.\n4. The Mediterranean Diet\nEven diets that have been associated with health benefits can cause weight gain, depending on the quality of the foods you choose and your portion sizes.\nFor example, the Mediterranean diet has been linked to better heart health and brain health, and it was recently ranked the No. 1 best overall diet. It is rich in fruits, vegetables, whole grains, legumes, nuts, fish and healthy oils. But that doesn’t mean you can eat as much of these foods as you want.\nOne of Young’s clients was gaining weight because she was eating a whole cup of nuts, about 800 calories worth, along with two avocados (240 calories each) daily.\nSalad dressings made with lots of olive oil can also be calorie culprits. “You could easily be having four tablespoons of olive oil on your salad, and that’s 500 calories,” Young said.\nGrains should be consumed with caution, too. “If you eat three cups of whole-wheat pasta and a couple of slices of whole-wheat bread and oatmeal every day, you could gain weight eating too many whole grains,” Young said.\n“You could gain weight from the Mediterranean diet if you do not portion-control,” Young said.\nPreparing foods healthfully when following the Mediterranean diet can be just as important as portion control. A meal with steamed spinach and carrots, grilled fish and a baked potato is nutritious, but if you sauté your spinach with extra oil, bread and fry your fish, smash your potatoes and add sour cream and butter, and glaze your carrots, you are adding extra calories and fat that would go against your weight loss goals, Arevalo explained.\n5. Vegetarian Diets\nVegetarian diets can also lead to weight gain if foods are not chosen carefully.\nArevalo counseled one young girl who had gained weight after she started her vegetarian diet. “When I asked her what she was eating, she told me that she was eating crackers in the morning with orange juice. For lunch, she was having white rice, potatoes and water. For dinner, she was having pasta, bread and juice. The amount of starch and juice she had to eat and drink to feel satisfied had increased, and that’s where the weight was coming from.”\nYoung added, “I’ve seen a lot of vegetarians gain weight. They are so fixated on making sure they don’t have meat … but sometimes people end up eating a loaf of bread.”\nThey might top off the bread binge with soy ice creams or coconut-based frozen yogurt — creating a perfect recipe for unwanted pounds.\nWritten by Lisa Drayer for CNN.\n™ & © 2019 Cable News Network, Inc., a Time Warner Company. All rights reserved."", 'Should You Go Gluten-Free? The Truth Behind The World’s Most Hated Protein\nDoes it have any benefit for people without celiac disease? Here’s what you need to know\nIn recent years, more and more people are declaring themselves “gluten free”, exasperating dining partners, waiters, and researchers alike. Gluten is a protein found in wheat and many other grains – meaning it’s found in many common Western foods such as bread and pasta.\nThe gluten-free diet can be a life saving treatment for people diagnosed with genuine medical illnesses. That’s a fact.\nBut does it make any difference for the people who worship it as a weight loss diet? Or as a way of “cleansing” their bodies?\nThe gluten-free diet has been around for more than half a century, when doctors first established a link between gluten and the symptoms of celiac disease, an autoimmune condition where gluten triggers intestinal damage.\nThere is no cure for celiac disease or other gluten sensitive conditions, but the gluten-free diet allows those diagnosed to live comparatively healthy, symptom free lives (albeit ones without things like wheat bread).\nBut when carb-cutting took centre stage for people looking to lose weight, it was inevitable that some people would take it to the extreme and adopt the gluten-free diet.\nAfter all, why cut out just wheat when you can also eliminate barley, rye, and other grains as well?\nDoes this strategy offer any advantage for the people who don’t have celiac disease?\nAccording to a 2015 study published in the British Journal of Nutrition, the answer is maybe, but not if you’re relying on gluten-free products to fill the new gaps in your diet.\nWhen Is a Gluten-Free Diet Healthy?\n“The gluten-free diet can be healthy if the focus is on whole foods: vegetables, fruits, whole grains, lean proteins, nonfat or low-fat dairy, seeds, nuts, legumes, beans,” says Nancee Jaffe, registered dietitian to the UCLA Digestive Health and Nutrition Clinic.\nBut, she emphasises, grains like whole wheat and barley offer health benefits as well. A gluten-reduced diet may be the best option.\nOne benefit: Reducing your intake of gluten means that you need to increase your intake of other foods. Because food manufacturers are now catering to the gluten-free demo, there’s a host of new, healthful products on the market.\nFor grains, work amaranth, buckwheat, millet, or quinoa, into your diet. Each variety has different vitamins and minerals that can help you round out your diet.\nRelated: Is Gluten Really That Evil?\nWhen Is a Gluten-Free Diet Unhealthy?\nIf you’re cutting out healthy grains like whole wheat and barley and supplementing your diet with processed gluten-free products, you may be doing more harm than good.\nGluten-free snacks like pretzels and cookies are often higher in fat and sugar and lower in fibre and protein than their gluten equivalents, according to new research. They are also often not enriched or fortified with key nutrients.\nThat means that you’re probably paying more money for less nutritious food: gluten-free products can cost up to 200 percent more than their gluten-containing counterparts, according to research from Nova Scotia.\nFocusing on whole foods is an excellent way to maintain a healthy diet, but think carefully about your overall health before you decide to cut out gluten altogether.']"	['<urn:uuid:85c249ce-67ba-4bfc-a716-1105c5cec050>', '<urn:uuid:dc71cbbb-cb72-47ca-a0db-c23de0e80d51>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	7	100	1909
34	What role did Theodore Roosevelt play in forest conservation historically, and what current challenges are our forests facing due to climate change?	Theodore Roosevelt was instrumental in forest conservation, creating the U.S. Forest Service in 1905 and conserving 230 million acres during his presidency, with more than half given to the Forest Service under Gifford Pinchot's leadership. Today, forests face several climate-related challenges: increased vulnerability to fires, with projections suggesting a doubling of annual fire rates by 2040; greater pest pressure, with warmer temperatures allowing pests like the pine beetle to complete two life cycles per year instead of one; and changes in rainfall affecting moisture availability for growth. Unlike agricultural systems, forests are less adaptable to these changes since they represent long-term investments that cannot be easily modified once established.	['On a recent trip to the home of Gifford Pinchot, our conservation policy intern was surprised to learn an intriguing lesson about another conservation visionary—Theodore Roosevelt\nTheodore Roosevelt was a champion of fish and wildlife conservation because he was a champion of public lands. While TR might be most famous for adding five national parks and a big chunk of Yosemite to our public lands system, more than half of the 230 million acres that he conserved during his presidency was given to the U.S. Forest Service, an agency he created in 1905. He entrusted this land to Gifford Pinchot, the first chief of the Forest Service and a member of TR’s Boone and Crockett Club.\nI recently had the opportunity to visit Pinchot’s home in Milford, Pennsylvania, and learn a little bit more about him, the Forest Service, and TRCP’s namesake. It turns out that the two founding fathers of conservation were close friends. Roosevelt even introduced Pinchot to his wife Cornelia.\nConsidering the role that Pinchot wound up playing in the conservation of U.S. forests, it’s ironic that his family fortune was made in the logging business. His grandfather profited from a time when it was common to purchase and clear-cut forests with no regard for the long-term health of the land. In his career as a forester, Gifford Pinchot felt a deep responsibility for correcting this misuse.\nLike many of us, he grew up with an affinity for the outdoors, and his father encouraged him to turn that passion into a career. The Pinchot family even created an endowment at the Yale School of Forestry—which held summer field classes at Grey Towers, the family estate—to help future generations discover the values of the outdoor lifestyle.\nGrey Towers was built in the 1880s and donated to the Forest Service in 1963. The partnership between Roosevelt and Pinchot created an agency that now manages 193 million acres of public land. Their understandings of conservation lead the Service to view forest management as “protecting lands against overgrazing, controlling and combating fire, protecting fish and game, and providing public recreation.” Proper natural resource management has kept our national forests open for public enjoyment for more than a hundred years.\nI couldn’t help but think about all the recreational opportunities we are privileged to access because of the foresight of these two men. And what they might say about the current conversation around giving up our public lands.\nI think it might go a little something like this: “Conservation means development as much as it does protection. I recognize the right and duty of this generation to develop and use the natural resources of our land but I do not recognize the right to waste them, or to rob, by wasteful use, the generations that come after us.” – Theodore Roosevelt\nShannon Fagan is the TRCP summer intern through the Demmer Scholar Program. She is going into her senior year at Michigan State University where she is majoring in Social Relations and Policy and minoring in Science, Technology, Environment and Public Policy.', 'The USDA has released two comprehensive reports on expected effects of climate change on agriculture and forestry. The reports --Climate Change and Agriculture: Effects and Adaptation and The Effects of Climate Variabilityand Change on Forest Ecosystems: A Comprehensive Science Synthesis for the U.S. Forest Sector -- will be part of the upcoming National Climate Assessment.\nAccess both reports here.\nThe reports’ predictions are not comforting with claims that increases in temperatures, atmospheric carbon dioxide, altered precipitation patterns and increasing extreme weather events will definitely influence agriculture. And while some crops may actually benefit and expand into areas once impossible to grow in, the overall picture is not rosy. Management of weeds, insect pests, and diseases will gain even more importance in coming years.\n“The National Climate Assessment is produced every four years,” said Bill Hohenstein, Director of the Climate Change Program Office in USDA’s Office of the Chief Economist during a Feb. 7 interview with Farm Press. “That is done to meet a statute called the Global Change Research Act, which was passed in 1992.\n“So, government-wide, the United States puts out a comprehensive synthesis of climate change science. The last one came out in 2009. This one – out for public comment, right now -- will come out (this year). Generally, it takes a couple of years to pull a report like this together.”\nWhile the topic of climate change remains controversial, climate change proponents do include farmers. In a letter to Congress released shortly after the USDA reports, National Farmers Union President Roger Johnson said, “Mitigating and adapting to climate change is of significant concern to our membership and will be a defining trend that shapes the world. National Farmers Union has long played a leadership role in the agriculture community in regard to addressing the challenge of climate change. Our members are acutely aware of changing weather patterns and extreme weather events that affect their operations.”\nAmong Hohenstein’s other comments:\nRegarding findings on row crops…\n“The report does go through the major commodity crops and what can be expected.\n“The other part of the story that’s very important is what will happen regionally. How is climate change expected to play out in, say, the Southeast, the Southwest, or the Northeast? The implications regionally are expected to be different.\n“We’re obviously worried about higher temperatures and their effects on cropping systems, moisture. But there are a variety of implications. Especially over the next 20 to 30 years, there will be a mix of consequences for agriculture.\n“One is we expect longer growing seasons. For some crops, ranges within the United States will expand. Also, the effects on rainfall won’t be evenly distributed across the country. Some regions will see an increase in rainfall and moisture availability.”\nOn the effects on the livestock sector…\n“The report has a section on livestock production. Many of the effects are a function of temperature.\n“Again, you see two types of impacts. One is the negative effects of low winter temperatures. In the summer, there will be greater heat stress and with that comes losses in production and sometimes mortality.”\nWhat about infrastructure? Did you look at how river traffic might be impacted?\n“We didn’t in any detail.\n“However, there was a report put out by the Department of Transportation looking at the effects of climate change on transportation systems in the country. One of the things they looked at was the importance of things like the Mississippi River for grain transport. They looked at how areas of the Delta might be affected by sea level rise and how that could hit ports and some of the low-lying, major highway arteries.”\nSee DOT map of projected impacts here.\nWhat about forestry? Are you predicting more wildfires? More difficulty controlling insects?\n“Certainly both of those things.\n“Forestry systems are less adaptable than the agricultural systems. Within agriculture, you can plant cultivars of crops, change the timing of when you’re growing a crop. There’s just a lot more capacity to adapt to changes.\n“That isn’t true of forestry systems. Once you have a forest in place, that’s a long-term investment.\n“There are three things we’re particularly concerned about in terms of climate change and forest resources.”\n- The effect changes in rainfall will have on moisture availability for growing.\n- Changes in pests.\n“In some ways the changes in pests will be driven by the forest’s health. But another driver will be the fact of warmer temperatures. Some of the pests currently being knocked back during the winter will be able to get a jumpstart.\n“Some pests like the pine beetle are a real concern. We used to see one life cycle from that pest during a year. Now, there are two. Increasing temperatures will only increase that pest pressure.”\n“The increases in pressures from drought, pests and temperatures will make forests more vulnerable to fire. We project that there could be a doubling of the annual rate of fires by 2040.”\nAny suggestions about what the USDA response should be?\n“The reports integrate the effects of climate change as well as work on adaptation, or how we need to respond. There is a lot of information about how to make our natural resources, whether agriculture or forestry, more resilient to climate change and variability.\n“A lot of those investments will have pay-offs today. That includes things like improving the water-holding capacity of soils. That makes farmland more resilient to drought. Improving forest health has benefits today and also in the future with the changes we expect.\n“Many of the practices we recommend in terms of adaptation responses to climate change turn out to be smart conservation practices.”']	['<urn:uuid:c0ca988a-5e7c-4453-9c49-894d545db6e7>', '<urn:uuid:321a03f7-a35f-46f4-a86d-6919804a5eac>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	22	109	1453
35	My toddler snores loudly and breathes heavily at night - is this normal?	While noisy breathing during sleep is common in children, it could be a sign of difficulty breathing. In young children, enlarged tonsils and adenoids can contribute to narrowing of the airway during sleep. If your child has pauses in breathing or loud gasping/snorting sounds, this may indicate obstructive sleep apnea. Before seeing a doctor, you can try some home remedies like having them sleep in a relaxed position, keeping them in a cool atmosphere, and avoiding exposure to allergens. However, if you're concerned about their breathing patterns, it's important to consult a healthcare provider for proper evaluation.	"['Obstructed sleep apnea in children occurs when your child experiences brief pauses in their breathing pattern that last from a few seconds to minutes, resulting in your child feeling tired and sleepy the next day.\nThere are several types of apnea in children:\n- Obstructive sleep apnea: This is the most common type of apnea and is usually caused by a blockage of the airway due to enlarged tonsils and adenoidal tissue near the nasal passages.\n- Central sleep apnea: This happens when the part of the brain that controls breathing doesn’t start or maintain the breathing process properly. Common in very premature infants.\n- Mixed sleep apnea: A combination of central and obstructive apnea, mixed apnea is usually a sign of an immature breathing pattern and may occur when a child is awake or asleep.\nA sleep medicine expert can help get to the bottom of your child’s sleep issues with an overnight sleep test called polysomnography, which will measure your child’s quality, quantity and breathing patterns during sleep.\nDepending on the results from your child’s sleep study, treatments for apnea may include:\n- medications: to relieve nasal congestion and allergies\n- increased activity and better nutrition: recommended for overweight children\n- continuous positive airway pressure (CPAP): a nasal and/or mouth mask that forces air to send oxygenated air into the air passages and lungs\n- surgery: to remove large tonsils and adenoids that make it difficult to breath\nFrom Nemours\' KidsHealth\n- Apnea of Prematurity\n- Obstructive Sleep Apnea\n- Enlarged Adenoids\n- Sleep and Newborns\n- Sleep and Your 1- to 3-Month-Old\n- What Causes Night Terrors?\n- Should I Be Worried About My Child\'s Nightmares?\n- Sleep and Your Preschooler\n- Sleep and Your 1- to 2-Year-Old\n- Sleep and Your 4- to 7-Month-Old\n- Sleep and Your 8- to 12-Month-Old\n- Bruxism (Teeth Grinding or Clenching)\n- Sleep Problems in Teens\n- All About Sleep\n- Night Terrors\nTrusted External Resources\nAll About Sleep\nSleep — or lack of it — is probably the most-discussed aspect of baby care. As new parents quickly discover, the quality and quantity of their baby\'s sleep affects the well-being of everyone in the household. And sleep struggles rarely end when child moves from a crib to a bed. Instead of cries, it\'s pleas or refusals; instead of a 3 a.m. feeding, it\'s a nightmare or request for water.\nSo how do you get kids to bed through the cries, screams, and avoidance tactics? How should you respond when you\'re awakened in the middle of the night? And how much sleep is enough for your kids?\nHow Much Is Enough?\nSleep quantity needs vary based on age. But common ""rules"" about how many hours of sleep an infant or a 2-year-old need might not be helpful when it comes to your own child. These numbers are simply averages reported for large groups of kids of particular ages.\nThere\'s no magical number of hours all kids need in a certain age group. Two-year-old Lilly might sleep for 12 hours, while 2-year-old Marcus is just as alert the next day after sleeping for only 9 hours.\nStill, sleep is very important to kids\' well-being. The link between a lack of sleep and a child\'s behavior isn\'t always obvious. When adults are tired, they can be grumpy or lack energy, but kids can become hyper, disagreeable, and have extreme changes in behavior.\nHere are some approximate numbers based on age, with age-appropriate tips to help you get your child to sleep.\nBabies (Up to 6 Months)\nNewborns\' internal clocks aren\'t fully developed. They sleep up to 18 hours a day, divided about equally between night and day. Newborns should be wakened every 3 to 4 hours until they have good weight gain, usually within the first few weeks. After that, it\'s OK if a baby sleeps for longer periods.\nAfter those first weeks, infants may sleep for as long as 4 or 5 hours at a time — this is about how long their small bellies can go between feedings. If babies do sleep a good stretch at night, they may want to nurse or get the bottle more often during the day.\nJust when parents feel that sleeping through the night is a far-off dream, their baby usually begins to sleep longer stretches at night. At 3 months, a baby averages about 14 hours of sleep total, with 8 to 9 hours at night (usually with an interruption or two) and two or three daytime naps.\nIt\'s important to know that babies can cry and make all sorts of other noises during light sleep. Even if they do wake up in the night, they may only be awake for a few minutes before falling asleep again on their own.\nBut if a baby under 6 months old continues to cry, it\'s time to respond. Your baby may be truly uncomfortable: hungry, wet, cold, or even sick. But routine nighttime awakenings for changing and feeding should be as quick and quiet as possible. Don\'t provide any unnecessary stimulation, such as talking, playing, turning on the lights, or using a bright mobile device while waiting for your child to sleep. Encourage the idea that nighttime is for sleeping. You have to teach this because your baby doesn\'t care what time it is as long as his or her needs are met.\nIdeally, place your baby in the crib before he or she falls asleep. It\'s not too early to establish a simple bedtime routine. Any soothing activities (bathing, reading, singing) done consistently and in the same order each night can be part of the routine. Your baby will associate them with sleeping and they\'ll help him or her wind down.\nThe goal is for babies to fall asleep by themselves and learn to soothe themselves and go back to sleep if they should wake up in the middle of the night.\n6 to 12 Months\nAt 6 months, babies still need an average of 14 hours of sleep a day, with 2 to 3 daytime naps, lasting anywhere from 2 hours to 30 minutes each. Some babies, particularly those who are breastfed, may still wake at night. But most no longer need a middle-of-the-night feeding.\nIf your baby wakes in the middle of the night, but you don\'t think it\'s due to hunger, wait a few minutes before going to your baby. Sometimes, babies just need a few minutes to settle down on their down. Those who don\'t settle should be comforted without being picked up (talk softly to your baby, rub the back), then left to settle down again — unless they are sick. Sick babies need to be picked up and cared for. If your baby doesn\'t seem sick and continues to cry, you can wait a little longer, then repeat the short crib-side visit.\nBetween 6 and 12 months, separation anxiety, a normal part of development, comes into play. But the rules at night are the same through a baby\'s first birthday: Try not to pick up your baby, turn on the lights, sing, talk, play, or feed your child. All of these activities do not allow your baby to learn to fall asleep on his or her own and encourage more awakenings.\nFrom ages 1 to 3, most toddlers sleep about 12 to 14 hours over a 24-hour period. Separation anxiety, or just wanting to be up with mom and dad (and not miss anything), can motivate a child to stay awake. So can the simple toddler style of always saying ""No!""\nIt\'s important to set regular bedtimes and naptimes, and to stick to them. Parents sometimes make the mistake of thinking that keeping kids up will make them sleepier at bedtime. But the truth is that kids can have a harder time sleeping if they\'re overtired. Though most toddlers take 1- to 3-hour naps during the day, you don\'t have to force your child to nap. But it\'s important to schedule some quiet time, even if your toddler chooses not to sleep.\nEstablish a bedtime routine to help kids relax and get ready for sleep. For a toddler, the routine might be 5-30 minutes long and include calming activities such as reading a story, bathing, and listening to soft music.\nWhatever the nightly ritual is, your toddler will probably insist that it be the same every night. Just don\'t allow rituals to become too long or complicated. Whenever possible, let your toddler make bedtime choices within the routine: which pajamas to wear, which stuffed animal to take to bed, what music to play. This gives your little one a sense of control.\nEven the best sleepers give parents an occasional wake-up call. Teething can awaken a toddler and so can dreams. Active dreaming begins at this age, and for very young children dreams can be pretty alarming. Nightmares are particularly frightening to a toddler, who can\'t distinguish imagination from reality. (So carefully select what TV programs, if any, your toddler sees before bedtime.)\nComfort and hold your child at these times. Let your toddler talk about the dream if he or she wants to, and stay until your child is calm. Then encourage your child to go back to sleep as soon as possible.\nPreschoolers sleep about 11 to 12 hours per night. Those who get enough rest at night may no longer need a daytime nap. Instead, they may benefit from some quiet time in the afternoon.\nMost nursery schools and kindergartens have quiet periods when the kids lie on mats or just rest. As kids give up their naps, they may go to bed at night earlier than they did as toddlers.\nSchool-Age Kids and Preteens\nSchool-age kids need 10 to 11 hours of sleep a night. Bedtime problems can start at this age for a variety of reasons. Homework, sports and after-school activities, computers, TVs, mobile devices, and hectic family schedules all can contribute to kids not getting the sleep they need.\nSleep-deprived kids can become hyper or irritable, and may have a hard time paying attention in school.\nIt\'s still important to have a consistent bedtime, especially on school nights. Leave enough technology-free time before bed to allow your child to unwind before lights-out. A good rule of thumb is switching off the electronics at least an hour before bed and keeping TVs, computers, and mobile devices out of kids\' bedrooms.\nTeens need about 9 hours of sleep per night, but many don\'t get it. Early school start times on top of schedules packed with school, homework, friends, and activities mean that many are chronically sleep deprived.\nSleep deprivation adds up over time, so an hour less per night is like a full night without sleep by the end of the week. Among other things, a lack of sleep can lead to:\n- being less attentive\n- inconsistent performance\n- short-term memory loss\n- delayed response time\nThis can lead to anger problems, trouble in school (academically and with teachers and peers), the use of stimulants like caffeine or energy drinks to feel more awake, and car crashes due to delayed response times or falling asleep at the wheel.\nTeens also undergo a change in their sleep patterns — their bodies want to stay up late and wake up later, which often leads to them trying to catch up on sleep during the weekend. But this irregularity can make getting to sleep at a reasonable hour during the week even harder.\nIdeally, a teen should try to go to bed at the same time every night and wake up at the same time every morning, allowing for at least 9 hours of sleep.\nNo matter what your child\'s age, establish a bedtime routine that encourages good sleep habits. These tips can help kids ease into a good night\'s sleep:\n- Stick to a bedtime, and give your kids a heads-up 30 minutes and then 10 minutes beforehand.\n- Include a winding-down period in the routine.\n- Encourage older kids and teens to set and maintain a bedtime that allows for the full hours of sleep needed at their age.\nReviewed by: Rupal Christine Gupta, MD\nDate reviewed: September 05, 2017', 'Whats Causing My Childs Fever\nA high temperature can be a sign of infection including infections in the upper respiratory tract and lungs. Fever helps children and adults to fight infection.\nYour childs fever could also be caused by other illnesses, or by vaccinations.\nIn itself, a fever is not dangerous. It is the cause of the fever that is the concern. Always seek medical advice if you are worried.\nWeird Things Babies Do In Their Sleep\nOften, new parents are completely freaked out by some of the baby’s behavior.\nNothing in this world is quite as precious as a new baby. Their tiny little feet and hands move with such uncoordinated innocence. Even though their sweet eyes can’t see but a few inches in front of them, they pierce mom’s soul every time they look at her. The way they smell is intoxicating, spit-up and all. Their skin is so soft it’s like a tangible whisper. They are, by far, the most beautiful creation on this planet.\nNow that this baby infatuation has been made perfectly clear, let me get my head out of the clouds where all the baby-delivering storks are flying.\nIn addition to their absolute perfectness, babies are funny and strange. Some of the things they do are not exactly easy to understand. More specifically, weird things that babies do while sleeping perplex us grown ups.\nOften, new parents are completely freaked out by some of their baby’s behavior. Odd little things like sleeping with his or her eyes open and making funny noises can scare a new parent into calling grandma or mom for some reassuring guidance.\nWelcome to the wonderful world of parenting, for it truly is wonderful. Here are 15 weird things babies do in their sleep. Enjoy!\nHow To Stop Heavy Breathing While Sleeping\nBefore actually hitting a clinic there are some home remedies and exercises you can try first.\n- Sleep in a relaxed position\n- Diaphragmatic breathing is another way to treat this\n- Drink coffee in moderation caffeine is known to relax muscles\n- Quit smoking and avoid exposure to allergens\n- Stay in a cool atmosphere\n- Follow pursed-lip breathing\nAlso Check: Does Medicare Cover Sleep Apnea Oral Appliance\nWhat Is Obstructive Sleep Apnoea\nWhen someone falls asleep, the muscles of the throat relax and the airways at the back of the nose and in the throat become narrow. This can cause snoring or noisy breathing.\nAlthough noisy breathing during sleep is common in children, it may be a sign that the child is having difficulty breathing. In young children, the tonsils and adenoids may grow to be quite large, and this can contribute to narrowing of the airway during sleep. The photo below shows enlarged tonsils in a child with obstructive sleep apnoea.\nObstructive sleep apnoea is a condition where narrowing of the airways at the back of the nose and in the throat during sleep is enough to cause a child to have difficulty breathing or pauses in their breathing. The child will then wake up briefly because they cannot breathe properly, often with a loud gasp or snort. These episodes can happen many times through the night and the disturbed sleep can result in changes in behaviour during the day such as sleepiness, hyperactive behaviour, impulsivity, poor attention and difficulty learning at school.\nTop 10 Signs And Symptoms Of Sleep Apnea In Children\nAs parents, you want the best for your child. One of your biggest fears is that something would come along to compromise the health of your child or place that health at risk. Sleep apnea is a condition that affects nearly three percent of children, according to a scholarly article appearing in the Korean Journal of Pediatrics.\nThat is why it is so important for you to be on the lookout for these:\nAlso Check: How To Sleep After Smoking Crystal\nHow To Diagnose Sleep Apnea\nAccording to the American Academy of Sleep Medicine , the only way to properly diagnose sleep apnea is to have your child participate in an overnight sleep study in a sleep lab. During the sleep study, which is called a polysomnogram, sensors are placed on your child to monitor their brain waves, heartbeat and breathing activity during different sleep stages. This helps detect problems and can pinpoint the nature of the breathing problem.\nBefore taking your child to a sleep lab, youll want to make sure its accredited for children, because the way sleep studies are interpreted in infants and children are different from adults. Moreover, pediatric sleep labs offer the expertise of having technicians that are adept at working with children.\nMake Sure Your Baby’s Airway Is Clear\nWhen the babys breathing is not smooth, it can a frightening moment. If possible try to quickly inspect your babys nasal and oral airways in general terms. Perhaps, try to clear the nasal congestion with topical saline drops and then gentle use of a nasal aspirator. Look in your babys mouth and see if there is a foreign object that is easily seen and removed.\nRecommended Reading: Can You Lose Weight By Sleeping Too Much\nMildly Erratic Breathing In Babies\nA grandmother who was babysitting called an ambulance after her 3-week-old granddaughter paused repeatedly while breathing. The contented, sleeping baby would breathe rapidly for about 20 seconds and then stop breathing altogether. The woman feared the baby had inherited her husband’s sleep apnea.\nAlthough this intermittent breathing can look alarming, “periodic breathing” is common. Babies normally breathe faster than older children because their lungs are small relative to the size of their body. However, researchers suspect that the reason for irregular breathing is that the chemical sensors that detect carbon dioxide aren’t fully developed in a newborn. This means that babies sometimes just don’t know they need to breathe, and they pause until carbon dioxide levels become high enough to trigger these sensors.\nWhen to Worry About Erratic Breathing in Babies\nIf your baby ever has a blue or gray color around her mouth, or seems to be struggling to breathe, contact your doctor immediately.\nHow Is Transient Tachypnea Of The Newborn Diagnosed\nDoctors usually diagnose transient tachypnea of the newborn in the first few hours after a baby is born.\nA doctor will examine the baby and also might order one or all of these tests:\n- Chest X-ray. This safe and painless test uses a small amount of radiation to take a picture of the chest. Doctors can see if the lungs have fluid in them.\n- Pulse oximetry. This painless test measures how much oxygen is in the blood. A small piece of tape with an oxygen sensor is placed around a baby’s foot or hand, then connected to a monitor.\n- Complete blood count . This blood test checks for signs of infection.\nDon’t Miss: Sleep Number C4 Smart Bed\nWhy Is My Baby Gasping Or Sobbing In Her Sleep\nIf its 3:30am and youre googling why is my baby gasping or sobbing in her sleep? DO NOT PANIC. Its probably normal.\nIf your baby is making a sound like she had been crying hard but shes completely asleep, you probably dont need to worry.\nMy daughter did it. Let me walk you through it.\nToot has been doing this in her sleep since she was still in a bassinet next to my bed. Ive freaked out many times, frantically googled, and called the nurse hotline at 2:00am.\nI was ever vigilant about her breathing because I thought she would just randomly stop and that would be it. Motherhood has turned me into a constant worrier.\nTurns out I had nothing to worry about, and you probably dont either.\nThe Basics Of Newborn Sleep\nNewborns are naturally restless sleepers. Sure, there are minutes and hours when they sleep soundlywhen even the sound of a lawnmower outside or a babbling toddler in the next room doesnt wake them. However, much of the time, newborns cycle through a range of sleep, from fitful and light to deep and restful, from quiet to loud, from gurgly and grunting to squeaky and snoring, and back again.\nNewborn sleep can be full of sound and action. Many babies are easily wakeful, move and jolt while sleeping, flail their little arms and legs, make sucking sounds, and whimper for foodthis is all normal.\nRead Also: Sleep Number Bed Labor Day Sale\nIf Your Child Stops Breathing\nIf your child has stopped breathing and is not responsive, immediately begin CPR and call 911.\nIf your child ceases breathing for 15 seconds or more, and then resumes breathing, visit the pediatric ER. Even if your child seems fine, it is important to make sure the underlying reason for the episode has been resolved.\nMany children between the age of 6 months and 6 years experience breath-holding spells, involuntary breath holding that usually occurs when the child is crying or upset. Children who experience these spells do not need to seek emergency care unless the incident results in unconsciousness or a seizure. In these cases, it is best to visit the pediatric ER to make sure there are no other reasons for the seizure or unconsciousness.\nSnore Like A Freight Train\nYou didn’t know the Union Pacific was scheduled to stop in your bedroom did you? Well, in bringing home baby you also brought home a tiny person who can challenge the roar of a freight train! Maybe you can blame all the snoring on your husband and how that trait comes from his side of the family.\nMore than likely, your baby takes after both of you perfectly and his or her snoring is completely normal. Although it does sound awful, and the extreme noise can be disturbing, snoring doesn’t have to freak you out.\nBabies have small airways that are still filled with secretions, so breathing can sound labored. After all that time in the womb with amniotic fluid-filled lungs, can we blame them for making their breathing known to the rest of the world?\nAlso Check: Does Sleep Number Have Sales\nTreatment For Heavy Breathing While Sleeping\nTreatment depends on the cause and each cause needs different treatment based on the severity and period of existence. All the causes cant be treated with one simple method. Depending on the cause and the diagnosis reports a health care provider will suggest treatment that may include invasive or non-invasive methods. In rare cases, surgery may also be recommended.\nHeavy breathing while sleeping in pregnant women is also seen due to internal changes especially due to the enlargement of the uterus. The reasons for heavy breathing while sleeping differ from person to person, before visiting a doctor try to figure out the condition so that your doctor can understand your situation clearly.\nSudheendra is a passionate blogger for 8 years and holds a Degree in Journalism & Mass Communications. His writings particularly focus on health, medicine, diet & lifestyle. For him, everything that interlinks and relates to health & medical world entices him. His write-ups aim at educating people not by just giving facts but by infusing human touch.\nYour Babys Breathing Is Usually Normal\nThose first few weeks with baby can be scary. Its natural to analyze every move and every sound baby makesafter all, you just want him/her to be healthy. But the good news is that most of the time those funny noises babys making are totally normal. Babies breathe faster than adults do and its unusual for healthy, full-term babies to have breathing problems. Try to relax and have faithand keep baby close those extra snuggles will do you both good.\nAs always, if you have any concerns about your babys breathing, call your pediatrician immediately.\nDon’t Miss: How To Boost Metabolism While Sleeping\nVisit Your Doctor If:\n- your child is vomiting after they cough\n- has bouts of coughing that last over a minute several times a day, or\n- has a cough that lasts for longer than 3-4 weeks\nIt is very helpful to doctors and nurses if you can video the cough on your mobile phone. This is because different coughs are signs of different conditions:\n- long term conditions like cystic fibrosis and primary ciliary dyskinesia\nIs Sleep Apnea Life\nSevere sleep apnea can be life-threatening. If left untreated, it can affect the childs sleep architecture because the brain continually wakes the child up to breath. If this happens often enough, the infant doesn’t get restful sleep, which can affect functioning during the day. After time, this can cause long-term damage to the heart, brain and other parts of the body as well.\nAlso Check: Flexfit 3 Sleep Number Bed\nHow Can I Help My Child Breathe Better At Night\nThe most important thing you can do for your child is advocate. Your child needs you to see this through, by ensuring their health is given a thorough investigation. Work with an interdisciplinary team for the best care: an AirwayCentric® dentist, allergist, and an ear, mouth, and throat specialist. Above all, I cannot stress this enough: Your child must complete their treatment in order to make a permanent change to their health, breathing, and brain health.\nIn terms of specific dental treatments, we can use one or more of the following, where applicable:\nSleep Easy For Parents\nSIDS is the biggest concern for any parent, and we know that its a real risk even if SIDS is rare. Using one of the best baby breathing monitors gives you the peace of mind to be able to sleep well.\nAlways talk to your doctor if you feel as if this is the right decision for you. He might be able to let you know if its appropriate for your baby or not. Most importantly, remember that these monitors just ring the alarm. They wont do anything to save your baby in the event of an emergency. So, always have a plan, including babysitters.\nAlso Check: Sleep Number Adjustable Base Price\nHow Often Should I Check My Babys Breathing\nTypically, your newborn will have been assessed by a skilled healthcare provider before youre fully responsible for her wellbeing. If there were any major respiratory concerns, your baby wouldnt have been discharged from care.\nIf you have a home birth, or give birth at a freestanding birth centre, you will usually be discharged about 4-8 hours after birth.\nDuring the immediate postnatal period, your midwife will check your babys vital signs and make sure shes safely adjusting to life outside the womb. If youre discharged and not transferred, rest assured your baby is showing all the proper signs of good health.\nYour midwife will instruct you and your partner on how often you need to check babys vital signs until she comes back to follow up. You should take your babys vital signs as often as recommended. If you need more reassurance, you can obviously check more frequently, but make sure you also find time to rest.\nIf you give birth in the hospital, the medical staff will taken your babys vital signs as often as they feel necessary, based on how baby is adjusting.\nOnce youre discharged, your baby wont need regular vital signs checked at specific intervals. However, if youre doing an early discharge staff might recommend you take vital signs until you follow up with another healthcare provider.\nBaby Breathing Patterns And Sounds While Awake\nNow that weve covered the basics, lets talk about all those odd sounds your little one produces. In addition to baby breathing fast, its common for baby to make raspy, snorting, grunting or whistling noises.\nTo new parents, these noises can be unsettling, but think about your own breathing patterns. You probably dont even realize it, but adults make all kinds of weird noises too! Baby breathing patterns are just as varied as your owntheyll make different sounds, depending on their mood. A happy baby is going to make different sounds than a crying baby.\nDon’t Miss: Sit And Sleep Thousand Oaks\nWhen To Take Your Child To The Er For Changes In Breathing\nAbnormal breathing can be troubling for any parent to witness in his or her child. While some changes in breathing are temporary and relatively harmless, other abnormal breathing episodes may indicate a larger problem.\nA variety of pediatric respiratory conditions can affect your child’s breathing. If you notice chronic breathing concerns, visit a pediatric pulmonologist to determine if it may be due to chronic childhood allergies or pediatric asthma.\nWhat Are The Complications Of Poor Rest Oral Posture And Mouth Breathing For Your Child\nThough mouth breathing may seem harmless, as a long-term habit it can cause several developmental and health issues:\n- Dry mouth The lack of saliva caused by breathing through the mouth can interfere with the normal function of their mouth microbiome and the natural protection against gum disease and cavities.\n- Teeth grinding and jaw pain.\n- A lower IQ.\nIf you suspect your child is struggling with an airway sleep disorder, early intervention is key. The sooner your child is treated, the more easily reversible the symptoms. Im happy to use guided growth techniques for children as young as 30 months.\nAlso Check: Sleep Number I10 Vs Tempurpedic']"	['<urn:uuid:3f94aee8-e56e-4b08-87e4-c5d757816960>', '<urn:uuid:5bda011e-8404-4b3b-a4ce-1d312f135d6c>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	13	97	4914
36	How are historical churches used for storage and religious pilgrimage?	Historically, fortified churches served as storage facilities for valuable community possessions, including textiles, grains, wine, and even today continue to store foods like bacon, ham, and sausage in their tower roofs. As pilgrimage sites, major churches across Europe and the Middle East attract visitors to significant religious locations, such as Santiago de Compostela's cathedral (reputed burial place of Saint James), the Church of the Holy Sepulchre in Jerusalem (site of Jesus's empty tomb), and St. Peter's Basilica in Vatican City.	"['A fortified church is a church that is built to play a defensive role in times of war, built by the Saxons and Szeklers of Transylvania. Such churches were specially designed to incorporate military features, such as thick walls, battlements and embrasures. Churches with additional external defences such as curtain walls and wall towers are often referred to more specifically as fortress churches or Kirchenburgen (literally ""church castles"").\nThe most spectacular and well preserved monument is in Chair of Udverhely in Székelyderzs. The construction history, architectural styles, decorations and artifacts can be well observed on/in the church – this is why this edifice is frequently mentioned in art and architecture history books. Its stunning architecture, its splendid frescoes and the gastro-cultural offer of the community makes this place one of the most attractive tourist destinations in the region.\nIn 1334 the village’s priest, Tamás, has already been paying the tithes papal income tax, whereas the beginning of the church construction can pe dated to the 13th-14th century. First built in Romanesque style, the church was transformed into a Gothic one in the latter part of the 15th century with the support of the Petky family. In the same period, in 1419, the frescoes are painted on the walls of the already enlarged and renovated church. The fortifications’ oldest and most visible element, the tower, crucial part of the defense system, is to the south of the church. Before 1605 the church was attacked and destroyed by foreigners more than once. In the following decades the church was reconstructed in its present form: the 5 m high walls form a square; their bastions (4) are at an oblique angle and jut out from the walls at all four corners. A similar bastion is found in the middle of the western wall. The semi-cylindrical rib-vaulted arches, which reach into the nave and the choir, date to the first half of the 16th century. An extra floor was added (above the ship and the shrine), in order to have enough space for the men (weapons and ammunition) defending the church. Over the 18th century the church gradually lost its role as a fortress and shelter: the towers got covered with tile, the protective revetments were demolished and got replaced with storage rooms. They stored here the most valuable things of the church and community, such as textiles, grains or wine. The measuring bushels are preserved in a quite good condition and can be seen in their original places. The bacon, ham, sausage are still stored (like in the old days) in the roof of the tower.\nIn 1887 József Huszka discovered the frescoes on the church walls, which in recent years have been carefully restored. The most detailed mural is a Conversion of Paul the Apostle that includes the painter’s portrait. On the southern wall three saint bishops are painted. One of them, Saint Michael is represented as an armored soldier. The biblical tale of Saul’s (Apostle Paul) conversion is also visible here. On the northern wall of the ship the mural depicts Saint Ladislaus resting in the arms of a maiden and other painted scenes from the legends of Saint Ladislaus. The background is also a detailed work of art: on the dark main background star-like petals are drawn with much attention. On the built up window of the church a brick with szekler runes was discovered, dating from 1419.\nSince 2006 the building complex is part of the UNESCO World Heritage Site.\nThe whole building complex is a wonder. First of all, because the tourist cannot just simply “trip” on it, secondly, because one must see and taste those famous sausages (it is advisable to announce the visit at the parish).', 'Curious about the top Christian travel sites? Europe and the Middle East are full of these, but we have made a simple list of the very best. Read below to find some great travel destinations for Christians!\nThere is no doubt that the Vatican City is one of the best Christian destinations you could possibly visit.\nIt is home to St. Peter’s Basilica, the most famous church in the world. It was designed by all the top artists at the time, such as Bernini, Bramante, Maderno and Michelangelo. The tradition also says that Saint Peter, who was an apostle of Jesus, and later became the first bishop of Rome, was buried at this site and that his tomb is below the high altar.\nYou can expect to wait in line for hours when visiting the basilica. But it’s all worth it in the end. It’s a beautiful church and there is so much to see on the inside.\nIn the Vatican City we also find the Apostolic Palace – the official residence of the Pope. This building is also home to the Vatican Museums and the Vatican Library, of which you can only explore some parts. Many people visited the Apostolic Palace to see the Sixtine Chapel.\nTo visit the Vatican City, you must obviously go to Rome. The Vatican is a small enclave within Rome. It is arguably one of the best travel destinations in the world, and you can see countless amazing attractions after you’ve already visited the Vatican.\nSantiago de Compostela\nThe historic city of Santiago de Compostela is a famous destination among Christian pilgrims. In recent times, it has become more of a general tourist destination with all types of people doing the pilgrimage.\nChristians will be particularly fascinated with the cathedral. It is reputed as the burial place of Saint James who was one of Jesus’s apostles. The large cathedral is an impressive structure and many people choose to enter it on their knees.\nAside from the cathedral, take a walk around the city and enjoy the old streets. Santiago de Compostela has many small shops, nice restaurants and it is generally a beautiful city. The entire Galicia region of Spain is a gem, so it makes sense to take lots of time to explore it.\nJerusalem is a holy city for all the Abrahamic religions, and of course, Christianity is no exception. It was a place where Jesus did many of his teachings. The Last Supper, which was the final meal with the apostels, took place in Jerusalem. The crucification was also believed to be nearby, on the hill of Golgatha/Calvery.\nThe Church of the Holy Sepulchre is one of the holiest sites in Christianity. An impressive site in itself, but even more so because of the history. Jesus’s empty tomb is believed to be located here. Many pilgrims are going to Jerusalem each year, and upon arrival they can enjoy. a visit to this church, which is conveniently located in the Christian Quarter of the city. In total, the Christian Quarter has about 40 holy places, so it is full of nice attractions for interested visitors!\nBethlehem is the birthplace of Jesus Christ. Naturally, it has attracted visitors for many centuries because of this fact. Many Christians wish to walk around in the city where Jesus was born and where he spent a part of his life.\nChurch of the Nativity, the basilica in Jerusalem, is a holy place for Christians. The grotto in Jerusalem is known as the oldest site that has continuously been used as a site of worship in Christianity. It is also a World Heritage Site.\nDespite being born in Bethlehem, the city of Nazareth was considered the home of Jesus during his childhood. He was also known as “Jesus of Nazareth”. Like Rome, Santiago and Jerusalem, it is a center of Christian pilgrimage. There are many shrines around the city that commemorate Jesus and events from the Bible.\nIn early Christianity, Damascus was a highly important city. It is said that Saint Paul encountered Christ on his way there – his baptism happened when he actually arrived in Damascus.\nMany early Christians lived in Damascus, including St. Thomas and many others. In the old city of Damascus, you should visit the Christian Quarter. Overall, it’s an incredibly fascinating city, and it is one of the oldest in the world. Just be sure to take safety precautions, in light of the recent troubles in Syria.\nRio de Janeiro\nRio de Janeiro is famous for its huge statue of Christ – they call it Cristo Redentor in Portuguese. This monument is so famous that many Christians wish to see it.\nAside from this statue, there are many beautiful churches in Rio. The city has a rich history, and we should not forget that Brazil is one of the most Christian countries in the world. It is absolutely a place worth visiting!\nThe main attraction here will be the Shroud of Turin. It is also known as the Holy Shroud.\nTo explain it in simple terms, it is the linen cloth that shows a negative image of a man, often believed to be Jesus Christ. The Gospels tell us that the body of Christ was wrapped by a linen cloth. Many Christians believe this to be the one, although there are several burial cloths around Europe that claim the same. Yet, this one is a highly respected one by many Christians, and it is in the Vatican’s property.\nThe Shroud of Turin is only available to see during certain expositions, decided by the Vatican. So check up on that before you go. It is located in the Cathedral of Saint John in Turin.\nDue to its rich history, Turin also has many other churches worth visiting, and it is easy to have an enjoyable trip there.\nAnother Italian city with strong links to Christianity is Milan. Since Milan and Turin are located somewhat near each other, you can easily visit both during the same trip.\nFirst and foremost, Leonardo Da Vinci’s famous mural painting, The Last Supper, is found in Milan’s church, Santa Maria delle Grazie. It is an amazing attraction and arguably one of the most famous artworks in world history.\nMilan also has one of the largest cathedrals in the world. You will find that in the deep center of the city.\nAside from the Christian attractions in Milan, the city is huge and it is also a world leader in fashion, gastronomy, art, music and sports.\nIn Moscow, we find one of the most picturesque cathedrals in the world. We are referring to Saint Basil’s Cathedral, located in the Red Square. It was built in the 16th centuries on orders from Ivan the Terrible. The outcome, however, was the opposite of terrible – it is a beautiful Orthodox cathedral, it is a UNESCO World Heritage Site, and it is one of the most remarkable Christian sites, at least visually, in the entire world.\nDon’t forget that Russia is one of the more Christian nations in the world, so you’ll also find many other churches around this giant city. It is absolutely full of cultural attractions.\nThe Ukrainian capital is home to the Kiev Caves Lavra, which is considered to be the cradle of the Orthodox Church. It’s a huge monastery with a long history, and there is a lot to see. Furthermore, the architecture of the churches in Kyiv is just beautiful and much different than in Western Europe.\nOxford is quite a gem for Christian travelers. You might be familiar with C.S. Lewis and his influential writings. Mere Christianity is arguably one of the best works in modern Christianity. When you visit Oxford, you can take the C.S. Lewis & J.R.R Tolkien Tour and see everything that inspired these two Christians. And when you’re in Oxford, don’t miss out on the University Church or some of the many museums!']"	['<urn:uuid:44f083aa-fcf5-411d-8834-28c0c326767a>', '<urn:uuid:b4d1731c-cb5b-4405-8879-39a25735caad>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T15:30:12.136021	10	80	1942
37	when was cec commission environmental cooperation established	The Commission for Environmental Cooperation (CEC) was established in 1994 by the governments of Canada, Mexico and the United States through the North American Agreement on Environmental Cooperation.	['Richard A. Morgan announced as new Executive Director of the North American Commission for Environmental Cooperation (CEC)\nMontreal, 5 July 2019—The Council of the Commission for Environmental Cooperation (CEC) today announced the appointment of Richard A. Morgan as the ninth Executive Director of the CEC Secretariat.\nRichard A. Morgan is a former Commissioner for the International Joint Commission (IJC). During his time at the helm of the Canadian section of the IJC, he was responsible for overseeing projects that affect the level and flow of water across the Canada-US border and investigations of transboundary issues. He has over 30 years’ experience in public affairs, business and government administration in both the public and private sector.\nBorn in Sherbrooke, Quebec, and raised in Montreal, Richard A. Morgan brings to the CEC a breadth and depth of experience in public policy, international relations and change management. He began his career in the political arena as the executive assistant to the Prime Minister of Canada immediately after graduating in business administration from the University of Ottawa in 1986. He later became Director of Operations in the Prime Minister’s Office, assuming the leadership of large multidisciplinary teams and initiatives and was also responsible for the organization of major international summits.\nFrom 1994 to 2014, Morgan worked as a strategic planning, communications and public affairs executive in management consulting and public relations. He most recently completed a four-year term with the International Joint Commission, where he was appointed by the Government of Canada to serve as Commissioner.\nCanadian Minister of Environment and Climate Change Catherine McKenna, as the new Chair of the CEC Council, welcomed Morgan to the position.\n“Mr. Morgan is taking on this leadership role as the CEC celebrates its 25th year of successful environmental cooperation across North America. On behalf of the Council, we look forward to working with him to continue to advance environmental and trade priorities, working with governments, local and Indigenous communities, academia, business and youth”, said Minister McKenna.\nRichard A. Morgan will succeed CEC Executive Director César Rafael Chávez who concludes his three-year term with the CEC on July 15. In its 2019 Ministerial Statement, the CEC Council thanked Chávez for his valuable contribution and commitment in leading the CEC Secretariat.\nAbout the CEC\nThe Commission for Environmental Cooperation (CEC) was established in 1994 by the governments of Canada, Mexico and the United States through the North American Agreement on Environmental Cooperation, a parallel environmental agreement to NAFTA. As of 2020, the CEC is recognized and maintained by the Environmental Cooperation Agreement, in parallel with the new Free Trade Agreement of North America. The CEC brings together a wide range of stakeholders, including the general public, Indigenous people, youth, nongovernmental organizations, academia, and the business sector, to seek solutions to protect North America’s shared environment while supporting sustainable development for the benefit of present and future generations\nThe CEC is governed and funded equally by the Government of Canada through Environment and Climate Change Canada, the Government of the United States of Mexico through the Secretaría de Medio Ambiente y Recursos Naturales, and the Government of the United States of America through the Environmental Protection Agency.']	['<urn:uuid:dd1b1a2e-de6c-4333-a242-62b476f0231a>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	7	28	528
38	I've recently moved near Dobrich, Bulgaria, and I've noticed they make a lot of bread here. What is the historical significance of bread making in this area, and what makes their bread so healthy?	Bread making in the Dobrudja area (Bulgaria's Granary) has a rich history, with 40-50 bakeries operating in Dobrich at the beginning of the last century. The area's first industrial bread factory was established in 1934 by Constantin Dimitrescu. As for the health benefits, the bread produced in this region often uses whole grains which are rich in vitamins B1, B2, B3, folic acid, copper, iron, zinc, calcium, phosphorus and fiber. Additionally, when these grains are sprouted, they undergo a fundamental change that increases protein content, breaks down complex carbohydrates into simple sugars, and increases availability of antioxidants and vitamins.	"['The rich history of bread making in the fertile and sunny Dobrudja area, known as Bulgaria\'s Granary, goes back a long time.\nThe pro-active entrepreneurs in Dobrudja region believed in the words of a wise man who said that no one is larger than bread and handed down to the coming generations his respect to this noble occupation. People have been making bread ever since the world began and our ancestors worshipped it as a relic.\nAt the beginning of the last century, bread making in Dobrich was thriving. There were between forty and fifty bakeries in the city.\nAbout 80 years ago, in 1934, the Romanian Constantin Dimitrescu decided to expand the capacity of his bakery located at Yordan Yovkov St. This bakery differed from all the others, which is why everyone called it a factory. He established contacts with German companies, signed a contract for the delivery of equipment and dug a well. The water that was essential for kneading the dough gushed from a depth of 2 metres. The dream of the wealthy Romanian came true. Engineers and craftsmen arrived, the equipment was mounted and the furnaces were constructed.\nIn 1947 the factory was nationalised and became a municipal enterprise. This was the start of the three-shift bread production.\nDuring the 1950s and 1960s the enterprise underwent different structural transformations.\nGradually, bread making gained a new image, research was allowed and attention was paid to training young people. In the 1970s, the construction and equipment of the new bread making plant began.\nThe new facility was completed in the city of Dobrich in 1975. Until 1979, the bread making plant worked at full capacity, based on a three-shift system. The old Polish equipment was replaced by newer equipment made in the Czech Republic. Special attention was paid to the products sales system.\n1997 – The company built its own mill, which was expanded in year 2000 with equipment from the top Swiss manufacturer Bühler AG, with capacity of 150 t per hour. The production was expanded as a result of the new Rommel stone module with capacity of 1 t per hour for production of new high-quality rye, whole grain and Graham flours suitable for diet breads and enriched types of bread.\n1998 – A fifth production line was introduced for the production of basic types of bread.\n1998 - A new semi-automated production line of the German company MIWE was installed for the production of new innovative white and brown types of bread, sliced and packaged, launched for the first time on the Bulgarian market.\n2000 – A new fully automated production line of the company Werner & Pfleiderer was introduced, with capacity of 2400 loaves of bread per hour, for the production of sliced and packaged bread, which was an innovation in the Bulgarian bread making industry and marked the pathway for development of a number of competitors in this field.\n2003 – A new production line for basic types of bread was integrated, with capacity of 2400 loaves of bread per hour, with equipment by the German company WINCKLER.\n2004 – A new bread making plant was built in Varna.\n2005 – The strongest brand of Dobrudjanski Hliab AD was created – ""VITA"" and the company started producing new diet and healthy breads range under the ""VITA"" brand.\n2006 – Dobrudjanski Hliab introduced a Certified Food Quality and Safety Management System under BSS EN ISO 9001-2008 & HACCP. A certificate was issued by the German certification company TÜV Thüringen e.V.\n2009 – The company started producing croutons with bacon, mushrooms and plain croutons in packages of 200 g and 1 kg.\n2009 - The company built a new warehouse for finished products and new transport facilities for transportation, cooling and slicing the finished products, which modernised the process and made it fully automated. The modern new warehouse for selling the finished products has five electro-hydraulic levelling loading ramps that can be serviced simultaneously.\n2010 – With the assistance of the consulting marketing agencies Partners MMD OOD and NOEMA OOD, the company carried out a marketing audit based on market studies, which was followed by the development of a marketing strategy for the short-term and long-term company development.\n2010 – Re-positioning of the diet product range of the Vita breads. Logo and packages re-design. Two healthy sub-series of whole grain breads were introduced – the diet series Vita Silhouette and the healthy series Vita Balance.\n2010 – Dobrudjanski Hliab AD and Max Mel EOOD constructed their own automated grain storage facility with a dryer made by the top Italian company MULMIX. The grain storage facility has a capacity of 16 000 t and the production capacity of the equipment is 100 t per hour. An automated flour mixing and conveyor system was also built, as well as additional equipment for improving the mill’s efficiency and, respectively, the flour quality. They were developed with funding from measure 123 of Agriculture State Fund supporting small and medium-sized enterprises.\n2011 – Specialised industrial starter culture equipment of the Austrian company HB-Technik was introduced and installed, which brought the production processes to a new and higher level of technology. The starter cultures (starters) input in the production enhanced the products taste and flavour and increased their shelf life.\n2011 - New bread cooling conveyor routes (spirals) were built, made by the Italian company ALIT S.r.l.\n2011 - A new cutting and packaging machine for basic bread was installed, made by IBONHART, with capacity of 3600 loaves of bread per hour.\n2012 – Renovation of the mill laboratory - new laboratory equipment was purchased which brought about a more accurate analysis of flour and flour mixes. The renovation was funded by Operational Programme ""Development of the Competitiveness of the Bulgarian Economy"", the procedure for certification under internationally recognised standards.\n2012 – A new production line was commissioned in the company’s plant in the city of Varna, with fully renovated production capacities. It is designed for the production of specialised and luxury breads, croissants and products made of puff pastry and bunder pastry. The production equipment is made by the German company FRITSCH and the Italian company Mak.Pan.\n2012 – The plant in the city of Varna started to produce quick-frozen products under the new brand ""TeigMeister"".\n2012 – The project ""Introduction and certification under IFS of Dobrudjanski Hliab AD"" under contract ZMS-02-39/01.06.2011 was completed. The existing management system in Dobrudjanski Hliab AD was integrated in line with the requirements of the IFS v.5 for the two company plants.\nToday, Dobrudjanski Hliab AD is a modern and dynamic company that respects both tradition and innovation, invests in its human resources, production facilities and capacity in order to offer top quality products to its clients and partners.\nThe company management believes that their proximity to consumers and their knowledge about the needs of the consumers, the strict production control, the products’ excellent quality and modern image are the key to taking a leading position on the market.\nThe two company plants in the cities of Dobrich and Varna specialise in the production of bread and bakery products.\nThe company produces basic (factory-made) types of wheat, rye-wheat and brown breads, toaster breads and diet types of breads, Easter breads, croutons and hardtack, bread crumbs and flours in its production facility in the city of Dobrich.\nThe boutique products are made in the production facility in Varna: HoReCa (resorts, hotels and restaurants) products and bakery products, such as quick-frozen products and fresh products, special and luxury bakery products, snacks from puff pastry and bunder pastry, croissants, Kaiser bread, baguettes and bagels, ""rural"" and ""rustic"" type hand-made breads, round loaves, pita breads, etc.\nWe offer a wide variety of freshly baked products under the brand ""Dobrudjanski Hliab"": Basic and toaster breads, whole grain breads and Easter breads, ritual breads, flour, bread crumbs, croutons, etc. Our registered trademarks are ""Dobrudjanski Hliab"", ""Gergana"", ""Nashenski vkus"", ""Dobrotitsa"" and other graphic and word marks.\nVITA product range\nThe VITA brand is the leader in the product portfolio of Dobrudjanski Hliab AD with its two series of whole grain products:\nVita Silhouette are 100% whole grain breads without white flour among their ingredients. The diet breads from the Vita Silhouette series are essential for everyone struggling to lose weight. They make following a diet not only effective, but also healthy and tasty.\nVita Balance are whole grain breads boasting a unique balance between good taste and health benefits. They are made of 60% whole grain flour.\nThe ""TeigMeister"" brand includes both freshly baked and frozen products, prepared according to established Austrian, German, Russian and other recipes, which contribute to the variety of the product range and ensure the guaranteed German quality and technology developed by our production lines’ manufacturer.\nBaking the products in hearth furnaces gives them a crispy crust and an authentic taste and our own starters used for preparing them not only enhance their taste, but also extend their shelf-life, without adding any preservatives.', 'More about bread\nSprouted grain breads are an ancient food with a modern day twist. In ancient times most breads and grains were sprouted really by default. It was a challenge to keep large quantities of grains in dry storage, so most grains just started to germinate and were used in breads and other foods in sprouted form. Only after advances in dry grain storage did this allow for long term storage. This of course led to modernization and ever increasingly advanced methods of processing and refining grains into white floursof today. When processed and refined grain is separated into husk, germ and bran over half of the vitamins B1, B2, B3, folic acid, copper, iron, zinc, calcium, phosphorus and fiber are lost. In addition eating refined grains is unhealthy and can be linked to rising cases of diabetes, heart disease, stroke, obesity among other health issues.\nWhen grains germinate a fundamental change occurs in the physiology of the grain and the densest nutritional value is attained in order to accommodate the growing sprout. Chemicals and enzymes are broken down to prepare the endosperm and germ to nourish the growing plant. The seed undergoes such achange that our bodies digest the grain as a vegetable instead of a starch. Sprouting increases protein content, breaks down complex carbohydrates into simple sugars, increases availability of anitoxidents, potassium, vitamins A, B1,2,3,5,6 & C. And similarly to sourdough levian, sprouting breaks down gluten and enzyme inhibitors allowing for greater mineral availability and absorption. And of course who can forget the fiber, oh the fiber, fiber is very abundant in sprouted grains. Sprouted grain breads have a lowerGI and help stabilize sugar levels for longer, digests slower and leaves you feeling satisfied for longer. A multiplier effect is present when combined with benefits of sourdough, spelt, rye in bread form. A deep nutty flavor with multiple layers of taste thresholds, paired with sourdough the taste bouquet will leave you discovering tastebuds you never knew you had.\nWe use a variety of organic GMO-free sprouted grains. Rye, spelt, wheat, barley, millet, lentil, mung & cannelloni beans. All with their own unique set of plentiful beneficial attributes.\nSpelt is an ancient grain dating back to biblical times. Spelt, a cousin of wheat boasts a more robust nutritional profile, a pleasant nutty flavor and a water soluble gluten that sets it apart from wheat. Spelt is easily digestible and loaded with essential amino acids, vitamins, nutrients and minerals.\nSpelt fell out of favor with farmers and bakers due to extra steps that need to be taken while harvesting and baking. With advances in modern farming wheat became the grain of choice due to the combined harvester which allowed the harvesting and hulling of wheat to occur in one step. Spelt has a hefty husk that is tough to crack and requires added steps to hull spelt. But that hard outer hull protects spelt from pests and elements making it incredibly immune to whatever attacks it, not requiring pesticides and insecticides it is the natural choice for people looking for an organic or natural grain. Another reason it does not lend itself well to commercialized milling is the fact that it is incredibly hard to separate the bran and endosperm from the spelt berry essentially guaranteeing a whole grain flour. As opposed to wheat where the husk, bran and endosperm are easily separated and further processed and refined where most of the nutritious value is lost.\nSpelt was used in small batch baking in ancient times and once almost completely absent in America, it is enjoying a comeback due to the health conscious consumer. Spelt continues to be used in small batch artisan baking today. Due to the high water solubility in spelt it remains strictly a small batch artisan bread and does not translate well to large volume baking because of its delicate nature. Very much a “high maintenance” flour that if used in large facilities need to bolstered with additional gluten, various gums and a host of other ingredients, often used just as an additive in mass produced health-food bread in order to satisfy labeling requirements.\nWhen comparing one spelt berry to 5 wheat berries the sole spelt berry outperforms the wheat in nutritional value yet has a relatively low calorie load. Spelt contains a special type of carbohydrate called Mucopolysaccharides. Mucopolysaccharides possess anti-inflammatory traits which supports and strengthens our bones, joints and cartilage and contributes to a healthier bone structure for longer. These complex carbohydrates consist of long chains of glucose molecules which result in slower digestion, longer availability of energy and a more subtle rise in blood sugar for longer. A favorite amongst athletes.\nSpelt bread can be consumed by health conscious individuals and also individuals suffering from some specific health issues, ranging from allergies, cardiovascular disease, IBS, diabetes, arthritis, migraines, atherosclerosis, high cholesterol, to autoimmune disorders will benefit from eating spelt. As always, consult your doctor if you have any concerns with health complications.\nSpelt is described as part of the wheat family by the FDA. So technically it is not a “wheat-free” food but possess a completely different gluten profile than wheat and when sprouted and in a sourdough – spelt maybe a good fit with individuals suffering from gluten sensitivities and maybe with an enzyme regiment some individuals suffering from gluten allergies may be able to consume spelt bread. Of course this a decision to be made with your doctor.\nSpelt is gaining popularity as a health food because it contains a broad spectrum of nutrients, including complex carbohydrates, soluble and insoluble fiber, vitamins, minerals, fatty acids and more. Spelt is high in vitamin B2, niacin, manganese, thiamin, copper, and magnesium. Because spelt is highly water soluble, these nutrients are easily absorbed by the body. In addition to being loaded with essential nutrients, spelt is a good source of dietary fiber, which has been shown to lower total and LDL cholesterol levels. And it tastes great too!\nHere at Kolatek’s we pride ourselves in being Spelt-Central. We offer 5 different types of spelt bread baked daily in our store front bread oven. Various spelt pastries and sweets. A wide variety of spelt pastas, spelt flours, rolled spelt & spelt berries. And you can be sure there is more to come.\nSourdough bread is baked using sourdough starter or levain to help facilitate dough rise. Used often in combination with yeast and sadly with decreasing popularity on its own. Sourdough is versatile with all grain breads but is particularly effective when paired with rye. Quite possibly the perfect bread. A Superfood.\nDating back to ancient times, probably the first form of levain available to humans. Beneficial bacteria Lactobacillus (probiotic) help facilitate lactic acid fermentation that predigests starches and weakens gluten by breaking down protein bonds. This was a natural method to preserve food and help fortify the body with essential vitamins, nutrients and minerals while providing an easier to digest, highly satiable bread that helps increase glucose tolerance by lowering insulin response. Specifically pure sourdough rye, without yeast is effective. Without the aid of yeast the sourdough levian undergoes a lengthy rise. This allows plentiful time for beneficial bacteria to snack on starches and increase lactic acid that in turn creates an adequate ph level for the enzyme pytase which breaks down pytates which allows for improved mineral absorption in the body, which also happen to be plentiful in the bread. Superfood.\nSourdough makes available to us:\nStudies found that by incorporating this type of bread in a balanced diet allows for a higher success rate in reaching goals and is increasingly the choice for health minded individuals, athletes, and people engaging in physically demanding activities.\nHowever the pure sourdough bread does not lend itself well to industrialized large batch bread manufacturing. Due to long rise times and an the impossibility of bending sourdough to industrial machinery it remains strictly a small batch hand made artisan bread that demands outmost attention and has long since been glossed over by the high volume focused baking industry. And when sourdough levian is used in large scale production it is used as a flavor additive and not as the primary functioning ingredient in bread, often times aided with vinegar to achieve the sourdough tang taste desired by many.\nSourdough bread, a bread from ancient times with a boatload of health benefits and flavor. A distinctive richly flavorful tanginess that dazzles the senses and pairs well with melted cheeses, cured meats, sautéed onions, fresh vegetables, olive oil, various mustards and is very good toasted, grilled or pressed.']"	['<urn:uuid:ce2900a4-9263-4f7a-b430-b2d2cacf9860>', '<urn:uuid:ac745bdc-b84e-4962-959d-eaab65723cdd>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	34	99	2926
39	social effects childhood trauma long term	Both childhood neglect and traumatic brain injuries have significant long-term social effects. Neglected children often develop maladaptive attachment patterns, poor social skills, and coping abilities, leading to withdrawal from relationships. Similarly, brain injuries can result in reduced social competence, affecting participation in groups and number of friendships. These social difficulties are linked to impairments in cognitive processing and working memory, which affect the ability to handle social interactions appropriately.	"['The DfE published chapter and verse research on the impact of neglect in 2014 (Brandon et al – Childhood Wellbeing Centre). Of particular interest to adopters and special guardians are the long term consequences of childhood neglect as it is us who must live with them. Neglect has far reaching consequences, which must be factored into support for us by policymakers and legislators, who need to ensure we receive it. Instead of support however it can seem that children’s services suffer with ‘organisational amnesia’ and ‘silo approaches’ are taken where information is not shared and used throughout the organisation. It is a shame, and perhaps indicative of the skewed priorities of government and the DfE, that no consideration is given in this DfE report as to what sort of support will actually be needed for those parenting and caring for previously looked after children – where the problems arising from neglect may manifest, sometimes years later in adolesence, as attachment disorder, PTSD, anxiety, depression, ADHD or child to parent violence.\nThe system is failing us and we need the Adoption and Special Guardianship Leadership Board to let us in, so we can tell them about the problems we are experiencing and the gaps we fall through. Enough of political hyperbole and the rhetoric of ‘positivity’, we need formal policy evaluation that service users can feed into, which asks the right questions. We need to be able to learn from where the system is not working in order to improve it.\nOf course the system may fail children who go into care too. With no one to look out for a child – who will fight to ensure they receive the help and therapy they need? As Dame Christine Lenehan pointed out in the recent SCIE project on Mental Health of Children in Care, which she co-chaired, there is often no commissioning for children in care because no one commissions into a vacuum. So there can be very serious corporate neglect.\nThe way to resolve a silo organisational culture is with better transparency and the development of shared goals. Therein lies the problem. The DfE and various Regional Boards offer no transparency at all and it seems their priorities may not be focused on supporting families in the years that follow the Adoption Order and on into the child’s adulthood. Match funding from LAs with the Adoption Support Fund is far too low to suggest any real commitment to post adoption support. Just one child re- entering care, or going into secure accommodation, may involve expenditure that is greater than all adopted children have received throughout England. This makes no sense.\nAn Adoption Order is permanent but the Special Guardian Order ends when the child is 18, which is when children’s services cease being involved. The Adoption Support Fund finishes at 21, or 25, if there are special educational needs/ an EHC Plan. There is a gap here where support services are not in synch. What does this mean for the child and family? We are extremely concerned also that special guardians seem to get very little attention – none in this report on neglect – and are so often an afterthought or ‘add on’.\nWith no requirement for Local Authorities to do anything other than assess us, when we ask for help, we can end up in big trouble as families – as we are so often on the verge of crisis by the time support is requested, and an assessment may take several months or more to complete. Assessment is not support and may not lead to it either, especially if there is a ‘silo mindset’ and a solely ‘child focused’ approach, instead of the organisation properly getting behind us and providing whole family support. We suggest that a more holistic approach is needed. We would like to see the DfE and children’s services becoming more long term and more family centred in their focus.\nWhen considering the impact of neglect, it is not sufficient to simply ignore the past and expect adopters and special guardians to battle for support, be unable to access respite without our children having to re-enter care (with the destabilisation and added trauma this brings), worry about being blamed and overly critical approaches, and struggle to be heard at all by the Adoption and Special Guardian leadership and Regionalisation Boards – not when there is so much knowledge about the harm neglect does. Here is an excerpt from pages 7-9 of the Brandon Report commissioned by the DfE:\n“Neglect is considered to be at least as damaging, if not more so, than other forms of maltreatment because its impact is the most far-reaching and difficult to overcome (Gilbert et al., 2009). Neglect in the early years may be the most damaging from the point of view of long-term mental health or social functioning.\nImpact of neglect on children’s development\nNeglect is rarely life threatening but has the potential to compromise a child’s development significantly, across multiple domains. Because neglect frequently coexists with other forms of maltreatment, it can be difficult to disentangle its unique consequences. Nevertheless, there is now a relatively robust consensus based on a range of empirical evidence that demonstrates its adverse impact on all the seven dimensions of development identified in the Assessment Framework: health, education, identity, emotional and behavioural development, family and social relationships, social presentation and self-care skills (see Tanner and Turney, 2003; Norman et al., 2012; Hildyard and Wolfe, 2002; Manly et al., 2001).\nWhile neglect is thought to be particularly damaging in the first two to three years of life – a formative period for social, emotional and neurobiological development – it can compromise development throughout childhood and adolescence. There is good evidence from animal studies and increasing evidence from human studies that exposure to neglect, often alongside other forms of maltreatment, is associated with alterations in the development of the hypothalamic-pituitary-adrenal (HPA) axis stress response and differences in brain structure and function. Importantly it is thought that such changes serve to biologically embed vulnerability that may only later manifest in mental health problems (see for instance McCrory et al., 2010; McCrory et al, 2012).\nPersistent, severe neglect indicates a breakdown or a failure in the relationship between parent and child. This may be reflected in maladaptive attachment patterns; for example, neglected children are as likely as children maltreated in other ways to develop disorganised attachment styles (Barnett, Ganiban and Cicchetti, 1999). However, they differ from other maltreated children in that they show more evidence of delayed cognitive development, poor language skills, and poor social skills and coping abilities (Hildyard and Wolfe, 2002). They may also present as dependant and unhappy, and display a range of pathological behaviours (see Egeland et al, 1983; Ward, Brown and Westlake, 2012). Children who are neglected from early infancy may find that as their need for nurturing or responsive relationships goes ignored, they withdraw from relationships, feel a greater sense of failure and may even blame themselves for the neglect they experience (Manly et al, 2001).\nThere is some evidence that children who experience neglect without other forms of maltreatment have worse outcomes than those who are both neglected and physically abused (Egeland and Sroufe, 1981). Later in this paper we explore the observable indicators which signal impairment in pre-school aged children who are experiencing neglect.\nThe impact of neglect is not only widespread, affecting a wide range of developmental domains, it is also cumulative. Neglected infants and toddlers show a dramatic decline in overall developmental scores between the ages of 9 and 24 months (Egeland and Sroufe, 1981; Naughton et al, 2013). They also show a progressive decline in their cognitive function throughout the pre-school years (Strathearn et al, 2001). Neglected infants who initially display secure attachments increasingly develop insecure and disorganised attachment styles as they grow older (Howe, 2005). These findings imply that the longer pre-school children are exposed to neglect, the greater will be the harm. Longer duration of neglect has also been associated with an increased pattern of neural reactivity to social threat (McCrory et al., 2012).\nHindley and colleagues’ systematic review of risk factors for the recurrence of maltreatment highlights the cumulative nature of neglect since it is the most likely form of maltreatment for a child to re-experience (Hindley et al, 2006).\nFinally, neglect in the early years has long-standing consequences that can endure throughout childhood, adolescence and well into adulthood (Stein et al, 2009; Rees et al, 2011). Neglected babies and toddlers may develop both internalising (depressed, withdrawn, lacking in self-confidence) and externalising (acting out, aggressive, impulsive) behavioural problems in later childhood (English et al., 2005). Kotch and colleagues (2008) for example, have noted that ‘child neglect in the first two years of life may be a more important precursor of childhood aggression than later neglect or physical abuse at any age’ (p.725). Childhood aggression in turn is closely associated with adolescent violence and delinquency (Broidy et al., 2003). Anti-social behaviour and ADHD symptoms displayed by neglected and physically abused children may endure into adulthood, and both groups are significantly more likely to be arrested for violent offences than their peers (Widom, 1989).\nNeglect in childhood is also associated with a range of mental health problems in adulthood. As adults, neglected children are more likely to develop major depressive disorders (Widom et al., 2007) and post-traumatic stress disorder (Widom et al., 1999). Physical neglect in childhood is associated with greater functional and social impairment in adults with schizophrenia (Gil et al., 2009). Robust evidence has also been found for relationships between childhood neglect and anxiety disorders, suicide attempts, substance misuse, risky sexual behaviours and sexually transmitted infections in adulthood (see Norman et al., 2012 for further details). There are also long term physical consequences of neglect in adulthood which can include an increased risk of hypertension, and chronic pain syndromes (Anda et al., 2006).', 'Head injuries can make children loners, says a new study, ""Right Frontal Pole Cortical Thickness and Social Competence in Children With Chronic Traumatic Brain Injury: Cognitive Proficiency as a Mediator,"" published April 7, 2014 in the Journal of Head Trauma Rehabilitation that also suggests a potential treatment. New research, has found that a child\'s relationships may be a hidden casualty long after a head injury. Physical injury and social withdrawal are connected, says the new study.\nNeuroscientists at Brigham Young University studied a group of children three years after each had suffered a traumatic brain injury – most commonly from car accidents. The researchers found that lingering injury in a specific region of the brain predicted the health of the children\'s social lives.\nDo head injuries turn your child into a loner with less interest in friendships or group participation?\nThe study looked at kids three years after the initial incident and found that lingering injury in the brain\'s right frontal lobe is associated with lower social competence (participation in groups, number of friends, and socializing, such as meeting new people or being open to making more close friends. The study also suggests that therapy designed to improve working memory might \'treat\' the social difficulties.\n""The thing that\'s hardest about brain injury is that someone can have significant difficulties but they still look okay,"" said Shawn Gale, according to the April 10, 2014 news release, ""Head injuries can make children loners."" Gale is a neuropsychologist at Brigham Young University (BYU). ""But they have a harder time remembering things and focusing on things as well and that affects the way they interact with other people. Since they look fine, people don\'t cut them as much slack as they ought to.""\nGale and Ph.D. student Ashley Levan authored a study published April 10, 2014 by the Journal of Head Trauma Rehabilitation, the leading publication in the field of rehabilitation\nThe study compared the children\'s social lives and thinking skills with the thickness of the brain\'s outer layer in the frontal lobe. The brain measurements came from MRI scans and the social information was gathered from parents on a variety of dimensions, such as their children\'s participation in groups, number of friends and amount of time spent with friends.\nA second finding from the new study suggests one potential way to help. The Brigham Young University (BYU) scholars found that physical injury and social withdrawal are connected through something called ""cognitive proficiency."" Cognitive proficiency is the combination of short-term memory and the brain\'s processing speed.\nProcessing verbal and nonverbal cues\n""In social interactions we need to process the content of what a person is saying in addition to simultaneously processing nonverbal cues,"" Levan said, according to the news release. ""We then have to hold that information in our working memory to be able to respond appropriately. If you disrupt working memory or processing speed it can result in difficulty with social interactions.""\nSeparate studies on children with ADHD, which also affects the frontal lobes, show that therapy can improve working memory. Gale would like to explore in future research with BYU\'s MRI facility if improvements in working memory could ""treat"" the social difficulties brought on by head injuries.\n""This is a preliminary study but we want to go into more of the details about why working memory and processing speed are associated with social functioning and how specific brain structures might be related to improve outcome,"" Gale said, according to the news release. You also may be interested in the abstract of another study, ""Low Self-Awareness of Individuals With Severe Traumatic Brain Injury Can Lead to Reduced Ability to Take Another Person\'s Perspective.""']"	['<urn:uuid:19955bdc-44b1-4022-88a7-4832748c5b6b>', '<urn:uuid:240c09e0-704e-4b9a-a37c-2f5158e8f6f9>']	factoid	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T15:30:12.136021	6	69	2260
40	why do rolex watches glow blue at night instead green like others	Rolex's Chromalight has a signature blue tint because it uses europium as a dopant, which releases photons at wavelength of about 490nm. In contrast, the more common green glow in other watches uses copper as a dopant, releasing photons around 535nm.	['Light It Up!\nAs we dive into the annual festive season, we see all sorts of decorative lights hanging in the streets, in the trees and in our homes. But let’s not forget that in our watch boxes, there are also small little machines with their own decorative, and functional, lights. Just exactly how does our watches light up? We have a nerdy write up today to go into the science of illumination. The mechanics of the lume.\nPhysics of illumination\nBe it Christmas LED lights, television or the backlit screens on the classic F91W, the reason we see light is because there are packets of energy, called photons, entering our eyes. And when these photons vibrate at a particular wavelength, we see them as a particular colour. It is probably much easier if we looked at some familiar watches and explain from there.\nOne of the most efficient form of illumination is the light emitting diode, or LED as most of us know it. A current passes through a semiconductor which causes electrons to move, releasing photons.\nA more modern, but still classic example would be the Casio F-91W, which some Singaporeans would call the Army watch. It’s low cost, high versatility and ultra light weight makes it a great choice for intense outdoor training in the military. The LED backlight on this is green, suggesting that indium gallium nitride was used in the semiconductor.\nA newer, and higher end and fully mechanical watch that uses LED is the impressive HYT H4 Alinghi. I say impressive not so much because of the LED technology, but the engineering efforts to miniaturize a brushless dynamo into a corner of the watch, eliminating the need for a battery or electrical energy storage.\nLED bulbs tend to give an uneven illumination since it is a small bulb tucked away at the edge of the dial. Along comes electroluminescent (EL), or commonly known as Indiglo in Timex watches. Casio’s F-105W and uncountable other digital watches uses EL backlight in place LED bulbs for aesthetics reasons and better night time legibility. Underneath the dial, there is a very thin film consisting of essentially 4 layers: a conducting layer, a phosphor layer, an insulator and another conducting layer. When a current is applied across the thickness, the electrical current energizes the phosphor and causes photos to be emitted evenly across the film.\nPhosphorescence, or lume, is the go-to passive method of illumination on watches. We love our lume. There are even hashtags for lume like #frinightlume #lumeparty. What’s not to love about that gentle glow? To better describe how the glow is created, we’ll have to get into some very technical discussion that’s usually left for physicists. Phosphors are a group of materials that have the ability to absorb light energy (even if slightly outside the visible band, such as UV or IR) to bring electrons to an “excited” state. Once the energy source is removed, the material can continue a slow decay where the electrons will jump back to their “relaxed” stated. During which, the energy is released as photons of specific wavelength. There are a few types of phosphors, but most commonly used is zinc sulfide and strontium aluminate. This slow decay happens over a time frame between minutes to hours depending on the age and quality of the lume.\nA prominent lume that most readers have seen is the chromalight by Rolex, which has a signature blue tint instead of green. We now understand that the glow comes from a photon being released as the electrons jump from “excited” to “relax” states. Going one step deeper in the science, it is also possible to tune the colour we want, by adding in various activators to control the wavelength of the released photons. In the familiar green glow we know, the dopant is copper and releases photons around 535nm. In the more exotic Chromalight, europium is used and the releases photons at wavelength of about 490nm.\nMost lume only hold their charge for under an hour. Of course good watch brands such as Rolex are able to produce paints with superior phosphorescent properties and even then, it tends to last no more than a few hours. Pretty sure you have noticed that as the paint ages, it holds less charge and shows reduced brightness and glow time. To overcome the short glow time, some other form of activation has to be used. Introducing radioactive emission. As radioactive materials decay, they emit energy that can be absorbed by phosphor, and then the phosphor does it’s magic where electrons that surround it jumps from a higher state to a lower more stable state, and releases a photon in the process. As the radioactive material does not stop decaying, it is constantly bombarding the phosphor with electron beams which creates a persistent glow by the phosphor.\nAlmost a century ago, radium was the radioactive material of choice to be put in lume paint. On its own, it appears as white. Radium and all its isotopes are highly radioactive and glows on its own without further addition of phosphors. However, the detrimental health effects soon caught attention and it was completely phased out, albeit taking decades to be completely eliminated. In more recent history, radioactive material tritium (3H, a radioactive isotope of Hydrogen) became a more popular choice as shown in the Sinn EZM1. In those days, the tritium loaded paint (and radium) was applied as a paint. While harmless to the wearer, the operators who applied the paint manually were exposed to an amount that accumulated and became harmful. More so for those who had the habit of licking their paint brush tips to get it sharp for these precise applications. Eventually, tritium also became less popular and got replaced by light-activating lume.\nIn more recent years, engineers worked around the safety aspect of using radioactive materials and introduced the concept of using sealed tubes of tritium gas. Mb-microtec manufactures these tubes with the brand name Trigalight.\nIt’s a long glass tube internally coated with a phosphor (zinc sulphide with customizable dopants in this case) and then filled with tritium gas to an optimal pressure. The higher the pressure, the more radiation to activate the phosphor and thus increasing brightness. The long glass tube is then laser cut into the small required lengths, with the laser sealing the ends as it cuts. Although it will not need re-charging, the overall life of the lume depends on the half life of the tritium gas (about 12 years).\nProbably about half the watches available comes with some form of illumination; from the most economic Casio to haute horology such as A. Lange & Söhne Datograph Up/Down Lumen. You’re likely to own at least one that illuminates, so pick out your favourite and go join the festive night lights this holiday season!']	['<urn:uuid:cbde0d79-83e8-4f4b-90e6-be2de3a38fcc>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	12	41	1142
41	dermatologist here tattoo removing diy versus laser risks efficiency	DIY tattoo removal methods are ineffective and dangerous, potentially causing scarring, infection, and complications since they only affect the epidermis while tattoos are embedded in the dermis. In contrast, laser removal is safe and reliable, using specific wavelengths of light to break down ink particles that are then removed by the body's filtering systems, though it typically requires 5-20+ sessions depending on the tattoo type.	['Tattoo regret is quite common. The reasons for your regret can vary, from changes in personal taste to new job opportunities. But tattoo removal can be an expensive and time-consuming process, so you may be wondering: is there a way that I can safely remove my tattoo at home?\nTable of Contents (clickable)\n- 1 Can Tattoos Be Removed Naturally at Home?\n- 2 Natural Tattoo Removal Methods\n- 3 Mythbusting Natural Tattoo Removal\n- 4 Our Final Thoughts\nCan Tattoos Be Removed Naturally at Home?\nTattoo removal involves breaking down the ink particles in the skin so that they can be absorbed and eliminated by the body. But can it be done at home?\nThe short answer is no.\nTattoo ink particles are embedded deep within the skin’s layers. There are many at-home remedies on the internet, such as lemon juice, salt, sandpaper, or aloe vera, that are said to help fade tattoos, but they are ineffective and could potentially harm the skin or make the tattoo more difficult to professionally remove. In addiction, these at-home processes only address the upper layer of skin called the epidermis, but tattoos are embedded in the dermis.\nTattoos are designed to be permanent, which means they’re resistant to at-home attempts to remove them. These widely circulated practices can actually cause scarring, infection, or other complications.\nWhile expensive, laser technology is the most advanced way to remove your tattoo safely. It’s reliable, safe, and successful. It targets the ink specifically and doesn’t damage your skin or tissue in the process. To learn more about safe tattoo removal, check out our articles.\nNatural Tattoo Removal Methods\nWhile there are some popular natural tattoo removal methods floating around on the internet, we don’t recommend you try to remove tattoos naturally. Not only will it not address the deeper layers of your skin where your tattoo ink particles rest, but it could also, in fact, cause damage to your skin which prevents more professional tattoo removal processes from working successfully.\n- Lemon Juice: Lemon juice is said to have natural bleaching properties. Some people believe that lemon juice can lighten your skin pigments and therefore lighten your tattoo over time. While lemon juice can help exfoliate the skin, it’s not likely to remove any of your tattoo.\nIn fact, lemon juice can make the skin more sensitive to sunlight, which can increase your risk of sunburn.\n- Salt Scrub: This abrasive material is great for exfoliating your tattoo and for removing dead skin cells but will not remove your permanent tattoo. If you scrub too hard in an attempt to remove your tattoo ink, you could cause irritation, scarring, or infection. Learn more about removing tattoos with salt.\n- Aloe Vera: Aloe vera is actually a very soothing solution that makes your tattoo last longer rather than a natural removal method. Aloe vera hydrates the skin and, in turn, lengthens the life of your tattoo.\n- Sand Powder or Paper: Attempting to remove your tattoo with abrasive materials such as sandpaper or sand powder paste can cause severe damage to your skin, increasing your risk of scarring and infection. The tattoo ink is embedded deep within the layers of skin and is designed to be resistant to surface-level removal processes. Using this method could make it far more difficult for successful professional removal.\n- Topical Removal Creams: There are many tattoo removal creams advertised online, but not only are they expensive and often inefficient, but they also many not FDA-approved and could cause allergic reactions. If you want to consider tattoo removal cream, have a look at our guide to the best on the market.\nMythbusting Natural Tattoo Removal\nThere are many misconceptions surrounding natural tattoo removal methods, and we are here to clear the air about them all.\nMyth #1: Natural Tattoo Removal Methods are Completely Safe\nThis is false because many natural tattoo removal methods can actually be quite harmful to your skin. Natural doesn’t always mean right for you. These processes could cause damage to the surface of your skin, leading to scarring, infection, or other complications. Additionally, many natural remedies may make your skin more sensitive to sunlight, putting you at risk for sunburns.\nMyth #2: Natural Tattoo Removal Methods are Just as Effective As Laser Tattoo Removal\nNatural removal methods cannot penetrate deep enough into the skin to break down tattoo ink, which is designed to be resistant to the body’s natural process of eliminating foreign substances. Laser tattoo, on the other hand, uses high-powered machinery to specifically target the pigment deep within the dermis, breaking the ink down into small enough fragments so that the body can naturally process and eliminate them.\nMyth #3: Natural Tattoo Removal Methods Work for Everyone\nMore often than not, they don’t work for anyone! In fact, many of the natural methods advertised online could cause severe allergic reactions for many people.\nMyth #4: Natural Tattoo Removal Methods are Affordable\nWhile natural tattoo removal methods may seem more affordable at first, these methods require multiple treatments over an extended period of time to have even a small chance of being effective. This means that the cost eventually adds up - and so does your disappointment when you fail to see results. With the damage that these natural methods could potentially cause to your skin, you may also need to invest in medical care; another unexpected expense that you put yourself at risk for when using natural tattoo removal options.\nOur Final Thoughts\nIt is not recommended to remove a tattoo at home as it can be dangerous and potentially cause harm to your skin. Attempting to remove a tattoo at home can lead to scarring, infection, and other complications, as home remedies like using lemon juice, salt, or sandpaper can damage the skin and make the tattoo even harder to remove.\nIt is recommended that you seek the assistance of a professional tattoo removal clinic or dermatologist who can evaluate your tattoo and provide safe and effective options for removal. These options may include laser treatment, dermabrasion, or surgical removal, depending on the size, color, and location of the tattoo.\nFor more information about laser tattoo removal, check out our article, How to Remove Tattoos: How It Works, Process, Healing & Scarring.', 'Laser Tattoo Removal\nWhat is Laser Tattoo Removal?\nWe use the MedLite® C6 Q-switched Nd:YAG laser for our Laser Tattoo removal. The laser itself is a device, which is designed to produce a specific wavelength of light. It is by using specific wavelengths of light that are passed over the tattoo, into the skin and absorbed by the ink itself that gives us the desired lightening effect. The rapid absorption of light energy by the ink causes it to break into tiny particles, which is then removed by the body’s normal filtering systems.\nWith multiple wavelengths to choose from, your AFPLS Technician can use the optimum wavelength that passes into the skin that is absorbed by the specific colour of ink, customising the treatment for each patient and each individual tattoo.\nWill I be left with a scar?\nIdeally, laser tattoo removal will remove all evidence of the tattoo colour and image, leaving the skin as it was before the tattoo was placed, i.e. “normal”, with no mark whatsoever. While we always strive for this outcome, and use the most advanced laser technology available to achieve an optimal result, this may not occur.\nAfter the tattoo is placed some mild scar can form in the skin. Also, any previous attempts of tattoo removal can leave marking in the skin, and years of sun exposure on the tattooed skin may alter the skin’s appearance. These changes may not be apparent while the colour of the tattoo is still present, but once the tattoo colour is gone, some of these minor variations in texture, colour, scaliness, ect, can become more noticeable and make the tattooed area appear different than the surrounding skin. In general, after the final laser procedure is completed, the treated skin usually looks much like the normal skin surrounding the spot. However, occasionally use of laser to remove the tattoo will leave a mark.\nWith each treatment some redness, bleeding, bruising, swelling, blistering and crusting is common, but wound infection is rare. Irregular pigmentation and mild texture changes can occur as a result of laser treatment, but these changes usually return back to “normal” over a few months. Rarely these changes are permanent. True scarring with the newest, most advanced lasers are rare.\nHow many laser tattoo removal sessions will I need?\nWe can only ESTIMATE the number of treatments it will take to completely remove the tattoo. This is because:\n- The wide variation of inks and pigmented materials used for tattooing.\n- The inks themselves are made up of a combination of different chemical substances.\n- The amount of ink placed in the tattoo, as well as the depth of pigment placement varies from one tattoo to another.\n- The ability of your white blood cells to “break down and remove” different pigment particles varies.\n- Your healing response.\nAs a result of these many issues we cannot be absolutely certain how your particular tattoo will respond. Most “amateur or home-made” tattoos will generally require about 5-7 laser treatments. “Professional” tattoos and those done with harder inks, tend to be more variable in response, and average about 10-12+ laser treatments. Some stubborn tattoo inks may require even more treatments 15-20+. Some cosmetic tattoos on the lips and white or flesh coloured tattoos can undergo a dark brown or black colour change that may or may not fade in a couple of weeks.\nWhile the most noticeable lightening of the tattoo usually occurs after the first treatment, continued lightening occurs with each subsequent treatment, although it may not be as easily seen. Before and after photos have been taken of many patients demonstrating that every patient will fade to some degree after every treatment.\nBefore starting each treatment session, we often compare the tattoo with the original photos (taken before the first treatment) to assess the amount of colour that has already been removed. With this information we can evaluate your progress and discuss the need for continued treatment. The decision to continue is always made jointly and made with a clear understanding of potential outcomes.\nHow long between laser tattoo removal treatments?\nThrough experience we have found that waiting longer in between treatments is more beneficial as the tattoo will continue to fade with time. It is preferable that you wait at least 8 – 12 weeks before your next laser tattoo removal session.\nThe laser treatment may create a superficial burn wound. There is immediate whitening (frosting) of the treated area, which lasts for several minutes. Patients may then develop blisters, crusts or scabs within 24-72 hours, which may last for 1-2 weeks. Some patients may experience bruising or swelling. Do not pick at the scab or allow the skin to become abraded, as this may result in infection and scarring. The treated area may be pink or pale after the scab separates. Loss of skin pigment in the treated area (usually temporary) is common. Complete healing is usually seen by 6-8 weeks.\nWe advise patients to protect the wound as you would any other minor burn for a few days. A dressing may help to protect the area and minimize further injury. Cold compresses and over the counter analgesics like Panadol and Neurofen, can be taken for minor discomfort during the first few days. Most patients are able to resume normal activity and work the same day. Showering and bathing are usually not a problem, but sun exposure should be avoided in the treated area. It is very important to follow our instructions carefully especially regarding the use of any medications or specific wound care that we may recommend.\nFor the next 5-7 days following treatment:\n- Do not exfoliate or pick at skin\n- Do not use products containing AHA’s, BHA’s, Vitamin A (Retinol), astringents or exfoliating granules\n- Do not expose the skin to sun\n- MUST apply a sunscreen daily\n- Apply ice as required (skin is best to remain cool during this time)\n- Avoid vigorous exercise, saunas, chlorinated and salt water pools, steam baths and hot showers for 48 hours following your treatment\n- Avoid waxing, IPL, Laser or depilatory creams for 2 weeks following your treatment\n- Avoid applying tanning products for 2 weeks following your treatment\n- Apply a mild antiseptic or burns cream 2 x day\nA quote of treatment price will be given to you at the time of consultation and is relevant to size of the tattoo. Costs given are for single sessions and include the cost of topical anesthesia, treatment and dressings. Prices start from $250.\nIf I am interested, what now?\nIf our Dermal Therapists decide that you are a suitable candidate for laser tattoo removal and you want to go ahead with the procedure, a booking for the tattoo removal surgery is scheduled. The usual waiting period is approximately three to four weeks.']	['<urn:uuid:191185fc-fdd2-42a4-a2ab-95e665968f9b>', '<urn:uuid:2dd28533-e9b6-44db-8737-35860ec87dd3>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	9	65	2188
42	what are common foods causing allergic reactions	Common food reactions are caused by tree nuts, pine nuts, peanuts, fruit, eggs, milk products, and shellfish.	['Anaphylaxis Causes Whole-Body Reactions\n, Staff writer, originally distributed by Internet Broadcasting\nSpring and summer can bring on sneezing and scratching for a lot of allergy sufferers. But when symptoms get worse, an allergic reaction can quickly turn into anaphylaxis, a potentially fatal situation.\n“It is an allergic reaction that can cause itching, hives, nasal and eye symptoms, shortness of breath, coughing, nausea, light-headedness, passing out, and in the worst case scenario is death,” said Dr. Jill Poole, an internist at the University of Nebraska Medical Center. Insect stings are a leading cause of anaphylaxis, along with food and latex allergies.\nDoctors See More Anaphylaxis\nPoole said anaphylaxis is a hot topic among doctors because the incidence of reported allergy sufferers is rising in the U.S. Existing allergies or asthma can be a risk factor for critical or fatal reactions to allergens. Poole said most people who are allergic to insects probably won’t know until they’re at least 12 years old, and most don’t learn until they are in their 20s or 30s.\nInsect stings cause about 500,000 allergy – and anaphylaxis-related emergency visits and at least 50 deaths each year, according to a news release from the American College of Emergency Physicians. About 30,000 trips to the nation’s emergency rooms each year are blamed on food reactions.\n“Some of the more common food reactions are tree nuts, pine nuts (and) peanuts – people have allergies to fruit, eggs, milk products, shellfish,” said Dr. Rita Cydulka, a professor in the Department of Emergency Medicine at Case Western Reserve University School of Medicine.\nFood allergies tend to be diagnosed very early in life, the doctors said. In fact, there are medical specialties dedicated to helping the parents of infants and toddlers to identify food allergies and design menus.\nCydulka said summer is prime time for food allergies, too. More people eat at food festivals in the summer and can’t be sure what’s in their meals. She recounted a story of one of her own patients.\n“Probably one of the worst stories of my career was July 4. I was living in Chicago, working ER. They brought in a 23-year-old man who had a peanut allergy and went to Thai booth. He asked if they cooked with nuts. They said no, and he ate the food. He had a severe anaphylactic reaction, and rather than go to the medical tent at the festival, he started walking toward the hospital. He died of cardiac arrest on the way,” she said.\nWhy Are There More Problems?\nWhy are more people developing allergic and anaphylactic reactions? Poole said about 0.5 percent of children have problems with venom allergies, and 3 to 5 percent of adults will have bad reactions to stings. Poole said the jury is still out on causes, but one theory is known as the “hygiene hypothesis” – that Western countries are at a point of development where the body’s natural defenses aren’t used very much.\n“Clean water, antibiotics, no parasites – our immune system is built to fight off infection and bad things, so when we don’t have anything for the body to do, it tends to react to cats or pollen or food,” Poole said. “It’s a robust immune response.”\nPoole said other theories include more pets in people’s homes and more pollution in the air.\nAlso, people with asthma and eczema tend to be more susceptible. People whose parents have allergies tend to be more likely to develop them, too.\nSo how can you protect yourself, even if you don’t know you’re allergic?\n“Protection is avoidance. That’s the key,” Cydulka said. “If you know you have an insect allergy, if you know you’re sensitive, avoid places where there are lots of bees. Avoid wearing lotions that are scented. It’s not advisable to be outside eating during bee season. One of things we see is bees flying into a can of pop and then people drink it and they sting them.”\nAlso, sufferers are encouraged to carry epinephrine and Benadryl to administer to themselves in the event of a whole-body reaction.\nYellow jackets, the most aggressive of the stinging insects, tend to live around mounds or stumps, so be careful where you’re gardening or walking. Wear shoes outdoors so you don’t accidentally step on a stinging insect, and exterminate if you have an infestation.\nIf hives break out, breath gets short you have another extreme response, don’t wait to get help.\n“If you start wheezing or other symptoms – whole-body reactions – you want to get to emergency care and not just take a Benadryl and wait it out,” Poole said.\n“The data are pretty clear that one thing associated with death is lay treatment,” Cydulka said. “We actively encourage people to seek treatment. But treat yourself first, or be ready to.”']	['<urn:uuid:46daca7e-8656-48be-811d-0ae58202062f>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	7	17	801
43	natural vs synthetic fiber durability compare	Natural and synthetic fibers have different durability characteristics. Natural fibers like cotton and wool typically show significant loss over time - studies indicate about 80% of fibers are lost within four hours of transfer, with only 5-10% remaining after 24 hours. Synthetic materials like carbon fiber composites demonstrate exceptional stability of form, especially when heat treated, and can maintain their properties at temperatures up to 155°C. Additionally, synthetic fibers can be manufactured with specific characteristics for durability, while natural fibers are limited by their inherent biological properties.	"['580 likes | 778 Vues\nFiber Analysis. Physical Aspects of Forensic Science. . Textile Fiber Defined. Defined as the smallest part of a textile material Many objects in our environment (clothing, ropes, rugs, blankets, etc.) are composed of yarns made of textile fibers. Animal (hairs) Wool, cashmere, silk\nE N D\nFiber Analysis Physical Aspects of Forensic Science .\nTextile Fiber Defined • Defined as the smallest part of a textile material • Many objects in our environment (clothing, ropes, rugs, blankets, etc.) are composed of yarns made of textile fibers\nAnimal (hairs) Wool, cashmere, silk Vegetable Cotton, kapok, linen Mineral Asbestos Manmade Acetate, rayon, nylon, acrylic, polyester, and olefin Textile Fiber Categories\nFibers Fibers are very useful as trace evidence: • Vary widely in class characteristics color, shape, chemical composition, etc. • Easily transferred from one source to another (carpets, clothes, etc.) • Significant persistence (won’t degrade)\nImportance of Fiber Evidence • Perpetrators of crimes are not always aware or able to control the fibers they have left behind or picked up\nImportance of Fiber Evidence • In contrast to hair, fibers offer much greater evidential value because they incorporate numerous variables • Number of fibers in each strand, diameter of strands and fibers, direction and number of twists, type of weave, and dye content, as well as foreign material embedded or adherent to the fiber\nTrace > Fibers How are fibers used as evidence? • As with other trace evidence, fibers can be transferred to/from a person or objects linking them to one another.\nTrace > Fibers How long do fibers persist? Most fiber evidence is lost (fall off) a short time after the transfer occurs. The fibers that do remain will be persistent.\nTrace > Fibers Fibers can be classified into three main categories: • Natural (animal, plant, mineral) • Manufactured • Synthetic\nTrace > Fibers > Natural Natural Fibers: • Found in nature • Can be artificially colored or treated Cotton Wool Hemp\nAnimal Fibers Wool - Hairs from sheep • Most common of animal fibers • Hairs are spun to form thread Silk - comes from silkworm • Spun as double filament (separated before use) • Because of length, doesn’t shed easily Other Hairs from Animals\nAnimal Fibers • Woolen fibers occupy less than 1% of all fibers used in production of textile materials • Wool has a microscopic structure that is characteristic of hair • The cuticle (outer covering) is made of flattened cells, commonly called scales\nAnimal Fibers (continued) • The scales resemble shingles of a roof and are one of the most useful features to ID an unknown textile fiber as wool • Other animal hairs are not as frequently encountered so they can be quite valuable if they occur as evidence • Include goat (cashmere, mohair), llama (alpaca, vicuna, guanaco), and camel hair\nAnimal Fibers • Cattle and rabbit hair are found in the manufacture of certain kinds of felts • Felts are made from water suspensions of randomly arranged fibers. When the fibers settle out, the water is removed and the mass of fibers is pressed to form the felt • Some modern felts are no longer made exclusively from hairs but are mixtures with other fibers\nAnimal Fibers • Silk places a distant second to wool in occurrence, and its use has decreased since development of artificial fibers • Silk fibers are not very often encountered in crime investigations, probably because silk fabrics do not shed very easily\nTrace > Fibers > Natural Plant Fibers Cotton - seed hairs of cotton plant • by far most common fiber(find almost everywhere) Under microscope, fibers resemble twisted ribbon\nVegetable Fibers • Only cotton is found in any large extent in items of clothing • Approximately 24% of total US textile fiber production was cotton in 1979 • Other plant fibers, such as jute and sisal, are seen in various types of cordage and baggings\nVegetable Fibers • Cotton fibers have a distinctive flattened, twisted microscopic appearance, which is quite characteristic • The fibers resemble a twisted ribbon • In mercerizing process, fibers are treated with alkali, making them swell up and become more rounded and less twisted in appearance. • This process results in improved texture and feel, but the fibers are still recognizable as cotton under the microscope\nVegetable Fibers • Undyed cotton fibers are so common they have little value as physical evidence • Almost any surface or dust sample will be found to contain white cotton fibers Household Dust\nTrace > Fibers > Natural Other Plant Fibers: Linen - stem fiber from flax plant Kapok - from seed hairs of kapok plant Other fibers - Manila, hemp, sisal, jute\nMineral Fibers Asbestos - crystalline material • Used to be used for insulation • Fractures into thin rods that can get into your lungs; can kill you • Not used much anymore\nFilament vs. Staple Filament: Long continuous fiber (like silk) Staple: Filament is cut into smaller pieces; staples are spun together to form thread (like cotton)\nManadeFibers • Represent approximately 75% of total textile fiber production in US • Can be defined as a fiber of a particular chemical composition that has been manufactured into a particular shape and size, contains a certain amount of various additives, and has been processed in a particular way\nManmade Fibers • Within the 6 most seen of the 21 generic classifications established by the US Federal Trade Commission, there are well over a 1,000 different fiber types • Therefore, numerous fiber types can be present in the composition of textile materials • This is true before even considering differences in color\nManufactured Fibers Regenerated Fibers • Cellulose is dissolved, then resolidified to form the polymer fiber • Can occur in filament or staple form Example:Rayon\nSynthetic Fibers • Man made • Can also be filament or staple Examples: Nylon and Polyester\nSynthetic Fibers Acrylics • More common as evidence • Usually in staple form • Staples spun together, similar to wool\nTrace > Fibers > Analysis Begin by identifying and comparing class characteristics for unknown sample (evidence) and known sample. Known Unknown\nTrace > Fibers > Analysis Fibers from rug in a van. Fibers found on victim.\nTrace > Fibers > Analysis Class characteristics Color: microscopic examination Size: length and width can be measured Shape: cross section is viewed\nClass characteristics Refractive Index – n. The ratio of the speed of light in air or in a vacuum to the speed of light in another medium. Other microscopic properties (PLM)\nClass characteristics Chemical Composition: determined by advanced instrumentation\nThreads, Yarn, Rope, Cordage Smallest component is fibers (staple) twisted together to form thread or is a filament. This thread can then be twisted with other threads to form a thicker thread (string, etc.) This thicker cord can then be twisted with other thicker cords, etc.\nThreads, Yarn, Rope, Cordage Small cords or fibers twisted together to form larger cords • At each step, the number of cords can be counted. • At each step, the twist direction is either “S” or “Z”\nImportant to Remember: • It is important to collect evidence from both complainants and suspects as soon as possible • Studies show that some 80% of fibers can be expected to be lost in four hours, with just 5-10% remaining at the end of 24 hours\nMethods of Examination • In the recent past, the ID and comparison of fibers were at a relatively simple level which relied heavily on microscopy\nGeneric class Polymer composition Finish--bright/dull Cross-sectional shape Melting point Refractive Indices Birefringence Color Fluorescence Absorption spectrum Dye class Dye Components From Less than 1 cm of a 20 mm Diameter Fiber It is Possible to Determine:\nMicroscopy • Microscopic examination provides the quickest, most accurate, and least destructive means of determining the microscopic characteristics and polymer type of textile fibers.\nMicroscopic View Acetate Dacron\nStereomicroscope • Should be used first to examine fibers. • Physical features such as crimp, length, color, relative diameter, luster, apparent cross section, damage, and adhering debris should be noted. • Fibers are then tentatively classified into broad groups such as synthetic, natural, or inorganic.\nComparison Microscope • If all of the characteristics are the same under the stereoscope, then the comparison microscope is used. • A point-by-point and side-by-side comparison provides the most discriminating method of determining if two or more fibers are consistent with originating from the same source.\nComparison Microscopy • Side-by-side Comparison • Bright Field Adjustment\nComparison Microscopy • Characterization • Fluorescence • Chemical factors • Environmental factors\nComparison Microscope • Comparisons should be made under the same illumination conditions at the same magnifications. • This requires color balancing the light sources. • A balanced neutral background color is optimal.\nFluorescence Microscopy • The sample is illuminated by ultraviolet light, causing some phases to fluoresce so they can be observed, counted, sized and mapped. Kevlar fibers in complex composite material strongly fluoresce.\nPolarized Light Microscope • Perhaps the most versatile of all microscopes; allows the analyst to actually see and manipulate the sample of interest. • Refractive indices, birefringence, and dispersion can all be quantitatively determined.\nMicrospectrophotometry • To the unaided eye, 2 dyes may be identical. • Using a grating spectrometer, light absorbed by or reflected from a sample is separated into its component wavelengths, and intensity at each wavelength plotted.\nMicrospectrophotometry • Microscope linked to a Spectrophotometer • IR Absorption spectrum • UV/VIS Absorption Spectrum\nMicrospectrophotometry • IR spectography identifies generic subtypes indistinguishable by microscopic exam • Use of IR microscopes coupled with Fourier transform infrared (FT-IR) spectrometers has greatly simplified the IR analysis of single fibers', 'For exceptional accuracy, finish & performance, choose Carbon Fiber\nProto-pasta Carbon Fiber Composite HTPLA is a combination of milled carbon fibers and high-performance PLA. Resulting 3D printed prototypes and end-use parts are characterized by exceptionally stability of form and potential use up to 155 deg C (310 deg F) when heat treated.\nAdaptable to most PLA-compatible printers. Heated bed recommended for process ease, quality, and reliability, but not required. Printer should allow 3rd party filament, parameter adjustment, and nozzle replacement. Specialized machine adaptation and maintenance may be required for Proto-pasta materials particularly in continued use of abrasive materials.\nPlease consider all information below before purchase and use. More on getting started. More about our release of Carbon Fiber HTPLA.\n- Base material: Heat treatable PLA w/ high temp resistance\n- Characteristics: low odor, non-toxic, renewably-sourced\n- Molecular structure: Amorphous or partially crystalline\n- Amorphous as printed, part crystalline when heat treated\n- Melting resets crystalline structure to amorphous state\n- Additives: 10% by weight high-purity, milled carbon fiber\n- Max particle size: 0.15 mm (may limit resolution)\n- Density: approx. 1.3 g/cc\n- Length: approx. 360 m/kg (1.75 mm) & 136 m/kg (2.85 mm)\n- Min bend diameter: 40 mm (1.75 mm) & 100 mm (2.85 mm)\n- Glass transition (Tg) onset: approx. 60 deg C (140 deg F)\n- Melt point (Tm) onset: approx. 155 deg C (310 deg F)\n- Max use: Tg for amorphous, Tm for crystalline\n- Use limit is geometry, load & condition dependent\n- Material Safety Data Sheets (MSDS)\n- Hold filament end when unwrapping spool to avoid looping\n- Protect eyes when handling filament, particularly 2.85 mm\n- Coiled filament stores energy & may try to unwind\n- Do not bend tighter than min bend diameter (in tech specs)\n- Over-bending filament can cause breakage & projectile\n- When done printing, secure filament end to avoid looping\n- Store in cool, dry place away from UV light for peak performance\nProp 65 Warning! May cause cancer or reproductive harm.\nHowever, in an effort put this statement in perspective, consider our blog on this subject to better-understand actual risks.\nRoHS compliant - does not contain Cadmium (Cd), Lead (Pb), Mercury (Hg), Hexavalent Chromium: (Cr VI), Polybrominated Biphenyls (PBB), Polybrominated Diphenyl Ethers (PBDE), Bis(2-Ethylhexyl) phthalate (DEHP), Benzyl butyl phthalate (BBP), Dibutyl phthalate (DBP), or Diisobutyl phthalate (DIBP)\nRegarding food contact - though base resin may be safe for food contact, our process & additional ingredients may not be. Thus our materials are not certified for food contact even if the risk is low. Please consider additional coatings, treatments & testing before pursuing extended food contact or certification.\nRegarding skin contact - Not a known skin irritant, however, avoid recommending prolonged skin exposure without further testing.\n- Available in 1.75 & 2.85 (3.00) mm diameters\n- 50 g loose coil, 500 g on cardboard spool & 3 kg on plastic spool\n- Cardboard spool weight: up to 100 g\n- Cardboard spool dims: 20 cm dia x 6 cm wide w/ 5 cm opening\n- Carefully remove cardboard spool sides for Masterspool use\n- Loose coils also Masterspool compatible (instructions for use)\n- More details about Masterspool including download\n- Recycle cardboard spools locally\n- 3 kg spools weight: up to 1 kg weight\n- 3 kg spool dims: 30 cm dia x 10 cm wide w/ 5 cm opening\n- Return plastic spools for re-use\nSome machines may require specific considerations for filament placement, path, adjustments, settings, or other preparation & maintenance.\n- Spool should unwind with minimal resistance\n- Mount filament above machine or other unobstructed position\n- Filament path must not bend tighter than min bend diameter\n- Filament should remain clean, dry & dust-free\n- Check weight before printing to avoid material run out\n- Be careful not to place printer too cool or warm environment\n- Clean print surface with alcohol or water\n- Apply & reapply appropriate adhesion aid as required\n- Carefully control first layer gap for adhesion without jamming\n- Limit build rate to balance melting, cooling & movement limits\n- Adjust layer fan to balance part & nozzle cooling\n- Isolate/insulate heater block from layer fan for consistent heating\n- Maintain & replace nozzle & other components when worn\nAbrasive materials like Carbon Fiber and Metal Composites may cause premature wear of in line components such as bowden tubes, drive gears, nozzles, and other items in the filament travel path. Serviceable hardware including replaceable nozzles suggested. Wear resistant nozzles are recommended for extended use. Nozzles wear most quickly with flattening of the tip which affects nozzle diameter & distance to build plate. Inconsistent extrusion, inaccuracy & process instability. Extrusion width & first layer distance adjustments and/or replacement of nozzle. For more on nozzle replacement consider this blog demonstrating nozzle replacement & adjustments on a Prusa MK3. Reduce nozzle wear by minimizing over-extrusion & infill.\nProduct label suggest temperatures as a guideline based on typical nozzle set points. Appropriate settings can vary widely & in given good conditions, a wide range of temperatures can yield positive results. With relatively low print rates on hardware without hangups, HTPLA prints well at the low side of the recommended range. With high print rates on machines with hardware hangups, higher than labeled temperatures may be required for consistent extrusion. In some cases, oiling filament makes the difference between success & failure.\nOne specific problematic example is the Prusa MK3 which, by design, has a Prusa-specific heat break with an internal ledge that material can get caught on. To reduce need for nuance & risk of jamming, users should either replace the Prusa-specific heat break with a standard e3d v6 one, oil filament to help it slip past the Prusa-specific ledge, or print at an unusually high temperature. The trade off with high temperature as a solution is you should also match that with a high volume flow rate. Sounds great, right? It\'s okay except for the loss of detail when having to slow down for small part, fine feature, or high resolution printing.\nPrusa MK3-specific, Carbon Fiber HTPLA process recommendations:\n- 1st layer temp to overcome jamming on Prusa MK3: 255 deg C\n- Min temp to avoid jamming @ 9 cubic mm/s: approx. 240 deg C\n- Max recommended volume flow @ 240 deg C: 9 cubic mm/s\n- Min recommended volume flow @ 240 deg C: 1.5 cubic mm/s\nVolume flow = extrusion width x layer height x speed in mm.\nFor example, 0.5 mm extrusion width & 0.2 mm layer height for speed 20-90 mm/s.\nPoorly cooled cold sides of all metal hotends can yield a similar result & benefit from similar fixes to the Prusa MK3. Aggressive layer fans not isolated from heat blocks and/or nozzles can make for a jammy combination as well. Finding the balance between enough cooling fan when printing fast & a high enough nozzle set-point can be challenging. More isolated hotends with PTFE liners can allow slower printing with lower set points for more detail with less aggressive layer fan settings. Also, insulating your heater block and/or nozzle with a sock can help avoid unwanted layer fan cooling. Rapid changes in speed or print rate should also be avoided whenever possible.\nFor more on the subject of printing, consider our getting started guide.\nHTPLA is a semi-crystalline grade of PLA optimized for heat treating (also known as annealing or crystallizing) for higher temperature use. Without heat treating, ""as printed"" amorphous PLA loses significant stiffness (and the thus the ability to retain form) as the material approaches it\'s relatively low glass transition temperature. Heat treating creates a more crystalline molecular structure for maintaining stiffness to near melting, thus extending the useful range of HTPLA, but crystallization also creates shrinkage. HTPLA parts should be scaled in slicer to compensate for shrinkage when heat treating.\n- Typical heat treat temp: 95-110 deg C (200-230 deg F)\n- Typical heat treat time: 10+ minutes\nA large range of temperatures & times can yield acceptable results. With translucent grades and thin wall parts like a single wall vase, you can see a visual change from transparent to opaque begin in as little as 3 minutes with a full transition to opaque in 7 minutes. Parts with more mass will take more time. What\'s important is the core temp and time to ensure a comprehensive change in material structure to crystalline throughout the part.\n- Typical change in heat treating: -0.6% x/y, +1% z\n- Slicer scale in heat treating:1.006 or 100.6% x/y, 0.99 or 99% z\nHere\'s a demonstration of measuring shrinkage, determining change, and applying compensation in printing. Here\'s additional demonstration of application of scale, heat treating, and validation of form.\nAdditional post processes might include sanding or painting. The addition of Carbon Fiber can lend well to ease of sanding and adhesion of coatings like paint, however, there are also additional safety considerations when generating dust through sanding and fumes through coating. Please seek safe practices with appropriate personal protective equipment (PPE) and ventilation.\nExamples & Uses\nPrusa Extruder Assembly Housing\nHardinger Fiddle Musical Instrument\nFilament Testing Grips']"	['<urn:uuid:ab73ced1-8b91-4399-b8e8-1a7c62885bb9>', '<urn:uuid:0a72a8f7-d897-4570-bf0b-7a65e24dc76e>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T15:30:12.136021	6	87	3131
44	benefits healthy microorganisms soil vs ocean ecosystem services compare	Both soil and ocean microorganisms provide essential ecosystem services. Healthy soil microorganisms deliver critical services worth trillions of dollars annually, including climate regulation, water filtration, drought and flood mitigation, and soil erosion prevention. Similarly, phytoplankton in oceans provide crucial ecosystem services by producing about 50% of Earth's oxygen, supporting the marine food web, and playing a key role in the global carbon cycle through carbon dioxide absorption.	['The Many Benefits of Healthy Soils\nYou may have heard or read about CCF’s renewed commitment to improving the health of our soils. There are so many reasons for this.\nIt is often said that soil is the foundation of life. Healthy soil that is full of life (fungal and microbial) has the ability to draw carbon out of the atmosphere through photosynthesis, then store and use it to support the creatures living in the soil. This helps get the excess carbon out of the atmosphere, where it is wreaking havoc on our climate, and puts it back into the earth where it does a lot of good.\nDid you know that healthy soil also produces healthier food? People started paying a lot more attention to the microbes that perform functions critical to our health after results from the National Institute of Health’s five-year Human Microbiome Project were released in 2012. This Project helped the medical community map and understand which microorganisms are vital to human health.\nIn the last several decades, we have unwittingly destroyed many of these vital microbes in the human gut through the overuse of antibiotics and the consumption of highly processed foods. In the same way, industrial agriculture has devastated soil microbiota essential to plant health through the excessive use of toxic chemicals and the overworking of the soil (plowing, disking, harrowing). These destructive products and strategies inhibit the natural functioning of soils and plants.\nThrough the process of photosynthesis, plants produce sugars – some of which the plants use and some that are shared with bacteria and fungi as carbon-rich sugary root exudates. These sugars are then used by the soil microorganisms to access minerals that they digest and make available to the plant roots.\nThese bacteria and fungi help produce healthy plants that contribute to a healthier microbiome in humans, who all depend on plants for nutrition and health. The challenge for farmers is to figure out ways to bring back the soil life and to find the right balance of microbes, fungi and minerals to grow the healthiest and most nutrient dense foods possible.\nA lot of exciting research is going on in labs and on farms to identify ways to improve soil life. An abundance of soil microorganisms not only feeds the plants; it also protects plants against pathogens and other threats (insects, weeds, etc.).\nThe living soil is the Earth’s most valuable ecosystem. It provides services critical to human survival and worth trillions of dollars per year – climate regulation, water filtration, drought and flood mitigation, and soil erosion prevention. These are referred to as “ecosystem services”, a term which is now also being used by those who study the human microbiome to describe the critical functions that microorganisms play in human health.\nWe are experimenting with different cover crop mixes, as well as compost additions and additives of biodynamic nutrients, among other things. Our goal is to increase the microbial life in the soil and at the same time suppress weeds to eliminate the need for tillage, as well as to provide more nutrient dense products to enhance the health of our community.\nStay tuned for more on this topic and for updates on how we at CCF are faring with our efforts to increase soil health. There is so much potential.', 'NASA wants to identify phytoplankton species from space. Here’s why (Image Credit: Phys.org)\nThey’re small, but they’re mighty. From producing oxygen we breathe to soaking up carbon we emit to feeding fish we eat, tiny phytoplankton are a crucial part of ocean ecosystems and essential to life as we know it on Earth. To give us a new view of these extraordinary aquatic organisms, NASA is launching a satellite in early 2024.\nInstruments on the PACE (short for Plankton, Aerosol, Cloud, and ocean Ecosystem) satellite will peer down at the ocean and collect data on the colors of light reflecting off it, telling us where different types of phytoplankton are thriving.\nThe Ocean Color Instrument on PACE will be able to observe more than 100 different wavelengths, and is the first scientific satellite to do so daily on a global scale. This “hyperspectral” instrument will make it possible to identify phytoplankton by species for the first time from space.\nPhytoplankton and photosynthesis\nPhytoplankton are tiny organisms that float on the surface of the ocean and other water bodies. Like land-based plants, phytoplankton use photosynthesis to absorb sunlight and carbon dioxide and generate oxygen and carbohydrates, which are carbon-filled sugars. These sugars make phytoplankton the center of the ocean food web: They nourish larger animals—from zooplankton to shellfish to finfish—that are then eaten by even larger fish and marine mammals. The creation of those sugars from sunlight is called primary production.\nEven though phytoplankton make up less than 1% of the total biomass on Earth that can photosynthesize, they deliver about 45% of global primary production. Without phytoplankton, most oceanic food webs would collapse, which would be devastating for both marine life and humans who rely on fish for food.\nThe tiny organisms provide more than just nutrients. Through photosynthesis, phytoplankton create oxygen that is released into the ocean and atmosphere. In fact, since they began photosynthesizing over 3 billion years ago—more than two billion years before land plants and trees—phytoplankton have made about 50% of all the oxygen that has been produced on Earth.\nPhotosynthesis gives them a key role in the global carbon cycle as well, as they soak up carbon dioxide from the atmosphere. What phytoplankton do with that carbon depends on the species.\n“Like plants on land, phytoplankton are highly diverse,” said Ivona Cetinić, a biological oceanographer in the Ocean Ecology Lab at NASA’s Goddard Space Flight Center in Greenbelt, Maryland. Each of these diverse species has different characteristics that allow them to take on different jobs in Earth’s carbon systems, she said.\nPhytoplankton like Emiliana huxleyi incorporate carbon into their shell-like outer coating. When they die, the shells sink and sequester the carbon in the ocean depths. Other phytoplankton species fit a certain niche for picky eaters like oysters, which only eat phytoplankton of a certain size. Still other species of phytoplankton may capture carbon through photosynthesis, where it then remains on the ocean surface until the organisms decompose, releasing the carbon back into the atmosphere as carbon dioxide.\n“I hope that PACE, once it gives us a view of ocean phytoplankton diversity, can tell us so much more about global carbon flow in oceans, now and in the future,” Cetinić said.\nPhytoplankton in the cold\nEven in colder waters at higher latitudes, phytoplankton are crucial to ocean life. In polar regions, phytoplankton blooms—when the organisms grow and multiply in vast numbers visible from space—can follow the cycle of sea ice melt.\nWhen sea ice cover recedes, sunlight can reach the surface of the ocean and the phytoplankton that float on it, allowing them to photosynthesize and thrive after a long period of being covered. This produces fuel for other species. Polar species from clams and krill all the way up to walruses and whales rely on these timely blooms for their food sources.\n“An alteration of the timing of the blooms impacts the entire ecosystem,” said Aimee Neeley, a biological oceanographer at NASA Goddard.\nAs the timing and extent of sea ice retreat changes in a warming climate, PACE will be able to track changes to the timing of blooms, providing insights into the wider impacts to the ecosystem.\nIdentifying harmful phytoplankton\nNot all phytoplankton are beneficial for ecosystems. Some species can produce toxins that are dangerous for humans or other marine species. These harmful algal blooms can disrupt ecosystems as well as daily life for people near coasts, lakes, and rivers. Blooms of cyanobacteria, for example, can spoil drinking water and recreational water use with the toxins they generate.\nScientists have been using some satellite data to track and monitor these blooms and the conditions that cause them. PACE should make it easier to decipher these species and conditions, allowing people to develop ways to mitigate the impacts and prevent future blooms.\n“Not all phytoplankton create harmful algal blooms, so if we can use the satellite data to better separate harmful from non-harmful blooms, that would be helpful for water managers and scientists that are trying to understand phytoplankton communities in a region,” said Bridget Seegers, an oceanographer at NASA Goddard.\nPACE will not be the first satellite to let us see phytoplankton from space. The mission is a successor to missions like Terra, Aqua, Landsat, and SeaWiFS (the Sea-viewing Wide Field-of-view Sensor), which have gathered data on phytoplankton since the 1990s. PACE, which is being assembled at and managed by engineers at NASA Goddard, will significantly expand our ability to distinguish and track phytoplankton every day, all over the planet.\n“Hopefully, the hyperspectral nature of the Ocean Color Instrument will allow us to better tease apart the phytoplankton types from each other and from non-phytoplankton particles,” Neeley said. “To me, the opportunities for research will be endless.”\nNASA wants to identify phytoplankton species from space. Here’s why (2023, June 23)\nretrieved 24 June 2023\npart may be reproduced without the written permission. The content is provided for information purposes only.']	['<urn:uuid:01017f00-e9e3-4426-8afe-2d4747890114>', '<urn:uuid:38eabd7f-2b7f-490d-85a1-fc24da7b4919>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	9	67	1540
45	length measurements peterson pipe 124 1983	The 1983 Peterson pipe model 124 had the following measurements: total length of 6.25 inches (158.75 mm), bowl height of 1.92 inches (48.80 mm), chamber depth of 1.62 inches (41.33 mm), chamber diameter of 0.71 inches (18.05 mm), and outside diameter of 1.18 inches (30.00 mm). The pipe weighed 0.90 ounces (25 grams).	['This is the short story of a small, pencil-shanked shape that’s peculiarly Irish yet rarely seen, even in the Peterson catalog. The shape’s name seems to depend on what type of stem is attached to the end of the bowl—zulu, churchwarden or dublin.\nAfter World War II, or “The Emergency” as it was known in Ireland, Peterson re-established their trade ties and, like other pipe-makers, found the demand for their pipes even greater than it had been seven years before. At some level of consciousness in the Irish spirit there was a nostalgia for something older and more secure, something that spoke of home, stillness, and rest.\nCharles Peterson’s favorite Oversize house pipes, with their 7 and 12-inch mouthpieces, were a thing of distant memory. The smaller-bowled straight “reading pipes” of the Patent Era, which also symbolized a leisurely evening’s smoke at home, were also forgotten. But the ache for what they represented must have returned, eventuating in Peterson’s first batch of Specialty Briars “church wardens,” illustrated in the 1945 catalog and 1947 shape chart with the iconic 124, which Peterson dubbed a “dublin.”\nThe 124 disappeared from the company’s literature for 20 years, until the mid-1960s, when it reappeared with a 6.25-inch mouthpiece and 1/8th bend, making it (at least technically) a zulu shape. It’s interesting that it reappeared at a time of great social unrest, a time characterized in part by a renewed interested in the mythological, the spiritual and the mythopoeic: hashtag Joseph Campbell, Martin Luther King, J. R. R. Tolkien.\n1983 Rustic Churchwarden 124\nFollowing a now-familiar pattern, the 124 again disappeared for another twenty years. It resurfaced in 1983’s catalog as a churchwarden. While it was not listed in the Classic Range chart from the same catalog, it made an appearance in the Shamrock line, as documented in the beautiful pipe, box, and papers that came across my desk not long ago.\nWhat made the particular 124 seen above so much easier to date was the matching box and “Chat with a Smoker” brochure, which can be dated to 1983 according to p. 359 of The Peterson Pipe: The Story of Kapp & Peterson. [Yup, the book’s done and at the printer’s.]\nFor those interested in such plumbing devices, it is interesting to document that this 1983 pipe had a “stinger,” a pointed, aluminum tube that easily slides in and out and seems to have been extensively used by generations of pipemen before mine, then promptly forgot. (If you have experience using such devices, what’s your experience with them?)\nA little over a decade later, the shape made another “nostalgia” appearance in the 1996 Old English Collection, Peterson’s most elaborate set in the Dublin Era (1991–2018), albeit with a shorter and not (to my mind) quite so compelling mouthpiece. I haven’t a clue as to what the shape name might be here: “pencil-shank dublin”? The bowl has a slight cone, which makes “dublin” seem correct. But the stem is also slightly bent, and the overall effect just isn’t stout enough in my mind to qualify it for dublin status.\nAnd that’s the end of the story, at least as far as the pencil-shank version of the 124 goes. The shape was re-frased (reshaped) with a thicker shank and a different bend (again, I have no idea what the shape name might be) for the Outdoor Sportsman line of nose-warmer shapes in 2011. It has afterwards appeared in a number of lines: the Craftsman Series for January 2016, the churchwarden line (of course), the 2016 Short Classics, and doubtless a few others.\n1983 Shamrock 124 Measurements:\nLength: 6.25 in. / 158.75 mm.\nWeight: 0.90 oz. / 25 gr.\nBowl Height: 1.92 in. / 48.80 mm.\nChamber Depth: 1.62 in. / 41.33 mm.\nChamber Diameter: 0.71 in. / 18.05 mm.\nOutside Diameter: 1.18 in./ 30.00 mm.\n2018 Outdoor Natural 124 Measurements:\nLength: 4.51 in. / 114.55 mm.\nWeight: 0.80 oz. / 22.68 g.\nBowl Height: 1.90 in. / 48.26 mm.\nChamber Depth: 1.63 in. / 41.40 mm.\nChamber Diameter: 0.75 in. / 19.05 mm.\nOutside Diameter: 1.13 in. / 28.70 mm.']	['<urn:uuid:28e0af82-55eb-468a-bf26-97aab648f4ca>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	6	53	684
46	Are nutritional needs different between smokers and toddlers for vitamin C?	Yes, their vitamin C needs are different. Toddlers need up to 4 times more nutrients like vitamin C than adults for their size, requiring 15mg daily. Smokers, due to their bodies using up vitamin C faster from tissue damage, need an extra 35mg of vitamin C per day above the standard adult requirement of 75-90mg.	"['Toddler taste buds\nGetting the right balance of good stuff\nYour toddler needs more fat and less fibre than a grown-up. The extra calories in fat give them the energy they need to grow and run around. Too much fibre can fill their tiny tummy, which is an incredible 5 times smaller than an adult’s.\nTo help them get their fill of good stuff, they’ll need to eat 3 nutrient-packed and balanced meals, plus 2 healthy snacks a day. Fitting them in around their playtime might be easier said than done, as your toddler may be more interested in having fun than eating!\nTry to give them a good mix of foods from the food groups below, keeping treats to a minimum. And if you need more ideas on toddler meals, our recipes have been specially created with your active explorer in mind.\nThings to include in your toddler’s diet:\nBread, rice, pasta and cereals can help your busy toddler get the energy they need for all that dashing around. Try to include these in every meal and some snacks.\nFruit & veg\nFor their size, toddlers need up to 4 times more nutrients like vitamin C than adults. To help them get their daily fix, serve up plenty of fruit and veg at every meal, and as snacks. Offer a rainbow of colours and lots of tastes and textures. Remember, fresh foods are at their lowest prices when they’re in season, and that’s when they taste best too. And tinned, dried and frozen fruit and veg are great long-lasting options to keep on hand.\nFull-fat dairy products\nTo help them keep up with their rapid bone growth, your toddler needs 3 servings of calcium-rich foods, such as milk, cheese and yogurt every day.\nMeat, fish & veggie options\nProtein, iron and omega-3 play an important role in your toddler’s healthy development. It’s best to ensure they get 2 or 3 daily portions of foods like meat, fish, well-cooked eggs, lentils and beans.\nMilk & drinks\nWith all that charging around your toddler will need to stay hydrated. Aim for 6-8 drinks a day, with most of these being water. Your toddler still needs milk every day to ensure they’re getting the nutrients they need.\nThings to avoid:\nFoods high in sugar\nIt’s best to keep sugary foods like biscuits, cakes, jam and orange squash to a minimum, to stop your toddler from developing a sweet tooth and possibly tooth decay. For something yummy and naturally sweet, choose fresh fruit instead of desserts with added sugar.\nTempting a fussy eater\nEven happy little eaters have off days. In fact around half of all toddlers have fussy eating habits. It’s a natural phase that many go through as they become more independent and learn to say “no!”. So if your toddler suddenly refuses to touch their favourite food, don’t panic. Chances are they’ll love it again in a few days’ time. Just keep smiling and wait a while before you try it again. Read our fussy eaters article for more tips and advice.\nAdding texture and variety\nOur Careline often gets calls from mums looking for tips on ways to add more texture or variety to a toddler diet. Our articles on toddler feeding, and adding variety, plus our homemade recipes are all great inspiration for feeding your little one!\nAny more questions?\nOur specialist baby advisors and experienced mums are here to talk and ready to help whenever you need them. You can call us 24/7, or we are available on Live Chat 8am-8pm Monday-Friday.', 'Vitamin C is an antioxidant present in many fruits and vegetables. Also known as L-ascorbic acid, vitamin C has a wide variety of uses in the body. It supports normal growth and development and helps the body repair damaged tissue.[2, 3] Vitamin C also assists in the production of collagen, a protein that’s necessary for healthy skin, cartilage, tendons, ligaments, and blood vessels.\nTo say that vitamin C is beneficial would be an understatement. It influences iron absorption and helps fight cell-damaging free radicals. A 16-year study found that regular vitamin C supplementation promoted heart health. Additionally, people who consume foods rich in vitamin C or other antioxidants may lower their risk of high blood pressure.[7, 8, 9]\nHigh Doses of Vitamin C\nIn the 1970s, chemist and Nobel Peace laureate, Linus Pauling, proposed that high doses of vitamin C could help prevent the common cold. Many people swear by Pauling\'s claim that vitamin C can boost the immune system naturally, but the research is still inconclusive.\nA number of studies have examined whether high-dose vitamin C can provide extraordinary therapeutic results. Results thus far are inconclusive. However, animal studies have found that vitamin C may make traditional therapies more effective.\nNatural Dietary Sources of Vitamin C\nMany types of food are fortified with vitamins and vitamin C is usually in the mix. However, like all vitamins, it’s best to get your daily intake from organic, natural sources and the best, natural sources of vitamin C are fruits and vegetables. Below are some of the best foods for vitamin C.\n|Sources of Vitamin C|\n|Food and Serving Size||Vitamin C (mg/serving)|\n|Red or Yellow Bell Pepper, Raw, 1/2 cup||95|\n|Orange Juice, 3/4 cup||93|\n|Orange, 1 medium||70|\n|Grapefruit Juice, 3/4 cup||70|\n|Kiwifruit, 1 medium||64|\n|Green Bell Pepper, raw, 1/2 cup||60|\n|Broccoli, cooked, 1/2 cup||51|\n|Strawberries, fresh, sliced 1/2 cup||49|\n|Brussels sprouts, cooked, ½ cup||48|\n|Grapefruit, ½ medium||39|\n|Broccoli, raw, ½ cup||39|\n|Tomato juice, ¾ cup||33|\n|Cantaloupe, ½ cup||29|\n|Cabbage, cooked, ½ cup||28|\n|Cauliflower, raw, ½ cup||26|\n|1 Lemon Yield, 48g||18.6|\n|Potato, baked, 1 medium||17|\n|Tomato, raw, 1 Medium||17|\n|Spinach, cooked, 1/2 cup||9|\n|Green peas, frozen, cooked, 1/2 cup||8|\nDaily Intake of Vitamin C\nThe amount of vitamin C that a person needs may vary with factors like age or whether a person is smoking, pregnant, or even breastfeeding. These are the guidelines provided by the U.S. Office of Dietary Supplements:\n|Recommended Daily Allowances of Vitamin C|\n|Age||Female||Male||Pregnant female||Breastfeeding female|\n|0-6 months||40 mg||40 mg||N/A||N/A|\n|7-12 months||50 mg||50 mg||N/A||N/A|\n|1-3 years||15 mg||15 mg||N/A||N/A|\n|4-8 years||25 mg||25 mg||N/A||N/A|\n|9-13 years||45 mg||45 mg||N/A||N/A|\n|14-18 years||65 mg||75 mg||80 mg||115 mg|\n|19+ years||75 mg||90 mg||85 mg||120 mg|\nDangers of Vitamin C Deficiency\nA lot of people might think “scurvy” is just pirate lingo, but it’s actually a disease caused by a lack of vitamin C. Symptoms of scurvy include fatigue, gum disease, anemia, scaly skin, and easy bruising. Vitamin C deficiency is uncommon in the United States these days but some people remain at risk.\nPeople who get too little variety in their food may not receive adequate nutrition. Normally, when we hear ""malnourished"" many of us think ""starving"", but what it\'s more likely to mean is that a person is deficient in specific nutrients and it\'s affecting their health. Those who rely on a carnivorous diet might miss their daily quota for vitamin C as meat and dairy don’t contain much of this critical nutrient. Infants fed evaporated or boiled cow\'s milk may not get enough vitamin C, especially since cow’s milk is low in vitamin C to begin with. Breast milk and infant formula are both better sources of vitamin C.\nSome medical conditions can cause vitamin C deficiency. Digestive tract injuries or inefficiencies, genetic diseases, and other issues can negatively affect not just vitamin C absorption, but nutrient absorption as a whole. Kidney disease and some types of cancer can also cause vitamin C deficiency.\nSmoking cigarettes is a bad idea for many reasons. One of the effects of the tissue damage it causes is the body using up vitamin C at a faster rate than normal. As a result, smokers and people exposed to second-hand smoke may need an extra 35 mg of vitamin C a day.\nVitamin C Supplementation\nUsually, if you follow a balanced diet with a foundation of organic fruits and vegetables, you’ll get all the vitamin C you need. If you don’t, vitamin C supplementation might be something to consider and discuss with your healthcare provider.\nBe aware of the difference between synthetic and natural vitamins. Synthetic supplements are manufactured with unnatural ingredients and chemicals. They are made to mimic natural vitamins but not everyone is convinced of their efficacy. Conversely, natural supplements are made using ingredients drawn straight from their natural sources.\nVitamin C supplements are usually available as ascorbic acid, sodium ascorbate, or calcium ascorbate. Synthetic and natural ascorbic acid have similar properties, but I always recommend a natural, plant-based source.[15, 16]\nIf your diet isn’t providing you with enough vitamin C, you should consider that it\'s not providing you with all the other nutrients your body requires, either. In such a case, you may want to skip the vitamin C supplement and look for a solid multivitamin. I recommend IntraMAX® and believe, without a doubt, that it’s the best multivitamin available anywhere. It’s an organic, liquid formula loaded with all the nutrients you need, as well as powerful antioxidant and immune system stimulators. It\'s as complete as it gets.\nWhere do you get your vitamin C? Supplements? A cold glass of orange juice? Let us know in the comments.\n- ""Vitamin C."" MedlinePlus. U.S. National Library of Medicine, 9 Mar. 2016. Web. 11 Mar. 2016.\n- Zeratsky, Katherine, R.D., L.D. ""Too Much Vitamin C: Is It Harmful?"" MayoClinic.org. Mayo Clinic, 5 Feb. 2015. Web. 11 Mar. 2016.\n- ""Wounds."" University of Maryland Medical Center. University of Maryland, 5 Jan. 2015. Web. 11 Mar. 2016.\n- Boyera, N., Galey , I. and Bernard, B.A. (1998), Effect of vitamin C and its derivatives on collagen synthesis and cross-linking by normal human fibroblasts. International Journal of Cosmetic Science, 20: 151–158. doi: 10.1046/j.1467-2494.1998.171747.x.\n- Lynch, S. R. and Cook, J. D. (1980), INTERACTION OF VITAMIN C AND IRON. Annals of the New York Academy of Sciences, 355: 32–44. doi: 10.1111/j.1749-6632.1980.tb21325.x.\n- Osganian, S.k., M.j. Stampfer, E. Rimm, and D. Spiegelman. ""Vitamin C and Risk of Coronary Heart Disease in Women."" ACC Current Journal Review 12.5 (2003): 27. PubMed. Web.\n- ""Vitamin C (Ascorbic Acid)."" University of Maryland Medical Center. University of Maryland, 16 July 2013. Web. 11 Mar. 2016.\n- Juraschek, Stephen P et al. “Effects of Vitamin C Supplementation on Blood Pressure: A Meta-Analysis of Randomized Controlled Trials.” The American Journal of Clinical Nutrition 95.5 (2012): 1079–1088. PMC. Web. 11 Mar. 2016.\n- Ness, A. R., D. Chee, and P. Elliott. ""Vitamin C and Blood Pressure–an Overview."" J Hum Hypertens Journal of Human Hypertension 11.6 (1997): 343-50. PubMed. Web. 11 Mar. 2016.\n- ""High-Dose Vitamin C."" National Cancer Institute. National Cancer Institute, 11 Dec. 2015. Web. 11 Mar. 2016.\n- Bobroff, Linda B., and Isabel Valentin-Oquendo. ""Facts About Vitamin C."" University of Florida IFAS Extension. University of Florida, n.d. Web. 11 Mar. 2016.\n- ""Vitamin C Fact Sheet for Health Professionals."" National Institutes of Health. U.S. Department of Health & Human Services, 11 Feb. 2016. Web. 11 Mar. 2016.\n- ""Vitamin C Fact Sheet for Consumers."" National Institutes of Health. U.S. Department of Health & Human Services, 17 Feb. 2016. Web. 11 Mar. 2016.\n- Hoffman, Freddie Ann. ""Micronutrient Requirements of Cancer Patients."" Cancer 55.S1 (1985): 295-300. PubMed. Web. 11 Mar. 2016.\n- ""Micronutrient Information Center: Vitamin C: Supplemental Forms."" Linus Pauling Institute. Oregon State University, 27 Nov. 2013. Web. 11 Mar. 2016.\n- Yung, Susanna, Michael Mayersohn, and J. Barry Robinson. ""Ascorbic Acid Absorption in Humans: A Comparison among Several Dosage Forms."" Journal of Pharmaceutical Sciences 71.3 (1982): 282-85. PubMed. Web. 11 Mar. 2016.\n†Results may vary. Information and statements made are for education purposes and are not intended to replace the advice of your doctor. If you have a severe medical condition or health concern, see your physician.']"	['<urn:uuid:380304f1-2b48-49f9-be4b-1714898eb590>', '<urn:uuid:4effb8d9-08e6-47e9-9313-ca873d2ffd53>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T15:30:12.136021	11	55	1955
47	How do basic exercise programs differ from advanced ones?	Basic programs like Stronglifts use simple linear progression by adding weight each session, focusing on full body workouts three times per week with basic lifts. In contrast, advanced programs employ more complex approaches like daily and weekly undulation of weights and exercises, and include a greater emphasis on developing muscle size and power. This difference reflects the training principles that programs need to be progressive and that workload must increase drastically with each year of training.	['Training methodologies have evolved since the sport was developed. We could probably say that without any doub training methodologies were first refined as a result of different decisions made by sport authorities in regard to competition. For instance, weightlifting used to have single arm events and abolishing those events must have had an impact on how people trained after it was removed from competition. Hence, dumbbell and one handed snatches have pretty much disappeared from most programs around the world.\nThe use of the squat stance versus the split stance over time is another great example. Catching lifts in a split stance was part of the older technique, in which the bar did not touch the body and in which the bar was often caught very high to do a press a few milisecond later. Even the order of exercises have changed in competition, the time between lifts has been reduced, the weight classes have changed and drug testing policies have made or destroyed countries.\nIn recent years, a competitive event, the press, was also removed. This has definitely changed training methodologies and programming. The press used to be an exercise of first priority and was moved down the list to a GPP exercise. Laputin discussed that issue at length in his book ”Managing the training of weightlifter”. That is, if the press is not a competitive lift anymore, then how much pressing tonnage does an athlete need in the biathlon era? They concluded that they could reduce the pressing tonnage by quite a bit without hurting jerk results and recommended 10% of the total monthly tonnage to be various kind of presses.\nSo, there it is, understanding the rules of competition leads to the creation of philosophies about what an athlete needs or should need to succeed. We see lots of variation in philosophies across the world. The sensation of the moment, China, seems to advocate the usage of lots of accessory movements and seems to be using lots of pulls variations and bodybuilding/gymnastics. Russia seems to use lots of complexes at times and seem to do more high repetition work (2-4) at lower intensity around the year. Bulgarian elite lifters supposedly went to max on a daily basis. Supposedly, Bulgarian liked backing off to do doubles or waves loading. Recently, an American coach said he only programs full lifts with no variations.\nThe list of what is different can go on and on. Everybody has a different idea of what is required to build top level athletes and judging by the results of said countries, all of those philosophies can work as long as it is included in the right system and targeting the right kind of athlete. Rather than discussing the variance – or may be peculiarities – perhaps the best thing to do is to illustrate what is similar between the training methodologies used around the world. Through that observation, we may be able to illustrate training principles that – if respected – will lead to success.\nHere are 8 principles that I have noticed. Keep in mind that this is my own vision and interpretation of the available data. They are listed in not particular order of importance.\nPrinciple 1 : To be effective, training programs have to be progressive.\nWhether you work in stages/classes like the Soviet Union used to do, or used a LTAD model like some of the North American countries or auto-regulate your training, to benefit from your program you need to have a progressive approach. In the grand scheme of things, the point is to do your best bars on the competition plateform which means that intensity (weights) has to increase over time. You need to go from point A to point B which has to be accounted in how you program. Volume and Intensity have to increase over time.\nPrinciple 2 : Training programs need to include exercise variations\nAlthough weightlifters compete in the snatch and clean and jerk, it is common to see a large variety of assistance exercises being included. Many people may be led to believe that elite lifters only train the full snatch, full clean and jerk and some kind of squats. This may have been true of elite Bulgarian lifters in the 80’s although their prior formation did include variety. In other words, these lifters’ training plan removed variations/assistance exercises as they progressed. The Russians were known to include a large variety of exercises and drills in their programs as well. Russian researchers even suggested an even split of 50/50 between full lifts and variations. Word of mouth has it that Chinese lifters’ workload if comprised of 60%+ assistance exercises and 40% of full lifts.\nPrinciple 3 : Different Sets and reps schemes should be included\nIt seems like many people think all we do is singles all the time. Most top countries include a lot of singles in their programming but they also include doubles, triples and even 6+ reps. For instance, Bulgarian weightlifters were known to go up to a heavy single and back off to hit doubles. Russians lifters often did lots of reps and sets far from competition and reduced the reps work as they got closer to the competition. Different sets and reps schemes stress the body differently which forces the body to adapt to the new structural stress. Moreover, while singles are probably the best tool to get comfortable and accustomed to bigger weights, they don’t develop muscle mass and arguably muscle strength the same way sets of 3-5 reps do.\nPrinciple 4 : Heavy weights should be lifted frequently\nIt is well known that strength is best increased when the intensity is high (i.e when you lift heavy weights). It is also known that strength increase are bigger when following an hypertrophy phase (see principle 3). Lifting heavy weights is different than maxing out. You need tonnage of heavy weights (made lifts) to get stronger and not just a single at 95%. Countries have toyed around with this principle pretty much differently. Whether it is back off sets or wave loading or working with % (ie : 5×3 @ 85%) or using the Pireplin’s chart, the principle is the same. You need to gradually lift bigger weights to keep getting strong.\nPrinciple 5 : Technical refinement comes before strength training\nIn most countries where the lifter’s career starts early on, the focus is not on results but on technical refinement and athletic development (coordination, flexibility, body awareness, etc). By the time the kid is hitting puberty, serious strength training starts and the lifter benefits from the change in hormones and from his ability to recover faster. I know of 12 or 13 years old kids that are trained on singles (max snatch, max clean and jerk and heavy – but low volume – squats) 3 times a week. This type of philosophy differs from what is being done by the best countries and will not lead to big results in the long term. That is, the work capacity and athletic development will be very low in comparison to other lifters around the world (See principle 6).\nPrinciple 6 : Work load increases drastically with every year of training.\nWorkload has to increase to keep progressing. Russians calculated tonnage and K-values. Apparently, Chinese lifters calculate the tonnage they do above 80% as well. Data shows that with every year of training, the work loads increases. It means more sets, more weights, and higher average intensity (the higher the average intensity, the better you strength gain). If a kid specialize too fast in weightlifting -as in Principle 5’s example (ie : heavy singles and no accessory movements) – the kid won’t be able to increase his work load as much as others over the years mostly because his work capacity will be low (and so will be his muscle mass). The years of technical refinement serve to build a great foundation of athleticism to prepare the body for what is coming at the next level. Russians called that GPP.\nPrinciple 7 : It is encouraged to do non weightlifting activities that may transfer to weightlifting results (in a moderate way).\nIn most training camps around the world, a date – every week- is reserved to play a different sport whether it is volleyball or basketball. Lifters play that sport re creatively as a way to keep developing their athleticism.\nPrinciple 8 : Plans have to be goal oriented\nWhether it is a periodized approach or a ”bulgarian-ish” just winging it out approach, you need to be doing things that will make you better on the plateform. Most people work in phases as in – strength phase and then technical phases (precompetition phase). However you are doing it, you need to plan and create a plan that fits your goals and where you want to be. The approach has to be specific.', 'We’ve now looked at the major models and concepts associated with periodisation and started to break down how some of the most popular powerlifting programmes may be classified.\nNow we’ll sharpen our focus on some of these programmes and explain their premise in a little more depth.\nBasic linear programmes\nGenerally considered to be the entry point into ‘proper’ strength training, these programmes keep it nice and simple.\n- Train full body, three times a week\n- Train the big lifts, don’t worry about assistance work\n- Start light, focus on technique\n- Add load each session\nStronglifts 5 x 5\nStronglifts cycles two full-body sessions (A and B) performed three times per week. Session A consists of 5 working sets of 5 repetitions on the squat, bench press and barbell row. Session B consists of 5 sets of 5 of the squat and overhead press, and 1 set of 5 of the deadlift.\nProgression is achieved linearly by increasing the load in small increments (2.5 – 5 kg) each session, with lifters advised to begin the programme at 50% of 5RM.\nAlso a basic linear programme cycling two full-body sessions (A and B) performed three times per week. Session A consists of 3 working sets of 5 repetitions of the squat and overhead press, followed by 1 set of 5 of the deadlift. In session B the overhead press is substituted with the bench press. This programme continues until the load for the deadlift exceeds the load using for the squat. At this point, the deadlift in session B is replaced by 5 sets of 3 of the power clean.\nProgression is achieved linearly by increasing the load in small increments (2.5 – 5 kg) where possible.\nWeekly undulating programmes\nAgain, solid entry points into strength training that look to keep things simple. Weekly undulating periodisation (WUP) reduce the risk of push too hard given the changes in repetition ranges. These programmes tend to be conservative in regards total volume side.\nJim Wendler’s 5 / 3 / 1 is an example of weekly undulating periodisation (WUP) over a four-week block. The programme cycling two upper- and two lower-body sessions during the training week. The squat, bench press, deadlift and overhead press are each trained once per week in the same manner. In week one, the lifter works up to a single attempting 5 or more repetitions with 85% of a training max (< 90% of 1RM). In week two, the lifter attempts 3 or more repetitions with 90% of training max. In week three, the lifter attempts 1 or more repetitions with 95% of training max. The fourth week is a deload week where 5 repetitions are performed at 65% of training max.\n5 / 3 / 1 also allows the incorporation of assistance lifts. These are typically performed with a view to developing hypertrophy and, to some extent, strength. Where these are included, this takes the programme into concurrent territory. As the programme gives you the scope to vary the assistance lifts and their prescription, the programme could be set up and used in a conjugate fashion.\nProgression is sought through a linear increase in load at the start of each cycle, typically adding 2.5 – 5 kg to the training max. Using a training max of < 90% of 1RM helps to facilitate the linear progression.\nThe Juggernaut Method\nThe Juggernaut Method is similar to 5 / 3 / 1 in the split of four training days with either a squat, bench press, deadlift and overhead press focus. The inclusion of assistance work is integrated in a similar manner also. The main difference comes in a more detailed and progressive macrocycle.\nThe programme is split into four mesocycles (termed phases) based upon the number of reps prescribed – 1) 10’s phase, 2) 8’s phase, 3) 5’s phase, and 4) 3’s phase. Each block is then split into four weeks – 1) accumulation week, 2) intensification week, 3) realisation week, and 4) deload week. As with 5 / 3 / 1, there are number of open-ended sets. The programme advises how many reps to keep in reserve and periodise this around the training weeks.\nGiven the linear periodisation of the blocks, progression is achieved by increasing the relative intensities throughout each week of the block (by 7.5% 1RM) and from block to block (by 5% 1RM). The sequencing of the blocks in an example of conjugation – i.e. each block should potentiate the next.\nDaily undulating programmes\nDaily undlatiuammes (Dng progrUP) are generally considered a progression from linear programmes, although there’s no reason whs couldn’t utilise these too. The premise of DUP programmes is that you don’t lift the same load y beginnerin each session. Two arguments for why this works well:\n- Variation in the stimulus applied to the neuromuscular system\n- Variation in the mentality in which each session is approached\nAn example of DUP, cycling three total body sessions during the training week. These as are termed volume, light/recovery and intensity days. The squat is performed in all three sessions, the bench in two (or one, dependent on use of the overhead press), and the deadlift only in the volume session for a single working set.\nProgression is sought through a linear increase in load each week, again aided by starting the programme with loads of < 90% of each RM. When lifters begin to stall it is suggested that volume is reduced within the volume session, much in-line with linear principles.\nBill Starr / Madcow 5 x 5\nAn intended progression from the Stonglifts programme. Still linear in nature but now this programme employed some sight undulation of load between training days. Again, this uses two full-body sessions (A and B) performed three times per week.\nSession A consists of squat, bench press and barbell row and repeated twice a week (i.e. A1 = Monday and A2 = Friday). Session B consists of the squat, overhead press, and deadlift and performed just once (i.e. Wednesday). Session A1 is moderate-heavy day with the lifts performed for 5 x 5 with the load increasing on each set. Session B is a lighter day for the squat (equal to the third set on A1) performed for 4 sets of 5. The press and deadlift are performed for 4 sets of 5, with the load increasing on each set. Session A2 is a moderate-heavy day. This starts with 4 sets of 5 on each lift, followed by a heavy set of 3 and a back-off set of 8.\nAs with Stronglifts, load is added to the bar each week in small increments to progress.\nComplex undulating programmes\nI’ve termed these ‘complex’ undulating programmes as they employ principles of both DUP and WUP. These programmes are better suited to more advanced lifters and tend to place a great emphasis on the development of power (i.e. rate of force development) and hypertrophy.\nThe Cube Method\nThe Cube Method employs principles of DUP and WUP over a ten-week period with four training sessions per week. Each of the three power lifts are performed once per week with the heavy, explosive and repetition sessions cycled in a WUP fashion every three weeks.\nDuring each training week, one lift will be performed in a heavy session, one lift in a repetition session and one lift in an explosive session – thereby employing DUP. The fourth training session of the week is always designated as a bodybuilding/accessory session designed to target specific muscle groups with moderate to high repetitions (typically 8 – 20). Additional accessory work is also performed in the other three sessions following the main lift. Three cycles are completed before a one-week taper is employed in week 10.\nGiven the linear periodisation of the blocks, progression is achieved by increasing the relative intensities of each day (i.e. heavy, explosive, repetition) when they are next performed by 5% 1RM. The sequencing of the blocks in an example of conjugation – i.e. each block should potentiate the next.\nThe Westside Barbell Method\nPrimarily an example of DUP, the Westside approach employs two upper- and two lower-body sessions each week. Training is typically blocked into three-week mesocycles. One upper- and one lower-body session are each designated as a maximal effort session. The remaining upper- and lower body sessions are each designated as dynamic effort sessions. Accessory work for either the upper- or lower-body is performed following the main lift. The consistent focus on three different bio-motors, without any sequencing, makes Westside a concurrent training programme. Westside is therefore not conjugate!\nDuring the maximal effort session, lifters perform a variation of the bench press (upper-body) or squat/deadlift/good morning (lower-body) working up to a 1 – 3RM set. This lift is varied each week and typically not repeated in the current or successive cycle. In dynamic effort sessions, lifters perform a variation of the bench press (upper-body) or squat/deadlift (lower-body) for multiple sets (6 – 12) of low repetitions (1 – 3) at loads between 50 – 85 %1RM (an example of WUP). Each repetitions is to be performed as fast and explosively as possible. Cycles of dynamic effort training will commonly use the same variation for the duration of the three-week cycle. Accessory work is typically performed in a moderate repetition range (6 – 10).\nProgression is sought through the attainment of new repetition maxes within a given exercise variation. This is facilitated by the vast array of possibilities for achieving variation within the squat, deadlift, good morning and bench press.\nThe Sheiko Routines (#29 – #32)\nGranted, the true routines of powerlifting coach Boris Sheiko aren’t designed to be one-size-fits-all programmes. However, the example programmes from some of Sheiko’s translations have become widely used in this way.\nProgrammes #29, #30, #31 and #32 form a sixteen-week macrocycle split into four, four-week mesocycles or blocks. This approach employs principles of DUP and WUP across the cycles, although the general trend is for intensity and volume to progress in a linear manner. This programme is designed to bring the lifter to a peak at the end of the macrocycle. Here’s how the blocks can be recognised:\n- Sheiko #29 – preparatory block with moderate intensity and moderate volume\n- Sheiko #30 – accumulation block with moderate intensity and high volume\n- Sheiko #31 – intensification block with high intensity and moderate volume\n- Sheiko #32 – peaking block with high intensity and low volume\nThe Sheiko programmes are highly specific – focusing predominantly on the squat, bench and deadlift exercises with minimal variation (i.e. deadlift to knees). Assistance exercises are incorporated and are performed in a moderate repetition range (5 – 10).\nGiven the linear periodisation of the blocks, progression is achieved by increasing the relative intensities (%1RM) throughout the programme. The sequencing of the blocks in an example of conjugation – i.e. each block should potentiate the next.\nThe ‘Bulgarian’ Method (Perryman)\nDespite the name, this is not a real representation of the true Bulgarian weightlifting/powerlifting systems. Nonetheless, Matt Perryman’s book ‘Squat Every Day’ and the likes of Greg Nuckols and John Broz have popularised what we refer to in the West as the Bulgarian method.\nThe main principle of the Bulgarian method is to work up to a daily max in the back squat in each and every session. The ‘max’ should be smooth, comfortable and achieved without equipment and without psyching up. Following the max, the lifter then performs some comfortable back-off sets. It is intended that the lifter autoregulates the back-off sets too, however, 2-5 sets of 2-3 with 10% less than the daily max is a common recommendation. Because of the autoregulation of load and back-off sets, the Bulgarian system effectively becomes a DUP style of programme. It is recommended that deload weeks in incorporated in the programme, a principle of WUP.\nTypically, lifters will look train 4-7 days per week on the programme. The basic template is a simple one – squat/deadlift, press, upper body pull. The daily max and back-off sets are normally performed for the squat/deadlift and press, whereas the upper body pull is performed in a typical hypertrophy assistance style. It is recommended that more squat than deadlift sessions are performed (typically no more than two deadlift sessions).\nPeriodisation is achieved through a gradual increase in the load achieved for the daily max and, consequently, the load used for the back off sets.\nAs we highlighted last time, there is no ‘best’ programme.\nEvery strength programme seeks to add load to the barbell to achieve progressive overload.\n‘Beginner’ programmes tend to be linear in nature and demonstrate high levels of specificity.\nMore ‘advanced’ programmes tend to integrate undulation on a daily and/or weekly basis. These programmes place greater emphasis on the concurrent development of hypertrophy and/or RFD.\nUnderstanding how the programme is put together is crucial when seeking to determine which best fits your goals and needs at any given point in time.']	['<urn:uuid:237a1f1a-a4df-4da7-9c2a-3616d8e7cf77>', '<urn:uuid:d91083e8-6d8d-4ccf-b03d-678542aa2d1f>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	9	76	3638
48	What are the recommended food items and supplements for maintaining a healthy insectivorous lizard like the Madagascar Ground Gecko, including both the lizard's diet and cricket care requirements?	For Madagascar Ground Geckos, which are exclusively insectivorous, the diet should consist of 90-95% crickets of their head size and occasionally mealworms. Wax worms should only be given occasionally due to high fat content. For proper nutrition, crickets must be gut loaded and kept healthy themselves. Crickets should be fed crushed dry reptile food, crushed bird pelleted diet, or commercial cricket food, along with small pieces of greens like kale and collard greens. For supplements, crickets should be dusted with calcium/D3 supplement (frequency varies by age: every feeding for juveniles, every other feeding for sub-adults, and 2 times weekly for adults) and a separate multivitamin supplement with pre-formed Vitamin A (weekly for juveniles, every other week for adults).	['When considering what type of lizard you would like to keep, it is important to research the different species and their care requirements before you purchase the lizard.\nDo you want a pet that eats insects, knowing that you will also have to care for the crickets like another pet(see cricket care below)? Do you want a vegetarian lizard? One that eats rodents? How much room do you have in your home for the lizard’s enclosure? Who is going to take care of the lizard when you are out of town? Is the lizard that you are interested in going to get large and aggressive as it matures? Do you have enough money left after the purchase of the lizard to buy the proper heating/lighting and substrate? Are you going to be able to afford proper medical care for your lizard once you make the commitment of caring for one?\nPlease consider these variables before you purchase any pet. Rescue groups are having a difficult time keeping up with the numbers of animals given up each year, mostly because people didn’t research what they were getting into with their choice in pet.\nHealthy Vegetables For Vegetarian/Omnivorous Lizards\nThe most important thing to remember is that your lizard will be healthier in the long run if you offer a base variety of greens(the best being collard greens) and then supplement with other greens/vegetables.\nGreens: Collard greens, kale, mustard greens, escarole, romaine lettuce, red/green leaf lettuce, boston lettuce, endive, parsley, dandelion, spinach(though it containes oxalates, small amounts are safe to feed to increase variety in the diet)broccoli leaves and carrot tops.\nVeggies: Shredded sweet potatoe and carrot, green beans, peas, squash, zuccini, tomato.\nFruit: Kiwi, papaya, mago, melon(honey dew and cantalope) and any berries in season. (Different species of reptiles should have more or less fruit in their diet, be sure to know your pet’s requirements before feeding fruit).\nHousing: Keep your crickets in their own cage/container. Be sure the container has adequate air circulation because moisture build up will kill the crickets.\nSubstrate: Don’t use any substrate in the cage bottom. Place paper towel rolls, egg carton and/or crumpled newspaper in the cage to increase surface area.\nDiet: Feed your crickets crushed dry reptile food(that is appropriate for your reptile), crushed bird pelleted diet, and/or commercial cricket food. Check to make sure it is not “gut load”, but an actual cricket diet. You can offer small pieces of greens like kale and collard greens, too. Remove any old food before mold grows.\nWater: Fold a paper towel into quarters, and dampen with water. Place in the cage bottom away from the crickets’ food. Change the paper towel daily to prevent mold growth.\nPowder Supplements on Your Crickets, General Guidelines: Powder your crickets by placing a few in a cup with the supplement and gently shaking them so that the powder coats the crickets. Use a calcium/D3 supplement every feeding in juveniles, every other feeding in sub-adults and 2 times a week for adults. Use a separate multivitamin supplement(with pre-formed Vit A) once a week in juveniles, and every other week in adults. *Check with your herp vet for proper adjustments in the dusting protocol for your individual pet.\nFeeding Crickets to Your Pet: Remove any crickets that remain uneaten about 1 hour after feeding them to your pet. This is important, because the crickets that are left will not have food to eat and may bite your reptile/amphibian. The crickets will also clean off any powder supplements, so they won’t be as nutritious for the pet.\nRemember, healthy crickets make healthy herps!', 'Madagascar Ground Geckos (Paroedura picta) Madagascar Ground Geckos are tropical, terrestrial, nocturnal geckos with beautiful color patterns and w\nMadagascar Ground Geckos (Paroedura picta)\nMadagascar Ground Geckos are tropical, terrestrial, nocturnal geckos with beautiful color patterns and with large eyes. They are native to the southern part of the island of Madagascar, located east of Africa in the Indian Ocean. They are an excellent gecko for those who have experience with beginner-friendly geckos such as crested geckos (Rhacodactylus ciliatus) or leopard geckos (Eublepharis macularius).\nMadagascar Ground Gecko Availability\nMadagascar ground geckos are available mostly from breeders and are occasionally seen available as wild-caughts in the pet trade. They are typically available in banded and striped morphs and have a xanthic (lack of black pigment) version of both.\nMadagascar Ground Gecko Size\nApproximately 5-1/2 inches with tail. They are relatively wide bodied geckos with large heads.\nMadagascar Ground Gecko Life Span\nWith proper care the Madagascar ground geckos can live from 6-10 years.\nMadagascar Ground Gecko Housing\nA 10 gallon glass reptile terrarium with a screen top can house a male and two females. Keep them in male female pairs or a single male and multiple females. Males will fight with other males, possibly to their deaths. They can climb glass so a secure top is necessary to prevent escapes! Each gecko should have their own reptile hide to feel secure. When it comes to lizard habitat products, it should also consist of some rocks or pieces of wood for them to crawl on, anchored as necessary with glue to prevent tipping and crushing of the geckos. Small live or plastic plants may also be used.\nAs a choice of lizard substrate, paper towels, peat moss and/or medium grade orchid bark (to prevent accidental ingestion). They live amongst leaf litter in their natural environment so that may also be used instead of or in conjunction with other substrates.\nReptile Heating & Lighting for the Madagascar Ground Gecko\nA temperature range of the 77F to lower 88F during the day works best with the Madagascar ground gecko. Temperature can drop into the low 70Fs at night. If you need to provide heat to your terrarium to reach the proper temperature several options are available. Infrared or low wattage incandescent lighting are good choices to increase temperature as are UTHs (under tank heaters).\nUVB lighting is optional as a diet providing proper vitamin supplementation (ask your breeder what is recommended) makes it unnecessary to maintain your Madagascar gecko’s health. Fluorescent lighting may be used for live plants in the terrarium. A proper reptile light fixture will be required depending on which light you choose. Lights should be turned off at night (a reptile timer will make this easier).\nMadagascar Ground Gecko Food\nMadagascar Ground Geckos are exclusively insectivorous. They should be fed a diet of commercially available insects such as mealworms and crickets. Crickets should be 90-95% of the gecko’s head size – an adult should be fed a three week old or ¾” cricket. Wax worms may be given only occasionally due to their high fat content. Insect food should be gut loaded with nutrients and supplemented with a reptile specific supplement containing calcium, vitamin D3 and other essential vitamins and minerals such as phosphorus and Vitamin A (or the pre-cursors to make Vitamin A).\nMadagascar Ground Gecko Water and Humidity\nMist the sides of the terrarium daily to provide humidity (60-70%) and droplets of water for the geckos to drink. A reptile hygrometer can be used to measure the humidity in your habitat. You may also provide a shallow water dish.\nMadagascar Ground Gecko Tails\nMadagascar Ground Geckos can lose their tails if stressed. They will regenerate but will not look the same.\nMadagascar Ground Gecko Handling and Temperament\nHandling should be kept to a minimum with this active gecko from Madagascar. They do not appreciate handling and may nip you as a result of your attempt! Their bite is usually a warning that does not hurt.\nJulie Bergman is the owner of Gecko Ranch, LLC and has been breeding many species of geckos for over 20 years. She is a frequent contributor to Reptiles Magazine and is the author of the Advanced Vivarium Systems book “Geckos.” You can visit her website at www.geckoranch.com']	['<urn:uuid:bd352943-a633-4873-9ba7-e8ef4fbb1462>', '<urn:uuid:548a8065-13f3-4461-ad71-321c591325cc>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T15:30:12.136021	28	118	1314
49	How did Romans view silk clothing in ancient times?	Silk was considered a luxury item among Romans. During Tiberius' reign, they even passed a law against men wearing silk garments, as it was seen as disgraceful. For 200 years after Pliny's time, silk use was restricted to women only, until Emperor Elagabalos broke this tradition. The material was so expensive that Aorelian noted a pound of silk sold for twelve ounces of gold in Rome.	"['Vincent\'s Word Studies, by Marvin R. Vincent, , at sacred-texts.com\nCompare Eze 43:2.\nMightily with a strong voice (ἐν ἰσχύΐ́ φωνῇ μεγὰλῃ)\nLit., in strength with a great voice. Omit μεγάλῃ great, and read ἰσχυρᾷ φωνῇ with a mighty voice. So Rev.\nBabylon - is fallen\nThe Rev. improves on the A.V. by placing fallen in the emphatic position of the Greek: ""Fallen, fallen is Babylon."" Compare Isa 21:9.\nIs become (ἐγένετο)\nProperly, demons, which Rev., strangely commits to the margin. See on Mar 1:34. See Isa 13:20-22; Isa 34:13-15. Also on Luk 11:24.\nSee on Pe1 3:19, and see on Act 5:21. Rev., in margin, prison.\nThe word rendered above hold. Rev., hold. Some, however, explain it, not as a cage where they are kept, but as a place of safety to which they resort.\nOnly in Revelation, here, Rev 19:17, Rev 19:21. Compare Jer 50:39.\nHave drunk (πέπωκεν or πέπωκαν)\nSome, however, read πέπτωκαν have fallen. So Rev.\nOf the wine (ἐκ τοῦ οἴνου)\nThus if we read have drunk. If we adopt have fallen, ἐκ is instrumental, by. So Rev.\nOf the wrath\nThe wine of fornication has turned to wrath against herself.\nThe word originally means one on a journey by sea or land, especially for traffic. Hence a merchant as distinguished from κάπηλος a retailer or huckster.\nThe abundance of her delicacies (τῆς δυνάμεως τοῦ στρήνους αὐτῆς)\nLit., as Rev., the power of her luxury. Στρῆνος is akin to στερεός firm, hard, stubborn (see on steadfast, Pe1 5:9). Hence over-strength, luxury, wantonness. Only here in the New Testament. The kindred verb στρηνιάω to live deliciously occurs Rev 18:7, Rev 18:9.\nCome out of her\nCompare Jer 51:6, Jer 51:45; Isa 48:20; Isa 52:11; Num 16:26.\nHave fellowship with (συγκοινωνήσητε)\nThis compound verb is not of frequent occurrence in the New Testament. It is found only in Eph 5:11, Phi 4:14, and here. On the kindred noun συγκοινωνὸς companion, see on Rev 1:9.\nHave reached (ἠκολούθησαν)\nLit., followed. But the best texts read ἐκολλήθησαν clave. Compare Jer 51:9. For different applications of the verb see on Mat 19:5; see on Luk 15:15; see on Act 5:13. Compare the classical phrase for following up closely a fleeing foe, hoerere in terga hostium, to cleave to the backs of the enemy. See also Zac 14:5 (Sept.), ""The valley of the mountains shall reach (ἐγκολληθήσεται) unto Azal."" The radical idea of the metaphor is that of following or reaching after so as to be joined to.\nOnly here in the New Testament. Compare Isa 40:2; Jer 16:18; Zac 9:12. The Levitical law insisted on the double recompense. See Exo 22:4, Exo 22:7, Exo 22:9.\nLived deliciously (ἐστρηνίασεν)\nSee on Rev 18:3.\nOnly in Revelation. On the kindred word, βάσανος torment, see on Mat 4:23, Mat 4:24.\nI sit a queen and am no widow\nSee Isa 47:8; Zep 2:15.\nTherefore shall her plagues come, etc.\nSee Isa 47:8, Isa 47:9.\nWho judgeth (ὁ κρίνων)\nRead κρίνας judged.\nOnly here, Rev 18:12, and Act 21:3. From γέμω to be full. Hence, literally, lading or cargo. So Rev., in margin.\nThe main features of the following description are taken from that of the destruction of Tyre, Ezekiel 26, 27.\nFine Linen (βύσσου)\nSee on Luk 16:19.\nSee on Luk 16:19.\nProperly an adjective, meaning pertaining to the Seres. From Σῆρες Seres, a people of India, perhaps of modern China.\nBefore the time of Justinian, when silkworms were first brought to Constantinople, it was thought that the Seres gathered or combed the downy substance woven by the worms from the leaves of certain trees. Hence Virgil speaks of the Seres, how they comb (depectant) the fine fleeces from the leaves (""Georgics,"" ii., 121).\nSilk was a costly article of luxury among the Romans, so that Tacitus relates that in the reign of Tiberius a law was passed against ""men disgracing themselves with silken garments"" (""Annals,"" ii., 33). ""Two hundred years after the age of Pliny,"" says Gibbon, ""the use of pure or even of mixed silks was confined to the female sex, till the opulent citizens of Rome and the provinces were insensibly familiarized with the example of Elagabalos, the first who, by this effeminate habit, had sullied the dignity of an emperor and a man. Aorelian complained that a pound of silk was sold at Rome for twelve ounces of gold"" (""Decline and Fall,"" ch. xl.).\nAt the time of Justinian the Persians held a monopoly of this trade. Two missionary monks residing in China imparted to Justinian the project of introducing the eggs of the silkworm into Europe, and returning to China concealed the eggs in a hollow cane and so transported them.\nSee on Mat 27:6.\nThyine wood (ξύλον θύΐ́νον)\nOnly here in the New Testament. From θυία or θύα the citrus, a North-African tree, a native of Barbary, used as incense and for inlaying. Pliny speaks of a mania among the Romans for tables made of this wood. The most expensive of these were called orbes, circles, because they were massive plates of wood cut from the stem in its whole diameter. Pliny mentions plates four feet in diameter, and nearly six inches thick. The most costly were those taken from near the root, both because the tree was broadest there, and because the wood was dappled and speckled. Hence they were described by different epithets according as the markings resembled those of the tiger, the panther, or the peacock.\nSee on Pe1 3:7, and see on Act 9:15. Also see on goods, Mat 12:29; see on Mar 3:27; and see on strake sail, Act 27:17.\nOf ivory (ἐλεφάντινον)\nOnly here in the New Testament. References to ivory are frequent in the Old Testament. The navy of Tarshish brought ivory to Solomon with apes and peacocks (Kg1 10:22). His great throne was made of it (Kg1 10:18). Ahab\'s ivory palace (Kg1 22:39) was probably a house with ivory panels. ""Ivory palaces"" are mentioned in Psa 45:8, and ""houses of ivory"" in Amo 3:15. The Assyrians carried on a great trade in this article. On the obelisk in the British Museum the captives or tribute-bearers are represented as carrying tusks. The Egyptians early made use of it in decoration, bringing it mostly from Ethiopia, where, according to Pliny, ivory was so plentiful that the natives made of it door-posts and fences, and stalls for their cattle. In the early ages of Greece ivory was frequently employed for ornamental purposes, for the trappings of horses, the handles of kegs, and the bosses of shields. Homer represents an Asiatic woman staining ivory with purple to form trappings for horses, and describes the reins of chariot-horses as adorned with ivory. The statue of Jupiter by Phidias was of ivory and gold. In the ""Odyssey"" of Homer, Telemachus thus addresses his companion, the son of Nestor as they contemplate the splendor of Menelaus\' palace:\n""See, son of Nestor, my beloved friend,\nIn all these echoing rooms the sheen of brass,\nOf gold, of amber and of ivory;\nSuch is the palace of Olympian Jove.""\n""Odyssey,"" iv., 71-74.\nFrom μαρμαίρω to sparkle or glisten.\nMentioned as one of the ingredients of the holy oil for anointing (Exo 30:23), and as a perfume for the bed (Pro 7:17).\nAnd spice (καὶ ἄμωμον)\nThese words are added by the best texts. A fragrant Indian plant, with seed in grape-like clusters, from which ointment was made. Preparations for the hair were made from it. Virgil, describing the coming golden age, says: ""The Assyrian amomum shall spring up as a common plant"" (""Eclogue"" iv., 25; Compare ""Eclogue"" iii., 89). Forbiger (Virgil) says that the best was raised in Armenia, a poorer quality in Media and Pontus.\nFine flour (σεμίδαλιν)\nOnly here in the New Testament.\nSee on Luk 10:34.\nMerchandise of horses\nMerchandise is not in the text. It resumes the construction of γόμον merchandise with the genitive in Rev 18:12.\nA Latin word though of Gallic origin, rheda. It had four wheels.\nThe fruits (ἡ ὀπώρα)\nOriginally, the late summer or early autumn; then, generally, used of the ripe fruits of trees. Only here in the New Testament. Compare the compound φθινοπωρινὰ autumn (trees). See on whose fruit withereth, Jde 1:12, and compare Summer-fruits, Jer 40:10.\nThat thy soul lusted after (τῆς ἐπιθυμίας τῆς ψυχῆς σοῦ)\nLit., of the desire of thy soul.\nFrom λίπος grease. Hence, literally, fat. Only here in the New Testament. Homer uses it once in the sense of oily or shiny with oil, as the skin anointed after a bath. ""Their heads and their fair faces shining"" (""Odyssey,"" xv., 332). So Aristophanes (""Plutus,"" 616), and of oily, unctuous dishes (""Frogs,"" 163). Of the oily smoothness of a calm sea, as by Theocritus. The phrase λιπαροὶ πόδες shining feet, i.e., smooth, without wrinkle, is frequent in Homer. Thus, of Agamemnon rising from his bed. ""Beneath his shining feet he bound the fair sandals"" (""Iliad,"" ii., 44). Also of the condition of life; rich, comfortable: so Homer, of a prosperous old age, ""Odyssey,"" xi., 136. Of things, bright, fresh. Of soil, fruitful. The city of Athens was called λιπαραὶ, a favorite epithet. Aristophanes plays upon the two senses bright and greasy, saying that if any one flatteringly calls Athens bright, he attaches to it the honor of sardines - oiliness (""Acharnians,"" 638, 9).\nA too indefinite rendering. Better, Rev., sumptuous. See on Luk 23:11; see on Jam 2:2. Mostly in the New Testament of clothing. See on Rev 15:6.\nSee on Rev 17:4.\nFrom κυβερνάω to govern. Strictly, steersman. Only here and Act 27:11.\nAll the company in ships (πᾶς ἐπὶ τῶν πλοίων ὁ ὅμιλος)\nThe best texts substitute ὁ ἐπὶ τόπον πλέων, that saileth anywhere, lit., saileth to a place. So Rev.\nTrade by sea (τὴν θάλασσαν ἐργάζονται)\nLit., work the sea, like the Latin mare exercent, live by seafaring. Rev., gain their living by sea.\nCast dust on their heads\nCompare Eze 27:30. See on Luk 10:13.\nHath avenged you on her (ἔκρινεν τὸ κρίμα ὑμῶν ἐξ αὐτῆς)\nRev., more literally, hath judged your judgment on her or from her. The idea is that of exacting judgment from (ἐξ). Compare the compound verb ἐκδικεῖς avenge, or exact vengeance from (Rev 6:10). The meaning is either, that judgment which is your due, or what she hath judged concerning you.\nA mighty angel (εἷς ἄγγελος ἰσχυρὸς)\nLit., ""one strong angel.""\nA great millstone\nSee on Mat 18:6.\nWith violence (ὁρμήματι)\nLit. with an impulse or rush. Only here in the New Testament.\nSee on Rev 14:2.\nOnly here in the New Testament. There seems to be no special reason for changing the rendering to minstrels, as Rev. The term music had a much wider signification among the Greeks than that which we attach to it. ""The primitive education at Athens consisted of two branches: gymnastics for the body, music for the mind. Music comprehended from the beginning everything appertaining to the province of the nine Muses; not merely learning the use of the lyre or how to bear part in a chorus, but also the hearing, learning, and repeating of poetical compositions, as well as the practice of exact and elegant pronunciation - which latter accomplishment, in a language like the Greek, with long words, measured syllables, and great diversity of accentuation between one word and another, must have been far more difficult to acquire than it is in any modern European language. As the range of ideas enlarged, so the words music and musical teachers acquired an expanded meanings so as to comprehend matter of instruction at once ampler and more diversified. During the middle of the fifth century b.c. at Athens, there came thus to be found among the musical teachers men of the most distinguished abilities and eminence, masters of all the learning and accomplishments of the age, teaching what was known of Astronomy, Geography, and Physics, and capable of holding dialectical discussions with their pupils upon all the various problems then afloat among intellectual men"" (Grote, ""History of Greece,"" vi., ch. lxvii.).\nRev., flute-players. Only here and Mat 9:23. The female flute-players, usually dissolute characters, were indispensable attendants at the Greek banquets. Plato makes Eryximachus in ""the Symposium,"" say: ""I move that the flute-girl who has just made her appearance, be told to go away and play to herself, or, if she likes, to the women who are within. Today let us have conversation instead"" (""Symposium,"" 176). Again, Socrates says: ""The talk about the poets seems to me like a commonplace entertainment to which a vulgar company have recourse; who, because they are not able to converse and amuse one another, while they are drinking, with the sound of their own voices and conversation, by reason of their stupidity, raise the price of flute-girls in the market, hiring for a great sum the voice of a flute instead of their own breath, to be the medium of intercourse among them"" (""Protagoras,"" 347). Compare Isa 24:8; Eze 26:13.\nCompare Jer 25:10; Mat 24:41.\nBridegroom - bride\nCompare Jer 25:10.\nGreat men (μεγιστᾶνες)\nRev., princes. See on Rev 6:15.\nBy thy sorceries (ἐν τῇ φαρμακείᾳ σου)\nSee on Rev 9:21. Rev., more literally, with thy sorcery.\nWere deceived (ἐπλανήθησαν)\nOr led astray. See on Mar 12:24.']"	['<urn:uuid:bd5aa440-4f7a-4620-b539-ecef51ca8e49>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	9	66	2214
50	What are security standards' benefits and wireless transmission risks?	Security standards provide clear requirements for equipment designers and give purchasers confidence in their investments through certification. However, wireless transmission poses risks as signals can be intercepted since they travel through the atmosphere, diffuse in all directions, and can pass through physical barriers, making them vulnerable to hacking and data theft.	['There is little doubt that compliance with minimum security certification requirements is important, whether covering the security of card data or providing a measure of the physical resistance from attacks to the ATM security enclosure.\nPCI DSS (Payment Card Industry Data Security Standard) provides a clear and well-documented set of requirements with the primary objective of the protection of card data being processed and stored. Physical security requirements focused on the resistance to attacks against the safe or ATM security enclosure are well-covered by standards such as UL291 (Underwriters Laboratories) or the various CEN (European Committee for Standardization) standards.\nStandards are valuable to equipment designers and suppliers in that they provide some clearly defined rules and requirements that they will design their equipment to meet. Designing to a documented and internationally recognized standard helps suppliers have confidence that the product they supply has at least the same protection level (and often cost-base) as their competitors. The requirement for independent testing and certification of new products and modifications to existing products also provides an objective check that vulnerabilities have not been mistakenly introduced.\nFor purchasers and deployers of equipment, certificated proof that the product being procured meets, at a minimum, an internationally recognized level of security, helps provide confidence in their investment. By specifying compliance with a minimum-standards requirement during the selection process for new equipment is a primary filter for what will and what will not be considered suitable to carry their brand name. Compliance with a particular standard is also often used as a deciding factor when considering the insurance costs for an asset.\nMany standards are created and debated by a large group of experts, each of whom is a stakeholder whose own organization or discipline will be impacted in some way by the final details that are specified in the published standard. While this structure helps make sure that the broadest consideration is given to often conflicting needs, it does often create a time-lag between the desire to specify protection against a certain threat and final acceptance and publication of the standard.\nThe reality of criminality is that new attack techniques are constantly being invented and rapidly deployed to exploit weaknesses in the security of products and systems.\nAs this article is being written, there is a wide array of physical attacks ongoing against ATMs, proving that the criminal fraternity pays little notice to the label attached to the machine confirming it meets the latest physical security standard. The same can be said for the much-publicised card data compromise attacks against major card processors and personal information thefts from well-known service providers.\nDoes this mean that the industry is expending money, time and resources to create, design to, test against, purchase and certify compliance for no return?\nWhat must not be overlooked is the number of criminal attacks that have failed, perhaps even at the concept stage, for the simple reason that the potential target was seen to be protected to the extent that the expected return was outweighed by the risk to the potential perpetrators.\nThere is, and always will be, an absolute need to have internationally approved minimum-security requirements in the shape of measurable and certifiable standards.\nHowever, to increase confidence in the security of your assets, whether you are the supplier or the deployer, assessing current and emerging threats in the real world, in real time, must be a prioritised and ongoing process.\nThe above article was written by Douglas Russell, DFR Risk Management Ltd.', 'By Mohammad Bajwa, PhD, CPHIMS, CPHI, RHIA, MCSE\nWhile it has only just emerged this century, the use of mobile health technology (mHealth) in the delivery and management of healthcare is gaining traction due to the global implementation of health information technology. This is exemplified mostly by electronic health record (EHR) systems, wherein patients’ health information resides on computers and travels on computer networks. Normally, the information stored on computers is accessed through wires and is limited to places where such infrastructure exists. With the advent of wireless technology, electronic health information can be accessed by anybody, anytime from anywhere and on any mobile device as long as wireless connectivity is available. Despite this advantage, the security of health information stored on and accessed through mobile devices is cause for public concern. Wireless communication is prone to hacking, and mobile devices, given their size and value, increase the chance they will be lost or stolen.\nmHealth is defined as the use of mobile devices (mDevices) in the practice of medicine. These include mobile/cellphones, iPads, tablets, personal and notebook (laptop) computers, personal digital assistants (PDAs), and similar other devices that use wireless technology to access health information networks. Such devices often use radio waves for communication either through central access points (hot spots) or satellites. Of these, mobile phones have emerged as the greatest mobile technology gadgets.\nSince wireless signals travel through the atmosphere, diffuse in all directions, and can pass through most physical barriers, they are liable be intercepted. The intent of this article is to review wireless technology, its use in healthcare, its security issues, and best practices to make safe use of this novel technology for the delivery and management of healthcare.\nTypes of mHealth Security\nBefore discussing the security of mHealth, it is appropriate to enumerate four types of information security requirements: physical, network, application, and user security.\nPhysical security implies the security of computers, network hardware, and storage media against intruders and natural disasters. The most common mitigative security measures for this purpose includes using ID badges and biometric authentication for entry to premises, locking computers with desks, and regular backups for data restoration in case of any human-made or natural disasters.\nNetwork security denotes the protection of data and information residing on computers and in transit over the network/internet through techniques like encryption (encodes data, rendering it unreadable by unauthorized persons) and technologies such as firewalls (controls both inbound and outbound data based on the nature of traffic and source); virtual private networks (makes private data communication channel in the public network such as internet); network intrusion detection (notifies the network administrator of any intrusion by email, text message, alert); and intrusion detection and prevention systems (blocks intruder and reports to network administrator).\nApplication security requires validation of services accessing the information, level of information access and use permissions, software patches and updates, software logs, and input validation techniques (best defense against malware/spyware injection).\nUser security is a three-step process that entails user identification through user ID, authentication through password, biometric or two-factor authentication (verifies user), and authorization through permissions (what the user can access and do).\nData Transmission Media\nData and information on the network travels either on wires (wireline transmission media)—the most common being the copper cables (network and coaxial or TV cables) and fiber optic (FO) cables—or wirelessly through the atmosphere. For the discussion of data security, it is pertinent to examine how data travels on these two types of transmission media and their security vulnerabilities.\nWireline Transmission Media\nThe two types of wireline transmission media are copper cables and fiber optic (FO) cables. Copper cables transmit data through voltages (electrical signals), called electromagnetic (EM) waves, as pulses (5 volts representing 1) or no pulses (representing 0). The data from these cables can be stolen by placing a device close to the cable, which can sense pulses and covert them back to 1s and 0s. To avoid this, some copper cables are shielded (covered with aluminum foil) to stop pulses from being detected externally. One typical example of this type of cable is TV (coaxial) cable that not only shields data being braided but can also transmit it at multiple voltage levels, thus catering to the needs of TV, internet, and digital phone requirements simultaneously. Contrary to the copper cables, FO cables are made of glass or highly transparent plastic and transmit data as light pulses. Its twofold benefits include data transmission at almost the speed of light (several times faster than copper cables) and data security, as light pulses cannot be detected externally. Besides being fast and secure, FO cables transmit data in multiple frequencies, enabling several data channels in one cable (video, internet, phone). Verizon FiOS is one such example.\nWireless signals are also EM waves, but unlike the wireline media, they travel through the air or atmosphere at different frequencies (amplitudes). They are advantageous in areas where wired networks cannot be set up (old and historical buildings) or are required temporarily (training or classroom setup). Because the wireless signals radiate in all directions and can pass most physical barriers, they are very valuable in hospital environments or healthcare organizations with mobile workforces. However, they are the least secure, as they can easily be intercepted by monitoring devices, and data can be interrupted, interjected, modified, stolen, rendered unusable, or even destroyed.\nMobile Technology (mTechnology)\nmTechnology is based on wireless technology that moves with the user. It is a portable two-way communication network and includes internet-enabled devices like smartphones, tablets, notebook computers, and navigation devices. mTechnology touches all aspects of our daily lives from starting the day with the alarm clock in your phone; managing the day through your calendar, reminders, and notes; communicating through emails, texts, social media; and accessing online information like maps, books, magazines, files, photos, and videos. The common four types of mTechnologies are:\n- Cellular or radio networks: Use distributed cell towers that enable mobile devices (cellphones) to switch frequencies automatically and communicate without interruption across large geographic areas.\n- Packet switching technology (5G network): Organizes data into packets for transmission and reassembles packets into information at destination.\n- Wi-Fi: Radio waves that connect devices to internet through localized routers.\n- Bluetooth: Connects devices over short distances using short-wavelength radio waves.\nMobile Health (mHealth)\nmHealth is the use of mTechnology in healthcare. The World Health organization (WHO) defines mHealth as the “use of mobile and wireless technologies to support the achievement of health objectives,”1 while the National Institutes of Health (NIH) describes it as “use of mobile and wireless devices to improve health outcomes, healthcare services, and healthcare research.”2\nThus, mHealth is the practice of medicine and healthcare over mobile devices, like smartphones, tablets, and iPads, such as eVisit (only mode of remote healthcare delivery service during COVID-19 pandemic). Although traditionally used for wellness management, their use for healthcare both by patients and providers has increased exponentially.3\nCurrently, mHealth technologies are being used for patient monitoring, patient-provider communication, telehealth, and e-health. Although the lines between telehealth (also referred to as telemedicine) and e-health are blurred, they are two different technologies. E-health is the electronic communication of information for improving patient’s health, while telehealth uses video, smartphones, or any wireless tools or telecommunications technology (network/internet) for specific healthcare delivery and is named after the type of healthcare service: telenursing, telepharmacy, telerehabilitation, teleradiology, teletrauma care, telepsychiatry, telepathology, and teledermatology. Telehealth requires license and an infrastructure to practice, while e-health has no such requirements. Both became routine during COVID-19 pandemic.\nThe global health data breaches in 2020 increased to 24.5 percent against financial services breaches (7.3 percent), once being the reverse.4 Thus, it is pertinent to elaborate on some mHealth security issues to understand their specific security vulnerabilities.\nMobile devices use air interface to communicate and transmit data through the atmosphere. The wireless medium is broadcast (signals diffuse in all directions), and signals can pass through physical barriers (e.g., walls, roofs, and concrete). They are vulnerable to active (through injecting, deleting, altering the message) or passive (redirecting the information) attacks. The signals traveling through the atmosphere can be intercepted, modified, destroyed, hacked, and rendered unusable through ransomware. Being portable, mobile devices are small and lucrative and can be lost or stolen along with data, have low processing power to handle encryption, and can use public unsecure Wi-Fi. Bring your own device (BYOD) policies poses additional risks of healthcare cybersecurity, as healthcare workers can use their own devices to access organizational network, access patient data, and enter medical orders. BYOD policies are advantageous because of higher productivity, as the providers can access health information from anywhere and anytime; they increase job satisfaction by supporting flexible work arrangements; and they increase effectiveness due to more comfort and speed with the use of people’s own devices. However, they suffer from several disadvantages, like data breaches due to lost or stolen personal devices, lack of firewall and anti-virus software, dearth of encryption power, and increased IT costs for supporting personal devices.\nHIPAA and mHealth\nHIPAA requires security of the protected health information (PHI) through its Security Rule that mandates the security of health information through a three-pronged approach: administrative (policy-based), physical (physical access to facility, workstation, and storage devices), and technical (technological requirements to control access to information).5 mHealth devices have potential HIPAA Security Rule compliance issues such as physical loss or theft of devices, data transmission over unsecured Wi-Fi, unencrypted text messages/emails, inadequate or lack of authentication protocols, and poor adherence to BYOD policies and procedures.\nBeside the potential to have HIPAA security compliance issues, other specific mHealth device issues include interference with implanted medical devices (pacemakers) and their use in monitoring vital signs and supporting life-threatening and critical care situations that use a part of electromagnetic spectrum, like mobile devices. They also suffer from diagnostic interpretations due to small screen size, which may conceal some crucial details in EKG, ultrasound, MRI, and X-rays.\nBest Security Practices for Securing mHealth Devices\nAlthough no technique or technology can provide fool-proof security for the data contained in mHealth devices or accessed through them, some measures can minimize the security risks. These include implementation of strong user authentication (biometric or two-factor authentication), automatic lock after excessive number of incorrect logins, remote wiping when a device is lost or stolen, employing encryption for conversations (emails, text messages), developing an application policy for BOYD devices, regular updating to keep vulnerabilities mitigated, and installing security programs/antivirus, as being networked they are apt to all sorts of malware attacks.\nInternet of Things in Healthcare (IoHT)\nYet another breed of mHealth devices are Internet of Healthcare Things (IoHT) or Internet of Medical Things (IoMT) devices, which is the use of Internet of Things (IoT) in healthcare. IoHT devices are sensor-based interconnected devices used for tracking assets and resources. In hospitals, they can be used for locating patients, medical staff, and visitors (called smart hospitals), and collection and integration of health data (generation, collection, and communication of health data through wearable devices).6 Other benefits of IoHT devices include savings in medical cost, reduction in medical errors, improved patient experience, manageability of medical drugs and adherence, better control over wastage in healthcare sector, and better outcomes of medical treatments.\nSecurity Mitigation of IoHT Devices\nMitigation of security for IoHT devices include both technological and administrative measures. The technological mitigation measures comprises use of blockchain technology to protect data in IoHT devices, enable IoHT devices to use authentication to validate user identity and access privileges, enable IoHT devices to use encryption for all health-related communication, enable integrity on IoHT devices to verify devices to ensure that they are unaltered and uninterrupted, and ensure IoHT devices are patched and updated to avert any vulnerability.\nThe administrative mitigation measures comprise using the principle of least privilege for required actions and types of communication, logging all user activities and events and monitoring them regularly for unusual activities (duplicate device ID or elevation in privileges), reviewing device data regularly to identify unusual trends or patterns, implementing security best practices associated with protecting and securing sensitive data, and conducting formal education, training workshops, certifications, and participation in mobile security conferences.\nFuture of IoHT\nIoHT devices have a great future in healthcare applications, especially through the integration of artificial intelligence with IoHT in the form of ingestible sensors (ePills, which are pill-sized devices to monitor internal physiology and act as diagnostic devices sending medical information and images to outside connected devices); nano-devices to monitor human physiology and deliver drugs to targeted areas like cancer cells; connected lenses that would determine tear glucose and eyes diseases; and blood clot monitoring sensors to avert heart attacks. Their potential healthcare implications include smart hospitals, virtual clinics, microsurgery, vital sign monitoring systems (which can analyze real-time data inputs from critically ill intensive care unit patients), activity trackers for heart patients to collect lifestyle information, and fitness wearables, to name a few.\nIn view of the significance of mTechnology in healthcare, the Journal of Mobile Technology in Medicine is published to disseminate the mHealth research results.\n- World Health Organization. https://www.who.int/goe/publications/goe_mhealth_web.pdf\n- National Institutes of Health. https://grants.nih.gov/grants/guide/pa-files/PAR-14-028.html\n- Phaneuf, A. “How mHealth apps are providing solutions to the healthcare market’s problems” (2019). Retrieved form What Is MHealth? Apps, Examples & Mobile Health Industry Trends (businessinsider.com).\n- US Department of Health & Human Services. “A Retrospective Look at Healthcare Cybersecurity.” 2020. https://www.hhs.gov/sites/default/files/2020-hph-cybersecurty-retrospective-tlpwhite.pdf\n- US Department of Health & Human Services. Summary of the HIPAA Security Rule Guidance Portal. https://www.hhs.gov/guidance/document/summary-hipaa-security-rule-1\n- Bajwa, M. “Opportunities and Challenges for Internet of Things in Healthcare (IoHT).” International Journal of General Medicine and Pharmacy (IJGMP): Vol. 9 (2020): 1-4.\nMohammad Bajwa ([email protected]) is a professor of health informatics at the University of Maryland-Global Campus.\nLeave a commentSyndicated from https://journal.ahima.org/mobile-health-mhealth-security-matters-and-mitigation/']	['<urn:uuid:b7ffb06b-88ac-4750-897e-74250f685afa>', '<urn:uuid:556cfcd0-022c-4723-a4e5-a1f05c8b063e>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T15:30:12.136021	9	51	2871
51	email advertising newspaper ads reach customers	Email marketing and newspaper advertisements are different approaches to reaching customers. According to Document 2, email marketing delivers one of the highest returns on investment (ROI), around 122%, and is cost-effective since businesses only need to pay for email marketing software. Meanwhile, as shown in Document 1, newspaper advertising has been traditionally effective, especially in Japan where the Yomiuri newspaper reached 12.77 million circulation combining morning and evening editions. Newspaper ads are particularly effective at attracting immediate attention through flashy, appealing content and funny slogans, while email marketing works best for nurturing relationships with existing leads and customers.	"['So, as we’ve established, film fliers are ephemeral. Unless they happened to end up in the hands of collectors, they are supposed to be discarded after being distributed in conjunction with a film’s release. However, there is more ephemeral media than just film fliers: film advertisements within newspapers, for example, when a film is brought to the public, a variety of promotional methods could be used. Newspaper advertising is one of the traditional methods to promote films among a broad range of audiences.\nNewspapers report up-to-date news and information. After reading through a paper, most often, we immediately discard it. The advertisements are a sub-content of the paper, but can be more significant, for example, especially when a full page is designated for it. The newspaper’s nature, immediacy of information makes film advertisements in it a lot different from the contents of regular film fliers.\nWith this in mind, we will look at some interesting newspaper film advertisements from the Makino Collection.\nNewspaper advertisements are different because they include not only film images, but also fresh eye-catching copy, which cannot be seen in typical film ephemera. Since newspapers are not meant to be preserved over time, the contents of newspaper advertisements need to be up-to-date, fresh and entertaining. The big difference, which is also the most entertaining thing about them, is that film advertisements will be flashy, appealing, and include funny slogans with weird wording, in contrast with the more polished look of film fliers, which usually features an elaborate design.\n“Well-received! Horrendously popular! You should hurry!” (Goldfinger, 1965)\nThe mission of newspaper film advertisement is to instantly attract readers to movie theaters. Therefore, the small section of newspaper needs to be extremely appealing. Thus, the contents vividly reflect the box office success of a film, and will often announce a second-chance for viewer’s to see an extended film’s run.\n“The century’s masterpiece which were appreciated 650,000 viewers over 9 months will only be available until 9/20” (My Fair Lady, 1965)\n“If you have already watched it, you should feel the deep emotion again–” (My Fair Lady, 1965)\nAs a result, this promotional information in newspaper advertisements compels newspaper readers to go to movie theaters.\n“If you have not seen it, hurry!” (My Fair Lady, 1965)\nSince most films are generally released on Saturdays in Japan, the paper’s evening edition on Fridays is usually full of movie advertising. The advertising sections on Saturday and Sunday are full of eye-candy for film fans as well.\nWhat is the status of newspapers in Japan? To what extent can these film advertisements influence readers in Japan?\nNewspaper advertising is a true expression of a culture of one country, a life record; it is a precious record of economic evolution history and cultural history.\n(日本新聞広告史 : 電通創立四十周年紀念/Nihon shinbun kōkushi: Dentsu sōritsu yonjisshūnen kinen, 1940)\nFor people living in Japan, home newspaper subscription is an old tradition.\nThere are five main national newspapers in Japan: Asahi, Mainichi, Sankei, Yomiuri, and Nikkei. They are printed twice a day and in several local editions (The edition of Japanese newspapers that can be obtained in the U.S. is an international edition).\nThe influence of newspaper is proportionate to the subscription numbers. One of the national papers in Japan, the oldest paper, Yomiuri Shimbun, was founded in 1874. The Yomiuri reached 12.77 million (figure: Audit Bureau of Circulation- Japan) combining morning and evening editions for the first half 2014, and is credited with having the largest newspaper circulation in the world. In comparison, the top U.S. newspaper, the Wall Street Journal circulated 2.38 million copies in 2013 (Alliance for Audit Media); this number includes 0.9 million digital editions. Since the population in Japan is 126.9 million and the U.S.’s is 321.4 million (The U.S. Central Intelligence Agency, 2015 est.), you can infer how much a dairy newspaper influences Japan.\nIf you visit Japan and use public transportation during rush hour, you will witness a number of businesspersons reading newspapers on trains, buses, and on the subway during their commute. It is an ordinary scene in a Japanese morning.\nIn the 1960’s and 70’s, newspapers in Japan developed dramatically. In the midst of the Japanese Post-War Economic Miracle, people sought and craved a better lifestyle. For this reason, newspapers soon became a major part of people’s daily life. The competition among national papers was brisk. The 1960’s was also called cinema’s golden era; the culture of film advertisement in newspapers came into full bloom as an effect of this era.\nThe Makino Collection includes a number of newspapers, including the film advertisements from the 1960’s in neat scrapbooks, the golden era of cinema in Japan and newspaper as a medium itself. They are extremely precious, and highly worth looking at. Not many people collect this very ephemeral object, since it requires time and effort to preserve them properly. Therefore, you will greatly appreciate this specific form of film advertising within the Makino Collection.', 'The success of your small business depends on your marketing. Basically, marketing covers everything that has to do with getting and retaining customers. This seems to be the primary challenge among small business owners. According to a survey from Capital One, 41-percent of small business owners have trouble identifying and reaching prospects. Once you’ve solved this challenge, you’ve overcome the main hurdle of running and keeping a profitable business.\nFor small business owners, it helps to lay out solutions in the form of a marketing plan. With a marketing plan, you can easily evaluate, schedule, and measure your tactics. It includes your sales goals, your description of your target customers, as well as the small business marketing strategies you\'ll use to reach these goals and find these customers.\nIf you’re just getting started with your small business marketing plan, here are some tutorials that can guide you through the process:\n- MarketingHow to Write a Lean Marketing PlanCeline Roque\n- CustomersHow to Define a Target Audience (For Your Marketing Plans)Celine Roque\n- GrowthHow to Plan an Effective Small Business Growth StrategyAndrew Blackman\nIn this article, we summarize ten of the best marketing strategies for small businesses. We\'ll also point you to some other resources that may help you as you put together your own small business marketing strategy.\nThe Best Marketing Strategies for Your Small Business\nThe following marketing strategies are ideal for the typical budget range and goals of small businesses. Here\'s a list of small business marketing strategies that work:\n1. Email Marketing\nEmail marketing involves gathering the email addresses of leads and customers, sending them emails regularly, and getting them to open those emails or click the links in them.\nThis strategy is suitable for small businesses because of the minimal cost — usually, you only need to pay for the email marketing software — and the high returns. Email marketing is the strategy that delivers one of the highest returns on investment (ROI), even more than social media or direct mail. Statistics from the Direct Marketing Association show that there’s roughly a 122-percent ROI on email marketing. In other words, it\'ll be very profitable for a small business to include email in their marketing plans.\nBecause email works best when you already have a small following, it’s best for nurturing relationships with leads and customers, converting leads into sales, cultivating recurring customers, and increasing sales. If your primary goal is to increase brand awareness or introduce your business to potential prospects, it\'s best to try other small business marketing strategies first.\nIf you don’t have list of leads yet, start building your email list. But if you\'ve already gotten started with building your email list, you can use the following guides to improve your email strategy:\n- Email MarketingHow to Create an Email Marketing PlanDavid Masters\n- Email MarketingHow to Create an Effective Autoresponder SequenceDavid Masters\n- Email Marketing10+ Different Types of the Most Effective Marketing EmailsCeline Roque\n2. Content Marketing\nUnlike directly advertising your products and services, content marketing aims to reach your target audience through informational or entertaining content. The idea is that when your target audience is attracted by this content, they’ll learn about your business as a result. The content you use can come in any of the following formats:\n- case studies\n- and website copy (such as your homepage text or ""About Us"" page text).\nBecause content marketing doesn’t directly ask for the sale, it works best primarily as a brand engagement strategy rather than a sales strategy. Research from Content Marketing Institute found that while nearly 80-percent of marketers could show that content marketing increased audience engagement, only 58-percent could show that it increased sales. By complementing your content marketing strategy with more direct approaches like email marketing or advertising, you’ll be able to ensure that your branding efforts also help your sales efforts.\nYou can get started on your content marketing plan using these tutorials:\n- Content MarketingWhat Is Content Marketing?Andrew Blackman\n- Content Strategy7-Step Content Marketing Plan: A Quick-Start GuideLauren Holliday\n- Content MarketingContent Marketing Metrics: How to Measure Your ROIAndrew Blackman\nFor businesses, blogging means regularly posting new updates to your blog or website. Although blogging is technically part of content marketing, it often requires a separate plan since it’s a recurring project. Blogs need to be updated regularly, unlike whitepapers, case studies, or your “About Us” page.\nBecause you regularly post new content on your blog, it’s typically best for building brand awareness, recall, and loyalty. A blog can get the attention of new prospects who will hear about your business for the first time, as well as remind current prospects and customers of your business. These branding efforts then bring in more leads. Perhaps this is why, regardless of company size, According to HubSpot research businesses that blog more tend to get more leads.\nStart your business blog right by applying the following ideas:\n- List the common problems your target customers face, especially the ones that you help them with. Then, write a blog post about each problem and present a simple solution. For example, an online store that sells aromatherapy oils might list the top five oils that help reduce stress.\n- If your business already has a blog, look at your existing blog posts. Do your individual blog posts have images every 100 words? Research from Buzzsumo found that articles with images every 75 to 100 words get twice the number of shares than articles with fewer images. The more shares your blog posts get, the more people get to see them.\nIf you want to take your business\' blog to the next level, the following guides can help:\n- BlogHow to Start a Blog for Your BusinessAndrew Blackman\n- BlogHow to Create Unique Content for Your Authority BlogTom Ewer\n- Marketing5 Easy Tips for Getting 100,000 Visitors to Your Blog PostsCarol Tice\n4. Social Media Marketing\nUsing sites like Facebook, Twitter, Instagram, LinkedIn, Snapchat, and Pinterest to market your business means that you\'re using social media marketing.\nOne of the challenges of social media marketing is that it can be difficult to know if it\'s working. An Econsultancy study shows that as many as 60-percent of marketers report having difficulty tracking ROI. In fact, marketers see social media primarily as a driver of brand engagement. Since you can include photos, graphics, video, sound, and emojis in your social media posts, this makes it a good channel for expressing your brand values in a variety of ways.\nBut this doesn\'t mean that making sales has no place on social media. In fact, Pinterest drives a lot of sales traffic to ecommerce sites. Research from Shopify shows that 93-percent of online shoppers use Pinterest to plan purchases, and Pinterest is also the second most popular source of website traffic to online stores. Facebook and Instagram also have online shopping features, so it\'s just a matter of using the right social media site and the right features for increasing your sales.\nPlan your social media strategy right with the help of the following guides:\n- Social MediaHow to Use Social Media for Small Business (Beginner\'s Guide)Brenda Barron\n- Social MediaHow to Optimize Your Social Media ProfilesBrenda Barron\n- Social MediaHow to Track Your Social Media ROI (Get Better Results)Brenda Barron\n5. Pay-Per-Click Advertising\nPay per click (PPC) advertising is simply a form of online advertising where you pay for the clicks you get on an online ad. Usually, these are the ads you see when you type a keyword into a search engine and look at the results. But PPC ads are also present in other websites and even in some social media sites. Google Adwords and Bing Ads are two examples of networks that allow businesses to buy PPC ads.\nThe success of your PPC ads relies on choosing the right keywords, bidding the right amount, and having high-converting copy. Because the cost of these ads is determined by the number of clicks on the ad, the best place for them in your small business marketing plan is for gathering leads or making sales. Here are some approaches you can use with your PPC ads:\n- Set up your PPC ad so that it leads directly to a landing page or a squeeze page designed to gather leads.\n- You can also direct your PPC ads to specific product pages when you\'re using keywords that are targeting customers who are ready to buy.\nIf you want to get started on PPC ads, these tutorials on Google Adwords can help you become more familiar with how it works.\n6. Posters and Flyers\nDisplaying posters or giving away brochures and flyers is essential for small businesses that need to market within a specific location. This is because they\'re typically displayed and distributed in a particular area. For small businesses where location matters, flyers and posters should be an essential part of their small business marketing plan.\nOne challenge that comes with this approach is that it can be difficult to measure success. To overcome this, make sure your incoming leads from flyers and posters are easier to track. You can include coupon codes in the content or, in the case of flyers, you can even include tear-away paper coupons. That way, you can tell if the cost of the design, printing, and distribution was worth the returns.\nTo get started on flyer marketing, you can use the following resources and templates:\n- Flyers10 Design Tips to Make a Professional Business FlyerGrace Fussell\n- Marketing20 Business Flyer Templates With Creative Layout DesignsMarc Schenker\n- Marketing20 Best InDesign Brochure Templates - For Creative Business MarketingMarc Schenker\n7. Print Advertising\nOne of the more traditional marketing approaches, print advertising is simply about buying ad space on printed media such as newspapers, magazines, newsletters, direct mail, or yellow pages. While this might sound “old school” compared to online marketing strategies, there’s a reason why they exist — they’re still effective.\nResearch shows that print newspapers command the most attention when compared to other media such as short online videos, websites, or social media. Even among millennial consumers, a study found that 82-percent read direct mail and 49-percent use print coupons. Given these findings, print is clearly still relevant.\nSince print advertising tends to cost money, it’s best to make sure that they lead directly to measurable results, such as an increase in sales, customers, or leads.\n8. Referral Marketing\nAs the name implies, referral marketing is about getting new leads and sales from referrals. These could be from your existing customers or other people who simply know about your business. If you’ve ever participated in a “tell a friend” program, where you get a discount or a freebie by recommending a product or service to your friends, then you’ve participated in a referral marketing campaign.\nReferral marketing campaigns are very effective for making sales and getting new customers. Research from Nielsen shows that 77-percent of consumers are more likely to buy based on recommendations from family and friends, making referrals more persuasive than paid advertising and expert recommendations. It also drives five times more sales than paid ads according to a WOMMA study.\nHere are some examples of referral marketing campaigns:\n- In 2016, Tesla ran a referral marketing campaign that allowed customers with the most referrals to get invites to a tour of the Tesla factory, invites to a grand opening party, discounts, or get a new Tesla model that wasn\'t available to the public yet. These referrals earned 42 times more on every dollar spent on the campaign.\n- Harry\'s, which sells shaving products, was able to gather 100,000 leads from a referral campaign for the launch of their business. Leads who signed up on their homepage could get free products depending on the number of new leads they referred. Five referrals entitled them to a free bottle of shave cream, while fifty referrals entitled them to a year\'s supply of new blades.\nYou can use the above ideas and change them depending on the needs of your small business. With or without incentives, as long as your customers are happy to recommend your business to family and friends, your business will be in great shape.\n9. Event Marketing\nSmall businesses, especially those that have a storefront or serve a local community, can also benefit from hosting simple events that appeal to their target audience. This approach is known as event marketing. The idea is that by creating an event, your target customers will be gathered together in the same space. In this space, they can buy your products or at least become aware of your brand. Here are some examples of event marketing:\n- Apparel shop Miss Me regularly goes on a “summer tour” where they\'ve got a mobile roadshow of their products. The roadshow includes a photo booth, a contest, and a charity component.\n- Lean Cuisine’s “#WeighThis” campaign asked women on-location to measure their accomplishments instead of their weight. These were then painted on a weighing scale and display them in an art installation in Grand Central Station.\n- Sensodyne hosted “The Great Sensitivity Test,” which had a variety of activities for potential customers, including dental exams, games, and product samples.\nWhile the above examples might seem costly, you can create more scaled down events for your small business. The important thing is that you get your target customers in the door, and that your event is noteworthy enough to make it to local papers and blogs. For example:\n- A stationery store might invite a local artist to teach a calligraphy class in their store, with relevant products featured prominently on the shelves.\n- A dog groomer might set up a booth giving away free dog treats for people walking their dogs. The groomer’s flyer or business card could be included in the packaging of the treats.\nBecause of the costs and planning associated with events, it’s best to make sure that you get to capture leads or make sales. It\'ll help to measure any changes in sales or customers in the area as a result of the campaign.\n10. Business Partnerships\nTo make your small business stronger, sometimes it’s best to look for other small businesses and band together. This could be in the form of cross promoting each other’s products, bundling some of your products together, or co-hosting an event for your target customers.\nFind your prospective partners by starting with your target audience. Apart from your products, what other products are they likely to need? List these products and look for small businesses that provide them. These small businesses are the natural partners, and you can start approaching them to see if they’ll be open to a mutually beneficial promotion or event. Here are some examples of these natural partnerships:\n- If your business sells organic sunscreen, you can find a business that makes eco-friendly swimwear and bundle your products together for a summer promotion.\n- A lawn care business might want to partner with a cleaning service and offer a joint full home spring cleaning service.\n- A wedding photographer can partner with other wedding-related businesses such as a baker, a printer, and a wedding accessories provider and host a bazaar for brides-to-be.\nBy pooling together your resources, audiences, and skills, you can help each other grow.\nFind the Best Marketing Strategies That Work for Your Small Business\nIt\'s important for small businesses to find and test different small business marketing strategies to see which ones work best for attracting prospects and increasing sales. While you don\'t have to use all the above techniques, you can start by pursuing on from the above list that best fits your budget and goals. With the right strategy, your small business can be bringing in leads and sales just like a bigger business would.']"	['<urn:uuid:4b3f4005-d538-47bc-8cec-bf7e263ef5a6>', '<urn:uuid:922ec717-51bd-4385-9271-7f9e028e5dd1>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	6	98	3466
52	sakai hoitsu ruth asawa inspiration from nature compare	Both artists drew inspiration from nature but in different ways - Sakai Hoitsu utilized the Yamato-e style to reduce nature to decorative shapes, as seen in his screen paintings of cranes, while Ruth Asawa was inspired by natural forms like spiral shells, insect wings, and spider webs which she translated into three-dimensional wire sculptures.	"['Ruth Asawa’s story is one of extraordinary creativity in the face of adversity. Born in California in 1926, her life was shaped, like her art, by social and political impositions; by unjust restrictions on her liberties and supposed inalienable rights.\nAs a teenager in the early 1940’s, Asawa and her family were sent by Executive Order to an internment camp with approximately 120,000 fellow Japanese Americans. Under the tutelage of professional artists also held captive, Asawa began to make art, finding in it a sense of liberation at a time when the government had stripped her of her civil liberties.\nDespite the suffering she endured, Asawa exhibited great humility and, 50 years after the event, felt little resentment, saying: ‘I hold no hostilities for what happened; I blame no one. Sometimes good comes through adversity. I would not be who I am today had it not been for the internment, and I like who I am.’\nRuth Asawa, Untitled (SF.003, Undulating Parallelograms), circa 1951–1952. Ink and gouache on board. 27 x 27 in. (68 x 68 cm.)\nRuth Asawa, Untitled (WC.134, Self Portrait), circa 1960s. Ink on paper. 13 x 12½ in. (33 x 31 cm.)\nFollowing the close of the Second World War, in 1947, Asawa set off on a tour to Mexico, where she was captivated by the looped-wire baskets used in markets to sell eggs and other produce. Intrigued by the material’s potential as an artistic medium, she began to loop and twist wire, using continuous lines to create three-dimensional forms that appeared to play with space in which they hung.\nAsawa continued her formal artistic education at North Carolina’s Black Mountain College, studying alongside a group of artists who would grow to be amongst the 21st century’s most influential. ‘The late 1940s and early 1950s at Black Mountain were the now legendary institution’s most glorious years,’ comments Robert Storr. ‘Refugee Bauhaus master Josef Albers was at the centre of Black Mountain’s program, bringing diverse talents to the remote community both as teachers and as students.’\nBlack Mountain’s illustrious teaching staff included Merce Cunningham and Buckminster Fuller; Willem and Elaine de Kooning would later join the summer faculty, along with Franz Kline, and artists Robert Rauschenberg and Cy Twombly were amongst the student body. The institution’s Modernist and Bauhaus currents became a visible influence in Asawa’s practice, with the Bauhaus notion of transparency — allowing the viewer to conjure imagined spaces — a prominent aspect of works that cut and shape space.\nInstallation view, Ruth Asawa: Objects & Apparitions, Christie’s, 6–31 May 2013. Artwork: © Estate of Ruth Asawa\nOther aspects of the school influenced more than just Asawa’s visual language: Whilst there, the artist churned buttermilk daily, the activity forming part of a communal commitment to working for the ‘greater good’. The creed was one that Asawa continued to live by, even after leaving Black Mountain for a new life in San Francisco. Christie’s specialist Jonathan Laib comments: ‘Her art, her family, her garden, her activism—each seemingly grew to fuller capacity due to the existence of the other, not in spite of it’.\nFor Laib, Asawa’s works represent a ‘new sculptural invention,’ leaving flat canvas to explore the idea of ‘drawing in space’. ‘The wire served a purpose in assisting to explore space within and without,’ says Laib. ‘It was an “experiment” that created sculptures of a most articulate and stunning visual revelation…Stringing them from the ceiling in defiance of gravity revealed yet another great leap of faith and imagination’.\nRuth Asawa, Untitled (WC.046, Pink Orchid), 1995. Watercolor and graphite on paper. 10¼ x 10 in. (26 x 25 cm.)\nRuth Asawa, Untitled (WC.145, Hibiscus 2). Watercolor on paper. 9¼ x 6½ in. (23 x 16 cm.)\nDiscussing her practice in 1995, Asawa said: ‘My curiosity was aroused by the idea of giving structural form to the images in my drawings. These forms come from observing plants, the spiral shell of a snail, seeing light through insect wings, watching spiders repair their webs in the early morning, and seeing the sun through the droplets of water suspended from the tips of pine needles while watering my garden.’\n‘Throughout her life Asawa drew,’ says Laib. ‘Many of her most celebrated drawings were created during her stay at Black Mountain College or slightly thereafter during her early days in San Francisco. Overlooked are her botanical and figurative drawings made from direct observation. Seldom shown, these works deserve a closer look and greater consideration within the larger context of her artistic life’.\nFeaturing works from across artist’s career, Line by Line explores the interplay between Asawa’s two dimensional and sculptural work, whose concern with space has prompted comparisons to the Spatial Concepts of Lucio Fontana and the revolutionary hanging mobiles of sculptor Alexander Calder.\nRuth Asawa, Untitled (SD.070, Tied Wire Tree with 5-pointed Star in Center). Ink on rice paper. 6 x 6 in. (15 x 15 cm.)\nRuth Asawa, Untitled (S.330, Hanging Asymmetrical Tied Wire Eight-branched, Closed Center, Free-Form Based on Nature), 1965. Bronze wire. 22 x 27 x 27 in. (55 x 68 x 68 cm.)\n‘What is so surreal about Asawa’s work is the very fact that her art was every bit as outrageous as that of other, equally obsessive artists, such as Yayoi Kusama for example, but she maintained a nuanced life of modest responsibility,’ comments Laib. ‘Imogen Cunningham has taken many incredible photographs of Ruth. In nearly all she is working or caught in a rare moment between taping her fingers and bending more wire. She exudes a calm presence in these photographs, but do not be fooled. She was an adamant defender of the arts and arts education’.\n‘Contemplating her oeuvre with even a fraction of the concentration that she devoted to creating it makes our minds supple,’ comments Robert Storr. ‘The time is right for Asawa to return to our attention’.\nRuth Asawa: Line by Line is at Christie’s in New York 9 September – 3 October, and Christie’s in Tokyo 16 October – 6 November. All artwork © Estate of Ruth Asawa.\nFor more features, interviews and videos, visit Christie’s Daily', 'Artists with August Birthdays\nMonthly artists’ birthdays are a good way to introduce you to a variety of artists I actually adore, while contrasting art from vastly different cultures. I’m not going to call it “interesting,” but it is certainly notable that both of these artists died in January, almost one hundred years apart. They certainly left a lot of beautiful art behind!\n|Sakai Hōitsu (August 1,1762–January 4,1828, Japan), Cranes. Two-fold screen, ink, colors and gold leaf on paper mounted on wooden frame, 4\'9"" x 4\'9"" (143.5 x 143.3 cm). © 2018 Worcester Art Museum. (WAM-152)|\nIf you ever want to teach a lesson on positive and negative space, this painting would serve as a wonderful example. Its beautiful composition is also a good example of asymmetrical balance. There were so many wonderful schools of painting active through the Edo Period (1615–1868) in Japan, and they were nourished by the isolation of the Tokugawa dictators, who cut outside contact. This caused Japanese artists to look to lauded styles of past periods. One of the most popular styles that persisted through the Edo Period was the yamato-e, one that had come to full bloom during the Heian Period (794–1185).\nYamato-e (“Japanese painting”) originally flourished in the form of scroll painting, particularly in illustration of literature. It is characterized by decorative surface, contrasting patterns, bright color, and the liberal use of gold leaf. During the Edo Period it became a popular style in screen painting. The screen is called a byōbu, and a two-fold screen is a furosaki (or nikyoku) byōbu. The simplicity of this screen is a striking feature in the yamato-e aesthetic in which nature is reduced to shapes, and is typical of the work of Sakai Hōitsu. Although the birds cast no shadows, they do not really seem to float because they are anchored by the stylized stream behind them. The gold leaf background adds to the decorative effect of the screen, composed of 2"" (5.1 cm) square sheets of beaten gold leaf.\nSakai was the son of a samurai, the lord of Himeji Castle in Hyōgo Prefecture. You probably know what it looks like:\n|Himjei Castle, 1333, 1346, and 1601–1609. Himeji, Hyogo Prefecture, Japan. Photo by Bernard Gagnon. CC BY-SA 3.0.|\nFortunately for us, Sakai shunned the military life and became a painter instead. He was a devoted follower of the Kōrin School of painting, named for its “founder” Ogata Kōrin (1658–1716). This school was sometimes called “Rinpa”: “pa” for school, and “Rin” from Kō(rin). Along with the Tosa and Kanō schools—in both of which Sakai trained—the yamato-e style really flourished up to and through the 1800s. This style is an interesting contrast to the ukiyo-e style with which most Westerners are familiar. Yamato-epainting was favored by the nobility, particularly because of its historical connotations, and was, of course, too expensive for the middle class to afford.\nSakai successfully established a Rinpa school in Edo (Tokyo). He studiously copied many of Ogata’s paintings, often on the back of screens he had painted in his own style. He published two woodblock print albums of Ogata’s work, One Hundred Images of Kōrin and Album of Simplified Seals in the Ogata Style. Talk about hero-worship!\nCorrelations to Davis programs: Discovering Art History 4E: 4.4; The Visual Experience: 13.5\n|George Bellows (August 19,1882–January 8, 1925, US), Gorge and Sea, 1911. Oil on canvas. © 2018 Mint Museum, Charlotte, NC. (MIN-12)|\nI came to be a big admirer of the so-called Ash Can School artists many, many years ago, first when I discovered Twin Lights, Purple Rocks (1915) by John Sloan (1871–1951) at the Worcester Art Museum. I had never, I guess, thought of the Ash Can artists as colorists, but that painting convinced me they were. What further convinced me was when I saw Vine Clad Shore, Monhegan Island (1913) by George Bellows at the Smithsonian American Art Museum. I had never thought of the school as landscape painters, either.\nAfter Vine Clad Shore, I started to note other works by Bellows (and other Ash Can painters) that are infused with an almost Impressionist palette, such as Blue Morning (1909, National Gallery), which is a cityscape. Being a landscape painter, however, I’m so drawn to the landscapes of these artists. I particularly like the broad, fluid brush work of Bellows, no matter what palette he happens to be using. I guess it’s not surprising that they gravitated toward the Impressionist palette, since many of their works—like the Impressionists—lauded everyday city life and were often sketched on the spot.\nUnlike the other members of the group of “The Eight,” Bellows was a member of the conservative National Academy of Design in New York that had rejected “The Eight” members’ works and was the impetus for the creation of the “Ash Can School” in 1908. He was elected in 1909, the youngest person ever elected to the academy. Born in Columbus, Ohio, he studied at Ohio State University and then at the New York Art School, where he studied with American Impressionist William Merritt Chase (1849–1916) and Dark Impressionist mentor of the Ash Can painters, Robert Henri (1865–1929).\nWorks such as Gorge and Sea, above, show the dichotomy of Bellows landscape palettes. It’s dark tonalities and muted colors remind me more of Dark Impressionism than Impressionism. But there is a vitality and an exuberance of paint application that I just really like. Bellows was praised by Academy members for his works that displayed the “American temperament,” whatever that might mean. As much as his cityscapes captured the vitality of city life, so too did his landscape capture the excitement of nature.\nCorrelations to Davis Programs: A Community Connection: 6.4; Discovering Art History: 4E 15.1']"	['<urn:uuid:a23a988a-9260-4898-b6b6-fec01aff6308>', '<urn:uuid:5df95ecd-5f92-45e5-ae3f-af41cdd55ea4>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T15:30:12.136021	8	54	1970
53	why dark sky at night if universe full of stars everywhere explain	While every patch of the sky actually contains part of a galaxy, several factors keep the night sky dark to our vision: the reddening of light due to the expansion of space, the universe's dynamic nature, and the absorption of light by intergalactic dust and gas. These factors combine to reduce visible and ultraviolet light in the universe, making starlight invisible to the human eye and most modern telescopes.	"['Astronomers announced on October 13, 2016 that an analysis of data from the Hubble Space Telescope and other observatories has revealed there are at least 10 times more galaxies in the observable universe than previously thought. The announcement at HubbleSite said:\nThe results have clear implications for galaxy formation, and also helps shed light on an ancient astronomical paradox — why is the sky dark at night?\nAstronomer Christopher Conselice of the University of Nottingham, U.K. led the team that found that 10 times as many galaxies are packed into a given volume of space in the early universe than found today. HubbleSite explained:\nMost of these galaxies were relatively small and faint, with masses similar to those of the satellite galaxies surrounding the Milky Way. As they merged to form larger galaxies the population density of galaxies in space dwindled.\nThis means that galaxies are not evenly distributed throughout the universe’s history.\nAnd thus these results indicate that a significant galaxy evolution has taken place throughout our universe’s history. That is, as galaxies merged over time, the total number of galaxies was reduced.\nHow do we know the number of galaxies in the universe. One of the first attempts to quantify it came from the landmark Hubble Deep Field, a survey conducted in the mid-1990s. Subsequent sensitive observations such as Hubble’s Ultra Deep Field revealed a myriad of faint galaxies.\nThis early work led to an estimate that the observable universe contains about 200 billion galaxies. Now the new analysis of Hubble data shows that this estimate is at least 10 times too low.\nConselice and his team analyzed deep-space images from Hubble and the already published data from other teams. They painstakingly converted the images into 3-D, in order to make accurate measurements of the number of galaxies at different epochs in the universe’s history. In addition, they used new mathematical models, which allowed them to infer the existence of galaxies that the current generation of telescopes cannot observe.\nThis led to the surprising conclusion that in order for the numbers of galaxies we now see and their masses to add up, there must be a further 90 percent of galaxies in the observable universe that are too faint and too far away to be seen with present-day telescopes. These myriad small faint galaxies from the early universe merged over time into the larger galaxies we can now observe. Conselice said:\nIt boggles the mind that over 90 percent of the galaxies in the universe have yet to be studied. Who knows what interesting properties we will find when we discover these galaxies with future generations of telescopes?\nThe decreasing number of galaxies as time progresses also contributes to the solution for Olbers’ paradox, which is a famous paradox in astronomy, first formulated in the early 1800s by German astronomer Heinrich Wilhelm Olbers.\nOlbers ask the question: Why is the sky dark at night if the universe contains an infinity of stars?\nThe team concluded that indeed there actually is such an abundance of galaxies that, in principle, every patch in the sky contains part of a galaxy. However, starlight from the galaxies is invisible to the human eye and most modern telescopes due to the other known factors that reduce visible and ultraviolet light in the universe.\nThose factors are the reddening of light due to the expansion of space, the universe’s dynamic nature, and the absorption of light by intergalactic dust and gas. All combined, this keeps the night sky dark to our vision.\nThe research team’s study will be published in The Astrophysical Journal, a peer-reviewed journal.\nBottom line: Astronomers announced on October 13, 2016 that an analysis of data from the Hubble Space Telescope and other observatories has revealed there are at least 10 times more galaxies in the observable universe than previously thought.\nDeborah Byrd created the EarthSky radio series in 1991 and founded EarthSky.org in 1994. Today, she serves as Editor-in-Chief of this website. She has won a galaxy of awards from the broadcasting and science communities, including having an asteroid named 3505 Byrd in her honor. A science communicator and educator since 1976, Byrd believes in science as a force for good in the world and a vital tool for the 21st century. ""Being an EarthSky editor is like hosting a big global party for cool nature-lovers,"" she says.']"	['<urn:uuid:965b2ea9-d26b-4737-9bd7-7d4c044aae9d>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	12	69	728
54	What's the right way to use further and farther in sentences, and how do they actually mean different things?	Farther and further have distinct meanings. 'Farther' specifically refers to physical distance or more advanced positions, while 'further' is used when talking about additional or extra things. For example, if you're discussing improvements, you would say 'further improvement' rather than 'farther improvement'.	['Dear Students, here we have collected the most important rules and tricks for Spottings Errors. These tricks will help you to clear various competitive exams like SSC, BANK PO and IBPS etc.\nSUBJECT – VERB AGREEMENT\nTwo or more singular subjects connected by and usually take a verb in the plural.\nIncorrect- Hari and Ram is here.\nCorrect- Hari and Ram are here\nIf two singular nouns refer to the same person or thing, the Verb must be Singular.\nIncorrect- The Secretary and Principal are coming.\nCorrect- The Secretary and Principal is coming. (Here the same person is Secretary as well as Principal)\nIf the singular subjects are preceded by each or every, the verb is usually singular.\nIncorrect- Every boy and girl were ready.\nCorrect- Every boy and girl was ready.\nTwo or more singular subjects connected by or, ‘nor’, ‘either’….. ‘Or’, ‘neither’…. ‘Nor’ take a verb in the singular.\nIncorrect- Neither he nor I were there.\nCorrect- Neither he nor I was there.\nWhen the subjects joined by ‘or/nor’ are of different numbers, the verb must be plural, and the Plural Subject must be placed next to the Verb.\nIncorrect – Neither the Assistant Masters nor the Headmaster was present.\nCorrect- Neither the Headmaster nor the Assistant Masters were present.\nWhen the Subjects joined by or, nor are of different persons, the Verb agrees in person with the one nearest to it.\nIncorrect- Either he or I is mistaken.\nCorrect- Either he or I, am mistaken.\nA Collective Noun takes a Singular Verb when the collection is thought of as a whole, a Plural verb when the individuals of which it is composed are thought of.\nCorrect- The Council has chosen the President.\nCorrect- The military were called out.\nSome Nouns which are singular in form but plural in meaning, take a Plural Verb.\nIncorrect- Mathematics are a branch of study in every school.\nCorrect- Mathematics is a branch of study in every school.\nWords joined to a Singular Subject by with, together with, in addition to, or, as well as, etc. are parenthetical, and therefore do not affect the number of the Verb.\nIncorrect- The Chief, with all his men, were massacred.\nCorrect-The chief, with all his men, was massacred.\nWhen the Subject of the Verb is a Relative Pronoun care should be taken to see that the Verb agrees in Number and Person with the Antecedent of the relative.\nIncorrect- I, who is your friend, will guard your interests.\nCorrect- I, who am your friend will guard your interests.\nWRONG USAGE OF PARTICIPLES AND INFINITIVES\nAsk, advise, allow, command, force, forbid, invite, encourage, compel, beg, order•, imagine, instruct, permit, persuade, tell, require, remind, teach, etc. are followed by Object + To +V2\nIncorrect- He advised to do it by me.\nCorrect- He advised me to do it.\nBut if these are used in Passive Voice, then they are followed by To +V,.\nCorrect- She was permitted to go with him.\nKnow is followed by how/ where/when/why and Infinitive.\nIncorrect- I know to write a letter.\nCorrect- I know how to write a letter.\nAfter let, bid, behold, watch, see, feel, make etc. we use Bare-Infinitive and not To-infinitive.\nIncorrect- I heard him to speak on several subjects.\nCorrect- I heard him speak on several subjects.\nBare Infinitive is used after Modal Auxiliaries (can, could, may, might, shall, should, will, would, must, dare not,\nIncorrect- You need not to work hard.\nCorrect- You need not work hard.\nHad better, had rather, had as soon … as …, had sooner etc. are followed by Bare Infinitive.\nIncorrect- He had better to go now.\nCorrect- He had better go now.\nConjunction than is also followed by Bare Infinitive.\nIncorrect- He had better read than to write.\nCorrect- He had better read than write.\nWhen but is used as a Preposition and preceded by any form of the Verb do, then but is followed with Bare Infinitive.\nIncorrect- He did nothing but to wander.\nCorrect- He did nothing but wander.\nEvery Participle must have a Subject of Reference.\nIncorrect- Being a rainy day Vijay decided to stay at home.\nCorrect- It being a rainy day Vijay decided to stay at home.\nFor completed action Having + V is used in Active Voice, whereas Having + been + V or Being + V is used in Passive Voice. After should not be used in such a sentence.\nIncorrect- After the leader having been killed, the followers ran away.\nCorrect- The leader having been killed, the followers ran away.\nParticiples like considering, judging, referring, concerning, regarding, viewing, broadly speaking etc. do not take any Subject of Reference.\nCorrect – Considering the case, I took the decision.\nHere ‘I’ is not a Subject of Reference of considering. So, there is no Subject of\nReference for ‘considering, still the sentence is correct.\nWRONG USAGE OF VERBS\nWhen there are two Subjects in a sentence and they are not in the same Number, then we must have to use separate Auxiliaries (is, are, am, was, were, have, has) for both of them.\nIncorrect- Three killed and one were injured.\nCorrect- Three were killed and one was injured.\nA single Verb should be made to serve two Subjects, only when the form of Verb is same for both the subjects.\nIncorrect- I am seventeen years old and my sister fourteen.\nCorrect- I am seventeen years old and my sister is fourteen.\nTwo auxiliaries can be used with one principal Verb, only when the form of the principal Verb is appropriate to both the auxiliaries.\nIncorrect- He never has, and never will take such strong measures.\nCorrect- He never has taken, and never will take such strong measures.\nWhen there is only one auxiliary to two principal Verbs it should be correctly associated with the both.\nIncorrect- Ten candidates have passed one failed.\nCorrect- Ten candidates have passed, one has failed.\nA Past Tense in the main clause should be followed by a Past Tense in the subordinate clause.\nIncorrect- He succeeded because he works hard.\nCorrect- He succeeded because he worked hard.\nA Past Tense in main clause may be followed by a Present Tense in the subordinate clause when the subordinate clause expresses a universal truth.\nIncorrect- Our teacher said that the earth moved round the sun.\nCorrect- Our teacher said that the earth moves round the sun.\nWhen the subordinate clause comes after ‘lest’, the auxiliary Verb ‘should’ must be used, whatever be the Tense of the Verb in the main clause.\nIncorrect- We start early lest we shall miss the train.\nCorrect- We start early lest we should miss the train.\nAn Adverb or Adverbial phrase should not be placed between ‘to’ and verbal part of the infinitive. (This is called the split infinitive).\nIncorrect- I hoped to immediately reply to your letter.\nCorrect- I hoped to reply immediately to your letter.\nAn infinitive should be in the present tense unless it represents an action prior to that of the governing Verb.\nIncorrect- I should have liked to have gone-there.\nCorrect- I should have liked to go there.\nGerund if preceded by a Pronoun, that Pronoun must be in Possessive case.\nIncorrect – He emphasized me going there.\nCorrect- He emphasized my going there.\nThe Present Perfect Continuous Tense is used for an action that began in the past time and still going at the time of speaking. It is used with, Adverb of time introduced by ‘since’, ‘for’ and ‘how long’.\nIncorrect- How long are you working in this office?\nCorrect- How long have you been working in this office?\nA Verb when preceded by a Preposition must be the Gerund.\nIncorrect- They were punished for come late.\nCorrect- They were punished for, coming late.\nThe Future Indefinite Tense is not used in the clauses of time, place and condition. Here the Present Indefinite Tense is used.\nIncorrect- I shall wait for you till you will finish your work.\nCorrect- I shall wait for you, till you finish your work.\nThe Present Perfect Tense is not used with the Adverbs of past time like yesterday, in 1990 etc. Here Past Indefinite Tense is used.\nIncorrect- I have bought a cycle yesterday.\nCorrect-I bought a cycle yesterday.\nThe Past Perfect Tense is used to represent the earlier of the two past actions.\nIncorrect- When I reached the station, the train already left.\nCorrect- When I reached the station, the train had already left.\nModal Auxiliaries are not used together. But two Auxiliaries can be connected by a Conjunction.\nIncorrect-He should must do it.\nCorrect- He should and must do it.\nWhen need or dare is followed by not, it turns into modal auxiliary. In that situation it takes Bare Infinitive ‘and we cannot use ‘needs not’ or ‘dares not’.\nIncorrect- He needs not do it.\nCorrect- He need not do it.\nWRONG USAGE OF ADJECTIVES\nAdjectives of quantity show how much of a thing is meant. Adjectives of quantity (some, much, little, enough, all, no, any, great, half, sufficient, whole) are used for Uncountable Nouns only.\nIncorrect-I ate a few rice.\nCorrect- I ate some rice.\nNumeral Adjectives are used for Countable Noun only and they show how many persons or things are meant or in what order a person or thing stands.\nIncorrect- I have taught you little things.\nCorrect- I have taught you a few things.\nWhen cardinal and ordinal are used together ordinal precedes the cardinal.\nIncorrect- The four first boys will be given the chance.\nCorrect- The first four boys will be given the chance.\nLater, latest refer to time, latter and last refer to position.\nIncorrect- I reached at 10 AM. But he was latter than I expected.\nCorrect- I reached at 10 AM. But he was later than I expected.\nFarther means more distant or advanced; further means additional.\nIncorrect- He insisted on farther improvement.\nCorrect- He insisted on further improvement.\nEach is used in speaking of two or more things, every is used only in speaking of more than two.\nIncorrect- Every of the two boys will get a prize.\nCorrect- Each of the two boys will get a prize.\nTo express quantity or degree some is used in affirmative sentences, any in negative or interrogative sentences.\nIncorrect- Have you bought some mangoes?\nCorrect- Have you bought any mangoes?\nIn comparing two things, the Comparative should be used, the Superlative should not be used.\nIncorrect- Which is the best of the two?\nCorrect- Which is the better of the two?\nWhen two qualities of the same person or thing are compared, the Comparative in ‘er’ is not used. ‘More’ is used for this purpose.\nIncorrect- He is wiser than brave.\nCorrect- He is more wise than brave.\nWhen comparison is made by means of a comparative, the thing compared should be excluded from the class of things with which it is compared by using ‘other’ or some such word.\nIncorrect- He is cleverer than any boy in the class.\nCorrect- He is cleverer than any other boy in the class.\nWhen comparison is made by means of a superlative, the thing compared should include the class of things with which it is compared.\nIncorrect- He is the strongest of all other men.\nCorrect- He is the strongest of all men.\nWhen two persons or things are compared, it is important that the same parts of things should be compared.\nIncorrect- The population of Bombay is greater than Delhi.\nCorrect- The population of Bombay is greater than that of Delhi.\nDouble comparatives and superlatives should not be used.\n- Incorrect- He is the most cleverest boy in the class.\nCorrect- He is the cleverest boy in the class.\n- Incorrect- He is more wiser than his brother.\nCorrect- He ‘is wiser than his brother.\nThe comparative Adjectives superior-inferior, senior, junior, prior, anterior, Posterior prefer, etc., should be followed by ‘to’ instead of ‘than’.\nIncorrect- He is senior than me.\nCorrect- He is senior to me.']	['<urn:uuid:9d28731d-137f-4c94-8531-e702d49d62fb>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	19	42	1998
55	I play both tennis and badminton. Which one requires more strength training?	Tennis generally demands more strength training as players need sustained power and endurance for prolonged rallies and matches. While both sports are physically demanding, tennis emphasizes power and endurance, whereas badminton focuses more on speed and agility. Strength training in tennis is particularly important for preventing injury and enhancing performance, especially for the rotator cuff muscles in the shoulder.	"['Strength training and conditioning are becoming necessities in today\'s tennis game as play continues to get faster and players hit the ball with more power from everywhere on the court. As coaches, players and parents realize this they want to get their players involved in a strength and conditioning program, often at younger and younger ages. There are a lot of questions surrounding strength training, especially when younger players are concerned.\n""Strength training"" is synonymous with ""resistance training,"" and the two terms can be used interchangeably. Strength training uses the principle of progressive overload to force the body (muscles, bones, tendons, etc.) to adapt in order to be able to produce and resist greater forces. Strength training is not power lifting, nor is it bodybuilding. Also, one does not need to lift weights to strength train. Many exercises can be performed simply using a player\'s body weight as resistance.\nWhat are the goals/ benefits of strength training?\nStrength training for tennis can help to prevent injury and enhance on-court performance. It is important for all tennis players, even young players, to strengthen the muscles of the rotator cuff to maintain a proper strength balance in the shoulder. With young players, the goal of strength training should also be to increase muscular endurance. The goals of strength training should not shift to increasing maximal strength until after a player goes through puberty.\nIs strength training safe for young players?\nThe risk of injury is probably the primary concern of any coach or parent who has a child entering a strength training program. Any exercise or activity carries with it some level of injury risk--even a child running in the backyard can suffer an injury--so it is unrealistic to assume that injuries will never occur in conjunction with strength training. Both the National Strength and Conditioning Association and the American Academy of Pediatrics state that youth strength training can be safe and effective if:\n* A competent coach who is skilled in program design supervises every training session\n* Proper technique is taught and required in every repetition of every exercise.\nIsn\'t there a risk that growth plates will be damaged?\nMany parents and coaches are hesitant to begin strength training with young athletes for fear of damaging the bones and possibly stunting growth. The fact is that no growth plate fractures have been documented in athletes who engage in a resistance training program that follows the two guidelines listed above. The risk of injury to the growth plates can be further minimized if players do not lift heavy weight over their heads or attempt to lift extremely heavy weights. Growth plate injuries should be taken seriously because they can happen. However, with proper care the risk can be almost eliminated.\nDoes strength training work for young players?\nYes, it does. Most people believe that testosterone (a steroid produced naturally in the body) is necessary to build strength. However, resistance training helps to improve motor control and strength by ""teaching"" muscles how to work together in a coordinated manner, which leads to improvements in strength without an associated gain in muscle mass. Some other benefits of youth strength training are:\n* Improved strength and coordination\n* Increased bone density\n* Improved self-image and self-confidence\n* Potential to prevent injuries', 'Badminton and tennis are played across the globe, captivating audiences with their unique blend of agility and power. While both sports require a high degree of hand-eye coordination and fitness, they each offer distinctive challenges to their enthusiasts. These two racket sports are not just a test of physical prowess, but also mental sharpness, making them beloved by players and spectators alike.\nThis analysis provides a comprehensive comparison, delving into the nuanced differences and surprising similarities between badminton and tennis. From equipment and scoring systems to the physical and technical demands of each sport, join us as we explore what makes these games so engaging and why they continue to capture our collective imagination. Let’s embark on a journey to uncover the essence of these competitive pursuits and what it truly takes to excel in them.\nCore Differences Between Badminton and Tennis\nRacket sports like badminton and tennis share structural similarities yet differ significantly in gameplay. Tennis requires a larger playing area, with tennis courts measuring 23.77 x 10.97 meters for singles matches. Badminton requires a more compact space, with the game played indoors on courts that are smaller and designed for rapid exchanges.\nWhile both sports involve hitting the ball or shuttlecock, the equipment and techniques used are tailored to their unique dynamics. The scoring system in tennis is based on games and sets, with six games needed to win a set under most circumstances.\nIn contrast, advanced badminton follows a rally point scoring system, where players score points regardless of who serves. Comparing tennis and badminton reveals distinct paces and strategic considerations, heavily influenced by whether the game is played indoors or on an outdoor court, and whether the play involves a ball vs a shuttlecock.\n• Court Dimensions and Layout\nTennis courts are designed to facilitate a wide range of shot-making, with dimensions of 23.77 x 8.23 meters for doubles matches. A badminton court is 13.4 x 6.1 meters, allowing for fast-paced, close-quarters play. The smaller size of the badminton court reflects the game’s emphasis on speed and reflexes, while the larger tennis court caters to the game’s need for powerful strokes and endurance.\n• Equipment and Gear: Rackets and Shuttlecocks vs Tennis Balls\nThe equipment used in each sport is tailored to its specific demands. A tennis ball is designed for durability and bounce, suitable for the forceful impact of a tennis racket. In contrast, a shuttlecock is a conical projectile, optimised for aerodynamic stability and speed changes, making it a unique challenge when struck with a badminton racket.\n• Scoring Systems and Match Structures\nEach sport utilises a distinct scoring system that influences the rhythm and duration of play. Tennis employs a game, set, and match structure, where players must win a sequence of points to secure games and ultimately sets. Badminton’s scoring system is more straightforward, with players competing to reach a pre-determined number of points to win the match.\nPhysical and Skill Demands of Each Sport\nBoth badminton and tennis are physically demanding sports, each placing different emphases on the athletic qualities required for success. Tennis demands sustained power and endurance, with players often engaged in prolonged rallies and matches. Badminton, while also physically challenging, tends to emphasise speed and agility, with players expected to react quickly to rapidly changing situations on the court. Mastering either sport requires a deep commitment to physical conditioning and skill development. The finesse and precision necessary for advanced play come from years of dedicated practice, highlighting the remarkable athleticism of professional players in both badminton and tennis.\n• Agility, Speed, and Reflexes in Badminton vs Tennis\nTennis requires players to cover a larger court, demanding sustained speed and the ability to quickly change direction. Tennis players often have to run considerable distances to hit the ball, while badminton requires sharp reflexes to respond to the fast-moving badminton birdie. An overhead smash in badminton, for example, necessitates split-second timing and dexterity.\n• Strength and Endurance – A Comparative Look\nBadminton players must possess a blend of muscular endurance and explosive strength to maintain the intensity of play throughout a match. The requirement for speed and power in manoeuvring the shuttlecock demands a high level of physical conditioning specific to the sport of badminton.\nTechnical Mastery: Shots and Footwork\nThe technical aspects of both tennis and badminton are complex and require years of practice to master. Tennis ball handling involves a wide array of shots, including slice shots and drive shots, executed with a tennis racket. The explosive power required for serving and returning in tennis contrasts with the delicate touch needed for net play and drop shots in badminton.\nFootwork is crucial in both sports, with players needing to position themselves optimally to execute shots effectively. The level of technical skill in racket handling and body coordination highlights the depth of mastery required for both tennis and badminton.\n1. The Variety and Complexities of Tennis Shots\nTennis shots range from powerful serves to delicate volleys, each requiring precise control of the tennis racket. The explosive power needed to drive the tennis ball over the net contrasts with the subtle finesse required for slice shots. Players must also consider the weight of the ball and the surface of the court when executing their shots, adding layers of complexity to the game.\n2. Precision and Deception in Badminton Shot-Making\nBadminton requires a unique blend of precision and deception in shot-making. The ability to disguise shots and wrong-foot opponents is crucial for success, with players needing to maintain exceptional control over the shuttlecock’s trajectory and speed.\n3. Footwork Patterns: Tennis vs Badminton\nFitness is a foundational element for both tennis and badminton courts, where efficient movement can be the difference between winning and losing a point. In tennis, the ball can bounce, allowing players a fraction more time to reach it, whereas in badminton, the shuttlecock must be hit before it touches the ground. This difference significantly influences the footwork patterns required for playing tennis or badminton, with each sport demanding its own style of agility and coordination.\nTactical Play and Mental Challenges\nBoth badminton and tennis require strategic thinking and mental resilience. Badminton players must constantly anticipate their opponent’s next move, while also devising their own tactical plays. The fast pace of badminton often leads to rapid shifts in momentum, testing a player’s mental fortitude and ability to stay focused under pressure.\nIn tennis, the longer match format and the possibility of extended rallies demand a high level of concentration and strategic planning. Players must be adept at reading their opponent’s game and adjusting their tactics accordingly, showcasing the mental challenges inherent to both sports.\nThe Role of Strategy in Tennis\nStrategy in tennis is multifaceted, encompassing shot selection, positioning, and fitness management. Players must be able to adapt their game plan in real time, responding to the strengths and weaknesses of their opponent while also considering their own physical and mental state throughout the match.\nBadminton Tactics: Speed and Shuttle Control\nIn badminton, rapid shuttlecock hitting and swift manoeuvres are essential for maintaining the offensive. Badminton requires players to execute lightning-quick shots, often with less than half a second to respond. Mastery of shuttle control allows players to place the shuttlecock precisely, forcing opponents into difficult positions, thereby capitalising on speed to dominate the rally.\nMental Toughness and Concentration in Competitive Play\nCompetitive play demands high mental toughness and concentration, where players must maintain focus amidst the intensity of the match. Both badminton and tennis require players to anticipate opponents’ moves and swiftly decide their own, all while managing the psychological stress of the competition.\nComparison Chart: Side-By-Side Overview of Badminton and Tennis\nBadminton and tennis are played on courts with different dimensions, using distinct equipment; badminton utilises lightweight rackets and a feathered shuttlecock, whereas tennis players wield heavier rackets to strike a rubber ball.\nScoring systems differ too, with badminton games played to 21 points and tennis employing sets. The physical demands vary, as tennis often emphasises strength and endurance, while badminton focuses on agility and reflexes.\nKey Statistics and Facts for Quick Comparison\nBadminton matches typically last around 45 minutes, while tennis matches can extend for several hours. A badminton player hits the shuttlecock an average of 400 times per match, indicative of the sport’s high-speed nature. Tennis, with its variety of shots and strategic depth, offers a different rhythm of play.\nVisual Chart Depicting Differences and Similarities\nA visual chart would illustrate the stark contrasts in court size, with badminton’s smaller dimensions promoting a faster game, and tennis’s larger court favouring powerful serves and groundstrokes. The equipment section would highlight the difference between the feathered or synthetic plastic shuttlecock and the felt-covered tennis ball.\nThe Spectator Experience\nSpectators of both sports enjoy different experiences; tennis offers a mix of tension-filled quiet moments and exhilarating rallies, while badminton provides a consistently fast-paced and dynamic spectacle. Both sports have passionate fan bases that appreciate the unique challenges and skills displayed in each match.\nWatching Tennis: A Test of Patience and Excitement\nWatching tennis can be a test of patience, with matches often lasting several hours, building suspense and excitement. Fans relish the strategic play, the psychological warfare, and the moments of sheer brilliance when a player hits a winning shot after a prolonged rally.\nThe Dynamic and Fast-Paced Nature of Badminton\nThe dynamic nature of badminton, with its rapid exchanges and sharp reflexes, makes it an exhilarating sport to watch. The shuttlecock’s high speed and the players’ agility contribute to a lively spectator experience, with little downtime between points.\nConclusion: Which Racket Sport Reigns Supreme?\nDeciding which racket sport reigns supreme between badminton and tennis is a matter of personal preference and individual strengths. Badminton rallies are high-intensity bursts requiring agility and precision, while tennis demands greater strength and endurance. The compact court dimensions and lighter badminton rackets facilitate rapid gameplay, contrasting with the broader tennis courts and heavier rackets designed for a different strategic depth.\nBoth sports feature their unique scoring systems and match structures, with tennis allowing the ball to bounce once and badminton rallies continuing in a fast-paced sequence without such allowance. Doubles and mixed doubles matches add a layer of complexity and teamwork in both disciplines, each with its unique tactical considerations.\nIn the end, whether you gravitate towards the dynamic, fast-paced nature of standard badminton or the strategic, endurance-based game of tennis, each sport offers a rich and rewarding experience.']"	['<urn:uuid:e2ce0ebc-8415-4d46-bee5-b8c01718d622>', '<urn:uuid:8af89654-9575-4f5a-a0bb-6afd4612521f>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T15:30:12.136021	12	59	2284
56	How does a one-degree temperature rise affect Earth?	Although a one-degree Fahrenheit temperature increase might seem small, it has significant global impacts. This warming causes ice caps to melt and sea levels to rise. It leads to crops dying and creates problems with air quality and clean water access in many places. Additionally, experts believe this climate change increases the frequency of natural disasters like wildfires. So while we might not notice a one-degree change in daily weather, this worldwide temperature increase has major consequences.	['Today’s Wonder of the Day was inspired by Laurel from Huntington. Laurel Wonders, “How is the world changing because of climate change?” Thanks for WONDERing with us, Laurel!\nWhat’s the weather usually like where you live? Is it warm enough for you to bask in the Sun every day? Do you need to bundle up before going outside? Is it always rainy, or usually dry? You’re describing the climate of your region!\nClimate is the usual weather of a place. While weather can change in a matter of hours, climate measures the weather over a long period of time. Most areas have pretty stable climates. Even though some years see more rainfall or cold weather than normal, the overall climate remains the same. However, sometimes climates change. Climate change alters the usual weather of an area. An area could go from very rainy to mostly dry. It could even go from warm to chilly! However the climate changes, it has a big impact on the lives of people who live there.\nClimate change can also happen across the globe. It can be a change in the planet’s temperature or a change in patterns of rainfall. Have you ever heard of the Ice Age? It was a time of extreme cold across the planet. Today, the opposite is happening. Instead of freezing, the Earth is warming. Its temperature has gone up about one degree Fahrenheit over the last century.\nThat may not sound like a big deal. After all, most of us wouldn’t even notice a one-degree change in the weather outside. However, one degree worldwide has a big impact. The warming makes ice caps melt, sea levels rise, and crops die. It also causes poor air quality and lack of clean water in many places. Experts believe climate change even increases the number of natural disasters like wildfires.\nWhat’s causing the Earth to warm? Sometimes, the climate changes on its own. Things like our distance from the Sun can change the climate. Disasters like volcanic eruptions also have an effect. However, most scientists today say the rising temperature is caused by people.\nHave you ever ridden in an automobile? Does your school run a heater during the winter? Do you cook food on a stove at home? These are all activities that add to climate change. When we burn coal, oil, and gas to create energy, we send gases into the atmosphere. Those gases trap heat and cause the planet’s temperature to rise.\nAre you WONDERing what you can do to help stop climate change? The best thing to do is to use less energy and water. Turn off lights and water when you’re not using them. Walk instead of riding in cars when you can. It also helps to look for other energy sources. Many people already use solar and wind power, which can lower the use of coal, oil, and gas.\nWhatever we do, we need to do it soon! Many scientists believe the rising temperature could ruin the Earth if we don’t turn things around. Are you ready to save the world? Spread the word. Help your friends and family make choices that are good for the planet. It’s the only home we have, so we need to take care of it.\nStandards: NGSS.ESS3.A, NGSS.ESS3.B, NGSS.ESS3.C, NGSS.ESS3.D, CCRA.R.3, CCRA.R.6, CCRA.R.1, CCRA.R.2, CCRA.R.10, CCRA.W.2, CCRA.SL.1']	['<urn:uuid:8ce09532-cc10-48b1-8e23-b242fa2323e0>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	8	77	557
57	number hidden states hmm music classification	The model used 4 hidden states in the Hidden Markov Model for music genre classification, though testing different values is recommended to optimize predictions.	"['Music Genre Classification using Hidden Markov Models\nMusic genre classification has been an interesting problem in the field of Music Information Retrieval (MIR). In this tutorial, We will try to classify music genre using hidden Markov models which are very good at modeling time series data. As Music audio files are time series signals, we expect that HMMs will suit our needs and give us an accurate classification. An HMM is a model that represents probability distributions over sequences of observations. We assume that the outputs are generated by hidden states. To learn more about HMM click click here. Also, you can find a good explanation for HHMs in the time series use case here.\nDataset & Features\nWe will use a dataset provided by Marsyas (Music Analysis, Retrieval, and Synthesis for Audio Signals) which is an open source software, called GTZAN. It’s a collection of 1000 audio tracks each 30 seconds long. There are 10 genres represented, each containing 100 tracks. All the tracks are 22050Hz Mono 16-bit audio files in .au format. In our tutorial, we will use all provided genres (blues, classical, jazz, country, pop, rock, metal, disco, hip-hop, reggae). For music genre classification, we will be easier for us to use WAV files, because they can be easily read by the scipy library. We would, therefore, have to convert our AU files to WAV format. The dataset can be accessed here.\nFor audio processing, we needed to find a way to concisely represent song waveforms. Mel Frequency Cepstral Coefficients (MFCC) is a good way to do this. MFCC takes the power spectrum of a signal and then uses a combination of filter banks and discrete cosine transform to extract features. More details about MFCC can be found here.\nLet’s get started by importing the necessary libraries for our project.\nfrom python_speech_features import mfcc, logfbank from scipy.io import wavfile import numpy as np import matplotlib.pyplot as plt from hmmlearn import hmm from sklearn.metrics import confusion_matrix import itertools import os\nLet’s pick one song from our dataset and extract MFCC and Filter banks features.\nsampling_freq, audio = wavfile.read(""genres/blues/blues.00000.wav"") mfcc_features = mfcc(audio, sampling_freq) filterbank_features = logfbank(audio, sampling_freq)\nWe take a look at the shapes of the extracted features.\nprint (\'\\nMFCC:\\nNumber of windows =\', mfcc_features.shape) print (\'Length of each feature =\', mfcc_features.shape) print (\'\\nFilter bank:\\nNumber of windows =\', filterbank_features.shape) print (\'Length of each feature =\', filterbank_features.shape)\nMFCC: Number of windows = 1496 Length of each feature = 13 Filter bank: Number of windows = 1496 Length of each feature = 26\nNow, let’s go through some samples of our dataset. We loop over the genres folders and visualize the MFCC features for the first song in each folder.\nimport glob import os.path as path genre_list = [“blues”,”classical”, “jazz”, “country”, “pop”, “rock”, “metal”, “disco”, “hiphop”, “reggae”] print(len(genre_list)) figure = plt.figure(figsize=(20,3)) for idx ,genre in enumerate(genre_list): example_data_path = ‘genres/’ + genre file_paths = glob.glob(path.join(example_data_path, ‘*.wav’)) sampling_freq, audio = wavfile.read(file_paths) mfcc_features = mfcc(audio, sampling_freq, nfft=1024) print(file_paths, mfcc_features.shape) plt.yscale(‘linear’) plt.matshow((mfcc_features.T)[:,:300]) plt.text(150, -10, genre, horizontalalignment=’center’, fontsize=20) plt.yscale(‘linear’) plt.show()\nBuilding Hidden Markov Models\nWe built a class to handle HMM training and prediction by wrapping the model provided by hmmlearn library.\nclass HMMTrainer(object): def __init__(self, model_name=\'GaussianHMM\', n_components=4, cov_type=\'diag\', n_iter=1000): self.model_name = model_name self.n_components = n_components self.cov_type = cov_type self.n_iter = n_iter self.models =  if self.model_name == \'GaussianHMM\': self.model = hmm.GaussianHMM(n_components=self.n_components, covariance_type=self.cov_type,n_iter=self.n_iter) else: raise TypeError(\'Invalid model type\') def train(self, X): np.seterr(all=\'ignore\') self.models.append(self.model.fit(X)) # Run the model on input data def get_score(self, input_data): return self.model.score(input_data)\nTraining & evaluating the Hidden Markov Models\nFor training the Hidden Markov Models, We loop over the subfolder in our dataset, we iterate over the songs in the subfolder in order to extract the features and append it to a variable. We should store all trained HMM models, so we will be able to predict the class of unseen songs. As HMM is a generative model for unsupervised learning, we don’t need labels to build HMM models for each class. We explicitly assume that separate HMM models will be built for each class. Note that we have used 4 as the number of components which is exactly the number of hidden state in the HMM models. Finding out the best number of states is about testing and experimenting with different values and pick the one that optimizes the predictions.\nhmm_models =  input_folder = \'genres/\' # Parse the input directory for dirname in os.listdir(input_folder): # Get the name of the subfolder subfolder = os.path.join(input_folder, dirname) if not os.path.isdir(subfolder): continue # Extract the label label = subfolder[subfolder.rfind(\'/\') + 1:] # Initialize variables X = np.array() y_words =  # Iterate through the audio files (leaving 1 file for testing in each class) for filename in [x for x in os.listdir(subfolder) if x.endswith(\'.wav\')][:-1]: # Read the input file filepath = os.path.join(subfolder, filename) sampling_freq, audio = wavfile.read(filepath) # Extract MFCC features mfcc_features = mfcc(audio, sampling_freq) # Append to the variable X if len(X) == 0: X = mfcc_features else: X = np.append(X, mfcc_features, axis=0) # Append the label y_words.append(label) print(\'X.shape =\', X.shape) # Train and save HMM model hmm_trainer = HMMTrainer(n_components=10) hmm_trainer.train(X) hmm_models.append((hmm_trainer, label)) hmm_trainer = None\nNow it’s time to evaluate our models, We iterate over the test dataset subfolders, we extract the features then we iterate through all HMM models and pick the one with the highest score.\ninput_folder = \'test/\' real_labels =  pred_labels =  for dirname in os.listdir(input_folder): subfolder = os.path.join(input_folder, dirname) if not os.path.isdir(subfolder): continue # Extract the label label_real = subfolder[subfolder.rfind(\'/\') + 1:] for filename in [x for x in os.listdir(subfolder) if x.endswith(\'.wav\')][:-1]: real_labels.append(label_real) filepath = os.path.join(subfolder, filename) sampling_freq, audio = wavfile.read(filepath) mfcc_features = mfcc(audio, sampling_freq) max_score = -9999999999999999999 output_label = None for item in hmm_models: hmm_model, label = item score = hmm_model.get_score(mfcc_features) if score > max_score: max_score = score output_label = label pred_labels.append(output_label)\nTill now, We have the real label for each song in our test dataset and we have estimated the predicted ones. As our problem is a multiclass classification, the best way to evaluate our model’s performance is to look at the confusion matrix (read more here).\nWe use the confusion matrix provided by sklearn and we visualize it using matplotlib library.\nFirst, let’s define a function to take care of the matric plotting. This function prints and plots the confusion matrix. Normalization can be applied by setting\ndef plot_confusion_matrix(cm, classes, normalize=False, title=\'Confusion matrix\', cmap=plt.cm.Blues): if normalize: cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis] print(""Normalized confusion matrix"") else: print(\'Confusion matrix, without normalization\') print(cm) plt.imshow(cm, interpolation=\'nearest\', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = \'.2f\' if normalize else \'d\' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape), range(cm.shape)): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black"") plt.tight_layout() plt.ylabel(\'True label\') plt.xlabel(\'Predicted label\')\nTime to compute the confusion matrix and visualize it!\ncm = confusion_matrix(real, pred) np.set_printoptions(precision=2) classes = [""blues"",""classical"", ""country"", ""disco"", ""hiphop"", ""jazz"", ""metal"", ""pop"", ""reggae"", ""rock""] plt.figure() plot_confusion_matrix(cm, classes=classes, normalize=True, title=\'Normalized confusion matrix\') plt.show()\nFor a perfect classifier, we would have expected a diagonal of dark squares from the left-upper corner to the right lower one, and light colors for the remaining area. As we can see from the matrix plot, The HMM model for the ‘Pop’ class is able to perfectly predict the class of the songs. ‘Country’ and ‘Hip-hop’ are also performing good. However, the worst model is the one for ‘Rock’ class! sorry, rock-fans! For an even better evaluation, It’s always recommended to use the precision and recall metrics for the classification problem. sklearn made it easy for us to have a detailed report about the precision and recall of our multiclass classification. We just need to provide the real and predicted values and our classes names.\nfrom sklearn.metrics import classification_report print(classification_report(real, pred, target_names=classes))\nThe output looks like:\nprecision recall f1-score support blues 0.50 0.44 0.47 9 classical 1.00 0.67 0.80 9 country 0.50 0.89 0.64 9 disco 0.50 0.44 0.47 9 hiphop 0.70 0.78 0.74 9 jazz 0.60 0.67 0.63 9 metal 0.67 0.67 0.67 9 pop 0.75 1.00 0.86 9 reggae 0.80 0.44 0.57 9 rock 0.33 0.22 0.27 9 avg / total 0.64 0.62 0.61 90\nThat brings us to the end of this tutorial. The performance of our Hidden Markov models is relatively average, and obviously, there is a huge room for improvement. Indeed, the Hidden Markov Models are interpretable and powerful for time series data, however, they require a lot of fine-tuning (number of hidden states, input features …). Next, It would be good to try other approaches for classifying music genres such as a recurrent neural network (LSTM).']"	['<urn:uuid:37f9598e-7799-4b1c-a53b-b978ee9195e9>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	6	24	1428
58	I'm curious about groundhog families - how many babies do they usually have?	A mother groundhog typically has between 3 to 6 babies in a litter, though she may have up to 10 babies. These babies are called 'pups', 'kits' or sometimes 'cubs'. The female groundhog is pregnant for about 32 days before giving birth to these pups, which only weigh a few ounces when born.	"['The life of a groundhog baby starts underground in the burrow that the mother groundhog has dug.\nShe prepares a birthing den and lines it with soft grasses to keep her and her new family comfortable.\nThe burrow is also called a cette (pronounced ""set""), and may be up to 80 feet long, but the birthing chamber is usually only about three times the mother\'s length, retaining warmth and helping her to keep her babies close.\nA mother groundhog may have up to 10 babies, but 3 to 6 is the most common litter size. Baby groundhogs are called ""pups"", ""kits"" or sometimes ""cubs"".\nWhen they are full grown, they may be called woodchucks, or whistlepigs.\nNewborn baby woodchucks are pink, completely hairless, and only about the size of a matchbox car. Their ears are folded closed and their eyes don\'t even have lids to open yet.\nThe only senses that are working in a newborn groundhog are touch and smell.\nThe babies are very sensitive to temperature, and huddle next to their mother for warmth. If they should wander a little, they can find their way back to the nest by scent, and by the warmth given off by the bodies of their littermates and mother.\nThey make soft squeaking noises, and will cry louder if they are hungry or cold, so that their mother will know and tend to them.\nThe pups consume only mothers milk for the first 3 weeks, and then may begin to dig around in the burrow and chew on things they uncover, learning the skills of foraging. At 4 weeks their eyes open and they are fully furred too.\nTheir mother will bring soft foods like flowers and grass into the burrow for the youngsters to sample.\nThe biggest predators of baby groundhogs are snakes, who can detect their sounds and movements through the ground and will come right through the front door of the burrow.\nGopher snakes found in the deserts of the Southwest feed on young groundhogs, rabbits, prairie dogs and of course, gophers.\nAlthough baby groundhogs are no match for a full grown Gopher snake, mother groundhogs have been known to defend their babies bravely, and even kill and eat invading snakes.\nAs the little woodchucks grow, they slowly begin to leave the nest and explore for short periods.\nThey are extremely alert, and will dive back into the den at their mother\'s warning, which is a sharp, high-pitched whistle.\nAt about 44 days they are weaned, and can survive without mothers milk.\nThe mother groundhog starts to move them out of her den, and before they are even 2 months old, the little groundhogs have dug their own burrows and started living alone.\nThe only time groundhogs socialize is during mating season, in early spring, but many males will actually leave their burrows in February and March to visit the dens of females in their territory.\nThey may cruise around for a day or two, spending a few minutes at a time getting acquainted, and then go back to their burrows for another few weeks of hibernation.\nThis unusual activity, may well be the inspiration for the whole ""Groundhog day"" tradition, but it actually serves as a bit of a pre-courtship, which speeds up the whole process come spring.\nThe idea is that baby groundhogs need lots of time to get ready for their first winter, and timing mating just right is critical for the survival of the species. Too early, and their might not be enough forage for the baby groundhogs as they wean, too late and they won\'t be strong enough to last the winter.\nNevertheless, when spring comes around, males may still fight over the attention of local females. They will whistle and shriek and grab each other by the scruff, shaking mightily. Fights are not usually serious, though, and mating is usually a quick event as well.\nThe female is pregnant for about 32 days and gives birth to an average of 3 to 5 babies. Groundhog babies are called pups. The pups are only a few ounces when born. They are hairless, with closed eyes and ears, and are completely helpless.\nThe new pups grow very rapidly, and are weaned at about 5 weeks. They may move just a few yards away from the family sette, and dig their own burrow at just 6 weeks old. By the time they are 2 months old, they have moved off and are completely on their own.\nOct 11, 16 10:27 PM\nAfrican Animals facts photos and videos..Africa is a wonderland for animal lovers, and a schoolroom for anyone who wants to learn about nature, beauty and the rhythm of life\nOct 11, 16 10:26 PM\nBaboon facts, photos, videos and information - Baboons are very distinctive looking monkeys with long, dog-like snouts and close set eyes.\nOct 11, 16 10:25 PM\nGreat apes facts, photos and videos..Human beings did not evolve from chimpanzees, modern chimps and gorillas do not appear in the fossil records until much more recently than homo sapiens..']"	['<urn:uuid:36f50767-4ddf-46fd-9673-036727fda77c>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	13	53	845
59	digital revolution effects typography alignment	The digital revolution in the 1980s transformed typography in multiple ways. It introduced new forms of typography for PCs, music videos, and computer games, with innovations like laser printers enabling greater experimentation. However, this digitalization also brought challenges in text alignment. While software made basic alignment options widely accessible, achieving proper justified text became more complex, requiring careful attention to parameters like word-spacing, letter-spacing, and hyphenation to maintain readability and avoid issues like trapped white space or rivers in the text.	['There have been some magnificent trends in typography since the Second World War, established by ingenious individuals and marvelous movements.\nWhat follows are some of the standout highlights in typography from the latter half of the twentieth century.\nIf you like this article, you might be interested in some of our older articles on Free Professional Fonts, In Defense of Comic Sans, Best Sites to Download Free Fonts, and Text Tutorials.\n- Herman Zapf (1918-, Germany) is one of the most prolific and important designers of the twentieth century. Throughout his long career he has created types for hot metal composition, phototyping and ultimately the digital revolution. Among his famous types are Melior and Optima. A true giant of typography.\n- One of the most popular forms of typography in the 1950s, the International Style (also referred to as ‘Swiss School’) assimilated the typographical principles of Bauhaus, De Stilj and Jan Tschichold’s New Typography, negating decoration and emphasizing the social potency of typography.\n- Paul Rand (1914-1996, USA) was an incredibly influential member of the ‘New York School’, greatly raising the profile of art directors. Fond of simplicity and easily recognized designs, one of his most famous creations is the IBM logo. When art went pop in the 1960s, it radically affected aesthetics, reflecting the disposal of consumer culture. Pop art created an allusive form of typography, referencing consumer products. Campbell soup anybody?\nWhat would Jimi Hendrix and Frank Zappa albums look like without the flowing art nouveau influenced typography of the Psychedelia movement? Hallucinogens greatly inspired the wild textual and pictorial distortions.\n- In the 1970s the earliest computer typesetting was created by fusing photocomposition with new digital techniques. Ultimately, this revolutionized the capabilities of design professionals to design on their computer screens.\n- Based in Madison Avenue, New York, the International Typeface Corporation (ITC) promoted the rights of designers and stimulated the creation of new designs and type revivals. Ed Benguiat (1927-, USA) and Tony Stan (1917-1988, USA) were two of their most famous designers.\nThe digital revolution in the 1980s saw new forms of typography created for PCs, music videos and computer games. Films like Tron helped disseminate the new digital aesthetic. Print was revolutionised with the development of laser printers and a great spirit of experimentation swept through professional design.\n- Neville Brody (1957-, UK) revolutionist magazine graphic design working for The Face and Arena throughout the 80s. Top types included Insignia (1989), Arcadia (1990) and FF Harlem (1993).\n- Founded in 1981 by Matthew Carter and Mike Parker, Bitstream was the first digital type foundry and continues to operate today. Developing fonts for businesses, Carter created fonts like Charter (1993) and Verdana (1994).\n- Adobe started in 1982, creating the PostScript graphics programmed, which continues to be the standard graphic design language. Sumner Stone (1945-, USA) was extremely influential as Adobe Director of Typography (1984-1991), creating the Stone series of fonts (1987), Print (1991) and EndsMeansMends (1991).\n- When the Macintosh was launched in 1984, its initially low resolution demanded new screen fonts. Set up in 1984 by Zuzana Licko (1961, Czechoslovakia) and Rudy Vanderlans (1955, Holland), Emigre magazine addressed the issue of Mac font limitations, creating fonts like Modula (1985), Matrix (1986), Senator (1988) and Dogma (1994).\n- In 1990, the famous functional type Arial was designed for Microsoft by Monotype, quickly becoming known around the globe due to it being the primary Windows font. Also in 1990, Barry Deck (1952-, USA) designed Template Gothic, which became one of the most successful fonts of the 1990s.\n- Postmodern typography became all the rage in the 90s, as graphic designers like David Carson (1956-, USA) and Carlos Segura (1956-, Cuba) broke all the established rules, altering body text, flipping letters and creating fonts so decorative they became more akin to visuals. There wild styles continue to influence graphic designers today.', 'This post is an introduction to alignment typesetting and an encouragement to look at the details of alignment choices out in the world. Regarding text alignment, there is left aligned, centered, right aligned, and justified text.\nIn left-aligned text, the words are in rows one right below the other on the left side of the page, and the right side of the text is uneven. This uneveness of non-aligned text is called a “rag.”\nIn right-aligned text, the words line up on the right side of the page, and the left side is uneven.\n♦ left aligned = flush left = ragged right = rag right\n♦ right aligned = flush right = ragged left = rag left\nIn center-aligned text, text is equidistant from a center point.\nIn justified text, all the lines of type are the same length, so text on both the left side and right side are aligned.\nIn the West, most documents we write (type) today are left aligned, and we let our software take care of the rag. Word-spacing and letter-spacing are done automatically for optimal readability, no matter our software—Adobe InDesign, Quark, MS Word, or other writing software application.\nYou can chose basic alignment settings in word processing applications, and there are more alignment options in layout applications such as InDesign.\nText Alignment and White Space\nAlthough most of your work will be with left-aligned text, there are times when a right-aligned block of text can serve you well. For example, when an image is placed on the right side of a page, right-aligned type anchors text and image. In the first example below, there is a gap between the text and the image, leading to “trapped white space.” In the second example, the text is right-aligned, freeing up the white space and connecting type to image.\nThere are also times when right-aligned text balances an image on the left or adds a dynamic alternative to the predictability of left-aligned text. This is often seen in business card designs.\nLove the look of justified text? Just Hit the Justify Button?\nJustified text is what we are used to seeing in our newspapers, novels, and text books. However, these printed materials have been typeset by printers or designers, and there is skill involved. Yes, a quickie justify—just hit the justify button—is possible, even in MS Word, and sometimes this will look just fine. However, automatic justification can result in some problems for your readers. See the three versions of justified text below of the quote from Designing with Type by James Craig and William Bevington, page 99.\nProblem #1: You could end up with words and letters too tightly packed together resulting in text that is hard to read.\nProblem #2: You could end up with distracting variations in word-spacing or letter-spacing or “rivers” of white space making distracting columns running through the text, which can make for ugly and difficult-to-read pages.\nRivers are more likely to happen when you do not allow text to be hyphenated, when the line length produces a narrow column, and when word-spacing (tracking) and letter-spacing (kerning) are not adjusted. For more examples, google “rivers in justified text.”\nFor those working with Adobe InDesign who want to know how to create well-justified text in long documents, see Designing a Book by Nigel French on Lynda.com. The entire video book course (4–5 hours) is well worth the time spent.\nThe three most relevant movies to this post are (1) Chapter 3: Creating and Applying Paragraph Styles, (2) Chapter 4: Styling the Text, and (3) Chapter 6: Finessing the Text.\nProper justified type takes time to master, but it is worth the effort to learn more about word-spacing, letter-spacing, orphans, widows, and other type elements if you want to craft beautiful readable typography.\nBefore I spent time learning from Nigel French how to set justification parameters within Adobe InDesign, I never used justified text for long documents. I had seen too many bad examples of sloppy justified text. I would, however, justify short paragraphs of text, but this could be time-consuming. In what was an agonizingly slow process, I have manually changed word-spacing, font size, and letter-spacing, and sometimes word choice for each line of text to get typography that worked. All to prevent the rivers or distracting white space between words.\nIn the HPV poster below (created in Adobe Illustrator), the justified text was done manually by changing the length of the lines, color, font size, word choice, letter-spacing (kerning), and word-spacing (tracking) to eliminate rivers and wide gaps between words.\n- Craig, James and William Bevington. Designing with Type: A Basic Course in Typography (4th ed). New York: Watson-Guptil. 1999.\n- Dowding, Geoffrey. Finer Points in the Spacing & Arrangement of Type (Revised ed.). Vancouver, BC: Hartley & Marks Publishers. 1998.\n- Highsmith, Cyrus. Inside Paragraphs Typographic Fundamentals. Boston, MA: Font Bureau. 2012.Middendorp, Jan. Shaping Text: Type, Typography and the Reader. Amsterdam, NL: BIS Publishers. 2011.\n- Strizver, Ilene. Fontology: Justified.\nhttp://www.fonts.com/content/learning/fontology/level-2/text-typography/justified-type. Accessed May 14, 2015.\nTechnical Resource Notes\n- Lynda.com is a great online resource for learning about graphic design and typography, whether you are interested in learning the principles of design or honing your technical skills in various applications, including Adobe InDesign and MS Word.\n- While this is a subscription service, there are courses free without a subscription and there is also often a free month trial subscription where all the video courses would be available to you.\nTwo Short Free Online Lynda.com Videos on Kerning and Tracking\n- Kerning and Kerning Pairs: http://www.lynda.com/Design-Design-Skills-tutorials/Kerning-kerning-pairs/106698/121861-4.html\n- Kerning and Tracking: http://www.lynda.com/player/embed/110189?fs=3&tr=utm_medium=ldc-partner;utm_source=SSPRC;utm_content=11889;utm_campaign=CD18364;aid=CD18364;bid=11889&w=560&h=315&subID5=110189http://www.lynda.com/player/embed/110189?fs=3&tr=utm_medium=ldc-\n©2015 Barbara Kristaponis. To re-use or re-post images/graphics copyrighted by Kristaponis from this post, just ask. If there is no copyright symbol connected to an image or graphic, these are licensed under a creative commons license. You are free to use these but be sure to credit the artist, in this case Barbara Kristaponis. Thanks.']	['<urn:uuid:9e12c84a-6aec-4131-a233-7bdb791050e0>', '<urn:uuid:67d7e670-18c0-4fb7-88b2-f864ae88adc4>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	5	81	1633
60	How do studio training and professional development intersect in art?	Studio training and professional development in art are closely integrated. In studio settings, students develop technical skills through hands-on work with various media, including painting, printmaking, ceramics, and sculpture, using professional equipment and receiving staff guidance. This practical training directly supports diverse career paths: fine artists use these skills in galleries and private studios, while others apply them to commercial work in animation, photography, or design. Programs typically combine practical studio work with theoretical knowledge like art history, preparing students for either traditional artistic careers where 60% are self-employed, or emerging fields like digital animation and art therapy.	['Loudoun Art Program\nWelcome to the Art Department at NOVA’s Loudoun Campus!\nOur faculty are an energetic and imaginative group of fine artists and art historians. We offer classes from beginner to advance levels in painting, drawing, design, printmaking, ceramics, sculpture and art history. Our classes are beneficial to students who may want to transfer to a four-year program or who are simply interested in developing their artistic skills without degree requirements.\nThe art facilities on the Loudoun Campus have the latest equipment to meet your artistic needs and someone on our staff is always available to answer your questions.\n- Art History\nPainting courses provide students with skills needed to investigate and experiment with visual communication, both traditional and contemporary. Students are encouraged to apply established theories, concepts and media in the development of their own art, eventually leading up to expression of individualized concepts, feelings, viewpoints, style and techniques.\nDrawing classes focus on the fundamentals of drawing from observation. Working from still life, the human figure, and landscape, students develop basic technical, perceptual, and conceptual skills of drawing. Through lab projects, sketchbook assignments, and class critiques, students develop ways to render objects/figures from life.\nDesign courses concentrate on two and three dimensional form through the manipulation of black, white and gray in studying the elements of design and their relationships. Students present visual 2 and 3D solutions to a series of problems incorporating the concepts of point, line, value, mass, and texture as well as illusions of space plane, monolithic, concave, convex and penetrated mass. The study of the interaction of color including the properties of color and color materials, color analysis and the relativity of color is addressed in advanced course work. Classes are comprised of lectures, class discussion, and individual and group critiques.\nStudents are introduced to etching, lithography and relief with a structured survey studio course. Techniques are presented through lecture, demonstration, and examples from the print studio’s extensive print collection. Learning is hands-on with students working with a number of tools and presses. Lab projects and class critiques encourage students to develop and demonstrate their understanding of printmaking. Opportunities to specialize in printmaking techniques of particular interest to the student including color printmaking techniques and paper making are available in advanced courses.\nThe Loudoun Campus ceramics program offers classes in beginning through advanced ceramics. While addressing both technical and aesthetic issues, the program fosters the development of creativity, artistic vision, and personal style. Both hand-building and wheel-thrown techniques are available.\nThe art history program at the Loudoun campus has classes in the history of Western art from the beginning of time to the 21st century, courses in Far Eastern art, and specialized courses in the history of design and modern art. Most of the history courses require museum visits and, occasionally, field trips with the instructor. One can major in art history and receive an Associate degree in liberal arts with a specialization in art history. Classes in American art, Renaissance and Baroque art, and museum studies are available at the Manassas and Annandale campuses to supplement studies at the Loudoun Campus.', 'Aspiring artists have more career options today than they ever have before. The digital age has opened up not only the way art can be taught, but also the types of art that can be created. Each type of art degree requires that the artist master some basic skills sets such as sketching, drawing and painting with each taking on a slightly different skill set as the artist’s education progresses. Additionally, the type of degree depends on the end career goal of the artist. While some artists want to work in a gallery or museum, others have a more commercial venue in mind or even art therapy.\nFine Art (Drawing, Painting, Sculpting)\nArtists who study fine art display their work in galleries and museums, according to the Bureau of Labor and Statistics. They make their work environments in lofts, artists’ studios or their homes. Artists work in a variety of media, including paper, canvas, sculpture, art metals or ceramics. Up to 60 percent of fine artists are self-employed, although not all can make all of their living from art. While some of these artists take up art-related careers such as museum curating, teaching or fine art publishing, others work other jobs not related to art.\nAnimation careers become the outlet for many aspiring artists working in the field. With the proliferation of video games the career prospects for many aspiring artists has improved dramatically. Artists looking to become animators can find animation degrees online and on traditional campuses. According to All Art Schools, would-be animators learn a variety of skills in animation school including how to paint and draw, how to draw storyboards and how to create characters. They also become acquainted with the latest animation software.\nWith the pioneering work of photographer Alfred Stieglitz, photography became a fine art in its own right during the 20th century. Aspiring photographers take their place in the ranks of many of the finest art schools in the country and learn basic art skills like drawing and art history in addition to their photography related studies. Additionally, while many photographers work in still photography, some get degrees in photography related art degrees, which include film and videography as well as digital photography, according to the Bureau of Labor and Statistics.\nIllustration and Design\nIllustrators and designers work in complementary fields to one another, and designers as well as illustrators often do the same core course work in their art degree programs. These artists create graphics that offer insight into a client or organization’s mission, advertising campaign or other business related topics that need visuals in order to get the message across.\nFor those artists who are equally concerned about creating art as they are about helping people, there is the profession of art therapy. The foundations of art therapy rely on the use of the visual arts-drawing, painting, sculpture and other forms of visual art-to access the inner world of the therapy patient. Through the use of art and counseling, an art therapist helps treat many issues such as depression, anxiety or even pain therapies like migraine relief. To prepare for a career in art therapy, aspiring art therapists should take at least 15 credits of art plus 12 credits of psychology before going on to get a Master’s degree in art therapy.\nAllied Health Profession, Art Therapist, AHP\nBureau of Labor and Statistics, Artists and Art Related Workers, BLS\nBureau of Labor and Statistics, Graphic Designers, BLS\nBureau of Labor and Statistics, Photographers, BLS\nMasters of Photography, Alfred Stieglitz, MOP\nAll Art Schools, Animation Career, AAS\nArt Study, Choosing Art School: Type, AS']	['<urn:uuid:50068827-9f0d-4421-b148-5101cb5f6601>', '<urn:uuid:0364b8e6-9b51-421d-9d1c-38fe1d7658d3>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	10	98	1119
61	difference between fuel tank vs resonator	The fuel tank and resonator have distinct functions in a vehicle. The fuel tank is designed to store gasoline or diesel and must be sealed to prevent contamination, with vents to let air replace used fuel. The resonator, on the other hand, is like a mini muffler that acts as an echo chamber for exhaust, where exhaust energy bounces around to change and reduce engine noise to more acceptable levels.	['The Fuel system is the heart of a car. It stores and delivers fuel to the engine, where it is mixed with air and burned to create energy. The fuel pump draws fuel from the tank and delivers it to the injectors. Then, it travels through the fuel lines, filters, and pressure regulator to reach the cylinder chamber. The fuel is burned and the engine starts. To operate, the Fuel system must function properly. But what exactly does it do?\nThe Fuel system is comprised of various components. The fuel tank stores the gasoline or diesel. The fuel pump then draws the fuel from the tank and delivers it to the engine through a fuel filter. The cylinder chamber receives this fuel and the system works to ensure that it burns efficiently. The nozzles are located in the gas tank and deliver the fuel to the cylinder chamber. The Fuel system is the final component of the engine and is a crucial component for good performance.\nThe fuel system is made up of various parts. There are two basic parts: the fuel tank and the pump. Each one is designed for a specific purpose. The tanks are closed to prevent contamination, and the vents are designed to let air into them. The air in the system replaces the fuel that is demanded by the engine. To operate a vehicle correctly, the Fuel systems must function. The system must also be functional and efficient.\nThe Fuel system is made up of several components. Each component is important to the vehicle’s performance. The first part of the fuel system is the fuel tank. The second part is the fuel pump. Both of these components must function efficiently. The fuel pump and the injectors need to work properly. They must be in good condition to keep the engine running smoothly. And the entire Fuel system must function correctly for the vehicle to perform at its best.\nThe fuel system is composed of several components. A fuel tank and fuel lines are essential to the car’s engine and must function properly. When the fuel tanks are malfunctioning, the car’s performance is affected. The last part of the Fuel System is the carburetor. It must be installed in a way that it does not affect the other components of the vehicle. If you’re not a mechanic, there are plenty of ways to repair your vehicle’s fuel.\nThere are several different types of fuel tanks used in a vehicle. Each is designed for a specific purpose. All tanks must be closed to avoid contamination. Some have vents that let air in. The fuel must be vented to keep air in the tank. A car’s gas tank is a system. If the fuel pressure is too low, it cannot function properly. The regulator keeps pressure at a stable level. The fuel pump controls the pressurization.\nThe Fuel System also includes different types of fuel tanks. Each tank is designed for a specific purpose. The fuel tank must be sealed properly and vented properly so that air cannot enter it. You must make sure the fuel pump is functioning correctly and that the hoses are long enough. In addition, make sure to check the valves on the gas tank to ensure they are working properly. Inspecting the valves is important for the fuel tank and the filter are connected.\nThe Fuel system contains various components and is essential to the overall performance of a car. The fuel tank is the main storage area for gasoline and diesel. The fuel pump pumps the fuel through the filter and delivers it to the cylinder chamber. This system also stores fuel for cars with automatic transmissions. It is usually located in the rear of the car. Its purpose is to supply the engine with the proper amount of fuel. The gas tank also controls the temperature of the fuel.\nThe Fuel System is made up of many components. A fuel tank is the source of fuel. The pump delivers the gas to the engine. The injectors, for example, send the fuel to the engine. The pump pumps the gas into the cylinders. Then, the fuel is burned in the engine. The combustion process produces energy and power, and the fuel system is an essential part of a vehicle. It is essential for a car to run.', 'A vehicle’s exhaust system is designed to direct harmful gases away from the driver and passengers, reduce the emissions the vehicle releases into the environment, control the delivery of hot exhaust, provide information to the vehicle’s computer to improve vehicle performance, and significantly reduce the amount of noise the vehicle makes.\nThe only component of the exhaust system that can easily be seen is the tailpipe, located under the back of the vehicle. The entire system, however, is actually much larger and more complex. The exhaust system begins at the engine combustion chambers and runs along the undercarriage of the vehicle, eventually ending with the visible tailpipe.\nThe major components of the exhaust system include the exhaust manifold(s), oxygen sensors, catalytic converter(s), resonator, exhaust pipes, muffler, and tailpipe.\nThe exhaust manifold is connected directly to the engine and has the job of harnessing the combustion gases into the exhaust system. It is possible, depending on the size of the engine, for there to be two exhaust manifolds. The manifold, comprised of smooth curving passages to improve the flow of exhaust, can be made of steel, aluminum, stainless steel, or more commonly, cast iron. Cracking, warping, and leaking due to broken mounting bolts are common manifold ailments.\nAll modern fuel injected cars utilize oxygen (O2) sensors to measure how much oxygen is present in the exhaust. From this information, the computer can add or subtract fuel to obtain the correct mixture for maximum fuel economy and optimal performance. In most cases, there are two O2 sensors in a four-cylinder engine. One is located on the exhaust manifold before the catalytic converter. The other is located after the catalytic converter on the exhaust pipe. In V6 or V8 engines there are four O2 sensors. Two O2 sensors are located before the catalytic converter on each cylinder bank. The other two O2 sensors are located after the catalytic converter on respective banks.\nThe catalytic converter reduces harmful emissions from engine exhaust. The converter, mounted between the exhaust manifold and the muffler, uses a combination of heat and metals that act as catalysts. A catalyst is a metal, or sometimes a chemical, that causes other chemicals to go through a reaction without being affected itself. The inside of the catalytic converter consists of materials such as platinum, palladium, and rhodium. These materials are the catalyst that causes the carbon monoxide and hydrocarbons to react and produce water vapor and carbon dioxide which are much less harmful to the atmosphere.\nThe resonator is like a mini muffler with less restriction. It is an empty echo chamber the exhaust travels through where the exhaust energy bounces around, resonates, and some of the noises cancel each other out. A resonator doesn’t just remove sound, it changes it to be more acceptable. The resonator can be either before or after the muffler in the exhaust system.\nExhaust pipes connect all the components of the exhaust system. They are designed to route the exhaust in the most efficient way possible as it travels toward the rear of the vehicle, and to keep the hot exhaust away from heat sensitive components in the engine compartment and along the undercarriage of the vehicle. Exhaust piping is usually made of steel, but can be aluminized steel tubing, or stainless steel, which lasts longer due to its corrosion resistance. Connections are generally made with clamps, gaskets, or welds.\nThe muffler quiets the noise of the engine. There are two kinds of mufflers. One uses baffled chambers to reduce noise. As sound waves move through this type of muffler, they bounce off the baffles and expend their energy inside the muffler, losing force and volume. The other type forces the exhaust straight through a perforated pipe that contains metal, fiberglass, or some other kind of sound-absorbing material. This type of muffler is designed to reduce back pressure, which occurs when exhaust travels back up the pipes toward the engine, and consequently makes more noise.\nThe tailpipe comes out of the muffler, past the rear bumper of the vehicle, and directs exhaust gases away from the vehicle. Some vehicles may have more than one tailpipe. The tailpipe often ends with just a straight or angled cut, but may include a fancy tip. The tailpipe is often larger in diameter than the rest of the exhaust system. This produces a final reduction in exhaust pressure, and is sometimes used to enhance the appearance of the vehicle.\nThe worst enemy of an exhaust system is corrosion, more commonly known as rust. Moisture or water vapor is present in the exhaust as a by-product of combustion and the catalytic converter. Corrosion may also result from outside elements such as rain, snow, and salt. If you live in an area where road salt is used during the winter, make sure to wash the underside of your vehicle with water every few weeks. Salt speeds up the corrosion process and getting it off as soon as possible will help stop the corrosion.\nOther symptoms of exhaust system problems may include decreased power, decreased fuel economy, hissing noises, metallic rattling noises, an overly loud engine, exhaust fumes, the presence of a check engine light, and physical damage due to the system hanging low beneath the vehicle.\nMany factors, such as climate and driving conditions, can affect the life of your exhaust system. The cleaner the engine runs, the cleaner the exhaust traveling through your exhaust system will be, which will help prevent catalytic converter and O2 sensor failures. It is important to have your exhaust system inspected by an ASE certified technician at least annually, or whenever you experience symptoms possibly related to the exhaust system. Each component should be inspected for damage, rust, leaks, broken bolts, cracks, noises, secure mounting, and to make sure all critical components are present.\nIf you are buying a used vehicle, make sure the entire exhaust system has been thoroughly inspected before you make your purchasing decision. Having the exhaust system inspected will ensure the vehicle is operating efficiently, will help keep your passengers safe, and may help you avoid costly repairs.']	['<urn:uuid:b3a51a36-242e-48ed-9e18-43bc68f34394>', '<urn:uuid:d18ede61-ba1a-41a7-9dee-f8068a61c1d2>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T15:30:12.136021	6	70	1742
62	What's the best deck design for a sloped backyard?	For a sloped backyard, a raised deck or multilevel deck design works best. A raised deck is ideal when the first level of your home is above grade or with steep terrain. You could even add an under-deck patio below if the lot allows. To ensure structural integrity on sloped terrain, it's crucial to implement a continuous load path (CLP) that transfers loads through the frame to the ground and adjacent support structure. For elevated decks, consulting both geotechnical and structural engineers is essential. The design can incorporate multiple platform decks at different levels, connected by steps, creating an engaging outdoor space with dedicated areas for different activities. For stability, if the deck's width is less than or equal to its depth from the house, additional bracing may be required.	['What is CLP? It stands for Continuous Load Path. A continuous load path is construction method that creates a series of solid connections within your deck’s structure. Why is it important? There are many different kinds of forces that work against your deck’s structure and weaken it. Having a continuous load path helps your deck withstand:\n- Gravity – downward pressure typically caused from people standing on the deck or from snow and ice.\n- Lateral pressure – a back and forth (horizontal) motion caused by people walking on the deck and/or leaning on a railing. Wind and earthquakes can also create lateral movement.\n- Uplift – wind flows under the deck creating a lifting effect. A person standing on the overhang of the deck also creates upward pressure on the connection that attaches the deck to the adjacent support structure (typically your home).\nA continuous load path transfers load through its frame to the ground and adjacent support structure (commonly your home).\nOk sounds good – what do I need? Start with metal connectors. Connecting the frame of a house together with metal connectors increases the strength of a home. Connections made with metal connectors are much more difficult to be pulled apart. It takes greater force to separate a connector than a toe-nailed connection. Adding connectors is a relatively inexpensive investment. Depending on the size of your deck, it can cost anywhere between a few hundred dollars to maybe a thousand dollars.\nAll decks need lateral stability, but if you are building an elevated deck, the need for bracing increases. Generally speaking, if the width of a deck (across the house) is less than or equal to the depth (perpendicular to the house), bracing may be required. The wider a deck is compared with its depth, the easier it is to stabilize.\nIf the decking boards will be laid perpendicular or parallel to the house, the deck will need angle braces between the columns for elevated decks, along with bracing under the joists. Diagonally installed decking significantly stabilizes the structure by tying all the floor joist structure back to the attachment to the house – and it looks clean and cosmetically appealing.\nAlso plan for what will be placed on the deck. Reinforce the structure where a heavy grill or other item will add a constant load; doubling or tripling the joists where the grill will be is usually adequate. Supporting a spa requires additional columns and beams that make a continuous load path down to the footings. Double the joists under spas, and shorten the span between beams.\nA continuous load path is like a chain that ties the house together from the roof to the foundation. For decks, it’s just as important. Especially for elevated decks, consulting both a geotechnical and a structural engineer is crucial. It’s not cheap, but the more details you can provide the engineers for review, the less their charges will be. The alternative, risking a deck failure only a few years after it’s built, it no fun for anyone.Share It', 'Quality matters when hiring for a big project. Call a Best Pick now!\nHow to Choose the Right Deck DesignFebruary 24th, 2014 by\nSome home improvements are demanding not because of the large amount of work or the high cost but because of the numerous design choices that must be made. Building a deck is a good example; creating a structure intended for recreation and relaxation, limited only by your imagination, may tax your creative powers.\nWhen trying to choose the right deck design, focus your process by asking a few key questions. An examination of your existing situation will reveal challenges and opportunities that shape what kind of deck you build.\nWill your deck be used for entertaining? If you plan to invite the neighborhood to a deck party, you’ll need sufficient space—room for guests, a grill, a table, and perhaps a bar as well. A roomy, ground-level platform deck should be both large and safe enough for all invited. Stairs leading down to the backyard will allow the party to spread out further.\nIs privacy a priority? Without a fence, hedge, or other barrier between you and your neighbors, it may be difficult to find privacy, even on your own deck. However, there are several ways you can transform the structure into a more discreet retreat. First, consider a raised deck built off the second story, which will make you a little more aloof in addition to improving the view. Latticework panels or a privacy fence integrated into the deck can further increase the levels of peace and quiet.\nHow much sun should your deck get during the day? In the US, a southern exposure often guarantees maximum sunshine. However, if you live in a region with sweltering summer temperatures, you may want to limit the amount of sun your deck receives. Keeping in mind that shade patterns are seasonal, study the levels of sun and shade in your yard during the times of the day when you would be most likely to use your deck. If the planned building site seems too sunny, you may be able to relocate to a spot where the house or trees cast a few shadows. If not, an umbrella or awning over your deck is an attractive and cost-effective way to shun the sun.\nDecks can be low to the ground or raised to accommodate a steep yard or provide second-story access. Connecting decks of several different elevations with stairs is also an option.\nPlatform decks. Built low to the ground, usually on level lots, platform decks are generally expansive and safely able to accommodate a lot of guests. A low platform deck may also be the best design for parents of young children who might find a way to thwart safety railings.\nRaised decks. If the first level of your home is above grade or if you have a steep backyard, a raised deck may be in order. A raised deck is also the obvious choice for a deck built onto the second floor; if your lot is flat, you could even add an under-deck patio below. Of course, reliable railings are a priority, as is maintenance that will ensure your raised deck remains sturdy for years to come.\nMultilevel decks. Several platform decks, each at a different level, can be connected with steps to create a more engaging outdoor living space. Different levels can even be dedicated to different aspects of an outdoor gathering, such as grilling, mingling, or playing board games.\nOther Design Characteristics\nLooking at well-executed examples of decks is the best way to generate ideas. An experienced contractor can contribute by showing you examples of his or her own work. Far from being utilitarian platforms, you may be surprised at the design elements decks can incorporate.\nThe direction of the planks is one often-overlooked feature that can bring a great deal of dynamism to a deck. Aligning the planks at right angles or parallel to the edge of the house is fine, but installing the planks on a diagonal is generally much more visually interesting.\nRather than being a simple square, your deck could have curved sides. Although installing a curved rim joist—which involves making small cuts called kerfs into one side of the board to make it more pliant—might be too advanced for a do-it-yourselfer, a professional contractor will be able to handle the task without any problems. The final effect of a deck with curved sides will be to create a transition between the straight lines of your house and the more natural contours of your backyard.\nFinally, remember that no deck is truly complete without some creative landscaping. Design your deck while keeping in mind which plants you’ll install to add shade or decoration. An outdoor lighting system can also make your deck beautiful and usable at night.\nMaking the Deck a Reality\nWhen it’s time to move from the conceptual phase to something more concrete, drawing may be helpful. Measure the dimensions of your house and backyard, and then use graph paper or a ruler to make a proportional drawing of the area. Make the drawing large—perhaps using a one-inch-per-foot scale—so that you can demonstrate your proposed design in detail. Your contractor can elaborate on the design and eventually use the site plan to apply for a building permit.\nClearly, there are many design decisions that go into creating a deck. However, they’re easy enough to handle; you can get ideas by determining the main function of your deck, examining your existing backyard, and even sketching a scale diagram of your vision. Throughout the design phase, consult with a reputable contractor, whose years of experience may be your biggest asset.']	['<urn:uuid:9a52bafa-dd24-4b1e-9d4a-7c6edac079e9>', '<urn:uuid:4e39c1e1-c472-4440-9184-dab93a1a9f22>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T15:30:12.136021	9	130	1457
63	hydroponic greenhouse water usage truffle spores	Hydroponic greenhouse systems use only one-tenth of the water required for traditional soil growing, making them highly drought-resistant. These systems can effectively use harvested rainwater stored underground, which is superior to treated water for hydroponics. This water efficiency could be particularly beneficial for truffle cultivation, which requires specific moisture levels. However, truffle reproduction specifically depends on spores being carried by animals like wild boars and foxes in their feces, and the spores need to mature during winter to form new truffles, making traditional outdoor cultivation necessary for natural truffle reproduction.	['Truffles are hypogeal fungi that grow underground and live in symbiosis with the roots of certain trees.\nThis mushroom is a delicacy and a health delight.\nTruffles do not get the sun and they receive carbohydrates and vitamins from the roots of the plant that the tree obtains through photosynthesis. The plant receives nutrients from the soil and water from the fungus in addition to promoting the expansion of its roots.\nThe areas without weeds around the tree are produced because the fungus extracts its nutrients causing them to die and in turn these herbs decompose with moisture and nourish the fungus and the tree. Therefore, a degree of humidity is important.\nThe reproduction of the fungus is by spores carried by animals such as wild boars, foxes, rabbits, mice, in their feces. In turn, the animals enjoy the pleasure of its taste. They find them by their pleasant smell. Truffles, when buried, cannot spread their spores.\nWhen they are extracted there are also spores, so we will have truffles in the same place again.\nThese spores mature during the winter to form the new truffles\nSpain Produces between 30% and 40% of all world production.\n- Winter black : it occurs from November to March.\n- Autumn truffles : from November to March.\n- Summer : from May to September.\n- Asian : from November to March.\n- Blanca del Piedmont : from October to December and the most sought after on the market. One of the most expensive condiments in the kitchen.\nIt is convenient that they produce shade and at the same time separated enough so that they are aerated and brush can be produced that secretes nutrients to the soil.\nThe type of tree depends on the type of terrain. They can be wild trees or plantations prepared for this crop.\nIn no case are puddles suitable. The most beneficial thing is a microclimate of humidity that favors the proliferation of truffles. They resort to covering the area with weeds or even black plastics.\nHeavy frosts do not suit him. The land must be sheltered if it is cold. Summers of medium temperature.\nIn areas like Teruel, tree plantations are being made to grow truffles.\nFases de la producción:\n- Choose the terrain, limestone and with a suitable pH. Plantations work best in areas where truffle trees are already known to exist.\n- Plant the trees, approximately 300 / ha, which must already be mycorrhized, that is, they already carry spores in their roots and are adapted to the environment.\n- Controlled irrigation, especially in the area of burns that occur around the tree by the action of the truffle.\n- Apply fertilizers when production declines.\n- Do proper pruning.\n- Fencing the land to prevent the entry of wild boar fauna.\nHarvesting the truffles\nBeing a product buried in the roots of a tree, in Spain normally holm oaks, its collection is complicated.\nThere are usually marked areas that are visualized under the tree due to the lack of undergrowth, but the exact area is usually marked with a special dog, which detects it by smell and begins to dig over it.\nWith a knife, the truffles are extracted and the hole is filled with soil and weeds to start the cycle again. Not all of them should be removed.\nDevices that detect odor are being developed.\nOnce extracted from the earth, they are immersed in water and with the help of a brush the remains of earth are removed.', 'In a previous post, we introduced our proposal for Chicago’s West Side called AgriFlats, the final piece of a Food Innovation District aiming to revitalize neighborhoods by starting with “Why?” This post drills down on the details.\nMerely reducing our carbon emissions is insufficient to stifle global warming. We have to return vast amounts of carbon to living ecosystems. AgriFlats will demonstrate how to do that, by using dense urban agriculture to liberate outsize tracts of land in rural areas currently being traditionally farmed.\nThe primary energy powering AgriFlats is renewable solar energy. The transparent skin of a greenhouse admits a broad range of solar energy that gets trapped inside, heating the interior during winter. During extreme cold, active energy harvested by solar thermal collectors mounted on the packing house, live-work, and residential rooftops will supplement the passive solar energy. These collectors will heat circulating water with the sun’s energy and pump it to underground storage tanks, insulated with vacuums like thermos bottles. Instead of relying on ancient solar energy banked in carbon fossil fuels to heat its greenhouses, AgriFlats will use energy stored from the previous season.\nSince Chicago’s climate demands three times as much energy annually to warm its buildings and cool them, AgriFlats prioritizes low-grade thermal energy over high-grade photovoltaic energy. Solar thermal collectors will harvest much more of the sun’s energy, at up to 70 percent of the sun’s available energy, compared with only 20 percent for photovoltaic panels.\nSummertime cooling will still be necessary. To provide passive cooling, roof vents in the Dutch Venlo greenhouses will open, allowing hot air to escape. Circulating water stored underground at a stable 55F temperature will cool the incoming air. Likewise, the nutrient solution, which resides below grade, will also contribute to cooling the greenhouse during the summer.\nThe residential units along Kildare will be heated and cooled with an active-slab radiant system that maximizes the space’s comfort with minimal energy use. Combined with the hyper-insulated, thermally massive structure, residents will have their energy needs met on site. Energy recovery ventilators (ERVs) will introduce fresh air into the apartments, recycling the greenhouse’s heat and moisture.\nAgriFlats can generate electricity on-site with a Combined Heat and Power system (CHP) fueled by gas produced by an anaerobic biomass digester on-site or located in Auburn Gresham’s Urban Farm. These digesters can also produce natural gas for heating. See the illustration at the top of the post for a diagram of the closed-loop process. The biomass will come from the growing operation itself and the waste from restaurants and community members connected with AgriFlats. Growers can purchase any additional electrical energy required from off-site renewable sources.\nIn addition to powering pumps, the facility will use electricity to illuminate the greenhouses during winter and the residences year-round. The apartment interiors will be illuminated solely with LED sources to conserve energy. During winter, high-efficiency HID lamps, or spectrally selective LEDs will light the interior to extend the natural day to 14 hours. All waste heat emitted from these light fixtures will supplement greenhouse space heating. Greenhouse lighting will only be visible at night along Kostner.\nIf a shortfall of stored heat occurs during winter, an anaerobic digester’s gas will provide the energy. If additional heat is required, natural gas obtained from the grid will heat the space. Unlike most projects, though, equipment will capture the CO2 liberated by fossil fuel combustion for enriching the greenhouse, where rapidly growing plants will sequester it. In addition to 13 nutrients and water, plants solely build themselves with the atmospheric elements of carbon, hydrogen, and oxygen.\nOur team designed AgriFlats to adapt to the escalating challenges of climate change. In 2019, Illinois farmers experienced the worst growing conditions in over 50 years. Most importantly, AgriFlats will use the vast majority of rain and snow on-site for its internal processes, rendering stormwater irrelevant. Since rain is superior to treated water for hydroponics, it will be harvested and stored underground until needed, even when it comes in fits and spurts. After snows, some of the thermal energy stored in underground tanks heat the greenhouse’s gutters, promptly clearing the roof so growers can harvest the sun’s energy when skies clear.\nSince hydroponic and aquaponic growing uses a tenth of the water required for growing in soil, it is exponentially more resistant to drought. Engineers will size the underground process water reservoirs for 100-year drought conditions, securing the District’s food supply.\nAgriFlats will also collect rain for non-potable (grey-water) uses in the live-work units and residences, such as toilet flushing and laundry. Contractors will isolate these systems, which will divert twice their volume from City sewers, to maintain health by preventing cross-contamination. They will install low-flow fixtures throughout the residences, further conserving domestic water use. During the cooling season, valves will divert the condensate from cooling systems for grey-water use.\nAs proven by the efficiency of greenhouses in the Westland region of the Netherlands, Dutch Venlo greenhouses are machines for growing. Their clear height accommodates vertical vine crops like tomatoes, cucumbers, eggplant, peppers, squash, and strawberries. Growers use hydroponic drip irrigation systems, fed from pipes running along the greenhouse floor, to grow these crops.\nThe Dutch Venlo greenhouse’s wide bays also provide a broad area for growing leafy greens, vegetables, and herbs horizontally. Farmers can grow plants on rafts floating on aerated shallow ponds (deep water culture) or in continuously irrigated channels in a nutrient film technique (NFT) setup. Aquaponic growers can use the Dutch Venlo, too, since they can situate fish farm tanks next to hydroponic crops. In AgriFlats, the fish tanks will remain above ground to maintain ideal water temperatures throughout the year.\nYou can download our complete AgriFlats proposal here.\nSimilar Triple-Bottom-Line Enterprises\nProbably the best-known triple-bottom-line project similar, but much larger than, AgriFlats is AppHarvest in Moorehead, Kentucky. The B-Corp is seeking to become a publically-traded enterprise in early 2021. Their 60-acre production greenhouse, three times the size as AgriFlats, will harvest its first crop in 2021 in an area formerly dependent on coal mining. The CEA technology AppHarvest is deploying is nearly identical to what we proposed for AgriFlats. If successful, AppHarvest will be the third publicly-traded B-Corp on the NASDAQ, joining Esty. Illinois currently has 30 privately owned B-Corps, including five in the food industry.']	['<urn:uuid:3666bbbc-29ee-4541-b987-2e1bf7149b99>', '<urn:uuid:d3d4138d-1e96-4fdd-8b5b-43b55c5838eb>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T15:30:12.136021	6	90	1633
64	early spanish settler population patterns nuevo mexico historical documentation figures	The Oñate Expedition brought 336 settlers to establish New Mexico in 1598-1600, while earlier expeditions like Coronado's included 322 settlers exploring as far as Kansas in 1540-1542. The Spanish population patterns show that prior to 1620, approximately half of Iberia's male population came to settle the Americas. Historical documentation of these settlements comes from sources like Gaspar Pérez de Villagrá's 'Historia de la Nueva México', which chronicles the first Spanish-Mexican settlements around Santa Fe.	"['Site ContentsThe Conquistadors - A list of almost 2000 individuals that served the King with\nCortes and stayed in Mexico (with some notable exceptions).\nThe conquest of the Mexico lasted from 1519 to 1521. 5 pages.\nCoats of Arms - Conquest of Mexico - A list of 145 of the individuals that\nreceived coats\' of arms for their part in the conquest of Mexico from 1525 to 1589.\nThe Conquistadors of the Yucatan - A list of 160 of the individuals that served\nthe King with Francisco de Montejo from 1526 to 1546. 1 page\nThe Conquistadors of Nueva Galicia - A list of 280 of the individuals that\nserved the King with Nuno de Guzman in 1530. Many went on to settle the\narea and had links to the earlier Conquistadors. 1 page\nEarly Settlers of Nueva Galicia - 137 settlers arriving after the conquest of\nthe area. Some had links to the earlier Conquistadors. 2 pages\nThe Coronado Expedition - A list of 322 settlers that served with the Captain\nFrancisco Vazquez de Coronado. The expedition set out for New Mexico\nand Arizona but made it as far north as Kansas from 1540 to 1542.\nThe Luna Expedition - A list of the 251 settlers that served with the Captain\nTristán de Luna y Arellano. The expedition set out to settle Florida in 1559-\n1564. 1 page\nThe Onate Expedition - A list of the 336 settlers that served with the Captain\nGeneral Juan de Onate. The expedition set out to settle New Mexico in 1598-\n1600. 1 page\nNew Spain - 433 surnames of the early settlers making their way north\ninto Northeastern Mexico. Many descendants of the early Conquistadors.\nTheir date of first mention in public records for the area is listed in\nmost cases. 8 pages\n1700-1726 - New wave of settlers to Northeastern Mexico. Some details are\ngiven on 81 surnames. 2 pages\nCalifornia 1769-1800 - A list of names of over 1,700 of the earliest\nrecorded male settlers of Alta California. 1 page\nSurname Research - Here are Hispanic resources traceable to 320\nsurnames found in the American Southwest or Mexico. 3 pages\n*** The Genealogy of Mexico DNA Surname Project - Find relatives using\nDNA technology. With DNA it is possible to find out if you share a common\nmale ancestor within 7 generations at a probability of 50% or 23\ngenerations (around the time of the Conquest) at a 90% probability. This\nproject is open to anyone with ancestors in Mexico along their father\'s\nfather\'s .... line. 1 page\nNew!Genealogy of Mexico Mitochondrial DNA Page.New!Genealogy of Mexico Family Finder DNA Page. See how we are\nall related to each other using autosomal DNA.\nClick here for more information Family FinderPersonal Genealogy - Personal family surnames of Felix, Loera, Santoscoy,\nMoran, Vejar and Castro are discussed or mentioned. Additional links of\ncousins doing research on these names are provided as well as other links.\nPhoto Gallery - Old family photos. 1 page\nSince the dawn of man people have always traveled in groups, with those\nfounders of the group having the most descendants within the group.\nThe population of Mexico has seen wide historical changes. At the\ntime of the conquest in the early 1500\'s the population was estimated\nto be 20 million. By 1600 due to disease and war effecting the Native\nAmerican population of Mexico, the population dwindled to 1 million. In\nthe mean time, prior to 1620 or so, it is estimated that half the male\npopulation of Iberia came to settle the Americas. It is this founder\npopulation that this website will focus on, as there is genealogical\nand scientific information allowing us to link back to them.\nAs late as 1910 the population of Mexico was still only 15 million after\nalmost 400 years. With the population now estimated at over 125 million.\nThis accounts for an amazing degree of inter-relationships considering\nthere was relatively little immigration to Mexico during this time.\nThis website is organized for the most part, by founder groups. First\nthe Conquistadors of Mexico. Next, a list of those that distinguished\nthemselves during the Conquest. Next, a list of the earliest settlers\nof the Yucatan. Followed by a list of the earliest settlers of Nueva\nGalicia (parts of Aguascalientes, San Luis Potosi, Jalisco and Zacatecas).\nFollowed further by a list of those exploring the furthest reaches of\nthe north of New Spain (Coronado Expedition). Followed by a list of\nthose attempting to colonize Florida from Mexico (Luna expedition). Next\na list of the earliest colonizers of New Mexico (Onate expedition).\nNext is a list of the earliest recorded surnames of Northeastern New\nSpain (Mexico). Followed by a list of subsequent surnames found in\nNortheastern New Spain. California is then examined with a list of the\nsurnames of its earliest recorded male settlers. Followed by a library\nof references pertaining to research of the most common surnames of\nMexico. This is followed by new scientific research using DNA linking\nourselves to those at the time of the conquest. Finally there are links\nused in my own research and my family photo page.\nThis website is provided as a gateway to our ancestrial past and is\norganized to allow us to discover them from the conquest forward.\nYour host on this journey,\n* Source of some of the above statistical information. ""Mexico - population""SEARCH ""The Genealogy of Mexico website"" BELOW\nOnline 01/20/98 Counter set to zero\nUpdate 02/25/01 Added new page of the names of the earliest recorded\nsettlers of California.\nUpdate 04/08/01 Added ""The Last Will and Testament of Hernando Cortes""\nThis link can be found on this page under Cortes\nUpdate 04/21/03 Finished adding more Cortes Conquistadors names.\nAdded 1100 newly discovered Cortes Conquistadors.\nUpdate 04/21/03 Added New links on Personal Genealogy page on\nnew ways to research your ancestry through DNA.\nUpdate 05/20/03 Added many more Biographies of Cortes\' Conquistadors\nUpdate 09/07/03 Added DNA Surname Project page\nUpdate 01/02/04 Added Coats of Arms page\nUpdate 01/01/11 Added Mitochondrial and Family Finder pages\nYou can support the research on this site by purchasing books, using\nthe link to the left of the title. A safe, easy and convenient way to\nWith All Arms A Study of a Kindred Group\nby Carl Laurence Duaine.\nThis book brings to life the story of our ancestors from Northeastern\nMexico. The Author identifies the founders and this is backed up by\nour study on Y-DNA. This book was the inspiration for this website.\nThe author spent over 20 years tracing his mothers side of the family\nto some in a group of over 150 families of Iberian decent that\nsettled Mexico from the time of Cortes. Much of the book documents\nthe lives and descendants of these people. Includes land grant and\ncoat of arms information. Recommended for your genealogical library.\n346 8 1/2 X 11 pages.\nThe Discovery & Conquest of Mexico\nMany details relating to the conquest are from the first hand\naccount in the book ""The Discovery and Conquest of Mexico 1517-1521""\nby the Conquistador Bernal Diaz del Castillo (translated to english).\nThe Author viewed the writings of others who were not there and\nwanted to set the record straight. 468 pages\nThe Spanish Borderlands Frontier 1513-1821 by John Francis Bannon.\nStarts out describing the travels of the Conquistadors and takes you\nthrough three centuries of Mexican History and the individuals that\nlived it. 238 pages. Paisanos: Spanish Settlers on the Northern Frontier of New Spain\nby Oakah L. Jones. This book is about the colonists sent by Spanish\nAuthorities to settle the northern frontier of New Spain. 260 pages\nwith added references and an essay to 332 pages.\nThe Encomenderos of New Spain 1521-1555\nby Robert Himmerich Y Valencia. Excellent source of genealogical and\nhistorical information on the early settlers and much more information\non the conquistadors. Recommended for your genealogical library.\n506 settlers are profiled. 304 pgs.\nConquest: Montezuma, Cortes, and the Fall of Old Mexico\nby Hugh Tomas. A comprehensive study and an excellent book. See the\nonline reviews posted on amazon.com. 602 pages.\nThe Conquistadors First-Person Accounts of the Conquest of Mexico\nby Patricia de Fuentes. Accounts by Juan Diaz, Andres de Tapia, Cortes,\nFrancisco de Aguilar, Pedro de Alvarado, Diego del Pilar and ""the\nAnonymous Conquistador"". All give you a feeling of what it must have been\nlike during the conquest as well as information on the cultures and life\nin those days. 208 pages.\nFUNDADORES DE NUEVA GALICIA, GUADALAJARA, TOMO I by Guillermo\nGarmendia Leal. Excellent source of information on the early settlers\nof Nueva Galicia. Covers the years 1529 to 1650. Recommended for your\ngenealogical library. Spanish text. 165 pages.\nSumaria Relacion de las Cosas de la Nueva Espana by Baltasar Dorantes de\nCarranza. Excellent source of genealogical information on the Conquistadors.\nRecommended for your genealogical library. Spanish text. 419 pages.\n""Relacion Secreta de Conquistadores informes del archivo personal del Emperador\nCarlos I"" by Mariano Gonzalez-Leal.\n""Indice Geobiografico de Cuarenta Mil Pobladores Espanoles de America en El\nSiglo XVI"" Tomo I 1493-1519 and Tomo II 1520-1539 by Peter Boyd Bowman.\nList and some details of the first conquistadors to the New World.\n""Don Juan de Onate, Colonizer of New Mexico 1595-1682""\nby George P. Hammond And Agapito Rey (available through your libraries book\nexchange program). Currently out of print.\nNew Mexico\'s First Colonists\nby David H. Snow. A compilation of what is known genealogically, of these\nearly settlers. 131 pages.\nDiccionario Heraldico, Apellidos y Nombres Propios by Lander Munoz\nExcellent source of Heraldry information with coat of arms information on those\ncoming to the Americas and family histories. Recommended for your\ngenealogical library. Spanish text. 417 pages.\nCortes Conquering the New World by William Weber Johnson.\nA very interesting book with an excellent Epilog.\nVery easy reading.\nBooks on others researching their culture and personal historyThe Acclaimed Bestseller RAIN OF GOLD\nby Victor Villasenor. Follow the history of the authors family for several\ngenerations. This history could easily be our own. This bestselling\nnon fiction work easily reads like a novel. See the reviews on\nAmazon.com by following the link above.\nPlaces Left Unfinished at the Time of Creation\nby John Phillip Santos. The Author John Santos (de los Santos) is a Rhodes\nScholar. He traces his family history by retracing the route Cortes\ntook to Tenochtitlan and visits the great pyramids of Mexico. He\nintroduces the reader to his family in Texas and traces their history\nfor several generations. He brings to life his family history that is\noften times lost when we are removed from the old country. See the\nreviews on Amazon.com by following the link above.\nJust publishedWho\'s Who of the Conquistadors\nby Hugh Thomas the author of ""Conquest: Montezuma, Cortes, and the Fall\nof Old Mexico"". Biographies of over 2000 of Cortes\' Conquistadors. From\npreviously unpublished materials of Mexico and Spain. See more by\nfollowing the link above. Hardcover - 464 pages.\nThe Seven Daughters of Eve: The Science That Reveals Our Genetic Ancestry\nby Bryan Sykes a leading world authority on DNA and human evolution.\nThe science of finding our ancestors through DNA. Now, utilizing the information\nin this book we can find our ancestors using mitochondrial DNA or Y\nchromosome research. Hardcover - 320 pages.\nThe Conquistadors With Michael Wood (2001) on VHS Tape\nby Michael Wood. In his latest series, he walks in the footsteps of\nCortés, Pizzaro, and their fellow conquerors to explain what happened\nwhen two cultures met for the first time. Michael Wood brings history\nalive as he relates this story of conquest, heroism and greed and shows\nus how the Conquistadors have changed the way that we see the world, in\nterms of history and civilization, justice and human rights. Wood\'s\npassion for the story he\'s telling makes Conquistadors compelling viewing.', 'October 12, 2015\nFNS Editor’s Note: As part of our special coverage marking the historic transformation of Columbus Day into Indigenous Peoples Day, we offer this contribution by Nicolás Cabrera. He is a graduate student who specializes in Spanish literature at NMSU. Cabrera’s piece discusses Latin American manifestations of the movement, as well as historical accounts of the early conquest period that provide essential background information to today’s debates and controversies.\nOctober 12, 2015\nNMSU Student Series\nFor several decades there have been movements to redefine the holiday that falls on October 12th. In the United States this holiday is traditionally known as Columbus Day while in many Latin American countries it goes by a different name. Recently Albuquerque joined dozens of other cities in a change that is being led by Native American and Chicano leaders and supporters.\n“It is true that this specific movement is about indigenous peoples,” said Albuquerque City Council member Rey Garduño. “As a Chicano and an ally I stand with my Indigenous sisters and brothers.”\nGarduño was instrumental in having the Albuquerque City Council join other municipalities across the country vote in voting October 7 to recognize Indigenous Peoples Day with a proclamation for October 12th.\nNative American activist Melanie Yazzie explained why this is an issue for everybody. She said, “Because the history of conquest, slavery, and globalized violence that Columbus actively participated in is one that continues and affects us all.”\nThe redefinition of the holiday began in Latin America in the 20th century where the trend to rename and redefine the October 12th has had the most success.\nFor example, in Costa Rica it’s called “El Día de las Culturas” (Day of the Cultures) while in Mexico and many other Latin American countries it’s called “El Día de la Raza” which is best translated as “Day of the People.”In Nicaragua it is called “Día de la Resistencia Indígena” (Day of Indigenous Resistance) and Peru calls it “Día de los Pueblos Originarios y del Diálogo Intercultural” (Day of the Native Peoples and Intercultural Dialog).\nLastly, in Spain October 12th was originally called “Fiesta de la Raza” but has since been changed to simply “Fiesta Nacional.”\nAndrea Runyan is another activist working to change people’s perceptions of the holiday. Runyan said the movement to replace the holiday is growing because people are “recognizing the resilience of Native communities that have persisted despite genocidal efforts to eradicate them.”\nGarduño’s solidarity, Yazzie’s resistance, and Runyan’s activism are rooted in history that is accessible to everybody who wants to learn more about what took place during the initial decades of contact.\nThe first Spaniards who made contact with Native Americans left several accounts of tremendous historical, anthropological, and literary value. The authors, who were all men, tell the story of conflict and struggle from their own personal experiences.\nChristopher Columbus wrote the first accounts and his two most important pieces are “Carta del descubrimiento” and “Diario de a bordo.” In these works he describes the first contact he and his crew had with Native Americans as they toured several Caribbean islands.\nHernán Cortés, the famed conqueror of the Valley of Mexico, wrote “Cartas de relación” of which“Segunda carta de relación” is the most important. In the second letter he gives a detailed account of his journey to topple Tenochtitlán, the Aztec capital.\nBernal Díaz de Castillo wrote “De historia verdadera de la conquista de la Nueva España” that presents his personal account of the conquest of Mexico as a solider of Cortés.\n“Naufragios” was written by Álvar Núñez Cabeza de Vaca. This work describes the Native Americans he and his fellow shipwrecked explorers met and encountered during their eight-year journey. They were lost near present-day Galveston, Texas and together they wandered from the Gulf Coast through the present-day U.S. Southwest back to Mexico City.\nIn “Brevísima relaciónde la destrucción de las Indias” author Bartolomé de las Casas gives an extraordinary account of several encounters he saw first-hand as he traveled throughout the Americas.\nHe was in a unique position as a friar and bishop to travel and he took an extended journey to an unprecedented number of places. His work is valuable because it documented many of the atrocities and the brutal treatment committed by the Spaniards against the local populations.\nUpon his return to Spain he became one of the most vocal critics of the abuse and cruelty he witnessed. Casas helped to push for new laws and protections thereby becoming one of the first Europeans to fight for human rights in the Americas.\nAn important local account about New Mexico can be found in “Historia de la Nueva México” which was written by Gaspar Pérez de Villagrá. In it he gives his account of the arrival of Juan de Oñate, soldiers, and colonists. It begins with the founding of the first Spanish-Mexican settlements in and around Santa Fe.\nBut most importantly it chronicles the brutalities committed against the members of the Pueblo de Ácoma by Oñate and his men.\nAll of these historical accounts of the first contacts between Native Americans and Europeans have been translated to English. As documents in the public domain, they can be consulted online as well as in public and university libraries.\nFrontera NorteSur: on-line, U.S.-Mexico border news\nCenter for Latin American and Border Studies\nNew Mexico State University\nLas Cruces, New Mexico\nFor a free electronic subscription']"	['<urn:uuid:54631eb1-114b-452d-8314-3ac0e2a3dbb5>', '<urn:uuid:d7b80510-804f-4da2-90cc-bfd3515e9ed6>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	10	74	2875
65	veni project flood risk duration dates	The project runs from February 1, 2014 to January 31, 2018.	"['The number and impact of catastrophic floods have increased significantly in the last decade, endangering both human lives and the environment. With on-going climate change, the risk of flooding is likely to increase even further. This prompts the urgent question: how to use the limited resources available for flood risk management in an equitable and efficient way?\nIn the flood risk literature, this question is often reduced to a monetary optimization problem, ignoring the moral aspects. Neither is it recognized that people may have different, often conflicting, views on what equity and efficiency amount to. The goal of this project is to develop an ethical framework for reconciling efficiency and equity considerations in flood risk management.\nIn my PhD research, I developed a reflective equilibrium model for reconciling conflicting criteria when distributing responsibilities in a research team. In this VENI-project, I will explore a new line of distributive problems at a societal level, where we are confronted with a plurality of moral views and where collective interests may put pressure on individuals’ rights to be safeguarded against flooding. In such situations, it is extremely difficult to find a morally acceptable trade-off between efficiency and equity.\nThe overall research question is: how to reconcile equity and efficiency in flood risk management in a way that is acceptable in a pluralistic society. By developing a procedural theory, this innovative project addresses two major challenges for risk governance and risk ethics. First, equity and efficiency will be properly conceptualized and operationalized for the context of flood risk management, which is required before any trade-off can be made. Second, equity and efficiency will be developed such that they are acceptable in a pluralistic society.\nThe approach followed in this project is based on political philosophical literature on distribution problems and empirical research on stakeholders’ opinions. The project will lead to action-guiding recommendations for flood risk management, enabling a morally acceptable implementation of flood risk policy.\nVENI-grant for innovational research from the Netherlands Organization for Scientific Research (NWO) (grant nr. 016.144.071). The project runs from February 1, 2014 - January 31, 2018.\nRelated Scientific publications (selection)\n- Doorn, N. forthcoming. ""Allocating responsibility for environmental risks: A comparative analysis of examples from water governance"". Integrated Environmental Assessment and Management (DOI: 10.1002/ieam.1799). available online\n- Doorn, N. forthcoming. ""Resilience indicators: Opportunities for including distributive justice concerns in disaster management"". Journal of Risk Research (DOI:10.1080/13669877.2015.1100662). available online\n- Toonen, Th. A. J. and N. Doorn. forthcoming. ""Good governance for the commons: Design for legitimacy"" In: The Design Turn in Applied Ethics. Eds: M. J. Van den Hoven, S. Miller and Th. W. M. Pogge. Cambridge, Cambridge University Press.\n- Doorn, N. 2016. ""Distributing responsibilities for safety from flooding"". FLOODrisk 2016. 3rd European Conference on Flood Risk Management, France. E3S Web of Conferences 7, 24002 (2016) (DOI: 10.1051/e3sconf/20160724002). click to open pdf\n- Doorn, N. 2016. ""Governance experiments in water management: From interests to building blocks"". Science and Engineering Ethics 16(3) (DOI: 10.1007/s11948-015-9627-3). available online\n- Doorn, N and S.O. Hansson. 2015. ""Design for the value of safety"". In: Handbook of Ethics and Values in Technological Design. M. J. Van den Hoven, P. Vermaas and I.R. Van de Poel (eds.). Dordrecht, Springer.\n- Doorn, N. 2015. ""The blind spot in risk ethics: Managing natural hazards"". Risk Analysis 35/3: 354-360 (DOI: 10.1111/risa.12293). available online\n- Doorn, N. 2014. ""Equity and the ethics of water governance"" In: Infranomics - Sustainability, Engineering Design and Governance. Topics in Safety, Risk, Reliability and Quality. Eds. Gheorghe, A., M. Masera, and P.F. Katina. Dordrecht, Springer, pp. 155-64.\n- Doorn, N. 2013. ""Water and justice: Towards and ethics of water governance"" Public Reason 5/1: 95-111 click to open pdf\nRelated Professional/Popular publications\n- Doorn, N, B. Broekhans, and E. Mostert. 2013. ""Redactioneel: Normen en waarden in water governance"" (in Dutch) Water governance 3: 5 click to open pdf']"	['<urn:uuid:02832499-537b-4156-841a-1367e242d316>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	6	11	642
66	How did President Kennedy's handling of the Cuban Missile Crisis compare to his management of the Bay of Pigs invasion in terms of their outcomes and decision-making approaches?	The two events showed markedly different approaches and outcomes. During the Bay of Pigs invasion in April 1961, Kennedy made several problematic decisions: he authorized an invasion plan while trying to disguise U.S. support, cancelled a crucial second air strike after U.S. involvement became public, and ultimately saw the invasion crushed with nearly 1,200 brigade members surrendering and over 100 killed. In contrast, during the Cuban Missile Crisis in October 1962, Kennedy's leadership was more successful - he carefully weighed options with his advisory committee, used diplomatic channels with both the Soviet Union and Cuba, and ultimately reached an agreement on missile removal and ending the blockade by November 20. The Bay of Pigs failure actually influenced Kennedy's later handling of the Cuban Missile Crisis, leading him to initiate Operation Mongoose to destabilize Cuba's government.	"['Teaching the Cuban Missile Crisis\nThe 47th anniversary of the Cuban missile crisis passed with little public notice this year—except for an occasional reference to the U.S.-Iranian nuclear dispute as ""the Cuban Missile crisis in slow motion.""\nNonetheless, ""No event during the Cold War has generated more popular and scholarly attention . . . it has become perhaps the most studied international confrontation of the twentieth century, "" according to the editors of the Cuban Missile Crisis 1962 Documents Reader. And no wonder. In October 1962, The United States looked down ""the gun barrel of nuclear war,"" according to Kennedy speechwriter Theodore Sorenson.\nSince 1987, scholars and archivists have used the Freedom of Information Act (FOIA), to obtain, consolidate, and archive documents across the spectrum of Federal agencies about the Cuban missile crisis, events that led up to it, and its aftermath.\nNewly declassified records gave new perspectives to four critical areas of analysis: 1) Why the Soviet Union placed missiles in Cuba; 2) why the United States responded as it did; 3) the leadership role of John F. Kennedy during the crisis; and 4) the consequences of the near-miss of nuclear weapons deployment. Newly available records also let historians study the Cuban missile crisis from the point of view of the Soviet Union and Cuba as well as the United States.\nThe issues are so complex and the ramifications so profound, getting a grasp on the critical points of the Cuban missile crisis focus on diplomatic history, on understanding the situation from the top down. The following resources filter only a few among the many available materials to provide an overview of the issues and main actors.\nThis simple interactive site from the National Endowment for the Humanities summarizes the situation from the point of view of the Presidential cabinet. It poses the question, ""Mr. President, you\'ve been briefed about the presence of medium-range missiles in Cuba."" Visitors examine the choices Kennedy and his advisors faced and get a brief analysis of the consequences of each, the opinions of key members of President Kennedy\'s advisory committee, and an introduction to primary sources—particularly the National Intelligence Estimate, a key briefing document for the advisory committee. It\'s a helpful site for summarizing the complexities from the viewpoint of the Cabinet.\nThe Cuban Missile Crisis 1962 Documents Reader, from the National Security Archive at George Washington University, is extraordinary in the depth and breadth of materials and narrative. It is a research site for teachers and advanced students. Resources include primary source materials, a detailed timeline, and documents and audio files, from both the United States and the Soviet Union charting causes and events of the Crisis.\nEqually important, the Documents Reader explains how our understanding of the event continues to change as more documents are declassified or retrieved from a spectrum of agencies who were major players in the flow of information and decision-making in process in 1962. Analytical essays from historians sift through the historiography and recent evidence, discussing how this new evidence has rearranged assumptions and conclusions about the missile crisis. "". . . [M]isinformation, miscalculation, and misjudgment"" characterized the decision-making process in Washington, Moscow and Havana,"" according to then-Secretary of Defense Robert McNamara.\nBoth the Documents Reader and the U.S. State Department suggest using the blockbuster film Thirteen Days as an analytic tool for students. At the Documents Reader, the essay Turning History on Its Head explains why the film misrepresents actual causes, events, and outcomes of the crisis—a crisis clearly neither begun nor finished during these 13 October days. (In fact, the Soviet Union and the United States continued to negotiate on missile removal and ending the blockade until reaching agreement on November 20.)\nThe U.S. State Department offers an activity idea, suggesting watching Thirteen Days and comparing the story told by historic documents with the version relayed by Hollywood using the Motion Picture Analysis Worksheet from the National Archives. The U.S. Department of State website for youth also includes a number primary sources on the Cuban missile crisis that can spark classroom discussion.\nThe PBS American Experience series The Presidents also provides well-selected primary source documents incorporating correspondence circulating among Kennedy, Khrushchev, and Fidel Castro on the Cuban missile crisis.\nFeatured website reviews lead you to NSA and the Cuban Missile Crisis, a resource for 100 declassified documents related to Soviet and American involvement in Cuba.\nLearn about 19 short video clips where scholars discuss The Cuban Missile Crisis: Considering Its Place in Cold War History.\nDownload a quiz from the Clearinghouse archive—Textbook Twisters: Cuban Missile Crisis—and see if you can tell which which country wrote which account of the event in their school textbooks.', 'In 1959, Fidel Castro came to power in an armed revolt that overthrew Cuban dictator Fulgencio Batista. The U.S. government distrusted Castro and was wary of his relationship with Nikita Khrushchev, the leader of the Soviet Union.\nBefore his inauguration, John F. Kennedy was briefed on a plan by the Central Intelligence Agency (CIA) developed during the Eisenhower administration to train Cuban exiles for an invasion of their homeland. The plan anticipated that the Cuban people and elements of the Cuban military would support the invasion. The ultimate goal was the overthrow of Castro and the establishment of a non-communist government friendly to the United States.\nPresident Eisenhower approved the program in March 1960. The CIA set up training camps in Guatemala, and by November the operation had trained a small army for an assault landing and guerilla warfare.\nJosé Miró Cardona led the anti-Castro Cuban exiles in the United States. A former member of Castro’s government, he was the head of the Cuban Revolutionary Council, an exile committee. Cardona was poised to take over the provisional presidency of Cuba if the invasion succeeded.\nDespite efforts of the government to keep the invasion plans covert, it became common knowledge among Cuban exiles in Miami. Through Cuban intelligence, Castro learned of the guerilla training camps in Guatemala as early as October 1960, and the press reported widely on events as they unfolded.\nShortly after his inauguration, in February 1961, President Kennedy authorized the invasion plan. But he was determined to disguise U.S. support. The landing point at the Bay of Pigs was part of the deception. The site was a remote swampy area on the southern coast of Cuba, where a night landing might bring a force ashore against little resistance and help to hide any U.S. involvement. Unfortunately, the landing site also left the invading force more than 80 miles from refuge in Cuba’s Escambray Mountains, if anything went wrong.\nThe original invasion plan called for two air strikes against Cuban air bases. A 1,400-man invasion force would disembark under cover of darkness and launch a surprise attack. Paratroopers dropped in advance of the invasion would disrupt transportation and repel Cuban forces. Simultaneously, a smaller force would land on the east coast of Cuba to create confusion.\nThe main force would advance across the island to Matanzas and set up a defensive position. The United Revolutionary Front would send leaders from South Florida and establish a provisional government. The success of the plan depended on the Cuban population joining the invaders.\nThe first mishap occurred on April 15, 1961, when eight bombers left Nicaragua to bomb Cuban airfields. The CIA had used obsolete World War II B-26 bombers, and painted them to look like Cuban air force planes. The bombers missed many of their targets and left most of Castro’s air force intact. As news broke of the attack, photos of the repainted U.S. planes became public and revealed American support for the invasion. President Kennedy cancelled a second air strike.\nOn April 17, the Cuban-exile invasion force, known as Brigade 2506, landed at beaches along the Bay of Pigs and immediately came under heavy fire. Cuban planes strafed the invaders, sank two escort ships, and destroyed half of the exile’s air support. Bad weather hampered the ground force, which had to work with soggy equipment and insufficient ammunition.\nOver the next 24 hours, Castro ordered roughly 20,000 troops to advance toward the beach, and the Cuban air force continued to control the skies. As the situation grew increasingly grim, President Kennedy authorized an “air-umbrella” at dawn on April 19—six unmarked American fighter planes took off to help defend the brigade’s B-26 aircraft flying. But the B-26s arrived an hour late, most likely confused by the change in time zones between Nicaragua and Cuba. They were shot down by the Cubans, and the invasion was crushed later that day.\nSome exiles escaped to the sea, while the rest were killed or rounded up and imprisoned by Castro’s forces. Almost 1,200 members of Brigade 2056 surrendered, and more than 100 were killed.\nThe brigade prisoners remained in captivity for 20 months, as the United States negotiated a deal with Fidel Castro. Attorney General Robert F. Kennedy made personal pleas for contributions from pharmaceutical companies and baby food manufacturers, and Castro eventually settled on $53 million worth of baby food and medicine in exchange for the prisoners.\nOn December 23, 1962, just two months after the end of the Cuban Missile Crisis, a plane containing the first group of freed prisoners landed in the United States. A week later, on Saturday, December 29, surviving brigade members gathered for a ceremony in Miami’s Orange Bowl, where the brigade’s flag was handed over to President Kennedy. “I can assure you,” the president promised, “that this flag will be returned to this brigade in a free Havana.”\nThe disaster at the Bay of Pigs had a lasting impact on the Kennedy administration. Determined to make up for the failed invasion, the administration initiated Operation Mongoose—a plan to sabotage and destabilize the Cuban government and economy. The plan included the possibility of assassinating Castro. Almost 50 years later, relations between Castro’s Cuba and the United States remain strained and tenuous. The United States embargo against Cuba (dubbed by its opponents in Cuba and Latin America as el bloqueo, Spanish for “the blockade”) is a commercial, economic, and financial embargo partially imposed on Cuba in October 1960 (almost two years after the Batista regime was deposed by the Cuban Revolution). It was enacted after Cuba nationalized the properties of United States citizens and corporations and it was strengthened to a near-total embargo on February 7, 1962.\nTitled the Cuban Democracy Act, the embargo was codified into law in 1992 with the stated purpose of maintaining sanctions on Cuba so long as the Cuban government continues to refuse to move toward “democratization and greater respect for human rights.” In 1996, Congress passed the Helms-Burton Act, which further restricted United States citizens from doing business in or with Cuba, and mandated restrictions on giving public or private assistance to any successor government in Havana unless and until certain claims against the Cuban government are met. In 1999, U.S. President Bill Clinton expanded the trade embargo even further by ending the practice of foreign subsidiaries of U.S. companies trading with Cuba. In 2000, Clinton authorized the sale of certain “humanitarian” US products to Cuba.\nBeyond human rights, the United States holds $6 billion worth of financial claims against the Cuban government. The Cuban-American position is that the U. S. embargo is, in part, an appropriate response to these unaddressed claims. It has been argued that pro-embargo Cuban-American exiles, whose votes are crucial in Florida, have swayed many politicians to also adopt similar views. The Cuban-American views have been opposed by some business leaders who argue that trading freely would be good for Cuba and the United States.\nAn 80 year old exhiled Revolutionist in Castro’s army, stated Castro had lied to them while recruiting them in the mountains. Promising a better Cuba. Cuba has never been the same. The era amongst Cubans that exiled to America, is BC, Before Castro. Old Cubans have voted Republican, they felt Democrats are the cause of not invading Cuba, that’s why it is not a free Cuba due to the decision President Kennedy at the time, made. I constantly voice my opinion; it was not America’s problem to invade Cuba, start a war. What would we benefit?Nothing. The shift in the thinking of the young Cubans that have made America home, has changed and shifted to more Democratic thinking. People are Voting in Cuba, because they have no Choice. Castro came out as the enforcer. Most Cubans know if they don’t vote, they will pay a great consequence. We know for sure, Cuba continues to educate it’s people and have the best doctors in the world.']"	['<urn:uuid:0f7aa70d-ee39-4a88-9b08-5b3319d84ea0>', '<urn:uuid:70de3df6-7346-40af-9dbc-e315028c6e97>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	28	135	2101
67	what methods control nox emissions from industrial facilities and power plants including catalytic non catalytic reduction and air permits	There are multiple methods to control NOx emissions from industrial facilities and power plants. At the source level, low NOx burners are used to reduce flame temperatures and minimize thermal NOx formation. For post-combustion treatment, two main technologies are used: Selective Catalytic Reduction (SCR) which can achieve over 90% NOx reduction using catalysts and ammonia injection at 300-400°C, and Selective Non-Catalytic Reduction (SNCR) which achieves 30-50% reduction using ammonia injection at 850-1050°C without catalysts. Additionally, facilities must obtain legally enforceable air permits that specify pollution control requirements, emission limits, and monitoring/testing requirements. These permits help ensure facilities implement appropriate control technologies and maintain compliance with emission standards.	"[""Controlling Air Pollution from Facilities\nIndustrial and commercial operations emit pollutants into the air. These pollutants can make breathing difficult, form urban smog, impair visibility, and attack ecosystems. Some can cause cancer or other serious health effects. Among the many pollutants emitted into the air each year are sulfur dioxide, nitrogen dioxides, carbon monoxide, benzene, mercury and dioxin.\nMany of the sources of this air pollution are large facilities, such as petroleum refineries and chemical plants. Other pollution sources are smaller -- gasoline filling stations, dry cleaning operations, and paint spray booths. This page links to information about air source permits and other measures used by the state to reduce emissions from these stationary sources of air pollution.\nControlling Emissions from Stationary Sources\nAir Facility Permits\nTechnology can prevent or limit the escape of pollution from industrial and commercial facilities. Regulatory agencies like DEC have the authority to require facilities to install pollution control technologies or to change operating practices that pollute the air. The air facility permit is the principal vehicle for requiring these measures.\nOperating permits are legally enforceable documents issued to air pollution sources, specifying the air pollution control requirements for the facility. Operating sources must comply with the terms of this permit, and must publicly report their emissions. In addition, they may be required to monitor or test their emissions.\nSmall Business Compliance and Technical Assistance for Stationary Sources\nThis program helps small businesses comply with air emission requirements. It includes the Small Business Environmental Ombudsman, who handles complaints about regulations and is the small business advocate, providing information on complying with environmental regulations, and helping to locate sources of compliance financing. The Small Business Assistance Program provides small businesses with free technical assistance.\nEmission Reduction Credits (ERCs)\nERCs are generated when a facility shuts down, or voluntarily reduces its permitted emissions by accepting a federally-enforceable emission cap. To help keep air pollution controls as economical as possible, emission trading programs allow sources with more expensive control costs to trade emission allowances with sources that have cheaper control costs.\nA facility owner planning new or increased emissions must obtain enough emission reduction credits to offset these emissions before DEC will issue a permit to construct or modify an air pollution source. DEC maintains a registry where permit applicants can locate the necessary credits.\nDry Cleaner Regulation\nTo minimize the public's exposure to hazardous perchloroethylene vapors, the state's more than 5,000 perchloroethylene dry cleaners are subject to regulatory requirements aimed at reducing and containing solvent vapors.\nClean Air Interstate Rule (CAIR)\nCAIR is a Federal regional emissions trading program developed for states within the eastern region of the United States. It is designed to mitigate interstate transport of nitrogen oxides (NOx) and sulfur dioxide (SO2) to help reduce ozone and fine particulate matter (PM2.5) formation in CAIR states located in the eastern United States.\nAir Toxics Program\nAir toxics regulations specify the degree of air cleaning required for sources of toxic air pollutants, based on an Environmental Rating determined by evaluating the pollutant's toxicity and the pollution load already going into the air. The program employs the latest in computer modeling, air monitoring, and risk assessment methods to determine the control measures that will be required for any emission of a toxic air pollutant.\nMore about Controlling Air Pollution from Facilities:\n- Air Facility Permits and Registrations - New York State's Air Program requires sources of air pollution to obtain a permit or registration.\n- Assistance with Air Regulations for Small Businesses - New York provides expert assistance to small businesses trying to meet the regulatory requirements of the air pollution program.\n- Emission Reduction Credits (ERCs) - Emission Reduction Credits (ERCs) registry, required forms for download\n- Dry Cleaner Regulation - New York State's perchloroethylene dry cleaning facilities regulation, Part 232, requires at least an annual compliance inspection of dry cleaning facilities using perchloroethylene.\n- Crematories - Crematories in NYS are regulated by the NYSDEC to ensure that the air emissions from these facilities meet state and federal air pollution control requirements.\n- Sewage Sludge Incinerator Operator Training Certification - Instructions for DEC Approval of Training/Certification Programs\n- 6 NYCRR Part 231 Implementation Guidance - Documents to assist with implementation of Part 231\n- Air Toxics Program - Controlling Air Toxics through the application of science and engineering\n- DAR-20 Economic and Technical Analysis for Reasonably Available Control Technology (RACT) - The Clean Air Act requires RACT on existing sources of air pollution in areas that do not meet national ambient air quality standards. This policy includes procedures for the economic and technical feasibility analysis used to determine RACT and evaluate requests for source specific RACT determinations."", 'System to Control Nitrogen Oxides (NOx) in Coal Fired Power Plants\nBy Dr. Prasanna Seshadri\nCommonly referred to as NOx, Nitrogen oxides are one of the primary pollutants emitted by high temperature combustion systems such as pulverized coal (PC) fired boilers. Due to its role in both acid rain and ozone formation, NOx is a regulated pollutant under the EPA’s Clean Air Act. The options to reduce NOx emissions include source reduction and post combustion treatment of the flue gas. To control NOx at source, fuel and air distribution, otherwise referred to as staging, are modified to reduce flame temperatures. This is typically achieved by using low NOx burners.\nPost combustion measures for NOx control include both catalytic (SCR) and non-catalytic (SNCR) where NOx in the flue gas is reduced to molecular Nitrogen by reaction with Ammonia. However, it is quite common for utilities and other industrial combustion systems to employ both source reduction and flue gas treatment to meet required regulations. This paper will particularly focus on the two tail end options, SCR and SNCR, for flue gas NOx control.\nNitrogen oxides (NOx) denote the combined emissions of nitric oxide (NO), nitrogen dioxide (NO2) and other nitrogenous species in combustion derived flue gas. While NOx comprises all oxygenated nitrogen species, NO is the most dominant species in combustion gases accounting for anywhere between 95-99% of the total NOx in the gas stream. Together with SOx and Particulate Matter (PM), NOx is strictly regulated. NOx contributes to the formation of smog and acid rain, as well as increasing the amount of ozone (O3) in the earth’s troposphere. When NOx and volatile organic compounds (VOCs) react in the presence of sunlight, they form photochemical smog, which is a significant form of air pollution. The high concentrations of ozone in the atmosphere typically arise from high NOx emissions together with other reactive hydrocarbons, termed as VOCs. At lower levels, ozone is created by a chemical reaction between nitrogen oxides (NOx) and VOCs in the presence of heat and sunlight. Production of O3 follows the chain reaction mechanism as follows7:\nNOx + VOCs + Sunlight → O3 + other products (1)\nNO2 + hν → NO + O(3P), λ<400 nm (2)\nO(3P) + O2 → O3 (3)\nPeroxy-radicals (HOCO) in the atmosphere, produced as a result of carbon monoxide (CO) oxidation with the hydroxyl radical OH, go on to react with NO to produce NO2, which is then photolysed by UV-A radiation to give a ground-state atomic oxygen. This further reacts with molecular oxygen to form ozone7. While NOx emission limits vary globally, the U.S has one of the strictest limits in the world, especially for existing coal power plants. For new installations, the limits are even more stringent requiring the use of Best Available Control Technology (BACT) in both attainment and non attainment areas. Selective Catalytic Reduction, referred to as SCRs, is the current BACT for NOx control in power plants and also for other industrial applications. NOx is also regulated in combustion engine vehicles, especially vehicles using diesel engines.\n2. Nitrogen Oxides (NOx) From Fuel Combustion\nTypically, NOx formation happens during fuel combustion through two major pathways. In the first case, Nitrogen present in combustion air reacts with oxygen forming Nitrogen Oxide species. This is referred to as Thermal NOx and is a result of high combustion temperatures. The other major source of NOx is fuel NOx which is the result of fuel Nitrogen reacting with the oxygen in combustion air. Unlike thermal NOx, fuel NOx formation can happen at much lower gas temperatures. Fuel NOx is a significant contributor especially in coal power plants and can account for over 50% of the overall amounts of NOx generation. The control of NOx in combustion derived flue gas usually involves a two-phased approach – during fuel combustion and post treatment of flue gas. The primary focus of combustion control is to minimize thermal NOx formation and the latter option is to reduce the overall amount of already formed NOx.\n3. Control Techniques\n3.1. At Source Reduction\nWith the development of advanced burner technologies, NOx control begins with fuel combustion. Low NOx burners are designed to reduce the overall combustion temperature, thereby reducing the total amount of thermal NOx. Low NOx burners control fuel and air mixing to create larger and more branched flames. They accelerate fuel ignition and intensify combustion by achieving fuel rich conditions in the burner zone1. By improving ignitibility in rich fuel flame areas and producing moderate combustion in moderate fuel flame areas, the production of NOx emissions is reduced2.\nFigure 1. Low NOx type NR burner by Mitsubishi Hitachi Power Systems.\nNote. Adapted from https://www.mhps.com/products/boilers/technology/low-nox-burner/index.html\nCombustion reduction and burnout are achieved in three stages within a conventional low NOx burner. In the first stage, combustion occurs in a fuel rich, oxygen deficient zone where NOx formation takes place. In the second stage a reducing atmosphere follows where hydrocarbons react with the already formed NOx and reduce them to molecular Nitrogen. In the third and final stage of combustion an oxygen rich environment finalizes combustion of the fuel. Control of secondary air is important to ascertain full combustion, but not enough oxygen to regenerate NOx2.\nHowever, source control by itself is not enough to meet NOx emission standards. Downstream technologies are also required to further reduce NOx. The two most common downstream measures to control NOx are Selective Catalytic Reduction (SCR) and Selective Non-Catalytic Reduction (SNCR). Both technologies convert NOx in the flue gas to inert molecular Nitrogen by reaction with ammonia. The different reactions for NOx reduction are shown below and is applicable for both SCRs and SNCRs.\n4NO + 4NH3 + O2 → 4N2 + 6H2O (4)\n2NO2 + 4NH3 + O2 → 3N2 + 6H2O (5)\nNO + NO2 + 2NH3 → 2N2 + 3H2O (6)\nThere are process, efficiency and cost differences between the two technologies and due diligence is required to make a proper selection. The selection of NOx control systems needs to be done in coordination with the overseeing environmental agency.\n3.2. Selective Catalytic Reduction (SCR)\nSCRs are state of the art systems used to eliminate or reduce NOx emissions from the flue gas stream. In this process, ammonia (NH3) is injected into the flue gas stream to create a chemical reaction between NOx and NH3 forming inert Nitrogen molecules and water vapor as products. These chemical reactions are shown in equations 4-6. For the chemical reactions to occur, the gas needs to be between the optimum temperatures of 300°C and 400°C; therefore, the system is usually placed before the air preheater, at the exit of the economizer.\nThis technology uses a catalyst which facilitates the breakdown of NOx and increases the overall conversion efficiency. Most catalysts used in coal power plants consist of vanadium making up the active catalyst, and the substrate (or catalyst support) is usually made of titanium3. Active catalyst is finely dispersed across the support media. However, the final composition can consist of many active metals and support materials to meet specific requirements in each SCR installation. Typically, over 90% reduction in NOx can be achieved with the installation of a SCR6. SCRs are also a widely used technology in the off-gas treatment of large gas combustion turbines.\nFigure 2. Schematic of a SCR process.\nNote. Adapted from https://www3.epa.gov/ttn/ecas/docs/SCRCostManualchapter7thEdition_2016.pdf\nSCRs are designed to handle dust loads and do not require dust capture equipment upstream of the process. The system is relatively easy to maintain and capable of stable operation. While the system offers high NOx control, it is capital cost intensive and requires considerable plant outage time for retrofit applications.\nFigure 3. Selective Catalytic Reduction (SCR) System.\nNote. Adapted from https://www.mhps.com/products/aqcs/lineup/flue-gas-denitration/\nSCR capital costs vary by the type of unit controlled, the fuel type, the inlet NOx level, the outlet NOx design level, and reactor arrangement. Data collected on new installations between 2012- 14 indicated SCR costs ranged from $270/kW to $570/kW5. Typical operation and maintenance costs are approximately 0.1 cents per kilowatt-hour (kWh)5. Operating costs for SCR consist mostly of replacement catalyst and ammonia reagent, and while historically the catalyst replacement has been the higher cost, the reagent cost has become the most substantial portion of operating costs for most SCR. Since the gas pressure drops across the SCR system, the plant may also require ID fan modifications to compensate for the increased pressure loss.\n3.3. Selective Non-Catalytic Reduction (SNCR)\nWhile SCR is considered as the BACT, in certain applications, SNCR technology can be deployed at a relatively low capital cost. However, SNCR does not employ a catalyst and NOx reduction is typically a modest 30-50%4 for most systems. This technology utilizes atomizing nozzles to inject ammonia directly into the hot gas to chemically reduce NOx to Nitrogen and water vapor. Unlike the SCR, NH3 injection is at a much higher temperature window between 850-1050 C4. The goal is to maximize NOx control performance while optimizing chemical utilization with low reagent consumption.\nFigure 4. Schematic of SNCR process.Note. Adapted from https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomment-6-5- 2015.pdf\nAdditionally, urea can also be used as a reagent for SNCR applications. Urea (NH2CONH2) is easier to handle and store than NH3. Hence, urea-based systems are more common than ammonia based deployments, but operating data reveals higher NOx reductions occur with ammonia reagent. This is mainly due to improper and incomplete gas mixing with urea injection compared to anhydrous ammonia. Urea dissociates into ammonia first and then reacts with NOx as shown in chemical reactions 4-6. The dissociation of urea to ammonia is as follows: NH2CONH2 + H2O -> 2NH3 + CO2 (7)\nFigure 5. Urea based SNCR process schematic.\nNote. Adapted from https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomment-6-5- 2015.pdf\nThe effect of temperature is critical for SNCR applications. Not only does temperature affect conversion efficiencies, but it can also have a debilitating effect if ammonia or urea is injected outside the recommended range. At lower temperatures NO and the ammonia do not react. Ammonia that has not reacted is called ammonia slip and can react with other combustion species, such as sulfur trioxide (SO3), to form ammonium salts. These salts can later form hard deposits on downstream equipment thereby reducing operating efficiencies. At higher temperatures ammonia decomposes to form more NOx.\nSNCR is applied in a wide range of industrial processes, including cement and steel production. While not a BACT option for power plants, it is suited for areas under Reasonable Available Control Technology (or RACT) protocol. Retrofits are also much easier and require a small area for installation. Available data for Best Available Retrofit Technology (BART) analyses for 11 cement kilns indicates estimated NOx reductions for SNCR systems between 35 percent and 58 percent with a median reduction of 40 percent6.\nThe mechanical equipment associated with an SNCR system is simple compared to SCR and hence capital costs for SNCR installations are generally low. Based on available data, the installed capital cost of SNCR applications ranged anywhere between $5–20/kWe (kilowatt) for power generation units6. The absence of an expensive catalyst reduces CapEx requirements for both new systems as well as retrofits. Most of the cost of using SNCR is an operating expense. Reagent costs currently account for a large portion of the annual operating expenses associated with this technology. The annual cost of reagent purchase in $/yr is estimated using the reagent volumetric flow rate, the total operating time, and the unit cost of reagent. One of the bigger challenges with SNCRs is the stack emission of ammonia slip and this concentration is typically higher than SCRs due to larger injection rates. The temperature window at which SNCR typically operates plays a role in NH3 slip levels. Distribution of the reagent can be challenging especially in larger coal-fired boilers because of the long injection distance required to cover the relatively large cross-section of the boiler. Multiple layers are required to adjust to constantly changing boiler loads and this makes it challenging to fine-tune. Additionally, when urea is injected, large quantities of water are required, which can result in efficiency losses.\nDepending on the type of process, PEC Consulting can help in the selection of the best control technology to help reduce NOx emissions. We conduct an objective analysis to select the most feasible and efficient option to meet overall process and environmental requirements.\nSTEAM / Its Generation and Use, The Babcock & Wilcox Company, 42nd Edition 2. International Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Low NOX Burners”, https://www.iea-coal.org/low-nox-burners/\nInternational Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Selective Catalytic Reduction (SCR) For NOx Control”, https://www.iea coal.org/selective-catalytic-reduction-scr-for-nox-control/\nInternational Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Selective Non-Catalytic Reduction (SNCR) For NOx Control”, https://www.iea coal.org/selective-non-catalytic-reduction-sncr-for-nox-control/\nChapter 2, Selective Catalytic Reduction,\nhttps://www3.epa.gov/ttn/ecas/docs/SCRCostManualchapter7thEdition_2016.pdf 6. Chapter 1, Selective Noncatalytic Reduction, https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomm ent-6-5-2015.pdf\nReeves, Claire E.; Penkett, Stuart A.; Bauguitte, Stephane; Law, Kathy S.; Evans, Mathew J.; Bandy, Brian J.; Monks, Paul S.; Edwards, Gavin D.; Phillips, Gavin (2002-12-11). “Potential for photochemical ozone formation in the troposphere over the North Atlantic as derived from aircraft observations during ACSOE”. Journal of Geophysical Research: Atmospheres. 107 (D23)\nDr. Prasanna Seshadri is an experienced engineer with strong background in process design, project engineering and management, technology and new product development for a wide range of energy conversion and environmental control processes used in power and other heavy manufacturing industries. He is Subject Matter Expert (SME) for solid fuel combustion, acid gas, and heavy metals emission control for thermal conversion systems and wastewater treatment. He has a BS in Chemical Engineering from the University of Madras, India, MS in Environmental Engineering and PhD in Energy Engineering from the University of North Dakota.']"	['<urn:uuid:8bc471d0-b587-40c8-a813-09df98efe67a>', '<urn:uuid:87540a4c-f429-4cc7-9f8c-4a4655c65286>']	open-ended	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T15:30:12.136021	19	107	3047
68	tokyo party boats yagata bune night activities odaiba harbor details	In Tokyo, yagata-bune (party boats) cruise up and down the city's rivers at night filled with guests. These boats can be seen particularly well from Odaiba, Tokyo's shopping and recreation area on the bay, where dozens of them appear in the harbor at sunset with twinkling lanterns. During daytime, these boats are docked for cleaning and preparation for evening activities. The boats come in different colors, including pink, and are equipped with amenities like zabuton (cushions) for guests.	"['Old Japanese candy shop (summer, 2006)\nHere is a sketch I did back in my neighborhod on the east side of Tokyo. It is a ""dagashiya-san"" which is basically a candy shop with an old time atmosphere. The neighborhood dagashiya-san has changed very little over the years, and adults can are surprised to walk in with their kids and find the same variety of candy and toys that existed when they were kids -- and still at amazingly low prices.\nYou can\'t find these candies at any other shop, which is one reasons these little neighborhood shops have not gone extinct. I suppose one reason for the amazing low prices is the small portions and the low quality of the candy. I bought some chocolate there and found it to have practically no taste. But for a little kid who has nothing but a ten yen coin, the ability to go in by yourself and buy several pieces of candy is like heaven. All the neighborhood kids gather at these places. I got here in the late afternoon, and learned that my 9 year old daughter had left this shop with her friends just minutes before I had arrived.\nThese shops are so popular that you can now find them in modern shopping malls, but they are still decorated to resemble something out of the 1950s since that old nostalgic atmosphere is the reason for their popularity.\nLike many neighborhood shops, this one is run by an old widow who lives on premises above the shop. You can even see laundry hanging on the roof. When there are no customers the owner sits in a back room just behind some curtains to watch television. Painted in red above the entrance is the logo and name of ""Dai-ichi Pan"" which is a bread company that financed the store sign. On the street to the left is painted ""TOMARE"" which means stop. This sketch was done with brush and ink plus watercolor.\nThe train line that connects my neighborhood with the rest of Tokyo (and Japan) is called a subway although it runs above ground as it leaves central Tokyo. As a matter of fact by the time it reaches my neighborhood, it\'s elevated high above the street. The local station for this line actually spans a river! I think it\'s the only station in Japan that is built over a river. As you wait for the train you can look out the window and see the Kyu (old) Nakagawa River below. This was the most prominent river in Tokyo during the Edo era, but was reduced to a fraction of its original size when the Arakawa River was built nearby.\nA small park runs along both banks of the Kyu Nakagawa, and under the station. The place where I stood as I looked up at the station would have been under water during the Edo era. Since the Nakagawa winds through much of Tokyo\'s east side, many bridges span it, and three are visible from this spot, beyond the station. You can see the last two kanji for the sign which identifies this train line as the Shinjuku line. On rainy days I have stood under the shelter this station to sketch what I could see from there. This is a very peaceful spot, but it can get very noisy when the trains run overhead.This sketch was done with brush and ink plus watercolor.\nThis is Kanda Jinja (Jinja means shrine). I had heard about this shrine, and knew it was a big one somewhere in Central Tokyo just north of the Ochanomizu area, but despite the fact I had roamed all over the area I had never found it until now. It is a very impressive place.\nAll the standard jinja elements were there including the shishi guarding the front and the large paper lanterns on both sides of the entrance. As I sketched, several people came up behind me to watch silently. One old man told me that this was a very popular place for sketchers. I sketched this during rainy season on one of the few days when it wasn\'t raining. However it rained soon after I left. This sketch was done with brush and ink plus watercolor.\nThe following week I went back to the old part of Tokyo called Asakusa. I love this area, as I have mentioned before. If you ever visit Tokyo make sure you visit Asakusa to get a glimpse of old Tokyo. It\'s best if you take a bus or taxi (rather than the subway) into the area so you can see the old houses in the surrounding neighborhood as well. This is one of the shrines on the Sensoji temple grounds. Sensoji is another way to read the kanji for ""Asakusa"" and I suspect there is some kind of humor there somewhere. I\'ve sketched this shrine before. As a matter of fact, the shishi on the right is the mate of the shishi on page 2.\nDuring early summer all the shrines seem to have these huge hoops made of woven grass standing in front of the entrance. They apparently are intended to bring good luck to those who pass through them. As I sketched this the woman at the left of it passed through it several times in a row. The shrine in my own neighborhood had a row of about a dozen loops set up, resembling a tunnel. I guess that would save time and energy since you could pass through them all in a row.\nThis sketch looks a lot like the one of Kanda Jinja above, but I reversed the order of sketching. This sketch was done with brush and ink plus watercolor. I painted it in watercolor first and then went over it in ink. Actually, I think the results are the same except some of the ink lines are more defined since they are not coated with paint. And maybe the ink lines are slightly more loose in this one since they didn\'t have the task of defining the subject.\nOn my sketching days sometimes I take a train into town, and sometimes I walk down to the Arakawa river to sketch the boats, one of my favorite subjects. This is another ""yagata-bune"" or a party boat, a pink one. At night dozens of these boats filled with happy guests go cruising up and down Tokyo\'s many rivers. Not only do they leave their cares behind for the evening, they also leave the land behind!\nIf you go to Odaiba, which is Tokyo\'s new shopping and recreation area on the bay, you will see dozens of yagata-bune with their twinkling lanterns appear in the harbor as the sun goes down. During the day, these boats are docked, where they are cleaned and prepared for another evening\'s revelry. This pink yagata bune is all ready to go, and waits for the sun to set. You can see a stack of blue zabuton (cushions) in one of the windows. This sketch was done with brush and ink plus watercolor.\nNext page >>']"	['<urn:uuid:33e0bc57-bcdc-4e75-93b6-339a5cf9cfed>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	10	78	1185
69	fluid regulation process nephron renin synthesis connection blood pressure control	The nephron regulates fluid balance through multiple mechanisms. It filters blood through the glomerulus, with subsequent modification in the renal tubule system. The juxtaglomerular cells, located in the afferent arteriole wall, synthesize and secrete renin, which is crucial for blood pressure regulation through the renin-angiotensin-aldosterone system. The nephron's collecting duct system, influenced by antidiuretic hormone (ADH), can reabsorb up to three-fourths of water from urine through aquaporins. This complex system maintains blood volume, pressure, and electrolyte balance, with the juxtaglomerular apparatus serving as a key control point for renin release and blood pressure regulation.	"['Keeping in Touch: Renin-Producing Cells Require Cell Adhesion Molecule to SurviveKeeping in Touch: Renin-Producing Cells Require Cell Adhesion Molecule to Survive https://pediatricsnationwide.org/wp-content/uploads/2021/01/Urinary-system-No-Background-kidney-header-1024x575.jpg 1024 575 Lauren Dembeck Lauren Dembeck https://pediatricsnationwide.org/wp-content/uploads/2021/03/Dembeck_headshot.gif\n- May 24, 2021\n- Lauren Dembeck\nUncovering the molecular mechanisms underlying the renin-angiotensin-aldosterone system may provide insights into the long-term effects of hypertension and antihypertensive medications.\nAs the main source of renin in the human body, juxtaglomerular (JG) cells are crucial for blood pressure and fluid-electrolyte homeostasis. However, these cells cannot be cultured in vitro, making it challenging to study the molecular mechanisms underlying the renin-angiotensin-aldosterone (RAAS) system.\nUsing an in vivo model, pediatric nephrologists Tahagod Mohamed, MD, of Nationwide Children’s Hospital and Maria Luisa D. Sequeira Lopez, MD, at the University of Virginia have discovered that cell-to-cell and cell-to-extracellular matrix interactions are essential for JG cell survival. The findings were recently published in Hypertension.\n“Juxtaglomerular cells are classically known for their role in the kidney. However, their role extends beyond the kidneys to the overall regulation of fluid and electrolytes in the human body,” says Dr. Mohamed, who was the lead author of the study, which she performed as a fellow at University of Virginia. “Also, these cells are present not only in the kidney as JG cells but in other organs as well.”\nIntegrins are the largest family of cell adhesion molecules, which mediate cell-to-cell and cell-to-matrix interactions. Total body loss of the β1-integrin subunit, which is the integrin subunit expressed most abundantly in the kidney, is known to be incompatible with life.\nTo test the hypothesis that cell-matrix interactions are fundamental not only to maintain the identity and function of JG cells but also for their survival, the investigators created a unique murine model that has lost β1-integrin only in the JG cells.\n“The RAAS system has been known for several decades, and it is highly conserved among different species; thus, the lessons we learn from this murine model will inform our understanding of the system in humans,” Dr. Mohamed explains.\nAt birth, the β1-integrin mutant mice were undistinguishable from the control mice; however, with time, the mice failed to thrive and, at 6 weeks of age, began to lose weight and became dehydrated and moribund. Periodic blood chemistry and health monitoring combined with histopathological studies showed that the JG cells died by apoptosis, resulting in decreased circulating renin, hypotension, severe renal-vascular abnormalities, and ultimately, chronic kidney disease and renal failure.\n“We now understand that the communication between the JG cells among themselves, with the extracellular matrix, and other neighboring cells is extremely important for their function and survival,” says Dr. Mohamed.\nThe clinical significance of these findings may extend to the use of medications that block the cascade of the RAAS system, which are one of the most common medications used in adults, children and even infants with hypertension and chronic kidney disease, explains Dr. Mohamed.\n“This system is not just for blood pressure regulation. It involves other parts of the body, and we don’t know what the long-term effects of using those medications in people are,” she says, “Understanding the molecular mechanisms used by JG cells will help us formulate questions and design additional studies to uncover those long-term consequences.”\nHow JG cells detect changes in blood pressure is not fully understood, but it likely occurs through mechano-sensation. Dr. Mohmed and colleagues now hypothesize that integrins that connect JG cells to the extracellular compartment and to neighboring cells, including those of the kidney vasculature, may transmit changes in blood pressure to the JG cells. This hypothesis will be the basis of follow up studies.\nMohamed TH, Watanabe H, Kaur R, Belyea BC, Walker PD, Gomez RA, Sequeira-Lopez MLS. Renin-expressing cells require β1-integrin for survival and for development and maintenance of the renal vasculature. Hypertension. 2020 Aug;76(2):458-467. doi: 10.1161/HYPERTENSIONAHA.120.14959.\nImage credit: Nationwide Children’s (artist, Mandy Root-Thompson)\nAbout the author\nYou might also like\nIdentifying Early Urinary Biomarkers of Kidney Damage in Type 1 DiabetesIdentifying Early Urinary Biomarkers of Kidney Damage in Type 1 Diabetes https://pediatricsnationwide.org/wp-content/uploads/2021/01/Urinary-system-No-Background-kidney-header-1024x575.jpg 1024 575 Mary Bates, PhD Mary Bates, PhD https://secure.gravatar.com/avatar/c6233ca2b7754ab7c4c820e14eb518c8?s=96&d=mm&r=g\nTwo Novel Biomarkers for Urinary Tract Infection Diagnosis IdentifiedTwo Novel Biomarkers for Urinary Tract Infection Diagnosis Identified https://pediatricsnationwide.org/wp-content/uploads/2017/04/DSC_1346_Small-header-1024x575.gif 1024 575 Mary Bates, PhD Mary Bates, PhD https://secure.gravatar.com/avatar/c6233ca2b7754ab7c4c820e14eb518c8?s=96&d=mm&r=g\nDoes Twinkle Artifact on Ultrasound Indicate a Kidney Stone in Pediatric Patients?Does Twinkle Artifact on Ultrasound Indicate a Kidney Stone in Pediatric Patients? https://pediatricsnationwide.org/wp-content/themes/corpus/images/empty/thumbnail.jpg 150 150 Abbie Roth Abbie Roth https://pediatricsnationwide.org/wp-content/uploads/2021/02/062019ds5821_abbie-profile-new.jpg', 'SOS Children has tried to make Wikipedia content more accessible by this schools selection. SOS Children works in 45 African countries; can you help a child in Africa?\n|Nephron of the kidney without juxtaglomerular apparatus|\n|Nephron. Diagram is labeled in Polish, but flow can still be identified.|\n|Gray\'s||subject #253 1221|\nA nephron (from Greek νεφρός (nephros) meaning ""kidney"") is the basic structural and functional unit of the kidney. Its chief function is to regulate the concentration of water and soluble substances like sodium salts by filtering the blood, reabsorbing what is needed and excreting the rest as urine. A nephron eliminates wastes from the body, regulates blood volume and pressure, controls levels of electrolytes and metabolites, and regulates blood pH. Its functions are vital to life and are regulated by the endocrine system by hormones such as antidiuretic hormone, aldosterone, and parathyroid hormone. In humans, a normal kidney contains 800,000 to one million nephrons.\nTypes of nephrons\nTwo general classes of nephrons are cortical nephrons and juxtamedullary nephrons, both of which are classified according to the location of their associated renal corpuscle. Cortical nephrons have their renal corpuscle in the superficial renal cortex, while the renal corpuscles of juxtamedullary nephrons are located near the renal medulla. The nomenclature for cortical nephrons varies, with some sources distinguishing between superficial cortical nephrons and midcortical nephrons; other sources simply call all non-juxtamedullary nephrons superficial nephrons.\nFunctionally, cortical and juxtamedullary nephrons have distinct roles. Cortical nephrons (85% of all nephrons in humans) mainly perform excretory and regulatory functions, while juxtamedullary nephrons (15% of nephrons in humans) concentrate and dilute urine.\nEach nephron is composed of an initial filtering component (the "" renal corpuscle"") and a tubule specialized for reabsorption and secretion (the ""renal tubule""). The renal corpuscle filters out large solutes from the blood, delivering water and small solutes to the renal tubule for modification.\nComposed of a glomerulus and Bowman\'s capsule, the renal corpuscle (or Malpighian corpuscle) is the beginning of the nephron. It is the nephron\'s initial filtering component.\n|Glomerulus||The glomerulus is a capillary tuft that receives its blood supply from an afferent arteriole of the renal circulation. The glomerular blood pressure provides the driving force for water and solutes to be filtered out of the blood and into the space made by Bowman\'s capsule. The remainder of the blood (only approximately 1/5 of all plasma passing through the kidney is filtered through the glomerular wall into Bowman\'s capsule) passes into the narrower efferent arteriole. It then moves into the vasa recta, which are collecting capillaries intertwined with the convoluted tubules through the interstitial space, and which the reabsorbed substances will also enter. This then combines with efferent venules from other nephrons into the renal vein, and rejoins the main bloodstream.|\n|Bowman\'s Capsule||Bowman\'s capsule (also called the glomerular capsule) surrounds the glomerulus and is composed of a visceral inner layer and a parietal outer layer, both formed by simple squamous epithelial cells. Fluids from blood in the glomerulus are collected in the Bowman\'s capsule (i.e., glomerular filtrate) and further processed along the nephron to form urine.|\n|Scheme of renal tubule and its vascular supply.|\n|Gray\'s||subject #253 1223|\nThe kidney tubule, also renal tubule, is the portion of the nephron of the kidney containing the tubular fluid filtered through the glomerulus. After passing the tubule, the filtrate continues to the collecting duct system.\nThe components of the kidney tubule are:\n- Proximal tubule\n- Loop of Henle\n- Descending limb of loop of Henle\n- Ascending limb of loop of Henle\n- Thin ascending limb of loop of Henle\n- Thick ascending limb of loop of Henle\n- Distal convoluted tubule\nTubule component functions\nThe following table describes each component of a tubule in detail.\n|Proximal tubule||convoluted||S1||The proximal tubule as a part of the nephron can be divided into an initial convoluted portion and a following straight (descending) portion. Fluid in the filtrate entering the proximal convoluted tubule is reabsorbed into the peritubular capillaries, including approximately two-thirds of the filtered salt and water and all filtered organic solutes (primarily glucose and amino acids).|\n|Loop of Henle||The loop of Henle (sometimes known as the nephron loop) is a tube, it is often u-shaped in diagrams for simplicity but in reality it looks more like one loop of a coil (hence, \'loop\'). It extends from the proximal tube and it consists of a descending limb and ascending limb. It begins in the cortex, receiving filtrate from the proximal convoluted tubule, extends into the medulla, and then returns to the cortex to empty into the distal convoluted tubule. Its primary role is to concentrate the salt in the interstitium, the tissue surrounding the loop.|\n|Descending Limb||Its descending limb is permeable to water but completely impermeable to salt, and thus only indirectly contributes to the concentration of the interstitium.\nAs the filtrate descends deeper into the hypertonic interstitium of the renal medulla, water flows freely out of the descending limb by osmosis until the tonicity of the filtrate and interstitium equilibrate. Longer descending limbs allow more time for water to flow out of the filtrate, so longer limbs make the filtrate more hypertonic than shorter limbs.\n| Ascending Limb\n||Unlike the descending limb, the ascending limb of Henle\'s loop is impermeable to water, a critical feature of the countercurrent exchange mechanism employed by the loop. The ascending limb actively pumps sodium out of the filtrate, generating the hypertonic interstitium that drives countercurrent exchange. In passing through the ascending limb, the filtrate grows hypotonic since it has lost much of its sodium content. This hypotonic filtrate is passed to the distal convoluted tubule in the renal cortex.|\n|Distal convoluted tubule||The distal convoluted tubule is not similar to the proximal convoluted tubule in structure and function. Cells lining the tubule have numerous mitochondria to produce enough energy (ATP) for active transport to take place. Much of the ion transport taking place in the distal convoluted tubule is regulated by the endocrine system. In the presence of parathyroid hormone, the distal convoluted tubule reabsorbs more calcium and excretes more phosphate. When aldosterone is present, more sodium is reabsorbed and more potassium excreted. Atrial natriuretic peptide causes the distal convoluted tubule to excrete more sodium. In addition, the tubule also secernates hydrogen and ammonium to regulate pH.|\nAfter traveling the length of the distal convoluted tubule, only about 1% of water remains, and the remaining salt content is negligible.\nCollecting duct system\nEach distal convoluted tubule delivers its filtrate to a system of collecting ducts, the first segment of which is the collecting tubule. The collecting duct system begins in the renal cortex and extends deep into the medulla. As the urine travels down the collecting duct system, it passes by the medullary interstitium which has a high sodium concentration as a result of the loop of Henle\'s countercurrent multiplier system.\nThough the collecting duct is normally impermeable to water, it becomes permeable in the presence of antidiuretic hormone (ADH). ADH affects the function of aquaporins, resulting in the reabsorption of water molecules as it passes through the collecting duct. Aquaporins are membrane proteins that selectively conduct water molecules while preventing the passage of ions and other solutes. As much as three-fourths of the water from urine can be reabsorbed as it leaves the collecting duct by osmosis. Thus the levels of ADH determine whether urine will be concentrated or diluted. An increase in ADH is an indication of dehydration, while water sufficiency results in low ADH allowing for diluted urine.\nLower portions of the collecting duct are also permeable to urea, allowing some of it to enter the medulla of the kidney, thus maintaining its high ion concentration (which is very important for the nephron).\nUrine leaves the medullary collecting ducts through the renal papilla, emptying into the renal calyces, the renal pelvis, and finally into the bladder via the ureter.\nBecause it has a different origin during the development of the urinary and reproductive organs than the rest of the nephron, the collecting duct is sometimes not considered a part of the nephron. Instead of originating from the metanephrogenic blastema, the collecting duct originates from the ureteric bud.\nThe juxtaglomerular apparatus occurs near the site of contact between the thick ascending limb and the afferent arteriole. It contains three components:\n|macula densa||a tightly-packed area with diverse population of cells, including the renin granular cells|\n|juxtaglomerular cells||specialized smooth muscle cells in the wall of the afferent arteriole|\n|extraglomerular mesangial cells||couple to arterioles|\nJuxtaglomerular cells are the site of renin synthesis and secretion and thus play a critical role in the renin-angiotensin system.\nBecause of its importance in body fluid regulation, the nephron is a common target of drugs that treat high blood pressure and edema. These drugs, called diuretics, inhibit the ability of the nephron to retain water, thereby increasing the amount of urine produced.']"	['<urn:uuid:2f61d232-98a4-4245-80ef-09e81247aa81>', '<urn:uuid:66c1fc1a-e77f-4821-9dd3-49e266059490>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	10	94	2211
70	where catch snook what equipment needed different locations	Snook can be found in diverse habitats including river mouths, near shore reefs, salt marshes, seagrass meadows, and Gulf beaches, with preference for mangrove-fringed estuaries. They're widespread from North Carolina to Brazil and along Florida's Gulf Coast. For different locations, use appropriate gear: light saltwater rods for live bait fishing, surf tackle for beach fishing, and spinning/bait-casting equipment for general use. Look for them around structures like rocks, grass flats, oyster beds, and bridges in brackish waters.	['The Common Snook or the snook fish is a widely popular recreational fish in the Gulf Coast, known for its intense fighting ability and quality meat. It goes by other common names such as robalo and sergeant fish.\nThe snook fish (Centropomus undecimalis) belonging to Centropomidae family is a hermaphrodite, meaning it starts its life off as male which on reaching maturity transforms into a female.\nThey are identified by their distinct two-part dorsal fins, a black lateral line that extends until their tail. They have a robust, concave shaped body, protruding jaw in a golden-yellow shade with the ones found in rivers being darker than the ones found in the ocean.\nSnook are tropical fish. They can’t tolerate cold water. Even if the temperatures falls to just 60° F they begin to become lethargic and die eventually.\nThese fish commonly range from 50-140 cm in length. On an average they grow up to 50 cm (1.6 ft). The highest recorded weight of a snook fish ever caught is around 24.3 kg.\nSnook are euryhaline, meaning they spend time in both fresh and saltwater . They are never too far from the sea when they’re up river and never too far from freshwater when in the sea. They live in a wide variety of habitats, including river mouths, near shore reefs, salt marshes, seagrass meadows, and along Gulf beaches, but have a preference for mangrove-fringed estuaries.\nIt is widespread throughout the tropical waters of the western Atlantic Ocean from the coast of the North Carolina to Brazil. They are found along the Atlantic coast of Florida south through the Key between the Florida panhandle and Galveston, Texas, but are plentiful along the rest of the Gulf Coast.\nThe live baits which are most useful with catching are any small fish, live shrimp or crabs.\nThe artificial baits commonly used include mirror plugs, buck-tail and soft plastic jigs, jerk plugs, spoons and topwater plugs. Pilchards are effective as both live chum and bait.\nTo get those big Snook go for large dead baits such as mullet heads, ladyfish heads or halves. Spinnerbaits are also excellent lures in tidal rivers and come in handy for novice anglers.\nSnook fishing needs one to have quite the skill and be thorough with sight casting and fly fishing techniques.\nSpinning and bait-casting are commonly used. Light saltwater\nrods are useful when using live baits to catch the Snooks. Surf tackle can be useful at times, although surf Snook are usually close to the beach, in easy range of casting gear.\nOne of the most popular rigs for Snook is the Carolina Rig. Normally used with soft baits, this rig allows your weight to move up and down your line so it isn’t attached close to your lure.\nSnook fish are commonly found to take shelter in areas such as rocks, grass flats, mangroves, oyster beds and bridges. Make sure to thoroughly search the brackish waters – do not limit your area of search.\nWhen the fish feed actively, during the summer months, you should use fast lures, and then gradually move to slower lures as colder days kick in and the fish’s metabolism slows down.\nFishing for Snook depends a lot on the season. One more thing to consider is the water temperature. You will find Snook in warm waters, almost exclusively as they can’t stand the cold waters. So the best time to fish for them is from spring to early fall when the waters are warm.\nThe slob fish will move around estuaries, inlets, and bays from April until September, and then gradually start migrating towards cover structures to rest in rivers and piers around trees, mangroves, and other water structure.It is also adviced to go fish for snook during early hours of the days or late in the night.', 'Choosing the right spinning fishing rod is essential for a successful and enjoyable angling experience, whether you’re targeting inshore species close to the shore or venturing offshore for larger game fish. Different types of rods are designed to handle specific conditions and species, and understanding these nuances will greatly enhance your fishing prowess. In this guide, we’ll explore the factors that determine the best spinning fishing rod for both inshore and offshore fishing, and discuss which type of rod is better suited for certain fish.\nInshore Fishing Rods:\nInshore fishing refers to angling in the shallower waters near the coast, including bays, estuaries, flats, and marshes. In these areas, you’ll encounter species like redfish, snook, trout, flounder, and smaller sharks. When selecting an inshore spinning rod, consider the following aspects:\nRod Length: Opt for a rod length between 6 to 7.5 feet. A shorter rod provides better accuracy for casting in tight spaces, while a longer rod offers increased casting distance and better line control.\nPower and Action: A medium to medium-heavy power rod with a fast or moderate-fast action is a versatile choice for inshore fishing. It offers the sensitivity to detect subtle strikes and the strength to handle larger fish.\nLine Weight: Choose a rod with a line weight rating between 6 to 15 pounds, depending on the species you’re targeting. Lighter lines are ideal for finesse presentations, while heavier lines provide better control over larger fish.\nLure Weight: Consider the range of lure weights you plan to use. An inshore rod should be able to cast lures in the 1/8 to 1/2 ounce range effectively.\nOffshore Fishing Rods:\nOffshore fishing involves venturing farther from the coast into deeper waters to target larger game fish such as tuna, marlin, sailfish, and mahi-mahi. Offshore spinning rods require more power and durability to handle these powerful species:\nRod Length: Choose a longer rod in the range of 7 to 9 feet. A longer rod provides better leverage for fighting large fish and helps absorb the shock from their powerful runs.\nPower and Action: Opt for a heavy to extra-heavy power rod with a fast or moderate-fast action. These rods offer the backbone needed to handle big fish and heavy tackle.\nLine Weight: Look for a rod with a line weight rating between 15 to 40 pounds or more. Offshore fishing demands stronger lines to withstand the stress of battling powerful fish.\nLure Weight: Offshore lures tend to be heavier due to the size of the target species. Choose a rod that can handle lure weights in the 1/2 to 4 ounce range or even higher.\nMatch the Rod to the Fish:\nDifferent fish species require specific rod attributes to ensure successful hookups and landing. Here’s how to match the rod to the fish for both inshore and offshore scenarios:\nRedfish: A medium-power rod with a fast action is suitable for these hard-fighting fish found in shallow waters.\nSnook: Opt for a medium-heavy rod with a fast action to handle the acrobatic jumps of snook near structures.\nTrout: A medium-power rod with a fast action is ideal for detecting subtle bites while targeting speckled trout.\nFlounder: Choose a medium to medium-heavy rod with a moderate-fast action for effectively working bottom baits.\nSharks: When targeting smaller sharks, a medium-heavy rod with fast action can handle their power nearshore.\nTuna: Select a heavy-power rod with a fast action to handle the speed and strength of tuna.\nMarlin: An extra-heavy rod with fast action is crucial for battling the immense power of marlin.\nSailfish: Opt for a heavy-power rod with a fast action to handle the lightning-fast runs of sailfish.\nMahi-Mahi: A heavy-power rod with fast action is suitable for handling the aggressive strikes and acrobatics of mahi-mahi.\nConsider the Reel:\nPairing your spinning rod with a suitable reel is equally important. Look for reels that match the line weight and provide a smooth drag system. For offshore fishing, consider a larger reel with a high line capacity to accommodate the demands of battling large fish.\nIn conclusion, selecting the best spinning fishing rod for inshore and offshore fishing involves assessing factors like rod length, power, action, line weight, and lure weight. Tailoring your rod choice to the specific species you’re targeting will significantly improve your chances of success. Whether you’re chasing inshore species in shallower waters or embarking on an offshore adventure to battle big game fish, the right spinning rod will serve as an invaluable tool in your angling arsenal.']	['<urn:uuid:4062aa21-489e-4d56-b83c-65f5e93c9e26>', '<urn:uuid:668fcb3d-269c-4c1f-9b07-2ab816ba8252>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-12T15:30:12.136021	8	77	1385
71	My son's teacher suggested he might need speech therapy, but I'm concerned about the cost - what are the different places where I can get speech therapy services and are there any free or low-cost options available?	There are several affordable options for speech therapy services. Early Intervention (for children up to three years) is a federally funded program that provides services for free or at low cost. The public school system is required by federal law to provide free speech evaluations and therapy for children aged 3-21 if they qualify. Additionally, you can access services through community hospitals, outpatient clinics, university clinics (where graduate students provide supervised therapy), or private practices. Government or not-for-profit organizations may offer free services, though there might be waiting lists.	['In the past, I posted a vlog about what to expect during your child’s speech language evaluation. I have also covered red flags to watch for that may indicate your child should should be evaluated by a Speech-Language Pathologist for feeding difficulties, language delays , and/or speech sound (articulation) difficulties.\nIf you are concerned that your child is demonstrating delays in their speech sounds, language, or feeding/swallowing, it is time to seek out a professional evaluation. You may should also have your child evaluated, if he or she is showing signs of stuttering or problems with their voice. An evaluation should be completed by a certified Speech-Language Pathologist, which means they hold their Certificate of Clinical Competence.\nRequirements for Certificate of Clinical Competence\n- Completed intensive course work through an accredited graduate school\n- Passed a national examination\n- Completed a clinical fellowship year\n- Participates in ongoing continuing education courses\nWhere to Locate a Certified Speech-Language Pathologist\nBased on your child’s age, areas of weaknesses and your medical insurance, there are several different options when determining where to bring your child for a Speech-Language evaluation.\nAge Specific Speech Language Evaluation Options\n- Early Intervention (Birth to Three Years)- A federally funded program that is mandated by law, early intervention programs provide a wide range of services to children and their families. These services are provided for free or at a low cost, depending on the state. Services provide may include speech therapy, occupational and physical therapy, vision therapy, educational services, deaf and hard of hearing services and more. Please visit The National Early Childhood Technical Assistance Program to locate specific contact information by State.\n- Public School System (Ages 3 to 5 Years)- Under federal law, your local public school system is required to provide your child with a full speech language evaluation. Based on assessment results, your child may qualify for speech therapy services through the school system. Contact the special education department in your local school district to schedule the assessment.\n- Public School System (Ages 5 to 21) – At age 5, your local school district is mandated under federal law to assess your child’s speech and language skills, should you have concerns. If he/she meets the requirements for services, they will receive individual and/or group therapy while at school. The frequency of services are based on your child’s severity level. Service frequency may also be impacted by caseload size although it shouldn’t. You will need to be a strong advocate for your child, remaining involved in his/her treatment and requesting an increase in services should you feel it is necessary.\n- Hospital Setting (All Ages) Community hospitals may or may not provide speech and language services for children. Children’s hospitals provide a wide range of services for all pediatric speech and language disorders, including feeding disorders, voice disorders, and augmentative alternative communication needs.\n- Outpatient Clinic (All Ages) There are two main types of outpatient clinics, full rehabilitation centers that may include speech, physical and occupational therapy, and stand alone Speech Therapy businesses.\n- Private Practice (Age Range Varies) Speech-Language Pathologist’s who run a practice may or may not accept insurance. Some private practice’s specialize in specific diagnosis and/or disorders.\n- University Clinic (All Ages) Universities or colleges that have Speech Language programs provide evaluation and therapy services as part of the graduate school curriculum. Therapy services will often be carried out by a graduate student who is supervised under a professor, associate professor, or doctorate student.\nImportant Resources to Locate a Speech-Language Pathologist in Your Community\n- To search for Speech-Language Pathologist by state, disorder or age specialty, please visit the American Speech-Language-Hearing Association .\n- Visit the Specialty Board on Swallowing and Swallowing Disorders to locate a SLP certified in the evaluation and treatment of feeding and swallowing disorders.\n- Check out the Specialty Board on Fluency Disorders for a provider trained in stuttering.\nBe sure to come back for tomorrow’s post on how to prepare for your child’s speech language evaluation. If you found this post interesting or helpful, please post it to Pinterest or Tweet about using the socialize buttons below. Stay up to date on these weekly speech, language, and feeding articles, by subscribing to the posts by entering your e-mail address in the left sidebar.', 'Speech pathologists study, diagnose and treat communication disorders, including difficulties with speaking, listening, understanding language, reading, writing, social skills, stuttering and using voice. They work with people who have difficulty communicating because of developmental delays, stroke, brain injuries, learning disability, intellectual disability, cerebral palsy, dementia and hearing loss, as well as other problems that can affect speech and language. People who experience difficulties swallowing food and drink safely can also be helped by a speech pathologist.\nWhere do speech pathologists practice?\nSpeech pathologists work across a range of health settings including hospitals, schools, community health centres, residential and aged care facilities, disability services, mental health facilities, juvenile justice centres and private clinics.\nServices provided by government or not for profit organisations may be free of charge though there is often a waiting list for public services. Private services are provided by speech pathologists who may work in a sole practice or with other speech pathologists and in multidisciplinary practices.\nWhen should I see a speech pathologist?\nThere are many reasons why a person might benefit from seeing a speech pathologist. A formal referral is not generally required to see a speech pathologist. Some typical issues that may lead a person to see a speech pathologist include:\n- A child that has difficulty being understood by other children in the child care centre\n- A high school student who stutters and wishes to speak more fluently and with confidence.\n- A person experiencing difficulty speaking clearly after a brain injury\n- A person with dementia who needs assistance communicating with family and carers and making choices about their future\n- A person who is experiencing difficulties swallowing safely following a stroke.\nWhat services do speech pathologists provide?\nSpeech pathologists offer a broad range of services to support people in managing their communication skills and capacity to swallow safely. Examples of the type of work speech pathologists undertake with people are:\n- Providing communication strategies and augmentative or alternative communication devices for a person who cannot communicate verbally\n- Teaching a person who has had a stroke to swallow safely without choking and/or regain their communication skills\n- Helping children who have difficulty with their speech such aschildhood apraxia of speech\n- Helping pre-school and school aged children who have difficulty understanding others or difficulty sharing their thoughts, ideas and feelings\n- Helping children and adults who stutter to speak more fluently and with confidence\n- Providing advice to parents/carers who have babies or toddlers with feeding and swallowing difficulties.\n- Helping individuals with voice disorders including difficulties with voice quality, pitch or loudness.\nHow are speech pathologists qualified?\nIn order to practise as a Certified Practising Speech Pathologist (CPSP), speech pathologists must meet the following requirements:\n- Complete a recognised undergraduate or masters level qualification\n- Complete continuing professional development as set out by Speech Pathology Australia\n- Demonstrate that they have practised as a speech pathologist in the previous five years for a minimum of 1000 hours to meet Recency of Practice requirements.\nSpeech pathology is a self-regulated profession. Speech Pathology Australia is recognised by the Department of Education and Training as the assessing authority for speech pathologists in Australia.\nFor more detailed information about speech pathology, please visit the Speech Pathology Australia website.\nFind a speech pathologist\nSpeech Pathology Australia has a Find a Speech Pathologist service that can be accessed here.']	['<urn:uuid:ebb836a8-6d89-4669-8f5c-cec30cc777c2>', '<urn:uuid:2c073f8e-3e84-4268-9372-11d28375cf84>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T15:30:12.136021	37	89	1279
72	geese better than chickens weed control	While chickens are not very effective at weed control despite pecking holes in plants and churning soil, geese are naturally better at it. Geese naturally consume weeds as part of their foraging behavior and can increase produce yields by eliminating competing plants.	['Pest Eating Chickens Play Role in Creating Sustainable Agriculture Systems\nAugust 12, 2012 | Minda Berbeco\n“We got our chickens in the spring of 2006 when the local producer of pastured, organically-fed chicken eggs stopped selling.” Not interested in buying conventionally grown and processed meat and produce, Laylin and her family sought a way to ensure a steady supply of fresh and sustainable food.\n“We started out with eight [chickens] that first year, and we are now up to between 30 and 40, including the roosters.” These birds give Laylin and her family 6-7 dozen eggs a week in the winter and even more to barter with in the summer.\nAs her family became flush with eggs, Laylin started to notice something many other managers of free-range livestock have discovered: In addition to being great for eggs, meat and entertainment, chickens are excellent at pest control.\n“I have found that they eat different things depending upon what they learn to be tasty, perhaps as chicks. The ones we have right now love ticks, stink bugs, crickets, slugs, worms, and other bugs and animals.”\nWithout meaning to, Laylin and her family became part of a larger movement in organic and free-range food management, one that is starting to emphasize animals’ capabilities rather than their requirements. Instead of keeping livestock as passive receivers of management, organic agriculture has started to recognize livestock as active participants in creating sustainable agricultural systems.\nWith chickens, the benefits seem obvious: they are egg and meat-producers, creators of natural fertilizers and when left to their own devices, will gladly tear apart a field in search of worms, grubs and other treats. Moreover, permitting chickens to consume insects can allow managers to reduce the amount of protein in their diet (3) which can greatly impact a chicken’s flavor (2).\nLaylin notes that the only downside to the improved flavor is that “you can no longer tolerate eating what the rest of the world calls eggs.”\nSo if chickens are perfect for converting pests into fresh eggs or delicious meat, why aren’t more farmers using them as a natural biological control agent?\nLaylin is the first to admit that chickens are just not “forest-wise”. She finds them to be too domesticated to let them run around on their own, as “they are more likely to sacrifice themselves to the local predator population.” Letting them run free can also expose them to more diseases and increases the risk of mortality. These factors require her to keep the hens in enclosures or let them free in well-guarded, restricted areas.\n“During nine months of the year, they roam a one-acre fruit orchard, freely going in and out of a tractor [coop] that is surrounded by a portable electric mesh fence.We move them weekly to take advantage of fresh grass and bugs.” This keeps them safe, while still allowing them to hunt pests.\nThis style of management isn’t just good farming, it’s good science. Researchers have noted that fruit trees within 100 meters of a hen house have fewer insects. In addition, releasing chickens into apple orchards can reduce the number of apple sawflies by as much as 75%. Meanwhile, keeping chickens under pear trees decreases potential skin damage from midges (3).\nPlants aren’t the only agricultural organisms that poultry can protect, as they can be used to manage pests that infect other livestock as well.\nResearchers have suggested that chickens, when intermixed with cattle, could feed on ticks while providing farmers with an additional source of fresh meat and eggs. In fact, when allowed to graze with cattle on an African savanna, they consumed as many as 81 ticks per bird per hour (4). A delicious treat for the birds equaled a health-preserving cleansing for the cows.\nThe effect of poultry on reducing ticks is something Laylin has noticed with her free-range guinea hens in Virginia. “Since we’ve had them around, we have had very few ticks, which is awesome considering most of us have dealt with Lyme at least once in this family.”\nShe also notes that guinea hens are a little more “forest-wise” than chickens. She says they can run free “without ending up as lunch for one of the foxes, coyotes, bobcats, raccoon, opossum, or skunks that live up here in them thar hills.”\nChickens and guinea hens aren’t the end-all of pest management, however. Laylin notes that though her chickens are great insectivores, they are not great weeders. “They will eat weeds,” she says, “though I don’t think they would be useful for weed control, per se.”\nThis is something that has been echoed in the literature as well. Though great at pecking holes in plants and churning the soil looking for insects, chickens are not very effective at weed control. As a result, researchers have suggested integrating other poultry into organic agriculture, such as geese. These birds naturally consume weeds as part of their foraging behavior and can increase produce yields by eliminating competing plants (5).\nFrom chickens to guinea hens to geese, poultry have the ability to change the way people perceive animals in agriculture, from the managed to the managers. Though Laylin and her family are part of a larger agricultural movement, her goal was always to grow wholesome food and reconnect with nature. She warns though, that chickens are a ‘gateway animal’.\n“Once you get them, the next thing you know you might be tempted to try raising a few lambs, a duck or two, and maybe even some pigs.” If you don’t believe her, she says, just ask her husband.\n- Hermansen JE, Strudsholm K & K Horsted (2004) Integration of organic animal production into land use with special reference to swine and poultry. Livestock Production Science, 90:11 – 26. http://orgprints.org/6097/1/6097.pdfHermansen\n- Horsted K, Allesen-Holm BH & JE Hermansen (2010) The effect of breed and feed-type on the sensory profile of breast meat in male broilers reared in an organic free-range system. British Poultry Science Volume, 51(4):515—524. http://orgprints.org/18212/3/181212.pdf\n- Pedersen HL, Olsen A, Horsted K, Kprsgaard M & B Pedersen. Combined production of broilers and fruits. http://orgprints.org/14522/1/25_Pedersen_131_136.pdf\n- Samish M, Ginsberg H & I Glazer (2004) Biological control of ticks. Parasitology, 129: S389–S403. http://cbpv.org.br/artigos/CBPV_artigo_006.pdf\n- Clark SM & SH Gage (1996). Effects of free-range chickens and geese on insect pests and weeds in an agroecosystem. American Journal of Alternative Agriculture, 11 , pp 39-47. http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=6362368']	['<urn:uuid:301fff48-b524-4b25-b83f-22a4ecccb952>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	6	42	1062
73	I've been trying different methods to adjust the white balance settings on my camera, but I'm not sure about the quickest way to do it - what are the available shortcuts for changing white balance?	There are several quick ways to adjust White Balance settings. You can use the Quick Settings screen by pressing the Info Edit button (once if the Shooting Info screen is visible, twice otherwise), highlight White Balance, and press OK. Alternatively, you can set the Fn (Function) button on the side of the camera to control White Balance directly, then rotate the Command dial while pressing the button to cycle through available settings. However, if you assign this function to the Fn button, it will no longer serve its default Self-Timer Release mode purpose.	['How to Change White Balance Settings on a Nikon D5100\nA digital camera (like your Nikon D5100) compensates for different colors of light through white balancing. Simply put, white balancing neutralizes light so that whites are always white, which in turn ensures that other colors are rendered accurately. If the camera senses warm light, it shifts colors slightly to the cool side of the color spectrum; in cool light, the camera shifts colors the opposite direction.\nThe current White Balance setting your camera is set on appears in the Shooting Info screen. Settings other than Auto are represented by icons. During Live View and movie shooting, an icon representing the setting appears in the top-right corner of the monitor.\nYou can adjust the White Balance setting in a few ways:\nQuick Settings screen: You can get to this screen by pressing the Info Edit button. Press once if the Shooting Info screen is already visible; otherwise, press twice. After highlighting the White Balance option, press OK to display the menu shown on the right. Highlight the desired setting, and press OK.\nShooting menu: You also can adjust the White Balance setting through the Shooting menu. Going this route gives you access to some additional White Balance settings.\nFn (Function) button plus Command dial: You can set the Fn button on the side of the camera to bring up the White Balance setting directly. You then rotate the Command dial while pressing the button to cycle through the available White Balance settings. If you set the Fn button to this role, however, it no longer serves its default purpose, which is to set the camera to the Self-Timer Release mode for your next shot.\nOkay, now for the details about using the Shooting menu to adjust the White Balance setting. Via the menus, you can accomplish the following additional white balance goals:\nFine-tune the settings: If you choose any setting but Fluorescent, pressing the Multi Selector right takes you to a screen where you can fine-tune the setting.\nSelect a specific type of fluorescent bulb: When you choose Fluorescent from the menu, pressing the Multi Selector right displays a screen where you can select a specific type of bulb. Select the option that most closely matches your bulbs and then press OK. Or, to go to the fine-tuning screen, press the Multi Selector right.\nAfter you select a fluorescent bulb type, that option is always used when you change white balance through the Shooting Info display (as outlined previously) and choose the Fluorescent White Balance setting. Again, you can change the bulb type only through the Shooting menu.\nCreate a custom white balance preset: Selecting the PRE option enables you to create and store a precise, customized White Balance setting. This setting is the fastest way to achieve accurate colors when your scene is lit by multiple light sources that have differing color temperatures.\nYour selected White Balance setting remains in force for the P, S, A, and M exposure modes until you change it. So you may want to get in the habit of resetting the option to the Auto setting after you finish shooting whatever subject it was that caused you to switch to manual white balance mode.']	['<urn:uuid:d57fd471-6329-4926-a620-40c9c66b1a43>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	35	93	539
74	compare cash flow methods CFAT FCFF	Cash Flow After Taxes (CFAT) and Free Cash Flow to Firm (FCFF) are different measures of a company's cash generation. CFAT is calculated by adding non-cash items like amortization, depreciation, restructuring costs and impairments to net income, and is particularly useful for evaluating a company's ability to pay dividends. FCFF, on the other hand, represents the cash flows available for distribution to both debt and equity holders after investing in working capital and fixed assets. FCFF is more commonly used by analysts for valuation purposes as it incorporates the complete enterprise value of a company, including debt.	"['- Cash Flow After Taxes - CFAT\n- A measure of financial performance that looks at the company\'s ability to generate cash flow through its operations. It is calculated by adding back non-cash accounts such as amortization, depreciation, restructuring costs and impairments to net income.\nAlso known as ""After-Tax Cash Flow"".\nCFAT is important for investors because it gauges a corporation\'s ability to pay dividends. The higher the CFAT, the better positioned a business is to make distributions. CFAT also measures the company\'s financial health and performance over time and in comparison to competitors.\nInvestment dictionary. Academic. 2012.\nLook at other dictionaries:\nCash flow after interest and taxes — Net income plus depreciation. The New York Times Financial Glossary … Financial and business terms\ncash flow after interest and taxes — net income plus depreciation. Bloomberg Financial Dictionary … Financial and business terms\ncash flow — n. The total money generated; the cash remaining when all expenses are paid. The Essential Law Dictionary. Sphinx Publishing, An imprint of Sourcebooks, Inc. Amy Hackney Blackwell. 2008. cash flow … Law dictionary\ncash flow — the flow of internal funds generated within the business as a result of receipts from debtors, payments to creditors, drawings and cash sales. Glossary of Business Terms The cash receipts and payments of a business. This differs from net income… … Financial and business terms\nCash flow — For other uses, see Cash flow (disambiguation). Accountancy Key concepts Accountant · Accounting period · Bookkeeping · Cash and accrual basis · Cash flow management · … Wikipedia\nCash flow — In investments, it represents earnings before depreciation , amortization and non cash charges. Sometimes called cash earnings. Cash flow from operations (called funds from operations ) by real estate and other investment trusts is important… … Financial and business terms\nCash Flow — 1. A revenue or expense stream that changes a cash account over a given period. Cash inflows usually arise from one of three activities financing, operations or investing although this also occurs as a result of donations or gifts in the case of… … Investment dictionary\ncash flow — noun Date: 1954 1. a measure of an organization s liquidity that usually consists of net income after taxes plus noncash charges against income 2. a flow of cash; especially one that provides solvency … New Collegiate Dictionary\nFree cash flow — In corporate finance, free cash flow (FCF) is a cash flow available for distribution among all the security holders of a company. They include equity holders, debt holders, preferred stock holders, convertibles holders, and so on.There are two… … Wikipedia\nDebt-Adjusted Cash Flow - DACF — A financial ratio commonly used in the analysis of oil companies, representing the after tax operating cash flow, excluding financial expenses after taxes. Debt adjusted cash flow (DACF) is calculated as follows: DACF = cash flow from operations… … Investment dictionary\nfree cash flow — A measure of the cash that a company is generating or consuming. Free cash flow is often defined as after tax operating profit less net capital expenditure: free cash flow = revenues – operating expenses + depreciation – taxes – capital… … Accounting dictionary', ""Today we’ll cover how to find the intrinsic value of a stock.\nThis is the same approach we used to develop our stock valuation algorithm.\nFirst we'll define intrinsic value.\nNext we'll review the models used most frequently by analysts to calculate intrinsic value.\nThen we'll look at an intrinsic value example.\nLet’s dive right in.\nIntroduction to Intrinsic Value\nIntrinsic value is defined in many different ways.\nWe define intrinsic value based on Warren Buffett’s definition:\n“Intrinsic value is an all important concept that offers the only logical approach to evaluating the relative attractiveness of investments and businesses. Intrinsic value can be defined simply: It is the discounted value of the cash that can be taken out of a business during its remaining life.\nWhen the stock temporarily overperforms or underperforms the business, a limited number of shareholders – either sellers or buyers – receive outsized benefits at the expense of those they trade with.”\n- Warren Buffett\nSaid differently, at any given time a stock has an intrinsic value, which is the actual value of a company’s discounted future cash flows.\nIntrinsic Value = Present Value of a Company’s Lifetime Discounted Cash Flows\nIf a company's intrinsic value was obvious, you would either buy or sell the stock depending on whether the market price was higher or lower.\nUnfortunately, a stock’s intrinsic value is never obvious.\nThis results in stock prices being driven by available information and each investors’ own interpretation.\nHow To Calculate Intrinsic Value\nThere isn’t one model or formula that investors agree is the best model for forecasting future cash flows.\nBut almost all of the leading investment banks use some version of a discounted cash flow (DCF) model to calculate the value of a stock.\nThere are two main DCF models that get the most attention from investors:\n1. The Dividend Discount Model (DDM)\nThe Dividend Discount Model is the most simplified form of a DCF model.\nThis approach calculates a stock’s current intrinsic value based on the sum of expected future dividends, discounted back to present value.\nThe DDM works well when valuing companies with some history of paying dividends.\nIt also works better for companies that have future cash flows that are difficult to predict.\n2. Free Cash Flow to the Firm (FCFF) Model\nFree cash flows to the firm are company cash flows, after investing in working capital and fixed assets.\nFCFF represents the remaining cash that can be distributed to debt and equity holders after a company reinvests earnings.\nThis model is the most commonly used valuation approach by financial analysts, because it values the total company, including debt, and not just equity.\nThe value of a company is determined in a FCFF model by discounting future cash flows, adding non-operating assets, and subtracting outstanding debt.\nIn this way, it incorporates the complete enterprise value of a company.\nOther Valuation Models\nIt’s important to note that intrinsic value is not an absolute number.\nIt should be viewed as an expected value within a range of other alternative values.\nThere are several other valuation models that calculate a company's intrinsic value, and each one may return a different value:\n- Economic Value Added (EVA) Model\n- Benjamin Graham Valuation Formula\n- Reverse DCF Model\nThese models are generally used less frequently than the DDM or FCFF model.\nRecapping The Intrinsic Value Models\nEach of these valuation methods have strengths and weaknesses.\nBut the most widely used approach is the free cash flow to firm model.\nLet’s dive deeper into this model in our case study over the intrinsic value of Walmart (NYSE:WMT).\nIntrinsic Value Example: Calculation of Walmart (WMT) Intrinsic Value\nFree cash flow to the firm models can be completed in four steps:\n- Estimate the company’s free cash flow during the growth period.\n- Discount the estimated free cash flows by the weighted average cost of capital (WACC).\n- Calculate the terminal value.\n- Determine the enterprise value and the value of equity.\nLet’s complete each of these steps to find the intrinsic value of Walmart.\nStep #1: Estimate the Company’s Future Free Cash Flow\nThe first step in the intrinsic value calculation is to find the Company’s current free cash flow.\nCurrent Free Cash Flow\nYou should use the most recently available information in your model, which is the trailing twelve month (TTM) period results.\nFree cash flow is calculated in a number of different ways.\nIn our model, we use the formula:\nFCFF = EBIT (1 - tax rate) - (Capital Expenditures - Depreciation) - (Change in Cash-Free, Debt-Free Net Working Capital)\nFor example, in our valuation model for Walmart, we calculate the TTM free cash flow based on the following:\nThe current period free cash flow is used as the base year in the FCFF model.\nProjected Growth Rate\nThe growth rate used in the model has a big impact on the overall valuation.\nFour key factors should be considered when determining the growth rate:\n- Is the Company impacted by economies of scale?\n- Does the growth rate reflect the different business cycles of the Company?\n- Is competition likely to enter the market and impact the Company’s future growth rate?\n- Can the Company sustain their return on investment, and is the growth rate supported by the total value of the industry and the Company’s current market share?\nIn reality the projected growth rate will be an estimate. There is no way to know future growth with 100% certainty.\nA few common methods for estimating a growth rate are:\n- Use analyst estimates\n- Use actual historical growth rates of a company\n- Use a combination of both\nIn our model, we use a combination of analyst estimates and historical growth rates of EBIT, EPS, and return on capital.\nUsing this approach, we calculated Walmart’s expected growth rate at 3.20%.\nProjected Growth Period\nIn our DCF analysis of Walmart, we use a two-stage growth model.\nWhich means our model includes a high growth period of 3.20% year-over-year for the first five years.\nThe growth rate for the last five years of the model is reduced each year until it reaches the expected terminal growth rate of 2%.\nThe larger a company gets, the harder it is to maintain high growth rates.\nThe two-stage growth model takes this into account, which results in a more accurate projection of future cash flows.\nFuture Projected Free Cash Flows\nOur model projects Walmart’s free cash flow over the next 10 years at:\nStep #2: Discount the Estimated Free Cash Flows by the Weighted Average Cost of Capital (WACC)\nThe next step in the FCFF model is to discount the projected free cash flows to net present value (NPV).\nThe discount rate that should be used in a FCFF model is the WACC.\nThe WACC model uses different inputs to calculate a company’s cost of capital including both debt and equity.\nYou can read more about the WACC and use our WACC calculator here.\nFor Walmart, our algorithm calculated a WACC of 4.47%:\nUsing the WACC as the discount rate, the present value of Walmart’s future cash flows during the two-stage growth period is $89.8 Billion:\nStep #3: Calculate the Terminal Value\nThe intrinsic value of a company is theoretically the present value of future cash flows from now until forever…\nBut since it is impossible to project cash flows over an infinite period, a terminal value is calculated.\nThe terminal value represents the ongoing value of a company after the original growth period.\nIn our model, this occurs after 10 years.\nThis value is purely theoretical, but it is the best method available to calculate the total value of a company’s future cash flows.\nThe terminal value starts with the free cash flow from the last year in your growth stage valuation, and a constant growth rate is applied.\nAnalysts often use the risk free rate as the constant growth rate in the terminal value calculation.\nWe use a rate of 2% in our model to be conservative, and a normalized WACC as the discount rate to calculate the terminal value.\nStep #4: Determine the Enterprise Value and the Value of Equity\nWith the future cash flow projection completed, the next step is simple math to arrive at Walmart’s intrinsic value:\nFirst, the terminal value is added to the present value of the discounted cash flows:\n$212.8 Billion = $90.8B + $212B\nThis is the value of the Company’s operating assets.\nSecond, the value of cash and other non-operating assets are added to arrive at an enterprise value.\nEnterprise Value = Value of Operating Assets + Cash and Non-Operating Assets\nThe enterprise value is then reduced by total debt to determine the equity value of the Company.\nIn our model, we combine the enterprise value and equity value calculation using net debt, which results in the same outcome:\nThe last step is to calculate the intrinsic value per share:\nIntrinsic Value Per Share = $161B / 2.95B Shares Outstanding = $54.58 Per Share\nFinal Analysis and Fact Checking\nOur calculated intrinsic value indicates that Walmart is currently overvalued.\nBut it’s important to continue your analysis beyond the intrinsic value.\nThere could be other factors impacting the Company’s market valuation.\n- Is the growth rate used in the model reflecting all available information?\n- Are there significant market or economic trends impacting the valuation that are not included in the model?\n- Are the reinvestment rates used in the valuation model reasonable?\nRemember that Intrinsic value changes over time.\nContinue updating your valuation model frequently to understand how fundamental and overall market changes impact a stock’s intrinsic value.\nNow that you know how to find the intrinsic value of a stock, you can better understand a company before investing.\nFor more tips on how you can take your stock analysis to the next level, check out our post covering fundamental analysis 101.\nAnd if you’re looking for awesome, undervalued stocks to buy right now, we break down 4 simple screens to find undervalued stocks here.""]"	['<urn:uuid:3e921a16-c0cd-4bca-968d-62cbd5661048>', '<urn:uuid:425ef4d0-a07e-4da7-8d3f-70584f14eb78>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T15:30:12.136021	6	97	2206
75	What are the three main schools of thought in language learning?	The three main schools of thought in language learning are: Innatism (some syntactic settings are innate), Behaviorism (language is learned through conditioning), and Hypothesis Testing (children learn syntactic rules through postulation and testing hypotheses).	['Toward the end of the nineteenth century, as theories in logic began to advance and philosophies regarding the mind began to change drastically from previous accounts, a revolution in understanding language occurred. This event is referred to as the “linguistic turn.” Philosophers began to focus on the meaning of language, the use of language, the cognition of language, and how language and reality relate to one another.\nCOMPOSITION OF A SENTENCE AND LEARNING\nThe philosophy of language attempts to understand how meaning comes about from the parts that make up a sentence. In order to understand the meaning of language, the relationship between entire sentences and parts that are meaningful need to first be examined. According to the principle of compositionality, a sentence can be understood based on an understanding of structure (syntax) and the meaning of the words.\nThere are two accepted methods in understanding how meaning comes about within a sentence:\nThe syntactic tree focuses on grammar and words that make up the sentence, while the semantic tree focuses on meanings of words and the combinations of these meanings.\nIn regard to learning language, there are three main schools of thought:\n- Innatism: The notion that some syntactic settings are innate and based on certain parts of the mind.\n- Behaviorism: The notion that a very large amount of language is learned through conditioning.\n- Hypothesis Testing: The notion that children learn syntactic rules through postulation and testing hypotheses.\nThe roots of the “linguistic turn” occurred in the mid-nineteenth century, as language started to be viewed as the focal point in representing the world and understanding belief, and philosophers began to place emphasis on the meaning of language.\nJohn Stuart Mill\nIn his work in empiricism, John Stuart Mill examined the meaning of words in relation to the objects they refer to. Mill claimed that in order for words to hold meaning, one must be able to explain them based on experience. Therefore, words stand for impressions made from the senses.\nWhile some disagreed with Mill’s empiricist viewpoint, many philosophers agreed with Mill’s belief that denotation should be the basis of meaning, rather than connotation.\nAccording to John Locke, words do not represent external things; rather, they represent ideas within the mind of the person saying them. While these ideas are presumed to then represent things, Locke believed the accuracy of the representation does not affect that word’s meaning.\nWith that in mind, Locke set out to eliminate the natural shortcomings of language that naturally arise. He suggested that people should never use words without having a clear idea of those words’ meanings; people should attempt to identify the same meanings of words used by others so as to have a common vocabulary; people should be consistent with their use of words; and if a meaning of a word is unclear, one should then define it more clearly.\nThe work of German philosopher and mathematician Gottlob Frege focused mainly on logic. However, as his investigations in logic became more in-depth, Frege realized that, to continue pursuing his work, he first needed to understand language. By doing so, he created some of the most groundbreaking work in the philosophy of language.\nFrege questions identity, names, and the expression a = b. For example, Mark Twain is Samuel Clemens. However, if a = b is informative, how come a = a is trivial and doesn’t actually provide any new information?\nFrege believed that it is not simply the objects that are relevant to the meaning of a sentence, but how the objects are presented. Words refer to things in the external world—however, names hold more meaning than simply being references to objects. Frege broke sentences and expressions up into two parts: the sense and the reference (or meaning). To Frege, the sense of a sentence is the objective, universal, and abstract thought the sentence is expressing and the “mode of presentation” of the object that is being referred to. The reference, or meaning, of a sentence is the object in the real world that the sentence is referring to. The reference represents a truth-value (whether something is true or false) and is determined by senses.\nFrege expresses this theory as a triangle:\nThe intersection of line a and line b is the same as the intersection of line b and line c. Therefore, this statement is informative because we are presented with two different modes of presentation. To say the intersection of line a and line b is the same as the intersection of line a and line b only presents one single mode of presentation, and is therefore trivial.\nFrege concludes that there are three parts to a name (though all three are not necessarily needed in every case):\nSign: The word or words used (for example, Mark Twain).\nSense: The way to get at what is being referred to by the sign (for example, the psychological implications we have of Mark Twain—he is a humorist; he is the author of Tom Sawyer; etc.).\nReferent: The actual object being referred to (for example, Mark Twain is also Samuel Clemens, who is also the author of Tom Sawyer).\nTHE USE OF LANGUAGE\nIntentionality is another important topic with regard to the philosophy of language. Intentionality is defined as the particular mental states that are directed toward objects or things in the real world. Intentionality is not about one’s intention to do something or not do something, but rather, the ability of our thoughts to be about something. For example, you can have a belief about roller coasters, but a roller coaster itself cannot be about anything. Therefore, mental states like fear, hope, and desire have to be intentional because there must be an object that is being referenced.\nNineteenth-century German philosopher Franz Brentano argued that only mental phenomena could show intentionality. Later, twentieth-century philosopher John Searle questioned how the mind and language has the ability to force intentionality onto objects when such objects are not intentional on their own. In his theory of speech acts, Searle concludes that actions have intentionality as well, because language is a form of human behavior and an action on its own. Therefore, by saying something, one is actually performing an action, and intentionality is present in actions.\nIn a much-debated discussion on artificial intelligence, Searle argued that machines would never have the ability to think. Searle claimed that machines lack intentionality and that only an organized mind, like that of a human being, is able to perform intentionality.']	['<urn:uuid:81974a08-9ee9-4924-910d-285bb8fe5b5e>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	11	34	1084
76	biological mechanism permanence ink tattoo safety concerns chemicals discuss	The biological mechanism of tattoo permanence involves the immune system's response to ink in the dermis layer. When tattoo needles deposit ink, macrophages attempt to digest it but some remain trapped in dermal cells. While dermal cells die and are absorbed, the ink transfers to new cells, maintaining the tattoo. However, safety concerns exist as tattoo inks can contain hazardous chemicals like PAHs (polycyclic aromatic hydrocarbons) which are carcinogenic, and nickel which can cause severe allergic reactions. Studies show these substances can be transported from the skin, with up to 32% of pigments decreasing after 42 days in test subjects.	['What Makes A Tattoo Permanent?\nThink about this for a while, what actually makes tattoos permanent. You could be forgiven for thinking that the reason has something to do with the depth in which the ink is deposited into the skin, but as you are about to discover things are not quite that simple.\nIn actual fact the microscopic war between your immune system and the dye is what actually enables tattoos to take up a permanent residency under your skin.\nWhen someone gets a tattoo it acts as a sort of infection and this “infection” is attacked by the body’s immune system resulting in a sequence of events that enable the tattoo to happily stay put, evading your body’s defensive system.\nThe Human Skin Cycle\nTo better understand how tattoos work we will take a little detour and briefly explain how the human bodies largest organ, the skin, repairs and regenerates itself.\nYour skin is made up of several layers with the very top layer that you can see on the outside being the epidermis. The epidermis is composed of cells that are made of keratin. In case you have never heard of keratin it is a hard, strong substance that also makes up your hair and nails. The individual cells of these are known as keratinocytes and these grow at the lowest level of the epidermis which is fused with the second layer of skin called the dermis. The keratinocytes push up their way from the bottom until they finally reach the top where they die and then break away. As old cells die and are shedded from the skin new keratinocytes are ready to take their place completing a full cycle which takes roughly around a month.\nBack To Tattoos\nSomething doesn’t seem quite right here, if our skin replaces itself every month or so why does the tattoo remain? Put simply they do not shed as the ink is deposited in the dermis layer of the skin, underneath the epidermis which has the monthly shed cycle. This helps explain why tattoos don’t shed away but not why they are permanent.\nTattoo needles work by punching their way through the epidermis at 50 to 3,000 times a minute, depositing the ink deep into the dermis. The dermis is composed of collagen fibres, nerves, glands and blood vessels – every time the needle penetrates the skin it causes a wound that alerts the immune system cells to the source of the damaged skin to begin repair efforts and to decrease inflammation. It is this process that is responsible for the permanent nature of tattoos.\nStage 1: The Clean Up Process\nSpecialised cells named Macrophages are sent in to “digest” the ink in an attempt to clean up the inflammatory mess. Some of these cells travel back to the lymph nodes full of ink while others remain in the dermis. As these cells have no way of disposing of the ink the pigment stays in the remaining Macrophages and can be visible though the skin. Other ink particles that are left are engulfed by dermal calls that are called fibroblasts.\nStage 2: Epidermal Regeneration\nWhen the tattoo needle injects ink into the dermis, the ink is also deposited in the epidermis as well. However as the skin heals itself from the puncture wounds caused by the tattoo needle, the epidermal cells are shed and are then replaced by new skin cells, that are dye free. This process usually takes around 2-4 weeks for full epidermal regeneration.\nStage 3: The Death Of Replacement Of Dermal Cells\nUnlike the epidermis cells the dermal cells remain in place until they die. They are then absorbed, ink included, by new and younger cells nearby resulting in the tattoo staying put.\nStage 4: The Fight To The End\nThe tattoo will slowly fade over time as the immune system is locked in a constant battle to remove the alien pigment from the body. Although the immune system is forever passively trying to remove the tattoo its ability to do so dramatically diminishes after the first round of macrophages.\nPlease watch this video by Ted Ed which animates the process described above:', 'Recommendations regarding maximum values for various contaminants in tattoo products can be found in Resolution „ResAP (2008)1 on requirements and criteria for the safety of tattoo and permanent makeup of the Council of Europe. These values take into account both health aspects and the current state of technology.\nThe persistence of restricted chemicals in tattoo inks continues to be problematic as highlighted by their continual presence in weekly RAPEX reports. These reports continually inform technicians about pigments that didn’t meet the requirements and criteria for the safety of tattoo and permanent makeup and its distribution is not permitted.\nWhat are the most hazardous substances found in tattoo inks?\nPAH – polycyclic aromatic hydrocarbons\nThe group of PAH consists of more than 100 substances, eight of which are classified as carcinogenic. Insufficient data is currently available on the health effects of tattoo colors. Thus little is known on how the components of the tattoo colors are distributed in the body and how they are metabolized. However, it is to be assumed that immediately after tattooing, part of the PAH is released into the areas surrounding the tattoo, into the lymph stream and possibly even into the blood stream. In addition, it is likely that PAH are constantly released from the tattoo and are thus distributed in the body.\nPAH substances are ubiquitous in the environment. The are formed through incomplete combustion of organic materials as well as during frying, smoking and grilling of foodstuffs. PAH are also contained in tobacco smoke. PAH posses properties which are harmful to human health. According to Appendix VI of Regulation (EC) 1272/2008, eight representatives of this substance class are currently classified as carcinogenic, and if the amount of PAH content is more than 0,5 ppm, must be marked as a carcinogenic product or cannot be sold on the market at all.\nCarbon – carbon black\nCarbon is permitted in cosmetic products and is not subject to any restrictions in tattoo products either. Though the German supervisory body (Regulation on Tattooing Products) tested three tattoo products that only use carbon as a black colorant, and it were determined that carbon black pigments carries impurities as PAH or BaP. Consequently if carbon black is chosen as a black colorant, it needs to be tested carefully before placed on the market. Although as discussed several times in European professional publications the amount of carbon based black pigments will be restricted in the near future.\nPAH and BaP are to be expected in black tattoo pigments, although the proportion of black color in inks is likely to vary to great extend depending on the color of the pigment. Consequently the exposure is depending on the type of the color.\nNickel – as a trash element of iron-dioxide\nSeveral permanent makeup pigment poses a chemical risk because it contains high concentration of nickel. According to the Council of Europe Resolution ResAP (2008)1, nickel levels in tattoo inks must be as low as technically achievable as nickel has it has a high allergenic potential. The application under the skin of tattoo inks containing nickel results in permanent contact with a sensitizing allergen. A sensitivity to nickel most often causes a raised, itchy skin rash, which can be quite severe.\nAbout the absorption of hazardous substances of tattoo inks……………….\nScientific studies confirm that pigments are transported from the skin. Tests were conducted using a nude mouse model to establish the transport behavior of pigment sin the skin. For this purpose, mive were tattooed on their back. On average, 584 micrograms of pigment was injected per animal. The decrease in pigment concentration after 42 days amounted to 32 percent. This means that pigments and pigment trash elements are transported from tattooed skin areas and possibly become systemically available.\nGiven the complexity of the exposure and the limited data available, we can state that a serious health risk resulting from the use of tattoo pigments containing hazardous compounds is very probable. Unfortunately no validated method currently exist for determining the PAH in permanent makeup products, and very little is known about the kinetic parameters such as distribution and metabolism of the substances from tattoo products. For this reason, it is not possible to estimate what quantities of the hazardous substances injected under the skin become locally or even systemically available. More research is urgently needed in this area. In addition, it is very likely that hazardous compounds are continuously released from the pigments deposited in the skin, so that tattooed individuals are additionally subject to long-term exposure.']	['<urn:uuid:5c11dc75-c8ad-4650-a725-6e6df1420234>', '<urn:uuid:cc7ce0c8-b4cc-484b-8277-61a2c8e1cc61>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	9	100	1450
77	how to make customer journey map simple and effective key points	To create a simple and effective customer journey map, you should avoid capturing every single detail and instead focus on creating a useful abstract that highlights key touchpoints. If there are too many variations, it's better to create multiple maps divided by customer persona. The map should focus on crucial moments where marketing can influence customers, including both opportunities to meet customer needs and potential risk points where customers might leave. It's important to keep the map visually clear and avoid cluttering it with endless branching paths, sticking to a single narrative for maximum effectiveness.	['Before the customer journey map, there was the traditional marketing and sales funnel. Marketing and salespeople revered it because it was a simple way to understand how linear customer journeys works. Potential customers entered at the top, buyers trickled out the bottom. Maximize the number that enter at the top, and barring a leak or two, you can increase how many actually convert.\nBut then came the data boom. And while the funnel still has value as a content strategy tool, these days marketers have far more insight into the customer journey. We know it’s not one straight slide through the funnel. There are an infinite number of ways that customers go about becoming aware of a need, researching alternatives, and ultimately deciding on a solution.\nA customer journey map attempts to take all of that data and make it useful for creating strategy. Mapping your business’ unique customer journey can help you focus your strategy, create more relevant content, and ultimately guide more customers to purchase.\nHere’s how to create a customer journey map that’s unique to your business and your customers.\nHow to Create a Customer Journey Map\n1. Invite Stakeholders from Across the Organization\nYour map will be a valuable document for setting marketing strategy. But creating it shouldn’t be a task confined to the marketing department. Your organization is probably already moving to break down silos, share information between departments, and consolidate data to help shape the customer experience. It’s vital to the future success of the organization to have departments working together toward shared goals.\nThe customer map project is a perfect opportunity to start bringing other departments to the table. The sales department can share details about interactions with prospects, for example. They can talk about frequently asked questions, referral sources, what causes prospects to bail, and more.\nYour customer service department can help with statistics on customer interactions, demographics, common complaints, and more. Even the finance department and HR might have data that helps complete the map. And, of course, it’s a good idea to involve leadership, too. They likely have unique insight into the buying process, and it’s good to get their buy-in early on.\nWhen you involve all your stakeholders, the end result is a more comprehensive view of the customer for everyone, not just the marketing department.\n2. Tell a Story\nAt the heart of it, your map should be a narrative. It tells the story of how a particular type of customer encounters your brand and chooses to make a purchase. As you pull together data, keep the story in mind. This template, called “The Story Spine,” is a good way to keep the narrative front and center:\n- Once upon a time, there was a ______\n- Every day, _______\n- But one day, __________\n- Because of that, ________\n- Because of that, ________ (repeat)\n- Until finally, ________\n- Every day after that, _____\nIntuit’s map is a great example of applying narrative to the customer journey. It tracks the emotional journey as well as the economic one, using emoji to illustrate customer feelings during each phase. It’s easy to see at a glance when customers might be at risk for dropping out.\n3. Keep the Customer Journey Map Simple\nAs you pull in data and begin to visualize the customer journey, it’s natural to want to capture every single detail. Just remember the goal is to create a useful abstract; an at-a-glance look that hits the key touchpoints.\nDon’t get bogged down in minutiae or clutter your map with endless branching paths. Stick to a single narrative. If you find there are too many variations to include in a single customer journey, it’s okay to create more than one map. You can divide your customers by persona and create a simpler map for each. Paring down the information will help you focus—the process of elimination will bring into sharp relief what matters most about your customer journey.\nFor example, compare the Intuit map using emojis to the multi-layered map above. The latter has a great deal more information in it, but it loses the at-a-glance immediacy. It can be hard to tell what the biggest priorities are when there’s so much information on display. It’s worth seeing how simple and effective you can make your map.\n4. Focus on Key Touchpoints\nPart of keeping your map simple is to only include the crucial moments where marketing can exert influence. Identify opportunities—times the customer has a need that your marketing can meet. But also look at potential risks, points at which the customer could choose to move on without you.\nThe sales department can be a great help at this stage. They can provide a wealth of anecdotal evidence on what causes prospects to lose interest and how marketing can help bring them back around. That data can help make your map more precise, and also help with sales enablement in the future.\nThe map from Digital Experiences has an elegant depiction of key touchpoints for a hypothetical hot chocolate store. This map makes it easy to see exactly when there is the greatest opportunity for marketing to make a difference, how different marketing technologies affect consumer choices, and what marketing’s end goal is for the ideal customer.\n5. Make It Visual\nThe goal should be for your map to be as widely seen in the organization as possible. As such, it should be pleasing and rewarding to look at. A well-designed customer journey map is equally at home on a PowerPoint slide and hanging on the wall above your desk.\nIt’s worth having your design team participate in distilling your data into something both visually appealing and easily understandable. Here are three potential templates to work with:\n- A fully-designed infographic, like the one for the hot chocolate shop above, can convey the information with a creative twist. If you have the design resources, going full visual will help keep the information manageable and consumable.\n- A simple table or graph puts the focus more squarely on the data. It’s a good idea to still add some design flourish for visual interest, though.\n- A grid or flowchart is the most “pure data” version, but it can still be visually appealing (as the example shows).\nA Good Customer Journey Map Helps You Understand Your Customers\nThe days when marketers could only do a few things to cause customers to passively fall through the standard-issue marketing funnel are over. Today, marketing journeys take countless shapes, with many diverse touchpoints.\nBut one thing hasn’t changed: The goal of all your marketing efforts is still to help customers—who come to you through many different media, channels and paths—to realize that your brand is the best solution for a particular problem. A good customized customer journey map will help you understand how your customer searches for a solution. And ultimately, it will enable you to meet the customer wherever they may be, and become a trusted guide for the rest of their journey.']	['<urn:uuid:acbbcc64-a109-43c1-ab0c-238165294e4b>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	11	95	1174
78	How does tile-based visualization help both network analysis and social media monitoring?	Tile-based visualization aids network analysis by enabling visual traffic analysis through graph representations, with hosts as nodes and traffic as flows. For social media monitoring, it allows revealing anomalous events through multi-level tile-based map interfaces. The technique scales to billions of data points by leveraging distributed computing and enables interactive exploration in web browsers for both use cases.	"['Big Data and IT Network Data Visualization\nDepartment of Engineering Technology, Mississippi Valley State University, USA.\nReceived on January 16, 2017\nAccepted on April 03, 2017\nVisualization with graphs is popular in the data analysis of Information Technology (IT) networks or computer networks. An IT network is often modelled as a graph with hosts being nodes and traffic being flows on many edges. General visualization methods are introduced in this paper. Applications and technology progress of visualization in IT network analysis and big data in IT network visualization are presented. The challenges of visualization and Big Data analytics in IT network visualization are also discussed. Big Data analytics with High Performance Computing (HPC) techniques, especially Graphics Processing Units (GPUs) helps accelerate IT network analysis and visualization.\nKeywords- Big data, Visualization, Network intrusion detection, Graphics processing units (GPUs), Data mining, Machine learning.\nWang, L. (2018). Big Data and IT Network Data Visualization. International Journal of Mathematical, Engineering and Management Sciences, 3(1), 9-16. https://dx.doi.org/10.33889/IJMEMS.2018.3.1-002.\nConflict of Interest\nAkoglu, L., Tong, H., & Koutra, D. (2015). Graph based anomaly detection and description: a survey. Data Mining and Knowledge Discovery, 29(3), 626-688.\nAttunity (2012). Enabling big data analytics by replicating oracle data to Greenplum, database trends and applications, September 12.\nBarrionuevo, M., Lopresti, M., Miranda, N., & Piccoli, M. F. (2015). Solving a big-data problem with GPU: the network traffic analysis. Journal of Computer Science & Technology, 15(1), 30-39.\nCai, X., Nie, F., & Huang, H. (2013). Multi-View K-means clustering on big data. Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI), 2598- 2604.\nCao, L., Wang, Q., & Rundensteiner, E. A. (2014). Interactive outlier exploration in big data streams. Proceedings of the VLDB Endowment, 7(13), 1621-1624.\nCao, N. (2014). Big data analytics lecture: data visualization. Columbia University.\nDatameer, Inc. (2013). The guide to big data analytics, White Paper, 1-39.\nElhenawy, I., Riad, A. E. D., Hassan, A., & Awadallah, N. (2011). Visualization techniques for intrusion detection- a survey. International Journal of Computer Science and Engineering Survey, 2(3),107-119\nGaffar, A., Darwish, E. M., & Tridane, A. (2014). Structuring heterogeneous big data for scalability and accuracy. International Journal of Digital Information and Wireless Communications, 4(1), 10-23.\nGangone, M. V., Whelan, M. J., & Janoyan, K. D. (2011). Wireless monitoring of a multi-span bridge superstructure for diagnostic load testing and system identiﬁcation. Computer-Aided Civil and Infrastructure Engineering, 26(7), 560–579.\nGlisic, B., Yarnold, M. T., Moon, F. L., & Aktan, A. E. (2014). Advanced visualization and accessibility to heterogeneous monitoring data. Computer‐Aided Civil and Infrastructure Engineering, 29(5), 382-398.\nGoodall, J. R., Lutters, W. G., Rheingans, P., & Komlodi, A. (2005). Preserving the big picture: Visual network traffic analysis with TNV. In IEEE Workshop on Visualization for Computer Security, (VizSEC 05). 47-54.\nHan, J., & Kamber, M. (2006). Data mining: concepts and techniques, second edition. Morgan Kaufmann Publishers.\nHertzmann, A., & Fleet, D. (2012). Machine learning and data mining lecture notes. University of Toronto Publisher, February 6.\nKeim, D. A. (2014). Exploring big data using visual analytics. EDBT/ICDT Workshops.\nLi, S., & Wu, Z. (2008). A model-free method for damage locating and quantifying in beam-like structure based on dynamic distributed strain measurements. Computer-Aided Civil and Infrastructure Engineering, 23(5), 404–413.\nLiao, Q., Shi, L., & Sun, X. (2012). Anomaly analysis and visualization through compressed graphs. IEEE LDAV Poster Session.\nLynch, J. P. (2007). An overview of wireless structural health monitoring for civil structures. Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 365(1851), 345-372.\nMadden, S. (2014). Tackling the challenges of big data: visualizing twitter. Workshop, Massachusetts Institute of Technology.\nMansmann, F., Fischer, F., Keim, D. A., & North, S. C. (2009, November). Visual support for analyzing network traffic and intrusion detection events using TreeMap and graph representations. In Proceedings of the Symposium on Computer Human Interaction for the Management of Information Technology (pp. 19-28). ACM.\nMarode, M. D. K., & Chavan, R. K. (2014). Survey of network traffic visualization techniques. International Journal of Computer Technology & Applications, 5(3), 876-883.\nRavet, F., Briffod, F., Glisic, B., Nikles, M., & Inaudi, D. (2009). Submillimeter crack detection with Brillouin-based ﬁber-optic sensors, IEEE Sensors Journal, 9(11), 1391-1396.\nSigurdardottir, D. H., Afonso, J. P. S., Hubbell, D. L. K., & Glisic, B. (2012). Streicker bridge: a two-year monitoring overview, bridge maintenance, safety, management, resilience and sustainability-Proceedings of the Sixth International Conference on Bridge Maintenance, Safety and Management, Stresa, Italy, July 8–12, pp.790–797.\nStreit, M. (2011). Guided visual analysis of heterogeneous data. Dissertation. Graz University of Technology, Graz, Austria, February.\nSuthaharan, S. (2014). Big data classification: problems and challenges in network intrusion prediction with machine learning. ACM SIGMETRICS Performance Evaluation Review, 41(4), 70-73.\nWang, L., & Alexander, C. A. (2016a). Additive manufacturing and big data. International Journal of Mathematical, Engineering and Management Sciences, 1(3), 107–121.\nWang, L., & Alexander, C. A. (2016b). Machine learning in big data. International Journal of Mathematical, Engineering and Management Sciences, 1(2), 52–61.\nYoussef, A., & Emam, A. (2011). Network intrusion detection using data mining and network behaviour analysis. International Journal of Computer Science & Information Technology, 3(6), 87-98.', 'Detecting anomalous events of a particular area in a timely manner is an important task. Geo-tagged social media data are useful resource, but the abundance of everyday language in them makes this task still challenging. To address such challenges, we present TopicOnTiles, a visual analytics system that can reveal the information relevant to anomalous events in a multi-level tile-based map interface by using social media data. To this end, we adopt and improve a recently proposed topic modeling method that can extract spatio-temporally exclusive topics corresponding to a particular region and a time point.\nTopicOnTiles: Tile-Based Spatio-Temporal Event Analytics via Exclusive Topic Modeling on Social Media\nSTExNMF: Spatio-Temporally Exclusive Topic Discovery for Anomalous Event Detection\nWe propose herein a novel topic modeling approach on text documents with spatio-temporal information (e.g., when and where a document was published) such as location-based social media data to discover prevalent topics or newly emerging events with respect to an area and a time point. We consider a map view composed of regular grids or tiles with each showing topic keywords from documents of the corresponding region. To this end, we present a tile-based spatio-temporally exclusive topic modeling approach called STExNMF, based on a novel nonnegative matrix factorization (NMF) technique.\nTile-Based Spatio-Temporal Visual Analytics via Topic Modeling on Social Media\nWe present a visual analytics system that supports the geospatiotemporal analysis of social media data based on a large-scale distributed topic modeling technique. Through the analysis of social media data in a given time and region, we can identify critical events in real time. However, it takes significant time to perform such analyses against a large amount of social media data. As a way to handle this issue, we developed an efficient tile-based topic modeling approach, which divides textual data into multiple subsets with respect to different regions and time frames at different zoom levels and applies topic modeling to each subset.\nGraph Mapping: Multi-Scale Community Visualization of Massive Graph Data\nGraph visualizations increase the perception of entity relationships in a network. However, as graph size and density increases, readability rapidly diminishes. In this cover article for the July 2017 edition of the Information Visualization journal, we present an end-to-end, tile-based visual analytic approach called graph mapping that utilizes cluster computing to turn large-scale graph (node-link) data into interactive visualizations in modern web browsers.\nMulti-Scale Community Visualization of Massive Graph Data\nGraph visualizations increase the perception of entity relationships in a network. However, as graph size and density increases, readability rapidly diminishes. In this paper, we present a tile-based visual analytic approach that leverages cluster computing to create large-scale, interactive graph visualizations in modern web browsers. Our approach is optimized for analyzing community structure and relationships.\nGlobal to Local Pattern of Life Analysis with Tile-Based Visual Analytics\nIn this paper, we present in-progress work on applications of tile-based visual analytics (TBVA) to population pattern of life analysis and geo-temporal event detection. TBVA uses multiresolution data tiling, analytics and layered high-fidelity visualization to enable interactive multi-scale analysis of billions of records in modern web browsers through techniques made familiar by online map services.\nTile Based Visual Analytics for Twitter Big Data Exploratory Analysis\nThe widespread use and adoption of web-based geo maps have provided a familiar set of interactions for exploring extremely large geo data spaces and can be applied to similarly large abstract data spaces. Building on these techniques, a tile based visual analytics system (TBVA) was developed that demonstrates interactive visualization for a one billion point Twitter dataset.\nLouvain Clustering for Big Data Graph Visual Analytics\nInteractive Data Exploration with ""Big Data Tukey Plots""\nOur goal is to develop new automated tools to produce effective raw data characterization on extremely large datasets. This paper reports on the development of a web based interactive scatter plot prototype that uses tile-based rendering similar to geographic maps and interaction paradigms.\nText Analytics and New Visualization Techniques\nText analytics have significantly advanced with techniques such as entity extraction and characterization, topic and opinion analysis, and sentiment and emotion extraction. But the visualization of text has advanced much more slowly. Recent visualization techniques for text, however, are providing new capabilities. In this talk we offer an overview of these new ways to organize massive volumes of text, characterize subjects, score synopses, and skim through a lot of documents. Together, these techniques can improve workflows for users focused on documents.\nValue in the Details: Understanding Data Through Visual Exploration\nAnalysts and data scientists work with large amounts of data, but the common approach, which dates back 20 years, is to roll up all the data into summary tables or charts, resulting in loss of detail. In contrast, direct visual exploratory analysis of massive amounts of raw data can yield insights that are otherwise overlooked. Here we highlight high-density visualizations that directly plot hundreds of millions of data points for applications such as market opportunity, periodicity analysis, and anomaly identification.\nUntangling Influence and Desire: Visual Analysis of Massive Graph Data\nThis session demonstrates using open source tools and techniques for visually exploring massive node-link graphs in a web browser by visualizing all the data. Seeing all the data reveals informative patterns and provides important context to understanding insights. Examples will highlight large scale graph analysis of social networks, customer purchase history, and health care industry data.\nHighly Scalable Tile-Based Visualization for Exploratory Data Analysis\nThe open source Aperture Tiles provides interactive data exploration with continuous zooming on large scale datasets. By leveraging big data software stacks Hadoop, Apache Spark and HBase for distributed computation and storage, the technique can scale to data sets with billions of data points or more. In this talk we review the Tile-based visual analytic methodology, challenges and highlight some example applications.']"	['<urn:uuid:286c04be-6d99-4d82-8db4-b4503e6148b5>', '<urn:uuid:b0c62a79-7ede-4202-84c3-5846e78b8399>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	12	58	1801
79	I've been hearing about massive garbage accumulations in our seas. Can you tell me about any recent discoveries of large trash formations in the Caribbean region?	In 2017, a floating trash 'island' was spotted in the Caribbean Sea near Roatan. Caroline Power's video and images showed floating rubbish stretching for miles in the once pristine Caribbean waters.	['Prevention is better than cure, we know it, but too often ignore it until it is too late. Over the last year or so, increasing evidence of plastic polluting the oceans has made global headlines, and now the plastic story is hitting home. A local study demonstrated that a popular local fish have been found having ingested plastic (RTHK) the study reports 60% of flat head grey mullet had ingested plastic. Granted it was a small study, but that does necessarily mean the results can be ignored.\nEven bottled drinking water, sold on the basis of safer water, is under scrutiny revealing it is not just fish enjoying plastic diet. A study found plastic in the majority of bottled drinking water (BBC report), to which, the manufacturers responded there is no Standard, implying measurement, monitoring, controlling or limiting the quantity of plastic in drinking water is not needed. It’s a problem, standards are created retrospectively, we can’t create a new Standard for products that do not yet exist, in the case of bottled drinking water nobody, and that really means Govt., has been watching the store.\nAbroad, some countries have started, somewhat belatedly, to act, and that will impact the thousands of manufacturers involved in the plastics business. In the UK outright bans are threatened for certain plastic products, an attempt to prevent further damage, but no word how the existing pollution in the environment will be removed from the oceans.\nWhat is interesting, the key motivator has not been green groups, the news reporting in the mainstream media has been instrumental, and highlighted these environmental issues, with shocking photos and videos circulated through social media. Justin Hoffman’s photo below is a good example.\nIt’s a photo that I wish didn’t exist but now that it does I want everyone to see it. What started as an opportunity to photograph a cute little sea horse turned into one of frustration and sadness as the incoming tide brought with it countless pieces of trash and sewage. This sea horse drifts long with the trash day in and day out as it rides the currents that flow along the Indonesian archipelago. This photo serves as an allegory for the current and future state of our oceans. What sort of future are we creating? How can your actions shape our planet? . thanks to @eyosexpeditions for getting me there and to @nhm_wpy and @sea_legacy for getting this photo in front of as many eyes as possible. Go to @sea_legacy to see how you can make a difference. . #plastic #seahorse #wpy53 #wildlifephotography #conservation @nhm_wpy @noaadebris #switchthestick\nIt’s a new era, striking video and images, that have been circulated widely and easily through social media have motivated the public, in turn, pressured politicians into action.\nOn 18 April 2018, the UK Govt. announced its intention to ban plastic straws, plastic stirrers, and cotton buds using plastic, the latter because of the seahorse image, we might never know.\nBanning single-use plastic is undoubtedly a step in the right direction, and avoids difficult waste management questions, how did that drinking straw or cotton bud leap from the consumer into the ocean. We know there are islands of floating garbage in worlds ocean, has seafill taken over the role of landfill?\nA shocking example must be the 2017 Floating Trash ‘Island’ Spotted in the Caribbean Sea Near Roatan #seafill (there is a video on youtube (https://youtu.be/GSMGKwZBaWM) Caroline Power’s video and images shocked the world showing the floating rubbish stretching for miles in the once pristine Caribbean waters (Telegraph article). Was this island of waste washed from the Caribbean land islands or seafill?\nOf course, it’s on Facebook too, if you can bear it view more images https://www.facebook.com/carolinepowerphotography\nSadly, the fact remains, you have to admit we did it, and the damage is already done, the UK Govt. estimate there are 150 million tonnes of plastic waste already in the world’s oceans, the UK’s ban on micro-beads, single-use plastics today may help slow the disaster, but its not a cure.']	['<urn:uuid:7dc49601-acbc-41e6-b25b-19909b846f43>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	26	31	672
80	How do modern movements balance innovation with social responsibility?	Modern movements demonstrate a complex balance between innovation and social responsibility. In the cryptocurrency sector, while the technology promotes financial democratization and innovation through decentralized systems, it faces criticism for its environmental impact, consuming massive amounts of energy (128.84 TWh annually for bitcoin mining alone) and contributing to greenhouse gas emissions. The industry is working to balance these concerns by adopting sustainable practices like renewable energy and more efficient mining techniques. Similarly, in Lebanon and Iraq's protest movements, while pushing for innovative systemic change and revolution, demonstrators maintain social responsibility by ensuring their movements remain non-violent, inclusive across class and sectarian lines, and focused on sustainable long-term change rather than just immediate reforms. Both movements show that innovation and social responsibility can be complementary rather than conflicting goals.	"['From Iraq to Lebanon and back: the people want the fall of the regime\nProtestors are clear: they do not want reforms, they want a revolution.\nIn Beirut’s Riad al-Solh Square, a group of protestors gather around a beating drum and chant, “From Iraq to Beirut, one revolution and it won’t die.” In Baghdad, protesters hold a banner with the words, “From Baghdad to Beirut, we shall continue. No Sunni no Shia; our state must be secular.” As the two countries erupt in protest, social media brims with such messages of solidarity, sent from Iraq to Lebanon and vice versa.\nSome scenes from both countries bear uncanny resemblance: young, working class men revolting shirtless; women leading the chants, standing defiantly in front of security forces; satirical songs, aimed at the ruling elite, spreading like wildfire; previously-deserted concrete buildings occupied, floor after floor, by demonstrators raising flags; and protesters swaying to “Mawtini, Mawtini”, Iraq’s national anthem, and for many that of Palestine as well, equally emotional and angry, their raspy voices calling out for the downfall of a sectarian regime.\nProtestors in Lebanon chanting: ""From Baghdad to Beirut one revolution that will not die""\nThis week, in both countries, the government agreed to resign and protesters took to the streets, once again, jubilant and in tears. They celebrate, but only momentarily, claiming there is still a lot of work left to do before there are real, tangible political results.\nLumping protests in both countries together is dangerous -- given their many historical and structural differences, and their continuous denouncement by Iran-backed politicians as conspiratorial or foreign-influenced. But these protests do have key similarities that explicate important political and regional trends.\n“Against sectarianism, against sectarianism”\nSectarianism, despite its straightforward definition, is not easily graspable. It requires zooming into a state’s specific history, its institutions, and state-building rhetoric. The way sectarian identities are used by politicians and militias, and the way these identities are experienced and expressed, are not fixed; they are ever-changing and ambiguous. But in both Iraq and Lebanon, sectarianism has been used as a political tool by the ruling elite to maintain and control power.\nLebanon’s very foundation, even prior to its independence in 1943, was based on the idea of power-sharing between the country’s many different sects. This system of power-sharing, referred to as consociationalism, extends to political representation, legal matters (i.e. personal affairs, such as marriage and divorce, are relegated to the different religious laws instead of a civil law) and public sector job divisions, among others. Political elites provide both ‘security’ and ‘welfare’ to members of their sect, thereby tying those in need of protection and financial support, i.e. the working class and lower middle class, to them. In this sense, sectarian identity is used as a means of trapping people and ensuring they continue to vote for the leaders of their sect.\nIn Iraq, since the crumbling of Saddam Hussein’s authoritarian regime, the political landscape bears a resemblance to Lebanon’s system of sectarian power-sharing. However, prior to 2003, the regime repressed modes of religious or sectarian expression and pursued an authoritarian, hegemonic form of governance under the Baath regime. The Iraqi constitution, which was ratified in 2005, changed this: Iraq was re-established as a federal and parliamentary democracy, whereby the country’s different religions, sects, and nationalities are acknowledged and legitimized. This switch, from a collective Iraqi identity to sub-identities, enabled regional power players to exploit the power vacuum.\nIn both countries, sectarian identities are used by the ruling elite to pit people against one another; yet, simultaneously, the project of national unity and state-building depends (since Lebanon’s inception and since post-2003 Iraq) on the idea of the sectarian quota (‘muhasasa al ta’ifiyye’). The regional conflicts and especially in the last decade the revolution in Syria, and its ensuing geopolitical and proxy layers, both sowed and amplified sectarian sensitivities in both countries, especially between Sunnis and Shias.\nTherein lies the key similarity between Lebanon and Iraq: both are sectarian regimes (‘anthima ta’ifiyya’), operating as parliamentary democracies, that aim to “accommodate” the country’s different sects. In the same way that Lebanon’s presidency is reserved for a Maronite Christian, its prime ministership for a Sunni, and its speaker of parliament for a Shia, Iraq’s presidency is for Kurds, its prime ministership for Shias, and its speaker of the house for Sunnis. But, beyond that, and perhaps even more importantly, clientelistic networks and modes of power domination among the ruling elite continue to rest on the exploitation of these sectarian identities (‘hawiyyat ta’ifiyya).\nAnd today, why have protests erupted?\nOn one level, and perhaps the most rudimentary one, the protests erupted for economic and social reasons -- high levels of unemployment, corruption and the elite’s pickpocketing of wealth -- and a total disdain of public services -- from both states’ weak infrastructure to their lack of tangible insurance or social security. Across both countries, revolts were in the name of the hungry, targeting the political class as one broad face representing those who have led to this nation-wide impoverishment.\nIn Lebanon, one of the most resounding chants has been “all of them means all of them” (kelon ya’ni kelon), to illustrate that all politicians have a hand in this. This chant has also been used as an opportunity to point fingers at Hezbollah Secretary-General Hassan Nasrallah, whose supporters refuse the narrative that Nasrallah is one of the corrupt politicians, retorting back to protestors, “all of them means all of them; but Nasrallah is the most dignified of them”. In Iraq, it has been “the people rose alone, don’t ride the wave” (hatha al-shai’b wehda tela’a, la terkab al-mawja). Iraq’s slogan is largely directed against the Shia political leader and cleric Muqtada al-Sadr, a veteran of the ethnic-sectarian quota, known for riding the wave of previous protest movements..\nBut, on a secondary level, the revolts have been against the political system itself. Chants have been against sectarianism, with the aim of bringing down regimes dependent on the mantra of divide and conquer. In both countries, anti-establishment protestors have refrained from using sectarian narratives and, on the contrary, shifted the discourse to one of unity.\nLebanese protesters and analysts have said the revolution has finally ended the civil war: its painful memory of sectarianism and fear of the ‘other’. In northern Tripoli, a Sunni-majority city, there have been chants for Sour, a southern Shia-majority city; and in the Shia-majority city of Baalbeck there have been chants for the Sunni-majority town of Beddawi; and in the Christian city of Jal el-Dib, vigils were held for the Shia-majority town of Nabatieh. Vigils across the country were held for protesters who were attacked and beaten up by Hezbollah and Amal supporters.\nProtestors are clear: they do not want reforms, they want a revolution, a revolution that unseats sectarianism and overhauls those who rule in its name. In Iraq, similarly, people stood up against all powerful figures and groups, making it clear that no politician can override this movement as they have done in the past. Protesters spoke out against their corrupt government, against the pressures of Iran’s proxy politics in Iraq, against the religious establishment (Marjaiyya), and more. This is why Hariri and Abdul Mahdi’s set of reforms were not enough; this is why their resignation is not enough.\nOn a third level, protests have been non-hierarchical, decentralized, and leaderless in a similar vein to the initial phases of the Arab uprisings and the horizontal movements across the world, of which Chile, today, is one of them. This utterly confuses (and frightens) political parties, debunking the entrenched idea of people being sheep in need of a shepherd.\nProtests have also happened across different classes. In Lebanon this is distinct from previous protests in the country’s recent history - such as the 2015 ones - whereby protests were centralized in Beirut and spearheaded by civil society members and middle-class/upper middle class factions. The biggest shock from the current movement came from the working class revolting and saying, loud and clear, that they have suffered long and hard from their leaders (‘zu’ama’). People, from those selling corn on carts to professors from elite universities to yoga instructors, are standing side by side in the country’s major cities, demanding change.\nProtests in Iraq are mostly in Shia-dominated areas, due to Sunni-majority areas in the western and northern parts of the country arguably fearing a government-led propagandist war on them for being “sleeping ISIS cells”. Iraq, unlike Lebanon, is still in the early stages of post-sectarianism and continues to recover from its war with ISIL in 2014; and Iraqi protesters have been met with heavier security measures from the government and a certainly bloodier response from militias. Yet, the Iraqi movement has also been leaderless, youth-led, and grassroots, erupting by word of mouth.\nAnd, finally, on the fourth level, these protests have broken a barrier of fear and announced their denouncement of Iran. In Lebanon, people are speaking up against President Michel Aoun and his son in law Minister Gebran Bassil; they are speaking up against Prime Minister Saad Hariri as well as every other major politician from Lebanese Forces’ Samir Geagea, to the Progressive Socialist Party’s Walid Jumblat, and despite attacks by supporters of Hezbollah and Amal movement, also against Hassan Nasrallah and Speaker of Parliament Nabih Berri .\nNasrallah has made his party’s position clear by reiterating in his speeches, following the revolution, that he will continue to defend the current government’s mandate and work with other political parties. Iraqi protesters, too, point their fingers at Iran and the militias it backs, attacking and torching their offices and burning Iranian flags in Baghdad. Despite the heavy violence protesters met by protesters from security forces and militias, with around 250 dead since October, they continue to fill up the squares. Last Friday, despite the military grade tear gas, Baghdad’s Tahrir Square witnessed its largest protest movement since the fall of Saddam Hussein.\nThe Arab spring is not dead\nFrom Algeria to Sudan, from Iraq to Lebanon, people continue to march on, demanding social and economic reforms, demanding a revolution, demanding an end to corruption and authoritarianism. Every time there is talk of the Arab spring being a thing of the past, people rise up to say it isn’t. In fact, the protest movements across Lebanon and Iraq reflect how much youth have learnt from other Arab countries. Both Lebanese and Iraqis are making it loud and clear that the change includes the entire constitution, judiciary, ruling class, and, most importantly, the system. From the streets, protesters respond to televised speeches of politicians: we will not be fooled. They respond to politicians’ adamant insistence on calling this an uprising or a series of protests: no, this is a revolution and it is a revolution against you. The people are rebelling against sectarianism, asserting a future where their differences aren’t used against them by politicians.\nThe future is murky. Can systemic change in the current geopolitical status quo be achieved? What will it take? And how will protesters navigate the role of Iran, other external players, and entrenched structures?\nWhat is clear is that protesters are relentless. They say what they have achieved in the streets - from reclaiming public spaces to Iraq’s tuktuks rushing wounded protesters to nearby hospitals and Lebanon’s protesters recycling waste from demonstrations - the state hasn’t been able to ever achieve.\nGet our weekly email', 'As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry.']"	['<urn:uuid:4c4caf39-ff96-4ecb-a1f2-1b45ceb2c59c>', '<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	9	128	2847
81	I want to pursue social work education - what's the key difference between getting a bachelor's versus a master's degree in social work in terms of job opportunities?	With a bachelor's degree in social work (BSW), you can enter entry-level positions, representing about 12% of social workers. In contrast, those with a master's degree or higher make up 45% of social workers and have broader career opportunities. The master's degree allows for more advanced practice, including clinical work, and provides the foundation needed for obtaining higher levels of licensure. Social work is a broad profession offering various possibilities, from healthcare settings to community organizing.	"['5 edition of Educating the Baccalaureate Social Worker found in the catalog.\nby Ballinger Pub Co\nWritten in English\n|The Physical Object|\n|Number of Pages||250|\nWhy this book is important for prospective social workers: Social work is a much broader profession than many students realize; this book highlights the possibilities for those holding an online master\'s of social work, from working in a health care setting to becoming involved with community organizing. 4. Social work is a field of study in social sciences. It is mainly concerned with the study of social problems and social interventions. Social work sets to study individuals, groups, community and.\nThe Social Work ASWB® Bachelors Exam Guide: A Comprehensive Guide for Success Test focuses on knowledge acquired while obtaining a Baccalaureate degree in Social Work (BSW). A small number of jurisdictions license social workers at an Associate level and require the ASWB Associate examination. Mary Ann Suppes is a professor at Mount Mary University where she taught and chaired the Social Work Department for many years. She currently teaches two courses per semester at Mount Mary and remains involved with committees. In addition to previous editions of The Social Work Experience, she has published several periodical is a member of the Baccalaureate Social Work Program Price: $\nIf you are considering becoming a social worker, you have come to the right place. How to Become a Social Worker: A Quick Guide seeks to answer your most important questions about the social work field as a whole, salary and job figures, possible educational paths, and the best schools in . Social Work Education. Because education is the foundation upon which the social work profession is built, many researchers have dedicated their time to examining and exploring social work education models. Cohen provides information for educators by discussing the debate between design-based practice (DBP) and evidence-based practice (EBP).\nNavy negative undelivered orders\ntrials in His Majestys Court of Kings bench\nPractice of inter-communion and the doctrine of the church\nCable Car Book\nGoldsmiths Natural history\nInterim hearing on public financing of political campaigns ... July 17, 1974 ... San Francisco, California\nMitigating threats and managing the Ramsey Canyon leopard frog in Arizona\nPhysical design and query compilation for a semantic data model (assuming memory residence).\nSocratic education inPlatos early dialogues\nLooking at schools\nCross sections of the Hudson River estuary from Troy to New York City, New York\nRoddy Owens Africa\nContinuous improvement in the mathematics classroom\nThe Roman Empire, 27 B.C.-A.D. 476\nEducating the Baccalaureate Social Worker: A Curriculum Development Resource Guide [West Virginia University Undergraduate Social Work Curriculm] on *FREE* shipping on qualifying offers.\nEducating the Baccalaureate Social Worker: A. Educating the Baccalaureate Social Worker: Report of the Undergraduate Social Worker Curriculum Development Project [Baer, Betty, Federico, Ronald C.] on *FREE* shipping on qualifying offers. Educating the Baccalaureate Social Worker: Report of the Undergraduate Social Worker Curriculum Development Project.\nWest Virginia University. Undergraduate Social Work Curriculum Development Project. Educating the baccalaureate social worker. Cambridge, Mass.: Ballinger Pub. Co., © (OCoLC) Document Type: Book: All Authors / Contributors: Betty L Baer; Ronald C Federico; West Virginia University.\nUndergraduate Social Work Curriculum. The Undergraduate Social Work Curriculum Development Project was funded by the Department of Health, Education, and Welfare to strengthen the education and practice competency of baccalaureate social workers, the entry-level professional social workers.\nThe project report identifies the purposes of social work and the major objectives, functions, and activities necessary to attain by: Get this from a library. Educating the baccalaureate social worker: report of the Undergraduate Social Work Curriculum Development Project.\n[Betty L Baer; Ronald C Federico; West Virginia University. Undergraduate Social Work Curriculum Development Project.].\nProgram structure and curriculum for the undergraduate social work student are discussed in this second volume from the Undergraduate Social Work Curriculum Development Project. In Part I the major themes and curriculum implications that emerged from the first two years of the Project\'s work are identified and the activities of the third year are summarized.\nEditorial Policy and Submission Requirements. The Journal of Baccalaureate Social Work is a refereed journal emphasizing contributions of the baccalaureate community in promoting the continuing development and excellence of baccalaureate social work education and practice.\nTo this end, the journal facilitates sharing of a wide range of experiences, knowledge, ideas, and research as well as. ® The Association of Baccalaureate Social Work Program Directors Duke Street, Suite • Alexandria, VA • Office: • Fax: The Association of Baccalaureate Social Work Program Directors, Inc.\n(BPD) is dedicated to the promotion of excellence in Baccalaureate Social Work Education. Information on the governance and leadership of the association can be accessed below. Books about social work are often available online as instant downloads, but not all of them are free. Textbooks, for instance, are more expensive than mainstream books, just as they are in hardback or inyou also can find some great deals, as you’ll see from this list of 25 essential e-books that every social worker should read.\nIndividual books are listed as well as. Darlyne has written numerous articles and book chapters, Prof. Joyner appoints leaders in social work education to all the various commissions, councils, and task forces of CSWE.\nShe is a member of the National Association of Social Workers; the Association of Baccalaureate Social Work Program Director’s (BPD) and former president; and a. Founded inthe Council on Social Work Education is the national association representing social work education in the United States.\nIts members include over accredited baccalaureate and master’s degree social work programs, as well as individual social work educators, practitioners, and agencies dedicated to advancing quality. Journal of Social Work Education ( - current) Formerly known as.\nJournal of Education for Social Work ( - ) Books; Keep up to date. Register to receive personalised research and resources by email.\nSign me up. Taylor and Francis Group Facebook page. Taylor and Francis Group Twitter page. An official transcript indicating that the applicant acquired a baccalaureate degree prior to April 6,from an accredited institution of higher education in one of the following fields: guidance and counseling, human services, marriage and family counseling, psychology, sociology, vocational/disability rehabilitation, and social work.\nCurrent Number of Social Work Programs. As of the June COA meeting, there are. accredited baccalaureate social work programs; accredited master\'s social work programs; 15 baccalaureate social work programs in candidacy; 23 master\'s social work programs in candidacy; Resources. Learn About Degree Mills and Accreditation Mills.\nAbout The Social Work License. Social work licensure is a requirement by each state and must be met to attain a state license. This usually involves a social work licensing exam and an educational background from a Council on Social Work Education (CSWE) accredited school.\nThe New Social Worker magazine publishes articles on social work careers, social work education, social work ethics, technology, books, and more. Published since The Licensed Baccalaureate Social Worker or LBSW is the first level of licensure recognized for Social Workers in a number of states.\nIt is from the initial license that one is able to accumulate the required experience to earn more advanced licensure and is a critical first step for Licensed Social Workers in a few states to enter this career.\n10 Books Every Social Worker Should Read. Ma by Gabriela Acosta Social workers are faced with helping clients and patients work through various issues — from substance abuse and depression to a lengthy adoption process and eating disorders.\nWith so many resources online, it can be tough to find the best ones. ""I joined BPD to be in an environment that understands the challenges and opportunities presented within teaching and supervising in a baccalaureate program.\nThe skills learned here help me to be a better instructor, field director, and social work professional.""- Tammy Molina Moore. USC\'s online Master of Social Work program from top-ranked USC Suzanne Dworak-Peck School of Social Work, is designed for aspiring social work leaders.\nThe Advanced Standing track, available to BSW holders, can be completed in as little as one year. obtain employment as social workers is known; therefore, it is not possible to determine the size of the total pipeline of social workers with a formal social work education. • The ACS describes the following three main educational pathways to working as a social worker: a master’s degree or higher (45% of social workers), a BSW (12%), and a.The mission for the Virginia Wesleyan University’s Baccalaureate Social Work (BSW) Program was created specifically to address the profession’s purpose, values and ethics.\nWe referred to NASW’s Mission of Social Work to ensure compliant language and consistency in the development of our Program’s mission.', 'Get a discounted first session with your therapist. Margaret Martin Betterhelp Therapist LCSW …\nLCSW stands for licensed clinical social worker. They are social workers who have gone on to obtain their master’s in social work (MSW) and complete the requirements in their state to acquire their expert license. By obtaining their MSW and license, they can work in a range of environments, explore different expertises, and even open their own personal practice.\nWhat does a Licensed Medical Social Worker Do?\nA certified medical social worker provides treatment to clients with psychological and mental issuesExternal link: open_in_new that are affecting their lives. They work with their clients to listen to their requirements and provide the support and resources needed to manage those concerns.\nLCSWs likewise have the capability to detect and deal with the problems of their clients, although this might vary by state. This can be in the form of offering treatment, supplying recommendations, and working with other specialists like medical professionals to come up with a reliable treatment prepare for their customer.\nWhere do LCSWs work?\nLCSWs can operate in a selection of settings. Some work in offices for research study purposes, and others may visit their customers in schools, their house, recreation center, healthcare facilities, assisted living centers, and more. The work environment of an LCSW differs depending upon their location of expertise.\nHow do I become a LCSW in my state?\nThere are other online resources to help you find out how to become a certified scientific social worker in your state. One of those resources provides information on how to end up being a LCSW in your state which permits you to pick your state and evaluation education requirements and LCSW exam requirements (if relevant).\nIs a LCSW considered a physician?\nLCSWs have the ability to supply psychiatric therapy to their clients, nevertheless, their training focuses on connecting their clients with the abilities and resources needed to satisfy their needs. LCSWs can easily collaborate with psychiatrists and medical professionals to develop comprehensive treatment plans for customers.\nWhile those in the field of psychiatry can go on to medical school and make their Physician of Medicine, the master’s in social work (MSW) is the highest level of education that LCSWs obtain. However depending on an LCSW’s career objectives, they may decide to finish a DSW program down the line.\nHow much does a LCSW make?\nAs of May 2020, the median yearly wage for social workers was $51,760 External link: open_in_new, according to the BLS. The salary of an LCSW differs based upon aspects such as their company, specialized, and the amount of time they work. Many social workers tend to work full-time, however some might be on call.\nSocial work licensure in Pennsylvania can take 6 to 12 years. For a provisionary license, a social worker needs to work for at least 3 years after their BSW before applying. It can take 7 years to acquire this temporary social worker license or online social work degree. For the LSW, a social worker needs at least an MSW, which can take one to 3 years of extra schooling, depending on if the trainee participates in full-time or part-time or starts in advanced standing. Usually, it takes two years to complete an MSW; the LSW does not need post-degree experience. A social worker can get LSW licensure in six years. For the LCSW, a candidate should acquire an LSW and then accrue 3,000 hours of supervised experience, which can take six years. It can take 12 years of combined education and experience to become an LCSW.\nA psychologist is a social scientist who is trained to study human habits and psychological procedures. Psychologists can work in a range of research study or medical settings. Psychology degrees are readily available at all levels: bachelor’s, master’s, or doctorate (PhD or PsyD). Postgraduate degree and licensing are required for those in independent practice or who provide client care, including medical, counseling and school psychologists.\nPhD programs in medical psychology emphasize theory and research methods and prepare trainees for either academic work or professions as professionals. The PsyD, which was produced in the late 1960s to address a lack of practitioners, stresses training in treatment and counseling. Psychologists with either degree can practice therapy however are needed to finish several years of monitored practice before becoming licensed.']"	['<urn:uuid:8f43fb9f-c282-4e34-b4d1-d9b97d3c0445>', '<urn:uuid:9470fa3e-2913-4c71-98ae-a09a42016162>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	28	76	2166
82	biggest health problems from long term city noise exposure	Long-time exposure to high noise levels can lead to stress, high blood pressure, insomnia, and impairment of cognitive performance in children.	['1, 2German Institutes of Textile and Fiber Research, Denkendorf, Germany\n3Institute for Textile and Fiber Technologies, Stuttgart, Germany\n2, 3Corresponding authors\nVibroengineering PROCEDIA, Vol. 31, 2020, p. 103-108.\nReceived 24 January 2020; accepted 11 February 2020; published 7 May 2020\n45th International Conference on Vibroengineering in Dublin, Ireland, May 7, 2020\nSince the first usage of absorbing structures to modify architectural acoustics the dampening of low frequencies has proven to be a difficult issue. Due to the rise of the population and concentration of said population in urban areas, also known as urban densification, the noise level has risen over the last years. A long-term exposure to noise can lead to serious health problems such as high blood pressure and sleep deprivation. The omnipresent sound in urban areas has a direct impact on the personal well-being. Currently used broadband absorbers work well in a frequency range from 300 Hz to 5 kHz. The dampening of frequencies below 300 Hz, especially below 200 Hz, requires large voluminas due to the wavelength and the absorbing mechanism. To achieve absorption of low frequencies a textile resonator with multiple absorbing mechanisms is proposed. The conversion of energy from the acoustic pressure field in mechanical oscillations as well as heat provides the possibility for efficient absorbers without large voluminas. Compared to common membrane resonators, which similar to Helmholtz resonators use a closed cavity behind the membrane, the textile resonators do not need a closed cavity to generate friction and visco-thermal losses.\nKeywords: textile resonator, absorber, resonances, visco-thermal-dampening.\nThe ongoing urbanization that took place in the last decades led and still leads to a higher density in population and thus leading to more noise pollution due to growth in building construction as well as public and private transport . For example, the population of Munich increased by 1.13 % from 2018 to 2019 . Given the fact that noise can lead to higher stress levels followed by serious health problems, this is an alarming development. A long-time exposure to high noise levels can lead to stress, high blood pressure, insomnia, impairment of cognitive performance in children and other severe diseases [3-6]. About 56 million people in the European Union in areas with more than 250 000 residents are exposed to road noise of more than average 55 dB per year . The urban densification is a health risk for humankind. The solution for this task is the reduction of noise either by reducing the sources or by increasing the effectivity of sound absorption in the relevant environment.\nConventional absorbers work well in a frequency range starting from 200 Hz. This sound absorption can be achieved by using foams, wool or other porous structures. The efficient sound absorption of low frequencies through porous media (< 200 Hz) by visco-thermal dampening requires large volumes, more precise great depth. A porous absorber is most effective when it is positioned at the location of the maximum sound particle velocity in front of a reverberative wall. For example: the lower limiting frequency of 100 Hz requires a depth of /4 hence 85 cm porous structure (in air). The sound transmission loss through a media is given by the mass law:\nbeing the transmission loss, the media mass and the sound wave frequency [8, 9]. The prediction of the mass law states that doubling the mass of the media results in a 6 dB higher transmission loss. Furthermore, it is obvious that lower frequencies require thicker porous media to maintain a given transmission loss. Even the usage of Helmholtz resonators leads to structures with great volume/depth. The resonance frequency of a Helmholtz resonator is defined by the following equation:\nwhere the variable is the resonance frequency, is the speed of sound, the cross section area of the neck, the effective length of the neck and the volume of the cavity . It can be seen that the volume is the key factor in reducing the resonance frequency. In favor of working or living space those high volume structures are generally not considered for the acoustic design.\nThe presented research focusses on textiles with local resonating structures in order to dampen impinging sound in frequencies lower than 200 Hz. The introduction of a mass on a textile surface, for example an embroidered structure, results in a tunable resonance frequency of such surface. The textile surface itself provides the spring as well as the damper to create a mass-spring-damper system. The impinging sound forces the textile surface to vibrate when the resonance frequency is hit, thus resulting in a rise in the mechanical movement of the textile membrane. The mechanical energy is directly transformed from the incident wave. The movement of the textile was analysed by using laser vibrometry, observing Chladni-sound-figures and FFT-spectrum-analysis of the sound resulting from an impact on the textile surface. In sum, the transformation of sound energy leads to a higher local mechanical energy density which is dampened subsequently by friction in the textile. The performance of the absorber is furthermore increased by visco-thermal effects due to the movement of the textile surface. The movement of the surface provokes an air flow through the pores of the fabric hence leading to friction losses. In summary, the sound energy is reduced by conversion into heat by friction. In conclusion the research in textiles with resonant structures achieved by local resonators allows the manufacturing of thin absorbing panels with a focus on frequencies < 200 Hz. In contrast to currently used low frequency absorbers like Helmholtz or membrane absorbers, the textile resonators have no necessity for a high volume chamber to achieve a significant absorption. A significant health benefit for affected people can be expected.\n2. Experimental analysis\nTo obtain the influences of the textile parameters on the resonance frequencies, a design of experiments based on statistic methods, namely the 2 factorial design, is drafted. Four different parameters (4) are taken into account. The fiber material for all experiments is polyester with a fiber fineness of 167 dtex. The fabric has a grammage of 95 g/m2.\nThe four parameters are:\n– weaving construction (linen or twill);\n– mass of the resonator (1 gram or 11 gram);\n– width of the area the mass is distributed on (3,8 cm2 and 21,2 cm2);\n– fabric stiffness (added metal filaments with 0,3 mm and 0,5 mm diameter to vary the stiffness).\nBased on these parameters a set of 16 (24) resonators with three samples of each combination are produced. Fig. 1(a) shows the top view and Fig. 1(b) the bottom view of the textile resonator.\nHaving established these parameters a roadmap for experiments is generated. Two different analyses are performed.\n1) Forming of “Chladni sound figures” by scattering black fine sand onto the surface as resonances of the textile structure are hit (Fig. 2). The experiment is conducted by placing the frame with the textile on a rig with a speaker, and exciting the structure with rising frequencies.\nFig. 1. Textile resonator: a) top view and b) bottom view\nFig. 2. Chladni sound figure on textile resonator\n2) FFT-spectrum-analysis of the sound resulting from an impact on the textile surface. Measured with the PULSE-Software and a microphone by Brüel & Kjæer.\nFollowing the determination of the influences, a new designed set of resonators is drafted. Based on the achieved resonator results an improved resonator regarding fibre stiffness, weaving construction and mass is selected for frequencies between 50 and 100 Hz. With this defined resonator the influence of varied forms of frames is tested.\n– freely vibrating area of the textile surface (3600 mm2 or 6400 mm2);\n– geometry of the frame (square or round).\nThese four resonators with different frames each are examined for their resonance frequencies with a Polytec PSV-500-3D Scanning Vibrometer.\nThe visual interpretation of the “Chladni sound figures” shows resonance frequencies from 51 Hz to 224 Hz, depending on the structural design.\nThe FFT-spectrum-analysis determines the first eigenmodes for the textile structures from 38 Hz to 109 Hz. The statistic evaluation of the FFT-spectrum-analysis determines that an added mass on the textile surface has a high impact on a resulting resonance frequency. Adding 10 grams of mass will lower the frequency by 36 Hz. Weaving constructions as well as the fabric stiffness have an effect on the resonance although such effect is significantly lower than the effect of added mass achieved from a set of carried out experiments. The width of the mass distribution is without effect on resonances. In addition to these effects there are reciprocal effects occurring due to the symbiosis between parameters.\nThe measurements with the laservibrometer showed resonances depending on the different resonator structures. The three following different resonances could be visualized:\n– piston-like movement of the added mass; equivalent to the vibration of a speaker membrane;\n– tilting of the added mass in two directions of space with two different frequencies;\n– oscillations of the textile surface without movement of the mass.\nFig. 3 shows the three different resonances. Blue areas display no movement and red areas indicate a large deflection.\nFig. 3. Different resonances of the textile resonator: a) piston-like movement, b) tilting and c) oscillation of the textile surface\nThe piston-like movement has a resonance frequency of 72 Hz for the small squares, 65 Hz for the small circles, 30 Hz for the large squares and 39 Hz for the large circles. The amplitude of the small surfaces measured by laservibometry, is lower than 10 nm whereas the amplitude for the larger surfaces is significantly higher than 40 nm.\nThe tilting happens in two distinct directions in space. These two directions are always orthogonally oriented to each other and vary approximately 10 Hz. Regardless of the direction the amplitudes are in a range from 25 nm up to more than 100 nm. The tilting on the small areas occurs between 38 and 48 Hz while the tilting on the large areas occurs between 60 and 74 Hz. The geometry of the area seems to have no effect at all.\nThe large areas show oscillations of the textile surface starting from 95 Hz with significant peaks following up at 143 Hz and 192 Hz. Even more resonances can be observed at higher frequencies. For the smaller areas the first resonance occurs at 143 Hz followed up by 192 Hz and other resonances with higher frequencies. The collective resonance frequency of 143 Hz has an amplitude of 8 nm for both larger textile surfaces and 6 nm for both smaller textile surfaces.\n4. Discussion of results\nThe visual interpretation of resonating frequencies via “Chladni sound figures” is a viable experimental analysis in order to gain a first impression of the change in resonance frequencies due to changes in structure. Added mass by the sand as well as the optical interpretation must be seen as critical and as mentioned before this analysis is only suitable for a first impression.\nThe FFT-spectrum-analysis has proven the functionality of textile resonators at frequencies below 100 Hz. Furthermore, it can be stated, that by altering the textile configuration a change in resonance is achieved. The results offer the opportunity to tune the textile resonator to a desired frequency. Fig. 4 displays the FFT-spectrum as well as the time signal of an exemplary measurement.\nThe results from the laservibrometer show that a larger textile area leads to a lower resonance frequency (piston-like). This result is similar to the occurrence of eigenfrequencies depending on the geometry of square plates. The tilting in two different directions could be explained by the weaving construction. Warp and weft are rectangularly oriented with different warp densities thus resulting in different tensions.\nThe three occurring resonances of the textile surfaces all contribute to airflow through the pores of the textile and thus resulting in visco-thermal losses or in other words leading to an increase in the absorption at the corresponding frequencies.\nFig. 4. a) FFT-spectrum and b) time signal of an impact on a textile surface with a local resonator\nThe introduction of textile resonators has proven to be successful. The acoustic characteristic of the textile resonators were analyzed by FFT-spectrum-analysis, observing Chladni sound figures and laservibrometry. By altering the textile parameters a change in the resonance frequencies can be achieved. Three different types of resonances occur, whereby these resonances broaden the bandwidth of the absorption capability that already can be achieved with textile resonators. The use of round resonators is a reproductive way of generating the resonators, as the deviation between the resonators of each probe is minimalized with a circular recess below the textile surface.\nA first trial of experiments in an impedance tube has shown that a rise of the absorption coefficient for frequencies below 180 Hz is achieved by using the textile resonator compared to a textile surface without added mass. Further trials about the maximization of the effect are planned with a second iteration of changes in the added mass. Changes in the textile structure, for example the use of knitted fabrics, are planned to vary the visco-thermal losses as well as the mechanical properties of the textile resonator. These will further determine the effects of the textile structure on the resonance frequency as well as on the absorption. In additional experiments a closed chamber behind each resonator, with the textile surface being a wall of said chamber, will be examined to directly compare the textile resonators with common membrane resonators. The enclosed air will act as a spring for the moving textile membrane and will be a tuning parameter for the absorbing structure.\nThis research would not have been possible without the support and contribution of Robert Mörl from the Polytec GmbH.\nThis work was supported by the Federal Ministry of Economics and Technology (BMWi) of the Federal Republic of Germany. Funding code: ZF4060056.\n- Morillas J., Gozalo G., González D., Moraga P. Noise pollution and urban planning. Current Pollution Reports, Vol. 4, 2018, p. 208-219. [Publisher]\n- Munich, Germany Population 1950-2020. Macrotrends, https://www.macrotrends.net/cities/204371/ munich/population. [Search CrossRef]\n- Daiber A., Herzog J., Münzel T., Schmidt F., Sørensen M., Steven S. Environmental noise and the cardiovascular system. Journal of the American College of Cardiology, Vol. 71, 2018, p. 688-697. [Publisher]\n- Bateni H., Blaschak M., Vaizasatya A. The effect of 80 dB environmental noise on control of posture in healthy young adults. Human Factors and Ergonomics in Manufacturing, Vol. 23, 2013, p. 213-221. [Publisher]\n- Goines L., Hagler L. Noise pollution: A modern plague. The Southern Medical Journal, Vol. 100, 2007, p. 287-294. [Publisher]\n- Stansfels S., Matheson M. Noise Pollution: non-auditory effects in health. British Medical Bulletin, Vol. 68, 2003, p. 243-257. [Publisher]\n- Babisch W. Exposure to environmental noise: risks for health and the environment. Workshop on Sound Level of Motor Vehicles, 2012. [Search CrossRef]\n- Warnock A. Factors affecting sound transmission loss, Canadian Building Digest, CBD - 239, 1985. [Search CrossRef]\n- Ebbitt G., Hansen M. Mass law – calculations and measurements. SAE Technical Paper 2007-01-2201, 2007, https://doi.org/10.4271/2007-01-2201. [Publisher]\n- Greene C., Argo Iv T., Wilson P. A Helmholtz resonator experiment for the Listen Up project. Proceedings of Meetings on Acoustics, Vol. 5, 2008, p. 025001. [Publisher]']	['<urn:uuid:15a8a975-016a-42db-b281-6888118ea3f8>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	9	21	2516
83	Are Svalbard and the Arctic Circle at similar latitudes?	"No, Svalbard lies further north than the Arctic Circle. The Arctic Circle is located at approximately 66.5 degrees (66° 33' 44"") north latitude, while Svalbard is situated even further north of the Arctic Circle, in an area where the summer sun never sets."	['On a voyage aboard the new National Geographic Endurance, discover the remote islands and fjords of northern Norway, including the high Arctic archipelago of Svalbard. Spend several remarkable days amid the ice floes of Svalbard and follow nature’s rhythms in search of walruses, seals, and polar bears. Continue to Bear Island to encounter towering cliffs populated with thousands of seabirds. Then head further south to the spectacular isles of Søroyø and Senja, where nature flourishes amid vibrant coastal cultures.\nThis trip is offered in partnership with Lindblad Expeditions.\nDepart on an overnight flight to Oslo and check in to our hotel upon arrival. In the afternoon, embark on a tour of Norway’s capital city. Stroll through Frogner Park among the famed Vigeland sculptures—hundreds of life-size human figures by native sculptor Gustav Vigeland. Then visit the Fram Museum, dedicated to the wooden ship sailed by Norwegian pole explorers such as Nansen and Amundsen.\nDepart Oslo on a private charter flight and enjoy breathtaking vistas en route to Longyearbyen, the largest settlement in Svalbard. On arrival, visit the Svalbard Museum to learn about the history of Svalbard and the Arctic, from 17th-century whaling to modern scientific research. Explore the Svalbard Art Gallery, home to a permanent collection of historic items as well as contemporary exhibitions of local artists. Later, the embark the National Geographic Endurance, your floating base camp for the next six days.\nNational Geographic Endurance through Day 9\nSvalbard lies north of the Arctic Circle, where the summer sun never sets. Here, in true expedition style, we let nature guide our course. Our flexible itinerary and years of experience in the archipelago allow us to take the best advantage of ice and weather conditions. With our fully stabilized, ice-class ship, we probe deep into the ice in search of wildlife. Join our spotters on the bridge, scanning the edges of the pack ice for huge walruses, arctic foxes, reindeer, polar bears, and other resident species. Venture ashore to hike the tundra, abloom with wildflowers. Sail deep into beautiful fjords and kayak quiet coves surrounded by sparkling icebergs. Board Zodiacs to view tidewater glaciers and spectacular ice formations.\nWith our National Geographic photographer, experts, and seasoned naturalists, we make the most of our days here. Enjoy nightly recaps and talks about the natural wonders we encounter, and view underwater video footage taken some 1,000 feet below the surface by our remotely operated vehicle (ROV). On this extraordinary voyage, every day is different, and every day is an adventure.\nSail south to isolated Bear Island, the southernmost island of the Svalbard archipelago. Board a Zodiac to explore the coves and inlets of this mist-shrouded island. Gaze up at sheer cliffs and use your binoculars to take in the spectacular sight of tens of thousands of fulmars, kittiwakes, and murres.\nOff Norway’s northern coast lies the mountainous island of Sørøya, known for its scenic bays, pristine nature reserves, and long sandy beaches. It also is renowned for its large populations of skrei, or cod, which migrate to these waters during winter and spring. We plan to make an in-depth visit to this beautiful island, with opportunities to explore its surrounding fjords by Zodiac and kayak, and hike on shore.\nSenja, the second largest island in mainland Norway, is characterized by dramatic mountains, colorful fishing villages, and a coastline split by beautiful fjords. In the southern part of the island, you’ll find Ånderdalen National Park—more than 70 square miles of pristine wilderness that is home to reindeer, elk, red foxes, and more. Spend a day exploring the island and fjords, and, if you wish, join our naturalists on a spectacular hike.\nDisembark in Tromsø, known as the “gateway to the Arctic” for the numerous Arctic expeditions that originated here. Explore this charming coastal city, then board our charter flight to Oslo and check in to our hotel.\nAfter breakfast, transfer to the airport for your return flight.', 'The coordinates latitude and longitude are used to plot and find specific locations on the Earth. Anyone can use the printable World Map with Latitude and Longitude to determine the precise location of a place by utilizing its latitude and longitude. The World Map with Latitude and Longitude can also suggest a country’s time zone.\nThe equator is at 0°, and the latitude is in degrees. To the north of the equator, the Northern space reaches 90 degrees, while to the south, the Southern margin reaches 90 degrees. Longitude is an angle measured from the Greenwich Meridian that points west or east. If the Prime Meridian is the Greenwich Meridian, the maximum longitude is 180 degrees east of the Prime Meridian and 180 degrees west of the Prime Meridian. On the world map with latitude and longitude, the coordinates refer to angles measured in degrees: minutes of arc and seconds of arc. 60 minutes of arc = 1 degree 60 seconds of arc = 1 minute\nYou may correctly discover the exact position of any site on the world using the latitude and longitude world Map. You can also check out the following maps:\nTable of Contents\nWorld Map with Latitude and Longitude\nMap of World with Latitude and Longitude\nWorld Map with Latitude\nWhat is Latitude?\nWorld Map with Longitude\nWhat is longitude?\nThe location of the Equator\nTropic of Cancer\nTropic of Capricorn\nLocations of the Arctic and Antarctic Circles\nLocation of the Horse’s Latitudes\nHow to Figure Out Whether a Meridian Is East or West\nDistance between lines of longitude?\nThe Prime Meridian and what it means\nThe International Date Line and how it works\nA World map with latitude and longitude will help you locate and understand the imaginary lines forming across the globe. Longitudes are the vertical curved lines on both sides and curves facing the Prime Meridian; these lines intersect at the north and south poles. Latitudes are the horizontal straight line around the globe on both sides of the equator.\nBy drawing latitudes and longitudes all over the globe, there is the formation of a grid which is helpful to find out the position of any location referring to the equator and prime meridian. For example, a point described as 50° N, 20° W, for example, is located 50° of arc north of the Equator and 20° of arc west of the Greenwich meridian.\nThe framework formed with the combination of latitudes and longitudes helps us to locate the position of any place geographically. The locations anywhere all around the globe, east, west, north, and south, can be easily found and described with the help of this formed grid.\nThe physical world map latitude and longitude, centered on Europe and Africa, depicts all continents, sovereign nations, dependencies, oceans, seas, oversized islands, island groups, countries with international borders, and their capital cities at 30° intervals. You can use the above map for educational or similar purposes.\nLatitudes and longitudes are helpful in earth geographical coordination and measure the distance of any particular location from the center of the earth. Degrees (°) and minutes (′) are the two scales of measurement used. The Equator lies at the 0° latitude; meanwhile, the prime meridian lies the 0°. From the prime meridian moving towards east or west, the location varies from 0° to 180°. We can also use Greenwich for 0° longitude, i.e., Prime Meridian. The map of the world with latitude and longitude will help you to know more about locations on the Earth.\nLatitudes help measure the North and south positions of any location on both sides of the equator. Geocentric, astronomical, and geographic are the three kinds of plays. The most significant possible margin is 90° north to 90° south as all spaces parallel the equator. These latitudes are from Tropical climate zones, south polar regions and north polar regions. The World Map with Latitude shows this.\nParallels, often known as lines of latitude, are imaginary lines that split the Earth. They run east to west, but you can use them to calculate your distance north or south. The most well-known parallel is the equator. It splits the Earth into the Northern and Southern hemispheres equally at 0 degrees latitude. As you travel north or south from the equator, latitude increases, reaching 90 degrees at each pole. Other named lines of latitude exist. They assist us in comprehending climate, weather, and ocean currents because they’re based on the sun’s location during Earth’s orbit. The Tropic of Cancer, located at around 23 degrees north, and the Tropic of Capricorn, located at roughly 23 degrees south, define the tropics. The Arctic and Antarctic Circles are at approximately 66 degrees north and south, respectively. They restrict the Arctic and Antarctic areas. Check out the world latitude and longitude map to see all these.\nOn the Earth’s surface, each degree of latitude covers around 111 kilometers. One degree of freedom is divided into 60 minutes and one minute into 60 seconds. Only roughly 30.7 meters is covered by a second of play. Latitude lines are parallel, unlike longitude lines, which get closer to one other at the poles. Latitude lines are the same distance apart no matter where you are on the planet.\nSince ancient times, latitude and longitude have been helpful in astronomy and navigation. Using simple equipment, navigators could calculate their approximate margin by calculating the angle between the horizon and a celestial object (typically the sun or the North Star). Because the calculations were straightforward, it took hundreds of years to determine accurate longitude readings at sea. The observer was in 60 degrees latitude if the North Star was 60 degrees above the horizon (north). In the southern hemisphere, where the North Star is not visible, the technique was more complicated.\nWe still use the sky to establish our location, but the technology is more advanced. Over 30 global positioning satellites orbit the Earth, delivering signals to land-based receivers. Over 30 global positioning satellites orbit the Earth, providing calls to land-based receivers. The National Geodetic Survey of the NOS oversees a network of Continuously Operating Reference Stations, which are stationary global positioning satellite receivers (CORS). Processed CORS data, when paired with additional positioning data from the National Spatial Reference System, can offer latitude, longitude, and height positions that are accurate to a few millimeters.\nLongitude is very important in measuring and determining positions on the world Map. The measurement of the Prime meridian east or west is one with the help of longitude at the Greenwich. These imaginary lines pass from one point at both poles and a middle line (prime meridian) from the Greenwich of London, Uk. On both sides of the prime meridian, east and west are measured up to 180°. These lines intersect the equator at the right angle (90°) and farthest at the equator and the poles. The World Map with Longitude shows this.\nThe measurement east or west of the prime meridian is known as longitude. Longitude is determined by imaginary lines that go vertically (up and down) around the Earth and meet at the North and South Poles. Meridians are the names given to these lines. Each meridian measures one arc degree of longitude. The circumference of the Earth is 360 degrees. The sequence of 0 degrees longitude, or prime meridian, travels through Greenwich, England, is universally recognized. At 180 degrees, the antemeridian is halfway around the planet. The International Date Line is based on it.\nThe Eastern Hemisphere, or half of the globe, is measured in degrees east of the prime meridian. The Western Hemisphere, on the other hand, is located degrees west of the prime meridian. Sixty minutes are divided into degrees of longitude. Longitude can be divided into 60 seconds for each minute. The longitude of Paris, France, for example, is 2° 29′ E. (2 degrees, 29 minutes east). Brasilia, Brazil’s longitude is 47° 55′ W. (47 degrees, 55 minutes west). At its broadest point, a degree of longitude is around 111 kilometers (69 miles). Near the Equator, where the Earth bulges out, the most extended stretches of longitude are the most extensive. The actual distance of degrees, minutes, and seconds of longitude depends on the distance from the Equator due to the Earth’s curvature. The shorter the gap between meridians, the larger the space. At the North and South Poles, all meridians meet.\nThe measurement of distance north or south of the Equator is related to longitude. Parallels are latitude lines that run parallel to each other. Parallels and meridians are frequently used to create a grid on maps. A coordinate is a location on the grid where parallels and meridians intersect. We can use Coordinates to find any place on the planet.\nFor military, engineering, and rescue operations, knowing the exact coordinates of a site (degrees, minutes, and seconds of longitude and latitude) is essential. Military authorities can use coordinates to determine where weapons or opposing forces are. Engineers use coordinates to determine the optimal location for a building, bridge, well, or other structure. Coordinates assist pilots in landing flights or delivering humanitarian shipments to precise spots.\nCheck the world map with latitude and longitude to see where the equator splits the planet into the Northern and Southern Hemispheres, where 0 degrees latitude is found. The angular distance north or south of the equator is represented by space. The equator denotes the point on the globe equidistant from both the North and South Poles. The Equator is the only latitude line with a large circle; all other latitude lines are tiny circles. The equator is around 24,901 miles (40,075 km) long and crosses 78.7% sea and 21.3 percent land.\nIn the World map with latitude and longitude, you will be able to see the tropic of cancer. At this latitude, the Tropic of Cancer denotes where the sun reaches its zenith. The precise play is not known. However, the most recent measurement for 2014 was 23° 26′ 14.675′′ (23° 26′ 16′′). The summer solstice, which occurs on June 20 or 21, is the day when the sun shines vertically above this parallel.\nThe Tropic of Capricorn is a parallel line of latitude on the world map with latitude and longitude that moves every year and is currently placed at 23° 26′ 14.440′′. The winter solstice, which falls on December 21 or 22, each year, is the day when the sun shines vertically across this line.\nIn the latitude and longitude world map, the Arctic and Antarctic Circles are around 66.5 degrees (66° 33′ 44′′ (or 66.5622°) latitude parallels. The Arctic refers to the area above the Arctic Circle, including the North Pole. The Antarctic is the territory south of the Antarctic Circle, consisting of the South Pole.\nAround 30 degrees north and south of the equator are the Horse Latitudes. The horse latitude zones in the subtropics where the prevailing winds divide and flow towards the poles (known as westerlies) or the Equator (known as equatorial winds) (known as trade winds).\nWhile we use parallels to describe lines of latitude, meridians are used to represent sequences of longitude.\nDistances west of the Prime Meridian are denoted by a – (negative values), whereas lengths east of the Prime Meridian are represented by a + (positive numbers) (-180 degrees of longitude west and 180 degrees of longitude east).\nAs you get closer to the equator, the distance between longitudes narrows. The distance between each line of longitude gets smaller as you get closer to the poles until they converge at the North and South Poles. At the equator, the distance between longitudes is the same as between latitudes, around 69 miles. The distance between them at 45 degrees north or south is about 49 miles (79 km). As the meridian lines converge at the poles, the distance between longitudes reaches zero.\nThe Prime Meridian is the line of longitude where the degree is zero. It is also known as the Greenwich Meridian. It passes through the Royal Observatory at Greenwich, England, and divides the Earth into two halves, the Eastern Hemisphere and the Western Hemisphere.\nThe International Date Line is a line on Earth that marks the transition from one calendar day to the next (IDL). The bar is usually found 180 degrees from the Prime Meridian. However, it skirts around some regions and islands to prevent splitting contiguous regions and countries into two days. The world is divided into several time zones by 23 one-hour slices and two 30 minute slices. I traveled over the International Date Line from east to west, advancing the calendar by one day.']	['<urn:uuid:5237ebc6-25f6-4cb9-b467-f9ae906488d7>', '<urn:uuid:f14a88ce-eb0b-4d4c-92df-3dc8df684646>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T15:30:12.136021	9	43	2756
84	Could you tell me the key change that led to scoring twice as many goals in professional hockey around 1929?	Forward passing was permitted in the 1929-30 season, which doubled scoring. The Boston Bruins increased from 89 goals to 179 goals to lead the league, and Cooney Weiland won the scoring title with 43 goals and 73 points in 44 games, compared to the previous season's leader Ace Bailey who had just 22 goals and 32 points in 44 contests.	"['The NHL is holding a research and development camp Wednesday and Thursday in Toronto featuring a number of top 2011 draft prospects who are being put through their paces by coaches Ken Hitchcock and Dave King.\nThose paces include a number of tinkerings with things like overtime formats, line changes and faceoff configurations. While most are likely to fall by the wayside, it’s interesting – and relieving – that the NHL is taking a proactive approach with tweaking things rather than its usual reactive one.\nAfter looking over the rules experiments, we got to thinking about the game as it was played in eras past and how different it was in form and function. We waxed poetically about $50 fines for match penalties, eight-man teams and standing-still penalty shots 38 feet from the goal a la football (that’s soccer for you colonials).\nOh how the game has changed. And while some rules have come and gone over the years, others remain staples of hockey today, defining the game from end to end.\nHere are THN.com’s Top 10 rules that changed the game.\n10. Red line introduced, 1943\nPrior to this there were just two bluelines and a lot of offside calls. The red line reduced those and sped the game up. The NHL considers the introduction of the red line the beginning of the league’s modern era.\n9. Video replay, 1991\nToday it seems every other goal goes to replay, but it wasn’t always so. Video took the guessing out of game-changing events and made sure a goal is really a goal. Just don’t tell that to Buffalo Sabres fans…\n8. Mandatory helmets, 1979\nAlthough many were already wearing helmets, the days of wind-blown manes began to wane when it was deemed that all players entering the NHL must don head protection. The last player to go helmetless was Craig MacTavish. He retired in 1997.\n7. Blade regulations, 1969\nThere was a time when NHLers played with their blades like kids who play in the cul-de-sac. Banana blades were all the rage in the ’60s thanks to Bobby Hull and his Black Hawks teammates; pucks flew threw the air like supersonic knuckleballs and literally scared goalies into wearing masks. But in 1969 blade curves were restricted to 1-inch maximums, a year later they were further reduced to a half-inch.\n6. Standardized periods, 1927\nAlthough three 20-minute periods had been around since 1910, stop-time and intermissions were introduced in 1927, making each contest considerably longer (especially since squads could dress a max of 12 skaters at the time). Teams were made to change ends after each period and sudden-death overtime was introduced.\n5. Icing, 1937\nDuring the ’30s NHL leaders were in tough to average a point per game and if teams scored 2.5 goals per contest they led the league. Icing was introduced to do exactly what it does today, keep panicky teams from simply firing the puck down the ice when on the defensive. The rule remains today just as it was 73 years ago.\n4. One goal on minor penalties, 1956\nThere was a time when a minor penalty meant two minutes of shame no matter how many times the opposition scored. That all changed in ’56 thanks to the overwhelming power play of the Montreal Canadiens. With names like Beliveau, Richard(s), Geoffrion and Harvey, the Habs ran a clinical attack that at times was just plain unfair.\n3. Stand-up goalie eliminated, 1917\nHere’s one for you: What would goaltending look like today if the butterfly style was outlawed? Well, there are many stories from the NHL’s early days of ""goalers"" pretending to trip or fall just to go to the ice and have the puck hit them. They’d have to put on a good show, though, netminders were penalized for leaving their feet.\n2. Offside rules codified, 1930\nIn the wake of a huge up-tick in scoring, the NHL realized it had to do something to stanch the flow of players into the offensive zone. Offside, although not a new rule, was refined to basically what it is today. Players were not allowed to enter the offensive zone before the puck and play was blown dead when the rule was broken.\n1. Forward passing permitted, 1929\nTo promote offense, the NHL had experimented earlier in the decade by allowing forward passing in the defensive and neutral zones, but not until the 1929-30 season was it allowed anywhere else on the ice. The result? Scoring more than doubled. Boston became the first team in league history to score 100 goals, but all other teams followed suit that season. The Bruins potted 179 goals to lead the league after managing just 89 the season prior, which also led all teams. Cooney Weiland won the scoring title with 43 goals and 73 points in 44 games. The leader in ’28-29, Ace Bailey, had just 22 goals and 32 points in 44 contests. Forward passing birthed the modern game.\nThe THN.com Top 10 appears Wednesdays only on TheHockeyNews.com.\nIf you enjoy this feature, subscribe to The Hockey News magazine for the exclusive countdown, Starting From 5, and other great features from the world of hockey.']"	['<urn:uuid:4592bb9c-6ee6-44d1-9d69-6422863e6f08>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T15:30:12.136021	20	60	864
85	How do the roles and responsibilities differ between machine manufacturers and third-party safety assessors, and what are the essential guarding requirements they need to jointly ensure?	Machine manufacturers are responsible for conformance through the Machinery Directive requirements, whether they are OEMs or end users integrating machinery. They may use third parties, which must be technically and regulatory competent, to assist at various stages. These third parties can be either notified bodies, which are limited to certification and compliance monitoring, or inspection bodies, which can provide complete solutions throughout the CE marking process. Regarding guarding requirements, all parties must ensure that guards prevent contact with dangerous parts, are securely attached, protect from falling objects, and allow safe lubrication without removal. The guarding must prevent reaching over, under, around or through, while not creating new hazards or interfering with work efficiency. Additionally, assessment and implementation must follow the OSHA/ANSI hierarchy of hazard control: first attempting to eliminate hazards by design, then controlling through guarding, and finally utilizing personal protective equipment.	['David Collier, business development manager at Pilz Automation Technology, claims that machine manufacturers and their end users should consider the benefits of working with an inspection body rather than a notified body to ensure safety conformance\nThe single European market depends on the setting of common and high standards for safety of specific products, including machinery. Directives and standards supporting this free market set guidelines, which include the ‘Essential Health and Safety Requirements’ found in the Machinery Directive 2006/42/EC, and ‘Inspection of Product’ in the standard EN ISO 17020.\nNew machinery introduced to the market will have CE marking applied through self assessment. However, inspection by an accredited body reduces the risk for businesses and their customers.\nThe conformance of machinery is the responsibility of the manufacturer through the requirements defined in the Machinery Directive. A manufacturer could be an OEM introducing a machine to the market, or an end user integrating interlinked machinery in order to create an assembly of machines. The manufacturer may use third parties to assist at various or all stages of the safety lifecycle.\nThird parties carrying out such work must be competent at the technical and regulatory level. The use of an accredited organisation is a means for clients to manage the associated risks. Such accredited organisations include inspection bodies and notified bodies.\nNotified body v inspection body\nA notified body is authorised by any EU member state to conduct the certification procedures or to perform compliance monitoring on products that are subject to EU directives, or where the conformance procedures for such directives designate a role for a notified body for EC type examination. In relation to machinery, the notified body process applies primarily to Annex IV (particularly dangerous) machines.\nNotified bodies point to their worldwide presence, accreditation, test laboratory capabilities and independence from any solution or product - yet herein lies a gap in satisfying real market demand. According to the Machinery Directive 2006/42/EC, a notified body is required ‘to maintain it’s impartiality and independence from all applicants and in no circumstances should it take on the role of authorised representative’.\nThis means that a notified body is prohibited from offering complete solutions throughout the CE marking process - it can indicate where a machine fails to conform to essential health and safety requirements (EHSRs), but it can not propose a remedy or solution. In today’s de-skilled and pressurised work environment, manufacturers are looking for remedies and solutions, not just tick box pass/fail reports.\nFor this reason Pilz Automation Technology has elected and achieved two accreditations as an alternative to a notified body. Firstly, Pilz has achieved DAkkS accreditation as a ‘Type C’ inspection body, which proves international impartiality and objectivity (with Quality Management Systems to IEC 17020) and permits the continuance of lifecycle service beyond any inspection and test phase to safety concept. Secondly, Pilz has TÜV accreditation to IEC 61508 for the integration of functional safety, which is key to the subsequent stages of safety design, system implementation, validation and CE marking, and provides confidence in the engineered solution.\nA change of directive\nA key change in the new Machinery Directive is the liberalisation of the conformity assessment procedures for machinery listed in Annex IV (particularly dangerous) machinery. Previously, Annex IV machines needed EC-type examination by a notified body.\nHowever, since the end of 2009, if machinery is designed in accordance with the relevant harmonised standards (generally this means machine specific ‘C’ type standards, such as those covering presses, packaging machines, woodworking machines etc), the new directive gives manufacturers the option to self certify machinery, thereby avoiding the previous directive’s requirement to deposit the technical file with a notified body. Nearly all Annex IV machines have an applicable harmonised ‘C’ standard, which means that manufacturers of these types of machines can self certify or mandate an ‘authorised representative’ to carry out the conformity assessment, usually a more cost effective route to CE certification without the need for EC type examination provided by a notified body.\nThe Machinery Directive anticipates that machine builders may need to appoint competent persons to carry out some or all of the administrative and assessment procedures required to CE mark a machine. The current Machinery Directive defines an authorised representative as ‘any natural or legal person established in the community who has received a written mandate from the manufacturer to perform on his behalf all or part of the obligations and formalities connected with this directive’.\nThe role of an authorised representative is described under the new Machinery Directive EC 2006/42 - Annex II A. The EC declaration of conformity produced for each machine must contain the following particulars:\n1. Business name and full address of the manufacturer and, where appropriate, its authorised representative.\n2. Name and address of the person authorised to compile the technical file, who must be established in the community (based/resident in Europe).\nWhen mandated by its customer, Pilz can act as an authorised representative, established in the community to perform on that company’s behalf all or part of the obligations and formalities connected with this directive.\nAs an authorised representative Pilz can be the point of contact for the statutory authorities (The HSE in the UK) enforcing the directive. Pilz can perform quality assurance on the conformance assessment procedures followed and the Technical Construction File (TCF) prepared by the machine builder. To assure CE marking services are provided in accordance with the defined requirements, Pilz can also prepare and co-sign the Declaration of Conformance.\nPilz will retain the TCF for a period of at least ten years from the last date of manufacture of the machinery, keep the original EC declaration of conformity or incorporation, make the technical files available to the competent authorities, and the Pilz name and address would appear on the CE marking plate as the authorised representative.\nInspection body advantages\nThere are advantages for a manufacturer that uses an inspection body as an authorised representative rather than a notified body:\n• The inspection body is not prohibited from providing comprehensive support throughout the machine safety lifecycle, so it can help the manufacturer identify non-conformity, propose solutions, engineer these solutions, validate them, certify and CE mark the machine.\n• It can significantly reduce administration and engineering costs as well as time to market.\n• Inspection bodies work continuously in the field of machine safety, and have experience across industry sectors (such as in paper, food and beverage, pharmaceutical, automotive, rail etc), so are well placed to offer practical guidance on automation which not only improves safety (for which there are moral and regulatory requirements) but also productivity (which is important financially and competitively).', 'SAFETY SOLUTIONS: Machine Guarding\nThe primary function of all machine guarding is to\nprevent an operator or bystander from being\ninjured by a potentially dangerous portion of a\nmachine. Often the hazard involves moving parts with the\npotential to cut, crush or draw-in body parts, although\nbarrier guards may also be necessary when electrical,\nthermal or chemical dangers are present.\nGuarding or protecting people from harm may be accomplished\nwith simple physical shielding of machine parts, or\nmore advanced techniques such as photo light curtains. It\nis also often necessary to employ interlocks to disable\npower to the machine when a guard has been removed.\nAccident risks can be reduced with adequate machine\nsafeguarding. Identifying obvious and hidden hazards\nshould be the first step in planning and reviewing the need\nfor machine tool safeguarding. Some hazards are subject\nto the ‘seven-foot rule, which dictates that these potentially\ndangerous operations be enclosed or guarded if they\nare located less than seven feet above the floor or platform\nlevel. Guards are required to have one-half-inch or\nsmaller openings. Blades and other overhead hazards—\nsuch as pulleys, belt rope and chain drives, overhead horizontal\nbelts, vertical and incline belts, gears, sprockets\nand chains—must comply with this rule.\nMost incidents leading to injury are the result of inadvertent\nor unwise contact with moving machine parts. Because\nof the great diversity of machine designs and functions,\nappropriate safeguarding to protect workers from such hazards\nmay also have numerous forms. Certain principles,\nhowever, are basic to any effective safeguarding design.\nEvaluation and Design\nA uniform process should be applied and used to evaluate\neach of the hazards on the machine to develop the required\nlevel of safeguarding. The evaluation can be performed by a\nknowledgeable and experienced person or, for more complicated\nmachine designs and safeguarding issues, the evaluation\ncan be conducted by a qualified third party.\nThe OSHA/ANSI hierarchy for controlling machine hazards\nis as follows:\nIf the results of the hazard evaluation show the equipment\nto be safe (that is, poses no hazard to the employee),\nchanges to the equipment may not be necessary. This\nmay be true for manually-powered equipment.\n- Eliminate the hazard by design\nControl the hazard by guarding or devices\n- Personal protective equipment\nAssessing the Risk: Many times the employer will want\nto know the answer to this question, “What do we fix\nfirst?” Once you have identified the danger areas (hazards),\nyou should assess the risk (how likely it is to cause\ninjury, and how severe the injury could be.)\nFor every hazard that you identify on a machine, you\nmust then assign a risk factor to it. On the Probability\nchart listed below, decide what is the most predicable\ninjury that could occur, then assign a risk number to it.\nIf you score a 1 or 2, do something NOW.\nIf you score a 3 or 4, plan to do something soon.\nIf you score a 5 or 6, plan to review the risk in the future.\n| CONSEQUENCE |\nHow severely could it hurt someone?\n| What is the likelihood of it happening?\nFirst aid only,\n|VERY LIKELY: |\n|VERY UNLIKELY: |\nIt could happen,\nWhen designing machine guards the safeguards must\nmeet these minimum general requirements:\nIn summary, employees working on or near machinery\nwith hazardous moving parts must be protected. OSHA\nrequires that such parts be guarded to ensure worker\nsafety. If unguarded, those parts could entangle a worker’s\nhands, hair or clothing and lead to injury or even death.\nEmployers therefore should inspect equipment daily to\nmake sure that all guards are intact—especially after\nmaintenance or repair.\n- A guard is a protective device that PREVENTS anyone\nfrom reaching over, under, around or through the guard\nor guarding device.\n- An awareness barrier or chip shield can be used to\nwarn a person that they are coming in close proximity\nto a danger area. Other protective measures such as\ntraining or color coding must also be used so the person\nis reminded that they could potentially be injured if\nproper procedures are not followed.\n- Prevent contact: The safeguard must prevent hands,\narms, and any other part of the operator’s body from\nmaking contact with dangerous moving parts. A good\nsafeguarding system eliminates the possibility of the\noperator or another worker placing parts of their bodies\nnear hazardous moving parts.\n- Secure: Operators should not be able to easily remove\nor tamper with the safeguard, because a safeguard\nthat can easily be made ineffective is more dangerous\nthan no safeguard at all.\n- Protect from falling objects: The safeguard should\nensure that no objects can fall into moving parts. A\nsmall tool which is dropped into a cycling machine\ncould easily become a projectile that could strike and\nseriously injure someone.\nCreate no new hazards: A safeguard defeats its own\npurpose if it creates a hazard of its own, such as a shear\npoint, a jagged edge, or an unfinished surface which\ncan cause a laceration. The edges of guards should be\nrolled or bolted in such a way that they eliminate sharp\nCreate no interference: Any safeguard which impedes\na worker from performing the job quickly and comfortably\nmight soon be overridden or disregarded. Proper\nsafeguarding can actually enhance efficiency as it can\nrelieve the worker’s apprehensions about injury.\n- Allow safe lubrication: If possible, one should be able\nto lubricate the machine without removing the safeguards.\nLocating oil reservoirs outside the guard, with\na line leading to the lubrication point, will reduce the\nneed for the operator or maintenance worker to enter\nthe hazardous area. Guards and safety devices should\nbe made of durable material that will withstand the\nconditions of normal use.\nOSHA 1910.212 requires guards to be attached to the\nmachine where possible or secured elsewhere if attachment\nis not possible. The guard or guarding device\ncan not create a hazard in itself.\nFor more information, click on the author biography at the top of this page.\nThis article was written by Jack and Alan Podojil.\n|Back To Top|']	['<urn:uuid:15a10bb2-f5da-4aca-a312-71f5085a928b>', '<urn:uuid:33cb5a5d-857b-4891-94cb-a8cf981285d7>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	26	142	2115
86	signs of elder mistreatment physical emotional protection	Warning signs of elder abuse include physical indicators like bruises, broken bones, and burns; emotional signs such as sudden withdrawal from activities, unusual depression, and anxiety; and neglect indicators including poor hygiene, bedsores, and unexplained weight loss. Other red flags are unauthorized financial activity and strained relationships between elderly person and caregiver. These signs require immediate reporting to Adult Protective Services or law enforcement for investigation.	['What are the elder abuse laws in SC?\nThere are few things more terrifying than suspecting that a caregiver is abusing your elderly mother, father, grandparent, or other loved one in a care facility or even in their home. When a caregiver who is entrusted with protecting a vulnerable adult instead takes advantage of or hurts them, the caregiver and their employer may be liable criminally and civilly.\nIn this article, we will discuss elder abuse laws in SC, including:\n- The SC Omnibus Adult Protection Act,\n- Criminal charges and civil lawsuits for elder abuse,\n- Types of elder abuse,\n- Warning signs of elder abuse, and\n- What to do if you suspect elder abuse.\nElder Abuse Laws in SC\nSC’s “Omnibus Adult Protection Act,” found at SC Code § 43-35-5, is intended to protect vulnerable adults in our state. A “vulnerable adult” could include any adult who has a physical or mental condition that limits their ability to care for themselves, including a person of advanced age or a resident of a care facility.\nSC law on elder abuse provides criminal penalties under some circumstances, but the caregiver or facility is also liable for damages for negligence or intentional abuse in the civil courts.\nBecause abusers often are not charged criminally, filing suit against the responsible parties and making them pay the maximum compensation allowable under SC law and the facts of your case is the primary way that we can hold abusers responsible for their conduct and prevent future cases of elder abuse.\nCriminal Charges & Civil Actions for Elder Abuse in SC\nSC Code Section 43-35-85 makes it a felony criminal offense to knowingly and willfully abuse, neglect, or exploit a vulnerable adult.\nIn civil court, we must prove either negligence or intentional conduct that resulted in harm to the elderly person. To be charged in criminal court, the state must prove intentional or willful conduct, which is a different standard of proof.\nAlthough a criminal conviction may be indisputable evidence of liability in your civil case, the abuser doesn’t need to be charged with a crime before we file suit or take action to protect your loved one.\nThe potential penalties for the criminal offense of elder abuse or neglect include:\n- Up to five years in prison for knowingly and willfully abusing, neglecting, or exploiting a vulnerable adult,\n- Up to 15 years in prison for knowingly and willfully abusing or neglecting a vulnerable adult when it results in great bodily injury, and\n- Up to 30 years in prison for knowingly and willfully abusing or neglecting a vulnerable adult when it results in death.\nTypes of Elder Abuse Under SC Law\nThere are many ways that a caregiver, neighbor, family member, or another person can hurt a vulnerable adult.\nBecause not all abuse results in visible injuries, the victim does not always report the abuse, and the abuser may be working to cover up the abuse, many cases of elder abuse go unreported, and the abuse continues undetected.\nThe types of elder abuse that family members should be on the lookout for include:\n- Physical abuse: including slapping, hitting, pushing, or any physical actions that result in bodily harm to the elderly person,\n- Psychological or emotional abuse: threatening, yelling, belittling, humiliating, or isolating the vulnerable adult,\n- Sexual abuse: any unwanted sexual contact with an elderly person or any sexual contact with an elderly person who is unable to consent due to their mental state or disability,\n- Neglect: failure to provide for the vulnerable adult’s personal needs, including bathing, hygiene, needed supervision, or access to medications, and\n- Financial exploitation: forging checks, use of credit cards or bank cards without authorization, misuse of a power of attorney, or taking a vulnerable adult’s funds or resources through trickery or coercion.\nSigns of Elder Abuse\nHow do you know if your loved one is being abused or neglected?\nBecause the abuser will most likely take actions to conceal their misconduct and medical issues like dementia or Alzheimer’s may call into question your loved one’s reports, it is necessary to investigate before filing a lawsuit or charging a caregiver with a crime.\nThere are many warning signs, however, that may justify taking further action. Some examples include:\n- Poor hygiene: Unwashed hair, dirty clothes, body odor, and other signs that a vulnerable adult’s personal hygiene needs are not being looked after is a red flag that may indicate neglect,\n- Bruises, broken bones, or other unexplained injuries: Unexplained falls that result in broken bones, scratches on the arms or face, or other unexplained injuries could indicate physical abuse or defensive wounds,\n- Bedsores: Bedsores could be an indication that caregivers are failing to turn a patient, assist them to a chair, or walk with them as needed, and this is another form of neglect,\n- Unexplained anxiety, fear, or emotional response: If your loved one suddenly begins to panic when you are leaving them, it could be a red flag that they are afraid of someone or something at the facility,\n- Unexplained financial activity: Unauthorized ATM withdrawals, unexplained credit card purchases, or a lack of funds to pay bills could be a red flag indicating financial exploitation.\nWhat to do if You Suspect Elder Abuse\nIf you suspect elder abuse, you should ask questions and investigate further. If you are a mandated reporter, you must report the abuse if you “have reason to believe” that the abuse has occurred or is occurring.\nIf you are not a mandated reporter, you must still report elder abuse if you have “actual knowledge that a vulnerable adult has been abused, neglected, or exploited.”\nMandated reporters listed in SC Code Section 43-35-25 include a “physician, nurse, dentist, optometrist, medical examiner, coroner, other medical, mental health or allied health professional, Christian Science practitioner, religious healer, school teacher, counselor, psychologist, mental health or intellectual disability specialist, social or public assistance worker, caregiver, staff or volunteer of an adult day care center or of a facility, or law enforcement officer.”\nFailing to report elder abuse is a misdemeanor criminal offense that is punishable by up to a year in prison.\nQuestions About Elder Abuse Laws in SC?\nIf you suspect that your loved one is being abused at a nursing home, elder care facility, or in their home, your SC elder abuse lawyer can help you to investigate, take steps to protect your loved one, and file a lawsuit for compensation on their behalf.\nCall the Law Office of Nicholas G. Callas, P.A. at 803-369-3968 or contact us through our website for a free consultation.\nReady To Speak With An Attorney?\nLet’s discuss the details of your case and see if we can help.', 'Frequently Asked QuestionsServing Northern California and The Bay Area\nWe prepared a list of questions and answers to the most frequently asked questions about elder neglect and abuse.\nIf you think you or a loved one is suffering from one or more of these types of abuse, doing something about it as soon as possible is important. There is no time to waste.\nCall (877) 270-4700 or our closest local office for a free consultation.\nWhat is Elder Abuse and What is Elder Neglect?\nElder abuse is a term that refers to any knowing, intentional, or negligent act by a caregiver or any other person that causes harm, or a serious risk of harm, to a vulnerable adult. In California, anyone age 65 or older is protected by a special set of laws called The Elder Adult and Dependent Adult Civil Protection Act (“The Elder Abuse Act”), which is a powerful set of laws designed to help prevent neglect and abuse to California seniors. (Welf & I C 15600 et seq.) Under The Elder Abuse Act, “abuse” is defined broadly and includes any physical abuse, neglect, financial abuse, abandonment, isolation, abduction, or other treatment resulting in physical harm or pain or mental suffering. Therefore, any form of neglect to anyone age 65 or older is a form of elder abuse in California!\nThe specific laws vary from state to state, but broadly defined, abuse to an older person or senior citizen may be:\n- Physical Abuse – Inflicting, or threatening to inflict, physical pain or injury on a vulnerable elder, or depriving them of a basic need.\n- Emotional Abuse – Inflicting mental pain, anguish, or distress on an elder person through verbal or nonverbal acts.\n- Sexual Abuse – Non-consensual sexual contact of any kind.\n- Exploitation – Illegal taking, misuse, or concealment of funds, property, or assets of a vulnerable elder.\n- Neglect – Refusal or failure by those responsible to provide food, shelter, health care or protection for a vulnerable elder.\n- Abandonment – The desertion of a vulnerable elder by anyone who has assumed the responsibility for care or custody of that person.\nElder abuse and neglect of seniors can affect people of all ethnic backgrounds and social status and can affect both men and women. See What Is Elder Neglect for more information on the types of elder abuse, which you now know includes elder neglect.\nWhat are the Warning Signs of Elder Abuse or Senior Neglect?\nWhile one sign does not necessarily indicate abuse, some tell-tale signs that there could be a problem are:\n- Bruises, pressure marks, broken bones, abrasions, and burns may be an indication of physical abuse, neglect, or mistreatment.\n- Unexplained withdrawal from normal activities, a sudden change in alertness, and unusual depression may be indicators of emotional abuse.\n- Bruises around the breasts or genital area can occur from sexual abuse.\n- Sudden changes in financial situations may be the result of exploitation.\n- Bedsores, unattended medical needs, poor hygiene, and unusual weight loss are indicators of possible neglect.\n- Behavior such as belittling, threats and other uses of power and control by spouses are indicators of verbal or emotional abuse.\n- Strained or tense relationships, frequent arguments between the caregiver and elderly person are also signs. For more information on how caregivers may prevent elder abuse, please review How to Choose a Nursing Home.\nMost important is to be alert. The suffering is often in silence. If you notice changes in personality or behavior, you should start to question what is going on.\nWhat is Self-Neglect and What are the Signs?\nTragically, sometimes elders neglect their own care, which can lead to illness or injury. Self-neglect can include behaviors such as:\n- Failure to take essential medications or refusal to seek medical treatment for serious illness\n- Leaving a burning stove unattended\n- Poor hygiene\n- Not wearing suitable clothing for the weather\n- Inability to attend to housekeeping\nSelf-neglect accounts for the majority of cases reported to adult protective services. The problem is typically related to the self-neglecting elder’s declining health, isolation, Alzheimer’s disease or dementia, or drug and alcohol dependency.\nIn some of these cases, elders will be connected to support systems within their communities that can allow them to continue living on their own. Moreover, some conditions, such as depression and malnutrition, may be successfully treated through medical intervention. If the problems are severe enough, a guardian may be appointed.\nWhat Factors Make Older Persons More Vulnerable to Abuse?\nSocial isolation and mental impairment (e.g., dementia or Alzheimer’s disease) are two factors that may make an older person or senior citizen more vulnerable to abuse. However, in some situations, studies demonstrate that living with someone else (for example, a caregiver or a friend) may actually increase the likelihood for abuse to occur. Not surprisingly, a history of domestic violence may also make a senior more susceptible to abuse. Please review our webpage entitled “What Is Elder Neglect?” for more information about risks for abuse and neglect.\nWho are the Abusers of Older People?\nAbusers of older adults are both women and men. Family members are often abusers. Unfortunately, the abusers of our elderly loved ones are sometimes the very people who we entrust with caring for them and protecting them. They may be the untrained, unskilled, over-worked, and underpaid caregiver. Remember, one of the main objectives or goals of most nursing homes and long-term care facilities is to generate profits for the company or corporation! One obvious way to increase profits is to reduce the staff, including caregivers; or replacing skilled nurses with less expensive unskilled or untrained staff. It’s most unfortunate, but it’s the reality. The Nursing Home & Elder Abuse Law Center believes that most caregivers are compassionate and do a good job of caring for their patients. The work is not easy and it is typically unending. However, as in many areas of life, there are some nursing homes and some caregivers who are neglectful; and a few are intentionally abusive. The other unfortunate reality is that our elderly loved ones, given their age and/or physical conditions, simply do have possess the health and strength to withstand such neglect and abuse. Thus, the consequences of neglect, even when there was no ill intent by the nursing home, can result in severe consequences.\nAre there Criminal or Civil Penalties for Abusers?\n- Criminal Penalties – Although state laws vary across the United States, in most states there are several laws that address criminal penalties for various types of elder abuse. Some states have increased penalties for those who victimize older adults. Increasingly, across the country, law enforcement officers and prosecutors are trained on elder abuse and ways to use criminal and civil laws to bring abusers to justice. This is excellent news for elder rights activists. Abuses to the most vulnerable persons in our society, namely children and the elderly, must be reported and the perpetrators prosecuted.\n- Civil Restitution – If your loved one has been the victim of elder abuse or neglect and has sustained serious injuries, they may be entitled to monetary compensation and restitution for all losses, including payment of their medical bills, future treatment and care, pain and suffering, etc. So remember, if a nursing home neglects a resident, and such neglect causes a serious injury, then the nursing home or long term care facility may be liable for elder abuse. If you are unsure, contact the appropriate state agency or contact an experienced elder law attorney who specializes in elder abuse and elder neglect cases.\nHow Many People are Suffering from Elder Abuse in the U.S.?\nIt is difficult to say how many older Americans are abused, neglected, or exploited, in large part because surveillance is limited and the problem remains greatly hidden and vastly under-reported. Approximately 1.6 million people live in some 17,000 licensed nursing homes on any given day throughout the United States, and over a million persons live in approximately 45,000 plus residential care facilities, variously known as personal care homes, adult congregate living facilities, domiciliary care homes, homes for the aged, and assisted living facilities. (See Elder Abuse in Residential Long-Term Care Facilities: What is known about Prevalence, Causes, and Prevention, June 18, 2002.) While a number of studies have estimated that between three and five (5%) percent of the elderly population in the U.S. have been abused, the Senate Special Committee on Aging estimated that there may be as many as 5 million victims of elder abuse and/or neglect every year. (National Center on Elder Abuse: ) Incredibly, the available data indicates that only one out of every fourteen cases of abuse/neglect is ever reported to authorities. Id. And, while the elderly are already extremely vulnerable, issues of mental impairment and dementia are additional significant factors that make seniors even more susceptible to elder abuse and/or neglect. Id.\nOne consistent finding, over a ten-year study period, is that reports have increased each year. (Please refer to the NCEA Fact Sheets and the 2003 National Academy of Sciences’ Study Elder Mistreatment: Abuse, Neglect, and Exploitation in an Aging America for more information.)\nWho Should I Call if I Suspect Elder Abuse or Elder Neglect?\nEach one of us has a responsibility to keep vulnerable elders safe from harm. The laws in most states require helping professions in the front lines — such as doctors, nurses, and home health providers — to report suspected abuse or neglect. These professionals are called mandated reporters. Call the Police or 911 immediately if someone you know is in immediate, life-threatening danger.\nIf the danger is not immediate, but you suspect that abuse has occurred or is occurring, please tell someone. Relay your concerns to the local Adult Protective Services, Department of Health, a Long- Term Care Ombudsman, or the police. If your loved one resides in California, please refer to our If You Suspect Elder Abuse webpage for a list of reporting numbers.\nFinally, if you have been the victim of abuse, exploitation, or neglect, you are not alone. Many people care and can help. Please tell your doctor, a friend, or a family member you trust, or call the Eldercare Locator help line at 1-800-677-1116, M-F from 6 a.m. to 5 p.m. PST (where trained operators should be able to refer you to a local agency that can help).\nWhat Should I Expect if I Call Someone for Help?\nWhen making the call, be ready to give the elder’s name, address, contact information, and details about why you are concerned.\nYou may be asked a series of questions to gain more insight into the nature of the situation.\n- Are there any known medical problems (including confusion or memory loss)?\n- What kinds of family or social supports are there?\n- Have you seen or heard incidents of yelling, hitting, or other abusive behavior?\nYou will be asked for your name, address, telephone number, etc., but most states will take the report and investigate your concerns even if you do not identify yourself. And, in many cases you may remain anonymous.\nHow can Elder Abuse and Elder Neglect be Prevented?\nEducating seniors, professionals, caregivers, and the public on abuse is critical to prevention. Reporting suspected abuse is also extremely important. Moreover, filing lawsuits against the perpetrators of elder abuse can be one of the biggest deterrents to a neglectful or bad nursing home or long term care facility. On an individual level, some simple but vital steps to reduce the risk:\n- Take care of your health and visit your elderly loved ones often.\n- Seek professional help for drug, alcohol, and depression concerns, and urge family members to get help for these problems.\n- Attend support groups for spouses and learn about domestic violence services.\n- Plan for your own future. With a power of attorney or a living will, health care decisions can be addressed to avoid confusion and family problems, should you become incapacitated. Seek independent advice from someone you trust before signing any documents.\n- Stay active in the community and connected with friends and family. This will decrease social isolation, which has been connected to elder abuse.\n- Know your rights. If you engage the services of a paid or family caregiver, you have the right to voice your preferences and concerns. If you live in a Nursing Home, Board & Care home, or Assisted Living facility, call your local Long Term Care Ombudsman. The Ombudsman is your advocate and has the power to intervene. Please visit our If You Suspect Elder Abuse webpage to learn more.\nAll states have Adult Protective Services (in California: 800-236-9747) as well as Long-Term Care Ombudsman programs (800-510-2020 in California), family care supports, and home and community care services that can help older adults with activities of daily living. You may also try to Eldercare Locator at 800-677-1116 for elder abuse support services, information, and referrals in your area.\nWhat is Currently Being Done to Prevent Elder Abuse?\nAt the national level, the Elder Justice Act legislation has recently been proposed to provide federal leadership to help reduce elder abuse. The Elder Justice Coalition is working to help pass the Elder Justice Act S. 333.\nCommunity collaborations, meanwhile, are playing an increasingly important role in educating the public and professionals. In recent years, State Attorneys General offices and law enforcement have stepped up efforts to prosecute elder abuse. On the front lines, “multidisciplinary teams” (social workers, nurses, lawyers, etc.) have begun to be created to better target interventions.\nThe Nursing Home & Elder Abuse Law Center was formed by civil attorneys and trial lawyers who desired to do their part in helping to prevent elder abuse by filing civil lawsuits against abusive nursing homes and long-term care facilities. Our desire is to make life safer for vulnerable elders.\nWhat can I do to Stop Elder Abuse and Neglect of Seniors?\nIn a nutshell: know the warning signs of abuse, visit your loved one often and with open eyes, report suspected abuse, and get involved. Knowing the warning signs of abuse is a first step toward protecting elders. If your loved one is in a nursing home, skilled nursing home, assisted living facility, board and care, or other long-term care facility for the elderly, visit them often. And keep a watchful eye out for loved ones who may be vulnerable. Be aware that the elderly are more vulnerable when they are suffering from dementia or other limiting disabilities. And, of course, speak up and call someone if you have concerns. If you’re not sure if there has been abuse, call your local Ombudsmen. This means that even if you are not sure, ask questions. You have a right to question. Get involved. Volunteer with older adults in your community. Support initiatives to increase and strengthen Adult Protective Services in your state.\nServing Northern California and The Bay Area\nHave Questions about Elder Care Law?\nContact an experienced California attorney who specializes in representing victims and family members of those who have suffered abuse or neglect in a nursing home or assisted living facility.']	['<urn:uuid:269de100-6516-41c9-bd11-561d72f93755>', '<urn:uuid:16aebb63-b2cd-4de4-9083-b3934c54ba67>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	7	66	3652
87	Does salt water make steam explosions worse?	No evidence suggests salt makes things worse. Pure water versus salt water made no difference in experiments measuring peak pressures for fuel coolant interactions in lab settings.	['From: Lee, Richard\nTo: Hoxie, Chris; Uhle, Jennifer\nCc: Gibson, Kathy\nSent: Sun Mar 27 10:35:35 2011\nSubject: RE: Brian’s Q\nConsulted both Mike and Dana on the matter – Salt effects on molten core materials:\nFrom Mike Corradini:\nThis will not likely generate a more energetic effect. It will be a dilution to the melt composition and energy.\nThink of this like any solid that must be melted and mixes with the melt. So no effect there. So if it does stop the melt there with in-vessel retention (which I suspect it would), it would add to the melt volume as it remelts and mixes and dilutes the corium.\nI would then expect it to lower the solidus temperature of the mixture. How much, I do not know, but we can see the effect on energetics by altering the solidus. Remember that the if we assume triggering (which we always do), steam explosions are a thermodynamic phenomenon altered minimally by local heat transfer.\nFrom: Dana Powers\nCore debris will hit the salt and cause it to melt and vaporize.\nSome fraction of it will be incorporated into the core debris, but much will vaporize – removing heat from the core debris.\nVaporized salt will condense on upper internals of the BWR vessel.\nSalt in the core debris will cause some vaporization of materials – including some fission products – as chlorides, but I would not expect the effect to be especially significant.\nAgain vaporized chlorides will condense on upper internal surfaces.\nFrom: Hoxie, Chris\nSent: Saturday, March 26, 2011 9:41 PM\nTo: Uhle, Jennifer\nCc: Lee, Richard; Gibson, Kathy\nSubject: Brian’s Q\nIn regards to Brian’s question about how salt water may influence the dynamics of a fuel coolant interaction:\nHere are two references:\nOn page 396, states that pure water vs. salt water made no difference in an experiment designed to measure peak pressures for fuel coolant interactions in a lab setting.\nAlthough it might not be one-for-one, here is a reference to research that indicates the salt might actually dampen the steam explosion (or at least it does maybe when lava hits sea water…. )\nCaveats: This reference 2 is not nuclear oriented. Not specific to the Japan case. This is really complex and should be answered by an expert. Depends so much on the actual conditions in the Japan plants…\nAt least I did not find anything that says salt makes things worse!\nImpure coolants and interaction dynamics of phreatomagmatic eruptions\nJames D. L. White\nGeology Department, University of Otago P.O. Box 56, Dunedin 9015, New Zealand\nReceived 12 January 1996;\nrevised 18 June 1996;\naccepted 18 June 1996.\nAvailable online 26 February 1999.\nPhreatomagmatic eruptions resulting from interaction of magma with groundwater are common in many terrestrial settings, and their explosivity is widely accepted to result from fuelcoolant interaction (FCI) processes.\nRelatively little attention has been given to the precise nature of the volcanic settings in which phreatomagmatic FCI’s take place, but several lines of evidence indicate that they almost inevitably involve mixing of magma with impure, sediment-laden water.\nConsideration of the effects of these impure coolants on the fuel-coolant interaction process suggests that:\n(1) impure coolants enhance the ability of magma to mix with large volumes of coolant; and\n(2) maximum unit-volume explosivity of FCI’s is damped relative to interactions with pure water.\nIt is probably unrealistic to back calculate water-magma mass ratios for most, if not all, phreatomagmatic eruptions because:\n(1) effects of impure coolants on fragmentation efficiency and eruption explosivity are not yet known; and\n(2) aspects of the vent environments in which phreatomagmatism occurs may influence fragmentation processes, explosive efficiency, and resultant particle populations as or more strongly than water-magma mass ratios.\nTo estimate mass ratios for individual bursts, or for eruptions as a whole, one must distinguish particle populations resulting from many different processes in phreatomagmatic vents, including primary fragmentation, induced fragmentation, vent-wall collapse and pyroclast recycling.\nIncorporation of accidental blocks beyond the zone of phreatomagmatic interaction and ejection of unvaporized water further complicate efforts at reconstruction.\n[toggle_simple title=”Related articles” width=”600″]\n- March 18th, 2011 – SPF Issues Investigated – Little Cladding Left – Fuel maybe in form of a rubble bed (enformable.com)\n- March 19th, 2011 – Risk of further explosions in SPFs – Should SFP be removed to temporary storage? (enformable.com)\n- March 23rd, 2011 – Questions on Boric Acid Solubility in Seawater (enformable.com)\n- March 15th, 2011 – Don’t know if Japan added hardened vent but I’m sure they are well aware of it (enformable.com)\n- TEPCO’s latest findings of Cu lead to questions about status of RPV and location of melted fuel (enformable.com)']	['<urn:uuid:45e61ddb-f42a-4f7d-b096-4acb15a67e78>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	7	27	777
88	What role do genes play in disease diagnosis?	Genes play a crucial role in disease diagnosis through multiple mechanisms. The human genome, consisting of DNA sequences storing genetic information, can reveal disease-causing variations through different analysis methods. Linkage analysis has successfully identified genetic bases of Mendelian diseases like Huntington's disease and cystic fibrosis, while genome-wide association studies have found thousands of SNPs associated with complex diseases like diabetes and hypertension. The Human Genome Project's complete sequencing efforts, combined with next-generation sequencing technologies, are improving disease diagnosis and treatment by helping identify both rare and common genetic variants underlying various conditions.	['Despite all the things that human being can do or are capable of doing, their bodies are built on and run by less than 100,000 protein molecules. Each of these protein molecules has a corresponding gene, which is responsible for ensuring that there is a timely flow of instructions. The human being, therefore, functions by a set of genetic instructions.\nThe Human genome is a complement of genetic material that is found in the human cell. Though there are over a billion people in the world today, there are only minute differences between genomes, which is why it is usually referred to as the human genome, as if there is only one in the whole world. The singular reference to the human genome is only intended to stipulate how little difference there is the genomes found in different human cells.\nYou might then wonder that if the human genome is all the same, how come each of us is so different. The human genome is distributed about twenty three different sets of chromosomes, which have been replicated in each of us since the fusion of egg and sperm at conception. It is this, which causes our individual uniqueness. Scientists have always said that our individual uniqueness is as a result of the seven trillion cells that are preserved in each of our bodies.\nEssential to note is that the human genome is a DNA or deoxyribonucleic acid. It is a natural polymer, which is built u of repeating nucleotides. Each of these nucleotides consists of a simple sugar, one of four nitrogenous bases and a phosphate group. If all the chromosomes are counted, it can be found that the human genome is comprised of about three billion base pairs of chromosomes. Also, vital to note is the fact that though the human genome has been read by DNA sequencing, it is still not well understood.\nSimply put, the human genome is a set of human genetic information that is stored as DNA sequences. We have what is called the haploid human genome and the diploid human genome. The haploid human genome can be found in the egg and sperm cells. Research shows that this genome has about three billion DNA base pairs. On the other hand, the diploid genome is found in the somatic cells and has twice the DNA content as the haploid human genome. As has been stated, the human genome is still not well understood. It is for this reason that the Human Genome Project was created.\nThis project is said to have been the first one to produce a complete sequence of separate human genomes. By 2012, the project had finished sequencing thousands of individual human genomes and had mapped out many more for lower level resolution. Research indicates that a more understanding of the human genome will lead to better diagnosis and treatment of diseases. It will also lead to more insight into human evolution and human biology. The advantages of a better understanding of the human genome are, therefore, endless.', 'The applications of genetic linkage and association analysis\nGenetic linkage and association analyses are the major tools to identify the genetic basis of diseases or traits. The primary difference between these two approaches is that linkage analysis looks at the relation between the transmission of a locus and the disease/trait within families, whereas association analysis focuses on the relation between a specific allele and the disease/trait within population. In the past three decades, many genetic variants underlying diseases or traits have been identified and different types of variants can be detected by these two methods. We introduce important applications of these tools and the relevant findings as follows.\nGenome-wide linkage analysis\nLinkage analysis has to be applied to family data since it needs the information of allele transmission within families. Because linkage analysis is based on the information of allele transmission within homogeneous family, it is robust to population stratification. In practice, a genome-wide linkage analysis requires the genotyping of several hundred highly polymorphic microsatellite markers or several thousand well-characterized single nucleotide polymorphism (SNP) markers evenly distributed across the whole genome.\nLinkage studies have successfully identified the genetic bases of many Mendelian diseases, such as Huntington’s disease, cystic fibrosis, or early-onset Alzheimer’s disease, which are caused by a mutation in a single gene (Online Mendelian Inheritance in Man (OMIM)). Causal variants of Mendelian diseases often have large effect and are rare in the population. In contrast to Mendelian disease, any disease caused by the joint effect of multiple genes and/or environmental factors is called complex disease. Many common diseases such as diabetes, hypertension, and various cancers are complex diseases. Although linkage analysis is powerful in gene identification of Mendelian disease, this approach has not been successful for complex diseases.\nGenome-wide association analysis\nIn the past decade, based on the idea of “common disease, common variant (CD-CV)” hypothesis, many researchers looked for common variants underlying complex diseases or traits and genome-wide association study (GWAS) has been the major approach. According to the National Institutes of Health (NIH), “a genome-wide association study is defined as any study of genetic variation across the entire human genome that is designed to identify genetic associations with observable traits (such as blood pressure or weight), or the presence or absence of a disease or condition”. GWAS relies on the information of millions of SNPs and patterns of linkage disequilibrium (LD) across the genome provided by the HapMap project, and the development of microarray technologies which can genotype millions of SNPs fast and accurately in a short time. Up to now, more than 2,400 GWAS have been published and more than 14,900 SNPs have been reported to be associated with human diseases or traits (GWAS Catalog). However, most GWAS have identified SNPs conferring small effects, which can explain only small proportion of risk of diseases or variation of quantitative traits. This phenomenon is called “missing heritability” of common disease.\nApplications of next-generation sequencing approaches\nRecently, to deal with the problem of “missing heritability”, more and more researchers tried to implement association studies of rare variants, which are based on the hypothesis of “common disease, rare variant (CD-RV)”. At the same time, great advances in “next-generation” sequencing (NGS) technologies make the whole-genome or whole-exome sequencing feasible, which could facilitate the identification of rare variants underlying Mendelian or complex diseases. The implementation of NGS approaches will generate a huge amount of sequence data and how to identify the pathogenic mutation(s) from this huge number of variants is a critical problem. Furthermore, since the cost of sequencing is still high, in practice, researchers have to adopt extra information to conduct cost-effective sequencing studies to identify the disease-causing variants. Accumulating examples suggest that integration of linkage analysis and NGS and combination of GWAS, NGS, and imputation could be powerful and cost-effective approaches to identify the disease-causing variants of Mendelian or complex diseases. Results of these studies are summarized in some review papers such as Brunham & Hayden (2013).']	['<urn:uuid:7512facd-1b38-4242-96f7-e8eb371bfe98>', '<urn:uuid:ac2db2ea-152f-43b8-a275-19b96e94eeae>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T15:30:12.136021	8	92	1163
89	fish farm disease spread prevention monitoring methods	Disease spread in fish farms can be prevented and monitored through two main methods. First, ocean current monitoring helps track viral transmission between farms - researchers use digital models with virtual virus particles to understand how diseases like pancreas disease spread through water pathways. Second, ozone sensors provide real-time monitoring of water quality and disinfection levels. These sensors help eliminate harmful bacteria, viruses, and parasites while enabling automated control of ozone dosing based on water parameters. Both methods work together to maintain fish health and prevent disease outbreaks in aquaculture facilities.	['Currents in the ocean and fjords spread viruses that are killing large numbers of farmed salmon. Where should fish farms be built? And should they all be in use at the same time? Researchers now know more about how to limit the virus problem.\nPancreas disease is a viral infection in farmed salmon that causes major economic losses because it reduces how much the fish grow and increases mortality. When farmed salmon are weakened, they also become more susceptible to other diseases. Pancreas disease (PD) can resolve on its own, but often mortality is high.\nIn addition to threatening fish welfare, every year the disease leads to major losses for Norwegian salmon breeders. A single outbreak can quickly cost a small aquaculture operation NOK 10 million.\nOcean currents are one of the most important pathways for the disease, carrying the virus from one fish farm to the next. Planning for where best to locate aquaculture facilities, and knowledge of which facilities to use when combatting the disease, can prevent the spread of the disease, new research from NTNU in Ålesund shows.\nDisease follows ocean currents\nResearchers in the project “Strategies for limiting the spread of PD” monitored ocean currents in the Romsdalsfjord and Nord-Trøndelag in 2014. Data from the observations were transferred to a digital model, where researchers created virtual virus particles.\nThe technology has made it possible to monitor how the disease can spread from one aquaculture facility to another.\n“The virus can survive for a long time in Norwegian coastal waters. But since it does have a limited lifespan, we’ve also removed the virtual virus in the data model after a while to simulate natural conditions,” says Lars Gansel, an associate professor at NTNU in Ålesund.\nThe researchers have also tested the real virus in the lab. This allowed them to figure out how much of the virus is needed to infect a salmon.\nCompared different models\nThe models show which fish farms are most susceptible to becoming infected and which are the most likely to transmit the disease to neighbouring facilities. Researchers can also use the models to predict how the virus moves when the current changes and how the currents are affected by temperature and the seasons.\nThe researchers released the virus – virtually – into four water pathway models and compared them to better understand the spread of pancreas disease.\nArticle continues below illustration.\n“This method allows us to calculate how much of the virus is transmitted to other fish farms. If the result is the same every time, then we know that the calculations are correct. By monitoring the ocean this way over a year’s time, we can detect probable fluctuations in ocean currents and do a better job predicting how viruses are spreading,” said Gansel.\nMore information needed on production areas\nNorwegian authorities have divided the Norwegian coast into production zones for farmed salmon. Large designated coastal areas without fish farms act as “firebreaks” to prevent the spread of infection.\n“In general, we need more information on where production areas should be located along the entire coast of Norway, what makes for good firebreaks and how we can use them better,” says Gansel.\nSome of the research indicates that actual firebreaks, such as Buholmsråsa in Sør-Trøndelag, do work.\n“Previously, firebreaks followed county boundaries, but luckily they’ve recently been changed to follow natural boundaries,” Gansel says.\nAn example of a natural boundary would be where the coastline directs the northbound coastal current out to the ocean, which lengthens the distance a virus has to travel from one fish farm to the next. And large islands or areas with slow water movement can prevent the virus from reaching a fish farm during the virus’s lifespan.\nFarms coordinate fish slaughter\nThe research project will also look at the economic consequences of regulating the use of fish farms during disease outbreaks.\n“If the ocean currents flow in a direction that results in one fish farm spreading the virus to others, one possibility may be to kill the fish. Farmers could also stop using multiple fish farms at the same time to create small firebreaks in a production area. This is where research can offer solutions for what measures are beneficial and what can be profitable in the long term,” says Gansel.\nThe breeders are required to kill all the fish in the net cages in the case of a viral outbreak if they happen to be in an area where PD is being combatted.\n“In areas with both big and small fish farms, breeders may disagree on when the fish should be slaughtered and which facilities need to slaughter their fish. The breeders prefer to keep the fish until they’re bigger to mitigate their financial loss. Improving our understanding of how currents spread the virus makes it easier to see where the salmon should be slaughtered first to keep the virus from spreading to the other fish,” said Gansel.\nThe researchers will also assess the economic impact of disease prevention measures and rank the overall costs after figuring out loss and gain in the short and long term.\nNew zone planning includes research results\nAuthorities have proposed new zone boundaries along the whole coastline of Norway. The first version of the proposal from the Ministry of Trade, Industry and Fisheries has been made available for public comment. A monthly virus screening of farmed salmon is also being proposed. The research project’s management group has received feedback that the PD research results are being taken into account in the planning.\n“Our results have transfer value to all the areas with fish farms in Norway. So we hope that the research will be used in determining future zones to prevent the spread of disease in farmed salmon,” said Gansel.', 'Aquaculture, the farming of aquatic organisms such as fish, crustaceans, and mollusks, plays a crucial role in meeting the growing demand for seafood. However, maintaining a healthy aquatic environment is essential for the success and sustainability of aquaculture operations. One important tool that aids in achieving this is the ozone sensor. Ozone sensors are designed to measure and monitor the levels of ozone in water, providing aquaculturists with valuable information to optimize water quality and animal welfare.\nBenefits of Ozone Sensors in Aquaculture\nOzone sensors offer several benefits in aquaculture settings. Firstly, they provide real-time data on ozone levels, allowing farmers to promptly detect any deviations from the desired range. This enables early intervention and corrective measures, preventing potential harm to the aquatic organisms. Additionally, continuous monitoring ensures that the ozone concentration remains within safe parameters, preventing overexposure and its associated negative effects.\nMoreover, ozone sensors help in maintaining optimal water quality. Ozone is a highly effective disinfectant, capable of eliminating harmful bacteria, viruses, and parasites that can cause diseases in aquaculture. By accurately measuring ozone levels, farmers can determine the appropriate ozone dosage for disinfection, ensuring a clean and healthy environment for the aquatic species. This reduces the reliance on chemical treatments, making aquaculture operations more sustainable and environmentally friendly.\nFurthermore, ozone sensors enhance automation and control in aquaculture systems. Integrated with monitoring systems, they can automatically adjust ozone dosing based on water quality parameters, ensuring consistent and precise ozone levels. This minimizes human error and labor-intensive tasks, allowing farmers to focus on other aspects of fish rearing and management. The automation provided by ozone sensors improves overall operational efficiency and productivity in aquaculture.\nChallenges and Considerations\nWhile ozone sensors offer numerous advantages, there are also challenges and considerations to keep in mind when using them in aquaculture. One challenge is the need for regular calibration and maintenance to ensure accurate measurements. Ozone sensors can drift over time, affecting their reliability and precision. It is essential to follow the manufacturer’s guidelines and calibration protocols to maintain sensor accuracy and prolong their lifespan.\nAnother consideration is the potential interference from other substances present in water. Dissolved organic compounds and particulate matter can influence the measurement of ozone levels, leading to inaccurate readings. Proper filtration and pre-treatment of water can help minimize such interferences, ensuring reliable data from ozone sensors.\nCost is also a factor to consider when implementing ozone sensors in aquaculture. While they provide invaluable benefits, the initial investment and ongoing maintenance expenses may pose financial challenges for some farmers. However, it is important to weigh the long-term benefits against the costs and consider the positive impact on overall productivity and the environment.\nFuture Developments and Applications\nOzone sensors in aquaculture are continuously evolving, with advancements in technology offering new possibilities. One area of development is the integration of ozone sensors with smart aquaculture systems. These systems utilize artificial intelligence and automation to optimize farming conditions, including ozone dosing. By analyzing data from ozone sensors and other parameters, smart systems can dynamically adjust ozone levels to match the specific needs of the aquatic organisms, further improving water quality and animal welfare.\nAdditionally, research is being conducted to explore the application of ozone sensors in disease management. Ozone has been shown to have therapeutic effects in mitigating certain fish diseases. By accurately monitoring and controlling ozone levels using sensors, farmers could potentially leverage ozone therapy as a preventive or curative measure against specific diseases, thus reducing the reliance on antibiotics and chemicals.\nThe role of ozone sensors in aquaculture is crucial for maintaining optimal water quality, preventing disease outbreaks, and promoting animal welfare. By providing real-time data, automation capabilities, and disinfection control, these sensors assist aquaculturists in achieving sustainable and efficient farming practices. Nevertheless, it is essential to address challenges related to calibration, interferences, and costs. The future holds exciting prospects for ozone sensor technology in aquaculture, with continued advancements paving the way for even greater benefits and applications. Find extra details about the topic in this external resource we’ve specially prepared for you. https://lucentwater.com, obtain worthwhile and supplementary details to enhance your comprehension of the topic.\nAccess the related links and discover more about the subject matter:']	['<urn:uuid:467f1358-9b95-4337-9071-123039fa7a0a>', '<urn:uuid:885362f0-09f0-4884-870d-458ca2a2e68a>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	7	91	1658
90	How does carbon storage in wetlands compare to tropical forests?	Coastal wetlands can store five times as much carbon as tropical forests over the long term, primarily in their deep wetland soils. While wetlands cover only about 6% of Earth's land surface, they store between 10-20% of its terrestrial carbon. The soil of mangrove forests alone may contain 22 billion tons of carbon, equivalent to more than two years of global emissions.	"['By Dick Kempka, The Climate Trust\nAs published by Sustainable Business Oregon – August 28, 2014\nIn many ways, the preservation of wetlands—transitional areas between terrestrial and aquatic environments—is vital to human existence, yet methods to preserve these valuable ecosystems are young and in many cases, still being developed.\nWetlands purify drinking water, protect urban areas from flooding, provide recreation areas, absorb wind and tidal forces, and provide habitat for thousands of wildlife species and other flora and fauna. In fact, more than one-third of the U.S. federally listed endangered species rely directly, or indirectly, on wetlands for their survival.\nAdditionally, wetlands play a critical part in the global greenhouse gas balance—acting as both a reservoir, storing carbon from the atmosphere, as well as producers of methane and nitrous oxide. The capacity of well-managed wetlands to offset manmade greenhouse gas emissions is significant. It is estimated that although wetlands cover about 6% of the Earth’s land surface they store between 10-20% of its terrestrial carbon.\nLeading the way in restorative work of wetlands ecosystems is Tierra Resources, a consulting firm exploring the utilization of wetlands as a carbon sink in the Louisiana Gulf Coast—an area with continued and dramatic wetlands loss. Funded by Entergy Corporation, Tierra Resources developed a new greenhouse gas accounting methodology, Restoration of Degraded Deltaic Wetlands of the Mississippi Delta that quantifies the carbon benefits from wetland restoration projects. This methodology has been approved for use by the American Carbon Registry.\nIn a project that has attracted national interest, Tierra Resources and The Climate Trust have also embarked on a first-of-its-kind wetlands pilot project using this method on property just outside of New Orleans. The objective is to develop a replicable project that produces high-quality credits that can be quantified and transacted on the carbon market—supporting both economic and ecosystem goals for these imperiled habitats.\nThis joint project is scheduled for completion in 2015, but preliminary findings around the carbon storage capacity of wetlands in the Gulf Coast has been promising. Tierra Resources and The Trust are focused on illustrating the importance of including wetlands project types in the California cap-and-trade compliance market.\nOther methods of wetlands restoration have recently emerged. The Louisiana Coastal Wetland Protection and Restoration Authority has also developed a Coastal Wetlands Creation greenhouse gas accounting method to help stem the rapid coastal land loss in Louisiana that occurs at a rate of one football field an hour.\nCloser to home, several stakeholders have partnered to develop a new wetlands method for the Sacramento–San Joaquin Delta and other California coastal regions. In this area, more than 2.5 billion cubic meters of organic soils have disappeared due to diking and draining for agriculture. Since the 1800’s, land on these deltaic islands have subsided up to 25 feet below sea level, and their soils are estimated to emit 1.5-2 million tons of carbon dioxide annually.\nAs reported in Ecosystem Marketplace, The Sacramento Utility District, the California Coastal Conservancy, California Department of Water Resources, and the Metropolitan Water District have combined forces to finance a new greenhouse gas accounting method in hopes of restoring California’s deltaic wetlands.\nFinally, a recent study in Washington State’s Snohomish Coastal Estuary suggests similar opportunities to mitigate climate change in the Pacific Northwest. This research finds that current and planned restoration projects in the estuary will result in at least 2.5 million tons of atmospheric carbon dioxide sequestered over the next 100 years—equivalent to one year of emissions from 500,000 cars.\nMany scientists believe these new coastal wetland methods, often referred to as “Blue Carbon,” could be the key to highlighting the importance of wetlands restoration and protection, while unlocking a valuable new sector for addressing carbon sequestration and climate change.\n©2017 The Climate Trust. Crafted by ILLUSIO', 'Along the fringes of coasts, where land and ocean meet, lie the world’s salt marshes, mangroves, and sea grasses. These coastal wetland ecosystems are found on every continent except Antarctica.\nThey provide nurseries for fish, feeding grounds for migratory birds, a first line of defense against storm surges and floodwaters, and natural filtration systems that boost water quality and recharge aquifers. Relative to their land area, they also sequester huge amounts of carbon in plants aboveground and in roots and soils below.\nCoastal wetlands can store five times as much carbon as tropical forests over the long term, mostly in deep wetland soils. The soil of mangrove forests alone may hold the equivalent of more than two years of global emissions—22 billion tons of carbon, much of which would escape if these ecosystems were lost.\nWetlands face a myriad of threats, but thanks to research and advocacy efforts, awareness is growing about the role they play in curbing climate change and coping with its impacts. It is vital to preserve healthy coastal wetlands—keeping a lid on the carbon they contain—while also rehabilitating and restoring those that already have been degraded.\ncoastal wetlands [vs.] tropical forests: Boyd, Robynne. “Blue Carbon: An Oceanic Opportunity to Fight Climate Change.” Scientific American. March 10, 2011.\nmangrove forests…two years of global emissions: “Editorial: Blue Future.” Nature 529 (2016): 255–256.\none-third of…mangroves…lost: Boyd, “Blue Carbon.”\n“sequestration…in ‘blue-carbon’ wetlands”: “Blue Future,” Nature; Ramsar Convention on Wetlands: Ramsar Convention Bureau. The Ramsar Convention Manual and Guide to the Convention on Wetlands, Ramsar, Iran, 1971. Gland: Ramsar Convention Bureau, 2013.\n“living shorelines”: Stutz, Bruce. “Why Restoring Wetlands Is More Critical Than Ever,” Yale Environment 360. July 28, 2014; Bilkovic, Donna Marie. Living Shorelines: The Science and Management of Nature-based Coastal Protection. Boca Raton: CRC Press, 2017.\nEuropean companies…in Senegal: Bird, Winifred. “African Wetlands Project: A Win for the Climate and the People?” Yale Environment 360. November 3, 2016.\nAccording to the journal Nature, the soil of mangrove forests alone may hold the equivalent of more than two years of global emissions […].\nProject Drawdown defines coastal wetlands as: the legal protection of carbon-rich mangroves, seagrasses, and saltmarshes, leading to reduced degradation rates and the safeguarding of carbon sinks. This solution replaces non-protected coastal wetlands.\nCoastal wetlands significantly impact global carbon cycles. Unlike most terrestrial ecosystems, coastal wetlands can continue sequestering carbon for centuries without becoming saturated. As a result, they have accumulated vast stores of carbon, making their global significance high despite their small area. Yet, these ecosystems are being degraded rapidly due to human activity, and relatively few are protected.\nCoastal wetlands also provide important ecosystem services. These ecosystems are being replaced for other uses, including coastal development and agriculture, releasing their stored carbon and preventing future sequestration. The coastal wetlands solution proposes increased protection of these important carbon reservoirs, with mitigation impact via emissions reduction and biosequestration.\nThe oceans are the world\'s largest carbon sink, yet their climate mitigation potential has received little attention. Coastal wetland protection is one component of a ""blue carbon"" strategy, which also includes restoration of coastal wetlands and the Coming Attractions marine permaculture and ocean farming.\nTotal Land Area \nThe total global area of coastal wetlands is 49 million hectares, of which 7.4 million hectares are already protected.  Thus, the total available land for the coastal wetlands solution is 41.6 million hectares. The modeling of the coastal wetlands was based on the individual modeling of mangroves, seagrasses, and salt marshes. The total land area available for this solution was modeled using the annual rate of degradation for mangroves, seagrasses, and salt marshes, i.e. 1.24 percent, 3.41 percent, and 1.50 percent, respectively. Thus, an estimation was made for future degraded and non-degraded mangroves, seagrasses, and salt marshes, and the non-degraded area (which is not yet protected) was considered the available area for protection as part of this solution.\nAdoption Scenarios \nFive custom adoption scenarios were developed for each of these three coastal wetlands using a linear growth curve. Given the small area of coastal wetlands, the high urgency because of the annual degradation of unprotected coastal wetlands, and the high mitigation efficiency of protection, several scenarios emphasized early peak adoption by 2030.\nImpacts of increased adoption of coastal wetlands from 2020-2050 were generated based on three growth scenarios, which were assessed in comparison to a Reference Scenario where the solution’s market share was fixed at the current levels.\n- Plausible Scenario: This scenario results in the protection of 30.5 million hectares of unprotected coastal wetlands.\n- Drawdown Scenario: Adoption is intensified under this scenario based on the most aggressive adoption scenarios, and results in the protection of 33.4 million hectares of unprotected coastal wetlands.\n- Optimum Scenario: Adoption is intensified under this scenario based on the most aggressive adoption scenarios, and results in the protection of 35.3 million hectares of unprotected coastal wetlands.\nAchieving 100 percent protection of unprotected coastal wetlands, even under the most aggressive adoption scenarios, was assumed to be unachievable due to the continuous annual degradation of coastal wetlands.\nEmissions, Sequestration, and Yield Model\nSequestration rates for mangroves are set at 1.7 tons of carbon per hectare per year, based on meta-analysis of 11 data points from 10 sources. Emissions from degraded or deforested mangroves are set at 31.3 tons of carbon dioxide-equivalent per hectare per year, based on meta-analysis of 15 data points from 8 sources. Salt marsh sequestration rates are 1.8 tons of carbon per hectare per year, based on 20 data points from 10 sources. Emissions from degraded salt marshes are 9.8 tons of carbon dioxide-equivalent per hectare per year, based on 2 data points from 2 sources. Seagrass sequestration is set at 1.1 tons of carbon per hectare per year, based on 8 data points from 5 sources. Emissions from degraded seagrass beds are set at 5.9 tons of carbon dioxide-equivalent per hectare per year, based on 3 data points from 3 sources.\nFinancials are not modeled, as costs are not necessarily carried out by the landowner or land manager.\nThe coastal wetlands solution was not affected by integration with other solutions, as coastal wetlands are largely distinct from terrestrial land uses. An exception is mangroves, which were included in the “forest” land use model but were given highest priority, and were therefore not limited by any other forest land use.\nTotal adoption in the Plausible Scenario is 30.5 million hectares in 2050, representing 62.2 percent of the total available land. Of this, 23.1 million hectares are adopted from 2020-2050. The emissions impact of this scenario is 3.2 gigatons of carbon dioxide-equivalent reduced by 2050. Total carbon protected is 14.6 gigatons of carbon dioxide-equivalent. Financial impacts are not modeled.\nTotal adoption in the Drawdown scenario is 33.4 million hectares in 2050, representing 90.2 percent of the total available land. Of this, 26.0 million hectares are adopted from 2020-2050. The impact of this scenario is 4.1 gigatons of carbon dioxide-equivalent by 2050. Total carbon protected is 15.9 gigatons of carbon dioxide-equivalent.\nTotal adoption in the Optimum scenario is 35.3 million hectares in 2050, representing 72.0 percent of the total available land. Of this, 28.1 million hectares are adopted from 2020-2050. The impact of this scenario is 4.5 gigatons of carbon dioxide-equivalent by 2050. Total carbon protected is 16.8 gigatons of carbon dioxide-equivalent.\nThe International Union for the Conservation of Nature and The Nature Conservancy report that if costal wetland destruction was halved, it would result in an annual emissions reduction of 0.23 gigatons of carbon dioxide per hectare per year (Herr and Landis, 2016). The Drawdown model shows a maximum annual reduction of 0.14-0.18, relatively close to this benchmark.\nNet costs and savings should be calculated for future upgrades of this solution. More data on salt marsh and seagrass sequestration and emissions rates are also needed. It would be also useful to model restoration in addition to protection.\nLike peatlands, coastal wetlands harbor a disproportionate share of the world\'s stored carbon. They also provide critical ecosystem services. Thus, aggressive efforts to protect these ecosystems are an important component of climate change mitigation efforts.\n To learn more about the Total Land Area for the Land Use Sector, click the Sector Summary: Land Use link below.\n Based on meta-analysis of protection rates from 6 sources.\n To learn more about Project Drawdown’s three growth scenarios, click the Scenarios link below. For information on Land Use Sector-specific scenarios, click the Sector Summary: Food link.\n For more on Project Drawdown’s Land Use integration model, click the Sector Summary: Food link below.']"	['<urn:uuid:60981bcb-9bb4-4c65-8908-e1d9b35ec4af>', '<urn:uuid:acb69013-21fe-4173-8190-97547886af2d>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T15:30:12.136021	10	62	2045
91	what is special about rieger organ recording location compared other organs	The Rieger Organ is unique because it was the first sampled organ recorded in a concert hall (specifically the Great Hall of the Wiener Konzerthaus) rather than in a church. This makes it blend perfectly with orchestral performances, unlike other organ recordings.	['Both the Rieger Organ from the Great Hall of the Wiener Konzerthaus and the Bösendorfer Imperial Grand Piano truly reflect the impressive sound of Vienna. This is the city’s legacy, transported to the 21st century by the means of technology, passion, and craftsmanship. These keyboard instruments are complemented by the Special Keyboards Collection, including prepared piano, harmonium, and harpsichord.\nThe new Vienna Imperial virtual grand piano is in a class of its own. The venerable Bösendorfer Imperial 290-755 that the Vienna team hosted at the Silent Stage for two months was equipped with the famed piano manufacturer’s CEUS technology, which allowed for the most precise and by far most extensive piano sample recordings ever conducted in the history of music technology. 1,200 recorded samples per key represent a magnitude of sampling detail that has been unthinkable up to now.\nWith the support of “Bösendorfer Klavierfabrik” we have developed our first sampled grand piano software instrument, a true recreation of a Bösendorfer 290 Imperial that was tuned and regulated by the piano manufacturer’s best engineers and piano technicians. The Vienna Instruments Collection Bösendorfer Imperial makes full use of our new proprietary engine and performance algorithms including automatic repetition performances. Its massive and unprecedented 54 GB sample set includes unlooped sustain samples in pedal up and pedal down variations and in 7 velocity layers, tone repetitions, real recorded sustain pedal resonances, multiple release samples, and key noises.\nThis is truly a first: The Vienna Instruments Collection Vienna Konzerthaus Organ is the first sampled organ recorded in a concert hall, not in a church, thus blending perfectly with orchestral performances. Obviously, it is also the first instrument not recorded at the Silent Stage, and yet it adheres to Vienna’s overall sampling concept since it has been captured in exactly the same space that provides the essential impulse responses for their revolutionary MIR reverberation and mixing engine. That space is the venerable Great Hall of the “Wiener Konzerthaus”, where the famous “Rieger Organ” was installed in 1913.\nThe 8 GB collection Special Keyboards offers a state-of-the-art approach to capturing exceptional, rare, but also classic sounds that every sound designer or composer should have. Featuring Harpsichord, Harmonium, and Prepared Piano, the Special Keyboards includes the full sets of samples along with the software instrument, featuring the Vienna Instruments engine.\nThe Harpsichord has a famously bright sound, rich in overtones. Typically a Baroque instrument, the harpsichord has played a minimal role in classical and romantic music. In modern literature, however, it has been rediscovered by a broad range of musicians, and is also used poignantly and effectively in film and TV music. In our own Silent Stage, the following single notes and repetition performances of a two manual harpsichord were recorded: 8’ register solo, 8’ double, and tutti (a combination of two 8’ registers and one 4’ register).\nThe Prepared Piano is a technique introduced in 1949 by John Cage, where objects like erasers, nails, wire, paper, etc., are inserted between a piano’s strings in certain places, causing them to produce additional tones, harmonics, or percussive sounds. For these recordings, a Bösendorfer grand piano was treated with all kinds of objects as well as with bare hands (e.g., glissandos over the strings) in order to create all-new, experimental, and sometimes scary sounds.']	['<urn:uuid:d56964d0-56af-4c1e-b502-292d11fa777b>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T15:30:12.136021	11	42	546
92	How long before a car trip should dogs be fed?	Dogs should be fed at least three hours prior to starting the journey to prevent upset stomach.	['If you are planning to hit the road with your dog, then a few important things must be kept in mind. You are probably thinking about these matters already, and that has brought you here. Although traveling with your pets can be a tricky situation to handle, it is nothing too difficult to do if planned properly. As you read on, you will learn about some tips and items that could really come in handy in such a situation.\nOptions to Use\nPlanning for something is like preparing a map, an outline that will serve as the key to achieving a certain goal. In a situation where you are moving your large dog like a Mastiff to a new location, planning can do wonders. Make a list of all the things that you might need for your dog, gather those things, and make sure they have been placed in the car before you leave.\nSeveral objects are available that can be utilized to limit the movement of the dog within your vehicle. This will prevent any accidents from happening. Some options that can be utilized include:\nPlacing your dog in a crate will minimize his movement. However, you must make sure that the size of the crate is appropriate for the dog so that it does not create discomfort during the journey. Also, the crate should be of good quality. Crash-tested crates are the best option for this purpose as they are safety-certified. Then, you must keep this in mind as well that the crate must be according to your vehicle.\nFor Car Backseat\nMost of the time, owners find it best to place their dogs in crates that can fit on the backseats of the car instead of the trunk. This is comfortable for both the dog and the owner as the dog’s movements are easy to monitor, and he remains calmer seeing you around.\nSeveral different dog crates are available for car back seats. Quality features like claw-proof material must be preferred. Various sizes are available, and you must pick the size that is right for your dog so that he does not get uncomfortable. Also, go for the material that is lightweight yet sturdy. Mostly, the crates for car back seats are preferred to be made of a durable fabric as they can adjust easily inside the car.\nIf you are planning to travel in an SUV, then metal dog crates can be used as well. These are designed to fit in SUVs, vans, and other such large vehicles. You must pick a crate with features like a leak-proof pan, carrying handle, and durable rust-proof material. The latches should be heavy-duty so that you do not have to worry about the safety of your dog throughout the journey.\nBesides crates, some other options can also be used for restraining your dog.\nLeash and Harness\nCarrying a leash and harness with yourself is a good option, especially when your dog is uncomfortable in a crate. Even if he is comfortable, he might want to get out of the crate sometimes when the journey is too long. In such a situation, a leash and harness can be a lifesaver. A harness helps the dog avoid getting entangled in the leash, so both these items go hand in hand.\nDog Seat Belt\nIf your dog has ever shown signs of travel anxiety, then it is important that you consult your dog’s vet as part of planning this trip. Also, it might be possible that putting your dog in a crate develops anxiety in him. In such a case, it is best that you avoid using a crate and instead get a seat prepared for your dog, which is equipped with a seat belt as well. You can have it fixed in the car. It will restrain the dog but be less captivating than a crate. Hence, it may relieve your Mastiff of the stress caused by placing it in the crate. Moreover, it will make the dog safe and prevent him from getting involved in an accident.\nDog Car Hammock\nYou have to make sure that the place where the dog is made to sit is comfortable. A dog car hammock is a great tool for this purpose. Moreover, while transporting a large dog that does not want to be in a crate, a hammock can be a protective shield for the seats. It will keep the seats safe from dog fur, scratches, and mud that come along with the dog’s paws.\nTop qualities in the best dog car hammocks include them being water-proof and washing machine-safe with an anti-slip bottom. If you love your vehicle as much as your dog, then you will love this amazing tool as it works to keep your car clean.\nDog Car Barrier\nInstalling a dog car barrier between the driver or passenger seats and the sitting area of the Mastiff can also be quite handy. This will ensure the safety of the driver as well as the pet. Also, it will prevent the driver from getting distracted by any unpredictable movements of your canine partner.\nA multipurpose car barrier can also be used to store the dog’s toys or blanket to make the journey less dull for your canine pal. The most important feature of a good-quality car barrier is that it should be easy to install and remove.\nIt’s important to make a list of all the situations that you may find yourself in and prepare for the best pet care during the journey. The details of different tips will eventually depend on the duration of your journey and your pet’s needs. However, you can keep the following points in mind so that the journey remains as smooth as possible.\nMake Time For Breaks\nTaking short breaks during the journey might slow you down, but with your pet along, it will be a good decision after all. If your journey is quite long, try to take a break after every one or two hours. Let your dog out of the car during this time. He might need to go defecate or even run around here and there to regain his energy.\nThe breaks should be about 15 to 20 minutes long, at least so that your furry pal can get enough fresh air. If you are traveling for a few days, you can plan for longer breaks at a dog-friendly rest house. This will be a good relief for your pet friend as traveling continuously can result in frustration in the dog’s behavior.\nA great piece of advice here would be to not feed your dog immediately before starting your journey. This can save you from a number of problems later on. Feeding the dog immediately before the journey can lead to an upset stomach. This can, in turn, cause a messy situation inside the vehicle. You will certainly not want that to happen. Therefore, it is really useful that you feed your dog at least three hours prior to starting the journey.\nHowever, if the journey is too long, then keeping an empty stomach throughout the journey might not be possible. In such a case, when you take breaks during the journey, make sure that you feed your dog with a small amount of dog food. Do not feed him too much as it can upset the stomach. Also, give him water to drink but that too in reasonable amounts. Too much or too little food or water will not be suitable for your canine.\nAn easy, comfortable, and smooth journey with your mastiff is possible with sufficient planning prior to starting the journey. All your pet’s needs must be kept in view, and measures must be taken accordingly. The better planning you have done, the easier the entire ride will be. During the trip, make sure you remain calm as your dog can sense your anxiety and develop stress himself. This is the single most important thing that you can do to keep your dog free of any sort of distress. In the end, the better you have planned the trip, the more enjoyable your moments will be as they will be stress-free and organized.']	['<urn:uuid:827e4bad-4f1f-421a-b548-0a83b7986e90>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T15:30:12.136021	10	17	1365
93	nubian pyramids vs solar pyramid purpose	The Nubian pyramids served primarily as elaborate tombs for kings and queens, containing burial chambers and being part of a larger religious complex symbolizing kingship and creation. In contrast, the Solar Pyramid serves as a functional timekeeper and educational tool, designed specifically to measure time through the year and help people understand solar patterns and natural rhythms.	['visit the store The Nubian Pyramids\nby Philip Coppens It is hard to keep up with the neighbours, especially if your neighbour is Egypt. Hence, whereas Egypt typifies pyramids, the Nubian pyramids, south of Egypt’s border, are often seen as the Nubians trying to keep up with the Egyptians, but utterly failing to do so. Still, there are 223 pyramids in ancient Nubia, roughly double the amount of pyramids in Egypt. However, they are small in size; sceptics could argue it is quantity, rather than quality. Nevertheless, the 20th century British writer Basil Davidson described Meroë, one location in which a huge concentration of Nubian pyramids is located, as one of the largest archaeological sites in the world. Unfortunately, all the tombs at Meroë have been plundered, most infamously by Italian explorer Giuseppe Ferlini (1800-1870) who smashed the tops off 40 pyramids in a quest for treasure in 1834. Ferlini tried to sell the treasure of Queen Amanishakheto’s pyramid (involving exquisite gold amulets, signet rings and necklaces) when he returned to Europe. Collectors did not believe that such a treasure could come from black Africa; they thought he was trying to pass off fakes. Indeed, they were jewels of great quality and beauty and often influenced by Greek art; customers didn’t expect to find deep in the heart of Africa depictions resembling those of Egyptian or classical Greek art. Still, Ferlini found only one cache of gold and it is unsurprising that he is not too fondly remembered in Sudan today. There were three Kushite kingdoms: the first had Kerma as capital and existed from 2400 to 1500 BC; the second centred on Napata (1000–300 BC) and the third was that of Meroë (300 BC–300 AD).Originally influenced by their northern neighbours, eventually, the Nubians were even able to invade, conquer and unify Egypt, when the king of Napata ruled as a Pharaoh of the 25th Dynasty, a rule that ended with the Assyrian conquest in 656 BC.\nThough the Nubian kingdoms must have been aware of the pyramids that lined the Nile valley further north, it was only during their domination of Egypt that the royal sites of Napata and its successor Meroë became associated with any pyramidal activity in Nubia itself. That the Nubian kings definitely saw the Egyptian monuments is known from an account of Pharaoh Piye. After taking control of most of Egypt, Piye set out for Heliopolis to worship the sun god and celebrate his coronation as king of Egypt, before returning to Napata The first series of Nubian pyramids were built at the site of el-Kurru and included the tombs of King Kashta and his son Piye (Piankhi), together with Piye’s successors Shabaka, Shabataka and Tanwetamani, and 14 queens’ pyramids. The Napatan pyramids were sited at Nuri, on the west bank of the Nile in Upper Nubia. This necropolis was the burial place of 21 kings and 52 queens and princes. The oldest and largest pyramid at Nuri is that of the Napatan king and 25th Dynasty Pharaoh Taharqa.\nBut the most extensive and best known Nubian pyramid site is at Meroë, which is located between the fifth and sixth cataracts of the Nile, approximately 100 kilometres north of Khartoum, the modern capital of Sudan. During the Meroitic period, over forty kings and queens were buried in that pyramid field. The Nubian pyramids differ from the Egyptian edifices: they are built of stepped courses of horizontally positioned stone blocks and range from approximately six to thirty metres in height; they are, to all intents and purposes, rather unimpressive. What makes them impressive is the sheer quantity of them that was cramped in a small area. What makes them a sad archaeological site is that all were plundered in ancient times, but wall reliefs preserved in the tomb chapels reveal that their royal occupants were mummified, covered with jewellery and laid to rest in wooden mummy cases, revealing that these Nubian pyramids definitely seem to have been tombs. El-Kurru, about 13 km south from Gebel Barkal, was excavated by G. Reisner in 1918-19. The site includes, as mentioned, the pyramid of Piankhi, which has a base length of about eight metres and a slope of probably about 68 degrees – much steeper than the 51 degrees of the Great Pyramid – though eight metres is, of course, miniscule compared to the floor plan of the Great Pyramid. It was the largest pyramid ever built at Nuri, and is unique among the Nubian pyramids in having been built in two stages. The first pyramid was encased in smooth sandstone. Drawings and written reports of the early 19th century reveal the truncated top of the inner pyramid projecting from the top of the decaying outer pyramid. The outer pyramid was the first of a type with stepped courses and planed corners. It had a sloped angle of about 69 degrees. An enclosure wall tightly encircled the pyramid, but Reisner was not able to unearth any traces of a chapel.\nInside, Reisner found a stairway of 19 steps that opened to the east and that led to the burial chamber. Piankhi’s body had been placed on a bed that rested in the middle of the chamber on a stone bench. The bench’s four corners had been cut away to receive the legs of the bed, so that the bed platform lay directly on the bench. This is conclusive proof that these pyramids were tombs, but it is in the pyramid of Nuri that is the most elaborate structure. King Taharqa was the first to build in this location and his pyramid is, by Nubian standards, impressive: 51.75 metres square and between 40 to 50 metres high. The entrance was by an eastern stairway trench, north of the pyramid’s central axis, reflecting the alignment of the original smaller pyramid. Three steps led to a doorway, with a moulded frame, which opened to a tunnel, widened and heightened into an antechamber that had a barrel-vaulted ceiling. Six massive pillars carved from the natural rock divided the burial chamber into two side aisles and a central nave, each with a barrel-vaulted ceiling. The entire chamber was surrounded by a moat-like corridor, with steps leading down from in front of the antechamber doorway.\nThe Nuri pyramids were generally much larger than those at el-Kurru, reaching heights of twenty to thirty metres. The last king to be buried at Nuri died in about 308 BC, before construction began in Meroë. It remained the royal cemetery for 600 years, until 350 AD. As at Nuri, the pyramids were stepped and built on a plinth, but now each triangular face was framed by smooth bands of raised masonry along the wedges, where the sides met. The Nubian Pyramid Age is far more recent than the Egyptian exercise. And that is what makes the endeavour intriguing. But, perhaps unlike Egypt, it was also directly linked with the Nubian economy. As Roman wealth declined, there was less demand for Meroë’s luxury goods. At the same time, Meroë’s Red Sea trade to the Indian Ocean was lost to her better placed neighbour, the Ethiopian kingdom of Aksum. In about 350 AD, the army of the Aksumite King Ezana invaded the island of Meroë. By then the capital had already been abandoned and the region was in the hands of a people whom the Aksumites referred to as the Noba.\nAs such, in 350 AD, the Pyramid Age along the Nile truly ended. The pyramid adventure of the Nubian kings seems to have been a method to integrate their nation with the Egyptians, who themselves had, by 600 BC, long abandoned pyramid building. The Nubian pyramids were clearly tombs, but is that all they were? The answer is no. The pyramids were, as in Egypt, just one aspect of a larger whole, in which the Nubian kings apparently rekindled ancient Egypt’s flame. The key seems to have been – once again – the involvement of the Heliopolitan priests, the men that have been commonly identified as those behind the original Pyramid Age. In a generalised way, we could state that pyramid building was a particular preoccupation of the Heliopolitan priesthood; temples were the main bailiwick of the Amunite priesthood, who had risen to the forefront of Ancient Egypt during the Middle and New Kingdom, when the capital had moved to Thebes/Luxor. It seems that with the Nubian invasion – which had largely only been possible because of the demise in power of Thebes/Luxor – the Heliopolitan priesthood was once again on the rise. Excavations at many Napatan sanctuaries, especially at Gebel Barkal, have revealed that the temples were generally built directly over the foundations of ruined New Kingdom (read: Amunite) temples. About 1450 BCE, the Egyptian pharaoh Thutmose III had extended his conquests to Gebel Barkal and established it as the southern border of his empire. The city he founded there was called Napata. The Egyptians remained only about 300 years.\nThe temple at Gebel Barkal was first excavated between February and April 1916 by George Reisner and his Boston team and again between March and April 1987 by the new Boston team led by Timothy Kendall. In 1989 Kendall and his staff resurveyed the monument. Their conclusion was that the Napatan rulers deliberately restored religious sites that had been abandoned by the Egyptian pharaohs when they evacuated Nubia some three centuries before. The Nubian kings used “Amunite temples”, but converted them to Heliopolitan sun worship. But they did not totally abandon the Amunite beliefs; after all, these Nubian temples had been built according to strict Amunite guidelines, and therefore would include features specific to that cult.\nBy restoring the old Egyptian cult places, especially Gebel Barkal, the Nubian kings were able to present themselves both within Nubia and especially at Thebes as the true successors of the pharaohs of the New Kingdom and the direct heirs to their throne. Taharqa was not only the one who created the most elaborate pyramid, he also transformed the Barkal site into the stage for the celebration of past, present and eternal monarchy, and the perpetual recreation of the world. Why here? Due to the reverse direction of the Nile at this location, Taharqa’s tomb, though still on the “west” bank, paradoxically lay to the east, the place of sunrise and rebirth. Gebel Barkal, on the “east” bank, lay paradoxically to the west, the place of sunset and death.\nAn important question to be posed is why the pyramid field lay around Gebel Barkal. The first explanation is economic, since the site was seen as an ideal crossing point for the Nile, as well as a crossing point for the caravans crossing the desert from Kawa to Meroë. The second explanation is religious. The sacred mountain Barkal (Barkal is Arabic, meaning both “holy” and “pure”) measures 74 metres, is made from sandstone and has a flat top. Seen from the west, it looks like the royal Egyptian ureaus, topped by the white crown. For the ancient Egyptians, the site must have been considered as the Creator God himself having sculpted the mountain, indicating it should become a royal, sacred location. From the east, it looks like the serpent with the sun disc on its top.\nThe interplay of the tomb and the mountain thus symbolised creation, death and rebirth simultaneously and Gebel Barkal, under Taharqa, was designed to be the ultimate and permanent centre of kingship in the Nile Valley. It was to be the eternal link between the creator god Atum-Ra and Mankind, and between the eternal king and the living king. We therefore find that in Nubia, the pyramid and the sites on which they were built contained far more symbolism than merely a funerary aspect – the sites were directly linked with kingship. And, as such, they conform to “the Pyramid Template”.', 'A visit to the Solar Pyramid will not only help to reconnect us with the rhythms of our natural environment, but stimulate us to consider to what extent the Sun is the ultimate source of the energy that powers our world.Dr. Robin Catchpole, Institute of Astronomy, Cambridge\nPush a stick in the earth and the shadow of the stick cast by the Sun on the ground will move through the day, marking the passage of time. At midday, local solar time, the Sun will be due south and the shadow will lie due north of the stick (in the northern hemisphere). This will not make a good solar clock, because, during the year as we go from summer to winter, the Sun will be at a different height in the sky at the same Solar time and the stick shadow will lie in a slightly different direction.\nThis problem is overcome in the conventional sundial by aligning the pointer that casts the shadow, parallel to the Earth’s axis of rotation. This means that the pointer is aligned north south and is tilted at an angle identical to the latitude of the sundial site. At night, the pointer points very close to the Pole Star, at the place around which the night sky seems to rotate.\nA sundial gives us time by the Sun, but this is no longer the sort of time we use to regulate our lives for the simple reason that “Sun hours” do not have the same length all the year round. This is the reason that a sundial has a table of corrections, referred to as the “Equation of Time”, which must be applied to the “Sun time” to get clock time. It is also the reason for the word “mean” in Greenwich Mean Time (GMT).\nThe Solar Pyramid\nThe Solar Pyramid is both a work of art and a giant sundial with the southern arm pointing north-south and set parallel to the earth’s axis of rotation. The two secondary arms also have astronomical significance as one is aligned toward the point on the horizon where the Sun rises and the other to where it sets at the time of the Summer Solstice, which is the longest day of the year.\nThe huge size of the Solar Pyramid allows us to draw a dial that can be read for every day of the year, although to avoid clutter, only the months of the year are marked. This means that it is possible to read Greenwich Mean Time directly from the dial as the shape of the hour and minute markers allows for the Equation of time. The markings also allow for the fact that the dial has a slope of 1:50 to allow rainwater to run off.\nThe dial also shows the time of sunrise and sunset at the Solar Pyramid and illustrates the difference in the number of hours of daylight from summer to winter.\nWhen the dial is designed to show the time for each day of the year, it is no longer necessary to have the gnomon pointing parallel to the Earth’s axis of rotation. For example, the gnomon could be vertical or correspond exactly to the angle defined by a 3:4:5 right angle triangle, as does Kahfre’s pyramid at Giza, one of the the original inspirations for the project. We have decided to follow conventional sundial design, uniquely anchoring the angle of the Solar Pyramid to its latitude. The design of the dial is unique to both latitude and longitude.']	['<urn:uuid:53680b3a-4f2b-4003-a782-b2e86084b343>', '<urn:uuid:ad4300f1-7ba1-4bcb-a576-034e96b8a68a>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T15:30:12.136021	6	57	2560
94	As a systems biologist studying disease resilience, I'm curious how complex networks respond to disruptions and what role neural systems play in maintaining that resilience. Can you explain both aspects?	Complex systems can exist in desirable or undesirable states and their resilience depends on their ability to withstand perturbations. When systems cross a tipping point, they may take time to break down (e.g., a year for an epidemic to spread globally). For neural systems specifically, normal physiological processes function on the edge of 'chaos' near a critical point, allowing for greater resilience and adaptation to changing conditions. However, aging is characterized by reduced adaptive capacity due to declining complexity of neural regulatory mechanisms, making the system more vulnerable to disruptions like anoxia and ischemia.	"['While preparing soup, have you ever wondered at what point does a pot of boiling water start evaporating?\nNow, we know from basic physics that water can be of two states- liquid or gas.\nBut is it possible to determine the exact interaction and energy between the billions of water molecules that converts liquid to vapor?\nCentury-old theoretical physics dictates that we can reduce these billions of interactions and parameters to a very few observables. In this case basically just one -temperature. We can at all times, figure out the state of water when we measure the temperature.\nBut what about other complex systems such as the weather patterns, ecology, disease states etc.? These systems interact via complicated networks, with many members in each system intricately linked through one another. Is it possible to quantify the state of such complicated systems?\nThat is precisely the focus of a new study by a team of researchers, including an Israeli applied mathematician at the Bar-Ilan University, Dr. Baruch Barzel, one of the lead authors.\nDrawing comparisons [to other systems] similar to water\'s \'desired\' state of the liquid form versus an \'undesired state\' of an evaporated form, Barzel says, ""A system can be in a desirable state, such as bees for example which pollinate flowers and thrive. Or that bees could be in an undesirable state as is happening over the past decade with the decline in bee population.""\nAlthough we are presented with examples of systems residing in either of two states, sometimes the number of states could exceed two. How to reconcile the observations then?\n""In order to quantify which one of the states the system is going to be in- meaning either on it\'s way to extinction or thriving, you need to measure many parameters,"" remarks Barzel.\nThe team of researchers led by senior scientist, Dr. Albert-Laszlo Barabasi, at the Northeastern University has investigated a central tenet of any complex system behavior- how does a system with inherently many parameters respond to perturbation?\nThere are several forms of perturbation. For instance, weather patterns are disturbed in light of global warming. In another instance, an ecosystem may be affected if a particular species faces extinction or a new species is introduced in it.\nGiven such versatile scenarios with a rich repertoire of interaction within systems, in this study, the researchers have addressed a very pertinent problem- Can the systems withstand such perturbations or are they vulnerable? In other words, how \'resilient\' is a system and at what stage does it breakdown?\n""Metaphorically speaking, what we were looking for is an analogy for \'temperature\' in these systems; Basically what are the relevant number(s) that can quantify the state of an complex system and take the millions of parameters that we would need to know and crush them all down into a single parameter that determines- is this system going to be liquid? That\'s good. Or is it going to be gas?"" remarks Barzel.\nBut real life systems are seldom as straightforward as the two physical states of water. The bigger challenge here is not just analyzing the states of such multi-dimensional systems but also quantifying it. That\'s where mathematics comes to the rescue! The researchers used mathematical equations to construct models and map the original complex system onto a single dimensional system to accurately predict the system\'s resilience.\nIndependent experts have hailed the work of Dr. Barabasi and his team as a very important and useful discovery within and outside the exciting new field of network science. The current work has defied popular belief that differences between biochemical, ecological and other networks (modeled using certain equations) are too large for any single solution to fit so many types.\nDr. Robert May, former Chief Scientific Adviser to the UK government and currently a professor of Zoology at the Oxford University emphasizes the broad impact of such predictive studies, ""There has, over the past several decades, been much research on the way natural ecosystems can be threatened by human disturbances. Likewise, the recent bad behaviour of some major banking systems raises important questions about stability and complexity. Although not explicitly addressed by these authors, their work is in many ways paralleled by efforts to understand how banking systems might be made more resilient and thus better able to handle the kinds of problems we have seen over the past few years in that area.""\n""Barzel\'s work claims to bridge these differences by providing an incredibly broad solution to a very important and difficult problem. If experiments back up Barzel\'s theory, it will be recognized as one of the most important and useful discoveries,"" says Dr. Neo Martinez, Associate Professor of Ecology and Evolutionary Biology at University of Arizona, not directly associated with this study.\nTo illustrate the practical adaptability of their theoretical model, the researchers used it to study the resilience of multidimensional technological system- power grids.\nThe scientists used the loss of power in the Northeastern United States, in the wake of snowstorms every year as a motivation to test their results on the resilience of transformers.\nDr. Jianxi Gao, another lead author at Northeastern University expresses his amazement on the versatility of their model, ""That this theory can be applied to AC power systems to predict blackouts, surprises me very much, because AC power system is more complex than the ecological and biological systems.""\nAn independent expert, Dr. Shlomo Havlin, Professor of Physics at Bar Ilan University and not a part of this study, highlights the benefits of such predictive studies for complex systems, which could be highly resilient yet quite fragile, ""This paper helps us understand three crucial aspects: 1. What are the different states or phases of a complex system? For instance, the power grid can be at a functioning state or at blackout. 2. What are the precise transition points? e.g., what is the critical load beyond which the power system risks collapse. 3. What structural characteristics enhance a system\'s resilience, namely what changes in design would make the power grid more reliable? These are very important findings, for which we lacked a theoretical understanding until now.""\nThe scientists posit that systems could be very similar. The moment that they cross the tipping point (in the water example, when water ceases to be in its liquid state and transforms into the vaporized state), it might take some time for the system to die out.\n""For instance, it might take an epidemic a year to spread to a global scale or about 24 hours before a cell becomes pathogenic,"" says Barzel.\nBut there is always time for intervention and the current study is a call for it!\nNow, what comes next? As Barzel puts it, ""In the pot of water, it\'s clear. You turn down the gas. But how do you do it in the case of Zika or Ebola?""\nIn other words, \'how\' to efficiently intervene so as to stem a disease from spreading, cross the tipping point and develop into a global epidemic?\nIn order to address such pressing issues several research teams are trying to identify and study early warning signals for systems - in the case of boiling water those are the bubbles.\nSuch studies should serve as a wake up call in identifying emerging problems across various platforms from biology to infrastructure and formulating intervention strategies to create resilient systems.\nHowever, resilience could be a double-edged sword. ""Cancer is a resilient condition able to withstand many pressures. In such cases, we need to impact resilience in the negative direction!"" remarks Barzel.\nThe scientists are optimistic however that sometime in the near future, the stock market or flu forecast will be done accurately with the aid of mathematical models akin to the one used in this study. And there will come a day, when such predictions will not be futuristic events any more! We will depend heavily on those similar to our absolute reliance on weather forecast nowadays.\nSo what\'s next as far as some of the exciting real life applications of this model is concerned? ""Network science and applications to brain- that\'s the upcoming decade. If I am allowed to be a prophet and not a scientist for a second,"" remarks Barzel and adds that there are some caveats to it.\nEcosystems for instance, can benefit tremendously from similar studies. Martinez expresses the importance of ecology and its studies on networks that determine whether ecosystems continue to support human life with the food and oxygen we need, ""These networks range from the networks of pollinating insects that fertilize our crops to networks of feeding interactions among species that move carbon in our atmosphere and store it in soils and fossil fuels.""\nEcologists are always keen to explore the dynamic behavior of these networks. And the only way to \'predict\' the success or the failure of these networks and their impact on the environment such as global warming is via theoretical models and statistical methods.\n""If it passes the tests, Barzel\'s theory would have significantly advanced ecology and have helped us predict and hopefully better manage the resilience of ecosystems ranging from orchards to fisheries,"" adds Martinez.\nAccording to May, an understanding of the network characteristics is becoming increasingly important as humanity\'s impacts on natural systems grow- such as ocean acidity and/or global warming.\nAs evidenced from this study a theoretical model seems to be a successful predictor of a system\'s resilience and collapse, yet sometimes its implications could be quite profound.\n""Infrastructure just as much it seems to be a matter of lifestyle, in many cases it is life or death. There are fatalities in the Northeast every year due to power outage during harsh winter storms. It\'s more than just the matter of a quality of life sometimes, it\'s sometimes just a matter of life!"" reflects Barzel.\nImage credits: Unsplash.com (Anthony Rossbach)\nEdited: May 12, 2016 (Included comments by Dr. Havlin and Dr. May)', 'Normal, healthy, physiological processes are regulated by a complex interplay of numerous, neuroendocrinal signaling pathways. Although there are many intermediary signaling events, the fundamental purpose of most signaling pathways is the transfer of regulatory information to and from the central nervous system (CNS). Functional CNS decline precedes the metabolic, reproductive, and cognitive disorders associated with aging. For example, recent advances in brain imaging technology have demonstrated that the structural changes eventually causing Alzeheimer’s Disease (AD), actually take place, long before the first symptoms are observed. Given the central role of the CNS in age-related pathologies, the dynamics of CNS function should, clearly, also be the central focus of anti-aging research. Preventing or slowing age-related changes in the CNS has the potential to maintain healthy physiologic function, as we grow older.\nNonlinear Dynamics and the Loss of Complexity Theory of Aging and Disease\nIn the past, simple reductionist approaches to the study of physiology have been very productive. However, given the inherent complexity of the dynamics of most physiological processes, including neurological function, future advances in the study of these processes will require the use of nonlinear, whole-systems approaches. (“Nonlinear” means that output is not proportional to input, but varies in more complex, often unexpected, ways.) In Neuroscience, the macro phenomena of cognition, emotion, motor activity, etc. are dependent upon the emergent, collective actions of billions of neurons, each of which is a nonlinear element. In general, physiology is the result of interactions of multiple feedback loops of nonlinear systems. The process of aging can be understood as a reduction in the complexity of these feedback and control systems. See: Loss of ‘Complexity’ and Aging.\nNormal, healthy physiological processes function on the edge of “chaos”, which is to say, near a critical point. This allows for greater degree of resilient vitality, and resistance to disruption than systems of a simple periodic, or stochastic nature. The nonlinear dynamics of neural systems facilitate functional adaptation to changing environmental conditions. In contrast, the aging phenotype is characterized by a reduced ability to adapt to stress and trauma reflective of a reduction in complexity of underlying regulatory mechanisms. Such a reduction in complexity may reflect the loss of a component, or the disruption of feedback coupling between components. For example, normal secretion patterns of glucocorticoids, sex steroids, and GH result from shrinkage of the hippocampus, loss of neurons, and declining neurogenesis. The number of dopaminergic neurons also declines with age, accompanied by a corresponding reduction in nigrostriatal signaling, which, in turn, produces such age-related disorders as Parkinson’s disease. See: Dopaminergic Neuronal Loss.\nSuch changes result in reduced complexity of the signaling dynamics of neural networks, resulting in a reduced adaptive capacity. The increased vulnerability of the aging brain to anoxia and ischemia, is one example of reduced neural adaptive capacity, which appears to be the result of compromised ribonomic ability to selectively translate stress-induced mRNA. See: Towards a dynamical network view of brain ischemia.\nClearly, many age-related disorders are the direct result of alterations in regulatory pathways of the CNS. A loss of neurological complexity as measured in EEGs has been found to characterize patients in a vegetative state. See: Complexity loss in physiological time series of patients in a vegetative state.\nNonlinear methods have been applied to the modeling of circadian rhythms. See: Modeling biological complexity. Many physiological processes depend upon circadian rhythms, which are regulated by a complex network of signaling and feedback mechanisms. Disruption of normal circadian rhythm can have profound health consequences, and is implicated in many diseases of the aging including heart disease, obesity, metabolic syndrome, psychiatric/neurological disorders, and even cancer. Many regulatory factors diminish with age, such as the age-related decrease in melatonin signaling, which results in a loss of regulatory complexity leading to circadian dysfunction.\n“Recent advances in molecular biology, neurobiology, genetics, and imaging have demonstrated important insights about the nature of neurological diseases. However, a comprehensive understanding of their pathogenesis is still lacking. Although reductionism has been successful in enumerating and characterizing the components of most living organisms, it has failed to generate knowledge on how these components interact in complex arrangements to allow and sustain two of the most fundamental properties of the organism as a whole: its fitness, also termed its robustness, and its capacity to evolve. Systems biology complements the classic reductionist approaches in the biomedical sciences by enabling integration of available molecular, physiological, and clinical information in the context of a quantitative framework typically used by engineers. Systems biology employs tools developed in physics and mathematics such as nonlinear dynamics, control theory, and modeling of dynamic systems. The main goal of a systems approach to biology is to solve questions related to the complexity of living systems such as the brain, which cannot be reconciled solely with the currently available tools of molecular biology and genomics. As an example of the utility of this systems biological approach, network-based analyses of genes involved in hereditary ataxias have demonstrated a set of pathways related to RNA splicing, a novel pathogenic mechanism for these diseases. Network-based analysis is also challenging the current nosology of neurological diseases. This new knowledge will contribute to the development of patient-specific therapeutic approaches, bringing the paradigm of personalized medicine one step closer to reality.”\nThe loss of complexity theory of aging represents a fundamental paradigm shift away from the classical assumptions of normal physiologic homeostasis. By directly challenging the classical assumptions of health and disease, it also points the way to a new class of therapeutic approaches based on whole-system targets, as opposed to treatments based on individual components, in isolation. It is my belief that such large-scale approaches will be necessary to effectively treat aging.\nNonlinear methods have a distinguished history of application in statistical physics. However, their application to physiology is a recent development, often requiring interdisciplinary approaches, since biologists aren’t historically trained in such methods. Despite their limited use, nonlinear methods have already yielded remarkable successes in our understanding of very disparate physiological processes. In addition to the examples previously mentioned, it turns out that the apparently simple, periodic process of a regular heartbeat is actually a complex, multifactorial process on the edge of chaos. See: Nonlinear dynamics of cardiovascular aging. , Chaotic Signatures of Heart Rate Variability. Pathologies of irregular heartbeat, in contrast, are the result of a reduced complexity resulting from a loss of regulatory feedback control mechanisms. Measures of heartbeat chaos have even proven to be the best predictor of mortality in heart patients. See: Heart rate chaos as a mortality predictor in mild to moderate heart failure.\nNonlinear approaches have had success in modeling the complex dynamics of stem cell populations, a process with direct implications for aging. See: Modelling Perspectives on Aging.\nFuture advances in genetics and epigenetics will undoubtedly rely very heavily upon advanced computational methods. With the mapping of the human genome, many single-gene, Mendelian, diseases have been identified. In fact, I believe it is safe to say that nearly all such disease have already been identified. (There are surely more, however, many extremely rare Mendelian conditions will likely never be identified, simply due to the small number of people affected by them.) See: Rare Genetic Disorders.\nHowever, most diseases do not result from a single gene, but from the complex interaction of many genes (and epigenetic factors), which individually may have little or no correlation with the pathological condition. In order to identify and better understand such multifactorial relationships, Multifactor Dimensionality Reduction (MDR) methods are now being used. For more information on modeling complex genetic interactions, please see the following references. A future discussion will focus, in greater detail, on the role of neuroendocrinal signaling pathways in aging and longevity.\n- hormone replacement therapy says:\n13. May 2011 at 08:13\nAging is factor which gives ignition to so many diseases and health problems. I know its impossioble to stop this aging process………does it possible to make the aging process to its slow pace? nice post thanks for sharing it\n- Victor says:\n14. May 2011 at 03:11\nPreventing or slowing age-related decline in the CNS, and related signaling pathways, has the potential to slow, or even reverse aging processes. Many encouraging examples of the reversal of age-related decline in physiologic systems, by restoring proper signaling, can be given. Unfortunately, our present understanding of the molecular effects of various pathways is inadequate.']"	['<urn:uuid:3a42c1c9-49ce-4c55-b0a4-42accf496897>', '<urn:uuid:81be0efe-e08d-48a6-8165-cba55ded0169>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	30	94	3041
95	ways restore damaged environments maintain genetic variability within plant species importance	Restoring a fully functioning ecosystem requires not just having all species present, but also substantial variation within each species. Current restoration approaches often lack the ability to restore plant genetic diversity. To restore an ecosystem that can withstand future disturbances and resist invasions, many genotypes are needed. This genetic diversity is crucial because ecosystems work best when there's substantial variation within species, enabling important services like nutrient processing, flood mitigation, and provision of wildlife habitat.	"[""Genetic Diversity Key to Ecosystem Restoration says USU Ecologist\nThursday, Feb. 13, 2014\nEcologist Karin Kettenring, assistant professor in USU's Department of Watershed Sciences and the USU Ecology Center, says plant genetic diversity is key to ecosystem restoration.\nKettenring and colleagues published a review paper in the Feb. 2014 issue of 'Journal of Applied Ecology' exploring a ‘genetic diversity-ecosystem function’ approach to ecological restoration.\nFollowing a devastating wildfire, flood or other disturbance, land managers scramble to restore affected areas as quickly and as closely as possible to their original condition. Goals in such endeavors include selecting plant materials that can be established rapidly, that will persist for the long-term and will restore a functioning ecosystem.\nHowever, current restoration approaches often fall short of these aims, says Utah State University ecologist Karin Kettenring.\n“These approaches often lack the ability to restore plant genetic diversity,” says Kettenring, assistant professor in USU’s Department of Watershed Sciences and the USU Ecology Center. “Ecosystems work best when all species are present, but also when there’s substantial variation within a species. These factors, together, restore a fully functioning ecosystem.”\nWithout biodiversity at all levels, she says, ecosystems can’t provide the level of services — including nutrient processing, flood mitigation and provision of wildlife habitat — on which we all depend.\nKettenring and colleagues from The Ohio State University, the University of Florida and Germany’s Institute of Freshwater Ecology explore the need for a “genetic diversity-ecosystem function” approach to ecological restoration in a review article published in the Feb. 2014 issue of the Journal of Applied Ecology.\nTypical restoration approaches employed by land managers include the use of “cultivars” — plant varieties chosen or bred to perform well under restoration conditions — or by sourcing plant materials locally.\n“Each of these approaches has benefits,” Kettenring says. “Cultivars optimized for rapid plant establishment are often selected for highly disturbed areas. Local plants, because they’re well-suited to local environmental conditions, may have a better chance of persisting for the long-term.”\nBut neither of these approaches, she says, necessarily ensures ecosystem functions and services will be restored.\n“To restore an ecosystem that can withstand future disturbances and resist invasions, you need lots and lots of genotypes,” Kettenring says. “And that’s an aspect of biodiversity that’s not as obvious. Variation within individual species is as important as having many different species reintroduced.”\nShe and her colleagues advocate integrating existing restoration approaches with a genetic diversity approach that leads to effective restoration.\n“Sourcing propagules — that is, parts of plants that give rise to new plants — from the wild can be costly and logistically difficult,” Kettenring says. “Combining this approach with other approaches could yield optimal results in a more cost-effective manner.”\nThe USU scientist is exploring these ideas with her own applied research on phragmites, an invasive grass spreading rapidly throughout Great Salt Lake wetlands.\n“Developing effective restoration techniques that account for variation within a species is especially important for Great Salt Lake wetlands because they’re naturally species-poor,” Kettenring says. “We have many issues and questions to consider.”\n- “Mighty Phragmites: USU Researcher Studies Wetlands Invader,” Utah State Today\n- USU Department of Watershed Sciences\n- USU Ecology Center\n- USU Quinney College of Natural Resources\nContact: Karin Kettenring, 435-797-2546, firstname.lastname@example.org\nWriter: Mary-Ann Muffoletto, 435-797-3517, email@example.com""]"	['<urn:uuid:9dcd900b-d892-40dd-a05c-2fec4c693783>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	11	75	546
96	I'm getting into financial markets and wondering about market makers - what are their key roles in ETF trading, and how do they operate differently in interbank forex markets?	In ETF trading, market makers known as authorized participants (APs) play a crucial role in providing liquidity and maintaining efficient pricing. Currently in Europe, two major independent liquidity providers, Jane Street and Flow Traders, dominate the market. These APs are responsible for creating and redeeming ETF shares and ensuring proper market functioning, though high barriers to entry make it difficult for new players to compete. In contrast, the interbank forex market operates differently, involving major banks like Deutsche Bank, HSBC, and Citicorp trading currencies directly with each other. The interbank forex market is more formal and mainly used by large financial institutions and governments, operating across twelve time zones and thirty-two countries, with prices quoted in US dollars.	"[""Competition issues in the authorised participant (AP) market have been thrown into the spotlight once again after fixed income liquidity provider Bluefin closed its trading business in Europe last month.\nTwo independent liquidity providers enjoy a huge market share across the continent, Jane Street and Flow Traders, with Bluefin’s exit highlighting just how difficult it is to make ground in a competitive market.\nWhile the market structure in the US is different from Europe, several factors have placed the diversity of APs under threat.\nIndustry insiders have suggested the firm’s exit from Europe was down to its high barriers to entry and high costs of trading on an exchange while others pointed to advances in technology – particularly in the credit space – have eaten into its market share.\nFurthermore, the fixed income trading space has undergone a revolution over the past couple of years, with a rise in electronic portfolio trading leading to an increase their market share for investment banks since the onset of the pandemic.\nA lack of diversity among the authorised participant network can cause several issues for the ecosystem, from widening spreads to raising the chances of mispricing.\nResearch looking at the US equity ETF market paper, published by the London Business School, found 15 out of 50 APs are responsible for 98% of all creations and redemptions, with fewer larger APs – mostly banks – responsible for the bulk of the transactions.\nDan Izzo, CEO and co-founder of GHCO, which is actively trying to increase its market share in Europe, highlighted how it is an extremely difficult market to break.\n“There are only two meaningful participants at the moment,” he said. “The barriers to entry in our business are unbelievably high. It requires a massive investment of people and technology resources.\n“You just cannot break in and compete with the likes of well-established multi-billion dollar enterprises such as Flow Traders and Jane Street without a lot of effort. We are the only ones who are trying.”\nA robust market?\nThe fear when a market maker such as Bluefin exits from trading is that the subsequent gap left in the market will cause liquidity to dry up\nWhile Bluefin is the most recent example, other market makers have left in recent years and concerns were also raised when investment banks were forced to shut up shop following the Global Financial Crisis (GFC).\nHowever, Keshava Shastry, managing director and head of capital markets at DWS, said while market makers have left in the past, it has had little to no impact on the ecosystem with other APs quick to take their spot.\n“Multiple market makers have come and gone, and it has had zero impact on the product support or the investor experience,” he said. “There are a lot of work being done by the issuers to make sure that there is good support for investors, so they can trade and especially during volatile times like at the start of the pandemic.”\nShastry pointed to research conducted by the Financial Conduct Authority (FCA) published in January 2021 titled Fixed Income ETFs: Secondary market participation and resilience during times of stress which found that while the top five APs accounted for 91% of the fixed income ETF trading volume, the market share of smaller APs rose in volatile markets showing the depth of the ecosystem.\n“It shows there was a very broad and deep number of participants involved. Just because you see two or three names regularly winning business does not mean they are the only ones active, they are just winning by a small amount when investors put market in competition for getting best execution,” Shastry said.\nCiaran Fitzpatrick, managing director and head of ETF servicing, Europe, at State Street, pointed to the fact ETFs traded more than they ever did during the March 2020 drawdown, adding there is always an AP to pick up the slack if one falls out of the market.\nHe said: “Regulators have been focused on the number of APs for each product to avoid the scenario where nobody is trading on the product if they decided to walk away, but that is not the case, the ETF issuer has several APs signed up.”\nDespite this, Izzo said the way ETFs held up during March and April 2020 hides a huge fragility in the market. At the onset of 2020 volatility, Izzo said the number of market makers trading on exchange “shrunk dramatically” due to cost or having either taken on too much bad risk.\nGHCO saw its client base grow by 40% in March to June 2020 as it played a key role in calming investors as one of the few market makers showing prices on the exchange, with larger independent firms continuing to trade requests for quotes (RFQ).\n“As a product, ETFs held up well but there is huge fragility that is overlooked,” he said. “Our market share rose as a direct result of firm’s exiting the market due to exchange-traded market making at great cost. Jane Street does zero exchange-traded market making and that tells you it is just a cost.”\nRise of electronic trading\nWhile smaller firms or those with a lower market share such as Bluefin are being squeezed out of the market by high costs of trading via the exchange, this only tells half the story.\nTechnology advancements, particularly in the notoriously more complex fixed income space have also significantly changed the AP market over the past couple of years. The rise of big data, processing power and algorithms has helped to speed up and iron out pricing anomalies improving the market structure.\nInvestment banks, whose inventories of bonds fell dramatically following the GFC, have got on board with electronic trading and can now buy ETFs and trade baskets of fixed income debt efficiently, taking up market share but also increasing the diversity of APs in the market.\nBlackrock, for example, has 26 “active” APs across its entire iShares range with more covering the traditionally more complex fixed income market – 24 – compared to 21 across its equity ETFs.\nFurthermore, the trend is noticeable across BlackRock’s iShares credit ETF space, with investment banks' market share growing by five times in the past five years, from 5% in 2016 to 23% in 2021.\nSander van Nugteren, managing director, EMEA global markets team lead for iShares at BlackRock, said: “The authorised participant ecosystem covering European-domiciled iShares ETFs is robust with a wide range of participants. Several authorised participants are increasing their market share and coverage of ETFs, adding depth to the market.\n“The shift to electronic portfolio trading in credit markets has driven this change as technological evolution and broader adoption by investment banks facilitates pricing and trading of aggregated risk – an essential component of ETF primary markets.”\nShastry said technology has played a vital part in establishing a more efficient process and while market makers have been investing for a while, investment banks have ramped up their spending over the past few years.\n“The banks are getting increasingly involved. A couple of years ago some of the banks mentioned that they are getting involved in a big way and others are hiring key people and also investing in technology. It is a trend when I speak to our partners on the banks side,” he said."", 'The foreign exchange market is the largest and most liquid financial market globally, with an average traded value of $5,000,000,000. Currency trading allows you to buy and sell currency pairs at their exchange rates to gain profit which is, for some, a hobby where they invest a percentage of their income to forex, while some forex trading is where they make a living.\nThere are two levels of the forex market which are the over-the-counter (OTC) and Interbank forex market. Interbank market usually consists of major players (traders), or you can say “more formal.” On the other hand, OTC is less formal and much more popular with individual traders, given you can trade any time of the week with this.\nInterbank FX Market\nWhat is the Interbank Forex market? The term “interbank” refers to transactions and balances among banks and financial institutions, regardless of country, kept entirely in an electronic database. These systems make transferring currencies instantly and fairly simple between banks without the need for multiple intermediaries. It is essential to understand what these systems do and why they are important for your financial well-being. The interbank market protects investors, companies, and consumers against systemic risks and further facilitates financial services by making trading more transparent and convenient.\nThe international interbank market is the largest in the world for financial instruments. It consists of trading across twelve time zones and thirty-two countries. The prices for these instruments are quoted in US dollars and vary by dealer, the underlying asset, and other factors. Because the market is global, forex traders may execute orders from any country at any time. The primary participants in the exchange include major banks, broker-dealers, trust companies, and financial institutions such as banks, brokerage firms, investment firms, and subsidiaries of these institutions.\nThe primary factors that determine interbank forex pricing include the Federal Reserve’s policy on payments between banks, changes in funding practices such as short-term funding programs and longer-term bilateral agreements between central banks, as well as changes in bank industry regulations. Following the Great Recession, standard theories about bank pricing were challenged – particularly, how such broad-based factors as capacity limits, capital adequacy, and profitability can influence interbank pricing across various currency pairs globally and in a practical way.\nWhen you are currency trading in an interbank broker, your profit or loss depends upon whether your broker has a good reputation and sources of liquidity (i.e., banks with large money reserves that they can use to lend directly to borrowers). A good reputation often results in lower transaction fees and more favorable pricing for borrowers.\nInterbank Bid-Ask Spread\nThe interbank bid-ask spread refers to the difference in the prices that banks are willing to pay for US dollars and other currencies. This spread reflects the actual cost of financing for lenders and borrowers across the financial system. Lenders use it to help determine how much credit should be available in an environment of tight liquidity. By establishing a consistent price for financial instruments for most financial institutions, the interbank bid/ask spread fair and efficient global market pricing for financial instruments available for trading across financial markets.\nBid-ask spreads represent what banks charge each other to exchange dollars for other currencies. For example, if the euro is being traded at USD 1.072, then the largest bank in France will have to pay their counterpart at least USD 1.072 to take their dollar and give it to them electronically. The spread represents how much each bank pays each other for facilitating the trade, both in terms of overhead costs (time, staffing, etc.) and revenue (checking and savings accounts vs. investments/trading assets/etc.).\nInterbank participants are often the world’s largest banks, such as Deutsche Bank in Germany, HSBC in Asia, Citicorp, etc. The interbank market is also used by the government and large trade firms. Finally, most interbank brokers only accept major currencies like the US dollar, Canadian dollar, and Japanese yen.\nThe interbank FX market is a significant and liquid financial market that giant financial institutions and the government use. The interbank is where the market is more situated to people who prefer a forex market that is “more formal” since the interbank is a subset of the forex market where big financial markets and the government itself use to trade currencies.']"	['<urn:uuid:82bc894b-049b-4f01-83e2-62bd126bc34a>', '<urn:uuid:ac1c0d73-ec91-40e0-b3cc-89190b16c9c5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T15:30:12.136021	29	118	1936
97	occupational therapist here need tips proper finger placement standard scissors	For standard scissors, the thumb, index and middle fingers (tripod fingers) should go through the scissor holes. Once a child has mastered basic cutting, they should put the thumb in one hole and the middle finger through the other hole, with the index finger sitting on the outside to stabilize and control the scissors. The ring and little fingers should be tucked away, which develops ulnar stability and allows the tripod fingers to move more accurately.	"['Viewing this page on your device?\nPlease adjust your settings to enable images!\nI use small photos to illustrate the information and activities that I share, and you will have a much better experience on this website if you can view the images.\nScissor cutting should play a big part in preschool and kindergarten activities, because learning how to use scissors correctly can also help with developing pencil control skills.\nCutting with scissors is considered to be a pre-writing skill.\nUse the quick links below to jump to answers to your questions on this page.\nHave a look at this pic of a mature pencil grip and you can see how these tripod fingers (thumb, index and middle fingers) are working well together to control the pencil.\nCutting with a proper scissor grasp, will give these fingers lots of practice in working together, and will strengthen your child’s hand muscles.\nThis will help to improve fine motor skills and to develop the correct pencil grasp and pencil control for good handwriting.\nCutting out on a line also helps to strengthen visual-motor skills, which may help improve handwriting.\nBecause we want the tripod fingers to get lots of practice (in preparation for holding the pencil), the thumb, index and middle fingers need to be through the scissor holes. However, the type and size of the holes affects the exact positioning of the fingers.\nIf your child is just learning to cut, or if your child has weak hands and poor fine motor control, then look for scissors that have a large oval hole that the index and middle fingers can both fit through.\nOnce your child has gained some mastery of scissor cutting, you can increase the fine motor challenge by using scissors that have just two small round holes. Have your child put the thumb in one hole and the middle finger through the other hole. The index finger then needs to sit on the outside of the scissors to help stabilise the scissors and give more control.\nHolding the scissors in this way enables the tripod fingers to work together well.\nYou can help your child keep the ring and little fingers tucked away by holding a little piece of paper under them. Keeping the ring and little fingers tucked away develops ""ulnar stability"" , which basically gives the side of the hand more stability, which frees the tripod fingers up to move more accurately.\nLeft-handed children should always cut with their left hand, using left-handed scissors.\nLeft handed scissors are vital to enabling them to cut successfully. This\nis important as it helps them to make their dominant hand strong and\nA left handed child who cuts with the right hand is missing out on strengthening and refining the hand muscles needed for good pencil control.\nSo-called “ambidextrous scissors” are a marketing ploy – it is not the comfort of the handles that makes a difference, it is the way the blades are fixed.\nLook at the picture above: the scissors on the left are for lefties, and you can see how the blades are attached differently from those on the right-handed scissors. This enables the child to see the line effectively when cutting out.\nWhen a left-handed child cuts with right-handed scissors in the left hand, the child usually ends up holding the paper or the scissors awkwardly in order to cut on the line, and this may result in untidy cutting.\nIf you are trying to get to grips with helping your left handed child, then visit this site designed especially to help left-handers (opens in a new window).\nOr check out the range of scissors that also come in left handed versions, from PFOT.\n""Cutting out"" involves two completely separate skills:\nIt is best to teach these two skills separately, and help your child\nto get the hang of opening and closing scissors before attempting any\n""proper"" cutting out.\nHead over to my detailed page to teach your child how to use scissors. Lots of photos and tips abound!\nHelp your child master the grasp-release motion and gain confidence with snipping before moving on to cutting on a line.\nLots of tips and photos to help you!\nHead over to my Cutting Activities page to get some free scissor cutting ideas and a free cutting template download!\nAll my scissor skills information and tips in an easily accessible e-book download!\nComes with a Fun Cutting Template containing cutting activities not found on my site!\nThese awesome cutting printables will give your child lots of opportunity to master scissor cutting while creating lovely end-products!\nPFOT stock a range of self opening, adapted and other special resources that are designed to help kids who struggle with scissor cutting. Check out their resources here!\nExclusive Offer to OT Mom Readers\nUse the coupon code OTmom\nand get 15% off your order of $35 or more at PFOT\nThis is an affiliate link and if you purchase something through my link,\nI will earn a small commission which helps to support my site!\nThank you for visiting my site!\nMitchell, A. W., Hampton, C., Hanks, M., Miller, C., & Ray, N. (2012). Influence of task and tool characteristics on scissor skills in typical adults. American Journal of Occupational Therapy, 66, e89–e97. https://dx.doi.org/10.5014/ajot.2012.004135\nRatcliffe, I., Concha, M. & Franzsen, D. (2007). Analysis of cutting skills in four and six year olds attending nursery schools in Johannesburg. South African Journal of Occupational Therapy. 37. 4-9.\nRatcliffe, I., Franzsen, D., & Bischof, F. (2010). Development of a scissors skills programme for grade 0 children in South Africa - A pilot study. South African Journal of Occupational Therapy. 41. 24-31.\nIf this page was helpful, please share it with your friends!\nDidn\'t find what you were looking for? Try a search of my site!']"	['<urn:uuid:0122b4cd-8aae-4c0f-9bc0-512db6f104e5>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T15:30:12.136021	10	76	971
98	What is the relationship between the concept of 'national pride' in post-war Japan and Hitler's vision for Berlin - did both involve glorifying their respective nations through physical monuments?	Yes, both involved physical manifestations of national pride, but in different ways. In Japan, this is seen through the Yasukuni Shrine and Yushukan Hall, which honor war leaders and showcase exhibitions about the 'glorious Japanese race.' In Berlin, Hitler's vision was expressed through his plans for 'Germania,' which included monumental buildings and a triumph arch 2.5 times larger than the one in Paris, designed to dwarf other European capitals and remain impressive even as ruins for thousands of years.	"['Resurgence of the Right and Japan\'s World War II Accountability\nby Chung Chinsung\nAt major intersections throughout Japan one can see the rightist groups with patriotic banners across their chests. They use relentless microphone-amplified diatribes to drive home their message as they stand atop their black vans. Their posters declaring ""Worried about the Nation"" has become a fixture of the Japanese urban landscape. After the Second World War such figures were considered comical. They seemed but hot-blooded nationalists; marginal crackpots far from the norm and with minimal influence. But before we knew it, the vague sense that perhaps their ravings contained a kernel of truth spread through Japanese society. Ironically, the far right has gained considerable power in Japanese society by insistently holding up the ideal of protecting the imperial system at the very time that civil society has achieved remarkable maturity in Asian nations.\nModernization was carried out under the auspices of the Japanese government in tandem with the war preparations so central to pre-war militarism. The trend throughout the world between 1910 and 1930 was towards socialism and labor activism. Yet such movements were ruthlessly suppressed by the government in Japan. Along the way, the rightist movement that had emerged within civil society was absorbed directly into the government.\nAfter the Second World War, democratic reforms implemented during the occupation led to the revival of the left. After a brief period of reform in the immediate post-war period, the previously progressive political course went into reverse. The outbreak of the Korean War and the ensuing conservative mood offered the opportunity for the right to retake the stage. Although the rightists came back to life, they often lacked credibility. Like many Japanese social movements, their ideological and intellectual foundation was weak.\nSocial activists and the student movement were transformed into the political presence known as the New Left after the Liberal Democratic Party came to power in 1955. Japan was swept by a wave of social action in the sixties and seventies. Notable among them were the citizen\'s movement, the anti-Vietnam war protest movement, the Women\'s Liberation Movement, and others. Yet they soon faded from prominence, proving how short-lived citizen\'s movements can be. Civil society in post-war Japan developed on the two legs of economic growth and political stability. In recent years the central movements that have emerged are concerned primarily with the pursuit of quality of life. Such powerful groups as the food cooperative movement, consumer organizations and the pre-school education movement are distinctive in their ability to draw members from both the left and the right. These movements promote an improved and healthier life and consequently have a broad appeal. Consequently, the discourse of civil society in Japan has lost its ideological and theoretical dimension.\nThis situation started to shift again in the early 1990s when a series of international controversies rocked Japan. Korea led the nations of East Asia victimized by Japan during World War II, demanding recognition of, and settlement for, Japan\'s historical crimes. For example, the campaign to receive apology and reparations for the enslavement of Koreans as military comfort women grew into a Pan-Asian movement. Japan\'s ethical standing was seriously shaken by these charges and the climate at home rapidly became more supportive of domestic social movements. Progressive political movements that had just managed to scrape by as small-scale study groups before now teamed up with activist citizens\' groups from other Asian nations. They demanded a full accounting of Japan\'s wartime responsibility. The social engagement thus generated was unique for post-war Japanese society in that women\'s groups, organizations of ethnic Koreans, peace groups, intellectuals and lawyers all joined in the campaign. They were tied together by a common thread: criticism of the imperial system.\nConservative groups launched a determined counter attack. With vast resources at their disposal, the rightists were able to bring together politicians, intellectuals and youths for their purposes. These neo-nationalist groups held that Japan was fully justified in its actions during the war. The right denigrated the citizens\' groups and victims who came forth from other nations in East Asia and branded as treasonous the statements of domestic critics. Japanese political discourse lurched to the right. Although a healthy Japanese society had developed in the post-war period, ultimately, the lack of ideological foundations for the new order made possible the resurgence of the right.\nIn the early nineties the forces of the left and right counterbalanced each other in Japanese society. Recently, the balance has tipped increasingly in favor of the right. The rightists sharpened their rhetorical weapons in battles to suppress the demands made by former comfort women. Rightists also demonstrated their prowess in the battle over history textbooks. In the 1980s and early 1990s, the Japanese government had acknowledged certain controversial actions in World War II. Revisions appeared in government-approved history textbooks. In 1994-95, however, textbook policy took a turn to the right.\nThe government has denied Japanese responsibility for the Second World War in recent editions of some history textbooks and restored nationalist content. In addition, the obligatory singing of the newly approved national anthem ""Kimigayo"" in praise of the emperor, and raising the Hinomaru flag are required by law and rigorously enforced in formal events at many schools. A string of new laws, starting with the Emergency Security Law, has been implemented and serious debate concerning revision of Japan\'s Peace Constitution has ensued. As a result, rightist social movements have spread their wings and flown to every corner of the country. They carry out campaigns such as the demand for the revision of history textbooks and violently attack Japan\'s tradition of peaceful engagement with the world. Starting with the prime minister, a long line of officials now pays public visits to the Shinto holy place Yasukuni Shrine. The leaders and martyrs associated with Japan\'s conquest of Asia during the Pacific War are interred and honored there. Adjacent to the Yasukuni Shrine is the Yushukan Hall, a museum featuring exhibitions for the education of Japanese visitors about the ""glorious Japanese race"" and the new educational system.\nAt the same time, many citizens\' groups that demand accountability for Japan\'s wartime actions have turned their backs on issues such as the punishment of those responsible for the crimes of World War II and the legitimacy of the imperial system. Participants in citizens\' movements nowadays are often passive figures who have given up hope of successfully combating the rightists, but continue to find a certain personal meaning in ethical activity.\nThe long-term stagnation of the Japanese economy has nourished a mentality receptive to the arguments offered by the rightists since the 1990s. Japan has witnessed the demise of its legendary system of life-time employment. Foreign workers have entered Japan in growing numbers. The social disruptions resulting from such cataclysmic changes have proven difficult for most Japanese to absorb. At the same time, neighboring China is aiming the heavy cannons of diplomatic warfare at Japan over unresolved historical issues. The regional configuration of East Asia today demands a new role of Japan. Political conservatism and economic development can no longer simply work hand and hand without friction. Similarly, social movements in Japan cannot remain secondary activities subordinate to larger political priorities.\nIt is not immediately obvious whether the groups that once took such pride in their power such as the food cooperative movement or volunteer organizations will recognize their complicity in the swing to the right, or whether it is possible to have critical intellectuals in Japan.\nThis article appeared in Hankuk Ilbo on May 3, 2004. Chung Chinsung is Professor of Sociology, Seoul National University. In August 2004 she was appointed to a post at the United Nations Human Rights Commission.\nTranslated from Korean for Japan Focus by Emanuel Pastreich. Emanuel Pastreich teaches in the Department of East Asian Languages and Cultures at the University of Ilinois. His recent publications include work on the internet and globalization.', 'There are traces of the Third Reich everywhere in Berlin: buildings, memorial sites and of course the scars of the war.\nDriving south with the S2 while looking out of the window of the train you can spot one of the strangest remains of the twelve years of Nazi rule in Berlin: The heavy load-bearing body.\nHitler had big plans for Berlin. He wanted to remodel the city into ‘Germania’, the world capitol.\nHe wanted to dwarf London and Paris especially, make it only comparable to Babylon or Rome.\nHe said: „Nowadays we look back at the ruins of old Rome and are still impressed by them!“\nThe same should be said about the buildings in Berlin, even in a thousand years as ruins they should still be impressive.\nHe had several ideas on how the new Germania should look, up until the end of the war he still discussed with his architect Albert Speer details of the realization.\nAt the heart of their plans was a long avenue, going from north to south with monumental buildings along the street.\nAs part of the big plans they also planned several ring streets around the center. Where the second ring and the north-south-axis met Hitler wanted a triumph arch, similar to the one in Paris – he just wanted it to be 2,5 times the size.\nBut Speer was concerned about the stability of the ground. As you already know from a previous entry the Berlin underground can be quite unstable.\nTo test the ground the company Dyckerhoff & Widmann constructed a big cylinder that went more than 18 meters into the ground. On top of this they put a wider cylinder made out of concrete to apply enough pressure to the ground. In the end the whole construction weighed around 12.650 tons, applying a pressure of 12,65 kg per cm2!\nEven during the construction the Degebo (Deutsche Gesellschaft für Bodenmechanik) started to measure the effect on the ground.\nSpeer said, that the ground was only stable enough if the heavy load-bearing body wouldn’t sink into the ground more than 6cm.\nBut before the measuring stopped in 1944 it had already sunken into the ground more than 19cm.\nWhat really prevented the realization of ‘Germania’ in the end was of course not the unstable ground, but the loss of the war.\nIn the post war time nobody really bothered to take down the big chunk of concrete. It would have cost quite a bit of money and it was close to a residential area, detonating it would have been complicated.\nThe Degebo continued to measure the ground until the end of the 70’s, after that the area was mostly ignored.\nBut since it is basically the only thing left of the ‘Germania’ – plans of the Nazis in 1995 it was declared a protected monument. 5 years ago it was renovated, information for visitors was added and the whole place was opened up to the public. It can be visited between April and the end of October.\nFor more information check out the website: http://www.schwerbelastungskoerper.de/\nAt the end of the war, at a point where Hitler was already living in the bunker, he came out to meet with Speer. Standing beside the big model that Speer had built for Hitler of ‘Germania’ they talked for the last time. Leaning over the model Hitler said:\n“Well, my dear Speer, there is at least one good thing about all those bombings: Once we’ve won the war we won’t have to tear down so much ourselves!”\nOne week later Hitler committed suicide.']"	['<urn:uuid:b5ca95ca-e0c7-41ba-86f1-5f51af4cc3dd>', '<urn:uuid:07880145-1f72-428a-af5b-0bc30c11d9ff>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T15:30:12.136021	29	79	1915
99	ladder safety guidelines fall risk stats	According to CPSC, nearly 15,000 ER-treated injuries occur during holiday decorating, with over 40% involving falls and about three falls being fatal per season. For safe ladder use, NSC guidelines recommend placing straight/extension ladders one foot from the wall for every four feet of height, maintaining three points of contact, keeping hips between side rails, and using slip-resistant feet and shoes. Additionally, users should face the ladder while climbing, grip rungs not rails, and descend immediately in inclement weather.	['‘Tis the season for merriment—and we hate to be the ones to mention it, but it also can be the season with the potential for increased liability. Now’s the time to make your list and check it twice to ensure that your business liability insurance is up to date for some common potential hazards that can occur during the holiday season.\nHoliday Hazard 1: Fires\nMost statistics about holiday fires revolve around home fires, but that’s not to say that the risks aren’t equally real in a business setting—and involve many of the same behaviors that put your house at risk. Whether you want to enjoy the fresh smell and ambiance of a live tree, or your employees are plugging in multiple extension cords as they decorate their workspace, you’re playing with fire if you’re not extra attentive to fire risks.\nTalk to your landlord about precautions for safe decorating, which typically involves keeping live trees out of the workplace and insisting on “fake” candles only. And, check your fire insurance to make sure you’re covered for full replacement value for all your costly equipment. It’s also a smart idea to double-check your back-up storage to ensure that important files aren’t lost should a fire occur.\nHoliday Hazard 2: Holiday parties\nIt’s hard to believe that festive merriment can be a hazard, but in the era of #MeToo, firms are warier than ever of socializing—especially when alcohol is involved. In fact, fewer companies are holding holiday parties, with one survey finding the lowest number since the recession of 2009. Those that do need to be wary of potential risks and do what they can to mitigate them.\nConsider holding your event in the day for more control over revelry. If you do decide to host an evening soiree, limit the amount of alcohol that is offered to each guest (if any). Trained servers can help make sure imbibers are legally supposed to drink and that no one is being overserved, to the best of their knowledge. You might want to look into liquor liability insurance, although a one-time event is likely to fall under the umbrella of “social host liquor liability,” which means that your general business liability insurance policy should cover an alcohol-related incident. Of course, that protection won’t extend to any negligence. And note that social host liquor liability laws vary by state.\nHoliday Hazard 3: Icy conditions\nSometimes you’ll find that the weather outside is “frightful,” this time of year, as the beloved song says. When ice and snow accumulate, your business could be at risk from clients, patrons or employees slipping in the parking lot or on icy stairs or front walks or even sliding as they enter, as the floors become soggy from people tracking in rain or mush.\nMake sure that you routinely check parking lots and front areas and keep walks and steps clear. Use moisture-wicking, non-slip mats inside your doors and try to keep up on drying the floors. Post hazard signs to warn visitors of wet, potentially slippery conditions. And, of course, make sure you have adequate commercial umbrella insurance in place.\nHoliday Hazard 4: Decorating mishaps\nDecking the halls is fun and festive, but can also be the cause of accidents, from falling off ladders while hanging lights to cuts, bruises or scrapes from sharp decor. In fact, the U.S. Consumer Product Safety Commission (CPSC) reports an average of nearly 15,000 ER-treated injuries related to decorating each holiday season. Of those, more than 40% involve falls, and about three of the falls per season are fatal.\nWhen using a ladder, follow guidelines offered by the National Safety Council (NCS) which offers suggestions on choosing the right ladder for the job and making sure you have safe surroundings before climbing. It’s also smart to have a spotter or other bystander helping you out. And in general, make sure that you take care when decorating, including wearing appropriate clothing, such as pants and shirts that cover your skin, gloves, eye covering if needed and closed-toe, slip-proof shoes. As we head to the festive season, now is the time to review your insurance policies to double-check that everything is in place. Contact O’Kane and Tegay Insurance Brokers today to discuss any questions you have, and ensure that your holidays stay merry and bright.', 'Sturdy ladders always should be used when putting up decorations and lights (never use chairs or other objects not intended for this purpose). The National Safety Council (NSC) also advises using ladders “with slip-resistant feet and wearing clean, dry and slip-resistant shoes when climbing a ladder.” Other ladder tips from the NSC:\n- A straight or extension ladder should be placed one foot away from the surface it rests against for every four feet of ladder height.\n- When you climb, always face the ladder and grip the rungs to climb – not the side rails.\n- Always keep three points of contact on the ladder, whether two hands and one foot, or two feet and one hand.\n- When climbing, keep your hips between the side rails and do not lean too far or overreach. Re-position the ladder closer to the work instead.\n- When using ladders outdoors, get down immediately if high winds, rain, snow or other inclement weather begins. Winds can blow you off the ladder and rain or snow can make both the rungs and the ground slippery.\nAlleviate stress by taking time each day to rest, reflect, listen to soothing music and do yoga or other exercise that helps you feel peaceful. It’s also wise to limit the number of errands on your daily to-do list.\nFires frequently are caused by malfunctioning heating equipment, particularly space heaters. Be sure to keep your furnace in top working condition (Standard Heating can provide a thorough inspection – just give us a call!) and follow manufacturer instructions if you use a space heater. Finally, never use candles as a heat source, and pay attention to where candles and holiday decorations are placed. Here are some additional fire-prevention tips from the National Safety Council:\n- Never leave burning candles unattended or sleep in a room with a lit candle\n- Keep candles out of reach of children\n- Make sure candles are on stable surfaces\n- Don’t burn candles near trees, curtains or any other flammable items\n- Don’t burn trees, wreaths or wrapping paper in the fireplace\nElectrical outlets should not be overloaded. Notes the U.S. Commerce Department website: “Connect no more then three strands of mini lights on any one extension cord. Extension cords should be placed against the wall to avoid tripping hazards, and around the top portions of the door opening.”\nYour health is influenced by how you handle food. The U.S. Department of Health and Human Services provides some holiday food safety tips. Here are a few:\n- Do not rinse raw meat and poultry before cooking\n- Use a food thermometer to make sure meat is cooked to a safe temperature\n- Refrigerate food within two hours\n- Most leftovers are safe for four days in the refrigerator\n- Bring sauces, soups and gravies to a rolling boil when reheating\n- Wash your hands frequently when handling food\nWe hope you have found these safety tips informative and helpful. Everyone at Standard Heating wishes you and your family a healthy, happy and safe holiday season and a wonderful new year!']	['<urn:uuid:df45e40c-e426-4e27-8c22-80e81e60f1d1>', '<urn:uuid:20c76b69-ff75-4240-9e25-105b960c2689>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T15:30:12.136021	6	79	1237
100	Can you explain how simulations help students learn about springs and gravity, and what unexpected things happen to orbits near black holes?	Simulations like PhET's Masses and Springs allow students to explore spring motion, kinetic and potential energy, and how springs behave under different gravitational conditions on various planets. The simulation helps visualize these concepts through interactive experiments. As for orbits near black holes, they exhibit bizarre behaviors - an object can circle multiple times very close to the black hole before either escaping back outward or falling in, with the difference between escape and capture being extremely thin. This delicate balance is due to the strong gravitational effects in highly curved space-time near black holes.	"['the Physics Education Technology Project\nthe UTeach - University of Texas\nThis middle school lesson plan blends a classroom experiment with the interactive PhET simulation Masses and Springs. Students design and conduct an experiment to test their predictions about the behavior of springs with masses attached. The simulation is used to explore kinetic and potential energy of spring motion and to visualize how the spring moves on different planets with varied gravitational constants. The lesson provides extensive background information for both educators and learners, scripted teacher discussion, student guides, and assessment materials with answer keys.\nThe ""Masses & Springs"" simulation (which is required to complete this activity) is available from PhET at: Masses & Springs.\nThis lesson is part of the PhET (Physics Education Technology Project), a large collection of free interactive science simulations.\n3-5: 1B/E1. Scientific investigations may take many different forms, including observing what things are like or what is happening somewhere, collecting specimens for analysis, and doing experiments.\n3-5: 1B/E2b. One reason for following directions carefully and for keeping records of one\'s work is to provide information on what might have caused differences in investigations.\n6-8: 1B/M1b. Scientific investigations usually involve the collection of relevant data, the use of logical reasoning, and the application of imagination in devising hypotheses and explanations to make sense of the collected data.\n6-8: 1B/M2ab. If more than one variable changes at the same time in an experiment, the outcome of the experiment may not be clearly attributable to any one variable. It may not always be possible to prevent outside variables from influencing an investigation (or even to identify all of the variables).\n4. The Physical Setting\n4E. Energy Transformations\n6-8: 4E/M1. Whenever energy appears in one place, it must have disappeared from another. Whenever energy is lost from somewhere, it must have gone somewhere else. Sometimes when energy appears to be lost, it actually has been transferred to a system that is so large that the effect of the transferred energy is imperceptible.\n6-8: 4E/M4. Energy appears in different forms and can be transformed within a system. Motion energy is associated with the speed of an object. Thermal energy is associated with the temperature of an object. Gravitational energy is associated with the height of an object above a reference point. Elastic energy is associated with the stretching or compressing of an elastic object. Chemical energy is associated with the composition of a substance. Electrical energy is associated with an electric current in a circuit. Light energy is associated with the frequency of electromagnetic waves.\n3-5: 4F/E1bc. The greater the force is, the greater the change in motion will be. The more massive an object is, the less effect a given force will have.\n4G. Forces of Nature\n3-5: 4G/E1. The earth\'s gravity pulls any object on or near the earth toward it without touching it.\n9. The Mathematical World\n9B. Symbolic Relationships\n3-5: 9B/E2. Tables and graphs can show how values of one quantity are related to values of another.\n11. Common Themes\n3-5: 11B/E3. A model of something is similar to, but not exactly like, the thing being modeled. Some models are physically similar to what they are representing, but others are not.\n3-5: 11B/E4. Models are very useful for communicating ideas about objects, events, and processes. When using a model to communicate about something, it is important to keep in mind how it is different from the thing being modeled.\n6-8: 11B/M4. Simulations are often useful in modeling events and processes.\n12. Habits of Mind\n12C. Manipulation and Observation\n6-8: 12C/M3. Make accurate measurements of length, volume, weight, elapsed time, rates, and temperature by using appropriate devices.\n12E. Critical-Response Skills\n3-5: 12E/E3. Seek reasons for believing something rather than just claiming ""Everybody knows that…"" or ""I just know"" and discount such claims when made by others.\n6-8: 12E/M5b. Notice and criticize the reasoning in arguments in which the claims are not consistent with the evidence given.\nCommon Core State Standards for Mathematics Alignments\nStandards for Mathematical Practice (K-12)\nMP.3 Construct viable arguments and critique the reasoning of others.\nMeasurement and Data (K-5)\nRepresent and interpret data. (1-5)\n3.MD.4 Generate measurement data by measuring lengths using rulers marked with halves and fourths of an inch. Show the data by making a line plot, where the horizontal scale is marked off in appropriate units— whole numbers, halves, or quarters.\nRatios and Proportional Relationships (6-7)\nUnderstand ratio concepts and use ratio reasoning to solve\n6.RP.1 Understand the concept of a ratio and use ratio language to describe a ratio relationship between two quantities.\nAnalyze proportional relationships and use them to solve real-world\nand mathematical problems. (7)\n7.RP.2.b Identify the constant of proportionality (unit rate) in tables, graphs, equations, diagrams, and verbal descriptions of proportional relationships.\nCommon Core State Reading Standards for Literacy in Science and Technical Subjects 6—12\nKey Ideas and Details (6-12)\nRST.6-8.3 Follow precisely a multistep procedure when carrying out experiments, taking measurements, or performing technical tasks.\nCommon Core State Writing Standards for Literacy in History/Social Studies, Science, and Technical Subjects 6—12\nText Types and Purposes (6-12)\n2. Write informative/explanatory texts, including the narration of historical events, scientific procedures/ experiments, or technical processes. (WHST.6-8.2)\nResearch to Build and Present Knowledge (6-12)\nWHST.6-8.7 Conduct short research projects to answer a question (including a self-generated question), drawing on several sources and generating additional related, focused questions that allow for multiple avenues of exploration.\nNSES Content Standards\nCon.B: Physical Science\n5-8: Motion & Forces\n9-12: Motions & Forces\n9-12: Conservation of Energy & Increase in Disorder\nUTeach - University of Texas. (2012, June 22). PhET Teacher Activities: Experimental Design with Forces. Retrieved May 30, 2016, from Physics Education Technology Project: http://phet.colorado.edu/en/contributions/view/3543\nUTeach - University of Texas. PhET Teacher Activities: Experimental Design with Forces. Boulder: Physics Education Technology Project, June 22, 2012. http://phet.colorado.edu/en/contributions/view/3543 (accessed 30 May 2016).\nUTeach - University of Texas. PhET Teacher Activities: Experimental Design with Forces. Boulder: Physics Education Technology Project, 2012. 22 June 2012. 30 May 2016 <http://phet.colorado.edu/en/contributions/view/3543>.\n%0 Electronic Source %A UTeach - University of Texas, %D June 22, 2012 %T PhET Teacher Activities: Experimental Design with Forces %I Physics Education Technology Project %V 2016 %N 30 May 2016 %8 June 22, 2012 %9 application/pdf %U http://phet.colorado.edu/en/contributions/view/3543\nDisclaimer: ComPADRE offers citation styles as a guide only. We cannot offer interpretations about citations as this is an automated procedure. Please refer to the style manuals in the Citation Source Information area for clarifications.', 'The Anomalous Advance of the Perihelion of Mercury\nOften in discussions of Mercury’s perihelion advance, the effect is shown greatly exaggerated. Sometimes even the orbit is shown much more eccentric than it really is. Before we dive in, let’s take a moment to get some perspective on the true shape of the orbit and appreciate the scale of the solar system.\nAll orbits are ellipses, but most planetary orbits are close enough to circular that it can be hard to tell at a glance. Mercury is the biggest exception with an eccentricity of 0.21. (Or Pluto at 0.25). What’s interesting, and what this post is all about, is that Mercury’s orbit is not stationary. The ellipse slowly shifts around, it’s perihelion point advancing by about 574 arcseconds (or 0.159 degrees) per century. Much of this (531 arcseconds) can be explained by the perturbations from the other planets. However, the remaining 43 arcseconds per century are “anomalous”, unaccounted for by Newtonian mechanics.\nThe puzzle of this additional observed precession was first noted by LeVerrier in 1859. Following the successful prediction of the existence and location of Neptune from similar perturbations on Uranus’ orbit, LeVerrier proposed that Mercury’s orbital precession could be caused by another undiscovered planet inside of Mercury’s orbit, which he named Vulcan. Astronomers searched for this missing inner planet during solar eclipses. Of course, Vulcan was never found…\nThe mystery was not solved until Einstein began developing his general theory of relativity. When he applied his general relativistic equations to the problem in 1915, he found that they exactly predicted the additional 43 arcseconds per century. He considered this one of his greatest achievements.\nImagine my joy at the recognition of the feasibility of general covariance and at the result that the equations correctly yield the perihelion motion of Mercury. I was beside myself for several days in joyous excitement.\nSo, why does it happen? Is there an extra energy involved? (Spoiler: Yes, in a way!)\nFor an elliptical orbit, we can think of the orbital motion as being made of two parts: an oscillation in azimuth (angle around the Sun), and an oscillation radially (in and out). In Newtonian mechanics, the periods of these two oscillations are exactly equal. That is, each time the planet completes one cycle around the Sun, it also exactly completes one cycle radially in and out. Therefore in Newtonian mechanics there is no precession of the orbit (besides that which is caused by the influences of the other planets).\nIn general relativity this is no longer true. The periods of the two oscillations are different! To see why, it is easiest to examine the shape of the “effective potential”, which you can think of as being just like the potential energy well around a spherical mass, except also taking into account the angular momentum of the orbiting body.\nIf you are unfamiliar with the concept of a potential energy well, imagine a rolling landscape of hills and valleys. If you place a ball at the bottom of a valley, it will just sit there. It’s in a stable equilibrium. On the other hand, the top of a hill is an unstable equilibrium. If you displace the ball slightly from the top of the hill and let it go, it will continue to roll down, exchanging gravitational potential energy for kinetic energy as it drops in height. Finally, if you have some bowl-shaped valley and drop the ball from rest at some initial height, then it will roll down to the bottom, and (ignoring friction) keep going, rolling back up the other side until it reaches its former height, and then roll back again, oscillating back and forth indefinitely.\nBecause the gravitational potential energy near Earth’s surface is simply proportional to your altitude, if you make a plot of the potential energy of a ball in this landscape as a function of its position, it will be exactly the shape of the landscape. This form of modelling of the potential is extremely useful. In general, we can use “potential energy wells” as a way to understand motions of objects subjected to different kinds of attractive and repulsive forces, from balls rolling down hills to the vibrations of atoms bound together in molecules.\nAn elliptical orbit is an oscillation in an effective potential well, as well. There is a hill to climb as you move outward due to the gravitational attraction of the star, and there is also a hill to climb as you move inward! This is because the sideways motion is associated with angular momentum, and angular momentum acts like a repulsive centrifugal force. By conservation of angular momentum, the sideways velocity increases as your orbit swings inward, increasing the centrifugal repulsion and making it harder to approach the center. If the angular momentum is large enough, then the inward fall can be halted, and you swing back out again, making an orbit!\nSo if we can compute the effective potential well for a planet orbiting the Sun, we can figure out some things about its motion. The anomalous precession of an orbit can also be understood this way, by comparing the motion from the Newtonian potential with what happens in the general relativistic version of the potential. Let’s try it!\nThe goal here is to try to understand how things work conceptually, and I will try to emphasize those concepts (especially physical concepts) as we go along. The hard truth is that much of it will still require going through the math in order to access it, but I’ll do my best to turn that math into something visualizable and comprehensible.\nIn Newtonian mechanics, the gravitational potential energy of a mass m a distance r around a spherical mass M is\nThis describes a simple curve which plummets downward indefinitely as you move to smaller radii. As it should. If you drop something near a massive object and give it no sideways motion (no angular momentum), then it will fall straight into it.\nIf we instead give the object some angular momentum L, then the potential is modified:\nwhere μ is the reduced mass, which for a planet orbiting the Sun (m << M), reduces to approximately m.\nLet’s see what this looks like for Mercury:\nAgain the way to think about this is that the potential describes a landscape that a ball will frictionlessly roll across. To visualize that, I added the large red dot to represent Mercury’s position as a function of time. The acceleration is determined by the slope of the potential, and I plotted the motion with 2 days per frame. With 44 frames in all, this traces out one complete period of Mercury’s orbit in the radial motion. I also added a horizontal line to show the total energy of Mercury (its gravitational potential energy + kinetic), which is constant along the orbit. Where this line is above the effective potential defines the range of distances from the Sun that Mercury’s orbit will cover. The vertical lines represent the extremes (Mercury’s perihelion and aphelion distances).\nWhy is the Newtonian effective potential shaped this way?\nBecause of Mercury’s amount of angular momentum, the shape of the effective potential that it sees near the Sun is a valley with hills on either side. The hill at large radii is due to the Sun’s gravitational attraction, while the hill at small radii is due to centrifugal repulsion. Even though the gravitational force grows stronger at closer distances, for an orbit the centrifugal force gets stronger more quickly, due to the conservation of angular momentum which increases your sideways speed.\nA useful concept to keep in mind here is that the period of the radial oscillation depends on the curvature of the potential well. If the well opens up more sharply, then the average acceleration is greater, and the period is shorter.\nAnd here’s one other useful trick. If the amplitude of the oscillation is not too large (does not reach too far away from the minimum of the well), then we can approximate the well as a parabola around the minimum. Then for a parabolic well the motion is described very simply as a simple harmonic oscillator. For Mercury this approximation isn’t particularly good (the well is noticeably asymmetric over the region Mercury covers), but it’s not terrible either.\nFor a simple harmonic oscillator, the frequency is given by\nwhich for the radial motion leads to\nThe frequency for the angular oscillation on the other hand is given by\nWhich leads to the exact same expression:\nSo we see in Newtonian mechanics the periods are exactly equal and there is no anomalous precession. Now let’s see how this gets modified when we move to General Relativity.\nThe Effective Potential in General Relativity\nMercury’s Orbit in General Relativity\n- The minimum in the effective potential well is deeper and displaced slightly inward.\n- The oscillation spreads over a wider range of radii for a given energy than before.\n- The well opens up a little more steeply.\nIndeed, with general relativity “turned on”, the radial oscillation is faster than before. But so is the angular oscillation, even more so! The two oscillation periods are unequal, and Mercury completes one 360° revolution in less time than it takes to complete one oscillation radially. Now at last we directly see the precession! Here it is as an orbital plot:\nWhy is the time to complete one angular orbit reduced so much, and more so than the radial one?\nBy conservation of angular momentum, the sideways velocity increases at smaller radii. Here we have a well which has changed shape and moved inward. With General relativity turned on, Mercury plunges inward a little closer to the Sun. There it not only moves faster sideways, but it also has a smaller circle to complete. These effects conspire to allow the orbit to cycle around faster than it otherwise would in Newtonian gravity, and grow out of phase with the radial motion.\n- Mercury: 42.98\n- Venus: 8.62\n- Earth: 3.85\n- Mars: 1.35\nGoing Further: The Bizarre Orbits in Strongly Curved Space-Time\nThe Sun is very far from being a black hole. To become one, it would have to be squeezed down into a space smaller than 6km across. Whereas Mercury orbits at around 60 million kilometers away. So Mercury doesn’t really explore this region of strong general relativistic effects. But, as I was working on making the plots for this post, I suddenly realized “I’ve just made something that simulates orbits in general relativity.”\nLet’s change some parameters, and look at some orbits that happen very close to a black hole. Don’t worry, there is no more math. Only some neat figures and explanations.\nThe effective potential near a black hole:\nHere I’ve plotted the potential for a particle with some angular momentum, and starting at 10 event horizon radii from the black hole. The mass of black hole I used is 4×106 solar masses which is similar to the supermassive black hole at the center of our galaxy. The energy of this particle is just enough so that its orbit can drop down close to the black hole without falling in. Notice the hill on the left side of the graph. The particle is starting at a point just below that peak’s height, so that when it “rolls down the well” it will stop and turn just below the peak.\nThis trajectory is delicately balanced on a knife’s edge. If it strayed just a little bit further in, it would plunge down the other side, into the black hole. Like so:\nSo this is one remarkable feature of motions near a black hole. You can get an orbit that drops down and then circulates around several times very close to the black hole, before either zooming back outward to safety, or fatally falling down in. And the difference between the two is precariously thin.\nDon’t miss discussion of the original post on the forum: it has a lot of interesting questions and answers.']"	['<urn:uuid:d0d28341-bce2-492a-94cc-7fffc4f30a20>', '<urn:uuid:8255a823-7df6-4531-81e9-52d85c836b84>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T15:30:12.136021	22	94	3085
