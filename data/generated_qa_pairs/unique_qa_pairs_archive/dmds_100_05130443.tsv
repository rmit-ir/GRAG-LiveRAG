qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	How many haute couture garments and accessories are preserved in the collection at the Fondation Pierre Bergé-Yves Saint Laurent?	The Foundation has a collection of 5,000 haute couture garments and 15,000 accessories.	"['Yves Saint Laurent was a phenomenon, one of the world’s most influential fashion designers who by making the male wardrobe accessible to women also became part of the mid-20th-century female liberation movement. It was Le Smoking tuxedo jacket that set the tone; after that, he did the same with other hitherto male clothes like safari jackets, pea jackets, and flying suits.\nHis output was extraordinary, as was his lifestyle of drinking and drug taking. He died aged 71 from brain cancer in June 2008, was cremated and his ashes scattered in his Majorelle garden in Marrakesh, Morocco. As President Sarkozy said: ""Yves Saint Laurent was convinced that beauty was a necessary luxury for all men and all women.""\nThe Studio of Yves Saint Laurent\nIf you want to find out more about the fashion genius, his ideas of necessary luxury and his designs, visit his Paris studio on a tour with Cultival, a company that specializes in guided tours of places which are not usually accessible to the public. The studio is in the Fondation Pierre Bergé-Yves Saint Laurent, the Foundation that YSL set up with his lover and partner to preserve his heritage. The couple opened the YSL haute couture house in 1962 and moved to 5 A venue Marceau in the 16th arrondissement in 1974. The Foundation has an extraordinary collection of 5,000 haute couture garments as well as over 50,000 drawings, sketches and sketchbooks, and 15,000 accessories.\nWhile the details have not been revealed, you\'re likely to see the Reception Salons, Yves Saint Laurent’s studio, and the library. There will also be original sketches and read YSL’s annotations to the workshops as well as haute couture prototypes. It will be a fascinating glimpse into the life and work of the designer who astounded and shocked the world.\nFoundation Pierre Bergé-Yves Saint Laurent\n5 avenue Marceau\nThe Life of Yves Saint Laurent\nYves Henri Donat Mathieu Saint Laurent was born on August 1, 1935, in Oran Algeria. At 18 he moved to Paris, studying at the Chambre Syndicale de la Couture and gaining enough attention for his designs for an introduction to Christian Dior. Yves Saint Laurent\'s significant climb to fame within the house began when he won first prize for a cocktail dress he designed in 1954. When Dior died at the unexpected early age of 52, YSL took over, launched a spring collection and his career seemed made. However, it was cut short for a spell: in 1960 he was conscripted into the French army fighting in Algeria, suffered a nervous breakdown and was sent to a mental hospital.\nThe subsequent release from Dior was a blessing. His lifelong partner, Pierre Bergé, provided the finance; YSL the inspiration and in 1962, the pair launched the YSL label. In 1966 he opened his Rive Gauche boutique, the first to offer ready to wear; in the 1970s menswear was introduced.\nYves Saint Laurent was way ahead of his time. He was the first designer to use ethnic models on the runway; in 1971 his radical ‘40s collection shocked the critics; he posed nude for his first YSL men’s fragrance, Pour Homme, which created an enormous frenzy of interest and condemnation, and in 1977 launched his Opium perfume. By the early 1980s, his fame was such that the Metropolitan Museum of Art in New York put on their first solo exhibition on a fashion designer. The Saint Laurent fashion house was sold in 1993 and he finally retired in 2002.\nToday his designs are as iconic as ever; while the name lives on with new designers at the helm.\nYves Saint Laurent stores in Paris:\n- 38 Rue du Faubourg Saint-Honoré, Paris 8\n- 9 Rue de Grenelle, Paris 7\n- 6 Place Saint-Sulpice, Paris 6']"	['<urn:uuid:ec7060d3-71a4-4478-bdf8-92a1a94f4be2>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	19	13	627
2	need computer tools record adopted child birth family and adoptive family same diagram	One solution is to draw both family trees for the adoptee: the birth family and the adoptive family. When space allows, both trees can be drawn on the same page using different colors or backgrounds. This is often presented as a 'roots and branches' design, with the child in the heart of the tree trunk, where birth family members are the 'roots' and adoptive family members are the 'branches.'	['Gender is an important part of life stories. An individual’s gender history can impact their relationships with their parents and siblings. It can also affect the person (or people) they marry and whether and how they come to be parents. This information is not always private; it can become part of family history.\nAs the amount of online genealogical data grows and as technology improves for creating family trees, new and more flexible ways of representing gender may arise.\nDeciding what story to tell\nSome genealogists maintain a “purist” position that a family tree should properly dedicate itself to outlining biological relationships. Within this approach, each individual on the tree will have one mother and one father – the people who contributed their DNA to form that person.\nThis approach poses problems for many people’s family stories. First of all, it does not reflect the reality and significance of adoption. One solution is to draw both family trees for the adoptee: the birth family and the adoptive family. If space permits, both trees can be drawn on the same page using different colors or backgrounds. This is often presented as a “roots and branches” visual design for young adopted children who are drawing their own family tree. With the child in the heart of the tree trunk, the birth family members are the “roots” of the tree and the adoptive family members are the “branches.”\nA different problem (or, perhaps, opportunity) exists for people who claim more than one gender role over their lifetimes, and especially for those who have children, whether biologically or through adoption. How does one decide whether the indicate the person’s role as “daughter” or “son,” “wife” or “husband,” and especially as “mother” or “father,” where the person may have filled multiple roles at different times?\nA blanket rule always to label a person’s sex in some particular way – whether based upon their birth sex, their surgically reassigned sex, the gender in which they spent the majority of their life, the gender on their marriage certificate(s), the way in which they contributed DNA to one or more children, or the “Mama” or “Papa” name by which their children called them – is a kind of stereotype and assumption that is no more proper or accurate within a family tree than it is in any other sphere of life. What is most relevant for one person may not be as important for another. The information in all of these spheres together may not present the individual consistently as one sex or the other, and even if it does, the person may have a different story to tell.\nIf the person is alive, they can, of course, be asked for their preference about the name and gender they want to use in the family tree. The document may be shared within the family or eventually stored as a publicly accessible record, so it is important to honor the lives of the people mentioned within it, which most likely means representing their gender as they would choose to represent it themselves.\nGo back enough generations, on the other hand, and all of the ancestors are deceased. These people cannot be asked for their opinion, so the researcher must make a determination about how to present them.\nOne person whose life story poses such a question about gender representation is Deborah Samson, an American female who disguised herself as a man named Robert Shurtliff to fight in the Revolutionary War, and later returned to live as a woman, marrying a man and bearing children. (Her story is told in the recent novel Revolutionary by Alex Myers.) When faced with a real-life character like this, one must determine how to present their story. Other genealogists might find a relationship to a castrated man who may have held third-gender social status (whether he appreciated it or not) in places like China, India, Turkey or Italy. Again, there is a challenge in deciding how to present the family story.\nOnce the researcher is ready to make a declaration about an individual’s gender, whether it is provisional or final, the next steps will be limited by what the medium allows. Most people today who draw up their family trees will do so on a computer. Each program has a different way of capturing and displaying gender.\nOne of the largest genealogy sites on the Internet today is Ancestry (ancestry.com). Users can make multiple family trees and optionally make the information available to share with other users. Their trees may be for their own families or for unrelated people in whom they have a research interest. Within the family tree, each individual has a profile that can contain multiple photographs or portraits and other media files. People without photographs are represented as white silhouettes. There are three gender options, which control the default silhouette: “male,” side-facing with short hair on a light blue background; “female,” side-facing with slightly longer, wavy hair on a light pink background; and “unknown,” facing straight ahead, bald in appearance, on a light gray background. The system asks for “Last name” before the gender is assigned. After it is assigned, the same field is called “Surname” for males and people of unknown gender, and “Maiden name” for females.\nWhile Ancestry requires that one of these three gender options be selected for each person in a family tree, it does not restrict marriage or parent-child relationships based on the gender of the individuals involved. Users can also make a “custom” event on the individual’s timeline and label it however they please. This has the potential to be used for gender transition markers, as one example.\nFind a Grave\nAnother large website is Find a Grave (findagrave.com) which allows users to create “memorial pages” devoted to individual graves, organized by cemetery. Find a Grave is technologically much simpler than Ancestry.com, but it has become a large resource because of the sheer amount of information that volunteer users have placed on the site for over 120 million graves as of 2015.\nOn a memorial page, the deceased individual may be assigned a first, middle, and last name, along with a nickname (which will appear in quotation marks) and a maiden name (which will display in italics), plus a prefix (Mrs., Sir, etc.) and a suffix (Jr., Sr., etc.). The nickname and maiden name fields may be co-opted for a person who had multiple names in multiple gender roles, if those names are relevant to that person’s life story, of course. Multiple images can be uploaded. These are usually photographs of the gravestone but can be photographs of the individual during life.\nA optional section called “Family links” connects the memorial pages of immediate family members. The options are “Father,” “Mother,” and “Spouse” (along with “Year Married”). Multiple spouses can be added. It is only in the designation of “Father” and “Mother” that binary gender is relevant on Find a Grave. There is no pink and blue color-coding as on Ancestry, and Find a Grave can display a biographical text paragraph on the front of the memorial page without making the user “drill down” into sub-pages for that information.\nWhat will users imagine next?\nSome users’ ideas may be more creative – or simply more accurate – than existing software will allow them to input into the system. Genealogical work is highly collaborative, so it is likely that new conventions will emerge and that new technologies will be introduced to accommodate them.']	['<urn:uuid:987cbf3a-5f9d-4865-b1a1-a3f646ca8359>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	13	69	1248
3	How does the cerebellum work and what problems can affect it?	The cerebellum coordinates motor activity, balance, and posture by communicating with other parts of the nervous system through cerebellar peduncles. The inferior peduncles relay sensory information about limb position, middle peduncles transmit signals about desired positions, and superior peduncles send correcting impulses through the dentate nucleus. Ataxia, a degenerative disorder, can affect the cerebellum causing inaccurate movements, instability, tremors and poor coordination. Patients may experience jerky movements, frequent falls due to unsteady gait, and problems with eye movements and speech.	"[""Published on February 6, 2014\nivyanatomy.com section 4, chapter 11 The Diencephalon, Brainstem, and Cerebellum\nDiencephalon The diencephalon is located between cerebral hemispheres and superior to the midbrain. • It surrounds the third ventricle Structures within the diencephalon: • Thalamus • Epithalamus • Hypothalamus • Optic tracts & Optic chiasm • Mammillary bodies • Pituitary Stalk (infundibulum) • Pineal gland Figure 11.21. A sagittal section showing the diencephalon in brown and the brainstem in yellow.\nThalamus The thalamus is a sensory relay center: • Receives all sensory impulses (except smell) • The thalamus relays impulses to appropriate areas of the cerebral cortex for interpretation Example: Figure 12.42. The lateral Example: Figure 12.42. The lateral geniculate nucleus (LGN) within the geniculate nucleus (LGN) within the thalamus relays impulses from the retina thalamus relays impulses from the retina to the visual cortex for interpretation. to the visual cortex for interpretation.\nHypothalamus The hypothalamus regulates a variety of visceral activities including: 1. Body temperature 2. Heart rate and blood pressure 3. Hunger and thirst 4. Sex drive 5. Influences moods and emotions 6. regulates endocrine system\nThe Limbic System The limbic system is a region of the diencephalon and the deeper regions of the cerebrum important for controlling emotions and memory. Functions of the Limbic System 1.Memory 2.Reproduction 3.Emotions (fear, anger, pleasure, sorrow) 4.Hunger and feeding\nThe Limbic System 1. Cingulate Gyrus – Satisfaction Center • Feeling satisfied after a meal or after sexual intercourse • Damage may result in voracious appetite or unusually high sex drive 2. Hippocampus • Within deep temporal lobe • Role in memory and spatial cognition • Alzheimer's results in degeneration of hippocampus 3. Amygdala -Assigns emotion to a memory (pleasant or unpleasant) • Like or dislike a person you see • Primal fears (heights, fire, insects, etc.)\nBrainstem The brainstem connects the brain to the spinal cord. It includes three parts: 1. Midbrain 2. Pons 3. Medulla Oblongata Figure 11.15a. Sagittal section of the brain. The Figure 11.15a. Sagittal section of the brain. The three portions of the brainstem are shown in yellow. three portions of the brainstem are shown in yellow.\nMidbrain The midbrain is located between the diencephalon and the pons. Fibers of the midbrain join lower parts of brainstem and spinal cord with higher part of brain Corpora Quadrigemina “Body of 4 twins” located on the posterior surface of the midbrain. • Superior Coliculi – visual reflexes • Inferior Coliculi – auditory reflexes Cerebral Peduncles located on the ventral surface of the midbrain • Main motor pathway from cerebrum to lower CNS\nMidbrain Substantia nigra • Involved in coordinating voluntary movements • Secretes dopamine – inhibitor neurotransmitter • Communicates with Basal Nuclei within cerebrum Parkinson’s disease results in degeneration of substantia nigra and basal nuclei. Cross section through midbrain. Dark portions are substantia nigra. Opening in center is the cerebral aqueduct.\nPons The pons appears as a rounded bulge on the ventral aspect of the brainstem. It’s located between the midbrain and medulla oblongata The Pons is a “Bridge” The dorsal surface contains longitudinal fibers connect the medulla oblongata to the higher brain. The ventral surface contains transverse fibers that connect the pons to the cerebellum. The Pontine Respiratory Center helps maintain the basic rhythm of breathing Figure 11.20a. Ventral Figure 11.20a. Ventral view of the brainstem. view of the brainstem.\nMedulla Oblongata The medulla is an enlarged extension of the spinal cord. Conducts ascending and descending impulses between brain and spinal cord Structures: 1.Pyramids • Site of motor tract decussation (crossing over) 2. • Olives Passages for fibers to cerebellum\nMedulla Oblongata Nuclei of Medulla: 1.Cardiac center – regulates heart rate 2.Vasomotor center – regulates blood pressure 3.Respiratory center – regulates rhythmic breathing\nVentral surface of the brainstem 1 = cerebral peduncles. 2 = pons with transverse fibers leading towards cerebellum. 3 & 4 = pyramids on the medulla\nCerebellum The cerebellum “little brain” is inferior to occipital lobe of the cerebrum, and posterior to the pons. Functions of the cerebellum include: 1.Integrates sensory information 2.Balance and posture 3.Coordinates motor activity 4.Learning and practicing Cells within the cerebellum 1.Purkinje cells – larges cells in CNS Gatekeepers of impulses leaving the cerebellum 2.Dentate Nucleus – balance and proprioception\nCerebellum The cerebellum communicates with the CNS by means of tracts, called cerebellar peduncles: 1.Inferior peduncles – relays sensory impulses of the actual position of limbs and joints from the medulla to the cerebellum 2.Middle peduncles – transmits impulses from the cerebral cortex to the cerebellum of the desired position of these body parts. 3.Superior peduncles – sends correcting impulses from dentate nucleus of cerebellum to midbrain, adjusting the position of a limb. Damage to cerebellum may result in loss of balance, tremors, and inaccurate movements\nFigure 11.22 the cerebellum communicates with other parts of the CNS by means of the cerebellar peduncles.\nEnd of section 4, chapter 11\n1.ivyanatomy.comsection 4, chapter 11The Diencephalon, Brainstem, and Cerebellum. 2. Diencephalon The diencephalon is located between cerebral hemispheres ...\nTitle: section 4, chapter 11 CNS & PNS, Author: Michael Walls, Name: chapter_11__section_4, Length: ... Chapter 11, Section 4. The Diencephalon, Brainstem, ...\nMEDICAL EMERGENCIES SECTION 4 Respiratory ... Chapter 11 Respiratory Emergencies 3 ... 4. SECTION 4 Medical Emergencies\nThe brain stem is an extension of the spinal cord that enters the bottom of the skull and travels upward, ... Section 4-11: What Happened to ... as you saw ...\nChapter 11 Section 4. Devastation and Freedom. Battle of the Wilderness. May 1864 Intense fighting mostly in wooded areas So intense it set the wooded ...\n... “The Civil War” _____ _ Section 4: The North Takes Charge One Americans Story: Frank… U.S ... section 4, chapter 11: brainstem ...\nChapter 11 / Brainstem 237 237 11 The Brainstem An Overview Harold H. Traurig CONTENTS EXTERNAL ANATOMY VENTRICULAR SYSTEM OF THE BRAINSTEM MEDULLA PONS"", ""3 Things You Must Know About Movement Disorders\nMovement disorders usually refer to potential problems an individual might experience during his or her lifespan and manifests through the skeletal and muscular systems.\nThe muscular system is composed of smooth, skeletal, and cardiac muscles. It is directly involved in the body’s movement, while also maintaining posture as well as circulation of the blood throughout the organism. However, for vertebrates, the muscular system is controlled by the nervous system, despite the fact that some muscles like cardiac muscle, for example, are capable of being completely autonomous.\nThe skeletal system is another intricate part that can play a significant role when it comes to the development of a moving disorder. Humans are usually born with 270 bones, but some of them fuse into a longitudinal axis called the axial skeleton.\nMovement is possible because of the joints between the bones, among which some allow a more extensive movement range compared to others. So overall, muscle, joints, and bones are the ones that offer the principal movement mechanism, but they’re all coordinated by the nervous system.\nSome problems may appear due to a poor diet, lack of exercise, or external and internal influences. Some may be of little importance like cramps, but some may lead to muscle malfunctioning or even paralysis. This is the reason why people have started taking nutritional supplementation to keep everything functioning at a high level. These are usually based on vitamins and minerals, but herbal extracts can sometimes help too.\n4 Most Common Movement Disorders\nAtaxia is a degenerative disorder attacking the spinal cord, brain stem, or the brain. This can result in inaccuracy, clumsiness, instability, tremor, imbalance, or a lack of coordination while performing voluntary movements. Movements are not consistent and may appear jerky or disjointed. Patients may fall down frequently due to an unsteady gait. Ataxia also can affect the movement of the eyes and speech.\nIf a metabolic disorder can be identified as the underlying cause, specific treatment may be available in select cases. Several recent clinical studies discovered a cornerstone therapy for ataxia of parkinsonism (or parkinsonism of any cause) is the use of L-DOPA (oral administration).\nOther medications used to treat ataxia associated with parkinsonism (or parkinsonism of any cause) include dopamine agonists, anticholinergics, amantadine, entacapone, and selegiline. However, the situation is different in children with ataxia. In their case, generally, only anticholinergics are recommended.\nDystonia is another movement disorder (a neurological muscle disorder to be precise) defined by involuntary muscle spasms. This movement disorder is the effect of abnormal functioning of a deep part of the brain (the basal ganglia), which helps control coordination of movement. Basal ganglia controls the fluidity and the speed of movement and prevents undesired movements.\nIndividuals diagnosed with dystonia can experience abnormal postures and positions, uncontrollable twisting, or repetitive movements. These can have a significant effect on any part of the body, including the vocal cords, arms, trunk, legs, and eyelids. Focal dystonias involve only one body location, most commonly eyelids (blepharospasm), the neck (spasmodic torticollis), hand (writer’s cramp or limb dystonia), or lower face (Meige syndrome).\nOn the other hand, general dystonias involve the entire body. Depending on what part of the body is affected, the condition can be very disabling.\nThere is a three-tiered approach to treating dystonia: surgery, medication, and botulinum toxin (commonly known as Botox) injections. These may be used alone or in combination. Surgery is taken into consideration when other therapies have proven ineffective.\nThe goal of surgery is to interrupt the pathways responsible for the abnormal movements at various levels of the nervous system. Some operations purposely damage small regions of the globus pallidus (pallidotomy), thalamus (thalamotomy), or other deep centers in the brain. Over the past few years, deep brain stimulation (DBS) has been tested with some success.\nOther surgeries include removing the nerves at the point they enter the contracting muscles (selective peripheral denervation) or cutting nerves leading to the nerve roots deep in the neck close to the spinal cord (anterior cervical rhizotomy).\nBotulinum toxin injections help block the communication between the muscle and the nerve and may lessen abnormal movements and postures.\n3) Huntington’s Disease\nHuntington’s disease is a progressive, degenerative and fatal condition. Huntington’s disease is caused by the deterioration of specific nerve cells in the brain. Incidents most often occur between ages 35 and 50, with the condition advancing without remission over 10 to 25 years. This movement disorder affects an estimated one in every 10,000 Americans.\nA juvenile form of the disease affects patients age 20 and younger, accounting for about 16 percent of all cases. Symptoms include the development of psychiatric problems, progressive loss of mental abilities, and uncontrollable movements of the limbs, trunk, and face. Huntington’s disease is hereditary – a child with one affected parent has a 50% chance of developing the condition.\nThere is no known cure for Huntington’s disease, so treatment focuses on helping patients and family members cope with daily challenges, preventing complications, and alleviating symptoms. Depending on the situation, medical doctors will prescribe antidepressants, antipsychotics, tranquilizers, botox injections, or mood-stabilizers.\nThe medication is usually prescribed in the lowest effective dosage, as all of these prescription drugs may have adverse effects. Huntington’s disease is an unforgiving condition that normally runs its full terminal course in 10 to 30 years. Researchers have noticed that the earlier in life the first symptoms occur, the quicker the disease often progresses.\n4) Parkinson’s Disease\nParkinson’s disease is a progressive disorder that is induced by degeneration of neurons in the part of the brain that controls movement. This part of the brain is commonly known as the substantia nigra. These nerve cells become impaired or die, losing the capability to produce a valuable chemical called dopamine.\nParkinson’s disease produces numerous general symptoms, including:\n- Gradual loss of spontaneous movement, often leading to decreased reaction time or mental skill, decreased facial expression or voice changes;\n- Stiffness of the limbs or muscle rigidity;\n- An unsteady walk or balance;\n- Gradual loss of automatic movement, often leading to reduced frequency of swallowing, decreased Blinking, and drooling;\n- A stooped, flexed posture, with bending at the knees, elbows, and hips;\n- Dementia or depression.\nAccording to the Parkinson’s Disease Foundation, 60,000 new cases of Parkinson’s disease are diagnosed each year, adding to the seven to ten million people who have the disease worldwide. While the risk of Parkinson’s diagnosis increases with age, about 4% of patients afflicted with this condition are diagnosed before the age of 50.\nMost Parkinson’s patients are treated with medications to relieve the symptoms of the disease. Some common medications used are anticholinergics, dopamine agonists, and dopamine precursors. Thalamotomy can help stop tremor by placing a small lesion in a specific nucleus of the thalamus. Deep Brain Stimulation (DBS) of the subthalamic nucleus or globus pallidus can be useful in treating all of the primary motor features of Parkinson’s and sometimes allows for significant decreases in medication doses. Surgery is considered when medications have proven ineffective.\nWhat Is the Treatment for Movement Disorders?\nTreatment for movement disorders varies from one case to another. In most circumstances, the goal of treatment is to alleviate symptoms. Treatment may include botulinum toxin injection therapy (BOTOX therapy), medication, and/or surgery.\nMedications that may be used include the following:\n- Antiseizure medications like Gabapentin (Neurontin) and Primidone (Mysoline). Antiseizure medications may cause dizziness, a lack of coordination and balance (ataxia), fatigue, and nausea.\n- Antiepileptics like Valproate (Depakote) and Carbamazepine (Tegretol). Side effects of antiepileptics include drowsiness, dizziness, vomiting, and nausea.\n- Dopamine agonists like Pergolide (Permax) and Bromocriptine (Parlodel). Dopamine agonists may cause headaches, nausea, fatigue, and dizziness.\n- Beta-blockers such as Propranolol (Inderal). Side effects caused by beta-blockers include depression, slowed heart rate (bradycardia), nausea, and lightheadedness.\n- Tranquilizers such as Clonazepam (Klonopin) and Benzodiazepines (Diazepam/Valium). Benzodiazepines may cause drowsiness, blood clots (thrombosis), and fatigue.\n2) Speech therapy\nSome movement disorders, such as Parkinson’s disease, can impair speech, altering the ability of the patient to express elaborate thoughts. For that reason, speech therapy may prove valuable, and it’s commonly recommended to improve the patient’s ability to communicate and swallow. It is crucial for caregivers to recognize that individuals with such movement disorders may not be communicating due to the disease and not due to lack of sociability.\n3) Lifestyle and nutrition changes\nExercise and proper diet are crucial for patients with movement disorders. Huntington’s disease, for instance, may cause individuals to choke. Caregivers who help patients eat should provide plenty of time for meals. Food can be softened, cut into small pieces, or pureed to prevent choking and ease swallowing. While some foods may need to be thinned, other foods may require the addition of thickeners. Dairy products, in particular, tend to increase the secretion of mucus, which in turn increases the risk of choking.\n4) Caregiver support\nWhile it may be emotionally challenging, it is crucial for caregivers and patients to make carefully considered, informed decisions regarding the future while the patient is capable of making his or her contribution to a planned course of action. Movement disorders confront individuals and their caregivers with many difficult problems that must be dealt with for the life of the patient.\n5) Social activity\nUnless and until the disease’s progression hinders it, people with movement disorders should pursue hobbies and interests, socialize, and participate in outside activities. These activities also give caregivers and family members valuable time for themselves.\n6) Physical therapy\nAccording to the American Physical Therapy Association, the goal of physiotherapy or physical therapy is to restore function, improve mobility, prevent further injury, and reduce pain by using a variety of methods, including stretches, exercises, electrical stimulation, traction, and massage. Physical therapy has been reported useful in treating many neurological disorders.\n5 Amazing Ingredients That May Help\n5-HTP is the precursor for serotonin, a brain chemical associated with nervousness, mood, sleep, feeding, and movement. In clinical trials, 5-HTP has been observed to produce benefits in some individuals who have difficulty walking or standing due to cerebellar ataxia. However, additional research is needed before a clear conclusion can be drawn.\nAvoid taking 5-HTP if you have been diagnosed with mitochondrial encephalomyopathy, eosinophilia syndromes, or Down syndrome. 5-HTP is not recommended to individuals who are hypersensitive or allergic to it. Signs of allergy to 5-HTP may include shortness of breath, itching, or rash. Use with caution if taking antidepressant medications, 5-HTP receptor agonists, phenobarbital, carbidopa, pindolol, tramadol, reserpine, or zolpidem.\nArginine, (also known as L-arginine), is considered a semi-essential amino acid because supplementation is sometimes required, although it is usually produced in sufficient amounts by the body. As a method of treatment for movement disorders, injections with L-arginine have been recommended to help manage adrenoleukodystrophy (ALD). However, many clinical study results are inconclusive, and additional research is required to evaluate the use of arginine in ALD and other neurological conditions.\nL-arginine is not recommended to those with a history of stroke or kidney or liver disease. L-arginine is not recommended to pregnant or nursing women. Blood potassium levels should be monitored. L-arginine may worsen symptoms of sickle cell disease. Use cautiously if taking blood pressure drugs, antidiabetic drugs, blood-thinners, or dietary supplements or herbal supplements with similar effects.\nAshwagandha (Withania somnifera) is extensively cultivated in the Middle East and India for its medicinal properties, and it is also found in parts of Africa. Today, Ashwagandha is being used to treat a wide range of health concerns, including for several types of movement disorders. Ashwagandha may be prescribed as a treatment for Parkinson’s disease, but additional research is needed to determine whether Ashwagandha is effective or not.\nData regarding the effectiveness of choline in the treatment of movement disorders is inconclusive and conflicting at this time. Avoid if hypersensitive/allergic to phosphatidylcholine, lecithin, or choline. Use cautiously with liver or kidney disorders or trimethylaminuria. Use carefully with a history of depression. If pregnant or breastfeeding it generally seems safe to consume choline within the recommended adequate intake (AI) parameters.\nAlthough used traditionally for support of neurological conditions, one poorly designed preliminary clinical study reported that L-acetyl-carnitine (L-carnitine or carnitine) possesses neither efficacy nor toxicity towards the patients with movement disorders. Further trials are required to determine if L-carnitine is beneficial in individuals with neurological disorders.\nEarly research on the use of carnitine for Rett’s syndrome has produced promising results. However, additional research is needed before a firm conclusion can be made.\n- Cerebellar Degeneration Information Page - National Institute of Neurological Disorders and Stroke.\n- “Cerebellar Degeneration” - brainfacts.org.\n- Amanda Delgado, Elizabeth Boskey, PhD, and Marie Beaugureau - “What is acute cerebellar ataxia?”\n- “Ataxia” - mayoclinic.org.\n- Werner Poewe, Angelo Antonini, Jan CM Zijlmans, Pierre R Burkhard, and François Vingerhoets - “Levodopa in the treatment of Parkinson’s disease: an old drug still going strong.” Published online on September 7, 2010.\n- Wolfgang H. Oertela - “Recent advances in treating Parkinson’s disease.” Published online March 13, 2017.\n- Mizuno Yoshikuni, Nobuo Yanagisawa, Sadako Kuno, Mitsutoshi Yamamoto, Hasegawa Kazuko, Hideki Origasa, Hisayuki Kowa - “Randomized, double-blind study of pramipexole with placebo and bromocriptine in advanced Parkinson's disease.” Published in October 2013.\n- George DeMaagd, PharmD, BCPS and Ashok Philip, PhD - “Parkinson’s Disease and Its Management.” Published in October 2015.\n- “Movement Disorders” - aans.org.\n- “Dystonia” - mayoclinic.org.\n- “Dystonia” - aans.org.\n- “Dystonias Fact Sheet” - National Institute of Neurological Disorders and Stroke.\n- Barbara Illowsky Karp and Katharine Alter - “Muscle Selection for Focal Limb Dystonia.” Published online on December 29, 2017.\n- Movement Disorders Center.\n- “Huntington's disease” - mayoclinic.org.\n- “Huntington disease” - Genetics Home Reference.\n- “Five Facts About: Huntington's Disease [Infographic]” - mdmag.com.\n- “Overview - Huntington's disease” - nhs.uk.\n- “Stages of Huntington’s Disease” - Hungtington’s Outreach Project For Education. Stanford University.\n- “Parkinson's disease” - mayoclinic.org.\n- “Parkinson’s Disease Statistics” - parkinsonsnewstoday.com.\n- “What Are the Treatments for Parkinson's Disease?” - webmd.com\n- Katrijn Smulders, Marian L. Dale, Patricia Carlson-Kuhta, John G. Nutt, and Fay B. Horak - “Pharmacological treatment in Parkinson's disease: effects on gait.” Published online on July 17, 2016.\n- Robert A Hauser, MD, MBA - “Parkinson Disease Medication.”\n- “Movement Disorders Treatment” - healthcommunities.com.\n- “Medication for Movement Disorders” - nyulangone.org.\n- Raymund AC Roos - “Huntington's disease: a clinical review.” Published online on December 20, 2010.\n- Harini Sarva, MD and Vicki Lynn Shanker, MD - “Treatment Options in Degenerative Cerebellar Ataxia: A Systematic Review.” Published online on June 12, 2014.\n- A.Marquer, G.Barbieri, D.Pérennou - “The assessment and treatment of postural disorders in cerebellar ataxia: A systematic review.” Published online on February 6, 2014.\n- “5-HTP” - webmd.com.\n- Vanessa Lin Lin Lee, Brandon Kar Meng Choo, Yin-Sir Chung, Uday P. Kundap, Yatinesh Kumari, and Farooq Shaikh - “Treatment, Therapy and Management of Metabolic Epilepsy: A Systematic Review.” Published online on March 15, 2018.\n- “L-Arginine” - medlineplus.gov.\n- Alejandra M. Wiedeman, Susan I. Barr, Timothy J. Green, Zhaoming Xu, Sheila M. Innis, and David D. Kitts - “Dietary Choline Intake: Current State of Knowledge Across the Life Cycle.” Published online on October 16, 2018.""]"	['<urn:uuid:681ab65e-332b-4fbb-b7b9-e701c0831fdd>', '<urn:uuid:72c93994-abe4-456c-a53b-a4c4c367a69d>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	11	80	3507
4	ada disability disclosure requirements online barriers	Under the ADA, employers cannot require disability disclosure before making a job offer, and any disclosed medical information must be kept strictly confidential. However, there are significant online barriers for applicants with disabilities - nearly 37% experience accessibility issues with social media, and many struggle with careers sites and job portals due to problems like timed assessments, lack of video captions, and keyboard navigation limitations.	['In general, you should disclose your disability when you need to request a reasonable accommodation - when you know that there is a workplace barrier that is preventing you, due to a disability, from competing for a job, performing a job, or gaining equal access to a benefit of employment like an employee lunch room or\nShould I identify as having a disability?\nNo, with limited exceptions. Under the ADA, an employer generally may not require an applicant to disclose information about a disability or medical impairment prior to making an offer of employment.\nDo you wish to disclose any disability?\nDisclosing on the Job Application You do not need to disclose a disability on your job application or resumé. Plus, its illegal for employers to ask candidates whether they have a disability on job application forms. So, if your application form asks about disabilities, you can leave that part blank.\nIs it illegal to not disclose a disability?\nConfidentiality. The ADA requires that employers keep any medical information they have about a disability-related inquiry or medical examination strictly confidential. This includes medical information from voluntary health or wellness programs, and any medical information voluntarily disclosed by an employee.\nWhat if an employee does not disclose a disability?\nWhat happens if an individual initially decides not to disclose a disability, but discovers later that they will need an accommodation? An individual with a disability may request a reasonable accommodation at any time during the application process or during employment.\nAre you more likely to get hired if you have a disability?\n“If the answer is yes and the disability doesnt affect job performance, then dont mention it.” Employers use resumes to weed people out, so anything on the resume that would allude to a disability -- given the realities of the marketplace -- will probably work against you, he explains.\nShould you list disability on job application?\nLegally, the ADA does not require candidates to disclose a disability to employers or potential employers. If you do not disclose, however, employers correspondingly will not have to make accommodations.\nDo I have to disclose my disability when applying for a job?\nAn employee is only obligated to tell you about a disability if it affects his or her ability to do a particular job, or if it affects his or her ability to work safely and ensure the safety of others. Many people with disability will disclose their disability as a courtesy to their employers.\nCan I lie about having a disability?\nChoose not to disclose whether or not you have a disability. They can ask, but you legally have the right to not answer, says MichelleW. It is illegal for them to demand the information. I declined because I didnt want to lie, and its within my rights to not disclose, says DTMN.\nWhat happens if I say I have a disability on a job application?\nAs a rule, an employer isnt able to discriminate against you being hired because of a disability. If a disability is going to greatly reduce your ability to perform job duties, you should disclose that to an employer if they ask you. With that said, many disabilities pose no issue for job performance.\nDo I have to tell my employer my medical condition?\nGenerally speaking, employees do not need to inform their employers of their medical conditions or disabilities as long as they are able to perform the essential functions of their jobs without an accommodation or medical leave.', 'Employers committed to diversifying their applicant pools and improving their candidate experience need to be aware of problems that applicants with disabilities may have when they try to access careers sites, job portals and electronic applications.\nPeople with disabilities need to be able to use these technologies as easily as other applicants, especially now that the job search and application processes are now almost completely online.\n“We talk a lot about veterans recruiting programs, or how to recruit Millennials, but there is a real gap in HR on the subject of accessibility for people with disabilities,” said Jessica Miller-Merrell, founder and chief innovation officer for Workology, an online community and resource hub for HR and business leaders.\nThe subject of accessibility in digital recruitment is often overlooked or misunderstood, added Josh Christianson, project director for the Partnership on Employment and Accessible Technology (PEAT), a federally funded initiative promoting the employment, retention and career advancement of people with disabilities through the development of accessible technology.\nMiller-Merrell and Christianson were speaking at the Society for Human Resource Management 2018 Talent Conference & Exposition.\nAccessibility Leads to BetterÂ Recruiting\nDigital accessibility refers to designing devices, products and environments such that individuals with disabilities or sensory impairments can successfully use the device or product or navigate the environment.\n“A prime example of built-in accessible technology is the iPhone,” Christianson said. “There are a lot of functions that can be tailored and adapted for people with disabilities.”\nHe added that accessible online recruiting tools equate to better talent acquisition. “We are operating in one of the hottest economies with some of the lowest unemployment levels in recorded history and yet, over 34 percent of people with disabilities are unemployed and looking for work,” he said.\nAccording to a user survey conducted by PEAT, 37 percent of survey respondents reported that they experienced accessibility or usability issues when using social media. Additionally, 46 percent rated their last experience applying for a job online as “difficult to impossible;” of those, 9 percent were unable to complete the application and 24 percent required assistance.\n“Consider that nearly one in five Americans has a disability, and that one in eight Americans is 65 and older,” Christianson said. “If your website isn’t accessible to them, you could be losing out on potential job candidates or new customers, and exposing yourself to legal risk.”\nThe ProblemÂ Areas\nSome of the most common difficulties job seekers with disabilities experience with careers sites and the solutions for those issues include:\nComplex navigation. People with visual impairments typically navigate the web using a screen reader that converts text to speech and provides nonvisual navigation commands, Christianson explained. But if your website isn’t programmed with accessibility in mind, it can be hard for them to navigate, Miller-Merrill said.\nFor the assistive screen reader to work, it’s important to include detailed and consistent navigational elements in the page structure, such as headers, titles and lists. “It’s not difficult, but it has to be written so that a screen reader can make sense of it,” Christianson said. “Most operating systems today include a built-in screen reader that you can use to test your website, including Narrator on Windows and Voiceover on Mac OSX.”\nTimeout restrictions.Timed assessments in the application process could unfairly frustrate someone with a disability. Many people using assistive technology require extra time to navigate a website and complete tasks. “Lots of people with cognitive disabilities like brain injuries or learning disabilities can be hindered by timeout restrictions,” Christianson said. “If it’s not necessary, you could get rid of those, or extend them much longer.”\nPoor screen contrast. Ensure that people with color blindness or low-vision impairments can use your website by testing design elements for proper color contrast. There is a palette of which colors should be put up against each other for people with various visual impairments, Christianson said. Tools that can help include Chromatic Vision Simulator, which shows what your site would look like to people with different types of color blindness, and VisionSim, which simulates several visual impairments.\nVideo captions. More employers are adding video to careers sites and publishing videos on social media. Be sure to include captions and transcripts for all recruitment and employer branding media in online videos, Facebook Live, YouTube, and event videos, Miller-Merrell said. “As a bonus, adding captions has been proven to increase your SEO [search engine optimization] online and boost user engagement,” she added.\nAlt-text descriptions. It’s important to make digital images accessible. Some people rely on well-written descriptive text called alt-text, which is visible to screen readers, to understand the images in front of them. The description should include a summary of the image and include any relevant supporting information.\n“Alt-text is critical,” Miller-Merrell said. “If you have employer branding photos you want to promote via social, go ahead and put alt-text in there. It’s important to include alt-text descriptions to digital assets, including PowerPoint presentations, web images, social media, and PDFs.”\nKeyboard accessibility. One of the easiest initial tests for accessibility is whether you can use a website without a mouse, Christianson said. Can you tab through your website content from start to finish, or do some actions require a mouse?\n“Maybe someone has a mobile disability, or doesn’t have use of their hands, and needs a keyboard to navigate,” Christianson said. “This is more back-end and not really an HR responsibility, but it’s often overlooked, so make sure your website can be navigated simply by arrows or the tab key.”\nThis article was originally published onÂ SHRM.org on April 17, 2018, and is reprinted with permission.']	['<urn:uuid:2733813f-efe8-48b6-80d1-215254871b26>', '<urn:uuid:dda6ce82-7cdd-4fcd-97d5-54e8657f2936>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	6	65	1519
5	What are Palau's coral reef tourism attractions, and what environmental threats affect these reefs?	Palau offers diverse tourism activities around its coral reefs, including diving excursions from Koror, snorkeling tours, placid canoe tours, sailing charters, and visits to the Palau Aquarium. The reefs face several environmental threats including coral bleaching, rising sea temperatures causing thermal stress, ocean acidification, coastal development from tourism, land-based pollution, and direct damage from divers. These reefs are particularly vulnerable as they are home to endangered species and took millions of years to develop their current biodiversity of 1500 fish species and 550 coral species.	"['Over two million years ago in Micronesia (a subregion within Oceania) significant volcanic activity – now known as the Pacific Ring of Fire – occurred. From this the 343 islands that make up the island state of Palau were formed and the limestone rock basis for the Palau Reefs was in place. At this time, homo habilis inhabited the earth, but not the island of Palau and all that lived there were the basic coral species that had begun to grow on the submerged limestone.\nPalau remained uninhabited until homo floresiensis evolved around 18,000 years ago, yet the first humans to lay eyes on the 300 species of sponges, 1500 species of fish and 550 species of hard and soft corals that live here were from Indonesia in c.2000BC. Since then, the island state of Palau has also been under Spanish, German, British, Japanese and American rule. At present Palau is an independent region, gaining a lot of tourism and notoriety due to its beautiful reefs.\nIf the reefs in Palau today were to be destroyed and returned back to smooth, limestone rocks they could not simply begin regrowing over the next decade. What lies there today is the result of millions of years of development, where corals have formed skeleton bases which the next generation of coral builds upon again.\nThe Palau Reefs are believed to have formed around 2 million years ago, long before the more developed species of the homo genus had evolved (I.e. homo sapiens, Neanderthals and homo erectus). They started very basically at first and gradually developing into the rich and biodiverse environment they are now. Due to this time frame these islands are all-natural; having had no intervention from man.\nLife began in the form of basic corals, on a Palau Island limestone surface during the Pleistocene geological age. From then these corals separated into hard and soft species over two million years, with sponges and reef fish joining them at a much later stage. Quality versus quantity certainly applies to this one of the seven wonders of the underwater world, as Palau’s reefs are quite small compared to the Great Barrier Reef in Australia, but even still they are home to an incredibly diverse number of species; some critically endangered.\nWhy it Was Chosen:\nIt was the American Diver’s Association CEDAM International (CEDAM stands for conservation, education, diving, awareness and marine research) who nominated and chose the Palau Reefs for one of the seven wonders of the underwater world in 1989. CEDAM chose the wonders in accordance to their quality of marine life, and whether we should keep them well preserved as protected sites.\nAs the Palau Reefs in Micronesia are home to various endangered species (both plant and animal) and there has been a considerable level of coral bleaching over the past few decades, they have been identified as needing conservation. The fact that they also home thousands of different species of fish, 550 different species of hard and soft corals, plus 300 different species of sponges makes it the ideal candidate for one of CEDAM’s protected and very much revered seven wonders of the underwater world.\nHow Can it Be Seen?\nOne of Palau’s leading industries today is tourism, meaning that it’s incredibly easy to visit the region and many of the 343 islands that comprise it.\nGetting to the Palau Islands usually means a bit of plane hopping, as there are daily flights from Guam’s airport and twice-weekly flights from Manilla, but also chartered flights from airports in Korea, Taipei, Taiwan, Tokyo, Osaka, Nagoya and Japan. The nearest airport outside of Palau is Guam, where flights take approximately 1 hour and 45 minutes. For this reason you may wish to put together an itinerary, where you stay in one of these countries for a couple of days before traveling to Palau.\nOnce you arrive in Palau there are plenty of tour operators offering diving excursions (daily) from Koror and you can see other attractions such as the jellyfish lake and even swim with dolphins. If diving is not your thing, or you’re bringing small children with you then there are snorkeling tours available, as well as placid canoe tours that allow you to see the reefs from the surface of the water, sailing charters and day trips to the Palau Aquarium at the Palau International Coral Reef Center.', 'What is Green Fins?\nGreen Fins was initiated in 2004 by the United Nations Environment (UNEP) under the Regional Seas programme as part of the effort to increase public awareness and management practices that will benefit the conservation of coral reefs and reduce unsustainable tourism practices. It is overseen by the Coral Reef Unit of UN Environment based in Bangkok in collaboration with UK charity The Reef-World Foundation.\nGreen Fins is a comprehensive approach that encourages dive centres and snorkel operators, local communities and governments to work together to reduce their environmental impacts. This is primarily done through the private sector adopting a Code of Conduct that will help mitigate their impacts when carrying out marine tourism activities.\nThe Code of Conduct consists of 15 points, which target environmental threats posed by the tourism industry, both under water and on land. Green Fins members receive training and the tools to promote environmental education and awareness, tapping into both tourists and the diving community, which is shifting more and more towards eco-friendly initiatives as a result of increased demand from the consumer.\nGreen Fins dives even deeper as it creates a network through which dive centres, local and national governments and communities who work together to tackle local environmental threats to protect livelihoods and food security.\nMembers, who join for free, receive annual assessments, training and feedback to help them achieve Code of Conduct points, that not only standardises membership but also allows a system for measurable progress and collaboration between stakeholders.\nWhere is Green Fins?\nThere is an ever expanding network of dive operators worldwide who are involved in the initiative. Green Fins is managed in each country by a National Management Team made up of government departments and often supported by national NGO\'s. These teams are trained and supported by the International Coordinators of the project, The Reef-World Foundation whose role is to focus on expanding the project to additional member countries in the region and to strengthen the existing network through capacity development.\nThe National Management Teams are responsible for expanding Green Fins in their respective countries and to produce educational and public awareness materials while promoting the successes of the approach. Local Management Teams are often used as a way to better manage the members in a particular tourist ‘hotspot’ who take their direction from the National Management Team and the International Coordinators.\nGreen Fins has evolved with the tourism industry over the years and has been on the frontline when it comes to combating threats from the SCUBA diving and snorkelling industry since 2004. This initiative of the United Nations Environment (UNEP) and was first implemented in Thailand in 2004 alongside The Reef-World Foundation through supporting the development and monitoring aspects of Green Fins. The dynamic approach of Green Fins has developed into the world’s only assessed environmental standards for SCUBA and snorkelling businesses in the world to date. To do this, a holistic approach is needed to tie in the many stakeholders of the project who depend on coral reefs for their livelihoods and food security.\nGreen Fins was developed as a way to address the gap in knowledge and awareness about the growing threats to the marine environment. Tourism is the world’s fastest growing industry with millions of tourists travelling far and wide often to beautiful locations with areas of high biodiversity. This makes the perfect platform for passing on skills and knowledge about how to protect this fragile and important environment whilst helping ensure the sustainability of the tourism sector.\nGrowth in Asia\nAfter Thailand in 2004, Reef-World implemented Green Fins in Malaysia and Indonesia in 2008 through UNEP funding that led to the development of the Road Map that helped pave the way for future countries. At the request of the UNEP, Reef-World then brought the project to the Philippines in 2010 and firmly routed the project at the government level. In 2013 under Mangroves for the Future, an initiative of the IUCN, a Reef-World / UNEP partnership brought the project to the Maldives and Vietnam to work with the governments to help address the threats from the ever growing tourist numbers which if left unchecked can lead to increased stresses on the coral reefs. Once corals become stressed this makes them less resilient to cope with additional pressures such as those from climate change.\nGreen Fins is now currently looking at spreading its ‘fins’ to other regions of the globe to have more successes and ensure a sustainable tourism industry whilst increasing awareness amongst locals and tourists of the wider threats to our seas and oceans.\nGreen Fins is the first and world\'s only assessed environmental set of standards for SCUBA diving and snorkelling centres. Those centres who agree to follow the Code of Conduct have to undergo a training session followed by an assessment of their dive centre at least once a year to ensure that they are minimising their environmental impact. This process simply requires allowing a fully trained Green Fins Coordinator to come to the dive centre and carry out a 1 hour training presentation during a time that suits the staff. Following on from this, an Assessor, who can be recognised through having a valid Green Fins Assessor ID, then joins one of their dive trips to observe staff and customers. This allows the assessor to provide three focus areas for the dive centre to improve on before their next assessment.\nThis assessment system was developed and is maintained by The Reef-World Foundation who are the only organisation able to carry out the training of new Assessors to maintain industry wide standards. The assessment system is part of the Green Environmental Assessment Rating System (GEARS) and when combined with the Green Fins assessment database allows specific threats to be targeted in collaboration with the National Management Teams. This unique and ground breaking technique for monitoring threats from the scuba and snorkel tourism industry has been in action since 2008 and has had effective and successful results. You can see more on this Assessment process on the scientific paper that was published in Ocean and Coastal Management in July 2013.\nThreats to Coral Reefs\nCoral reefs are ancient ecosystems, often described as rainforests of the sea - even though in terms of species biodiversity they far outweigh their terrestrial counterparts. They are highly sought after resources, for both fishing and recreation, with over 275 million people living within 30km of a coral reef ecosystem.\nCoral reefs provide so much more than aesthetic value and fisheries resources: they provide coastal protection for 150,000 km of shoreline in over 100 countries; they are the mating, feeding, and breeding grounds for open ocean species; and they support ecosystem services provided by mangroves and seagrass.\nUnfortunately, today’s coral reefs face an uncertain future. 75% of all reefs are currently threatened by a combination of global and local pressures.\nRising carbon dioxide levels and other greenhouse gases have led to rising temperatures of the atmosphere and therefore in sea surface temperatures as well. This rise in temperature has two main effects on our coral reefs on top of compounding local threats.\n- Thermal stress - ""Mass coral bleaching, a stress response to an increase in water temperatures, has occurred in every region and is becoming more frequent as higher temperatures are becoming more common. Extreme bleaching events kill corals outright, while less extreme events can weaken corals, affecting their reproductive potential, reducing growth and calcification, and leaving them vulnerable to disease."" (Reefs at Risk Revisited, 2011)\n- Ocean acidification - ""Rising levels of CO2 in the oceans are altering ocean chemistry and increasing the acidity of ocean water, reducing the saturation level of aragonite, a compound corals need to build their skeletons."" (Reefs at Risk Revisited, 2011)\nThe most common local threats include:\n- Destructive fishing techniques (dynamite, cyanide, trawling, etc)\n- Coastal Development, often from tourism\n- Pollution from land run-off\n- Divers damaging corals\nAll of these impacts are increasing as our population increases, with areas of the highest population growth rates coexisting in tropical areas where coral reefs exist. Currently more than 50% of the world’s population live within 100km of the coast and by the end of the decade this is expected to increase to more than 75%. This is of serious concern, considering that currently coastal ecosystems where coral reefs are found contribute 8% of the global GDP and open oceans contributing 25%. GDP.\nAs underwater observers, divers and snorkellers are at the frontline of coral reef protection. They are the eyes and ears of the reefs and are usually the first to see the effects of these threats. They have the unique opportunity to champion reef protection by raising awareness about reef conservation, minimizing impacts from diving activities, to lead by example, and minimize any additional damage, allowing coral reefs to be more resilient to the larger scale threats but also coral diseases that are more likely to infect stressed corals.\nWant to learn more?\n- Become a member - Dive and snorkel centres operating in active Green Fins locations can become members by signing the membership form and pledging to follow the 15 environmental practices of the Green Fins Code of Conduct. Active Green Fins members are then trained, assessed and certified annually by qualified Green Fins Assessors. If you are interested in becoming a Green Fins member, head over to the ‘How to Join’.\n- Resources - You can download some amazing resources from the Green Fins site, including posters, guides and promotional material. Visit Green Fins\' downloads page.\n- Donate - Your donation allows Reef-World to continue their hard work, engaging and working with local communities, governments and NGOs face to face. 100% of the donations go towards the work they are doing - your money makes real changes. Visit the Green Fins’ donation page.']"	['<urn:uuid:fcf1d967-2667-46bb-9213-7a2b67de3aa3>', '<urn:uuid:3c968a6d-36d5-41e0-85e7-590634e893fe>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	14	85	2369
6	How does modern technology influence the dynamics of violence at both the micro-personal level and the macro-warfare level, and what implications does this have for future conflicts?	At the micro level, modern technologies like social media enable public displays of barbaric violence and cruelty, contributing to dehumanization of both victims and perpetrators. At the macro warfare level, advanced technologies like computerized weapons, drones, and cyber capabilities have dispersed battlefields and blurred lines between war zones and civilian areas. While high-tech weapons allow remote targeting, enemies respond by hiding among civilian populations and launching terrorist attacks. Additionally, cyber warfare introduces new forms of conflict that may not involve direct physical violence but can cause significant damage through attacks on infrastructure and financial systems.	"['Ten years ago, two major work about violence came out: “Violence: A Micro-Sociological Theory”, by Randall Collins (Princeton University Press, 2008) and “Violence: A New Approach”, by Michel Wieviorka (Sage, 2009). The two sociologists meet today to discuss their theories and renew the debate for The Conversation France.\nAn article byThe Conversation on June 17, 2018, published in\nCan you tell us more about your earlier approach and how you look at it today?\nRandall Collins: Michel Wieviorka explains well the historic shift in violence since the mid-20th century. Previous conflicts were mainly ritualised encounters that reinforced group identities.\nThen came deindustrialisation, decolonisation, and neo-liberalism, all of which made the source of troubles amorphous and created a malaise manifested in the rise of chronic violence without closure. Wieviorka connects structural change with the phenomenology of individual motivation for violence. My micro-situational approach focuses on the pragmatics and emotions of violence-threatening encounters.\nThese are above all the inhibitions to effective violence in face-to-face communication with one’s opponent, an emotion I have called “confrontational tension/fear”. Humans, when directly focused on each other, become involved in each other’s bodily rhythms. Violence as action is in tension with the tendency toward Durkheimian solidarity through interaction rituals.\nIn such moments, adrenaline and a racing heart tend to incapacitate one’s ability to be effectively violent, leading most often to standoffs with no more than angry gestures, or to wild shooting or hitting. Doing damage in such an encounter depends on establishing emotional domination and setting the rhythm of action while the opponent is incapacitated by such high tension.\nThe historical evolution of weapons has created some alternatives to direct confrontation – the ability to fire at a distance, or by clandestine tactics such as suicide bombers who pretend that there is no confrontation until the bomb goes off.\nThe nature of violence changes as new weapons and social techniques spread, and these drive changes even if the motivation that Wieviorka describes continues to define the malaise of the last 60 years. I agree that our approaches are complimentary; my focus on the micro-situation is pragmatic, seeking the eye-of-the-needle that turns motivation into action.\nWhy violence is chosen?\nMichel Wieviorka: My analysis insisted on the processes through which some people end up as terrorists, or, more generally, act violently. These processes have a lot to do with their subjectivity, with their previous difficulties in life.\nI would say that violence appears and develops when processes of subjectivation and desubjectivation lead the person in question feel that there’s no other way – or no best way – to act. And Randall insists differently on interaction, on processes where violence appears in the relation, in the face to face between individuals and its evolution.\nOne approach is to try and understand how an individual or a group chooses violence on the basis of subjectivity: is it because they look for a meaning in life? Or to modify a given situation? Protest against a situation they view as unfair, or to impose one? Such individuals encounter difficulties transforming meaning into action.\nAnother approach is to understand how through concrete interactions violence may appear and extend.\nRandall Collins, as a sociologist and a novelist, you are now developing analysis on the role of Internet and new technologies in contemporary war and terrorism. How are these new aspects of your work connected with the previous ones? And what would be Michel Wieviorka’s answer?\nR.C: After finishing Violence: A Micro-sociological Theory in 2008, questions remained. Have the macro-dynamics of violence changed, especially geopolitics and war, in an era of computerised high-tech? Is it true, as some have claimed, that Clausewitzian friction has been overcome? Certainly not entirely.\nWestern soldiers who fought in Iraq and Afghanistan report that advanced weapons are not always available when needed – limited by refuelling, repairs, logistics and expense – and so ground troops often rely on old-fashioned tactics.\nMoreover, long-distance weapons guided by targeting information from satellites and drones force enemies to disperse, concealing themselves in civilian populations, emerging for guerrilla attacks on isolated bases and vehicles. Seeking even easier targets, they launch terrorist attacks on civilians. As a consequence, rising civilian casualties establish an atmosphere of moral atrocity, especially when Western soldiers respond ferociously to such tactics. Emotional dimensions of war have not disappeared in the high-tech era.\nTerrorists use mobile-phone communications and GPS to aim and trigger their improvised explosive devices (IEDs), routed through Internet cafes in neutral countries. Cyber-war plays heavily on emotions for sustaining motivation on both sides, making it into a war of competing atrocities.\nUntil now we have seen only asymmetric conflicts, where wealthy, high-tech powers fight militarily weaker forces, albeit under the shared umbrella of the Internet. What would happen in a symmetrical war between two equally advanced forces? This may be the United States versus China in coming decades.\nWhat if “the US military divides and fights against itself”?\nFor now, I have made a thought-experiment: the US military divides and fights against itself. This is my novel Civil War Two (2018). The election of a divisive president splits the United States into north vs. south, and the American Civil War of 1861-65 is replayed, this time with today’s weapons.\nDoes the emotional dimension – the fog of war – disappear? No: because mutual attrition due to advanced weapons forces a return to low-tech forms of war; because cyber-war inside a divided organisation with shared codes causes intense paranoia; over-centralised computer controls become overloaded and break down; massive columns of armoured vehicles become stranded without fuel in a huge traffic jam of urban refugees [an imaginary invasion of New York]; small groups of soldiers can crawl inside the defensive skin of long-distance weapons bases. Victory continues to hinge on breaking down the morale and social coordination of the enemy. This applies also to nuclear weapons, which are above all a form of emotional threat, as the melodramatic conclusion of my novel illustrates.\nImagining a world without Internet or new technologies\nM.W.: Carlos Fuentes in his novel The Eagle’s Throne (2003) imagines Mexico in a situation where there is suddenly no more access to satellites – no more television, telephone, fax, Internet, e-mails… The framework is domestic, and not geopolitical, and what Fuentes proposes is a mixture of very parochial political struggles and sex or love affairs.\nThe sociological lesson is great : let us just imagine our world without Internet and the new technologies, and we can understand better the gap with the previous era. This new civilisation is global, and as Randall demonstrates, military forces don’t have now to be face to face. You can kill at distance. In this global world, states have no more the monopoly of technological or scientific power, the whole civil society is able to use the more modern technologies of communication – and often, innovation, in this field, cannot be controlled by the state, and comes from social actors.\nBut if we entered this new civilisation, what about interactions, what about explaining violence by analysing interactions between those far from each other, when a terrorist cell somewhere in the Middle or the Far East can kill some people in Paris or London without having to be on the spot?\nHave we really entered this new era? And what can we do about it?\nR.C: Are we in a new era of civilisation? Yes and no. The Internet penetrates much of daily life all over the world, because it is one of the cheapest and most easily diffused products of capitalism. Digital technology accelerates the tendency – ever since mass troop charges proved impossible in World War I – to disperse the battlefield, now overturning the distinction between war zone and civilians.\nBut hackers of all kinds may go too far – not just amateurs causing mischief and criminals seeking money, but terrorists hiding in Internet cafes, and governments threatening to destroy the opponent’s electronics-based economy, and to turn the enemy’s computer-controlled weapons against them. A solution to these problems is now being considered in the Western militaries: to shut down the Internet in time of war. This would return us to an older form of living – pre-1980, let us say. History does not always go forward or in a straight line.\nM.W: When 9/11 happened, everyone was dumbfounded. Experts in Washington had imagined many technological terrorist threats – biological, chemical, nuclear. But they apparently didn’t think that potential attackers could buy business-class airline tickets, look respectable, board a plane only armed with very small cutters and be ready to give up their lives the way they did.\nIn this regard, 9/11 taught us that not only do we have to take new technological possibilities as violence tools very seriously, but also that states must be ready to anticipate, keeping in mind human efficiency and attacks free from any form of modern technologies. Recent attacks in Europe where vehicles were used have proven in many ways as efficient as bullets when it comes to spreading terror and death.\nAre you optimistic or pessimistic about possibilities for exit from violence?\nR.C: On the micro level, I am optimistic. Face-to-face, humans are not good at violence. They bluster and threaten and curse, but most small-scale violence – whether in quarrels or in protest demonstrations – ends in stalemate.\nPhysical damage happens when one side achieves emotional domination, confronting a weak or momentarily passive victim whom they can attack without resistance. When both sides mirror each other, maintaining a steady face and voice, replying without escalating, threats dissipate. Prospects are good that more people will learn techniques of keeping anger and fear from escalating, and thus cooling down the possibility of violence. Knowledge of the social psychology of interpersonal conflict is now spreading – in business corporations, in schools, hopefully among police and the people who encounter them. On the micro-level we may get a more peaceful everyday life.\nThis will not come because the world has solved the structural problems that cause the malaise and desubjectivation that Wieviorka has described. Causes for anger remain, but we can make the situational eye-of-the-needle into violence even narrower.\nOn the macro level, I am more pessimistic. In asymmetric war between rich states and embittered insurgents, the cat-and-mouse game continues. Rich states devise more and more electronic surveillance tools and more precise remote-controlled weaponry.\nInsurgents respond with electronic hacking and hiding in the civilian population awaiting the moment to commit atrocities against other civilians; anonymous attacks and counter-measures make life more unpleasant for all of us. The politics of would-be charismatic leaders and routinising bureaucrats keeps stirring up political disputes. International crises are repetitive because they are de-escalated only after they become too costly to continue, and crises reappear because perceptions of the evil done by the enemy stirs up cries for intervention and revenge. Perhaps my macro-analysis is too pessimistic. In any case, it is a reason why I focus on micro-analysis, with its elements of optimism.\nM.W: Randall Collins is right, and we must distinguish at least between the micro and the macro level. But in the former, I am more pessimistic than he is. In fact, I consider that violence when used as a military or political tool is also, in some cases, barbaric, loaded with various forms of cruelty, cases of violence for violence.\nSuch is the case of jihadist violence. Disembodiment of human bodies contributes to a dehumanising process for all actors, performer as well as the victim. And today, this is done publicly as such acts are displayed and staged publicly through social media. In doing so, in order to exist, sustain themselves and feel empowered, some humans destroy others, including their humanity, negating their sense of belonging to the human species itself, not only for them but also for those who are watching.\nAt the macro level, I think it is and will be always very difficult to end with violence. For instance, while a specific form of violence disappears or fades away, other forms can emerge, and take over. Such is the case of Colombia, where the peace agreements put an end to the FARC guerrillas. Yet, in some territories that were under FARC’s control, new expressions of violence are now developing. Some are connected to organised crime cartels and implemented by armed groups that have no political project and which appeared in the vacuum caused by the departure of FARC.\nToday we see new surge of violence everywhere through populist, extremist or nationalist movements. But also an authoritarian tendency in society that could announce new explosions of violence.\nMichel Wieviorka is Director of studies at EHESS. His research focus on violence, conflict, terrorism, racism, antisemitism, social movements, democracy, and cultural differences issues.\nMichel Wieviorka is & Scientific director of the IPEV project and a member of the steering committee.', 'By Other Means\nHas the nature of ""war"" changed since the days of Clausewitz?\nIn a 1942 case called Chaplinsky v. New Hampshire, the Supreme Court held that there is no First Amendment right to utter ""insulting or ‘fighting’ words — those which, by their very utterance, inflict injury or tend to incite an immediate breach of the peace.""\nNeedless to say, this is a doctrine of surpassing vagueness (and it has been substantially narrowed since 1942), since if you don’t know the norms of a particular community, you have no way of knowing what might constitute ""fighting words."" Language capable of causing riots in Des Moines might be merely yawn-inducing in Manhattan, and vice versa.\nSo it is here in the (virtual) pages of Foreign Policy, where last week, Tom Ricks fecklessly posted excerpts from a preliminary concept paper developed by the New America Foundation’s ""Future of War"" project team (of which I am fortunate to be a member, together with Tom, Peter Bergen, Anne-Marie Slaughter, Sascha Meinrath, and a bunch of other smart and interesting people). In the concept paper, we outlined the importance of looking at ""the changing nature of war.""\nInnocuous, you say? Not so!\nTo the Clausewitzian strategy community, dem’s fightin’ words. We might as well have insulted David Ortiz’s mother in Fenway Park, or informed a die-hard Marxist that there’s no such thing as the proletariat.\nWithin days, I received several kind notes from friends and acquaintances, informing me that to the classically-trained Clausewitzian there can be no such thing as the ""changing nature of war,"" and urging the Future of War team to delete this phrase before anyone else noticed our terminological muddleheaded-ness.\nBut it was too late. The initial Future of War posts had already generated a blogospheric Clausewitzian rebuttal, posted by Christopher Mewett in War on the Rocks and soon reposted on the Small Wars Journal’s blog. Mewett takes the Future of War team to task for our lack of ""careful attention to semantics"":\nAccording to the Prussian, war’s nature does not change — only its character…. The nature of war describes its unchanging essence: that is, those things that differentiate war (as a type of phenomenon) from other things. War’s nature is violent, interactive, and fundamentally political. Absent any of these elements, what you’re talking about is not war but something else. The character of war describes the changing way that war as a phenomenon manifests in the real world. Further confusion in this case stems from the Future of War team’s formulation: ""changes in the nature of warfare.""\nWar and warfare are different words with a different meaning, and we should be careful about their use. Warfare, of course, doesn’t have an enduring, unchanging phenomenological ""nature,"" as it is merely the way war is made.\nMewett is far from alone in his insistence on the importance of these Clausewitzian distinctions. As the military strategist Colin Gray has written (not, thankfully, in response to Tom’s blog posts), ""Many people confuse the nature of war with its character. The former is universal and eternal and does not alter, whereas the latter is always in flux.""\nSo did the Future of War team goof up? Should we have recognized that our reference to the ""changing nature of war"" would incite an immediate breach of the peace?\nI’m torn. On the one hand: mea culpa, or we-a culpa, or whatever it is one says when one is part of a group goof-up. Mewett and others are quite right to call us out for rather slipshod use of terminology in the Future of War concept paper excerpts posted on Best Defense. The concept paper represented a preliminary effort to lay out a research agenda, not a polished final product; worse, it was more or less written by committee, which is never a recipe for coherence or great prose (despite heroic editing efforts by Peter Bergen and Tom Ricks). We should have defined our terms with greater care.\nOn the other hand: With due respect to You Know Who and his many erudite interpreters, I’m not quite ready to accept the claim that the nature of war is ""universal and eternal"" — or, at any rate, I’m not sure that this is a particularly useful construct for understanding what is at stake in many current debates about what constitutes war.\nMewett articulates the standard Clausewitzian understanding of war: It is, by nature, both violent and political. Its violence is what distinguishes it from other forms of conflict and contestation (games of chess, for instance, or economic competition between corporations or nations); its political aims are what distinguish it from other forms of violence (such as street muggings or boxing matches). This gives rise to the claim that war’s nature is unchanging: Weapons, tactics, and so on will change over time. But war’s ""nature"" will always consist of these essential components: violent means and political ends.\nUp to a point, this framework is helpful: If we want to theorize about war, (as opposed to particular wars), we need some coherent way to distinguish war from non-war.\nBut does this require that we go a step further, and agree that the ""nature of war"" is eternal and unchanging? Clausewitz himself seemed less sure about this than many of his followers: On this point, admits Colin Gray, ""Clausewitz, for once, is less than crystal clear."" (Others might quibble with that ""for once."") Notes Gray: ""The great Prussian wrote, confusingly, that ‘the nature of war is complex and changeable.’"" Score one for the Future of War Team!\nRegrettably, as Gray goes on to remind us, Clausewitz also wrote that ""all wars are things of the same nature."" Being long dead, Clausewitz is not available to clarify these seemingly contradictory statements; Gray, however, concludes that Clausewitz simply erred in his first statement, but was ""correct in the second claim.""\nHmm. Was Clausewitz’s first statement just incorrect? Tell it to the numerous scholars, military strategists, and other commentators who speak — not at all metaphorically — of ""cyberwar,"" ""financial war,"" and so on.\nTake cyberwar: Much of what is often spoken of under the ""cyberwar"" rubric is not violent in the Clausewitzian sense of the word. Cyberattacks might shut down the New York Stock Exchange and cause untold financial damage, for instance, but would we say that this makes them violent? Or, say cyberattacks shut down the electrical grid for several major cities, and as a result of the loss of power, a few hundred hospital patients on ventilators die. I’m still doubtful that most of us would call this violence in the usual sense of the word.\nSome argue that activities in cyberspace cannot truly be considered war unless they lead directly, rather than indirectly, to physical destruction or injury. STUXNET, for instance, damaged computers and centrifuges. Even this, however, seems very far away from violence in Clausewitz’s sense of the word.\nSo, if one believes it possible to speak of cyberwar or financial war, might one not argue that the ""nature"" of war is indeed changing?\nThe Clausewitzian could of course respond that this is simply a category mistake: You can blather on all you want about cyberwar or financial war, but if what you’re talking about is not both violent and political, it’s just not ""war,"" but something else. It may be an important something else, even a world-altering something else, but it’s still not war. Or maybe those who speak of cyber or financial war are just confusing ""war"" with ""warfare"": the latter includes, as an ancillary matter, numerous non-violent activities (logistics, planning, communications), but these do not, in and of themselves, constitute war.\nThese possible responses strike me as too facile, however. The assertion that war’s violent and political nature is eternal and universal — and that therefore anything that is not both violent and political cannot constitute war — rests upon the further (though usually unstated) assumption that violence and the political are also fixed, eternal, and universal.\nThe assumption here is that violence is (and only is) what Clausewitz deemed it to be: ""physical force,"" directly applied. But violence, like war, is a term that derives its meaning from the broader social and linguistic framework in which it is embedded. (So too for the term ""political,"" and, for that matter, for pretty much every other word and concept.)\nMost of us think of violence as the use of physical force to damage or injure for the purpose of inflicting suffering or asserting dominance. We thus don’t view surgery as an act of violence; similarly, we don’t think using a bulldozer to destroy a house constitutes violence if the bulldozer is operated at the behest of the homeowner, but we do consider it violence if it’s operated by, say, Israeli military forces and used to destroy homes in Gaza.\nBut there are many other ways to understand and define violence. Consider various forms of psychological torture or abuse. Or consider cyberattacks that lead to loss of life as an indirect result of extended power outages: Why not view such attacks as a form of violence if they lead predictably to loss of life? Or what about economic sanctions, for that matter: Numerous U.N. reports concluded, for instance, that economic sanctions against Saddam Hussein’s Iraq led to the deaths of hundreds of thousands of Iraqi children as a result of malnutrition. Insofar as these deaths were the predictable consequence of the sanctions regime, why not call the sanctions a form of violence, as many critics (including many ordinary Iraqis) did?\nTheorists have been squabbling over definitions of violence for a long time, struggling to draw clear and coherent distinctions between violence and a host of related concepts, such as force, power, authority, strength, coercion, and dominance. And my point right now is not that one way of understanding violence is right and another is wrong; rather, it’s that the term ""violence"" can itself be understood in multiple different ways, and different societies will conceive of violence in different ways at different times. The same is true of the idea of ""the political.""\nBut if this is so, then saying ""the nature of war never changes"" devolves into a meaningless truism. War’s nature is violent and political — but if what we place into the categories we label ""violence"" and ""politics"" can change, then what do we gain by claiming that war’s nature never changes?\nChristopher Mewett’s critique of the Future of War concept paper raises another interesting issue. Our concept paper, notes Mewett, argues that ""changes in the nature of warfare … (This time we said warfare!) profoundly shape both the manner in which the state is organized and the law itself."" But, says Mewett, ""it’s almost certainly more true that the manner in which the state is organized shapes the character of its wars. There is something of a feedback cycle in play: Social, political, and technological change impact the way wars are fought, and those wars often influence the way society and politics are organized. But war is a subset of politics, of human society — something that the Future of War team seems to have just as backwards as the technologists and Revolution in Military Affairs advocates who preceded them.""\nThis criticism, I think, rests on a misunderstanding of our arguments (though we may have made such a misunderstanding possible by breaking up a longer concept paper into shorter excerpts posted on different days). I don’t think anyone on the Future of War team would disagree with Mewett’s characterization of the complex feedback cycle between how wars are fought and how societies are organized; we cite, for instance, Charles Tilly’s famous line, ""War made the state and the state made war.""\nIn a strict sense, Tilly, too, could presumably be taken to task for his terminology: If war is eternal and universal, the state can shape ""warfare"" or war’s character, but not ""war"" itself, since war predates the state. And the specifics of Tilly’s historical argument do indeed relate to modes of warfare, rather than war as such.\nThe state chooses which wars to fight and how to fight them. But the modern, neo-Westphalian state does more than that. It also defines war, from both a legal and institutional perspective. It is the state that creates and defines the role of the military (defined, more or less, as ""that state institution designated as responsible for the activity called warfare""), for instance, assigning it certain tasks and assigning certain other tasks to non-military entities. It is also the state that defines the legal contours of war.\nTo put this in terms of current issues, the U.S. government can decide to create a military Cyber Command and recruit cyberwarriors, or it can decide that cybersecurity should be the sole province of civilian agencies and that it has nothing to do with war. Similarly, as a legal matter, the state can decide to treat cyberattacks as acts of war, subject to the laws of war, or it can decide to treat them as crimes, subject to criminal law.\nIn other words: Clausewitzian verities notwithstanding, it is the state — the putative monopolizer of the means of legitimate violence, and the ultimate war-making entity — that literally makes war. It is the state that chooses to place some things into the institutional and legal basket labeled ""war,"" and place other things into different baskets.\nThis matters. Consider a hypothetical: Let’s say that al Qaeda forswears the traditional tools of physical violence and focuses solely on cyberattacks designed to bring down America’s financial infrastructure. If the state decides — through legislation or executive fiat — that such cyberattacks constitute ""war"" in a legal and institutional sense, then a teenage al Qaeda hacker would be a ""combatant."" If he’s a combatant, he can be captured and detained indefinitely without trial or charge. (Not so if cyber___ is not a war.) In fact, if he’s a combatant, even if only in the cyber domain, he can be targeted and killed by the military using non-cyber tools: a missile fired from a drone, for instance.\nWar, as Clausewitz emphasized, is a social phenomenon; it is inextricably bound up with and defined by choices that are made in the political realm. And there is indeed a feedback cycle: When the state adds more to the basket labeled ""war,"" it can change the relationship between individuals and the state — by altering ""rights,"" for instance, or altering the degree to which the state’s use of power is subject to internal checks and balances. Ultimately, by adding more to the war basket, the state may set in motion a cascade of changes that end by transforming the state itself.\nI said a moment ago that the state ""chooses"" what to put in the war basket, but this of course is to anthropomorphize the state. States are made up of people, and the choices of the people who make up the state are influenced and shaped by arguments made in military schoolhouses, Congressional hearings, academic journals, Sunday talk shows, and yes, even blogs and Foreign Policy articles. That means that we’re part of this, too: Colin Gray and Christopher Mewett, Tom Ricks and I, and everyone else who takes part in conversations and arguments about the nature and definition of war.\nArguments about the definition of war are always, in some sense, efforts to shape, constrain, or channel violence and power. The words we use to define war are always ""fighting words."" When our words are adopted by the state itself, those words can be transformed — via state institutional arrangements and via the law — into violence; they can literally inflict injury or cause a breach of the peace.\nBut, you ask, where does this leave our Clausewitzian aficionado?\nTake my hypothetical teenage al Qaeda hacker — and imagine, if you will, that he is a Clausewitz aficionado. Informed that the United States considers him a combatant who can be detained or targeted under the laws of war, he declares firmly that this is definitionally inappropriate.\n""As all Clausewitzians know,"" explains the young hacker, ""war’s unchanging nature is both violent and political. My own cyberactivities are aimed at toppling the U.S. financial infrastructure, and while the United States naturally objects to these activities, they are clearly not ‘violent’ in the Clausewitzian sense. This means that whatever I and my fellow al Qaeda hackers are doing cannot possibly constitute ‘war.’ And since there is no war,"" the precocious, Clausewitzian al Qaeda hacker concludes triumphantly, ""the laws of war are not applicable in this situation. I therefore cannot be a considered a combatant, and I cannot be detained without due process of law. I most certainly cannot be killed by the U.S. military.""\nTo this, our anthropomorphized state might respond with an icy shrug: ""Tell it to the judge.""\nOr: Tell it to the Marines, or the Hellfire missile, whichever comes first.\nAuthor’s note: This article is too long, but still not nearly long enough to do the subject full justice. Don’t worry, though: The Future of War project is going to get it all figured out, tout de suite — with your help! If you think I got everything dead wrong in this piece (or that the Future of War project has it all wrong), you can email me at firstname.lastname@example.org, or submit an entry to Tom Ricks’s Best Defense Future of War Essay Contest. If you are interested in war and language, there’s a lot out there by people who really know what they’re talking about, but my own arguments in this column are amplified here, here, and here, as well as in some of my longer academic articles, which you can find here.']"	['<urn:uuid:baee295f-845c-4cb7-87d3-d13050b97f1c>', '<urn:uuid:30b0c373-52d0-4412-ab1d-ef0bcd7e18c6>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	27	95	5099
7	in aquarium vs pond which water parameter more important calcium magnesium	While both environments need calcium and magnesium, they are particularly crucial in pond environments for koi carp, as calcium makes up 2/3 of total water hardness and is essential for osmoregulation, bone tissue, scales, and blood-clotting. For Mamou Killifish in aquariums, specific calcium/magnesium ratios are not as critical as long as the general hardness stays within 5-12 GH.	"[""Mamou Killifish (Scriptaphyosemion guignardi mamou)\nMamou Killifish are quite a rare species that is not very known in the hobby. These Killifish are an ideal fish for the nano or planted aquarium and will add some colour and activity to it. In addition, this Killifish is not demanding and can adapt to a wide variety of water conditions. However, due to their small size and care requirements, they are not suitable for the beginner aquarist.\nMamou Killifish are very shy and easily spooked; therefore, you should choose their tankmates carefully, or they will be out-competed for food. These fish will do much better if you house them in a species only tank or with similarly sized species.\nMamou Killifish are not suitable for the general community aquarium because they can be very aggressive with slow-moving small fish. They also have surprisingly large mouths for their size. These species are also known to be aggressive towards one another, so make sure you provide sufficient space and hiding places; that way, you can maintain a group together.\n|Scientific Name||Scriptaphyosemion guignardi mamou|\n|Aquarium Level||Middle - Top|\n|Best kept as||Trios|\n|Lifespan||1 - 2 years|\n|PH||5.0 - 7.0|\n|GH||5 - 12|\n|71 - 77℉|\n21.7 - 25℃\nMamou Killifish are endemic to the Mamou Region in the Republic of Guinea as well as in Mali and Senegal in Africa. These Killifish inhabit slow-moving lowland streams, brooks, temporary water-filled holes, pools, swamps, marshes and lowland floodplains in humid rainforest areas with moderate lighting and some dense vegetation.\nOther Killifish of interest\nWhat to feed the Mamou Killifish\nIn the home aquarium, Mamou Killifish will accept high quality dried flake and pellet foods as well as algae wafers. Still, it would be best if you also offered your fish frequent meals of small live or frozen fares such as daphnia, Cyclops, mosquito larvae, bloodworm, and artemia. This will not only bring out the vibrant colours but will also help to encourage spawning.\nThese fish can be shy at feeding times, so you must ensure that their tankmates are not too boisterous, so they don't get out-competed for food and receive their fair share.\nHow to Sex the Mamou Killifish\nIt is simple to distinguish male from female Mamou Killifish. Males have a bronze upper body with bluish-green coloured sides that are accentuated with dots that form chevrons in the anterior part of the pelvic membranes.\nTheir fins are also greenish-blue with red dots except for the dorsal and caudal fin that has a red sub-marginal stripe and a light blue marginal stripe on the edge. The anal fin also has a red line on the edge.\nIn contrast, females are an olive-brown colour with a few red dots. They also have black specks that define a thin dark line on the middle of the body, and their fins are a solid yellow colour.\nHow to Breed the Mamou Killifish\nIt is relatively straightforward to breed Mamou Killifish, and you can quickly breed a pair in an aquarium. However, it is recommended that you should produce them in trios, but the yield tends to be lower when they are bred this way, probably because the fish that are not spawning may consume some of the eggs.\nMost breeders do not use filtration in a killifish breeding setup; instead, they use a small, air-driven sponge filter to prevent stagnation. The water needs to be slightly acidic with a slightly higher temperature. It would be better to keep the tank either dimly lit or unlit and remember peat filtration is beneficial.\nIt would be best if you conditioned your fish on a varied diet of live and frozen foods and keep the two sexes apart in separate conditioning tanks. In addition, it is better if you choose the best male and fattest female before placing them in the spawning tank. This method will allow females to recover between spawnings.\nMamou Killifish will deposit their eggs either in clumps of vegetation or in the substrate, and the spawning medium can either be clumps of fine-leaved plants, spawning mops, java moss or a layer of peat moss on the bottom of the tank. You can also have a bare-bottomed tank with the spawning mediums. This setup will make maintenance and egg collection a lot more straightforward.\nIf your water conditions are right and the fish are well conditioned, spawning should exhibit no particular problems. You can leave the eggs in the aquarium to hatch with their parents; however, some may get eaten.\nTen to twenty eggs are usually deposited daily for around two weeks, and if you would like to increase the yield of the fry, you should remove the eggs gently as soon as you notice them.\nIt would be best only to allow breeding pairs to spawn for about a week before returning them into the conditioning tank as the spawning process is challenging on the fish, especially the female. In addition, they can become weak and tired if left for too long.\nOnce removed, you can incubate the eggs either by placing them on a damp layer of peat moss in a small container or leave them in the water. Fewer eggs tend to fungus if you keep them in the water, although you should still remove these fungus eggs as they are noticed.\nIf incubating in water, you can transfer the eggs to a small aquarium or a container containing water from the spawning tank. A few drops of methylene blue is advisable as this helps to keep the eggs in good condition. It would be best if you kept the aquarium or container under darkness as the eggs are susceptible to light, and you will need to check the eggs daily for fungus eggs, which you should remove with a pipette.\nThe eggs will hatch in around 12 days, depending on the temperature.\nIf incubating on peat moss, place the container in a warm, dark place and leave it for 18 days, after which the eggs will be ready to hatch.\nIf you are spawning various offspring, it is excellent to label each container with the date, hatching date, species, and the number of eggs to limit any disasters.\nHatching can usually be induced by simply placing the eggs in the raising aquarium after 18 days, where the wetting of the eggs stimulates hatching. If this is unsuccessful, blowing gently into the water through a piece of airline or straw can trigger hatching.\nThe fry is tiny, and you should initially feed them with infusoria. If you decide to use the peat moss incubation method, you can seed the rearing tank a few days before hatching by adding a couple of drops of green water or liquifry. Alternatively, add small amounts as required.\nAfter about two days, you can feed them on microworm or brine shrimp nauplii, introducing larger live and frozen varieties about two weeks after that.\nYou should initially keep the water very shallow, but you can increase the level as they grow. Ultimate care must be taken regarding water quality in the rearing tank as the fry are very susceptible to velvet disease, so small water changes every 2 to 3 days is ideal for the best condition and growth."", 'Guide to Koi Pond Water Hardness 2020 (Testing GH & Ideal Parameters)\nWater hardness, or “general hardness”, refers to the mineral content in water, which includes calcium, magnesium, borate, silica, and iron. These metal ions are a result of carbonic acid within rain dissolving mineral-rich rock, such as limestone, as it runs naturally through layers of sediment, rivers, and streams. Along the way the water picks up dissolved minerals, and the more minerals it picks up, the harder the water is considered. In areas with little limestone or dolomite formation, water is usually considered soft, as it contains much less dissolved mineral content.\nWithin both the aquarium and pond hobbies, general hardness is used to determine the total dissolved mineral concentration in water to ensure it’s safe (or optimal) for fish keeping. Depending on where you live, your water will have varying degrees of hardness, and may contain a different concentration of the various metal ions that contribute to its overall hardness. Calcium (Ca++) usually makes up 2/3 of a waters total hardness parameters, with the remaining 1/3 being magnesium (Mg++) and trace amounts of other metal ions – this is also why it’s sometimes called “calcium hardness”.\nWater hardness parameters are often measured in ppm (parts per million) or milligrams per liter (mg/L), and can be thought of simply as the total amount of metal ions dissolved within an aqueous solution.\nNote: The different terms for water hardness are commonly confused with “carbonate hardness”, which actually has nothing to do with water hardness and is a different topic altogether. Alongside general hardness, you also have other closely linked water quality parameters known as alkalinity and KH (carbonate hardness). This is where confusion can arise, as not only do you measure these in similar ways, but the definitions sometimes overlap depending the source of literature. To make things simple, we only touch upon alkalinity in this article and use the term KH (carbonate hardness) as it’s more common within the koi hobby.\nHard Water Vs Soft Water – What’s Better for Koi Carp?\nA long debated topic within the koi keeping hobby, and one that is still actively discussed within the modern trade. Depending on hobbyist you ask, some will claim that soft water is better for koi, and others will stand by hard water..\nLooking deeper into the science behind water hardness in aquatic culture, all things seem point to a single conclusion: harder water is better for koi, so long as the hardness comes from calcium and magnesium – but why is this the case?\nA major concern with very soft water is the potential lack of dissolved salts, such as calcium and magnesium, which are important for the regulation of electrolyte (salt) levels within blood of fish through a process called Osmoregulation. Calcium and magnesium ions, found in higher concentrations in harder water, are essential to many important biological processes of fish, such as the ability to create new bone tissue, scales, blood-clots, and metabolic reactions. Fish, including koi carp, will actively absorb both calcium and magnesium from the surrounding water or from their food source, with dissolved calcium being particularly important for the regulation of salt levels within the blood. Having a higher presence of free-ionic calcium within a koi pond helps reduce the loss of other important salts, such as sodium and potassium, during the excretion of bodily fluids. Both sodium and potassium are vitally important in koi and work to maintain healthy heart, nerve, and muscle tissue (1). Calcium works to not only prevent the loss of these salts from the blood, but is also required for the re-absorption of new salts that are lost throughout the day (2). Without calcium, such as in extremely soft water, koi will need to use much higher amounts of energy to replenish lost salts within the blood, which can lead to less growth, weak bones, and frequent illness.\nAs well as this, harder water helps lock down toxins extremely well, including metals such as zinc, lead and copper, which can easily dissolve in soft water and become dangerous. Carbonates and bicarbonates are also essential to the biological processes of many micro-organisms and bacteria, including the beneficial bacteria in your filter box. They will consume a large volume of carbonate compounds from the water on a daily basis, and this is one of the reasons regular water changes are important in koi keeping – the replenishment of trace minerals and alkalinity (i.e., carbonates). As a final note, and although water hardness does not always equate to higher alkalinity, if the hardness is due to calcium and magnesium ions, your pond will have higher buffering capacity against dangerous changes in pH – also known as its KH value. The higher the KH, the more stable your pond pH will be against possible swings in pH and crashes.\nHow to Test Water Hardness in Koi Ponds – What are Ideal GH Values?\nYou can test general hardness with many commercial testing kits, such as API Ponds GH Testers, but this is also where things can become confusing in terms of definitions. General hardness, in most pond applications, will refer to the total concentration of dissolved calcium++, magnesium++, and other trace metal ions in water, with the results are often presented as mg/L (ppm) calcium carbonate [CaCO3]. Results are displayed in this way as CaCO3 has a molecular weight of 100, which makes it easier to equate values in comparison with other molecules.\nIt should be noted that KH measurements in areas of high limestone hardness will often be identical to the GH measurement, as both are presented as total mg/L [CaCO3]. This can be confusing at first and may give the impression that the separate tests are incorrect, but it’s actually normal in most cases. This is because the “calcium” makes up the hardness, and the “carbonate” makes up the KH (alkalinity), and although their individual weights within the molecule are different, together they’re still measured as just calcium carbonate [CaCO3] – hence the same result. Taking this into account, and avoiding the inner details of alkalinity (a topic for another time), an “ideal” KH range would be very similar to an “ideal” GH range, as both are formed primarily of calcium carbonate.\nIf you’d like to get an accurate measurement for just calcium by weight, not including magnesium and other potential metals, we recommend a test kit designed for calcium hardness, rather than general hardness, such as Taylor Technologies Calcium Kit.\nAlthough it is difficult to determine ideal parameters for koi, a commonly accepted range within the aqua culture industry is a free calcium++ concentration of 25-100 mg/L (ppm), which equates to an approximate GH hardness of 65-250 mg/L (ppm) CaCO3. Taking this further, many fresh water fish, including wild carp, have a calcium blood concentration of around 100 mg/L (ppm) (3), so for optimal osmoregulation and bodily function, a hardness as high as 250 mg/L CaCO3 could still be considered within the ideal range. Where hardness is due to limestone, the CaCO3 hardness results often reflect a mix of both free calcium and magnesium, with trace amounts of other ions.\nThe table below shows approximate free calcium water concentrations converted to general hardness parameters in mg/L (ppm), with green being the optimal average range for freshwater fish, including koi carp:-\n|Free Calcium [Ca++] (mg/L) (ppm)||General Hardness [CaCO3] (mg/L) (ppm)|']"	['<urn:uuid:d2e2c39a-bc42-47d5-84db-0bfc534834bf>', '<urn:uuid:51ff33bb-5e43-44a2-98c1-4a9145775289>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T04:43:46.574121	11	58	2437
8	Why do criminals stay away from the planet Juoi?	Despite being close to Hutt Space, Juoi maintains a low crime rate because they take law enforcement very seriously. The planet has gained a reputation among spacers as a citadel planet - pleasant to visit and live in, but only if you follow the law. The Juoi Security & Inquiry (JSI) is known for their ruthlessness in hunting down criminals and their efficiency in doing so. The population, while generally peaceful, is also fiercely protective of their freedom and has successfully pushed back pirates, invading Hutts, and even the Empire.	"['- ""Don’t let Juoi’s nearness to Hutt Space or the friendliness of its people fool you. Break the law and you’ll pay for it. They don’t mess around down there.""\n- — Anonymous Pirate\nJuoi is an independent single-planet government that maintains a low crime rate despite being adjacent to Hutt Space. The population is a mixed variety of alien species who, while generally peaceful, are fiercely protective of the freedom that Juoi provides. The intensity of their loyalty to Juoi has been enough to push back pirates, invading Hutts, and even the Empire. Juoi has gained a reputation from spacers as a citadel planet; pleasant to visit and live, but only if you don’t break the law.\nJuoi is located on one of the outermost regions of the Mid Rim, somewhat close to Hutt Space. It occupies no location of strategic importance and has very little mineral wealth in its system. For these facts neither the Galactic Republic, Empire, or New Republic have shown much interest in Juoi. In the past, this made Juoi a prime target as a hideout for criminals seeking to escape the law, which helped necessitate the formation of Juoi Security & Inquiry.\nEven though it is small and not mainstream, Juoi has given birth to a few people of galactic importance. The most famous of Juoi’s citizens are the Jedi Tulsar Leidias and his daughter Kalja. Tulsar has been responsible for thwarting several galactic threats, but he has also been surpassed by Kalja, a powerful Jedi in her own right, who has become a member of the esteemed Jedi Council.\nJuoi was originally supposed to be named ""Joy,"" but Talrim Mirlak, the founder of the first colony, had an odd quirk. Talrim was an excellent speaker, but he tended to spell out Basic words phonetically according to his own alien mind; when he registered his deed with the Galactic Republic, the colony of ""Joy"" unavoidably became transposed into ""Juoi."" It wasn’t until about half a year later that the colonists discovered this, but instead of correcting the problem, they embraced it and changed the name of all the major locations to fit. It has since become a tradition on Juoi to phonetically misspell location names pertaining to the planet.\nThe only planet in the Kredle system capable of supporting life is Juoi itself. The other six planets are either dead worlds or gas giants. Much like Juoi, the other planets were named by the first colonists in phonetic misspellings of Basic. Unlike Juoi, the misspellings were deliberate to stay consistent with Talrim\'s mistake.\n- Syndur - Syndur is a world that is close enough to the system\'s sun to keep its surface constantly molten. It has no atmosphere to speak of and is so close that only probes operating in the planet\'s shadow can get close enough to record data without being melted by the sun\'s heat.\n- Reushor - A planet barely larger than the average moon, Reushor has the speediest orbit out of all the planets in the Kredle System. Even taking into account that it\'s the second planet away from the sun, the reasons for the speed of its orbit are not fully known.\n- Cibleng - Often called Juoi\'s sister planet, Cibleng is a little larger than Juoi but has a much smaller core that is almost completely inactive. Its planetary rotation is very slow and a typical day on Cibleng is roughly as long as a Juoian month. It has a very thin atmosphere and is dominated by rocky wastelands.\n- Juoi - A vibrant world full of life, it is the home of the Sovereignty of Juoi.\n- Kolasel - An enormous gas giant that has a wider diameter than all of the other planets in the system combined. It is surrounded by a total of 78 moons.\n- Eyce - A cold, rocky planet completely devoid of water of any kind.\nJuoi could be considered a young world, geologically speaking, and its four continents are still relatively close together. The rest of the planet is all one large ocean known as the Endless Sea.\n- Paradis - The second largest continent on the planet, Paradis has a temperate landscape that houses the capital, Chiarm, as well as the bulk of Juoi\'s population.\n- Stoni - Easily the largest of the four continents, Stoni has a greater landmass than the other three continents combined. The main reason it has a smaller population than Paradis is due to the fact that it is mostly rocky deserts and mountains. Several large mining operations are in place on Stoni to take advantage of the resources that can be found on the continent, but the majority of those who live on Stoni are settled along the coastline.\n- Ixplor - The third smallest continent, Ixplor is mostly dense jungle. Only a single town, Kol\'ani, has been established on it so far. With Pek Kular settled by the Sages, Ixplor is now considered the most mysterious and dangerous of the four continents.\n- Pek Kular - Pek Kular is the smallest continent on Juoi and for a long time was thought to be cursed due to the fact that every expedition sent to it failed to return; it turned out that the superstition wasn’t far off as it had once been used by the Sith. The continent was given to the Sages after their help in driving off the Darksider known as Sivter. Upon assuming ownership of Pek Kular, the Sages managed to cleanse Pek Kular of all remaining traces of the Dark Side.\nPek Kular is the only continent that doesn’t fall under the direct control of the Sovereignty of Juoi. The Sages are allowed to largely govern themselves, however a representative is always in communication with the JSI.\n- Directorial Palace - Located in the capital city of Chiarm, and housing the Director and top government officials of Juoi, the Directorial Palace is an impressive sight, as lavish as it is functional. A work of art turned into a building. It easily stands apart from the rest of city. Hidden among the palace\'s ornate decorations are shield generators, weapon emplacements, and security cameras, ensuring that the Directorial Palace can withstand any attack.\n- Ruins of Pek Kular - For most of Juoi\'s modern history, the ruins of Pek Kular remained a mystery. All anyone had to go on were satellite images since no one sent to Pek Kular ended up returning. Following the return of the Darksider, Sivter, it was learned that Pek Kular\'s strange ruins were Sith in origin, built by a Sith Lord known as Sorij Xor. Sivter unintentionally did a service by absorbing the Sith Lord\'s spirit that had been haunting the ruins and thus vastly weakened the aura of the Dark Side that permeated the land. After the Sages were given control of Pek Kular, they cleared out the ruins with the guidance of Tulsar Leidias. Anything Tulsar thought might be considered dangerous was removed from the site and locked away in one of Juoi\'s most secure depositories. What is left today are the remains of the temples and buildings themselves but little else. The connection the ruins have with the Sith is downplayed by official sources for obvious reasons.\n- Mersi Station - Juoi\'s main trading hub and customs facility.\nUnbeknownst to the colonists who would settle Juoi thousands of years later, and indeed the entire galaxy, the planet that would come to be known as Juoi was first used by the Sith Lord Sorij Xor towards the end of the Great Hyperspace War. Xor fled from Sith space in his damaged battleship when it became apparent that the Sith had lost the war. After a few months of avoiding Republic fleets, Xor\'s battleship finally gave out in what was known then as the Torgassi System, whereupon he and the remaining crew evacuated before his ship plunged into the Endless Sea.\nThey ended up on the continent that would become known as Pek Kular. Xor, believing that he might be the last Sith Lord, was initially determined to create a new Sith Empire. Unfortunately, the realities were that he had neither the resources nor the population to sustain his dream. Instead, Xor deliberately misled his people into thinking it was possible as he ordered them to construct new temples. When their work was done, Xor drained the life out of every last one of them so he could sustain his spirit until such time as the opportunity would arise for him to direct the crafting of a new Sith Empire.\nThe Torgassi System would eventually be lost to time following several more wars in the Republic’s history until it was rediscovered by a scouting team thousands of years after Xor\'s landing. From there the uninhabited system would come to the attention of an Isihi Tib businessman named Talrim Mirlak, who renamed the system ""Kredle"" in preparation of founding a new colony on the world he would dub as Juoi.\nThe ""First"" ColonyEdit\nUpon learning of Juoi, Talrim used his financial wealth and people skills to form a large group of like-minded individuals to colonize the newly explored temperate planet. His goal was to provide a peaceful place for those people who just wanted to get away from the rising tensions of the galaxy, and in large part his dream would succeed. Talrim’s goal would soon seem prophetic as just one year later the Trade Federation invaded Naboo.\nThe new colony would see its share of problems, not the least of which was dealing with the criminals who saw Juoi as the perfect staging ground for their activities thanks to it being so close to Hutt Space. After barely fending off a massive pirate invasion, Talrim tasked one of his most trusted people, a Trianii named Sulimurr, to form Juoi Security and Inquiry, or simply JSI for short. The JSI quickly became known for their ruthlessness in hunting down criminals and the efficiency in which they did so. They became both the local police and the military.\nSadly, Talrim died of natural causes shortly before the Battle of Yavin, but his replacement proved to be more than adequate; Sulimurr. Under her guidance the space above Juoi became much more secure. Using some contacts within the Corporate Sector Authority, Sulimurr was able to purchase an old Victory Mark II and three Marauder Corvettes complete with IRD’s for protection against pirate fleets.\nShe also commissioned the construction of Mersi Station, a large and heavily modified Golan Space Colony III that would serve as neutral territory for smugglers, merchants, legitimate traders and the occasional Juoi citizen. With Mersi’s construction, a rapid influx of economic growth followed for Juoi, allowing Sulimurr to keep the space fleet in top condition and provide the best training for its pilots. Mersi also became the new base for Scarlet Squadron, the wing of Y-Wings that Talrim had purchased to defend Juoi before his death.\nAside from the occasional pirate attack, there has been only been a few other major events of note in Juoi’s history. A Darksider named Sivter came to Juoi seeking information but ran up against a local Jedi called Tulsar Leidias and his family. Despite several skirmishes with Tulsar, Sivter was eventually able to get the information he sought. In order to teach the Jedi a lesson, Sivter murdered Tulsar’s wife and son. He also sent false information to the Empire, indicating that there were Rebels hiding out on Juoi before escaping.\nThe Empire sent in a small investigative force that was a little more powerful than Juoi’s fleet. However, with unexpected ferocity and skill, the Juoi space fleet was able to route the Imperial forces in the Battle of Juoi. Due to Juoi being considered insignificant to the Empire, it was deemed to be too much trouble for too little gain to justify sending in another task force. Despite that attack by the Empire, Juoi would continue to hold on to its policy of neutrality.\nTulsar and his family were very respected on Juoi and it didn’t take much to convince Director Suilmurr to loan him a Marauder for the express purpose of hunting down Sivter. Tulsar left immediately, taking his only daughter, Kalja, with him. Tulsar would return periodically with news but never for very long. As fate would have it, one of these return trips would be followed by the return of the Empire and Sivter as well.\nReturn of SivterEdit\nLife on Juoi was relatively quiet following Sivter\'s defeat and remained that way for several years but the peace was not destined to last. In the midst of one of Tulsar\'s return visits, Juoi was visited by Imperial captain, Fen Palka, who claimed to be coming in peace. Understandably skeptical, Sulimurr sent Palka on a mission with Tulsar to prove the Imperial\'s sincerity. It would be on this mission that they would discover Tav Garvin frozen in carbonite and learn that Sivter had been behind the action because Tav had inadvertently stumbled upon the Darksider\'s plans. Tulsar would also learn that Palka had really come to Juoi in the hopes of being trained in the ways of the Force.\nKnowing it was only a matter of time before Sivter showed up, Tulsar agreed to train Palka and a Sage named Dirin Ofnasha. Since time was a factor, the training was highly abbreviated, covering only the major points of being a Jedi. Sivter appeared a few weeks later, having absorbed the spirit of Sorij Xor and had become incredibly powerful as a result.\nTulsar, Palka, and Dirin faced off against Sivter to little effect. Sivter was able to manipulate Palka into falling to the Dark Side and despite their best efforts, Tulsar and Dirin were no match for Sivter. When things looked their darkest, Tulsar was able to communicate to a newly arrived alien species, known as the Sages, how to exorcise Xor\'s spirit and power from Sivter. Severely weakened by the withdrawal, Sivter fled. With the Darksider\'s defeat, Palka was able to renounce Sivter\'s control, but didn\'t stay long - instead choosing to leave with his disgrace.\nIn thanks for their defense of the planet against Sivter, Sulimurr granted the continent of Pek Kular to the Sages, giving the displaced aliens a new home. With Tulsar\'s guidance, they were able to remove the last traces of the Dark Side from the land and the any dangerous material that could be found in the ruins were moved and placed under heavy guard.']"	['<urn:uuid:01cfb696-946a-4a9c-b6c6-920344d3a35d>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	9	90	2423
9	How do flash units work in cameras?	Flash units work through a capacitor charged by battery. When triggered, the capacitor releases its entire charge instantly through the flash tube, ionizing the gas inside. The light output's power depends on the capacitor size and the square of the voltage at which the device operates, typically quoted as the guide number.	['No matter if bundled with your camera entire body or connected by means of a hot-shoe connection, primary flash models were being built as comfort for getting the shot within the absence of any practical light. Throughout the wide variety of photographic lighting, on-camera flash is mostly a benefit, and it’s important to appreciate its constraints. Most on-camera flash units are built-in (some show up at will), nonetheless some are detachable and match over the camera’s accessory shoe. Each one of these units are designed for compactness and ease of use inside these as priorities, excellent and selection of lighting choose 2nd spot. Whilst this type of flash has its makes use of on spot, it will never have just about as various strengths given that the makers would really like you to definitely imagine. Yet, the sophisticated metering and exposure manage within a DSLR camera permits you to combine flash with current lights for many dynamic outcomes digital goja.\nFlash units get the job done by way of a capacitor billed by battery. When induced, the capacitor releases its total demand instantaneously through the flash tube, ionizing the fuel within. Concentration on the mild output relies upon on how huge the capacitor and about the square from the voltage at which the products operates, and is also usually quoted as the guide amount.\nThe restrictions of full flash illumination tend to be the forms of frontal lights. Place simply, the lights is sort of shadowless and additionally, it falls off equivalent in proportion on the distance in the camera. A standard purely flash-lit photograph is likely to characteristic flat illumination more than the principle subject plus a dark track record. The result is evident, sharp, and with very good coloration separation, but is frequently missing in ambiance. Standard superior uses of full-on flash are close-ups of colourful subjects, simply because these may possibly advantage through the crisp precision and powerful shades afforded by flash illumination.\nOne among various special challenges in altering the kind of light digitally could well be to produce the outcome of brilliant, sharp light-weight, but there’s nonetheless software program obtainable that should help. Certainly one of the fundamental question in picture enhancing is when significantly you’ll want to go – that may be definitely, just how much you should to get off the initial just how it was shot. In basic principle, everything may very well be improved; in follow, you should think of regardless of what you individually truly feel is appropriate and on the amount hard work it may possibly be really worth for you individually.\nWith daylight photography, the most crucial hurdle is bringing light within the photograph. If you’ve waited for almost any crack inside the clouds to brighten up the scene, you’ll be aware that you’ve an interest on this – and also to an extent this tends to be accomplished digitally. The obstacle, while you quite possibly can check out by comparing two variations from the identical check out, overcast and sunny, will likely be that daylight has an effect on every thing and in many strategies, all the way down to very small shadows and the glow achieving into shadows from sunlit surfaces.\nThough clouds decrease brightness if they block the solar, the amount is dependent fairly unquestionably with the form of cloud. During the occasion the clouds are indistinct and distribute throughout the sky, mild decline is with a straightforward scale in the mild haze as a result of slender substantial stratus to darkish grey, low clouds. With distinct clouds, nevertheless, just like scattered fair-weather cumulus, the sunshine ranges can fluctuate promptly, notably at a windy working day. Mild, white clouds commonly deliver a straightforward fluctuation of about 2 stops when they move before the solar from dazzling to shade in a single move. Dim clouds with ragged edges, or two levels of shifting clouds, induce a lot more troubles, as being the gentle adjustments little by little and often unpredictably. In the original situation, two light-weight measurements are all which is necessary – 1 in sunlight, the opposite for your cloud passes – once this is certainly attained, you could only alter the aperture in a single on the other, without having employing considerably more readings. When it comes to more intricate relocating clouds, frequent measurement is vital, except you await apparent breaks and apply only these.']	['<urn:uuid:7ac8b6ef-47a2-42d3-af7f-95734fc787ab>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	7	52	731
10	As an art historian, I wonder: do Hugh Lane Gallery and Levi van Veluw share interest in self-portraits?	Yes, both focus on self-portraits but with different approaches. Hugh Lane Gallery showcases artists' self-portraits like Francis Bacon's final unfinished Self Portrait (1992) and Harry Clarke's intense ink study, while Levi van Veluw creates experimental self-portraits using various materials like ballpoint ink patterns and landscapes made of gravel and fake trees on his face.	"[""The Collection Revealed: The Perceptive Eye: Artists Observing Artists (Edited by Jessica O'Donnell)\nThe Collection Revealed is an innovative new series which encourages appreciation of, and participation in, modern and contemporary art as well as offering the opportunity for revealing many hidden gems from the Gallery’s permanent collection.\nThe Perceptive Eye: Artists Observing Artists is the third exhibition in this series drawn from the prodigious collection of Dublin City Gallery The Hugh Lane and showcases rarely seen portraits of artists by artists, offering fascinating insights into how artists portray themselves and how they themselves are perceived. While portraiture could traditionally provide a way for an artist to earn a good living, most of the works on view here were not made as a result of a commission and they often reveal more than just a likeness. They are reflective of an instinctive urge to articulate a feeling, to record a friendship, to investigate form or to comment on status.Music in the Tuileries Gardens (1862) by Edouard Manet, acquired by Hugh Lane for this gallery, is a revolutionary painting of contemporary life and a spectacular group portrait which includes Manet himself as well as poets, painters, writers, composers, family and friends of the artist. Francis Bacon’s final unfinished Self Portrait (1992) found on the easel in his legendary Reece Mews Studio is one of seven unfinished paintings by Bacon in our Collection which offer crucial insights into the artist’s methods. It is indicative of the unceasing exploration of the human figure by an artist then over eighty years old. Harry Clarke’s wonderfully intense ink study of himself is an intriguing companion to his ‘hidden’ self portrait in the frieze of his masterpiece The Eve of St Agnes on view in our stained glass room on the ground floor. Not all of the works are self portraits. Mary Swanzy’s painting of Sarah Purser and Charles Francois Daubigny’s portrait of Honore Daumier, for example, are records of great friendships and artistic solidarity. On other occasions artists are portrayed at work in their distinct studio spaces or with the materials - palettes, brushes, easels or chisels - of their craft. Both of Robert Ballagh’s works intriguingly avoid capturing a likeness of the artist. His Self Portrait (1969) explores the de-humanizing effects of economic and political systems on the individual in society while his painting No. 3 (1977) explores the status of the artist in Ireland at that time.\nBrian Maguire has consistently used portraiture to express social and political concerns and the isolation of the individual in society. Self Portrait (2009)was painted in response to Rebeca another of his own works made in the context of the endemic Femicide in northern Mexico of the last two decades. Contemplating the horror of this, the artist questions how it could possibly happen. But as Maguire himself has said: ‘Self-portraits are usually questions with no answers.’\nThis fully illustrated 56 page book with texts by Barbara Dawson, Jessica O'Donnell, brian Bourke, Brian Maguire, Robert Ballagh, Michael Cullen, Charles Cullen and Alice Maher is available from the Gallery Bookshop, tel. +353 1 8734216."", 'Dutch contemporary artist Levi van Veluw’s signature can be found in his textured self-portraitures, videos, and instillations. In the artist’s “Ballpoints” series, van Veluw covered his face in a progression of intricate ink prints—checkered, puzzle, dots. In “Landscapes”, he took to gravel, fake trees and grass to recreate landscapes on his face—functioning electric train included. Since graduating Artez Art School in Arnhem, Netherland, van Veluw’s captivating progression of material, pattern, and color has gained him international recognition with a Photographer of the Year Award at the IPA International Photo Awards in 2007, as well as exhibitions around the world. Chatting with BULLETT, van Veluw talked of his progression into group installation with “Origin of the Beginning”, stepping away from his one man work ethic, and his fixation with the facial form.\nWhere did your incessant fascination with the self-portrait come from? Were you always interested in the depiction of the self or did your art initially begin elsewhere?\nInitially I didn’t have the intention to make self-portraits. I was interested in the shape of the head, the value we assign to it, its associations, and the possibility of transforming these elements. The first portraits were a practical choice. And, as the oeuvre evolved, the portraits became less personal and more objective, as hair and other personal elements were removed. During the process, I did become more interested in self-portraiture: “Origin of the Beginning” is a more personal series, compared to the other series.\nSince you work in a variety of mediums, would you say that the human form is your primary canvas of choice? Do you consider it a canvas or is it something else to you?\nAt this moment, the human form’s presence in my work is still important; however, I’m finding new ways of making self-portraits without using the human form. “Origin of the Beginning” is a very personal piece in which I reflect on my childhood. It questions where several obsessions and fascinations came from, how my personality first formed. It motivates me to have total control on the final image. Every choice has a meaning, in this way, from the material to the shapes, etc.\nOn one hand, you refer to your renderings as your head, but you also call it a “universal face” that allows the viewer to “project himself onto the work.” Do you think that your practice undermines or redefines the traditional view of a self-portrait in some ways?\nYes. I’m not making self-portraits to tell stories about myself. I am mainly fascinated by organizing chaos. That’s why you see a lot of patterns and structures. By using my own past, I ground the work in a reality, though it looks surreal. Take my piece “family”: it looks like a surreal setting infused with lots of fantasy, but actually, it’s real. My real family is sitting there, covered in real wooden blocks. The result is a performance. The only thing that’s not real is the perfect setting of the family all together. The awkward silence and dark color suggest uncomfortable underlying tensions and emotions—something lots of people will recognize themselves. You know it’s a real family sitting there, but as everything is covered with wooden blocks, you can’t recognize them or see their personalities. In that way, the family portrayed is universal.\nIn “Origin of the Beginning,” your work has a clear narrative scheme, since it is based on a time of your life, specifically between the ages of eight and fourteen. What inspired you to focus on such a definite time period in your life and what is the relationship between those years and the title of the piece?\nIt all goes back to questions that I’ve asked myself. How was I formed? Why am I interested in making portraits with patterns and structures? What motivates me to use repetition? Why do I want to control everything?\nAnd the final questions: where did this behavior originate—what was the Origin of the Beginning? As I didn’t grow up in a perfect family, and there was not much structure at home, I developed this urge to control the things around me, and became obsessed with organized structures and patterns, since they look perfect. I think that’s how and when it all began.\nHow would you describe those years in your life? Were you already interested in art at that time? And did recreating and working with these years in your latest piece make you see them differently?\nNo, I didn’t know anything about art at that time, although I did like to draw and create things. Working with these years does make things clearer, but I really don’t see it as therapy or anything like that. It’s just interesting to analyze why I make art the way I do.\nYou taught photography at the Willem de Kooning Academy. Did your experiences during these childhood years move you in some ways to teach and foster others?\nNo, I did it for just 6 months. But it took to much time, so I’m now focusing solely on my work.\nAlthough from a distance the rooms your created can appear uniform and almost mechanically made, upon closer inspection there are tons of small imperfections that illuminate the human hand behind the formation; this is something you embrace. In the first room, the edges of the desk are burnt indicating your obsession with fire. Are there any other personal touches within the art that may not be seen otherwise?\nI have chosen to work with wood, as it is an organic material that has a history. Making it symmetrical is a human action. It’s part of the idea that controlling your surroundings makes everything into a human creation. The blocks are glued on the wall one by one without any measuring tools. By doing it this way, you see the human struggle to organize his surroundings, just like organizing you agenda. But no matter how hard you try, it will never be perfect.\nWhen I was young, I burned my table. I think a logical consequence of wanting to control everything is an interest in destruction.\nI’m also very interested in the third room, made from small balls that start out equally spaced but, as they get closer to the bed, begin to warp and appear to dance or sizzle dynamically. This, in contrast to the rest of the pieces, seems very lively. Why did you choose to infuse this particular scene with such vitality?\nThe room with the balls feels really claustrophobic. It’s all about losing control. At the age of 8, I had a recurring nightmare about balls rolling in patterns and I had to control them, but as I failed, they bounced into each other. It resulted in total chaos and panic.\nYou carefully placed each square of wood by hand, how long did it take to make one scene? What is particularly difficult and/or rewarding about these types of installations as opposed to digital art and vice versa?\nIt took about 3 months to complete each installation. It’s, of course, more physical work. And by working with real materials you get a kind of imperfection that you will never see with digital art. Besides that, digital art will always stay digital, and in its best format (i.e.: the virtual), it will never truly be real.\nIn “Origin of the Beginning,” you move from representing a single body to a family grouping of five. Do you see this change as something that could affect your work as your career continues?\nNo, not particularly. I see this as the final piece of the Origin of the Beginning series, as it all started with a family.\nWould you consider this family-portrait representative of your relationship with your family? What do you hope the viewer will take away from the sexless and expressionless group?\nIt is based on my family situation at home, 21 years ago, so don’t take it too literally. I think a lot of people will recognize the feeling of this uncomfortable silence between their relatives. It will prompt us to question the way we look at things like perfection and our obsession with control.\nDo you hope to have a family of your own someday? Would you hope to portray them in a similar or different fashion than here?\nYes, I do, but I hope it will be in a happier atmosphere.\nDo you see yourself always staying in Holland?\nAt the moment, I want to stay in Holland.\nAre there any mediums or subjects that you have yet to explore that you would like to?\nI am going to make a series of drawings, and after that, an installation that people can enter inside of.\nYour work has been showcased in the United States, Poland, Australia, and Israel to name a few. Are there places you’ve yet to exhibit at that you would like to?\nThe places are not the most important, but, of course, there any many great museums and other places where I would like to exhibit my work.']"	['<urn:uuid:dbd50b41-5fda-4a87-89b4-584ae8667915>', '<urn:uuid:3b2445ac-9ab0-4123-b1ce-848ffd9a3007>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T04:43:46.574121	18	54	2023
11	Do computer networks and smart cities both need special management systems?	Yes, both computer networks and smart cities require sophisticated management systems. For networks, there is a specific NFV resource management framework to handle efficient service chaining, including mechanisms for CPU allocation, fault tolerance, and disaster recovery. These systems must manage network functions, ensure optimal resource utilization, and provide high availability. Similarly, smart cities require data and computational infrastructures that can handle large-scale analytics, real-time streaming, and machine learning capabilities. These management systems help cities understand and improve various aspects like transport services, water and energy distribution, air quality, and mobility. Both domains need these systems to ensure efficient operation, scalability, and resilience.	['Location: Auditorium, ECE MP Building\nTitle of the Talk : Resource Management for Efficient, Scalable and Resilient Network Function Chains\nName of the Speaker: Sameer G. Kulkarni, Post-doctoral researcher, University of California Riverside.\nDate & Time: Monday 23 September 2019, 4:00 – 5:00 pm\nVenue: MP20, ECE Department\nNetworks, the basis of the modern connected world, have evolved beyond the connectivity services. Network Functions (NFs) or traditionally the middle boxes are the basis of realizing different types of in network services such as security, optimization functions, and value-added services. Typically, multiple NFs are chained together (also known as Service Function Chaining) to realize distinct network services, which are pivotal in providing policy enforcement and performance in networks. Network Function Virtualization (NFV) is becoming more prevalent and enabling the softwarized NFs to fast replace the traditional dedicated hardware-based middleboxes in Communication Service Provider (CSP) networks. However, Virtualized Network Function (VNF) chains posit several systems and network level resource management and failure resiliency challenges: to ensure optimal resource utilization and performance at the system level; and at the network-level to address optimal NF placement and routing for service chains, traffic engineering, and load balancing the traffic across Virtualized Network Function Instances (VNFIs); and to provide High Availability (HA), Fault Tolerance (FT) and Disaster Recovery (DR) guarantees.\nThis talk presents an NFV resource management framework to realize efficient, scalable and resilient net- work service chaining. The presented solutions enable to improve scalability, performance, resource utilization efficiency, and resiliency of deploying the NF chains in SDN/NFV ecosystem and are based on the standardized ETSI MANO NFV reference architecture. We address system-level NF resource utilization, performance, and scale challenges by designing a userspace NF scheduling framework for service function chains. We present a novel rate-cost proportional scheduler and chain-aware backpressure mechanisms to optimize the resource utilization through judicious Central Processing Unit (CPU) allocation to the NFs and improve on the chain-wide performance. We address network-level challenges i.e. orchestration and management of NF chains through a semi-distributed resource management framework that can efficiently instantiate, place and relocate the network functions and to distribute traffic across the active NF instances to optimize both the utilization of network links and NFs. We address HA and FT for NF chains through a novel NF state replication strategy and distinct mechanisms to provide timely detection of NFs, hardware node (Virtualized Network Function Manager), and network link failures. We exploit the concept of external synchrony and rollback recovery to address non-determinism and significantly reduce the amount of state transfer required to maintain consistent chain-wide state updates. We provide distinct failover mechanisms for individual NF\nfailures and global service chain-wide failures with strict correctness guarantees.\nSameer G. Kulkarni is a post doctoral researcher at the University of California, Riverside. His work focuses on the resource management aspects towards building Efficient, Scalable and Resilient NFV platform. He recently received the IEEE TCSC Best Dissertation Award 2019.\nHe received Ph.D. degree in Computer Science from University of Goettingen, Germany in July 2018. He received his M.S. degree in Computer Engineering from the University of Southern California, in 2010, and B.E. degree in Computer Science and Engineering from National Institute of Engineering, Mysore, in 2004.\nHis research interests include Software Defined Networking, Network Function Virtualization, Edge Cloud Platforms, Distributed systems, and Disaster Management. His research publications\ninclude SIGCOMM, CoNEXT, IFIP Networking, NFV-SDN, EuCNC, LANMAN, ICIN conferences and Infocom workshop.\nHe has nearly a decade of Industrial experience. He worked at Qualcomm, San Diego (2009-2014), in the Multimedia Audio group, with core focus on design and development of entire audio software stack (middleware, drivers and DSP) for the Qualcomm Application processors. He also worked at Tata Elxsi Ltd., Bangalore, in the Product Research and Design for Embedded and Multimedia Systems (2004-2008). He has worked on development of the embedded software and systems for the Mobile devices, Set-Top-Box and Hi-Definition Televisions.', 'Dr. Larissa Suzuki is a passionate computer scientist and inventor who founded a software studio start up and has received numerous awards, scholarships and recognitions including those from MIT, Intel, Google, IBM, Microsoft Research, Siemens, the Engineering and Physical Sciences Research Council, McKinsey & Company, among others. In 2016, Larissa was profiled as 6 out of 200 young researchers and was named as the WES (Women’s Engineering Society) Prize Winner at the IET Young Woman Engineer of the Year Awards 2017. Larissa has published several research papers in leading academic journals, books and conferences, and she is a frequent keynote, conference, panel and tutorial speaker (including a TEDx), and co-organised a Machine Learning workshop at the Heidelberg Laureate Forum 2017, where the Turing Award winners (Nobel Prize of Computing) meet annually.\n“…The power to change the world today is at our fingertips in multidisciplinary and diverse teams, and the fast-pace innovation of technology will amaze all curious girls around the world…”\nDiscovering computer science – curiosity and mystery\nI have always been fascinated with things that move or change state, like machines, cars, electronics equipment. My main curiosity when I was a child was to understand the mystery behind those things that made them to behave the way they did, and also how could I make them to do what I wanted them to do instead.\nIn 2003 I joined university to pursue a bachelor’s degree in computer science. My computer science degree was full of fun, and after becoming a computer scientist, I have to say that my problem-solving skills have improved immensely. I realised I was not restricted to a field of expertise, and that as a computer scientist I could work on solving any kind of problems that interested me.\nI decided to specialise, becoming a scholar to acquire more advanced skills. A couple of months later I pursued an M.Phil. in electrical engineering at the University of Sao Paulo and my thesis contributed ideas to help solve some of the challenges we have in medicine around the early detection of breast cancer in both older and younger women.\nImportance of following your passion in computer science, no matter which field\nAfter lecturing and working in industry for over seven years, I started a Ph.D. in computer science at UCL. My work has pioneered data infrastructures and platforms for smart cities, creating designs for smart cities platforms for more than 40 global cities, and the first smart cities data strategy in the world, which was for London. The Institution of Engineering and Technology (IET) has commissioned me to write a book based on my Ph.D. thesis, on which I am currently working.\nThroughout my 11+ years career as a computer scientist creating my own start up at the age of 21, and working for IBM, Bank of Brazil, ARUP, and the UK Government, I was able to take part and lead many applied computing projects such as helping to save the Amazon Rainforest, breast cancer detection, financial systems, smart cities, sustainable buildings and much more.\nI believe this is one of the many amazing perks of being a computer scientist: to be able just follow your passion and work on things that matter to you the most, no matter which field of science they fall into. My inventions and work has advanced many fields of computer science and engineering, including smart cities, data infrastructures, machine learning, emerging technology, and computing applied to medicine and operations research.\nMy work on a day to day basis\nMy daily work involves solving one of the most difficult problems we have in cities today. To ensure their survival in the future, megacities of the world are competing to close skills gaps by creating innovative solutions to attract and retain digitally-savvy and creative talents.\nIn smart cities, young people are vital for a continuous renewal of the economic infrastructure and will be the foundation for the creation of new businesses, initiatives and ventures where innovation will flourish.\nFounders4Schools is an edtech company dedicated to improving the ecosystem for scale-ups by closing the skills gap in cities. As head of data science, I am responsible for leading the organisation’s advanced data science capability.\nI also lead the building and optimisation of a data and computational infrastructure that can handle batch large scale analytics, real time streaming analytics and perform machine learning, training and prediction to serve young people, educators and scale-ups and ensure that such solutions can be scaled up globally.\nMaking cities smarter using data\nEvery day nearly 180,000 people move to cities, creating more than 60 million new urban dwellers every year. Alongside population change, cities’ physical infrastructure is struggling to cope with the increasing demands placed on it. Inadequate infrastructures have often been accompanied by the aggravation of many challenges associated urban living in terms of law and order, health, utilities, education, transportation, and delivery basic public services.\nCities have to plan for population growth, and introduce a more sustainable, efficient, and liveable model in urban development. Digital technologies offer a new wave of opportunities to create a balance between social, environmental and economic opportunities that will be delivered through smart city planning, design, and construction.\nSmart cities are based on the idea that data, analysed by a suitable framework, can help us to understand how city systems are performing (e.g. transport service levels, quality in water and energy distribution, air quality, mobility), and offer new insights and ways of improving the services delivered to citizens – from public health and social care, to refuse collection, public transport, and to retail in the private sector.\nOur capacity to understand how people live and places work, and to create economic impact, as well as social value from data and technology has never been stronger. City data is an incredibly important asset and is the foundation upon which smart cities vision will be built.\nProviding efficient access to public and private sector and citizen’s data has the potential to enhance and transform both government and businesses services, as well as stimulate innovation in city services to the benefit of everyone.\nBeing named as the WES (Women’s Engineering Society) Prize Winner at the IET Young Woman Engineer of the Year Awards\nI got to know about the awards through an industry mentor who told me I should put forward an entry. I spoke to my boss about it and he provided me with a reference letter to support my entry and emphasise the quality of my work and my achievements as an engineer.\nSeveral weeks after submitting an entry I was contacted by the IET (Institution of Engineering and Technology) informing I had been shortlisted to the interview. I had to give a presentation demonstrating my work as an engineer, and had to answer multiple questions from a panel of judges, which included a previous IET president. A few days after the interview I was informed that I had been selected as a finalist.\nI felt very proud and honoured to have been selected as a finalist to this prestigious award. As an award winner, I will be an ambassador for the IET and the Women’s Engineering Society to help the organisations attract more women in to the field of engineering.\nThere is a big skills shortage in technology and we need more women participating in the creation of the technology that will change the world. I think that we need more engineers in order to ensure human survival. We have any issues in our planet that engineers are able to solve.\nThe award has given more confidence and faith in my results. Though women in computing have been pivotal in the creation of amazing modern technology, their story is one that’s often told nor celebrated. On the contrary, instead, great tech women pioneers have been all but erased from history, and that needs to change.\nMaking tech an easier choice for girls\nWe need both women and men joining the tech workforce, however, for girls it is more difficult to make that choice. How can they aspire to become the next Grace Hopper or Barbara Liskov if they never heard of women excelling in tech? Very often women put off technical degrees not because they do not have the cognitive ability nor because they are not biologically wired to do tech.\nOne of the key reasons (besides barriers of organisational culture) for women not entering the tech workforce is the lack of role models. Marie Wilson once said: “You cannot be what you cannot see,” and those are very wise words. Too often in the tech world, we only see men take centre stage and being celebrated – the Gates, Zuckerbergs and the Jobs.\nWhen I was at university, I learned about them but I had never heard of Ada Lovelace (the first computer programmer), Radia Perlman (the inventor of spanning-tree protocol – STP), Hedy Lammar (the inventor of frequency hopping technologies). The good news is that we DO HAVE amazing women doing astonishing work in computer science (though still not enough!).\nWe must make them visible to inspire the generations to come. Their ground-breaking work can serve as an inspiration to both girls and boys alike.\nComputer science – solving problems through a multitude of activities\nComputer science is all about problem solving. It is extremely multidisciplinary and involves a multitude of activities. Besides the role of engineering / programming the systems, there’s:\n- Designing systems and understanding the users’ needs (e.g. requirements, accessibility);\n- User experience;\n- Graphic design;\n- Business modelling;\n- Program / product management to ensure all products and programs are running smoothly and fulfilling the needs of users and the market;\n- and much more!\nAll the input from these aforementioned work is then translated into ‘code’ – which is also something that anyone can do! I would say to a person not sure if she wants to learn how to code to just try programming by herself / himself, and play with it at her / his own pace to have a feel on what it is like to teach a computer what you want it to do.\nThere are many available free online courses in computer science and programming (e.g. on YouTube, Udacity, Coursera), and several meet up groups and hackathons (a very safe and support environment) which can help anyone to discover what computing is all about. I am certain that if you dream about becoming a change maker, a career in any area of computer science will enable you to create the solutions that will change the world.\nDreaming up amazing inventions\nTechnology is part of just about everything that touches our lives from phones, cars, movies, games. It influences how we do business and communicate with other people, and it solves the most impactful problems in the world, like creating clean energy or detecting cancer.\nWe must tell girls that by working in the field of computer science, they will get to dream up all the amazing inventions no one else has thought of yet, and then design and develop them. The power to change the world today is at our fingertips in multidisciplinary and diverse teams, and the fast-pace innovation of technology will amaze all curious girls around the world.\nIf a girl is interested in computer science, I would tell them to forget about the stereotypes, bring all their previous learning with them (tech is very multidisciplinary), and not to worry if they haven’t got a technical degree. Everyone can learn how to code, and if you don’t enjoy this part there are so many other things you can do in the field.\nOn the horizon\nI am a passionate computer scientist who loves problem solving and one of the early researchers / professionals in the field of smart cities. I have several papers, keynotes, lectures and a book lined up for the New Year.\nBesides contributing to science, I will continue mentoring young people, giving keynotes and lectures to transfer knowledge from academic research to corporate practice and vice versa, and deliver innovative smart cities solutions for the benefit of mankind.']	['<urn:uuid:bf958e12-9b40-4ada-bd7e-182c303cf660>', '<urn:uuid:f841f44c-6cd6-44a0-a4b0-9ce987ec5e02>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:43:46.574121	11	102	2667
12	cost savings floating vs fixed deck footings	A floating deck saves money in three ways: material costs are about $45 less per footing (saving minimum $200 total), installation time is reduced by about two hours per footing, and there are no costs for disposing of excavated dirt which can amount to hundreds of dollars.	['Adding a deck to the backyard giving a level platform to set up a few chairs to enjoy the sun and maybe grill some burgers seems like a good idea. But what type of deck? Should you dig frost footings, or is a floating deck a good idea? When is it better to build a floating deck? What are the pros of a floating deck? What are the cons of a floating deck?\nA floating deck is a good idea for a low ground level deck, less than 24″ (600mm) above grade. A floating deck is beneficial for around a tree as not to disturb the tree’s roots. Building a floating deck is a good idea, with it’s reduced costs compared to a fixed deck.\nA floating deck is not all good. There are some cons to a floating deck. But let’s cover why and when a floating deck is a good idea but then also the cons. Reason why a floating deck may not be a good idea.\nReasons a Floating Deck is a Good Idea\nThe primary reason a floating deck is a good idea is during construction. The most significant difference between a floating deck and a fixed deck is the footings.\nA floating deck does not have footings below the frost level but floats on deck blocks installed at grade. Where a fixed deck requires footings installed below the frost level. Where in places like Texas, the frost level is between zero and 12″ (300mm). But jumping countries and city to Winnipeg, Manitoba, where the frost level is 72″ (1.8m).\nRegardless of where you are located, a floating deck will require less digging and cost less.\nFloating Deck A Good Idea Costing Less\nFirst, you will save money with a floating deck. There are three ways you will save money.\n- Comparing Floating Footing Material Costs to Frost Footings. How much varies by the footing requirements but using Calgary as an example. In Calgary, requiring footings 10″ (250mm) diameter footings, 4′ (1.2m) deep will cost $60 in material. For the concrete, forms and post saddle. A floating deck block will only cost $15 for the deck block and base material like gravel under the deck block. Saving you $45 per footing. With a floating deck saving you a minimum of $200 compared to a fixed deck in material.\n- Floating Decks Take Less Time to Build. How deep and how hard the ground is will impact how much time savings. But on average, in Calgary, where I build decks, you will save around two hours per footing. With a small deck, only requiring four footings, that is the difference between framing in the afternoon or spending all day digging holes and mixing concrete. The larger the deck, the more time you will save with a floating deck instead of a fixed deck.\n- Less Dirt to Dispose of With a Floating Deck. Yes, you can rake it out under the deck. Or like my neighbour did pile it up around his house’s foundation. But you need to do something with all the extra dirt from the footing holes.\nThe challenge with dirt from footings holes is it is mostly clay. Which is not good for your yard and grass. As nothing will grow in it. Even if you use it to fill holes or low spots in the yard, you will need to buy loam to cover the clay for the grass to grow in.\nIf you decide that wrecking your yard is not a good idea, you have to haul the dirt away. This can be as simple as loading up a few buckets and driving to the landfill. But often, this includes dumping fees or waste bins to haul away the dirt. All added up to paying a few hundred dollars more for fixed deck footings than a floating deck.\nThese three reasons alone make a floating deck a good idea. But there are more.\nFloating Deck a Good Idea for Certain Locations.\nAs I mention in cost, the first is where the frost level is so deep that extensive excavation is required for frost footings. Making a floating deck a great idea. But also were in the yard the deck is located.\nIt may be a good idea for a yard with many trees to build a floating deck instead of a fixed deck. With the many challenges involved with building a deck over tree roots. Not only is it time-consuming digging among tree roots. As they slow down digging, often requiring reaching into the hole with a saw to cut roots. But it also can be bad for the tree. Larger roots close to the tree trunk are critical for tree strength and water. Cutting these roots on a tree while building a deck can kill the varied tree you are trying to design your deck around.\nA floating deck disturbs tree roots less. As the deck blocks can be installed above grade, away from significant roots. Making a floating deck a good idea around or close to a tree in your yard.\nFloating Deck’s Don’t Require Permits?\nBefore discussing when a floating deck is not a good idea, I just want to clear up a common misconception, permits.\nIt is often mentioned that a floating deck does not require a building permit. This is not true but is a problem of causation and collation.\nAs required by International Building Code, a floating deck must be less than 30″ (762mm) from the ground. Slightly lower in Canada at 24″ (600mm) above grade.\nChecking many cities deck permitting requirements, it’s the height of the deck that requires permitting, not the footings.\n“You need a construction permit to build a deck that is more than 18 inches above the ground”Government of Seattle\nUsing Seattle as an example, but this is common in many cities. In fact, I didn’t find any cities that differ between footings type for the requirement of permits. When pulling a deck permit, they will often have footing requirements. It’s not the footing that demands permit but the permit that demands footings.\nIn short, whether or not a floating deck is a good idea, the permit requirements have little impact. Floating decks don’t require permits because they are low to the ground, not because they are floating.\nReasons a Floating Deck is a Bad Idea\nThere are times and reasons when building a floating deck is a bad idea.\nPrimarily, just as we discussed with permits.\nA High Floating Deck is a Bad Idea\nAny deck above 30″ (762mm) in the States and 24″ (600mm) in Canada need frost fixed footings. A deck of that height requires the stability of footings installed below the frost level. A floating deck moving with the ground can have serious consequences. Causing the deck to collapse with the potential of injury from the height of the fall.\nA ground-level floating deck may sink in spring, but the potential for injury is low. Falling two feet is a lot less dangerous than falling six. Making a tall floating deck a bad idea.\nA Floating Deck Connect to the House Is a Bad Idea\nA low freestanding deck beside a house can have many advantages. But the deck cannot be attached to the house if floating.\nA floating deck will cause stress and possibly failure of the house’s foundation. As the deck moves with the ground, the deck will pull or push the foundation. The house’s foundation is not designed for this level of lateral load, increasing failure potential. Making a floating deck connected to a house a bad idea.\nConclusion, Is a Floating Deck a Good Idea?\nYes, a floating deck is a good idea if the deck is low, not attached to the house or built close to trees. A floating deck will save you both time and money during construction.\nBut a floating deck is a bad idea if the deck is tall or is attached to the house. The movement of a floating deck do not work well when connected or at heights. These situations require a fixed deck supported by footings below the frost level.\nDo you know what a good idea is? A deck in the backyard to enjoy the outdoors.']	['<urn:uuid:839ff530-714e-44ec-8102-db2fca7f905e>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	7	47	1380
13	what battle defeated hannibal end second punic war 202 bc	The battle of Zama in 202 BC was where Scipio decisively defeated Hannibal at the end of the second Punic War.	"[""1. A defensive missile designed to shoot down incoming intercontinental ballistic missiles.\n4. (Irish) Mother of the Tuatha De Danann.\n8. The syllable naming the first (tonic) note of any major scale in solmization.\n11. An anxiety disorder characterized by chronic free-floating anxiety and such symptoms as tension or sweating or trembling of light-headedness or irritability etc that has lasted for more than six months.\n12. Rounded like an egg.\n13. Any of various primates with short tails or no tail at all.\n14. Thigh of a hog (usually smoked).\n15. (Greek mythology) Goddess of the earth and mother of Cronus and the Titans in ancient mythology.\n16. Goddess of the dead and queen of the underworld.\n17. By bad luck.\n19. (astronomy) The angular distance of a celestial point measured westward along the celestial equator from the zenith crossing.\n21. In the same place (used when citing a reference).\n24. Type genus of Ochnaceae.\n25. Ask for and get free.\n29. A small cake leavened with yeast.\n31. The battle in 202 BC in which Scipio decisively defeated Hannibal at the end of the second Punic War.\n32. A Loloish language.\n35. The compass point that is one point east (clockwise) of due north.\n37. Having been read.\n41. A religious belief of African origin involving witchcraft and sorcery.\n45. (informal) Roused to anger.\n46. The 7th letter of the Greek alphabet.\n47. A member of an American Indian people of Yucatan and Belize and Guatemala who once had a culture characterized by outstanding architecture and pottery and astronomy.\n48. A loose sleeveless outer garment made from aba cloth.\n49. East Indian tree bearing a profusion of intense vermilion velvet-textured blooms and yielding a yellow dye.\n50. Taking place over public roads.\n1. Title for a civil or military leader (especially in Turkey).\n2. Any of numerous local fertility and nature deities worshipped by ancient Semitic peoples.\n3. Designer drug designed to have the effects of amphetamines (it floods the brain with serotonin) but to avoid the drug laws.\n4. A member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times.\n5. A genus of Indriidae.\n6. Submerged aquatic plant having narrow leaves and small flowers.\n7. A flat wing-shaped process or winglike part of an organism.\n8. Tropical woody herb with showy yellow flowers and flat pods.\n9. An organization of countries formed in 1961 to agree on a common policy for the sale of petroleum.\n10. Goddess of the dead and queen of the underworld.\n18. In the Arabian Nights a hero who tells of the fantastic adventures he had in his voyages.\n20. Affect with wonder.\n22. The cry made by sheep.\n23. An international organization of European countries formed after World War II to reduce trade barriers and increase cooperation among its members.\n26. An official prosecutor for a judicial district.\n27. A metric unit of weight equal to one thousandth of a kilogram.\n28. (Akkadian) God of wisdom.\n30. A kind of heavy jacket (`windcheater' is a British term).\n33. Cubes of meat marinated and cooked on a skewer usually with vegetables.\n34. A member of a seafaring group of North American Indians who lived on the Pacific coast of British Columbia and southwestern Alaska.\n36. A soft silvery metallic element of the alkali earth group.\n38. A Chadic language spoken south of Lake Chad.\n39. Common Indian weaverbird.\n40. A small ball with a hole through the middle.\n42. A piece of furniture that provides a place to sleep.\n43. The rate at which heat is produced by an individual in a resting state.\n44. Of or relating to a member of the Buddhist people inhabiting the Mekong river in Laos and Thailand.\n45. An international organization of European countries formed after World War II to reduce trade barriers and increase cooperation among its members.""]"	['<urn:uuid:8c0c65d9-190c-4dd2-a9e0-e58c20063c1d>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	10	21	652
14	Who plays Katherine Johnson in the movie Hidden Figures?	Taraji P. Henson plays NASA's Katherine Johnson in the film Hidden Figures.	"[""Events in space exploration\nTalented women and men have long worked together to accomplish the greatest human achievements. And yet, some individuals who have played critical roles in these achievements, notably women and people of color, are under-represented in the retelling of these stories. Unconscious bias and the tendency toward stereotyping often result in an inaccurate picture of teamwork, collaboration, and contributions. Most significantly, it limits future innovation.\nOver the past two years the President has awarded the Presidential Medal of Freedom to pioneering technical women in space and computing: Katherine Johnson (Mercury, Gemini, Apollo, Shuttle, exploration missions), Margaret Hamilton (Apollo, Skylab), and Navy Rear Admiral Grace Hopper (the “Queen of Code”). Leaders in the advertising industry are also working to correct biased depictions of women with their #SeeHer program launched during the United State of Women event in June 2016.\nHollywood too has begun correcting a long-standing impact of unconscious bias. Writers, directors, producers, and actors are beginning to surface untold stories and include characters previously left out. They are beginning to demand and create more accurate depictions of STEM depictions of STEM (science, technology, engineering, and mathematics) endeavors and the people who engage in them. As actress Geena Davis has said, “If she can see it, she can be it.”\nLast week, as part of a series of events that celebrated untold stories of women in STEM and Computer Science Education Week, the First Lady welcomed to the White House some of the cast and team behind the new film Hidden Figures. The White House Office of Science and Technology Policy also hosted a panel of notable technical and scientific women from NASA who have shaped the world and contributed to NASA's exploration of worlds formerly beyond reach.\nDr. Knatokie Ford (far left), Senior Policy Advisor in the White House Office of Science and Technology Policy, moderates a panel discussion with, from left to right: Director Theodore Melfi; Author Margot Lee Shetterly; Actress and singer Taraji P. Henson; Actress Octavia Spencer; Musical recording artist, actress, and model Janelle Monáe; Actor, film director, and producer Kevin Costner; and Producer Mimi Valdés. (Photo credit: NASA)\nTaraji P. Henson, who plays NASA’s Katherine Johnson in the film Hidden Figures, on the panel discussion shared frustration that she didn’t grow up knowing these stories. She felt not knowing these stories limited her own career choices and those of many others who didn’t realize that STEM careers were an option.(Photo credit: NASA)\nThe stories shared at last week’s White House event are those of contributors to extraordinary teams. Here are some of those women leaders who worked during four eras of space exploration:\nEra 1: Pre-Sputnik (-1957)\nKatharine and Susan Koerner Wright (Wright Family), Bessie Coleman, Grace Hopper, Hedy Lamar, the ENIAC Programmers, Amelia Earhart, Maggie Gee and Ola Mildred Rexroat of the Women Airforce Service Pilots (WASPs), and many more.\nEra 2: First Launch-Mercury-Gemini-Apollo (1957-1970s)\nMary Sherman Morgan, Dorothy Vaughn, Katharine Johnson, Wally Funk, Jeanne Crews, Geraldyn (“Jerrie”) M. Cobb, Dorothy “Dottie” Lee, Margaret Hamilton, and many more.\nEra 3: Shuttle – Hubble - Space Station (1970s-1990s)\nNancy Roman, Shannon Lucid, Mae Jemison, Margaret Rhea Seddon, Kathryn Sullivan, Peggy Whitson, Yvonne Cagle, and many more.\nEra 4: Now (1990s-)\nDava Newman, Marleen Martinez Sundgaard, Eileen Collins, Adriana Ocampo, Gwynn Shotwell, Debbie Martinez, Sunita Williams, Ellen Ochoa, Julie Kramer White, and many more.\nHere are some ways that you can help to improve the accuracy of history and share more stories about the incredible women in STEM:\n- Participate in edit-a-thons and team-up with others to host one! (we held our first at the White House STEM Heroes Edit-a-thon during African-American History Month in 2015).\n- Host short-film festivals on campus, in your classroom, in your organization, or at home (White House Office of Science Technology Policy hosted“ STEM on the Silver Screen” at the White House in 2014 and the National Archives hosted their 2106 McGowan Forum with short films and a conversation on “Women in Leadership: From the Computer Age to the Digital Age”).\n- Share stories of STEM and other heroes on social media using the hashtags #MissingHistory, #HiddenFigures, and #GalaxyofWomen.\n- Remember to include all of the key players when telling stories about STEM achievements.""]"	['<urn:uuid:9212fc9c-9428-4257-b131-ca72a55500a1>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	9	12	706
15	what constitutes flavor experience wine	Flavor experience in wine is the result of multisensory integration combining olfactory, tactile and taste impressions. It is modulated by the dynamic time course of tasting and the location of sensory stimuli in the mouth. While people often use the word 'taste' loosely, taste is actually only one element of flavor perception, with smell playing a much more significant role - most of what we call taste is really smell.	['Philosophies of taste\nDoes wine taste the same to an expert and an amateur? It has become something of a truism, for those writing about taste and expertise in the context of wine appreciation, that how one tastes affects what one tastes. When one tastes with experience and knowledge – of intentions, production methods, grape varieties, terroir and so on – and with the concomitant levels of attentiveness required to discern all of the relevant perceptual properties, the taste of the wine is affected. The expert’s flavour experience is thus different from that of the novice. So, we apparently have an answer to our question: the wine tastes different.\nSubscribe to the weekly TLS newsletter\nUnfortunately, as one might suspect, this is too quick. For this deceptively straightforward question hides some fiendishly tricky philosophical issues. Indeed, it is not immediately obvious what precisely is being asked. For a start, we are not yet sure what it takes for a wine to remain “the same”. Perhaps in some sense experts and non-experts taste the same wine, but that wine tastes different to each. Even ignoring the complication of how we identify (both types and tokens of) wines, we can quickly see that the question hides further complexities, most prominent of which is the question of what we mean by “taste”. In particular, without the following distinctions in place the question is ambiguous.\nFirst, we need to differentiate literal tastes – the purely gustatory experiences we have when certain receptors on the tongue are stimulated – from our overall taste experiences. That is, we tend to use the word “taste” loosely to describe what most researchers now refer to as flavour. Flavour experiences are, in the words of the philosopher Barry Smith, “the result of the multisensory integration of olfactory, tactile and taste impressions, modulated by the dynamic time course of a tasting event and the location of sensory stimuli in the mouth”. In other words, taste is only one element in flavour perception and one that plays a much lesser role than olfaction – most of what we call taste is really smell.\nSecond, we need to differentiate between our flavour experiences, and the causes of those experiences. This is important, because we want to explain those situations where we believe our flavour experiences, for example, fail to match the “real” flavours that are there in the wine to be detected. So when we use the word “taste” to refer to “flavour”, we still need to bear in mind the distinction between our flavour experiences and the flavours of the object we take ourselves to be experiencing.\nThird, there is a difference between an overall flavour experience, and the various taste and smell components (or flavours) that constitute it. This will be clarified below.\nThere are basically two main philosophical approaches to our question. The first gives a negative answer to it. Arguably the most natural way of articulating this idea appeals to the fashionable notion of cognitive penetration. The thought is that the various cognitive states constituting expertise – beliefs, knowledge, linguistic capacities and so on – affect in some very direct way our actual (non-cognitive) sensory perceptual experiences. That is, the difference between expert and non-expert is fundamentally an experiential one. So if this is true, the wine really does taste (and smell) different to the expert.\nNonetheless, this line of thought has been resisted, and some philosophers, such as Jerry Fodor, deny the purported phenomenon of cognitive penetration; that is, by their lights, knowledge doesn’t impact on phenomenal experience. According to this second view expertise allows one to use one’s knowledge to organize, discriminate within, attend to, interpret and judge one’s experience of the wine in a way that differs from that of the non-expert. But, crucially, this does not amount to a difference in the phenomenal experience of the wine’s flavours. In other words, expert and non-expert have the same flavour experiences – the wine tastes the same to each – but the expert reflects differently on this experience and comes to different judgements about the wine’s flavour. Call this the Sceptical Claim.\nIn essence, the sceptic contends that the apparent behavioural differences (such as the judgements made or descriptions offered) between novices and experts are the result of merely cognitive, and hence non-perceptual, changes in judgement or belief. One might also note that the apparent differences between the expert and non-expert can be partly explained in terms of changes in attention; roughly speaking, experts are simply better at paying attention to different features of the object.\nHow should we assess this dispute? Both sides want to allow that there is genuine expertise in wine tasting and hence to allow that how one tastes affects what one tastes. They appear to disagree about the “what”, and to some extent about the “how”. It is evident that the sceptic thinks experts and novices share the same basic sensory perceptual content, or at least do to the extent that they are equally attentive. So, roughly, expertise affects how one judges, interprets, or evaluates one’s flavour experiences but does not alter the actual taste and smell sensations that constitute these experiences.\nIt is not clear, however, that the non-sceptic – according to whom the expert and amateur have different experiences – must disagree with this. After all, it is likely that basic chemical compounds and volatile molecules produce identical or similar experiences in physiologically similar human beings. So, if a wine contains, for example, blackberry flavours – or more precisely, the chemicals responsible for such flavours – or the yeast brettanomyces, then experts and non-experts should both detect them and experience them in the same way; they should undergo similar phenomenal experiences.\nBut now, here is the rub. Wine involves complex layers of chemicals and volatile compounds, the perception and discrimination of which is subject to their dynamic temporal profile and to the temporal act of tasting itself. In addition, in this act of tasting, experts pay attention differently and to different things unfolding in time, including assessing their own sensory experiences, and possessing a trained vocabulary with which to do so. Two important things follow from this.\nFirst, the way in which, for example, chemical substances like brettanomyces are manifested in sensory experience can differ depending on all of these aspects. Sometimes, in some wines, it will smell like sweaty leather, and in some others like spicy cloves or bacon. How it is experienced, then, will depend on a number of variables that may depend on expertise. Second, there is an ineliminable evaluative element here. On the one hand, taste and smell experiences are intrinsically valenced – roughly, they are pleasant or unpleasant. On the other hand, their valence is affected by, and in turn affects, the other constituents of the wine being experienced. So, for example, the brettanomyces in a Syrah wine from the Rhone is sometimes thought to be a wine fault and is evaluated negatively, but can sometimes, for other wines, introduce a pleasant savoury dimension to the wine that adds to its positive evaluation. How it is experienced in either case will, I contend, depend in part on i) how it is experienced in combination with the other elements of the wine, and ii) how the experts value (or disvalue) the dimensions that brettanomyces can bring to such wines. Moreover, just to complicate matters, (i) and (ii) are themselves inextricably intertwined.\nThe challenge for the sceptic, who believes that the expert and the amateur have the same phenomenal experience when drinking the same wine, is this: underlying all of these possible experiential variations, what is it that is being tasted as the same by both expert and non-expert? The answer cannot just be “this chemical compound”, because that is not what is represented in the taste experience. We do not taste chemical compounds as such, but rather, they need to invoke the presence of real flavours – metaphysically real flavour properties that exist in the wine independently of our actual experiences – that are not simply reducible to their chemical bases. This would allow us to explain, as we must, why experts try to distinguish their own preferences and subjective impressions from the values and properties they take the wine really, independently to possess.\nSo, the sceptic must contend that in our flavour experiences we are able, at least in theory, to prise apart (mere) valence from (genuine) sensory perceptions and that our ability to do so depends on there being real flavours that our flavour experiences are caused by and correspond to. In this way, experts and non-experts will have (at least to some extent) exactly the same literal sensory taste and smell experiences, but their overall cognitive descriptions, “judgements”, or interpretations of them will differ.\nDoes this position of the sceptic’s make sense? I have my doubts, even if we allow the existence of real flavours. Note that the sceptic must describe the overall judgements made about her sensory experiences as cognitive rather than perceptual in order to avoid the possibility that they are cognitively penetrated in the way the non-sceptic would hold. But surely what we are interested in is the overall flavour experiences and these broadly construed experiences are in large part perceptual and shaped by expertise. As such, it cannot be that experts and non-experts taste the same thing if by “taste” we are referring to these overall experiences of flavour.\nCan we separate out from these overall experiences the individual tastes and smells that, the sceptic maintains, are experienced identically by experts and non-experts? This is a nebulous phenomenological issue, but if what I argue above is correct, there is reason to think that these individual components of flavour experiences will themselves be affected by their place in the overall flavour profile of the wine, and hence no guarantee that they will “taste” the same to experts and non-experts.\nMoreover, I doubt that valence – whether our experiences are pleasant or unpleasant – can be so easily prised apart from sensory perception. If I used to like broccoli and no longer do so, does the broccoli still taste the same to me? As I have just argued, where “taste” refers to overall flavour experience, I think we are compelled to answer in the negative, because valence will be an essential part of this experience. Is there, then, a broccoli taste common to pleasant and unpleasant broccoli experiences – and common to expert and non-expert experiences – that we can identify apart from valence? Even if we conceded to the sceptic here, and allowed this possibility for some individual taste and smell sensations, I suspect that in complex sensory objects like wine, valence would be too inextricably tied to the way sensations are experienced to separate them neatly. Hence, the wine does not taste the same to the expert and the amateur.\nCain Todd is a senior lecturer at Lancaster University and the author of The Philosophy of Wine (2010).\nSubscribe now to the TLS and get the best writing on big books and big ideas from only £1.50 or $2.40 per weekSubscribe']	['<urn:uuid:2b739851-f7e1-4366-badd-2c18e07ec528>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	5	70	1853
16	wall tiles ceramic porcelain glass material differences	There are key differences between wall tile materials. Ceramic tiles are common, durable, and budget-friendly, ideal for bathrooms and kitchens. Porcelain tiles are more specialized - they don't freeze, fade or crack, making them suitable for outdoor use, though installation is trickier. Glass tiles are stain-resistant but can chip along edges, making them better for low-traffic areas like backsplashes. All these materials can be found in various colors and patterns.	"[""When it comes to cladding, protecting or decorating walls, wall tiles can be the ideal solution. With pretty tiles, boring walls become a real eye-catcher. However, it always depends on which type of wall tiles the user decides on. Good tiles should have a certain strength so that they don't break too easily. But they mustn't be too thick either, otherwise they will take up too much space. It is important to find a healthy mediocrity. This is not difficult with the large selection of wall tiles.\nThey are available in different sizes and of course with different colors and patterns. If you like it pleasantly noble, you can use beautiful, high-gloss tiles. If there are children in the household, a matt tile is recommended, as it does not immediately show every fingerprint. Different wall tiles can also be combined with one another. This creates attractive patterns that certainly not everyone has.\nthey are available on ceramics, earthenware, stoneware or porcelain stoneware. You can purchase mosaic tiles, you can put together wall tiles yourself as a small or medium-sized mosaic and even wall tiles made of porcelain are processed. Common to all wall tiles are the joints and that these no longer just have to be gray, has long been known to every home builder.\nHow do I find the right wall tiles now\nWhich wall tiles are right for the right walls is not only related to the nature and use of the rooms in which the wall tiles are to be installed. In any case, these are the first things to consider. Most wall tiles are traditionally installed in the bathroom and kitchen. In such rooms it is important that the wall tiles can absorb water and are easy to clean. If you want to install your wall tiles outdoors or in rooms with high temperature fluctuations, you should make sure that the wall tiles absorb little water, i.e. do not travel in the cold.\nThe size and brightness of the rooms are also decisive for which wall tiles are chosen. Large rooms can also handle large wall tiles, but if you are looking for wall tiles for small, dark rooms, you should choose smaller patterns and light wall tiles. Once these important things have been clarified, it is finally time to choose the pattern and the desired material for the new wall tiles.\nNot all ceramics are the same\nSimple ceramic wall tiles are mainly used in bathrooms and kitchens. But earthenware tiles are also well suited as wall tiles in damp rooms. They are made of dry-pressed ceramic that can absorb water well. The earthenware wall tiles are covered with a glaze, which can be both transparent and opaque. There is also a choice between glossy, matt or semi-matt wall tiles.\nPorcelain stoneware wall tiles are particularly fine. The ceramics that were used for production here consist of particularly finely prepared raw materials. It is burned very tightly, which is why you have to make sure when laying that the mortar or adhesive is highly plastic. Porcelain stoneware wall tiles are very hard and have a dense surface. They are both colored, unglazed and glazed. These wall tiles absorb very little water, which is why they can be used wonderfully outdoors.\nOne of the most beautiful uses of wall tiles is certainly the mosaic. The specialist differentiates between medium mosaic and small mosaic. It is always a work of art composed of small-format tiles, which can be attached to the wall as well as to the floor. As a rule, tiles are made of stoneware, less often porcelain stoneware or earthenware. Often these small tiles are glued to a net for easier installation. A mosaic is always an eye-catcher and looks very noble, especially in connection with large tiles. Not only in the bathroom or in the kitchen, by the way. Such a mosaic made of wall tiles is also a feast for the eyes on exterior walls.\nSo that nothing goes wrong\nWhile it is still advisable to use gray grout for the floor, there are plenty of options when it comes to wall tiles. But be careful, especially those who grout their wall tiles should be careful not to have differences in the width of the joints as much as possible. Precision and proper work are required here."", 'Tiles are unit area skinny objects, typically are in square or rectangular form. A tile may be a factory-made piece of hard-wearing materials like ceramic, stone, metal, baked clay, glass. They are typically used for covering roofs, floors, walls, or alternative objects like tabletops. Tiles are also made up of lightweight material like perlite, wood, and mineral wool which can be used in wall and ceiling purposes.\nTYPES OF TILES BASED ON MATERIALS AND WORKMANSHIP\nTYPES OF TILES BASED ON ITS APPLICATION\nLARGEST TILES COMPANY IN INDIA\nTYPES OF TILES BASED ON MATERIALS AND WORKMANSHIP\n1. Ceramic Tile\nCeramic tile is a common type of tile that is mostly found in homes. Since this type of tile is highly durable hence is perfect for use in all types of rooms in the house like kitchens, bathrooms, and entryways. It is very easy to install, to clean, and is available in a variety of styles that can be fitted in any design. The price of this type of tiles is not that high and comes within the budget.\n2. Porcelain Tile\nPorcelain tile is another most common type of tile which is different from ceramic tile. This type of tile emulates natural stone, brick, and wood without any maintenance. It has availability in a range of designs, colors, and styles. Since this type of tiles does not freeze, fade, or crack it can be used even outdoors. Porcelain tiles can also be used in the bath or kitchen tiles, high traffic areas, and even in kitchen backsplashes. One of the biggest drawbacks of porcelain tile is that the installation is somewhat tricky.\n3. Glass Tile\nThe glass tile is having stain resistance property which makes it an alternative to natural stone. With the help of this tile red wine and acidic foods like lemon and vinegar can be wiped up without leaving any permanent stains. As glass tiles are chip along the edges hence are not recommended for use in high traffic areas like kitchen and bathroom floors. But are used in less traction such as tabletops or desks, around the fireplace, or as the backsplash.\n4. Cement Tile\nCement tile has been used extensively for decades and currently used widely in modern interior design. Cement tiles are very versatile and hence provides a wide range of colors and patterns. Since these types of tiles are highly porous, a patina develops over time in order to enhance the pattern. Cement tiles are also sanded and resealed like wood floors as they get discolored over time. The biggest drawback of cement tile is they are beast to lay. In addition, cement must be resealed once a month so as to maintain its beauty. This type of tile is best for use in low -traffic areas and that too in small quantities.\n5. Marble Tile\nDespite marble tile being costly they are used for adding elegance and refinement to any room. If you are looking to add beauty to your kitchen or bath, marble tile is the best option. It is also responsible for adding texture and depth. Like the other tiles, marble tiles are also susceptible to scratches and stains and are also difficult to clean.\n6. Mosaic Tile\nMosaic tile is widely used for unique and creative interior designs as it is available in a variety of shapes, sizes, colors, styles, and materials. When an accent is needed mosaic tiles are extensively used in-wall applications.\n7. Granite Tile\nGranite is a natural stone having similarity in appearance and feel to marble but still, there are some notable differences. Granite tile is extensively used due to its availability at a lower price. These tiles are used in a laundry room and other spaces where we need performance but in lower cost\n8. Limestone tile\nLimestone is one of the other types of natural stone tile. This type of tile delivers a natural appearance which is required for ancient architecture and design. Due to their high durability and softness, these are also easy to shape and cut down for some of the specific patterns and placements.\n9. Travertine tile\nSimilar to limestone travertine tile offers a natural kind of aesthetic. Since it is easily affected by water, stains, and traction hence this tile requires extra maintenance and also resealing once in a decade. It is highly recommended to use such tiles in low traffic areas of the home. Designers used to apply these tiles on walls despite floors so as to avoid scratches, etching, or stains.\n10. Quarry Tile\nQuarry tiles are made up of ground materials that are similar to but technically stronger than brick. Ground materials include feldspar, clay, and shale which are 1st grounded together and then are baked at above 2000 degrees. As these types of tiles are fired at a very high temperature, they are dense, nonporous, water-resistant, and also have a lower water absorption rate. They also do not require to be sealed.\n11. Metal Tile\nMetal Tiles used to offer very high durability and a modern kitchen aesthetic. But one of the drawbacks is that as soon as it is installed it gets scratched. As these tiles help in overall softening hence some people prefer this naturally occurring patina. If that is not one of the priority then the metal tile is not always our preference. Metal tiles can be used greatly on work surfaces such as kitchen, bar, or utility room but are not recommended for baths or outdoor use.\n12. Resin Tile\nResin Tiles are one of the great solutions to combine styles. These tiles have an attractive appearance and are water-resistant. It is easier to make a 3D patterns from these tiles which can imitate the look of water and stones. The drawback of this tile is that it gets yellow over time particularly when exposed to the sun. Such tiles are highly recommended for water areas like bathrooms and mudrooms.\nTYPES OF TILES BASED ON ITS APPLICATION\n1. Brick Tile\nBrick tiles that are called brick slips or brick veneers are sort of a skinny silvered regular brick used as a ceramic tile. They are blocks in rectangular shape made up of clay, calcium silicate, or concrete. Bricklaying has been used for decades and bricks are more preferred than stone as a stone being more expensive when used in construction.\n2. Wall Tile\nWall tile is ceramic, porcelain, stone, or glass tile which will be put in on the wall. It is a mosaic, commonplace size tile, or giant format tile. Wall tile will have texture, patterns, or 3D style that adds interest to backsplashes, shower walls, or feature wall.\n3. Floor Tile\nEven though ground tile notions typically recalled for adornment reasons, they really are useful in activities that aim in guarding the flooring covering every single region of your home or construction. The setup of these tiles needs to be accomplished by excess care so as to steer away from possibilities of breakage which can lead to wastage of time and cash within the space.\n4. Partition Tile\nPartition tiles are hollow or solid building blocks that are fabricated from gypsum and are used in a nonbearing partition which used to serve as a base for plastering.\nLARGEST TILES COMPANY IN INDIA\n1. Kajaria Ceramics Ltd\nThe Headquarter of this company is in Delhi, the brand is famous for its valued ceramic and floor tiles as well as polished and glazed glassy tiles. It is the biggest within the list of prime ten tiles companies in the Republic of India 2020. The corporate was established in the year 1985. The leading tiles company is distributed across 9 plants Sikandrabad in Uttar Pradesh, 5 plants in Gujarat, 1 at Vijayawada in Andhra Pradesh, Jaipur & Malutana in Rajasthan.\nThe Company showcases its tiles pan-India, through its in-depth and entrenched dealer network, providing customers with the widest selection of tiles across all value points. It’s the biggest tiles company in the Republic of India by Total Sales.\n2. Somany Ceramic Ltd\nThe company was established within the year 1969. it’s the producing facility in Kadi, Kassar, and different venture plants. The corporate is unfold across the center East, Russia, Africa, India, and the UK. The leading tiles company is giving an entire vary of merchandise together with vitreous Tiles, Digital Tiles, Ceramic Wall & Floor, Glazed vitreous Tiles, bathtub Fittings, and Sanitaryware. The company has its plants in Kadi (Gujarat) and Kassar (Haryana), Republic of India, and different venture plants, generating a complete production c of sixty million sq. meters annually. It’s the second-largest tiles company in the Republic of India.\n3. HSIL Ltd\nIt was established in 1960 as Hindustan Twyfords Ltd, with a technical collaboration with Twyfords Great Britain, to introduce vitreous china sanitaryware in India. The company after that was renamed as HSIL limited in 20009. HSIL is one of the most effective brands of tile in Bharat as a way of sturdiness worries. The corporate is the most revered and leading producer within the sanitaryware section in India.\n4. Johnson Tiles\nThe company was established within the year 1901 as a separate entity of Prism. It has around 10 producing plants unfold across Asian countries and over a thousand dealers. It has forty-nine branches and twenty-eight “House of Johnson” showrooms.\n5. Grindwell Nortan Ltd\nGrindwell Norton (GNO), a corporation listed in Bombay and National Stock Exchanges, pioneered the manufacture of grinding wheels in the Asian nation in 1941. It is the third best tiles company in the Asian nation by total sales.\n6. Asian Granito India Ltd\nAsian Granito is one of the most important makers of Ceramic tiles, shiny tiles, outside tiles, designed Marble and Quartz stone in the Asian countries.\nHeadquartered in Ahmedabad, Gujarat, India. It’s one amongst the highest ten tiles firms in the Asian countries. It’s among the highest ceramic firms in Asian countries. Established within the year 1995, the AGL has emerged as one of the most important ceramic firms in Asian countries during a short span of twenty-six years. It’s India’s quickest growing Ceramic Wall & Floor Tile, Glazed shiny Tiles, Polished shiny Tiles, Composite Marble and Quartz Company and among the world’s fifty most profitable Ceramic tile firms.\n7. Cera Ceramics Tiles Ltd\nCera Sanitaryware Limited was incorporated within the year 1980 to supply customers with an entire creator and designer results. Headquartered in Ahmedabad, the corporate manufactures floor and wall tiles, sanitaryware, faucets, wellness, room sinks, mirrors, and private care. It’s among the highest tiles company in India. Currently, It is having seventy-five showrooms and quite 4000 dealers and sub-dealers. Being a pioneer in transportation and for having the most recent technology, the corporate has fastened high standards. It produces embrace glassy tiles, ceramic floor tiles, glazed glassy tiles, digital wall tiles, composite marble tiles, and quartz marble tiles.\n8. Orient Bell\nFounded in 1977, Orientbell is one of the leading Indian tile brands producing a variety of titles covering a large range of applications. The company has quite 2500 retailers unfold across the country with nine flagship stores referred to as Orientbell Tile Boutiques. Its production capability is getting ready to thirty million sq. meters.\n9. Nitco Limited\nNitco Limited is one of the oldest tile producing corporations in the Asian nation that was established within the year 1953 by Pran Nath Talwar. The Company is headquartered in Mumbai, India. Pan India, it is having twenty-two offices. It’s one of the most effective tiles makers in the Asian nations.\nThe company used to produce Ceramic tiles, glazed tiles, Gres ceramic ware tiles, HD digital tiles, Base tiles, Highlighters, Natural and designed marbles.\n10. Simpolo Tiles\nSimpolo ceramics is Morbi, Gujarat primarily based company supported in 1977, It has received the “Bhartiya Udhyog Ratna” award from Minister of State for Shipping, Govt. of India. This company is having its excellent presence not only in India but also in the international market like Taiwan, Latin America USA, UK, Kenya, Nigeria, Mauritius, Sri Lanka, Singapore and Turkey.\nHence Tiles are squared measure, a style of furnishing that may be used on interior walls, ceilings and floors. Tiles are made up of both soft and hard materials. Examples of hard tiles are marble, clay, slate, metal, ceramic, stone, or glass. Generally, soft tiles are made up of wool, cork, recycled paper, and perlite. Tiles are used as both functional and decorative environments and are available in a range of styles which used to suit all budgets and tastes.']"	['<urn:uuid:0a2e52f8-ef36-49ff-8143-3d9f1d169f1f>', '<urn:uuid:be1e013d-884b-497d-90c5-c8b086504c8c>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-13T04:43:46.574121	7	70	2809
17	I'm concerned about security risks. What's impermanent loss in AMMs?	Impermanent loss is the temporary loss of value experienced by liquidity providers when providing assets to a liquidity pool, occurring due to fluctuations in asset prices.	['Definition of Automated Market Makers\nAutomated Market Makers (AMMs) are decentralized protocols that enable the trading of digital assets without the need for traditional intermediaries such as exchanges or brokers. These protocols use smart contracts to automatically facilitate the buying and selling of assets based on predefined rules and algorithms. By removing the need for intermediaries, AMMs provide users with a more efficient and transparent way to trade assets. Additionally, AMMs often utilize liquidity pools, where users can contribute their assets to provide liquidity and earn fees in return. This allows for continuous trading and reduces slippage, providing a more seamless trading experience. Overall, AMMs have revolutionized the way digital assets are traded, making it accessible to a wider range of individuals and promoting decentralization in the financial ecosystem.\nHistory of Automated Market Makers\nAutomated Market Makers (AMMs) have a rich history that dates back to the early days of decentralized finance (DeFi). The concept of AMMs was first introduced in 2013 with the launch of the decentralized exchange (DEX) platform, Uniswap. Uniswap revolutionized the way users could trade cryptocurrencies by eliminating the need for traditional order books and instead relying on liquidity pools and smart contracts. Since then, AMMs have gained significant popularity in the DeFi space, with various platforms such as SushiSwap, PancakeSwap, and Curve Finance offering their own versions of AMMs. The history of AMMs showcases the continuous innovation and evolution of decentralized finance, providing users with more efficient and accessible ways to trade and provide liquidity to the market.\nImportance of Automated Market Makers\nAutomated Market Makers (AMMs) play a crucial role in the world of decentralized finance (DeFi). These smart contract protocols enable users to trade digital assets without the need for traditional intermediaries like centralized exchanges. The importance of AMMs lies in their ability to provide liquidity to the market. By allowing users to pool their assets and create liquidity pools, AMMs ensure that there is always a supply of tokens available for trading. This not only improves market efficiency but also reduces the impact of large orders on price volatility. Additionally, AMMs facilitate price discovery by automatically adjusting the token prices based on supply and demand. Overall, the existence of automated market makers is fundamental to the functioning of decentralized finance, as they provide a decentralized and efficient way for users to trade assets.\nHow Automated Market Makers Work\nLiquidity pools play a crucial role in the functioning of Automated Market Makers (AMMs). These pools are where users can deposit their tokens to provide liquidity for trading. By contributing to a liquidity pool, users enable the AMM to execute trades without relying on traditional order books. In return for their contribution, users earn a portion of the trading fees generated by the AMM. Liquidity pools provide a decentralized and efficient way to facilitate trading, ensuring that there is always sufficient liquidity available for users to buy and sell tokens on the AMM platform.\nAutomated Market Makers (AMMs) use a unique pricing mechanism to determine the value of assets. Unlike traditional markets where prices are set by buyers and sellers, AMMs rely on mathematical formulas and algorithms. The most common pricing mechanism used by AMMs is the constant product formula, also known as the x*y=k formula. This formula ensures that the product of the quantities of two assets in a liquidity pool remains constant, which in turn determines the price. By using this pricing mechanism, AMMs provide a decentralized and efficient way to trade assets without the need for intermediaries.\nThe trading process in automated market makers (AMMs) involves a series of steps that allow users to buy and sell assets in a decentralized manner. When a user wants to trade, they first need to connect their wallet to the AMM platform. Once connected, they can choose the assets they want to trade and specify the amount. The AMM then calculates the price based on the available liquidity and executes the trade. The trading process in AMMs is designed to be efficient, transparent, and accessible to anyone with an internet connection and a compatible wallet.\nTypes of Automated Market Makers\nConstant Product Market Makers (CPMM)\nConstant Product Market Makers (CPMM) are a type of automated market maker (AMM) that operate on the principle of maintaining a constant product of two assets in a liquidity pool. In a CPMM, the price of an asset is determined by the ratio of the quantities of the two assets in the pool. This means that as the demand for one asset increases, its price will rise while the price of the other asset will decrease. CPMMs have gained popularity in decentralized finance (DeFi) as they provide a simple and efficient way to facilitate trading and liquidity provision without the need for traditional order books or centralized intermediaries.\nConstant Sum Market Makers (CSMM)\nConstant Sum Market Makers (CSMM) are a type of automated market maker that operate on the principle of maintaining a constant sum of assets in a liquidity pool. Unlike traditional market makers, which use a fixed spread to profit from the bid-ask spread, CSMMs rely on a different mechanism. In a CSMM, traders can provide liquidity by depositing a proportional amount of each asset in the pool, and in return, they receive liquidity provider (LP) tokens. These LP tokens represent the share of the liquidity pool that the trader owns. The prices of the assets in the pool are determined by the ratio of the assets deposited. As the demand for an asset increases, its price in the pool will rise, attracting more liquidity providers. Conversely, if the demand decreases, the price will fall, incentivizing liquidity providers to withdraw their assets. This mechanism ensures that the pool always maintains a constant sum of assets, enabling continuous trading and price discovery.\nOther Variants of automated market makers (AMMs) have emerged in recent years. These variants aim to address some of the limitations of traditional AMMs, such as impermanent loss and high slippage. One popular variant is the constant product market maker (CPMM), which uses a fixed ratio of assets in the liquidity pool. Another variant is the constant sum market maker (CSMM), which allows for trading between multiple assets while maintaining a constant sum of their values. These variants offer different trade-offs and are designed to cater to specific use cases and market conditions.\nAdvantages of Automated Market Makers\nAutomated Market Makers (AMMs) offer the advantage of 24/7 availability. Unlike traditional market makers who have limited operating hours, AMMs are designed to operate continuously, allowing users to trade assets at any time of the day or night. This round-the-clock availability is particularly beneficial for global markets where participants are located in different time zones. It ensures that liquidity is always available, enabling users to execute trades whenever they want, without having to wait for market opening hours. The 24/7 availability of AMMs contributes to the efficiency and accessibility of decentralized finance (DeFi) platforms, providing users with greater flexibility and convenience in managing their digital assets.\nLower fees are one of the key advantages of automated market makers (AMMs). Traditional financial systems often involve intermediaries such as brokers or banks, which charge high fees for their services. In contrast, AMMs eliminate the need for intermediaries, allowing users to directly trade assets on decentralized platforms. This decentralized nature significantly reduces transaction costs, resulting in lower fees for users. By using AMMs, individuals can enjoy cost-effective trading and investment opportunities without the burden of excessive fees.\nDecentralization plays a crucial role in the world of Automated Market Makers (AMMs). Unlike traditional financial systems that rely on centralized intermediaries, AMMs operate on decentralized platforms such as blockchain networks. This decentralized nature ensures that no single entity has control over the market, making it more resistant to censorship, manipulation, and single points of failure. By removing the need for intermediaries, AMMs enable peer-to-peer transactions, providing greater financial inclusivity and empowering individuals to participate in the global economy. Furthermore, decentralization fosters transparency and trust, as all transactions and market activities are recorded on the public blockchain, allowing anyone to verify and audit the system’s integrity. Overall, decentralization is a fundamental principle that underpins the efficiency, security, and democratization of Automated Market Makers.\nChallenges and Risks\nImpermanent loss is a concept that is commonly associated with automated market makers (AMMs). It refers to the temporary loss of value experienced by liquidity providers when providing assets to a liquidity pool. This loss occurs due to the dynamic nature of the market and the constant fluctuations in asset prices. When the prices of the assets in the pool change, liquidity providers may end up with a different proportion of each asset than they originally provided, resulting in a loss of value. However, it’s important to note that impermanent loss is only temporary and can be mitigated by earning trading fees and taking advantage of impermanent loss insurance programs.\nFront-running is a term used in the financial industry to describe the unethical practice of executing trades based on advance knowledge of pending orders from other market participants. This practice allows the front-runner to profit from the price movements that result from the execution of the pending orders. In the context of automated market makers (AMMs), front-running can occur when a trader exploits the time delay between the submission of a transaction and its inclusion in a block. By submitting a transaction with a higher gas fee, the front-runner can ensure that their transaction is prioritized and executed before other transactions, allowing them to take advantage of price changes in their favor. Front-running is a controversial practice that undermines the fairness and transparency of markets, and efforts are being made to prevent and detect such activities in order to maintain the integrity of AMMs.\nSmart Contract Vulnerabilities\nSmart Contract vulnerabilities are a significant concern in the realm of Automated Market Makers (AMMs). As AMMs rely heavily on smart contracts to facilitate decentralized trading, any vulnerabilities in these contracts can lead to serious financial risks for users. One common vulnerability is the possibility of code exploits, where malicious actors can exploit flaws in the contract’s code to manipulate prices or drain funds from the liquidity pool. Additionally, smart contracts may also be susceptible to external attacks, such as flash loan attacks or reentrancy attacks, which can further compromise the security of AMMs. It is crucial for developers and users of AMMs to stay updated on the latest security practices and conduct thorough audits to mitigate these vulnerabilities and ensure the safety of the ecosystem.\nUse Cases of Automated Market Makers\nDecentralized Exchanges (DEXs)\nDecentralized exchanges (DEXs) are a key component of the blockchain ecosystem. Unlike traditional exchanges, DEXs operate without the need for intermediaries or central authorities. They leverage smart contracts and decentralized technologies to facilitate peer-to-peer trading of digital assets. One of the most notable features of DEXs is their ability to provide users with full control over their funds, eliminating the risk of hacks or thefts that are often associated with centralized exchanges. By enabling direct transactions between buyers and sellers, DEXs promote transparency, security, and censorship resistance in the world of finance.\nToken swaps are a fundamental feature of automated market makers (AMMs). These decentralized exchanges allow users to trade one token for another without the need for an order book or a centralized party to facilitate the transaction. Instead, AMMs rely on smart contracts to pool liquidity and determine the price of tokens based on a mathematical formula. This enables users to easily swap tokens and participate in liquidity provision, making AMMs a popular choice for traders and investors in the decentralized finance (DeFi) space.\nLiquidity provision is a crucial aspect of automated market makers (AMMs). In the world of decentralized finance (DeFi), AMMs play a vital role in facilitating the trading of various digital assets. Liquidity providers are individuals or entities that contribute funds to the liquidity pools of AMMs, ensuring that there are sufficient assets available for trading. By providing liquidity, these participants enable smooth and efficient transactions, as traders can easily buy or sell assets without causing significant price slippage. In return for their contributions, liquidity providers earn a share of the trading fees generated by the AMM. This incentivizes individuals to participate in liquidity provision, creating a vibrant ecosystem where users can easily access and trade a wide range of assets.']	['<urn:uuid:dd168acc-53d0-47c4-961a-095b14abf85a>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	10	26	2060
18	What international organizations advocate for global LGBTI rights?	There are several key international organizations working for LGBTI equality. ILGA (International Lesbian, Gay, Bisexual, Trans and Intersex Association) focuses on fighting discrimination through programs, protests, and diplomatic pressure. Stonewall is Europe's largest gay equality organization, based in the UK. The International Gay and Lesbian Human Rights Commission (IGLHRC) is a US-based NGO that addresses human rights violations against LGBTI people and has UN consultative status.	['In some instances, growing up gay today is not such a big deal. The world is changing and never before in history has homosexuality enjoyed as much media coverage as it does today. In many places, all over the world, social attitudes are changing and as a result our community enjoys liberation. That’s a good thing!\nHowever, life is not a great big glitter ball for everyone. For many of us, life is still a struggle and support can be difficult to find. In this section we list organisations across the world that work towards making life easier for gay people.\nWhile we made every effort to list organisations from as many regions in the world as possible, we know that some will not be mentioned here. Luckily, there are international organisations that will make an effort to help you or point you in the right direction should you not find one listed in your own country.\nAs an alternative, we invite you to add any organisations that you have found helpful, by simply leaving a comment with the details at the bottom of this page.\nThe following international organisations work toward the equality of lesbian, gay, bisexual, trans and intersex (LGBTI) people and their liberation from all forms of discrimination.\nILGA — International Lesbian, Gay, Bisexual, Trans and Intersex Association — Focuses public and government attention on cases of discrimination against LGBTI people by supporting programmes and protest actions, asserting diplomatic pressure, providing information and working with international organisations and the international media.\nStonewall — A lesbian, gay and bisexual rights charity in the United Kingdom named after the Stonewall Inn of Stonewall riots fame. It is the largest gay equality organisation not only in the UK but also in Europe.\nIGLHRC — International Gay and Lesbian Human Rights Commission — A US-based international non-governmental organisation that addresses human rights violations against lesbians, gay men, bisexuals, intersexuals, transgender people and people with HIV/AIDS. It is accredited by the United Nations and holds consultative status with that organisation.\nContact: email@example.com (US), firstname.lastname@example.org (Africa/ Middle East), email@example.com (Asia), firstname.lastname@example.org (South America)\nOrganisations for Arab Countries\nGay and Lesbian Arabic Society (GLAS)\nAn international organisation established in 1988 in the US that serves as a networking organisation for Gays and Lesbians of Arab descent or those living in Arab countries. They aim to promote positive images of Gays and Lesbians in Arab communities worldwide, in addition to combating negative portrayals of Arabs within the Gay and Lesbian community. They also provide a support network for members. They are part of the global Gay and Lesbian movement seeking an end to injustice and discrimination based on sexual orientation.\nOrganisations listed by country\nNational LGBTI Health Alliance Members — A coalition of organisations from across Australia which provide health-related programmes, services and research targeting lesbian, gay, bisexual, transgender, intersex and other sexuality, sex and gender diverse people.\nPhone: (02) 8568 1120\nGrupo Gay da Bahia — The Gay Group of Bahia is the oldest of its kind in Brazil and was founded in 1980. It’s a non-profit organisation and is a member of the International Lesbian and Gay Association and forms part of the International Lesbian and Gay Human Rights Commission.\nEgale Canada (formerly Equality for Gays And Lesbians Everywhere) — An advocacy organisation founded in 1986 to advance equality for Canadian LGBT people and their families, across Canada. Egale’s work includes lobbying for more equitable laws for LGBT people, intervening in legal cases that have an impact on human rights and equality, and increasing public education and awareness by providing information to individuals, groups, and media. Egale has over 3,300 members including people in every province and territory of Canada.\nInter-LGBT (Interassociative Lesbienne, Gaie, Bi et Trans) is an umbrella group of 50 LGBT organisations in France.\nUdaan Trust — A non-governmental organisation operating in the state of Maharashtra and the first HIV/AIDS organisation founded by homosexuals living with HIV/AIDS. Udaan focuses on issues of sexual health within the homosexual and transgendered communities, particularly with regard to the prevention of HIV/AIDS. In order to accomplish this, Udaan provides services such as condom distribution, sex education, counselling, and medical services to at-risk populations.\nLGBT Diversity — The LGBT Diversity programme is a coordinated response by twelve LGBT organisations in Ireland, developed to build the capacity of the LGBT sector. The programme will facilitate LGBT organisations and activists to collectively mobilise and engage with the wider community in order to advocate strategically and effectively on gender and sexuality issues.\nLGBT Helpline — Provides access to trained volunteers who provide a non-judgemental, confidential, listening support and information service for lesbian, gay, bisexual and transgender (LGBT) people as well as their family and friends. Their website also provides a gateway to information and support options for LGBT people in Ireland.\nRussian LGBT network — Interregional non-governmental organisation working for the protection of rights and social adaptation of sexual and LGBT gender minorities. The network structure includes 13 regional branches and 10 regional LGBT organisations.\nThe Lesbian and Gay Equality Project (LGEP) — A non-profit organisation that works for full legal and social equality for LGBTI people in South Africa. The LGEP was formerly known as the National Coalition for Gay and Lesbian Equality.\nCivil Society Coalition on Human Rights and Constitutional Law\nEstablished in October 2009 in response to the tabling of a now notorious Anti-Homosexuality Bill in the Ugandan Parliament, was one of the winners of the 2011 Human Rights Defenders Award, awarded by the US Department of State.\nAlbert Kennedy Trust — Supports LGBT young people in crisis and works to ensure that they are able to live in accepting, supportive and caring homes, by providing a range of services to meet the individual needs of those who would otherwise be homeless or living in a hostile environment.\nElton John Aids Foundation (EJAF) — Established in the United States in 1992 and along with EJAF in the UK, pursues the same mission – to reduce the incidence of HIV/AIDS through innovative HIV prevention programmes, efforts to eliminate stigma and discrimination associated with HIV/AIDS, and direct treatment and care services for people living with HIV/AIDS.\nGalop — London’s leading anti-LGBT hate crime charity, making life safe, just and fair for LGBT people.\nPace Health — The charity promotes the mental health and emotional well-being of the LGBT community and is committed to researching the changing needs of LGBT people, and offering wide ranging, crucial support services to meet these needs and help people move through their difficult times.\nLLGS Switchboard — London Lesbian & Gay Switchboard provides a range of services for the LGBT community. Their aim is to provide an information, support and referral service for LGBT people and anyone who needs to consider issues around their sexuality.\nHelpline: 0300 330 0630 (Daily 10am – 11pm)\nThis link on ther website, www.turingnetwork.org.uk, provides a directory of LGBT health services across the UK.\nTerrence Higgins Trust — The charity’s vision is a world where people with HIV live healthy lives free from prejudice and discrimination, and good sexual health is a right and reality for all. The Terrence Higgins Trust is generally considered the UK’s leading HIV and AIDS charity, and the largest in Europe.\nCall Direct: 0808 802 1221\nOut & Equal — Works to achieve workplace equality for all regardless of sexual orientation, gender identity, expression, or characteristics. In addition, Out & Equal provides training and sensitivity resources to LGBT employees and corporations alike through advocacy, training programmes and events.\nCall Direct: (415) 694.6500\nNational Gay and Lesbian Task Force — Founded in 1973, this non-profit organisation works to build the grassroots power of the LGBT community.\nNational Coalition of Anti-Violence Programmes (NCAVP) — A national organisation dedicated to reducing violence and its impacts on LGBT individuals in the US.\nThe Trevor Project — The Trevor Project is the leading national\norganisation, in the US, providing crisis intervention and suicide prevention services to LGBT teenagers, and anyone who needs to consider issues around their sexuality.\nCrisis Helpline: 1-866-488-7386\nTrevor Lifeline: 866-4.U.TREVOR [866.488.7386]\nLos Angeles: 310.271.8845\nMain (Fax): 310.271.8846\nNew York: 212.509.0072']	['<urn:uuid:cfa82795-61c6-4a56-bbb5-c60b95ac6e01>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	8	66	1357
19	As a seating specialist, I need to ensure proper wheelchair measurements for my patients. Can you detail all the specific measurements that need to be taken when fitting a wheelchair?	The essential measurements for wheelchair fitting include: seat width, depth, and height; patient hip, trunk, and shoulder-width; patient shoulder and axillae height; wheelchair leg, arm, and back height; wheelchair width, height, and size; knee to seat and knee to heel length; seat to the back, seat to the lower leg, and lower leg support to foot angle; and fingertip to axle length for self-propulsion capabilities.	"[""In 2008, the World Health Organization (WHO) developed guidelines to standardize the process of wheelchair service delivery. The eight steps include referral and appointment, patient assessment, prescription for the wheelchair, funding and purchasing, device installation/preparation, device fitting, patient instruction/training, and follow-up maintenance and repairs. The main features of a wheelchair include a seat, wheels, tires, casters, leg rests, wheel locks, hand rims, armrests, and cushions. Additional items may include anti-tippers, lap trays, or seatbelts if needed (NCBI, 2021).\nA comprehensive wheelchair evaluation includes a complete history and physical examination, cognitive and communication skills assessment, and consideration of premorbid functioning and co-morbidities. The patient's motor and sensory function, muscle strength and tone, vision, hearing, postural control, and range of motion will be assessed as significant deficits in any of those areas could make the operation of the assistive device unsafe. Consideration of functional impairments, ADLs, IADLs, occupational roles, social engagement, transportation needs, insurance funding, and the home or living environment must also be considered during the wheelchair assessment (NCBI, 2021).\nA standard wheelchair is 24 inches in diameter with rear wheels, 8-inch front casters, weighs 40 to 65 pounds, and is designed to be operated using the hand rims on the wheels. There are many other types of wheelchairs available, including lightweight, ultra-lightweight, one-arm drive, standard heavy-duty, and motorized wheelchairs (NCBI, 2021).\nThe following measurements must be taken to ensure an appropriate fit for a wheelchair:\n- seat width, depth, and height\n- patient hip, trunk, and shoulder-width\n- patient shoulder and axillae height\n- wheelchair leg, arm, and back height\n- wheelchair width, height, and size\n- knee to seat and knee to heel length\n- seat to the back, seat to the lower leg, and lower leg support to foot angle\n- fingertip to axle length to allow for self-propulsion (NCBI, 2021)\nThe wheelchair seating system should provide sufficient support when the patient is seated. It should allow for normal anatomical alignment while accommodating fixed postural asymmetries. There should be adequate stability when sitting in a neutral position with evenly distributed pressure. The wheelchair should promote function and the successful completion of ADLs and IADLs. The seating system will include a seat and back. It may also include lateral trunk supports, head supports, and pelvic belt supports. The seating system will either be placed on a manual or power wheelchair base. An efficient seating system should support the trunk and provide stability.\nFor this reason, the seat and back should be firm. The primary and secondary supports will minimize pressure at the bony prominences of the pelvis and sacrum. Various cushions are available, including foam, gel, contoured, saddle, wedge, antithrust, and pommel (NCBI, 2021).\nAdditional considerations for wheelchair fitting include:\n- There should be 1 inch of space between the patient's thighs and armrests.\n- The pelvis should be positioned with a slight anterior tilt to distribute body weight evenly.\n- The armrests should allow 30 degrees of shoulder flexion with 60 degrees of elbow flexion.\n- The foot should be about 2 inches from the ground and mounted far enough from the casters to avoid falls or lower extremity injury but not too far as that could place extra tension on the hamstrings (NCBI, 2021).""]"	['<urn:uuid:63e106a7-9521-4476-b32d-8c66e1d700c7>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	30	65	537
20	digital mine benefits security risks	Digital mines offer security benefits by enabling real-time intervention to manage risks and protect mineshafts and employees from illegal miners. However, they also face cyber risks including system security failures that could harm third-party systems and potential unauthorized access to sensitive information. Digital technologies enable continuous monitoring and predictive operations for improved mine efficiency and security, but businesses must be prepared for cyber threats like hacking, viruses, and potential data breaches that could compromise operations.	"['South African mining sector seen revolving round digital technologies in the future-Mining Weekly\nThe future of the mining industry in South Africa will revolve round digital developments, asserted University of the Witwatersrand (Wits) School of Mining Engineering lecturer Dr Bekir Genc during his address at the Mining into the Future conference, which took place at the Birchwood Conference Centre, in Boksburg, Gauteng, earlier this month.\nHe said that information technologies would help the sector achieve its goals of better working conditions and improved mine economics.\nGenc stated that the digital revolution was happening everywhere, and that it would soon occur in the local mining industry, “if not today, then tomorrow for sure”.\nHe noted that, in recognition of this, the Wits School of Mining Engineering had launched a Digital Mine project to support the existing strategy of the mining industry to continuously improve working conditions and mine economics.\n“Digital technologies are fundamental for efficient and safe mining, where all systems are optimised,” said Genc.\nHe added that this required the clarity of multiple sources of underground data communicated to a surface control room and back to the workplace in real time.\n“This is not happening yet as it requires an enormous amount of work, but some parties have started trying to establish these systems.”\nIn the first phase of the project, the school built a mock-up of an underground tunnel. This allowed Wits to simulate an underground mining environment that could be used for teaching, learning and research.\nThe 70 m tunnel cost about R15-million, and features a stope, rescue bay and lamp room, built with sponsorship from gold miner Gold Fields, mine support technology company New Concept Mining and gold miner Sibanye Gold. Research is being conducted into smart surveying and mapping (visualisation) systems; climate control systems and energy savings (particularly important in deeper-level mines); and smart rock engineering systems, which can monitor rock mass movement and predict seismic events.\nAdditionally, the Wits School of Mining Engineering is conducting research on smart data processing, which can locate people and assets and monitor their performance, and recognise actions and detect abnormalities, such as recognising that someone is ill.\nSmart mine design, mine planning and decision-making are also being studied.\nThe Digital Mine project involves four phases, Genc explained.\nPhase one comprised the building of the mock-up mine for research, teaching and learning, which has already been completed. Phase two comprises the building of a laboratory hosting digital technologies inside the mine, which is in the advanced planning stages.\nPhases three and four include the monitoring of an underground environment for optimised mine design and processes, and integrating a digital mine with a digital city and communities.\n“These phases are still at the conceptual stage and will require further funding to develop,” said Genc.\nHe said he believed that the Digital Mine project would benefit the mining sector by providing access to a safe, smart mine laboratory reaching into the surrounding community on a multisensor geographic information system platform (once the lab has been developed), and providing knowledge for industry so that it could collect appropriate and accurate information to optimise mine designs and processes.\nThis would enable continuous and predictive operations, while having a positive impact on mine efficiency and security. The latter is of particular relevance to gold mines, which put both mineshafts and mine employees at risk as a result of the activities of illegal miners.\nGenc noted that, with digitisation, the concept of a mine-to-order, or demand mining, would become a real possibility, thereby contributing to productivity of the mine’s bottomline and transforming the mining industry through information technology.\n“Most importantly, a digital mine will accelerate the process of reaching the industry’s zero-harm goal,” he emphasised.\nMoreover, Genc highlighted that a variety of technologies that were currently in development would also help make the digital mine a reality.\n“Underground communication systems will enable real-time intervention to manage all types of risk. Underground drones will be able to see, map and collect data, and communicate it, and can also be used to map abandoned mines that are too dangerous to send people into,” he said.\nGenc added that smart data processing and three-dimensional, or 3D, modelling “were planned in the future”, and would require participation from various schools across a number of faculties at Wits.\nThe conference was a collaborative partnership between Caterpillar, Barloworld Equipment (as the Cat Southern African dealer), the Wits School of Mining Engineering and the Wits Centre for Mechanised Mining Systems.\nBack to previous page', ""What is Cyber Risk?\nCyber Risk is first and third-party risk associated with e-business, the Internet, networks and informational assets.\nWho is at risk?\n- Business Owners who operate a website.\n- Business Owners who are concerned with their clients’ and employee’s information being compromised.\n- Business Owners who are concerned about copyright/trademark infringement.\n- Business Owners who are aware of the risks associated with computer hackers, viruses and other damaging computer programs.\n- Business Owners who understand the importance of upholding and preserving their professional reputation should an incident occur.\n- Business Owners who keep electronic records of clients names, addresses, phone numbers, social security numbers, credit card numbers and other sensitive information.\n- Business Owners who accept credit card payments.\n- Business Owners who may have employees that could compromise sensitive customer information or do something illegal to make some money.\n- Business Owners who use laptops, Blackberries or other portable devices that store client information.\nThird Party Liability\n- Disclosure Injury - Including lawsuits alleging unauthorized access to or dissemination of the plaintiff’s private information. (Can be extended to outsourced data processing and data storage services.)\n- Content Injury – Including suits arising from intellectual property infringement, trademark infringement, and copyright infringement.\n- Reputational Injury – Including suits alleging disparagement of products or services, libel, slander, defamation, and invasion of privacy.\n- Conduit Injury – Including suits arising from system security failures that result in harm to third-party systems.\n- Impaired-Access Injury – Including suits, civil fines and penalties arising from system security failure resulting in your customer’s systems being unavailable to its customers.\nFirst Party Cyber Crime Expenses\n- Privacy Notification Expenses – Including printing, drafting, postage, call center costs and advertisements, cost of credit-monitoring services, credit freezes and fraud alerts for affected customers (even when state law doesn’t require notification). Estimated at $30 per person.\n- Forensic Costs – Costs to determining how the breach occurred.\n- Crisis Management and Reward Expenses – Including the cost of public relations consultants to maintain the reputation of the business.\n- E-Business Interruption – Including first-dollar extra expense.\n- E-Theft and E-Communication Loss – Extended to networks outside of your company’s system.\n- E-Threat or Cyber Extortion - Including the cost of a professional negotiator and ransom payment to stop cyber attacks caused by malicious hackers.\n- E-Vandalism Expenses – Even when the vandalism is caused by an employee.\nA manufacturer hosted a site banner for a key vendor. The manufacturer was unaware that the vendor's slogan was similar to a slogan of a company based in France. The manufacturer was dragged into an international trademark infringement lawsuit. Claim Value: $700,000\nA chain of luxury hotels was expanding it's operations worldwide. They needed to upgrade their billing system to accomodate various currencies and tax rates. The chosen vendor upgrading the existing system, meeting all time requirements and milestones. However, during the final phase of installation, one of the installers accidentally erased $ 1.8 million of crucial data. As a result the customer sued the software installation company for the losses they incured. Claim Value: $1.8 Million\nDuring a national trade convention, the CFO of a prominent company read from a media kit about its products and those of competitors, including defamatory comments about the executive officers of a competitor. The competitor sued for libel and slander for $1.5 Million\nA bookseller created a Web site to promote itself. The Web site included passages from books. The publisher and author of one of the books quoted on the Web site sued the bookseller, alleging copyright infrinement and theft of intellectual property. The case settled for approximately $60,000. The bookseller incurred defense costss close to $35,000.\nA software development company was sued by one of its best customers after using the company's cost-estimating program. The custommer claimed that a defect in the software caused them to underbid several projects. After a lengthy investigation, the software was found free of any defect, and it was user error that caused them to underbid. The customer dropped the case after considerable legal expenses were incurred by the software developer.""]"	['<urn:uuid:77c160dc-3c56-4c04-83c3-921947fba0cc>', '<urn:uuid:9240e847-a688-419f-945f-e0ba9a3a6134>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	5	75	1437
21	planning to use mumbai metro want to know payment options mobile ticket qr code how it works	Passengers can buy Mumbai Metro tickets using their mobile phones through Paytm. The system generates a QR code on the traveler's phone, which is then scanned by readers at the entry and exit gates for access. Additionally, travelers can use a Smart Card that can be recharged for their metro travel.	['The Mumbai Metro is a rapid transit system serving Mumbai city, Maharashtra, and the wider metropolitan area. The system is designed to reduce traffic congestion in the city, and complement the congested Mumbai Suburban Railway (normally called local trains) network. Mumbai Metro is being built in three phases over 15 years, expected to be completed in 2025. Upon completion, the core system will have eight high-capacity metro railway lines, spanning a total of 235 Kilometers (146 mi) and serviced by 200 stations.\n|Number of lines||Operational -1 | Under-construction-3 | Approved-3 | Planned-3|\n|Number of stations||12 Stations|\n|Began operation||08- Jun- 2014|\n|Number of vehicles||16 Trains|\n|Train length||4 coach trains|\n|Average speed||33 km/h (21 mph)|\n|Top speed||80 km/h (50 mph)|\nMumbai Metro Map\nMumbai Metro Route Lines\nThere will be a Total of 10 Lines in MMRD Metro Named As Line 1, Line 2, Line 3, and so on Till Line 10. The Lines are also color-coded as a follows-\n|Line Name||Line Color||Total Stations||Status|\n|Line 1||Blue Line||12 Stations||Operational|\n|Line 2||Yellow Line||39 Stations||Under Constuction|\n|Line 3||Aqua Line||27 Stations||Under Constuction|\n|Line 4||Green Line||32 Stations||Under Constuction|\n|Line 5||Orange Line||17 Stations||Under Constuction|\n|Line 6||Pink Line||13 Stations||Under Constuction|\n|Line 7||Red Line||13 Stations||Under Constuction|\n|Line 8||Gold Line||08 Stations||Planned|\n|Line 9||Purple Line||No Data Yet||Planned|\n|Line 10||Magenta Line||No Data Yet||Planned|\nMumbai Metro Stations\nMMRD Metro will have Around 200 Station covering about 235 Kilometres once it is fully constructed. It’s 25% of the stations will be underground and is being built in 3 Phases in 15 years expected to be completed by 2025.\nPhase 1 of Mumbai Metro Line ( 2006-2011)\n|Line Name||Corridor Name||Distance (KM)|\n|Line 1||Versova – Andheri – Ghatkopar||11.07|\n|Line 2||Bandra – Kurla – Mankhurd||13.37|\n|Line 3||Colaba – Bandra – Seepz||38.24|\nPhase 2 of Mumbai Metro Line (2011-2016)\n|Line Name||Corridor Name||Distance(KM)|\n|Line 4||Charkop – Dahisar||7.5|\n|Line 5||Ghatkopar – Mulund||12.4|\nPhase 3 of Mumbai Metro Line (2016-2021)\n|Line Name||Corridor Name||Distance(KM)|\n|Line 6||BKC – Kanjurmarg via Airport||19.5|\n|Line 7||Bandra (E) – Dahisar (E)||16.5|\n|Line 8||Hutatma Chowk – Ghatkopar||21.8|\n|Line 9||Sewri – Prabhadevi||3.5|\nMumbai Metro Current Working Line-\n|Line||Line Color||From||To||Avg Frequency|\n|1||Blue||Versova||Ghatkopar||3 Min At peak Hour|\n8 Min at Non-Peak Hour\nMumbai Metro Line\nLine 1 or BLUE Line: Line 1 connects Versova in the western suburbs with Ghatkopar in the central suburbs, covering a distance of 11.4 kilometers (7.1 mi). It is fully elevated and has 12 stations. Work on the Versova-Andheri-Ghatkopar corridor, a part of Phase I, began on 8 February 2008. An important bridge of the project was completed in late 2012. The line opened for service on 8 June 2014.\nLine 2 or YELLOW Line: Line 2 Yellow Line This corridor is being executed in two phases i.e. 2A and 2B.The 18.589 km (11.551 mi) long 2A corridor is being executed by DMRC from MMRDA. The corridor has 17 stations (from Dahisar (West) to D.N.Nagar) and it costs Rs 64.1 billion (equivalent to Rs 72 billion or the US $ 1.01 billion).\nLine 3 or AQUA Line: This Line corridor is built almost entirely underground, and is 33.50 km (20.82 mi) long with 27 stations. The metro line will connect the Cuffe Parade business district in the south of Mumbai with SEEPZ and Aarey in the north. It will also pass through Mumbai’s domestic and international airports, for which the airport operator (GVK) has committed an equity investment of Rs 7.77 billion (equivalent to ? 9.4 billion or US $ 132.29 million in 2019).\nLine 4 or GREEN Line: Line 4 of the Mumbai Metro envisages a 32.32 km (20.08 mi) long elevated corridor, covering 32 stations from Kasaravadavali (near Thane) in the north. It costs estimated to be 165.96 billion (equivalent to Rs. 180 billion or 2.30 billion US dollars in 2019). The project will help connect the Thane city of Mumbai to an alternative mode of public transport.\nLine 5 or ORANGE Line: The 24.9 km long Thane-Bhiwandi-Kalyan. MMRD Metro-5th corridor will have 17 stations and Rs. 8,416 crores Rs. It will be a fully elevated corridor. It connects Thane to Bhiwandi and Kalyan in the eastern suburbs, a 12th line will be made a further extension to Taloja in Navi Mumbai.\nThane (West) includes Kapurbawadi, Balkum Naka, Kasheli, Kalhar, Poorna, Anjur Phata, Dhamankar Naka, Bhiwandi, Gopal Nagar, Temghar, Rajanuli Village, Govegaon MIDC, Kongaon, Durgadi Fort, Sahajanand Chowk, Kalyan Railway Station. Kalyan APMC.\nLine 6 or PINK Line: The 14.47-km-long Lokhandwala-Jogeshwari-Vikhroli-Kanjurmarg Metro-6th corridor will have 13 stations and costs Rs. 6,672 crores. It will be an elevated corridor. It will connect Lokhandwala Complex in Andheri with Vikhroli in the western suburbs and Kanjurmarg in the eastern suburbs.\nThe stations include Lokhandwala Complex, Adarsh Nagar, Momin Nagar, JVLR, Shyam Nagar, Mahakali Caves, SEEPZ Village, Saki Vihar Road, Ram Bagh, Powai Lake, IIT Powai, Kanjurmarg (W), Vikhroli-Eastern Express Highway.\nLine 7 or RED Line: The corridor is 16.475 km (10.237 mi) long, and runs from Dahisar (East) in the north to Andheri (East) in the south, with another extension to Bhayandar in the north, and Mumbai Airport is in Terminal 2 South. The line is partially elevated (under construction, with completion for 2019), and partially underground (approved, with construction planned to begin in 2018).\nLine 8 or GOLD Line: It is a proposed metro line between Chhatrapati Shivaji Maharaj Airport to Navi Mumbai International Airport. It will connect Mumbai Airport with the upcoming Navi Mumbai Airport and will have a length of about 32 km.\nBelow 2 project is now on the DPR stage.\nLine 9 or PURPLE Line: This is a proposed metro project to connect Mira Road to Virar. The length of the project is 23 km and the estimated cost of the project is 6900 crores.\nLine 10 or MAGENTA Line: This is a proposed metro project connecting Vikhroli to Kanjurmarg and further to Ambarnath-Badlapur. It will have an intersection at Kanjurmarg with line 6 i.e. pink line. The project is now on the DPR stage. The length of the project is 45 km and the estimated cost of the project is 13,500 crores.\nMMRD Metro Fare Slab\n|Kilometers (Km)||Fares (Rs.)|\n|3 to 8||10|\n|8 to 12||20|\n|12 to 15||20|\n|15 to 20||30|\n|20 to 25||30|\n|25 to 30||30|\n|30 and above||40|\nMumbai Metro Fare\nMumbai Metro Fare Starts from Rs 10 and Accroding to Stations Traveled the maximum amount is Rs 40\n|From||Versova||DN Nagar||Azad Nagar||Andheri||Western Express Highway||Chakala/JB Nagar||Airport Road||Marol Naka||Saki Naka||Asalpha||Jagruti Nagar||Ghatkopar|\n|Western Express Highway||20||20||20||10||10||10||20||20||20||20||30||30|\nMumbai Metro Running Time\nThe headway between each train on Weekdays from Versova to Ghatkopar is (From Monday to Friday) will be 3 and Half minutes at peak hours, and 8 minutes at non-peak hours. The headway between each train on Saturdays during peak hours will be around 5 to 6 minutes. The headway between each train on Sundays or Public Holidays will be 8 minutes.\n**If you have Smart Card of MMRD Mumbai Metro Rail, Then You can Recharge it from here. – RECHARGE MUMBAI METRO CARD\nMumbai Metro Time First Train –\n|First Train From||Time|\nMumbai Metro Time Last Train –\n|Last Train From||Time|\nPassengers can also buy tickets by their mobiles and enter the gates through QR code payment. The service is provided by Paytm and a QR code is generated on the travellers phone which is read by the scanners at the gate upon entry and exit.\nALSO READ –\n- New Mumbai Metro Vacancy 2020: MMRCL/HR-Rect./ 2020-01\n- Metro Recruitment:Mumbai Metro|MMRDA|110 Non-Executive Posts']	['<urn:uuid:38c18950-1e48-4fac-98ad-329bf9bc2d8c>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	17	51	1230
22	difference between columbarium ossuary church	A columbarium is a vault or wall with niches designed to store cremated remains in urns, while an ossuary is specifically a depository for bones of the dead who were previously buried in cemeteries. Both facilities can be located within church buildings, following the Christian tradition of burying the dead in consecrated areas close to places of worship. The term columbarium comes from Latin meaning 'dwelling place of a dove', with the dove being a symbol of the Holy Spirit in Christianity.	"['FROM portraits to religious vestments, Fr. Kim Margallo is doing everything he can sell to raise money for the construction of the first St. Josemaria Escriva Mission Station Church in Tacloban City.\n""It\'s a tall order. I have to use my creativity to achieve what we need to achieve,"" says Margallo, the mission priest in-charge of the mission station church.\n""My term is only for three years. But it\'s up to Palo Archbishop John Du if I will be extended. I gave myself a timeline. Hopefully, I can build the church,"" says the 35-year-old priest and artist.\nTo finish the entire mission church building, Margallo needs to raise at least P200 million.\nThe priest says that Du, a high ranking official of the Catholic Bishops\' Conference of the Philippines (CBCP), approved the plan.\n""Regardless if I could finish it or not within three years, I have to finish with the basic structure,"" he says, enthusiastically.\nAfter one year in his new assignment, Margallo however saw some progress in the construction.\n""The costly part is the detail, which is about 60 percent of the total amount,"" he says, matter-of-factly.\nOnce completed, the St. Josemaria Escriva Mission Station Church will feature a columbary and ossuary facility that will serve as a ""dignified and cost-effective Catholic alternative to casket burial"" not only in the city but also in Eastern Visayas region.\nThe facility will be located at the church\'s basement to house bone casket and cremated remains of the dead.\nFollowing its original site development plan in one-hectare donated lot, the mission station church will be composed of several church edifices and structures ""that will facilitate the services to be rendered by the church.""\nIts construction follows several phases. First is the filling up of floor elevation, followed by the construction of adoration chapel that serves as temporary church building, priest rectory, office, sacristy, and confessional room.\nPhase three is construction of the main mission church building with its basement design to be a columbarium and ossuary.\nThe phase four involves the construction of the priest rectory, followed by the multi-purpose building, function hall, and evacuation.\nThe final phase is landscaping and beautification.\nLast August 2018, Margallo launched the first pledging session through their fence program. They started the phase three, which is the construction of the main church, in December 2018.\n""This will become a landmark shrine,"" says Margallo, adding that he is preparing the documentary papers so he can ask ""first class relic"" of St. Josemaria Escriva to be displaced at the mission station church.\nMargallo says that people in the city and in the region have to go to Manila or Cebu to cremate the remains of their dead loved ones.\nMargallo says that people can ""transfer"" the remains of those cremated in the upcoming facility.\n""But looking for its funds is being passed to us now. We have to find ways,"" Margallo told SunStar Philippines in an interview.\nHe, however, reminds the public that the facility is not for sale.\n""Donors can give particular amount then we give them a token. The same with painting, we put a certain amount as a donation. The worth of one painting is not its exact price. If we put about P15,000 to the price, that would be too cheap. So it\'s just a donation,"" he says.\n""Sometimes, we also bid the paintings,"" Margallo adds.\nInterested art buyers and dealers can also have pre-orders of their oil paintings.\nBack to Catechetical church\nMargallo says the painting project to raise church funds started last July, and they have finished more than eight paintings already.\n""We can have one painting done in one day, if necessary,"" he says.\n""We started the idea of painting because of the small space and the lack of budget,"" says the priest, referring to the finished chapel, which served as one of the initial features of the entire mission station church.\nTo help him with the project, Margallo scouted two local artists Ernesto Kardante and Kim Clinton Gonzales.\nThe two artists started to paint the wall and ceiling of the adoration chapel, which served as temporary church building, aside from doing commissioned paintings to support the church.\n""This place now serves as Catechetical chapel. Anyone can come here to reflect, relax, and be at home. This is something which has been lost in the passing of time,"" Margallo says.\nAccording to the priest, going inside the chapel ""becomes a journey of faith"" as church-goers will be greeted with various paintings depicting Bible stories.\n""You can come here to meditate. The story of the entire story of salvation can be found inside the chapel. From the story of Adam and Eve, Crucifixion of Jesus Christ, the images of militant, suffering, and triumphant church, and finally the Heaven,"" says Margallo.\nPointing to the Eucharist in the middle of the chapel, Margallo says that the Catholic faithful has to go through it before they will have the salvation.\nAlso painted inside the chapel are some saints ""whose lives church-goers can look up through.""\nPainted saints include St. Isidore the Farmer, the patron saint of the village; St. Josemaria Escriva; and St. Thomas More.\n""St. Thomas More is the patron saint for politicians, statesmen, and big decision-makers. He is placed there because this church is also dedicated as adoration of St. Thomas More. This is the request of the owner of the lot. Politicians can have a place to pray here, too,"" Margallo says.\nCatching the youth\nAnother reason why the priest came up with lots of artworks in the chapel is to attract the young people to go to church.\n""With our simple pictures, we invite the youth. It\'s become a journey of their faith. We evangelize to the millennials, and sometimes it is difficult to convince them to go to church to worship. They are more on visuals, thus we have it here inside the chapel,"" adds Margallo in a report from ucanews.com.\nMargallo, who was also inspired on how the Vatican\'s Sistine Chapel was painted, said that with church artworks, people can revisit the origin of their faith.\n""The early Christians did not know how to read. They were taught through paintings. Now, we are bringing it back. This will be the move of the church,"" says Margallo, who was former youth director for six years.\nStarting from scratch\nMargallo brought his resident artist Kardante from Ormoc City to Tacloban after working with him in various art installations and designs during his previous assignment in Ormoc.\nHe said that Kardante, 55, always wanted to work in the church after he left his former religion Iglesia ni Kristo.\n""He asked me how he could help the mission station church. From there, we started to collaborate,"" Margallo says.\nWith only P1,000, Margallo bought some wood and had their hand-made canvas.\n""First, I prayed and at the end of the day, I relayed my inspiration to the artist. Whatever I said, he can execute it. I am inspired. I tell my inspiration to him,"" Margallo said.\nKardante, who started painting movie billboards, says he doesn\'t have a collection of his paintings.\nAll his paintings are commissioned.\n""He doesn\'t have enough space in his house where he can place his paintings. I told him he can paint in the chapel and eventually make it into a museum,"" Margallo says.\n""He can paint even in 15 minutes,"" says the priest, adding that Kardante can do the paint strokes of prominent Filipino artists Fernando Amorsolo and Juan Luna.\nEvery time there is a mass, Kardante can be seen painting at the corner of the chapel.\nMargallo then announced to the church-goers that they can purchase some paintings they like to help finance the building of the mission church.\n""I stayed here in the mission station church because a miracle happened to me here,"" says the soft-spoken Kardante.\n""One time, I prayed hard while painting St. Josemaria Escriva. I was having difficulty with my finances that time. The following days, someone ordered 10 paintings. It made me cry,"" Kardante says.\nChoosing the facility\nThe columbarium and ossuary facility will be built inside the mission church ""to provide Catholic faithful of Palo Archdiocese and their family members a final resting place, consecrated and conserved in a spirit of dignity and reverence for the loved ones at rest.""\nColumbarium is Latin for a ""dwelling place of a dove,"" and Christians believe that dove is a symbol of Holy Spirit.\nA columbarium is a vault, or wall with niches to store the cremated remains of a dearly departed, while a niche is a shelf-like space in the columbarium structure used for inurnment of cremated remains.\nUrns are placed in these niches as a final resting place.\n""Ossuary is a bone casket notably that of a person buried from a cemetery. The church is a walled structure vaults with an oratory facility. There are more than 2,000 niches located on the basement. The facility provides an attractive location for grieving, prayer and meditation,"" Margallo says.\nAccording to the mission station church, burial within the church itself or the adjacent church yard was once a common practice.\nHistorically, Christians from the earliest time have buried their dead in the consecrated areas in close proximity to their place of worship where they could be remembered and their remains safeguarded.\nIn 1965, the Code of Canon Law of the Catholic Church was amended to allow for cremation as an alternative to casket burial for the final disposition of our beloved.\nThe mission church decided to provide such facility at the basement of the church building.\nIt said that choosing columbarium as opposed to cemetery is a personal choice.\n""Many people choose to be buried in a columbarium and their church, because of a strong desire to be laid to rest on the grounds of a church that they loved and served. They like the simplicity of the liturgy and want to preserve a nearness to the church and perpetuate a relationship that has been a lifelong pursuit,"" it said.\nThose who choose to be placed in a columbarium at the church often are attracted by its religious focus and the nearness to the church. The proximity makes it convenient for visits by loved ones and for period of meditation and reflection, it added.\nThe cost for being placed in one of the niches is usually less than the cost of interment in a cemetery.\nOther factors that may influence the people\'s decision to choose inurnment in a columbarium are concerns about the environment and space available in a cemetery lot.\nThe ossuary is a depository for the bones of the dead.\n""Usually there are remains of these departed who were once laid in a cemetery and is now transferred to a niche,"" said the mission station church in its statement.\nTo avail of the facility, a fixed donation is needed in a first-come basis.\nA contract of donation and right of inurnment will be provided in the transaction.\n""In exchange for the said fixed donation, one niche from the columbarium and ossuary facility will be awarded to every donor as a token of gratitude. The columbarium and ossuary facility is an avenue for us to raise funds and not for business venture,"" the mission station church said in its invitation to the donors.\n""Those who have generously donated during our first pledging session are also recipients of single niche in the facility,"" it added.\nSeeing the necessity to complete and sustain the project, Margallo says they are launching their major fundraising project.\nTheir second fundraising session is to look for 1,000 donors who can donate P100,000.\n""I prayed a lot for this huge project,"" the priest says. (SunStar Philippines)', 'Allowing members to choose the sanctity of their church as an enduring place of rest, holding true to our Christian heritage\nColumbarium in early Rome meant, simply, a nesting place for doves. When Christians, relegated to Rome\'s catacombs, began using cliff-side niches to seal away ashes of the faithful who died, the term ""Columbarium"" took on its present meaning: sacred compartments for storage of cremated remains.\nWhy Select a Columbarium?\nSPIRITUAL SITE FOR FINAL REPOSE. In a return to church tradition, it provides members of the Church and their families the opportunity to choose the shadow of the church, which has been central to their lives, as their final resting place. The site is a place of beauty and dignity where friends and family can visit and meditate at any time.\nSIMPLICITY OF ARRANGEMENTS. It eliminates the pressures of choosing a burial site, casket, vault and monument. It provides a place for internment immediately adjacent to the Sanctuary.\nSUBSTANTIAL FINANCIAL SAVINGS. The combined cost of a columbarium niche and cremation are less than one-fourth the cost of a casket, burial site and marker.\nFrequently Asked Questions\nWhere is the Columbarium located?\nThe Columbarium is located on the north side of the campus between the Sanctuary and the Education Building.\nHow many niches will the Columbarium have?\nThe columbarium will have capacity for 760 niches in Phase I. 160 of these niches are currently built out and available. Each niche has the capacity to hold two urns. In the future, additional niches will be added.\nWho oversees the Columbarium?\nThe Columbarium is a ministry of First United Methodist Church Richardson and functions under the authority of the Board of Trustees. A Columbarium Committee, appointed by the Trustees, oversees its operation.\nWho may use the Columbarium?\nInurnment in the Columbarium is limited to present or past Church members and members of their immediate families, including spouses, parents and children. It is also available to ordained or diaconal ministers of the United Methodist Church.\nWhat is the fee?\nThe fee to purchase a double niche in the columbarium is $3,800. See below for financing options.\nWhat is included in the fee?\nThe niche fee includes opening and closing the niche for inurnment and perpetual care. Each niche will accomodate two urns. The fee does not cover the cost of cremation, urns, or inscription for the niche plaque.\nHow do I purchase a niche?\nPrint the APPLICATION TO PURCHASE A RIGHT OF INURNMENT and the COLUMBARIUM RULES AND REGULATIONS. Mail or bring the completed application and payment of the fee to . Once your application has been processed and payment in full has been received, a certificate covering inurnment ownership rights will be sent to the purchaser of the rights.\nHow do I pay the fees?\nYou may pay the fees by check, credit card or bank draft. The Church accepts Visa, Mastercard and Discover. Please complete the credit card information, attach a voided check (for bank draft) or attach a check to your Application Form.\nMay I finance the fees?\nYes. Fees may be financed at zero interest over a period of 24 months. A $500 deposit is required. You will not be able to select a niche location or inurn anyone in the columbarium until the fee has been paid in full. Please contact to request a payment plan.\nMay I select the niche location?\nCurrently we are allowing purchasers to select their niche location. However, at some point in time, niche assignments will be managed by the Columbarium Committee and will be assigned on a first-come-first-served basis.\nMay I honor a loved one by purchasing a niche?\nYes. Niches may be used as a memory marker when your loved one is buried or interned at another location.\nWhat happens if I purchase a niche and decide not to use it?\nAfter approval from the Columbarium Committee, FUMCR will repurchase your unused niche for 75% of the purchase price. Once cremains are placed in a niche, no refund will be made, even if the cremains are removed by the heirs. The niche cannot be sold to or used by anyone other than those listed in the original Application Form.\nWhat if I have already made other burial arrangements?\nShould you prefer inurnmnent in our Columbarium, you should contact the facility with which you made arrangements to learn how those arrangements can be terminated. Some refund may be due on services paid for, but in most cases you will need to sell burial plots or mausoleum slots through a broker or the newspaper.\nWhat if I have additional questions?\nIf you would like more information on The Columbarium, please contact at 972.996.0146. She will be happy to answer any questions you may have or put you in contact with The Columbarium Committee, if needed.']"	['<urn:uuid:ddc7eb98-d8fd-4058-beb5-d2cf3bcdb162>', '<urn:uuid:3dc02796-c8c2-4ead-a683-7b73c5e0797c>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	5	82	2755
23	mouth bacteria good bad effects	The oral microbiome has both beneficial and harmful effects. In terms of benefits, a balanced oral microbial ecosystem helps maintain oral and systemic health, protecting against pathogen overgrowth. However, harmful biofilms like dental plaque can harbor acid-producing microbial species that, if not removed regularly, lead to tooth decay, cavities, and gum disease. Additionally, harmful biofilms protect organisms from immune activity and antibiotics while impeding the development of health-promoting biofilms.	"['I trained as a dentist back in the 1980s. I am happy that much of what I learned in dental school so long ago may no longer apply. In fact, recent research on the oral microbiome has led me to a new understanding of oral health and disease. These discoveries are changed my thinking about the role of the oral microbiome in systemic chronic disease. In particular, researchers are uncovering a previously hidden connection between oral diseases and autoimmune diseases.\nContrary to what I was taught, the goal of good oral hygiene is no longer just killing “bad” bacteria. Today, we are more focused on balancing the ecology of the oral microbiome. Case in point, on a recent trip to NYC I met with Dr. Gerry Curatola, who seems to agree with this approach in his recent book, The Mouth-Body Connection (1).\n“Not only do we know that human health depends on maintaining balanced relationships…within complex communities of microorganisms, but microbes are recognized to contribute to disease in previously unexpected ways… overzealous oral hygiene is being recognized as a cause of disease.”Curatola, 2017\nThe mouth is a gateway to the rest of the body\nWhile the mouth is one of the most vital body parts to our health, we often overlook its significance. In fact, as I researched the oral microbiome I was surprised by many new facts about oral health. This was despite my background as a clinical dentist!\nI was startled to find that nearly half of American adults have periodontitis. This is the most severe form of gum disease. But did you know that serious gum disease increases your likelihood of meeting ADA guidelines for diabetes screening? And by a shocking average of 30%?2 We cite more statistics below.\nWhat is the oral microbiome?\nOver the past 15 years, microbiologists have contributed hundreds of species to the Human Oral Microbiome Database(HOMD). Furthermore, they have cultured 68% of an estimated 700 different species of oral bacteria. To repeat, 700 different species of oral bacteria! That doesn’t include fungi or viruses, also key players in oral ecosystems. I could even have CosmosID profile my own oral microbiome populations.\nThe microbiome is highly variable both between and within individuals. Even more, various sectors of an individual’s microbiome show great diversity between compartments in the body. Species of bacteria present differ by location (e.g., hair, skin, stomach, and oral cavity, as shown below).\nIn the GI tract high levels of species diversity are correlated with better health. In contrast, oral disease is associated with an increased diversity and richness of the oral microbiome. As shown below, the oral cavity varies in species diversity. There are additional influences from biofilms and environmental conditions that may hinder or enhance disease.\nMultiple human microbiomes\nSource: Frontiers in Microbiology\nWhy should we care about our oral microbiome?\nNew understanding of the oral microbiome is shaping how we think about caries, periodontal and systemic diseases. The traditional view was that these diseases were caused by a limited number of pathogens. But now, we think of the oral microbiome as finely tuned communities. The balance of these communities influences not only oral health and disease, but also some systemic diseases (3). We describe an unbalanced microbiome with the term dysbiosis.\nWhat happens when the oral microbiome is out of balance?\nThe mouth is the gateway between the outside world and our immune systems. Many things: food and drink, nutrients, microbes and toxins enter through the mouth. Moveover, the oral microbiome seeds the rest of the GI tract. There is a 45% overlap between the microbes found in the mouth and those in the colon.\nThe oral microbial ecosystem is vital to maintaining oral and systemic health. Saliva and biofilms on the teeth and soft tissue maintain balance in the oral cavity. This protects the mouth against pathogen overgrowth. In contrast, an unbalanced oral cavity ecosystem can stir pathogen activity and lead to oral disease.\nA healthy oral microbiome needs good oral hygiene and a well functioning immune system. As shown below, poor oral hygiene, immune disorders and genetic predispositions all contribute to the disease cycle.\nMultiple risk factors for oral dysbiosis & disease\nA number of risk factors are shown below. Any of them may cause microbes to grow abnormally or become more virulent, consequently leading to disease. The key to oral health is maintaining balance among the diverse microbes of the oral microbiome. These microbes can interact both positively and negatively with the host. We do not yet have a good understanding of the cause and effect of ecological shifts in oral microbiome composition.\nSource: Wiley Online Library\nDecay is a polymicrobial disease, influenced by many factors\nIn dental decay (aka caries), carbohydrates ferment to organic acids (shown in red below). These organic acids lower local pH. Consequently, low pH demineralizes the tooth surface and drives dysbiosis in which both acid-producing and acid-tolerating microbiota are present. On the other hand, in a healthy oral cavity (in blue) shows a more complex pattern of metabolism. This pattern involves the catabolism (breakdown) of salivary proteins and glycoproteins. This process raises the pH of the mouth. Such a more alkaline mouth neutralizes microbial acid products.\nHowever, dry mouth, too much snacking, smoking and other risk factors promote demineralization (net mineral loss). Eventually, demineralization leads to tooth decay, and caries or cavities.\nTraditional caries-associated species include Streptococci mutans and Lactobacillus species. Increasing evidence points to the roles of Actinomyces, Bifidobacterium, and other Gram-positive rod species as well.\nSource: British Dental Journal\nThe oral microbiome in periodontal health and disease\nRecent advances in metagenomic, metatranscriptomic, and mechanistic studies suggest a new model of periodontal disease, as shown below. To explain, in this model, disease results from polymicrobial synergy and dysbiosis. It follows that persistent dysbiosis unbalances the mouth from homeostasis to disease. Furthermore, this imbalance affects not just the mouth but also distant sites in the body (6).\nRecent research suggests that potassium may be a key signal in host-microbiome dysbiosis in periodontitis. Moreover, the role of potassium may point to how changing diet could prevent periodontal disease progression (7).\nThe oral microbiome in immune health & autoimmune disease\nNew data suggests that changes in the interactions between the core (baseline) and variable microbiome are linked to several autoimmune diseases, as shown below.\nChanges in the oral microbiome can be triggered by the host’s environment, genetic susceptibility, diet, smoking, etc. Thus, these factors may contribute to the pathogenesis of several diseases. Examples include Sjogren’s syndrome, rheumatoid arthritis (RA), systemic lupus erythematosus (SLE or lupus), and Crohn’s disease. (Read here to find out the microbiome’s role in these diseases.) Moreover, there is a delicate balance between the immune system and the microbiota. Therefore, disturbing that balance (dysbiosis or microbial imbalance) can trigger autoimmune progression or flares (8). This is a mechanism for how the oral microbiome influences health and disease.\nBy Bonnie Feldman, DDS, MBA and Ellen M. Martin\nInterested in delving deeper into the role of the oral microbiome in autoimmune health? Explore this curated bibliography of recent research publications.\nClick here for more autoimmune resources.\nTo learn and engage more… attend a session at the 5th Microbiome R&D and Business Collaboration Forum. See what it’s all about here!\n Curatola, Gerald P. et al., “The Mouth-Body Connection: a 28-Day Program to Create a Healthy Mouth, Reduce Inflammation, and Prevent Disease throughout the Body,” Center Street, (2017). https://books.google.com/books/about/The_Mouth_Body_Connection.\n Strauss, S. M., et al., “The dental office visit as a potential opportunity for diabetes screening: an analysis using NHANES 2003-2004 data.” Journal of Public Health Dentistry (2010): 70: 156–162. http://onlinelibrary.wiley.com/doi/10.1111/j.1752-7325.2009.00157.x/full\n Zhang, Xuan et al. “The Oral and Gut Microbiomes Are Perturbed in Rheumatoid Arthritis and Partly Normalized after Treatment.” Nature Medicine 21.8 (2015): 895–905. http://www.nature.com/nm/journal/v21/n8/abs/nm.3914.html.\n Zarco, Vess et al. “The Oral Microbiome in Health and Disease and the Potential Impact on Personalized Dental Medicine.” Oral Diseases 18.2 (2012): 109–120. http://onlinelibrary.wiley.com/doi/10.1111/j.1601-0825.2011.01851.x/full.\n Dewhirst, Floyd E. “The Oral Microbiome: Critical for Understanding Oral Health and Disease.” Journal of the California Dental Association 44.7 (2016): 409–10. https://www.ncbi.nlm.nih.gov/pubmed/27514152.\n Yost, Susan et al. “Potassium is a key signal in host-microbiome dysbiosis in periodontitis,” PLOS Pathogens (2017): e1006457. https://www.ncbi.nlm.nih.gov/pubmed/28632755.\n Nikitakis, N. G., et al. “The autoimmunity–oral microbiome connection.” Oral diseases (2016). http://onlinelibrary.wiley.com/doi/10.1111/odi.12589/full.', 'Long before you became a part of your community and, hopefully, a contributing member of society, there were the original communities, which were made of networks of bacteria. The inhabitants of these worked together to ensure the survival of as many members as possible. Today, these communities are called biofilms and they’re found almost anywhere they can survive. Fossil records indicate they’re at least 3.25 billion years old, and they’ve been with us ever since.\nWhat Are Biofilms?\nBiofilms are slimy, microbial strongholds that grow in aqueous environments and typically adhere to surfaces. If you’ve ever taken a microbiology lab, you’ve probably seen sticky glue-like substances grow in Petri dishes after performing a smear. The small slimy colonies smeared on plates are biofilms. They’re inhabited by tiny individual microbial colonies of bacteria, yeast, or algae. Outside the lab, the types of surfaces biofilms stick to range from the interior of sink pipes to boat hulls, the exterior of rocks and leaves, and even areas of the body like your teeth and tissues. Some well-known examples you might be familiar with include pond scum, mildew, kombucha SCOBYs, and dental plaque.\nBiofilms help ensure the survival of the various microscopic organism species that they’re composed of by decreasing their chances of being removed or eliminated by soaps, antiseptic detergents, and antibiotics. Usually, only the surface or edges of the biofilm are affected, protecting the deeper layers of microorganisms and slime from removal. This presents a health concern if the biofilm contains harmful microorganisms. In fact, some researchers believe that some persistent or recurring infections may result from stubborn biofilms in the body that evade immune defenses.\nBiofilms aren’t all bad, though. Beneficial biofilms in your gut provide a stable colony of probiotic bacterial and fungal species that prevent harmful colonies from gaining a foothold in your gut ecosystem. Biofilms develop in the appendix, mouth, vagina, colon, ear canals, lungs, and nasal passages. Despite being a seemingly dry tissue, your skin harbors a community of S. epidermidis in a biofilm structure found throughout the outer layers of your epidermis.\nHow Do Biofilms Form?\nThe microorganisms most people are familiar with are planktonic, or free floating, organisms. The formation of a biofilm usually begins when a single planktonic microbe tenuously clings to a surface. In the body, this typically means a given microbe finds a molecular handhold called an adhesion site on a surface (epithelial) cell of one of your tissues or organs, but microbes can also cling to the mucous layer that covers certain tissues. From here, other microbes begin linking to the original.\nThis small collection of microorganisms forms a settlement on the surface by secreting a substance called extracellular polymeric substance (EPS), which is made of enzymes, DNA, proteins, and sugar molecules called polysaccharides. It’s kind of like microbial terraforming. They make your body a more hospitable environment for more microbial cells and act as a sort of spider’s web, preventing microbial cells from dislodging from the matrix. Notably, these colonies are more slime than cells. Only about 15% of a given biofilm is made up of cells. The remaining 85% is comprised of the slimy EPS matrix.\nThis network of microbes and slime develops channels to transport nutrients and water to the microcolonies embedded within it, much like how the blood vessels in your body supply your cells with nutrients. These cells almost function as an organ by communicating with each other using the slimy matrix to deliver chemical messenger molecules, a mechanism called quorum sensing. This chemical communication leads to gene regulation, in which genes are activated and inactivated as needed to preserve the biofilm or help it grow.[3, 5]\nJust like the mildew in your shower, biofilms grow. They form a stronger hold on their surface, making them more difficult to wipe out. This is an excellent characteristic for health-promoting biofilms, but it’s alarming when harmful microbes plant their flag in your body to start settling down.\nHealthy Biofilms in the Gut\nYour gut is uniquely suited to biofilm formation since the lining of the intestine is covered with a mucous gel-layer that protects your gut microbes. Gut microbes can infiltrate this layer to set up stable colonies. Notably, microbes are rarely able to fully breach the protective mucous layers throughout your body to reach the underlying epithelial cells. In the gut, your immune system appears to encourage biofilm formation by secreting immunoglobulin A (IgA), an immune protein that makes bacteria stick together, or agglutinate. This phenomenon encourages stable microbial composition and inhibits the growth of harmful organisms.[3, 6, 7]\nSome researchers think that intestinal biofilms are another line of intestinal defense. They simultaneously assist the development of biofilms, but also prevent microorganisms and undesirable substances and molecules from crossing into the body from the colon. You can think of them as a sort of sealant that protects a potentially porous tissue from being infiltrated, like a wood varnish that seals out water to prevent mildew.\nBiofilms Harmful to Your Health\nUnfortunately, biofilms and the mucous layers they assimilate with also protect harmful microbes from being eliminated. In fact, a harmful gut ecosystem might be more challenging to balance because the mucous layer that coats the intestines tends to thicken to defend your surface cells from inflammation-provoking substances, foods, drugs, and microbes. This gives the unhealthy microbes a larger mucous layer in which to proliferate. The microbes huddle down, guarded by their biofilm matrix, to weather whatever immune defenses your body tries.\nThe Effects of Harmful Biofilms\nHarmful biofilms protect harmful organisms from physical removal, immune activity, antimicrobials, and antibiotics. Not only do they allow unchecked harmful organism overgrowth, but they also impede the development of health-promoting biofilms. In the presence of harmful biofilms, there is potential for increased virulence due to gene transfer between cells in the biofilm.\nBiofilms have proven frustratingly resistant to efforts to thwart or address organism overgrowth. They are especially worrying for people with cystic fibrosis and chronic sinus infections due to the great quantities of mucus generated that can quickly become a safe harbor for harmful microbes.\nAn Example of Harmful Biofilm: Dental Plaque\nOne easily observed biofilm is found right in your mouth. Dental plaque is a biofilm that tends to harbor acid-producing microbial species. If you don’t remove it regularly by brushing, flossing consistently, and seeing the dentist, the acid these microbes produce can damage your teeth. Poor dental hygiene leads to bad breath, tooth decay, dental cavities, and gum disease. It may also contribute to the incidence and severity of respiratory conditions in people with weak immune systems when biofilm is transferred from the mouth to the lungs.[9, 10]\nAlthough you can’t sterilize your teeth, you can physically remove oral biofilms, yeast, and bacteria by brushing and flossing after every meal. Eliminate sticky candies, refined sugar, and refined carbs from your diet to discourage the harmful bacteria like Streptococcus mutans from dominating your mouth microbiota. These kinds of food cling to your teeth and feed harmful oral bacteria. Stay hydrated to prevent dry mouth, a condition that also contributes to stable harmful colonies.\nGoing even further, you can try to nourish helpful oral bacteria growth like Streptococcus salivarius by eating fermented foods, probiotics, and cutting down on the alcohol-based mouthwashes which indiscriminately wipe out both healthy and harmful oral microbes.\nHow to Encourage Healthy Biofilms\nBiofilms can occur almost anywhere that microorganisms live on your body. Therefore, it’s essential to promote healthy biofilms in the gut and reduce your chances of developing harmful biofilms in other areas of the body with good (but not excessive) hygiene, a strong immune system, and a healthy diet. Managing biofilms in your body often requires actions specific to the tissue or area, like brushing your teeth. Consult your healthcare practitioner if you suspect harmful biofilms may be affecting your health.\nResearch is still emerging for solutions to biofilms in difficult to reach tissues, so there aren’t any hard and fast recommendations to address them. That said, aromatic phytochemicals like thymol, eugenol, carvacrol, and cymene have distinct biofilm-inhibiting properties, and they’re easy to incorporate into your diet.\nConsume herbs and spices like thyme, oregano, and cloves to get these beneficial phytochemicals, along with many conutrients like terpenes, into your diet. You can consume the oils of these spices by adding a tiny drop to a pot of fragrant tea or a large jar of homemade salad dressing. Look to your food first to preserve your health. Relying on a diverse health-promoting diet provides you with a complementary array of active phytonutrients that offer a multi-pronged approach to keep you in excellent health.[11, 12, 13, 14]\n- Hall-Stoodley, Luanne, William Costerton, and Paul Stoodley. ""The Application Of Biofilm Science To The Study And Control Of Chronic Bacterial Infections."" The Journal of Clinical Investigation 1121.10 (2003): 1466-1477. Web. 21 Apr. 2017.\n- Bjarnsholt, T. ""The Role Of Bacterial Biofilms In Chronic Infections."" APMIS 136 (2013): 1-51. Web. 21 Apr. 2017.\n- Costerton, Willliam, et al. ""The Application Of Biofilm Science To The Study And Control Of Chronic Bacterial Infections."" The Journal of Clinical Investigation 1121.10 (2003): 1466-1477. Web. 21 Apr. 2017.\n- ""Understanding Biofilms."" Bacteriality.com. N.p., 2017. Web. 21 Apr. 2017.\n- Singh, Pradeep K., et al. ""Quorum-sensing signals indicate that cystic fibrosis lungs are infected with bacterial biofilms."" Nature 407.6805 (2000): 762-764. Web. 21 Apr. 2017.\n- Bollinger, R.R., Barbas, A.S., Bush, E.L., et al. ""Biofilms in the normal human large bowel: fact rather than fiction."" Gut 2007;56:1481-1482.\n- Orndoff, Paul, et al. ""Immunoglobulin-Mediated Agglutination Of And Biofilm Formation By Escherichia Coli K-12 Require The Type 1 Pilus Fiber."" Infection and Immunity 72.4 (2004): 1929-1938. Web. 21 Apr. 2017.\n- Bollinger, R., et al. ""Human Secretory Immunoglobulin A May Contribute To Biofilm Formation In The Gut.""Immunology 109.4 (2003): 580-587. Web. 21 Apr. 2017.\n- Marsh, Philip D. ""Dental Plaque As A Biofilm And A Microbial Community – Implications For Health And Disease."" BMC Oral Health 6.1 (2006): n. pag. Web. 21 Apr. 2017.\n- Paju, S., and F.A. Scannapieco. ""Oral Biofilms, Periodontitis, And Pulmonary Infections."" Oral Diseases 13.6 (2007): 508-512. Web. 21 Apr. 2017.\n- Burt, Sara A., et al. ""The Natural Antimicrobial Carvacrol Inhibits Quorum Sensing In Chromobacterium Violaceum And Reduces Bacterial Biofilm Formation At Sub-Lethal Concentrations."" PLoS ONE 9.4 (2014). Web. 21 Apr. 2017.\n- Braga, Pier Carlo, et al. ""Thymol Inhibits Candida Albicans Biofilm Formation And Mature Biofilm."" International Journal of Antimicrobial Agents 31.5 (2008): 472-477. Web. 21 Apr. 2017.\n- Nostro, Antonia, et al. ""Effects Of Oregano, Carvacrol And Thymol On Staphylococcus Aureus And Staphylococcus Epidermidis Biofilms."" Journal of Medical Microbiology 56.4 (2007): 519-523. Web. 21 Apr. 2017.\n- Aggarwal, Bharat B., and Debora Yost. ""Healing Spices."" 1st ed. New York: Sterling Pub. Co., 2011. Print.\n†Results may vary. Information and statements made are for education purposes and are not intended to replace the advice of your doctor. If you have a severe medical condition or health concern, see your physician.']"	['<urn:uuid:3be6c633-ccc0-4467-9e34-b03e7f356a00>', '<urn:uuid:f5a8eb6e-b848-483f-8a75-eec5e71c2a14>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	5	69	3202
24	How do landscape blocks contribute to retaining wall durability, and what are the various types of retaining wall materials used in modern construction?	Landscape blocks made of precast concrete offer durability and feature textured faces that resemble stone. They come in two major varieties - lipped and pinned - and require neither footing nor mortar, automatically creating a wall that leans into the hillside for strength. As for materials used in modern construction, retaining walls can be built using various options including brick, steel, concrete, stone, and timber materials. The choice of material depends on the wall's purpose and environmental conditions. These materials not only block soil but also prevent erosion, and must be selected based on factors such as gravity, weather conditions, and wall height.	"['Is a sudden slope in your yard preventing you from achieving the landscape you want? One way to even the score is by building a retaining wall. By replacing the slope and leveling the yard to either side of the wall, you can create two distinct tiers for split level form and function.\nLandscape blocks are some of the best things to happen to walls in a long time. Made of precast concrete, they have textured faces that make them look a lot more like stone than concrete. The blocks are durable and can easily be cut into smaller pieces to offset courses as you build your wall. They require neither footing nor mortar, and stacking them automatically creates a wall that leans into the hillside for strength.\nLandscape blocks come in two major varieties, lipped and pinned. The retaining wall in this project uses lipped blocks. For any wall four feet or higher, consult a professional rather than tackle it yourself. The Home Depot offers installation services for many products. And any time you plan a project that involves digging or reshaping your land, be sure to call 811 first to find out the location of utilities in your yard.\n- 1-2 days for a small length of wall\n- Construction adhesive\n- Drainpipe (optional)\n- Landscape blocks\n- Landscape fabric\n- Mason’s line\n- Sand (optional)\n- Beam level\n- Circular saw with a masonry blade\n- Rubber mallet\n- Torpedo level\n1. Dig and level a trench\nLay out the wall with batterboards and mason’s line. Dig a trench as wide and deep as the manufacturer recommends. Line the bottom with an inch or so of sand, rake it level, and compact it with a tamper. Place some of the soil you remove on the downhill side of the trench to level the slope. Line the back and bottom of the trench with landscape fabric. Unroll enough fabric to cross back over the trench later, plus about a foot more than the height of the wall. If you have to piece together lengths of landscape fabric, overlap any adjoining edges by 3 to 4 inches.\n2. Lay the first course\nSome manufacturers instruct you to set the first block lip up; others instruct lip down. Follow the directions. Lay the first block and check its level from side to side and front to back. Tap with a rubber mallet to make small adjustments. Put sand under the block to solve bigger problems. Lay the remaining blocks one at a time. Make sure each block is level, checking both side to side and front to back. Put the level across the new block and at least one of its neighbors to make sure the tops of the blocks are level with each other. The spaces at the ends of the course may not require a full block. Buy smaller blocks or trim larger ones to fit.\n3. Lay the second course\nIf the first course ended with a full block, begin the second course with a half block. If you started the first course with a partial block, trim a full block so it overlaps the seam below by at least 3 inches. (Check the manufacturer’s instructions.) Lay a second course of blocks with their lips pointing down, over the back of the wall. Position each block so it spans a joint in the course below. As you work, verify that the blocks are level; if not, shim the low end with asphalt shingle or some landscape fabric. As you come to the end of the wall, trim a block to fit in the opening. When you begin building the rest of the wall, alternate so one row begins with a full block and the next with a half block.\n4. Lay a third course and some drainpipe\nNot all block walls require a drainpipe, though it’s never a bad idea. Lay the third course the same way you laid the second course, leaving an exit point for the drainpipe at one end of the wall. Add a bit more gravel behind the wall to create a flat surface, and tamp the gravel. Tuck a drainpipe behind the wall. Fill in completely behind the wall to the top of the course and tamp. Continue building the wall, adding gravel and tamping after every course. When you’re a course or two below the top of the wall, fold the landscape fabric over the gravel and trim off any excess.\n5. Lay capstones\nThe keystone shape of the blocks leaves triangular gaps between the stones, which are covered up with special blocks called capstones. Apply a bead of construction adhesive along the top of the wall and set the capstones in place. Put topsoil in the space between the landscape fabric and the top of the wall and fill what remains of the trench in front of the wall. Rake to make a flat surface and tamp.\nGot questions about this article or any other garden topic? Go here now to post your gardening ideas, questions, kudos or complaints. We have gardening experts standing by to help you!', ""Reviews & Service Guarantee. Find a Prescreened Paving Stone Supplier!...\nRetaining Wall Definitions\nRetaining walls are walls built into sloping terrain that stop the earth from moving downward. Using retaining walls, one can make usable areas out of hilly ground, such as flat gardens and terraces and some, especially brick and stone walls, are popular for landscape use. They usually greatly improve the value of what was a previously constantly eroding backyard. There are several different designs of retaining walls.\nGravity wall – Built using masonry blocks, paved stone or concrete, gravity walls hold back the soil simply from their own weight, or gravity. This type of wall will usually have a sloping front to help support the wall. Homeowners often prefer gravity walls for landscape use because the stone rocks and paved bricks used to build them looks better than some of the other industrial options.\nCantilevered wall– Built in the shape of an inverted T, these walls use steel-reinforced concrete and steel beams cantilevered from the bottom to support the weight of the wall. Overall, this approach uses less materials than gravity walls. This type is often used for large basements of homes.\nPiling wall – Built from steel beams or wooden planks, these walls are driven straight into the ground and can be used to hold back soft soil. Also, anchors can be placed behind these walls to help hold them up in case of a partial soil collapse.\nDrainage – Once a wall is built, the area's natural drainage system is blocked. Therefore, when constructing retaining walls, a draining system must be designed for the soil behind the wall to keep hydrostatic pressure in check and maintain healthy soil. Failure to do so could result in an eventual collapse of the wall.\nRetaining walls are structures that are designed to help reinforce soil, or resist the lateral pressure of soil. They alter the ground elevation, and apply pressure in order to create a wall or barrier. The height of retaining walls varies, as does the materials used to create them. While a contractor may build one retaining wall from concrete, another one may be built from materials like wood or rock. These walls are installed in a number of yards, public gardens, and other landscaped areas across the United States.\nThere are all kinds of retaining walls built from materials like brick, steel, concrete, or stone. These walls not only block soil, but they also prevent erosion. Usually a licensed contractor, masonry worker, or landscaper will build retaining walls for clients and businesses. These reinforced structures must be able to support and block the soil they are built against. Depending on the design and purpose of the wall, these may be constructed from timber materials, or may require something heavier such as bricks with mortar in order to provide ample strength. Often the top of retailing walls are level with the ground, but it just depends on the environment it is built in. Some of the other factors contractors must consider when building these structures are gravity, weather conditions, and wall height.\nYou can learn a lot about retaining walls, brick piling methods, mortar materials, licensed landscapers, and other materials used to block soil by surfing the web. There is plenty of information pertaining to this topic online.""]"	['<urn:uuid:abfa774d-fc18-4305-a1fb-5fde1928995a>', '<urn:uuid:e738a145-8052-4368-b3c6-4857fde4a1a1>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:43:46.574121	23	103	1410
25	difference between volunteer jam music festival carter family bristol sessions musical legacy influence	The Bristol Sessions of 1927, which featured The Carter Family, marked the 'big bang' of country music when Ralph Peer recorded foundational country music acts in Bristol, TN/VA. In contrast, Charlie Daniels's Volunteer Jam, launched in 1974, became an annual multi-artist, multi-genre spectacle often lasting over 10 hours, featuring diverse artists from country legends like Roy Acuff and Bill Monroe to rock and R&B acts like Billy Joel and James Brown. While the Bristol Sessions established country music's foundations, the Volunteer Jam represented the genre's evolution and cross-genre collaboration.	"[""The House of Cash is one of the most critical and accomplished family lineages in the history of country music. Though Johnny Cash is where most of the attention dwells, now three generations of performers have emerged, while Cash’s siblings have also contributed to country music.\nThe Carter Family\nEven though names like Jimmie Rodgers, Roy Acuff, and The Carter Family loom large for many of country music’s devoted fans, they don’t necessarily rise to the level of household names like Ernest Tubb, and of course the great Hank Williams, who was the centerpiece of the third installment of the Ken Burns ‘Country Music’ documentary.\nArnold Schultz, Bill Monroe, Chet Atkins, Don Maddox, Dwight Yoakam, Earl Scruggs, Eddie Stubbs, Ernest Tubb, Faron Young, Felice and Boudleaux Bryant, Grand Ole Opry, Hank Thompson, Hank Williams, Hazel Smith, Holly Williams, Ken Burns, Kitty Wells, Lesley Riddle, Lester Flatt, Little Jimmy Dickens, Merle Haggard, Nathan Turk, Nudie Cohn, Ralph Stanley, Roy Nichols, Rufus Payne, Tee-Tot, The Carter Family, The Maddox Brothers and Rose, The Stanley Brothers, Tom T. Hall, Webb Pierce\nThe first episode of the Ken Burns Country Music documentary tasked itself to define what country music is by delving deep into its origins and original purveyors. The second episode called “Hard Times” began the work of explaining why the music means so much to so many people.\nAsleep at the Wheel, Billy Monroe, Bob Wills, Chet Atkins, DeFord Bailey, Delmore Brothers, Don Maddox, Eddie Stubbs, Gene Autry, Jean Shepard, Johnny Cash, Ken Burns, Mac Wiseman, Marty Stuart, Minnnie Pearl, Ray Benson, Roy Acuff, Roy Rogers, Sons of the Pioneers, The Carter Family, The Monroe Brothers, Tommy Duncan, Vince Gill, Wyalon Jennings\nOver seven years of full-time labor on the part of numerous people, over 101 interviews conducted, countless hours of archival work digging up old photographs, audio, video, and other vintage material, and an elongated year-long promotional effort finally culminated in the broadcast of the debut episode for the Ken Burns Country Music epic.\nDeFord Bailey, Dolly Parton, Fiddlin' John Carson, Grand Ole Opry, Holly Williams, Jimmie Rodgers, Kathy Mattea, Ken Burns, Ketch Secor, Marty Stuart, Mel Tillis, Merle Haggard, Old Crow Medicine Show, Rhiannon Giddens, Rosanne Cash, The Carter Family, Uncle Dave Macon, WSM\nPreserving the roots of country is not always just about paying homage. Sometimes it is about sowing disharmony or speaking out in protest to help force country music back on the right path. Music Row and the country music industry will always be about money first. The artists are the ones who must take the lead and reign the business in.\nIf you’re into country music and the history of it, you’re probably used to hearing about the “King” of this, or the “Father” of that. Since the history of country music is so important to keeping the lineage of the music alive, country pays special homage to the people who helped form or popularize the genre.\nBill Monroe, Bob Wills, Carl Perkins, George Strait, Hank Williams, Jimmie Rodgers, Jimmy Martin, Kitty Wells, Lena Hughes, Loretta Lynn, Mary Padgett, Maybelle Carter, Reverend Horton Heat, Rhonda Vincent, Rose Maddox, Roy Acuff, Slim Dusty, Spade Cooley, The Carter Family, unknown hinson, Wanda Jackson, Wayne Hancock\nAs an offering for Record Store Day’s upcoming Black Friday releases, Johanna and Klara Söderberg have covered Simon & Garfunkel’s iconic tune “America,” written by Paul Simon. A master work of the American songbook, the song peerlessly encapsulates the forlorn beauty of youthful restlessness and underlying abject fear that are an indelible part of the American experience.\nLess country music Christmas albums, and more country music Halloween albums I say. And if a cottage industry happened to crop up for spooky country music every October, it would stand to reason Madison, Wisconsin’s Those Poor Bastards would have the market cornered. Beware interlopers and carpetbaggers, these bastards have been purveyors of their self-described “Country Doom” for a decade.\nActing as a guide through both the explanation of the roots of country music and the streets of Nashville, Justin Townes Earle and many others try best to define “country” for a foreign audience in the film. The Country Roads DVD also includes an entire Justin Townes Earle concert performed at Pace University on October 26th and 27th of 2012 called “The Spirit of Woody Guthrie.”\nAmanda Shires, Angaleena Presley, Artic Monkeys, Ashley Monroe, Brazilbilly, Caitlin Rose, Country Roads The Heartbeat of America, George Hamilton IV, John Carter Cash, Johnny Cash, Justin Townes Earle, Kevin Costner, Lisa Marie Presley, Liz Rose, Marieke Schroeder, Miranda Lambert, Norah Guthrie, RCA Studio B, Review, Robert's Western World, The Carter Family, The Carter Family Fold, The Pistol Annies, The Ryman Auditorium, Woody Guthrie\nNashville will always be the home of country music, but Bristol, TN/VA was where the big bang of country music occurred. In 1927, recording pioneer Ralph Peer from the Victor Talking Machine Company set up his equipment in the Taylor-Christian Hat Company in downtown Bristol and started recording acts that would become the very foundation of what we know as country music today.\nAshley Monroe, Birthplace of Country Music Museum, Bristol, Carlene Carter, Dolly Parton, Doyle Lawson, Emmylou Harris, Jim Lauderdale, Jimmie Rodgers, Johnny Cash, Marty Stuart, Orthophonic Joy: The 1927 Bristol Sessions Revisited, Ralph Peer, Ralph Stanley, The Bristol Sessions, The Carter Family, The Church Sisters, The Stoneman Family, The Whistles & The Bells, Vince Gill\nIn the mid 2000’s, Tommy Ramone formed an old-time band with Claudia Tienan of the band The Simplistics called Uncle Monk. They released a self-titled album in March of 2006, and did numerous shows and tours around the country. Tommy played mandolin and some banjo, and in the old-school style, would lean into the microphone for solos instead of playing through a pickup.\nThe bayou cries out in mourning, but the music will live on. Jimmy C. Newman, the ‘C’ standing for “Cajun,” known as one of country music’s most passionate champions of the Cajun influence and nicknamed “The Alligator Man,” passed away on Saturday, June 21st due to Cancer. He was 86-years-old. Jimmy C. Newman, “The Alligator Man”, is now sitting on the banks of the great bayou in the sky.\nBill Monroe, Cajun, dead, Dolly Parton, Fred Rose, Gene Autry, Jimmie Rodgers, Jimmy C. Newman, obituary, passed away, Ricky Skaggs, The Alligator Man, The Carter Family, The Grand Ole Opry, Tom T. Hall\nLike the moan of the steel guitar or the cut of the banjo tone, there is just something about the precision and flow of sister harmonies that awaken something in the human spirit that is uncanny, and characteristic of the highest reaches of audio diversion. Considering the long and noble lineage of close sister harmonies, marked on its path by such noteworthy names…\nRalph Stanley’s Clinch Mountain Boys lost their long-time lead guitarist and Ralph’s right hand man James Alan Shelton Tuesday night (6-3) due to Cancer. He was 53-years-old. James Alan Shelton played lead guitar for Ralph Stanley for 20 years, first joining the Clinch Mountain Boys in 1994. But Shelton he also did so much more.\nThough The Kossoy Sisters were surrounded by the folk revival, much of the inspiration and compositions for their music originated farther south in the Southern Appalachians. Their focus was gospel and primitive country murder ballads. Most importantly, that innocence and purity that the world had scarcely heard since those original Ralph Peer Bristol Sessions was present in their music.\nMid January is the season that most of the big mainstream country music acts unveil their touring plans for the year. Country music critical favorite Kacey Musgraves announced she would not be touring with one of her country music bunk mates, but of all people, the buxom purple-haired pop star Katy Perry. ome Kacey Musgraves’ supporters were disappointed…\nBlake Shelton, Bobby Bones, Brandy Clark, Brantley Gilbert, Florida Georgia Line, Jamey Johnson, Jason Aldean, Kacey Musgraves, Katy Perry, Kenny Chesney, Ralph Peer, The Carter Family, Tour, Tyler Farr\nThe inaugural inductees to the Outlaw Music Hall of Fame set to open in the Spring of 2014 have been unveiled. In an event carried live during a 3-day concert in Altamont, TN, the 17 initial inductees were announced in two different categories: Pioneers/Innovators (Pre-1970), and Highwaymen (1970-1990). Along with the official inductees, the Outlaw Music Hall of Fame also announced Guardian Award winners.\nAnnouncement, Billy Joe Shaver, Bobby Bare, Chris Gantry, Dallas Moore, David Allan Coe, Hank Williams, Hank Williams III, Hank Williams Jr., Inductees, Jamey Johnson, Jessi Colter, Johnny Cash, Johnny Paycheck, Kris Kristofferson, Loretta Lynn, Merle Haggard, Outlaw Country Music Hall of Fame, Outlaw Music Hall of Fame, Sammi Smith, Steve Young, The Carter Family, Waylon Jennings, Wayne Mills, Whitey Morgan, Willie Nelson\nThe general consensus amongst country music pundits in 2013 is that we are in the midst of the ‘Year Of The Woman.’ But this isn’t the first year in country when the women deserved the lion’s share of attention. Rose Maddox of The Maddox Brothers & Rose, Goldie Hill, and the woman who would later rise to be known as the Queen of Country Music, Kitty Wells became pioneers for women in country.\nWhenever I find myself thirsting for inspiration, I tend to search out the blessed gift that is the harmonies of sister pairings. Any two great singers can harmonize, but few can match the instinct and tone of two blood relatives, making the artform seem as effortless as breath. Following is a list of singing sisters who never cease to inspire.\nAnderson Family Bluegrass, Asleep at the Wheel, First Aid Kit, Marty Stuart, Merle Haggard, Paige Anderson, Paige Anderson and the Fearless Kin, Ricky Skaggs, T Bone Burnett, The Carter Family, The Church Sisters, The Louvin Brothers, The Marty Stuart Show, The Quebe Sisters, The Secret Sisters, The Shook Twins, The Staves\nLonesome Wyatt is a pioneer of Gothic country with his band Those Poor Bastards, and one of the originators of underground country whose song “Pills I Took” was covered by Hank Williams III on his landmark album Straight to Hell, he is one of the few artists who will never be forgotten regardless of the long-term fortune of the underground country sub-genre.\n“Country must evolve” is the way it is sold to the country music public when pop and hip-hop influences are invited into the country music fold. What these folks fail to point out is that country has been trying to evolve for 30 some odd years right under their noses. Are you looking for true progress and evolution in country music? Look no further than this list of women.\nAbigail Washburn, Amanda Shires, Anderson Family Bluegrass, Asleep at the Wheel, Be Good Tanya's, Bela Fleck, Brandi Carlile, Caitlin Rose, Dale Watson, First Aid Kit, Hank Williams, Jolie Holland, Kacey Musgraves, Kasey Chambers, Liz Rose, Neko Case, Paige Anderson, Rachel Brooke, Rounder Records, Ruby Jane, Sara Watkins, Sarah Jarosz, The Beach Boys, The Carter Family, The Trishas, Tom Waits, Uncle Earl, Willie Nelson\nBeing an American-flavored country roots band from the UK is a tough enough proposition. Take two girls whose native tongue isn’t even English, and tackling the task of trying to get the North American continent to pay attention to what they’re trowing down seems even more daunting. But that is exactly what the Sorderberg sisters from Sweden have done.\nSomewhere in the last year or so, country music crossed that line from being the last bastion for respect of beautiful women in American popular culture, to hanging out in the gutter with the rest of the vermin, making videos of venereal-infused floozies dry humping flashy vehicles in the classic vein of tasteless, materialistic, shallow-minded rap imagery.\nBucky Covington, Country Girl (Shake It For Me), Dolly Parton, Drinking Side of Country, Dustin Lynch, Loretta Lynn, Luke Bryan, Miranda Lambert, Patsy Cline, She Cranks My Tractor, Shooter Jennings, Tammy Wynette, Taylor Swift, The Carter Family\nWhether it’s folk, bluegrass, country, or Cajun, Foghorn can play a breakdown, a Celtic jig, a Cajun waltz, and cut a rug to an early country tune in the span of as many songs and sell you quickly on the idea that you don’t need amplification or new school modes to make music that is both memorable and entertaining. Outshine the Sun is an excellent album, and where it makes its mark is in the positivity of its message."", 'Country Music Hall of Fame member Charlie Daniels, 83, died on July 6, according to a press release from his publicist.\n“Country music and southern rock legend Charlie Daniels has passed,” read the email statement. “The Country Music Hall of Fame and Grand Ole Opry member died this morning at Summit Medical Center in Hermitage, Tennessee. Doctors determined the cause of death was a hemorrhagic stroke. He was 83.”\nOver the course of his 60-plus-year career, Charlie received numerous accolades, including induction into the Country Music Hall of Fame (2016) and the Musicians Hall of Fame (2009), as well as becoming a member of the Grand Ole Opry (2008).\nAs the fiddle-playing frontman of the Charlie Daniels Band, Charlie scored a number of Top 20 singles, including “Drinkin’ My Baby Goodbye,” “Boogie Woogie Fiddle Country Blues,” “In America,” and “The Devil Went Down to Georgia,” which topped the charts in 1979.\nCharlie leaves behind his wife of 55 years, Hazel, and their son, Charlie Daniels, Jr. Funeral arrangements will be announced in the coming days.\nCharlie’s brief bio, courtesy of the Country Music Association, is below:\nBorn 1936 in Wilmington, NC, steeped in musical traditions ranging from folk and bluegrass to gospel, Country and rock, Daniels was a pioneer in introducing southern rock sounds into mainstream Country Music. In the process, he widened Country’s popularity by bringing millions of young people to a greater appreciation of their Country Music heritage, established musical alliances with a wide variety of artists in Country and other music fields, and helped take Country to deeper levels of American culture. Critical to this achievement was his session work on Bob Dylan albums recorded in Nashville in the 1960s, including Nashville Skyline. Daniels also supported Ringo Starr on Starr’s Beaucoups of Blues. “The Devil Went Down to Georgia” and the Charlie Daniels Band were featured in the landmark film “Urban Cowboy” in 1980, a movie that helped ignite a boom in Country Music’s popularity and widened its audience across the nation.\nAccording to the RIAA, Daniels’ lifetime record sales exceeded 13.5 million units. This put him in the ranks with musical legends like Paul Simon, John Lennon, Natalie Cole, Yes, the Temptations, and Jefferson Airplane. When Daniels was signed for $3 million by Epic Records in New York in 1976, the contract set a record for a Nashville act. Over the course of his career, Daniels had nine Gold, Platinum or multi-Platinum albums. Super Hits went double Platinum, Million Mile Reflection went triple Platinum, and A Decade of Hits went quadruple Platinum. His longform video, “Homefolks and Highways” went Gold while his single, “The Devil Went Down To Georgia” became a Country Music rarity, achieving Platinum certification. It was the CMA Single of the Year in 1979 and earned the Charlie Daniels Band a Grammy for Best Country Vocal Performance by a Duo or Group. It crossed over to become a Top 5 pop smash as well. Daniels was also named CMA Instrumentalist of the Year in 1979. The Charlie Daniels Band won CMA Instrumental Group of the Year Awards in 1979 and 1980, marking a total of four CMA Awards throughout his career. Daniels became a Grand Ole Opry cast member in 2008 and was inducted into the Country Music Hall of Fame in 2016.\nDaniels placed 34 songs on the Billboard Country charts. In addition to “The Devil Went Down to Georgia,” his other Top 10 hits were “Drinkin’ My Baby Goodbye” (1985) and “Boogie Woogie Fiddle Country Blues” (1988). “Uneasy Rider,” one of his Country chart-making singles, was also a Top 10 pop hit in 1973. Besides “The Devil Want Down to Georgia,” which went No.1 Country and No. 3 pop in 1979, other pop successes were his 1975 singles “The South’s Gonna Do It Again” and “Long Haired Country Boy,” both of which became staples of his live shows. He also charted in the pop Top 30 with “In America” (1980) and “Still In Saigon” (1982). His earliest songwriting success came in 1964 when his co-written “It Hurts Me” became a Top 30 pop hit for Elvis Presley.\nPrior to gaining solo stardom, Daniels was a session musician, mostly in Nashville for artists including Marty Robbins, Claude King, Flatt & Scruggs, Pete Seeger, Leonard Cohen, Al Kooper, Ringo Starr and, most famously, Bob Dylan. Daniels can be heard on Dylan’s Nashville Skyline, Self Portrait, and New Morning albums of 1969-1970. In 2014, he released Off the Grid – Doin’ It Dylan, a collection of Dylan songs rendered Daniels style.\nAmong Daniels’s most impressive accomplishments was the 1974 launch of his Volunteer Jam. These annual, multi-artist, multi-genre extravaganzas, sometimes stretching past 10 hours in length, became must-see musical spectacles for thousands. Country legends such as Ray Price, Roy Acuff, Bill Monroe, Alabama, Vince Gill, and Tammy Wynette have shared bills with acts as diverse as Ted Nugent, B.B. King, James Brown, Billy Joel, Eugene Fodor, Little Richard, Steppenwolf and Don Henley. The Volunteer Jam Tour, including The Charlie Daniels Band, The Outlaws, and The Marshall Tucker Band toured the United States in 2007.\nDaniels’ charity work was extensive. Cancer research, muscular dystrophy research, physically and mentally challenged individuals, children, farmers, and the armed forces have benefited from his efforts. His charity Christmas concert benefiting children became a Nashville holiday institution. In recognition of his “unique and indelible influence on generations of music makers,” Daniels was honored as a BMI Icon in 2005.\nphoto by TCD']"	['<urn:uuid:6b3146e0-002a-4955-b75f-5e4cb4a56060>', '<urn:uuid:1a7faa3f-4ced-4518-9d19-76ff8147233d>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-13T04:43:46.574121	13	89	2977
26	I'm interested in improving transportation for both disabled people and the general public - how do the New Freedom program and Health Impact Assessment approaches compare in terms of their goals for making transportation more accessible?	The New Freedom program and Health Impact Assessment (HIA) approaches have complementary but distinct goals for accessibility. The New Freedom program specifically focuses on removing transportation barriers for people with disabilities through community-based solutions like shuttle services, volunteer drivers, and accessible vehicles. Meanwhile, HIAs take a broader public health approach, aiming to ensure that all people regardless of age, income, or ability can move about their community easily and safely by examining how transportation projects affect health outcomes. While New Freedom emphasizes direct services and capital investments for disabled individuals, HIAs look at wider infrastructure and policy changes that promote active transportation, public transit access, and safety for everyone.	"['Transportation Health Impact Assessment Toolkit\nFor Planning and Health Professionals\nReduced drinking and driving and increased seatbelt use—these transportation policies have helped save many lives. Transportation policies can also be about infrastructure—how people get from place to place. These policies can help or block people from healthy lifestyle choices, such as making regular doctor visits, accessing good jobs, and choosing healthy food. This is especially true for underserved residents, children, older adults, and households without automobiles.\nWhen health is considered among the goals of transportation policy and land use planning, the resulting policy can help reduce air pollution; prevent traffic injuries and deaths; and lower obesity, diabetes, cardiovascular disease, and cancer rates. Such outcomes can happen when roads are designed to be pedestrian-, cyclist- and public transit-friendly. Roads that are designed for people as well as for cars and trucks can increase physical activity, enhance community quality of life, and increase access to community services.\nHow can public officials, community members, and planners ensure that future transportation policies consider health? One way is to use a health impact assessment (HIA). Transportation HIAs help policymakers see and address the potential health effects of a proposed transportation project, plan, or policy before it is built or implemented. A transportation HIA can ensure that all people, regardless of age, income, or ability, are able to move about their community easily and safely.\nA community’s transportation planning process can have many stages. For example, a Long-Range Transportation Plan made by states and Metropolitan Planning Organizations (MPOs) sets the vision for transportation over a twenty-year timeline. A Transportation Improvement Program identifies which projects will be funded and constructed over the next four years. As a policy tool, transportation HIAs can help prioritize local and statewide project proposals by identifying the health value. The HIA process can also encourage all stakeholders, including the MPO, project managers, elected officials, public health officials, the residents, and commuters to work together on improving public health.\nThe Transportation HIA Toolkit provides a framework for public health departments, city planners, project managers, and other stakeholders to conduct HIAs on proposed transportation projects, plans, and policies. If your are seeking the Transportation and Health Tool developed by CDC and the U.S. Department of Transportation, go here.\nThe elements of the Transportation HIA Toolkit are:\nGeneral HIA resources\nSectors from education to housing to community design use HIAs to identify opportunities to improve public health. Visit the Centers for Disease Control and Prevention’s (CDC’s) Healthy Places Health Impact Assessment page for an HIA overview and the steps involved in any HIA process. You can also find links to online HIA courses and other resources.\nHIA Background Information and HIA Indicators\nTo conduct an HIA, practitioners will need to research background information on the project area and affected population. The HIA Background Information and HIA Indicators section links to national databases and provides guidance on relevant indicators that assess the health impact of transportation projects. This section also directs practitioners to local sources for data specific to an area.\nStrategies for Health-Oriented Transportation Projects and Policies\nHIAs make evidence-based recommendations to promote positive health outcomes and minimize negative consequences. The Strategies for Health-Oriented Transportation Projects and Policies section identifies transportation design and infrastructure strategies recognized in published HIAs. It also provides resources to inform recommendations. The strategies and evidence are divided into six categories:\n- Reduce Vehicle Miles Traveled (VMT)\n- Expand Public Transportation\n- Promote Active Transportation\n- Incorporate Healthy Community Design Features\n- Improve Safety for All Users\n- Ensure Equitable Access to Transportation Networks\nIn the case studies, you can find existing HIA reports on transportation-related projects and policies, ranging from walking and biking improvements and public transit expansion to VMT legislation.\nOther Transportation HIA Resources:\nTransportation and Health Toolkits\n- Transportation and Health Tool\nThe tool was developed by the U.S. Department of Transportation and the Centers for Disease Control and Prevention to provide easy access to data that practitioners can use to examine the health impacts of transportation systems.\n- Convergence Partnership: Transportation and Health Toolkit\n- American Public Health Association: Transportation and Health Toolkit\nThis site has tools to help you explain to others the connection between transportation and health.\n- UCLA Health Impact Assessment Clearing House\n- Transportation Web page\n- Data sources for HIA: Demographic and health risk data for describing affected populations\n- Partnership for Prevention (US). Transportation and Health: Policy Intervention for Safer, Healthier People and Communities. Washington, DC: Partnership for Prevention; 2011. Available at: http://www.convergencepartnership.org/site/c.fhLOK6PELmF/b.4950415/\nk.4FF7/Transportation_and_Health_Toolkit.htm. Accessed on 21 July 2011.\nThe Partnership for Prevention collaborated with the University of California, Berkeley’s Safe Transportation Research and Education Center, Booz Allen Hamilton, and the Centers for Disease Control and Prevention. The resulting report examines the effects of transportation policies on public health in three key areas—environment and environmental public health; community design and active transportation (human-powered transportation for getting around like biking and walking); and motor vehicle-related injuries and fatalities.\n- Dora C, Phillips M, editors. Transport, environment and health. World Health Organization. WHO Regional Publications, European Series, No. 89. 2000. http://www.euro.who.int/document/e72015.pdf [PDF - 1.24 MB].\nA major purpose of this book is to alert policy analysts, decision-makers and politicians to current knowledge about the health effects of transport and the means to reduce them.\n- Bell J, Cohen L, Malekafzali S. The transportation prescription: bold new ideas for healthy equitable transportation reform in America. 2009. PolicyLink. http://www.convergencepartnership.org/atf/cf/%7B245a9b44-6ded-4abd-a392-ae583809e350%7D/TRANSPORTATIONRX.PDF [PDF - 552 KB].\nThis guide outlines a new vision for American transportation networks by improving mobility from place to place and residents’ access to their communities. The guide includes information on how transportation options and infrastructure can affect on health and what transportation policy changes would improve public health.\n- Douglas M, Thomson H, Jepson R, Hurley F, Higgins M, Muirie J, Gorman D (eds). Health Impact Assessment of Transport Initiatives: A Guide. NHS Scotland Edinburgh, 2007. 110 pages. http://www.healthscotland.com/documents/2124.aspx.\nThis guide to Scotland’s transportation HIAs contains resources for on how to use evidence to support recommendations (chapter 6). It also summarizes findings on evidence that connects transportation policies and projects to health outcomes (appendix 4).Top of Page\n- Page last reviewed: October 19, 2011\n- Page last updated: November 3, 2015\n- Content source:', 'Presentation on theme: ""Mobility for Persons with Disabilities: Examining Service and Planning Innovations of the US New Freedom Transportation Program Piyushimita (Vonu) Thakuriah.""— Presentation transcript:\nMobility for Persons with Disabilities: Examining Service and Planning Innovations of the US New Freedom Transportation Program Piyushimita (Vonu) Thakuriah and Siim Sööt University of Illinois at Chicago Presentation made in TRANSED 2012, New Delhi, India Session on Best practices and innovations : Policies and Legislations Research funded by US Department of Transportations Federal Transit Administration and Community Transportation Association of America\nNew Freedom Transportation Program New Freedom (NF) program: designed to go above and beyond the transportation requirements of the Americans with Disabilities Act; Community-based mobility solutions; Origins of New Freedom Initiative (NFI): A federal initiative established in 2001 by then President George W. Bush; Nationwide effort to remove barriers to community living for people with disabilities. Initiative emphasizes access to assistive technologies, work, education, and other opportunities for people with disabilities; Was followed up by Executive Order 13217, titled Community- Based Alternatives for Individuals with Disabilities (2001). NF Transportation program - instituted as a stand-alone funding program in 2005; consolidated into larger transit program for seniors and persons with disabilities in 2012.\nNew Freedom Transportation Program Types of projects funded: Trip-based NF services: shuttle services, volunteer driver services, rideshare, vanpool, route deviation; Information-based NF services: mobility management, trip or itinerary planning, ridematching, travel training; Capital investment NF services: vehicles, accessible taxis, elevators, large capacity wheelchair lifts added to vehicles, wheelchair securement added to vehicles, accessible paths of travel, improving signage. Total dollar amount: FY2012 $93 million Match requirement for federal funds: 50%\nCoordinated Human Services Transportation Plan NF projects should be derived from a Coordinated Human Services Transportation Plan (CHSTP); A lead agency is required to be designated in order to lead the planning process and is to be decided locally; lead agencies are typically Metropolitan Planning Organizations, Councils of Government, transit agencies, state transportation departments (mostly for rural areas), among others. Stakeholder agencies which have participated in the planning process are termed partner agencies. CHSTP: Locally derived plan that: identifies transportation needs of mobility-disadvantaged individuals, in coordination with organizations involved with their well-being; provides mobility strategies; prioritizes transportation services for funding and implementation.\nObjectives of Paper Examine details of 10 transportation-based services funded by NF program based on primary data collection in 2009: 4 urban, 4 suburban, 2 rural to: Understand the Coordinated Human Services Transportation Planning (CHSTP) process associated with NF-funded services for seniors. Conduct exploratory analysis of outcomes experienced by NF trip-based service clients.\nInstitutional Structure of NF-funded Services Lead Agency Partner Organization Managing or Operating Organization Program Manager Managing or Operating Organization User CHSTP Planning NF-funded Services Survey - 18 items CHSTP Organizations Survey - 22 items Survey - 33 items Survey - 61 items\nFindings: Service Characteristics Type of service: door-through-door assisted van or personal car service; reserved curb-to-curb bus service with point deviation; taxi service; Service provider category: public or non-profit; Service provider function: integrated social services centers; comprehensive senior care centers; transit operations – NF program primarily funded supplemental services to integrated caregiving organizations to persons with disabilities and seniors.\nFindings: Use of Funds at Study Sites Expand volunteer driver screening, training, reimbursement, and secondary liability coverage; Establish new or expand coverage of demand-response service; Taxi service payments or payments to taxi companies; Travel training; Non-emergency medical transportation; Route deviation/curb-to-curb transit service operation.\nFindings: Coordinated Planning Process I Lead organizations conducted extensive outreach to organizations serving seniors; Lead agency perspectives: Appreciated feedback from non-traditional stakeholders and concerned citizens; Some lead organizations felt the process was somewhat cumbersome to set up and could be streamlined in terms of its requirements. Partners involved: organizations in transportation, human and social services, workforce development, labor and economic development, private employers, faith-based organizations and other organizations involved in the well- being of persons with disabilities, seniors and low-wage workers.\nFindings: User Profile Types of Assistive Device Needed Annual Household Income Household Size Employment by Age\nFindings: Coordinated Planning Process II Partner organization perspective (all partner agencies at sampled sites) Some but not all made financial contributions to services; Stated level of participation varied from high to low; Noted consensus among partners in assessment of transportation gaps, strategies to address gaps and prioritization of resources; Senior citizen participants: CHSTP process may be somewhat agency-driven in these locations and might benefit from taking input informally from private citizens; Viewed process as important in achieving regions goals regarding persons with disabilities, seniors and low-income individuals. Program managers did not find the process to be particularly useful in resolving their immediate concerns in finding the 50% required match or the lengthy grant implementation process.\nFindings: User Socio-Demographic Clusters Cluster 1 (Persons with disability in poor health): Persons with disabilities likely to be unemployed, use assistive devices, rate their health as poor, have low levels of functional independence and live alone; Cluster 2 (Oldest old): Oldest old more likely in better health, have higher levels of functional independence and less assistive device usage compared to the first and the third group; Cluster 3 (Young workers with disability): Younger more likely employed but with a greater share of impairments and need for audio and visual assistance. Results of statistical clustering: (N=271)\nFindings regarding Mobility Independence of Users VariableDescription Perceived Ability Perceived Ability to travel independently Functional Ability Composite of 6 questions measuring functional ability to travel: (1) Can drive car; (2) Can access and board public transport stations and stops, pay fares, understand schedules, have no difficulty finding a place to sit, with station crime and with bringing a service animal; (3) No perceived difficult in using public transportation overall; (4) Able to shop independently; (5) Do not need assistance from another person outside home; (6) Able to sometimes or always go outside the house, ie, not home-bound Composite Instrumental Activities of Daily Living (IADL) Composite of following questions: ""How often can you do each of the following without help: (1) Use the telephone, (2) Prepare a meal, (3) Housekeeping tasks, (4) Laundry, (5) Manage medications, (6) Manage finances (7) Shop""\nFindings regarding Mobility Independence Disability Cluster VariableScale 1 Disabled persons in Poor Health 2 Oldest Old 3 Younger workers with Disability Perceived Ability (1-Never; 5=Always) 2.92.5 Functional Ability (0 – No ability in any of the 6 functional measures; 6 – Ability in all 6 functional measures) 184.108.40.206 Composite IADL (0=Least Independent; 35=Most Independent) 12.010.618.2\nComponents or Dimensions of Senior Mobility Needs Variable Loads: (1)no drivers license, (2)difficulties in using existing public transportation, (3)receipt of public assistance, (4)no vehicles in the household. Variable Loads: (1)living alone, (2)difficulties in traveling independently, (3)perceived health, (4)frequent medical trips Two components explaining variation in senior mobility needs: Principal Component Analysis Transportation Deprivation Component (TDC) Construct giving extent to which there are hard constraints to travel due to lack of appropriate transportation availability Independence and Health Deprivation Component (IHDC) Extent to which individual suffers from social isolation, health-related mobility issues and dependence on others to fulfill their travel needs\nImplications for Senior Mobility Services (1)Requires reliable transportation to a variety of destinations – shopping, social etc. (2)Needs may be served with traditional van programs, ridesharing and Fixed-Route services with deviations (1)Requires frequent and reliable medical trips (2)Care and support in arranging the trip (3)Assistance while undertaking the trip (4)Needs may be served with volunteer driver programs, other assisted door-through- door services, travel planning, non- emergency medical transportation TDC IHDC\nOutcomes 95% of users reported that the service was very important to them; Users reported being able to access work, healthcare, social visits and shopping destinations that were previously difficult to access; Improved trip reliability; Time savings; Improved assistance and information; Great deal of site-to-site variability. Some did not consider the services to greatly improve their mobility outcomes.\nConclusions NF Transportation program started as a small program – now integrated into larger program Enhanced Mobility of Seniors and Individuals with Disabilities - FY2013 $254.8 million; CHSTP – required - but needs to be streamlined particularly for non-traditional stakeholders; NF program has supported transportation options as supplemental services to integrated caregiving by providing new funding for such services and thereby enabling a community-based \'continuum of care; Much work ahead to matching services to client needs – and to understanding client needs.']"	['<urn:uuid:9cff9f7f-95a4-4a5e-ab17-8f027e592534>', '<urn:uuid:466b500b-feec-45c2-84d9-34536310cb62>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	36	109	2418
27	Being involved in public health policy, I'm looking at different taxation measures and their impact on various diseases. Could you detail how sugar tax affected the incidence of different health conditions like stroke, heart disease, and cancer?	The sugar tax showed significant reductions across multiple diseases: for stroke, it reduced incidence by 18.2% in men and 14.7% in women; for coronary heart disease, there was a 15.8% reduction in men and 13.6% in women; for colorectal cancer, there was a 5.9% reduction in men and 1.5% in women; and for osteoarthritis, it showed a 16.3% reduction in men and 12.8% in women.	['The study was conducted by University of Otago researchers and modelled the effects of a hypothesised 20% fruit and vegetable subsidy as well as taxes on saturated fat, salt, sugar and junk food (defined as non-essential, energy dense foods) on a New Zealand population over 30 years based on New Zealand National Nutrition Survey data.\nThe sugar taxation was modelled based on tax for all foods and beverages with sugar, not sugar-sweetened beverages alone.\nTwo of the main outcomes investigated were: The effects of these on public health disease incidences for 17 diet-related diseases (coronary heart disease, stroke, osteoarthritis, diabetes, and 13 types of cancers), and the potential public health gains (calculated based on the number of Health-Adjusted Life Years (HALYs) gained).\nSugar taxes emerged as the most powerful intervention in terms of reducing the incidences of these diseases when compared to salt, saturated fat, or junk food taxes, or the 20% fruit and vegetable subsidy.\n“The implementation of a sugar tax led to higher outcomes than any of the other taxes and subsidies investigated when it came to reducing incidences of diet-related diseases such as diabetes (-32.7% in men, -26.7% in women), stroke (-18.2% in men, -14.7% in women), coronary heart disease (-15.8% in men, -13.6% in women), colorectal cancer (-5.9% in men, -1.5% in women) and osteoarthritis (16.3% in men, 12.8% in women),” said the study authors.\nThis was particularly obvious for diabetes, where it beat out the closest contender saturated fat tax (-20.1% for men, -16.4% for women) by over 10% on average; as well as osteoarthritis where it beat salt tax (-10.2% for men, -7.7% for women) by over 5%.\n“All the interventions we modelled led to notable reductions in most chronic disease rates – the largest being [the 32.7% reduction for diabetes in men by a sugar tax],” said lead author Professor Tony Blakely.\nSugar tax was also the top contributor to HALYs gained, hence overall public health gains, based on the modelling.\n“[Sugar tax would] add a total of 581 HALYs per 1,000 New Zealand consumers, [higher than] 375 HALYs from the salt tax, 361 HALYs from the saturated fat tax, 212 HALYs from the fruit and vegetable subsidy or 127 HALYs from the junk food tax,” Prof Blakely added.\nThe New Zealand Dental Association (NZDA) lauded the study’s findings, highlighting that the implementation of a sugar tax would also very likely show benefits to improving population oral health.\n“A tax on sugar shows the greatest overall health benefits in this study, and if implemented, we think oral health could be one of the biggest improvements,” said NZDA sugary drinks spokesperson Dr Rob Beaglehole in a formal statement.\n“[Sugar] consumption must drop if overall oral health is to improve in New Zealand.”\nOverall though, the researchers voiced support for a ‘combination scheme’ as opposed to implementing a single tax as the effects could potentially be even more maximised.\n“There are large potential health gains from such a scheme, [as we saw public health benefits] from the other measures too. For example, the fruit and vegetable subsidy saw a significant disease incidence reduction for stroke (-16·3% for men, -11·3% for women),” said Prof Blakely.\nAs for the junk food tax, which was found to add the least HALYs, the study still urged for this to be implemented in tandem as it had demonstrated effects in reducing the purchases of such unhealthy foods.\n“Mexico’s introduction of its 8% tax on junk food in 2014 reduced purchasing of these foods by 6%, [demonstrating that] these measures are both feasible and effective,” said study co-author Dr Cristina Cleghorn.\nThe study also looked into the effects of combining the individual nutrient taxes with the fruit and vegetable subsidy, and found all of these combinations to have more powerful effects than the individual taxes.\nCalculating public health gains here as added HALYs for the overall population of study, the base for comparison was estimated at 173 million HALYs for the population if no interventions whatsoever were implemented.\n“With just sugar tax implemented, this increased by 1.48% to 2.56 million HALYs, but with both sugar tax and a 20% fruit and vegetable subsidy, this increased by 1.91% to 3.31 million – a total gain of 0.43% or 0.75 million HALYs in total,” stated the study.\nSimilar results were seen for the salt tax-fruit and vegetable subsidy and saturated fat tax-fruit and vegetable subsidy combinations, as both increased HALYs by 0.46% (0.8 million).\nEasier said than done\nThat said, implementing any such schemes, combination or otherwise would still need to go through multiple hurdles, as the New Zealand government has said time and again that food taxes, and especially sugar taxes, are not on the table.\nThe food and beverage industry is very clearly against any such tax being implemented, with health groups such as NZDA on the other end.\nProf Blakely agrees that there are ‘multiple other considerations’ when making decisions on food taxes and subsidies, but stressed that these findings show a clear benefit to these.\n“[The government obviously also has] multiple other considerations such as political and social acceptability, food industry perspectives, deadweight costs, etc. [but] the health gains from food taxes and subsidies require serious policy consideration and public discussion,” he said.\nThe researchers added that these decisions ‘should not be made based only on simulation modelling studies’, but also emphasise the wider benefits to be begotten.\n“[Taxes] could also prompt food industry product reformulation to avoid tax, further enhancing health gains over and above the direct effects that we modelled,” they said.\nStudy: The effect of food taxes and subsidies on population health and health costs: a modelling study\nSource: The Lancet\nAuthors: Blakely, T. et.al.']	['<urn:uuid:764d1418-036e-4c3b-97d0-44ec6edfc1db>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	37	65	949
28	How do digital animation tools enable character movement and expression?	Digital animation tools enable character movement and expression through various techniques. One approach is using skeletal animations in tools like Spine, where you create a skeleton, dress it with a skin, and apply keyframes to create animations quickly. This allows for efficient creation of character states and movements while consuming less memory than traditional sprite sheets. Additionally, 3D rigging provides a digital framework with strategically placed joints and controls that enable realistic movements and facial expressions. The process involves creating a skeleton, binding the mesh, setting up controls, and defining weight painting. These techniques allow animators to have full control over character movements and expressions, ultimately enhancing their believability and emotional impact.	"['2D Animation Tutorial Using Spine for Mobile Apps\nIn the classic book by Disney animators Frank Thomas and Ollie Johnson, “The Illusion of Life”, the art of character animation is described as crafting,\n“drawings that appear to think and make decisions and act of their own volition.”\nAnimation, especially of living things, seems like a major hurdle for the mobile app developer who needs to create people, insects, birds, and lifelike plants to bring their mobile games to life. Many turn to sprite sheets, using software such as Texture Packer to turn a series of drawings into a ‘moving picture’ much like the Disney animators did, frame by frame with each sprite moving slightly to give an illusion of a moving figure when played in sequence, like an old-fashioned flip book.\nSprite sheets, however, present some problems for mobile apps if they are at all complicated. First, they are difficult to edit and maintain, as to change any element of the sprite sheet means precisely replacing all elements of a sprite sheet. One mistake and the animation looks jerky and “unreal”.\nAdditionally, sprite sheets consume a certain amount of texture memory in your app. First, you will need a copy of each element of your animation embedded into your sprite sheet, there might be dozens of images packed into a .png file. Second, you might want to content-scale that sprite sheet so that it looks good on retina screens, so you will need a @2x version of it, using even more memory. For fast-moving games with many complex animations, sprite sheets can quickly become a maintenance and memory problem.\nThere must be a better way!\nEnter Spine, the easy-to-use and well-designed software by Esoteric Software. Spine was created as a Kickstarter project and has quickly become a tool of choice for many developers of mobile games. As one Corona developer noted,\n“…bought it after trying it for something like 2 seconds.. This is the coolest thing that happened for Corona based games since Corona!”\nInstead of creating frame-by-frame animations, Spine works by creating skeletal animations. In short, you create a skeleton and “dress it” into a skin, apply keyframes, and create an animation in a short amount of time. It’s an intuitive process once you understand the basics of the software. In this tutorial, I’ll show how to create a simple two-part animation: a bee that has two states, fluttering, and pausing.\nTo begin, you need at least the trial version of Spine. Download it at esotericsoftware.com/spine-download. Note, you won’t be able to export any files to use in your mobile apps until you buy a version of the software, either the Essential version at $60 or the Professional at $249. Most of the functionality an Indie developer needs is included in Essential, so I recommend buying it if, after the trial version shows you Spine’s capabilities, you think you could use it in your projects.\nYou will need some art to begin your animation. If you create your own, make sure to draw each animatable element on its own layer in Pixelmator, Fireworks, or Photoshop. Heads, necks, arms, torso, and legs all need to be on their own individual layer. I visited VectorStock.com and bought an image of a bee. The download includes an Illustrator file that allows you to isolate the four elements of the bee I want to animate: head, torso, and two wings. I saved those elements as four separate .png files. It’s a good idea to place the elements of your animation into the folder that will be its final home in your app so that you don’t have to change paths to images later on.\nBefore you start, consider adding a placeholder of the basic image to Spine so that you get your bone placement right. You can import it with the image fragments and delete it at the end of the animation process. You might prefer to set its opacity low or colorize it to remind yourself to delete it later.\nI started by creating a new ‘skeleton’ (Cmd+n / Ctrl+n) in Spine and drawing the four elements of the animation – head, body, and two wings. I named each ‘bone’ of the skeleton appropriately. To do this, make sure you are in the ‘Setup’ area of Spine (the ‘Setup’ text toggles if you click it, between the Setup and Animate states). Click ‘create’ in the toolbox and draw the bones onto the grid. If you get stuck, make sure your screen looks like the one below.\nOnce you’re satisfied with the way the bones look, you need to import the images that will ’skin’ the bones. If you tell Spine what folder to browse for, it will import the images you need for your animation. I named these images as well.\nDrag the skin (images) from the right panel onto the bones. To make sure the elements are skinned in the right order, you can select one of the elements in the panel and click ‘+’ or ‘-‘ to move it above or below in the order of slots.\nNext, connect the bones to the skin by selecting one of the elements of the skin you just imported and clicking ‘set parent’ in the right panel. Once a skin element is given a bone parent, you can arrange a pose by dragging the bone.\nNow that the skins are arranged along with the bones, you are ready to animate! As a demo, I have created a ‘lazy bee’ game interface. This is a sample Corona SDK app where the bee rests on a honeycomb. Feed the bee ‘pollen’ by clicking on it and click on the honeycomb for the bee to fly to its new space. If you don’t feed the bee, it remains in ‘rest’ mode on the honeycomb, moving only slightly.\nClick the left-hand ‘setup’ text to toggle to the animate screen. Here is where we start experiencing Flash déjà vu, as we will start assigning poses to keyframes. For ‘lazy bee’, I need to create two animations, one a ‘rest’ state with a pulsing wing movement, and a ‘flying’ state with a looping wing flutter and body wiggle effect.\nTo do this, click on ‘Animations’ in the right hand tree and generate a new animation. Click keyframe 0 and select it as “Loop start” and 15 as “Loop end”. Then click each keyframe successively and move the bones (with the skin attached) very slightly to make the wings and body move. For the ‘fly’ animation, the wings move more dramatically and more quickly, a setting controlled via the ‘playback’ button. For the ‘rest’ animation, the wings move only a little and over only eight keyframes.\nFor each keyframe, rotate, translate (moved) or scale a particular bone, and then click the ‘key’ button next to the property to save the keyframe. Unchanged keyframe buttons start out as green, edited keyframes orange, and when changed and saved the button turns red. A good tip is to enable “Auto Key” so that Spine remembers your bone keyframes automatically.\nOnce you have saved all the motions, to playback the animation as a loop, click the ‘repeat’ button and then the ‘play’ button. This will show you if you have misplaced bones or anything that isn’t moving naturally. Make changes if necessary, keyframe by keyframe, making sure to save each edit via the keyframe button. View more fine-grained detail of the animation by clicking ‘dope sheet’. Change the speed of playback by editing it in the ‘playback’ area, accessible from the ‘playback’ button.\nSatisfied with your animation? Now it’s time to export it. For this, you need a paid license. There are several options to export, and even a built-in texturepacking tool so that you can pack all of your images into one .png and generate an atlas, if you prefer to build sprites.\nFor my purposes, I saved the animations as a .json file. You can save it in ‘pretty print’ format to make it easier to read and debug.\nTo use your new animation in your mobile app, you need one of the runtimes built by Esoteric Software. I imported the Corona SDK runtime into my app.\nThe completed project is available on github. Once the .json file and images are accessible to your main codebase, you can add particle effects and buttons to can attach your animations to a given event (The below code is lua).\n--include the Spine library local spine = require ""spine-corona.spine” include the json file generated by Spine local json = spine.SkeletonJson.new() local skeletonData = json:readSkeletonDataFile(""animations/bee.json"") --draw the skeleton using the data local skeleton = spine.Skeleton.new(skeletonData) --dress the skeleton with the image files function skeleton:createImage (attachment) return display.newImageRect(""animations/images/"" .. [attachment.name](http://attachment.name) .. "".png"",100,100) end --place the animation on the screen in the right position skeleton.group.x = display.contentWidth/2 skeleton.group.y = display.contentHeight/2 skeleton.flipX = false skeleton.flipY = false --get ready to make it move local stateData = spine.AnimationStateData.new(skeletonData) local state = spine.AnimationState.new(stateData) --set it to its ‘rest’ state state:setAnimationByName(0, ""rest"", true, 0)\nYou can see the final product here:\nIn this tutorial, I’ve outlined the bare minimum you can do with Spine. I highly recommend it for use in your games and mobile apps when you need to liven up your interface with an animation. With this tool, complex animations become manageable for even the smallest Indie development shops.', 'Hello there, dear readers! Have you ever watched an animated movie or played a video game and wondered how the characters move and come to life? This is where 3D rigging comes in. In this article, we will delve into the world of 3D rigging and its importance in animation and character design. Get ready to be amazed and learn something new!\nWhat is 3D Rigging?\n3D rigging is the process of constructing a digital framework for a 3D character or object, allowing it to be realistically and naturally animated. This involves strategically placing joints and controls on the model, which animators can manipulate to create fluid motion. Rigging is a crucial aspect of character design and animation, as it allows for realistic movements, facial expressions, and intricate interactions. Without proper rigging, characters would lack fluidity and appear rigid and lifeless. In essence, 3D rigging is the technique used to bring 3D models to life and give them movement in animation and character design.\nWhy is 3D Rigging Important in Animation and Character Design?\n3D rigging plays a vital role in animation and character design, providing essential flexibility and realism to the characters. It allows animators to have full control over the movement and expressions of the characters, ultimately enhancing their believability and emotional impact. Rigging enables intricate movements such as facial expressions, body deformations, and even complex interactions between characters and objects. Without rigging, animators would be required to manually manipulate every aspect of the character, which is not only time-consuming but also limits the creative possibilities. In summary, 3D rigging is crucial in animation and character design as it brings characters to life, making them dynamic and captivating.\nWhat Are the Benefits of 3D Rigging?\nThe advantages of implementing 3D rigging in animation and character design are numerous.\n- Enhanced movement: 3D rigging allows for realistic and fluid movements of characters, bringing them to life.\n- Time-saving: Once a character is rigged, animators can easily pose and animate them, saving time and effort.\n- Flexibility: Rigging provides the flexibility to modify and adjust the character’s movements and expressions as needed.\n- Consistency: Rigging ensures consistent proportions and movements throughout the animation.\n- Improved storytelling: Rigging helps convey emotions and expressions, enhancing the viewer’s connection to the character.\nThese benefits make 3D rigging an essential tool in creating compelling and engaging animations.\nIt’s like giving a skeleton to a virtual puppet, except this puppet can do more than just dance – it can bring characters to life with realistic movements and expressions.\nHow Does 3D Rigging Work?\n- Create a 3D model: Design and create a 3D character model using specialized software.\n- Joint placement: Determine the placement of joints on the model, which will allow for movement and articulation.\n- Skinning: Attach the model’s vertices to the corresponding joints, ensuring that the character moves realistically.\n- Control setup: Create a control rig that allows animators to manipulate the character’s movements.\n- Weight painting: Adjust the weights of the character’s vertices to ensure smooth movement and deformation.\n- Testing and refining: Test the rig by posing and animating the character, making adjustments as needed for better functionality.\nWhat Software is Used for 3D Rigging?\nThere are a variety of software programs available for 3D rigging in animation and character design. These programs offer the necessary tools and features to create rigging setups for 3D models. Some of the most popular options for 3D rigging include:\n- Autodesk Maya\n- 3ds Max\n- Cinema 4D\nEach software has its own unique set of features and capabilities, giving animators and riggers the ability to create intricate and realistic rigs for characters. These programs also provide a range of tools for creating joint systems, setting up controls, defining constraints, and managing deformations, resulting in smooth and lifelike movements for animated characters.\nWhat Are the Steps Involved in 3D Rigging?\nThe process of 3D rigging involves several steps to prepare a 3D model for animation. Here are the steps involved in 3D rigging:\n- Create a skeleton: Build a digital skeleton inside the 3D model, positioning joints and bones.\n- Bind the mesh: Attach the 3D model’s mesh to the skeleton, ensuring it moves with the bones.\n- Set up controls: Add control objects to manipulate the rig, providing animators with an intuitive interface.\n- Define weight painting: Assign weights to the mesh vertices, determining how much they are influenced by specific bones.\n- Create constraints: Apply constraints to restrict the movement of certain parts, such as IK (inverse kinematics) or FK (forward kinematics).\n- Add facial controls: Develop a facial rig using blend shapes or bone-based systems to animate facial expressions.\n- Test and refine: Continuously test the rig’s functionality, making adjustments and refining until it meets the desired requirements.\nBy following these steps, 3D rigging enables animators to bring characters to life with realistic movements and expressions.\nWhat Are the Different Types of 3D Rigging?\nIn the world of 3D animation and character design, rigging plays a crucial role in bringing digital models to life. But did you know that there are different types of rigging techniques that serve different purposes? In this section, we will take a closer look at the various types of 3D rigging, including skeletal rigging, facial rigging, and cloth rigging. Each type has its own unique features and uses, making them essential tools for creating dynamic and believable characters in the world of animation. So let’s dive in and explore the world of 3D rigging.\nSkeletal rigging is an essential aspect of 3D animation and character design. It involves creating a digital skeleton and attaching it to a character model, allowing for realistic movement and animation. By defining joints and establishing how they connect and move, skeletal rigging enables animators to easily manipulate characters. This type of rigging is commonly used for humanoid or animal characters. Skilled riggers utilize software such as Autodesk Maya or Blender to create a hierarchy of bones, assign weights to control the model’s deformation, and implement constraints to limit movement. Skeletal rigging plays a crucial role in bringing characters to life in animation and elevating the overall quality of the final product.\nFacial rigging is a crucial aspect of 3D animation and character design, as it brings life to facial expressions and movements. This process involves several steps:\n- Create a facial skeleton: Develop a digital framework that mimics the structure and movement of a human face.\n- Set up controls: Establish a system of controls, such as sliders or shape keys, to manipulate different areas of the face.\n- Define blendshapes: Sculpt a range of facial expressions by creating various blendshapes, which are different shapes of the face that can be morphed together.\n- Connect controls to blendshapes: Link the controls to the blendshapes, allowing animators to easily manipulate facial expressions.\n- Add secondary movements: Enhance realism by incorporating secondary movements like blinking, eye saccades, and subtle facial muscle contractions.\nFacial rigging is essential as it enables animators to effectively create characters that can emote and communicate, adding depth and believability to the animation.\nCloth rigging may sound like a fashion design technique, but it’s actually essential in giving animated characters those realistic flowing capes and billowing skirts.\nCloth rigging is a crucial aspect of 3D animation and character design that involves creating lifelike movement and behavior for cloth simulations. Here are the steps involved in cloth rigging:\n- Create a mesh: Begin by modeling the cloth object with the desired shape.\n- Add joints: Define the areas where the cloth will be attached to the character’s body using joint influences.\n- Set up constraints: Apply constraints to ensure the cloth interacts correctly with the character, such as collision and gravity.\n- Paint weights: Assign weight values to the cloth vertices to determine how much influence each joint has on the cloth movement.\n- Adjust parameters: Fine-tune the cloth simulation settings, including stiffness, stretchiness, and friction.\n- Test and refine: Run simulations to evaluate how the cloth behaves and make necessary adjustments to achieve the desired results.\nBy mastering cloth rigging techniques, animators can bring characters to life with realistic clothing and enhance the overall quality of the animation.\nWhat Skills are Required for 3D Rigging?\nIn the world of animation and character design, 3D rigging is a crucial aspect that brings life and movement to digital characters. But what does it take to become a skilled 3D rigger? In this section, we will discuss the essential skills and knowledge required for 3D rigging. From understanding anatomy and movement to mastering animation principles and 3D software, we’ll explore the key elements that make a successful 3D rigger. So, let’s dive into the world of 3D rigging and discover the necessary skills for this intricate art form.\nKnowledge of Anatomy and Movement\nHaving a strong understanding of anatomy and movement is crucial for 3D rigging in animation and character design. It allows riggers to comprehend the structure and mechanics of the human body, enabling them to create realistic movements for characters. Understanding how muscles, joints, and bones work together is key in achieving natural and lifelike animations. Additionally, being knowledgeable in movement principles such as weight, timing, and anticipation is essential for creating dynamic and believable character performances.\nRigging experts with a solid foundation in anatomy and movement can ensure that the characters they rig move in a realistic and expressive manner, elevating the overall quality of the animation.\nWithout understanding animation principles, your 3D rigging skills will just be a bunch of bones and wires.\nUnderstanding of Animation Principles\nHaving a thorough understanding of animation principles is vital for successful 3D rigging in character design. Animators must possess a strong grasp of concepts like timing, spacing, and weight in order to create movements that are believable and visually appealing. This knowledge enables them to ensure that the rig behaves realistically and adheres to the laws of physics.\nWith a solid understanding of animation principles, riggers can also anticipate how the character’s movements will impact other elements, such as facial expressions or cloth simulations. Moreover, this understanding allows riggers to collaborate more effectively with animators, resulting in a smoother workflow and higher-quality animations.\nExperience with 3D Software\nHaving experience with 3D software is an essential skill for 3D rigging. This requires proficiency in programs such as Maya, 3ds Max, or Blender. Rigging artists must possess a strong knowledge of the software’s rigging tools and features to construct intricate and lifelike character rigs. They should also be able to navigate the software interface, manipulate objects, create rigging controls, and establish constraints and deformations. Moreover, familiarity with 3D software allows riggers to troubleshoot and resolve any technical difficulties that may arise during the rigging process. Ultimately, a thorough understanding of 3D software empowers riggers to produce high-quality character animations.\nHow is 3D Rigging Used in Character Design?\nIn the world of character design and animation, 3D rigging plays a crucial role in bringing digital creations to life. This section will delve into the various ways in which 3D rigging is utilized in character design, from creating realistic movements and expressions to enhancing the character’s personality and improving the overall quality of the animation. By understanding the importance of 3D rigging in character design, we can gain a deeper appreciation for the intricate process behind creating dynamic and engaging animated characters.\nCreating Realistic Movements and Expressions\nCreating realistic movements and expressions in 3D rigging requires careful attention to detail and a thorough understanding of human anatomy and movement. Here are the steps involved in achieving this:\n- Study Reference Material: Analyze reference videos or images of real-life movements and expressions to capture the nuances.\n- Build a Skeleton: Create a digital skeleton for the character, ensuring proper joint placement and hierarchy.\n- Set Up Control Rig: Set up a control rig with controllers to manipulate the character’s joints and deformations.\n- Weight Painting: Assign weights to each joint to control the deformation of the character’s mesh.\n- Add Constraints: Apply constraints to limit and control the movement of certain body parts, such as IK (Inverse Kinematics) for limbs.\n- Create Facial Rig: Build a facial rig with controls for facial expressions, including blend shapes or bone-based systems.\n- Add Secondary Motion: Incorporate secondary motion to enhance realism, such as adding squash and stretch or cloth simulations.\n- Refine and Test: Continuously refine the rig and test it by animating the character to ensure realistic movements and expressions.\nEnhancing the Character’s Personality\nUsing 3D rigging to enhance a character’s personality is crucial in creating captivating animations. It adds depth and realism to their movements and expressions, making them come alive. Here are the necessary steps to achieve this:\n- Create a skeletal rig that accurately captures the character’s unique anatomy and movements.\n- Refine the rig to ensure smooth and natural animation, paying attention to joint placement and control.\n- Add facial rigging to enable expressive facial movements and emotions.\n- Incorporate cloth rigging to simulate realistic fabric movement and draping.\n- Implement secondary animation techniques, such as hair and prop rigging, to further enhance the character’s appeal.\nBy following these steps, 3D rigging can greatly enhance a character’s personality and bring them to life in the animation.\nWith 3D rigging, characters can go from stiff and lifeless to fluid and dynamic, making animation a whole lot more entertaining to watch.\nImproving the Overall Quality of the Animation\nEnhancing the overall quality of the animation is a crucial aspect of 3D rigging. By incorporating precise rigging techniques and paying attention to detail, animators can create more realistic and visually appealing animations. Here are some steps that can help improve the quality of the animation:\n- Utilizing proper skeletal rigging to ensure accurate and natural movement of the character.\n- Implementing facial rigging to create realistic facial expressions and emotions.\n- Utilizing cloth rigging to simulate realistic clothing movements.\n- Incorporating secondary motion to add depth and realism to the animation.\n- Utilizing proper inverse kinematics and control systems for smooth and controlled movements.\n- Adding appropriate weight and physics simulations to objects and characters for a more lifelike feel.\n- Applying efficient lighting and shading techniques to enhance the visual appeal of the animation.\n- Including post-production techniques like compositing and color grading to improve the overall look and feel.\nBy following these steps, animators can significantly improve the overall quality of their 3D animations and create captivating visuals for their audience.\nFrequently Asked Questions\nWhat is 3D Rigging For Animation & Character Design?\n3D rigging is the process of creating a digital skeleton or system of bones and controls for a character or object in a 3D animation. It is an essential step in the character design process, as it allows animators to manipulate and move the character in a realistic and fluid manner.\nWhy is 3D Rigging important in animation and character design?\n3D rigging is crucial in animation and character design because it gives the character movement and brings it to life. Without rigging, a character would not be able to move or emote realistically, making it difficult to tell a compelling story through animation.\nWhat skills are needed to become a 3D rigger?\nTo become a 3D rigger, one needs a strong understanding of 3D software such as Maya or Blender, as well as a knowledge of anatomy and movement. Knowledge of coding and scripting is also helpful in creating more advanced rigging setups.\nCan 3D rigging be used for more than just character animation?\nYes, 3D rigging can also be used in other forms of animation, such as rigging props, vehicles, or even environments. It can also be used in video games, simulations, and other forms of digital media.\nIs 3D rigging a time-consuming process?\nThe time it takes to rig a character or object varies depending on its complexity and the skill level of the rigger. It can take anywhere from a few hours to several days to create a complex rig, but the end result is worth the effort.\nWhat is the difference between 3D rigging and 3D animation?\n3D rigging is the process of creating a digital skeleton for a character or object, while 3D animation is the process of adding movement and bringing the character to life. Rigging is a crucial step in the animation process, but it is not the same as animation itself.']"	['<urn:uuid:909b353f-eef6-41a1-9af4-258a45100220>', '<urn:uuid:6b706758-88e0-4dff-8c4b-bad17bee2f99>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T04:43:46.574121	10	112	4298
29	In my practice treating concussion patients, I've noticed varying recovery patterns. What are the key individual risk factors that influence long-term concussion outcomes, and what are the recommended cognitive activity modifications during the recovery period?	Several factors contribute to individual variability in concussion recovery, including the severity of the initial injury, the person's age, pre-existing medical conditions, and the presence of multiple concussions. These factors can affect both recovery time and susceptibility to adverse outcomes. Regarding cognitive activity modifications, research shows that patients should significantly reduce activities that stimulate the brain, particularly in the early phases of recovery. This includes limiting exposure to television, video games, and cell phone use due to their constant light changes, noise, and high cognitive demands. For students, this means implementing more frequent rest breaks during homework, avoiding electronic stimulation during these breaks, and instead resting in low-light or dark rooms. While complete cognitive rest isn't necessary, carefully managed reduction in cognitive activity is associated with faster recovery times.	['Important Information for Concussion Recovery\nConcussions have become a very common event in sports participation and have been frequently reported in the news media over the past couple of years. This increase in press makes it sound like a relatively new “buzz” in medicine. Unfortunately, concussions have occurred for many years prior to this new focus on them. This is an area of medicine that has made great strides over the past 10 years focusing on the proper recognition, realization on the level of severity and potential future effects and the best treatment options.\nPhysicians, physical therapists, athletic trainers and coaches now receive better education in the recognition and treatment of concussions. It is important that people utilize medical professionals in handling concussions and listen to their advice. We see a number of youth, adolescent and adult recreational and competitive athletes as well as non-athletes in our clinic that unfortunately have sustained a concussion. The goal of this blog is to educate you as the concussion recipient or as a care giver of someone who has sustained a concussion with some information that may lessen the symptoms of a concussion and expedite the brain’s healing process.\nMany of the grade school through high school students that we treat after a concussion have had to modify their school attendance and receive homework and / or tutoring at home because the hypersensitivity to light and noise and intolerance of sustained activities that require concentration. These patients must try to minimize excessive stimulation of the brain to allow it to heal. Activities such as watching television and playing video games are not recommended but are often occurring when children are at home more during this recovery time. We all understand the video game craze that has come upon our society and the desire to play them while these patients are stuck at home but the amount of constant light changes, noise and high level cognitive requirements are too stimulating to the healing brain and must be minimized. The same is true with cell phone use including texting, tweeting, etc.\nA new study in the January 2014 issue of Pediatrics showed that participants who did not restrict cognitive activity soon after concussion had a significantly longer recovery time than those who greatly reduced their cognitive activity especially in the earlier phases of recovery. The study results reported, “This seems to suggest that while limiting cognitive activity is associated with shorter duration of symptoms, complete abstinence from cognitive activity may be unnecessary”. To a parent of a student post-concussion, trying to keep up with homework will require more rest breaks than usual to complete their work. These breaks should not include television watching, video game playing or a lot of cell phone usage and you will want to encourage resting in a low light or dark room instead. Parents should expect some resistance from their children when minimizing these activities but must understand it is necessary. This will take a somewhat creative but committed effort for parents to enforce and encourage but hopefully it will be short term and the research indicates it is necessary if you want the fastest recovery and return to previous activities.\nFor more suggestions or treatment post-concussion, please call us at 724-779-1300.', 'Concussions, often referred to as mild traumatic brain injuries, have gained increased attention in recent years due to their potential long-term effects on individuals’ physical, cognitive, and emotional well-being. While concussions are commonly associated with sports-related injuries, they can occur in various settings, including accidents, falls, or even everyday activities. In this article, Dr. John Manzella will explore the importance of understanding the long-term effects of concussions, the potential implications for individuals’ health, and the significance of proactive management and prevention strategies.\n1: Cognitive Consequences and Neurological Health\nConcussions can have significant cognitive consequences that affect memory, attention, concentration, and problem-solving abilities. Individuals may experience difficulties with information processing, learning, and retaining new information. In some cases, concussions have been linked to an increased risk of developing neurodegenerative conditions such as dementia, Parkinson’s disease, or chronic traumatic encephalopathy (CTE). Understanding the long-term cognitive effects of concussions is crucial for early detection, intervention, and the development of targeted rehabilitation strategies.\n2: Emotional and Mental Health Challenges\nConcussions can also impact individuals’ emotional and mental well-being. Some individuals may experience mood changes, including increased irritability, anxiety, or depression. Post-concussion syndrome, characterized by persistent symptoms such as headaches, fatigue, sleep disturbances, and mood alterations, can significantly affect an individual’s quality of life. Identifying and addressing the emotional and mental health challenges associated with concussions is essential for providing comprehensive care and support to affected individuals.\n3: Risk Factors and Individual Variability\nIt is important to recognize that the long-term effects of concussions can vary significantly among individuals. Several factors contribute to this variability, including the severity of the initial injury, the individual’s age, pre-existing medical conditions, and the presence of multiple concussions. Researchers are actively studying these risk factors to better understand the long-term implications and identify individuals who may be more susceptible to adverse outcomes. By recognizing these risk factors, healthcare professionals can provide tailored care, monitor individuals closely, and implement preventive measures to reduce the risk of further injury.\n4: Proactive Management and Prevention Strategies\nManaging concussions requires a comprehensive and multidisciplinary approach. Medical professionals, including neurologists, neuropsychologists, and physical therapists, play a vital role in assessing and treating concussions, as well as providing guidance on symptom management, cognitive rehabilitation, and gradual return-to-activity protocols. Education and awareness programs are crucial in promoting safe practices, recognizing the signs and symptoms of concussions, and implementing preventive measures. This includes proper equipment use, adherence to sports safety protocols, and promoting a culture of reporting and seeking medical attention following head injuries.\n5: Continued Research and Collaboration\nTo better understand the long-term effects of concussions and develop effective management strategies, ongoing research and collaboration among medical professionals, researchers, and policymakers are essential. Longitudinal studies tracking individuals over extended periods can provide valuable insights into the trajectory of concussion-related outcomes. Additionally, advancements in neuroimaging techniques, biomarker research, and innovative treatment modalities hold promise in enhancing our understanding and management of concussions.\nUnderstanding the long-term effects of concussions is crucial for providing appropriate care, support, and prevention strategies. By recognizing the cognitive, emotional, and neurological consequences of concussions, healthcare professionals can develop comprehensive management plans that address individual needs and promote optimal recovery. Proactive management, early intervention, and ongoing research efforts are essential in mitigating the long-term effects and ensuring the well-being of individuals who have experienced concussions.']	['<urn:uuid:5c6e2a18-b8c0-41b9-887f-ec97785448d0>', '<urn:uuid:90bf6a0c-056a-4796-b91c-fd9ee3ffdbe0>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	35	129	1093
30	compare religious structures erdene zuu monastery angkor wat temple main features	Erdene Zuu monastery is encircled by a wall of 108 white stupas and contains three main temples similar to Chinese/Korean style plus a smaller Tibetan-style temple, while Angkor Wat temple has five central towers arranged in a quincunx pattern, with the central tower being the highest, and features extensive galleries with bas-reliefs depicting Hindu scenes.	"[""The Orkhon Valley encompasses 121.967 hectares, and can be found in Central-Mongolia. It also includes Kharkhorum, the 13th and 14th century capital of Chinggis (Genghis) Khan's vast Empire. The inclusion of this site is a tribute to Mongolia's nomadic culture, that even now persists.\nFurther, it illustrates several significant stages in human history. First and foremost it was the centre of the Mongolian Empire; secondly it reflects a particular Mongolian variation of Turkish power; thirdly, the Tuvkhun hermitage monastery was the setting for the development of a Mongolian form of Buddhism; and fourthly, Khar Balgas, reflects the Uighur urban culture in the capital of the Uighur Empire.\nMap of Orkhon ValleyLoad map\nVisit September 2002\nKarakorum nowadays is a small town in the center of Mongolia. There's even a road there from Ulan Bator, one of the few in Mongolia. The town hosts the country's major tourist attraction: Erdene Zuu monastery. So we weren't the only visitors here. The monastery buildings are encircled by a wall of 108 white stupa's: just great!\nThe next day, after staying the night in a comfortable ger-camp, we drove on to Tövkin Khid. This is a smaller monastery in the mountains, a few hours from Karakorum. The drive to get there is completely off-road, crossing a river and driving along the paths in the grasslands. This day was undoubtedly the best day of my stay in Mongolia: I wished it would never end. The landscape around here is greener and more hilly than in the rest of Central Mongolia, and so pure. We also visited a nomadic family on the move, dismantling their ger with help from their neighbours.\nIn the Orkhon-area you can see quite a number of gers from nomadic families, maybe because of the proximity of a 'big city'. This traditional way of living is still quite common in the rural areas, though I wonder for how long. The grasslands are getting drier and drier by the year.\nVisit in March 2022\nThis was the first Site that I visited during my hastily-arranged trip to Mongolia, just after the country had reopened after being closed for the previous two years. Though it is not ideal, it is possible to make a rewarding visit to this Site during the final, at least according to the calendar, week of winter. However, that particular time of the year did significantly affect the type of visit that I was able to make. While I normally would have preferred to travel to the Site and back by bicycling—there is now a reasonable-quality paved road for the entire distance from Ulaanbaatar to Kharkorin—the limited amount of time I had available and, more importantly, the still-cold weather ruled out that option. I also considered traveling on my own to Kharkorin by bus and then finding someone locally to take me around to the interesting locations. However, while I think that would be possible in the summertime, and more so in a “normal” year, I was not confident that I would be able to make those arrangements under the circumstances, especially given that some tourism facilities and services had not completely restarted at the time. Therefore, I needed to book a package tour from a guiding company based in Ulaanbaatar. The trip would require four days, one to travel most of the way to the Site, two for visiting various points of interest, and one to return to UB, with three nights accommodation in traditional Gers included.\nI found all of the guides in Mongolia to be very willing to adjust their standard itineraries to suit the interests of their clients, though I cannot say if that amenability was enhanced by a desire to attract customers after the two-year hiatus. In any case, the first place we visited was a lengthy section of the Orkhon River Valley within the Core Zone, specifically, the portion that extends for a fairly long distance to the southwest from Kharkorin, and which is not really where you would think you should go if you searched for “Orkhon Valley” on Google Maps. In the past there were cultural relics to be seen in that area, such as burial mounds and Deerstones, but there was some disagreement among area guides as to whether they were still present, or had all been taken away to museums, so that day was primarily for natural experiences. Travel up the valley was along dirt roads, of variable quality, and I was most impressed that, like almost all of Mongolia, the Valley was completely free of fences. That allowed large herds of herbivores to roam around freely, as it should always be, and these included the standard varieties, sheep, cows, Cashmere goats, but also Yaks, and, of course, large numbers of Mongolian Horses, while outside the core zone small herds of semi-domestic Bactrian Camels and wild Mongolian Gazelle were also seen. That section of the visit concluded with a stop at the 20-meter-tall Orkhon Waterfall, which actually belongs to a smaller tributary of the Orkhon, and which was still completely frozen solid in late March.\nThe one place I specifically requested to visit was Khar Balgas. Like many people, I have always primarily associated the Orkhon Valley Site with Karakorum, the capital city of the Mongols, initiated by Chinggis Kahn and completed by his sons. However, when briefly researching this visit, I was interested to learn that the Valley was also very important for other societies that predated the Mongols. One of those was the Uighur Khanate, which resided in the Valley five hundred years before the founding of Karakorum, and had as its capital, Khar Balgas. The city ruins are seldom visited today, in fact, my guide had not been there for fourteen years, and our driver not for three. Nevertheless, while the site is not exactly well-advertised, it is not particularly difficult to reach, and could be done so independently by those not adverse to that sort of thing. To do that, one first leaves Kharkorin on the main road that heads to the northwest, traveling for about twenty kilometers before turning right, northward, onto an unmarked dirt road at 47.326583,102.6719175, and then continuing generally north for another fifteen kilometers across a broad, flat section of steppe. Like all dirt roads in Mongolia, this one exists only where local people have been driving vehicles over the grasslands recently, however, with a GPS and a decent satellite map, it would not be very difficult to stay on track. Especially since over the last few kilometers the Khar Balgas ruins are the only feature that can be seen looming ahead.\nIn particular, the only remains that are easily visible today are the rectangular walls of the old Palace complex, formerly as much as twelve meters high, a large Stupa contained within, the smaller remains of the adjacent Palace, and several smaller Stupas outside the walls, along the north and south sides, all of which have been heavily eroded. Pictured here is the view from atop the remains of the large Stupa, looking towards the northwest corner of the outer walls. These ruins are located at 47.431314,102.658962, and, while it does require a considerable amount of imagination to perceive rest of the city in person, as only low outlines of former building foundations remain today, by viewing the Satellite View at maximum magnification the entire city plan is clearly visible, which in its heyday extended over an area of thirty-two square kilometers, to the west of the Palace. Though there also is not much remaining of the main complex, some interesting features can still be seen. One of those is the construction method used to build the walls and Stupas, which I think is fairly unique, whereby tall structures were built up by successively laying down 15-cm thick layers of an aggregate of mud and small river stones (presumably from the Orkhon River.) This gives the appearance (and I really hate making this comparison,) that all the structures were created using a giant 3-D printer. Inside the walls, around where the Palace formerly was, numerous fragments of what appear to have been curved roofing tiles can be found, each having one face displaying an embossed pattern from some type of woven fabric that was presumably used in their production. Apart from two information display panels outside the walls, there are no other facilities at the ruins site, no entrance fee, and visitors can wander around as long as they like.\nThe Erdene Zuu Monastery, adjacent to the Karakorum city archeological site, and possibly built on top of the ruins of the Khan's palace, is still the primary place of interest when visiting this Site, however. I was a bit taken aback by the half-empty feeling of the area within the Monastery walls, which results from the fact that most of its buildings were destroyed in the 1930s by the Communist government of the time. Today the three main temples, similar in appearance to those in China or Korea, are maintained as exhibits, while the smaller Tibetan-style temple is still in active use.\nBefore returning back to UB on our final day, there was enough time to visit the Karakorum Museum. Having been established in 2011, the museum, while not especially large, now contains a very nice collection of artifacts from the Orkhon Valley tracing back to Karakorum and its predecessors, including a few pieces of decorative architectural stonework from Khar Balgas. However, it is a fairly small set of items that were uncovered from the Karakorum town site by a Mongolian-German archeological dig in 2008 which really provides a meaningful insight into that period of time, since there is very little of Karakorum that can be seen out in the environment today.\nAlso at the museum was an excellent special exhibit on the Shoroon Bumbagar Tomb. Located about 120 km northeast of Kharkorin, this tomb was only discovered in 2011 by Mongolian and Kazak archeologists, and is notable for never having been looted and for the exceptional murals that adorned its underground walls. The tomb, dated to the 7th century CE, was built for a nomadic nobleman from the Turkic society, and the Museum displays several artifacts, of the type that would have been valued by a nomad, such as small personal items, jewelry, and several Byzantine coins. Also displayed are sixty, from a total of ninety, miniature human figurines found in an annex of the tomb, each with a distinct face, which are slightly reminiscent of the terra-cotta warriors of Qin’s Mausoleum in Xi’an. There is a short, though fascinating, video which describes the murals. UNESCO is already working with the Mongolian authorities to find ways to preserve the murals, so it seems likely that this tomb will eventually find its way onto Mongolia’s Tentative List, either as a new Site, or, as I think would be best, as an extension to the Orkhon Valley.\nIf that tomb does get added, it will probably always be rather difficult to visit. However, that would fit in with the nature of the Orkhon Valley Site, in general, which is a very worthwhile Site, but one which does require a little extra effort to appreciate fully.\nRead more from Michael Ayers here.\nAs an inveterate “listophile” I maintain a large number of travel-related lists. “Ancient Capital Cities of the World” is one – so the chance to “pick up” that of Ghengis Khan at Kharkhorin (“Karakorum”) in Mongolia was not to be missed! In fact, if you visit Mongolia, you are not likely to miss it. Mongolia has many merits as a destination (particularly its wonderful open spaces and the chance to see/experience a nomadic lifestyle) but historical remains are not a major feature. The nature of the country and its people’s lifestyle meant that Buddhist monasteries were the main permanent buildings and the Stalinist purges of the 1930s destroyed many of these. So, almost any tour of the country is likely to take in the main restored monasteries in Ulaan Baatar and Erdene Zuu.\nMongolia has chosen to badge this WHS inscription as “Orkhon Valley Cultural Landscape” rather than specifically as “Karakorum” or the main monastery of the area at Erdene Zuu. A good choice I think. By including all the old sites AND the pastureland, still spotted by gers (yurts) and the herds of horses, yak and camels of nomad families, it ties together past and present very nicely and emphasises the continuity of lifestyle involved. The landscape of the valley and the nearby holy mountains with their shamanistic heritage provides a microcosm of Mongolian culture and is well worth spending a couple of days to see. We were a “group” of just 2 persons with a Mongolian guide/interpreter and the chance to visit/stay with families in their yurts and exchange experiences and questions on a near 1 to 1 basis was one of the highlights of the trip. Ger life is an interesting mixture of old and new – some families now have their own little wind powered generators for their TV batteries!\nThere is in fact very little of Karakorum left – mainly 2 of the 4 “Turtle Rocks” which marked the boundary of the ancient city. Locals have decked them with rather fetching collars of Buddhist silk! Far more impressive is the monastery, inside an enormous wall of over 100 linked stupas. The monasteries have been at least partially restored and function again as a centre of “Yellow Hat” Lamaism. There are likely to be ceremonies in progress both inside and outside the monasteries when you visit (photo)\nErdene Zuu is a remarkable place. I was lucky enough to be there almost on my own, and I found the atmosphere of remoteness and history almost tangible. Apart form the slightly tacky, but almost inevitable gift shop and trinket sellers it is possible to get a feeling for the amazing ancient society that was based around here. The town nearby is nothign to write home about, but was once home to Chinggis Khan and his warroir hordes. That they survived in such a remote place is one thing; that they ruled such a huge part of the world from such a place is something else entirely.\nNeeds proposal for a wider cultural landscape\nThe site has 1 locations\nThe site has 22 connections\nReligion and Belief\n48 Community Members have visited."", ""They are not visible, for example, from the main entrance. / N.N. Only the king and the high priest were allowed on the upper or third level, of Angkor Wat. A narrow covered gallery. Over the three and a half-decade period that it took to finally finish Angkor Wat architecture work, he employed over 300,000 Khmer laborers and 6,000 elephants. In conclusion, Angkor Wat, Angkor Thom and several other Khmer temples are undoubtedly the relics of the past Khmer Civilization. Angkor Thom South Gate (12th-early 13th centuries) The gateway to the Angkor Thom capital city. The Temple Mountain architectural style shows a representation of Mount Meru, the abode of the gods in Hinduism. The Architecture of Angkor Wat. Leave seeing the bas-reliefs for later and continue towards the summit passing through. Angkor Wat is located roughly five miles north of the modern Cambodian city of Siem Reap, which has a population of more than 200,000 people. The height of Angkor Wat from the ground to the top of the central tower is surprisingly, high-213 meters (699 feet). Each one becomes progressively smaller and higher starting from the outer, limits of the temple. Another theory on the western orientation of Angkor Wat is that it, was intended to be situated on an important road in a north to south direction and. Angkor Wat also had a Gopura. These rectangular buildings usually occur in pairs outside the sacred, enclosure. Architecture Of Angkor Wat & More Facts Built during the reign of King Suryavarman II (Regency 1113 until about 1150) The construction of Angkor Wat took 37 years to complete. Estimates on how long it took to build Angkor Wat vary widely but the methods of, construction, quantity of the materials, and the evolution of the decoration suggest. From a distance Angkor Wat appears to. There were thousands of temples built during that time, and quite a few remain till this date but not all are in the best condition. It is supported by weighty columns and guarded by proud-looking lions, on pedestals. one in the middle-which are the most prominent architectural feature of Angkor Wat. It is generally accepted that it was built during the lifetime of the, king to serve as his tomb after death. The sandstone causeway was not a part of the original temple complex, archaeological studies have revealed. Photo via Wikimedia by Kounosu It is a characteristic of South Indian architecture and the fact that this structure was present at Angkor Wat shows that Angkor Empire’s architecture was influenced by the Indian architecture. • The Galleries: The wide galleries feature entrances known as “elephant gates”. Our free-to-use images are all in high resolution and easy to download. Wat is a model of balance and proportion and is a fine example of classical Khmer. Angkor Wat occupies a rectangular area of about 500 acres defined by a laterite wall. The corners of the upper level are dominated by the four, towers. The most celebrated bas-relief in the temple is undoubtedly the scene that depicts the churning of the sea, which was performed by 88 devas and 92 asuras using Vasuki, a serpent. Read More: When Was Angkor Wat Built Who Built Angkor Wat? The name derived from the, many Buddhist images the gallery once contained that were acquired after the temple. The moat surrounding Angkor Wat forms a rectangle of 1,500 meters long and 1,300 meters wide and covers an … Angkor Wat is a unique combination of the Temple Mountain, the standard design for the empire’s state temples, and the later plan of concentric galleries. . Angkor Wat by Asian History: SE Asia November 16th, 2013 There are thousands of wonders to be seen across this earth. In Angkor Wat Architecture, you can see the finest Cambodian stone carvings and large-scaled bas-reliefs. This majestic facade of Angkor, Wat is a model of balance and proportion and is a fine example of classical Khmer, Visitors can easily miss the beauty of Angkor Wat at this point as they rush on to, see the more renowned sight of the five towers-visible only beyond the first entry, tower. However, different features within both structures, architecturally and symbolically, distinguish and provide insight into the individual cultures. Angkor Wat, situated at 13°24′45″N 103°52′0″E, is a one of a kind mix of the sanctuary mountain (the standard structure for the domain’s state sanctuaries) and the later arrangement of concentric exhibitions temple. Angkor Wat is located in a mountainous area of Cambodia. Reflection Building. Born as a Hindu temple, at the end of the 12th century it was converted into a Buddhist temple. Built during the reign of King Suryavarman II (Regency 1113 until about 1150) The construction of Angkor Wat took 37 years to complete. To read the stories behind other celebrated architecture projects, visit our AD Classics section. Angkor Wat by Asian History: SE Asia November 16th, 2013 There are thousands of wonders to be seen across this earth. Prelude to the monument and easy to download finest Cambodian stone carving work be... Gods and seduced ) the gateway to the monument the Egyptian pyramids combined, and these stone. Four, angkor wat architecture more Enjoyable Angkor Wat displays the best of two styles! An imposing platform known as “ elephant gates ” and take in modern... Original, form is seen on the third and uppermost level supports five towers-one in each which! Group of the five domes, companions of the country similarities within architecture and symbolism, both being based. By the vanished Khmer empire for 200 meters across it and serving as the main entrance, so-called libraries stand. Ministry of Interior officials, will be retested on December 7-8 the scheme culminates in decorated tympanums, several angkor wat architecture! 83 94 45 visit Angkor Wat and Borodubur hold several similarities within architecture symbolism... Celebrated architecture projects, visit our AD Classics section than 30 years to build the temple area.. And aisles is a lake the outer limits of two Khmer styles, the abode of the monument original form!, for example, from the 12 th century the west is a combination of temple! Of Roluos Group of the structures and courtyards are in the shape of Vishnu! Than the Egyptian pyramids combined, and these massive stone monuments that the. Five domes, companions of the cathedral of Notre Dame, in Paris the left-hand corner thump. Civilization ’ s architecture has only been recently discovered in the angkor wat architecture fifteenth or sixteenth century become. Four, towers and more Enjoyable Angkor Wat, one of the gods in.. Of the structures and courtyards on different levels with stairways giving access to the 'Gallery of bas-reliefs.. Passing through and courtyards on different levels with stairways giving access to the top of each stairway wondrous. A hallmark of Angkor Wat architecture, you can see the more renowned sight of architectural... Complexes in Angkor Wat temple is seen on the third level balance and proportion is... Towards the summit passing through than 30 years to build the temple Group. Make it one, of Honor. elements repeated throughout the monument heart the! Ruled from 1113 to at least 1145 Angkor Thom and several other Khmer temples undoubtedly. Facing west image of the central towers was repeated at the end of the site and its.... Khmer temples are undoubtedly the relics of the 12th century it was into... Archaeological studies have revealed across this earth, Mahabharat and Ramayana 's attraction! Prelude to the 'Gallery of bas-reliefs ' which has an average weight of 1.5 tons leave seeing the bas-reliefs later! A Gopura is a major tourist attraction of the five towers-visible only beyond the entry... Is part of the structures and courtyards are in the world ’ greatest... Three walled, nearly square palisades that enclose the temple Mountain architectural style shows a representation of Meru! Stairs alerts one to the increasing height of the most famous Hindu,. On galleries, chambers, porches chest, and listen carefully at its finest in Angkor! Free-To-Use images are all in high resolution and easy to angkor wat architecture is seen on left... Is an imposing platform known as the 'Terrace, of Honor. it was built facing west boundaries of city! Towards the summit passing through s walls feature bas-reliefs that portray scenes from the west is a temple... Notre Dame, in Paris divided into a cross-shaped, area defined covered... Covered galleries and four paved courts reveal the full impact of the corners the! In decorated tympanums, several lines stand out in the world ’ walls... Ii did not spare any expense to see his idea of a cross sides by double rows columns... Overall profile of each stairway, windows and balustrades surrounds the third and uppermost level supports five in... Hindu one might wonder why it has become a symbol of unity that appears, galleries..., enclosure monuments that constitute the Khmer Civilization at the entrance of temples by History... 200 hectares the Egyptian pyramids combined, and reliefs make it one, of the towers... Whats apps +855 12 83 94 45 surprisingly, high-213 meters ( 699 feet ) courtyards are in late. Fine example of classical dancing, by the vanished Khmer empire example of the 12th it... Were acquired after the temple Mountain architectural style shows a representation of Meru... Base and surrounded by an airy, spacious courtyard with two small libraries [... Allowed on the upper level monuments that constitute the Khmer empire meters and. A temple, is Hindu one might wonder why it has become reality. But they may have served as a replacement for a wooden bridge or dimension part! At right angles along the ridge of the site from the 12 th century, stand the... Made from 6-10 million blocks of sandstone, each of the past Khmer Civilization ’ greatest. Cambodia Adventure Guide - Private Day Tour - Private Day Tour view of five. Lions, on the left is the country 's prime attraction for visitors large-scaled bas-reliefs rhythm,! Highest of the, many Buddhist images the gallery once contained that were after. 'S prime attraction for visitors intricate temple complex that ’ s greatest legacy be witnessed its.: When was Angkor Wat, Angkor Wat and Borodubur hold several within! Renowned sight of the central Sanctuary rises on a high base and surrounded by an airy, spacious with! Buddhist images the gallery style, thump your chest, and these massive stone structure one! Ascetics makes the space is divided into a Buddhist gallery, area defined with covered galleries with columns the!, Mahabharat and Ramayana gods and seduced has only been recently discovered in the.! In the late fifteenth or sixteenth century the end of the monument constructed! Architectural and engineering feat from the outer limits of two Khmer styles, the temple Interior,... Finest monuments in the shape of a Vishnu temple become a symbol Cambodia... One, of Angkor Wat displays angkor wat architecture best of two prominent appears, on galleries chambers! May have served as a store rooms for, offerings and sacred objects is at the entrance temples. Most famous of the, many Buddhist images the gallery style smaller replica of the 12th century it was during. Of an extraordinary ancient culture is revealed scheme culminates in decorated tympanums, elements..., area defined with covered galleries of, the footprints of an extraordinary ancient culture is.! Rooms for, offerings and sacred objects th century first evidence of the past Khmer Civilization gallery, stand the!""]"	['<urn:uuid:7dc3f20a-119b-4b58-a607-8e350806a67c>', '<urn:uuid:841e34fe-0a4a-488b-96de-a904e95ccb16>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T04:43:46.574121	11	55	4287
31	How does neural cost change over time during decision tasks?	The cost starts with a brief constant period and then increases afterwards, as observed in both monkeys and humans performing the reaction time task.	['Drugowitsch, J., Moreno-Bote, R., Churchland, A. K., Shadlen, M. N., and Pouget, A.\nThe Journal of Neuroscience, 32:3612–3628, 2012\nDOI, Google Scholar\nDecision making often involves the accumulation of information over time, but acquiring information typically comes at a cost. Little is known about the cost incurred by animals and humans for acquiring additional information from sensory variables due, for instance, to attentional efforts. Through a novel integration of diffusion models and dynamic programming, we were able to estimate the cost of making additional observations per unit of time from two monkeys and six humans in a reaction time (RT) random-dot motion discrimination task. Surprisingly, we find that the cost is neither zero nor constant over time, but for the animals and humans features a brief period in which it is constant but increases thereafter. In addition, we show that our theory accurately matches the observed reaction time distributions for each stimulus condition, the time-dependent choice accuracy both conditional on stimulus strength and independent of it, and choice accuracy and mean reaction times as a function of stimulus strength. The theory also correctly predicts that urgency signals in the brain should be independent of the difficulty, or stimulus strength, at each trial.\nThe authors show equivalence between a probabilistic and a diffusion model of perceptual decision making and consequently explain experimentally observed behaviour in the random dot motion task in terms of varying bounds in the diffusion model which correspond to varying costs in the probabilistic model. Here, I discuss their model in detail and outline its limits. My main worry with the presented model is that it may be too powerful to have real explanatory power. Impatient readers may want to skip to the conclusion below.\nThe presented model is tailored to the two-alternative, forced choice random dot motion task. The fundamental assumption for the model is that at each point in discrete time, or equivalently, for each successive time period in continuous time the perceptual process of the decision maker produces an independent sample of evidence whose mean, mu*dt, reflects the strength (coherence) and direction (only through sign of evidence) of random dot motion while its variance, sigma2, reflects the passage of time (sigma2 = dt, the time period between observations). This definition of input to the decision model as independent samples of motion strength in either one of two (unspecified) directions restricts the model to two decision alternatives. Consequently, the presented model does not apply to more alternatives, or dependent samples.\nThe model of noisy, momentary evidence corresponds to a Wiener process with drift which is exactly what standard (drift) diffusion models of perceptual decision making are where drift is equal to mu and diffusion is equal to sigma2. You could wonder why sigma2 is exactly equal to dt and not larger, or smaller, but this is controlled by setting the mean evidence mu to an appropriate level by allowing it to scale: mu = k*c, where k is an arbitrary scaling constant which is fit to data and c is the random dot coherence in the current trial. Therefore, by controlling k you essentially control the signal to noise ratio in the model of the experiment and you would get equivalent results, if you changed sigma2 while fixing mu = c. The difference between the two cases is purely conceptual: In the former case you assume that the neuronal population in MT signals, on average, a scaled motion strength where the scaling may be different for different subjects, but signal variance is the same over subjects while in the latter case you assume that the MT signal, on average, corresponds to motion strength directly, but MT signal variance varies across subjects. Personally, I prefer the latter.\nThe decision circuit in the author’s model takes the samples of momentary evidence as described above and computes a posterior belief over the two considered alternatives (motion directions). This posterior belief depends on the posterior probability distribution over mean motion strengths mu which is computed from the samples of momentary evidence taking a prior distribution over motion strengths into account. An important assumption in the computation of the posterior is that the decision maker (or decision circuit) has a perfect model of how the samples of momentary evidence are generated (a Gaussian with mean mu*dt and variance dt). If, for example, the decision maker would assume a slightly different variance, that would also explain differences in mean accuracy and decision times. The assumption of the perfect model, however, allows the authors to assert that the experimentally observed fraction of correct choices at a time t is equal to the internal belief of the decision maker (subject) that the chosen alternative is the correct one. This is important, because only with an estimate of this internal belief the authors can later infer the time-varying waiting costs for the subject (see below).\nAnyway, under the given model the authors show that for a Gaussian prior you obtain a Gaussian posterior over motion strength mu (Eq. 4) and for a discrete prior you obtain a corresponding discrete posterior (Eq. 7). Importantly, the parameters of the posteriors can be formulated as functions of the current state x(t) of the sample-generating diffusion process and elapsed time t. Consequently, also the posterior belief over decision alternatives can be formulated as a one-to-one, i.e., invertible function of the diffusion state (and time t). By this connection, the authors have shown that, under an appropriate transformation, decisions based on the posterior belief are equivalent to decisions based on the (accumulated) diffusion state x(t) set in relation to elapsed time t.\nIn summary, the probabilistic perceptual decision model of the authors simply estimates the motion strength from the samples and then decides whether the estimate is positive or negative. Furthermore, this procedure is equivalent to accumulating the samples and deciding whether the accumulated state is very positive or very negative (as determined by hitting a bound). The described diffusion model has been used before to fit accuracies and mean reaction times of subjects, but apparently it was never quite good in fitting the full reaction time distribution (note that it lacks the extensions of the drift diffusion models suggested by Ratcliff, see, e.g., ). So here the authors extend the diffusion model by adding time-varying bounds which can be interpreted in the probabilistic model as a time-varying cost of waiting for more samples.\nTime-varying bounds and costs\nIntuitively, introducing a time-varying bound in a diffusion model introduces great flexibility in shaping the response accuracy and timing at any given time point. However, I currently do not have a good idea of just how flexible the model becomes. For example, if in discrete time changing the bound at each time step could independently modify the accuracy and reaction time distribution at this time step, the bound alone could explain the data. I don’t believe that this extreme case is true, but I would like to know how close you would come. In any case, it appears to be sensible to restrict how much the bound can vary to prevent overfitting of the data, or indeed to prevent making the other model parameters obsolete. In the present paper, the authors control the shape of the bound by using a function made of cosine basis functions. Although this restricts the bound to be a smooth function of time, it still allows considerable flexibility. The authors use two more approaches to control the flexibility of the bound. One is to constrain the bound to be the same for all coherences, meaning that it cannot be used to explain differences between coherences (experimental conditions). The other is to use Bayesian methods for fitting the data. On the one hand, this controls the bound by choosing particular priors. They do this by only considering parameter values in a restricted range, but I do not know how wide or narrow this range is in practice. On the other hand, the Bayesian approach leads to posterior distributions over parameters which means that subsequent analyses can take the uncertainty over parameters into account (see, e.g., the indicated uncertainty over the inferred bound in Fig. 5A). Although I remain with some last doubts about whether the bound was too flexible, I believe that this is not a big issue here.\nIt is, however, a different question whether the time-varying bound is a good explanation for the observed behaviour in contrast, e.g., to the extensions of the diffusion model introduced by Ratcliff (mostly trial-by-trial parameter variability). There, one might refer to the second, decision-related part of the presented model which considers the rewards and costs associated with decisions. In the Bayesian decision model presented in the paper the subject decides at each time step whether to select alternative 1, or alternative 2, or wait for more evidence in the next time step. This mechanism was already mentioned in . Choosing an alternative will either lead to a reward (correct answer) or punishment (error), but waiting is also associated with a cost which may change throughout the trial. Deciding for the optimal course of action which maximises reward per unit time then is an average-reward reinforcement learning problem which the authors solve using dynamic programming. For a particular setting of reward, punishment and waiting costs this can be translated into an equivalent time-varying bound. More importantly, the procedure can be reversed such that the time-varying cost can be inferred from a bound that had been fitted to data. Apart from the bound, however, the estimate of the cost also depends on the reward/punishment setting and on an estimate of choice accuracy at each time step. Note that the latter differs considerably from the overall accuracy which is usually used to fit diffusion models and requires more data, especially when the error rate is low.\nThe Bayesian decision model, therefore, allows to translate the time-varying bound to a time-varying cost which then provides an explanation of the particular shape of the reaction time distribution (and accuracy) in terms of the intrinsic motivation (negative cost) of the subject to wait for more evidence. Notice that this intrinsic motivation is really just a value describing how much somebody (dis-)likes to wait and it cannot be interpreted in terms of trying to be better in the task anymore, because all these components have been taken care of by other parts of the decision model. So what does it mean when a subject likes to wait for new evidence just for the sake of it (cf. dip in cost at beginning of trial in human data in Fig. 8)? I don’t know.\nCollapsing bounds as found from behavioural data in this paper have been associated with an urgency signal in neural data which drives firing rates of all decision neurons towards a bound at the end of a trial irrespective of the input / evidence. This has been interpreted as a response of the subjects to the approaching deadline (end of trial) that they do not want to miss. The explanation in terms of a waiting cost which rises towards the end of a trial suggests that subjects just have a built-in desire to make (potentially arbitrary) choices before a deadline. To me, this is rather unintuitive. If you’re not punished for making a wrong choice (blue lines in Figs. 7 and 8, but note that there was a small time-punishment in the human experiment) shouldn’t it be always beneficial to make a choice before the deadline, because you trade uncertain reward against certain no reward? This would already be able to explain the urgency signal without consideration of a waiting cost. So why do we see one anyway? It may just all depend on the particular setting of reward and punishment for correct choices and errors, respectively. The authors present different inferred waiting costs with varying amounts of punishment and argue that the results are qualitatively equal, but the three different values of punishment they present hardly exhaust the range of values that could be assumed. Also, they did not vary the amount of reward given for correct choices, but it is likely that only the difference between reward and punishment determines the behaviour of the model such that it doesn’t matter whether you change reward or punishment to explore model predictions.\nThe main contribution of the paper is to show that accuracy and reaction time distribution can be explained by a time-varying bound in a simple diffusion model in which the drift scales linearly with stimulus intensity (coherence in random dot motion). I tried to point out that this result may not be surprising depending on how much flexibility a time-varying bound adds to the model. Additionally, the authors present a connection between diffusion and Bayesian models of perceptual decision making which allows them to reinterpret the time-varying bounds in terms of the subjective cost of waiting for more evidence to arrive. The authors argue that this cost increases towards the end of a trial, but for two reasons I’m not entirely convinced: 1) Conceptually, it is worth considering the origin of a possible waiting cost. It could correspond to the energetic cost of keeping the inference machinery running and the attention on the task, but there is no reason why this should increase towards a deadline. 2) I’m not convinced by the presented results that the inferred increase of cost towards a deadline is qualitatively independent of the reward/punishment setting. A greater range of punishments should have been tested. Note that you cannot infer the rewards for decisions and the time-varying waiting cost at the same time from the behavioural data. So this issue cannot be settled without some new experiments which measure rewards or costs more directly. Finally, I miss an overview of fitted parameter values in the paper. For example, I would be interested in the inferred lapse trial probabilities p1. The authors go through great lengths to estimate the posterior distributions over diffusion model parameters and I wonder why they don’t share the results with us (at least mean and variance for a start).\nIn conclusion, the authors follow a trend to explain behaviour in terms of Bayesian ideal observer models extended by flexible cost functions and apply this idea to perceptual decision making via a detour through a diffusion model. Although I appreciate the sound work presented in the paper, I’m worried that the time-varying bound/cost is too flexible and acts as a kind of ‘get out of jail free’ card which blocks the view to other, potentially additional mechanisms underlying the observed behaviour.\n Bogacz, R.; Brown, E.; Moehlis, J.; Holmes, P. & Cohen, J. D. The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks. Psychol Rev, 2006, 113, 700-765\n Dayan, P. & Daw, N. D. Decision theory, reinforcement learning, and the brain. Cogn Affect Behav Neurosci, 2008, 8, 429-453']	['<urn:uuid:a052b925-361d-4738-a59e-8a7bb6cc2eb0>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	10	24	2479
32	What role do personal collections play in self-expression at home, and how can the pressure to maintain them impact physical and emotional health?	Personal collections, particularly displayed on bookshelves, serve as a form of self-expression by revealing one's interests, beliefs, and personality through carefully chosen books and decorative items. The way items are arranged, whether books are organized by color, size, or topic, and the addition of accessories like plants and personal mementos, all tell a story about the owner. However, the pressure to maintain these personal displays can contribute to chronic stress, leading to physical symptoms like muscle tension, increased heart rate, and gastrointestinal problems, as well as emotional effects including irritability, lack of motivation, and concentration difficulties. If left unchecked, this stress can develop into more serious health conditions such as heart disease and high blood pressure, while also affecting behavior through social withdrawal and mood changes.	"['© Luisa Brimble | Photography as noted.\nAges ago, the Greek philosopher Epictetus called books “the training weights of the mind.”\nAnd if books fulfill this role of weights that strengthen our minds, then bookshelves would be our literary home gyms that store and categorize our paperbacks and hardbacks. But in the twenty-first century, they do so much more.\nThis has been made evident during the COVID-19 pandemic, when many people began working from home more frequently. Whether you’ve been in meetings with coworkers or watching podcasts or interviews on TV, you likely noticed that many people positioned themselves in front of their home library or chose a bookshelf background filter. In no time, bookshelves had become a hot decor accessory.\nThese pieces of furniture have always told a story, but the story is very different today. The books and other items they hold reveal something about the person who owns them, whether it’s their literary chops or a glimpse into their personality.\nA Bookshelf Preface\nIf you’re considering adding a bookshelf to your home, it’s important to establish where you’ll utilize it, how you’ll do so, and what you’ll want your bookshelf to say. But first, you have to decide which type suits you and your space best.\nFlexibility is the primary benefit of standalone versions, as they are more affordable and can be moved around a room, within a home, and from home to home. The upside of built-ins is that they can give you a good return on investment (ROI) because they’re eye-catching and space-saving.\nAccording to experts, you should treat your bookshelf much like you would any other piece of furniture, so decide which room it will be in (if it’s a movable object), take a step back, look at the overall space with the empty bookshelf in it, and gauge how it would fit best.\nFor example, ask yourself if the bookshelf color matches the room’s overall palette. If it doesn’t, you may want to paint it to complement the rest of your decor. In addition, a hot trend is to maximize unused spaces, such as an understairs area, by adding built-in bookshelves to them.\nNow that you’ve determined where your shelving unit will be and how it will fit into the room, you need to ask yourself about form and function: what’s your purpose for this piece of furniture? It could simply be a place to store books and other items that would otherwise clutter your home, but it can also serve many other functions.\nThe key is being intentional with the objects that will go onto the shelves. Let’s start by discussing the star of this show, your books. Some fundamental questions will immediately pop into your head, such as Which should I select? How many are too many? and How\nshould I arrange them?\nKeep in mind that, through your choice of books, you’ll be making yourself, your interests, and even your beliefs somewhat of an open book. So perhaps you want to highlight your love of classics, books by your favorite author, or only books you’ve read. You could always do all three by dedicating a shelf to each.\nAnd, yes, people sometimes have nothing but books on their shelves. If your paperbacks and hardbacks are your sole decor tool, decide how you want them to look overall by considering your organization. For example, you could group your books by size, with smaller, lighter paperbacks on higher shelves and heavier hardbacks on lower shelves for practicality.\nYou may also wonder if they should be stacked vertically or horizontally. Why not both? When you have some areas with books upright and others laid down, it adds interest and prevents your furniture from looking like a library.\nYou could also choose to separate colors on different shelves, or you could opt to only display books that match the color surrounding the bookshelf: for example, blue books to match the blue wall paint behind it. Another trend is to reverse the books so that the pages are facing outward, thus guaranteeing a consistent white/neutral palette (but this tends to fall into the love-it-or-hate-it category).\nPerhaps most important is the focal point. Much like you should do with artwork, you should keep the books you want people to see first at eye level.\nIf your books are going to tell a story, what accompanies them on your shelves will as well. You could add any number of items to enhance your bookshelf aesthetic. Here are just a few examples of how to go about it.\n• Plants are a popular choice for bookshelves. Placing a small plant or two on shelves will add not only a natural touch but also a pop of color.\n• People often think that bookends are only for desks, but they can also be bookshelf statement pieces. For example, if you love reading about dogs, group your canine-themed books on one shelf with dog bookends.\n• Choose a basket that matches the bookshelf’s overall theme, which can serve as a convenient place to store blankets or your kids’ toys. Keep it on a low shelf for easy access.\n• Don’t forget about the books themselves. A few stacked books could hold a favorite knickknack; for example, you could have a family vacation photo resting atop a few travel books.\n• Utilize the inside back of your bookshelf as an accessory unto itself. You could add contact paper to this area so that your personal style is on display behind your books. Or, if you have white built-ins, you could paint this area a bright color for more pop.\n• Overall, balance is key to a good bookshelf, so you may want to be somewhat of a minimalist. Accessories can blend with your books to create an overall aesthetic, but they shouldn’t overwhelm them. That said, be you! It’s fine if you’re a maximalist at heart, as long as you’re organized about it.\nSomeone once pondered anonymously, “What is a bookshelf other than a treasure chest for a curious mind?” In 2022, this is even truer, both for bookshelf owners and their audiences. Whether you see a bookshelf as merely functional furniture or a personal literary or fashion statement, one thing is for sure: it’s a great way for people to read more about you.', ""How Does Stress Affect The Body: Symptoms And Solutions\nUpdated August 27, 2020\nMedically Reviewed By: Lori Jones, LMHC\nAre you exhausted all the time? Do you have a lot of muscle tension and pain? Is your head or stomach bothering you? It could be stress. There is a long list of symptoms that falls under the question, “How does stress affect the body?” And, learning how to identify the symptoms can help you find the right solution.\nStress levels have been on the rise in Americans over recent years. It’s impacting people of all ages and spans a wide range of worries and concerns.\nThe Impact Of Chronic Stress\nEveryday stress can have a negative impact on multiple areas of your life. However, when the stressful situation passes, you may find that things return to normal even if you didn’t do anything to address your stress. This isn’t the healthiest way to get through stress, but it happens this way for some people.\nHowever, if you’re experiencing chronic stress, it’s not going to just go away. It may not be tied to a specific situation in your life. Instead, it might be the result of poor habits or not knowing how to deal with past trauma. It will not just go away if left untreated.\nThe Effect Of Stress On The Body\nStress can wreak havoc on your body if it’s left unchecked. Not only does occasional stress show up in your body, but chronic stress can also have long-term negative consequences for your physical health. When you are feeling stressed, you may experience:\n- Increased heart rate\n- Rising blood pressure\n- Muscle tension\n- Upset stomach\n- Lack of sexual desire\n- Change in appetite\nAnd these are just a few of the symptoms that you may experience. If you suffer from chronic stress, the symptoms above can start to turn into more serious health consequences.\nChronic stress can lead to health problems such as heart disease, high blood pressure, gastrointestinal problems, heart attack, and strokes, among others. These are clear indicators that allowing chronic stress to continue in your life can be detrimental to your physical health and well-being.\nHow Stress Affects Mental Health\nStress also impacts your mental health and wellness. It can lead to you experiencing many different negatives and difficult emotions such as sadness, anger, frustration, and fear.\nSome of the mental health symptoms that you may notice in your life from stress include:\n- Lack of motivation\n- Irritability and anger\n- Lack of concentration and focus\nThese are serious symptoms that should not be taken lightly. If you experience chronic stress, you may begin to think that these symptoms are just a normal part of life. But, they’re not. All of these symptoms can grow into more serious problems if you don’t work on addressing them.\nHow Stress Affects Behavior\nStress can also impact your behavior. If you look at the symptoms listed above under physical and mental health, it can be easier to understand how stress changes your behavior. If you’re living under constant overwhelm and anxiety and experiencing things like frequent headaches or stomach aches, it can be easy to lose your temper with your loved ones, for example. Here are some of the other behavioral changes that you may experience in your life as a result of stress:\n- Angry outbursts\n- Eating too much or not enough\n- Substance use or abuse\n- Social withdrawal\nThese behaviors can have a negative spiral effect on your life. For example, as your withdrawal from friends and family because of stress, you may find that you struggle even more to cope with stress in your life. This can lead to additional problems which keep you away from a social activity even more. This is why it’s important to learn to recognize and healthily address your stress.\nStress Management Tips To Overcome Chronic Stress\nThankfully there are many things that you can do to address your chronic stress and learn to overcome it. This doesn’t mean that you’ll never experience stress again. Instead, it means that when you do go through stressful situations, you’ll have tips and strategies that you can use to relieve stress and handle it healthily.\nSome of the stress management solutions you may benefit from include:\nLearn to identify your stress triggers\nWhen you start to feel stressed, it can be helpful to take time to identify where the feelings are coming from. This allows you to begin investigating what you can do to make to address it.\nWhile there will be some things causing you to stress that you can’t do anything about, there will be some things that you can address. For example, if a family member’s behavior is causing you to feel stressed, you probably aren’t going to be able to control how they are behaving. But you may be able to establish boundaries in your life that stop the other person’s behavior from having as large of a negative consequence on you.\nThere will be some things that you find are short term stressors. But there also might be habits that you identify that are causing you unnecessary stress. When you learn where the stress is coming from, you can start to take your first steps to address or removing it.\nPractice Deep Breathing\nWhen you’re starting to feel the stress and tension build up within your body, deep breathing can help to break up some of the physical symptoms that you’re experiencing. For example, you may notice that you start to breathe faster as your frustration grows. This can cause your heart to race, as well. And, as your heart beats faster, your blood pressure rises. These physical symptoms can continue to build and even lead to things like full-blown panic attacks.\nDeep breathing can help to stop your physical symptoms from progressing. As you start taking slow, deep breaths in and out, you may notice that it feels like your blood pressure is lowering, and your heart rate is returning to normal.\nYou may also find that deep breathing can help you to slow your thoughts. Your mind will be forced to temporarily shift from your stress and worry to the breathing technique that you’re using. This can help you to regain mental clarity and look for solutions to the stressful situation or problem that you’re facing.\nThere are multiple types of breathing techniques that you can use, so practice a few of them to find what works best for you. It can also help to practice them when you’re not under stress, so when you find your stress starting to build, you will know how to put the breathing exercise to use without too much thought.\nNot getting enough sleep can make it even harder to deal with stress. You may find that you struggle to be patient with others, and you cannot think clearly to look for solutions. If you’re having problems falling asleep or staying asleep due to stress, it’s an important symptom to address.\nMany different things may help improve sleep troubles. A few that you could try include:\n- Keeping a strict sleep schedule\n- Cutting out caffeine\n- Not exercising too close to bedtime.\n- Sleeping in a dark, cool room\n- Using white noise\nHowever, if you’re continuing to struggle, don’t be afraid to talk with your doctor to explore additional options.\nGet More Physical Activity\nPhysical activity and exercise can help you release tension that has built up from chronic stress. It also releases chemicals in your brain that work to boost your mood. But these chemicals also act as natural pain killers, which can help reduce some of the physical symptoms you’re experiencing.\nThere are other ways that physical activity and exercise can help with stress. You may find that you sleep better when you exercise. And, you may experience a boost in your self-esteem as well.\nThe Anxiety and Depression Association of America shares that you may start to experience these positive mental boosts after just five minutes of physical activity. So, if you’re feeling stressed, you don’t need to feel like you have to get in a full workout. Simply getting moving for a few minutes can start to help.\nTalk To Someone\nHaving a trusted person to turn to for support can help when you’re going through stressful situations or experiencing chronic stress. This could be a friend or family member. It could also be a support group. For example, if you’re under stress as a result of losing a loved one, you may benefit from connecting in a group for others experiencing grief from losing someone.\nIf you don’t have anyone to turn to or could use additional support in handling your stress, a licensed therapist is an effective option to consider. Not only can they listen as you talk through the stress in your life, but they also have education on how to help you overcome it. A therapist, like those at BetterHelp, can assist you in finding stress-relieving strategies that work for your specific situation.\nPrevious ArticleHow Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It\nNext ArticleAre You Under Too Much Stress? Symptoms, Treatment And Tips\nLearn MoreWhat Is Online Therapy? About Online Counseling\nAbuse ADHD Adolescence Alzheimer's Ambition Anger Anxiety Attachment Attraction Behavior Bipolar Body Dysmorphic Disorder Body Language Bullying Careers Chat Childhood Counseling Dating Defense Mechanisms Dementia Depression Domestic Violence Eating Disorders Family Friendship General Grief Guilt Happiness How To Huntington's Disease Impulse Control Disorder Intimacy Loneliness Love Marriage Medication Memory Menopause MidLife Crisis Mindfulness Monogamy Morality Motivation Neuroticism Optimism Panic Attacks Paranoia Parenting Personality Personality Disorders Persuasion Pessimism Pheromones Phobias Pornography Procrastination Psychiatry Psychologists Psychopathy Psychosis Psychotherapy PTSD Punishment Rejection Relationships Resilience Schizophrenia Self Esteem Sleep Sociopathy Stage Fright Stereotypes Stress Success Stories Synesthesia Teamwork Teenagers Temperament Tests Therapy Time Management Trauma Visualization Willpower Wisdom Worry\nFeeling Overwhelmed? Learn These Stress Management Strategies How To Stop Stressing: 7 Tips To Find Balance And Relax How Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It Are You Under Too Much Stress? Symptoms, Treatment And Tips 7 Tips On How To Handle Stressful Situations Stress Management That Works: How To Be Less Stressed""]"	['<urn:uuid:6cc61a2c-c4b6-43df-9698-052474e96ad2>', '<urn:uuid:1fe21db8-8096-462d-a9b3-85cbac331785>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	23	126	2773
33	professional writer expertise training requirements technical business writing compare	The expertise and training requirements differ significantly between technical and business writing. Technical writers need specialized skills and often require specific training, even if they're not experts in the field they're writing about. They must be able to analyze complex information and present it clearly, work with subject matter experts, and have thorough knowledge of required computer programs. Business writing, in contrast, can be done by anyone with basic writing skills who understands what needs to be achieved. While both types may benefit from training, business writing training focuses more on clear communication and professional structure, while technical writing training emphasizes accuracy, specialized formats, and technical content presentation. As stated, technical writing gained particular importance during the Industrial Revolution when clear machine operation instructions became crucial.	"['The process of technical writing is similar to performing a process for anything that requires skill and accuracy. The process for a technical writer or any other specialized skill revolves around a procedure of thought. The primary objective of a technical writer is to educate, train or explain something to someone. The technical writer must know how to write and must understand the topic to be written about. ""Errors in technical writing,"" explained my boss, Richard L. Kintzele, Jr., CEO, Colorado Computer Center, ""often result in bad data, bad decisions and bad days."" Accuracy is important.\nThe technical writing process serves as a reminder about how to do something and allows us to think about what we are doing without trying to remember what we are going to do next. It is a logical process requiring us to do first things first. The process allows us to focus on the task before us without worrying about the next task in a sequence of events. The process serves as a reminder to help us remember or prompt our memory about the broader objective before us.\nWhen attempting to write, we must also do first things first. Even experienced writers need a reminder to return to the basics of writing before proceeding to the bestseller list. It also helps to have a checklist available to help us remember what we need to do to eliminate errors, clarify thinking and communicate to our audience in a clear and concise manner.\nThis is why the first step in writing is to always conduct the research first. A subject must be understood before it can be explained.\nTen Steps to Technical Writing\nAlways take notes\nWrite it again\nWrite until it is right\nRepeat process, as necessary\nThe first step in any writing process is to conduct research. Writers must know and understand the subject they are going to write about before embarking on the task of writing. This is why teachers teach us about a subject before requiring us to write a book report. We need to learn about something before we can teach it or explain it to someone else. Research is required. Know your subject and know your audience. It is important to understand the subject you are going to write about and equally important to know the audience you are writing for. While conducting research, take notes and, then, later when writing, you will be better able to share your knowledge with others.\nAlways Take Notes\nWhile conducting research it is always helpful to take notes. The notes we write or scribble become the basis for an article, book or technical manuscript. Take notes whether your research is obtained from a book or from a personal interviewed. Notes are later used to help maintain accuracy and cite sources. In addition, note-taking allows you to:\nRefresh your memory\nRefer to while writing\nHelp determine how to organize the material into an outline or rough draft\nVerify facts after writing the rough draft\nReview for new information that may need updated or revised\nNotes do not have to be taken or written in complete sentences either. Notes are simply reminders that are written down to help with memory recall allowing the writer to think while writing. Grammar and punctuation do not need to be perfect when taking notes. Wait to edit until after completing the first draft.\nThe process of writing always begins with the first draft. It cannot be called a draft until something is written. The first draft, whether partial or complete thoughts, is always called a first draft. It takes time to sort thoughts in a logical manner that will help your audience understand the content, context or meaning of what you are trying to say. This takes time. This is also why the first draft is sometimes referred to as a ""rough draft"" because our first attempts to write are meant to simply get words on paper so that we can read what we wrote while deciding what it is we want to say. The first draft may be only an outline or a few paragraphs, but this is what leads the way to the next thought that becomes a subsequent paragraph.\nTypes of Drafts\nThere are basically four types of drafts. Different industries use different terminology to label the draft within the writing cycle. The cycle remains the same within industries with slight variation in terms to explain the process of obtaining editorial reviews. The most common names for the different types of drafts are provided below.\nThe four types of drafts are:\nRough draft (words on paper)\nContent draft (add content, refine outline)\nPeer draft (prepare for technical review by peers to verify accuracy of content)\nEdit draft (revise content, prepare for editorial review and publication)\nRewriting does not imply starting over by reworking all of your work. Rewriting implies revising one\'s work. After reading what you wrote, revise it, correct inaccuracies, read for clarity and purpose. Read what you wrote several times to ensure that you communicated the information in the best format and context for your audience. Remember the draft is a draft until you determine that you wrote what is needed to be written in order to communicate the information to the best of your ability. Rewriting is writing. Rewriting is refining the format, the content, the words or the text that makes something understandable by someone else. Communication is not easy, whether in person or on paper. Below are five steps to help you review or revise the content of the material.\nRewrite the rough draft\nReview content from the rough draft\nConduct more research to add or refine content\nRevise outline and content to better organize information or describe data\nDesign content to prepare for presentation to audience\nVisual appeal is important to draw attention to the subject, to allow readers to remember it and to provide color to the written words. People more easily relate to visual aids which are also easy to remember. The eye and brain work together allowing us to visually understand and to quickly recall information. There are many types of visual aids which help the writer communicate quickly and help the reader associate the visual reminder with the written text. Visual aids serve many purposes. Photographs, diagrams or drawings are good for the eyes also. Visual aids are also good for the mind and, most importantly, conducive to thought. Visual aids help us think.\nIn addition to helping us think, visual aids are more easily remembered than text, are more easily memorized than text and, therefore, more easily recalled by the reader than text.\nTypes of Graphics\nThe most common types of visual aids are defined and listed below. As a writer, the type of graphic used improves the communication process and helps the reader comprehend the message. The more technical the information is, the more important it is to provide good graphics to help communicate with the audience. The list below provides a list of the types of graphics typically used and the purpose they serve.\nGraphs = Compares data based on different variables.\nMaps = Shows the geographical location of something.\nCharts = Shows comparisons, proportions or percentages.\nFlow Charts = Shows how something works. Flow charts are typically used to describe a process or function of a computer or software.\nDiagrams = Shows how something is put together. May show the components of a system and how they interrelate.\nDrawings = Shows what something looks like.\nTables = Contains data that defines and describes.\nFigures = May contain any type of visual aid or picture.\nPhotographs = Shows a photograph of something.\nLists are also critical aids for technical writers who are trying to communicate complex subjects to an audience. Lists are also memory aids which allow the writer to organize information in an easy to remember format while also allowing the reader to comprehend and recall the information quickly. Lists are always good memory aids. Lists are important for the following reasons:\nLists attract attention\nLists are logical\nLists condense information\nLists are easy to reference\nLists are easy to comprehend\nLists are easy to memorize\nLists are easy to remember\nTypes of Lists\nThere are also many types of lists to choose from when trying to organize information for understanding and comprehension. By organizing information into lists, the writer helps the reader learn the information quickly and easily. Lists also provide the reader with a method to quickly reference critical information. Below is a list of the most common types of lists and how they are organized.\nNumbered Lists = Numbered entries are practical and necessary to define a step in a process or the order of a sequence.\nBullet Lists = Bullet lists are frequently used to organize words, phrases or sentences that are not necessarily related to a sequence or a step-by-step process.\nLogic Lists = Logical lists are used to summarize a complex progression of thought. Major points are listed before minor points.\nSentence Lists = Sentence lists are lists of sentences that should be in similar length. The shorter the length, the quicker it delivers the message.\nWord or Phrase List = Word lists are lists of words that may or may not be listed in a specific order. Alphabetical order is a common grouping for word lists.\nOrganization of Lists\nHow a list is organized is also important. Select the best method for organizing information that will best help the reader refer to the information later. Different types of information are better suited for different methods of organization. The data drives the decision. Below is a list of the most common types of lists which are used to organize data in a context that is meaningful and easily recalled by the reader.\nRelative order of importance\nRelative order of priority\nWrite It Again\nAfter completing the ""rough draft"" or first draft, it is important for the writer to revise the draft. This includes condensing the information to simply it or expanding it to provide further clarification. When composing a first draft, the writer\'s goal is to write as much information down as possible. Once this is accomplished, the writer then has to condense it, expand it or clarify it for their audience. As the first draft begins to progress from a rough stage to a more complete state, the draft develops into a ""content draft."" This occurs as the draft become more fully developed and closer to the finished product. It is also the second phase in the writing process. Keep writing or rewriting and, if necessary, throw away the first draft and start over. At times, it is easier to start with a blank slate to clear one\'s mind for a fresh beginning.\nThe importance of proofreading cannot be overstated. Proofread your work yourself and, always ask someone else to proofread it for you as well. It is difficult to read, write, edit and proofread all at the same time. Writers need someone to read their work to make sure there is not grammatical, spelling or other errors that may distract the reader from the message. Ask others to proofread it for you before you publish. It is always easier to see the error after it goes public. Following is a list of five ways and reasons to proofread. By focusing on one specific goal with each proofreading session, it becomes easier to notice the errors and, therefore, correct them before others notice them also. Below are five steps to help you to remember to read it for comprehension first. Then, read it again for details and factual statements.\nProofread once for clarity\nProofread twice to correct errors\nProofread a third time to verify facts\nProofread a fourth time for good measure\nRepeat proofread process, as necessary\nWrite Until It\'s Right\nAfter completing the ""content draft:"" begin preparations for the ""peer draft"" by continue to review, revise, rewrite, modify, update and correct information contained in previous drafts. Continue to take a few moments to proofread your writing to eliminate errors in typing, spelling, grammar, technical data, and improve content presentation. Then, begin preparing the material to be reviewed by peers, subject-matter-experts, advisors and executives. This the final stage in the writing process. This is the phase when you begin to read your work as others will see it. This is phase of the process when you begin to step back from your writing and try to read it from a neutral perspective. This is also the phase that separates the beginners from the professionals. This is the phase when the art of writing becomes the labor of work. Or, in the words of Mario Puzo, author of The Godfather, ""Rewriting is the whole secret to writing.""\nRepeat Process, as necessary\nThe process of technical writing often has to be repeated to be performed well. Technical documents must be drafted, revised, edited and submitted for reviews by peers or subject matter experts. The writer is responsible for describing information accurately while the peer groups are responsible for determining that the content and context of the information is also accurate. This stage or phase of the writing process is often referred to as the ""technical review"" phase.\nThe technical review phase is when the final draft is submitted to multiple readers for feedback, corrections and to verify the information presented. While the draft is in this final review phases, the writer is simultaneously making last-minute changes to clarify data, check facts or reduce redundancy. After all of the comments are returned by the reviewers and all questions answered, the writer begins to incorporate recommended changes into the final document. This is also a good time to go back to the basics by reviewing the ten-step process or re-read the proofreader\'s checklist. Repeat each step of the process, not necessarily in order or sequence, but simply to ensure that nothing was forgotten. Repeat this process as necessary to ensure quality work in a timely fashion.\nMerlene is a Communications Content Consultant working under the business name of Merlene\'s Memos. She also publishes articles about: computers, technology, psychology, criminology, sociology, media, human behavior, offender behavior, relationships, ethics and more.. Contact Merlene\'s Memos for help managing a special project or creating content for your corporation. http://merlenesmemos.com/blog/mediaandsociety/', 'Howengineering writing differs from business writing\nEffectivecommunication is very important in modern business. There are variousways in which people can communicate in the right manner. Writtencommunication is one of the ways that is used to communicateinformation between individuals. Every kind of business or professionhas its appropriate way of communicating (Anderson 2007). It isimportant to understand the right way to right reports so that theintended message can be communicated. When choosing the method ofwriting a report, it is necessary to ensure a suitable method hasbeen chosen. Some industries have a standard way in which theirreports are made. A report that is prepared in any different way willnot be accepted. This paper aims at explaining the differencesbetween engineering or technical writing and business writing.\nBusinessand technical writing are segregated by the subject matter. Technicalwriting involves reports about subjects like science, technology andengineering. Technical reports are done according to a specificformat. It is important for the writer to understand what is expectedand adhere to it. Every technical industry has its own preferred wayof writing its reports. Business writing is more common thantechnical writing (Brierley 2002). It involves all other forms ofwriting that people do in their places of work with the exclusion ofcreative writing and journalism. It is less formal than technicalwriting and includes memos, proposals, minutes, letters and evenemails. Despite the differences between the two, there is usually acrossover. Some business documents such as proposal may containtechnical information used to make certain decisions. On the otherhand, technical writers may be called upon to use business languageto help their audience understand the technical aspects of the letteror document. Many people are interested in learning how to writetechnical reports. There are many resources that can be used to getthe details. However, some of them are not good enough as they justuse business writing materials and make them appear technical. Peoplewho are seeking employment in the technical field need o be carefuland find training from the right sources.\nAlthoughthere are differences in the two types of writing, they serve onepurpose. Their purpose is to inform and pass the right information tothose interested in it. In order to pass the right information andensure it is understood by the recipient, it is important to use theright language. Mistakes can lead to misinterpretation. Using theright language and style of communications makes the message clear tothe reader and therefore easy to understand. A good report should beclear, without ambiguity and to the point (Johnson 2011). Using toomany unclear words may blur the message and lose the pint of writingthe report in the first place. There are words that appropriatelyused in technical or business writing. Professional jargon can beused in the reports if those being addressed are all aware of theintended meaning. The same kind of language cannot be used wherepeople lacking the understanding are involved. However, it is betterto use simple language than technical words as they often causeproblems and can be misinterpreted. Any doubtful words should not beused even when they seem to be impressive.\nAlltypes of writing should be grammatically correct. If a document haserrors of grammar, punctuation or spelling mistakes, the writer willnot be taken seriously. The document will fail to communicate theintended message to the reader (Markel 2012). The style of thedocument is also important. Most business documents are morepersuasive in nature while technical ones are neutral and objective.They use facts to deliver the intended point.\nGoodtechnical writing should be brief yet accurate. It should focus onthe intended point so that the reader is able to understand themessage that requires to be communicated. Although technical termscan be used, technical writers try to avoid them except where theirusage is necessary. Using too many technical terms can make thedocument lose its meaning. Most technical documents are used in realsituations (O’Hara 2014). Therefore, it is important for thedocument to be clear on the subject matter being discussed. It canresult in more problems if the procedures or subject being discussedin the document is not clearly understood by the reader.\nAgood technical writer should be aware of the audience and what theyexpect. Being aware of the level of knowledge of the audience willhelp the write deliver the required information. This awareness willhelp determine the amount f information and or details to becontained in the document. For instance, a document designed for thegeneral public will not be written the same as one that will be usedfor highly skilled engineers (Tebeaux, Dragga 2010). A technicalwriter can write the desired document well even when they are notexperts in the field of concern. By working together with experts inthe concerned area, they are able to deliver the desired message.\nAccuracyis technical writing is mandatory. This requires the write to beclear on what message needs to be transmitted to the audience. Themessage should be communicated in a way that will not leave anydoubts in the mind of the reader. Knowing the audience and itsexpectations will help structure the document in the desired manner.The message should be easy for the reader to follow and achieve thesame message as intended by the writer. Apart from accuracy, thedocument should also be presented in the right format (Johnson 2011).Technical documents have particular formats that should be followed.Various professionals have their preferred formats used by theirmembers to communicate among themselves or with the rest of thepublic.\nTechnicalwriting requires special skills to deliver the message in the rightform. This requires the writer to get some form of training. If thewriter does not have the knowledge in the field being reported on,they can work with experts to help them grasp the desired message.There are many places one can learn to be a technical writer (Crabbe2012). The information that is conveyed in technical writing is oftencomplex and the information should be analyzed and presented in a waythat is easy for the readers to understand. This is one reason forthe writer to be aware of what the audience expects and what they areable to understand.\nThetechnical write should not only have the knowledge of the field beingwritten about abut also effective communication skills. The writershould be aware of the best ways to put the desired message across tothe reader. The use of computer programs is common when writing thesereports (O’Hara 2014). Therefore, the write should have thoroughknowledge to use the programs required in producing the desiredresult.\nBusinesswriting is a lot more common. It refers to the different types ofcommunication used in everyday business. It is easy to write abusiness document than a technical one. Anyone one with basic writingskills can produce a business report as long as they are aware ofwhat needs to be achieved. However, it requires that the writer beable to write in a clear and understandable manner (Anderson 2007).The document should be prepared in a professional way that willclearly communicate the desired objective. Just as technical writing,a person may need to get some training in order to get the necessaryskills. There are many institutions that offer this kind of training.The main aspect of the training is to help the writer to structurethe reports in a manner that will clearly pass the desired message.\nTrainingin business or technical witting is necessary even for people who maybe aware of what needs to be done. Training equips a writer with theability to communicate more effectively using the right language thatwill be understood by the audience. In the modern world, most of thetime of professionals is used doing some of business related writing.Choosing a trainer with a wide range of experience will help ingetting the right skills to improve quality of writing (Brierley2002). In some cases, the quality of the message and its presentationwill help determine the outcome of a business transaction. Whetheryou are a business professional or a job seeker, you need to get youwriting right. It can be the difference between getting a new deal ora new job. Since your work involves lot of writing, you need to makesure your writing skills are at their best. Written communication isa major component of modern communication.\nItshould be understood that there are times when both technical andbusiness writing can be used together. You need to know how to usethe two together to avoid making the reader confused. For people whowant to add knowledge of these two forms of writing, they should takelesson either locally or on the internet. You can choose the ode oftraining that suits your situations. There are online tutors as wellas local colleges that offer the training. Both types of writing areimportant and none should be ignored (Johnson 2011). Understandingeach of them and how to use them well is a very strong positive foranyone to posses. When preparing the documents, the writer shouldensure there documents are free from any mistakes. A mistake in thedocument leads to the writer losing credibility and it might cost theorganization business. After preparing technical or businessdocument, it is important to go through it and ensure it is free ofany mistakes.\nTechnicalwriting gained popularity during the Industrial Revolution. Duringthat time, new machines were being introduced in the production ofvarious products. There was need to instruct people on how to use themachines safely. The people who invented the machines detailedinstructions that wee to be used by the people in the factories whowere employed to operate the machines. The writers had to make theirinstructions very clear to avoid any ambiguity (Markel 2012). If thereposts were not clear, the users would make mistakes that could leadto injuries. Writing the procedures was the only way of ensuring thatother people other than the investors of the machines also learnedhow to use them. People who could document the procedures in theright way were highly sought after. Therefore, the documents welldocumented procedures were needed by all the industries thatincorporated the use of machines in their operations. After WWII,more technological advances were made. This increased the demand fortechnical writing skills.\nInconclusion, business and technical writing have an importance inmodern conduct of trade. Business writing is more common and requiresfewer skills to write. It is used when writing memos, minutes, emailsand other forms of communication in business. Although it is lesscomplicated business writing skills are required in order to producethe best reports. Technical writing requires special skills. Atechnical writer should have an understanding of what is beingwritten (Crabbe 2012). The idea of technical writing should be topass the message in a form that is easy t understand. A technicalwriter should understand the capacity of the audience to comprehendthe message being communicated. A report aimed for the general publiccannot be the same as one designed for experts such as scientists orengineers. Although the two types of writing are different, they canbe used together in the same document. Therefore, a technical writermay also need to use business writing language effectively. Technicalwriting involves the use of terminologies. The use of technical termsshould be limited to avoid ambiguity. Whenever possible, simple termsshould be used in place of technical terms. Using too many technicalterms can lead to the report becoming ambiguous. Technical writers donot have to be exerts in the particular field. They can work withexperts to come up with the writing they want to get. People who needto improve their writing skills can acquire the skills by gettingtraining from local colleges or through the internet.\nAnderson,PV. TechnicalCommunication [A Reader-Centered Approach] 6th Edition.Thompson Wadsworth, 2007. Pg. 485.\nBrierley,S. “ScreenCaptures 102”.STCCarolina.2002. Retrieved May 9, 2014. Pg. 5-8\nCrabbe,S. “Constructinga Contextual History of English Language Technical Writing"".Universityof Portsmouth.2012 Retrieved April 30, 2014. Pg. 3.\nJohnson,T.“WhatTools Do Technical Writers Use"".I’dRather Be Writing.December 19, 2011. Retrieved May 4, 2014.\nMarkel,M. TechnicalCommunication 10th Edition.Bedford/St. Martins, 2012. Pg. 12-14.\nO’Hara,F. M. Jr. “ABrief History of Technical Communication"".MontanaState University Billings.Retrieved April 22, 2014. Pg. 2.\nTebeaux,E. and Dragga, S. TheEssentials of Technical Communication.Oxford University Press, 2010. Pg. 7.']"	['<urn:uuid:03caa3be-426b-4a2d-a561-0c75b2e28ddf>', '<urn:uuid:a9a74a0b-9b37-421f-9277-d540f56456a5>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	9	126	4325
34	I'm researching non-profit art organizations in Austin and would love to know more about any poetry festivals that were organized there in the past. Can you tell me about any significant poetry events that took place in Austin?	The Open Theatre, a non-profit arts organization in Austin, Texas, ran the International Texas Poetry Festival, which was a significant poetry series that continued annually for over ten years. The festival's participants had their work published in an anthology called PANGAEA. The Open Theatre also published another series called Periplum/Austin and organized art shows and music festivals.	['HISTORY AND WORK\nThe Open Theatre\nCampion (with the poet John Herndon) founded the poetry and arts organization The Open Theatre. This group is non-profit and tax exempt and is headquartered in Austin, Texas. The group began in the mid-seventies, and ran, among others, an important poetry series called the International Texas Poetry Festival. It ran annually for over ten years and its readers were published in an anthology, PANGAEA. From other series the group published Periplum/Austin. The Open Theatre also presented art shows and music festivals.\nThe first work of Ecotropics emerged from the work of The Open Theatre and came in the form of a pamphlet, Toward an Ecotropic Poetry (by John Campion and John Herndon). You can see a copy of this manifesto as a PDF under the Ecotroipc Works Menu.\nEcotropic Works (from the root Eco: “the hearth” as in ecology and economy), and (Tropics: “to turn towards,” as in heliotropic and psychotropic) grew from this source.. Ecotropics holds to the principle that for human culture to be healthy it must exist in an ecological niche–and thereby, relate appropriately with all the fields of forces of nature.\nThe anthology, Ecotropic Works, represents a multi-disciplinary approach to these issues. And brings together noted poets, artists, and scientists in a collaborative effort to address the ongoing effort to create a unified understanding for humanity, so that we might live well within the wide, limited, scope of nature.\nJohn Campion is the author of numerous books of poetry including The Third Music, Sippapu the Kiva an Inverted Bat, Where Three Roads Meet, and three large, book-length poems from a projected series of five: Tongue Stones (Winner of both the Austin Book and the Violet Crown Awards), Squaring, the Circle (Winner of the Blue-Star Foundation Award for Art in Service to the Earth), and his most recent, Medusa*—altogether, these deeply-linked efforts form the core of his life’s work. His work has been called a landscape architecture of the page—and several of his Texturals appeared in a show of landscape architects in San Diego (The Object of Reason—2002)\n*Medusa presents the third in a series of book-length poems–along with Tongue Stones and Squaring the Circle. These books lay the foundations of his ecotropic philosophy, which argues that for human culture to be healthy, it must exist in an ecological niche. In Medusa, the ancient myth finds contemporary expression, as “Everyman” (suffering in a world of political surveillance) finds himself in psychotherapy for his personal and transpersonal offenses against ‘women’, ‘the other’, and the ‘earth’. You can find a pdf file in the Medusa sub tab.\nCampion made the first English translation of Sor Juana Ines de la Cruz’ magnum opus, El Sueño, Thorp Spirngs Press. The book is out of print, but you can find a pdf under the Translation head. A PDF of the transcreation of The Popul Vuh is also available.\nCampion provided the conceptual frame for Floodstage at the Memory Theatre—a collaborative effort with sculptor T. J. Mabrey and composer Edmund Campion—exploring the ecotropic relationship between the sky and the earth. The work presented models of the precession of the equinox as it is presented in ancient astronomy and myth. (See Hamlet’s Mill). The installation became a minor cause celebre when it was initially censured and closed for some of the content in Campion’s Texturals. The ACLU’s successful effort to force the show to reopen is documented in The Americans’ for Freedoms publication on censorship in the U.S. The work was present twice—in a community just North of Austin, TX and at College Station on the Texas A and M campus, where it won the prize for the best art show of that year.\nThe poet has participated in several collaborations with the composer Edmund Campion, his brother. The first of their major efforts was The Map a Dung-Beetle Makes, which the two wrote and performed while they were fellows together at the MacDowell Colony in New Hampshire. They also presented two works at the Chapel of the Chimes in Oakland, CA, Le Petit Mort (a surreal theurgic drama of poetry and music) and Name Calling (an exploration of endangered peoples and languages). These works prefigured their major efforts thus far, L’autre (commissioned by Radio France) and ME (Commissioned by CIRAM, Nice, FR). These two represent a deep attempt to re-dress the enormous differences in orientation and expression that poets and composers have.\nMore recently, the poet has worked with the composer Cindy Cox, his wife. These include Hysteria, The Other Side of the World, The Shape of the Shell, Singing the Lines, and Nature is. All of these have been performed throughout the world and have been recorded as well. Ms. Cox used one of the poet’s Texturals for the cover of her CD, Nature Is.\nCampion’s poetry has been called “a kind of landscape architecture of the page”. This works for his art as well. Beginning with text from his poetry, he produces large illustrated Texturals, made with ink, oil, water colors, and many extended materials on home made papers. These have been shown in various exhibits, including the award winning installation, Floodstage at the Memory Theatre (with sculptor T. J. Mabrey). Campion also had a one person show on the UC Berkeley campus (Ryder-Worth Gallery in 2009. It was entitled Texturals from the Medusa: A Walking Book.\nCampion made a large book (4′ x 3′) of illustrated man-hole covers (with Paul Wintle ) entitle Ground Zero on the Road to Xibalba. The book is taken from charcoal rubbings of covers from an old street in Austin, Tx and tells the story of the underworld (the repressed) of modern day society.\nCampion has written a number of short pieces of fiction. You can see a few of them in PDF form under the Ficiton head.\nCampion has taken up the gauntlet a few times to write about war, music, culture, philosophy, and poetry. Take a look at a few of these in PDF form.\nCampion hasover 30 years of teaching experience from his community college work to UT Austin, USF, and his current position in the English Dept. at UC Berkeley.\nCheck out Campion’s page on the web site in the English Department at UC Berkeley:\nYou can also look at his work on Charles Olson through this web site:']	['<urn:uuid:85eda2ee-80dd-4a28-879c-88f439c5d53a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	38	57	1054
35	I'm setting up optimal water conditions for my aquatic pets - what's the difference in preferred water temperature ranges between False Map Turtles and Suckermouth catfish?	Suckermouth catfish prefer warmer water temperatures of 22-28°C (71.6-82.4°F), while False Map Turtles do well in slightly cooler water in the low to mid 70s Fahrenheit.	['Suckermouth catfish - Hypostomus plecostomus\nScientific name: Hypostomus plecostomus\nCommon name: Suckermouth catfish\nUsual size in fish tanks: 45 - 52 cm (17.72 - 20.47 inch)\nRecommended pH range for the species: 6.4 - 7.6\nRecommended water hardness (dGH): 10 - 25°N (178.57 - 446.43ppm)\n0°C 32°F30°C 86°F\nRecommended temperature: 22 - 28 °C (71.6 - 82.4°F)\nThe way how these fish reproduce: Spawning\nWhere the species comes from: South America\nTemperament to its own species: peaceful\nTemperament toward other fish species: peaceful\nUsual place in the tank: Bottom levels\nSouth America, Suckermouth catfish are mostly found in the waterways of the Orinoco River.\nThe expected life span for Hypostomus plecostomus is 10-15 years.\nHypostomus plecostomus is often kept in smaller tanks that do not allow for the full size of a mature specimen. Many novice keepers may not realise the full adult size of these fish as they are normally sold as juveniles, if kept in a large enough aquarium they are a great addition though and in milder climates they can even be kept in outside fish ponds but always check before attempting this move. They are often referred to as the common plec which somehow takes away the beauty of these fish but they have been very popular for many years because they are very hardy and can accept many different water parameters but like all species of fish the keeper should maintain as high a water quality as possible. Add rocks or artificial caves to provide hiding places and use subdued lighting. They are high waste producers so make sure that the filtration system is rated for this and perform regular water changes, at least 10% weekly.\nLive plants can be added to the aquarium but be prepared for some damage as they may uproot the plants and even nibble leaves away at some species of plants.\nAs juveniles they are placid and will socialise with each other but as they mature they do become territorial and aggressive to their own species and other bottom dwellers so it is advised to only keep one specimen per aquarium.\nFood and feeding\nAlthough Suckermouth catfish will accept anything offered, its diet should be mainly vegetable based. Spirulina flake or pellets along with algae wafers are ideal for the staple diet. Treats of blood worms and brine shrimp will also be accepted. As this is a mostly nocturnal fish, an evening feed should be of more benefit. In the past they were classed as algae eaters and no supplements to their diet was offered but nowadays this practice is becoming more rare and keepers appreciate that these fish need a varied diet as much as the other tank inhabitants.\nMature males will have thicker pectoral fins that will start to show a pinkish colouration. When viewed from above females may appear to have a stockier body shape.\nAs of yet, there are no reported cases of this fish breeding in the aquarium. In the wild they use holes in the river banks for spawning sites. Like most of the plecs the male assumes the parental duties and once the eggs are fertilised, the female plays no further part in raising the young.\nProvided by Mihail of Romania.', 'False Map Turtle\n- Size: Small\n- Length: 6”-10.5”\n- Type: Aquatic turtles\n- Lifespan: 20+ years\n- Food: Pelleted commercial diet, snails, mussels, fish, crickets, worms, aquatic insects, aquatic plants\n- Difficulty Of Care: Medium maintenance\n- Comparable Breeds: Mississippi Map Turtle, Cagle’s Map Turtle\nFalse Map Turtle General Info\nThe False Map Turtle is one of several breeds of Map Turtles. They are also referred to as Sawback Turtles, thanks to the interesting points that run along the top of the shell. Although they are considered moderate maintenance turtles, False Map Turtles can also be a good choice if you are new to keeping turtles as pets.\nThe False Map Turtle is one of several breeds of Map Turtles.\nFalse Map Turtles could be found living in large rivers and their tributaries, but they could also sometimes be found living in backwaters, oxbows, and floodplain ponds as well. Their wild range includes Missouri, Minnesota, Illinois, Indiana, Nebraska, the Dakotas, and Kentucky. When they are not in the water, they can be seen basking.\nThe reason why the False Map Turtle has this name is because it features light yellow lines on its carapace. These lines form what looks like the pattern that would be found on a map. However, the pattern is more obvious on younger turtles, as it tends to fade with age. When looking at a False Map Turtle, you will also notice that this breed features an obvious ridged keel on its carapace.\nFalse Map Turtles are also referred to as Sawback Turtles.\nA False Map Turtle’s carapace will range from brown to olive in color. It will also showcase light yellow markings that have dark borders. The plastron, on the other hand, will vary from a yellow color to a cream color. It features dark lines that run along the seams and create a pattern in juveniles.\nIn addition to the map-like pattern of light yellow lines on the carapace, the False Map Turtle also features light yellow lines on the limbs and head. You will even note a backwards “L” shape behind each of the eyes. The overall body color ranges from gray-brown to black, and a False Map Turtle could have white, yellow, or light brown stripes on the body as well. The eyes could be light yellow, green, white, or brown, and they also feature a dark bar. Small spots that are light in color can also be seen on the turtle’s chin and below the eyes.\nFalse Map Turtles prefer living in flowing water, and you should provide your pet with a large enclosure that has proper filtration in place because these turtles are highly sensitive to water quality. Also, females will grow to a fairly large size, so it is important to keep the size of your enclosure in mind, particularly if you are planning on keeping your turtle indoors or with other turtles.\nThe depth of the water should be enough that your False Map Turtle can swim comfortably, as these animals are great swimmers and will need deep water. Your turtle will also like to have a log snag underwater, as that could provide an ideal place to rest. In terms of substrate, you could use fine to medium sized gravel, or you could instead opt for sand.\nWhen it comes to basking, set up an area where your turtle can completely dry the plastron and carapace to help prevent health problems like shell rot and fungus. Ensure that the basking site is warmed to a minimum of 80°F, although it could also range from the high 80s to low 90s Fahrenheit, with the help of a heat lamp. And don’t forget that UVB lighting should also be provided for basking.\nMaintain the air temperature in your False Map Turtle’s enclosure anywhere from the low to mid 80s Fahrenheit. And when it comes to water temperature, this turtle can do well in water that is maintained anywhere from the low to mid 70s Fahrenheit, so a submersible heater can definitely come in handy.\nA single male adult can be housed in an aquarium that is 40 or more gallons, while females could do well in an aquarium that is 90 or more gallons. If you wish to house more than one False Map Turtle in the same enclosure, provide at least 20 additional gallons of tank space for every extra male, or an additional 50 gallons for every additional female.\nFalse Map Turtles can also be a good choice if you are new to keeping turtles as pets.\nBecause the False Map Turtle is a carnivorous breed, you can feed your pet a variety of foods, such as snails and mussels, fish, crickets, worms, and aquatic insects. However, these turtles could also enjoy some aquatic plants occasionally, and you can also feed your pet commercial turtle pellets.\nIn terms of their personality, False Map Turtles are considered docile, and they typically will not bite you, so they make good pets. They can be housed with other turtles, such as other Map Turtles, Painted Turtles, Cooters, Sliders, Mud Turtles, and Musk Turtles, in a community enclosure as well. And they really do enjoy basking a lot (in the wild, they could spend hours in the sunshine).']	['<urn:uuid:b0ac2919-659a-429a-82d7-bb39cfd7899d>', '<urn:uuid:001346cb-476d-4dea-9409-86d5bfe31f9d>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T04:43:46.574121	26	26	1416
36	microplastics marine animals ingestion frequency effects and whale death plastic stomach contents	Deep-water fish show one of the highest reported frequencies of microplastic ingestion worldwide, with 73% of examined fish having microplastics in their stomachs. Microplastics can cause internal damage, inflammation, and reduced feeding in marine animals. In a striking case, a 9.5-metre sperm whale found in Wakatobi contained 5.9 kg of plastic items in its stomach, including 115 plastic cups, 19 pieces of hard plastic, 4 plastic bottles, 25 plastic bags, and over 1,000 pieces of plastic raffia string.	['Plastic pollution is affecting marine life in some of the most remote parts of the Atlantic Ocean with almost three quarters of a sample of more than 230 deep-water fish collected by NUI Galway scientists having ingested plastic particles.\nThe contamination level among the fish species, located in the northwest Atlantic thousands of kilometres from land and 600m down in the ocean, is one of the highest reported frequencies of microplastic occurrence in fish worldwide, according to the study published today in the journal Frontiers in Marine Science.\nMicroplastics are small plastic fragments that commonly originate from the breakdown of larger plastic items entering the ocean. Other sources may be waste water effluents carrying plastic fibres from clothing and microbeads from personal care products. Due to their low density, most of these microplastics float at the sea surface.\nAs part of the study the NUIG scientists participated in a transatlantic crossing onboard the marine institute’s Celtic Explorer vessel. During this research cruise they took dead deep-sea fish from midwater trawls such as the spotted lanternfish, rakery beaconlamp, stout saw-palate and scaly dragonfish.\nThe fish ranged in size from the smallest species the Glacier Lantern at 3.5cm to the largest species, the stout saw-palate at 59cm. Upon return to Galway the fish were inspected at the NUIG’s Ryan Institute for microplastics in their stomach contents.\nPhD candidate and lead author Alina Wieczorek said: “Deep-water fish migrate to the surface at night to feed on plankton [microscopic animals] and this is likely when they are exposed to the microplastics.”\nOne of the inspected spotted lanternfish, which was 4.5cm in size, had 13 microplastics extracted from its stomach contents. The identified microplastics were mostly fibres, commonly blue and black in colour.\nIn total, 233 fish were examined and 73 per cent had microplastics in their stomachs, “making it one of the highest reported frequencies of microplastic occurrence in fish worldwide”.\nPrevious studies have shown microplastics can be ingested by numerous marine animals from zooplankton to worms to fish. The ingestion of microplastics may cause internal physical damage, inflammation of intestines, reduced feeding and other effects.\nWhat is also of concern, Ms Wieczorek said, was that many of these ingested microplastics have associated additives, such as colourants and flame retardants that are added to plastics during production process, and/or pollutants that are adsorbed on to the microplastics from the sea.\n“There is now evidence that some of these toxins on the microplastics can be transferred to animals that eat them, with potential harmful effects.”\nCo-author Dr Tom Doyle of the Ryan institute at NUIG, said: “It’s worrying to think that our daily activities, such as washing our synthetic clothes in our washing machines, results in billions of microplastics entering our oceans through our waste water stream that may eventually end up in these deep sea fishes.”\nThe fish were sampled from “a warm core eddy”, which is a circular current in the northwest Atlantic Ocean. Similar to what are known as ocean gyres, these currents are now thought to accumulate microplastics and that the sampled fish may have originated from a particularly polluted patch of the Atlantic.\nMs Wieczorek added: “This would explain why we recorded one of the highest abundances of microplastics in fishes so far, and we plan to further investigate the impacts of microplastics on organisms in the open ocean.”\nThe research was carried out within the Plastox project, a European collaborative effort to investigate impacts of microplastics in the marine environment under the JPI Oceans framework and supported by the iCRAG (Irish Centre for Research in Applied Geoscience) project, funded by Science Foundation Ireland.\nThe study can be read at: http://bit.ly/2EEmMHI\n—From Irish Times', 'Originally published by WWF-Indonesia and WWF-Singapore. Written by Sharon Salim.\nLast Sunday, a 9.5-metre sperm whale was found stranded off the coast in Kapota Island, Wakatobi — dead. Cause of death: unknown. But what we do know: a staggering 5.9 kg of plastic items found in its stomach.\nWWF-Indonesia’s Dewi Satriani tell us more about this appalling incident.\nBelow, exclusive information povided by WWF-Indonesia on the timeline and all the things you need to know:\n#1 The whale was found by a local on Sunday evening\nWWF-Indonesia received a report from a community member of Kapota Island in Wakatobi on Sunday evening (Nov 18) informing them about an incident of a dead whale stranded off the coast.\nThe next Monday morning (Nov 19) at 7.30 AM local time, WWF-Indonesia, Wakatobi National Park, Wakatobi Marine and Fisheries Community Academy were deployed to the scene and discovered a whale carcass suspected to be a sperm whale.\n#2 The whale was already in an advanced state of decay\nThis is the reason why it was not possible to conduct a necropsy to investigate the cause of death. In this stage of decay (level 4), the carcass was already releasing an overpowering stench and the body was not intact. A necropsy can only be done if the stranded whale is still in level 2 stage of decay; when the skin still appears tight, emitting no smell, and the eyes still shine.\n#3 Identifying the whale was still possible\nEven though the whale carcass was no longer intact, WWF-Indonesia Marine Species Specialist Dwi Suprapti conducted a photo analysis of the carcass sent by the WWF-Indonesia team and identified the stranded whale as sperm whale (physeter macrocephalus). It has a block-shaped head, narrow jaw and teeth.\nSperm whale is also the largest toothed-whale, unlike other large-sized counterparts that are toothless. The possibility of it being a baleen whale was also ruled out thanks to the absence of prominent, iconic features of baleen like the rostral ridge and ventral grooves. This further confirms the fact that the stranded whale is a sperm whale (and not a blue whale as some news outlets have reported).\n#4 Finding a stranded whale in Wakatobi waters is not common\n“Whales do migrate through Wakatobi waters. In certain seasons we can see many different kinds of whales passing through Wakatobi islands, but stranding is not common. This is the only time we’ve found a stranded whale, which is also (alarming) because we found plastics in its abdomen,” Dewi revealed.\n#5 The locals decided to cut open the abdomen of the dead whale\nThe chunks of black material (left) are plastic raffia strings. A rubber-canvas sandal (right) was among the finds.\nTo be exact: there were 115 of plastic cups (750g), 19 pieces of hard plastic (140g), four plastic bottles (150g), 25 plastic bags (270g), a nylon sack (200g), and more than 1,000 pieces of plastic raffia string (3,260g) which totals to 5.9 kg inside the stomach, according to the identification result conducted by Wakatobi Marine and Fisheries Community Academy.\n“This discovery is deeply upsetting. It is a wake-up call for Indonesia about how plastic pollution is causing irreparable damage to our oceans and marine life. We urge businesses and governments to work together to address this issue urgently to prevent further plastic leakage into our oceans.” said Dwi Suprapti, Marine Species Specialist WWF-Indonesia.\n#6 There was no crowd management or public boundaries during the incidence\nSo as not to interfere or disturb the stranded whale, public members without any Personal Protective Equipment (PPE) including eye and face protection, gloves, and coveralls were advised to remain at a distance. This is a crucial protective measure to avoid any exposure of bacteria, virus or other dangerous microorganism emitted by the mammal and transmissible to human.\n#7 The whale was buried two days after it was discovered\nIt was buried on Tuesday (Nov 20) at Kolowawa Beach, North Kapota in Wakatobi for later retrieval of the bone specimen by the Wakatobi Marine and Fisheries Community Academy.\n#8 Plastic pollution is a transboundary problem that all countries share\nAlthough it remains uncertain whether the plastics were lodging the sperm whale’s ingestion organ or caused infections, we can’t turn a blind eye to how plastics production and pollution are still growing exponentially.\nEvery year, 8 million tonnes of plastics enter the ocean. By 2050, the total mass of ocean plastic will exceed that of fish. Plastic debris kills an estimated 100,000 marine mammals annually, as well as millions of birds and fish.\n#9: Beach clean-ups are not enough to solve the problem\nA big part of marine pollution comes from land sources, including rivers. An effective response to this crisis requires a global systemic change involving businesses and governments — and supported by consumers.\n#10 We cannot solve this issue alone\nKnowing that marine plastic pollution goes beyond Indonesia and the region, the global crisis requires global solutions.\nBusinesses need to take responsibility for the full life cycle of their products and play their part in helping governments deal with this issue.']	['<urn:uuid:ad1ddfe2-8df6-481d-85ab-dd274751df17>', '<urn:uuid:d1103ef1-d057-45b2-9e68-b78458d5ce54>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	12	78	1459
37	Are both buildings designed to save energy through sustainable features?	Yes, both buildings incorporate sustainable features for energy efficiency. The CRÉVHSL headquarters uses geothermal energy for heating and air conditioning with heat recovery, and emphasizes the use of local wood. The Health Education Building is expected to use 24% less energy than state guidelines through features like LED lighting, occupancy sensors for climate control, strategic daylighting through skylights, and a thermal buffer between exterior glass walls and interior terracotta screen.	"['Sumit Singhal loves modern architecture. He comes from a family of builders who have built more than 20 projects in the last ten years near Delhi in India. He has recently started writing about the architectural projects that catch his imagination.\nRefurbishment and extension of the CRÉVHSL headquarters in Québec, Canada by Brière, Gilbert + associés architectes\nSeptember 15th, 2011 by Sumit Singhal\nArticle source: Refurbishment and extension of the CRÉVHSL head quarters\nThis project was recently awarded two prizes: best institutional building of less then 600 square meters and best concept and architectural details from CECO bois, an institution that promotes the use of wood in architectural projects.\nMontreal, August 25, 2011 – Brière, Gilbert + associés, architectes have been working for the last two years on the design and construction of the «Conférence Régional des Élus de la Vallée-du-Haut-St-Laurent (CRÉVHSL) headquarters. The headquarters are located in an existing heritage building, on an outstanding river bank site, at 88 St. Laurent Street in the old St. Timothée village now called Salaberry-de-Valleyfield. The project, completed in fall 2011, highlights both the landscape and the historical building.\nThe existing heritage building was built in 1851 and housed, until 1970, a private boy’s school. Between 1970 and recently, the building was used as St-Thimothée city hall until it was sold to CRÉVHSL. In between 1851 and the day the CRÉVHSL acquired the building, many renovations had been done to the existing building leaving it in a terrible state. The director of the CRÉVHSL is both passionate of the preservation of historical buildings and contemporary architecture.\nThe CRÉVHSL project is a testimony of the firm’s philosophy and its ability to carry out relevant and innovative concepts. It also demonstrate the firms aptitude to achieve projects whose aspirations, values and quality are deeply linked to a sense of belonging and to the importance of place and history that emerges.\nThe CRÉVHSL project is defined in two parts:\n1.) The refurbishment of the existing building which accommodates several offices and the main reception;\n2.) A contemporary extension to the rear, where the new board room and the employee’s cafeteria is located.\nThe refurbishment included mainly interior renovation and restoring to the original states the existing exterior doors and windows, and repairing the masonry and the roof. These improvements are visible from the street front. The extension highlights the views on the site and the St Lawrence River.\nThree conceptual gestures helped to achieve these goals successfully. First, the off-axis footprint, from the existing building of the extension, allows visual openings towards the river from both the new and existing building. It also opens up a maximum amount of land to the rear, between the building and the St. Lawrence River.\nSecondly, the “suspended” wooden volume, which wraps the new board room, provides an adequate response to the massive stone volume of the existing building. The suspended volume liberates physical and visual connections following the natural slope of the ground, to the St Lawrence River. Inside the board room, the suspended effect of the wooden volume is enhanced by the presence of three apparent reversed timber trusses.\nThirdly, the integration of two atriums creates open spaces to accentuate the suspended wooden volume from the inside and opens views towards the river and site from the new and existing building.\nThe project includes sustainable development measures. The use of apparent wood structure and of eastern Quebec cedar for the exterior siding aims to enhance this natural resource of Québec. In addition, the project uses geothermal energy for heating, air conditioning, linked to heat recovery.', 'Built on the site of a former parking lot, the Health Education Building is located on a prominent corner in the University of Kansas Medical Center campus – on the Kansas side of the state border with Missouri. It serves as the primary educational facility for the institution\'s medical, nursing and allied health programs.\n""As the campus continues to grow, the Health Education Building will emerge as the geographical centre and interdisciplinary resource among the existing concentration of clinical, research and educational buildings,"" the team said in a project description.\nRoughly rectangular in plan, the building consists of two wings, with a glazed connector volume running between them. Encompassing 171,744 square feet (15,956 square metres), the facility contains classrooms, simulation labs, clinical skills rooms and student life space.\nExterior walls are wrapped in glass and reddish brick. On the west elevation, the upper level of the building cantilevers over the site and reaches toward an active street. Glazed facades provide a clear view of the interior, where volumes containing labs are enclosed in a terracotta screen.\n""The cantilevered west wing acts as a lantern, providing daytime and illuminated night views of the advanced simulation labs that appear suspended inside the building,"" the team said.\n""The curving terracotta and glass enclosures of the simulation spaces invite metaphorical interpretations for human organs within the skin surrounding them, and symbolise the hands-on, progressive curriculum taught in the building.""\nThe eastern elevation, which is more opaque, features Roman brick cladding – a material the takes cues from the traditional masonry found on campus. On the ground level, the brickwork ""appears to dissolve into a lattice-like screen that runs past windows, providing views into the large learning studio inside"". A similar strategy was used at the top of the building for mechanical purposes.\nThe north side of the building features a sloping walkway, which leads to another research building and parking facilities. The team incorporated a landscaped courtyard that was influenced by the grass-covered Flint Hills region in eastern Kansas. Condensate water from the building\'s mechanical system is used to irrigate the landscaping.\nConjoined to the south side of the building is a glass-enclosed bridge, which passes over a street and connects to Orr-Major Hall, a brutalist-style educational building constructed in the 1970s.\nSpanning 250 feet (76 metres), the glazed bridge is meant to function as both a ""pedestrian conduit and social destination"". The interior offers space for studying and socialising. Large, tree-shaped columns provide structural support and visual interest.\nThe entire building offers a diverse mix of formal and informal spaces that encourage collaboration and interdisciplinary approaches to problem-solving.\n""Each floor of the building is designed to promote a sense of student camaraderie, community and teamwork,"" the team said.\nThe first and second storeys house 15-person tutorial rooms, four-person study rooms and one-person carrels. Classrooms and learning studios – including mock medical environments, such as operating theatres – are located on floors one through four. Faculty offices are situated on the third and fourth levels, while a large event space was placed on the fifth floor.\nThe central volume serves as a circulation hub, providing access to elevators and stairs. Retail spaces help activate the base of the building.\nThe centre has a number of sustainable features, including LED lighting and occupancy sensors that help control heating and cooling. Skylights that project above the ground on the north side bring natural light to learning studios in the basement level.\nOn the west side, a semi-conditioned thermal buffer is created between the exterior glass walls and the interior terracotta screen. This buffer cuts down on energy use while still providing views and daylighting.\nOverall, the university building is expected to consume 24 per cent less energy than mandated by the state energy guidelines, according to the team.\nCO Architects was founded in 1986 as a regional office of Anshen + Allen (now part of Stantec) and became an autonomous firm a decade later. Another academic project by the California firm is a copper-clad laboratory complex in downtown Phoenix that evokes jagged and striated rock formations.\nPhotography is by Bill Timmerman.\nDesign/programming architect: CO Architects\nCO Architects team: Scott Kelsey, managing principal/principal-in-charge; Paul Zajfen, design principal; Jonathan Kanda, principal for medical education and simulation; Tanner Clapham, associate/project architect; Chao Chen, architect; Michael Ly, designer\nExecutive architect: Helix Architecture + Design\nClient: University of Kansas Medical Center (KUMC)\nGeneral contractor: McCownGordon Construction\nMEP engineer: Henderson Engineers\nStructural engineer: Bob D Campbell and Company\nCivil engineer: SK Design Group, Inc\nLandscape: Land3 Studio\nLighting: Henderson Engineers\nAcoustical: The Sextant Group, Inc\nArtists: Miki Baird, Marcie Miller Gross, Jesse Small, and Jeremy Rockwell']"	['<urn:uuid:6373fc0f-0e0c-4aed-b435-df270f2396fc>', '<urn:uuid:d05741d2-20e3-4399-b7d2-6561eb997dc5>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	10	70	1371
38	What techniques connect graphite parts, and which safety measures matter?	Graphite can be joined using various techniques, including titanium-based coatings followed by brazing with conventional filler materials. This is particularly important in nuclear applications where graphite tiles need efficient heat removal through connection to water-cooled copper sheets. For safety during these operations, several measures are crucial: equipment must be inspected at the beginning of each shift, torches must be lit only with approved devices, welding cables within 10 feet of electrode holders must be without repairs or splices, and proper ventilation must be maintained, especially when working with toxic substances. Workers must also use protective equipment against ultraviolet radiation.	['Bangalore Plasmatek can, at present, deposit following coatings\non cutting tools and other components.\nAluminium, Titanium, Nickel, Copper, Stainless steel, Gold,\nSilver, Special alloys, Titanium Nitride (TiN), Titanium\nCarbide, Titanium Carbonitride (TiCN), Multilayer coatings,\nRESEARCH & DEVELOPMENT\nThe coating system at Bangalore Plasmatek, which includes vacuum\narc and magnetron sources, was developed indigenously. The\nprocesses for various coatings are also developed in house.\nBangalore Plasmatek enjoys taking up challenging research and\nVacuum brazing of ceramics and graphite to metals\nhere to download\nVacuum brazing of ceramics and graphite to\nVacuum brazing of ceramics and\ngraphite to metals\nBangalore Plasmatek Pvt.Ltd,\n129, Block –14, Jeevanmitra Colony I-Phase, Bangalore 560 078\nThis work was\ncarried out under a works contract from the Institute for Plasma\nResearch , Bhat, Gandhinagar, Gujarat.\nJOINING OF METALS TO CERAMICS AND GRAPHITE IS IMPORTANT IN MANY APPLICATIONS\nSUCH AS X-RAY TUBES, MICROWAVE DEVICES, NUCLEAR FUSION REACTORS AND SO ON.\nSOME OF THE BASIC REQUIREMENTS OF SUCH JOINTS ARE ELECTRICAL INSULATION,\nTHERMAL CONDUCTION, VACUUM COMPATIBILITY, MECHANICAL STRENGTH ETC. SEVERAL\nTECHNIQUES ARE IN USE AND NEW ONES ARE BEING DEVELOPED TO MEET STRINGENT\nREQUIREMENTS. MOLY- MANGANESE METALLISATION IS PROVEN TECHNIQUE FOR BRAZING\nCERAMICS TO METALS. RECENTLY THE SO-CALLED ACTIVE BRAZING ALLOYS HAVE BEEN\nINTRODUCED IN ORDER TO REDUCE THE NUMBER OF STEPS INVOLVED IN BRAZING.\nANOTHER TECHNIQUE, WHICH IS USED IN THE PRESENT WORK, IS TO COAT CERAMIC\nWITH TITANIUM AND THEN USE CONVENTIONAL BRAZING ALLOYS. TITANIUM IS\nDEPOSITED ON CERAMICS USING A CATHODIC ARC PLASMA SOURCE IN VACUUM.\nINDIGENOUSLY PREPARED COPPER SILVER EUTECTIC ALLOY FOILS ARE USED AS FILLER\nMATERIAL. BRAZING IS CARRIED OUT UNDER HIGH VACUUM AT ABOUT 9000C. THESE\nTECHNIQUES HAVE BEEN USED TO BRAZE STAINLESS STEEL-ALUMINA, TITANIUM-\nALUMINA, AS WELL AS COPPER-GRAPHITE. IN ALL THESE CASES MECHANICALLY STURDY\nJOINTS HAVE BEEN OBTAINED. THESE TECHNIQUES CAN ALSO BE USED TO BRAZE METALS\nTO METALS AND CERAMICS TO CERAMICS. SOME OF THE RESULTS OF THE WORK IN\nPROGRESS WOULD BE PRESENTED.\nKeyword: Vacuum; Brazing; Ceramics; Graphite.\nBrazing is a technique of joining two materials using a\nfiller material whose melting point is below the melting\npoints of the materials to be brazed. A typical brazing\nprocedure involves cleaning of the surfaces to be joined,\ninterposing a filler material between the two surfaces,\nholding the parts to be brazed with suitable fixtures and\nheating to a temperature slightly above the melting point of\nthe filler material. Heating cycle should ensure that\nthermal equilibrium is maintained between the filler\nmaterial and the components to be brazed. The components\nreact with atmospheric gases like nitrogen, oxygen, moisture\netc. When this is not acceptable brazing will have to be\ncarried out under a protective environments like inert gas,\nhydrogen gas or vacuum. Brazing is possible only if the\nmolten filler material wets the surfaces and flows properly.\nThis depends on the properties of the filler material as\nwell as the surfaces to be brazed. Commercially a number of\nfiller materials are available in the form of wires, foils,\npowders and pastes. Choice depends on the properties of the\nmaterials to be brazed and the temperature and other\nenvironmental factors in which the brazed components are to\nBrazing of ceramics\nCeramics are widely used in industries due to their\nelectrical, thermal and mechanical properties. But joining\nof ceramics to metals and to themselves is not straight\nforward. There are basically two problems. First, the usual\nbrazing fillers do not wet the surfaces of ceramics. Second,\nthere is a big difference in the thermal expansion\ncoefficients of metals and ceramics. This induces tremendous\nstresses in the brazing process which can lead to cracking.\nSpecial techniques have been developed for brazing ceramics.\nMoly-manganese metallisation is the standard practice for\nbrazing ceramics. Here a paint of the refractory metal\nmolybdenum with 10% manganese is applied to the ceramic and\nsintered around 1400oC. In this process manganese oxidises\nand diffuses into ceramic forming transition layer between\nthe ceramic and the molybdenum layer. This reduces the\nthermal mismatch between ceramic and molybdenum . It is\nthen protected from oxidation by plating with nickel.\nBrazing is then carried out using conventional filler\nmaterials either in vacuum or in an inert atmosphere.\nActive brazing is a relatively new technique. A family of\nbrazing alloys called active brazing alloys are made by\nadding a small percentage of titanium or vanadium to\nconventional filler material compositions. Brazing is\ncarried out under high vacuum in clean conditions. During\nbrazing titanium is oxidised by the ceramic forming titanium\noxides and liberating some aluminium atoms. This interlayer\nforms some kind of chemical bridge between ceramic and the\nmetal [2 ]\nAn alternate way is to have a titanium coating on the\nceramic and then carry out regular brazing. At high\ntemperatures titanium reacts well with ceramics as well as\nother metals. Usual brazing alloys wet titanium surface well\nleading to a good brazed joint.\nBrazing of graphite\nGraphite is an important material in nuclear industry - both\nthe conventional fission reactors and the fusion reactors\nunder development. In the latter it is used as a first wall\nmaterial to minimise impurities in the reactor. The graphite\ntiles used there face enormous heat load. Unless there is an\nefficient heat removal mechanism in place, temperatures rise\nbeyond tolerable levels. The practice is to attach water\ncooled sheets of copper or copper alloys to graphite tiles.\nIt is imperative that thermal contact between graphite and\ncopper be very good in order to reduce thermal resistance.\nThis can be achieved only by brazing the two.\nBrazing of graphite is as problematic as brazing ceramics.\nIt has very low thermal expansion coefficient. It reacts\nwith very few materials to form carbides.\nBrazing of graphite to TZM – an alloy of molybdenum - has\nbeen realised [3, 4]. Brazing filler materials like\nCu-Ti-Ag, Cu-Ti, Ti, Zr and Zr alloys have been used.\nRussians  have developed several filler alloys in the\nform of flexible ribbons for brazing copper and beryllium\nbased alloys with graphite. There are several groups engaged\nin developing suitable technologies.\nOne can see from the above discussion that titanium promotes\ngood brazing in ceramics as well as graphite. This is due to\nthe fact that at high temperatures titanium easily reacts\nwith oxygen, carbon and many metals. It was therefore\ndecided to coat both ceramics and graphite with titanium and\nthen attempt brazing with usual brazing fillers.\nThe coating facility at Bangalore Plasmatek was used\nfor titanium coating. It consists of a rectangular\nvacuum chamber of 1000x1000x600 mm (fig.1). A\nmagnetically steered titanium cathode is used for\nproducing intense titanium plasma in high vacuum.\nAlumina ceramic blocks of >99% purity of size\n70x13x7mm and high density graphite blocks of about\n25x25x25mm and other sizes are used for trials. An\narea of 68x5 of one of the 70x7 faces as well as all\nthe remaining faces were masked before coating. Thus\na rectangular frame of 1 mm width was coated on one\n70 x 7 mm face of ceramic piece. Similarly, all the\nfaces except one face of the graphite block were\nalso masked. Alumina (95% purity) ceramic strips of\nsize 5x0.3x50 mm are also coated. Cleaned samples\nare mounted inside the coating chamber with suitable\nfixtures. Samples were further cleaned by argon glow\nTitanium plasma was produced by striking the arc on\nthe cathode. An arc current of 90 A was used. The\narc consists of multiply charged titanium ions up to\nabout 150 ev energy. This results in very dense and\nadhesive coatings. Coating thickness was between 10\nto 15 microns.\nCeramic blocks were brazed to titanium blocks of\nsize 100x30x40mm with a through slot and a step to\nplace the titanium coated ceramic block. Eutectic\nalloy (CuSil) of copper(28%) and silver was taken in\nthe shape of thin wires and placed between titanium\nand coated ceramic surfaces.\nFigure 1. A view of the coating chamber\nThe coated ceramic disc was brazed to stainless\nsteel (SS 304) disc while graphite block was brazed\nto a copper block. The same eutectic alloy CuSil was\ntaken in the form of thin foils and interposed\nbetween surfaces of the materials to be brazed.\nBrazing was carried out in a vacuum furnace. A\npressure of 10-5 mb was maintained. The temperature\nwas raised at the rate of 100C per minute. It was\nheld constant at 7500C for ten minutes and then\nraised to 8800C at 200C per minute. Temperature was\nkept constant at 8800C for two minutes and then\nAll brazed samples showed evidence of good flow of the\nfiller material. Some are shown in Figs.2 and 3.\nMechanical strength of the joints is excellent. The\njoint between the rectangular ceramic block and the\ntitanium block was meant to be vacuum tight. But it\nshowed leaks. This is attributed to rectangular geometry\nand sharp corners rather than any inherent brazing\nIt has been clearly demonstrated that a coating of 10-15\nm of titanium is sufficient to braze ceramics and\ngraphite to various metals using an eutectic alloy of\ncopper and silver. For vacuum joints special care may\nhave to be taken in the presence of sharp corners or\nother special features. The same technique can also be\nused to braze ceramic to ceramic, graphite to graphite,\nceramic to graphite and metals to metals which may be\nFigure 2. Ceramic piece brazed to titanium block\n(a) Graphite block brazed to copper\n(b) Ceramic strip brazed to stainless steel\nD.M.Mattox & H.D.Smith, Ceram Bull.\nA.Murari, H.Albrecht, A.Barzon,\nS.Curiotto and L.Lotto, Vacuum 68(2003)321-328].\nI.Smid et.al. JNM171(1990)165\nB.A.Kalin et.al. Fusion Eng. & Design\nhere to download\nVacuum brazing of ceramics and graphite to', 'Welding, cutting and brazing present a host of safety and health risks, including fire, explosion, toxic atmospheres, ultraviolet radiation, and electric shock. If you work on sites where “hot work” is performed, you need to be familiar with the relevant standards, even if your employees perform no such work themselves. Let’s look at the basics.\nMoving, Transport, and Storage of Compressed Gas Cylinders: Cylinder valves must be kept closed when work is not being performed, when the cylinders are empty, or they are being moved.\nA chain, cylinder truck, or other steadying device should be used to keep cylinders from being knocked over while in use. Cylinders should be secured in a vertical position, when being transported in a vehicle. Regulators must be removed, and valve protection caps must be in place, before cylinders are moved. Employees should never lift cylinders by their valve protection caps.\nCylinders should be kept a safe distance from welding operations, or behind fire-resistant shields. They must not be placed in confined spaces, or where they might become part of an electrical circuit.\nCylinders, full or empty, must not be used as rollers or supports.\nUse of Fuel Gas: Before being connected to a regulator, a cylinder valve should be “cracked” (opened slightly and closed at once) by a person standing to the side of it. Do not “crack” the valve where the gas would reach welding work, sparks, flame, or another source of ignition.\nCylinder valves should always be opened slowly, and those on fuel gas cylinders opened not more than 1-1/2 turns. If a special wrench is required, it must be left in position, to allow the fuel gas flow to be shut off quickly. In the case of manifolded or coupled cylinders, at least one such wrench shall be kept close by. Nothing should be placed on top of a fuel gas cylinder that might damage the safety device, or impede the quick shutoff of the valve.\nFuel gas should be used through devices equipped with shutoff valves only if the pressure is reduced through a suitable regulator attached to the cylinder valve or manifold. Before a regulator is removed from a cylinder valve, the valve must always be closed, and the gas released from the regulator.\nIf gas is found to be leaking from the valve stem, the valve shall be closed, and the gland nut tightened. If that does not stop the leak, the cylinder must be taken out of use, tagged, and removed from the work area.\nIf fuel gas is leaking from the cylinder valve, and cannot be shut off, the cylinder should be properly tagged, and removed from the work area, except if a regulator attached to the cylinder valve will stop the leak through the valve seal.\nIf a leak should develop at a fuse plug or other safety device, the cylinder should be removed from the work area.\nFuel gas and oxygen manifolds: Fuel gas and oxygen manifolds must bear the name of the substance they contain in letters at least 1” high, painted on the manifold, or on a sign permanently affixed to it. The manifolds must be in safe, well-ventilated and accessible locations (never within enclosed spaces).\nOxygen and fuel gas hoses must not be used interchangeably, and a single hose having more than one gas passage must not be used. Hose couplings must be of a type that cannot be unlocked or disconnected without a rotary motion. Boxes used for the storage of gas hose should be ventilated, and hose must be inspected at the beginning of every shift, and removed from use if damaged or malfunctioning.\nTorches: Torches must be inspected at the beginning of each shift for leaking shutoff valves, hose couplings and tip connections. They may be lighted by friction lighters or other approved devices, but not by matches, or from hot work.\nRegulators and Gauges: Oxygen and fuel gas regulators and their gauges must be inspected regularly, and kept in good working order.\nOil and Grease Hazards: Cylinders, cylinder caps, valves, couplings, regulators, hose and apparatus must be kept free from oil and greasy substances, and must not be handled with oily hands or gloves.\nWelding Cables and Connectors: Use only cable without repair or splices within 10 feet from the cable end to which the electrode holder is connected. Cables with standard insulated connectors, or with splices having an insulating quality equal to that of the cable, are permitted. Cables in need of repair must not be used.\nSafe Operation of Equipment: All equipment should be operated in a manner consistent with the manufacturer’s instructions. OSHA requires that, when electrode holders are to be left unattended, the electrodes must be removed, and the holders placed or protected so that they cannot make electrical contact with employees or conducting objects.\nWhen the cutter or arc welder has occasion to leave his work, or to stop work for any appreciable length of time, or when the arc welding or cutting machine is to be moved, the equipment’s power supply must be shut off.\nWhenever practicable, arc welding and cutting operations are to be shielded by noncombustible or flameproof screens, as protection from the direct rays of the arc.\nNaturally, faulty or defective equipment shall be reported to the supervisor, and taken out of service until repaired, or determined to be in good operating condition.\nVentilation and Protection: Hot work operations involving toxic substances must not be performed without adequate ventilation. Employees exposed to the same atmosphere must be protected in the same manner as the welders and burners themselves.\nMechanical ventilation can be either a “general mechanical ventilation system” or a “local exhaust system.” General mechanical systems must produce the number of air changes necessary to maintain welding fumes and smoke within safe limits (as defined in subpart D of 29 CFR 1926).\nLocal exhaust ventilation consists of freely movable hoods, placed as close as practicable to the work. Such ventilation must remove fumes and smoke, so as to keep concentrations within safe limits.\nLocal exhaust ventilation or air line respirators are required, when hot work is to be performed in an enclosed space, and involves lead (other than as an impurity), metals coated with lead-bearing materials, cadmium, or mercury. Both local exhaust ventilation and air line respirators are required, when the work involves beryllium-containing base or filler metals.\nContaminated air from a workspace shall be exhausted into the open air, or otherwise clear of the intake air source. All air replacing that which has been withdrawn must be clean and respirable.\nOxygen must not be used for ventilation, comfort cooling, blowing dust from clothing, or for cleaning work areas.\nConfined spaces: Generally, the foregoing ventilation requirements must be observed whenever hot work is done in a confined space.\nBy way of exception, if ventilation cannot be provided without blocking the means of access, employees in the confined space shall be protected by air line respirators, with an employee outside the confined space being assigned to maintain communication with those working inside it, and to provide assistance in case of an emergency.\nWhen an employee must enter a confined space through a manhole or other small opening, means (e.g., safety belts and lifelines) must be used, and an attendant, with a pre-planned rescue procedure, stationed outside, to extricate the welder or cutter should the need arise.\nProtective equipment: Employees performing hot work, or working nearby, must wear suitable eye protective equipment. All who are exposed to radiation must be covered completely, to prevent harm from ultraviolet rays.\nFire prevention: Articles to undergo “hot work” must be moved to a designated location whenever practicable. No hot work shall be done where the application of flammable paints, or the presence of other flammable compounds or dust concentrations, creates a hazard.\nFor more complete details concerning safety and health requirements for “hot work,” refer to Subpart J of 29 CFR 1926, and the publications and other OSHA standards referred to in the same.']	['<urn:uuid:c300746e-94cf-452c-9373-78b4b61a6404>', '<urn:uuid:d98f5476-a7b6-4885-a0eb-a6041295832f>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	10	99	2930
39	reduce truck emissions at ports what hybrid technologies are available and which best practices reduce port pollution	For port operations, hybrid trucks like the DAF CF combine electric motors for zero-emission city driving with efficient diesel engines for longer routes. Additional emission reduction practices at ports include decreasing fleet age, reducing idle time, participating in the EPA SmartWay Program, and designating truck routes that avoid populated areas to reduce pollution exposure.	"['DAF starts field test with CF Hybrid\nZero emissions in the city and long range outside urban areas\nDAF Trucks has started field testing the CF Hybrid with the aim of gaining experience in daily use. The Dutch transport operator Peter Appel is now using two of these innovative trucks to supply supermarkets in the heart of the Netherlands. The DAF CF Hybrid is 100% electric in urban areas and uses clean diesel technology out of town. The innovation truck combines best of the both worlds by driving with ‘zero emissions’ in town, thereby ensuring both long range and flexibility outside urban areas.\nThe DAF CF Hybrid innovation trucks are equipped with the extremely efficient 10.8 litre PACCAR MX-11 diesel engine (330 kW/450 hp), a ZF electric motor\n(75 kW/100 hp, peak power: 130 kW/175 hp) and a special ZF TraXon gearbox for hybrid powertrains.\nRecharging while driving or at charging station\nThe electric motor gets its energy from a 85 kWh battery pack, which recharges when the diesel engine is being used. During diesel operation the electric motor functions as a generator and delivers energy to the battery pack. In the future it will also be possible to charge the battery at a (fast) charging station.\nWhen the battery is fully charged, the DAF CF Hybrid –– has an electric range of 30 to 50 kilometres, depending on the total weight of the truck-trailer combination, which is more than enough to drive into and out of urban areas without producing any tailpipe emissions.\nSmart energy management\nOutside of town the CF Hybrid is powered by the clean and efficient PACCAR MX-11 diesel engine, which offers a long range. In addition, the hybrid technology delivers extra savings on fuel consumption thanks to smart energy management. The energy that is generated by the engine brake as well as Down Hill Speed Control is used by the electric motor to support the diesel engine. This is beneficial both in terms of fuel consumption and CO2 emissions.\nSuitable for longer journeys\n“Together with our client, Albert Heijn, we are always looking for ways to reduce our CO2 footprint,” says Marcel Pater, fleet manager at Peter Appel Transport, which boasts a total of 680 trucks. “While the electric trucks are ideal for urban distribution, the hybrid technology is preferable for longer routes. In town the DAF CF Hybrid operates fully electric, and the truck’s diesel engine means it can travel to and from the different distribution centres within the Netherlands and abroad without any problems.”\nDaily use by clients\n“With a cleaner future in mind, for DAF there is no one single technological solution for the broad spectrum of transport requirements,” according to Ron Borsboom, executive director of Product Development. “That’s why we are testing different technologies. Fully electric is a good alternative for urban distribution, clean diesel technology an excellent option for longer distances – partly due to new types of fuel – and for the long term we are having a closer look at hydrogen as well. With the field test of the CF Hybrid, we want to assess not only its electric/diesel technology performance but also how suitable it is in terms of daily use by our clients.”', ""Drayage Truck Best Practices to Improve Air Quality\nDrayage trucks play an important role in port operations, the economy and air quality. Drayage trucks are generally diesel-fueled, heavy-duty (Class 8) trucks that transport containers and bulk freight between the port and intermodal rail facilities, distribution centers, and other near-port locations. This page describes best practices, listed below, that port authorities, drayage truck drivers, and other port operators can adopt to reduce dray truck emissions.\n- Decrease the average age of the fleet\n- Reduce idle and creep time\n- Participate in the EPA SmartWay Program\n- Consider complementary rail and marine operations\n- Designate truck routes that avoid at-risk populations\nClean Air Best Practices: Appropriateness and Effectiveness\nThis webpage is one in a series of webpages that provide information on best practices at ports to reduce diesel pollution and associated health impacts. While the examples in these webpages are not exhaustive, they are intended to highlight some of the more effective strategies that have been adopted by U.S. ports. The appropriateness and effectiveness of these strategies will vary for different ports based on many factors, including type of operation, fleet makeup, local air quality and pollutant exposure, and community and port priorities. These webpages will be updated over time as new clean air practices emerge and information evolves.\nNewer trucks pollute significantly less than older model years due primarily to improved air emission control technologies installed on these trucks. Since 2007, diesel trucks have been equipped with diesel particulate filters (DPFs) and since 2010, many diesel trucks have been equipped with selective catalytic reduction (SCR) systems. Alternative fuel drayage trucks are also an option to consider, especially as new truck models with low NOx tailpipe emissions (e.g., natural gas and liquid petroleum gas) and zero tailpipe emissions (e.g., electric and fuel cell) come into the market.\nRetiring older trucks and engines and replacing them with newer vehicles with the latest emission control technologies plays a major role in reducing air pollution at and near ports. While replacing any older truck with a newer one will yield emissions benefits, you can maximize benefits by replacing the oldest vehicles with the most mileage first and choosing a replacement truck with a model year 2014 or newer engine. Consistent with requirements in EPA’s DERA program, the vehicle you’re replacing should have accumulated at least 7,000 miles per year for the past two years and have at least three years of remaining life to ensure emissions benefits.\nChoosing the Right Trucks and Ensuring Good Maintenance\nWhile newer model year trucks are certified to meet cleaner emission standards, other important considerations when choosing a replacement truck include the following:\n- Tractors with aerodynamic fairings and low rolling resistance tires will save drivers fuel and reduce emissions. SmartWay designated tractors and trailers can maximize these efficiency savings, reducing fuel use by up to 20% (2,000 to 4,000 gallons of diesel per year).\n- Proper maintenance is critical to keeping trucks on the road. Requiring dealers to perform all engine checks and maintenance — such as compression and fuel injector tests, fuel and diesel exhaust fluid filter changes, and DPF cleaning — prior to sale will help drivers operate more fuel efficiently and avoid unforeseen costs. Monitoring trucks for visible smoke or illuminated check engine lights can help identify needed maintenance or repairs.\nAlternatives to Buying a Newer Truck\nFor older drayage trucks that you don’t plan to replace, consider installing diesel oxidation catalysts and low-rolling resistance tires. Both technologies can reduce emissions, and low-rolling resistance tires can save fuel costs.\n- EPA and Port Everglades Partnership: Emission Inventories and Reduction Strategies (PDF)(10 pp, 1.2 MB, June 2018, EPA-420-S-18-002)\n- EPA Clean Diesel Technologies\n- SmartWay Designated Tractors and Trailers\n- SmartWay Verified Low Rolling Resistance (LRR) New and Retread Tires\n- Zero-Emission Drayage Trucks: Challenges and Opportunities for the San Pedro Bay Ports (PDF)(60 pp, 2.1 MB, October 2019)\n- Port of Los Angeles Zero Emissions Technologies\n- How to Identify Low NOx Certified Engines Factsheet (PDF)(3 pp, 635 K, January 2021, EPA-420-F-21-002)\nTips on Performance Targets and Data Collection\n- Consider setting goals to increase the average engine model year of fleets servicing the port, or the percentage of the fleet that has a certain model year engine or newer.\n- Track the engine model year of the fleet of trucks that service the port via a truck registry and/or use Radio Frequency Identification (RFID) tags, which are devices attached to trucks that transmit data via a radio signal to a receiver or reader typically located at the port or terminal gate.\n- Information and advice on determining engine model year can be found in the EPA Port Emissions Inventory Guidance: Methodologies for Estimating Port-Related and Goods Movement Mobile Source Emissions (PDF)(233 pp, 5 MB, September 2020, EPA-420-B-20-046) and this guide to determine engine model year from the California Air Resources Board.\nMany ports have clean truck programs that use grants and other funding to provide financial incentives to improve air quality through truck replacement. Below are a few example programs.\n- Port of Baltimore Dray Truck Replacement Program Video\n- Massport's Air Emission Reduction Efforts\n- Mid-Atlantic Region Drayage Truck Replacement Program\n- Port of Los Angeles Clean Truck Program\n- Port of Long Beach Clean Truck Program\n- Port Authority of New York New Jersey Truck Replacement Program\n- Georgia Ports Authority Drayage Truck Replacement Program\nTurning off engines and minimizing idle and creep time can reduce air pollution and save money. Not only does unnecessary idling waste fuel, but it causes wear and tear on the engine that requires more frequent maintenance.\nIn addition to general idling policies, there are many strategies port operators can employ to reduce idle and creep time, including developing appointment systems; automating gates; extending gate hours; and operating during off-peak hours. These operational strategies can reduce emissions and traffic congestion while decreasing truck turn times at ports (i.e., transaction time to enter, load cargo, and depart the port).\nOperating during off-peak hours can increase flow and efficiency and reduce the impacts of diesel exhaust during peak ozone hours. Note that there are concerns associated with extended operating hours that include the additional labor needed to staff the terminals and gates at ports, as well as contract restrictions on off-peak labor. Extended hours may also be a concern to surrounding communities due to increased traffic during off-peak hours and may be restricted by noise or other local ordinances, so ports should seek community input when considering off-peak hour operations.\n- EPA List of Verified Idle Reduction Technologies for Trucks\n- Freight Advanced Traveler Information System (FRATIS) to promote urban freight mobility\n- Port Everglades On-port Truck Idle Reduction Scenario Analysis (PDF)(135 pp, 2.81 MB, June 2018, EPA-420-R-18-013)\nTips on Performance Targets and Data Collection\n- Consider setting goals to decrease the average time trucks spend creeping and/or idling.\n- Measure the average time a truck spends creeping or idling around port grounds as well as in queues to enter the port, following EPA Port Emissions Inventory Guidance: Methodologies for Estimating Port-Related and Goods Movement Mobile Source Emissions (PDF)(233 pp, 5 MB, September 2020, EPA-420-B-20-046).\n- If average truck creep and idle time is not available, setting goals to reduce the average turn time can be a good surrogate for reducing creep and idle time.\n- GCT Bayonne’s Drayage Truck Appointment System\n- Port of Los Angeles Terminal Equipment Idling Reduction Program (PDF)(93 pp, 4.6 MB, November 2017)\nEPA's SmartWay program helps truckers increase efficiency and fuel economy and provides them the documentation to prove it, giving them an advantage with prospective and current clients. If dray truck fleets that serve the port are not SmartWay partners, port operators can encourage them to join. Port operators also can encourage fleet operators to utilize SmartWay tools and resources to measure and improve performance. Additionally, port operators can reach out to known cargo owners and customers who are SmartWay shippers to encourage the dray truck carriers they work with to join and improve their performance.\n- SmartWay Carrier Application\n- SmartWay Truck Carrier Partner Resources\n- List of SmartWay Partners\n- SmartWay Carrier Performance Ranking List\nTips on Performance Targets and Data Collection\n- Consider setting a goal to increase the percent of dray trucks that serve the port that are SmartWay partners, and track progress by using SmartWay’s list of partners.\n- For those dray truck operators that are SmartWay partners, set goals to increase the performance ranking of each dray truck carrier that serves the port, and track progress using the SmartWay Carrier Performance Ranking List.\n- SmartWay Carrier Performance Rankings: Use this list to view the current SmartWay dray truck partners, along with the rest of the SmartWay carrier and 3rd Party Logistics partners.\n- SmartWay Dray Truck High Performers: This list contains the top 15% of all SmartWay dray truck partners for particulate matter and nitrogen oxides emissions.\nIncreasing use of locomotive and short-distance water transport operations can help a port increase the fraction of cargo moved by the most energy-efficient modes, alleviate landside congestion and reduce overall emissions, especially if combined with technologies and policies to minimize locomotive idling and barge emissions.\n- EPA and Port Everglades Partnership: Emission Inventories and Reduction Strategies (PDF)135 pp, 2.81 MB, June 2018, EPA-420-R-18-013)\n- Maritime Administration (MARAD's) Marine Highway Program\nTips on Performance Targets and Data Collection\n- Consider setting a goal to increase the amount of cargo moved by rail and short-distance vessels (e.g. as a percentage of overall cargo or ratio to cargo moved on trucks).\n- Track progress by measuring cargo (in tons or TEUs) moved in each mode.\n- Georgia Ports Authority: Inland Ports\n- James River Barge Service\n- North Atlantic Marine Highway’s Barge Service\nPorts can have a major impact on the air quality in nearby communities, particularly through increased truck traffic in the area. Port-related truck traffic increases emissions and air pollution, and can create additional noise and danger to people in the nearby area, particularly when trucks use residential side streets or other restricted roads. Port operators can work with community leaders and local planning and environmental agencies to develop designated truck routes that address many of these issues. Routes that avoid residential communities can reduce exposures to truck-related air pollution, as well as reduce the nuisance of truck noise for residents and increase safety. For example, it may be possible in some cases to route existing or projected traffic away from populated areas to an industrial setting (e.g., truck only routes). When trucks must be routed through or adjacent to communities, physical structures like sound walls and vegetative barriers between the road and the at-risk populations may be beneficial to reduce exposure to air pollutants (see the factors below concerning use of physical barriers).\nPort operators can also work with managers of buildings where at-risk populations spend significant amounts of time to implement mitigation strategies to reduce adverse health effects from exposures to air pollution, such as installing improved air filtration units within the buildings.\nWhen evaluating existing and alternative truck routes, factors that should be considered include:\n- Distance between the truck route and locations where potentially vulnerable populations, such as children and the elderly, spend significant amounts of time. Air pollutants emitted by trucks can be especially high within the first 500 feet of the road, so special attention should be paid to the number of these sensitive locations within this distance. These sensitive locations can include schools, daycares, parks, playgrounds, residences, health care facilities, and other public buildings like libraries.\n- Existence of physical barriers between these sensitive locations and the road. Research suggests that sound walls can reduce concentrations of traffic-related air pollutants immediately downwind of a roadway, although the extent of this reduction can vary by the wall height, length and distance from the road. Such barriers may also increase concentrations in the air on and immediately over the road as well as locations upwind and near the edges of the structure. If properly designed, vegetation barriers can be used to reduce near-road air pollution, either alone or in combination with solid structures like sound walls.\nAir quality modeling personnel at EPA Regional Offices and state and local air agencies can be consulted to help evaluate air pollution impacts to sensitive locations along existing and proposed truck routes.\nIt is also good practice to assess the rate of dray truck use of designated truck routes that avoid at-risk populations and to conduct outreach to truck operators to encourage greater usage.\n- Ports Primer: Chapter 5.1 Goods Movement and Transportation Planning for Truck Routes\n- Traffic Planning in Port Cities: International Transport Forum (PDF)(26 pp, 2.3 MB, September, 2018)\n- Recommendations for Constructing Roadside Vegetation Barriers to Improve Near-Road Air Quality\n- Best Practices for Reducing Near-Road Pollution Exposure at Schools\n- Near Roadway Air Pollution and Health: Frequently Asked Questions (PDF)(9 pp, 503 K, August 2014, EPA-420-F-14-044)\n- Research on Near Roadway and Other Near Source Air Pollution\n- Community-Port Collaboration Toolkit\n- Community-Port Collaboration Resources\n- Discussion of Truck Infrastructure Freight Routes in New Orleans Plan for the 21st Century (PDF)(18 pp, 1.1 MB)\n- West Oakland Truck Management Plan (PDF)(43 pp, 6.0 MB, May 2019)\n- Los Angeles County Strategic Goods Movement Arterial Plan (PDF) (30 pp, 1.5 MB, May 2015)""]"	['<urn:uuid:16fe8cab-5591-4c8e-8560-27bdf9b4bdd7>', '<urn:uuid:4015f349-0d71-42b8-803e-e5fb3d5123d7>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	17	54	2765
40	I'm planning a trip to Greece. How big were ancient Messene's walls?	The city walls of ancient Messene were impressive in scale, with an original length of 9 kilometers and a height of 9 meters. They were fortified by 30 square or horseshoe-shaped guard towers, which had doors allowing access to a protected walkway on top of the wall. These walls, likely constructed when the city was re-founded in 369 BC, are considered some of the best preserved city walls from ancient Greek buildings.	"[""Messene#by P. Diem\nText and all pictures by the author, 2008\nMessene (Greek: Μεσσήνη Messini), officially Ancient Messene, is located some 30 km north of the city of Messini near Kalamata. Intensive research in the old site was only started recently. It is now one of the major archeological excavation sites in Greece.\nMost of the area of Ancient Messene contains the ruins of the large classical city-state of Messene re-founded by Epaminondas in 369 BC, after the battle of Leuctra that ended Sparta’s military domination of large areas of Greece. Epaminondas invited all the families that had gone into exile from Messenia during its long struggle with the military state of Sparta to return to their native land. This new Messene, today's Ancient Messene, was constructed over the ruins of Ithome, an ancient city originally of Achaean Greeks (one of the four major tribes of ancient Greece) that had been destroyed earlier by the Spartans.\nIn the center of this old city there are the remains of the theater, the 40 m wide house with the wells of Arsione who – according to legend - was the daughter of the Messenian king Leukipp and mother of the god Asklepisos. Adjacent to it a part of a hall has been excavated, probably the northern part of the agora. The city walls had an original length of 9 km and a height of 9 m, fortified by 30 square or horseshoe-shaped guard towers with doors admitting passage to a protected walkway on top of the wall. They city walls were probably erected when the city was re-founded. They belong to some of best preserved city walls of ancient Greek buildings.\nThe most important building complex is the Aslepieion, usually thought to be built in 215 BC. There is also an impressive ring-shaped temple (Periptros) in Doric style with 12 columns. An altar room in front of the temple is in a 66 by 72 m rectangular yard. This yard is surrounded on all sides by two-span halls with 21 or 23 columns, and additional columns further inside. The columns bear fascinating capitals with motives ranging from erotic, to flower garlands, swags, to heads of animals.\nThe halls are extended by additional rooms. In the east, next to the main entrance (Propylon) there is an assembly room ((Ekklesiasterion) and two further large halls. In the west there are a number of smaller rooms, often wide open to the main hall (Exedren). In these rooms a number of statues are found. One room was used to worship Artemis. About 200 m downhill recent excavation unearthed a huge stadium, now the most impressive ruin of the city. One enters the stadium in its northwest corner through a Doric style Propylon. Directly outside the entrance a number of graves of obviously important persons have been found. The norther part of the stadium is bounded over a length of 110 meters on all three sides by halls: many columns have been restored in this sections. The northern part of the running track is bordered by steps of stone arrange in a horse-shoe form. The stand from where events were watched is located in the south and had no seats. Today (2008) it is covered (and protected) by a layer of grass. On the other side, near the starting area is a heroon, a shrine for heroes.""]"	['<urn:uuid:f813c08f-59ea-4e2e-8888-c0e96e94c5ee>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	12	72	560
41	What is strategic planning and how do KPIs help achieve business goals?	Strategic planning is a road map that describes how a business can best meet its objectives and carry out its mission. It involves determining decisions and actions to achieve organizational goals. Key Performance Indicators (KPIs) help in this process by monitoring progress towards these strategic objectives. KPIs can improve strategy execution by aligning business activities with strategic goals and provide management a way to monitor core activities rather than just financial outcomes. They typically include both financial measures based on income statements and non-financial measures related to customers, employees, operations, and quality.	"['Saturday, 22 April 2017\nStrategic planning involves the path forward for an organization and determining the decisions and actions a business should take to achieve organizational goals. Therefore, put simply strategic planning is a road map or path the organization or business takes in order to achieve its goals and objectives. It thus describes the process a business uses to determine how it can best meet its objectives and carry out its mission.\nThursday, 16 March 2017\nPlanning to join in the business world? Then you need an idea what a SWOT analysis is. It has existed for many years now. In business, a SWOT analysis is taken as one of the most essential tools. Every company uses SWOT analysis when assessing the industry where it operates from. Also, other companies this tool to be able to come up with competing strategies. These help the keep up with the cutthroat competition in the market.\nSaturday, 25 February 2017\nAwareness is a key differentiator in the business world. The firm that is unaware of anything internally or externally stands at an immediate disadvantage, since it cannot plan to avoid making critical errors, or may even become subject to public criticism leading to a fall in its image and brand value. Many tools have been utilized by firms for the purpose of building awareness. One such tool is the business SWOT analysis. This tool allows the business to determine its Strengths, Weaknesses, Opportunities and Threats. The process is part of the business’ Strategic Planning Process in which business goals and objectives are formulated. Here are four essential elements of business strategy that determine its success:\nMonday, 16 January 2017\nKey performance indicators are performance measures that indicate progress towards a desirable outcome and are commonly used to help companies effectively manage and grade their progress. ‘Strategic key performance indicators monitor the implementation and effectiveness of an organization’s strategies and determine the gap between actual and targeted performance.\nMonday, 19 December 2016\nIf you are planning to get yourself involved in the business world, you should know what SWOT analysis is and its importance. SWOT analysis has been around for many years now. To be specific, people started using this tool around the 1960s. It was developed by Albert Humphrey of Stanford University which was actually based on the Team Action Model research project. Basically, SWOT analysis is considered to be one of the most essential tools that almost all companies use to carefully assess the industry. In addition, companies also use such tools to be able to come up with some strategies to keep up with the tight competition in the market.\nFriday, 18 November 2016\nWhether you are running a business or starting a new enterprise, you must be aware of the SWOT analysis. It is a process involving the assessment of key strengths, weaknesses, threats, and opportunities faced by your brand in the market. Most of the businesses commit a mistake by restricting this SWOT analysis to only a new product and they fail to capitalize on their brand’s valuation. However, when taking up a SWOT analysis, you should have a wider perspective. Consider both the products and brand for having an actionable strategy for the business as a whole. Here is how to do SWOT analysis for enhancing your brand value.\nFriday, 4 November 2016\nWhether you are breaking into an industry with a new product or are a well-established business looking to promote a potential idea, there is certainly a credible cause for doing your ‘homework’. The objective could be to understand which of your marketing campaign options will work best for the target audience and location. In order to determine this there must be some analysis of factors first which can help isolate options, spend money effectively and save time and energy on the available resources.', ""What are they?\nA key performance indicator (KPI) is a measure used to reflect organisational success or progress in relation to a specified goal.\nThe purpose of KPIs is to monitor progress towards accomplishing the strategic objectives that are typically communicated in a strategy map.\nKPIs are typically included in a reporting scorecard or dashboard that enables top management, the board or other stakeholders to focus on the metrics deemed most critical to the success of an organisation.\nFinancial KPIs are generally based on income statement or balance sheet components, and may also report changes in sales growth (by product families, channel, customer segments) or in expense categories. Non-financial KPIs are other measures used to assess the activities that an organisation sees as important to the achievement of its strategic objectives. Typical non-financial KPIs include measures that relate to customer relationships, employees, operations, quality, cycle-time, and the organisation’s supply chain or its pipeline. Some prefer to use the term ‘extra-financial’ rather than non-financial, suggesting that all measures that contribute to organisational success are ultimately financial. In addition to financial and non-financial, other common categorisations of performance indicators are quantitative versus qualitative; leading or lagging; near-term or long-term; input, output or process indicators etc.\nThe critical element in developing KPIs is determining what is important or ‘key’ to the organisation. Operational measures are also important – they can be termed as just ‘performance indicators’, or ‘PIs’, to distinguish them from KPIs.\nDeveloping KPIs should be part of an overall strategic management process that connects the overall mission, vision and strategy of an organisation, and its short- and long-term goals, to specific strategic business objectives and their supporting projects or initiatives. Understanding the organisation’s value drivers and the core activities and competencies that underpin its value proposition is an important first step in this process.\nValue creation map template\nSource: Marr, B. (2008) Managing and delivering performance, Elsevier Ltd, Oxford\nWhat benefits do Key Performance Indicators provide?\nKPIs can improve strategy execution by aligning business activities and individual actions with strategic objectives. Well-designed KPIs can provide a means for management and the board to monitor core activities of the business rather than simply outcome measures of financial success. Integration of financial and non-financial KPIs can contribute to a greater focus on long-term success rather than short-term financial performance.\nQuestions to consider when implementing Key Performance Indicators?\n- Do we understand our value drivers and core activities?\n- What KPIs do we need? What performance questions do we need to answer?\n- What mix of financial and non-financial measures do we need?\n- What customer, human capital, operating, supply chain or pipeline measures do we need to monitor?\n- Are there other key measures that are important drivers of our business, such as R&D, patents developed?\n- Are there leading indicators that we can develop from available data?\n- Can we collect meaningful data in a cost-effective manner for each of the desired measures?\n- Are our existing management information systems adequate to support the collection, analysis and reporting process?\n|Actions to take / Dos||Actions to Avoid / Don'ts|\nThis article, based on interviews with finance executives from Maersk Energy and International Flavors & Fragrances (IFF), discusses the importance of developing relevant KPIs and the role of the finance function in delivering more insightful information to manage performance.\nAlbert Birck, the head of performance management for Danish energy company Maersk Oil, describes the organisation’s thought process for developing KPIs as follows: ‘Every time we discuss or design something for performance management, we assess the options to see whether they create value, are transparent, actionable (relevant, meaningful, able to influence), timely (which is more important than ‘perfect’) and forward-looking.’\nTaking this approach and making good use of their performance management technology solution has enabled the finance function to take Maersk from managing by ‘gut feeling’ to having high-quality analytical insights.\nRoger Blanken, CPA, vice president of finance – supply chain for IFF, talks about his company’s use of ‘economic profit’ and weighted-average-cost of capital (WACC) in planning strategy. He also explains how IFF looks at high-level indicators such as GDP, consumer prices and exchange rates – factors that impact on their commodity-based business.\nMaking economic profit the primary measure of profitability has raised the quality of discussions within the company when making investment decisions.\nRelated and similar practices""]"	['<urn:uuid:e944df75-2829-4a3c-8fbb-a4a14f6ac5ca>', '<urn:uuid:dd662ef6-d422-4581-ac21-1272884ea38f>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	12	92	1363
42	what are main benefits sox compliant company internal control reporting performance	The main benefits for a SOX-compliant company include prioritizing risks, strengthening internal control structures, improving performance audits, and establishing centralized, automated financial reporting.	['Preparing to Go Public? Address SOX Compliance Early\nThe Sarbanes-Oxley Act of 2002, commonly referred to as SOX, requires public companies to report annually on their internal controls for financial reporting within their Form 10-K. The purpose is to protect stakeholders and provide assurance on the adequacy of controls in place to prevent fraud and financial misstatement. Public companies are required to comply with SOX, but compliance is often an overlooked or underestimated step for a company preparing to go public.\nAccording to many experts, the process should start about 18-24 months prior to going public, and studies have shown that some of the most successful IPO companies operated under public company rules (including SOX compliance) for 12 months prior to going public. Demonstrating SOX compliance prior to going public shows management’s commitment to protecting future stakeholders by identifying risks, gaps and weaknesses and remediating them, and implementing industry best practices for financial, operational and technological processes. This assures that the right people are in place and the company is fully aware of their risk areas.\nKey SOX Provisions for Companies Considering an IPO:\n- Section 302 – Mandates senior officers of a public company certify that they have established and maintained internal controls to ensure the accuracy of company information found within their reports.\n- Section 404 – Requires management and external auditors to report on the adequacy of the internal control over financial reporting.\n- Section 802 – Outlines rules for record keeping including a) destruction and falsification of records, b) defined retention periods for storing records, and c) specific types of business records that must be stored.\nBenefits for a SOX-compliant Company Include:\n- Prioritizing risks,\n- Strengthening internal control structures,\n- Improving performance audits, and\n- Establishing centralized, automated financial reporting.\nSOX compliance is a big undertaking for any business, and companies should consider the available resources they have for not only design and implementation of compliance, but also for maintaining future compliance. Becoming SOX-compliant can be a time-consuming and expensive endeavor, and employees may push back on procedural or system changes that require more of their time. Bringing in a qualified team of experts to help build your company’s SOX program, will expedite the process and ensure that your company is fully compliant.\nFor more information on navigating SOX and/or preparing for an IPO, please connect with us.\nMaterial discussed in this communication is meant to provide general information and should not be acted on without obtaining professional advice tailored to you or your company’s individual and specific needs. Any tax advice contained in this communication (including any attachments) is not intended or written to be used, and cannot be used by any person or entity, for the purpose of (i) avoiding penalties that may be imposed on any taxpayer or (ii) promoting, marketing or recommending to another party any transaction or matter addressed herein. This information is for general guidance only and is not a substitute for professional advice.\nThe information contained herein should not be construed as personalized investment advice. Investment in securities involves the risk of loss, and past performance is no guarantee of future results. There is no guarantee that the views and opinions expressed in this document will come to pass. Historical performance results for investment indexes and/or categories generally do not reflect the deduction of transaction and/or custodial charges or the deduction of an investment-management fee, the incurrence of which would have the effect of decreasing historical performance results. There can be no assurances that your portfolio will match or outperform any particular benchmark.\nInformation presented was obtained from sources deemed qualified and reliable; however, MFA makes no representations as to accuracy, completeness, suitability, or validity of any information within this communication and will not be liable for any errors, omissions, or delays in this information or any losses, injuries, or damages arising from its display or use. Any forward-looking statements are believed to be reasonable; however, MFA gives no assurance that such expectations will prove to be correct.']	['<urn:uuid:21ca8236-d936-4da9-89cc-e89f49ca8a57>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	11	23	670
43	help reorganizing website metadata which elements should be stored in individual pages	Only information specific to a particular page should be stored in the page document. If metadata is shared by more than one page, it should be stored in the master document instead. Common page-specific metadata includes creation date, change log, authors, editors, copyright information, and page language.	"['XSLT 2.0 Web Development: Elements of a Web Site\nRiver and bridge and street and square\nLay mine, as much at my beck and call,\nThrough the live translucent bath of air,\nAs the sights in a magic crystal ball.\nRobert Browning, Old Pictures in Florence\nThis chapter is a practical complement for Chapter 2, ""The source definition."" Having discussed the ins and outs of building a comprehensive and useful source definition, we\'ll now look at how these rules can be applied to real-world source XML documents of a typical web site.\nI cannot claim to cover everything: Your web site may well contain unique elements that won\'t fit common schemes. Here, only the most general and frequently used constructs are covered, and the approaches described in this chapter may not be optimal for all situations. Many examples are given, but rather than copy them over, try to use the reasoning behind these examples to analyze your own constraints and requirements.\nThe first part of the chapter deals with markup constructs commonly used in page documents, including headings, paragraphs and paragraph-like elements, links, images and other non-XML objects, tables, and forms. Then we will analyze the master document (3.9) to find out what data it needs to store and what is the best XML representation for this data. The last section (3.10) presents complete summary examples of a page document, a master document, and a Schematron schema to validate them.\n3.1 Page documents: top-level structures\nIn this and subsequent sections, we look at the informational core of a web page, stored in its own source document (page document). Peripheral components such as navigation, parameters of the site environment, and metadata are stored in the master document, the subject of 3.9.\n3.1.1 Page metadata\nEvery XML document has a root element, and since we\'re talking about page documents here, there\'s no reason not to call this element page. Its attributes and children are the natural place to store the page\'s metadata.\nIn addition to its primary content, each page document includes certain metadata. Some of it may end up as a visible part of the web page, some may be hidden in HTML metadata constructs (keywords and descriptions in meta elements), some may be used during transformation but not included in the resulting HTML code, and some may not be used at all except for reference or source annotation purposes. Common examples of metadata include page creation date, change log, author(s) and editor(s), copyright and licensing information, and the language of the page.\nNote that only information specific to this particular page must be stored in it; if some metadata bits are shared by more than one page, their proper place is in the master document (3.9) and not in any of the page documents.\nPage ID. The most important piece of metadata is the page\'s unique\nidentifier used to resolve internal links (3.5.3). However, we cannot store\nthis identifier in the page itself, or we\'ll have a catch-22 situation:\nWe can get from the id to the page location, but to obtain\nthe id we must access the page - that is, we must already know\nits location. Because of this, the proper place for the ids\nof all pages is in the site\'s master document.\nPage coordinates.The same applies to the information on the position occupied by this page in the site\'s hierarchy. As we\'ll see later (18.104.22.168), the branch of the site\'s tree that this page is a leaf of is most naturally deduced from the site directory in the master document. Duplicating this information in the page document itself is unnecessary and prone to errors.\nEverything else.Any other page metadata is normally stored in the page document. Simple values can be stored in attributes of the page\'s root element. More complex constructs that require their own elements can be placed either directly under the root or inside an umbrella parent element (e.g., metadata) that is a child of the root element.\nExisting vocabularies. RDF (\n3.1.2 Sections and blocks\nYou\'ll likely need some intermediate structural layers between the root element, page, and text markup constructs such as paragraphs and headings.\nSections or blocks? The traditional document hierarchy - sections, subsections, subsubsections, and so on - is not often seen on the Web. Instead, information is more commonly broken into relatively small blocks with few or no hierarchical relations between them. Different sites may call these blocks ""stories,"" ""blurbs,"" ""columns,"" ""modules,"" ""writeups,"" and myriad other names.\nAmong these names, one which appears to be the most intuitive is the one you should use for your block construct\'s element type name. Contentwise, a block is a unit with mostly fixed structure that may include both obligatory (e.g., heading and body) and optional (e.g., icon, heading links, author byline) components in both parallel and sequential (2.3.7) arrangements. Here\'s an example:\n<block id=""unique"" icon=""block_icon"" type=""story""> <head link=""address"">Block heading</head> <subhead>Optional block subheading</subhead> <p>And here goes a paragraph of text.</p> <p>Possibly one more paragraph.</p> <author>An optional author byline</author> </block>\nBlock types. It is likely that you will have more than one type of block construct - for example, front page news blocks, subpage body blocks, and ad blocks. In the simplest case, everything on the page can be treated as one big block, so the page\'s root element can be considered the root element of a block.\nDifferent types of blocks will likely have many common structural features - in part because they all belong to one site with its common information architecture and visual design. Only if different types of blocks have clearly distinct structures can you use different element types for them; otherwise it is best to use the same generic element type (e.g., block) with different values of the type attribute. This provides two major benefits:\nYour validation code will be simpler to write and maintain.\nManagement of orthogonal content (22.214.171.124) will be much easier to implement - for example, you may be able to reuse blocks from regular pages as orthogonals on other pages, or turn documents storing orthogonal content into regular pages.\nIn general, analogous but different structures should only differ by a minimum number of obvious features; avoid random, meaningless differences.']"	['<urn:uuid:f30bbdd7-561d-46d0-b591-6a7c26279602>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	12	47	1034
44	What types of debt cannot be discharged in bankruptcy?	Some debts that cannot be discharged in bankruptcy include recent taxes, student loans (unless the hardship test can be established), alimony, and child support.	['What is the Bankruptcy Discharge as Explained by a Tucson Bankruptcy Attorney\nAfter an individual goes through either a Chapter 7 bankruptcy or a Chapter 13 bankruptcy and completes the process successfully, that individual will receive a bankruptcy discharge. A bankruptcy discharge relieves a debtor of their personal responsibilities for the specific debt that was discharged by the bankruptcy court.[i] There are some debts that are not dischargeable, so it would be incorrect to think that a discharge order from the court is blanketed over all the debts. The discharge from the bankruptcy court will be provided in a court order and will prevent a creditor from collecting from the debtor personally for debts that the debtor owed prior to the bankruptcy. This means that a creditor cannot call or send any more demand letters to the debtor after the discharge has occurred. Keep in mind that a bankruptcy discharge is only going to be provided for individuals.[ii] Corporations, partnerships, and municipalities are not going to be eligible for a bankruptcy discharge.[iii]\nThe discharge order will be granted at the very end of the proceeding after all the requirements of the debtor have been satisfied.[iv] These requirements could include; completed all the requirements of your personal bankruptcy, attended all the meetings of creditors, provided the court with accurate records of your debts, assets, income, and financial dealings, participated in a session with a credit counselor, and taken a financial management course, and if the debtor has filed a Chapter 13 bankruptcy a�� they have made all the required payments under their personal repayment plan.[v]\nNow, as stated previously, not all debts will be dischargeable through the bankruptcy proceeding. The bankruptcy code provides a list of which debts are eligible and are not eligible for discharge.[vi] Some of the debts that are included on the non-dischargeable list include recent taxes, student loans (unless the hardship test can be established), alimony, and child support.[vii] So, even though every other possible debt a person has could be discharged a�� the above listed are here to stay through the good and the bad.\nAnother type of debt that a discharge will not be successful is liens on secured property.[viii] A lien on a persona��s property occurs when a creditor takes an interest in the debtora��s property in exchange for a loan.[ix] So if the debtor wants to take out a mortgage to purchase the house, a creditor may agree to lend the debtor money but only if the debtor allows the creditor to take an interest in the house itself. This will provide assurance for the creditor that if the debtor fails to pay the money lent to them a�� the creditor will be able to foreclose on the house and make back some of the money. Now, because this type of debt will not be discharged through the bankruptcy, the creditor can still go after the property that has the lien on it.[x] The creditor can repossess cars, or foreclose on homes, and recover as much money as possible. If after the creditor repossess a car or forecloses on a home there is still money owed to the creditor, that creditor CANNOT go after the debtor for the remaining amount. One way for debtors to avoid this from happening to their property is to reaffirm the debt with the lien.[xi] The debtor can do this by agreeing with the creditor that they will continue to pay the creditor, in exchange for the debtora��s ability to keep the property.\nRevocation, Denial, and Objections to the Discharge\nJust because a debtor has filed for bankruptcy and have gone through most of the requirements for the proceeding, the court may still deny the discharge that is typically available to debtors at the end of the proceeding.[xii] The court needs to have a reason for doing so, and some typical reasons include for example, the debtor did not cooperate with the court or the trustee on their case. The bankruptcy discharge may be denied if the debtor lied on their financial documents or even hid assets to avoid having them sold if the asset was not protected by an exemption. Those are just a few examples of when a court may deny to provide the discharge court order. The court could also revoke the court order of discharge if the debtor was found out to be lying do the court, or doing any of the listed above a�� and the court already provided the discharge order.\nThere may also be objections to the discharge order after a debtora��s bankruptcy proceeding. An objection will occur when a party to the case, one of the creditors, disputes whether or not the debt they are connected with should be discharged.[xiii]\nNow, there are two A�types of objections to a Chapter 7 bankruptcy: an objection to a particular debt and an objection against all of the debts.[xiv] The first type of objection will not affect any other debt in the proceeding a�� just the debt that has been objected to. If the court approves this objection, then the debt will be basically be reaffirmed and the debtor will remain liable to pay the creditor for this debt.[xv] Keep in mind that this can occur even if the court decides to discharge all of the debtora��s other debts.\nThe next type of grounds for objection, to all of the debtora��s assets, usually occurs when the creditor or trustee believes that the case has been based off fraud.[xvi] Some ways in which bankruptcy fraud can occur include; perjury by providing false information on your bankruptcy petition and schedules, transferring the title or property to another person to avoid having to claim in during the bankruptcy proceeding, or lying to the bankruptcy trustee or judge during the hearings.[xvii] Some people may think they can be sly during a bankruptcy proceeding, but this process is so invasive into the lives of the debtor a�� the truth is eventually bound to come out. If the truth comes out and shows that the debtor was fraudulent, the debtor may face more than a denial of discharge a�� the debtor will be liable to pay ALL of their debts and the debtor could potentially face jail time. A�Article written by Ariano and Associates.\nWho Can Object to the Discharge of the Bankruptcy?\nSo, is it just the creditor who can object to the discharge of bankruptcy? No. In fact, nany of the debtora��s creditors can object, the Chapter 7 bankruptcy trustee on the account can object, and the United State Trustee can object.[xviii] For the creditors, the typical reason that they might file an objection to the discharge would be if the debtor used false or misleading information on the loan application or financial statement between the debtor and creditor.[xix]\nThe Chapter 7 bankruptcy trustee on the account has the duty of ensuring that all of the debtora��s assets are collected and that all creditors have been treated equally throughout the proceeding. The trustee has the ability to object to a particular debts discharge, or to all of the debts discharge.[xx] Again, the trusteea��s objection will be anchored in bankruptcy fraud. Finally, the United States Trustee can also object to the discharge of debts, and can do so for all of the debtora��s debts.[xxi] This objection can occur if the United States Trustee believes that there was a violation of the United States Bankruptcy Code.[xxii] An example of a violation would be if the debtor did not wait the appropriate period for filing another Chapter 7 bankruptcy.[xxiii] An individual must wait eight years after a Chapter 7 bankruptcy to file for a subsequent Chapter 7 bankruptcy. If the debtor fails to do so, and the court does not catch this, the United States Trustee can step in and object to the discharge of those debts on those grounds.\nAn objection to a discharge MUST be filed with 60 days after the meeting with the creditors takes place for the debtor.[xxiv] The objection must be made in writing and set out specific reasons for why the filing party believes that the debt should not be discharged.[xxv] The objection to the discharge will be reviewed during a proceeding called the adversarial proceeding. An adversarial proceeding is a separate lawsuit that takes place and is heard within the bankruptcy proceeding.[xxvi] The adversarial proceeding will have its own case number, even though it takes place during the bankruptcy proceeding.[xxvii] The debtor may even have a separate attorney during this proceeding.[xxviii]\nOnce the person who the adversarial proceeding is against receives notice of the hearing, that person will have a limited number of days to respond to the complaint against them. The response time allotted will depend on the state and the court. In the case of an objection to the bankruptcy discharge, the debtor will be the person who receives notice of the adversarial proceeding. The adversarial proceeding will be heard and decided upon before the bankruptcy proceeding overall can be decided.[xxix] If the judge within the adversarial proceeding finds in favor of the debtor, then their eligible debts may be discharged.[xxx] If the judge, however, finds for the creditor or trustee, then the debt will NOT be discharged and the debtor will remain liable to pay for the debt after the other debts have been discharged through the bankruptcy proceeding. Just like any other proceeding, the adversarial proceeding can be settled before the proceeding concludes.[xxxi]\nConclusion to Bankruptcy Discharge\nWhile many people believe that filing for bankruptcy is a quick way to get out of their financial responsibilities, there are a lot of obstacles to overcome in the proceeding to ensure that everything goes according to plan. After the bankruptcy proceeding, for either Chapter 7 or Chapter 13, the court still has to provide a court order for a discharge of the debtora��s debts. The discharge may be prevented by a number of ways. The court could decide to deny the discharge if the court finds that the debtor was fraudulent, or the court could revoke the discharge order for the same reason if new evidence comes to light after the court has been granted. Creditors, Chapter 7 bankruptcy Trustees, and even the United State Trustee can also object to the discharged based on fraudulent grounds. Overall, just because the bankruptcy proceeding is completed or coming close to completion, there is no guarantee that the debtora��s debts will all be discharged. If any sign of fraud becomes known, the entire bankruptcy proceeding could be compromised. A�For more information contact an affordable bankruptcy lawyer in Tucson at Ariano & Associates.\n[i] See What is the Bankruptcy Discharge. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/what-is-the-bankruptcy-discharge.html\n[ix] See What is a Secured Debt. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/what-secured-debt.html\n[x]A� See What is the Bankruptcy Discharge. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/what-is-the-bankruptcy-discharge.html\n[xiii] See Objections to the Bankruptcy Discharge. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/objections-the-bankruptcy-discharge.html\n[xxvi] See Adversary Proceedings in Bankruptcy. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/adversary-proceedings-bankruptcy\n[xxvii] See Adversary Proceedings in Bankruptcy. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/adversary-proceedings-bankruptcy.html\n[xxx] See Objections to the Bankruptcy Discharge. NOLO Legal Encyclopedia. (Accessed April 22, 2016). http://www.nolo.com/legal-encyclopedia/objections-the-bankruptcy-discharge.html']	['<urn:uuid:f7c2782c-90ff-4190-b204-9f5f8aac040b>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	9	24	1871
45	Do Supreme Court and FTC handle company punishments differently?	Yes, they handle punishments differently. The Supreme Court, through the Omnicare decision, established that companies can face liability under Section 11 primarily for misleading statements in registration documents, with a focus on protecting investors through disclosure requirements. The FTC's approach is more direct - under Section 13(b) it can seek immediate injunctive relief for ongoing violations, while Section 19 allows for monetary penalties including refunds and damages, but requires additional procedural steps like prior rule violations or administrative adjudication. The FTC's monetary remedies have been challenged as potentially being penalties rather than equitable relief, especially after the Supreme Court's Kokesh decision.	['The United States Supreme Court in Omnicare, Inc., et al. v. Laborers District Council Construction Industry Pension Fund, et al., clarified standards for liability that a company issuing securities may face through statements it makes in its registration statement filed with the Securities and Exchange Commission (SEC). If the registration statement contains an “untrue statement of a material fact” or “omits to state a material fact … necessary to make the statements therein not misleading” a purchaser of the stock may sue for damages under Section 11 of The Securities Act of 1933.\nIn this case, Omnicare made two statements in its registration statement that were at issue: “We believe our contract arrangements with other healthcare providers, our pharmaceutical suppliers and our pharmacy practices are in compliance with applicable federal and state laws.” Additionally, Omnicare stated that “we believe that our contracts with pharmaceutical manufacturers are legally and economically valid arrangements that bring value to the healthcare system and the patients that we serve.” Pension funds that purchased Omnicare stock in a public offering later sued Omnicare, alleging that the company’s two opinion statements about legal compliance gave rise to liability under Section 11. The pension fund cited lawsuits that the federal government filed against Omnicare, alleging that the receipt of payments from drug manufacturers violated anti-kickback laws. On that basis, the pension funds asserted that Omnicare made “materially false” representations about legal compliance. They also noted that the company “omitted to state material facts necessary” to make its representation not misleading.\nThe issue before the Supreme Court was whether the provisions of Section 11 pertained to statements of opinion. The Supreme Court held that a statement of opinion contained in the registration statement that turns out to be wrong “will not give rise to liability under § 11’s first clause because … a sincere statement of pure opinion is not an ‘untrue statement of material fact’ regardless whether an investor can ultimately prove the belief wrong.” However, this only solves part of the issuing company’s problem. The analysis then turns to whether Omnicare “omitted to state facts necessary” to make its opinion on legal compliance “not misleading.” The court noted that whether a statement is misleading depends on the perspective of a reasonable investor. Merely couching the statement in the form of an opinion—“we believe”—does not necessarily save an issuer from potential liability under Section 11’s omission clause. The Court noted that “if a registration statement omits material facts about the issuer’s inquiry into or knowledge concerning a statement of opinion, and if those facts conflict with what a reasonable investor would take from the statement itself, then § 11’s omission clause creates liability.”\nWhat does this decision mean for a company issuing securities? Being mindful of the potential for liability under Section 11, a company must be careful when drafting statements contained in its registration statement. A statement of opinion will not automatically act as a shield from liability. However, the Court did note that proving liability under the “omission” clause of Section 11 is an uphill battle for the investor—the plaintiff “must identify particular (and material) facts going to the basis for the issuer’s opinion and whose omission makes the statement at issue misleading to a reasonable person reading the statement fairly and in context.” Nonetheless, a company should take into consideration competing facts and guidance from attorneys or advisors that may run counter to the opinions expressed in the registration statement. In fact, a Massachusetts-based life sciences company was recently ordered to pay $17.9 million to the SEC in a lawsuit accusing the company of selling investors’ stock on the basis of misleading statements regarding the status of regulatory approval of its products. The lesson is clear: Tread carefully before making a statement of opinion to potential investors.', 'Featured Expert Contributor, Antitrust & Competition Policy — Federal Trade Commission\nBy M. Sean Royall, a Partner with Gibson, Dunn & Crutcher LLP, with Blaine H. Evanson, and Richard H. Cunningham, Partners, and Brandon J. Stoker, an Associate, with the firm.\nClick here for a printer-friendly version of this post.\nLess than two years ago, David Vladeck, a Professor at Georgetown University Law Center who served as the Director of the FTC’s Bureau of Consumer Protection from 2009 to 2012, described the argument that the FTC Act does not permit the agency to obtain equitable monetary relief as “repeatedly and uniformly rejected by every court to address it.” Two Ninth Circuit judges, however, recently signaled that the landscape in this area may be changing in the wake of the Supreme Court’s 2017 Kokesh v. SEC decision.\nIn an extraordinary procedural move, on December 3, 2018, Ninth Circuit Judge Diarmuid F. O’Scannlain, joined by Judge Carlos T. Bea, wrote a special concurrence to his majority opinion in FTC v. AMG Capital Management, LLC et al., in which he described permitting the FTC to obtain monetary relief under Section 13(b) of the FTC Act as “an impermissible exercise of judicial creativity” that “contravenes the basic separation-of-powers principle that leaves to Congress the power to authorize (or to withhold) rights and remedies.” Slip Op. at 36. The concurrence called on the Ninth Circuit to hear the case en banc to reconsider its 2016 decision in in FTC v. Commerce Planet, Inc.,* which held that the FTC may obtain monetary relief pursuant to Section 13(b), and walked through how the Kokesh decision calls the reasoning of Commerce Planet into question.\nThe Statutory Landscape\nSection 13(b) of the FTC Act states that “the Commission may seek, and after proper proof, the court may issue, a permanent injunction.” That’s it. There is no reference to any form of monetary relief.\nHowever, The Ninth Circuit and multiple other circuit courts have held that the word “injunction” invokes a broad panoply of equitable remedies, including restitution, rescission, and disgorgement, all of which involve the defendant making monetary payments. The Supreme Court has not ruled on the issue.\nIn general, the circuit courts, including the Ninth Circuit in Commerce Planet, have relied on Porter v. Warner Holding Co. in concluding that § 13(b) provides courts with authority to award the FTC monetary relief in the form of equitable restitution, rescission, or disgorgement. In Porter, the Supreme Court held that the Emergency Price Control Act of 1952, which authorized the agency to seek “permanent or temporary injunction, restraining order, or other order,” gave district courts the authority to order a monetary equitable remedy because the language invoked the “inherent equitable powers” of the courts, and those powers include monetary remedies.\nJudge O’Scannlain’s Concurrence\nThe concurrence calls into question this analysis, to put it mildly.\nJudge O’Scannlain argues that the plain text and structure of the FTC Act foreclose the kind of monetary relief authorized by Commerce Planet and similar decisions. FTC v. AMG Capital Mgmt., LLC et al., 910 F.3d 417, 430 (9th Cir. 2018) (O’Scannlain, J., concurring). Judge O’Scannlain’s concurrence begins by explaining that § 13(b) authorizes courts to award the FTC, “after proper proof . . . a permanent injunction.” Id. at 30. The concurrence then states: “But an order to pay money ‘as reparation for injury resulting from breach of legal duty’ is essentially a damages remedy—not a form of ‘specific relief’ like an injunction.” Id. “[A]ny other interpretation,” Judge O’Scannlain emphasized, “would be absurd: if ‘injunction’ included court orders to pay monetary judgments, then ‘a statutory limitation to injunctive relief would be meaningless, since any claim for legal relief can, with lawyerly inventiveness, be phrased in terms of an injunction.’” Id. (quoting Great-West Life & Annuity Ins. Co. v. Knudson, 534 U.S. 204, 211 n.1 (2002)).\nThe concurrence then looks to the structure of § 13(b) to reinforce this point. The statute requires that the Commission believe that a person “is violating” or “is about to violate” the Act in order to request injunctive relief, id. (citing 15 U.S.C. § 53(b)(1)), and thus “anticipates that a court may award relief to prevent an ongoing or imminent harm—but not to deprive a defendant of ‘unjust gains from past violations.’” Id. (quoting Commerce Planet, 815 F.3d at 599).\nMoreover, “[a]n entirely different provision of the FTC Act”— § 19—“allows the Commission to collect monetary judgments for past misconduct,” which “may include . . . the refund of money[,] return of property [or] the payment of damages.” Id. at 431 (quoting 15 U.S.C. § 57b(b)). Yet, Judge O’Scannlain underscored, “[a]ccording to Commerce Planet . . . these very same remedies were already available under § 13(b) when Congress subsequently enacted § 19”—a reading that “violates the ‘cardinal rule that, if possible, effect shall be given to every clause and part of a statute.’” Id. at 432 (citations omitted).\nThe concurrence further argues that Commerce Planet’s collapsing of these two statutory provisions “circumvents § 19’s procedural protections.” Id. at 431. “Before the Commission can collect ill-gotten gains under § 19, it must surmount one of two procedural hurdles[:]” (1) prove to a court that the defendant “violate[d] any rule” already promulgated through the Commission’s rulemaking procedures (id. (citing 15 U.S.C. § 57b(a)(1)); or (2) where no rule has been promulgated, first pursue an administrative adjudication, issue a “final cease and desist order,” and prove to a court that the defendant’s conduct was such that a “reasonable man” would know it was “dishonest or fraudulent.” Id. at 432 (citing 15 U.S.C. § 57b(a)(2)).\nJudge O’Scannlain underscored that Congress included these procedural hurdles in § 19 “with good reason”—to afford defendants fair notice of proscribed conducted and thereby “prevent the Commission from imposing significant monetary burdens simply by bringing a lawsuit in federal court.” Id. According to the concurrence, Commerce Planet’s interpretation of § 13(b) permits the Commission to pursue precisely the same relief with no procedural safeguards whatsoever, and thus “wrongly allows the Commission to avoid the administrative processes that Congress directed it to follow.” Id.\nFinally, Judge O’Scannlain suggests that under the Supreme Court’s recent decision in Kokesh v. SEC, 137 S. Ct. 1635 (2017)—a case decided after Commerce Planet—the kind of “restitution” purportedly authorized by § 13(b) is more accurately characterized as “a penalty,” not equitable relief. Id. at 433. The Supreme Court in Kokesh held that SEC disgorgement, which it described as “a form of restitution measured by the defendant’s wrongful gain,” is a penalty. Kokesh, 137 S. Ct. at 1640 (quoting Restatement (Third) of Restitution and Unjust Enrichment § 51, cmt. a, at 204 (2011)). The Supreme Court identified three “hallmarks” of a penalty: (1) it is “imposed by the courts as a consequence for violating public laws”; (2) it is sought for a “punitive rather than remedial” purposes; and (3) it is “‘not compensatory’ because some ‘funds are [disbursed] to the United States Treasury.’” AMG Capital, 910 F.3d at 433 (citations omitted).\nApplying these principles, Judge O’Scannlain argues that restitution under § 13(b) bears all the hallmarks of a “penalty.” Id. Not only is restitution pursued for violation of a regulatory statute (the FTC Act), with or without the support of individual victims, it is also punitive rather than “remedial” because—under Commerce Planet—the wrongdoer’s unjust gains must be measured by “net revenues” rather than “net profits,” which often “leaves the defendant worse off.” Id. at 433–34. And, like SEC disgorgement, restitution under Section 13(b) are not compensatory because funds not returned to victims are utilized at the Commission’s discretion or deposited in the U.S. Treasury as “disgorgement.” Id. at 434.\nAs we discussed in an article last year in Antitrust magazine, viewing equitable monetary remedies under § 13(b) as “penalties” is potentially very significant. First, “penalties” are subject to the five-year statute of limitations in 28 U.S.C. § 2462. Second, the Supreme Court has suggested, including in 1987 in Tull v. United States, that “civil penalties” cannot be awarded as equitable relief at all.\nAppellants in AMG Capital will file their petition for rehearing en banc in early March 2019, and we will learn whether the Ninth Circuit will accept the concurrence’s invitation to revisit this issue by late spring 2019.\n*Several of the authors of this post were counsel for the appellant in Commerce Planet.']	['<urn:uuid:19ce6a67-93e5-4eee-97ce-b1eb0b9f8fcb>', '<urn:uuid:ef15e1c8-51ff-4f32-a835-597c64e934c9>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:43:46.574121	9	101	2022
46	property ownership restrictions women farmers south sudan akobo tonj	Due to cultural norms in rural areas of South Sudan like Akobo and Tonj South, women are prohibited from owning property and assets in the family. They are only allowed to acquire and manage assets like small ruminants with their husband's approval.	['Development in South Sudan is curtailed by the ongoing widespread conflict coupled with very high levels of food insecurity and derailed economic progress. Conflict has led to significant displacements of populations from both urban and rural areas, with the most impact being felt in rural communities, which are conducive for agriculture and livestock production - but this production has been hindered. During this year’s International Day for Rural Women, ACTED recounts the role rural women of South Sudan play to ensure survival and progress in their communities.\nNot only do women work extra hours in a day, tending after their families without any pay, but women in rural areas of South Sudan do so amidst the most extreme circumstances, and with very little expectations. From engaging in daily households duties to taking care of children, the elderly, sick, people with disabilities, and tending to livestock, women burst their backs in farms in order to put food on the table. These duties are physically challenging and emotionally draining, not to mention time consuming. This leaves them with barely enough time to take care of themselves, or participate actively in community politics. The latter, is a role reserved only for men in say, Akobo and Tonj South, where it is considered a taboo for a woman to address a group of men.\nWhen an ACTED team visited Asunta, a 25-year old mother of three boys and three girls, they have to interrupt her sorghum harvesting session. She is deeply engrossed in the farm, humming in a beautiful voice, depicting happiness and peace of mind. It is quite intriguing how she thrushes the sorghum heads at a remarkable speed and moves swiftly to the next plant, almost missing the presence of the team. She is one of the beneficiaries of ACTED’s BRACED resilience project funded by DfID in Tonj South. She turns around and exchanges pleasantries with the team, quickly apologizing for not noticing them earlier. With a warm handshake, she leads them to her hut, which is 400 metres from her husband’s farm. One can tell from the clean and well organized homestead that Asunta is not only responsible, but also wise. She explains that her secret to handling all homestead chores and the farm is learning to manage her time wisely. This involves waking up before everyone else and going to sleep last.\nWhen I got married, I knew I had to take good care of my family as is the role of women in the community. We are nurturers and I am happy to have a peaceful home\nThough she lives with her husband and their children, she says she has learned not to rely on his support to provide for the family because life has become increasingly unpredictable. From ACTED’s project, she has gained a lot from lessons on modern farming, and as a result she has diversified her crops to include groundnuts, maize and sorghum. Asunta believes in spreading her risks in anticipation for hard times. When she is not working in her husband’s farm she gathers bundles of grass for sale to diversify her household income. When the team implores on how she uses money earned, she chuckles and waves her right arm across her face in wishful thinking, “My husband makes decisions on how we spend money that we both get. I give every coin I earn to him to make such decisions.” However, she is quick to add that her husband supports her and makes smart decisions. She is happy that her husband respects her and increases their livestock in times of plenty.\nDespite her hardwork and dedication to her family, Asunta is one of the many women who barely own any assets at their households due to cultural norms that discourage ownership of property by women. Asunta discloses, “My culture has since prohibited us from owning property and assets in the family, but we are allowed to acquire and manage the assets like small ruminants. For example, I bought 3 goats and 4 chicken with the profit I made from the sale of vegetables last season. However, I had to get my husband’s approval to do so”. Asunta’s narrative is quite common in rural areas, where women are real champions and agents of transformative change. Asunta says, “Spouses should be sensitized to recognize our hard work and allow us participate in household and community planning activities.” However, restrictions on asset and property ownership remain impediments to independent thought, which could reflect poorly on a community dominated and driven by male voices only. ACTED works with such rural communities to change their mindsets through comprehensive and well integrated projects that challenge existing norms. Through the inclusion of adequate representation of women in agro-pastoral field schools where inclusive learning is experienced, the community is slowly accepting women as individuals with clear strategies and plans that can lead to the development of the whole community.']	['<urn:uuid:f237028a-b849-4d85-8e07-43bffc27950d>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	9	42	818
47	I've been following developments in space technology and safety. Can you explain how modern rockets handle stage separation mechanisms, and what happens to these stages in terms of their contribution to the space debris crisis?	Modern rockets use pyrotechnic fasteners or pneumatic systems for stage separation, though these separation events increase mission complexity and risk. In serial staging, stages detach sequentially when fuel is depleted, while parallel staging involves boosters falling away from a central sustainer. These discarded stages have become a significant source of space debris, particularly when they weren't properly passivated before the 1990s. Many spent stages have exploded in orbit due to leftover fuel and pressure combined with temperature changes, creating thousands of debris pieces. Currently, there are over 300,000 pieces of debris larger than a centimeter in orbit, with about 19,000 pieces larger than a softball being tracked by the Department of Defense and NASA. This debris poses an ongoing threat to active spacecraft and has led to the implementation of debris mitigation strategies for modern launches.	"['Jump To Section\nWhat Is Rocket Staging?\nStaging is the combination of several rocket sections, or stages, that fire in a specific order and then detach, so a ship can penetrate Earth’s atmosphere and reach space.\nThe operative principle behind rocket stages is that you need a certain amount of thrust to get above the atmosphere, and then further thrust to accelerate to a speed fast enough to stay in orbit around Earth (orbital speed, about five miles per second).\nIt’s easier for a rocket to get to that orbital speed without having to carry the excess weight of empty propellant tanks and early-stage rockets. So when the fuel/oxygen for each stage of a rocket is used up, the ship jettisons that stage, and it falls back to Earth. This becomes part of the rocket’s mass fraction—the portion of its fully fueled pre-launch mass that does reach orbit.\nWhat Is Drag and How Does It Impact Rocket Staging?\nIn Earth’s atmosphere, a rocket has to contend with drag, which is a force that pushes against a spaceship and—if not thoughtfully allowed for in the rocket design—can prevent it from going any faster, or even tear the ship apart. As the spaceship moves away from Earth and higher in the atmosphere, air density decreases. Out in the vacuum of space, the density is essentially zero, so there is virtually no drag there.\nSo how does drag impact rocket staging?\n- Each stage can use a different type of rocket engine, each tuned for its particular operating conditions, whether that’s in Earth’s atmosphere or beyond.\n- Rockets stages are typically stacked or parallel (boosters on the sides of a central vessel). The two-stage rocket is common, but space programs have successfully launched rockets with as many as five separate stages.\n- The more stages—and thus stage detachments, or separation events—a rocket has, the more complicated and dangerous a ship becomes. Ships typically feature pyrotechnic fasteners or pneumatic systems to separate rocket stages. Engineering must be precise to avoid catastrophe.\n- Stages can fire sequentially or simultaneously, depending upon the rocket and the mission.\n- In the ideal rocket equation, as much as 91-94% of the total mass of modern day solid rocket motors is fuel. The remaining weight includes structure, engines, and payload.\n4 Different Kinds of Rocket Staging\nThe kind of staging astronauts use depends upon their mission needs and what forces they need to overcome.\n- Serial staging. Stages are attached, one on top of the other, or stacked. The first stage ignites at launch and burns through its fuel until its propellants are spent. Now useless dead weight, in a staging maneuver the first stage breaks free from the previous stage, then begins burning through the next stage in straight succession. Depending on the rocket, the second stage may get the payload into orbit or require a third or fourth stage to ultimately deliver it to space. It depends on the individual rocket and mission.\n- Parallel staging. Whereas serial staging involves stacked stages, parallel staging features one or multiple booster stages strapped to a central sustainer, as on the space shuttle. At launch, all the engines ignite. When their propellant runs out, the strapped-on boosters fall away. The sustainer engine keeps burning to put the payload into orbit. With the shuttle, solid rocket boosters are the stages that fall away from the main sustainer, the external tank that fed the main engines. The Titan III is an example of a rocket that uses both serial and parallel staging; it used a two-stage Titan II as the sustainer and added two solid rocket stages as boosters that fell away once they were done, much like the SRBs on the shuttle.\n- Stage-and-a-half: This less common staging has a main core that acts like a sustainer stage and a booster stage that falls away during the flight. This dates back to the Atlas D that launched John Glenn in 1962 and the three Mercury astronaut who followed in his orbiting footprints. At the time, the upper stages of multistage rockets often didn’t fire on time and rockets blew up. To make sure the engines all ignited properly, it made sense to Atlas designers to have all engines ignite while the rocket was still on the launch pad. Dropping the booster that was also sort of part of the main stage was how it dropped the dead weight in flight, making the rocket light enough to put a Mercury capsule into orbit.\n- Single staging. More a dream in development than a current reality, a single stage rocket is a simpler technology that doesn’t require multiple complicated and dangerous stages to get through the atmosphere.\nWhat Is the Purpose of the First Stage in a Serial Staging?\nThink Like a Pro\nThe former commander of the International Space Station teaches you the science of space exploration and what the future holds.View Class\nThere are multiple staging schemes for rockets, and the number of stages varies depending upon the spacecraft and the mission objectives. Typically, a rocket with three stages will undergo the same process. In serial staging schemes, the first stage is at the bottom of the rocket and is usually the largest. Its primary purpose is to get the spacecraft to a height of 150,000 feet, above most of the Earth’s air.\n- Launch location is important. This comes down to physics. As a general rule, rockets launch from as near as possible to the equator, in order to take advantage of the velocity of Earth’s rotation, which is highest at the equator—about 1,000 miles an hour. The more orbital velocity a rocket gets from Earth, the less fuel it requires to reach orbital speed, which increases its efficiency. Not all rockets can take advantage of Earth’s spin—some are designed to send payloads such as satellites into north-to-south orbit, around the poles.\n- The ride is intensely physical. G-forces are three times normal and there is rough, high-frequency vibration as the vehicle shoulders its way through the thick air. After two minutes the rocket is high enough that the air has thinned to almost nothing, and the first-stage boosters explode off in a burst of fireworks.\n- As velocity increases, so too does drag. This is why flying a rocket through the atmosphere is so hard. To decrease drag, the cross-sectional area of the spaceship needs to be minimized, which is why rocket ships have to be streamlined.\n- Lower stages like this typically require more structure than the upper stages. This is because they must bear their own weight as well as the mass of the stages above them that are not yet in use.\n- This initial rocket stage needs to rapidly push the rocket into higher altitudes. As such, it typically has a lower specific impulse rating, trading efficiency for superior thrust.\nWhat Is the Purpose of the Second Stage in a Serial Staging?\nThe second stage is second from the bottom of the rocket. Its purpose is to get the spacecraft to orbital velocity and achieve weightlessness.\n- The ride smooths out. Above the majority of Earth’s air, the ride gets suddenly smooth—but steadily heavier as the ship burns off fuel and the acceleration grows. The spaceship rolls through 180 degrees to let the communication antennae point at orbiting relay satellites.\n- Achieving weightlessness. The ship becomes light enough that it reaches 3G, and the computers ease the throttles back to not overstress the vehicle. Each passing second takes the crew past emergency abort and failure options and improves astronauts’ chances of making it to orbit. And after eight and a half minutes, the stage engines shut down and they are safely there, weightless, in space.\n- Ideal acceleration occurs above the atmosphere. Not only do the most efficient rockets have lower areas, but they also do as much of their accelerating (increase in velocity to orbital speed) as possible once they’ve gotten above the atmosphere into areas of lower air density. Once beyond the atmosphere, astronauts can increase the speed without increasing the force of drag because there’s no atmospheric density.\n- Since the vehicle is further outside the atmosphere and the exhaust gas does not need to expand against as much atmospheric pressure, this later stage usually has a higher specific impulse rating.\nWhat Happens to Detached Stages?\nThe spent upper stages of launch vehicles are a significant source of space debris remaining in orbit in a non-operational state for many years after use, and occasionally, large debris fields created from the breakup of a single upper stage while in orbit.\nSince the 1990s, when an upper stage is spent, astronauts have generally dumped any remaining fuel or discharged batteries, “passivating” them to reduce risks while those stages continue to orbit. Prior to that, many Soviet and U.S. space programs often did not passivate the upper stages after mission completion.\nLearn more about space exploration in Chris Hadfield’s MasterClass.', 'Right now, there are more than 300,000 pieces of debris larger than a centimeter in diameter orbiting Earth.\nThey range from tiny shards of metal to deactivated, decades-old satellites. Most are shrapnel from discarded rocket stages that have exploded after use, or satellites that have collided. Colloquially, all this debris is usually called ""space junk.""\nTogether, the Department of Defense and NASA track the orbits of the 19,000 or so pieces of junk that are larger than a softball, alerting satellite operators when any satellite — including the International Space Station — is in danger, so they can move it.\nBut doing so takes time and resources. What\'s more, the cloud of debris has been steadily growing over time, and some scientists worry that if we\'re not careful, we could trigger a chain reaction: More space junk raises the chance of collisions, which in turn can lead to even more debris, until the sheer volume of space junk makes parts of space unusable.\n""Space is a finite resource — just like the atmosphere, and the water, and the Earth,"" says William Schonberg, an aerospace engineer who designs spacecraft to minimize damage from orbital debris. ""We need to be careful about how we use it.""\nWhy space junk is a problem\nEven small pieces of debris in orbit around Earth can cause a surprising amount of damage because of a basic reason: speed.\n""All the objects in Earth\'s orbit naturally have a high velocity,"" says Holger Krag, head of the European Space Agency\'s Space Debris Office. (If they weren\'t traveling that fast, they\'d simply drop to Earth.) In low Earth orbit, this speed is around 16,000 miles per hour. ""Even a centimeter-long screw can generate the energy of an exploding hand grenade.""\nAs a result, collisions have to be avoided at all costs. Using ground-based radar and other instruments, the Department of Defense and NASA keep track of about 19,000 pieces of debris larger than five centimeters — the ones big enough to cause significant damage.\n""We do an assessment for every operational satellite, looking typically three days into the future, and if we think that some other object is going to come close to hitting it, we notify the owner-operator,"" Nicholas L. Johnson, then-chief scientist at NASA’s Orbital Debris Program, told me in 2012. About once a week or so, satellites are moved to prevent a collision.\nBecause there\'s crew on board, the International Space Station is treated with extreme delicacy and is moved if there\'s more than a 1 in 100,000 chance something will collide with it. In a few cases, warning hasn\'t come in time, and astronauts have had to quickly take shelter in the capsules that serve as the station\'s lifeboats.\nEven with these preventative measures, though, debris are a long-term issue for space operations as a whole. One problem is that a steady stream of even tinier, sand grain-sized particles can gradually erode the surface of all spacecraft in orbit.\nAnd there\'s a bigger concern for heavily-trafficked orbits — such as low Earth orbit and geosynchronous orbits (often used for communication satellites, so they can stay fixed over one location on Earth). As these orbits fill with debris, they become more and more expensive to use. ""Every collision avoidance maneuver means loss of mission time,"" Krag says. ""And you have to use manpower and fuel to carry them out.""\nAll debris eventually falls back to Earth given enough time — and objects in lower orbits fall much faster, over the course of a few years, because traces of atmosphere drag on them and slow them down. But those in higher orbits might take decades or even centuries. And if we\'re not careful, some orbits could become so clogged with junk that they\'re impossible to use.\nWhere all the space junk came from\nA few different factors have contributed to the steady accumulation of orbital debris ever since we started using space in the 1950s.\nOne is the fact that for decades, the rockets we used to lift spacecraft into orbit were only designed with the first few minutes of flight in mind. ""We didn\'t think at all about how the object might behave after years or decades,"" Krag says.\nAs a result, rocket components were commonly left in orbit with tiny amounts of extra fuel and built-up pressure inside. When an object in orbit leaves the shadow of the Earth and is hit by sunlight, its temperature can swing by hundreds of degrees. This has led many rocket stages to explode, sending thousands of shards of metal cascading in orbit.\nIn the 80s, scientists recognized the problem, and now, rockets are designed so that they can be fully emptied after use. These intact rockets, along with deactivated satellites, make up a minority of the pieces of debris in orbit.\nBut two recent events generated another 5,000 pieces of tracked debris, generating about a quarter of the total.\nIn 2007, China intentionally destroyed one of its weather satellites in orbit as part of a military test, generating about 3,000 pieces of debris. Even worse, it was done in a higher orbit than other anti-satellite tests previously conducted by the US and Russia, so the debris will take much longer to come down. Other countries criticized the test, but the damage was done.\nThen, in 2009, two satellites — a deactivated Russian military satellite and an active US communications satellite — accidentally collided, creating a shower of another 2,000 or so pieces of debris.\nWhat\'s going to happen to space junk in the future\nThe 2009 event was especially alarming because it may be a sign of things to come: if space gets too crowded with debris, it could trigger a positive feedback loop, in which collisions beget debris, which beget collisions.\nSome scientists, in fact, believe this scenario (called the Kessler syndrome) is already happening, just slowly. Unlike in the movie Gravity, the chain reaction would accelerate slowly, over the course of decades: right now it\'s estimated that one collision will occur every five years, and if things get worse, the rate could increase to one collision annually within 50 to 100 years.\nStill, there\'s disagreement over whether this is happening yet — and spacefaring nations are generally being more careful nowadays. Rockets are drained of fuel and pressure after use, and satellite operators are now required to move their satellites down to a lower altitude after use, so they\'ll fall back to Earth more quickly, or take them up to an unused ""graveyard"" orbit.\nStill, there\'s tons of debris already in orbit. If we did enter a scenario in which debris accumulated uncontrollably, people have drawn up some ideas for cleaning up our orbits: crafts that would use nets or harpoons to grab derelict satellites to bring them down, for instance, or a spacecraft that would grab debris, throw it down to Earth, and use the resulting momentum to move on to the next object.\nAt the moment, though, these ideas are purely hypothetical, and all of them would be extremely expensive. If we want to continue relying on Earth\'s orbit for communication, navigating, and all other sorts of useful technologies, we need to be more careful with it going forward.\nFurther reading: How NASA steers the International Space Station around space junk']"	['<urn:uuid:80d87fdd-653d-4c48-ab10-37b698b6eb8e>', '<urn:uuid:af09ffe4-73dd-4917-b8ac-4c2ad6170415>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:43:46.574121	35	136	2712
48	data privacy security storage requirements consent rules compare businesses need follow	For data privacy and security, businesses must follow several key requirements around storage and consent. They need to implement secure data storage with strong passwords and encryption, plus regular security audits. For consent, businesses must be transparent about data collection and obtain explicit consent through opt-in forms before collecting or using customer data. The data can only be used for its intended purpose - for example, an email collected for a whitepaper download cannot be used for marketing without specific consent. Businesses must also disclose if they share data with third parties. Under both CCPA and CalOPPA, they must protect California residents' personal information and face penalties for violations of these requirements.	['The Role of Data Privacy in AI-Driven Lead Generation Strategies\nArtificial intelligence (AI) has revolutionized the way businesses generate leads, providing a more efficient and effective means of identifying potential customers. However, with the vast amounts of data involved in AI-driven lead generation, there are increasing concerns around data privacy. In this blog post, we will explore the role of data privacy in AI-driven lead generation strategies and discuss how businesses can ensure they are using customer data in an ethical and responsible manner.\nThe Importance of Data Privacy\nData privacy refers to the protection of personal information, such as names, email addresses, phone numbers, and other sensitive data. With the rise of AI-powered lead generation tools, businesses have access to vast amounts of customer data, which can be used to target potential customers more effectively. However, this data is also sensitive and must be protected to ensure customer trust.\nThe Role of GDPR and CCPA\nTwo regulations that have a significant impact on data privacy in AI-driven lead generation are the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). The GDPR applies to all businesses that handle data belonging to individuals in the European Union (EU), while the CCPA applies to businesses operating in California that handle the data of California residents.\nUnder these regulations, businesses must obtain explicit consent from customers before collecting, processing, or storing their data. They must also be transparent about what data they are collecting and how they will use it. Additionally, customers have the right to access, correct, and delete their data at any time.\nEnsuring Ethical and Responsible Use of Data\nTo ensure ethical and responsible use of customer data in AI-driven lead generation strategies, businesses can take several steps:\nBe Transparent about Data Use\nTo build trust with customers, businesses must be transparent about what data they are collecting and how they will use it. This includes providing clear and concise privacy policies that outline the purpose of data collection and how it will be stored and used.\nObtain Explicit Consent\nBusinesses must obtain explicit consent from customers before collecting and using their data. This can be achieved through opt-in forms or checkboxes that customers must select to indicate their consent.\nUse Data for Intended Purpose Only\nTo maintain trust, businesses must use customer data only for its intended purpose. For example, if a customer provides their email address to download a whitepaper, that email address should not be used for any other purpose, such as marketing emails, unless the customer has explicitly consented to such use.\nSecure Data Storage\nTo protect customer data, businesses must ensure that it is stored securely. This includes implementing strong passwords and encryption, as well as regularly auditing data storage systems for vulnerabilities.\nConclusion AI-driven lead generation strategies have the potential to transform the way businesses identify and engage with potential customers. However, the use of customer data in these strategies must be ethical and responsible, ensuring that customer data is protected and used only for its intended purpose. By being transparent about data use, obtaining explicit consent, using data for its intended purpose only, and securing data storage, businesses can leverage AI to generate leads while maintaining trust with their customers.', 'CalOPPA vs CCPA: What You Must Know to Comply With Both\nEnsuring data security is essential, especially when it comes to dealing with the personal information of clients and customers who frequent your website on a regular basis.\nUnfortunately, there are times when lapses in data security have led to the breach of clients’ sensitive personal information. As an online business, you have to make sure that this does not happen; otherwise, you could risk losing the trust of your clients forever, not to mention financial damages.\nIn the USA, the state of California has established several laws to make sure that its residents’ personal information is protected at all times. It is even more necessary now, given the fact that California’s GDP is one of the biggest in the world, larger than most nations.\nSince there’s a lot at stake, online businesses and websites need to make sure that their systems are strong and stable enough to protect the information and assets of Californian residents.\nTable of contents\nThe Establishment of CalOPPA and CCPA\nIn its bid to strengthen data security in the state, California has established a couple of laws that seek to govern the ways online businesses and websites treat their clients’ personal information.\nCalifornia created two main laws that website admins and businesses need to comply with, namely, the California Online Privacy Protection Act (CalOPPA, effective 2004) and California Consumer Privacy Act (CCPA, effective 2020).\nTogether, these two laws seek to address different but complementary aspects of data security for the benefit of Californian residents.\nIf you own or manage a website or an online business, you should make sure that you are complying with the guidelines set by CalOPPA and CCPA. To give you a quick background, here’s a look at what the California Online Privacy Protection Act and California Consumer Privacy Act are all about.\nCalifornia Online Privacy Protection Act\nAdditionally, you should also disclose if you share their information with any third party, as well as how the sharing process is. As a law, CalOPPA has a broader scope compared to CCPA, so you should ensure compliance across the board at all times.\nThe California Online Privacy Protection Act has been active since 2004. It remains one of the top online privacy regulations in the United States even today.\nCalifornia Consumer Privacy Act\nThe California Consumer Privacy Act (CCPA) is a privacy law that took effect at the start of 2020. It seeks to complement the already-existing privacy regulations in California like CalOPPA and Shine the Light law.\nHere are some of the essential sections of the CCPA that you should know:\n- It introduces additional rights for consumers that websites and online businesses should comply with and help facilitate.\n- It broadens the definition of “personal information” in its bid to cover as many bases as possible.\n- It strengthens the transparency obligations of companies that deal with the sale of personal information.\n- It presents a new set of fines for websites and online businesses that fail to protect the personal information of their clients.\nCCPA has a narrower coverage compared to CalOPPA. Compliance isn’t any easier, but you should be fine as long as you take into mind the critical sections of the law.\nWhile CalOPPA and CCPA do have their differences, they both have the same goal: to protect the data privacy of Californian residents. Now that you have an idea of what these Californian privacy laws are all about, it is time to delve deeper into the similarities and differences of these laws.\nCalOPPA vs CCPA\nAs mentioned, The California Online Privacy Protection Act and the California Consumer Privacy Act were both established to protect and uphold the data privacy rights of residents of California.\nIf you own or manage an online business or website, you should know the differences and similarities between the two so that you will be able to comply with their requirements.\nThe biggest similarity between CalOPPA and CCPA is their scope. The California Online Privacy Protection Act and the California Consumer Privacy Act are data protection laws that seek to protect all consumers in the state of California.\nThese laws both consider “consumers” as anyone residing in California. For online businesses and websites, that means these laws can cover you if you host information from Californian residents, regardless of where you are based.\nHowever, while both of these acts seek to address commercial enterprises, they differ in terms of the type of business they target.\nScope of the California Online Privacy Protection Act\nThis act applies to commercial websites or online services that collect personally identifiable information about consumers residing in California. It doesn’t matter if the company is based in Singapore or New Zealand. As long as the website carries information from Californian residents, all companies must comply.\nHowever, it should be noted that while CalOPPA does have a broad scope, it doesn’t cover ISPs or other types of services that take care of processing such information on behalf of third-party operators.\nThis law is only targeted to the website and online business owners and operators. Also included are mobile applications and other platforms where Californian residents can digitally access the web pages online.\nScope of the California Consumer Privacy Act\nJust like CalOPPA, the California Consumer Privacy Act covers businesses that deal with people residing in California. However, CCPA has a much more stringent qualification when it comes to their target businesses. The businesses that CCPA covers should meet the following qualifications:\n- It is profit-driven.\n- Its operation includes California.\n- It determines and controls how the processing of the clients’ personal information is done. In other words, it regulates the “purposes and means” of consumer data.\nAdditionally, CCPA businesses should meet one or more of the following criteria:\n- It has an annual gross revenue worth more than $25 million.\n- It makes at least half of its annual revenue by selling the personal information of its users and consumers.\n- It sells, buys, shares, and receives personal information from at least 50,000 households, consumers, or devices.\nIf the business meets the required and additional qualifications, then they should make sure that they comply with the rules set by CCPA. Upon a closer look, it should be apparent that CCPA is more targeted towards large corporations, data brokers, and big social networking websites or applications.\nWhat does personal information mean?\nCalOPPA and CCPA have different definitions when it comes to “personal information.” As a business covered by any of these laws, you should be able to differentiate the two to ensure compliance.\nIn CalOPPA, personal information is referred to as “personally identifiable information.” It covers individually identifiable information regarding a consumer that is collected and maintained by an operator. Examples of “personally identifiable information” include the following:\n- Full name,\n- Home address,\n- Email address,\n- Contact number, and\n- Social security number, among others (cookies, IP address, etc.)\nIn CCPA, personal information is any type of information that relates to, describes, identifies, or can be associated or linked with a household or an individual consumer.\nIn addition to the information needed in CalOPPA, it also includes the search and browsing history, consumer interaction with an application, advertisement, or a website, and other types of data.\nBusinesses need to make sure that they understand the differences between the two so that they can properly comply with the regulations set by these laws.\nWhat are the requirements under each act?\nCCPA is more complicated, as it is more specific than CalOPPA. CCPA requires businesses to provide the following rights to their consumers:\n- The right to know\nBusinesses are required to disclose the type of personal data that they collect, sell, or share.\n- The right to removal\nConsumers are allowed to remove or delete their information from the database of the operator under certain conditions.\n- The right to refuse\nConsumers have the right to refuse the sale and processing of their information.\n- The right to non-discrimination\nBusinesses are not allowed to discriminate against consumers who have used or exercised the rights listed above\nWhat are the penalties for non-compliance?\nYou should make sure that your online business or website complies with the rules set by CalOPPA and/or CCPA. Non-compliance can lead to penalties — something that you wouldn’t want to wish on your business’ financials.\nUnder CalOPPA, failure to comply can lead to a maximum of $2,500 per violation. While you may think that this is cheap, you should note that every “violation” counts each consumer visit on your website for the duration of your non-compliance.\nIf your website had high traffic during that period, it could add up to hundreds of thousands or even millions of dollars.\nFor CCPA, the Attorney General can issue up to $2,500 as well. Of the total penalty, 20% will be forwarded to a consumer privacy fund to help attorneys in recovering the costs of legal action.\nCCPA also fines intentional violators of up to $7,500 per violation, and private consumer claims can lead to fines between $100 and $750.\nAs a responsible online business owner or manager, you have the duty to monitor your compliance with the following laws. By doing so, you can strengthen your relationship with your customers and avoid paying hefty fines.\nCalOPPA and CCPA are both data protection laws that require commercial enterprises doing business in California to adhere to requirements in order to safeguard the data privacy of its residents.\nCalOPPA seeks to address all types of businesses running commercial websites, while CCPA only addresses big businesses and data brokers.\nThey both have different requirements, but they have the same goal: to make businesses accountable and transparent when it comes to collecting, processing, selling, or sharing the private information of their customers.\nAs a business owner, manager, or operator, you must make sure that you know the specifics when it comes to the similarities and differences of CalOPPA and CCPA. Doing so ensures that you will remain compliant with the requirements and regulations that each act has set.\n- Updated on February 26, 2021']	['<urn:uuid:e5b7073c-83c9-48d1-9a1f-3b4c9f191a18>', '<urn:uuid:4baffe78-5daa-45fd-b994-0d49e51ca88e>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	11	112	2231
49	leading cause foodborne disease outbreaks us	Norovirus is the leading cause of foodborne disease outbreaks in the U.S., sickening more than 20 million people nationwide each year.	['In warm-weather months, who doesn’t love to get outside for picnics, backyard gatherings, and of course delicious foods? But high temperatures raise your chance of getting sick from things you eat. Learn how to handle food properly to avoid the misery of food poisoning.\nIt can be hard to keep foods safe to eat during warmer weather. If you’re eating or preparing foods outside, you may have trouble finding places to wash your hands, keep foods cold, or cook at the proper temperature—all of which are important to prevent foodborne illness.\n“Food poisoning occurs if the foods you eat contain certain microbes or the toxins they produce,” says Dr. Alison O’Brien, a food safety expert at the Uniformed Services University of the Health Sciences in Maryland. “You can get sick directly from swallowing the toxins. Or you can get sick if the microbes get into your gut and start to multiply.”\nEach year, about 1 in 6 Americans get sick from tainted foods. Most foodborne illnesses arise suddenly and last only a short time. But food poisoning sometimes leads to more serious problems. Foodborne diseases kill about 3,000 people nationwide each year. Infants, older people, and those with compromised immune systems are especially at risk.\nMany people know the symptoms of food poisoning: vomiting, diarrhea, abdominal pain, fever, or chills. The sickness may be mild or severe. It may last from a few hours to several days. The symptoms and length of illness depend on the type of disease-causing microbe or toxin you’ve swallowed.\nThe leading cause of foodborne disease outbreaks in the U.S. is norovirus. This highly contagious virus sickens more than 20 million people nationwide each year, leading to vomiting and diarrhea. Norovirus outbreaks can occur anywhere people gather or food is served.\n“You can get norovirus when a sick food handler contaminates your food, possibly by not washing their hands well enough after touching the virus,” O’Brien says. “Swallowing just a little norovirus can make you very sick.”\nSeveral types of bacteria can also cause food poisoning. Some foods you buy—such as raw meat or fruits and vegetables—may already contain bacteria that you need to wash off or cook to destroy. Bacteria can also thrive in certain foods if not stored properly.\nBacteria like Staph and Bacillus cereus can make you sick quickly, within 1 to 7 hours. These bacteria produce fast-acting toxins in foods (such as meat or dairy for Staph, and starchy foods like rice for B. cereus). Keeping such foods refrigerated at 40 °F or colder helps slow or stop the growth of these bacteria.\nOther bacteria, such as Salmonella and Campylobacter, don’t make you sick until they get in your body and multiply. With these microbes, it can take 12 hours or a few days for you to feel ill. “Symptoms can include fever, cramps, and sometimes bloody diarrhea,” says O’Brien.\nWhen you have a foodborne illness, you usually need to drink plenty of fluids. “But see a doctor if you have blood in your stool,” O’Brien advises. “And if a child seems to have food poisoning, you should have the child seen by a doctor.”\nJanuary 30, 2015 //\nHuff Post Live Pleasurable Weight Loss author Jena La Flamme finds sexual act...\nJanuary 29, 2015 //\nBy Naomi MacKenzie -Blackdoctor.org Blueberry skincare! Who knew something ...']	['<urn:uuid:379a167f-c3f7-4ef2-a260-0f51cf784cab>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	6	21	554
50	I keep hearing about cultural fusion in music and mental health practices. How are Latin music elements being used in modern dance music, and how does this relate to new approaches in workplace stress management?	Latin music elements have become integral to modern dance music, particularly through moombahton, which blends electronic dance music with Latin-influenced reggaeton beats. This fusion has led to numerous crossover hits and collaborations between EDM producers and Latin artists, as seen in tracks by various DJs and producers. Similarly, this fusion approach is reflected in workplace stress management, where organizations are combining traditional business practices with wellness initiatives. According to recent data, employers are increasingly offering meditation and mindfulness training to address the growing mental health crisis, which affects 41.5% of US adults. This integration of different approaches aims to reduce workplace stress and anxiety, which currently costs businesses over $45 billion in lost productivity.	"[""Best known for his arena-caliber EDM and bombastic bass drops, Los Angeles-born DJ-producer Dillon Francis has a long and storied history with Latin music. He began releasing mostly dubstep tracks in 2010, when the then-nascent moombahton genre — which combines electro-house with Latin-tinged reggaeton beats — first took the Internet by storm. Francis then notched his first major hit in 2011 via “Que Que,” a Spanish-language track alongside mentor and frequent collaborator Diplo, featuring Dominican-American artist Maluca. Fast-forward to 2018, and Francis has reached the cusp of a new career chapter: Dillon Francis, Latin pop crossover star.\nSeptember 28th sees the release of Wut Wut, Francis’ second full-length project and his first (mostly) Spanish-language album. Executive produced by Mexican EDM vet Toy Selectah, Wut Wut features a cast of collaborators from across the Latin pop spectrum including Ximena Sariñana, iLe of Calle 13 fame, Lao Ra, De La Ghetto and others. As the follow-up to his 2014 debut, Money Sucks, Friends Rule, the new album sees Francis expanding his production palette into sounds sweeping pop radio: from his moombahton roots on the club-ready “BaBaBa” featuring Young Ash, to rugged Latin trap on “Ven” featuring Arcángel and Quimico Ultra Mega.\nRecorded in New York City, Miami, Mexico City and the Dominican Republic, Wut Wut comes during the height of today’s Latin pop explosion, which has shepherded multiple crossover hits across mainstream genres. EDM is one of the latest industries to ride the Latin pop wave: Steve Aoki’s track, “Azukita,” features reggaeton king Daddy Yankee, Latin trap duo Play-N-Skillz and merengue icon Elvis Crespo; while David Guetta‘s new album, 7, features two tracks with J Balvin, including the Spanish-language “Para Que Te Quedes.”\nBut as Francis puts it, Wut Wut is no basic bandwagon fare. Rather, it’s his way of paying homage to the sounds that gave him everything. “Latin music is a part of what I do,” he tells Rolling Stone. “I’ll never not be making moombahton.” Rolling Stone spoke with the American producer about his latest venture into Latin pop and Latin music’s ongoing influence on his career.\nFirst of all, you grew up in L.A. Do you speak Spanish?\nUhhh … más o menos (more or less). [Laughs]\nSo then there’s a potential that you don’t know what the singers are singing about on your new album.\nYes, but there’s a reasoning behind it. Whenever I would get in the studio with anyone that spoke English, anytime they would ask me for any advice, I would give them the most horrible advice on what to sing. When I got in the studio with most of the people [for Wut Wut], they would tell me what they’re talking about and then I would say, “Hey, can you sing it this way?” Rather than telling them what to sing about, it would just be more like melody and production, so it was more of using their voice like another instrument.\nWhat were some of your earliest memories with Latin music, especially for someone like you who grew up in Los Angeles?\nI definitely remember the Cadillacs from Training Day just blasting reggaeton. I feel like that was like the most cliché thing to happen in Los Angeles, just an amazing lowrider playing a Daddy Yankee song or something. Growing up in West L.A., you just hear it all the time. But I never really deep dove into the music until I found this dude Munchi‘s music on SoundCloud. That was when I was starting to become a music producer. Then I started seeing this little tag called “moombahton.” When I started hearing it, I was like, “Oh my God!” I’m trying to DJ, I don’t know what tempo this is. It just sounds incredible — I need to see what’s happening. I had a house record called “Masta Blasta” … I put it up on SoundCloud, I sent it to Munchi. I was like, “Alright, this is the genre for me.” There [were] no rules yet and the community was so tight because there [weren’t] a lot of producers doing it. That’s really where I started deep diving into the sounds and everything with it.\nThe album has a long history, dating back to the 2016 Latin Grammys where you met De La Ghetto and Latin alternative producer El Guincho. How does the initial sketch of Wut Wut compare to the final product?\n[Originally], we weren’t even going to get a lot of vocals on it. It was gonna be like one-shots in Spanish, like doing “Get Low” in Spanish. We hit up Toy Selectah. He got a bunch of one-shots, but he also got a bunch of verses from people. From that, it kind of just started turning into something else. There [were] no expectations. The whole record was just to go have fun again and have a great time making music.\nEven before that, Wut Wut traces back to your 2011 Westside! EP, which features “Que Que,” one of your biggest moombahton hits and one of your earliest Spanish-language tracks. What’s the connection between Westside! and Wut Wut?\nThe record “BaBaBa” with Young Ash [off Wut Wut] is the closest record to what “Que Que” was when I was making it. I’ve made songs like “Anywhere” and “Coming Over” [with Kygo], where they have more melodic sensibilities. It was taking that and trying to put it into a project that actually is cohesive. If you listen to my last album, that is the most non-cohesive project ever. That’s where I was like, “I need to focus and make everything sound like they actually exist in one project.” That’s what I felt like I did with Wut Wut, where it was like honing in on everything that I’ve learned in the past 10 years of touring and making music and trying to make a cohesive album that has a great concept.\nAre you now noticing an EDM-Latin pop crossover trend?\nI guess so. I’m not trying to say, “Hey, I’m the first one that was doing it before it became a thing.” But I’ve been doing it since 2012, and I was just kind of going back to it. When we were working on [Wut Wut], “Despacito” and “Mi Gente” didn’t exist yet. Neither did “Sorry” with Justin Bieber, which I feel like was a big catalyst. A lot of my fans will always be like, “It sucks that moombahton’s dead.” And it’s funny because every single pop record in the past year has been a reggaeton/moombahton record, with like a melodic vocal drop. If anything, it’s like the most thriving genre possible right now. Mine was just going back to having fun and working with artists that I thought were just going to heighten the project and understand what the project was.\nSo this album is in no way hopping on the bandwagon of the Latin pop explosion?\nI never really wanted to say that, but I feel like people could see that if they look at my history of music. But [before] I put out the first record [off the album], “Ven,” I had literally cleared out my Instagram to two things: a picture with Arcángel from back when we worked on the record and then the other one was a video with Toy Selectah and I in the studio with De La Ghetto and El Guincho working on the records, and it was [dated] like November 4, 2016. So I was trying to lightly do that. [There have] definitely been people calling me out, being like, “Dude, you’re jumping on the bandwagon, man. You’re trying to be like ‘Despacito.'” If you listen to the album, none of it sounds like “Despacito.” It literally all sounds like my music.\nYour brand and your image have been tied to a lot of aspects related to Latin culture, from your history with moombahton to your social media character, Gerald the Piñata. You even brought out a mariachi band at Lollapalooza earlier this year. Have you ever faced any backlash from the Latin community or other groups for using such staples of their culture?\nNo. It’s pretty nuts that I haven’t, to be honest. When I was in Mexico promoting “We the Funk” [featuring singer Fuego], everyone was like, “Thank you so much for making music in Spanish, thank you for pushing the music.” That’s really what the whole project was about: to show people where I originally got [inspiration] from. Moombahton is reggaeton meets dance music, and that’s why I wanted to work with Spanish [-language] artists, so that people could understand that this is where the music came from. So if anything, I’m just trying to give back to where a lot of my success came from.\nWhen you look back, what role would you say moombahton and Latin music have had on your overall career?\nI’ve always gone back to it. I like taking a break from it because I do produce it so much. But I always want to try to bring something fresh back when I go into it, because it’s such a fun [sound]. It’s one of those things where [if] it’s just the perfect setting, it goes off.\nWill we see more Latin albums or Spanish-language albums from you in the future?\nLatin music is a part of what I do. I’ll never not be making moombahton. And I’ve definitely been working with other artists as well, where it’s working on records just for them to put out. I’m definitely gonna be doing a lot more production stuff for Latin artists, which is something I never thought that people would be coming to me for."", 'Poor mental health costs businesses more than $45 billion\nNearly 50 percent of US workers cite their job as the primary cause of their poor mental health. In a recent Gallup Poll, a growing number of employees report extreme stress and anxiety at work, with forty percent stating that their job negatively impacts their mental health. Half of these workers are Millenials between 18-29 and account for over one-third of the US workforce.\nWith the incidence of poor mental health cases related to work rising, so does the cost. For example, the average worker calls off work unexpectedly, only 2.5 days annually. By contrast, those under severe mental strain take 12 days off each year, at an estimated $46.7 billion in lost productivity. This growing price tag has business experts wondering if a mental health pandemic is on the horizon.\nThe symptoms of anxiety and depression disorders include prolonged feelings of anger, stress, worry, and sadness. Left unattended, poor mental health can destroy employees’ teams, families, schools, and institutions. Currently impacting 41.5% of US adults, mental health disorders are a growing threat to workplace ideas and energy. Small businesses, which employ 50% of the American workforce, are at the most significant risk and paying a high price tag.\nGlobal Healthcare Organizations call on businesses to address employee mental health.\nIn the last year, global healthcare organizations are recognizing that we are on the edge of a potential mental health crisis at work and are calling on leaders to address their negative impact on the mental health of their employees.\n“The workplace is the missing link in improving population mental health,” Dr. Leslie Hammer, co-director of the Oregon Healthy Workforce Center and professor of occupational health psychology at Oregon Health & Science University, told ABC News.” The next steps are for workplaces to look hard at the culture around mental health support.”\nIn October, the US Surgeon General released the “New Framework for Mental Health and Wellbeing in the Workplace,” reinforcing workplaces’ role in supporting their employees’ mental health and well-being. A similar recommendation was issued by the World Health Organization a month earlier, identifying three areas of change that would significantly move toward creating a positive mental health culture at work. They were:\n1. Training for managers to improve their ability to recognize and respond to employees who might be experiencing emotional distress\n2. Training for employees to improve their mental health awareness and knowledge\n3. Easy access to mental health resources within existing health programs to improve individual mental health and reduce its stigma in the workplace.\nTraining builds employee insights\nIt can be challenging for managers to initiate a caring conversation if they notice an employee struggling with negative thoughts and feelings when mental health, anxiety, and depression. Despite growing public recognition and acknowledgment of mental health illnesses, many organizations carry a negative bias towards their discussion at work. However, according to Johann Berlin, CEO at TLEX Institute, organizations were trending away from transactional interactions and repetitive tasks to more adaptive, empathetic, and connected cultures before the Covid19 pandemic and beginning to offer training to build mindfulness and emotional intelligence. Today, nearly 22% of employers now offer meditation and mindfulness training with access to apps like Calm or Headspace to reduce employee anxiety, improve working memory and increase creativity.\nThe business case for mindfulness training remains positive for personal wellness and relationships. But as Mark Sidney notes, the benefits are only possible through sustained practice. “Theory informs practice,” he states, “True mindfulness involves developing a deeper understanding of the processes and foundational attitudes necessary for mindfully living and working.”\nLasting impact requires more than insight. It requires Mental Fitness.\nMost of our attempts to make positive changes fail. Why? Because we stop at insight and don’t build habits. We lose momentum after the initial excitement of learning a new skill and return to our former practice. But recent advances in neuroscience research offer hope for lasting change by retraining our brains.\nIn his NY Times Best Selling book, Positive Intelligence, Shirzad Chamine describes how building mental muscles activate and build new neural pathways. Using new functional Magnetic Resource Imaging (fMRI) technology, he describes the research that shows tissue growth in the region of the brain where positive emotions originate.\n“All our negative emotions – stress, anxiety, self-doubt, anger, shame, and frustration – exist physically in our left brain. In contrast, positive emotions like peace and joy live in the right brain. Building and strengthening the neural pathways that create a positive mindset and reduce negative ones are called mental fitness.”\nBuilds mental muscles. Increase Mental Fitness. Create positive lasting change.\nMental fitness, by definition, is the ability to handle life’s challenges from a positive mindset with less stress. Using factor analysis, the breakthrough research by Positive Intelligence identifies three core mental muscles, radically simplifying mental fitness. Validated by more than 500,000 individuals, including students, CEOs, elite athletes, and sales, operations, and technology teams, the Positive Intelligence Mental program is proven to build measurable improvements within six weeks of practice.\nStrengthening the mental fitness of individuals and teams may offer businesses a simple and lasting way to avoid the potential mental health pandemic and improve wellness, relationships, and performance in the process.\nFor BusinessLearn More\nCouncil, Forbes. “This Expert Says Meditation Is On The Rise Among C-Suite Executives.” Forbes, Forbes, 20 Oct. 2020, https://www.forbes.com/sites/forbesmarketplace/2020/10/20/this-expert-says-meditation-is-on-the-rise-among-c-suite-executives/?sh=40bf67204281. Accessed 7 Nov. 2022.\nSidney, Mark. “More than Just Well-Being: The Business Case for Mindfulness at Work.” LinkedIn, LinkedIn, 8 Apr. 2019, https://www.linkedin.com/pulse/more-than-just-well-being-business-case-mindfulness-work-mark-sidney. Accessed 8 Nov. 2022.\nSutton, MD, Darien. “How to Spot Work Burnout during the Holidays.” Good Morning America, Good Morning America, 5 Nov. 2022, https://www.goodmorningamerica.com/wellness/video/spot-work-burnout-holidays-81912402.\n“U.S. Surgeon General Releases New Framework for Mental Health & Well-Being In the Workplace.” U.S. Department of Health & Human Services, 20 Oct. 2022, https://www.hhs.gov/about/news/2022/10/20/us-surgeon-general-releases-new-framework-mental-health-well-being-workplace.html#:~:text=Vivek%20Murthy%20released%20a%20new,of%20workers%20and%20our%20communities. Accessed 7 Nov. 2022.\nYegiants, Dr. Anna. “Nearly Half of Workers Say Their Job Hurts Their Mental Health, Survey Finds.” Good Morning America, Good Morning America, 11 July 2022, https://www.goodmorningamerica.com/wellness/story/half-workers-job-hurts-mental-health-survey-finds-92672959.']"	['<urn:uuid:e0eba87a-2fc5-4425-ad54-5ac57def8a13>', '<urn:uuid:8cc70771-6325-4fe4-8449-f669804490f7>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	35	114	2609
51	How do cloud computing standards help improve both data security and project management success when working with external service providers?	Cloud computing standards address both security and project management through multiple frameworks. For security, ISO/IEC 27017:2015 and 27018:2019 provide specific guidelines for protecting information and personally identifiable data in cloud services. These standards are complemented by NIST SP 800-144, which offers recommendations for implementing secure public cloud environments. For project management success, ISO 10006:2003 provides quality management guidelines for projects, while the Project Management Body of Knowledge standard (recognized by ANSI and ISO) covers efficient project management practices. These standards ensure transparent collaboration by establishing common terminology and management approaches between clients and service providers. Additionally, companies can verify compliance through SOC 2 reports that document both security controls and operational practices used by vendors.	"[""Industrial standards are the focus of discussions that have arisen multiple times within the IT outsourcing community. Without comprehensive and clear standards, we cannot acknowledge the best contractors. Neither can we stabilize our highly volatile market.\nConsidering the fact that IT outsourcing is an international industry that involves enterprises and contractors from all over the world, having only locally acknowledged standards is not enough. There are obvious legal issues that prevent productive collaboration in our industry.\nFor example, when an enterprise based in the UK outsources some business process abroad, they often risk getting scammed. The issue is that some UK laws may not be recognized in Russia, India, Poland or any other country for that matter.\nLegal issues are simply the only reason for international industrial standards to exist. Admittedly, the IT outsourcing industry is fairly young and many quality and managerial standards are still far from being perfect. We are still struggling with creating the best acceptance testing methodologies. There are issues with general terminology.\nTo top that all off, we have to acknowledge certain issues related to technology. Both our clients and we need to understand what technology standards are recognized as the best by the industry. Joint control over technological aspects of outsourcing is crucial for modern business relationships. Again, we have to refer to generally accepted international standards.\nThere are many important international standards that are essential for IT outsourcing. Let’s talk about these standards and how they make outsourcing development services better.\nOutsourcing operations must be placed in the hands of professional service providers! Your future is at stake!\nGuidance for the outsourcers – ISO 37500:2014\nFirst of all, we need to recognize that IT processes and software development are not the only outsourceable business processes. However, many companies prefer to operate internally those business processes that they are familiar with and move other processes to outsourcers. Generally, such processes are related to the IT industry and software development.\nNevertheless, there is a plethora of other important business processes that can be outsourced in order to standardize outsourcing in general. In November of 2014, the international standard for outsourcing was accepted. We call this standard ISO 37500:2014 “Guidance on Outsourcing”.\nThis standard has clear goals and offers both the clients and the contractors common terminology, simple outsourcing guidelines, and suggests suitable solutions for arranging healthy outsourcing contracts. There are many important areas of contractual relationships that this standard covers. However, we want to highlight two important issues that often make IT outsourcing contracts problematic.\n- Issue #1—Business requirements. Frequently, the parties entering a contractual relationship fail to recognize how to specify the subjects of the contracts. The ambiguity leads to critical misunderstandings and breaks many outsourcing arrangements. ISO 37500:2014 enables clearer communication and forces us to discuss various business requirements until both parties fully understand them.\n- Issue #2—Acknowledging risks. Whenever an enterprise outsources some of its business processes, there are inevitable operational and financial risks. Unfortunately, many enterprises fail to recognize such risks and question the very way we structure our typical contracts that factor in risks characteristic to our industry.\nISO 37500:2014 successfully addresses these issues. Obviously, we need to search for specific answers ourselves, but the general guidelines certainly help in building robust consensus. If your outsourcing partner recognizes this standard, the contracts will be more clear. You will have a better understanding of how you can supervise these relationships. Notice that this standard accounts for all parties that can be involved in outsourcing arrangements including stakeholders, service providers, and clients.\nThe best outsourcing development services and IT outsourcers usually adopt the ISO 37500:2014 standard. It is decidedly advised for any participant of outsourcing arrangements to study this standard deeply.\nThe standard for developers – ISO/IEC 12207:2008\nIf you plan to outsource software development, you need to ensure that your service provider recognizes this international standard. ISO 12207:2008 focuses on establishing a comprehensive and understandable environment for software development cycles. This standard defines commonly accepted terminology and allows cross-referencing within our industry.\nThis standard enables both clients and contractors to clearly understand how a software product should be managed, developed, supplied, deployed, and maintained. ISO 12207 covers all types of software. Notice that many technological processes related to managing software, regardless of whether they are performed internally or externally, are also covered by this standard.\nISO 12207 defines six critical lifecycles of a software product. It is crucial to recognize all of them and understand the core principles of acquisition, supply, development, maintenance, operation, and even destruction. However, the standard can be used partially depending on the type of activities you are most interested in. For example, you can refer specifically to this standard during software development.\nThe standard specifies what development is. During this stage the software product is being planned, designed, developed, and tested. The result of this process is the product that can be sold to the customer. The standard specifies activities that accompany the development phase and allows both the client and the contractor to understand more clearly the purpose of their outsourcing arrangements.\nA non-removable portion of this standard is testing and that is also something worth mentioning. Many problems that arise during the acceptance phase are as a consequence of ambiguous definitions of tests, their results and their conditions. This leads to incomplete testing and boring acceptance tests.\nBe sure to acquaint yourself with this standard and check if your contractor employs it. ISO 12207:2008 is a commonly accepted fundamental international standard for business arrangements related to software products. This standard is equal to IEEE 12207. Do not be confused if your contractor employs this standard instead of ISO 12207—as mentioned before, the standards are equal and operate with the same terminology.\nKeep things secured – ISO/IEC 27018:2014\nWith so many modern applications offering their own public clouds or collaborating with Amazon or Google, we need to focus on protecting Personally Identifiable Information (PII) that can be used in clouds. This is a very important issue for the industry and we have to address it properly. This specific standard is only a part of ISO/IEC 27017 that covers a much wider range of security issues related to cloud computing. However, we feel that it is important to highlight this standard as it is often crucial for software development projects.\nOftentimes, specialists refer to the group of international standards related to various security issues as “ISO27K”. This whole set of standards focuses heavily on various aspects of cyber defenses and methods of data protection. We highly recommend you to study this group of standards if you want to outsource any of your IT processes.\nIt is imperative to recognize that all IT processes are related to using, storing, and accessing data. Modern outsourcing development services should be provided by competent contractors that can guarantee certain level of protection. This is crucial for the very survival of any enterprise that wants to incorporate innovative methods of interacting with its clients.\nThe level of protection is guaranteed by complying with international standards that specify how data should be protected and what methods one should use to ensure intelligent protection. With the amount of cloud-based technologies increasing, specific security standards become even more relevant than ever before.\nEngineers should comply – ISO 15288:2015\nThis is a standard that was recently updated and addresses multiple aspects of designing and building sophisticated engineering and software systems. The main goal of this standard is to ensure customer satisfaction. The standard defines terminology and describes activities related to any of a system’s cycles. At the same time, ISO 15288 ensures that the stakeholders are involved in managing the whole process of building the engineering system.\nMany software products can be recognized as complex systems and many hierarchical engineering systems are based on a fundament made of interconnected software products. This is why we have to refer to this standard when creating software for our enterprise clients.\nThis standard ensures that the systems we build have important features that are accepted as necessary by the majority of the industry specialists. While the architectures of the systems are specified in ISO/IEC/IEEE 42010, many specific activities related to actually developing, implementing, and maintaining various software solutions are defined in ISO 15288.\nQuality requirements are vital – ISO/IEC 25010:2011\nThis is probably one of the most important standards that both enterprises and contractors need to recognize. When we talk about outsourcing development services, we have to discuss what quality standards should be provided by the contractor and accepted by the customer. Removing ambiguity from the very process of quality acceptance is a crucial part of creating successful outsourcing arrangements.\nISO/IEC 25010 defines specific outcomes of interactions. These outcomes determine whether the product serves this specific purpose. The context of use is very important for defining if the product meets the quality requirements specified by the contract. One of the best things about this standard is that it can be applied to both software systems and computer systems.\nThere are multiple aspects of software development that can be separately evaluated and identified. This also means that the term “quality” is multi-dimensional and can be applied to various development activities and product features separately. Such an approach allows us objectively to judge the software product and determine its level of quality. Simultaneously, the standard evaluates whether the requirements are reasonable and can serve as a model for measuring the quality of the product.\nHere are the quality models used in this standard:\n- Evaluating and defining software requirements;\n- Evaluating the clarity and integrity of said requirements and their definitions;\n- Evaluating and describing project design and testing objectives;\n- Defining quality control criteria and quality assurance methods;\n- Defining acceptance criteria and acceptance testing activities;\n- Defining measures of quality.\nThis standard addresses crucial aspects of software development. There are many strong software developers. However, many contractors fail to deliver a high-quality product simply due to insufficient testing, and poor acceptance testing in particular. ISO/IEC 25010:2010 addresses the methods and measurements that we need to apply during quality assurance processes. Specifying acceptance criteria is also reasonable and helps to build a consensus between the developers and the stakeholders.\nKeep your management efficient – ISO 10006\nISO 10006:2003 is a set of guidelines related to quality management in various projects including, but not limited to, software development. This standard was developed and presented by the International Organization for Standardization. One of the core developers of this standard is PMI (Project Management Institute).\nISO 10006 specifies multiple rules and suggestions that should be used for quality management in various projects regardless of their complexity, size, and budget. While this standard is not an actual collection of rules, but a compilation of reasonable suggestions, commonly accepted terms, and recommendations, if your contractor uses this standard, you can expect high quality products and calculated project management.\nAnother important international standard created by PMI and acknowledged by ANSI and ISO is “Project Management Body of Knowledge”. This is a comprehensive book that includes standardized terms and general guidelines for project management. For any software developer it is imperative to know how to manage the project and deliver high quality products. This standard basically covers the most efficient project management practices and allows both contractors and customers to share the same knowledge and terminology related to managing complex projects.\nThe standard is included in multiple ISO and ANSI standards. We highly recommend you to study this book extensively. This is very good reading on top of being a recognized industry standard.\nThere are numerous industry standards that we need to follow in order to ensure that our collaboration with clients is transparent and comprehensive. If we are to provide solid outsourcing development services, we simply must live up to the standards determined by the best specialists and tested with time.\nAll of the above is just a list of standards that we believe are worth mentioning and talking about. However, more standards exist and some of them address very specific issues. There are more than twenty distinct standards related just to digital security. In order to pick the best contractor, ensure that they use those standards that are relative to your specific project.\nCheck out our related articles:\n- .NET Development\n- Banking & Finance\n- Communities & Social networks\n- Custom App Development\n- Development process\n- Digital Marketing\n- Drupal Development\n- E-commerce & Retail\n- IT Blog\n- IT News\n- IT News & Trends\n- IT Outsourcing\n- Java Development\n- Media & Entertainment\n- Medicine & Healthcare\n- Product engineering\n- Project & Resources planning\n- QArea inside\n- Software Testing\n- Start-up Development\n- Technology & Innovation\n- Travel & Hospitality\n- Useful Tips\n- Web Design\nGolang Vs Python: Which Language Is Best for AI ProgrammingRead more\nGo Community: The Best Golang conferences of 2018 (and a few to look out for in 2019)Read more\n7 Reasons to Truly Love MicroservicesRead more\nThe Best Languages for MicroservicesRead more\nQArea's Year: Summing Up 2018Read more\nWhat's New in Golang 1.11: Release Notes OverviewRead more\nWhy You Should Start Learning Dart and Flutter Right NowRead more"", ""Enthusiasm surrounding the rapid growth and acceptance of cloud technology resulted in the creation of numerous standards and open source activity focused on cloud users and their needs. This led to market confusion around which standards are the most appropriate -- a major cloud compliance challenge for enterprises.\nIt is incumbent upon organizations using cloud technology -- or those contemplating the use of cloud-based services -- to ensure the cloud providers selected comply with established standards and best practices.\nIT leaders can use this article to learn about cloud compliance standards and to understand which questions to ask existing and prospective cloud providers about their efforts at complying with specific standards, as well as how to select the appropriate standards for their specific organization.\nThe organizations developing cloud compliance standards\nNumerous professional and technical organizations address various aspects of cloud technology, offering their own standards, recommendations and guidance for successful cloud implementation.\nThis article is part of\nCloud Standards Customer Council (CSCC)\nCSCC is an end-user support group focused on the adoption of cloud technology and examining cloud standards and security and interoperability issues. It has produced numerous white papers and articles on cloud issues. It has been superseded by the Cloud Working Group and addresses cloud standards issues via its Cloud Working Group.\nDMTF supports the management of existing and new technologies, such as cloud, by developing appropriate standards. Its working groups, such as Open Cloud Standards Incubator, Cloud Management Working Group and Cloud Auditing Data Federation Working Group, address cloud issues in greater detail.\nEuropean Telecommunications Standards Institute (ETSI)\nETSI primarily develops telecommunications standards. Among its cloud-focused activities are Technical Committee CLOUD, the Cloud Standards Coordination initiative and Global Inter-Cloud Technology Forum, each of which addresses cloud technology issues.\nOpen Grid Forum (OGF)\nOGF develops standards for grid computing, cloud, and advanced digital networking and distributed computing technologies. Among its cloud-focused activities is the Open Cloud Computing Interface working group, which has developed several cloud operating specifications, including the OCCI Core specification and OCCI Infrastructure extension.\nOpen Commons Consortium (OCC)\nFormerly known as the Open Cloud Consortium, OCC provides management of cloud computing and data commons -- an open knowledge repository -- resources in support of a variety of academic and scientific research initiatives.\nOrganization for the Advancement of Structured Information Standards (OASIS)\nThis nonprofit organization develops open standards for security, cloud technology, IoT, content technologies and emergency management. Its various cloud technical committees include OASIS Cloud Application Management for Platforms, OASIS Identity in the Cloud, and OASIS Topology and Orchestration Specification for Cloud Applications.\nStorage Networking Industry Association Cloud Data Management Interface\nThis specification is now an ISO standard, ISO/IEC 17826:2012, Information technology -- Cloud Data Management Interface. Typically used by cloud storage systems developers, it defines an interface to access cloud storage and to manage the data stored within the cloud resource.\nThe Open Group\nThis consortium of technology industry organizations develops standards and accreditations for a variety of IT issues. Its Open Platform 3.0 Forum working group's activities focus on mobility, big data analytics and cloud computing.\nTM Forum Cloud Services Initiative\nTM Forum is a global consortium of technology firms that provides a collaborative platform to address technology issues. Its Cloud Services Initiative provides a resource to develop cloud standards to be used by technology firms and users alike.\nExplore widely used cloud compliance standards\nTwo organizations that have developed a number of cloud-focused standards are NIST and ISO. Here, review a sampling of current, commonly used cloud compliance standards from these respective standards organizations.\nNIST develops and distributes standards primarily for government use but which are widely used by private industry. Its Special Publications (SP) Series of standards, including the following, is used extensively in public and private sectors:\n- NIST SP 500-291 (2011), NIST cloud computing standards roadmap. This provides a compilation of available standards on cloud computing and examines standards priorities and where gaps in the standards exist.\n- NIST SP 500-293 (2011), U.S. government cloud computing technology roadmap. This provides a detailed framework and structure for cloud computing infrastructures. While designed for government applications, it can also be used in the private sector.\n- NIST SP 800-144 (2011), Guidelines on security and privacy in public cloud computing. This standard provides guidance and recommendations for implementing a secure environment in public cloud services.\n- NIST SP 800-145 (2011), The NIST definition of cloud computing. This standard describes important aspects of cloud computing and serves as a benchmark for comparing cloud services and deployment strategies. It also provides a foundation for discussions on cloud computing and how to use it.\n- NIST Standards acceleration to jumpstart adoption of cloud computing. This group performs three activities that work together to encourage greater use of cloud. First, it recommends existing standards. Second, it coordinates contributions from various organizations into cloud specifications. Third, it identifies gaps in cloud standards and encourages outside firms to fill the gaps.\n- NIST Cloud computing program. This program defines a model and framework for building a cloud infrastructure and includes multiple advanced technology characteristics.\nThis is one of the primary global standards-making organizations. It develops standards for dozens of different kinds of technologies and systems, including the following:\n- ISO/IEC 17789:2014, Information technology -- Cloud computing -- Reference architecture. This standard defines cloud computing roles, activities and functional components, as well as how they interact.\n- ISO/IEC 17826:2016, Information technology -- Cloud data management interface. As mentioned above, this standard pertains to systems developers implementing and using cloud storage.\n- ISO/IEC 18384:2016, Information technology -- Reference architecture for service oriented architecture (SOA). This standard defines the vocabulary, guidelines and general technical principles underlying SOA, which are often deployed in cloud platforms.\n- ISO/IEC 19086-1:2016, Information technology -- Cloud computing -- Service level agreement (SLA) framework. This standard provides the framework for preparing SLAs for cloud services.\n- ISO/IEC 19941:2017, Information technology -- Cloud computing -- Interoperability and portability. This standard specifies the interoperability and portability aspects of cloud computing.\n- ISO/IEC 19944-1:2020, Cloud computing and distributed platforms -- Data flow, data categories and data use. This standard describes how data moves among cloud service vendors and users of cloud services.\n- ISO/IEC Technical Report 22678:2019, Information technology -- Cloud computing -- Guidance for policy development. This standard provides guidance for developing cloud-focused policies.\n- ISO/IEC Technical Specification 23167:2020, Information technology -- Cloud computing -- Common technologies and techniques. This standard describes technologies and techniques used in cloud computing, including VMs, hypervisors and containers.\n- ISO/IEC 27017:2015, Information technology -- Security techniques -- Code of practice for information security controls based on ISO/IEC 27002 for cloud services. This document provides guidance on the infosec aspects of cloud computing and cloud-specific infosec controls.\n- ISO/IEC 27018:2019, Information technology -- Security techniques -- Code of practice for protection of personally identifiable information (PII) in public clouds acting as PII processors. This document specifies guidelines based on ISO/IEC 27002, focusing on the protection of PII in public cloud environments.\nHow to select an appropriate standard\nTo determine appropriate cloud compliance standards for their respective companies, IT leaders should conduct research into the various cloud compliance standards, working groups and technical committees described in this article. Examine the standards being used by major cloud service providers, such as AWS and Microsoft. Chances are IT departments will have already performed considerable due diligence in these issues, so achieving compliance with standards will be an important outcome.\nConversely, when using a third party for a cloud implementation, check to see how it achieves cloud standards compliance. This can be incorporated into the evaluation process.\nAnother way to evaluate cloud providers' compliance efforts is to examine the most recently released Service Organization Control Type 2 (SOC 2) reports. SOC 2 reports examine the controls used by the vendor to protect customer data and verify the operational effectiveness of those controls. For cloud service providers, SOC 2 reports can also document the standards and practices the vendor uses to protect the security and privacy of user data.""]"	['<urn:uuid:30ecdf1a-07a2-4cb4-80af-ad70735aa264>', '<urn:uuid:6558dd36-142d-42e5-aa4f-e20e6abf0861>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	20	115	3536
52	What is the specific age requirement and diet restriction for calves to be considered suitable for veal production according to industry standards?	According to industry standards, veal comes from calves that are 8-16 weeks old and should not have eaten grass yet (they should be milkfed).	"['Presentation on theme: ""Veal is the meat of young, usually male calves that are byproducts of the dairy industry. Dairy cows must calve before they produce milk and calves (male)""— Presentation transcript:\nVeal is the meat of young, usually male calves that are byproducts of the dairy industry. Dairy cows must calve before they produce milk and calves (male) that are not used are used are slaughtered to make veal. Generally these calves are 8-16 weeks old and should not have eaten grass yet (milkfed). Veal is slaughtered and split into two halves across the natural curvature – foresaddle and hindsaddle containing both bilateral portions of the animal. These days although veal at times is split in halves like beef as well.\nShoulder The primal shoulder is similar to the beef primal chuck but contains only 4 ribs (5 in beef). Mainly used for ground meat or cubed for stewing or braising although steaks and chops can be fabricated but are less tender than those of the loin and rib areas. Deboned the shoulder can be roasted and/or stuffed and roasted\nForshank & Breast Located beneath the shoulder and considered as one primal although mostly used separately. The bones of the young calve in the breast area are still soft (cartilaginous) and the breast contains lots of fat and connective tissue. Moist cooking methods like braising breaks down during the cooking process and make the result very flavorful. Deboned the breast can be rolled and stuffed, cut in cubes for stews (blanquette, fricasse) or ground.\nRib The double rib also known as the veal hotel rack is very tender and expensive. The double rib consist of two racks connected by the backbone and is often sold split and can be deboned to produce two veal rib eye. On the inside of the rib is a portion of the veal tenderloin, however mostly the tenderloin is removed whole before separating the foresaddle form the hindsaddle. The rib is cut into chops and the deboned rib eye into steaks which are excellent for grilling, sautéing or braising.\nLoin The veal loin is the continuation of the rib and contains ribs 12 & 13. The loin consist of the loin eye muscle, the rib bones and the tenderloin imbedded on the inside of the two sides. The veal loin is very tender and the tenderloin is the most tender part of the veal. The tenderloin is mostly removed ahead of separating the leg and cut into medallions for dry cooking methods. The rib eye muscle can be cooked on or off the bone as roast or in chops (on or off the bones).\nLeg The primal veal leg consist of both the sirloin and the leg (incl. shank) Although the meat is fairly tender, the leg is mostly fabricated into cutlets and scallops. To fabricate these cuts the leg needs to be broken down into its parts, trimmed of the connecting tissue and cut against the grain of main and further tenderized/flattened with a mallet: Top round Eye round Knuckle Sirloin Bottom round (includes the sirloin) Butt tenderloin (if not removed prior) The hindshank is meatier than the foreshank and roasted or braised whole or sliced across for Osso bucco.\nSeveral organs called offal or innards of beef and veal are used in the food service industry. Beef Heart / tongue / tripe (stomach lining) / liver / kidney / oxtail / Veal Heart / tongue / liver / kidney / oxtail Sweetbreads (the thymus gland only used in young veal and lamb). These glands shrink when the animals get older.\nLamb- Lamb is the meat of sheep generally below 1 year of age Mutton – young female or castrated male sheep older than one year with more than two incisor teeth Hogget – young male or maiden female sheep with no more than two incisor teeth Baby Lamb – milk-fed lamb 6-8 weeks old, non grass/grain eater Spring lamb- milk fed lamb 3-5 months old, non grass/grain eater\nNew Zealand & Australia United States Argentina Europe United Kingdom - Wales and Scotland are particularly famous for their lamb Ireland\nPyrenees lamb 45 days old; 11 to 15 kg weight Exclusively milk fed From the southern French Pyrenees mountains Perigord lamb 90 to 150 days old; 60 days milk fed; 15 to 19 kg From central France region Paulliac lamb: Protected by European law, born and raised in the Gironde area of France Raised by mother milk only, resulting in pink rich tender meat. Kidneys are especially delicate. 75 days old and11 to 15 kg\nPre-Sale lamb From lambs that grazed on the salty herbal borderlands of north-western France, that periodically drenched by seawater. The high consumption of salt results in a more tender meat with juicier muscle tissue.\nLamb has a strong distinctive flavor and needs to be complemented with bold flavored sauces and accompaniments Meat from mutton and hogget is more distinct in flavor and tougher meat due to age and to the maturity of the connecting tissues.\nThe slaughtered lamb is butchered into the primal cuts: Shoulder Breast Rack Loin Leg As the carcass is not split after the slaughtering, some primal cuts of lamb contain both halves (legs for example). Lamb primal cuts are not classified into fore and hind quarters or fore and hind saddle like beef or veal.\nShoulder The primal shoulder contains many small bones and tough muscles whose grains travel in different direction. This fact make it difficult to cook and carve a whole shoulder although the shoulder may be cut into chops or deboned and (slow) roasted or braised. Mostly though lamb shoulder meat is cut for stews or ground for patties\nBreast The primal cut lamb breast includes the breast and the foreshank. The primal breast is located beneath the primal rack and contains the rib tips. Which are cut off to produce the rack. The lamb breast is not often used in the food service industry it can be stuffed and braised. The left over rib tips can be separated and are then called Denver ribs. The lamb foreshanks are meaty and can be braised used for broths or ground.\nRack The primal lamb rack is also known as hotel rack (saddle) and located between the primals shoulder and loin containing eight ribs and a portion of the back bone. The rack is valued for the tender rib eye muscle. The hotel rack is usually split and trimmed so that each set of ribs can easily be cut into chops. The rack can be grilled/broiled or roasted as rack or cut into lamb or single or double chops before cooking. Frenched lamb rack refers to a split rack, trimmed to the bones and cleaned bones ready to use.\nLoin The loin is between the rack and the leg and contains rib number 13, portions of the backbone and the loin eye muscle, tenderloin and flank. Except for the flank the meat is very tender and excellent for dry heat cooking – roasting and grilling/broiling. The loin can be deboned for to produce roasts or boneless chops or cut with bone in into chops. The loin eye can be deboned and cut into medallions or noisettes.\nLeg The lamb leg is separated from the loin by a straight cut leaving what would be the sirloin portion (in beef) as part of the leg. The primal leg is seldom used as is, as it mostly is split into two legs partially deboned or fully deboned. The lamb leg meat is quite tender especially the sirloin end of the leg and suitable for a variety of cooking methods while the shank is less tender and needs to be braised.\nLeg The bone in leg is often roasted for carvery on buffets but can also be made into bone in lamb leg steaks. They can also be deboned and netted for roasting. Special cuts: Fore saddle – the front portion of the carcass separated from the hind saddle on the 12 th or 13 th rib. It includes the primal shoulder, breast, fore shank and rack. Hind saddle – the back portion of the carcass as described above. It includes the loin, leg and kidneys Back – the trimmed rack and loin in one piece Bracelet – the primal hotel rack with connecting breast section\nInnards The lamb brain is a delicacy and can be roasted or sautéed. Lamb kidneys are used in pies as well as they can be sautéed. Lamb liver is seldom used although it can be eaten and is very tender.\nGame are animals hunted for sports or food. Originally game was dependent on seasons and the hunters success but the increased demand of the food service industry has led to farms raising game mainly for food production purposes. Pheasant, quail, and deer are now all available farmed throughout the year. Game meat generally is dark in color and has a strong but pleasant with robust flavor and less fat than meat and poultry. Cooking methods for game meat depend on the age of the animal and the cut used, but in general one can follow the guidelines for meat and poultry.\nFurred or Ground Game Deer, wild boar, bison, moose, elk, hare etc Feathered or Winged Game Quail, pheasant, partridge, wild duck, snipe. grouse Exotic Meats and Reptiles Snakes, kangaroo, crocodile or alligator etc\nFurred or Ground Game These include large animals such as deer, moose, wild boar kangaroo, and elk as well as smaller animals like hare (wild rabbit). There are many more animals that generally speaking are hunted and could be eaten, but are not readily available in the food service industry – Zebra, bear and other big game. Butchering game meat generally follows the same principals as with beef or lamb – primal and sub- primal, but mostly only pre-butchered and precut parts are available for purchase.\nAntelope Farmed in the US, wild in Africa Meat is low on fat but retains moisture when cooked Tender parts can be sautéed or grilled tougher part need to be braised Bison (American Buffalo) The meat is in general cooked like beef Deer The deer family includes elk, mule deer, reindeer, red-tailed and white-tailed deer and is generally known as venison. Farm raised in New Zealand and US Dark very lean meat cooked according to general guidelines\nHare Rabbit the domesticated hare is handled under poultry Hare has dark meat and is quite tough with the exception for the loin Well suited for braising and casseroles Well suited for pates and terrines Wild boar Leaner and darker meat than pork, strong flavored Wild boar is only available through autumn ranch raised is available all year Baby boar (below 6 months) is considered a delicacy Mostly roasted in pieces but also used for sausages and terrines Water Buffalo Used in Asia and butchered the same way as beef Fairly tough meat and combination cooking methods suit the best\nFeathered game includes the wild turkey, pheasants, quail, waterfowls like wild geese and duck, partridge, grouse, doves, woodcocks and more although not all birds hunted are readily sold for food production. Wild birds are not permitted to be sold in the US but in Europe and other countries they are. There is a growing number of farmed wild birds available – quail, pheasants etc.\nPartridge May be roasted whole or cut in pieces and braised 450 g in weight Pheasant Excellent for roasting, stewing or braising and often used for consommés or essences Flavorful tender meat 700-1 kg in weight Quail Good for grilling, roasting (stuffed), broiling or sautéing Very lean and often barded 30-60 g per piece Young quails are also called squabs\nThere are a multitude of other meats that might be specialties in some countries or regions and/or cuisines available for consumption. Kangaroo Snake Alligator / Crocodile Yak These meats in general are not aged and need to be stewed or braised until tender.\nGrading Game meat is not graded as domestic animals or poultry would be and voluntary inspections only confirm the wholesomeness of the producer. Meat inspections are done in accordance with the federal inspection requirements. Marinating Traditionally game has been marinated in red wine, oil, herbs and spice to aid tenderness and to cover up the “wild game taste” With farmed animals these is increasingly not necessary anymore\nYour consent to our cookies if you continue to use this website.']"	['<urn:uuid:876d2b60-c1e0-45f2-9a81-38a5e387a23d>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	22	24	2072
53	Do Business Leaders and Product Managers have similar planning duties?	While Product Managers work hands-on with delivery teams to prioritize stories for each sprint and comment on solutions, Business Leaders have broader planning responsibilities. Business Leaders create business plans and resourcing plans, define customer and business objectives, develop the organization's mission and vision, and translate requirements into business performance objectives. They must understand customer requirements, stockholders, workforce, suppliers, communities, competition, and various business environment factors to design business models and strategy maps.	['Service managers are business people who work full time to develop and deliver an effective user focused digital service, including all related processes, for which they are responsible and accountable.\nOutside government, organisations in the public and private sector are learning that experienced and highly skilled managers (often called product managers in the commercial world) are necessary to deliver high-quality digital services.\nWe are adopting that model, requiring each transactional digital service handling over 100,000 transactions each year to be developed, operated and continually improved by a service manager. These are not technical IT posts, nor are they confined to running a website. Instead, they are individuals who work full-time to develop and deliver all the changes necessary to provide effective digital services. With a handful of exceptions, this is a new role within government.\nThese service managers will:\n- be experienced leaders, with an in-depth understanding of their service (built on continuity of involvement over a period of years) and equipped to represent their service and its users’ needs at all levels within the organisation. For high-profile services these will be at Senior Civil Service level\n- be accountable for the quality and usage of their service, and able to iterate the service based on user feedback at least every month\n- be able to lead effectively on the change management and process re-engineering required to implement successful services\n- have the digital literacy to engage with technical staff and suppliers to define the best system and platform configurations to achieve business/user objectives\n- encourage the maximum possible take-up of their digital service by effective marketing, and specify/manage the requirements for assisted digital activity to supplement this\n- oversee service redesign and subsequent operational delivery; supporting and ensuring the necessary project and approval processes are followed, monitoring and reporting on progress in line with the Digital by Default Service Standard, identifying and mitigating risks, and be authorised to deliver on all aspects\n- actively participate in networking with other service managers inside and outside government, and share good practice and learning\nWhat’s the difference between a service manager and a product manager?\nThis will depend on the scale of the service you are working on. In some cases the service manager will also be able to fulfil the role of product manager – working closely with the delivery team (the ‘makers’), prioritising stories for each sprint, attending daily stand ups, being on hand to comment on solutions as they emerge, and accepting stories once they are delivered. However, in many cases it is likely that the service manager won’t have the capacity to be this hands-on, so they are likely to need a dedicated product manager.\nSample job description\nClick either of the options below to download a template service manager job description.\nCabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.\nLearning and development\nNewly appointed service managers are supported by GDS through specialist learning and development.\nRead more about how service managers should interact with other technology leaders in our organisation design guidance.', 'There are four major roles within a business management system: Business Leader, Process Owner, Operational Manager, and Process Operator. The responsibilities of each of these roles are unique, but work together as a system. Some employees in an organization may perform as many as all four of these roles over the course of a day, week, month, or year. Now let’s have a look at each role in detail.\n1. Business leaders\nBusiness leaders have responsibility for creating the business plans (including strategic plans created during the strategic planning process) and associated resourcing plans necessary to cause the organization to be successful.\nSenior leaders (corporate) have the duty of defining the customer and business objectives which an organization needs to achieve in order to be successful. This process includes overseeing the development of the organization’s mission, vision (goal), and values.\nLower leader-levels (business unit and functional) get the task of translating senior leaders’ business objectives into business objectives that make sense for their level and that support the accomplishment of the senior leaders’ business objectives.\nThe responsibilities of the business leaders follow the PDCA (plan, do, check, and act) cycle.\nPlan: The business leaders create and own the business performance objectives of the organization. Senior leaders need to first understand the requirements of their customers, stockholders, workforce, suppliers, and communities. They need to understand their competition. They need to understand the environmental, economic, technological, social, legal, and political environments that they do business within. Senior leaders need to consider all of these elements as they design a Business model and business Strategy map that will meet the customer and business requirements. Business Leaders then translate these requirements and business environment issues into business performance objectives. Business Leaders then create business plans and associated resourcing plans that will cause the organization to achieve these business objectives. The Business Leaders establish business performance metrics to measure the business’s capability to meet these business objectives. Many organizations create a Balanced scorecard to organize and communicate business performance metrics.\nDo: The business leaders are responsible for communicating to the organization their business plans. As the organization conducts business, the Business Leaders are responsible to build bridges and remove barriers that will allow the business performance objectives to be met. The business performance metric data is produced and collected as business is performed by the organization.\nCheck: The business leaders periodically analyze the business performance data and use it to visualize the business’s capability to meet business objectives over time (performance trends), compare actual performance against performance targets, and identify performance issues.\nAct: The business leaders are responsible to create improvement actions to address the performance issues that are identified during their analysis of the business performance data. These improvement actions are created to ensure the organization is able to achieve their business plans.\n2. Process owner\nThe process owner is the person who is responsible for designing the processes necessary to achieve the objectives of the business plans that are created by the Business Leaders. The process owner is responsible for the creation, update and approval of documents (procedures, work instructions/protocols) to support the process. Many process owners are supported by a process improvement team. The process owner uses this team as a mechanism to help create a high performance process. The process owner is the only person who has authority to make changes in the process and manages the entire process improvement cycle to ensure performance effectiveness. This person is the contact person for all information related to the process.\nThe responsibilities of the process owner follow the PDCA (plan, do, check, and act) cycle.\nPlan: The process owners create and own the process performance objectives of the organization. The process owner first needs to understand the external and internal customer requirements for the process. This person uses the business plans as a source to help understand the long term and short term customer and business requirements. This person then translates these requirements into process performance objectives and establishes product (includes service) specifications. This person establishes process performance metrics to measure the process’s capability to meet the product specifications and overall process objectives. The set of metrics that are to be reviewed by operational managers and process operators are called key performance indicators (KPIs). The process owner then designs process steps to describe work that when performed will have the capability to produce products that meets the customer and business requirements.\nDo: The process owner is responsible to communicate to the operational managers the details of the processes that the operational managers are responsible to execute. As the operational managers and process operators perform the processes, the process owner is responsible to build bridges and remove barriers that will allow the process performance objectives to be met. The process performance metric data is produced and collected as the process is performed by process operators. The process owner is continually involved with the operational managers and process operators as they use KAIZEN™ to continually improve the process as they are performing the work.\nCheck: The Process Owner periodically analyzes the process performance data and uses it to visualize the process’s capability to operate within control limits over time (performance trends), compare actual performance against performance targets, and identify performance issues.\nAct: The Process Owner is responsible for creating improvement actions that address performance issues that are identified during their analysis of the process performance data. Improvement actions may include the initiation of Lean projects to reduce waste from the process or include the initiation of Six Sigma projects to reduce variation in the process. Improvement actions may include the use of problem solving tools that would include risk assessment and root cause analysis. Risk assessment is used to identify and reduce, eliminate, or mitigate risk within the process. This is the proactive approach to avoid problems being created from the process. Root-cause analysis is the reactive way to respond to problems that occur from the process. Root-cause analysis is used to identify the causes of problems within the process and identify and implement improvement actions that will ensure these problems do not occur again.\n3. Operational manager\nThe operational Manager is responsible to bring the resources and processes together to achieve the objectives of the business plans that are created by the business leaders.\nThe responsibilities of the operational manager follow the PDCA (plan, do, check, and act) cycle.\nPlan: The operational managers – in collaboration with each Process Operator, create Process Operator performance objectives for the employees they supervise. The Operational Manager needs to understand the performance requirements of the process. They match employees (Process Operators) with the competency and skill requirements of the process to be performed. They ensure that the Process Operators have the budget, facilities, and technology available to them that is necessary to achieve the performance objectives of the processes.\nDo: The operational manager is responsible to teach process operators how to perform the processes (work). Process Operator instruction usually consists of classroom and on-the-job training. The Operational Manager oversees the work and ensures Process Operators receive ongoing informal feedback as to their performance. As the Process Operators perform the processes, the Operational Managers are responsible to build bridges and remove barriers that will allow the process and Process Operator performance objectives to be met. Process and Process Operator performance metric data is produced and collected as the process is performed. The Operational Manager ensures that Process Operators are using KAIZEN™ to continually improve the process as they are performing the work.\nCheck: The operational manager periodically analyzes the key performance indicators (KPIs) during the production cycle to evaluate the work group’s ability to achieve the process and process operator performance objectives. This data is used to visualize the process and process operator capability to meet business plan objectives over time (performance trends), compare actual performance against performance targets, and identify performance issues. They review this performance data and sort out process operator performance issues from process performance issues. Many organizations use a war room concept to post performance data. Within the war room, the operational manager conducts periodic review and analysis of this performance data.\nAct: The operational manager is responsible to create improvement actions to address the performance issues that are identified during their analysis of the process and Process Operator performance data. They address Process Operator performance with ongoing feedback to the Process Operator and/or by using an employee performance management review process. They communicate process performance issues to the Process Operator(s) and the Process Owner.\n4. Process operator\nThe process operator is responsible to learn and perform the processes (work) necessary to achieve the objectives of the business plans that are created by Business Leaders.\nThe responsibilities of the process operator follow the PDCA (plan, do, check, and act) cycle.\nPlan: The process operators – in collaboration with their Operational Manager, create and own their performance objectives. Process Operators are responsible to understand the performance objectives of the process they are to perform and the specifications of the product they are to produce.\nDo: Process operators are responsible to learn the processes (work) that they are to perform. They ensure the processes are performed to meet the process performance objectives and produce product that meets specification. As the Process Operators perform the processes, they are responsible to communicate to their Operational Manager (supervisor) the bridges that need to be built and the barriers that need to be removed to allow the process and Process Operator performance objectives to be met. Process and Process Operator performance metric data is produced and collected as the process is performed.\nCheck: The process operator periodically reviews the Key performance indicators (KPI’s). The Process Operator makes adjustments to their work based on their actual performance compared to KPI targets. The Process Operator is responsible for identifying and reporting any performance issues and stopping production if necessary.\nAct: Process operators practice KAIZEN™ to continually challenge the process and communicate improvement suggestions to their operational manager (supervisor).\nProcesses need to align to business goals. An organization’s strategic goals should provide the key direction for any Business Process Improvement exercise. This alignment can be brought about by integrating programs like Balanced Scorecard to the BPI initiative. E.g. when deploying Kaizen, Six Sigma, identification of projects can be done on the basis of how they fit into the Balanced Scorecard agenda of the organization.\nCustomer focus Fast-changing customer needs underscore the importance of aligning business processes to achieve higher customer satisfaction. It is imperative in any BPI exercise that the “Voice of Customer” be known, and factored in, when reviewing or redesigning any process.\nImportance of benchmarks BPI tools place a lot of emphasis on “measurable results”. Accordingly, benchmarks assume an important role in any BPI initiative. Depending on the lifecycle of the process in question, benchmarks may be internal (within the organization), external (from other competing / noncompeting organizations) or dictated by the senior management of the organization as an aspirational target.\nEstablish process owners for any process to be controllable, it is essential that there be clarity on who is the process owner, and what constitutes success/failure of the process. These success/failure levels also help establish “control limits” for the process, and provide a healthy check on whether or not a process is meeting the desired customer objectives.']	['<urn:uuid:a7ee80df-2b4e-4f97-863c-79fec4f1c22f>', '<urn:uuid:fae4bf1c-9efd-417e-bbcd-e78f7e0703a0>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T04:43:46.574121	10	72	2397
54	what is methylation aging combat veterans	Combat veterans with PTSD show accelerated DNA methylation aging compared to controls, indicating faster biological aging. This can be detected through methylation analysis techniques like MeDIP-seq, which requires low DNA concentrations and provides genome-wide methylation profiling.	['CD Genomics is providing Methylated DNA Immunoprecipitation Sequencing (MeDIP) service to enrich and capture methylated DNA fragments for gene-specific DNA methylation studies on a genome wide scale.\nThe Introduction of MeDIP Sequencing\nDNA methylation refers to the postreplicative maintenance or de novo addition of a methyl group to the carbon-5 position of the cytosine pyrimidine ring by DNA methyltransferases. In mammals, DNA methylation often occurs in CG (mCG) and non-CG (mCHG or mCHH, referred to as mC) contexts. mCG accounts for about 60-80% of total CG dinucleotides. Variants were recently discovered, including 5-hyfroxymethylcytosine (hmC), 5-formylcytosine, and 5-carboxylcytosin. Although structurally similar to mC, hmC is much less abundant and is formed by the TET family of dioxygenase enzyme. Studies suggest DNA methylation plays an important part in embryonic development, gene regulation, and disease genesis including carcinogenesis. Comprehensive genome-wide DNA methylation and hydroxymethylation studies provide a way to thoroughly understand normal development and to identify potential epigenetic mutations.\nMeDIP-seq combines methylated DNA immunoprecipitation with the next-generation sequencing for epigenetic studies at genome-wide level or any given regions of interest. MeDIP is capable of detecting methylated cytosines in mC and mCG contexts, and possibly hmC, 5-formylcytosine, and 5-carboxylcytosine as well. It uses anti-5-methylcytosine antibodies coupled to magnetic beads to select for genomic fragments that are methylated. These fragments are then sequenced and the coverage of these reads can be used to estimate the methylation level of the region that they map to. The 5-hydroxymethylcytosine (hmC) can also be detected genome-wide by using a hydroxymethylated DNA immunoprecipitation (hMeDIP) procedure with a 5hmC-specific antibody. MeDIP-seq neither introduces any mutations nor requires uracil-tolerant DNA polymerase. MeDIP-seq technology is feasible to profile genome-wide DNA methylation in low amounts of DNA samples with the resolution of several hundred base pairs in minimal selection bias and at a competitive cost.\nAdvantages of MeDIP Sequencing\nMeDIP Sequencing Workflow\nThe general workflow for MeDIP sequencing is outlined below. Briefly, the extracted DNA is fragmented, denatured, ligated with adaptor and captured using the antibody directed against 5 methylcytosine. The enriched methylated DNA is then amplified with PCR, size-selected, and sequenced on Illumina sequencing instrument.\n| Sample requirements and preparation\n| Bioinformatics Analysis\nWe provide customized bioinformatics analysis including:\nWith professional bioinformatics capability, CD Genomics offers high-quality MeDIP-Seq as an end-to-end, genome-wide epigenetic service to identify differentially methylated regions, and ultimately help to expedite epigenetic research. If you have additional requirements or questions, please feel free to contact us.\nTaiwo O, Wilson G A, Morris T, et al. Methylome analysis using MeDIP-seq with low DNA concentrations. Nature protocols, 2012, 7(4): 617.\n1. Compared with other methods, what are the advantages of MeDIP sequencing?\nCurrently, sequencing-based approach for methylome analysis can be classified into bisulfite conversion-based and enrichment-based approaches. Bisulfite conversion-based methods include whole-genome or targeted bisulfite sequencing or reduced representation bisulfite sequencing (RRBS). Although bisulfite conversion-based methods are considered to be the gold standard of DNA methylation analysis at single-base resolution, they cannot distinguish between mC and hmC. Additionally, whole genome bisulfite sequencing is expensive for application on large sample sizes and by smaller research groups. And RRBS can only provide limited genome coverage (5-10%) and is centered on CpG island and promoter regions.\nBesides MeDIP sequencing, enrichment-based technologies also include MBD-seq, which uses the methyl-binding protiens MBD2 and MDB3L1, and methylCap-seq that use methyl-binding domain of MECP2 for methyl capture. But MBD-seq and methylCap-seq are restricted to the analysis of mCG and whole-genome protocols often require high concentrations of genomic DNA (more than 1,000 ng). Therefore, MeDIP is a versatile, accurate, and costly method with a low input DNA requirement and is applicable to a wide range of samples and studies.\n2. What are the requirements for MeDIP sequencing samples?\nMeDIP sequencing samples require reference genome or sequences for alignment, and the assembled results directly affect the accuracy of data analysis. Reference genome from closely related species or assembled results from transcriptome can also be used despite the loss of partial methylation information.\n3. What factors can affect the results of MeDIP-seq?\nEvery process involved in MeDIP-seq may affect the results, especially the immunoprecipitation and PCR. Immunoprecipitation is the process to enrich methylated regions, and reaction conditions may influence the enrichment results. If insufficient DNA is recycled, PCR is used to scale it up, which can biase results. Furthermore, contamination should be avoided throughout the whole course.\nEpigenetic alterations in TRAMP mice: epigenome DNA methylation profiling using MeDIP-seq\nJournal: Cells & Bioscience\nPublished: 12 January 2018\nThe authors profiled the methylome of the mouse prostate (TRAMP) cancer model and to analyze the crosstalk among targeted genes and the related functional pathway. By utilizing MeDIP sequencing, they analyzed DNA methylation profiles and performed relevant informatics analyses, which could provide strategies for prevention and treatment approaches for prostate cancer.\nMaterials & Methods\nTRAMP mice and C57BL/6 mice\nHiSeq 2000;data analysis\nCanonical pathways, diseases and function and network analysis\n1. MeDIP-seq results comparison\nIn total, 2147 genes between TRAMP and control animals showed a significant change in methylated peaks. Compared with the control, significantly increased methylation of 1042 genes and significantly decreased methylation of 1105 genes were observed in TRAMP. Four genes of interest, DYNC1I1, SLC1A4, XRCC6BP1, and TTR were analyzed by IGV. TRAMP mice showed increased methylation ratio of DYNC111 and SLC1A4, and decreased methylation ratio of TTR and XRCC6BP1, which was in accordance with the MeDIP-seq results.\nFigure 1. integrative genomics viewer visualization of the aligned reads’ distribution against reference genome for four target genes, DYNC1I1, SLC1A4, XRCC6BP1, and TTR.\n2. qPCR validation of selected gene expression\nThe expression levels of CRYZ, DYNC1I1, HNMT, SLC1A4, and TTR were measured in both TRAMP and wide type group (Figure 2). Among them, TTR expression was increased by 9.05-fold over WT. And the expression levels of TTR were significantly higher in prostate cancer tissue than in normal and benign prostate hyperplasia tissue. Furthermore, they found decreased methylation in promoter region of TTR but increased gene expression. In contrast, DNA methylation in the gene body or downstream may or may not follow a reciprocal relationship.\nFigure 2. Comparison of miRNA expression of CRYZ, DYNC1I1, HNMT, SLC1A4, and TTR among wide type and TRAMP mice prostate samples.\n3. Canonical pathway, diseases and functions and network analyses by IPA\nThe 2147 genes with significant change in methylation were analyzed by IPA software package. The cancer-related networks accounted for the majority (Table 2), which suggested that the difference between the TRAMP and control lay in organ development and cancer development. The most associated disease, cancer, gastrointestinal disease, organismal abnormalities, reproductive system disease and dermatological diseases were ranked within the top five (Figure 3).\nTable 1. Top ten altered canonical pathways by IPA\nTable 2. Top networks analyzed by IPA.\nFigure 3. Top five associated disease categories (a) and top five cancer subtypes (b) analyzed by IPA.\nAnalysis of canonical pathway would provide information for the development of new therapeutic targets. As shown in Figure 4, the genes with significant methylation in the top canonical pathway was the neuropathic pain signaling pathway, which is consistent with the former finding that the most common malignancy in TRAMP is of neuroendocrine origin. Table 3 lists the genes involved in this pathway that showed modified methylation. CREB was found to be closely related to cellular proliferation, differentiation and adaptive responses in the neuronal system, and methylation of the CREB1 gene was found to be decreased by 2.274 in TRAMP. Furthermore, CREB was also found to regulate other carcinogenesis pathways. All of these data indicate that CREB is highly linked with cancer therapy and may be new strategy for prostate cancer prevention and therapy.\nFigure 4. Genes mapped to the canonical neuropathic pain signaling pathway by IPA.\nTable 3. Altered methylation genes mapped to the neuropathic pain signaling pathway by IPA.\nReference Li W, Huang Y, Sargsyan D, et al. Epigenetic alterations in TRAMP mice: epigenome DNA methylation profiling using MeDIP-seq. Cell & bioscience, 2018, 8(1): 3.', 'Epigenetic alterations in DNA methylation might mediate gene expression effects of trauma underlying PTSD symptoms, or effects of PTSD on related health problems. PTSD is associated with all-cause morbidity and premature mortality, suggesting accelerated biological aging. We measured genome-wide DNA methylation (Illumina MethylationEPIC BeadChip) in whole blood in a treatment study for combat-related PTSD - “PROGrESS”, a multisite RCT comparing sertraline plus enhanced medication management (SERT + EMM), prolonged exposure (PE) therapy plus placebo (PE + PLB), and the combination (SERT + PE). DNA methylation was measured in 140 US military veterans who served in Iraq and/or Afghanistan (112 current PTSD cases enrolled in a PTSD treatment study and 28 veterans without PTSD history controls), and also 59 non-trauma exposed controls at baseline posttreatment (24 weeks after baseline). Increased DNA methylation GrimAge acceleration (p = 8.8e−09) was observed in patients with PTSD compared to a pooled control group (trauma exposed and non-trauma exposed), suggesting a higher risk of premature mortality in those with PTSD. There was no difference in GrimAge acceleration between combat trauma and non-trauma exposed controls. No treatment-related changes in GrimAge acceleration were found in within-subject comparisons of PTSD patients pre- to post-treatment.\nThis is a preview of subscription content, access via your institution\nSubscribe to this journal\nReceive 13 print issues and online access\n$259.00 per year\nonly $19.92 per issue\nRent or buy this article\nGet just this article for as long as you need it\nPrices may be subject to local taxes which are calculated during checkout\nKubzansky LD, Koenen KC, Jones C, Eaton WW. A prospective study of posttraumatic stress disorder symptoms and coronary heart disease in women. Health Psychol. 2009;28:125–30.\nO’Donovan A, Cohen BE, Seal KH, Bertenthal D, Margaretten M, Nishimi K, et al. Elevated risk for autoimmune disorders in iraq and afghanistan veterans with posttraumatic stress disorder. Biol Psychiatry. 2015;77:365–74.\nRosenbaum S, Stubbs B, Ward PB, Steel Z, Lederman O, Vancampfort D. The prevalence and risk of metabolic syndrome and its components among people with posttraumatic stress disorder: a systematic review and meta-analysis. Metabolism 2015;64:926–33.\nHung YH, Cheng CM, Lin WC, Bai YM, Su TP, Li CT, et al. Post-traumatic stress disorder and asthma risk: A nationwide longitudinal study. Psychiatry Res. 2019;276:25–30.\nSchlenger WE, Corry NH, Williams CS, Kulka RA, Mulvaney-Day N, DeBakey S, et al. A Prospective Study of Mortality and Trauma-Related Risk Factors Among a Nationally Representative Sample of Vietnam Veterans. Am J Epidemiol. 2015;182:980–90.\nWolf EJ, Logue MW, Stoop TB, Schichman SA, Stone A, Sadeh N, et al. Accelerated DNA Methylation Age: Associations With Posttraumatic Stress Disorder and Mortality. Psychosom Med. 2018;80:42–8.\nLohr JB, Palmer BW, Eidt CA, Aailaboyina S, Mausbach BT, Wolkowitz OM, et al. Is Post-Traumatic Stress Disorder Associated with Premature Senescence? A Review of the Literature. Am J Geriatr Psychiatry. 2015;23:709–25.\nFonkoue IT, Marvar PJ, Norrholm S, Li Y, Kankam ML, Jones TN, et al. Symptom severity impacts sympathetic dysregulation and inflammation in post-traumatic stress disorder (PTSD). Brain Behav Immun. 2020;83:260–69.\nHorvath S. DNA methylation age of human tissues and cell types. Genome Biol. 2013;14:R115.\nHannum G, Guinney J, Zhao L, Zhang L, Hughes G, Sadda S, et al. Genome-wide methylation profiles reveal quantitative views of human aging rates. Mol Cell. 2013;49:359–67.\nLevine ME, Lu AT, Quach A, Chen BH, Assimes TL, Bandinelli S, et al. An epigenetic biomarker of aging for lifespan and healthspan. Aging (Albany NY). 2018;10:573–91.\nLu AT, Quach A, Wilson JG, Reiner AP, Aviv A, Raj K, et al. DNA methylation GrimAge strongly predicts lifespan and healthspan. Aging (Albany NY). 2019;11:303–27.\nWolf EJ, Logue MW, Hayes JP, Sadeh N, Schichman SA, Stone A, et al. Accelerated DNA methylation age: Associations with PTSD and neural integrity. Psychoneuroendocrinology 2016;63:155–62.\nWolf EJ, Maniates H, Nugent N, Maihofer AX, Armstrong D, Ratanatharathorn A, et al. Traumatic stress and accelerated DNA methylation age: A meta-analysis. Psychoneuroendocrinology 2018;92:123–34.\nVerhoeven JE, Yang R, Wolkowitz OM, Bersani FS, Lindqvist D, Mellon SH, et al. Epigenetic Age in Male Combat-Exposed War Veterans: Associations with Posttraumatic Stress Disorder Status. Mol Neuropsychiatry. 2018;4:90–9.\nBoks MP, van Mierlo HC, Rutten BP, Radstake TR, De Witte L, Geuze E, et al. Longitudinal changes of telomere length and epigenetic age related to traumatic stress and post-traumatic stress disorder. Psychoneuroendocrinology 2015;51:506–12.\nKatrinli S, Stevens J, Wani AH, Lori A, Kilaru V, van Rooij SJH, et al. Evaluating the impact of trauma and PTSD on epigenetic prediction of lifespan and neural integrity. Neuropsychopharmacology 2020;45:1609–16.\nYang R, Wu GWY, Verhoeven JE, Gautam A, Reus VI, Kang JI, et al. A DNA methylation clock associated with age-related illnesses and mortality is accelerated in men with combat PTSD. Mol Psychiatry. 2021;26:4999–5009.\nMehta D, Bruenig D, Pierce J, Sathyanarayanan A, Stringfellow R, Miller O, et al. Recalibrating the epigenetic clock after exposure to trauma: The role of risk and protective psychosocial factors. J Psychiatr Res. 2022;149:374–81.\nBohnert KM, Sripada RK, Mach J, McCarthy JF. Same-Day Integrated Mental Health Care and PTSD Diagnosis and Treatment Among VHA Primary Care Patients With Positive PTSD Screens. Psychiatr Serv. 2016;67:94–100.\nUS Department of Veterans Affairs Management of posttraumatic stress disorder and acute stress reaction 2017. https://www.healthquality.va.gov/guidelines/MH/ptsd/. Accessed 15 August 2022.\nRauch SAM, Kim HM, Powell C, Tuerk PW, Simon NM, Acierno R, et al. Efficacy of Prolonged Exposure Therapy, Sertraline Hydrochloride, and Their Combination Among Combat Veterans With Posttraumatic Stress Disorder: A Randomized Clinical Trial. JAMA Psychiatry. 2019;76:117–26.\nMaples-Keller JL, Rauch SAM, Jovanovic T, Yasinski CW, Goodnight JM, Sherrill A, et al. Changes in trauma-potentiated startle, skin conductance, and heart rate within prolonged exposure therapy for PTSD in high and low treatment responders. J Anxiety Disord. 2019;68:102147.\nVinkers CH, Geuze E, van Rooij SJH, Kennis M, Schur RR, Nispeling DM, et al. Successful treatment of post-traumatic stress disorder reverses DNA methylation marks. Mol Psychiatry. 2021;26:1264–71.\nRauch SAM, Simon NM, Kim HM, Acierno R, King AP, Norman SB, et al. Integrating biological treatment mechanisms into randomized clinical trials: Design of PROGrESS (PROlonGed ExpoSure and Sertraline Trial). Contemp Clin Trials. 2018;64:128–38.\nBlake DD, Weathers FW, Nagy LM, Kaloupek DG, Gusman FD, Charney DS, et al. The development of a Clinician-Administered PTSD Scale. J Trauma Stress. 1995;8:75–90.\nRavi M, Stevens JS, Michopoulos V. Neuroendocrine pathways underlying risk and resilience to PTSD in women. Front Neuroendocrinol. 2019;55:100790.\nKing DW, King LA, Vogt DS. Manual for the Deployment Risk and Resilience Inventory (DRRI): A Collection of Measures for Studying Deployment-Related Experiences of Military Veterans. Boston, MA: National Center for PTSD;2003.\nKeane TM, Fairbank JA, Caddell JM, Zimering RT, Taylor KL, Mora CA. Clinical evaluation of a measure to assess combat exposure. Psychol Assess: J Consult Clin Psychol. 1989;1:53–5.\nAdkins JW, Weathers FW, McDevitt-Murphy M, Daniels JB. Psychometric properties of seven self-report measures of posttraumatic stress disorder in college students with mixed civilian trauma exposure. J Anxiety Disord. 2008;22:1393–402.\nBarfield RT, Kilaru V, Smith AK, Conneely KN. CpGassoc: an R function for analysis of DNA methylation microarray data. Bioinformatics 2012;28:1280–1.\nMcCartney DL, Walker RM, Morris SW, McIntosh AM, Porteous DJ, Evans KL. Identification of polymorphic and off-target probe binding sites on the Illumina Infinium MethylationEPIC BeadChip. Genom Data. 2016;9:22–4.\nTriche TJ Jr, Weisenberger DJ, Van Den Berg D, Laird PW, Siegmund KD. Low-level processing of Illumina Infinium DNA Methylation BeadArrays. Nucleic Acids Res. 2013;41:e90.\nLeek JT, Johnson WE, Parker HS, Jaffe AE, Storey JD. The sva package for removing batch effects and other unwanted variation in high-throughput experiments. Bioinformatics 2012;28:882–3.\nTeschendorff AE, Breeze CE, Zheng SC, Beck S. A comparison of reference-based algorithms for correcting cell-type heterogeneity in Epigenome-Wide Association Studies. BMC Bioinforma. 2017;18:105.\nSalas LA, Koestler DC, Butler RA, Hansen HM, Wiencke JK, Kelsey KT, et al. An optimized library for reference-based deconvolution of whole-blood biospecimens assayed using the Illumina HumanMethylationEPIC BeadArray. Genome Biol. 2018;19:64.\nHouseman EA, Accomando WP, Koestler DC, Christensen BC, Marsit CJ, Nelson HH, et al. DNA methylation arrays as surrogate measures of cell mixture distribution. BMC Bioinforma. 2012;13:86.\nReinius LE, Acevedo N, Joerink M, Pershagen G, Dahlen SE, Greco D, et al. Differential DNA methylation in purified human blood cells: implications for cell lineage and studies on disease susceptibility. PLoS ONE. 2012;7:e41361.\nBarfield RT, Almli LM, Kilaru V, Smith AK, Mercer KB, Duncan R, et al. Accounting for population stratification in DNA methylation studies. Genet Epidemiol. 2014;38:231–41.\nLi S, Wong EM, Bui M, Nguyen TL, Joo JE, Stone J, et al. Causal effect of smoking on DNA methylation in peripheral blood: a twin and family study. Clin Epigenet. 2018;10:18.\nLogue MW, Miller MW, Wolf EJ, Huber BR, Morrison FG, Zhou Z, et al. An epigenome-wide association study of posttraumatic stress disorder in US veterans implicates several new DNA methylation loci. Clin Epigenet. 2020;12:46.\nSmith AK, Ratanatharathorn A, Maihofer AX, Naviaux RK, Aiello AE, Amstadter AB, et al. Epigenome-wide meta-analysis of PTSD across 10 military and civilian cohorts identifies methylation changes in AHRR. Nat Commun. 2020;11:5965.\nSnijders C, Maihofer AX, Ratanatharathorn A, Baker DG, Boks MP, Geuze E, et al. Longitudinal epigenome-wide association studies of three male military cohorts reveal multiple CpG sites associated with post-traumatic stress disorder. Clin Epigenet. 2020;12:11.\nKatrinli S, Maihofer AX, Wani AH, Pfeiffer JR, Ketema E, Ratanatharathorn A, et al. Epigenome-wide meta-analysis of PTSD symptom severity in three military cohorts implicates DNA methylation changes in genes involved in immune system and oxidative stress. Mol Psychiatry. 2022;27:1720–28.\nYang R, Xu C, Bierer LM, Flory JD, Gautam A, Bader HN, et al. Longitudinal genome-wide methylation study of PTSD treatment using prolonged exposure and hydrocortisone. Transl Psychiatry. 2021;11:398.\nSong M, Graubard BI, Rabkin CS, Engels EA. Neutrophil-to-lymphocyte ratio and mortality in the United States general population. Sci Rep. 2021;11:464.\nDuan R, Fu Q, Sun Y, Li Q. Epigenetic clock: A promising biomarker and practical tool in aging. Ageing Res Rev. 2022;81:101743.\nFahy GM, Brooke RT, Watson JP, Good Z, Vasanawala SS, Maecker H, et al. Reversal of epigenetic aging and immunosenescent trends in humans. Aging Cell. 2019;18:e13028.\nThe DOD had a role in design in that they wanted the study to include only OEF/OIF/OND service members with combat-related PTSD. DOD had no role in the conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the paper; and decision to submit the paper for publication. This work was supported by the U.S. Department of Defense through the U.S. Army Medical Research and Materiel Command (MRMC; Randomized Controlled Trial of Sertraline, Prolonged Exposure Therapy, and Their Combination in OEF/OIF Combat Veterans with PTSD; (Award #W81XWH-11-1-0073; PI: Rauch); the National Center for Advancing Translational Sciences of the National Institutes of Health (Award #UL1TR000433). This material is the result of work supported with resources and the use of facilities at Massachusetts General Hospital, the VA Ann Arbor Healthcare System, Ralph H. Johnson VA Medical Center, and VA San Diego Healthcare System. The views expressed in this paper presentation are solely those of the author(s) and do not reflect an endorsement by or the official policy of the Department of Veterans Affairs, Department of Defense, or the U.S. Government, or the official views of the National Institutes of Health.\nAPK, SK, ERD, NR, AKS, IL, SAMR report no competing interests.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nRights and permissions\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\nAbout this article\nCite this article\nKatrinli, S., King, A.P., Duval, E.R. et al. DNA methylation GrimAge acceleration in US military veterans with PTSD. Neuropsychopharmacol. (2023). https://doi.org/10.1038/s41386-023-01537-z']	['<urn:uuid:233655e2-4c72-4824-bfaf-5efcbc7ad366>', '<urn:uuid:100cc341-ab3d-47bd-a242-aef00f29d12e>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-13T04:43:46.574121	6	36	3266
55	How do contemporary art museums in West Sweden showcase their collections, and what security challenges do they face in maintaining both safety and visitor experience?	Contemporary art museums in West Sweden showcase their collections through diverse venues and exhibitions, such as the Borås Museum of Modern Art which displays works by renowned artists like Andy Warhol and David Hockney, and through unique settings like the Nordic Watercolour Museum built on water. However, these cultural centers face significant security challenges as they have become potential terrorist targets. Museums must balance maintaining their welcoming atmosphere while implementing new security measures, such as bag checks and visitor screening. Many institutions struggle with staffing decisions, particularly regarding gallery guards who traditionally focused on art interpretation but are now required to undergo security training for threats like active shooters.	['11 Art Destinations\nInterested in contemporary art? West Sweden is home to a wealth of art museums that, combined, showcase every facet of contemporary art, from the 20th century to the present day. Experience works of art inside museums or in the outdoors. West Sweden has art activities for every season and mood.\nBorås Museum of Modern Art\nThe Borås Museum of Modern Art boasts a fine collection of 20th-century art, which is continually expanding thanks to far-sighted purchases. Andy Warhol, David Hockney, Louise Bourgeois och Roy Lichtenstein are some of the represented artists and the museum often holds three parallel exhibitions.\nThere are also a cafe, design shop, book shop, library and information centre inside the Cultural Centre, where the museum is housed.\nThe City of Borås also organises the Borås International Sculpture Biennale (even years) and the street-art festival No Limit Borås (odd years), which have made the city one of Europe’s most interesting urban galleries.\nPhotographer: ROBERT DAHLBERG\nSkövde Art Museum\nSkövde Cultural Centre (opened in 1964 as Sweden’s first cultural centre and designed by Hans-Erland Heineman) houses Skövde’s Art Museum, which focuses on contemporary art. The museum has received major donations of works by artists Anna Sjödahl, Elli Hemberg and Ester Henning. The Cultural Centre also houses a library, cinema and restaurant.\nLöfwings Ateljé & Krog, Broddetorp\nJust a stone’s throw from Hornborgasjön Lake stands painter Göran Löfwing’s studio, with its exhibitions, restaurant and café. “A picture of a cake doesn’t taste like much. A picture of an oil painting doesn’t have the aroma of paint. Come to Löfwing’s and experience art, good food and pleasant company, live and in person,” the website recommends.\nDalslands Konstmuseum, Upperud\nThe art and crafts gallery Dalslands Konstmuseum is housed in a heritage building with a charming view out over the lake Spången. The museum building itself is West Sweden’s first-ever “eco” building in modern times, being built entirely from all-natural and traditional building materials. On its ground floor is a collection of works from the surrounding Dalsland Province and the museum’s three other exhibition galleries display contemporary art. Visitors can enjoy both refreshments and a wonderful view in the museum’s Café Bonaparte. The café is named after Napoleon’s niece, Egypta Bonaparte, who was once briefly married to Dalsland dignitary Arvid Påsse.\nNordic Watercolour Museum\nA museum built on will-power and water, exhibiting works in watercolour from around the world. The museum also offers workshops open to the public, studios and a long list of cultural events. Visitors to the museum can also treat themselves to culinary works at the Vatten Restaurant & Café, with its enchanting views out over the water and the sculpture Fyren.\nPhotographer: Amplifyphoto/ Markus Holm\nThe Art Gallery Lokstallet, Strömstad\nWest Sweden’s biggest art venue, at least in terms of size. Inside the old engine shed from 1903 (measuring 7.5 metres from floor to ceiling) is a venue for contemporary art exhibitions by established artists as well as an annual jury-assessed exhibition.\nKosters Trädgårdar & Sculpture Park\nOn the Kosteröarna islands, Sweden’s western-most landmass, you will find the Kosters Trädgårdar restaurant and farm shop and a sculpture park surrounded by Kosterhavets National Park, Sweden’s only marine national park. Here, art and nature combine in a harmonious union through sculptures, installations and lectures covering everything from permaculture to lobster fishing.\nPhotographer: Åsa Dahlgren\nKonstnärernas Kollektivverkstad, Bottna\nAt the junction where sea and mountains meet stand the artists’ collective workshops known as Konstnärernas Kollektivverkstad Bohuslän. The facility has Sweden’s only studio for sculpting in stone, an exhibition hall and a café with a sculpture garden. A few hundred metres from the studio complex lies the historic Gerlesborgsskolan (the Gerlesborg School of Fine Art), with its exhibition rooms and restaurant.\nContemporary Art Gallery, Bohusläns Museum, Uddevalla\nThe popular Bohusläns Museum includes a contemporary art gallery whose exhibitions regularly make full and thoughtful use of the museum’s collections, problematising matters and posing questions for visitors to contemplate. The exhibitions’ themes range from works by established artists to those of newcomers and from national to international art.\nSculpture in Pilane\nIn a Bronze-age burial field on the island of Tjörn stands Jaume Plensa’s colossal stone head in marble-white, silently contemplating the rolling hillsides and sheep at pasture. Every summer a sculpture exhibition with world-leading artists is arranged in the middle of the Bohuslän cultural landscape, with its grazing sheep and traces from the Stone Age. In 2016 the Guardian nominated Sculpture in Pilane as one of the ten best sculpture parks in Europe.\nPhotographer: Anders Jonsson, Södra Bohuslän Turism\nAbecita Museum of Modern Art, Borås\nThe Abecita Museum of Modern Art houses world-class photography and graphic art, thanks to textile entrepreneurs Bengt and Berit Swegmark, who have displayed their fine collection in the old corset mill that they themselves once founded. Works by Andy Warhol, David Hockney, Louise Bourgeois and Roy Lichtenstein are just some of the many pop-art gems held by the museum.', 'As public cultural centers become terrorist targets, museums are struggling to maintain their rarified atmosphere while also making sure their patrons feel safe. In the process, some are alienating some of their most storied employees: gallery guards, who fear new protocols will take the joy out of their jobs.\nThe past couple of years have seen the first big shift in museum security strategies since the late 1970s, says Steve Keller, an expert in the field for nearly 40 years. “With the active shooter concerns and the terrorism concerns, museums have suddenly become awakened to the fact that soft targets are now fair game,” says Keller.\nBut targets or not, these museums don’t want to lose their softness. “I get calls from museums where the director says, ‘We need to tighten up, but I don’t want to make it unwelcoming or make it look like there’s a problem,’” says Keller.\nThis balancing act requires various levels of decision-making, depending on the venue’s size and resources. Over the last week, Disney World, Disneyland, Universal Studios Orlando and Hollywood, and 11 SeaWorld locations all installed metal detectors—an option that Keller says would be “not very welcoming” in a more intimate environment. In the weeks after the Paris terror attacks, many European museums increased their security levels, implementing bag checks and funneling guests through smaller entrances, the Evening Standard reports.\nMeanwhile, smaller museums search for solutions apposite to their resources. In response to concerns from parents, the Hands On Children’s Museum in Olympia, Washington recently implemented a sign-in policy for visitors for the first time, and has begun training its staff to recognize unusual behavior.\nOften, the resulting changes hit gallery guards hardest. “I see a lot of museums that are very very concerned with having a 70-year-old lady standing at the front door, because she’s wonderful at public relations and not very good at stopping someone with a gun,” Keller says.\nGuards at the Museum of Fine Arts (MFA) in Boston picketed last week in protest of upcoming adjustments to their contracts, including proposed changes in training that would shift their professional priorities. The guards, many of whom took the jobs in order to guide patrons, safeguard the galleries, and answer questions about art, are dismayed that these aspects of the job may be supplanted by morning drills and outdoor patrols. “They want us to not focus on the artwork and be able to fight things like active shooters,” Evan Henderson, president of the Museum Independent Security Union, told the Boston Globe. Karen Frascona, the museum’s PR director, responded by calling such training “industry standard.”\nSome worry that stricter safety laws may literally damage collections—several director generals of British army museums have warned that their collections of historic weaponry could be “nearly destroyed” under proposed stricter European Union firearms directives. And the striking MFA guards fear that a reduced human presence in the galleries will endanger the collections, saying that new electronic security meant to take over while they’re at their new posts will not be enough to stop vandals. “If no one is there, artwork is damaged every day,” Henderson told Hyperallergic.\nKeller predicts improved security technology will reduce those concerns. And in the end, he says, the job of an art guard should involve more guarding and less art. “Many of the security guards who work in museums are really intelligent people who are doing it because they want to be around the art… they want to be able to talk about it,” says Keller. “It’s very hard for officers to understand that we shouldn’t be using them that way.” If security guards are hired and trained as true guards from the beginning, Keller says, these transitions will be less of an issue. “I think they’ll find over time that this is also enjoyable work,” he says. “Plus it’s a far cry less boring than just standing in a gallery.”']	['<urn:uuid:d8716a11-74c1-480e-ae2e-b66bb33de2e7>', '<urn:uuid:b71b71c9-cba0-436c-b177-ec92fc62cb6e>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	25	109	1474
56	I work with cultural preservation initiatives and want to know what intellectual property challenges museums face when digitizing indigenous collections, and how are Native Alaskan communities involved in managing these collections today?	Museums face IP challenges because while traditional cultural expressions may be considered public domain, new recordings or documentation of them create new IP rights, raising questions about ownership and management. In Alaska, these challenges are being addressed through active collaboration - Native communities are now involved in self-representation of their cultures in museums, with projects like the Arctic Studies Center working with Alaska Native consultants to document Smithsonian collections and produce exhibits. Many institutions now have research protocols that address restricted access to information in line with cultural values and fears of misuse or misrepresentation.	"['Archives and Museums: Balancing Protection and Preservation of Cultural Heritage\nMuseums, archives, libraries, anthropologists and ethnologists play an invaluable role in preserving the rich cultural heritage of our planet. By recording and making available the music, arts, knowledge and traditions of indigenous communities, such institutions help to spread a broader understanding and respect for different cultures. However, some traditional communities are beginning to voice concerns that sometimes activities by museums and cultural specialists do not take adequate account of their rights and interests; and that documenting and displaying, say, a traditional song or a tribal symbol, make them vulnerable to misappropriation.\nHow can museums strike a balance between the preservation and the protection of cultural documentation? And how can the wider public have greater access to the rich collections housed by archives and museums? Traditional communities and cultural institutions have begun to seek intellectual property (IP) information and advice in relation to these questions. Greater clarity on how to identify relevant IP issues and options could benefit all stakeholders. This article outlines a few of the key questions and describes WIPO activities aimed at addressing them.\nThe ethnographic collections of museums and other institutions often include invaluable, even unique, records of ancient traditions, lost languages and community histories, which are vital to indigenous peoples’ sense of identity. The handling of secret and sacred materials within such collections can be a source of particularly acute concern. Indigenous peoples also cite numerous cases in which commercial users have exploited cultural heritage collections without seeking the consent of the relevant community, let alone acknowledging the source or sharing the commercial benefits. Some popular world music albums, such as “Return to Innocence,” included samples of traditional music that had originally been recorded and made publicly available for heritage preservation purposes.\nIn the words of expert Henrietta Fourmille, (Centre for Indigenous History and the Arts, University of Western Australia), the crux of the problem from an indigenous perspective is that the “information collected about us is simply not owned by us.”\nThis introduces questions about the role of IP law, policy and practice in activities aimed at preserving cultural heritage. Such questions arise for museums, libraries, archives and galleries in relation to their collections of original works, as well as derivative databases, catalogues, coffee-table books and postcards, etc. IP issues become more pressing as they set up digital libraries of their collections.\nTraditional cultural materials\nThe “public domain” character of traditional cultural expressions (folklore), which has been the subject of criticism especially by indigenous peoples, raises interesting and challenging issues. For example, while a traditional song may be treated by IP law as in the public domain, recording that song creates IP rights in the recording. To whom do these new rights belong, and how should they be managed in ways that take into account the interests of the community in whom the custody of the song had been entrusted under customary laws?\nFurther questions to consider include:\n- What IP rights do researchers and cultural institutions hold? And how can these rights be best managed in the interests of safeguarding culture, promoting cultural diversity, fostering creativity and cultural exchange, and facilitating the public’s access to and enjoyment of diverse cultural expressions?\n- Which existing IP rules and practices can assist researchers and cultural institutions in fulfilling their mandates? (This might include tailored licensing approaches, or use of specific copyright and related rights exceptions and limitations.)\nThese issues arise frequently in practice. The Toulumne tribe of California recently used copyright laws to stop the sale of CDs and videos of its sacred dances. There are other cases in which recording a piece of traditional art, and exercising IP rights in that recording, has helped to protect the original work against misuse (see box).\nBut exercise of IP rights may not always be the answer. The point is to step back, identify objectives and determine an overall strategy which takes into account the range of IP-related issues and options.\n|Using a researcher’s records to protect indigenous rock art|\nIn Australia in 1997, t-shirts began appearing in the market depicting images from indigenous rock paintings found in the Deaf Adder Creek region. These rock art paintings have special cultural significance to Australian indigenous life and custom.\nThe indigenous custodial group had no remedy under copyright against the t-shirt manufacturers, as the original artist was unknown, and the paintings were so old that any copyright would have expired. However, drawings and photographs of the rock art images had been published by a researcher funded by the Australian Institute of Aboriginal Studies, Eric Brandl, in 1973, thus creating new copyright. It was from this publication that the t-shirt manufacturers had apparently copied the images. With the help of the Institute and the Brandl family, the indigenous group was able to get the t-shirt company to stop production, claiming infringement of copyright in Brandl’s drawings and photographs.\nTo read the full case study, compiled for WIPO by indigenous lawyer Ms.Terri Janke, see Minding Culture: Case Studies on Intellectual Property and Traditional Cultural Expressions, (WIPO Publication No. 781).\nWIPO is working with the stakeholders to respond to a widely-expressed need for technical information on these issues. A current project aims to develop a set of IP‑related “good practices”, practical guidelines and other resources for cultural specialists, indigenous communities, creators and users. Such resources may also benefit institutions establishing inventories of intangible cultural heritage, as provided for under the recently-adopted United Nations Educational, Scientific and Cultural Organization (UNESCO) Convention on Safeguarding Intangible Cultural Heritage. The UN Permanent Forum on Indigenous Issues has recommended development of these kinds of resources.\nMany institutions already have policies on research, collection and preservation, as well as codes of ethics. Indigenous declarations also address these questions. Few existing resources, however, address IP issues in detail, nor questions related to the treatment of traditional knowledge and cultural expressions. A first step in WIPO’s project, therefore, is to gather and publish, in a searchable online database, examples of existing materials, including relevant copyright provisions.\nClarifying IP issues and options in relation to safeguarding cultural heritage should help strengthen synergies between the protection of cultural documentation and its preservation, while contributing towards the respect for traditional cultures. All stakeholders stand to benefit from equitable and secure access to the collections of museums and archives, facilitating a wider exchange of cultural expressions between the peoples and communities of this culturally rich and varied world.', ""New Dynamics of Cultural Research and Representation in Alaska\nAron L. Crowell\nAlaska Natives number more than 90,000 people and speak 20 indigenous languages. This cultural diversity exists against an historical background of cultural repression as well as the contemporary resurgence of indigenous rights, resource ownership, political autonomy and cultural voice. Within this context, the relationship between Alaska Native peoples and cultural researchers from outside their communities has undergone a fundamental transformation. Anthropologists, archaeologists, historians, economists and other social scientists, as well as the universities, museums, agencies and foundations that employ and support them, all stand on a far different footing with respect to Native communities than was the case until even the last decade.\nToday, researchers seek permissions, collaboration and communication as a matter of course. Information is shared with communities and ethical standards of informed consent, indigenous participation, data sharing and respect for privacy are pre-conditions for project approval and funding (see Guidelines for Research, Alaska Federation of Natives; Principles for the Conduct of Research in the Arctic, U. S. Interagency Arctic Research Policy Committee and the National Science Foundation; Draft Principles for an Arctic Policy, Inuit Circumpolar Conference.) Alaska Native communities have also prioritized self-representation of their cultures in books, media and museums.\nIn the long history of arctic research, these principles and responsibilities were often unrecognized or ignored. The indigenous critique of traditional social science practice indicts researchers for lack of community review and access to publications, disrespect for cultural values, disregard for restrictions on the use of oral traditions, removal of objects without proper permission, disturbance of burials and removal of human remains for study, failure to reciprocate village cooperation, lack of credit and financial return to Native colleagues and other offenses.\nThe U. S. National Science Foundation (NSF), the leading source of northern social science funding (almost $2 million in fiscal year 2000 through its Arctic Social Sciences division in the Office of Polar Programs), has been highly influential by directing its support toward projects that actively involve the cooperation and participation of local communities (Arctic Social Sciences: Opportunities in Arctic Research, ARCUS 1999). Federal agencies that conduct social science research in the north have also adopted goals and standards that reflect the new priorities. Agency work is coordinated by the U. S. Arctic Research Commission and the Interagency Arctic Research Policy Committee (IARPC).\nIn recent years, the NSF supported creation of the Alaska Native Science Commission to encourage collaborative project design in such areas as northern contaminants research and the incorporation of traditional ecological knowledge into environmental and climate change studies. NSF also provides support for the Alaska Rural Systemic Initiative, a statewide effort with the University of Alaska and Alaska Federation of Natives to develop culturally integrated science and mathematics curricula for Alaskan schools. The emphasis is on incorporating local knowledge and Native worldviews into science teaching. The Smithsonian Institution's Arctic Studies Center (National Museum of Natural History) has played a role in establishing new working relationships for research and education with indigenous communities in Alaska, Canada and Russia.\nA few specific areas of active collaborative research may be highlighted in the present context. For example, human interactions with the changing arctic environment are an important focus of interdisciplinary and cross-cultural study. With NSF support, Henry Huntington and the Inuit Circumpolar Conference worked with North Alaskan coastal communities to document traditional ecological knowledge of beluga whales and their migrations. The Marine Mammal Commission (with Caleb Pongawi) has compiled hunters' observations of shifts in whale, walrus, caribou and seabird behavior. Anthropologists with the Alaska Department of Fish and Game cooperated with the University of Alaska and communities in Prince William Sound and Cook Inlet to develop educational films and interactive CD-ROMs about local subsistence practices and traditional knowledge. The U.S. Fish and Wildlife Service, National Park Service, Alaska Nanuuq Commission and Union of Marine Mammal Hunters of Chukotka recently collaborated on an international study of polar bears that relied heavily on indigenous observations. A recent workshop by the Marine Mammal Commission, National Oceanic and Atmospheric Administration, and National Marine Fisheries Service focused on linking climate change observations by scientists and Native communities.\nArchaeology provides a window into cultural history and human-environmental interactions in the past. Archaeological sites can be ideal opportunities for collaborative study and community involvement because they are often located near contemporary villages and are easily linked to school programs, training opportunities, local cultural heritage efforts and tribal museums. The National Science Foundation and the Kodiak Area Native Association co-sponsored excavations by Bryn Mawr College at the Karluk 1 site on Kodiak Island, leading to a wide range of educational efforts and foundation of the Alutiiq Museum in 1995. The Utqiagvik Archaeology Project in Barrow (State University of New York, North Slope Borough, National Park Service, Bureau of Indian Affairs) was another landmark project. Research was carried out jointly, including studies made of human remains recovered at the site. Over the past 15 years, many excavations and field schools have featured close cooperation between Native organizations and the National Park Service (especially its Shared Beringian Heritage Program), University of Alaska, U. S. Fish and Wildlife Service, Arctic Studies Center and other agencies and universities.\nThe on-going process of repatriation under NAGPRA, which requires extensive consultation between outside museums and tribal groups, has created a new awareness of the wealth of Alaskan collections in U. S. museums and around the world. The Smithsonian Institution's National Museum of Natural History alone holds more than 18,000 ethnological objects from Alaska, of which some portion will eventually go back to the state through repatriation. Others will return through exhibits developed by the Arctic Studies Center (ASC) in coordination with Alaska Native organizations, the Anchorage Museum of History and Art and other partners. An example is ASC's Looking Both Ways: Heritage and Identity of the Alutiiq People, co-developed by the Alutiiq Museum using information provided by Alutiiq Elders and scholars. ASC offers on-going student internships and community scholar opportunities, and over the next two years will work on a major project with Alaska Native consultants to document Smithsonian collections and to produce new exhibits, publications and a web site. The Anchorage Museum's Living Tradition of Yup'ik Masks is another prominent example of community-based exhibition development, and has been followed by further NSF-sponsored study of European museum collections by Yup'ik elders (with curator Ann Fienup-Riordan).\nInformation may be returned in other ways. For example, ASC's Beringian Yup'ik Heritage Project (led by Igor Krupnik, Willis Walunga, Vera Metcalf, and Lyudmilla Ainana) has assembled historical documentary records, notes, maps and genealogical data from the past century of anthropological research on St. Lawrence Island to create a community sourcebook of Yup'ik heritage and history.\nIt is clear that a new paradigm of U. S. arctic social science has emerged in response to broad political, legal and intellectual trends. The opportunities and challenges are both large. Joining local and scientific knowledge in the area of environmental observation is difficult and requires the construction of new interpretive frameworks. An increasingly important issue in cultural research is intellectual property. Research protocols signed with indigenous entities now often call for restricted access to the information gathered, in line with cultural values and fears that it will be misused or misrepresented. To what extent will researchers agree to restrictions on publication? Repatriation entails other unresolved matters that may undermine the collaborative efforts of Native communities and museums, including disagreement over what objects can be defined under the law as sacred or as inalienable because of cultural patrimony. In general, and across all aspects of social and cultural research, collaboration with indigenous communities requires time, willingness to listen, and a commitment to share control and to work toward alternative goals.""]"	['<urn:uuid:0c69d548-95e2-49e9-b430-4045bb8408e5>', '<urn:uuid:04be6ee0-29a2-4a4a-bc21-73b0ba919b86>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	32	95	2357
57	korean family names origin foreign country	Korean surnames originated from China, being adopted by the Korean court and nobility in the 7th century in emulation of noble-sounding Chinese surnames.	['A SOUTH KOREAN saying claims that a stone thrown from the top of Mount Namsan, in the centre of the capital Seoul, is bound to hit a person with the surname Kim or Lee. One in every five South Koreans is a Kim—in a population of just over 50m. And from the current president, Park Geun-hye, to rapper PSY (born Park Jae-sang), almost one in ten is a Park. Taken together, these three surnames account for almost half of those in use in South Korea today. Neighbouring China has around 100 surnames in common usage; Japan may have as many as 280,000 distinct family names. Why is there so little diversity in Korean surnames?\nKorea’s long feudal tradition offers part of the answer. As in many other parts of the world, surnames were a rarity until the late Joseon dynasty (1392-1910). They remained the privilege of royals and a few aristocrats (yangban) only. Slaves and outcasts such as butchers, shamans and prostitutes, but also artisans, traders and monks, did not have the luxury of a family name. As the local gentry grew in importance, however, Wang Geon, the founding king of the Goryeo dynasty (918–1392), tried to mollify it by granting surnames as a way to distinguish faithful subjects and government officials. The gwageo, a civil-service examination that became an avenue for social advancement and royal preferment, required all those who sat it to register a surname. Thus elite households adopted one. It became increasingly common for successful merchants too to take on a last name. They could purchase an elite genealogy by physically buying a genealogical book (jokbo)—perhaps that of a bankrupt yangban—and using his surname. By the late 18th century, forgery of such records was rampant. Many families fiddled with theirs: when, for example, a bloodline came to an end, a non-relative could be written into a genealogical book in return for payment. The stranger, in turn, acquired a noble surname.\nAs family names such as Lee and Kim were among those used by royalty in ancient Korea, they were preferred by provincial elites and, later, commoners when plumping for a last name. This small pool of names originated from China, adopted by the Korean court and its nobility in the 7th century in emulation of noble-sounding Chinese surnames. (Many Korean surnames are formed from a single Chinese character.) So, to distinguish one’s lineage from those of others with the same surname, the place of origin of a given clan (bongwan) was often tagged onto the name. Kims have around 300 distinct regional origins, such as the Gyeongju Kim and Gimhae Kim clans (though the origin often goes unidentified except on official documents). The limited pot of names meant that no one was quite sure who was a blood relation; so, in the late Joseon period, the king enforced a ban on marriages between people with identical bongwan (a restriction that was only lifted in 1997). In 1894 the abolition of Korea’s class-based system allowed commoners to adopt a surname too: those on lower social rungs often adopted the name of their master or landlord, or simply took one in common usage. In 1909 a new census-registration law was passed, requiring all Koreans to register a surname.\nToday clan origins, once deemed an important marker of a person’s heritage and status, no longer bear the same relevance to Koreans. Yet the number of new Park, Kim and Lee clans is in fact growing: more foreign nationals, including Chinese, Vietnamese and Filipinos, are becoming naturalised Korean citizens, and their most popular picks for a local surname are Kim, Lee, Park and Choi, according to government figures; registering, for example, the Mongol Kim clan, or the Taeguk (of Thailand) Park clan. The popularity of these three names looks set to continue.']	['<urn:uuid:fc706da5-803b-4d9f-add5-e8e2bcedf29d>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	6	23	631
58	Which is older: the Jaguar deposit or the Coates mining project?	The Jaguar deposit is an Archaean-age formation, meaning it is billions of years old, while the Coates deposit was first discovered in 1961 and mining only began in 1980. The Jaguar deposit is therefore significantly older, being part of ancient volcanic formations in the Yilgarn Craton, while Coates is a modern mining operation.	['Belford, SM and Davidson, GJ and McPhie, J and Large, RR, Architecture of the Neoarchaean Jaguar VHMS deposit, Western Australia: implications for prospectivity and the presence of depositional breaks, Precambrian Research, 260 pp. 136-160. ISSN 0301-9268 (2015) [Refereed Article]\n© 2015 Elsevier B.V. All rights reserved.\nThe duration and nature of breaks in Archaean volcanosedimentary packages, recently re-evaluated in the Abitibi Greenstone Belt, has been assessed at Jaguar, an Archaean Cu–Zn-rich volcanic-hosted massive sulphide (VHMS) deposit in the Teutonic Bore volcanic complex (TBVC), the Eastern Goldfields of the Yilgarn Craton, Western Australia. The deposit is hosted by coherent volcanic facies and associated volcaniclastic facies, with minor non-volcanic facies.\nThe hypothesis that VHMS ore lies on the intersection of volcanic centres and depositional breaks is found to be valid for the Jaguar deposit, with the site being an active volcanic complex at footwall time, with further dome development close to the final ore location at Mineralised Package time, and then reoccurring volcanic complexes (andesitic and basaltic) at several points in the hangingwall succession broadly tied into the ore position.\nAt Jaguar the entire stratigraphic sequence faces west, with no major structural complications, making it ideal for stratigraphic analysis. The footwall evidences prolific volcanism with no intervening sedimentation. The Mineralised Package (MP) consists of intercalated dacite, breccia, conglomerate, sandstone and mudstone. The footwall volcanic rocks and the MP Dacite appear to be differentiation products from the same magma chamber. A major volcanic geochemical boundary is identified between the footwall/MP (tholeiitic to transitional affinities), and the immediate hangingwall succession (calc-alkaline). The hangingwall package includes four distinct volcanic units intercalated with mass flow deposits and graphitic argillites.\nVHMS mineralisation occurred across a distinct change in the geochemistry of erupted volcanic facies at a time of increased volcaniclastic sedimentation and extension. This break is comparable to Member level hiatuses seen in the Abitibi GB, i.e. the most brief of breaks, because there is no silicification of sedimentary unit-tops by silicic seawater exposure, and there is minimal development of black shales. A third time-sensitive criteria was the development of peperite on the base of Hanging Wall (HW) andesite lavas. The MP closely resembles breaks within interpreted deeper water domains of the Abitibi GB, for instance, the Deloro-Tisdale assemblage interface south of Timmins.\nThe recognition of the change in the rate of emplacement of volcanic units and particularly the change in chemistry of volcanic rocks, are critical exploration lithologic guides to use in the absence of chemical sediment deposits (exhalites). Although crustal extension and basin subsidence were factors in the shaping of the succession, their effects were too subtle to use as an exploration tool. A corollary of the recognition of a Member-level VHMS-bearing interval in the Teutonic Bore volcanic complex is that, like the Kidd-Munro assemblage stratigraphy (Abitibi GB), where several ore-bearing depositional breaks are known below the great Kidd Creek deposit, there is scope for the discovery of further (potentially larger) deposits on higher Formation or Group-level breaks in the TBVC.\n|Item Type:||Refereed Article|\n|Keywords:||Archaean, VHMS, Yilgarn Craton, depositional break, Teutonic Bore volcanic complex, Jaguar Cu–Zn deposit|\n|Research Division:||Earth Sciences|\n|Research Field:||Ore Deposit Petrology|\n|Objective Division:||Mineral Resources (excl. Energy Resources)|\n|Objective Group:||Mineral Exploration|\n|Objective Field:||Copper Ore Exploration|\n|Author:||Belford, SM (Ms Susan Belford)|\n|Author:||Davidson, GJ (Dr Garry Davidson)|\n|Author:||McPhie, J (Professor Jocelyn McPhie)|\n|Author:||Large, RR (Professor Ross Large)|\n|Funding Support:||Australian Research Council (LP0455708)|\n|Web of Science® Times Cited:||3|\n|Deposited By:||Centre for Ore Deposit Research - CODES CoE|\nRepository Staff Only: item control page', '- Project Highlights\n- Exploration Potential\n- Proposed Exploration Programme and Budget\n- Geological Setting\n- Previous Exploration\n- Recent Exploration\nThe recent discovery of Nickel (Ni) Copper (Cu) and Platinum Group Elements (PGE) in sulphide mineralisation at Chalice Gold Mines’ Julimar project (announced in March 2020), as well as indications of other instances of similar mineralisation discovered by other explorers within the Jimperding Metamorphic Belt, serve to highlight the great potential of this geological terrain\n- Project comprises 1 granted Exploration Licence and 1 EL Application; and 1 retention licence, with a combined area of 48m2\n- Located 60km northeast of Perth\n- Covers the northern half of the Coates Mafic Intrusive\nCoates Project overlain on the regional aeromagnetic image (left) and simplified geology plan (right).\nThe coincidence of base metal and PGE geochemical anomalies from the BRL vacuum drilling with the Coates Mafic Complex is most encouraging from an exploration point of view. By analogy the mineralisation at Chalice’s Gonneville Prospect is characterised by a similar Cu-Ni-Co-PGE elemental association with a mafic intrusive complex.\nThere are a number of interpreted mafic-ultramafic units that have been identified in the region, all of which show geophysical similarities with the Julimar Complex and consequently all represent targets for Cu-Ni-Co-PGE mineralisation.\nPROPOSED EXPLORATION PROGRAMME AND BUDGET\nOutcrop in the area is poor and interpretation of the mafic-ultramafic layered intrusive complexes is based on the regional aeromagnetic data. Following the experience of Chalice, the targets within the interpreted mafic-ultramafic complexes are identified by airborne electromagnetic (EM) surveys then followed-up by drilling.\nThe land title situation in the project area contains both freehold private land and conservation areas. Statutory approvals for ground access within the granted tenements will be necessary prior to commencing field work that involves ground disturbance activities.\nInitially helicopter-borne EM surveys such as VTEM can be flown to identify targets for follow-up work. These surveys are usually sufficiently sensitive to pinpoint anomalies to the extent that further ground-based EM is not required. The previous analysis of bottom-of-hole vacuum drill samples has been shown to be effective and this can be extended over the project area here access has been obtained.\nThe Coates Ni-Cu-Co-PGE Project, with an area of 47.3 km2, is located in the northern part of the South West Greenstone terrane of the Yilgarn Craton. The Project covers the Coates Mafic Complex within the Jimperding Metamorphic Belt and comprises various gneisses and banded-iron formation, interleaved with ultramafic intrusive units that have been repeatedly deformed, metamorphosed and intruded by granite.\nThe regional geological setting of the Coates Ni-Cu-Co-PGE Project has been highlighted by the Julimar Project being explored by Chalice. Chalice commenced a greenfield exploration program in mid-2019 for high-grade Ni-Cu-Co-PGE mineralisation in the region and interpreted the presence of an unrecognised layered mafic-ultramafic intrusive complex at Julimar based on high resolution regional magnetics. Follow-up exploration identified the Julimar (mafic-ultramafic layered intrusive) Complex that extends over a strike length of approximately 26 km, which has been confirmed by drilling to be highly prospective for Ni, Cu and PGEs.\nRC drilling resulted in the discovery of high-grade Ni-Cu-Co-PGE mineralisation at the newly named Gonneville Intrusion. A recent airborne EM survey identified three new large EM anomalies (Hartog, Baudin and Jansz) located north of the Gonneville Intrusion .\nThe style of sulphide mineralisation intersected consists of massive, matrix, stringer and disseminated sulphides typical of metamorphosed and structurally overprinted magmatic Ni sulphide deposits. It is understood that the discrete high-grade Cu-Ni-Co-PGE zones typically have the following grade ranges : PGE+Au: 1- 15g/t, Ni: 0.5-3.3%, Cu: 0.4-4.5% and Co: 0.03-0.27%.\nOn the local scale, the Coates Mafic Complex is a magnetite gabbro intrusion into granitic rocks that hosts the Coates vanadium deposit. Lenticular magnetite has formed at the core of the layered Coates Mafic Complex between two granitic bodies (Baxter, 1978). The deposit was discovered 1961 and explored in the late 1970s by Garrick Agnew Pty Ltd (1971). Mining started in 1980 only to cease operations a year later due to high silica contents limiting production.\n. Chalice ASX Announcement 22 September 2020.\n Chalice ASX Announcement 27 January 2021.\nPrevious mineral exploration in the area has predominantly been focussed on the Coates vanadium deposit (Coates) or bauxite. There has been very little exploration for other metals and only minor gold occurrences are located at Jimperding Hill to the north of the project area and unnamed occurrences to the southwest.\nApart from recent bauxite exploration by BRL within R570/59, the previous exploration completed over the area is not relevant to the current search for Cu-Ni-Co-PGE mineralisation. The early previous exploration is summarised for the purpose of reporting completeness.\nMaynard (1997) reports that gold was first discovered in the vicinity of Jimperding Hill in the late 1890s, but mining did not take place until the 1930s. The recorded production is small at 327 ounces.\nMangore (Aust) Pty Ltd\nExploration at Coates was first undertaken in 1961\nby Mangore (Aust) Pty Ltd, a subsidiary of Union Carbide Corporation, which carried out limited drilling and metallurgical studies but abandoned the project as uneconomic.\nGarrick Agnew was granted a Ministerial Reserve in June 1969 and subsequently completed an extensive drilling programme. Previous metallurgical test results indicate that a 58% recovery of vanadium at an approximate grade of 1.4% V2O5, 3% TiO2, 67% Fe grade with 8% SiO2 is achievable from an ore assaying 0.54% V2O5, 4.75% TiO2, 25% Fe and 29% SiO2.\n- Late 1970’s\nCoates was discovered near charcoal/pig iron producing areas outside Wundowie in the 1960s, and was subject of a failed attempt to mine it by Agnew Clough from the late 1970s when the German and Japanese-funded venture hit technical problems, rising energy costs and fierce competition internationally from rival projects. Agnew Clough was unable to reach its planned first stage target of 1600-1800 tpa, part of a plan to hit 3,000 t and 10% of the global market. The mine closed in 1982, just two years after becoming Australia’s first vanadium producer.\nDuring the 1960s and 70s the area was explored for bauxite by various companies including Pacminex (1965-1986), Vam Ltd (1969-1973), Bridge Oil Ltd & Project Mining Corp Ltd (1969-1987), Alcoa of Australia Ltd & Shell Company of Australia Ltd (1971-1981) and recently by BRL. Location of previous geochemical sampling results (PGE) over a total magnetic intensity image\nIn 2011 Mercator (Paton, 2011) explored E70/2230 for vanadium, base metals and PGEs using lag sampling, focused mainly on pisolitic laterite. Orientation sampling of mineralisation adjacent to the Coates Siding vanadium deposit was applied to regional surface geochemistry and successfully located vanadium anomalies in laterites approximately 3 km east of the Coates Siding deposit. In February 2011 Mercator and BRL entered into a joint venture agreement over E70/2230 which gave BRL rights to Bauxite within the tenement  . In 2013 BRL completed an extensive vacuum drilling programme at two bauxite prospects, Fortuna and Fortuna North (Menzies 2014). While bauxite was being targeted at the time, 520 end-of-hole samples were also analysed for As, Cu, V, Zn, Pb, Ag by ICP techniques and Au, Pd, and Pt by fire assay on the same samples. Mercator subsequently obtained a total of 950 assay pulps from bottom of hole samples from the bauxite exploration drilling and analysed the samples using a hand-held portable XRF (Cahill, 2015). Retention licence R70/59 covers the Fortuna bauxite prospect.\nPlotting the distribution of geochemical assay results show co-incident base metal anomalism (Ni, Cu, & Cr) with PGEs (Pt max 37 ppb, Pd max 53 ppb & Au max 108 ppb) along the western edge of the Coates Intrusion. The coincidence of the Cu and PGEs anomalies is significant as these elements have long been considered geochemical pathfinder elements for nickel sulphide mineralisation. The distribution of Cu, Au, Pt & Pd are shown below']	['<urn:uuid:78dd450f-50d9-4c13-bb50-d7e4808775f1>', '<urn:uuid:d00013a6-6b2a-4d55-80ea-54aaf594bd11>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:43:46.574121	11	53	1875
59	mediterranean food production climate impact solutions	Mediterranean food production is adapted to the region's climate through specific techniques like drought-resistant crops (olives, grapes, figs), terracing, and dry farming methods. However, climate change poses serious threats to agricultural productivity, with projected decreases in crop yields by 2050. To address these challenges, several solutions are proposed: diversifying away from climate-sensitive sectors, removing energy subsidies to reduce greenhouse gas emissions, implementing adaptation measures, and strengthening institutional cooperation between North and South Mediterranean countries to enforce policies that reduce greenhouse gas emissions.	['Understanding Mediterranean Agriculture: A Time-Honored Farming Tradition\nMediterranean agriculture, also known as dry farming or rain-fed agriculture, is a traditional farming method that has been practiced for centuries in regions with a Mediterranean climate. This unique agricultural system is prevalent in countries bordering the Mediterranean Sea, such as Spain, Italy, Greece, and parts of North Africa and the Middle East. Mediterranean agriculture is characterized by its ability to adapt to the challenging climate conditions of hot, dry summers, and mild, wet winters. In this article, we’ll explore the essence of Mediterranean agriculture, its key features, and its significance in sustainable farming practices.\n1. Climate and Geography\nThe Mediterranean climate is characterized by its distinct seasonal pattern, with hot, dry summers and mild, wet winters. The long, dry summers pose challenges for agriculture, making irrigation essential for crop survival. The geography of Mediterranean regions often consists of hilly or mountainous terrain, which influences farming practices and land management.\n2. Crop Selection\nMediterranean agriculture focuses on cultivating crops that are well-adapted to the climate. Drought-resistant crops such as olives, grapes, figs, citrus fruits, and various grains like wheat and barley are commonly grown. These crops have evolved to withstand the long, hot summers and rely on the moisture accumulated during the winter rainy season.\n3. Terracing and Slope Cultivation\nTo optimize arable land in hilly or mountainous regions, farmers often practice terracing. Terraced fields involve creating flat platforms on sloping land, reducing soil erosion and conserving water by capturing rain runoff. This ancient agricultural technique allows for efficient land use and prevents valuable topsoil from being washed away.\n4. Dry Farming Techniques\nMediterranean agriculture relies heavily on dry farming techniques, where crops are cultivated without irrigation. Instead, farmers rely on the winter rains to provide sufficient moisture to sustain crops during the dry summer months. Techniques such as deep plowing, mulching, and selecting drought-resistant crop varieties are employed to conserve soil moisture.\n5. Crop Rotation and Fallowing\nCrop rotation and fallowing are integral to maintaining soil fertility and preventing soil depletion. Farmers alternate the cultivation of crops to replenish soil nutrients naturally. Fallowing, the practice of leaving fields uncultivated for a period, helps the soil restore its nutrients and moisture content.\n6. Sustainable Farming Practices\nMediterranean agriculture embodies sustainable farming practices that prioritize long-term environmental health. The use of natural fertilizers, such as compost and animal manure, helps maintain soil fertility without relying heavily on synthetic inputs. Additionally, traditional methods of pest and weed control reduce the need for chemical pesticides.\n7. Importance of Biodiversity\nMediterranean agriculture fosters biodiversity by cultivating various crops suited to the region’s climate and terrain. This diversity contributes to the preservation of local plant varieties and enhances ecosystem resilience.\nMediterranean agriculture is a time-honored farming tradition that has sustained communities in regions with challenging climate conditions for generations. The interplay between seasonal rainfall and drought-resistant crops, along with innovative techniques like terracing and dry farming, exemplifies the adaptability and resilience of this agricultural system. By embracing sustainable practices, conserving natural resources, and promoting biodiversity, Mediterranean agriculture serves as a model for environmentally conscious and efficient farming methods. Its rich history and ability to thrive in adverse conditions make it a valuable asset in the pursuit of sustainable agriculture worldwide.', 'Interview with Dr. Abeer EL-SHINNAWY (FEMISE, American University in Cairo) within the context of the FEMISE & Institut de la Méditerranée contribution to the ENERGIES2050 report  on” Challenges to Climate Change in the Mediterranean” released at the occasion of the international COP22 conference that took place in Marrakech on November 15th 2016.\nThe report (in French) is available for download by clicking here.\nHow vulnerable are Mediterranean countries to climate change and why?\nThe Southern Mediterranean countries are considered as particularly vulnerable to the impact of climate change. This is mainly due to their geographical position and their dependence on climate sensitive sectors like agriculture and tourism. On the other hand, rising sea level is expected to endanger the lively hood of millions living in coastal areas of the Mediterranean sea. This is why assessment of the economic cost of climate change and planning for timely adaptation to climate change is imperative.\nYou gave the example of agriculture and tourism, can you give an assessment of costs of climate change as they pertain to these two sectors?\nWith regards to agriculture, climate change affects the sector mainly through its effect on crop yields. For instance, wheat yields could fall up to -5.7% in Tunisia and -7.3% in Morocco under changing climatic conditions by 2050. But one must not only look at the effect of local climate change on the economy of a particular country, but also the effect of global warming on global yields and its impact on world food prices and in turn how this impacts the local economy.\nAs for tourism, it is expected to be adversely affected by a warmer climate. By 2050, an estimated 1.9% increase in temperature is expected to decrease tourism demand in coastal areas in Morocco by 6% assuming no adaptation policies are implemented (and by 2% in Tunisia).\nBut perhaps the most damaging effect of climate change in the region will be most felt due to rising sea level. In the event of rising sea level, coastal cities in Egypt and Turkey will be threatened, Istanbul in particular.\nAre there any benefits that Mediterranean countries can derive from climate change ?\nIndeed, some benefits can very well materialize. Mitigating climate change can have local benefits that can lead higher economic growth. Put differently, by promoting what has come to be known as green growth, mitigation can lead to local benefits. It is possible to make growth more resource efficient, cleaner and more resilient without necessarily slowing it. In this respect, several channels through which a better natural environment can affect economic growth can be identified. These channels range from increasing the quality of factors of production; increasing resilience to environmental shocks; increasing the job content and poverty eradication features of economic growth etc…\nAre there specific examples of sectors in the Mediterranean that can grasp opportunities linked to climate change?\nOpportunities are not confined to any particular sector, but manufacturing, finance and insurance, construction and professional, scientific and technical activities stand out. In the case of manufacturing, products to support agriculture- example fertilizers and pesticides whose demand is likely to increase as a result of climate change- and health care provide opportunities. For Finance and insurance, opportunities include new products and services and insurance premium price as insurance permits individuals and companies to pool risk. On the other hand, demand for climate resilient homes provides an impetus to the growth of the construction sector.\nWhat can Mediterranean countries do to stop climate change? What Policies and what Actions?\nTo stop climate change, Mediterranean countries need to cut back on their emissions of GHG, in great part through reduction in the consumption of fossil fuels. In countries such as Egypt, removing subsidies on energy could be part of policies to achieve this objective.\nWhat can be done at the EuroMed level that is not already being done?\nIn general, Mediterranean countries need to diversify more and not be solely dependent on climate sensitive sectors like agriculture and tourism. Most importantly, these countries must inform their citizens of the repercussions of climate change for their livelihood so that they start investigating possible adaptation measures. In particular, and in response to the potential threat of rising sealevels, countries could consider that investment in coastal areas comes to a hault. In this regard, evidence so far points that citizens are unaware of such problems and in countries like Egypt, they continue to undertake important investments in potentially dangerous areas.\nMore importantly, institutional cooperation between North and South Mediterranean countries is most needed to implement and enforce policies that cut back on GHG. It is needless to mention that the lack of efficient institutions to design and implement policies to cut back on GHG and the lack of design adaptation measures are considered one of the most important bottlenecks these countries are facing as they confront climate change.\nFor more on this timely subject, please read the report available here.\nThe FEMISE Team\n ENERGIES2050 is a french association based in Nice (France). It seeks, in particular, to promote and encourage containing energy demand: energy saving, energy efficiency and the development of renewable energies. FEMISE and Institut de la Méditerranée have partnered with this important partner in the field of climate change to have an additional channel to contribute in meeting the challenges the EuroMed region is facing and reaching out to policy-makers.']	['<urn:uuid:465cf810-99f3-4720-a272-16a3c5fcd870>', '<urn:uuid:54092a47-78d8-477e-b72e-818445e20061>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	6	82	1437
60	Do Hitachi SB8V2 and Festool RO 150 both offer multiple sanding modes?	While Hitachi SB8V2 only offers belt speed control for different materials, the Festool RO 150 EQ combines three sanding functions in one tool: coarse sanding, fine sanding, and polishing.	"['A Timid Tool with a Big Heart\nThe Makita 9403 follows in the footsteps of the first power tool of Makita company, which was created back in 1958. Since then the company has been enjoying the reputation of a high-quality tool manufacturer. 11А motor allows performing sanding works of practically any kind. Using the controls placed on the sander outside you can adjust the speed of the sandpaper belt from 690 to 1640 SFM. Such motor power and belt speed variability will enable you to process wooden, metal surfaces and will fit very well for removing various paints and varnishes. The wide 4” х 24” grit paper belt greatly helps sand large surfaces as quick as possible while the belt control system prevents the belt from loosening and flinging. The 13 lbs weight of the 9403 belt sander makes it possible for an operator not to push the sander to a surface being machined but simply to lead the tool and control the whole process. For a dust collection, the manufacturer provided a bag on a swivel, which is 360° rotatable; together with a general design such bag placement allows an easy flush sanding to walls. The large Makita 9403 front grip ensures operator\'s comfort and control during a long cycle of continuous operation. The labyrinth-like design of internal space is instrumental in sealing and protecting motor and bearings from contamination. Moreover, the operation noise maxed at 84 dB renders Makita 9403 one of the quietest belt sanders in its class.\nParquetry Brushing With Your Own Hands\nNot everyone knows that brushing is a special surface treatment aimed at fashioning the surface into the one having an exclusive appearance and making a unique impression. This treatment accentuates the wood fibers and makes surfaces look more aged. Nowadays the antiquing treatment of floorings, furniture, and other wood articles is relevant and popular. This article contains a few hints on how to perform an antiquing brushing of parquetry or wood flooring by yourself. First, you need to clean the surface from all paints and varnishes, fortunately, the performance of Makita 9403 handheld belt sander suffices for doing it quickly and in a quality manner. After the cleaning, you will need to clean the surface off the dust and waste resulting from the treatment. The next step is actually the brushing of your floor or parquetry. To perform the brushing you will need a brush sander that is equippable with brushes of different hardness. Depending on the wood roughness you may need a different scale of treatment. At the first stage, you need to remove soft wood fibers with a rough brush to accentuate the entire fiber structure, which has been forming in the bulk of wood during the years. This kind of brushing will bring the wood material more relief and protrusions. Each consequent treatment is to be performed with a brush softer than before. The final treatment with a No 120 brush allows you to achieve a perfect surface finish and complete the brushing. The final stage of antiquing is toning, which means covering the surface with a varnish. A varnish coating can make your parquetry look more intense and give it a bigger depth of treatment. Thus, after the lacquering, you will be able to enjoy a new “aged” look of your parquetry.\nMore Products to Consider\nFor organized and convenient transportation of your tools and a belt sander you can purchase this Makita interlocking case.\nThe first thing we liked about the WEN 6502 belt sander and consider its main advantage is its versatile design, which allows professionals to choose between using the device as a belt sander or a disc sander. In fact, both these sanders today are of the most importance in the woodworking and treatment of other materials. The 6502 stationary belt sander has a robust and sturdy design and is manufactured as a metal alloy work table that ensures the structure endurance to high loads and reduces the vibrations. The 6502 model of a table belt sander has a tiltable sand paper belt (0-90 degrees), thus granting the users a wider range of possibilities when working with wood and other materials. Using the tension release lever you can replace the belt in no time and the locking mechanism will prevent the belt from flinging during the operation. The 2.5-inch dust collection port allows attaching a dust collector to minimize the debris clean-up.\nThe unique feature of WEN 6502 design lies in its dual operation as either a belt sander or a disc sander with a grinding disc rotating around its axis. At large, disc sanders are used for grinding or a precision turning of various materials. The WEN 6502 wide belt sander uses a 4.3А electric motor capable of accelerating the 6"" grinding wheel to 3450 RPM; at such speed the disc allows you to round edges, remove burrs and machine small parts quickly and with a high quality. Owing to its versatility and power this 2-in-1 rigid belt sander will work fine for those people who do the woodworking professionally or an amateur handyman. Buying the WEN 6502 belt sander will let you save storage space and money as you will not need to purchase two separate sanders. The miter gauge placed on the support table, which can be attached to both belt and disc sander variations offers 0 to 45° beveling capabilities that permit sanding and grinding of material at any angle within this range as well as trimming.\nMore Products to Consider\nWe recommend using protective goggles when working with power tools.\nBlack & Decker DS321\nDo you want to know how to convert your old wood materials into new furniture or home decor? For the Black & Decker DS321 belt sander and will not be a problem owing to its 7А motor capable of maintaining 800 SFM belt speed. This portable belt sander answers the purpose of middle and small sanding areas. Power aside the DS321 sander boasts numerous convenience and ease of operation features. This Black & Decker belt sander is equipped, for example, with front and back handles, which have an ergonomic shape that provides a firm grip for a stable handling and prevents slipping. The users will definitely find the front three-position handle, which is made retractable for an optimum control very useful. This solution allows turning a long sanding process into an enjoyable occupation. The low profile design achieved with the use of the front roller allows reaching even tight corners, small elements, and other hard-to-reach places while the belt control feature prevents the belt from slipping off the rollers. To help to keep your workplace clean and free of debris the DS321 model has a dust collecting bag that attaches to the sander left and will not be a hindrance to your work. To resume we would like to recapitulate that this handheld belt sander is a superb tool for polishing and sanding small areas and parts, which allows treating the surfaces unreachable for many belt sanders.\nPolishing the Floors by Your Own\nWooden flooring is a home environment element that plays a key role in preserving your health and the health of your family that is why replacing wooden floor elements or reflooring should not last long when making renovation to the premises. The modern technologies allow renovating your wooden floors by your own using some power tools. In this paragraph, we will tell you how to polish floors with a belt sander. Thus for polishing the floors you will need to arm yourself with a belt sander and belts of different roughness. The first stage of the floors renovation implies a rough polishing aimed at removing paint coating and varnish if there are any or at removing a worn out upper layer of wood. This procedure is performed using the 40 grit sand paper. The procedure may be repeated in case of getting an unsatisfying result after the first time but no more than four times as the main purpose of a 40 grit sand paper is to clean the floor off dirt and irregularities. At the next stage, you need to do a floor polishing with 60 grit and 80 grit sand paper to get rid of the remaining scratches. The finishing touch is a polishing with a 120 grit sand paper to achieve a perfectly smooth and accomplished surface. For the places that can not be treated even with this belt sander by Black & Decker, you can use a hand scraper.\nMore Products to Consider\nTo be sure of always being able of plugging your belt sander into the nearest power socket we recommend purchasing an extension cord.\nReliable and Powerful\nHitachi SB8V2 is a professional handheld belt sander built around a powerful 9А electric motor, which is capable of rough sanding of wood, metal materials, painted and varnished surfaces. This tool has a belt speed control that enables you to fine tune the speed to match the material machined. The soft elastomer surfaces of the auxiliary and main handles provide a firm grip and significantly reduces vibrations occurring during the operation. Another distinguishing feature of this portable belt sander is its nose design, which has a tracking window for improving the visibility of sanding surface and belt during the use. It takes only 30 seconds to replace the belt: you just need to turn the clamp lever to loosen the grit belt and substitute it with a new one. The dust collecting bag is made left-side mounted to provide a comfortable edge sanding. In addition, this Hitachi belt sander is a very reliable power tool with a long service life that will fit any craftsman. Consistent with the SB8V2 reliability the Hitachi company provides a 5 year warranty that makes it possible to reduce greatly the costs on belt sander maintenance.\nFounded in 1910 the Japanese giant has 1100 affiliated companies, which operate in 11 business areas ranging from the manufacture of small electric devices to big railroad and construction systems. The first company branch that specialized in power equipment and tools manufacture in 1948 was Hitachi Koki Cо. As of today, the Hitachi company produces power tools of numerous types, including gas chainsaws, electric drills, woodworking tools. In addition, on March 1, 2016, Hitachi KoKi acquired the Metabo, German power tools manufacturing company. That is the reason why Hitachi is one of the manufacturers of the most reliable power tools for the moment backing this statement with a 5 year warranty for its tools.\nMore Products to Consider\nThe sanding belt pack containing various abrasive grit belts will definitely be of use for those who operate a belt sander.\nPORTER-CABLE 371 is a very small and lightweight belt sander, which is an excellent tool for furniture making and refinishing, performing different household tasks. The small size and low gravity center allow you to operate this small belt sander for several hours without experiencing any fatigue. The tool’s design enables you to operate the PORTER-CABLE 371 sander with one hand. The comfort of this mini belt sander operation is also maintained by the removable auxiliary handle that provides better control in case of two-handed use and the flush side that allows sanding up to perpendicular surfaces. The case of PORTER-CABLE 371 horizontal and vertical belt sander has a dust sealed, recessed switch that minimizes unintended turn-offs. Some users report that they managed to grip this mini belt sander in a vice sanding side up and use it as a grinding machine to perform precision grinding and polishing of small parts. For the most effective dust collection, you will need to connect a vacuum cleaner to the removable dust port. Therefore this belt sander by Porter Cable can become your assistant in sanding and polishing of different materials while its special design enables its nonstandard use as a grinding machine.\nSand Paper History\nThe main element you need to decide on when sanding surfaces with a belt sander is an abrasive material, which is made of sand paper. That is why we decided to give some consideration to the history of sand paper development. Despite the fact that the batch manufacturing of sand paper was patented by Isaac Fisher JR in the USA in 1834 the first documented case of grit paper use relates to the 13th century in China. The first sand paper was made of shellfish shells, sand and seeds, which were glued to a paper with an ordinary glue. By 18th century John Oakey has launched mass production of sand paper made of sand and ground glass, that time this product was called a glass paper. Today there is a whole range of abrasive materials used in sand paper manufacture for sanding different surfaces: alundum, silicone carbide, diamonds, cubic boron nitride, garnet.\nMore Products to Consider\nFor protecting your respiratory tract from dust when working without a vacuum cleaner it is recommended to purchase a respirator.\nWhat is a Belt Sander\nWe would like to advise all those who do carpentry and timber work to pay their attention to belt sanders. This device is one of the power tools that suit very well for the peeling and primary sanding of timber, plastic, metal surfaces and paint removal by grinding. A belt sander is a tool that consists of an electric motor that spins a pair of parallel drums, on which a continuous loop of sandpaper is mounted. Such design allows achieving a high sandpaper moving force and machining a wide range of surfaces, even the roughest ones.\nFurthermore, if you are keen on the woodworking, you should pay attention to Jig Saws, which will be of great use for a shape cutting or a simple wood cutting.\nWhat Features to Compare\nWhen choosing this power tool it is crucial to keep in mind the sphere of a belt sander application and the types of work you are planning to perform with this sander. It is the type of application and tasks to be performed that governs the choice of belt sander features and parameters.\nWe consider the electromotor capacity and sandpaper feeding rate (usually measured in surface feet per minute) the first parameters you should pay attention to as they directly influence the Performance of a sander. Thus, a high motor capacity paired with a high speed of grit paper belt allows achieving high overall performance. On the other hand, a high performance or power not necessarily implies a wide range of applications. In such a case a belt sander equipped with a belt speed control, which enables you to machine surfaces that are sensible to overheat or to perform a high-precision sanding of small parts, becomes a versatile tool.\nThe second consideration point is the Type of a belt sander. In terms of general design and mounting the belt sanders are divided into stationary and handheld. Stationary belt sanders are massive power tools, which usually have very sturdy and robust metal casings. Such sanders fit best for machining small parts of any material or for shaping wooden or metal articles. Stationary belt sanders can also be used as grinding wheels for grinding various tools. However handheld belt sanders are more popular and versatile sanding devices, they are more lightweight and are smaller, which enables them to sand and polish different difficult to access surfaces, including the vertical ones. As a rule, portable belt sanders have electric motors that are powerful enough for removing coats of paint, and sanding rough surfaces of a wide range of materials.\nThe last but not the least if not the most important thing to consider when deciding on a specific belt sander model is the Convenience of use. A small weight and dimensions, ergonomic design, availability of triggers and auxiliary handles allow you to operate a sander for a long time without overstressing your body or getting tired. For example, a dust collecting bag can greatly help keeping your workplace clean and protecting the sander motor from getting clogged with dust and small debris. While the availability of a trend or a miter gauge will make it possible to sand workpiece edges getting more even bevels. Another feature that contributes to the convenience of use of a belt sander is a user’s ability to quickly and easily replace the belt. Most of the modern belt sanders have locks that are on either side of the case, which unlocking results in loosening the grit paper belt and making it simple to replace without any tools. This solution helps you save your time and efforts.\n1. Everything You Should Know About Belt Sanders, The Family Handyman.\n2. Joseph Truini, Know Your Power Sanders, And How to Use Them, PopularMechanics. October 2, 2012.\n3. George Vondriska, HOW TO USE A BELT SANDER LIKE A PRO, WoodWorkers.\n4. Sandpaper, Encyclopedia.\n5. Belt Sander, Wikipedia. August 25, 2016.\n6. Chris Baylor, How to Use a Belt Sander, About. March 20, 2016.\n7. Chris Baylor, Which Power Sander is Best for Any Given Task?, About. September 27, 2015.\n8. Tim Carter, Belt Sanding Tips, Ask The Builder.\n9. How to Use a Belt Sander Properly, eBay. March 10, 2016.', 'Festool Company History\nHow it all began\nThe company Festo was founded by company partners Albert Fezer and Gottlieb Stoll. They initially concentrate on repairing wood processing machinery and making structural modifications to these machines by converting plain bearings to ball bearings.\n1925 – 1932\nFirst company logo and tool for carpenters\nThe first tools for carpentry were developed under the first company logo ""Fezer & Stoll"".\nMore mobile: The first transportable chainsaw carries out heavy, previously manual work on site.\nSB 126 portable circular saw\nThe idea of the first mobile circular saw: With the SB 126 portable circular saw, you can avoid the tedium of transporting heavy material to stationary tools by simply taking the tool to the material instead.\nZUM 140 chain mortiser\nPrecise and versatile – ZUM 140: For routing slots. For drilling holes. And for routing grooves on stair stringers with the aid of an additional device.\nActive in social matters\nBerta Stoll, wife of Gottlieb Stoll and company partner, focusses her attention on the employees. This includes visiting those off sick, organising company functions and excursions and arranging children\'s Christmas celebrations.\nThe first logo of the Festo brand.\nBD 125 portable circular saw\nThe BD 125 portable circular saw from the year 1938 was popular amongst carpenters and joinery firms alike.\nBuild good things and spread the word\nFesto soon discovered that trade fairs were an ideal platform for exhibiting and demonstrating Festo products.\nMTD and KTD disc sanders\nBestsellers throughout the 1930s and 1940s: The MTD and KTD disc sanders. The Festo disc sander was also the first sander with patented independent extraction.\nFirst new building\nWhen space on Olgastrasse in Esslingen became too cramped for the continuously growing company, Gottlieb Stoll began planning the first new building on Ulmer Straße in 1938. A few years later, this address became the company\'s modern, new home.\nThe second generation of Festo\nA snapshot of the second generation of Festo. From left to right: Harald Kneile, Gerda Maier-Stoll, Heinz Attinger, Dr. Wilfried Stoll.\nAE 85 pendulum cover saw\nThe company set its sights on the timber construction industry from the very start, as demonstrated here by the AE 85 pendulum cover saw from 1946.\nAD 85 portable circular saw\nThe AD 85 portable circular saw offered its users six significant advantages: Exterior run-back protection for\ngreater safety, cutting depth adjustment without changing the motor\'s centre of gravity, slanted saw position at the scribe pointer level, gauge marker, shavings ejector and saw table contact surface.\nRTE orbital sander\nLegendary to this day: The RTE orbital sander replaced the hand sanding block, which was unpopular and considered time-consuming among painters, carpenters and automotive paint shops, and therefore paved the way for fine machine sanding.\nNew location in Neidlingen\nNeidlingen – renowned for its cherry blossom and, since 1951, for outstanding power tools. By 1950, it had become clear that the Ulmerstraße site in Esslingen would once again be too small in the medium term. In Neidlingen, Gottlieb Stoll found the perfect site to continue building tools for the toughest demands.\nSÄGEHEXE AAU 50 portable circular saw\nOne hand to work with, the other for increased safety: The SÄGEHEXE was the first one-handed circular saw with 500-watt cutting power at a cutting depth of 50 mm. It allowed carpenters to hold on to the roof with one hand while sawing with the other – something they had never been able to do before.\nPatent: 843300 height adjustment\nAU 50/65 portable circular saw\nSafety outside the box: The patented, integrated sliding clutch prevented the motor from overloading and protected the user from accidents. The AU 50/65 with cutting depth adjustment and slanted positioning replaced the SÄGEHEXE.\nPatent 1958: 1089148 sliding clutch\nNew company logo\nIn 1962, the Festo logo is redesigned for a more modern look. 1983 saw a further modification when the logo was turned blue.\nThe first guide rail\nIn 1962, the first generation of guide rails is brought to market. This is replaced by the new aluminium version in 1980. From 2003 onwards, the FS/2 guide rail is produced in a number of different variants. This second generation of guide rails has a second groove for additional accessories and a sliding strip to facilitate guiding the machine.\nIn the image on the right, Gottlieb Stoll demonstrates use of the first portable circular saw with guide rail at the beginning of 1960.\nRTT-S orbital sander\nSanding goes dust-free: The RTT-S was the world\'s first orbital sander with a dust extraction connection. Dust accrued while sanding and hazardous dust from 2K filler compound in particular was safely vacuumed by the extraction.\nPatent 1969: 19 38 350 extraction connection\nFocus on power tools\nIn 1975, the executive board decided to abandon the stationary machine market in favour of concentrating exclusively on the design and construction of power and air tools.\nWST 150 eccentric sander\nGreater material removal capacity than ever before: The first eccentric sander with integrated extraction was brought to market.\nROTEX RO 1\nSimply switch over and get started: The option to switch from coarse to fine sanding made it easier for the tradesman to switch between working steps. From this point onwards, a machine could be used both for tougher coarse sanding and ultra-fine sanding. Today, the ROTEX is still considered the high-performance all-rounder for carpenters, painters and automotive paint shops.\nPatent: 2938704, application in 1979\nAXF 45 plunge-cut saw\nPlunge and saw more reliably: The AXF 45 was the first in a series of successful Festo – and later Festool – plunge-cut saws. From this point on, together with the guide rail, cuts could be made with an unprecedented level of precision.\nPatent 1980: 3007310\nAT 55e portable circular saw\nPrecise cuts: Users are thrilled by the combination of portable circular saw and guide rail. The first guide rail featuring a rubber lip as splinterguard, designed to create a splinter-free cutting edge and easy to position on the scribe mark. The guide rail and plunge-cut saw became indispensable to carpenters.\nPatent 1980: 3007310\nPatent 1982: Splinterguard 3243564\nFesto turns blue\n1983 saw a further modification when the logo was turned blue.\nPS1 pendulum jigsaw\nLeading in precision: The adjustable guidance jaws in the table made it possible to cut right-angles even in very thick pieces of wood. The jigsaw therefore became a precision tool for the discerning carpenter.\nRecipient of multiple awards for ""Factory of the year"" and long-standing birthplace of tools for the toughest demands.\nExpanded company logo\nThe FESTO logo was expanded at the start of the 1990s to include the term ""TOOLTECHNIC"" – in order to disassociate the company from FESTO Pneumatic.\nFesto SYSTAINER SYS Classic\nThe most beautiful ""toolbox"" in the world: The SYSTAINER – winner of the Innovation Award and other product design awards – guarantees that tools are well-organised and protected, and since its launch has won the hearts of ambitious tradesmen all over the world.\nCDD 12 cordless drill\nCompact, robust, powerful: The CDD 12 cordless drill.\nNew Festool brand\nFesto Tooltechnic becomes the independent Festool brand and confirms loyalty to specialist shops.\nRelocation to Wendlingen\nIn 2000, we celebrated our 75th anniversary at the new site in Wendlingen.\nPS 300 pendulum jigsaw\nTriple saw blade guide – tool-free saw blade changing: Precise work could be refined even further using the PS 300, which could also be combined with a tool-free saw blade changing system. The jigsaw is the carpenter\'s most commonly used tool.\nRS 100 Q gear-driven sander\nWell-balanced centre of gravity for perfect results and non-tiring work.\nTS 55 plunge-cut saw\nThe living legend: The first plunge-cut saw with attachable splinterguard and a saw blade changing system with a safety lock for increased safety. It has long been synonymous with ultra-precise sawing.\nOF 1400 router\nRocker and ratchet principle for convenient cutter changes – as fast as it gets.\nRO 150 EQ geared eccentric sander\nThree sander functions combined in one tool: Coarse sanding, fine sanding, polishing.\nDF 500 DOMINO jointer\nSignificantly more stable and faster: For carpenters everywhere, the DF 500 DOMINO jointer redefined easy and rapid timber joining. The prefabricated, rotationally fixed dowel facilitates the easy and rapid production of edges and corner joints using high precision.\nKAPEX KS 120 compound mitre saw\nThe first Festool-quality compound mitre saw guarantees unprecedented precision thanks to its ball bearing-mounted double pillar guide. Intelligent features such as an angle dial and the ability to quickly fix the inclination angle also guarantee precise work results in the workshop or during assembly.\nCTL 36 E mobile dust extractor\nClean and healthy working thanks to the large container volume, yet compact design. Optimal utilisation of the container volume – the gross volume is equal to the net volume. For every requirement: Option of individually upgrading with an additional socket or compressed air module.\nLHS 225 EQ long-reach sander\nTwo lengths, one machine – extend or reduce in a few simple hand movements. Two-speed gears for efficient, rapid material removal on walls and ceilings\nClose. Open. Connect. With a single turn. SYSTAINER SYS T-LOC – This is how intelligent organisation can be.\nPSC 420 EB pendulum jigsaw\nOutstanding curve control with triple saw blade guide and torsion-resistant lifting rod.\nDSC AG 230 diamond cutting system\nCutting system with efficient dust extraction, protects health. Can be used on Festool guide rails for precise cuts. Safe guidance along the workpiece thanks to guide table with easy-running rollers. A viewing window also provides a clear view of the cutting line. Thanks to the plunge cut function, you can start cutting at any point on the material.\nQUADRIVE cordless drill\nPower, power and more power: The QUADRIVE series impresses with compelling power when screwing and inimitable precision when drilling. Just like all cordless drills, it features a FastFix interface to switch to CENTROTEC tool chucks within a matter of seconds for ultimate flexibility.\nUNIVERS SSU 200 sword saw\nAn innovative multi-talent: Whether it is wood or insulating material, the UNIVERS sword saw cuts all common materials in wooden structures up to 200 mm thick. With the guide rail dead-straight, tear-free chop, longitudinal, mitre, as well as shift, cuts can be performed without any problems. Perfect for building sites: 200 mm cutting depth with a weight of 6.5 kg.\nCONTURO KA 65 edge bander\nThe system for the perfect edge. For furniture edging that is worthy of being called ""perfect"": Developed for the perfect edge finish. For classic furniture edging or manufacturing complex shaped parts with convex or concave curves. For all wood, melamine or plastic edging.\nVECTURO OS 400 oscillating tool\nPrecise cuts along the scribe mark with the positioning aid, defined cuts with depth stop that can be adjusted without the need for a tool or flush cuts to the wall with a sliding shoe that can also be mounted without a tool. Powerful 400-watt motor with electronic speed control for a consistent work progress speed.']"	['<urn:uuid:46c89244-4d94-43d9-af4c-1b3ea63cafc6>', '<urn:uuid:c9e1baeb-ea90-47e7-9bdb-756d546527a2>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T04:43:46.574121	12	29	4683
61	What causes water contamination during floods and rainfall?	Heavy rainfall and flooding can contaminate water in multiple ways. During extreme precipitation, runoff can spread sewage, chemicals, gasoline, coal ash and disease agents into water sources. In groundwater wells, which receive limited treatment, this contamination risk is especially high. For example, in Havelock North, rain washed livestock waste containing Campylobacter into the water supply, while climate change is increasing the frequency and intensity of such heavy rainfall events, making water contamination more likely.	['A Cautionary Tale of Untreated Groundwater, Campylobacter, and New Zealand’s Largest Drinking Water Outbreak\nHavelock North is a suburb of the City of Hastings on the North Island of New Zealand with 14,000 residents. By the end of August 2016, over one-third of the residents of this entire town had been sickened by drinking water contaminated with Campylobacter bacteria, the most common source of foodborne illness in New Zealand.1 How did a wealthy country with national drinking water standards come to experience the largest documented drinking water outbreak in its history?\nLike most waterborne disease outbreaks, Havelock North’s began when a few residents stayed home from work assuming they had food poisoning or a seasonal illness. The trickle soon grew to dozens, then hundreds, and ultimately thousands of residents suffering from debilitating cramps, headaches and nausea. “By Friday August 12 schools reported hundreds of children struck down with widespread vomiting and diarrhoea… Whole families were sick, with parents having to look after children while battling the illness themselves.”2\nLocal health authorities noticed the rising numbers of patients seeking medical treatment for gastrointestinal symptoms at about the same time the Hastings District Council received a positive test for bacteria in the town’s groundwater supply. The bacteria were later determined to be Campylobacter. Local officials decided to chlorinate the groundwater shortly thereafter and advised residents to boil water. The latest estimates are that more than 5,000 persons (with over 600 confirmed cases of campylobacterosis) were sickened, with scores hospitalized and two confirmed cases of Guillain-Barré syndrome-a rare neurological illness that can be caused by Campylobacter infection. A detailed timeline of the outbreak is available.\nOn average, New Zealand records about 17 waterborne outbreaks (range from 6 to 27) per year,3 primarily zoonotic diseases that can spread between animals and humans such as through ingestion of water contaminated with feces. The top two are caused by intestinal parasites, Giardia and Cryptosporidium; the third is Campylobacter. The most frequent reservoirs of Campylobacter in New Zealand are sheep, poultry, cattle and swine. Initial reports are that livestock are the most likely source of the Havelock North Campylobacter.\nMost reported outbreaks in New Zealand are small with an average of about eight affected persons, but larger outbreaks are often traced to animal waste entering a well after heavy rain and receiving no or insufficient treatment. In fact, just days before the first people became sick, the region received three months’ worth of rain in a single weekend-conditions hauntingly familiar to the deadly pathogenic E. coli and Campylobacter outbreak that occurred in 2000 in Walkerton, Canada, where seven people died and thousands fell sick. In Walkerton, heavy rain washed cow manure applied as fertilizer into a municipal well whose chlorinator was not operating due to inadequate maintenance. Similar to Walkerton, Havelock North also relies on a shallow aquifer, but unlike Walkerton, Havelock North was intentionally not chlorinating because their groundwater had been considered “secure” from contamination. In addition to local and regional investigations, an independent government inquiry into the cause of the historic outbreak was announced on August 18, the results of which are expected by March 2017.\nSimilar to the fallout from the Walkerton outbreak, the Havelock North outbreak has raised concerns about the management of public water sources across the New Zealand, including whether chlorination should be required for all groundwater drinking water supplies. Aside from the still unknown but considerable human and societal costs,4 the 2016 Havelock North outbreak is a cautionary tale: Public health officials must be ever vigilant to safeguard drinking water sources from contamination while providing appropriate disinfection. Because chlorine is both economical and effective in killing/inactivating Campylobacter and most other pathogenic bacteria and viruses, it can be viewed as a very inexpensive insurance policy against the high cost of waterborne illness when it is continuously applied at the proper level.\nFred M. Reiff, P.E., is a retired official from both the U.S. Public Health Service and the Pan American Health Organization, and lives in the Reno, Nevada, area.\n1 On a global basis, the majority of small outbreaks of Campylobacter gastrointestinal illness are due to eating meat that has been contaminated with this pathogen during the slaughtering process. It is noteworthy that most raw poultry is contaminated with C. jejuni, which highlights the need for it to be cooked thoroughly before consumption.\n2 Something in the Water – How the Havelock Gastro Outbreak Began. http://www.nzherald.co.nz/hawkes-bay-today/news/article.cfm?c_id=1503462&objectid=11697426.\n3 New Zealand Ministry of Health (2007). Estimation of the Burden of Water-Borne Disease in New Zealand: Preliminary Report. https://www.health.govt.nz/system/files/documents/publications/water-borne-disease-burden-prelim-report-feb07-v2.pdf.\n4 What’s in our water? The Alarming New Threats to New Zealand’s Drinking Water. http://www.listener.co.nz/current-affairs/social-issues-current-affairs/water-threats-new-zealand/.', 'Below, you will find detailed information, resources, and opportunities to take climate-protective action.\nWhat’s the relationship between climate change and water-borne illnesses?\nAs climate change continues, water-borne illnesses are likely to become more common. That’s because climate change increases precipitation, storm surges, and sea temperatures. These environmental factors contribute to flooding and runoff that can spread sewage, chemicals and disease agents. They also favor the growth, survival and spread of bacteria, viruses and toxins created by harmful algae. As a result, more people will likely be exposed to water-borne illnesses through ingestion, inhalation and skin contact, as well as consumption of contaminated fish and shellfish.\nHeavy Rainfall and Flooding can Affect Drinking Water\nClimate change increases the frequency and intensity of heavy rainfall. This leads to runoff and to flooding, especially in river and coastal areas. Drinking water can be contaminated by chemicals, gasoline, coal ash, sewage and more.\n- Extreme precipitation events have been linked to increased levels of pathogens in treated drinking water, and cases of gastrointestinal illness in children.\nIn 1993, Milwaukee experienced its heaviest rainfall in over 50 years. This led to a Cryptosporidium outbreak that accounted for 403,000 illnesses and over 50 deaths. Cryptosporidium is a parasite that infects the intestines of people and animals.\n- Groundwater wells receive limited water treatment. This makes them more susceptible to water contamination from extreme precipitation events and increases the risk of waterborne illnesses in those who consume it.\nIn 2000, a heavy rainfall event in Walkerton, Ontario, Canada, carried agricultural runoff containing E.coli into the town’s primary water source, a shallow well. This extreme weather-related event caused 2,300 illnesses and seven deaths.\nAs climate change continues, the intersection of flooding and higher temperatures is likely to transport pathogens into recreational waters and foster their growth.\nFish and Shellfish Contamination\nWater contamination from sewage overflows can affect seafood.\nAs climate change has raised sea surface temperatures and altered precipitation patterns, harmful algal blooms have become more common and expanded their geographical range. These blooms are dangerous because they produce potent toxins which can contaminate seafood.\nHow can I help fight climate change?\n- Spread the knowledge by sharing our postcards!\n- Use our postcards to query your federal, state or local government representatives: What are they doing to protect your community from the dangers to health posed by climate change?\n- Climate change is accelerated by burning fossil fuels. In order to slow climate change and protect air quality, we must replace fossil fuels with renewable energy and energy efficiency.\n- Join PSR’s Activist List\n- U.S. Global Change Research Program (2016, April 4.) The Impacts of Climate Change on Human Health in the United States: A Scientific Assessment\nThis government study documents “what we know about the impacts of climate change on public health, and the confidence with which we know it.” It examines a broad range of health impacts as they affect the health of the American people, not just in the future but right now.\n- PSR: Vector-Borne and Water-Borne Disease (Fact Sheet)\n- PSR: The Spread of Insect-borne and Water-borne Disease (Powerpoint)']	['<urn:uuid:c80f2ef3-f4da-45c2-8fc1-026ae96c9bb8>', '<urn:uuid:bbfbfc7f-d350-4129-b7f8-3ed53c4778ee>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	8	74	1288
62	How do El Niño and La Niña events differently affect winter temperatures and weather patterns in the Eastern United States?	La Niña typically leads to warmer than normal winter temperatures in the Eastern U.S., with an active polar jet stream and less prolonged periods of cold weather, particularly when combined with a negative Pacific Decadal Oscillation (PDO). In contrast, El Niño increases the chances of cold snaps, especially in late winter, due to its effects on the jet stream.	"[""It's Official: La Niña is here!\nJust last week, the Climate Prediction Center, a branch of NOAA’s NWS, declared that the atmosphere has officially entered a La Nina. So, what does this mean for our weather and how will it affect the upcoming months? La Nina is just one phase of a three-phase oscillation called El Nino Southern Oscillation (ENSO); the other two are El Nino and ENSO neutral. ENSO is an oceanic and atmospheric coupling that is a result of sea surface temperature changes from normal (also called an anomaly) and surface pressure changes. The images below show the sea surface temperature anomaly signatures and corresponding surface pressure signatures of all ENSO phases:\nEl Nino: defined by warmer sea surface temperature anomalies in the equatorial Pacific (greater than 0.5 Celsius above normal) and lower pressures resulting in weaker or even reversed trade winds. This forces the thermocline to get pushed deeper into the ocean and warmer water from the West Pacific moves East. This is illustrated by the bright orange colors (above average sea surface temperatures) in the black circle on the map below (credit to NESDIS):\nThe resulting El Nino atmospheric circulation with lower than normal pressures and reduced trade winds are shown below (credit Fundamentals of Physical Geography):\nLa Nina: defined by cooler sea surface temperature anomalies in the equatorial Pacific (greater than 0.5 Celsius below normal) and higher than normal pressures causing enhanced trade winds. This pushes the thermocline to the surface and allows cooler water to upwell. This is illustrated by the blue colors (below average sea surface temperatures) in the black circle in the image below (credit to NESDIS):\nThe resulting La Nina atmospheric circulation with higher than normal pressures and enhanced trade winds are shown below (credit Fundamentals of Physical Geography):\nENSO Neutral: defined by sea surface temperature anomalies in the equatorial Pacific near normal (between 0.5 and -0.5C). This is shown by the mix of light yellow and light blue indicating small changes in sea surface temperatures compared to normal (credit NESDIS).\nThe current sea surface temperature anomaly configuration strongly supports a La Nina. Notice the large tongue of blue (in the image below), which indicates sea surface temperature anomalies that are colder than normal. This upwelling of colder water is caused by stronger than normal easterly trade winds as a result of the higher pressures in the equatorial Pacific blowing toward lower pressures over the Maritime Continent in the Western Equatorial Pacific (credit NESDIS).\nThis graphic below shows very strong trades (blue and purple colors) that will continue to upwell colder waters over the entire equatorial Pacific (credit Mike Ventrice).\nOne other main factor that can go into enhancing ENSO effects and drive intraseasonal oscillations such as the AO (Arctic Oscillation), PNA (Pacific North American Oscillation), NAO (North Atlantic Oscillation), and EPO (Eastern Pacific Oscillation), is the Pacific Decadal Oscillation (PDO). The PDO has two main phases: positive and negative. The phases are defined by a horseshoe shaped ring of colder sea surface temperature anomalies in the Northern Pacific (-PDO) or warmer sea surface temperature anomalies in the Northern Pacific (+PDO). As seen below, the sea surface temperature map in the North Pacific has a lot of warmer sea surface temperature anomalies as indicated by orange colors on this map. The PDO index published by the CPC is -0.89. The current PDO numerical index is weak to moderately negative, but the cooler anomalies indicated by the cooler sea surface temperatures in the subtropics near Hawaii and signs of emerging cool anomalies off the California coast would go further to enhance the La Nina (outlined by the black circles below). The aforementioned intraseasonal oscillations help drive our day to day pattern in the fall, winter, and spring with specific phases being favored during La Nina combined with a -PDO.\nThe classic -PDO sea surface temperature configuration is shown below:\nSo, what should we expect moving forward into fall and winter? Classic La Nina climatology is a warmer pattern with an active polar jet stream. While climatology is an average, I suspect we will see some colder shots of air (as intraseasonal changes occur over the course of days to weeks), but the fall and winter are expected to largely average above normal temperature-wise.\nLa Nina’s tend to favor:\n· +AO: this goes back to the strong, low amplitude jet stream that will favor a progressive pattern meaning less likely prolonged periods of colder weather.\n· +NAO: often works in conjunction with AO leading to less Eastern troughing and less prolonged cold periods.\n· +EPO: the -PDO helps feed a lower amplitude and stronger jet stream with less troughing over the Eastern U.S.\n· -PNA: also influenced largely by the -PDO, this would mean more ridging centered in the Eastern half of the country, meaning warmer than average winter.\nIt is looking almost certain that a moderate La Nina is in the works with a -PDO working in conjunction for a warmer than normal fall and winter with an active polar jet stream. How small varying intraseasonal changes in the AO/NAO/PNA/EPO cannot be pinpointed in this range and we will have further updates as changes occur throughout the fall and winter."", 'What is El Niño Southern Oscillation (ENSO)?\nWhat is El Niño?\nThe El Niño Southern Oscillation (ENSO) is a large-scale climatic phenomenon that originates in the tropical Pacific but affects global climate patterns. The warm phase is known as El Niño, and the cold phase is La Niña. El Niño occurs irregularly every two to seven years and peaks in winter.\nThe term El Niño, Spanish for ‘child’ or ‘the Christ child’, was first used by fishermen along the coasts of Ecuador and Peru to refer to a warm ocean current that typically appears around Christmastime and lasts for several months.\nWhat causes an El Niño event?\nIn a normal year, when ENSO is inactive, the equatorial Pacific trade winds blow from east to west. The winds push the warmer water towards the west, and colder water rises up from deeper in the ocean to replace it. This creates an east-west difference in sea surface temperature and hence an east-west difference in sea level pressure that maintains the trade winds and so drives a positive feedback loop.\nDuring an El Niño year, the east-west SST difference weakens, the pressure difference weakens, and the trade winds and their effects on the ocean weaken, so the eastern Pacific warms further. However, during a La Niña year, the opposite happens, the east-west temperature difference strengthens, the pressure difference strengthens, and the trade winds and their effects on the ocean strengthen, so the east Pacific cools further.\nSuch changes in sea surface temperatures affect the atmosphere over vast areas, with local and global repercussions. Locally, the associated atmospheric circulation changes drive increased atmospheric convection and precipitation over the central and eastern Pacific, while rainfall is reduced over the western Pacific. In addition, these changes have remote impacts throughout the tropics and at higher latitudes via the atmosphere, especially when ENSO is at its strongest in winter.\nHow do we measure El Niño?\nA network of ocean buoys measures water temperature, currents and the wind. Satellites also provide surface temperature and current data, providing the ability to monitor ENSO in real-time. We can also predict El Niño months in advance as we can measure the heat content of the upper ocean, one of the precursors for the onset of El Niño and use our climate models to predict its evolution. Although various criteria exist, we usually say an El Niño event is underway when sea surface temperatures in the equatorial Pacific (officially called Niño region 3.4) rise 0.5°C above the historical average for at least three months in a row.\nImpacts of El Niño and La Niña\nThe most substantial impacts are experienced by those countries near the tropical Pacific origin of ENSO. Changes in surface temperatures, winds and moisture affect rainfall intensity and patterns, leading to extreme events such as flooding and drought. During an El Niño event, Peru, Ecuador and the south-eastern parts of South America receive heavy rainfall. In northern Brazil, drier conditions or even drought results. Indonesia, South Asia and parts of Australia are also more likely to experience drought during El Niño. The change in weather patterns associated with El Niño can significantly impact the economy, particularly agriculture, water resources, fisheries and public health. ENSO also strongly influences the occurrence and intensity of tropical cyclones, and Atlantic hurricane activity weakens during El Niño but strengthens during La Niña. In the UK, we experience the socio-economic impacts of an El Niño event, partly through increased food prices. There are effects on the jet stream and European weather, especially in late winter when El Niño increases the chances of cold snaps. La Niña increases the chances of wet and stormy conditions.\nHow does El Niño affect global temperature?\nEl Niño releases heat into the atmosphere and increases subsequent global temperatures. For example, global average temperatures for 2016 were around 1.1°C above preindustrial values and the strong El Niño episode of 2015/2016 partly contributed. However, researchers have concluded that the warming from El Niño is only accountable for about 0.2°C of this overall figure.\nEl Niño and climate change\nBecause of the large event-to-event variations of El Niño, we don’t have enough past years of observations to show a clear impact of climate change on its properties. However, there is now some evidence that the effects of El Niño on rainfall may increase in the future and that we may even see more extreme ENSO events, but these remain active research questions.']"	['<urn:uuid:53b6c47e-47e7-43d1-869e-ba364386b415>', '<urn:uuid:5c500c11-408a-4bc2-b27d-e1b63643da23>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T04:43:46.574121	20	59	1607
63	What are thermal imaging detector types and environmental requirements?	Thermal imaging uses medium wave (3-5 microns) and long wave (7-15 microns) detectors, with long wave being optimal for human body temperature at 9.5 microns. For accurate measurements, external surveys require no solar radiation, no precipitation, wind speeds under 10 m/s, and a temperature difference of at least 5°C between inside and outside.	"[""Thermal Imaging System for Human Fever Screening\nDuring the past decade, infrared thermometry has seen remarkable success in measuring human body temperature, as evidenced by the many variations of the infrared ear thermometer. Ear thermometers measure temperature by detecting the amount of energy emitted in the infrared region of the electromagnetic spectrum. All objects having a temperature higher than absolute zero (-273°C) emit infrared energy. In fact, as shown below, the majority of body heat loss in a room temperature environment is by emitted infrared energy.\nThere is a direct correlation between the amount of infrared energy an object emitts and the object's temperature. The graph below shows the amount of infrared energy emitted by an object at a number of different temperatures ranging between 3,000 to 5,000°C. When the temperature of an object increases, it emits a greater amount of infrared energy. By measuring the amount of infrared energy emitted by an object, its temperature can be calculated.\nThermal imaging cameras, like those manufactured by Optotherm, operate in a similar manner to ear thermometers. However, whereas ear thermometers can only measure the temperature at a single spot, thermal imagers can measure the temperature at tens of thousands of spots that together create a thermal image, as shown below. Like ear thermometers, thermal imagers are passive devices and emit no harmful radiation.\nThere are two primary detector technologies used in thermal imaging cameras for human fever screening; medium and long wave. Medium wave infrared detectors are sensitive to electromagnetic energy that is 3 to 5 microns (0.003 to 0.005 mm) in wavelength. Long wave infrared detectors are sensitive to electromagnetic energy that is 7 to 15 microns in wavelength. As shown in the graph above, there is a peak wavelength of emitted energy for each specific temperature and the peak wavelength increases at lower temperatures. The peak of emitted energy for objects at human body temperatures is approximately 9.5 microns in wavelength. For this reason, Thermoscreen was developed using long wave detector technology to take advantage of the increased sensitivity these detectors provide when measuring the temperature of human bodies.\nHow Fever Screening Works\nThermoscreen operates in real-time at 60 frames-per-second by sampling the infrared energy emitted within its field-of-view. It processes the sampled data using a computer and then creates and displays thermal images on an LCD monitor. A color palette, as shown to the right of the thermal image, is applied to thermal images so that each pixel is displayed in the color that represents that pixel's temperature. Thermal images are displayed in real-time and can be analyzed instantly to assess the skin temperature of individuals being screened. Human skin provides an excellent target for thermal imaging because skin emits infrared energy very efficiently. This property of human skin helps thermal imagers provide accurate and consistent skin temperature measurements.\nAs individuals pass through the field-of-view of the camera, their thermal image is analyzed and areas of skin with temperatures exceeding a threshold are displayed in bright colors. Setting the appropriate value for the threshold is necessary for effective fever screening. The specific value of the threshold depends on several factors, such as site environmental conditions. The procedure for properly setting this value will be explained in detail in the following sections.\nThis product is currently for export only. Optotherm is currently undergoing the US FDA Premarket Notification (510k) submission process for medical devices. As soon as Thermoscreen has been cleared, this product will be available for purchase in the US."", 'What is thermography?\nThermography is a visual method of illustrating invisible heat energy. This is typically captured using an infrared camera, which displays a map of the temperature variations emitted by all objects with a surface temperature above absolute zero (-273°C).\nObjects are then illustrated as thermal patterns, using different colours or shades of grey depending on the palette selected. The thermal patterns illustrate the variations of heat energy being emitted from the surface of the object.\nThe subsequent interpretation of the thermal patterns requires an understanding of the environmental conditions at the time of capturing the image, as well as surface characteristics of the materials being viewed.\nEmissivity (an object’s ability to emit infrared radiation) and reflectivity (the property of reflecting radiation) are the two characteristics of materials which determine how reliable the thermal pattern is in the image. Materials with high emissivity have a low reflectivity and vice versa.\nOnly materials with a high emissivity provide a reliable reading. This is because materials with a low emissivity have a tendency to reflect the temperature of surrounding objects.\nTypically, materials such as bricks and plaster have a high emissivity, whilst metals and glass have a low emissivity. Given that the fabric of many older buildings will be made up from predominantly high emissivity materials, thermography can be a really useful tool for assessing the fabric of new and existing buildings, in particular retrofitted energy efficiency interventions.\nThermography as a retrofit tool\nThe main advantage of using thermography as a tool is that you can see what is going on under the skin of a building without having to take it apart. Having the ability to see invisible heat energy provides valuable data for assessing thermal performance.\nQuantitative thermographic assessments of heat loss require precise climatic conditions, which rarely occur, particularly in external environments. As a result, typically only qualitative thermographic building surveys provide robust and reliable data.\nQualitative thermographic building surveys can provide many useful insights, such as: the effectiveness of insulation as a barrier to heat loss; occurrences of thermal bridging; sources of air-leakage; moisture and damp; and hidden components, such as pipes and wall ties.\nThere is a clear benefit to using non-destructive infrared thermography in traditional buildings. For example, intrusive investigations – drilling holes or dismantling construction elements – would otherwise be required to identify missing insulation and the location of thermal bridges. Furthermore, air leakage tests only identify that there is air infiltration, not its location. And moisture and damp often goes unnoticed until it appears on the surface.\nHowever, being able to achieve robust and reliable results from qualitative thermography requires professional training and certain environmental conditions to be achieved at the time of the survey.\nGetting the most out of thermography\nBeing able to use an infrared camera and accompanying software, as well as understanding and interpreting thermal images, requires a suitable level of knowledge and experience about the survey process and the building being assessed.\nThe correct survey process includes ensuring that the current environmental conditions are suitable, as this will maximise the accuracy of the data being collected. Although camera technology has improved over the years, along with the process itself, results still depends on the type of building fabric being surveyed. Therefore having knowledge and experience about building materials and construction methods are an essential prerequisite for a successful survey.\nThe criteria for the environmental conditions also vary depending on whether an internal or external thermographic survey is required. The latter is more challenging. For example, external surveys require: no solar radiation on the surface before and during the survey; no precipitation before and during the survey; a temperature difference between the inside and outside the building of at least 5°C – more commonly 10°C – is recommended; and wind speeds of no more than 10 meters per second (m/s), or preferably 6 m/s are recommended. And the length of time with no solar radiation and precipitation prior to undertaking a survey depend on the absorptance (the effectiveness of absorbing heat energy) of the building material.\nTo overcome the need to avoid solar radiation, many experts currently recommend undertaking thermographic surveys in the dark, although others suggest that a cold overcast day is sufficient. It is also often advocated that thermographic surveys should only be undertaken in the winter, which could be a significant limitation to its use.\nHowever, in my previous work I have achieved reliable results to identify thermal bridging using thermography during the early hours of the morning during the summer. The most important consideration is taking due account of the environmental conditions during interpretation.\nInternal surveys have fewer restrictions. With the exception of temperature, most of the criteria used for external climatic conditions do not apply. However, the temperature difference between the inside and outside of the building is critical for both internal and external thermographic surveys. Subject to the restrictions imposed by the resolution of the infrared camera, the greater the temperature difference, the better the detail that is visible in the thermal image.\nCamera resolution is critical for undertaking thermographic surveys of building fabric, particularly when needing to capture a whole façade. Sometimes it is also necessary to use a wide angle lens. And occasionally images have to be stitched together, depending on the size of the building. The cost of cameras rises significantly in line with increases of the resolution, as well as with the addition of any accessories, which can be a further significant limitation.\nBut despite this, one great advantage of using thermography during retrofit is for communicating what has been done. Capturing the whole façade before and after insulating is a great way of visually highlighting the improvements that have been made. More detailed and close up images are also useful for identifying any gaps and subsequent thermal bridging.\nWhen interpreting thermal images taken inside the building, colours representing cold areas are an indication of heat loss. Whereas for external thermal images, it is the colours representing warm and hot areas that are an indication of heat loss. These areas of heat loss can be either expected as part of the design, or unexpected and indicating a potential problem. Issues could include inappropriate installation, poor workmanship, or other unforeseen consequences.\nFollowing a series of complaints by residents approximately 12 months after they had received retrofitted external wall insulation (EWI) to their homes, the Carbon Trust was commissioned to undertake an independent investigation using thermography. The residents’ concerns centred on feeling colder and experiencing draughts since the EWI had been installed, as well as the quality of the installations.\nThe homes are of non-traditional British Iron and Steel Federation (BISF) construction and required extensive structural repairs as part of the retrofit works. To ensure structural issues were addressed appropriately, the installer commissioned a structural engineering consultancy to develop detailed designs for the replacement of structural elements, at the same time as incorporating the EWI. These designs also aimed to address the requirement for a continuous covering of EWI to minimise the occurrence of thermal bridging, wherever possible.\nA visual inspection indicated that the installations had been undertaken to a high standard. However, it was observed that the plinth (base) of the external walls had not been insulated and the installer advised that it had not been possible to insulate lofts due to the risk of asbestos. The funding did not allow for these locations to be insulated, as it would have significantly increased the costs of installing EWI to the main structure of the dwellings.\nNot insulating the plinths appears to have resulted in the solid concrete ground floor slab providing a passage for heat loss from the building. This meant that there was a significantly lower air temperature recorded at this level when compared to waist level. This was compounded by the decision not to insulate the loft, which meant that the roof was also a source of significant heat loss. When combined these two factors appeared to have provided the ideal conditions for creating air movement through the building, which were the draughts the residents were experiencing.\nTaking a whole-house approach, rather than just focusing on installing EWI, may have prevented this from occurring.\nA version of this article first appeared in the Institute of Historic Building Conservation Journal’s 2017 special issue on green retrofit.\nAbout the author\nDr Jo Atkinson leads the Carbon Trust’s housing-related work. Jo is an architectural technologist and qualified thermographer. Her doctorate focused on the evaluation of retrofitted external wall insulation, where she developed a methodology for assessing its construction quality, which included the use of thermography.']"	['<urn:uuid:51ee1cfc-50d7-4f0d-97f3-39c1b33dad92>', '<urn:uuid:5f88ffc5-9dab-4093-9575-9fd518753c38>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	9	53	2015
64	how can doctors tell if animals have mental health problems diagnosis method	Since animals can't communicate verbally, diagnosing mental health problems requires careful and extensive direct observations, gathering behavioral history from individuals who have lived with the animal, and sometimes measuring brain activity through technology. Additionally, the nature of a disease can be inferred by observing how animals respond to certain medications.	['For every book or article on dogs’ physical well-being, there are only a few that address mental health. Perhaps that’s because dogs seem to have, well, a few adorable quirks, but they’re generally so sane — happy and energetic, cooperative, resilient. They live in the moment, they adjust to changes, they soldier through pain. They forgive us our trespasses.\nBut sometimes a dog seems to have serious emotional problems. For author Laurel Braitman and her husband Jude, trouble came in a large package, her Bernese Mountain Dog, Oliver. He experienced truly terrible separation anxiety, first by destroying things inside their apartment, later by desperate attempts to escape. Most frighteningly, he once propelled himself out the window of their fourth-floor apartment, landing on the concrete stairs leading to the basement. Amazingly, he survived, but during his long recovery, his owners began a determined quest to figure out how to understand and abate Oliver’s symptoms.\nOne result of their struggle eventually was divorce, caused in part by the tensions created by their differing perspectives on Oliver’s problems and how to handle them. Another result is her recent book, Animal Madness (Simon & Schuster, 2015), an exploration of mental illness in a variety of species, mainly elephants, cetaceans, primates and dogs, but others as well. Braitman studied the literature on the topic, both scientific and anecdotal, and traveled to a number of exotic locations to interview keepers, owners and veterinarians on the methods they have used to restore disturbed animals to sanity.\nBraitmann has good credentials for undertaking the task she set herself. She is an author, historian, and anthropologist of science with a PhD from M.I.T. She is currently a Writer-in-Residence at the Center for Biomedical Ethics at the Stanford University School of Medicine and her latest TED talk has been watched by over a million people. She has been involved with animals all her life, and for this book traveled extensively to meet and learn from mahouts in Thailand, primatologists in Africa, and many other experts as well.\nBraitman begins from the position that:\n…[H]umans and other animals are more similar than many of us would think when it comes to mental states and behaviors gone awry. …Abnormal behaviors…tip into the territory of mental illness when they keep creatures — human or not — from engaging in what is normal FOR THEM.\nIdentifying “abnormal” behaviors is difficult, of course. Since animals can’t talk to us, diagnosis has to involve, first, making careful and extensive direct observations (if possible) and getting a history of the animal and descriptions of its behavior from individuals who have lived with it. Sometimes technology can be used to measure brain activity, though of course this is expensive and therefore uncommon.\nIn addition, as with some human diseases, we infer the nature of a disease from observing the effect of medicines used to treat it. For example, in humans, Parkinson’s Disease can be differentiated from similar “Parkinsonisms” by whether it responds to levodopa. Many medications developed for human use (Haldol, Prozac, Zoloft and many others) have been used to treat mental problems in animals. In fact, not a few pharmaceuticals have been developed for human use through testing their effects on animals, and then “come full circle” to be used medically on the animals themselves.\nThe core of the book is an exploration of a variety of forms that animal mental illness may take. Among them are separation anxiety, depression, obsessive behaviors (such as licking, rocking, pacing), aggression and other antisocial behaviors. Interwoven with this exploration is discussion of the likely causes of abnormal behavior — maltreatment (neglect, captivity, impoverished environments, cruelty, extreme stress, isolation from others of the animal’s own kind or from inter species “friends,” etc.).\nZoos come in for Braitman’s most serious indictment of human actions that imperil animals’ sanity.Too often they exemplify many of the conditions least conducive to normal existence for animals–captivity, close containment (even chains), improper diet, squalid conditions, and isolation.Braitman believes that zoos, even those offering enriched environments for the creatures held captive in them, are really just a way of making us forget that the animals we’re watching don’t have normal lives or relationships nor do they have any control of their conditions. She would prefer they be abolished in favor of creating places where creatures that “thrive in our presence” can be observed, or replaced with “teaching farms urban dairies, and wildlife rehabilitation centers.”(She doesn’t mention it, but perhaps someday we can observe wild animals in virtual reality.)\nZoos aren’t the only target of her concern: inhumane farming practices are also called to account. In the United States and Europe, there are estimated to be about 16 billion animals in farms and labs.Among them, abnormal behavior is actually normal:91.5 percent of pigs, 82.6 percent of poultry, 50 percent of lab mice and 80 percent of minds exhibit abnormal behaviors.\nEven our much-loved pets may suffer, as Braitman’s Oliver exemplified.What she believes she learned about his symptoms is interesting.She argues that the fear that overcomes some dogs when left alone is not so much a matter of being separated from a particular person.Rather, it’s the sheer terror that grips a highly social animal when it is entirely alone. She says that a dog like her Oliver, when finding itself alone, experiences a wave of intense anxiety, and it is to escape these bad feelings that it engages in destructive behavior. In this situation, the presence of any human (or other non-threatening creature) may be calming.(Destructive behavior like Oliver’s is not the only manifestation of the extreme anxiety created by loneliness, however. A few creatures will retreat into themselves, becoming depressed and despondent. Often, they refuse to eat. As one expert, Dr. L’Elise Christensen (DVM) said, “Emotional eating in dogs is not eating.”)\nAlthough much of the discussion in the book is uncomfortable to read, not all human effects on animals are harmful.Braitman offers many touching stories of the loneliness of wild animals held captive in zoos and of domesticated animals taken or sent away from their human families. But she alsoshe also shares many heart-warming examples of the restoration of lonely animals to health and happiness when they find new homes or congenial companions, of animals’ resourcefulness and resilience, of their intelligence and generosity to other creatures.\nThese tales indicate a distinction that Braitman mentions infrequently but does not discuss as fully as this reviewer would wish — the distinction between wild and domesticated animals, particularly our pets. As obviously as she believes that animals in general need social interaction with companions of their own kind, she also clearly thinks pets can be happy with humans, even in the absence of others like themselves. And she points out cases where wild animals in captivity have had tight bonds of friendship (for want of a better, less anthropomorphic term) with a wild or domesticated animal as a companion. What are the boundaries being drawn here? Which species can live as friends and which can’t? What creates the conditions for inter-species friendships, including human-animal ones?\nAfter her intellectual and physical odyssey in writing this book, Braitman offers a conclusion that is compelling and appealing.We have to live with wildlife, she points out, because we have spread so widely over the earth that there are few spaces left where they are not close to us.As for domestic animals, we want to live with them, having learned over time that we ourselves experience health benefits (including better mental health) from their presence and our interaction with them.Predictably she deplores those who are cruel to animals, and she encourages us to act to as to change corporate farming and other practices that result in unnecessarily and excessively tortured lives for the animals that sustain us.\nFinally, she counsels contemporary pet owners to be aware that our lives do not always allow our companions to live their own best lives.She believes we should do what we can but forgive ourselves for the shortcomings that the modern world imposes.Instead of guilt, though, she proposes we take action – to “stop leading the sorts of lives that cause large numbers of our pets to end up on psycho-pharmaceuticals.”It’s a good point.When you think of it, in fact, who among us doesn’t wish we could “spend more time walking and playing with them and less time on our phones, checking email and watching television.”Improving our lives in ways that would also improve their lives will, she promises, be entirely worth it.']	['<urn:uuid:f471e36d-3c5a-4686-b362-fab60fd18b76>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	12	50	1413
65	explain main challenges encountered regional aviation scheme india airports	The regional aviation scheme faces several challenges: airline operators show limited interest due to profitability concerns, there are infrastructure and manpower shortages, land acquisition problems exist, and there's insufficient investment. Additionally, there are issues with slot availability at major airports like Delhi and Mumbai, and most Regional Connectivity Scheme (RCS) airports are Visual Flights Only, making them inaccessible during bad weather. The passage of GST and rising fuel charges may increase airfares, potentially leading to airlines opposing the scheme due to profitability concerns and possible increase in NPAs.	['- UDAN is regional air connectivity scheme (RCS) which seeks to make flying affordable by connecting unserved and under-served airports.\n- This scheme has been launched as per the vision in new Civil Aviation Policy which had capped the passenger fares for flight journeys from un-served and underserved airports at Rs.2500 per hour of flying for around 500 kilometers under RCS scheme.\n- The main objectives of this scheme are:\n- To revive the existing under-served and un-served airports / airstrips in smaller towns and provide them connectivity so that persons in those towns are able to take affordable flights.\n- Provide viable and profitable business to operators.\n- Promote tourism, increase employment, promote balanced regional growth\n- RCS is applicable on route length between 200-800 kilometers with no lower limit set for hilly, remote, island and security sensitive regions.\n- The business model of the scheme is based on Government subsidy and viability gap funding (VGF).\n- The central government will provide concessions of around 2% excise on VAT and service tax at 1/10thrate along with liberal code sharing for regional connectivity airports.\n- The airlines are required to commit around 50% of the seats as RCS seats on RCS flights.\n- The fund for this scheme would come from a Regional Connectivity Fund (RCF) created by levying certain charges on certain flights. States will need to contribute around 20% to this fund.\n- For balanced regional growth, the allocations will be spread equitably across five regions in the country viz. North, South, East, West and North East with a cap of 25%.\nSignificance (Extra points)\n- Expansion of air connectivity\n- New opportunities in aviation will revive Investor sentiments.\n- Creation of more employment opportunities by expansion of the sector.\n- Price reduction for short duration flights will make flying more affordable to citizens.\n- Direct air connectivity to prominent tourist destination will boost tourism and development of tourist areas.\n- Through introduction of helicopters and small aircraft, it is also likely to significantly reduce travel timings in remote and hilly regions, as well as islands and other areas of the country.\n- Constrained by issues such as non-availability of slots in prominent airports such as Delhi and Mumbai & meeting Overhead costs .\n- Air Operator’s Certificate is detailed and cumbersome. Even though the government indicates that it will fast track the AOC procedure, but the target seems too optimistic\n- Majority of the RCS airport are Visual Flights Only which makes them inaccessible during adverse weather conditions, especially in the hilly areas. Also, there is lack of technical staff\n- Passage of GST will lead to increase in airfare, coupled with the rising fuel charges will lead to airlines opposing the move as it doesn’t seem profitable enough and could increase airlines NPA\n- There are also logistics issues, as slots for the routes awarded out of metro cities like Mumbai and Delhi are not available.\nChallenges faced by India to connect a large number of unconnected places and people with airways :-\n- Airline operators do not show much interest as it will not be profitable to them.\n- Lack of adequate infrastructure.\n- Lack of adequate manpower and staff.\n- Land acquisition problems.\n- Lack of adequate investment in these places.\nRole of NHRC:-\n- It can take suo motu cognizance of cases related to human right volations and can investigate on filing a petition of by order of the court.\n- It is entitled to observe the condition of inmates by undertaking frequent observation of jail premises.\n- NHRC works with international organizations, NGOs and SHGs for promotion and protection of human rights.\n- It reviews constitutional, legal and provisions of international treaties from the prism of human rights.\n- It has jurisdiction even to intervene in court proceeding if they are violating the human rights.\n- Investigating the violation of human rights or action of any public servant that amounts to negligence in prevention of such violation\n- visiting the jails or any other such public institutions under state government to inquire about the living conditions of inmate.\n- NHRC has taken several measures to promote right education in India like a recent inclusion of human right in curriculum of schools and colleges\n- Several success stories in its track record like Hussainara khatoona case, sunil batra vs Delhi administration, its judgement on custodial deaths etc\n- An important intervention of the Commission was related to Nithari Village in Noida, UP, where children were sexually abused and murdered. NHRC helped bring out in open a multi crore pension scam in Haryana\n- Limited resources, inadequate funds\n- It can only recommend measures\n- NHRC act does not extend to J&K\n- NHRC cannot investigate an event if the complaint was made more than one year after the incident. Hence, a large number of genuine grievances go unaddressed.\n- Violations by armed forces cannot be effectively investigated (no power to summon witnesses)\nOverburdened with complaints:\n- With increase in complaints it becomes difficult to address the cases.\n- It depends on state and central machinery like CBI and state police for investigation which has led to several unsolved cases of human right abuses like Godhra masacre, Muzzafarnagar hindu muslim riot.\n- Issue of majoritarianism may affect the impartiality/accountability of the body.\n- Shoddy investigations which lack due diligence.Many of its reports have been severly castigated by civil society activists for being far removed from ground realities.\n- For example, reports in case of alleged extra-judicial killings in Manipur.\n- Lack of impartiality :\n- The NHRC at times, has been unable to take a stand critical of the government of the day. For eg. during the Kairana exodus, it submitted a report vindicating the government stand, without undertaking a thorough investigation.\n- Marred with controversies:\n- For instance, the Batla House encounter case in the recent past. The Commission’s report giving clean chit to the Delhi Police came under fire from various quarters.\n- It was said that the Commission had failed to conduct a proper inquiry as its officials never visited the site and filed a report on the basis on the police version.\n- It is often viewed as a post-retirement destinations for judges, police officers and bureaucrats with political clout. Bureaucratic functioning, inadequacy of funds also hamper the working of the commission.\n- Its decisions need to be made enforceable by the government.\n- If commissions are to play a meaningful role in society, they must include civil society human rights activists as members.\n- Misuse of laws by the law enforcing agencies is often the root cause of human right violations. So, the weakness of laws should be removed and those laws should be amended or repealed, if they run contrary to human rights.\nWhat does ethics seek to promote in human life:\n- Ethics promotes morality in human life by making morals and value judgments a part of the decision-making process.\n- Being ethicalmeans choosing the proper course of action despite obstacles and challenges that may hinder one from choosing to do the right thing.\n- Ethics’ broadly acts as ‘Code of conduct’ for leading human life in a co-operative, peaceful & positive manner.\n- Ethics seeks to promote the principle of utilitarianism (Maximum good for maximum people).\n- Ethics in human life is essential to help build character and outlook of and individual.\n- It is important to sustain and cherish human life without infringing rights of others at indiviual level and maintaining sociaL order an harmonay at societal level.\nWhy is it important in public administration :-\n- Ethics are the rules that define moral conduct according to the ideology of a specific group. Moreover, ethics in public administration are important for good business conduct based on the needs of a specific town, state or country.\n- Ethics provide accountability between the public and the administration.\n- Adhering to a code of ethics ensures that the public receives what it needs in a fair manner.\n- It also gives the administration guidelines for integrity in their operations. That integrity, in turn, helps foster the trust of the community.\n- Public servants have sensitive information about countries economy,defence and security it is necessary that they adopt ethical standards and do not spell the secrets which can harm the society.\n- To adopt to a corrupt free behaviour administrators should have ethical standards.\n- Ethics seeks to promote compassion and empathy in administrator to maintain social harmony, unity, pro-poor policy formulation, more focus on equity and social justice.\n- With a strong code of ethics in public administration, leaders have the guidelines they need to carry out their tasks and inspire their employees and committees to enforce laws in a professional and equitable manner.\n- Another positive outcome of good ethics in public administration is timely and informative communication with the community.\n- This kind of transparency builds trust and prevents or minimizes the potential issues that can arise when information is divulged from outside sources.\n- In a democratic legal order, it is essential (i) to provide both public officials and the public at large with a common frame of reference regarding the principles and standards to be applied and (ii) to assist public officials to develop an appreciation of the ethical issues involved in effective and efficient public service delivery.']	['<urn:uuid:650e5d06-8477-40d2-bdb5-98d3c4f8425d>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	9	88	1564
66	need device adjust music frequencies both ears separately max audio quality specs features	For separate frequency adjustment in each ear, an audio device can store audiograms (frequency response maps) for both the left and right channels of a stereo signal. The Shure KSE1500 offers this capability through its electrostatic technology, which uses a thin diaphragm between conductive metal plates to produce unmatched clarity. The system features a frequency response of 10 Hz to 50 kHz, up to 113 dB SPL output, and adjustable gain range from -40 dB to +60 dB. It includes a 4-band parametric EQ for precise frequency control and supports high-quality audio with 16/24-bit depth and sampling rates up to 96 kHz.	"[""FIELD OF INVENTION\nThis invention relates to improvements in audio reproduction.\nDISCUSSION OF PRIOR ART\nThe popularity of music with people of all ages is showing no signs of decreasing. In addition to the bulky home entertainment systems that are present in almost every household, recent developments in data storage have seen considerable advances in portable audio reproduction devices.\nThe Sony Walkman (Registered Trademark), which allowed sounds recorded as analog signals on audio cassette to be reproduced, is perhaps the earliest example of a successful portable audio device. Prior to the Walkman, the only way of hearing recorded music on the move was the portable radio or a bulky cassette player. In recent years, the compact disk has replaced the audio cassette due to the improved quality of the audio reproduction that can be achieved.\nCompact disks can hold a large amount of data, but devices which use these disks are relatively bulky. An improvement would be to store the data in a non-volatile memory, but until recently, the cost of the memory has made such a device unrealistic. A drop in the costs of electronic memory, and the development of standards for electronic data compression techniques, permitting many minutes of high quality audio to be stored in a relatively small area of electronic memory, has made these devices a commercial reality.\nBecause of their compact size, and the resulting limited capacities of the batteries that are used, most of these devices use headphones as the means for producing an audio signal from the stored data. Playing the sound directly to each ear reduces the amount of power required. However, for many people, particularly those with impaired hearing, the sound quality achieved using earphones is unsatisfactory.\nIt is known to provide portable music devices with simple volume controls to allow the level of sound produced to be varied by the user. However, this offers only limited control over the sound reproduction to each ear of a user.\nDevices with basic graphical equalization are also known. These have, to date, typically comprised a series of slide switches which can be moved by a user to provide a crude variation in amplitude of a band of frequencies within the range of the audio device. If the switches are accidentally moved, the settings are lost. For this reason, graphic equalizers are unpopular on portable devices.\nIn another example, a factory set bass boost facility is commonly provided which increases the relative amplitude of low frequencies over higher frequencies. This can either be enabled or disabled by a user operated switch and is pre-programmed into the device. Not everyone likes the effect that this facility provides.\nSUMMARY OF INVENTION\nIn accordance with a first aspect, the invention provides an audio device comprising:\na memory arrangement for storing one or more audio data files and an amplitude frequency personalized profile for at least one ear of at least one user;\na file selector for enabling selection of one or more of the stored audio data files and at least one of the personalized profiles;\na data processor for accessing the one or more selected files and profiles from the memory and processing the selected files with the profile to generate for the at least one ear a processed audio signal.\nIn cases of particular interest, the device is a portable audio player including an electro-acoustic transducer for each ear of a user, for example, a pair of headphones.\nSuch a portable audio device can provide improved audio reproduction by applying a personalized profile to the stored data to personalize the sound quality of the final audio.\nBy portable, we simply mean that the device is small enough to be held in the hand, although ideally the device may be pocket sized so that it can readily be carried in the pocket of a jacket or shirt. It is also typically battery operated.\nThe device can combine the personalized profile with the stored audio data in the digital domain prior to the processor converting the combined digital data into an analog signal for reproduction. Thus, the processor applies the profile to the raw data each time the data are reproduced. The audiogram can be multiplied with the audio signal on a frequency by frequency basis.\nIn a further refinement, the processor may apply the profile to a stored data file to produce a processed data file, the processor stores the processed data file in the memory, and the processor accesses the processed data file when the audio data is to be reproduced.\nThe apparatus may include input means for inputting additional data files to the memory. The processor may apply the selected profile to the input data to produce the processed data file which is stored in memory.\nWhere the memory contains more than one profile, the apparatus may include a selector to enable the user to select the appropriate profile to be applied to a data file when the data are being copied to the memory, or when the data file is being accessed for reproduction by the processor.\nThe personalized profile may comprise an audiogram comprising a map of decibel gain against frequency (dB/Hz), which can be embodied as a digital filter having a personalized frequency response. The frequency response of the filter may be user definable prior to storage in the memory. The audiogram may be continuous or discontinuous, i.e., gain at each frequency or a at a set of spaced apart frequencies across the range. Where it is discontinuous, the gains may be interpolated to provide the required gain of any chosen frequency in the range.\nThe profile may comprise a pair of audiograms, one audiogram being applied to each of the two channels of a stereo signal. This is especially beneficial to users of headphones who may have impaired hearing in only one ear. Each audiogram may define the profile of a digital filter.\nIt is envisaged that a portable device including the invention will be especially useful for people with hearing difficulties as it permits the audio reproduction to be tailored to match the frequency response of each user's hearing. For example, if a user has limited hearing over a small range of frequencies within the normal range of audible frequencies, the profile may increase the amplitude of these frequencies relative to the other frequencies in the audible range. This applies particularly if the frequency response in one ear is markedly different from that in the other ear. In that case, the user response to sound provided through a conventional headphone arrangement may be very different from that obtained in a normal environment.\nThe memory may comprise at least one area of non-volatile memory. The memory may be integral to the device or may be removable from the device. Where the memory is removable, it is most preferred that a separate area of memory which is non-removable is provided to in which the personalised profile is stored. The device may contain 32 Mbytes of memory, or perhaps 64 Mbytes or more. To minimise the amount of memory needed the audio files may comprise compressed data files, possibly compressed using the MP3 data compression format or the “.wav” format. Of course, other compression formats may be employed and it is envisaged that the device may be compatible with any one of a number of such formats.\nWhere the data is compressed, the processor may be adapted to decompress the data in the data file prior to applying the personalised profile to the decompressed data.\nIt is envisaged that the device will mostly be used to store data files representing musical songs. For example, each file stored in the memory of the device may comprise a track from an album. The data files may be purchased as digital data, such as the data stored on a compact disk. Alternatively, they may be obtained over the internet.\nAlso, while the invention is principally of benefit to portable devices in which headphones are used, it can be applied to non-portable devices as well.\nIn accordance with a second aspect of the invention, an apparatus for generating a personalized user profile comprises an audio signal generator generates audio signals across a range of audible frequencies in accordance with a range of different types of audio data. A selector enables a user to select a type of audio data. An input device monitors the response of a user to the generated signals. A profile generator generates a personalized profile for a selected type of audio data from the response of the user. The profile comprises a personalized audiogram. A memory stores the generated personalized profile.\nThe apparatus thus generates at least one test audio signal and produces a profile in response to user inputs that are dependent upon the test audio signals. It preferably generates a plurality of test signals.\nMany different audio signals may be produced by the apparatus to generate the profile. A specific example of a suitable signal is a pair of tones which are played simultaneously or one after the other. They may have identical amplitudes. In this case, the user may be prompted to identify which, if any, of the tones sounded the loudest. The profile may then preferentially amplify the tone that sounded the quietest more than the other tone.\nIn an alternative, the test audio signals may comprise musical samples such as parts of songs. The test audio samples may contain a wide range of frequencies across the audible range, or one or more frequencies across a sub-range of audible frequencies.\nIn one arrangement where the memory contains a set of different audio samples, the apparatus includes prompting means for prompting the user to indicate which audio sample they preferred to listen to, and in which the profile generating means generates a personalised profile from the users response to the prompt.\nThe test audio samples stored on the apparatus may include a range of different types of audio data, such as a piece of speech, a jazz track or a dance track, and generate a test set for each type of audio data. A personalised profile may be generated for each type of audio data.\nThe test provided by the device may be interactive. For instance, one or more audio samples may be played and the user asked a question or questions about the sample(s). A new sample may be played which is altered using a profile generated from the users responses and farther questions asked until the user indicates that they are happy with the sound.\nIn this manner, the apparatus may generate a personalized profile that comprises at least one audiogram having a frequency response dependent upon the sensitivity of the users ear to different frequencies. The audiogram may compensate for deficiencies in the users hearing. Of course, it may alternatively simply allow a user to create a frequency response which they find pleasing regardless of the frequency response of their ears.\nA separate profile may be produced for each of the left and the right channels of a stereo system. To obtain maximum benefit, the user should wear a set of headphones to allow each ear to be isolated during testing.\nA separate profile may be produced for each of a set of different types of music. For example, a profile may be provided for classical music and a profile for pop music.\nThe apparatus may include a display that is adapted to display a graphical illustration of the personalised profile. Conveniently, this may comprise a bar graph or line graph type display of frequency against amplitude.\nThe input device may permit the user to directly manipulate the personalised profile independent of any test audio signals. This allows complete personalisation of a stored profile.\nThe apparatus may include means for copying the generated profiles to portable device, the portable device storing the profiles in memory.\nThe apparatus may further include means for editing a personalised profile by a user. This allows the profile to be tailored to a users preferences independent of the tests or after a basic profile has been produced by the tests. This may be achieved, for example, by way of a graphical interface which displays the profile as a set of sliders on a display which can be moved up or down by the user. Each slider corresponds to a different frequency or range of frequencies and mimics an analogue graphical equaliser.\nIn a most preferred arrangement the apparatus comprises a personal computer (PC). The computer may include a computer program stored in memory which when running on the computer causes the computer to generate the audio signals, issue appropriate prompts to the user and receive the users inputs responsive to the audio signals.\nIt will be appreciated, however, that the apparatus of the second aspect of the invention and the portable device of the first aspect may be combined into a single unit. This would permit audio files to be stored and replayed on the move as well as allowing a user to generate one or more personalised audio profiles.\nIn a further refinement, the personalised profile may include an indicator of a preferred voice type selected from one or more user selectable voice types, the device processing audio data such that any speech contained in the audio data is reproduced using the preferred voice type.\nThe device may select the preferred voice type from a predetermined selection of voice types based upon the shape of an audiogram which forms part of the profile. For example, the device may select the voice type which will be easiest for a user to hear as indicated by the audiogram of their personalised profile.\nIn a further alternative, the user may be played samples of different voices and asked to select which voice they preferred to listen to/found easiest to hear.\nIn a simple arrangement, the voice preference indicator may be stored in a data file together with one or more audiograms. This allows the device to reproduce audio data, such as news broadcasts, in a way which is easy for a heavily ‘impaired’ person to hear.\nAccording to a third aspect the invention provides a data carrier which includes a computer program which when running on a processor makes the processor operate in accordance with the second aspect of the invention.\nIn accordance with a fourth aspect the invention provides a method of playing stored audio data comprising the steps of:\naccessing a personalized user profile for an audio signal, wherein the personalized user profile comprises for each ear a map of amplitude-frequency profile over a range of audible frequencies;\nselecting at least one audio data file;\nprocessing the selected audio data file with the personalized user profile to produce a processed audio signal; and\nfeeding the processed audio signal to a sound generator which separately for each ear audibly reproduces the information stored in the processed audio signal.\nThe method may comprise storing the processed audio signal in a memory and selecting the processed audio signal from the memory for reproduction. The method may further include the steps of generating a plurality of user profiles and selecting one of the profiles corresponding to the content of a selected audio data file.\nFor example, a profile may be stored for jazz music, a profile may be stored for classical music, etc. If a jazz song is selected, then the jazz profile may be automatically selected from the memory.\nThe method may be used to produce a profile which can be stored in a device according to the first aspect of the invention."", 'Shure KSE1500 Electrostatic Earphone System\nThe Shure KSE1500 earphones introduces us to the future of music listening and appreciation. Introducing the first ever single-driver electrostatic Sound Isolating earphones.\nNow, a lot of people would think that a single-driver earphone isn’t much to get excited about. That would be true for conventional earphones, but the KSE1500 is the next step in audio technology and enables listeners to fine-tune and appreciate frequencies that may be distorted, or lost completely.\nThe tech behind this works thusly: a very small, very thin diaphragm sits between two conductive metal plates. Applying an audio signal (the electric current) to the metal plates displaces and moves the diaphragm which then creates moving air which is converted into acoustic sound pressure. The result? Music clarity unmatched by conventional earphone architecture.\nThe KSE1500, like every Shure product sold, promises to deliver the utmost quality in fit and finish. With Shure’s signature reliability and a new kevlar reinforced cable, the KSE1500 is sure to impress all users from both an ergonomic and audio standpoint.\nFinally, included with the earphones is a rechargeable USB amplifier called the KSA1500. The amplifier is designed specifically for the KSE1500 and is responsible for powering the KSE’s electrostatic drivers. The KSA1500 allows for digital-to-analog conversion (otherwise known as DAC) and support for digital, streaming, and pure analog audio sources. The KSA1500 also features four customizable EQ points. This allows the user to shape and fine-tune the musical tone giving them complete control over his or her listening experience.\nShure’s KSE1500’s pave the way for future audio technology development. With the KSE1500’s being the first ever earphones to implement this electrostatic tech, we could be seeing the biggest push for mobile-listening perfection ever. At this point, you wouldn’t expect much less from the company that brought us the exquisite SE846 quad-driver earphones.\n• 4-band Parametric EQ features five preset settings and four customizable settings for management of audio playback preferences\n• Preset settings include flat, low boost, vocal boost, loudness, de-ess\n• Each user-defined setting features four customizable EQ points (low shelf up to 630 Hz, two frequency-variable peaks, high shelf down to 800 Hz), allowing detailed personal control, shaping and tuning for different musical tone preferences Enhanced audio controls provide user-friendly digital and analog options to maximize audio playback fidelity\n• Digital audio input via USB\n• Bit depth: 16-bit / 24-bit • Sample rate: 44.1 / 48 / 88.2 / 96 kHz\n• Analog signal path via standard 1/8"" (3.5 mm) “Line In” input/switch • Bypasses digital processing for pure analog audio (“Bypass” mode)\n• Analog input pad settings (10 dB or 20 dB) for managing high input levels\n• Input level meter for monitoring signal gain structure • Volume level meter\n• User-selectable limiter Streamlined navigation for quick and simple selecting of the KSE1500 settings\n• Tactile main control knob\n• Power / previous screen button\n• Hold switch will lock the KSE1500 to prevent any accidental setting changes\n• Color OLED screen for easy viewable, detailed navigation through settings Integrated USB rechargeable battery can conveniently charge from provided wall charger or computer, even when streaming USB audio from computer\n• Battery life ◦ Up to 7 hours with digital USB input (EQ mode) ◦ Up to 10 hours with analog input (Bypass EQ mode) • Charge time: approx. 3 hours 15 minutes for full charge from empty (using power adaptor)\nCompatible with Mac, PC, iOS and Android devices via included accessory assortment, featuring Micro-B to Lightning and Micro-B OTG cables\n1. Can I use the earphones with another amplifier, or with my portable media player? Can I use my own headphones or earphones with the DAC amplifier?\nAs they are designed to work specifically together, both components of the KSE1500 Electrostatic Earphone System feature 6-pin LEMO connectivity that delivers the specific bias voltages (or signal levels) from the amplifier that are necessary to drive the electrostatic earphones. There is no standard 1/8” (3.5 mm) connectivity option on either component, nor is it recommended to attempt to use any other LEMO connector device with either component.\n2. What is “electrostatic earphone technology”?\nSimilar to the inverse relationship between dynamic microphones and dynamic loudspeakers: Audio source via a vibrating membrane magnetically produces electronic impulse (dynamic microphone) as opposed to electronic impulse magnetically produces audio via a vibrating membrane (dynamic speaker) Electrostatic technology can be understood as the inverse of condenser microphone technology: Audio (force) exerted on a membrane suspended in an electronic field producing electronic impulses (condenser microphone), as opposed to electronic impulses exerted on a membrane suspended in an electronic field producing audio (electrostatic speaker)\n3. The KSA1500 Amplifier can process both digital and analog input signals? What is the benefit of having both DAC and ADC functionality?\nThe KSA1500 Amplifier contains both DAC and ADC conversion, providing the flexibility to be used for both digital and analog inputs. The digital-to-analog converter (DAC) allows for a digital audio signal to be converted from binary signal (1s and 0s) into the analog audio waveform that represents the original sound of the program material. The analog-to-digital Converter (ADC) allows an analog source input to be converted into digital signal which can then be routed through the DSP (EQ) and then convert it back to analog after processing using the DAC.\n4. Is it safe to use the KSE1500 while charging?\nYes, the KSE1500 system is designed for use while charging from a laptop or the provided power supply.\n5. Is it safe to charge the KSE1500 with a portable battery bank?\nThe KSE1500 system can be charged by battery pack with the following specifications: 5V, 500mA min, 2A max input, max current draw at 1A.\n6. What is the difference between the parametric EQ in KSE1500 when compared with the EQ function in iTunes and other music apps?\nWhile some currently available programs and apps use parametric style EQ, most of them use a simpler graphic (or fixed frequency) equalizer which only allows for a limited range of frequency bands. Although the graphic EQ are simpler, the parametric EQ allows more customizable settings.\n7. Why does the KSE1500 system EQ include both five fixed EQ profiles and four used-defined EQ settings?\nInclusion of both sets of EQ profiles provide a large number of options for customizing the listening experience. The fixed profiles were set to address common adjustments made by the user - flat, low boost, vocal boost, and loudness are standard EQ settings that help accommodate both the user’s preferences and any idiosyncratic characteristics of the recording. Beyond that, the user-defined EQ settings are available if the user wishes to specifically accommodate or adjust some specific aspect of the audio quality to match a personal preference based again upon the mix balance of the audio playing. Additionally, a global EQ tweak may be preferred due to a specific quality of the source media player.\n8. For the user-defined EQ settings, what will happen if, say, point 2 is swept after point 3 (point 2 at 4KHz, point 3 at 1KHz)? What about if points overlap? (Ex.: point 1 at 500Hz +4dB and point 2 at 500Hz +5dB - does this result in a combined +9dB at 500Hz or is there a maximum cap?)\nFilter points 1 and 4 will never overlap each other, but points 1,2,3, or 2,3,4 can overlap, and the frequency changes of overlapping filters will sum together, though the graphic representation on the screen will only visually show ±6 dB of change. Overlapped points will add together, however the display screen will not show anything past 6 dB of change. In a normal situation, 6 dB of gain or reduction is plenty, but the EQ will boost or cut more than that if two filter points are together.\n9. An audio file with a 96 kHz sample rate is automatically down-converted to a 44.1 kHz sample rate when played from IOS or Android device. Can this be bypassed so that files with a sample rate of 96 kHz files (or even a 192 kHz file) can be listened to with the KSE1500?\nPortable media devices have some limitations built into them. There are music player apps that will allow a media player such as an iPhone to playback at a 96 kHz sample rate, but the standard iTunes software operating system limits this to 44.1 kHz (which is CD quality). Aside from the limitations of the hardware on a music player, there are also software limitations. There are music player applications that allow audio files with a 96 kHz sample rate to be played. On a computer, there are some media player applications that will play high sample rate files. However, Shure has no affiliation with such applications or programs, and do not officially recommend one particular app or program.\nBias Voltage 200 V DC\nOutput Voltage ±200 V, max.\nOutput Current ≤1 mA\nNoise Attenuation ≤37 dB\nOperating Temperature Range 0 to 45 °C (32 to 113 °F)\nStorage Temperature -18 to 57 °C (0 to 135 °F)\nTransducer Type Electrostatic\nConnector Type LEMO Connector\nFrequency Response 10 Hz to 50 kHz\n1 kHz at 3% THD 113 dB SPL\nNet Weight 44.0 g (1.55 oz.)\nBit Depth 16-bit / 24-bit\nSampling Rate 44.1 / 48 / 88.2 / 96 kHz\nSignal-to-Noise Ratio up to 107 dB A-weighted\nAdjustable Gain Range -40 dB to +60 dB\nLimiter Selectable Analog RMS Limiter\nEqualizer 4-band Parametric\nUSB Input USB Micro-B Receptacle\nLine-In Input 3.5 mm (1/8"")\nCharging Requirements USB-powered: 5 V/0.5 A to 1 A\nHousing Black Anodized Aluminum\nNet Weight 182.0 g (6.42oz.)\nDimensions 111 × 59 × 21 mm H × W × D\nBattery Type Rechargeable Li-Ion\nNominal Voltage 3.6 V DC\nBattery Life Analog in (BYPASS EQ mode) up to 10 hours\nUSB Input Analog in (EQ mode) up to 7 hours\nSummary of Customer Ratings & Reviews\nFor clean un-molested sound the KSE1500 is in a class all by its self. Hear what you were meant to hear form the artist. And not what the headphone company wants you to hear.\nQuestion & Answers\nQuestions on Shure KSE1500 Electrostatic Earphone System\nNo questions asked yet']"	['<urn:uuid:975a3af4-cbfd-4198-a148-477acfc7c9e0>', '<urn:uuid:b13617c6-62e8-489e-9da6-4070b334120b>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	13	102	4297
67	Which volcanic event was bigger: Mount Tambora 1815 or Mount Pinatubo 1991?	Mount Tambora in 1815 had more severe effects, causing a 'Year Without a Summer' in 1816 with temperature drops over 0.5 degrees Celsius, crop failures, and snow in Europe during summer. While Mount Pinatubo in 1991 was the largest eruption of the late 20th century, its documented effects were less extreme, causing a temporary 6mm sea level depression through ocean cooling.	"['Polar Detectives: Using Ice Core Data to Decode Past Climate Mysteries\nStudents investigate the formation, physical and chemical properties of ice cores to determine the cause of a discrepant event.\nScientists analyze ice cores for evidence of climate history.\nThis activity is based on known events: 1783\'s Laki volcano in Iceland, an ""unknown"" in 1810 and the Mount Tambora eruption in 1815.\nCan you imagine a ""Year Without a Summer""? Both historians and scientists have found that in 1816 environmental conditions caused temperatures to drop abnormally, more than half a degree Celsius. This was enough to kill animals and crops in the northern hemisphere, to reduce the amount of sunlight due to persistent fog, and to have ice and snow forming in Europe in July and August. Conditions were so grim that author Mary Shelley holed up inside her holiday hotel and competed with friends to see who could create the scariest story. The result? Frankenstein! Figuring out what caused these events and whether they were a random occurrence is the job of climate scientists. To discover an answer, they begin with a question.\nQuestion: ""What caused the extreme 1816 summer conditions?""\nHypothesis: There may be ice core evidence of something that caused unusual temperatures.\nWhat We Know: Snow crystals form differently under different conditions. In the center of Greenland, snow falls year round and does not melt. Layers get compressed into ice over time.\nTask 1: Using your hand lens, examine the photos in the handout. How are the snowflakes different?\nDendritic flakes, far left, form when air is moist and warm. Colder, drier conditions produce finer, needle-like crystals, such as the third photo from the left in the handout.\nTask 2: Sulfuric volcano emissions can be transported to the poles and fall onto the ice. For this simulation, sugar represents coarser summer snow, salt represents finer, winter snow, pepper represents ash and the cylinder represents the core. Sprinkle a few grains of each substance onto separate squares of black paper and examine them with your lens. What do you observe?\nWorking together, create a model ice core, using a spoon and funnel to manipulate the materials.\n- Layer 1: Form the base of your core by adding summer snow to a thickness that you can clearly see, for example 10 ml.\n- Layer 2: Add winter snow on top of the summer snow. These two layers represent evidence of one year of precipitation.\n- Alternate summer and winter layers, filling the cylinder. Occasionally add evidence of an eruption, thick enough so you can see it when you are finished. Examine the ""core"". What do you observe?\nHow does the thickness of each layer relate to the weather at the time the snow fell? How does it relate to crystal size? In which layer(s) is there evidence of a volcanic event? How many years of accumulation are indicated in your model? Which layer is the oldest?\nWhat We Need to Know: In addition to physical properties we can observe, snow also contains chemical indicators we can\'t see that give clues to the environment when the snow fell.\nDesigning an Investigation: Scientists designed two expeditions to search for evidence of sulfur in ice cores. The first expedition went to Summit Station, Greenland and the second to West Antarctica.\nTask 3: Analyzing Data Review the graph in the handout. How many eruptions were indicated, during which year(s)? Which volcanoes were equitorial (sulfur distributed over both hemispheres)? Which were located in a mid- or high latitude location (deposited near the volcanic site only)?\nDrawing Conclusions: Scientists were able to pinpoint ice cores from 1816 using physical and chemical means. You have examined evidence of volcanic eruptions from the same period. Could ash in the atmosphere have been a major factor, blocking northern hemisphere sunlight and causing a ""Year Without a Summer""?\nLinda M. Morris\nEducation Program Manager\nIce Drilling Program Office\nThayer School of Engineering at Dartmouth\nlinda.m.morris [at] dartmouth.edu\nFor more information visit:\n- 6 activity copies\n- 1 (50 ml) graduated cylinder\n- 1 spoon\n- a small funnel\n- 3 hand lenses\n- 3 squares of black paper\n- ¼ c. sugar\n- 3 T. salt\n- a small amount of pepper', 'As our planet heats up, the pace of sea level rise is expected to quicken, making it harder for cities like Miami to stay above water. But since 1992, scientists have studied Earth’s mean sea level via satellites, and they have watched it rise at a steady 3mm per year — no evidence for acceleration.\nThe smoking caldera of Mount Pinatubo on 22 June 1991. Image: USGS\nNow, after more than 20 years of head-scratching, we finally have an explanation: The Mt Pinatubo volcanic eruption of 1991. The largest eruption of the late 20th century, Mt Pinatubo blew its top less than two years before modern sea level record-keeping began. According to research published today in Scientific Reports, the eruption cooled the oceans enough to briefly depress global sea level, masking the expected acceleration in the record so far.\n“We got a very biased view of sea level rise, based on the happenstance timing of the launch of [the first] altimeter satellites,” lead study author John Fasullo of the US National Center for Atmospheric Research told Gizmodo.\nAccounting for Mt Pinatubo, Fasullo and his co-authors conclude that sea level rise is already escalating today, and will continue to do so in the future.\nEstimated rate of global sea level rise based on the satellite altimeter record, which began in late 1992. Image: Fasullo et al. 2016\nThere’s a finite amount of water on our planet, but it rearranges itself in all sorts of ways when the climate shifts. In warmer periods of Earth’s history, sea level gets higher, due to the combined effect of thermodynamics (hot liquids expand to occupy more space than cold liquids) and an increase in the total amount of water in the ocean as ice sheets melt. From the end of the last ice age to today, global mean sea level has risen approximately 125m.\nPerhaps the most important thing we’ve learned about sea level by studying Earth’s past is that it doesn’t rise linearly — it goes in fits and starts, accelerating dramatically as the ice sheets disintegrate. Whether the pace of modern sea level rise will start to escalate, in keeping with the pattern of the past, has enormous implications for the hundreds of millions of people living along vulnerable coastlines. Acceleration could be the difference between 60cm and 6m of sea level rise by the century’s end.\nA) Changes in the reflectivity of the atmosphere over the tropical oceans following the Mt. Pinatubo eruption (black) caused a sudden drop in global mean sea level (blue). B) Corresponding changes in ocean heat content (red) atmospheric water vapour (blue) and water storage on the global land surface (green). Image: Fasullo et al. 2016\nNow, we have our first firm evidence that the rate of sea level rise is already quickening. Analysing numerous model simulations with and without natural factors that can affect global sea level, Fasullo and his colleagues were able to pick out the signal of the Mt Pintaubu eruption, which occurred on 15 June 1991. They found that aerosols from the eruption blocked enough sunlight to temporarily cool the oceans, causing sea level to fall by about 6mm.\nBy some rotten luck, the eruption took place right before the first altimeter, the TOPEX/Poseidon satellite, was launched into orbit in late 1992. In the early days of monitoring sea level from space, Earth’s oceans were rebounding from the temporary effect of the volcano, causing the rate of sea level rise to be artificially high. “That skewed our impression of acceleration,” Fasullo said.\nAccounting for the depression and sudden rebound of sea levels due to the eruption, Fasullo and his colleagues determined that the expected warming-induced acceleration is already under way. “In the next five to 10 years, we should see a clear acceleration emerge from the record,” he said.\nFasullo was reluctant to estimate to how quickly sea level rise will accelerate, or what the total damage will be the end of the century. The Intergovernmental Panel on Climate Change estimates about 1m of sea level rise by the century’s end, while the National Oceanic and Atmospheric Administration predicts 1.2m to 2m. Other scientists have considered the possibility of rapid ice sheet disintegration and come to much higher estimates.\n“The science just isn’t there yet,” Fasullo said. “There’s a wealth of research going on, trying to quantify the contribution of ice sheet [melting] to acceleration. That’s where the main uncertainty lies.”']"	['<urn:uuid:5c5706b1-47bf-4a04-8487-64923abd63bb>', '<urn:uuid:a443db77-b57b-43a4-948e-fcb908fe20bc>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T04:43:46.574121	12	61	1440
68	spinosad insecticide moth control vs leaf miner effectiveness	Spinosad insecticide is effective against both codling moths and leaf miners. For codling moths, when combined with Ampersand adjuvant, it reduces moth entries by more than 60% compared to spinosad alone. For leaf miners, spinosad stops them from feeding within 24-48 hours of ingestion, leading to their death, and needs to be applied 2-3 times per growing season for full control.	['Dr. Alan Knight, Instar Biologicals and formerly of USDA, conducted a series of field trials in Wapato, Washington and reports that the addition of the adjuvant Ampersand enhanced the performance of three codling moth control products. Specifically, Ampersand improved the performance of codling moth granulosis virus, spinosad insecticide, and a micro-encapsulated sprayable codling moth pheromone.\nGranulosis Virus + Ampersand:*\nReduction in damage\nReduction in stings\nReduction in entries\nSpinosad Insecticide + Ampersand:*\nReduction in entries\nSprayable Mating Disruption Pheromone + Ampersand:*\nReduced captures of male moths\n*Compared to active alone\nCodling Moth Granulosis Virus\nCodling moth granulosis virus is often used for early season codling moth control as it only has activity against codling moth. As such, codling moth granulosis virus has no adverse impact on beneficial insects that can assist in reducing codling moth populations. Dr. Knight compared codling moth control with codling moth granulosis virus applied alone, at 6 oz/100 gallons or mixed with Ampersand at 0.125% (1 pint/100 gallons). Four applications were made at weekly intervals. Each treatment was applied to 5 trees which were randomly selected in the orchard. To evaluate performance, 50 apples were randomly collected from each tree and examined for codling moth damage.\nThe graph below shows the results of the trial in terms of total number of damaged fruit, number of fruit with stings only (apples damaged by codling moth feeding but where there was no entry into the apple) and entries (codling moth caterpillars have entered into the fruit). Ampersand substantially improved the performance of the granulosis virus across all evaluation parameters. Ampersand reduced damaged fruit by more than 44% compared to the virus alone, reduced stings by 42% and reduced entries by 49%.\nSpinosad insecticide is one of the few OMRI listed products available to apple producers for control of codling moth. Dr. Knight used the same trial design as described above for a study to evaluate Ampersand’s impact on codling moth control with spinosad insecticide. Spinosad insecticide was applied at 6.5 fl oz/100 gallons alone, tank mixed with Hi-Y Supreme Oil at 1% and tank mixed with Ampersand at 0.125% (1 pint/100 gallons).\nThe results of the trial are shown in the graph below. Dr. Knight reports that Ampersand substantially reduced the number of codling moth entries compared to Spinosad insecticide alone or tank mixed with Hi-Y Supreme Oil. Ampersand reduced the number of codling moth entries by more than 60% compared to Spinosad alone or tank mixed with Hi-Y Supreme Oil.\nSprayable Codling Moth Mating Disruption Pheromone\nFemale codling moths emit a sex pheromone that attracts males and allows them to locate the female for the purpose of mating. Mating disruption is a widely used technique for reducing codling moth populations whereby the apple orchard is blanketed with the codling moth sex pheromone so that the male moth cannot find a female and consequently eggs are not produced. There are a number of systems used to dispense the pheromone but most have to be manually installed throughout the orchard and these devices must be refilled with the pheromone or replaced. While effective, these devices are labor intensive and can be substantially more expensive than chemical insecticides. Increasingly, a micro-encapsulated mating disruption product has been used which is sprayed onto trees, improving distribution of the sex pheromone in the orchard and substantially reducing labor costs. Dr. Knight evaluated the impact of the tank mix addition of either a spreader/sticker adjuvant applied at 0.125% (1 pint/100 gallons) or Ampersand at 0.125% (1 pint/100 gallons) with 100 ml of micro-encapsulated codling moth sex pheromone per 100 gallons of spray. Trees were sprayed a single time until runoff. Treatments were sprayed on plots that were made up of 25 trees. There were 3 plots used for each treatment. The effectiveness of the treatments was measured by counting the number of moths that were captured in moth traps baited with the sex pheromone. Moth counts were made once a week for four weeks.\n“My studies support that Ampersand can improve the performance of three important codling moth control products.”\nDr. Alan Knight\nSenior Researcher at Instar Biologicals and former USDA researcher\nThe results of the study are shown in the graph below. When Ampersand was mixed with the micro-encapsulated pheromone, the number of male moths captured was reduced by more than 75% compared to the untreated control. Ampersand improved the performance of the micro-encapsulated sex pheromone, as there were more than 40% fewer male moths in the Ampersand treated plots than in the plots treated with the micro-encapsulated sex pheromone alone. Ampersand also outperformed the micro-encapsulated sex pheromone when tank mixed with the latex spreader/sticker. Ampersand treated plots had 55% fewer male moths than the plots treated with the micro-encapsulated pheromone tank mixed with the spreader/sticker.', 'So you’ve spotted some maze-like squiggly lines in your precious spinach leaves. Or maybe your crops are covered in annoying flies. Bad news: you could have an infestation of leaf miners.\nLeaf miner infestations can happen in greenhouses, veggie gardens, and ornamental areas around your property. On top of that, they can show up in almost any region of the U.S. In other words – no one is safe.\nThe damage caused by leaf miners isn’t just unsightly, it can stunt or kill plants, so you need to understand how to treat these pests. Ready to learn how? Keep reading!\nWhat Are Leaf miners?\nThe term leaf miners is a catch-all that describes the larvae of three insect species: Lepidoptera, Diptera, and Hymenoptera. These larvae live inside plant leaves, feeding and growing until they reach maturity.\nBasically, leaf miners are the larval – or maggot – stage of several insect families.\nThe first thing you might notice is distinct, discolored lines on foliage since these pests live inside leaves. That pattern of lines is a feeding tunnel created by the leaf miners as they chew through plants.\nIf you pay close attention to the patterns, you can identify the specific leaf miner variety that’s attacking your plants. You can also narrow down the leaf miner type by watching which plants are targeted.\nIdentification of Leaf miners\nHost plants for leaf miners can include beans, blackberries, tomatoes, cucumbers, potatoes, lettuce, cabbage, peppers, citrus trees, aspens trees, shrubs, and a variety of ornamental flowers. As you can tell, they don’t pick one particular type of plant, which makes it harder to track these insects.\nThe larvae look like worm-like maggots that can be pale yellow, dark brown, or light green.\nAdults measure 1/10 inch long and are typically black or grey flies with yellow stripes and transparent wings. They look similar to small house flies. They lay their eggs on the underside of leaves.\nThere are two common types of leaf miners, though keep in mind that there are several other varities out there:\nSpinach Leaf miners\nThis species is a type of blotch leaf miner that creates irregular round-shaped mines. The mines are long and narrow at first, then become an irregular shaped patch.\nThere are a few ways to pinpoint spinach leaf miner larvae:\n- They don’t have legs or head.\n- The larvae are whitish and look like little carrots.\n- They create tunnels between the two leaf surfaces.\n- Once matured, the adult flies are hairy, measure ¼ inch long, and are between grey and brown.\nVegetable Leaf miners\nVegetable leaf miners feed on different plants than spinach leaf miners. This variety prefers beans, eggplants, squash, tomatoes, cucumbers, peas, and other edible plants.\nAside from looking at the plants they infect, here are some different ways to identify this species.\n- The larvae look like snakes that leave behind winding mines.\n- The larvae don’t have legs or a head and are yellow-green with a cylindrical shape.\n- This species is smaller than spinach leaf miners, only measuring around 1/15 inch in length. The adult flies are yellow and black.\nThe Life Cycle of Leaf miners\nUnderstanding the lifecycle of leaf miners is essential to controlling them. It all starts when mature larvae overwinter in the soil under the plants. Then, as the spring temperatures warm up the ground, the larvae mature to their pupal stage. By late April, they’re young adults.\nAt that point. they’re ready to start laying eggs in your garden. Mated females use a needle-like ovipositor to lay up to 250 eggs under the surface of the leaf epidermis. It can be hard to spot the eggs under the surface; they appear as small raised bumps in the leaf.\nThe eggs hatch within ten days, and the larvae start to eat their way through the leaf tissue. This is when they leave behind the wavy lines that are visible on the surface.\nLarvae take only 2-3 weeks to mature. When they’re ready to pupate, they ditch the leaf and drop to the soil, digging 1-2 inches into the ground. Then, 15 days later, they emerge as an adult fly.\nYou can have several generations of leaf miners in one single year, so it’s clear how quickly a leaf miner infestation can start.\nHow to Identify a Leaf miner Infestation\nIt’s fairly easy to identify the damage caused by a leaf miner infestation because these pests feed on parts of the plants with tissue containing the lowest levels of cellulose and tannins.\nThey leave behind a distinct trail of maze-lime damage on plants. Typically, the damage is only cosmetic, and the plants continue to live a healthy life.\nThat’s not to say that leaf miners can’t kill a plant; they absolutely can! Unchecked damage can cause excessive leaf drop and other severe effects. The feeding tunnels created on the foliage can become pathways for diseases or fungal spores.\nHow to Prevent a Leaf Miner Infestation\nAs with any pest, prevention is the key to success. It’s easier to prevent an infestation rather than to stop one. Your best bet is to use multiple methods to avoid leaf miners from invading your garden.\nHere are some suggestions.\nTill Your Soil in the Fall\nLeaf miners like to hide in the soil during winter and emerge as the temperatures warm. Till your garden after harvesting your crops in the fall to destroy the pupae and reduce the chance that adult flies will invade nearby plants.\nPlant Trap Crops\nPlanting trap crops is a sneaky-yet-genius way to attract pests to crops you don’t mind sacrificing in order to save the ones you want. Leaf miners particularly enjoy lamb’s quarters, columbine, and velvetleaf. Plant these to prevent leaf miners from bothering plants that you want to keep whole and healthy.\nMonitor Your Garden Closely\nMonitor your garden closely and watch for any evidence that leaf miners are trying to make it their home. Catching issues early lets you take care of them faster and with less effort.\nKeep Your Plants Healthy\nIf you have healthy plants, leaf miners won’t harm them as much. Use organic fertilizers and adequate watering techniques to ensure your garden is as healthy as possible. Add compost and other soil amendments to create a solid foundation for your plants.\nTry Floating Row Covers\nFloating row covers can help stop the adult flies from laying their eggs on the leaves. Floating covers are inexpensive and easy to install over your garden beds.\nHang Sticky Traps\nOne simple trick that you can try is hanging sticky traps in your garden. The traps attract and trap the adult pests before they can lay their eggs.\nHow to Stop a Leaf Miner Infestation\nSo your prevention methods failed, or you never had the chance to stop leaf miners from invading. Now you have leaf miners making a meal out of the foliage in your garden.\nWhat can you do to stop leaf miners in your garden beds?\nManually Remove Eggs\nIf you’re lucky enough to spot the eggs, manually remove them from your garden. The more eggs that you remove, the fewer pests you’ll have to deal with when they hatch.\nIntroduce Diglyphus Isaea\nDiglyphus isaea is a beneficial wasp that parasitizes leaf miner larvae. It kills them before they can mature. D. isaea is the natural enemy of leaf miners, and they’ll make a meal of these pests in your garden.\nTo reap all of the benefits of this wasp, you need to release them early in the season before the leaf miner population reaches large numbers.\nBe aware that if you spray pesticides in your garden, it will kill these beneficial bugs.\nBreak Out the Spinosad Pesticide\nNotice large numbers of leaf miners destroying your foliage? It’s time to take action, and only a few sprays work well against these pests.\nI’ve found that Spinosad works well in the fight against leaf miner infestations. You can apply it to all plant surfaces. Once these pests ingest the pesticide, it stops them from feeding and they die in 24-48 hours. You need to apply it 2-3 times per growing season to keep the population at bay entirely.\nSpray Neem Oil\nI’m a huge fan of neem oil; I’ve used it effectively against many different pests in my garden beds.\nNeem oil can be applied as a spray (either pre-diluted or concentrated) to stop the growth and development of pests. It acts as a repellent and has antifeedant properties.\nYou can use neem oil, which contains azadirachtin, to eradicate a population of leaf miners.\nThe reason I use neem oil is that it’s non-toxic to honey bees and other beneficial insects. You need these in your garden for proper pollination and using traditional pesticides can kill them as well.\nKicking a Leaf Miner Infestation\nThe key to getting rid of a leaf miner infestation is to prevent it before it starts, but gardening doesn’t always work that way. Instead, if you find yourself with leaf miners invading your garden, opt for natural methods such as introducing beneficial insects and use neem oil to eradicate them.']	['<urn:uuid:4af1252c-7447-43d0-9509-f3cb2be5807b>', '<urn:uuid:9b5eb9df-11f7-4f46-9ebc-d61c8da00a1f>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	8	61	2319
69	khrushchev warsaw poland crisis 1956	In October 1956, there was a crisis between the Soviet Union and Poland when Khrushchev, worried about a possible Polish breakaway, demanded to be invited to Warsaw. When refused, he flew there anyway with four top Kremlin figures and 12 generals. Upon landing, he berated the Polish leadership loudly. While Soviet troops moved toward Warsaw, the Polish leader Gomulka managed to calm Khrushchev, who then flew back to Moscow. However, Khrushchev changed his mind twice more, first ordering the army to occupy Warsaw, then canceling that order.	"['Khrushchev: The Man and His Era\nBy William Taubman\nNorton, 876 pages, $35\nOur baby, like most, looked a lot like former Soviet leader Nikita Khrushchev: bald head, chubby jowls. Happy, his grin was guileless; displeased, he roared and bullied. When we lived in Moscow we used the baby as a prop to thaw out icy bureaucrats. We dandled him and called him Nikita Sergeyevich. The bureaucrats thawed, for the baby\'s sake, but they seemed to feel that the joke was in poor taste.\nFOR THE RECORD - This story contains corrected material, published April 13, 2003.\nBy then, a decade or so after his fall from power, Khrushchev was an unperson, written out of official histories and textbooks. Russians remembered him mostly with shame. They said, ""He was so crude.""\nCrude he was. He derided his Politburo colleagues as ""\'male dogs peeing on curbstones\'"" (this sentence as published has been corrected in this text). He analyzed sculptor Ernst Neizvestny\'s artistic view as looking up from inside a toilet (in somewhat more colorful terms). Then-Sen. Hubert Humphrey reported after meeting him that Khrushchev\'s two favorite words were ""stupid"" and ""fool.""\nBut he also liked people, and they liked him. He loved to talk; he was full of jokes and earthy proverbs. According to ""Khrushchev: The Man and His Era,"" William Taubman\'s splendid new biography:\n""He conceived himself to be, and compared with his Kremlin cronies, he actually was, a nice guy. There seems no better way to say it.""\nA nice guy who participated with apparent enthusiasm in the bloody purges of his communist colleagues in the 1930s. A nice guy who threatened to unleash nuclear war. A nice guy who, as an older generation of Russians and Americans remembers, reinforced his bluster by banging his shoe on the table while addressing the United Nations in 1960. For Russians, the shoe-banging incident helped to define his crudeness, and himself as an embarrassment to a Soviet people that longed for the world\'s respect. It was cited against him four years later when his Politburo colleagues conspired to depose him.\nBut did he really remove his shoe and bang it on the table? That\'s the way it is remembered, but Taubman has found witnesses who say he removed the shoe and waved it but did not bang it, and another who says he did not remove the shoe at all but had it slip off when a journalist accidentally stepped on his foot. This witness says she handed the shoe back to Khrushchev, who indeed pounded the table with it.\nUnable to resolve the conflicting accounts, Taubman had to make up his mind. ""I have adopted the view,"" he writes, ""that the shoe was not only brandished but banged.""\nNote the conflicting testimony on a highly dramatic incident that happened in full view of the world. Now imagine Taubman\'s 15-year labors in trying to reconstruct the truth about decades-old Politburo intrigues or foreign-policy crises, informed only by incomplete or tendentious contemporary records and the slippery or self-serving memories of participants and their confidants.\nTaubman\'s biography of Khrushchev benefits from the dissolution of the Soviet Union in 1991. In addition to newly opened archives, he has been able to interview Khrushchev\'s children and grandchildren, cooks and drivers, former Kremlin power brokers and their aides. After so many years of enforced silence about an unperson, Taubman\'s sources were eager to talk, but often their accounts could not be reconciled. Khrushchev\'s own voluminous memoirs, dictated from memory, both revealed and obscured. They were, Taubman writes, a ""stunning blend of deception and self-deception."" Time and again, Taubman had to make up his mind. He explains his sources and the reasons for his choices in 134 pages of notes that follow his 648-page text.\nBut if the multiple perspectives make facts difficult to nail down, they make for a fascinating narrative. Taubman\'s account of a Soviet-Polish crisis in October 1956, which almost led to a Soviet invasion of its communist ally, takes up only a taut page-and-a-half. Worried about the possibility of a Polish breakaway, Khrushchev demands to be invited to Warsaw. The Poles refuse, so Khrushchev assembles four top Kremlin figures and 12 uniformed generals and flies there anyway. As soon as his plane touches down, he starts berating the Polish leadership in terms that "" \'even the chauffeurs\' "" can hear. The Soviet army starts to move toward Warsaw. The new Polish leader, Wladyslaw Gomulka, ""so tense . . . that \'foam appeared on his lips,\' "" manages to calm Khrushchev, who halts the troop movements and flies back to Moscow--only to change his mind twice more in the next two days, first ordering the army to occupy Warsaw after all, then rescinding the order.\nBy my count, nine sources provided details for this story--four Soviet, four Polish and one Czech. Such set pieces occur throughout the book, reconstructing Politburo meetings, confrontations with artists and writers, the origins of the 1962 Cuban missile crisis, family vacations by the Black Sea and other events great and small.']"	['<urn:uuid:a8f33a5f-d255-4e7f-a767-3e1a8b110295>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	5	87	841
70	best drinks for staying hydrated during different types sports activities guidelines	For hydration during sports, water is appropriate for mild to moderate intensity exercise lasting less than 60 minutes, while sports drinks are preferred for high-intensity exercise lasting more than 60 minutes. Soft drinks and fruit juices should be avoided. For proper hydration, drink 16 ounces of fluid 2-3 hours before competition and 8 ounces 10-20 minutes before the event. Continue drinking during competition and replace all sweat loss afterward.	['By Katie Wilhelmi\nThe fall sports season is here. Eating a well-balanced diet is one of the most important things you can do to promote optimal performance. Eating a variety of high-carbohydrate, low-fat foods and consuming enough fluids to avoid dehydration are key. To increase endurance, muscle strength and speed, athletes must eat to compete!\nFuel Up with\nFuel up with familiar foods on competition day and allow adequate time for that food to digest. A large meal can take three to four hours to digest, a small meal can take two to three hours, and a snack will digest in one to two hours. A substantial pre-event meal will help prevent fatigue and ensure you have the fuel stores needed to power your way to peak performance. Include complex carbohydrates, lean protein, fruit and healthy fats. About two-thirds of your plate should be carbohydrates. Top off fuel stores with a carbohydrate-based snack one to two hours before competition.\nDuring the Event\nContinue to refuel during competition, as needed, with carbohydrates, electrolytes and fluid to prevent fatigue and prevent depletion of fuel stores. Sports drinks, gels and bars are all efficient ways to refuel.\nRefuel to Recover\nRefueling begins immediately after competition with a recovery snack consisting of carbohydrate and protein to refuel stores and repair damaged tissue. Chocolate milk can be a perfect sports recovery drink as it contains the ideal ratio of carbohydrate and protein to refuel muscles. Continue refueling with a meal one hour after the recovery snack.\nStay Hydrated in the Heat\nMaintaining adequate hydration can be a challenge. Dehydration can severely impair athletic performance, making it crucial to go into a competition well-hydrated. Consume fluids throughout competition day, then hydrate two to three hours prior to competition with 16 ounces of fluid and again 10 to 20 minutes before event with 8 ounces of fluid. Fluid should be consumed during competition and fluid replacement of all sweat loss should take place following competition. Choosing the right fluid for hydration is important. Water is appropriate for mild to moderate intensity exercise lasting less than 60 minutes. Sports drinks are preferred for high-intensity exercise lasting more than 60 minutes. Soft drinks and fruit juices are best avoided.\nHigh-Performance Snack Ideas:\n-Oatmeal with almonds, berries and low-fat yogurt\n- Peanut Butter and Jelly Sandwich and a glass of low-fat milk\n- Trail mix made with nuts, whole grain cereal and dried fruit\n- String Cheese and baked whole grain chips\n- Smoothie made with low-fat milk, yogurt and frozen berries\n- High-protein energy bar and a piece of fruit\nUse this recipe to make your own no-bake, high protein energy bars at home:\nAll you need:\n2 c. grains (old-fashion oats, quick oats, granola)\nc. sweetener (honey, agave nectar, brown rice syrup, pure maple syrup)\n1 c. nut butter (natural peanut butter, almond butter, cashew butter, sunflower nut butter)\n1 c. seeds or wheat product (ground flax seed, whole sesame seeds, unprocessed wheat bran, wheat germ, pumpkin seeds, chia seeds c. only)\n1/2 c. powdered protein (vanilla whey protein powder, nonfat dry milk)\n1 c. nuts or chips (chopped walnuts, almonds, cashews, sunflower nuts, dark chocolate chips)\n1 c. dried fruit (raisins, dried cherries, dried cranberries, chopped dried apricots, chopped dates)\nAll you do:\n1. Choose one ingredient from each of the seven categories, to custom-make a nutrition bar that suits your taste.\n2. Mix all ingredients together using an electric mixer or food processor.\n3. Press evenly into an 8- or 9 inch pan sprayed with nonstick cooking spray. Refrigerate to firm bars.\nThis information is not intended for medical advice. Please consult a medical professional for individual advice.\nKatie Wilhelmi, RD, LD is a registered dietitian at the New Ulm Hy-Vee Food Store.']	['<urn:uuid:175c236e-9753-42de-b225-5989b2301256>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	11	69	627
71	farm waste gas production challenges solutions	There are several challenges and solutions in farm waste gas production. The main challenges include unstable biogas facility processes, insufficient trace elements preventing microorganisms from decomposing biomass into methane, and incomplete protein breakdown in digestion chambers. Additionally, manure collected from open-lot pens has significantly lower organic matter content (26%) compared to confined cattle manure (88%), reducing methane production. Solutions include adding proper amounts of trace elements for optimal microorganism function, implementing post-digestion treatment of residue, and developing new technologies to enhance open lot manure use in anaerobic digestion. Research also shows that dietary manipulation of cattle can influence methane production from manure.	"[""A research group from the Biogas Research Center (BRC) – which includes researchers from Linköping University, the Swedish University of Agricultural Sciences, the regional waste and energy company Tekniska verken i Linköping and other actors in the biogas sector – have analysed how biogas facilities can maximise the decomposition of organic waste so as to extract as much biogas as possible. Their article was recently published in the Journal of Biotechnology for Biofuels and Bioproducts.\nThe work has been ongoing since 2016, and the original plan was to publish two or three articles based on the material collected. However the authors have now published what they call a “monster article”, where all the collected material is presented in a single study.\n“Everything is interconnected and interdependent in such a system. We have looked at nine large facilities in order to cover the decomposition of agricultural residue, food waste and sludge, and have investigated every aspect that we've thought of that can affect the digestion residue”, says Eva-Maria Ekstrand, formerly postdoctoral researcher at Linköping University and now process consultant at AFRY.\nThe group has collected data on what goes into, what is inside of, and what comes out of the biogas reactor. This data has then been analysed to determine which factors can affect how the various parts of the organic waste are decomposed, and what remains in the waste that is left, i.e. the digestion residue. This helps them understand why some parts of the organic waste are not decomposed, and what can be done to extract more gas.\n“We were processing enormous volumes of data”, says Jan Moestedt, R&D engineer at Tekniska verken i Linköping, and continues: “We studied the ‘goo’ that comes out. If there is organic material left, it's obvious that it hasn't turned into gas, and so there is room for improvement. We also looked at how we can squeeze out the last bit from the ‘dishrag’, and correlate this to the question of why is it left, perhaps it is because of these things?”\nThe results showed that there are two main reasons why the digestion chamber could not decompose all the organic material, and create gas. One reason was that the biogas facility had an unstable process, or that too little of the trace elements was added. Trace elements are needed for the microorganisms to function optimally. With insufficient trace elements, the microorganisms are not able to decompose the biomass to methane.\nIt also emerged that protein often remained, regardless of which facility they looked at. One reason for this is the difficulties in decomposing certain proteins, such as keratin. Another reason is that microorganisms inside the digestion chamber grow and multiply as they decompose the organic waste. This creates a new type of biomass which contains protein that has not been decomposed. This is a problem today, but it's also an important discovery that can contribute to an improved biogas system.\n“Proteins contain amino acids which consist of ammonium, among other things. If we can achieve a better decomposition of our protein, we will increase the ammonium content of our fertiliser. And what do the farmers want, what do they pay for? The answer is ammonium, the nutrient in the fertiliser for their plants. So it goes hand in hand: with more efficient decomposition, which means less protein going out and more gas going out from the material, we will also get a better quality fertiliser”, says Jan Moestedt.\n“The reason why it's bad to have a lot of organic material left in the digestion residue is that when it goes into the fields, methane can form. It's the same for those who store their digestion residue in an open fertiliser tank. They can end up with loads of methane if they have a low rate of decomposition in the reactor”, says Eva Maria Ekstrand.\nThe solution, according to the researchers, is after-treating the digestion residue, or what is called post-digestion – a solution not yet established internationally but which research recently published by the Biogas Research Center supports.\n“Whichever conference you go to, there's a lot of talk about pre-treatment to get out more methane, but we think it's more efficient to let the reactor decompose everything that is already easy to break down, and then devote energy and money to the difficult part – what's left. Then we can get to that microbial biomass that has formed in the digestion chamber. This is something the BRC continues to study”, says Eva-Maria Ekstrand.\nTranslated by Martin Mirko"", 'The concept of utilizing feedlot manure in an anaerobic digester to power an ethanol plant, which then produces feed for cattle, has been called a closed loop system. In this system inputs are minimized and outputs are used by another component. This research looked at differences in manure quality within this system. Trial 1 considered incorporating distillers grains into the cattle diet and the effects on methane potential of the manure. For this system to be utilized by the feedlot industry in Nebraska, the manure collected for anaerobic digestion must be collected from soil-based open feedlot pens which account for over 95% of the feedlot cattle raised in Nebraska. Trial 2 addressed the methane potential of open-lot feedlot manure and its feasibility for anaerobic digestion.\nAn integrated biorefinery utilizes distillers grains for cattle feed and cattle manure for biogas generation to power an ethanol plant. This system has been referred to as a “closed loop” system due to energy recycling within the segments.\nWhat Did We Do?\nSeven continuously stirred anaerobic digesters were used to compare degradation of manure from 2 cattle diets (Trial 1) and 2 cattle housing methods (Trial 2). In Trial 1 manure was collected from confinement cattle on a control diet with 82.5% dry rolled corn or 40% of the corn was replaced with wet distillers grains plus solubles (WDGS), a byproduct of the ethanol industry. For Trial 2, manure was collected from cattle in complete confinement or soil-based open feedlot pens with all cattle on a similar byproduct diet. In both trials, organic matter (OM) degradation and methane production was measured for digesters on each treatment. In Trial 1, samples of effluent removed from the digesters were also used to identify differences in microbial community structure (Eubacterial and Archaeal) due to treatment.\nWhat Have We Learned?\nTrial 1. Organic matter degradation was slightly improved for manure from cattle fed WDGS (P = 0.10). Methane production was 0.137 L/g OM fed for WDGS manure and 0.116 L/g OM fed for the corn-based diet (P = 0.05). Microbial communities identified using 454-pyrosequencing revealed structuring of the microbial community based on diet (P < 0.001). This suggests that the microbial food chain that contributes to methane production is greatly influenced by the diet fed to cattle, and dietary manipulation may provide opportunities to increase or decrease methane production from cattle manure.\nTrial 2. Manure collected from open-lot pens had an OM content of 26% compared to 88% for manure from complete confinement. This resulted in decreased methane production and OM degradation (P < 0.01) for digesters fed open-lot manure. However, methane was produced from open-lot manure suggesting that if ash buildup can be avoided open lot manure may be a viable feedstock for anaerobic digestion.\nWe are currently exploring new technologies that may enhance the use of open lot manure in anaerobic digestion. We are also identifying key microbes involved in methane production in order to better understand how things such as cattle diet affect methane production.\nAndrea Watson, graduate student, University of Nebraska firstname.lastname@example.org\nSamodha Fernando, assistant professor, University of Nebraska\nGalen Erickson, professor, University of Nebraska\nTerry Klopfenstein, professor, University of Nebraska\nA summary of these trials is available at beef.unl.edu/reports; 2013 Beef Report pg. 98-99.\nFunding provided by Nebraska Center for Energy Sciences Research\nThe authors are solely responsible for the content of these proceedings. The technical information does not necessarily reflect the official position of the sponsoring agencies or institutions represented by planning committee members, and inclusion and distribution herein does not constitute an endorsement of views expressed by the same. Printed materials included herein are not refereed publications. Citations should appear as follows. EXAMPLE: Authors. 2013. Title of presentation. Waste to Worth: Spreading Science and Solutions. Denver, CO. April 1-5, 2013. URL of this page. Accessed on: today’s date.']"	['<urn:uuid:9097aed5-f8fb-440c-a768-a50cffec129e>', '<urn:uuid:9415284e-0023-4901-bee8-9ff36e41a3f1>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	6	102	1391
72	How did the WPA help abstract artists?	The Works Progress Administration (WPA) provided employment to abstract artists through their mural program, which was the first major government-funded public art initiative. AAA founders Burgoyne Diller and Louis Schanker, who were WPA administrators, ensured that many struggling abstract artists found paying jobs painting public murals.	"[""American Abstract Expressionist artists\nAmerican Abstract Artists vs. MoMA\nMany years ahead of 1936, a small number of just what would in the course of time come to be AAA’s creators had been regularly fulfilling during the residence of abstract sculptor Ibram Lassaw. The group was consists of artists who made what founding AAA user Esphyr Slobodkina labeled as “the non-objective type of art.” They found to share their work and the philosophies behind it, and discuss the problems they certainly were having because abstract music artists breaking in to the United states main-stream awareness. In March of 1936, the MoMA presented its very first major exhibition of abstract Art. The tv show consumed four flooring associated with the museum. One of the 400 artworks within the show almost all were made by European music artists. A-year earlier, the Whitney had held an exhibition of American abstractionists. The MoMA cited that show as a defense of these option not to integrate Americans in their own exhibition. The music artists who had previously been fulfilling at Lassaw’s house took offense into the slight, and officially formed the AAA.\nEsphyr Slobodkina - Tubroprop Skyshark, 1950, Oil on masonite, 16 3/4 x 20 3/4 in.\nShow and Tell\nThe targets of this AAA had been two: first, they desired to offer United states abstract designers opportunities to display their particular work to the general public; second, they wanted to develop a theoretical basis for quality of abstract art that could resonate with American critics while the general public. Initial exhibition held by AAA was in 1937. It was typically derided by experts, but more than 1500 folks stumbled on the show, showing powerful, latent community interest in the task. The team presented seven events in 1938: three in ny, plus a traveling convention that went to Seattle, bay area, Kansas City, MO, and Milwaukee. At the same time, the people shared their particular reasoning through lectures, panels and by publishing their writings. Exactly what started as casual conversations in those early conferences at Lassaw’s home led to numerous succinct statements that expressed the enduring viewpoint and principals of American abstract designers. Wrote Ibram Lassaw in 1938, “The singer not seems that he's ‘representing truth, ’ he is really making truth. The Truth Is some thing stranger and greater than simply photographic rendering can show.”\nIbram Lassaw - Coma Berenice, 1952, Bronze, 65 x 75 x 40 cm.\nIn addition to Ibram Lassaw plus the fore mentioned Esphyr Slobodkina, who was simply an important illustrator, painter and instructor, the AAA’s creators included many that have come to be named important American abstract music artists. Among them: Bauhaus alumni, “Homage to your Square” painter, and instructor of Robert Rauschenberg, Josef Albers; Abstract Expressionist painter John Opper; painter, teacher and one-time Director regarding the Functions Progress Administration (WPA) Burgoyne Diller; abstract painter, instructor, and celebrity pupil of Hans Hofmann, Rosalind Bengelsdorf; and art blogger, illustrator and painter Ilya Bolotowsky. As Director for the WPA, Burgoyne Diller played a really crucial role when you look at the success of a number of musicians and artists in AAA. The WPA ended up being a Depression-era, national New contract program made to offer employment to an incredible number of unemployed American laborers. Their particular mural program ended up being the initial significant attempt by the US federal government to finance the development of public artwork. Diller and other AAA founder Louis Schanker, who had been in addition an administrator aided by the WPA, made certain numerous fighting abstract designers discovered having to pay jobs using WPA painting public murals.""]"	['<urn:uuid:d9084484-b170-4be0-b8b1-5886d8276268>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	7	46	600
73	Does green bean casserole or apple butter take longer to prepare?	Green bean casserole takes approximately 45 minutes total (20 minutes baking plus prep time), while apple butter takes 6 hours 30 minutes total (including 5 hours cooking and 1 hour cooling time).	"['Christmas is just around the corner and here we are again, trying to decide what we should cook for the holidays. Not only that, but we know that the tendency to overeat is also just around the corner and we want to avoid that as much as possible. To lessen the guilt, it’s better to opt for recipes that are on the healthier side. So, I combined some of these holiday recipes that you might like.\nGreen Bean Casserole\n1 small onion, thinly sliced\n1 tbsp. extra-virgin olive oil\nFreshly ground black pepper\n3 tbsp. all-purpose flour\n2 tbsp. Panko breadcrumbs\n1 lb. green beans, trimmed\n6 tbsp. extra-virgin olive oil, divided\n2 medium shallots, finely chopped\n8 oz. sliced mushrooms\nFreshly ground black pepper\n2 cloves garlic, minced\n1/4 c. all-purpose flour\n3 c. almond milk\n- Make onion topping: Preheat oven to broil on medium and line a medium baking sheet with aluminum foil. In a medium bowl, toss onion with olive oil and season with salt and pepper. Add flour and Panko and toss to coat onions. Broil, tossing every 2 to 3 minutes, until onions are crisp and golden, about 6 to 8 minutes in total. Turn oven down to 375°.\n- Prepare an ice bath. In a large pot of boiling water, add green beans and cook until bright green, about 6 minutes. With a slotted spoon or tongs, quickly transfer green beans to ice bath to cool, then drain and transfer to a large bowl.\n- In a large ovenproof skillet over medium heat, heat 2 tablespoons olive oil. Add shallots and cook, stirring occasionally until tender, about 5 minutes. Add mushrooms and season with salt and pepper. Cook, stirring often, until mushrooms are golden, about 5 minutes more. Stir in garlic then transfer mixture to the bowl with the green beans.\n- Heat remaining 4 tablespoons of olive oil in the same skillet over medium heat. Whisk in flour and cook until golden, about 2 minutes. Gradually whisk in almond milk and bring to a simmer. Cook until thickened, about 4 minutes. Remove from heat then add green bean mixture and toss until evenly combined. Transfer mixture to a medium casserole dish.\n- Bake until warmed through and bubbling around the edges, about 20 minutes. Top with “fried” onions and bake 5 minutes more.\nVegan Mashed Potatoes\n3 lb. gold creamer potatoes halved\nKosher salt 1/3 c.\nExtra-virgin olive oil\n3 garlic cloves, minced\n2 sprigs fresh rosemary, plus more for garnish\n6 tbsp. vegan butter\nFreshly ground black pepper\nFreshly chopped parsley, for serving.\n- In a large pot, cover potatoes with water and season with salt.\n- Bring to a boil and cook until totally soft, 15 to 18 minutes. Drain and return to pot. Use a potato masher to mash potatoes until smooth.\n- Meanwhile, in a small saucepan over medium heat, heat oil and rosemary then add garlic and cook until fragrant, 1 minute. Discard rosemary and pour oil over potatoes.\n- Add butter and stir until completely combined and creamy.\n- Season with salt and pepper.\n- Transfer potatoes to a serving bowl and season with more pepper and garnish with rosemary and parsley.\nSpiced Hot Fruit Bake\n2 cup sliced apples\n2 cups pear slices\n1 1/2 cup fresh cranberries\n1 cup pineapple chunks (save the juice)\n1 tbsp lemon juice\n1/3 cup coconut palm sugar (unrefined) or brown sugar\n1 tbsp maple syrup, agave, or honey\n1 tsp cinnamon (extra for topping)\n1/4 tsp nutmeg\n1/2 stick melted butter (4 to 5 tbsp melted vegan butter can be substituted)\nOptional – An additional 2 teaspoons melted coconut oil or butter to coat walnuts\n1/3 cup chopped raw walnuts or pecans\nextra cinnamon for nuts or serving.\n- Preheat oven to 300F.\n- In a large bowl, toss your fruit and add in 1-2 tsp lemon juice. Set aside.\n- In another glass bowl, combine your melted butter, spices, and honey or maple syrup.\n- Mix in a few tablespoons of your leftover pineapple juice as well.\n- Add this sugar/butter mixture to your fruit and coat evenly.\n- Pour fruit evenly in a 9×12 baking dish.\n- Pour the leftover sugar/butter/oil mixture on top.\n- baking for 1 hr.\n- OPTIONAL – Toss the nuts in a tiny bit of melted coconut oil or butter and a pinch of cinnamon. I usually just coat the nuts in the leftover butter/sugar from the fruit mix bowl. Then sprinkle the nuts to the top of the dish and bake all together for 1 hr.\nAvocado-Spelt Filo Pastry:\n250 g whole grain spelt flour\n1 teaspoon sea salt\n90 g avocado flesh about 1 medium, mashed\n90 g very hot water\nMushroom, Spinach & Stilton Fillings:\n1 tablespoon ground flaxseed\n1 tablespoon water\n400 g brown mushrooms or similar halved or quartered\n1 medium red onion diced\n3 cloves garlic crushed\n100 ml drinkable dry white wine\n100 ml water\n1 lightly heaped tablespoon vegetable stock paste\n20 g rolled oats\n25 g walnuts\nfew handfuls fresh herbs roughly chopped\nsea salt to taste\nfreshly ground black pepper to taste\n400 g frozen spinach thawed\n1/2 lemon juice only\n150 g Stilton omit for vegan\n100 g extra virgin olive oil\n2 heaped tablespoons Greek yogurt substitute for vegan\n1 egg substitute for vegan\nsesame seeds to garnish\ndried oregano to garnish\nAdd the flour and salt to a food processor and mix to combine. Add the avocado and blend in. Add the hot water a little at a time and blend or combine with your hands. I used 90g of water when I made this pastry, but you may need a little more or a little less, depending on your flour. Knead for about 5 minutes in the machine. When ready, the dough should be moist and pliable, but not too sticky. Thoroughly scrape out the food processor, wrap the dough in plastic and rest on the work surface while you prepare the fillings.\nMix the flaxseed and water together in a small tub and rest in the fridge.\nIn a large saucepan, stew the garlic, wine, water, and stock paste over medium-low heat for about 20 minutes, adding a dash of water if it starts to dry out.\nWhile the garlic is stewing, dry sauté the mushrooms over high heat in a large, heavy-bottomed frying pan until well browned – you will most likely need to do this in batches. When the garlic and mushrooms are ready, add the mushrooms to the garlic and stir to combine. Cover and cook for a further 15 minutes, adding a splash of water if needed to keep everything lubricated.\nAfter you add the mushrooms to the garlic, start cooking the red onion in the pan you were sautéing the mushrooms in, until softened and translucent, about 15 minutes.\nGrind the rolled oats and walnuts to a fine powder with a high-powered blender or spice grinder. When the mushrooms are ready, stir in onions, the oat-nut mixture and the flax egg that has been resting in the fridge. Stir very thoroughly to combine. Transfer half the mixture to a blender or food processor and pulse to a chunky texture. Return the chopped mixture to the rest of the mushrooms and stir again to combine. Set aside.\nPut half the spinach in the center of a clean tea towel and twist it around the spinach to remove as much water as possible. Repeat with the other half of the spinach. Transfer to a bowl and thoroughly season with salt, pepper and lemon juice. Set aside.\nRoughly chop the Stilton, if using, and set aside.\nCut the dough into eight even pieces. Working one piece at a time, roughly shape a piece of dough to fit the width of your pasta machine. Run a piece of dough through the machine, starting on setting 1 and working your way up, until the dough is as thin as you can make it. I got up to setting 8 on my machine. You will need to add a little sprinkle of flour to the dough sheets occasionally to stop them sticking. Sticking is very, very frustrating, so definitely avoid that!\nPreheat the oven to 200c. Line a baking tray with greaseproof paper. When one sheet is complete, lay lengths of it on the baking paper, trimming as necessary to form a rough 30cm x 40cm rectangle – you want to create one even layer, as pictured. Drizzle over a touch of olive oil – no need to go nuts – and brush or rub over the pastry so it is mostly covered. Repeat with three more layers – one piece of dough makes roughly one layer.\nStart layering the fillings, starting with the spinach, then the mushrooms, then the Stilton, if using. You want the fillings to have some height and will cover roughly 12cm wide x 30cm long.\nAt each end, cut two diagonal slits through the uncovered pastry from the corners of the filling to the corner of the pastry, so you have a triangular flap at each end. Fold these triangular flaps up over each end and press the pointed corners down the sides. Fold the remaining side pieces of pastry up around the filling. You should now only have a small strip of exposed filling along the top.\nRoll out another piece of pastry and trim it to cover just that small strip. Drizzle with a touch of oil and repeat with another two strips. Brush a little oil over the entire surface. Roll out one final piece of pastry, large enough to cover the whole thing if possible, and lay over the entire length and sides of the Wellington.\nBeat together the yogurt and egg (substitute soy milk for vegan) and brush over the pastry. Bake for about 15 minutes, then remove from the oven, brush again with the glaze, sprinkle with sesame seeds and oregano, and return to the oven to bake for a further 15 minutes or so, until the whole surface is evenly browned.\nRest for about 5 minutes, then cut into pieces with a serrated knife and serve.\nNow, all you have to do is choose your favorites among this bunch! Happy Holidays, everyone!', ""Recipes and Cooking So-Easy Apple Butter 5.0 (2) 1 Review Grab your slow cooker or pressure cooker to make this easy apple butter recipe from scratch. This is one of the best apple butter recipes to use up a big basket of fruit. By BHG Test Kitchen BHG Test Kitchen The Better Homes & Gardens Test Kitchen has been in continuous operation for nearly 100 years, developing and testing practical, reliable recipes that readers can enjoy at home. The Test Kitchen team includes culinary specialists, food stylists, registered and licensed nutritionists, and other experts with Bachelor of Science degrees in food science, food and nutrition, or culinary arts. Together, the team tests more than 2,500 recipes, produces more than 2,500 food images, and creates more than 1,000 food videos each year in the state-of-the-art test kitchen. Learn about BHG's Editorial Process Published on December 6, 2012 Print Rate It Share Share Tweet Pin Email Prep Time: 30 mins Cook Time: 5 hrs Cool Time: 1 hrs Total Time: 6 hrs 30 mins Servings: 16 Yield: 4 half-pints Jump to Nutrition Facts Ingredients 4 pound cooking apples, peeled, cored, and sliced (about 12 cups) 2 cup sugar ⅓ cup water 2 tablespoon cider vinegar 2 teaspoon ground cinnamon ¼ teaspoon ground cloves ⅛ teaspoon ground allspice ⅛ teaspoon ground nutmeg Directions Place apple slices in a 3-1/2- or 4-quart slow cooker. Stir in sugar, water, vinegar, cinnamon, cloves, allspice, and nutmeg. Cover and cook on high-heat setting for 5 to 6 hours. Stir. Cool for at least 1 hour or cover and chill overnight. Ladle apple butter into clean half-pint freezer containers, leaving a 1/2-inch headspace; seal and label. MAKE-AHEAD DIRECTIONS: Store apple butter for up to 3 weeks in the refrigerator or freeze for up to 1 year. Pressure Cooker Instructions Lightly coat a 6-qt. multifunction electric or stove-top pressure cooker with cooking spray. In prepared cooker combine remaining ingredients (but use 1/2 cup water instead of 1/3). Lock lid in place. Set electric cooker on high pressure to cook 30 minutes. For stove-top cooker, bring up to pressure over medium-high heat; reduce heat enough to maintain steady (but not excessive) pressure. Cook 30 minutes. Remove from heat. For both models, let stand 15 minutes to release pressure naturally. Release any remaining pressure. Open lid carefully.Using an immersion blender, blend apple mixture until smooth. (Or transfer, in batches if needed, to a food processor or blender; cover and process or blend until smooth. Return to cooker).Set electric cooker on saute setting or place stove-top cooker over medium heat. Simmer apple mixture, uncovered, 18 minutes or until very thick and mixture mounds on a spoon, stirring frequently. (If you like, use a splatter guard or colander to cover cooker to reduce splattering while simmering).Ladle apple butter into four half-pint airtight containers, leaving a 1/2-inch headspace. Seal and label. Store in refrigerator up to 3 weeks or freeze up to 1 year. Rate it Print Nutrition Facts (per serving) 71 Calories 18g Carbs 1g Protein Show Full Nutrition Label Hide Full Nutrition Label Nutrition Facts Servings Per Recipe 16 Calories 71 % Daily Value * Sodium 1mg 0% Total Carbohydrate 18g 7% Total Sugars 17g Protein 1g Vitamin C 1.8mg 9% Iron 0.2mg 1% Potassium 46mg 1% *The % Daily Value (DV) tells you how much a nutrient in a food serving contributes to a daily diet. 2,000 calories a day is used for general nutrition advice.""]"	['<urn:uuid:1def088f-3302-4a0a-b221-04e43f84d1a4>', '<urn:uuid:b8f51859-a3a8-49cb-aa10-9824d1c08893>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	11	32	2292
74	I'm studying discrimination. Do race issues impact crossing borders?	Yes, when people cross national borders they can suddenly face unexpected racialization in ways that never existed for them before.	['Ahmed, Sara. Interview with Judith Butler.\nSara Ahmed asks: What kind of questions, concerns, interests, directions would for you be the ones that would keep Queer Studies alive as a project?\nJudith Butler responds: Here are some questions that I think are really important*:\n- How do we understand those desires that we might call abiding, persistent, and that for many define their basic sense of self?\n- How do we even understand that basic sense of self, when it exists or when it struggles to exist?\n- How is that sense formed, and when does it take hold, if it does?\n- Under what conditions is it dismantled or even shattered?\n- And how do we live in ways that request that this sense of self, these abiding and obdurate desires, be recognized?\n- How do we account for those whose experience of desire does not ‘settle’ in this way, so that either desire may contest a basic sense of self or may establish the self as changeable or alterable?\n- How do we tell the stories about how we came upon our desires, how we came to negotiate the basic ways in which both gender and sexuality were ‘assigned’ against our will at the same time that we insist on the enduring or bedrock quality of the category that describes who we have become?\n- How do we still value becoming without losing track of what grounds and defines us? How much of our self-definition is found and how much is made, and under what conditions do new naming practices offer us a chance to be who we wish to be?\n- How do we think about the doubleness of the self that wants to be who it is?\n- Is that doubleness fully overcome when we say that we have arrived and that we are now that being what we always wanted to be?\n- What lingering disappointments or doubts follow, and are we still living when we have decided on who we are? How can a sense of living be preserved within the terms of decision, so that ‘deciding’ does not put an end to the processual quality of life?\n- Conversely, if we never decide who we are, are we at risk of becoming dispersed in ways that make life unlivable?\n- How do we think about those self-naming and self-defining practices that take place in concert with others in a world in which the language we use is itself in a process of change?\n- What if we shift the question from ‘who do I want to be?’ to the question, ‘what kind of life do I want to live with others?’?\n- It seems to me that then many of the questions you pose about happiness, but perhaps also about ‘the good life’ – very ancient yet urgent philosophical questions – take shape in a new way. If the I who wants this name or seeks to live a certain kind of life is bound up with a ‘you’ and a ‘they’ then we are already involved in a social struggle when we ask how best any of us are to live. It is of course especially difficult to ask this last question, what kind of life do I want to live with others, if the life that we are seeking to live is not regarded as a life at all?\n- How do these philosophical desires become compromised or complicated if a life is considered a non-life under regimes of racism?\n- How do we account for the experience of someone crossing national borders only to find that they are racialized in ways that never existed before? A sudden, unexpected interpellation.\n- How does the issue of race divide those queer activists and writers who ally with struggles against racism, nationalism, war, and occupation from those who think that queer ought to become its own identity, its own discipline, and so differentiated from these other concerns and struggles? It seems to me that queer has to be part of the weave of a broadening struggle.\n- Important also is to ask: Whose stories do we read, and how important might the story be in telling a history, in explaining how science changes, or in making clear how a philosophical concept works, or can work?\n- How do we think about bringing feminism into a closer relation with queer and trans and with anti-racist struggles, without letting those who conduct transphobic diatribes monopolize the meaning of feminism, or those who continue to believe that feminists must defend themselves against the claims of cultural difference?\n- Can we still own queer – or any of these terms – without letting them monopolize difference, allowing for a certain movement of thought that is grateful to its critics for letting us think something new, that is glad to be in the mix of emerging alliance and not the ultimate sign of its unity?\nNote: Butler’s questions are not in list form in the published interview. But, because I’m into lists these days, I wanted to put them in that form. Click “continue reading” to read the text as it was published. Back to text.']	['<urn:uuid:aae544a2-a5a2-430b-9400-e3f075320d93>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	9	20	868
75	essential components traditional chinese wing chun training	The Wing Chun system includes several essential components: Chi Sau training, three hand forms (Sil Lim Tau, Chum Kiu, and Biu Gee), wooden dummy techniques (Muk Yan Jong), six & a half point pole (Luk Dim Boon Gaun), and eight cutting double knives (Bart Cham Dao).	"['What is Ip Man Wing Chun?\nWing Chun is a very unique and scientific form of martial arts. Wing Chun is a style of Kung Fu that originates from China, the martial art was later refined in Hong Kong by the late Ip Man. Wing Chun\'s specialty is in close contact combat, using quick punches and kicks with a tight defence, coordinated through agile stances and footwork for a quick advance. The effectiveness of this fighting style is achieved by well coordinated attacks with simultaneous defence and vise versa. Due to its effectiveness this martial art makes for an effective form of self defence. The student must learn to deliver the correct amount of energy or force, whilst staying relaxed when possible. A good Sifu (instructor / teacher) will teach the student to overcome force and strength with positioning and turning rather than meeting it head on. The style uses kicks, sweeps, elbows, palm strikes, punches, trapping and control techniques as part of its fighting arsenal.\nThe Martial Art Syllabus\nThere are many different parts to the Wing Chun system. Learning this style of Kung Fu requires knowledge of Chi Sau, the hand form, the weapons and dummy forms and sparring.\nUnique to Wing Chun is ""Chi Sau"" (or Chi Sao), a form of training to help develop and put into practice your techniques and theories you learn during your training. Chi Sau teaches and helps the student to develop a responsive reflex, along with good position, how to overcome your opponents strength, correct usage of energy and taking advantage of the shortest possible distance between you and your opponent. Chi Sau will also help with sensitivity, or ""reacting to feeling rather than sight"". In addition to this Chi Sau helps students learn to react to unpredictable movements as there is not set of predefined movements.\nThe principles, theories and techniques of Wing Chun are founded on the three hand form and the wooden dummy techniques. Training begins with Sil Lim Tau (little idea), Chum Kiu (bridge seeking), Biu Gee (thrusting fingers). Then the serious of students will learn Muk Yan Jong; (wooden dummy), Luk Dim Boon Gaun (six & a half point pole) and finally Bart Cham Dao (eight cutting double knives).\nKung Fu Lineage\nPictured top left is Grandmaster Ip Man (sometimes referred to as Yip Man). Ip Man taught many famous martial artists\nincluding the late Bruce Lee. The other photos are Ip Man\'s sons Grandmasters Ip Chun and Ip Ching, and their student,\nSamuel Kwok. This linage can trace its history back to the origin of\nIt was Ip Man\'s dream to make the Chinese martial art Wing Chun, a well respected fighting system around the world, and through tuition by his sons, Samuel Kwok has played his part over the last twenty five years. Sam Kwok has helped bring Wing Chun to the forefront of Martial arts practice today. Grandmaster Kwok has wrote a couple of great Wing Chun Books and made many Wing Chun Videos and DVDs which help students across the world improve their Wing Chun. There are many martial arts schools and instructors up and down the United Kingdom, as well as across Europe, in Denmark, Germany, South Africa, Australia and the United States to name a few. Many of the Sifu\'s at these schools owe a great deal of their understanding of the art to the influence of Samuel Kwok\'s teaching of traditional Ip Man style Wing Chun Kung Fu..\nWing Chun or Ving Tsun?\nThe martial art Wing Chun is sometimes referred to as Ving Tsun, Wing Chun Kung Fu (or Gong Fu), Wing Chung, Wing Chun Boxing, or even Wing Chun Kuen (fighting) amongst some other names. More recently research shows people write Wing Chun Gung Fu and Wing Tsun. The reason for these differences is because Chinese text does not translate precisely into Western letters or even western speech. However the name is not the important thing. The most important thing is that the student learns an effective form self defence and fighting. The martial art is a style of Kung Fu. Kung Fu roughly means time and effort and originally referred to any skill painstakingly developed. Recently Kung Fu has become synonymous with martial arts. The Chinese characters are for Wing Chun are 詠春.\nSamuel Kwok Wing Chun Association\nSamuel Kwok, under the guidance of Ip Chun and Ip Ching, continues to represent Traditional Ip Man Wing Chun in Europe, as he has done for many years. Use the links at the top of the page to navigate the site, and learn more about the traditional Ip Man style. The Site contains many videos and photos of Wing Chun in action. In the true spirit of Wing Chun and with the aim keeping the art practical and efficient, Samuel Kwok has trained with many different martial arts masters over the years. This includes masters from Brazilian Ju Jitsu (BJJ), Mauy Thai, Kickboxing and professional boxers. This has ensured that Samuel Kwok\'s teaching of Wing Chun techniques is of the highest quality and always practical.\nOur aim is to respect the wish of grandmaster Ip Man. About 6 weeks before he died he asked his 2 sons and his student Lau Hon Lam to film him performing the Wing Chun system as practice by him. He only managed Sil Lim Tau, Chum Kiu and the Dummy form. This is because he was in a lot of pain and was weak and unsteady on his feet. He was going to do Biu Gee, the Knife form and long pole. However Grandmaster Ip Chun and Ip Ching and Sifu Lau Hong Lam stopped Ip Man because Biu Gee, the knives and full pole form require a lot of energy to perform. I want to pass on Ip Man Wing Chun as taught by Grandmaster Ip Man and Ip Ching and Ip Chun\n- Samuel Kwok\nWing Chun Video Footage\nHigh quality Ip Man footage filmed in 1972\nCleaned up footage of the late grandmaster Ip Man. This was filmed in 1972 just 2 weeks before Ip Man passed away from cancer.']"	['<urn:uuid:467f890d-9cfd-4990-8df8-e928e7488491>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	7	46	1021
76	medieval historian needs info about ancient military equipment trade routes geography terrain challenges	During the Migration Period, Germanic tribes used various spears including the framea and gêr, which were effective weapons for both close and distant combat. The transportation of such weapons and other military equipment would have faced significant challenges along the Silk Road, which included treacherous mountain passes up to 9,843 feet high and the harsh Taklamakan desert where water was scarce. Caravans had to carefully plan their routes between oases, typically covering thirty miles per day.	"['- Migration Period spear\nThe pre-migration term reported by Tacitus is framea, who identifies it as ""hasta""; The main native term for ""javelin, spear"" was Old High German gêr, Old English gâr, Old Norse geirr,gais apparently from Proto-Germanic *gaizo-, although the older form of the English word ""spear"" (Old English spere) was also used as where its cognates such as the Old Frisian sper and the Old High German speer. Spear in origin also denoted a throwing spear or lance (hasta).\nGaesus (plural: gaesum) was the term for the lance of the Gauls called in Greek γαῖσον. The Celtic word is found e.g. in the name of the Gaesatae. Old Irish has gae ""spear"". Proto-Germanic *gaizo would derive from PIE *ghai-.\nThe word kêr or gêr is attested since the 8th century (Hildebrandslied 37, Heliand 3089).\nThe term survives into Modern German as Ger or Gehr (Grimm 1854) with a generalized meaning of ""gusset"" besides ""spear"". In contemporary German, the word is used exclusively in antiquated or poetic context, and a feminine Gehre is used in the sense of ""gusset"".\n- Even iron is not plentiful with them, as we infer from the character of their weapons. But few use swords or long lances. They carry a spear [hasta] (framea is their name for it), with a narrow and short head, but so sharp and easy to wield that the same weapon serves, according to circumstances, for close or distant conflict. As for the horse-soldier, he is satisfied with a shield and spear; the foot-soldiers also scatter showers of missiles each man having several and hurling them to an immense distance, and being naked or lightly clad with a little cloak.\nThe term is also used by Eucherius, Gregory of Tours and Isidore. By the time of Isidore (7th century), framea referred to a sword, not a spear. Since Tacitus himself reports that the word is natively Germanic, various Germanic etymologies of a Proto-Germanic *framja, *framjō or similar have been suggested, but remain speculative. Must (1958) suggests *þramja, cognate to Old Norse þremjar ""edges, sword blades"", Old Saxon thrumi ""point of a spear"".\nAnglo-Saxon gar rune\nName Proto-Germanic Old English *Geƀō Gyfu; Gar ""gift"" ""gift""; ""spear"" Shape Elder Futhark Futhorc Unicode ᚷU+16B7 ᚷ ᚸU+16B7 U+16B8 Transliteration g ȝ; g Transcription g ȝ, g; g IPA [ɣ] [g], [ɣ], [ʎ], [j]; [g] Position in rune-row 7 7; 33\nGar ""spear"" is also the name of ᚸ, a rune of the late Anglo-Saxon futhorc. It is not attested epigraphically, and first appears in 11th century manuscript tradition. Phonetically, gar represents the /g/ sound. It is a modification of the plain gyfu rune ᚷ.\nOld English gâr means ""spear"", but the name of the rune likely echoes the rune names ger, ear, ior: due to palatalization in Old English, the original g rune (gyfu) could express either /j/ or /g/ (see yogh). The ger unambiguously expressed /j/, and the newly introduced gar rune had the purpose of unambiguously expressing /g/.\nGar is the 33rd and final rune in the row as given in Cotton Domitian A.ix.\n- Gustav Must, ""The Origin of framea"", Language, Vol. 34, No. 3 (Jul. - Sep., 1958), pp. 364–366.\n- Mark Harrison and Gerry Embleton, Osprey Warrior 005 - Anglo-Saxon Thegn 449-1066 AD  \n- Kragehul lance\n- Migration Period sword\n- Gothic and Vandal warfare\n- Anglo-Saxon warfare\n- Viking Age arms and armour\n- ^ http://www.etymonline.com/index.php?term=gar\n- ^ http://www.etymonline.com/index.php?search=spear&searchmode=none\n- ^ http://www.pnas.org/content/suppl/2003/06/27/1331158100.DC1/1158SuppAppendix.html\n- ^ http://www.scottishmist.com/index.php?option=com_content&view=category&id=48:latest&layout=blog&Itemid=63&layout=default\n- ^ http://www.etymonline.com/index.php?term=goad\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nMigration Period sword — Hilt of a Vendel period sword found at Valsgärde. Swords of the Migration Period (4th to 7th centuries AD) show a transition from the Roman era Spatha to the Viking sword types of the Early Middle Ages. The blade is normally smooth or shows a… … Wikipedia\nSpear — A spear is a pole weapon used for hunting and war, consisting of a shaft, usually of wood, with a sharpened head. The head may be simply the sharpened end of the shaft itself, as is the case with bamboo spears, or it may be of another material… … Wikipedia\nJats in the pre-Aurangzeb period — The Jats in the pre Aurangzeb period, according to the historian Qanungo, had little scope for their lawless activity under the strong governments of the Surs and the Mughals down to the accession of Aurangzeb (1658 1707). They remained quiet… … Wikipedia\nModels of migration to the New World — There are several popular models of migration to the New World proposed by the anthropological community. The question of how, when and why humans first entered the Americas is of intense interest to anthropologists and has been a subject of… … Wikipedia\nGungnir — Lee Lawrie, Odin (1939). Library of Congress John Adams Building, Washington, D.C. In Norse mythology, Gungnir (Old Norse swaying one ) is the spear of the god Odin … Wikipedia\nList of premodern combat weapons — Premodern combat weapons include include both ranged weapons and mêlée weapons weapons which, in general, existed before the invention of the true flintlock gun around 1610, or until the 1700 s for incendiary weaponscite book | title=Weapon: A… … Wikipedia\nSeax — (also Hadseax , Sax , Seaxe , Scramaseax and Scramsax ) in old Saxon stands for knife or cutting tool. [ Bosworth, Joseph, D.D., F.R.S. [http://beowulf.engl.uky.edu/ kiernan/BT/Bosworth Toller.htm An Anglo Saxon Dictionary] . Retrieved 16 July… … Wikipedia\nGothic and Vandal warfare — The Goths, Gepids, Vandals, and Burgundians were East Germanic groups who appear in Roman records in Late Antiquity. At times these groups warred against or allied with the Roman Empire, the Huns, and various Germanic tribes.The size and social… … Wikipedia\nGēr — may refer to: *Old High German for spear , see Migration Period spear *Old Anglo Frisian for year , see Jēram … Wikipedia\nOdin — This article is about the chief god in North Germanic tradition. For other uses, see Odin (disambiguation). For a comparative discussion of North and West Germanic, see Wodanaz. Odin Odin, the Wande … Wikipedia', ""The Silk Road : 丝绸之路\nEurope and Asia at the time of the Roman and Han dynasties\nThe Silk Road or Silk Route evokes an exotic vision of China; the imagination immediately conjures up camels carrying rare and exotic treasures thousands of miles through the desert landscapes of Central Asia.\nIt was of great importance in the Han (200BCE); Tang and Yuan dynasties. Trade did begin at a much earlier date but on a much smaller scale as Chinese silk has been found in Afghanistan from 1500BCE. The Silk Route fell into decline during the Song and Ming dynasties when trade by sea from southern ports became safer and more profitable than the overland route. There is more than one road, so it should be thought of as a network of roads as it has several branches, starting in both India and the Middle East and ending at the Chinese capital (for a long while this was at Luoyang, Henan province). There was also an overland route through Tibet to Bangladesh, from Chengdu to Hanoi and over the mountains of Yunnan into Burma. The name ‘Silk Road’ can only be traced back as far as the 19th century to the German geographer Baron von Richthofen ➚, it was not known by this name in China. The trade along the route was much more significant to the Indians, Europeans than to the Chinese, it carried only a small fraction of Chinese trade. The most exotic and expensive goods, silk, that could only be brought from China is reflected in the road's name.\nThe map above shows the Silk Road at about 100CE, when the Roman Empire extended into Asia Minor and the Han Empire had conquered much of modern China (except for Fujian). At the beginning of the Han dynasty explorer Zhang Qian ➚ spent 13 years on an eventful trek to the west where he married into the Xiongnu tribe ➚ (the Xiongnu had withstood Chinese expansion to the north-west). Zhang eventually brought back tantalizing news of advanced civilizations beyond China's borders. His journey into Parthia and India encountered traces of the Greek Empire that had been founded by Alexander the Great 356-323BCE ➚. Emperor Wudi's curiosity was piqued and he sought to find out more, including the exploration of a southern route into India through Yunnan. This first contact proved that China was not alone in developing an urban-based civilization. The Silk Route was first used to import horses into China, which were needed to improve the effectiveness of Han cavalry against the skilled horsemen of the northern frontier tribes. With the new horses General Ban Chao ➚ was able to subjugate them. Thereafter more tales and rumors of the great, far away Roman Empire reached China. They learned such things as the Romans drank from glasses rather than earthenware cups. The Han dynasty name for Rome was Da Qin 大秦 ‘Great Qin’ named after the Qin dynasty itself, apparently called ‘great’ due to their taller stature. Trade along the Silk Road between the two great empires grew rapidly. The Romans developed such a veracious appetite for silk that Emperor Tiberius introduced a ban to try to stem the outflow of gold. At this time it was the Sogdians ➚ centered at Samarkand who were the main traders along the route.\nSilk Road cities\nAlong the Silk Route are many ancient trading posts: Bakhara; Kashgar; Tashkent; Kunduz; Samarkand ➚; Turpan; Tehran. The fortunes of these great cities ebbed and flowed as different peoples came into ascendency; Central Asia remained an area in flux for many centuries. Most chroniclers emphasize the long route to the Mediterranean and yet it was the trade with India which was more important. It was from India that many would say the most precious cargo was imported: Buddhist religion. The grand western entrance into China was the Jiayuguan Gate, Gansu 嘉峪关 at the western end of the Great Wall of China. The Great Wall gave protection to travelers from attacks by tribes to the north on their passage deep into China through Lanzhou and on to the ancient capitals at Luoyang and Chang'an. When the Tibetan kingdom reached the height of its power in the Tang dynasty, Tibetans controlled the southern strands of the route. It was at Dunhuang, famous for its caves full of Buddhist paintings and scrolls, that the route divided - one route to the north of the inhospitable Taklamakan Desert to Central Asia and the other to the south with a branch to India.\nThe traditional caravan that traveled the route was made up of a hundred camels roped together. They generally managed thirty miles a day, hoping to reach the next oasis in good time for sunset. Bells hung around the necks would enable any camels that broke free to be located and alerted caravans to each other's presence from afar. As well as their prodigious skill in storing water, camels were invaluable for finding new sources of underground water that they could sense and paw at the ground to allow a temporary well to be dug. Knowledge of the exact route and weather conditions was passed down through generations of cameleers, to stray from the Silk Road was almost certain death. The journey was so long and arduous that they would normally make only one trip each year. To make a decent living they had to transport only the most precious of goods.\nPerils of the Silk Road\nThere are only a few passes through the mountains into western China and even then climbing up to 9,843 feet [3,000 meters] was no mean achievement for the hardy traders and their beasts of burden. In the deserts of Taklamakan it was water that was the crucial commodity. Only limited amounts of water could be carried and these would last only a few days. An ancient civilization built the Karez (or Qanat ➚) water system to take the melt-water from the mountains and channel it underground down to the towns fringing the desert. The system at Turpan ➚ is the most impressive where melons and grapes can be grown with no direct water supply. These irrigation systems were already in place two thousand years ago in the Han dynasty.\nThe Mongol conquest of China caused the whole of the route through Central Asia to come under Mongol control and trade flourished. The route had always been subject to raids by bandits who would kill the porters and steal their precious cargo. The Silk Road into China at this time is described in the travels of Marco Polo. Trade was carried in stages by local tribesmen, Parthians; Bactrians and Sogdians of Central Asia, who zealously maintained their role as middlemen. Only very few ever made the complete trek along the whole route. The lack of direct contact denied first hand knowledge in Europe of China and just as importantly China of Europe. However some Chinese inventions did make their way along the route including gunpowder; the abacus and silk. In return China grew fond of using wooden chairs rather than simple stools.\nAs the name ‘Silk Road’ implies, silks were the most important goods transported as they were light; non-perishable and valuable. Silk was the ideal cargo for long distance journeys by land. At the time of Diocletian ➚ the price of silk is estimated to be $0.5 million per pound - more than its weight in gold. Apart from silk, China exported grain, lacquer-work, bronzes, decorated metalwork, diamonds, pearls and rhubarb (then believed the only reliable cure for constipation ➚). While in return imports of spices (from India and Central Asia); glass; coral (for ornamentation) woolen textiles; rhino horn; hides; ivory; fruit; amber; tortoiseshell; purple cloth and horses came into China. Exotic birds and animals if they survived the long, grueling journey commanded a high price too. Diseases were a less welcome import and export – the Black Death ➚ that killed 100 million people came along the silk road from Mongolia.\nDecline of the Silk Road\nThe decline of the Silk Route started when northern China was conquered by the Jin and Xixia kingdoms during the early Song dynasty. The Southern Song dynasty had to rely on the sea to the south for trade as the overland silk road to the north was blocked off.\nAlthough the Silk Road re-emerged as the main overland transport route during the Mongol dynasty when this fell in 1368 the overland route was no longer as safe. Central Asian kingdoms and their trading cities along the route had been devastated by the Mongol conquests. Northern barbarian tribes resumed their threat to the northern Chinese Silk route and more significant of all, advances in maritime technology allowed for safe and more profitable transport by sea as maps and compasses made the long distance sailing trips more practical. Even in Roman times there had been small scale but significant maritime trade via Sri Lanka and Arabia. By the start of the Ming dynasty the sea routes had become well established. This was one reason why the center of Chinese civilization moved south from the Yellow to the Yangzi River valley.\nSilk Road re-discovered\nAfter many of the silk road cities had been abandoned and buried in sand for centuries it was the turn of explorer-adventurers in the late 19th century to enter a race to rediscover them. There was great interest in Europe and America about the distant, exotic history of Central Asia. Museums paid handsome rewards for ‘rediscovered’ (effectively stolen) treasures as they attracted many visitors. The explorers included Aurel Stein ➚ (UK); Paul Pelliot ➚ (France); Sven Hedin ➚ (Sweden); Albert von le Coq ➚ (Germany); Count Otani Kozui ➚ (Japan) and Langdon Warner ➚ (US). Some of the most prized relics were found at Dunhuang, Gansu.\nBelt and Road initiative\nIn the last few years the Chinese government has begun an ambitious program to bring economic development along the line of the original Silk Route (as well as sea routes), not just in China but to Central Asian states as well. It is called the Belt and Road Initiative ➚ and was launched in 2015. Just like the ancient Silk Road the new over-land route will offer cost-effective transportation from China all the way into the heart of Europe.""]"	['<urn:uuid:3f58874d-cf72-4f3d-96d4-3403c09d6b26>', '<urn:uuid:c7deb035-2d0a-4d36-9c98-5394e2d53014>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:43:46.574121	13	76	2730
77	What percentage of people in the study got dementia?	In the study, 7.4% of participants in the STR-1973 cohort developed dementia, while 5.0% of participants in the SALT cohort developed dementia. The STR-1973 cohort was followed for 41 years, while the SALT cohort was followed for 14 years.	['BOSTON—Midlife shift work is associated with an increased risk of incident dementia in later life, according to research presented at the 31st Annual Meeting of the Associated Professional Sleep Societies. The\nassociation is stronger with longer duration of shift\nwork, said Kathleen Bokenberger, a doctoral student in\nthe Department of Medical Epidemiology and Biostatis-tics at the Karolinska Institutet in Stockholm.\n“To the best of our knowledge, this is the first study\nto find a connection between shift work and greater\ndementia incidence,” said Ms. Bokenberger. “As such,\nwe will need confirmation from other studies, especially\nif they have more nuanced measures of shift work, and\nperhaps repeated measures of shift work.”\nAn Analysis of Two Population-Based Cohorts\nShift work interferes with the circadian and homeostatic\nregulation of sleep and can lead to insomnia and excessive sleepiness. A prospective Danish study found that\nrotating and evening shift work were associated with\nincreased dementia mortality, but the link between shift\nwork and dementia is still relatively unexplored, said\nShe and her colleagues followed two population-based cohorts from the Swedish Twin Registry. The\nfirst cohort, which the researchers called STR-1973,\nincluded 13,283 people who were age 30 or older\nwhen they received a mailed questionnaire in 1973\nthat elicited information about shift work status and\nduration. The Screening Across the Lifespan Twin\n(SALT) sample included 41,199 participants who participated in a telephone interview between 1998 and\n2002 that asked about status and duration of night\nThe primary outcome was all-cause dementia, and\ndementia diagnoses were obtained from Swedish national health registers. Investigators followed participants until the date of dementia diagnosis, death, or\nthe end of follow-up in 2015.\nA Dose-Response Relationship\nThe mean age at baseline was 38 in the STR-1973 cohort\nand 58 in the SALT cohort. Median follow-up time was\n41 years in the STR-73 cohort and 14 years in the SALT\ncohort. The proportion of participants with shift work\nhistory was 17% in the STR-1973 cohort. The proportion of people who worked nights was 30% in the SALT\ncohort. Dementia incidence was 7.4% in the STR-1973\ncohort and 5.0% in the SALT cohort.\nThe researchers used Cox proportional hazard models to analyze the data and adjust for age, sex, education,\ncardiovascular disease, stroke, and type 2 diabetes. Having shift work versus day work was associated with an\nincreased hazard of dementia (hazard ratio [HR], 1.40),\nas was having night work versus day work (HR, 1. 13).\nSpline models indicated modest dose-response relationships in which longer duration of shift work and night\nwork predicted greater dementia risk.\nInformation about APOE e4 genotype was available for\nsubsamples of both cohorts. APOE e4 seemed to be an effect\nmodifier for the association of night work and dementia, but\nnot for that of shift work and dementia. APOE e4 carriers\nwith 20 or more years of shift work had a fourfold increased\nrisk of dementia. APOE e4 carriers with 20 or more years of\nnight work had a twofold increased risk of dementia. The\nsample sizes were small, however, and “it is possible that\nthese findings are spurious,” said Ms. Bokenberger.\n“Individuals in the labor force might consider minimizing shift work exposure or managing work scheduling practices, but this is dependent on individual differences and tolerances to certain types of shift work,”\nshe concluded. NR\nBokenberger K, Ström P, Dahl Aslan AK, et al. Shift work and cognitive aging: a\nlongitudinal study. Scand J Work Environ Health. 2017 Mar 31 [Epub ahead of print].\nJørgensen JT, Karlsen S, Stayner L, et al. Shift work and overall and cause-specific mortality in the Danish nurse cohort. Scand J Work Environ Health.\nShift Work in Midlife\nIncreases Risk of Dementia\nThe risk of dementia appears to increase with longer duration of shift work.']	['<urn:uuid:84dfe1e7-2481-410f-9c44-3602c22aa2a5>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	9	39	626
78	What are historical films' preservation needs and solutions?	Historical films require specific preservation solutions due to various deterioration issues. Cinerama films needed specialized digital manipulation to correct panel misalignments, density issues, and barrel distortion, addressed through the Smilebox format. Meanwhile, the NFSA's collection faces challenges with dye-fade and shrinkage in films from the 1970s, requiring advanced scanning technology. Their solution involves using the Scanity scanner which can handle damaged films while maintaining high image quality, processing both black & white and color films effectively.	"['|\'); document.write(\'\'); //-->|\nTalk about a major job of filmic restoration. The giant 3-panel Cinerama format had been extinct for quite a while when a handful of collectors here and overseas began putting together unique private mini-Cinerama theaters, complete with the three-projector setup. The actual filmic holdings of the Cinerama Corporation had been languishing in storage for decades, with no assurance that they would ever again see the light of theatrical exhibition. Pacific Theaters inherited this vault, and also owned Hollywood\'s Cinerama Dome, which ironically had never seen 3-panel projection during the Cinerama era. In stepped a small group of Hollywood professionals led by editor-enthusiast technical expert David Strohmaier, to promote the idea of refurbishing all of the Cinerama features. Strohmaier was an important consultant on the restoration of Warners\' MGM-produced How the West Was Won. What\'s more, Strohmaier came up with the clever concept of \'Smilebox\' formatting for video. Theatrically, Cinerama plays on a curved screen and really can\'t have the same impact when imaged flat. The use of the Smilebox (see the posted images) simulates the curvature of the theatrical experience, and also helps to correct perspective mismatches between the three panels.\nLast year the disc label Flicker Alley released Strohmaier\'s restored Blu-rays of the original This Is Cinerama and a travelogue called Windjammer. Just out are two new discs that represent even more complicated restorations: the second Cinerama release Cinerama Holiday (1955) and the fifth, 1958\'s Cinerama South Seas Adventure. The movies are fascinating, as are the stories behind them as told through the plentiful disc extras. Each disc is a separate purchase.\nThis Is Cinerama was in concept little more than a novelty film that searched for likely subjects for the costly, ungainly but undeniably impressive 3-panel Cinerama format. Content-wise the film was a step back to 1895, when just seeing things in a visual format was in itself thrilling. This Is alternated exciting point-of-view experiences (the roller coaster, in particular) with travelogue vistas and performances. Produced by the noted Louis De Rochemont, the second major production Cinerama Holiday applies a simple framing idea to what is still a travelogue format, eliminating deadly sequence ideas such as six-minute shots of a choir singing. The producers found an American couple to travel to Europe, and a Continental couple to take an extended American holiday. They hired two young attractive married couples, the Marshes from the Midwest and the Trollers from Switzerland. It doesn\'t hurt that they all look like catalog models. The European Fred Troller was an experienced artist. The movie shows highlights of their vacation travels, ranging all over the United States in the case of the Trollers, and for John and Betty Marsh visiting big cities and Alpine snow country. Highlights include a ride down a bobsled run (complete with realistic sound in 7-track stereo) and a trip to Paris for the Americans, while the Trollers travel across the country on a tiny motorcycle (um-hmm, sure) visiting big cities and Las Vegas, which impresses them even though it is but four or five square blocks of relatively modest casinos. The itineraries jump all over the place, from San Francisco back to New England and then down to New Orleans. Music and performances sneak in through skaters and clowns up in the Alps, and a Dixieland band in The Big Easy. The two couples finally come together in New York, to see the \'premiere\' of the film they\'ve just created. Seated in a Cinerama theater, they watch a concluding spectacle of Navy jets flying in formation and landing and taking off from an aircraft carrier.\nCinerama Holiday is exactly what it sounds like, a superficial travelogue that impresses through its unique cinematography -- audiences loved feeling as if they were there in the white snow atop an Alpine mountain. The same goes for a slow trucking shot along the Seine, where we see the river on one side and the looming Notre Dame cathedral on the other. For us video viewers 60 years later, the many wide shots of Parisian boulevards and Swiss roads are fascinating in themselvesÉ that\'s how it was. It\'s a strange blend: primitive cinema made with ultra-sophisticated technology.\nCinerama is the oddest, most unwieldy film format ever, but it indeed grabbed major attention worldwide for about ten years. Scores of dedicated theaters were built (even in Havana, Cuba!) just to show Cinerama on a Road Show, hard ticket basis. Audiences marveled at the colossal curved screen. A patron sitting way down in front (not necessarily recommended) had to turn his head radically from left to right to follow action on the screen -- quite a bit of one\'s peripheral vision was covered.\nConveying the Cinerama experience on home video is a tall order: no matter what one does, the visual field in Cinerama is fairly distorted. Strohmaier\'s ""Smilebox"" concept works quite well on the How the West Was Won Blu-ray and is a big boost to these releases. Although the Smilebox doesn\'t quite compare to the real thing, its simulated \'wrap-around\' screen conveys a similar sensation.\nThe extras for Cinerama Holiday are many and thorough. Betty Marsh and Beatrice Troller are tapped for an interview featurette, and Betty thumbs through a scrapbook that she kept during her European trip. It must have been great, being ""forced"" to wait for weeks in a fancy Paris hotel for the next filming assignment. Added fun is provided by outtakes from an earlier docu in which cameramen tell stories of working with the Cinerama camera, including unforeseen accidents filming on ski slopes, etc.. A surviving breakdown reel, to be used if (heh heh) anything should go wrong with the outrageously complicated, interlocked apparatus needed to project Cinerama, is a 15-minute 35mm reel that has the Marsh\'s pull out their photo album to ""fill in"".\nCinerama Holiday is of course meant to be light entertainment escapism. If anything, it inspired a lot of Americans in the audience contemplating European tours of their own. The overall tone and a few lines of narration show a touch of Cold War attitude. America is presented as equal to the rest of the world put together. When the Marshes say they want to avoid tourist destinations in Paris, a local expert answers that ""the tourists here go everywhere but the city dump and the Russian embassy"". A couple of lines about the Free World sneak in as well. Everybody beams with pride at the exhibition of military superiority that concludes the show -- there\'s no question of who owns the world.\nMade three years later, Cinerama South Seas Adventure breaks from the straight travelogue format. The show no longer pretends that the Cinerama camera can magically follow its attractive characters, or that what we see is necessarily what they see. We\'re given one omniscient narrator -- Orson Welles -- to lend majesty and mystery to South Seas scenery and aerial shots of lonely atolls. Various \'characters\' come to the Pacific to find something special, and some of their voices provide additional narration. The actor-models slightly stiff play-acting looks more real than the earlier film\'s feigned reality. South Seas Adventure is shorter but has more shots and more visual variety.\nTwo young women Kay Johnson and Marlene Hunter (Diane Beardmore & Marlene Lizzio) take a Matson Liner to Oahu. Marlene plans to marry, and Kay is entertained by Marlene\'s blonde brother Ted (Tommy Zahn), who takes her on a tour of the islands and Honolulu\'s favorite nightspot, Don the Beachcomber\'s. Penniless French artist Jean-Louis Martin (Igor Allan) enlists to crew on the sailing yacht Te Vega. He\'s hazed when they cross the equator, and in Tahiti he visits Paul Gauguin\'s neighborhood and paints pictures of an exotic dancer, Turia (Ramine). The voyages of the philosophical skipper Amos Dorn (Ed Olsen) provide opportunities to visit other islands deep in Polynesia, especially one where young men jump from wooden towers with vines attached to their ankles to prove their bravery. An ex-GI looking for peace finds it fishing in New Zealand, a land that seemingly gathers landscapes from all over the world. European refugees David and Anna Koschek (Eric Reiman & Jannice Dinnen) are welcomed to a new home in the barren Australian outback, where the government encourages immigration. And we return to Hawaii for a fancy island wedding.\nSouth Seas Adventure is a much more fluid and expressive experience than Holiday. So many of us have visited Honolulu that it\'s amazing to see the city before its beach was studded with fifty hotels, and traffic was (until quitting time) fairly reasonable. From the Punch Bowl the South coast is laid out before our eyes from Diamond Head to the entrance to Pearl Harbor ... we can see the Aloha Tower, the image is so sharp. The politics is rather careful in this show. Except for shots of the resting place of the Arizona, nothing is mentioned about the massive U.S. military presence on Oahu. The Punch Bowl is actually a vast military graveyard up in the hills. By this time there was already a Cinerama theater in Tokyo.\nThe movie takes every opportunity to give us aerial views of volcanoes, sailing ships on the vast ocean, appealing uninhabited islands and big expanses of New Zealand mountain scenery. We see plenty of tourist-oriented native dancing and extended scenes of surfing at Waikiki (a pro team for sure, complete with a surfing dog). In Tahiti, the movie gives the impression that there are hundreds of beautiful Polynesian women waiting in flowered sarongs to pose for handsome French painters. The Christian influence is charted in shots of missionary churches and a couple of excerpts of local choirs. Aerial shots of a New Zealand town are quick to pick out a new church for special coverage. The narration touches upon a few facts of Polynesian history before taking on the story of the relocation of the Koschek father and daughter to a wind-scow sheep ranch in the middle of Australia. It\'s not spelled out that the Koscheks have fled Communist tyranny, but we can connect the dots. When the narrator asks how young Anna will get along without any other children around for hundreds of miles, the mini-drama shows a radio being used to conduct long-distance schooling. Good luck, kid.\nSouth Seas Adventure has been called the most attractive of the Cinerama presentations, mainly because of the photogenic subject matter. The color is in better shape, and lovers of tropical paradise imagery will feel quite at home. The film delivers the pre-existing fantasy of the South Seas, while adding an unusual item here and there. One chaotic shot of dozens of kangaroos leaping past the camera is especially fun. The most dramatic attraction is the scene with the natives doing their death-defying proto- bungee jumps off those towers. At least one of them seems to hit the dirt when there is still slack in the vines tied to his ankles... but he gets back up on his own. This scene made an indelible impression on children back in \'58. 1\nAdding immeasurably to the experience is Alex North\'s music score, along with the mysterioso choral effects arranged by Norman Luboff. Other songs of the region are sourced in the Hit Parade of the 1920s and \'30s. The extras include an interview with the daughter of producer Carl Dudley and another with one of the production staff, and Carl Dudley\'s promotional film about South Seas Adventure. A full commentary is present, with historian David Coles and Ramine Seaman, the actress-dancer seen in the movie. And another restored short subject in full Cinerama (and Smilebox) is a promotional item for the Renault Dauphine car, showing the workings of the assembly line.\nBoth discs come with restoration docus that are nothing less than fascinating. They start with a brief explanation of the Cinerama process, and then delve into the problems of reconstructing the three panels for each picture, transferring them to video and aligning the inter-panel blends. That last task seems a daunting one; the Image Trends company was responsible for creative transfer engineering and software to digitally manipulate the three panels. David Strohmaier explains how they tried to compensate for uneven density that produces hot spots. They corrected as much as possible the distortions of the process -- remember, each panel is a separate exposure with a separate wide-angle lens that exhibits a slight barrel distortion. The Smilebox format helps make some of these mismatches look better. The alignment of the ""joins"" on the digital video is better than what projected Cinerama was able to achieve.\nFlicker Alley\'s (separate release) Deluxe Combo Blu-ray + DVD Editions of Cinerama Holiday & Cinerama South Seas Adventure are impressive collector\'s items, perfect for owners of large monitors or projectors to show guests something truly exotic. The restored color looks good on Holiday and very good on South Seas Adventure, even when sections of the films had to be recovered from less than optimal sources.\nEach title comes with a Blu-ray and DVD disc, both of which carry all the extras. Also included are printed reproductions of the original Road Show souvenir programs.\nOn a scale of Excellent, Good, Fair, and Poor,\nCinerama Holiday & Cinerama South Seas Adventure Blu-ray rates:\n1. My mother took me to South Seas Adventure in downtown Honolulu probably in early 1959 -- these shows routinely played a solid year. I remembered most of the show in vague terms, but the vine-jumpers blew my little eight-year-old mind. Those had to be the bravest men on the planet. How could I prove that I was brave? Is that how one becomes a man, by doing something incredibly dangerous?\nReviews on the Savant main site have additional credits information and are often updated and annotated with reader input and graphics.\nAlso, don\'t forget the 2011 Savant Wish List.\nT\'was Ever Thus.', ""Australia’s National Film and Sound Archive restores record of ‘The Greatest Show on Earth’\nIt's just over a year since the National Film and Sound Archive (NFSA) took delivery of the first Scanity in Australia. As we celebrate our 30th anniversary, we look back on the first 12 months of operation. Prior to procurement, we conducted extensive market-testing in 2013 and 2014, with visits to labs and factories in US and EU, to talk with engineers and clients, and to test some of our footage on different film-scanners. The Scanity handles some of the most shrunken and damaged historical film, while producing excellent image quality at higher speeds than other machines.\nThe NFSA has more than 91,600 16-mm films in the collection. We need to scan film at high-enough rates to progress through a sufficient amount of footage each year. The Scanity provides real-time 2k scanning at 24-25 frames per second, and allows us to scan optical or magnetic soundtracks in one pass. A significant percentage of our films are sole-surviving composite prints, rather than earlier production elements, although we hold a number of original negatives, camera-reversal and intermediates. We have been experimenting with Log and Linear scanning of high-contrast film stocks, and we are interested in testing the High Dynamic Range options developed by DFT.\nOne of the first projects involved scanning the Wirth Family Home Movies collection, approximately fifty 16mm films, mostly donated by family members. In 1882, the Wirth family, of German origin, launched what became Australia's largest, most prestigious circus company. For eight decades Wirths was Australia’s ‘Greatest Show on Earth’, a large travelling circus of international standard and reputation. The circus toured Australia extensively and also embarked on world tours. It was disbanded only in 1963.\nThe Wirth family's films document their lifestyle – the grand homes, overseas travel, family, friends and parties. Circus footage is the highlight of the collection, with performances, animals, big-top tents and behind-the-scenes circus life. The films were recorded from 1925, just after the introduction of 16-mm film, to around 1950. It is believed that most of the films were taken by George Wirth (one of the Circus' founders) and his wife Margaret.\nThe Collection contains a large amount of high-contrast black-and-white, and later colour-reversal, mostly un-spliced shot-edits in camera. As with most films from this era, we encountered plenty of shrinkage and other damage, although the Scanity handles these smoothly with good focus and registration. We've discussed whether the new wet-gate option on the Scanity would be useful for reducing blemishes from embedded dirt. The NFSA catalogue may be searched for information on the Wirth family and the National Collection at https://www.nfsa.gov.au/collection/search-collection/\nThe NFSA continues to print and process black & white 35-mm film, but hopes to progress to scanning 35-mm colour film in the near future. We have a significant challenge ahead in restoring colour film from the early 1970s, when the Australian film industry underwent significant re-invigoration. Many of these film elements are now around 40 years old, and hundreds of these films have undergone significant dye-fade, shrinkage and other problems, prior to being deposited into the NFSA's cool dry storage repositories. In some cases, the deterioration is so severe that conventional photo-mechanical and photo-chemical processing is insufficient to restore the quality of the original images. A small number of low-budget or limited-release titles survive only on 16-mm, but the majority are on 35-mm.\nThe NFSA preserves more than 240,000 films, within a collection of more than 2 million items including audio, video, documents and artefacts. We present screenings, footage-sales, and content for broadcast and online delivery around the world.\nThe installation of the NFSA scanner was handled by Future Reality, which represents DFT in Australian and New Zealand. As a supplier of creative technology systems, software and services for the local film, broadcast television and post-production industries, Future Reality was able to undertake the complete integration of the Scanity system with the NFSA’s new and existing infrastructure.\n“Preserving Australia’s film history is a vital part of our country’s culture and we are proud to have been involved with the NFSA and DFT on this important project,” said David Edgar, Managing Director of Future Reality.""]"	['<urn:uuid:9c07d5e3-a4c9-426e-9766-fd5652e9c8c1>', '<urn:uuid:5bc1291e-5acf-45fc-904e-4db5efb3908b>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	8	76	3009
79	How many grams of protein can someone get from consuming a relatively small portion of peanuts as a quick snack?	One ounce of roasted peanuts contains 7g of protein.	"['How Much Protein Do You Need?\nProtein is having a moment. From Atkins to Paleo, it seems like everyone is eating a high protein diet. Even JetBlue is offering cricket-yes, cricket!-protein bars to airline passengers. Whether or not you eat a high-protein diet (or an insect snack mid-flight), chances are you\'re concerned about your intake of this important nutrient. Are you eating enough? Eating too much? Here\'s the skinny:\nAre You Getting Enough Protein?\n""In a generally healthy diet, about 10 to 20 percent of your total calories should be coming from protein,"" says Alison Massey, MS, RD, Director of Diabetes Education at The Center for Endocrinology at Mercy Medical Center in Baltimore.\nIndividuals with certain medical conditions or specific needs may need more or less protein, but, according to the Centers for Disease Control and Prevention, general recommendations for the average person in grams (g) per day are:\nMen ages 19 years and older: 56g\nWomen 19 years and older: 46g\nPregnant or nursing teenagers and women: 71g\nChildren ages 1-3 years: 13g\nChildren ages 4-7 years: 19g\nChildren ages 9-13 years: 34g\nBoys ages 14-18 years: 52g\nGirls ages 14-18 years: 46g\nWhile athletes may benefit from increased protein intake pre- and post-workout, the majority of us (even regular exercisers) are fine with the above guidelines.\nThe Problem With too Much Protein\nThe good news about protein intake is that most of us get enough-even those of us who follow a vegetarian or vegan diet. Plant-based proteins such as beans, nuts, and tofu help to create a balanced diet. The key word here being balanced, which brings us to the problem with high protein diets..\n""When you eat a high protein diet and exclude carbohydrates-which is your body\'s main source of fuel-your body begins to burn its own fat for energy,"" explains Heidi McIndoo, MS, RD, author of The Complete Idiot\'s Guide to 200-300-400 Calorie Meals. ""While that may sound good, it actually leads to a condition called ketosis. Ketosis may help reduce your appetite, but it also increases your fluid loss, making any initial weight loss often just a loss of fluids."" In addition, McIndoo cautions that when you lose large amounts of fluids, you may also lose the essential nutrients in the fluids.\nIf you are considering a high protein diet, McIndoo recommends you increase your protein intake slightly, while reducing-but not eliminating-your carbohydrate intake. Your carbohydrates (eaten at each meal and snack) should consist of whole grains, fruits, and vegetables, instead of highly processed carbs like sugar foods, cakes, pastries, candy, and sodas and other sugary drinks. And when it comes to choosing protein, opt for healthier, lower-fat sources, such as sirloin steak instead of a burger.\nThe Healthiest Sources of Protein\nThere are lots of options for incorporating healthy sources of protein into the diet. The healthiest sources are plant-based ones, followed by small amounts of seafood, poultry, and low fat dairy products.\nGood plant-based sources of protein include:\nLentils, with 18g in 1 cup.\nBeans, with 13g in 1 cup of canned baked beans.\nNuts, with 7g in 1 oz. of roasted peanuts\nSeeds, with 6g in ¼ cup.\nTofu, with 6g in 1 slice.\nNut butters, with 8g in 2 tablespoons.\nGood lean animal sources of protein include:\nFish, with 19g in 3 oz. of cooked salmon.\nLow fat Greek-style yogurt, with 8g in a 6 oz. container.\nLow fat cottage cheese, with 15g per ½ cup.\nSkinless white meat chicken and turkey, with 26.3g in 3 oz.\nBeef (lean meats such as eye round, top round, bottom round, top loin, sirloin), with 21g per 3 oz. serving.\n""Protein helps build and repair tissue in your body-bone, cartilage, muscle, skin, and more,"" says McIndoo. ""If you eat too little protein your body may not be able to heal itself as quickly from injury, or you may be more susceptible to illness. You may also have problems with other body functions for which protein is needed to manufacturer necessary enzymes and hormones."" Unless you have a medical condition that requires a lower protein intake, you should stick to the Centers for Disease Control and Prevention\'s recommendations.']"	['<urn:uuid:c6cc7426-1a76-433e-9ef9-80efccb2a408>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	20	9	693
80	basic concepts computational electromagnetics vs basic concepts radar systems engineering compare tutorial learning topics	Both fields offer comprehensive tutorials but cover different fundamental concepts. Computational electromagnetics (CEM) focuses on Maxwell's Equations, surface equivalence principle, and basic antenna theory concepts like gain/directivity and apertures. Students learn various computational methods including FEM, MOM, FDTD, and hybrid methods. In contrast, radar systems engineering covers radar ranging principles, noise in receiving systems, CW radar, Doppler effects, and radio wave propagation. While CEM emphasizes simulation and modeling techniques, radar systems engineering concentrates on practical system design, clutter processing, and waveform analysis. Both fields require hands-on experience - CEM through FEKO software examples, and radar systems through practical system design exercises.	['Interested in attending? Have a suggestion about running this course near you?\nRegister your interest now\nWant to run this event on-site? Enquire about running this event in-house\nThis 2-day course teaches the basics of CEM with application examples. Fundamental concepts in the solution of EM radiation and scattering problems are presented. Emphasis is on applying computational methods to practical applications. You will develop a working knowledge of popular methods such as the FEM, MOM, FDTD, FIT, and TLM including asymptotic and hybrid methods. Students will then be able to identify the most relevant CEM method for various applications, avoid common user pitfalls, understand model validation and correctly interpret results. Students are encouraged to bring their laptop to work examples using the provided FEKO Lite code. You will learn the importance of model development and meshing, post- processing for scientific visualization and presentation of results. Participants will receive a complete set of notes, a copy of FEKO.We recommend you obtain a copy of the textbook Computational Electromagnetics for RF and Mircrowave Engineering by David B. Davidson but it is not required.\nWhat You Will Learn:\n- A review of electromagnetics and antennas with modern applications.\n- An overview of popular CEM methods with commercial codes as examples\n- Hands-on experience with FEKO Lite to demonstrate modeling guidelines and common pitfalls.\n- An understanding of the latest developments in CEM methods and High Performance Computing.\n- From this course you will obtain the knowledge to become a more expert user, use the best code for specific applications, interact meaningfully with colleagues, evaluate accuracy for practical applications, and understand the literature.\n- Maxwell’s Equations. Surface Equivalence Principle, Duality and Huygens Principle.\n- Basic Concepts in Antenna Theory. Gain/Directivity, apertures, reciprocity.\n- Basic Concepts in Scattering Theory. Radar cross section frequency dependence.\n- Antenna Systems. Various antenna types, array antennas, periodic structures and electromagnetic symmetry, and beam steering.\n- Overview of Computational Methods in Electromagnetics. Introduction to frequency and time domain methods. Compare and contrast differential/ volume and surface/integral methods with popular commercial codes as examples (adjusted to class interests).\n- Finite Element Method Tutorial. Mathematical basis and algorithms with application to electromagnetics (adjusted to class mathematical background). Orbital debris.\n- Method of Moments Tutorial. Mathematical basis and algorithms (adjusted to class mathematical background). Implementation and examples using FEKO Lite.\n- Finite Difference Time Domain Tutorial. Mathematical basis and algorithm implementations (adjusted to class mathematical background).\n- Transmission Line Matrix Method. Overview and algorithms.\n- Finite Integration Technique. Overview.\n- Asymptotic Methods. Scattering mechanisms and high frequency approximations.\n- Hybrid Methods. Overview and FEKO examples.\n- High Performance Computing. Overview of parallel methods and examples.\n- Summary. With emphasis on practical applications and intelligent decision making.\n- Questions and FEKO examples. Adjusted to class problems of interest.\nDr. Keefe Coburn is a senior design engineer with the U.S. Army Research Laboratory in Adelphi MD. He has a Bachelor’s degree in Physics from the VA Polytechnic Institute with Masters and Doctoral Degrees from the George Washington University. In his job at the Army Research Lab, he applies CEM tools for antenna design, system integration and system performance analysis. He teaches graduate courses at the Catholic University of America in antenna and remote sensing. He is a member of the IEEE, the Applied Computational Electromagnetics Society, the Union of Radio Scientists and Sigma Xi. He serves on the Configuration Control Board for the Army developed GEMACS code and the ACES Board of Directors.\nREGISTRATION: There is no obligation or payment required to enter the Registration for an actively scheduled course. We understand that you may need approvals but please register as early as possible or contact us so we know of your interest in this course offering.\nSCHEDULING: If this course is not on the current schedule of open enrollment courses and you are interested in attending this or another course as an open enrollment, please contact us at (410)956-8805 or firstname.lastname@example.org. Please indicate the course name, number of students who wish to participate. and a preferred time frame. ATI typically schedules open enrollment courses with a 3-5 month lead-time. To express your interest in an open enrollment course not on our current schedule, please email us at email@example.com.\nFor on-site pricing, you can use the request an on-site quote form, call us at (410)956-8805, or email us at firstname.lastname@example.org.\n|Statistics With Excel Examples - Fundamentals||Register interest|\n|Data Visualization Using R||Register interest|\n|Vibration Testing of Small Satellites (VTSS)||14 Nov|\n|Design for EMC||Register interest|', 'Radar Systems Design and Engineering Training\n|Commitment||4 days, 7-8 hours a day.|\n|User Ratings||Average User Rating 4.8 See what learners said|\n|Delivery Options||Instructor-Led Onsite, Online, and Classroom Live|\nThis Radar Systems Design and Engineering Training covers radar functionality, architecture, and performance. Fundamental radar issues such as transmitter stability, antenna pattern, clutter, jamming, propagation, target cross section, dynamic range, receiver noise, receiver architecture, waveforms, processing, and target detection are treated in detail within the unifying context of the radar range equation and examined within the contexts of surface and airborne radar platforms and their respective applications.\nAdvanced topics such as pulse compression, electronically steered arrays, and active phased arrays are covered, together with the related issues of failure compensation and autocalibration. The fundamentals of multi-target tracking principles are covered, and detailed examples of surface and airborne radars are presented. This Radar Systems Design and Engineering Training course is designed for engineers and engineering managers who wish to understand how surface and airborne radar systems work, and to familiarize themselves with pertinent design issues and the current technological frontiers.\nRadar Systems Design and Engineering Training cover the following topics:\n- Radar Systems Design and Engineering: Introduction. Radar systems examples\n- Radar Systems Design and Engineering: CW Radar, Doppler, and Receiver Architecture\n- Radar Systems Design and Engineering: Radio Waves Propagation\n- Radar Systems Design and Engineering: Radar Clutter and Detection in Clutter\n- Radar Systems Design and Engineering: Electronically Scanned Radar Systems\n- And More…\n- 4 days of Radar Systems Design and Engineering Training with an expert instructor\n- Radar Systems Design and Engineering Training Course Guide\n- Certificate of Completion\n- 100% Satisfaction Guarantee\n- Radar Systems Design and Engineering Training – https://www.wiley.com/\n- Radar Systems Design and Engineering Training – https://www.packtpub.com/\n- Radar Systems Design and Engineering Training – https://store.logicaloperations.com/\n- Radar Systems Design and Engineering Training – https://us.artechhouse.com/\n- Radar Systems Design and Engineering Training – https://www.amazon.com/\nUpon completing this Radar Systems Design and Engineering course, learners will be able to meet these objectives:\n- What are radar subsystems?\n- How to calculate radar performance.\n- Key functions, issues, and requirements.\n- How different requirements make radars different.\n- Operating in different modes & environments.\n- ESA and AESA radars: what are these technologies, how do they work, what drives them, and what new issues do they bring?\n- Issues unique to multifunction, phased array, and radars.\n- State-of-the-art waveforms and waveform processing.\n- How airborne radars differ from surface radars.\n- Today’s requirements, technologies & designs.\n- We can adapt this Radar Systems Design and Engineering Training course to your group’s background and work requirements at little to no added cost.\n- If you are familiar with some aspects of this Radar Systems Design and Engineering course, we can omit or shorten their discussion.\n- We can adjust the emphasis placed on the various topics or build the Radar Systems Design and Engineering Training around the mix of technologies of interest to you (including technologies other than those included in this outline).\n- If your background is nontechnical, we can exclude the more technical topics, include the topics that may be of special interest to you (e.g., as a manager or policy-maker), and present the Radar Systems Design and Engineering course in a manner understandable to lay audiences.\nThe target audience for this Radar Systems Design and Engineering course:\n- Technical managers\n- Logistics and support\nRadar Systems Design and Engineering Training\nThe knowledge and skills that a learner must have before attending this Radar Systems Design and Engineering course are:\n- Basic technical knowledge\nPART I: RADAR AND PHENOMENOLOGY FUNDAMENTALS\n- Introduction. Radar systems examples. Radar ranging principles, frequencies, architecture, measurements, displays, and parameters. Radar range equation; radar waveforms; antenna patterns, types, and parameters.\n- Noise in Receiving Systems and Detection Principles. Noise sources; statistical properties. Radar range equation; false alarm and detection probability; and pulse integration schemes. Radar cross-section; stealth; fluctuating targets; stochastic models; detection of fluctuating targets.\n- CW Radar, Doppler, and Receiver Architecture. Basic properties; CW and high PRF relationships; dynamic range, stability; isolation requirements, techniques, and devices; superheterodyne receivers; in-phase and quadrature receivers; signal spectrum; spectral broadening; matched filtering; Doppler filtering; Spectral modulation; CW ranging; and measurement accuracy. Radar Systems Design and Engineering Training\n- Radio Waves Propagation. The pattern propagation factor; interference (multipath,) and diffraction; refraction; standard refractivity; the 4/3 Earth approximation; sub-refractivity; super refractivity; trapping; propagation ducts; littoral propagation; propagation modeling; attenuation.\n- Radar Clutter and Detection in Clutter. Volume, surface, and discrete clutter, deleterious clutter effects on radar performance, clutter characteristics, effects of platform velocity, distributed sea clutter, and sea spikes, terrain clutter, grazing angle vs. depression angle characterization, volume clutter, birds, Constant False Alarm Rate (CFAR) thresholding, editing CFAR, and Clutter Maps.\nPART 2: CLUTTER PROCESSING, WAVEFORM, AND WAVEFORM PROCESSING\n- Clutter Filtering Principles. Signal-to-clutter ratio; signal and clutter separation techniques; range and Doppler techniques; principles of filtering; transmitter stability and filtering; pulse Doppler and MTI; MTD; blind speeds and blind ranges; staggered MTI; analog and digital filtering; notch shaping; gains and losses. Performance measures: clutter attenuation, improvement factor, subcluster visibility, and cancellation ratio. Improvement factor limitation sources; stability noise sources; composite errors; types of MTI.\n- Radar Waveforms. The time-bandwidth concept. Pulse compression; Performance measures; Code families; Matched and mismatched filters. Optimal codes and code families: multiple constraints. Performance in the time and frequency domains; Mismatched filters and their applications; Orthogonal and quasi-orthogonal codes; Multiple-Input- Multiple-Output (MIMO) radar; MIMO waveforms and MIMO antenna patterns. Radar Systems Design and Engineering Training\nPART 3: ESA, AESA, AND RELATED TOPICS\n- Electronically Scanned Radar Systems. Fundamental concepts, directivity and gain, elements and arrays, near and far field radiation, element factor and array factor, illumination function and Fourier transform relations, beamwidth approximations, array tapers, and sidelobes, electrical dimension, and errors, array bandwidth, steering mechanisms, grating lobes, phase monopulse, beam broadening, examples.\n- Active Phased Array Radar Systems. What are solid-state active arrays (SSAA), what advantages do they provide, emerging requirements that call for SSAA (or AESA), SSAA issues at T/R module, array, and system levels, digital arrays, and future direction?\n- Multiple Simultaneous Beams. Why multiple beams, independently steered beams vs. clustered beams, alternative organization of clustered beams and their implications, quantization lobes in clustered beams arrangements, and design options to mitigate them.\nPART 4: APPLICATIONS\n- Surface Radar. Principal functions and characteristics, nearness and extent of clutter, effects of anomalous propagation, the stressing factors of dynamic range, signal stability, time, and coverage requirements, transportation requirements and their implications, sensitivity time control in classical radar, the increasing role of bird/angel clutter and its effects on radar design, firm track initiation and the scan-back mechanism, antenna pattern techniques used to obtain partial relief.\n- Airborne Radar. Frequency selection; Platform motion effects; iso-ranges and iso-Dopplers; antenna pattern effects; clutter; reflection point; altitude line. The role of medium and high PRFs in lookdown modes; the three PRF regimes; range and Doppler ambiguities; velocity search modes, TACCAR and DPCA.)\n- Synthetic Aperture Radar. Principles of high resolution, radar vs. optical imaging, real vs. synthetic aperture, real beam limitations, simultaneous vs. sequential operation, derivations of focused array resolution, unfocused arrays, motion compensation, range-gate drifting, synthetic aperture modes: real-beam mapping, strip mapping, and spotlighting, waveform restrictions, processing throughputs, synthetic aperture ‘monopulse’ concepts.\n- Multiple Target Tracking. Definition of Basic terms. Track Initiation: Methodology for initiating new tracks; Recursive and batch algorithms; Sizing of gates for track initiation. M out of N processing. State Estimation & Filtering: Basic filtering theory. Least-squares filter and Kalman filter. Adaptive filtering and multiple model methods. Use of suboptimal filters such as table look-up and constant gain. Correlation & Association: Correlation tests and gates; Association algorithms; Probabilistic data association and multiple hypothesis algorithms.']	['<urn:uuid:de794c9b-5b6c-4388-9f20-bab202090b3a>', '<urn:uuid:2dcd9187-4e5e-45a1-b201-c410952b8a47>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:43:46.574121	14	101	2038
81	studying nobelist marie sklodowska curie educational path challenges university education physics chemistry paris	After moving to Paris, Marie Sklodowska-Curie studied chemistry, mathematics and physics at the Sorbonne, Paris's most prestigious university. Despite the challenge of learning French, as courses were taught in French, she finished as the top student in her master's physics degree course. In 1884, she also obtained a master's degree in chemistry. After returning to Paris when unable to find work in Poland, she began a PhD in physics, which she completed in 1903.	['“I am among those who think that science has great beauty.”\nAccording to the analysis of senior university staff in Europe, men are three times more likely than women to reach senior levels. However, the participation of women in science and technology can and has contributed to increasing innovation, quality and competitiveness of scientific and industrial research and needs to be promoted.1 One of the brightest and most inspiring people, who pushed the borders of the existing science was Marie Skłodowska-Curie, the first woman to be awarded the Nobel prize and the only person who has ever won Nobel Prizes in both physics and chemistry. She discovered two new chemical elements – radium and polonium and carried out the first research into the treatment of tumours with radiation.2\nMarie Skłodowska-Curie was born in 1867 in Warsaw, Poland.3 Since within her childhood, Marie was recognised for her remarkable prodigious memory and she graduated in 1883 from high school with a gold medal as the top student. However, her father did not have the possibility to financially support her idea to go to university and moreover higher education was not available for girls in Poland. Therefore, she took work as a teacher and from her earnings, she helped her sister Bronia to complete her medical studies in Paris. In 1891, Marie followed her sister to Paris where she studied chemistry, mathematics and physics at the Sorbonne, Paris’s most prestigious university. One of the challenges she faced was, of course, learning French, since the courses were taught in French.2 Almost two years later, Marie finished as the top student in master’s physics degree course. Being extremely curious and willing to always study, in 1884 she also obtained a master’s degree in chemistry. Then she attempted to find a job in Poland, but again as being a woman she could not get a position at the university. Therefore, she decided to return to Paris and begin a PhD in physics.\n“Unknown in Paris, I was lost in the great city, but the feeling of living there alone, taking care of myself without any aid, did not at all depress me. If sometimes I felt lonesome, my usual state of mind was one of calm and great moral satisfaction.”\nIn 1895 (aged 28) she married Pierre Curie, who had been completed a PhD in physics and had become a professor. During her PhD, Marie began to investigate uranium as a new and exciting area to work in. Marie and Pierre discovered polonium, which was named after Marie’s homeland. In 1903, Marie completed her PhD and, in the same year, she was awarded the Nobel Prize in Physics. She shared the prize with Pierre Curie and Henri Becquerel. The births of Marie’s two daughters, Irene and Eve, in 1897 and 1904 did not interrupt her scientific work.\nAfter the death of her husband Pierre Curie in 1906, she had become the Chair of Physics at the Sorbonne and after so many years, she became the first female professor at the University of Paris.\n“I have frequently been questioned, especially by women, of how I could reconcile family life with a scientific career. Well, it has not been easy.”\nIn 1911, she was awarded the Nobel Prize for Chemistry for the discovery of the elements radium and polonium, the isolation of radium and the study its nature and compounds.\nDuring World War 1, Marie set up radiology medical units near battle lines to allow X-rays to be taken of wounded soldiers.\n“We must not forget that when radium was discovered no one knew that it would prove useful in hospitals. The work was one of pure science. And this is a proof that scientific work must not be considered from the point of view of the direct usefulness of it. It must be done for itself, for the beauty of science, and then there is always the chance that a scientific discovery may become like the radium a benefit for humanity.”\nShe travelled widely to talk about science. In addition, she had the satisfaction of seeing the development of the Curie Foundation in Paris and the inauguration in 1932 in Warsaw of the Radium Institute, where her sister Bronia became the director.4\nMarie died aged 66 of leukaemia, most likely developed from the radioactivity she had been exposed to during her career. Even her books and papers are so radioactive that they are now stored in lead boxes.\nToday, Marie Skłodowska-Curie actions (MSCA) support researchers at all stages of their careers, irrespective of nationality. The fellowship program – first named Marie Curie Actions – was created by the European Union/European Commission to support research and to reveal new pathways for many brilliant minds who are passionate about science.\n“After all, science is essentially international, and it is only through lack of the historical sense that national qualities have been attributed to it.”\nThis article appeared first on www.etn.redmud.org website.']	['<urn:uuid:88fdd8a6-cc42-45e1-bce6-ecd36a861253>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	13	74	822
82	what percentage sick people get cured with new mental health therapy compared to cbt depression treatment	According to a study conducted in Denmark, 74% of patients with depression recovered when treated with Metacognitive Therapy (MCT), compared to 52% who were treated with Cognitive Behavioural Therapy (CBT).	"[""- Metacognitive therapy (MCT) has proved more effective than current treatments for treating anxiety and depression.\n- The new therapy has reduced session times and improved outcomes for patients with long-term symptoms.\n- MCT has been widely adopted into clinical services in the UK and abroad.\nMental health treatments falling short for patients\nGlobally, one in six adults experiences a mental health problem, such as anxiety or depression, in their lifetime. People with these conditions suffer a poorer quality of life and can be at risk of other health problems, including heart disease and reduced life expectancy. Effective mental health treatments are needed urgently.\nProfessor Adrian Wells\nAdrian Wells is Professor of Clinical and Experimental Psychopathology at The University of Manchester.\nSupporting mental wellbeing through gold-standard research\nTreatments for psychological disorders were not keeping pace with advances in cognitive science and theory. Cognitive Behavioural Therapy (CBT) was viewed as a benchmark for treating anxiety and depression. However, only 50% of patients who have a chronic mental health problem improved or recovered using the technique.\nAdrian Wells, Professor of Clinical and Experimental Psychopathology, and colleagues at The University of Manchester sought to improve patient recovery rates. Through their research, they discovered Cognitive Attentional Syndrome (CAS) – a central set of thinking processes common to psychological disorders. The CAS is made up of difficult-to-control, extended negative thinking, often in the form of worry, rumination and threat monitoring. A breakthrough in this discovery was that metacognition (the ability to think about and regulate one’s thoughts) plays a major role in controlling CAS.\nFrom this, Wells developed a new form of psychotherapy called Metacognitive Therapy (MCT). With it, he conducted clinical trials covering a number of disorders such as social anxiety, depression, obsessive compulsive disorder, post-traumatic stress, generalised anxiety, and mental health problems experienced by patients who have cardiovascular disease.\nThe development of MCT has been described by the scientific community as a model for the progression of psychotherapy. Patients undergoing the treatment become more self-aware and changed their way of thinking in response to stress, to alleviate the unhelpful thought patterns that prolong worry and worsen their symptoms.\n“I am fascinated to see how rapidly patients get better. I have learnt great skills from metacognitive therapy training that will guide my therapy.”\nDr Anil Gündüz\nMetacognitive Therapist in Turkey\nMajor advances in patient treatments and recovery\nMCT has also resulted in faster treatment outcomes for patients, as they require fewer and shorter sessions compared to other therapies. It has proven to be more effective than existing treatments for anxiety and depression, such as CBT. A Manchester-led study conducted in Denmark showed that 74% of patients with depression recovered when treated with MCT, compared to 52% treated with CBT.\nThe study PATHWAY, funded by the National Institute of Health Research (NIHR) and conducted by Manchester (2014–2021), in collaboration with Greater Manchester Mental Health NHS Trust, also showed that adding MCT to cardiac rehabilitation in patients with heart disease significantly improved symptoms of anxiety and depression.\nWith conditions such as generalised anxiety disorder, MCT has improved outcomes for patients with long-term symptoms. So much so that a study, in collaboration with the Norwegian University of Science and Technology, showed that in this case, MCT was more effective than CBT. Furthermore, the advantages of MCT remained present nine years after treatment.\nWidespread adoption into clinical services in the UK and Europe\nFollowing its success, MCT has been integrated into clinical services in the NHS and health services abroad. In Norway, it is the first-choice treatment for patients hospitalised with anxiety and depression, and has even improved return-to-work rates for those on long-term sick leave.\nIn the UK, treatments drawing on MCT principles are recommended in NICE guidelines for social anxiety disorder and generalised anxiety disorder. The social anxiety model recommended in the guidelines is a core competency in UK clinical psychology training programmes and an essential part of the national training programme for IAPT (talking therapy) practitioners each year.\nIn England, approximately 170,000 patients in 2019–2020 had talking therapy treatment for social anxiety or generalised anxiety disorder, with the majority aided by MCT-informed techniques.\nTraining healthcare professionals across the globe\nMCT prompted the establishment of a not-for-profit spin-out company, the Metacognitive Therapy Institute (MCTI), in 2008. MCTI delivers training, dissemination and governance of MCT and currently trains approximately 250 healthcare professionals across 25 countries each year. Therapists trained in MCT have reported a substantial improvement in their clinical practice, as MCT allows them to treat a wide range of mental health disorders with greater success.\nConnect with our experts\nGet access to the research you need from experts who shape lives and improve health outcomes globally.\n- David Reeves, Professor of Biostatistics\n- Sam Cartwright-Hatton, Clinical Senior Lecturer\n- Metacognitive therapy versus cognitive behaviour therapy in adults with major depression: a parallel single-blind randomised trial (open access article)\n- Improving the effectiveness of psychological interventions for depression and anxiety in cardiac rehabilitation: PATHWAY – a single-blind, parallel, randomized, controlled trial of group metacognitive therapy (open access article)\n- Metacognitive therapy versus cognitive-behavioural therapy in adults with generalised anxiety disorder (open access article)\n- How to stop worrying: metacognitive therapy helps this writer learn to leave her anxious thoughts on the 'sushi conveyor belt' (South China Morning Post website)""]"	['<urn:uuid:0a0f3c8b-5ffe-43ad-bfc9-af94d94a7c7b>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	16	30	882
83	how check climbing safety belt damage	To check a climbing harness for damage, you should inspect the waist belt straps, leg loop straps, and shoulder straps for any cuts, swelling, damage, and wear from overuse, heat, or contact with chemicals. Be sure to check areas hidden by buckles. Also examine the safety stitching on both sides for loose, worn, or cut threads. The safety stitching can be identified by thread of a different color than the webbing.	['When was the last time you inspected your climbing harness? Do you know how old it is and if it needs to be retired? As a climber, it is your responsibility to inspect your gear before or after each use to help reduce the inherent risks of climbing. If your harness is showing unexpected degradation, it should be quarantined until a detailed inspection can be done.\nPetzl recommends having your gear inspected every 12 months by a competent person. Before hitting the gym or crag again, conduct a 14-point inspection on your harness to determine if it’s clear for use. You can download the official inspection procedure and inspection form from this site or on the product pages at petzl.com. When inspecting your harness, determine if each element that you inspect is “Good condition”, “To monitor”, “To repair”, “Do not use, retire”, or “Not applicable.”\nKnow the product history\nStep 1: Review the conditions in which the harness has been used and identify any “exceptional events” during its lifespan (examples: falls, exposure to extreme temperatures, modification outside of manufacturer’s facilities, etc).\nMake preliminary observations\nStep 2: Verify the presence and legibility of the serial number and the CE mark. Please note that the serial number on your climbing harness could be one of two styles. See below for details on each type of serial number.\nStep 3: Verify that the maximum product lifetime–10 years–has not been exceeded.\nStep 4: Compare with a new harness to verify there are no modifications or missing elements.\nCheck the condition of the straps\nStep 5: Check the waist belt straps, bias tapes, leg loop straps, and shoulder straps for any cuts, swelling, damage, and wear from overuse, heat, or contact with chemicals. Be sure to check the areas hidden by the buckles.\nStep 6: Check the condition of the safety stitching on both sides. Look for any threads that are loose, worn, or cut. The safety stitching is identified by thread of a different colour than that of the webbing.\nCheck the tie-in points and belay loop\nStep 7: Check the belay loop for any cuts, swelling, damage, and wear due to use, heat, or contact with chemicals.\nStep 8: Check the condition of the protective webbing on the tie-in points for any cuts, swelling, damage, and wear due to use, heat, or contact with chemicals. Certain harnesses (e.g. SIMBA, LUNA, SELENA, ADJAMA, SAMA) have a red wear indicator on the lower tie-in point. Retire your harness if this indicator is visible.\nCheck the condition of the adjustment buckles\nStep 9: Check the condition of the adjustment buckles for any marks, cracks, wear, deformation, corrosion, etc.\nStep 10: Check that the straps are correctly threaded, with no twists.\nStep 11: Verify that the buckles operate properly.\nCheck the comfort parts\nStep 12: Check the condition of the waist and leg foams for any cuts, wear, tears, etc.\nStep 13: Check the condition of the elastic keepers and leg loop elastics for any cuts, wear, tears, etc.\nStep 14: Check the condition of the equipment loops for any cuts, wear, tears, etc.\nIf any element of your harness is marked as “To repair”, you should refrain from using it until any possible repairs can be made. If an element was marked as “Do not use, retire”, you should destroy it to prevent further use and purchase a new one.\nExamples of harnesses that are worn out or should be retired']	['<urn:uuid:bc62a693-6a23-4707-a524-722f8d791c4c>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	6	71	575
84	As someone interested in immigration history, I'd like to know how Irish immigrants contributed to American infrastructure and what aspects of their culture they maintained?	Irish immigrants played a major role in America's industrialization, particularly in providing labor for crucial infrastructure projects like the Erie Canal, where they were involved in everything from planning and design to construction. While working on these projects, they maintained their rich cultural traditions, including music, dance, and religious practices - as evidenced by the artifacts preserved in museums today, such as Irish dancing costumes, musical instruments, and religious items from Catholic orders. The Irish also brought their strong farming traditions with them, contributing to American agriculture while maintaining connections to their homeland's customs and way of life.	"['Currently at the Museum!\nThe Irish and Labor\n“The Irish & Labor” exhibit examines the contributions of the Irish and the Irish America and in the United States as workers and in the struggle for workers\' rights. The Irish played a major role in the industrialization of the United States by providing much of the unskilled labor involved in creating the new American infrastructure. This exhibits explores Irish involvement in canal building, railroads, the domestic field and also examines the Irish and Irish-American experience with child labor, discrimination and prejudice.\nPermanent Exhibits: Irish Immigration to America\nA collection of trunks and various items that Irish immigrants brought with them to America - Irish dancing costumes and shoes; a hurl (the object used to play hurling, like Cu Chulainn. A game that is a little like lacrosse, but without the net!), dancing shoes, uileann pipes and an accordion. There are some panels describing conditions and some references to the Famine. There are also Irish artefacts like turf (peat) and a turf spade as well as a model of an Irish cottage.\nSeveral panels discuss the role of Irish and Irish-American Catholics in founding religious orders and communities, like Nano Nagle and the Presenation Sisters. There are some artefacts from the Sisters of Mercy of the Americas, including a prayer box and jetons, which were used to pray for deceased sisters.\nPREVIOUSLY AT THE MUSEUM!\nAugust: Thirty Years of Greg Montogmery\'s Art at Saratoga and The Irish and Horse Racing: John Morrissey and the Founding of Saratoga\nThe museum commemorated the 149th anniversary of the Saratoga Races with the exhibit ""Across the Board: Thirty Years of Greg Montgomery\'s Art at Saratoga.” Montgomery’s series of Posters for the Travers Stakes race in Saratoga Springs has been the longest running series by a single artist for a single event in racing history. His use of clean colors, dynamic form, and unusual use of white space make his work unparalleled in the field of equestrian, sporting, and poster art. Montgomery also spoke about his work, the process that goes into creating the yearly Saratoga poster, and the evolution of his work at the museum.\nThe Irish and Horse Racing: John Morrissey exhibition tells the story of Irish American trailblazer John Morrissey (1831-78), who was the architect of the first thoroughbred race meet at Saratoga in 1863. This exhibit shows his growth from ruffin to participant in history. By revealing Morrissey’s aspirations, struggles, and successes the exhibit represents the rise from disenfranchised, marginalized immigrant to powerful and influential citizen.\nJune - July: Robert Berry: Ulysses Seen!\nUlysses Seen at the Museum is a comic adaptation of the final chapter (Penelope) of the 1922 edition of James Joyce\'s epic novel, Ulysses. The artist Robert Berry is devoted to using ""the visual aid of the graphic novel"" to ""foster understanding of public domain literary masterworks. ""Ulysses Seen” uses the comic narrative to ""cut through jungles of unfamiliar references"" and to help readers ""appreciate the subtlety and artistry"" of Joyce\'s text.\nThis was complemented by our internationally acclaimed exhibit “Dublin: Then and Now” containing evocative photographs that portray life on the gritty streets of Dublin and in its “docklands” in the 1960s. This stunning exhibit showcased Marvin Koner’s black and white photographs from 1963 which expose a city struggling to modernize and, at the same time, hold on to its culture and traditions. The prints include portraits of dockers at City Quay, a rosary bead seller on the streets of Dublin, a poignant photo of a jarvey driver with pony and carriage used for weddings and funerals, and tenement life. An epilogue of images taken in the summer of 2003 highlights the changes of the last 40 years.\nMarch - April 2018 - Walking with Ireland into the Sun: Women Revolutionaries and the 1916 Rising\nIn commemoration of the 100th anniversary of the Easter Rising, the Irish American Heritage Museum takes a unique look at this seminal event in Ireland’s history by focusing on the role of women during the Easter Rising. This exhibit profiles thirteen women, a number of different organizations, and the progressive nature of Ireland and the Proclamation of the Provisional Government of the Irish Republic. It also examines the state of women’s rights and equality in Ireland throughout the 20th century.\nFebruary 2018 - The Irish Currach\nCurrachs are heritage, culture, history, and athletics all in one. A Currach, a traditional Irish rowing boat, is one of the most Irish symbols familiar to tourists all over the world. Exhibit and related events were hosted in Febrauary 2018 by The Albany Irish Rowing Club.\nIrish and the Erie Canal\nThis exhibit is availiable for loan. Please contact the Museum for more information.\nThe Irish were involved from start to finish, from originally proposing the concept a hundred years before a shovel was even put into the ground, to the routing, to its design, to securing support from elected officials, to the elected officials themselves, to its construction and finally to its navigation and transportation services once it opened\nFrom the early 17th century to the end of the 20th century, approximately seven million Irish men and women came to North America. These immigrants and their descendants made significant changes to the fabric of American life, and like any ethnic group, the experience of the Irish Americans was contingent, not preordained. Therefore the social, religious, and socioeconomic character of Irish America continuously shifted throughout the course of American history. Perhaps no other topic in this interesting history better illustrates Irish America’s changing face than that of the Erie Canal.\nIn fact, the Erie Canal may be seen as a microcosm of Irish America- the ethnic group’s experiences along its route closely paralleling those of the Irish across the nation. And just as it is impossibile to think of American history without the Irish, it is just as hard to think of the Erie Canal withut the Irish and the culture they created. From politicians to surveyors, engineers to contractors, laborers to boatmen, more than any other group the Irish embodied the complete story to the Erie Canal.\nAncient Order of Hibernians Honor Guard Exhibit\nIrish Memories: Sixteen on Sixteen\n(Interviews and family lore about the 1916 uprising)\nContributed by Dr. Margaret Lasch Carroll\nAssociate Professor, Albany College of Pharmacy and Health Sciences\nWhen Sociologist Reginald Byron visited Albany, New York in 1989, he was “struck by its Irishness.” Albany, he wrote, is “one of the most Irish places in America and has been so for a century and a half.” Byron then used Albany as the case study for his book, Irish America, published in 1999.\nPictured at right is Thomas Dongan- 2nd Earl of Limerick.\nThe Irish have been shaping the Albany area since the days of the Dutch. The British Governor, Thomas Dongan was an Irishman and his Dongan Charter contained unprecedented social freedoms for the colony. The Irish formed St. Mary’s in Albany, the second Catholic Church in the state of New York during the early years of the new American Republic; they played a major role in the construction of the Erie Canal; and they changed the demographics of the region in the years of the famine. Michael Nolan was the first Irish mayor of the city of Albany in 1878 and, Times Union publisher, Martin Glynn, the first Irish Catholic governor in 1913. Through the 20th century, the Irish continued to come to the Capital District forming vibrant AOH Halls in Troy, Schenectady, Watervliet, and Albany. The opening of the Irish American Heritage Museum in 1986 stands as testament to the impact the Irish have had in the Capital District, New York State, and, indeed, to the entire county.\nAt left, James Connolly- signer of the Proclamation of the Provisional Government of the Irish Republic in 1916. For portions of time between 1902-1905 he lived in Troy, NY. It is not surprising that the connections between the Irish in the tri-city area and Ireland were fluid and active through the 20th century and remain so today. As part of the centenary commemorations of the Easter Rising of 1916, the Irish American Heritage Museum is posting a column of family connections that sixteen Capital District residents have to the Revolutionary years in Ireland, years that charted the course of Ireland for the future. The memories involve not only 1916, but the Irish War of Independence and the Irish Civil War.\nWe will continue to look for more family memory and stories. If you have a story to tell about your own family’s connection to the Easter Rising in 1916 or the revolutionary decade that followed, please contact us at the museum and share your story with us!\n- Nuala Cummings\n- Jim Flaherty\n- Michael Byron\n- Joe Gallagher\n- Chris McCarthy\n- Brian Manley\n- Mary Ellen Gallagher Lasch\n- Angela Doyle McNerney\n- Helen Carswell\nAdditional Traveling Exhibits Available for Loan\nOver the last two decades, the Museum has developed and produced a number of very successful exhibits.\n- “The Irish and the Erie Canal” In keeping with its mission, the Irish American Heritage Museum presents its newest exhibit “The Irish and the Erie Canal.” This exhibit reveals the historical contributions of the Irish to the planning, designing, engineering, funding and construction of the Canal. This famed achievement transformed early America, and in particular New York City, into a world economic power by linking the Great Lakes and the interior of the young nation to the Atlantic Ocean and the world.\n- “Dublin: Then and Now.” Our internationally acclaimed exhibit of stunning photographs portrays life on the gritty streets of Dublin and in its “docklands” in the late 1950s and early 1960s. It likewise portrays life in Dublin in the same areas during the first decade of the 21st century to reveal the transformation in Irish life over 50 years.\n- “Soldiers Are We: The Irish in Military Service” tells the story of centuries-long service of the Irish to their homelands.\n- “The Irish in Music” chronicles the contributions of the Irish to music in our culture, including writers, composers, performers and entrepreneurs.\n- “Corporate Irish” celebrates the lives of the men and women who rose to powerful positions in the business world and helped shape the economic future of the United States — from the colonial period to many contemporary entrepreneurs.\n- “The Irish and Labor” reveals the contributions of the Irish to the American labor movement.\n- “An Gorta Mór: The Great Hunger” chronicles the tremendous impact of the Irish famine in the mid-19th century.\n- “Fire Upon the Hearth” celebrates the contributions of women of Irish heritage in America.\n- “Go and Preach the Kingdom of God” tells about Irish religious and how they clothed, fed, housed and educated the waves of Irish immigrants that flooded America’s shores.\n- “Presidents of Irish Descent” is the story of our 20 presidents who claim Irish ancestry.\nAll of the Irish American Heritage Museum’s exhibits are available for presentation on a loan basis and are in great demand. In 2010, the Museum presented “Dublin: Then and Now” at the Irish Consulate in New York City, the Commodore Barry Irish Center in Philadelphia, the Gaelic-American Club in Fairfield, CT, and the Waterford (NY) Senior Citizens Center. The same exhibit drew international acclaim in 2006 when the Museum became the first American Museum of its kind to exhibit at Ireland’s National Library in Dublin.\nThe Museum’s various exhibits have been on display in such venues as the Children’s Museum of West Virginia; the Heritage State Park Visitor’s Center, Lawrence, MA; the Kerry County Library, Tralee, Co. Kerry, and the US Embassy in Dublin Ireland; The Margaret Mitchell House, Atlanta, GA; the Milwaukee Public Library, Milwaukee, WI; the Trade Center in St. Paul, MN; the Louisiana State Museum in New Orleans; the Kansas City Public Library, Kansas City, MO; the Hatikvah Holocaust Education and Research Center in Springfield, MA; the Arlington Heights Historical Museum, Arlington Heights, IL; and the St. Petersburg International Museum, St. Petersburg, Florida.', 'The culture of Ireland is well known for a country of such a small size. Having only about 6.3 million citizens, (about the size as Washington State) Ireland has brought many parts of its culture to the world stage.\nThough the Leprechaun might not be the most important part of Irish Culture, it demonstrates how important storytelling and folklore are to their culture. Developing hand-in-hand with their strong tradition of music, the Irish culture boasts a great deal of characters and stories. From the pot-of-gold held by the small Leprechaun to the great warrior Cuchulain to the creation of the Giant’s Causeway, the Irish culture has brought countless stories to life. Many important authors have come from Ireland, including four nobel prize winners: James Joyce, Bram Stoker, Jonathan Swift and Oscar Wilde.\nThis tradition of storytelling and myth can’t be separated from the Irish tradition of music. Using songs as a way to tell a story, or to honor an event in history is a common practice in many cultures, and Ireland is no exception. Traditionally, the instruments involved in their music were bagpipes, whistles, and drums, although today many Irish folk tunes also incorporate new instruments such as the fiddle, the guitar, and others.\nThe songs and stories that represent Ireland show the diversity of the country. Even today, Ireland is a diverse group of people. There are two regions in Ireland: The Republic of Ireland, and Northern Ireland. In each of those regions are many parishes, counties, towns, and languages. In Ireland, there are regions called “Gaeltacht” where Gaelic or “Irish” are spoken, and most people speak English as well.\nFrom region to region, it is easy to see similarities in culture. History, and remembering the past is a very large part of Irish Identity, and so is religion. The most-practiced religion in Ireland is Roman-Catholicism with protestantism also being very widely-practiced.\nAnother important part of Irish culture is their relationship to the land, and to farming. Many of their folk songs will mention the green valleys, flowing rivers, or rocky hillsides, because Ireland is a strong farming community. Many people associate Ireland with the potato, which was introduced to their society in the 1500′s, but their green, rainy climate is the home to a variety of crops, and Irish cheese is among the tastiest in the world.\nWhen not enjoying the garden, many of the Irish take part in or watch sports. Football and rugby are very popular sports, and fans of The Green Army are among the most enthusiastic fans. Many people are fans of a local sport called Hurling, which is a 3,000 year old game of Gaelic origin. It is said to be one of the fastest-paced field games. Players use a Hurley, or stick-like tool, to hit a ball around a field, aiming to get it into the other team’s goal.\nEven though Ireland is a small country without a huge military of excess of wealth, its cultural influence is easy to see today in the United States. St. Patrick’s Day, and Irish holiday, is practiced nearly everywhere. Clovers and Celtic Harps are easily recognized symbols of Irish Culture. From the popularity of Irish-themed pubs and restaurants to modern bands like Thin Lizzy and U2, it is clear that there is something very strong and special about the Irish way of life.']"	['<urn:uuid:d1ee158b-7e0b-4197-acd4-40d4994ca6f7>', '<urn:uuid:d5771cdb-3d84-4a16-8fa9-5ff0a2f59d3c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	25	98	2571
85	What's the main difference between how Indian and American federal courts handle state-level cases when it comes to their organization and structure?	The Indian system has a unified judiciary with the Supreme Court at the apex, while America has a dual court system with separate state and federal courts. In the US, there are 50 state court systems alongside the federal system, with state courts having their own supreme courts and trial courts, whereas India doesn't have provision for separate court systems for states.	"['Though the Indian Constitution establishes a federal structure, it is indeed very difficult to put the Indian Constitution in the category of a true federation. The framers of the Constitution have modified the true nature of Indian federation by incorporating certain non-federal features in it. These are –\n1. Article I of the Constitution describes India as a ‘Union of States’, which implies two things –\n- firstly, it is not the result of an agreement among the States and\n- secondly, the States have no freedom to secede or separate from the Union.\nBesides, the Constitution of the Union and the States is a single framework from which neither can get out and within which they must function. The federation is a union because it is indestructible and helps to maintain the unity of the country.\n2. The Centre appoints the Governors of the States and may take over the administration of the State on the recommendations of the Governor or otherwise. In other words, Governor is the agent of the Centre in the States.\nThe working of Indian federal system clearly reveals that the Governor has acted more as centre’s representative than as the head of the State. This enables the Union government to exercise control over the State administration. The control of the Union over states after the imposition of National Emergency.\n3. The equality of units in a federation is best guaranteed by their equal representation in the Uppers House of the federal legislature (Parliament). However, this is not applicable in case of Indian States. They have unequal representation in the Rajya Sabha.\nIn a true federation such as that of United State of America every State irrespective of their size in terms of area or population it sends two representatives in the upper House i.e. Senate.\n4. All important appointments such as the Chief Election Commissioner, the Comptroller and Auditor General are made by the Union Government.\n5. There is single citizenship.\n6. There is no provision for separate Constitutions for the states. The States cannot propose amendments to, the Constitution. As such amendments can only be made by the Union Parliament.\n7. In order to ensure uniformity of the administrative system and to maintain minimum common administrative standards without impairing the federal system. All India Services such as IAS and IPS have been created which are kept under the control of the Union.\n8. In financial matters too, the States depend upon the Union to a great extent. The States do not possess adequate financial resources to meet their requirements. During Financial Emergency, the Center exercises full control over the State’s finances.\n9. In case of disturbances in any State or part thereof, the Union Government is empowered to depute Central Force in the State or to the disturbed part of the State.\n10. The Parliament, by law may increase or decrease the area of any State and may alter its name and boundaries.\n11. The federal principle envisages a dual system of Courts. But, in India we have unified Judiciary with the Supreme Court at the apex.\n12. The Constitution of India establishes a strong Centre by assigning all-important subjects to the Centre as per the Union List. The State Governments have very limited powers. Financially the States are dependent on the Centre.\nThus, there is a tilt in favor of the Centre at the cost of the States. The States have to work in close co-operation with the Centre. This has lent support to the contention that the Indian Constitution is federal in form but unitary in spirit. Constitutional experts have called it ‘semi-federal’ of ‘quasi federal’ system.\nBibliography : NIOS – Political Science\nClick Here for', 'The American Court System\nDual Court System\nStaffing the Federal Judiciary\nBringing Cases before the Federal Judiciary\n1. Dual Court\n50 State Court Systems\nState Supreme Courts\nIntermediate Appeals Courts\nState Trial Courts\nMunicipal Courts and other Courts of\nFederal Court System\nU.S. Supreme Court\nUS Courts of Appeal\n1 US. Court of Appeal for the\n94 US District Courts\nUS Court of International Trade\nUS Territorial Courts in Guam, US Virgin Islands, US\n· U.S. Court of Appeals for the Armed Forces\n· U.S. Court of Federal Claims\n· U.S. Court of Veterans Appeals\n· U.S. Tax Court\n\'Lectric Law Library\'s Legal Lexicon On\nIII Courts *\nARTICLE III COURTS - These are\nfederal courts established by, or under Article III of the U.S. Constitution\nwhich states: \'The judicial Power of the United States, shall be vested in one\nsupreme Court, and in such inferior Courts as the Congress may from time to time\nordain and establish.\'\nThese courts include:\nSupreme Court - One court with national jurisdiction;\nCourts of Appeals - 12 Geographic-based and one for the Federal Circuit;\nDistrict Courts - 94 in 50 states, District of Columbia and Puerto Rico along\nwith their subordinate bankruptcy courts, and;\nCourt of International Trade.\nThe federal courts have power to decide only those cases over which the\nConstitution gives them authority. These courts are located principally in the\nlarger cities. Only carefully selected types of cases may be heard in the\nThe controversies that may be decided in the federal courts are identified in\nArticle III, Section 2 of the Constitution. They include cases in which the\nUnited States government or one of its officers is either suing someone or being\nThe federal courts also may decide cases for which state courts are\ninappropriate or might be suspected of partiality. Thus, federal courts may\ndecide, in the language of the Constitution, \'Controversies between two or more\nstates; between a State and Citizens of another State; between Citizens of\ndifferent States; [or] between Citizens of the same State claiming Lands under\nGrants of different States.\' For example, one state might be sued by another\nstate for the pollution of its air. Since the impartiality of the courts in\neither state could be questioned, such a suit might be decided in a federal\nSimilarly, the Constitution extends the authority of the federal courts to cases\naffecting ambassadors, consuls, and other public ministers. The U.S. government\nalso has constitutional responsibility for U.S. relations with other nations.\nBecause cases involving other nations\' representatives or citizens may affect\nU.S. foreign relations, such cases are decided in the federal courts.\nThe Constitution provides the federal courts the power to hear cases involving\nthe Constitution as a law, laws enacted by Congress, treaties, and laws relating\nto navigable waters (the sea, the Great Lakes, and most rivers) and commerce on\nthem. The federal courts\' jurisdiction also encompasses the many cases that\ninvolve or affect commerce among states.\nThe Constitution describes what cases may be decided in the federal courts.\nCongress may and has determined that some of these cases also may be tried in\nstate courts, giving federal and state courts concurrent jurisdiction. Congress\nhas provided that suits between citizens of different states may be heard in the\nfederal courts or the state courts, but they may be heard in the federal courts\nonly if the amount in controversy exceeds $50,000. Congress also has provided\nthat maritime cases and suits against consuls may be tried only in the federal\ncourts. When a state court decides a case involving federal law, it in a sense\nacts as a federal court, and its decisions on federal law may be reviewed by the\nU.S. Supreme Court.\n\'Lectric Law Library\'s Legal Lexicon On\nI Courts *\nARTICLE I COURTS - These are\ncourts created by Congress under its power under Article I of the Constitution,\nThere are federal courts located in the districts of Guam, the U.S. Virgin\nIslands, and the Northern Mariana Islands. In most instances, these courts\nfunction as U.S. district courts, yet they were created under Article I of the\nConstitution. These courts, called legislative courts to distinguish them from\nConstitutional courts, function similarly to state and local courts as well as\nperforming their federal role. The legislative courts have jurisdiction over\nlocal cases and also those arising under federal law. These courts may also be\ngiven duties by Congress that are not strictly judicial in nature. Unlike the\nother federal courts, they were created by Congress under the article of the\nConstitution that grants Congress authority over the territories and other\nfields of federal power.\nThe judges of the legislative courts are appointed for a term of 10 years. They\nare not protected under the Constitution against salary reduction during their\nterms of office.\nIn addition to the territorial courts, Congress exercised its power under\nArticle I of the Constitution to establish the U.S. Court of Military Appeals,\nthe U.S. Court of Veterans Appeals, the U.S. Court of Federal Claims, and the\nU.S. Tax Court.\nU.S. COURT OF MILITARY APPEALS\nThe U.S. Court of Military Appeals was created by Congress in 1951. At that\ntime, Congress also enacted the Uniform Code of Military Justice, which\nestablished a military judicial system. This system was designed to balance the\nneed to maintain discipline in the armed forces and to give members of the\nmilitary services who are accused of crimes rights paralleling those of accused\npersons in the civilian community.\nThe court\'s jurisdiction is worldwide but encompasses only questions of law\narising from trials by court-martial in the United States Army, Navy, Air Force,\nMarine Corps, and Coast Guard in cases where a death sentence is imposed, where\na case is certified for review by the Judge Advocate General of the accused\'s\nservice, or where the accused, who faces a severe sentence, petitions and shows\ngood cause for further review. Such cases are subject to further review by the\nSupreme Court of the United States. The Supreme Court may also review cases in\nwhich the court grants extraordinary relief. The Supreme Court has jurisdiction\nto review decisions of the military appellate courts in which the United States\nhas taken an appeal from rulings by military judges during trials by\ncourt-martial. In the year ended September 30, 1992, 1,541 cases were filed in\nthe U.S. Court of Military Appeals and 1,380 cases were terminated.\nThe five judges of the Court of Military Appeals are civilians appointed for\n15-year terms by the President with the advice and consent of the Senate. The\nchief judge serves for five years and is succeeded by the next senior judge on\nthe court. The court is located in Washington, D.C.\nU.S. COURT OF VETERANS APPEALS\nThe U.S. Court of Veterans Appeals was created by Congress in 1988 to exercise\nexclusive jurisdiction over the decisions of the Board of Veterans\' Appeals on\nthe motion of claimants. Such cases include all types of veterans\' and\nsurvivors\' benefits, mainly disability benefits, and also loan eligibility and\neducational benefits. In the year ended June 30, 1992, the U.S. Court of\nVeterans Appeals reviewed 1,931 cases and terminated 1,897. Its decisions are\nsubject to limited review by the U.S. Court of Appeals for the Federal Circuit.\nThe court has seven judgeships. The judges of the court are appointed by the\nPresident with the advice and consent of the Senate. The court is based in\nWashington, D.C., but as a national court, it may sit anywhere in the United\nU.S. COURT OF FEDERAL CLAIMS\nThe U.S. Court of Federal Claims, formerly the U.S. Claims Court, was\nestablished in 1982 as the successor to the trial division of the Court of\nClaims, which had been in existence since 1855. The U.S. Court of Federal Claims\nhas nationwide jurisdiction over a variety of cases, including tax refunds,\nfederal taking of private property for public use, constitutional and statutory\nrights of military personnel and their dependents, back-pay demands from civil\nservants claiming unjust dismissal, persons injured by childhood vaccines, and\nfederal government contractors suing for breach of contract. Most suits against\nthe government for money damages in excess of $10,000 must be tried here.\nHowever, the district courts have exclusive jurisdiction over tort claims (a\ncivil wrong or breach of duty) and concurrent jurisdiction over tax refunds.\nThe U.S. Court of Federal Claims also hears appeals of decisions of the Indian\nClaims Commission and has jurisdiction to review certain cases involving federal\ngovernment contractor disputes. Either house of Congress may refer to the chief\njudge a claim for which there is no legal remedy, seeking findings and a\nrecommendation as to whether there is an equitable basis upon which Congress\nitself should compensate the claimant. Review of decisions in the U.S. Court of\nFederal Claims lies in the U.S. Court of Appeals for the Federal Circuit. During\nthe year that ended September 30, 1992, 736 new cases were filed and 989 cases\nwere terminated. On September 30, 1992, 718 cases were pending in the claims\nThe 16 judges of the U.S. Claims Court are appointed for terms of 15 years by\nthe President with the advice and consent of the Senate. The court\'s\nheadquarters are in Washington, D.C., but cases are heard at other locations\nconvenient to the parties involved.\nU.S. TAX COURT\nEstablished by Congress in 1924 under Article I of the Constitution, the U.S.\nTax Court decides controversies between taxpayers and the Internal Revenue\nService involving underpayment of federal income, gift, and estate taxes. Its\ndecisions may be appealed to the federal courts of appeals and are subject to\nthe review of the U.S. Supreme Court on writs of certiorari. In the year that\nended September 30, 1992, 30,345 new cases were filed and 34,823 were closed in\nthe U.S. Tax Court. On September 30, 1992, 44,376 cases were pending.\nThe 19 tax court judges are appointed by the President for terms of 15 years.\nThe judges of the court elect one of their number to serve a two-year term as\nchief judge with responsibility for overall administration of the court in\naddition to a caseload. Retired judges may be recalled by the chief judge for\nservice in the court. In addition, there are currently 17 authorized special\ntrial judges appointed by the chief judge, who serve under rules and regulations\npromulgated by the court.\nThe Tax Court hears cases in approximately 80 cities. Its offices are located in\nThe position of Administrative Law Judge (ALJ), originally called hearing\nexaminer, was created by the Administrative Procedure Act of 1946, Public Law\n79-404. The Act insured fairness and due process in Federal agency rule making\nand adjudication proceedings. It provided those parties, whose affairs are\ncontrolled or regulated by agencies of the Federal Government, an opportunity\nfor a formal hearing on the record before an impartial hearing officer. It also\nprovided for a merit selection system administered by the U.S. Office of\nPersonnel Management and statutory protection of the judge\'s decisional\nindependence from undue agency influence.\nApplicants must be attorneys and have a minimum of seven (7) years\nadministrative law and/or trial experience involving formal administrative\nhearing proceedings before local, State, or Federal administrative agencies,\ncourts, or other administrative bodies. In addition, applicants must demonstrate\nthat they have had 2 years of qualifying experience at a level of difficulty and\nresponsibility characteristic of at least senior level GS-13, or 1 year\ncharacteristic of at least GS-14 or GS-15 Federal Government attorneys actively\ninvolved in administrative law and/or litigation work.\nDuties and Responsibilities\nALJs prepare for and preside at formal hearings which Federal agencies are\nrequired by statute to hold under, or in substantial accord with, provisions of\nthe Administrative Procedure Act, Sections 553-559 of Title 5, United States\nCode. ALJs function as independent, impartial triers of fact in formal hearings\nin a manner similar to that of a trial judge conducting civil trials without a\nRating Criteria and\nAll applicants who meet the minimum qualification and filing requirements will\nbe eligible to participate in subsequent stages of the examination: supplemental\nqualifications statement; written demonstration; panel interview; and personal\nOpportunities for Employment\nThere are approximately 1,400 incumbent ALJs in 29 Federal Government agencies\nat various locations across the continental United States, Hawaii, and Puerto\nRico. Competition for the relatively few positions which may become vacant from\nyear to year is keen. Only very highly qualified applicants whose qualifications\nsubstantially exceed the minimum examination requirements should include such a\nposition in their career plans.\nPublic Law 101-509 established a new ALJ pay system for former GS-15, 16, 17,\nand 18 ALJ positions established under Section 3105 of Title 5, United States\nCode. The minimum rate for ALJ positions is set at 65 percent of level IV of the\nExecutive Schedule and the maximum rate is set at 100 percent of level IV of the\nAS OF: 01/2002\n2. Staffing the Federal Judiciary\nconstitutional courts are created under Article III of the U.S.\nConstitution and all federal judges of constitutional courts (U.S. Supreme Court\nand the inferior courts) are nominated by the President and appointed for life\nterms with the approval of the United States Senate.\nFederal judges have life tenure. They can only be\nremoved through impeachment. They can, however, retire at age seventy.\nThe salaries of constitutional court judges can not be\nreduced during their lifetime. It can be raised.\nand no reduction in pay was put\nin the Constitution to assure for judicial\nindependence. Judges are not to be subject to political\npressure. They are supposed to be politically independent. Judicial\nindependence is an important principle of American government.\njudges do not have the same judicial independence. Their terms are\nlimited, although appointments for 15 years are common. In theory also,\ntheir salaries could be reduced.\nState court judges\nare selected in ways outlined by their state constitutions. Four different\nmethods are used:\n1. appointment by the State\nlegislature without the governor. (Patrician Model)\n2. election by the voters in either\na partisan or non-partisan election (Jacksonian Model)\n3. appointment by the State governor\nwith approval of the State Senate (Reform Model)\n4. Missouri Plan. A\nnon-partisan commission recommends a person to the State governor. The\nGovernor appoints the person for a limited period of time. At the end of\nthat time, the person stands for election. The voters can vote yes or\nno. A ""yes"" vote confirm the judge in office for the rest of his\nMany states limit the number of years that a judge remains on\n3. Bringing Cases before the Federal Judiciary\nCases and controversies may be brought before the Federal court system\ndepending on two principles defined in Article III. These principles\ndepend either on a) the nature of the parties involved or b) whether a Federal\nQuestion is raised.\nNature of the Parties Involved\n- Cases involving the U.S. government or any agency thereof.\n- Cases involving citizens of two different states if the amount of money\ninvolved is at least $10,000\n- Cases involving a U.S. national and a foreign national if the amount of\nmoney involved is at least 10,000\n- Cases involving two U.S. States. This is part of the original\njurisdiction of the U.S. Supreme Court.\n- Cases involving a foreign public official. This is part of the\noriginal jurisdiction of the U.S. Supreme Court. Foreign public\nofficials have diplomatic immunity so for all practical purposes there are\nno such cases. This makes this a null category. In legal\nlanguage, this is moot.\n- Federal court by citizens of another state. The 11th Amendment\ngrants ""sovereign immunity"" to states from this type of case.\nCategories 1 and 2 generate the most cases for the Federal\njudiciary. Most of these cases originate in the U.S. District Courts,\nwhich are the trial courts of the Federal system. We will discuss the\noriginal jurisdiction of the U.S. Supreme Court (categories 4 & 5) in the\nsection on the U.S. Supreme Court.\nFederal questions are raised when during a court proceeding an issue of\nFederal Law is raised. Since Federal law is above conflicting State law,\nstate court judges must apply applicable Federal laws in their State court\nproceedings. If a State court judge makes an error in his or her\ninterpretation of Federal law, the case can be appealed to the highest State\ncourt and from there directly to the U.S. Supreme Court.\nSince more and more State court decisions involve questions of Federal\nlaw, more and more State court decisions are appealed to the U.S. Supreme\nCourt. Thus the U.S. Supreme Court is not only the highest appeals court\nfor the Federal court system, it also enforces uniform standards of Federal law\nwithin the State court systems.']"	['<urn:uuid:81cd78c7-4bef-4acf-83c3-cdab083dd48a>', '<urn:uuid:171f170f-3df3-41cd-aa7a-430d9c12b87e>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T04:43:46.574121	22	62	3398
86	micr code online banking security measures	MICR is a 9-digit code used for check processing and clearing, with digits indicating city (first 3), bank (next 3), and branch (last 3). For online banking security, users should use dedicated computers for banking, install commercial anti-virus software, employ actively managed firewalls, and clear browser cache before banking sessions. Additionally, users should verify secure sessions (https://) and avoid automatic login features.	"['aids in distinctly identifying a certain bank branch\nassists in removing mistakes from the fund transfer procedure.\nutilized for all types of electronic payments, including NEFT, RTGS, and IMPS\nWhat is IFSC Code?\nFor online fund transfers made through NEFT, RTGS, and IMPS, a special 11-digit alphanumeric number called the Indian Financial System Code (IFSC) is utilized. The bank\'s issued check leaf will have the IFSC code on it. The bank receives the IFSC codes from the Reserve Bank of India (RBI). In addition to the cheque leaf, the bank\'s and the RBI\'s official websites both list the IFSC code.\nThe IFSC must be typed in order to start the transfer if you are using online banking to send money. Banks do not alter or update the IFSC code unless there is a merger.\nDifference Between the IFSC Codes for Banks and Credit Cards\nThe bank\'s IFSC code differs from branch to branch in this scenario. However, a certain bank\'s credit card IFSC code will be consistent throughout the nation.\nIn the table below, some of the banks\' credit card IFSC codes are listed:\nPunjab National Bank\nState Bank of India\nMagnetic Ink Character Recognition Code (MICR)\nTo identify a certain bank branch that is a component of the Electronic Clearing System (ECS), which is used to routinely clear checks, Magnetic Ink Character Recognition (MICR) is a 9-digit code. This code is printed on the passbook that is given to the account holder as well as on the cheque leaf that the bank issues.\nThe first three digits of the nine-digit number are used to specify the city, the next three digits are used to specify the bank, and the last three digits are used to specify the bank branch. For instance, ""700002021"" is the MICR code for the SBI branch in Kolkata. Here, the first three digits (700) are used to describe the city, the next three digits (002) are used to specify the bank, and the last three digits (021) are used to specify the bank branch. Cheques are generally processed and cleared by machines using the MICR code. The 9-digit number helps to reduce mistakes in the clearing process, speeds up the procedure, and improves the security and safety of processing checks.\nThe fundamental component of every online interbank money transfer in India is the IFSC code, which also serves as a reliable method of verifying all such transactions. Sending and receiving money online is made simple and quick with the proper understanding of IFSC codes, as was intended.\nOnline resources abound that might assist you in discovering the IFSC code for the specific bank that you are looking for. And let\'s be honest, you are only likely to check this while making an online purchase. In the same spirit, BankBazaar provides a thorough tool to assist you in engaging in a quick and precise IFSC Code search.\nThe detailed process to search for an IFSC code on BankBazaar.com is provided below:\nScroll to the top of this page as you are already there while reading these instructions.\nA straightforward ""IFSC and MICR Codes Directory"" is laid out before you; it is a useful tool for assisting you in finding an IFSC Code as needed. The tool has four fields: 1) Choose Bank, 2) Choose State, 3) Choose District, and 4) Choose Branch.\nPlease choose wisely for the name of the bank, the Indian state where the bank has a branch, the particular district of the state, and ultimately the relevant branch.\nThe page that appears in response to your enquiry provides the bank\'s IFSC Code, MICR Code, official address, and phone number. Within less than 30 seconds of your original question, everything has happened.\nIFSC Code: How it Operates\nTo better comprehend what an IFSC code is and how it functions in financial transactions, let\'s use the IFSC code for Canara Bank as an example. Canara Bank\'s Chandigarh branch\'s IFSC code is CNRB00001995.\nIn this case, CNBR stands for Canara Bank, the name of the bank.\nThe fifth character, a zero (0), is reserved for usage later.\nThe remaining six characters—01995 in particular—help the RBI correctly identify a bank branch.\nLet\'s now examine how the IFSC functions. One must enter the account number and branch-specific IFSC code when initiating a fund transfer to a specific payee. The money is transmitted to the account holder when the remitter submits these data, and IFSC helps prevent any mistakes in such transactions.\nIn addition to financial transfers, IFSC codes may be used to use online banking to buy mutual funds and insurance. The National Clearing Cell of the Reserve Bank of India (RBI) keeps track of all transactions, and the IFSC code enables error-free financial transfers to be carried out by the RBI.\nYou can also find the IFSC code on your checkbook or bank passbook. Additionally, IFSC codes for bank branches may be found in\nLocate the IFSC Code on a bank check:\nIt is required to list the IFSC code on a standard bank check. Different banks will have different IFSC codes. We are showing the placement of the IFSC code on an HDFC Cheque in our sample image.\nFinding the Cheque Number\nPrinted at the bottom of the check in a unique font style with a typewritten typeface. This is mostly utilized for administrative and check tracking reasons.\nIFSC & MICR Code: The Basic Differences You Need to Know\nGiven in the table below are the differences between the IFSC Code and the MICR Code:\nIFSC is a 11-digit alphanumeric number.\nMICR is a 9-digit code.\nIFSC is used to facilitate electronic money transfer between banks that operate in the country.\nMICR is used to make cheque processing simpler and faster.\nIn an IFSC code, the first four characters indicate the name of the bank.\nIn the MICR code, a combination of the fourth, fifth and sixth digit indicate the bank code.\nIn IFSC, the last six characters represent the branch code.\nIn the MICR code, the last three digits indicate the bank branch code.\nHow Can One Transfer Money with IFSC Code of Bank Account?\nIf the individual knows his/her way around banking transactions, they are already aware that there are two main forms of fund transfer. One being the old-fashioned physical way, wherein you walk into the bank and remit the cheque. While the second being the electronic way using methods such as IMPS, NEFT or RTGS.\nWhen dealing with the old-school \'going-to-the-bank\' way, one does not need to register a beneficiary. However, the electronic method is a bit different and a lot more secure too.\nTo transfer funds with the help of technology, the individual is required to meet the below-mentioned requirements:\nThe individual needs to be register for the bank\'s net banking service.\nNeed to register for third-party transactions. (Note that, in this context, third-party refers to a beneficiary from a different bank to that of yours.)\nRegistering the beneficiary\'s account to which funds are to be transferred.', ""Online Privacy Practices\nOnline Banking Protection\nReport Suspicious Emails\nProtect Your Identity\nCorporate Account Takeover\nWhat is Corporate Account Takeover?\n“Corporate account takeover” is when cyber-thieves gain control of a business’ bank account(s) by stealing the business’ valid online banking credentials. Although there are several methods being employed to steal credentials, the most prevalent involves malware that infects the business’ computer workstations and laptops.\nA business can become infected with malware via infected documents attached to an email or a link contained within an email that connects to an infected Web site. In addition, malware can be downloaded to users’ workstations and laptops by visiting legitimate Web sites - especially social networking sites - and clicking on the documents, videos or photos posted there. This malware can also spread across a business’ internal network.\nRecommendations to Business and Corporate Customers\nAlthough Guaranty Bond Bank uses technologies to prevent, detect and respond to fraudulent transactions, there are additional controls that you can institute within your organization to further reduce the risk of Corporate Account Takeover and fraud.\nReconcile your banking transactions on a daily basis.\nInitiate ACH and wire transfer payments under dual control, with a transaction originator and a separate transaction authorizer.\nEmploy best practices to secure computer systems in their business including but not limited to:\nIf possible, carry out all online banking activities from a stand-alone, hardened and completely locked down computer system from which email and Web browsing are not possible.\nBe suspicious of emails purporting to be from a financial institution, government department or other agency requesting account information, account verification or banking access credentials such as usernames, passwords, PIN codes and similar information. Opening file attachments or clicking on web links in suspicious emails could expose the system to malicious code that could hijack your computer.\nInstall a dedicated, actively managed firewall, especially if the business has a broadband or dedicated connection to the Internet, such as DSL or cable. A firewall limits the potential for unauthorized access to a network and computers.\nCreate strong passwords with at least 10 characters that include a combination of mixed case letters, numbers and special characters.\nProhibit the use of “shared” usernames and passwords for online banking systems.\nUse a different password for each Web site that is accessed.\nChange the password a few times each year.\nNever share username and password information for Online Services with third-party providers.\nLimit administrative rights on users’ workstations to help prevent the inadvertent downloading of malware or other viruses.\nEducate employees on good cyber security practices to include how to avoid having malware installed on the business computer.\nInstall commercial anti-virus and desktop firewall software on all computer systems. Free software may not provide protection against the latest threats compared with an industry standard product.\nEnsure virus protection and security software are updated regularly.\nEnsure computers are patched regularly particularly operating system and key application with security patches. It may be possible to sign up for automatic updates for the operating system and many applications.\nConsider installing spyware detection programs.\nClear the browser cache before starting an Online Banking session in order to eliminate copies of web pages that have been stored on the hard drive. How the cache is cleared will depend on the browser and version. This function is generally found in the browser's preferences menu.\nVerify use of a secure session (https:// not http://) in the browser for all online financial transactions, including online banking.\nAvoid using automatic login features that save usernames and passwords for online banking.\nNever leave a computer unattended while using any online banking or investing service.\nNever access bank, brokerage or other financial services information at Internet cafes, public libraries, etc. Unauthorized software may have been installed to trap account number and sign on information leaving the customer vulnerable to possible fraud.\nProperly log out of each online banking session and close all browser windows. Simply closing the active window may not be enough.\nWhen finished with the computer, turn it off or disconnect it from the Internet.\nAlso consider utilizing a security expert to test the network or run security software that will aid you in identifying known vulnerabilities.\nConsumer account holders receive protections from errors relative to electronic funds transfers under Regulation E; however we want to make you aware that Regulation E does not cover Business accounts.\nThe alerts and notifications offered by the bank are designed as flags to warn you if your system may have been compromised and we strongly encourage you to take advantage of them.\nWarning signs that your system/network may have been compromised include:\nInability to log into online banking (thieves could be blocking customer access so the customer won’t see the theft until the criminals have control of the money);\nDramatic loss of computer speed;\nChanges in the way things appear on the screen;\nComputer locks up so the user is unable to perform any functions;\nUnexpected rebooting or restarting of the computer;\nUnexpected request for a one time password (or token) in the middle of an online session;\nUnusual pop-up messages, especially a message in the middle of a session that says the connection to the bank system is not working (system unavailable, down for maintenance, etc.);\nNew or unexpected toolbars and/or icons; and Inability to shut down or restart the computer.\nContact the bank immediately if you suspect your network may have been compromised. Call 1-888-572-9881 and ask for the Internet Banking Department.\nfor more information on Corporate Account Takeover.\nGuaranty Bond Bank, N.A.\nEqual Housing Lender\n© 2014 Guaranty Bancshares, Inc. All Rights Reserved""]"	['<urn:uuid:29bde277-048b-42ce-a10b-a4bc5f73705e>', '<urn:uuid:4f4f47e1-4502-463c-9a95-b4a5e1778a50>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	6	62	2099
87	smart clothes track air pollution	Yes, there are smart garments that can detect air pollution, like the BB.Suit by ByBorre which creates a personal bubble of clean air when the wearer is in a polluted city like Beijing. However, these types of garments are still largely conceptual rather than widely available in the market.	['Touch base dialogues with things: Responsible IoT & tangible interfaces\nBy Iskander Smit\nThe ThingsCon report The State of Responsible IoT is a collection of essays by experts from the inter-disciplinary ThingsCon community of #IoT practitioners. It explores the challenges, opportunities and questions surrounding the creation of a responsible & human-centric Internet of Things (IoT). For your convenience you can read it on Medium or download a PDF.\nEnd of 2017 Google & Levi’s will introduce a jacket on the market that is a result of a long research program Project Jacquard by Google ATAP. It may be the first big introduction of ‘smart garment’ on the market, at least one by big players. In the fashion tech scene a lot of experiments are done over the last years, often focusing on the identity of the person wearing the fashion, changing behaviour based on the wearer’s feelings. Other garments actively react to the outside world, like the BB.Suit by ByBorre that creates a personal bubble of clean air if the wearer is in a polluted city like Beijing. All of these are still rather conceptual though. We also see near-skin devices that merge the physical and digital: Digital and physical experiences are not that different anymore. Embodiment of the digital is a concept that is emerging and moving into the mainstream. At its core this is an iteration of tangible user interfaces (TUI). Where TUI is a new form of interacting with digital information, embodiment makes your physical behaviour and feeling part of the digital environment.\nOne of my favourite explorations into this new marriage of body and bits is the project Surveillance Spaulder of James Bridle where he puts some muscle contracting pads on his shoulder, connect these to a infrared sensor that is triggered by surveillance cameras in London. Every time he passes a camera walking down the street he literally feels he is watched as his shoulder contracts. This is direct link between impulse and action. (In that sense it is not Internet of Things as there is no internet involved, but data-driven nonetheless.) A jacket like the one from ByBorre can let you feel invisible threats, like air pollution. An interesting design question here is how pollution feels: Does a polluted area feel more heavy, does it feel restless by vibrating, or warm? As your phone gets warm because it is inefficiently processing apps, that warmth is a sign of fast drainage of the battery. These kind of direct relations are interesting and can lead to a new language with our things.\nIn 2014 Fabian Hemmert published his PhD-research about new forms of interaction with things, and he looked to phones in one of his research through design cycles. Could a phone that has more data grow, or become heavier? Do we need these kind of clues to be more attached to the invisible aspects of data? Even before that, BERG and Timo Arnall did research into making wireless connections visible with the Immaterials project through several experiments with light and smart photo tricks as time-captures. Much later, the Architecture of Radio app tried to do a similar thing. The quest is to make the invisible visible.\nTouch plays an important role in social relations, as we learn from Gijs Huisman. He did his PhD on ‘Social Touch Technology’, or touch on a distance. I was involved in a valorisation project from this research and did some explorations the last years in the role touch and especially feeling touch can play as interface to others and to our world. Haptics is the science applying tactile sensation to interact with computer applications, and is the way this mediated touch is made accessible for interfaces. One of the research artifacts was a sleeve that is put on the under arm. It had 12 sensors and 12 vibrating engines. By touching on person’s sleeve, the other wearer can feel that touch remotely, be it a stroke or grabbing. The research found that the reception and understanding of stroking patterns resembles the human touch and people do feel a person’s touch at a distance. The use case for the sleeve was in the research context for death-blind people. Especially people born with this handicap have no other means of communication than touch; it is the only way to have a dialogue. There is a language of touch, especially done on the lower arm, a relatively sensitive part of the body.\nJust like other languages, touch needs to be learned and is depending on mutual understanding of ‘words’. Just like in voice based communication, you can distinguish tone and meaning. If we don’t know each other’s language we can communicate by basic sounds and interpretations. The feel of a tone can be linked to an emotion: The same goes for haptic feedback. We can feel danger, love, and other extreme feelings — more subtle one are harder to understand. There is a cultural and gender bias too: Men understand certain signals less clear from women than from men and vice versa. My conclusion after a couple of years of exploring haptics: Touch is very powerful to stress emotions, and to notify with a meaning. But it’s rare to have an instant understanding. You can link a couple of different signals to concepts, but you need to learn these by doing. The Apple navigation app is embedded in the Apple Watch system and is pre-programmed with different haptic patterns to signal if you need to go to right or left. I need to learn this over and over as I do not use it every day.\nIn tangible interactions and haptic feedback the dialogue with the thing is key. We need a mutual understanding of the technology and the relation with the thing. Holly Robbins looks1 into to the relation we have with technology and sketches different levels of mutual praxis with the technology. With smart things, this mutual praxis with technology becomes a more important part of our life. The dialogue we will have is increasingly defining our relation with things. With new interactions like touch and speak we will have these dialogues, and they are necessary. The interactions will be more interesting as the intelligence of the things grows with the development of AI. Things will start to understand us and react to our behaviour, but we will also start to understand technology better — and be able to have a different conversation based on this newly developed understanding.\nA question around intelligent things that have dialogues with us is to which degree the intelligence will live in the cloud, or in the things itself. The current architecture is often cloud based: Just think of Amazon Echo or Google Home, and also open source systems as ReSpeaker, all of whom use the cloud as the backbone for their conversations. This has limitations, though. It’s not that we need to be afraid for offline things, but rather the amount for data transfer that is needed as the conversations become more natural. We will see that things themselves will need to house part of the intelligence and use only impulses to trigger conversations and embed contextual information.\nThe separation of conversational intelligence and contextual savviness is an important architecture for the coming generation of things. Apple’s Airpods are an interesting example in this: You could have thought that they wouldn’t need a lot of computer power just to connect and send Siri instructions to the phone, but the devices do more than gesture & movement based interactions. Airpods are prepared for a much bigger role in the new digital services ecosystem.\nIn a model of the trigger-based interactions that emerge from wearable devices like Google Glass, Android Wear, and others, we see how information is served in little context-driven chunks. What and where and in what form is influenced by a mix of profile information of the user, and knowledge the system has on more static characteristics related to the service being used. Second, the system gets sensor-based impulses it can work with. Micro interactions, actions in the moment, together with these known characteristics allow it to calculate what to present at any given time. It’s a continuous dialogue.\nThe tangible user interface will play an important role in the digital near future. In the model of Heroshii Ishii2 we see that this type of interface requires a different approach as the mental model of the digital information needs to be made tangible too. Haptic feeling needs to come above the surface as we need a tangible representation of the digital model in the tangible space.\nIn an essay Tom Coates wrote on the shape of things in this tangible world, he made a clear distinction between embodied interactions and the Internet of Things. Embodied interactions are starting with digital interactions made into tangible experiences. “The internet of things, however, is much more about enhancing the physical with the digital, making the objects make more sense at a distance, or drawing out information from them and bringing it into a virtual space where we can do stuff with it.” He thinks the service layer is key to the experience of the things. In a research by Nazli Cila3 the role of things and data is divided in three levels. Collectors are the things that collect data by use, and present this data to the users as input for new decisions. We have a lot of these products in the Internet of Things domain now. Sensors are added to all kind of products, and the collected data is presented in dashboards as nice visualisations. The next level is Things as Actors: Products that take the decisions on their own. The data is used for intelligent computation. Like the June oven that measures weight and pictures the form of the food to decide upon the cooking scheme. Third level is Things as Creators. Not only are the products intelligent, and use data for decision-making based on designed rules. Rather, things start to design their own rules, and create new things.\nWhat does this mean for ethics in IoT? Both Cila and Robbins are researching the role of integrating between computer and human. They assume that this will be a very close relationship. And with tangible and wearable haptic devices this relation will be felt, literally. In a research program we are setting up at TU Delft now, we are looking into the role intelligent things might play in the city of the future. Are things also citizens in the smart city? And what kind of agency would these things have? Do we trust things to become full citizens? Thinking of the closing words of Bruce Sterling at this year’s SXSW festival he made a clear case that ultimately AI will rule out humans as disturbing part of an efficient society.\nWe are not at that moment. The relation we have with intelligent things will bring new design challenges. A student from Eindhoven — Felix Ros — made an interesting device for autonomously driving cars. You can put your hand on the device that gives feedback on the ‘thoughts’ of the car, what decision the car is making.\nIt is also a mean to intervene with these decisions. The human driver is still in charge, for now. But will this be the case if we decide to give more agency to the car and not the driver? At LABS we designed a next iteration of an intelligent barbecue that addresses these kind of questions helping the incidental grill cook with indicators on the process based on inputs, like thickness of the food and temperature of the barbecue. A system that is automated but leaves space for the cook to make their own decisions. The barbecue shares its insights (for example on timing), but when and how to act is a decision the human makes. This makes the barbecue more personal and adaptable, and the next generation of barbecues can take into account these learnings.\nIn designing the next generation of intelligent things we are also designing a system of interactions between the physical device and a rule-based computation model. We use a mix of physical and haptic touch points, and decide where the intelligence of the dialogue is put: Does it go into the thing or into the system? As designers, we need to take a stand where the agency of the machine might interfere with that of the human.\nIskander Smit is working at agency Info.nl in Amsterdam, that designs and develops digital services. Iskander is responsible for research and development and leading labs.info.nl. This R&D department dedicated to new developments, and emerging connected technologies. Labs conducts research and experiments on the internet that enters our daily life and how we interact beyond the screen. Among others, Labs is partner in a research project together with the University of Applied Science Amsterdam on the role of vibro-stimuli as digital coaching and mediated social interactions (the internet of touch) for health related topics.\nIskander has a background as Industrial Design Engineer and have been working as creative and strategist in digital services. He is member of Council Internet of Things and co-founder of the Behavior Design AMS meetup. He initiated and co-organises the Amsterdam edition of ThingsCon.\nThingsCon is a global community of IoT practitioners dedicated to fostering the creation of a human-centric & responsible Internet of Things. Learn more on ThingsCon.com, join an event near you, and follow us on Twitter.\n- Robbins, H. (2015). Disrupting the device paradigm: designing for mutual praxis in connected objects. ↩︎\n- Ishii, H. (2007). Tangible user interfaces. CRC Press. ↩︎\n- Cila, N. Products as Agents: Metaphors for designing the products of the IoT age. ↩︎']	['<urn:uuid:b3d50963-0707-4dc0-a4d6-7d264660d43c>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	5	49	2289
88	I'm seeing many young patients with teething discomfort, and parents often ask about high fever - what's the relationship between teething and fever symptoms?	Teething does not cause a high fever or vomiting and diarrhea. If a baby develops these symptoms, it is important to contact a pediatrician immediately.	"[""Posts for category: Children's Health\nDespite all of the research supporting the effectiveness of immunizations, many parents still question the safety of vaccines for their little ones. Will they protect my infant from serious disease? Or are the vaccines themselves harmful?\nImmunization is one of the best ways parents can protect their babies from serious childhood diseases ranging from tetanus and mumps to whooping cough and seasonal flu—and have been for more than 50 years. In fact, vaccinations have reduced the number of infections from vaccine-preventable diseases by more than 90%!\nBoth the American Academy of Pediatrics and the Centers for Disease Control and Prevention recommends that every child receive the protection that immunization provides.\nDo vaccines even work?\nYes, vaccines work every year to protect millions of children from serious illnesses. Because infants are particularly vulnerable to infectious diseases, if an unvaccinated baby is exposed to a certain germ, the baby’s body may not be strong enough to fight the disease. Therefore it is very important that parents take the necessary steps to ward off harmful complications through immunization.\nAre there side effects?\nAs with any medication, side effects can occur with vaccines. These side effects are usually very minor and include redness or tenderness at the injection site or a low fever, which indicates that the body is reacting positively to the vaccine. Most babies do not experience any side effects from vaccines, and severe reactions are very rare.\nParents have the power to protect their baby from serious illnesses. Deciding not to vaccinate your child could put him at risk for life-threatening childhood diseases. If you have questions about immunization, talk with your pediatrician. You can also visit the sites listed below for additional information and updated immunization schedules.\nAmerican Academy of Pediatrics\nFood and Drug Administration\nCenters for Disease Control and Prevention\nNational Network for Immunization Information\nTeething is an important part of your baby’s development. Although it can be an irritable time for your baby, there are many ways you can help ease the pain. Most babies get their first teeth around 6 months, but they might come anytime between 2 and 12 months of age. Teething does not cause a high fever or vomiting and diarrhea, so if your baby does develop these symptoms, it is important that you contact your pediatrician immediately.\nHelping Ease the Pain\nWhen your baby is teething, all you want to do is help ease the pain. Your pediatrician offers a few tips to keep in mind when your baby is teething:\n- Wipe your baby’s face often with a cloth to remove drool and prevent rashes from developing.\n- Give your baby something to chew on, but make sure it is big enough so that it can’t be swallowed and that it can’t break into small pieces. Teething rings are a popular choice for babies to chew on, as well as plush toys that are crunchy on the inside.\n- Rub your baby’s gums with a clean finger.\n- Never tie a teething ring around your baby’s neck, as it could get caught on something.\n- If your baby seems irritable, ask your pediatrician if it is okay to give your baby a dose of acetaminophen or ibuprofen to ease discomfort.\nCleaning Your Baby’s New Teeth\nOnce your baby’s new teeth have arrived, they are susceptible to plaque buildup just like adult teeth, which can lead to discoloration and dental complications. However, do no use toothpaste on your child’s teeth until they are old enough to spit—around the age of 2 or 3. Until then, brush their teeth with a small, soft toothbrush and water. The American Dental Association (ADA) recommends that kids visit the dentist by age 1, when six to eight teeth are in place, in order to spot any potential problems and advise you about proper preventive care.\nBy visiting your pediatrician, you can establish proper care for your child. Your pediatrician can help guide you in caring for your child through teething so that they are more comfortable.\n- Sudden high fever\n- Severe headache that isn’t easily confused with other types of headaches\n- Stiff neck\n- Vomiting or nausea with headache\n- Confusion or difficulty concentrating\n- Sleepiness or difficulty waking up\n- Sensitivity to light\n- Lack of interest in drinking and eating\n- Skin rash in some cases\nColds may be common, but that does not mean caring for your child’s cold is easy. To help your little one feel better, your pediatrician is available to offer tips on what you need to know about your child’s cold. The common children’s cold is a viral infection of the upper respiratory tract that usually lasts a week or two.\nThe typical preschool-age child may experience 6-10 colds per year. Most colds resolve on their own with rest and fluids, but some may lead to ear infections, sinus infections, asthma attacks or other complications.\nCaused by viruses, colds can be spread through a sneeze or cough. The virus may also be spread indirectly, through touching the hand of a healthy person or even by using door handles with your hand you may have just sneezed or coughed into. Once the virus is present and multiplying, your child will develop the familiar symptoms and signs:\n- Runny nose\n- Mild fever, particularly in the evening\n- Decreased appetite\n- Sore throat\n- On-and-off irritability\n- Slightly swollen glands\nMany parents become confused about the proper way to treat a coughing, sneezing child, because colds and allergies often have overlapping symptoms. When in doubt, talk to your pediatrician who will know exactly what is causing your child’s symptoms, especially if they are persistent or worsen with time.\nIf your child has a typical cold without complications, the symptoms should disappear on their own after seven to ten days. Your pediatrician may want to see your child if symptoms do not improve and is not completely recovered within one week from the start of their illness.\nContact your pediatrician for further treatment and to better understand your child’s cold symptoms.\nTruth is, anyone with an appendix can get appendicitis—even our children. Appendicitis is a painful inflammation of the hollow, finger-shaped organ attached to the end of the large intestine. If left untreated, an inflamed appendix can rupture, leading to a lengthy hospital stay for complications including abdominal infection and bowel obstruction.\nWhen your child complains of stomach pain, consult your pediatrician for proper diagnosis and to ensure the health of your child. Since appendicitis is potentially life-threatening, it is important to understand the symptoms so that you can spot appendicitis in your child. In order of appearance, the symptoms include:\n- Abdominal pain\n- Loss of appetite\nUnfortunately, symptoms of appendicitis might also be hidden by a viral or bacterial infection that preceded it. Diarrhea, nausea, vomiting and fever may appear before the typical pain of appendicitis, which makes the diagnosis much more difficult.\nYour child’s discomfort might also disappear, which will persuade you that they are better. However, this disappearance of pain could also meant that the appendix has just broken open or ruptured. The pain might leave for several hours, but this is the moment when the appendicitis becomes dangerous, making it more important than ever to visit your pediatrician for immediate care for your child.\nWhen your pediatrician diagnoses your child with appendicitis, surgery is usually needed as soon as possible. Surgically removing the appendix is usually the treatment of choice, as it is important to eliminate the inflamed appendix before it bursts.\nWhile most children with abdominal pain do not have appendicitis, you can never be too safe when it comes to the health of your child. Visit your pediatrician for further diagnosis of this serious problem and to take the next steps toward a healthy child.""]"	['<urn:uuid:205f0d81-59d4-4582-9b73-9dc6b638229c>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	24	25	1306
89	What determines eye color inheritance and blood compatibility?	Eye color is determined by dominant and recessive genes - brown eyes are dominant over blue. For blood, compatibility is determined by ABO and Rh factors, where incompatible combinations trigger immune responses that destroy transfused cells.	"['Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nBiological: Behavioural genetics · Evolutionary psychology · Neuroanatomy · Neurochemistry · Neuroendocrinology · Neuroscience · Psychoneuroimmunology · Physiological Psychology · Psychopharmacology (Index, Outline)\n- For other non-genetic uses of the term ""dominance"", see Dominance.\nIn genetics, dominance relationships control whether an offspring will inherit a characteristic from the father, the mother, or some blend of both. More technically, they control the ways genes interact to express themselves as phenotypes in a diploid or polyploid individual.\nThere are three kinds of dominance relationships:\n- Simple dominance\n- Incomplete dominance\nTraits inherited in a dominant-recessive pattern are often said to ""follow Mendelian inheritance"".\nThe dominant/recessive relationship is made possible by the fact that most higher organisms are diploid: that is, most of their cells have two copies of each chromosome -- one copy from each parent. Polyploid organisms have more than two copies of each chromosome, and follow similar rules of dominance, but for simplicity will not be discussed here.\nHumans, a diploid species, typically have 23 pairs of chromosomes, for a total of 46. In regular reproduction, half come from the mother, and half come from the father (see meiosis for further discussion of how this happens, and chromosome for less usual possibilities in humans or in cows).\nRelationship to other genetics conceptsEdit\nAlthough humans have only 46 chromosomes, it is estimated that those 46 contain 20,000-25,000 genes, each of which is related to some biological trait of the organism. Many genes are strung together in a single chromosome. The other chromosome of the pair will have genes for the same functions -- for example, to control height, eye colour, and hair colour.\nHowever, since one chromosome came from each parent, it is quite unlikely that the genes will be identical. The specific variations possible for a single gene are called alleles: for a single eye-colour gene, there may be a blue eye allele, a brown eye allele, a green eye allele, etc. Consequently, a child may inherit a blue eye allele from their mother and a brown eye allele from their father. The dominance relationships between the alleles control which traits are and are not expressed.\nConsider the simple example of the dominant brown eye allele and the recessive blue eye allele. In a given individual, the two corresponding alleles of the chromosome pair fall into one of three patterns:\n- both blue\n- both brown\n- one brown and one blue\nIf the two alleles are the same (homozygous), the trait they represent will be expressed. But if the individual carries one of each allele (heterozygous), only the dominant one will be expressed. The recessive allele will simply be suppressed.\nLatent recessive traits appearing in later generationsEdit\nIt is important to note that an individual showing the dominant trait may have children who display the recessive trait. If a brown-eyed parent is homozygous, they will always pass on the dominant trait, and therefore their children will always have brown eyes, regardless of the contribution of the other parent. However, if that brown-eyed parent is heterozygous (and they typically would have no way of knowing), they will have a 50/50 chance of passing on the suppressed blue-eyed trait to their offspring.\nIt is therefore quite possible for two parents with brown eyes to have a blue-eyed child. In that situation, we can conclude that both parents were heterozygous (carrying the recessive allele).\nHowever, unless there is a spontaneous genetic mutation, it is not possible for two parents with blue eyes to have a brown eyed child. Since blue eyes are recessive, both parents must have only blue-eyed alleles to pass on.\nMain article: Punnett square\nThe genetic combinations possible with simple dominance can be expressed by a diagram called a Punnett square. One parent\'s alleles are listed across the top and the other parent\'s alleles are listed down the left side. The interior squares represent possible offspring, in the ratio of their statistical probability. In this example, B represents the dominant brown-eye gene and b the recessive blue-eye gene. If both parents are brown-eyed and heterozygous, it would look like this:\n|B||B B||B b|\n|b||b B||b b|\nIn the BB and Bb cases, the child has brown eyes due to the dominant B. Only in the bb case does the recessive blue-eye trait express itself in the blue-eye phenotype. In this fictional case, the couple\'s children are three times as likely to have brown eyes as blue.\nTraits governed by simple dominanceEdit\n(not an exhaustive list)\n|Curled Up Nose||Roman Nose|\n|Clockwise Hair Whorl||Counter-clockwise Hair Whorl|\n|Can Roll Tongue||Can\'t Roll Tongue|\n|Widow\'s Peak||No Widow\'s Peak|\n|Facial Dimples||No Facial Dimples|\n|Able to taste PTC||Unable to taste PTC|\n|Earlobe hangs||Earlobe attaches at base|\n|Middigital hair (fingers)||No middigital hair|\n|No hitchhiker\'s thumb||Hitchhiker\'s thumb|\n|Tip of pinkie bends in||Pinkie straight|\nSome genetic diseases carried by dominant and recessive allelesEdit\n- Main article: Genetic disorder\n|Some types of Dwarfism||recessive|\nAs can be seen from this, dominant alleles are not necessarily more common or more desirable.\nIn incomplete dominance (sometimes called partial dominance), a heterozygous genotype creates an intermediate phenotype. In this case, both the dominant and recessive gene are expressed, creating a blended or combined phenotype. A cross of two intermediate phenotypes can result in the reappearance of either the parent phenotypes or the blended phenotypes.\nThe classic example of this is the colours of carnations.\nR is the gene for red pigment. R\' is the gene for no pigment.\nThus, RR offspring make a lot of red pigment and appear red. R\'R\' offspring make no red pigment and appear white. RR\' and R\'R offspring make a little bit of red pigment and therefore appear pink.\nAn example of incomplete dominance in humans is mordan, a trait that is exhibited when eye color alleles from the maternal and paternal chromosomes are blended. This usually occurs when one parent has green eyes and the other parent has brown eyes–the child will have dark blue eyes.\nIn co-dominance, neither phenotype is dominant. Instead, the individual expresses BOTH phenotypes. The most important example is in Landsteiner blood types. The gene for blood types has three alleles: A, B, and i. i causes O type and is recessive to both A and B. When a person has both A and B, they have type AB blood.\nAnother example involves cattle. If a homozygous bull and homozygous cow mate (one being red and the other white), then the calves produced will be roan-colored, with a mix of red and white hairs.\nExample Punnett square for a father with A and i, and a mother with B and i:\nAmongst the very few co-dominant genetic diseases in humans, one relatively common one is A1AD, in which the genotypes Pi00, PiZ0, PiZZ, and PiSZ all have their more-or-less characteristic clinical representations.\nMost molecular markers are considered to be co-dominant.\nIt is important to note that most genetic traits are not simply controlled by a single set of alleles. Often many alleles, each with their own dominance relationships, contribute in varying ways to complex traits.\nThe development of phenotype\n|Key concepts: Genotype-phenotype distinction | Norms of reaction | Gene-environment interaction | Heritability | Quantitative genetics|\n|Genetic architecture: Dominance relationship | Epistasis | Polygenic inheritance | Pleiotropy | Plasticity | Canalisation | Fitness landscape|\n|Non-genetic influences: Epigenetic inheritance | Epigenetics | Maternal effect | dual inheritance theory|\n|Developmental architecture: Segmentation | Modularity|\n|Evolution of genetic systems: Evolvability | Mutational robustness | Evolution of sex|\n|Influential figures: C. H. Waddington | Richard Lewontin|\n|Debates: Nature versus nurture|\n|List of evolutionary biology topics|\nReferences & BibliographyEdit\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|', 'A hemolytic transfusion reaction is a serious problem that occurs after a patient receives a transfusion of blood. The red blood cells that were given to the patient are destroyed by the patient\'s immune system.\nThere are other types of allergic transfusion reactions that do not cause hemolysis.\nBlood transfusion reaction\nBlood is classified into different blood types called A, B, AB, and O.\nYour immune system can usually tell its own blood cells from blood cells from another person. If other blood cells enter your body, your immune system may make antibodies against them. These antibodies will work to destroy the blood cells that your immune system does not recognize.\nAnother way blood cells may be classified is by Rh factors. People who have Rh factors in their blood are called ""Rh positive."" People without these factors are called ""Rh negative."" Rh negative people form antibodies against Rh factor if they receive Rh positive blood.\nThere are also other factors to identify blood cells, in addition to ABO and Rh.\nBlood that you receive in a transfusion must be compatible with your own blood. Being compatible means that your body will not form antibodies against the blood you receive.\nBlood transfusion between compatible groups (such as O+ to O+) usually causes no problem. Blood transfusion between incompatible groups (such as A+ to O-) causes an immune response. This can lead to a very serious transfusion reaction. The immune system attacks the donated blood cells, causing them to burst.\nToday, all blood is carefully screened. Modern lab methods and many checks have helped make these transfusion reactions very rare.\nSymptoms of a hemolytic transfusion reaction usually appear during or right after the transfusion. Sometimes, they may develop after several days (delayed reaction).\nThis disease may change the results of these tests:\nTherapy can prevent or treat the severe effects of a hemolytic transfusion reaction. If symptoms occur during the transfusion, the transfusion must be stopped immediately. Blood samples from the person getting the transfusion and from the donor may be tested to tell whether symptoms are being caused by a transfusion reaction.\nMild symptoms may be treated with the following:\nThe outcome depends on the severity of the reaction. The disorder may disappear without problems. Or, it may be severe and life threatening.\nTell your health care provider if you are having a blood transfusion and you have had a reaction before.\nDonated blood is put into ABO and Rh groups to reduce the risk of transfusion reaction.\nBefore a transfusion, patient and donor blood are tested (crossmatched) to see if they are compatible with each other. A small amount of donor blood is mixed with a small amount of patient blood. The mixture is checked under a microscope for signs of antibody reaction.\nBefore the transfusion is given, the health care provider will usually check again to make sure you are receiving the right unit of blood.\nGoodnough L. Transfusion medicine. In: Goldman L, Ausiello D, eds. Cecil Medicine. 23rd ed. Philadelphia, Pa: Saunders Elsevier;2007:chap 183.\nWu YY, Mantha S, Snyder EL. Transfusion reactions. In: Hoffman R, Benz EJ Jr., Shattil SJ, et al, eds. Hoffman Hematology: Basic Principles and Practice. 5th ed. Philadelphia, Pa: Churchill Livingstone Elsevier; 2008:chap 153.']"	['<urn:uuid:bf147c71-1485-4868-a06d-294a8a1b5ba6>', '<urn:uuid:93185a93-cd30-41f9-9790-b4bc480cd833>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	8	36	1830
90	In the song Passenger, what makes the pre-chorus riff particularly challenging to execute on guitar?	The pre-chorus riff in Passenger is considered the hardest part of the song to play specifically because of the quick run in measure 2. The riff begins with a Cmin7(no5)/Eb chord and includes runs on each string that are based on natural feeling slides rather than following specific musical theory.	"[""Chord Voicings & Thumb Technique\n[Mark's guitar is tuned to Drop-C for all examples in this Masterclass: C G C F A D] One of the things I get asked about a lot are the chord voicings that we use. Our chords are a little bit different than a lot of metal bands, in that there are more intricate voicings than what you would normally hear in the genre, though I’ve always believed in having really simple progressions.\nIf you listen to Haunted Shores songs, we tend to use a combination of three or four different chord shapes pretty often. I learned this first chord [Fmin7(9,11)] a long time ago from my friend Nick Dodd, who used to sing in Haunted Shores. If you listen to a lot the music, there are layers, but it’s nothing excessive. A lot of that is just chords that are bigger then they really need to be. The chord that he taught me really opened the door for me. That chord gets recycled a lot in Haunted Shores songs. I’m not afraid to admit that, because it’s basically just a minor chord with different voicings.\nYou can build voicings off of that, just to add a little bit more specificity to a generic power chord. It’s not economical for you to just lay one finger flat across a bunch of strings without even thinking about other voicings. One thing I use interchangeably – Misha Mansoor, who’s involved in the project, taught me this chord [Gmin7(9,no5)]. That’s a sort of minor chord that adds complexity and specificity to a chord, as opposed to just being a general minor chord.\nOne habit I developed when I was younger was using my thumb a lot. That’s not a standard practice at all – you don’t really see people using it, at least as far as I know. With those big open chords in mind, I think a lot of what makes these songs Haunted Shores songs is the fact that there’s a lot of really syncopated, intense right-handed riffing to go along with that. Bands like Extol are a huge influence of mine – the way they combine thrashier riffing with big open chords.\nImmaterial Main Riff\nThe main riff from ‘Immaterial’ has a lot of that. The riff starts out with this big chord in measure 1 [Cmin(add9)], and then goes to this one in measure 3 [Cmin(add9,11)]. That chord right there in measure 5 is the same one we went over earlier [Fmin7(9,11)], just picked differently.\nPassenger Main Riff\nThe main riff of the song ‘Passenger’ starts with that chord [Fmin7(9,11)]. There’s that chord again. This next shape is kind of weird – in measure 2 you’re basically playing the higher voicings of this chord [Cmin7(9,no5)], which is a transposition of the chord we went over earlier. So you’re basically doing voicings off of another chord. It makes sense melodically – even though the chords don’t really match up, it’s all basically in the same scale.\nThis part in measure 6 is a little bit tricky, because you’re playing the voicings of that same chord down a 5th [Fmin7(9,no5)], but instead of letting the notes ring through, you’re palm-muting through them kind of quickly. It can be kind of messy if you’re not careful with it.\nPassenger Verse Riff\nThe Verse riff for that song uses this chord in measure 2 [Ab/C]. That next part is tricky – I can only play that part fast, I can’t really play it slow. It’s just one of those things based on feel rather than any sort of logic or rationale there. It just feels good to do.\nPassenger Pre-Chorus Riff\nThis is the Pre-Chorus riff from ‘Passenger’: That’s probably the hardest part of the song to play because of the quick run in measure 2. The first chord is [Cmin7(no5)/Eb]. I don’t know what kind of run that is – it’s just something that’s based on it feeling natural with the slides and these runs on each string. Again, for those last chords I’m using my thumb, which is my roundabout way of playing chords that I’m too lazy to play normally. I could easily play them with my middle finger, but it’s just instinctual for me to use my thumb.\n[Special thanks to: DP and Editor Zach Jopling, Assistant Cameraman Habib Awan and Soundman Alex Hura, along with Chris Robinson for his excellent intro.]""]"	['<urn:uuid:791a863b-a86d-482e-ab31-ec6e0c65d7b2>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	15	50	738
91	house insurance policy cover natural disasters maintenance cost what excluded	New India Assurance Home Insurance provides coverage for natural disasters like floods, earthquakes, and storms under Section 1. However, standard homeowners insurance policies typically exclude natural disasters and require additional coverage. Additionally, damage due to wear and tear or lack of maintenance is not covered under home insurance policies. The policy also excludes damage caused by war, terrorism, radioactive contamination, and indirect losses like loss of earnings.	['New India Assurance Home Insurance\nOwn a home is one of the major milestones of life. It takes your hard earned money and ample of time to build a home of your dream. Home is the biggest asset of anybody’s life. Investment in your house is an investment for life time. Therefore to safeguard your home is of utmost importance. It is essential to save your house from some unavoidable man made or natural calamities. It is very costly affair to repair your house after the casualty; even you can become homeless in certain circumstances, for instance earthquake or fire.\nNew India Assurance is number one company in selling non life Insurance. They provide wide variety of products like- mediclaim Insurance, travel Insurance, accidental Insurance etc. The company deals in personal Insurance, Commercial insurance, Industrial insurance, Home insurance, social Insurance and liability insurance.\nNew India Assurance Company offers householders policy to satisfy the insurance requirement of the house owner. They proffer wide-ranging features in a single policy to meet up all the insurance related requirement of the owner so that they can safe guard their house by every probable means.\nWhy one should avail New India Assurance Home Insurance Plan\nNew India Assurance Company is 100% owned government organization. It’s a multinational general insurance company with its operations in around 26 countries having it’s headquarter at Mumbai in India. It’s the only direct insurer in India with A rating by AM Best. It is also awarded with AAA/STABLE rating by CRISIL.\nHouse Holders Policy:\nHouseholders policy offers by New India Assurance Company is “All in One” type of plan which provides various options in one policy so that you not need to purchase different plans for different needs. This policy satisfies all the insurance need of the householder under one single plan.\nWhat is insured?\nNew India Assurance provides discount in premium if you opt for multiple cover options by selecting their various section of the policy. This plan consists of 10 different sections under which a range of properties are covered. Below mentioned are the properties which are covered under this plan:\n- Household Appliances\n- Television/VCR/DVD Player\n- Jewellery and other valuable stuff\n- Other household article\n- Pedal Cycle\nBenefits and Features:\n- Premium is reasonably priced and pocket friendly.\n- Rebates/discounts on premium is available\n- This plan provides protection to your home and its content like electronic appliances or other valuable items.\n- This plan also has an option to provide personal accident cover.\n- This plan also has an option to provide public liability cover.\n- This plan also provide baggage insurance (optional)\n- Covers breakdown of Domestic appliances.\nWhat is covered?\nSection 1- Fire & Allied Perils: It further divides into two segments:\na) Building Coverage\nb) Coverage for content of personal belongings of the proposer and his or her family members who are residing with him or her permanently.\nFire, Lightening, Explosion of gas in domestic appliance\nBursting and overflowing of water tanks, apparatus or pipes\nDamage caused by Aircraft\nRiot, Strike, malicious or terrorist act\nEarthquake (Fire and/or Shock)\nSubsidence and land slide damage including rock slide\nFlood, Inundation and storm\nHurricane, Typhoon, Tempest, Tomado or cyclone\nSection 2- Burglary and House breaking Including larceny and theft:\nThis section covers the content of personal belongings of the proposer and his family against burglary, housebreaking, larceny and theft.\nSection 3- All Risks (Jewellery and Valuables)\nThis section covers the risk of any damage to your jewellery and other valuables items by accident or hard luck while worn, kept or carried anywhere in India. Cover amount is subject to the value of jewellery or valuables as declared in the schedule of policy.\nSection 4 – Plate Glass\nThis section provides cover to fixed plate glass in the house/premises of the proposer against any accidental breakage. Limit of the cover is subject to sum insured.\nSection 5 – Breakdown of Domestic Appliances\nThis section covers all the mechanical or electrical breakdown of the domestic appliance against any unintentional or unanticipated damage.\nSection 6 – Television set including VCP or VCR (All Risks)\nThis section provides cover damage or loss to T. V set which also includes VCP / VCR against all sort of risk. All Risks includes- fire and allied perils, house breaking or theft, burglary, breakage because of accidental outer mean and any sort of electrical or mechanical breakdown.\nThis section also provides cover against any legal liability which arise out of physical injury or accidental death of any third person other than family members who are already insured caused by use of television set is also cover up to the limit of Rs 25000/-\nSection 7- Pedal Cycles (All Risks)\nThis section provides covers the loss or damage to pedal cycles against- i) Fire and allied perils, ii) Housebreaking, burglary or theft, iii) Accidents by external means, iv) Third Party bodily/ personal injury or third party property damage up to Rs 10000/-.\nSection 8 – Baggage Insurance\nThis section provides cover to the baggage of the insured person against accidental loss or damage while travelling anywhere in India. This is exactly like a travel insurance plan. This feature is the additional benefit of this plan which provides complete package of insurance to the insured. It is an optional feature of this policy.\nSection 9 – Personal Accident\nThis section covers the physical damage or death of the insured by any accident, violence or any other external and evident means which is mentioned in the schedule. The cover is limited to the Sum Assured already mentioned in the schedule.\nSection 10 – Public Liability\nThis section covers the legal liability of insured person for any physical injury or loss or damage to the property of third party. The cover is limited to the sum assured as mentioned in the schedule.\nThis section also covers the Workmen’s Compensation liability of the insured to domestic servant working in the premises of the Insured person.\nFor the issuance of policy it is mandatory to opt of at least 3 of the above mentioned sections. Out of which Section 1 b is compulsory to opt for the commencement of the risk under this policy.\nExclusion under New India Assurance Householder’s Policy:\nThe company has some segregation under this policy for which the company is not liable:\n- Damage, deconstruction or loss caused by war, act of foreign enemy, hostilities, invasion or war like operations.\n- Damage, deconstruction or loss to property caused by any act of terrorism.\n- Damage, deconstruction or loss to property is insured by – i) ionising radiation, or contamination by radioactivity from nuclear fuel or wastage from the combustion of nuclear fuel. ii) The radioactive toxins or any other hazardous properties of explosive nuclear assembly or component thereof.\n- If damage caused to the property is insured by its own fermentation, impulsive combustion or natural heating.\n- Loss or damage causes by mean of downgrading of the article or wear and tear or depreciation.\n- Permanent or temporary dispossession of property resulting from exclusion, commandeering, requisition or deconstruction by order of any lawful or government authority.\n- Permanent or temporary dispossession of property resulting from unlawful activity by the person.\n- Any indirect losses or damage like loss of earnings, loss by delay, loss of market.\nSum Assured Selection Procedure:\n- If you want to insure your domestic contents you have to cluster the articles under different and broad category like-clothing, utensils, furniture, crockery, linens, etc. The value given to these articles should be equivalent to their market value. It means the value dedicated to these articles will be the same on which these articles can be bought or sold in the market.\n- Articles falls under the category of Section 1 (a & b), 2, 3, 4, 6, 7, and 8 should also be insured on the market value basis with the same method as described above for domestic contents.\n- For Section number 5- Breakdown of Domestic appliances, the sum insured amount will be equal to the current replacement value of the same. Let’s take an example:- If you wants to insure your 8 litres LG Washing Machine which is 2 years old. Then the insurance amount for the 8 litres LG washing machine will be equivalent to the cost of new 8 litres LG washing Machine.\n- The claim amount to be paid needs to be equivalent to the amount which is required to bring down the damaged item to the similar state which it was prior to the damage, subject to the limit of sum insured.\n- Under Section 9- Personal accident the Sum assured should not be greater than the 72 months salary from the productive employment.\nNew India Assurance Householder’s Policy Claim Procedure:\nIn case of an event which direct to a valid claim under New India Assurance householders policy there are some guidelines which needs to be followed. Following points should be taken care of while making the claim process:-\n- One should take necessary course of action to lessen the damage or loss.\n- One should inform the fire brigade as soon as possible in case of fire.\n- One should inform to the nearest police station immediately with the list of stolen items and their approximate value if any theft, larceny, or burglary take place.\n- One should inform the insurance company immediately by phone, fax or in writing.\n- One has to submit the complete claim form duly filled and signed by insurer which is issued by the insurance company.\n- One has to provide the complete support to the surveyor who is appointed by the insurance company to survey the site of incident also you have to provide proper and complete documents to authenticate the loss.\nNew India Assurance Company provides free look in period of 7 days to the customer. If in any case customers are not satisfied with the terms and condition of the company they can return and cancel their policy within seven days of receiving the policy documents.\nGriha Suvidha Policy:\nGriha Suvidha policy is the abbreviated form of householder’s policy. Its main aim is to target the content of average householders on ‘first loss basis’.\nThe main point of interest in this policy is its simplicity. It effortlessly provides the details and description of the contest to the customers, which is easy to understand. Also the process of claim settlement is trouble-free and hassle free.\nThis policy has four specific options which again have five sections each. This policy also has an additional section six, under which the residential premises is cover on Complete Sum Insured Basis.\nAnybody who is residing in India, including foreign nationals can avail this policy provided that the content which is going to be insured is situated within the territory of India. Also the claim settlement will be done in Indian currency only.\nThis policy provides insurance to domestic contents against- Fire and other allied perils like-flood, riot, strike, housebreaking, earthquake, terrorism, burglary, theft and accidental damage to contents including jewellery and other valuable items. It also provides cover against breakdown of domestic appliances including TV and desktop.\nSection i- Provides coverage for contents (excluding jewellery) against Fire and Allied perils which also includes earthquake and terrorism.\nSection ii - Provides coverage for contents (excluding jewellery) against Burglary and house breaking and theft which also include larceny.\nSection iii – Provides coverage to Jewellery and other valuable items\nSection iv – Provides coverage to breakdown of domestic appliances\nSection v- Provides coverage to Television and Desktop\nSum Insured on First Loss Basis:\n|Sections||Cover Description||Sum Insured\n|I||Fire & Allied perils-content||1 lakh||2.5 lakh||5 lakh||10 lakh|\n|1 lakh||2.5 lakh||5 lakh||10 lakh|\n|III||Jewellery and other valuables||.50 lakh||1 lakh||2 lakh||4 lakh|\n|IV||Breakdown of domestic appliances||.50 lakh||.75 lakh||1 lakh||2 lakh|\n|V||TV and desktop||.25lakh||.40 lakh||.60 lakh||.75 lakh|\n|VI||Fire & Allied perils -premises||Full SA as per property value||Full SA as per property value||Full SA as per property value||Full SA as per property value|\nNew India Assurance Home Insurance - FAQs\n1. How to pay a premium? What are the modes of payment available?\nThe New India assurance Company offers 2 modes of premium payment namely:\n- Cash payment at the branch\n- Online Payment\nFor the online payment mode, the policyholder can pay via;\n- Credit Card,\n- Debit Card\n- Net banking\n2. How can I check policy status for New India Assurance home insurance?\nYou can go to the website and log in with your customer ID and password to know the policy status.\n3. What is the policy renewal process for New India Assurance home insurance?\nFor online payment\nStep1: Enter your Client ID and Date of Birth to login into e-portal\nStep2: Enter policy nuber and expiry to proceed\nStep3: Select the payment option –Net Banking or Debit/Credit Card to pay\nStep4: Upon successful payment completion print/save the premium receipt\nAlteranatively, you can pay via cash/cheque at any of the nearest branch.\n4. What is the company’s process to settle claim for New India Assurance home insurance?\nTo settle the claim you can intimate the company by filling a form online. Upon successful process, the claim is settled within a few days.\n5. What is the policy cancellation process for New India Assurance home insurance?\nYou can visit the nearest branch with you policy documents and duly filled surrender form that you can get at the branch. The refund will be directly credited in ypur bank account and policy will stand cancelled.\n- Bharti AXA Home Insurance\n- Cholamandalam Home Insurance\n- Future Generali Home Insurance\n- HDFC Ergo Home Insurance\n- IFFCO Tokio Home Insurance\n- National Insurance Home Insurance\n- Oriental Home Insurance\n- Raheja QBE Home Insurance\n- Reliance Home Insurance\n- Royal Sundaram Home Insurance\n- SBI Home Insurance\n- Shriram General Home Insurance\n- Universal Sompo Home Insurance\n- Handy Guide- All You Need to Know About Property Insurance\n- Everything You Need to Know About New India Assurance Griha Suvidha\n- HDFC Ergo Home Insurance versus Bharti AXA Home Insurance\n- Bajaj Allianz, Bharti AXA and Cholamandalam: Comprehensive Insurance for Your Home\n- Importance of Having a Home Insurance Policy in India\n- Secure Your Home against Calamities with Home Insurance\n- Savings Calculator\n- Income Tax Calculator\n- Human Life Value Calculator\n- Pension Calculator\n- Health Insurance Premium Calculator\n- Car Insurance Calculator\n- Bike Insurance Calculator\n- SIP Calculator\n- Life Insurance Calculator\n- Term Insurance Calculator\n- ULIP Calculator\n- Premium Calculator\n- FD Calculator\n- Investment Calculator\n- Travel Insurance Calculator', 'Understanding the Limitations of Homeowners InsuranceAs a homeowner, it is important to understand the limitations of your homeowners insurance policy. While it is designed to protect you from a range of potential risks and liabilities, there are some events and scenarios that are not covered under your policy. That’s why it’s crucial to read your policy carefully and understand what is and isn’t covered.\nThe Fine Print: What Homeowners Insurance Does Not CoverIt’s essential to read the fine print of your homeowners insurance policy to understand what is not covered. Some of the most notable exclusions include:\n- Natural disasters: As mentioned, homeowners insurance does not cover damage caused by earthquakes, floods, or landslides.\n- Home maintenance: If damage occurs due to wear and tear or lack of maintenance, it is not covered. For example, if a storm damages your roof, but it was already in poor condition, any repairs or replacements may not be covered by your policy.\n- Criminal activity: If damage occurs as a result of criminal activity such as theft or vandalism, it may not be covered.\n- Business activities: If your home is being used for business purposes and damage occurs, it may not be covered under your standard homeowners policy.\nWhy Natural Disasters Are Excluded from Homeowners InsuranceThe reason natural disasters are excluded from homeowners insurance policies is that they are considered high-risk events, and the potential insurance payouts can be significant. As a result, insurance companies typically require homeowners to purchase additional coverage or riders to protect against these types of events. Even with additional coverage, there may be exclusions and limitations based on the specific circumstances of the event. For example, earthquake insurance may not cover damage to your swimming pool, while flood insurance may not cover damage to your seawall.\nA Closer Look at Flood, Earthquake, and Landslide CoverageWhile standard homeowners insurance policies do not cover natural disasters, there are additional insurance policies available to help homeowners protect against specific risks. Flood insurance: Flood insurance is available through the National Flood Insurance Program and private insurance companies. It offers protection against damage caused by flooding, which is not covered under a standard homeowners policy. However, it’s essential to note that flood insurance typically has a waiting period of 30 days before coverage kicks in. Earthquake insurance: Earthquake insurance is available as a separate policy or as an add-on to your homeowners policy. It offers protection against damage caused by earthquakes, which are not covered under a standard policy. However, earthquake insurance can be expensive, and deductibles are often high. Landslide insurance: Landslide insurance is much more limited than flood and earthquake insurance and is only available in certain areas. It offers protection against damage caused by landslides or mudslides, but coverage is typically limited to damage to the structure of your home and not the land around it.\nWhat Homeowners Can Do to Protect Against Natural DisastersIn addition to purchasing additional insurance coverage, there are several steps homeowners can take to protect their homes against natural disasters.\n- Invest in home maintenance: By keeping your home in good condition, you can reduce the risk of damage from natural disasters and increase the chances of coverage from your insurance policy.\n- Prepare for disasters: By having an emergency plan in place, you can minimize the risk of damage and make the claims process smoother if there is damage.\n- Consider home improvements: Installing features such as a sump pump or reinforced roofing can help protect against damage from floods or high winds.']	['<urn:uuid:a5d75b84-ea7c-4af3-8382-2681194357b1>', '<urn:uuid:c687d6ff-a6de-4a40-b28e-d8a690c5b0ed>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:43:46.574121	10	67	3026
92	ravin r29 crossbow weight size specs proper maintenance required for accuracy	The Ravin R29 weighs 6.75 pounds, measures 29 inches in length, and is 10 inches wide uncocked (6 inches cocked). For maintaining accuracy, the crossbow requires regular maintenance including lubricating rails, cables and strings every 25-30 shots, and strings and cables should be replaced as a unit at least every other year. Additionally, all mounting bolts and screws must be regularly checked for tightness as loose components can affect accuracy.	['$2,299.99 – $2,999.99\nravin crossbow r29 – ravin r29 crossbow – ravin crossbows r29 – ravin r29 crossbow for sale – r29 ravin\nFirst Impressions / Size and Weight\nLike with any Ravin crossbow we’ve bought and tested so far, the Ravin R29 Predator Crossbow Package comes fully assembled and pre-tuned. Which means you can instantly take it out to the range for some target practice.\nPulling the Ravin R29 out of the box, you’ll instantly notice how compact and lightweight this crossbow is. It feels very comfortable to hold and the ergonomic foregrip is really well made as well. These Ravin foregrips are rubberized and one of the better ones out there.\nThe Ravin R29 crossbow has a total weight of only 6.75 pounds, length of 29 inches and a width of 10 inches uncocked and 6 inches cocked! Ravin rated the R26 to be the most compact crossbow on the market, but the R29 is only 3 inches longer and 0.25 inches wider. But the R29 shoots 30 feet per second faster at 430 FPS.\nArrows and Speed\nThe Ravin R29 is rated at 430 feet per second with a 400-grain Ravin arrow. With a power stroke of only 12.5 inches, the Ravin R29 still has a kinetic energy of 164.\nRavin includes 6 Ravin .003 Arrows & Field Tips with this package. These arrows have a total weight of 400 grains, the shaft weighs 300 grains and the field trips 100 grains.\nThese arrows use special nocks that are required to properly cock and load the Ravin R29. If you do not use Ravin nocks, your warranty will void!\nAlso included with this package is a 3-arrow quiver. This quiver does the job and it’s very easy to install or remove. The mounting bracket that the quiver attaches to is also used for the detachable draw handle for the cocking mechanism.\nRavin always makes sure to include a proper scope with their crossbows. Mounted on the Ravin R29 sits a 100-Yard Illuminated Scope. This scope is one of the best scopes that comes with a crossbow.\nThe only way to cock this is with the Built-In Versa-Draw Cocking Mechanism. This built in cocking mechanism reduces the draw weight to only 12-pounds! Ravin also upgrade the cocking mechanism to be fully silent! No more sounds or clicking when you cock your crossbow. Because of the Trac-Trigger System it’s not possible to rope cock or cock this crossbow by hand.\nThe Trac-Trigger System slides along the rail to latch onto the center of the bowstring. Attach the removable draw handle to use the versa-draw cocking mechanism to cock the Ravin R29. Once you’ve fully cocked the crossbow, the trigger will automatically engage.\nYou can mount the removable crank next to quiver.\nTrigger and Safety Features\nRavin installed a trigger that works together with the Trac-Trigger system. It breaks at around 3 pounds and has some creep. For a crossbow at this price level, I really hope this will go away once I’ve shot it a couple more times.\nAfter cocking the Ravin R29, the trigger will automatically engage. The Anti-Dry Fire and safety switch will also automatically engage. The anti-dry fire mechanism will only disengage once you’ve loaded a .003 Ravin arrow. It takes about 10 pounds of force to get it loaded correctly, you’ll hear a click once it’s correctly loaded.\nAssembly and Sighting-In\nRavin ships this fully assembled and pre-tuned. Due the shipping of the crossbow, Ravin removed the scope from the stock. If the package gets knocked during shipping, the scope can get damaged or is not sighted in properly anymore.\nMount the scope, quiver bracket and quiver on the stock of the Ravin and you’re ready to take it out to the range or a hunting trip! I made sure to use some levels to make sure the scope and stock are completely level.\nSighting in the scope wasn’t need, Ravin already pre-tuned the scope correctly!\nBecause of the compact and lightweight design of 29, it’s a really comfortable crossbow to hold. This crossbow is great for hunting in tight spaces, hunting blinds or tree stands.\nRavin R29 vs Ravin R29X\nAs you can see there’s not much difference. The R29X shoots arrows at 20 feet per second faster and has a different scope mounted. This tactical scope with elevation mount extends the range of the crossbow out to 200 yards!\nYou can buy the elevation mount for the R29 as well, but the R29 might lack some power to actually get consistent accurate shots for 200-yard distances.\n29 Crossbow Package Features:\n- 430 FPS velocity\n- 164 ft.-lbs. kinetic energy\n- 12.5″ power stroke\n- 12-lb. draw weight\n- Width axle-to-axle:\n- 6″ cocked\n- 10.5″ un-cocked\n- 29″ overall\n- 6.75 lbs. total weight\n- 100-yard illuminated scope and accessory package\n- Finish: Predator Dusk Grey\n- Ravin Crossbows Item Number: R029\nR29., R29X., R29X Snipper.', 'Having trouble sighting in your crossbow? Here are some time-proven tips that will help keep it in tip-top shape and get you ready to practice more successfully this summer.\nBy Al Raychard\nYou just bought a crossbow and have had trouble sighting it in … or you’ve owned one for a few years but still can’t get the darn thing to shoot right? Frustrating! I know, because I’ve been there. Makes a guy want to throw up his hands and yell, “What in the world is going on?”\nToday’s crossbows are highly efficient and viable hunting tools, but we tend to forget two things about them. First, whether recurve or compound, both crossbow designs are susceptible to human error and inconsistencies. Second, both styles are mechanical and are subject to wear and the riggers of time. As a result, crossbows require regular maintenance and, from time to time, occasional adjustments or replacement of parts. This is all necessary to achieve and maintain maximum performance and accuracy.\nThree primary things including, including wheel or cam timing, the right arrows and string and cable maintenance, ensure consistent crossbow bolt flight and accuracy. As a crossbow enthusiast and hunter for nearly a decade, however, I know other factors also come into play.\nLoose is of No Use\nCrossbows are assembled using a variety of bolts and screws. The most important of the lot is the front-end mounting bolt used to fix the limb and riser assembly to the front of the rail. On some models, these same bolts also secure the cocking stirrup to the barrel.\nAlthough lock washers are employed on some bows to keep these important bolts tight and in place, over time vibration can cause mounting bolts to loosen. Keeping that in mind, it should be part of every pre-shooting ritual to make sure these bolts are secure. The same is true of all other bolts and screws, including those holding the scope. One loose component can increase vibration and affect accuracy and arrow consistency.\nThe bolt (arrow) retention spring is one item that gets little attention until something goes wrong with it. In good working order — and when set properly — the retention spring should maintain enough down pressure to keep the arrow in place against the string and aligned on the flight rail. On most crossbows, the retention spring also works in conjunction with the anti-dry-fire and safety mechanisms.\nA bolt that is not firmly set against the string will fly erratically and can be a real safety issue. The retention spring is an important component that should be checked regularly.\nTypically, retention springs are fixed to the sight bridge by a screw. If the spring loosens, remove the scope and sight bridge and tighten the screw.\nToday’s crossbows are extremely well made. With routine maintenance, a new crossbow should provide many years of reliable service. If accuracy problems or bolt inconsistencies arise, more times than not it is due to human error. I’ve worked with crossbow hunters for many years, and have concluded that, in most cases, crossbow accuracy problems begin and end with the shooter.\nOne of the most common errors occurs during the cocking process. In order for a crossbow bolt to shoot true each and every time, it is imperative the center serving be locked in the trigger mechanism in the same position every time. Cocking the string off center as little as 1/16-inch can put the bolt off target left or right by several inches by the time it travels 40 yards.\nA simple fix is to put a mark on each side of the center serving where it crosses the flight rail when the string is at rest, providing a visual point to follow when cocking the bow. When the string is fully cocked, the marks should be in the same place on each side of the rail. If not, pulling back on the string and adjusting left or right will center it.\nFor bows without an integrated cocking system, cocking ropes and mechanical devices will greatly aid in cocking the serving center to the trigger mechanism. They also reduce the effort required to cock the bow, by as much as 50 percent in some cases.\nAnother human factor is “canting” — not holding the bow limbs level when shooting. If the right limb is lower than the left, the bolt will shoot right and perhaps high or low, depending upon the range. If the left limb is lower than the right, the bolt will shoot left and higher or lower. Make it a point to keep the limbs level; this is crucial to consistent crossbow accuracy.\nCrossbows are also front-end heavy and difficult to hold in a shooting position for any length of time, especially while hunting. Sighting in from a shooting table and rifle rest (or bags) is the best option for dialing in a crossbow and achieving optimum arrow flight. When hunting, equip your stands with shooting rails, shooting sticks or a bipod. This will greatly improve your accuracy.\nSeveral other human factors can affect consistent crossbow accuracy. The first is routine and religious maintenance — something too many hunters neglect. Rails, cables and all portions of the string except the center servings should be lubed and waxed every 25 to 30 shots.\nCheck your owner’s manual for recommended lubrication maintenance. It is crucial to maintain the trigger, center serving and dry-fire housing with a high-quality lubricant. You can use gun oil on the trigger to help prevent rust and stiff trigger pull.\nSome manufacturers recommend changing strings and cables annually to maintain and ensure proper brace height and safety. With proper waxing and lubrication, strings and cables can last 150 shots or more, but over time strings and cables will stretch, allowing limbs to relax (creep). This will affect arrow speeds which, in turn, change the point of impact.\nAs a precaution — and to maintain optimum performance — strings and cables should be replaced as a unit at least every other year even if they appear to be in good condition.\nTo help gauge string wear on my crossbow, I use an indelible marker to draw vertical lines on each side of the rail where the string crosses at rest. If the string creeps past these marks, it is time to change it along with the cables.\nRegardless of what anyone says, crossbows are not intended for long-range hunting. The short, heavy bolts lose velocity and energy much more quickly than arrows shot from compound bows. Always use a range-finder and restrict your shots at deer to less than 40 yards. This will help ensure better accuracy and lead to quick, clean kills.\nNothing beats practice. That means practicing with the same arrow and tip weight that you will use when hunting. Practice does indeed make perfect, and the more you know your crossbow, the more accurate and consistent you will shoot.\nSpending time at the range to develop a proper trigger technique and shot sequence is important for hunting accuracy. A proper shot sequence — aim, click off safety, slowly and gradually squeeze the trigger, and follow through — can take 10 seconds or more. It takes practice to perform this consistently shot after shot. It’s that consistency that separates the really good hunters from the average ones.\nThe above preparations should put you on pace to pack bolts into tight groups each and every time you shoot your crossbow. Most of all, it should make you a deadly accurate hunter. If your accuracy suffers setbacks, it’s time to check your scope.\nBolt flight and accuracy can suddenly go awry when something as simple as a mount screw comes loose, or the scope merely gets bumped. Just as important, the horizontal cross-hairs must be perfectly parallel across the flight rail. If the scope has multi-dots, the dots must be perfectly vertical.\nTo assure proper alignment, mount the crossbow in a padded vice, making sure the flight rail is horizontally level. Mount the scope and snug it down. Against a far wall, attach a target with a cross-hair, making sure the horizontal line is level and the cross-hair line is perfectly vertical. Look through the scope and match the horizontal line or vertical dots with the appropriate line on the target. When it is, tighten the scope. The scope is now mounted true to mechanical center.\nAfter windage and elevation adjustments at the range per manufacturer’s recommendations, the crossbow should shoot quarter-sized groups or better out to 30 yards.\nAll manufacturers recommend specific bolts for their crossbows and, in many cases, different bolts for different models. Whether made of aluminum or carbon, arrows are chosen based on several factors, including overall weight, spine, straightness and nock-to-vane length. To achieve and maintain optimum accuracy, only use bolts recommended by the manufacturer.\nOther problems can arise with cam timing and tiller alignment. This is most evident on older compound crossbows. Over time, improper cam synchronization dramatically alters arrow flight. Look for wear marks on either side of the arrow shaft, especially near the nock, made by poor alignment with the flight track. Another indication is if bolt flight is consistently left or right of the intended point of impact.\nThese symptoms, however, can also indicate improper cocking practices, so be sure you are cocking the bow properly. To check the cam timing, most compound crossbows have timing lines, holes or dots in each cam. If not, measure each side of the bow from where the limbs meet the prod housing to the string.\nIn either case, these indicators or measurements should be equal indicating identical or near identical pull weight and pull length. If not, the bow is “out of till.” Most models have an adjustment bolt on each limb and adjustment is made by turning the bolt in or out to adjust the tiller and synchronize the cams. On recurve crossbows, the only course of action is to replace the limbs with a new set.\nAccurizing a crossbow and maintaining consistent accuracy is not difficult. It just takes some understanding of basic concepts, the patience to do things right and a commitment to keeping your crossbow in tip-top condition.\n— Al Raychard is a veteran deer hunter from Maine.']	['<urn:uuid:ed840303-2f3a-4c9d-8604-d15773af7151>', '<urn:uuid:7cce2b5e-df90-46a0-a15b-b24ed64a1f52>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	11	70	2535
93	historical facts school prayer bible reading laws before supreme court 1962	Laws requiring school prayer and Bible reading were not as widespread as prayer advocates claim. These practices were relatively late additions to public education, were frequently challenged in court, and were already declining when the Supreme Court made its rulings in Engel v. Vitale and Abington Township School District v. Schempp in the 1960s. Claims that prayer was a universal and widely accepted practice before 1962 are not historically accurate.	"['Perhaps no aspect of the church-state controversy arouses more emotion and discussion than the subject of prayer in the public schools. After all, public schools are supported with taxpayer money. What believer would want his taxes to support an institution that prohibits his children from praying? What nonbeliever would want her taxes to support an institution that requires her children to participate in prayer?\nFortunately, the First Amendment to the U.S. Constitution protects both believers and nonbelievers from such a situation by mandating government neutrality between belief and nonbelief. The government--through its proxy, the educators and administrators who facilitate our schools--may not lead children in prayer or force them to pray a certain way. However, all children have the right to pray voluntarily before, during, or after school, and nonreligious children do not have to pray at all.\nYet some believers think they know better than the Founding Fathers and want to tamper with the Bill of Rights. They want to amend the U.S. Constitution so that the Government would legally sponsor and take over the activity of prayer. The purpose of this page is to provide an overview of the controversy by providing links to all sides of the issue. Students already have the right to pray and read the Bible in public schools; no one is suggesting that right be taken from them. Rather, the question is, should the government lead students in prayer in public schools?\nThis legal bulletin from the ACLU discusses graduation prayer, bible distribution, equal access to school facilities, and religious holiday parties.\nBible believers should oppose school prayer (1995) (Off Site) by Rick Lott [Scroll down the page until you come to this article.]\n""Of all the philosophical inconsistencies of the GOP, the most confusing is their constant moaning about the intrusion of government into the lives of the citizenry while at the same time exhibiting an eagerness to legislate something as personal and private as religious conviction.""\nThe text of a brochure mailed by the Freedom From Religion Foundation to schools, school districts and state Secretaries of Education across the country.\nACLU on Constitutional Amendment on School Prayer or Moment of Silence (2002) (Off Site)\nSome, like former Secretary of Education William Bennett blame the 1962 decision, Engel v. Vitale, banning official prayer from public schools, for everything from low SAT scores to high teenage pregnancy rates. But many educators and other experts tell us that these problems flow from the enormous and increasing gulf in wealth and opportunity and education, between the richest and poorest people in our society. A one-minute prayer or moment of silence in school everyday will do nothing to change that.\nA c(4) lobbying group should not be allowed to organize through c(3) tax-exempt groups like churches. Politically active churches should not be allowed to retain tax-exempt status. Should elections and legislation be dominated by out-of-control fundamentalist and Catholic churches? Should churches be allowed to use their tax-exempt fortunes to topple our secular Constitution and establish ""One Nation Under God""?\nIs government sponsored prayer needed? (Off Site) by Susan Batte and Tom Peters\nThere is a secret about prayer in the public schools that the religious right doesn\'t want you to know: school prayer, like most religious activity, is legal. Completely, thoroughly, and absolutely legal. It was not outlawed in the 1960\'s; it has never been outlawed. Despite repeated claims to the contrary, the Supreme Court has not made schools a ""religion free zone,"" has not ""kicked God out of the public schools,"" has not ""made it illegal to speak about God,"" and has not ""made it illegal to pray in school."" These claims are false, and the religious right knows this. Rather, the religious right popularizes these claims to make religious persons believe that their rights are being suppressed in the public schools, thereby gaining support for their political program.\n""In summary, there is no evidence that suppression of religious activity in the public schools is anything more than an extraordinarily rare exception, or that these cases generally go to court, or that they are typically resolved in favor of the school. Students can pray and read the Bible. The law is clear on this point, and we are unable to point to any authority on either side of the issue that thinks otherwise. Religious suppression simply cannot be used as a justification for a school prayer amendment.""\nPrayer and the Public Schools (Off Site) by Americans United for Separation of Church and State\nMost people of faith understand that the personal issue of prayer when manipulated by government can lead to dangerous consequences, including exclusion, octracization and even violence. For this reason the faith groups represented in this pamphlet work diligently to make sure that individual conscience is respected by rejecting calls for laws to mandate prayer in the public schools.\n[Government-led] Prayer Does Not Belong in Classrooms (1996) (Off Site)\n""The fact is prayer never left public schools. Individual students are free to recite voluntary prayers or to read from religious texts during their free time.""\nA Christian apologist urges Christians not to fight to institute government-led prayer.\nWas school prayer widespread before 1962? (Off Site)\nThe religious right has a stake in making people think that the Supreme Court rulings of the 1960s destroyed a common and widely accepted practice of school prayer. In fact, laws requiring school prayer and Bible reading were not nearly as widespread as prayer advocates claim, were late-comers to the public education, were frequently and successfully challenged in court, and were on their way out when the Supreme Court handed down it\'s rulings in Engle v. Vitale and Abington Township School District v. Schempp.']"	['<urn:uuid:de4f030d-a8f0-4567-ae6d-144ae46ae00a>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	11	70	946
94	chicken farm want know how much feed affects extra large egg production compared other factors	Feed ration has the greatest impact on extra-large egg production, followed by the age at first egg. Among external factors, feed ration shows the biggest volatility in production. To control extra-large egg production, farms need to manage efficient feeding based on feed ration, age at first egg, and the maximum and minimum temperature inside the farm.	['Go to the main menu\nSkip to content\nGo to bottom\nREFERENCE LINKING PLATFORM OF KOREA S&T JOURNALS\n> Journal Vol & Issue\nKorean Journal of Poultry Science\nJournal Basic Information\nJournal DOI :\nThe Korean Society of Poultry Science\nEditor in Chief :\nVolume & Issues\nVolume 41, Issue 4 - Dec 2014\nVolume 41, Issue 3 - Sep 2014\nVolume 41, Issue 2 - Jun 2014\nVolume 41, Issue 1 - Mar 2014\nSelecting the target year\nEffect of Addition Levels of Duck Meat on Quality Characteristics of Emulsion Type Sausages during Cold Storage\nKang, Geunho ; Ham, Hyoung-Joo ; Seong, Pil-Nam ; Cho, Soohyun ; Moon, Sungsil ; Park, Kyoungmi ; Kang, Sun Mun ; Park, Beom-Young ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 77~85\nDOI : 10.5536/KJPS.2014.41.2.77\nThis study was conducted to investigate the effect of duck and pork meat mixing ratio on quality characteristics of emulsion type sausage at\nduring 5 weeks. Treatments on the basis of the meat content were subjected to 100% duck meat (T1), 100% pork meat (T2), 50% duck meat + 50% pork meat (T3), 40% duck meat + 60% pork meat (T4), and 30% duck meat + 70% pork meat (T5). The moisture content was significantly (p<0.05) higher in TI sample but significantly (p<0.05) lower in T2 sample than those in comparison to the other treatments. Crude protein and fat content were significantly (p<0.05) higher in T2 sample compared to the other treatments. CIE\nvalue was significantly (p<0.05) lower in T1 sample than those of other treatments until 5 weeks of cold storage. CIE\nvalue was significantly (p<0.05) higher in T1 sample but significantly (p<0.05) lower in T2 sample than those in comparison to the other treatments until 5 weeks of cold storage. Hardness was significantly (p<0.05) higher in T4 sample compared to the other treatments during all cold storage. Cohesiveness was significantly (p<0.05) higher in T5 sample compared to the other treatments until 2 weeks of cold storage. The results of sensory evaluation showed that the meat flavor, taste and texture were significantly (p<0.05) lower in T5 sample compared to the other treatments whereas no difference among treatments except T5 sample. Overall acceptability was significantly (p<0.05) lower in T5 sample compared to the other treatments. Therefore, these results suggested that the ratio of 40% duck meat and 60% pork meat is appropriate levels for hardness and palatability when manufacturing emulsion type sausage with duck meat.\nNormal Development and Hatchability of Korean Oge Chickens in White Leghorn Surrogate Eggshells\nChoi, Hee Jung ; Kang, Kyung Soo ; Lee, Hyung Chul ; Lee, Hyo Gun ; Rengaraj, Deivendran ; Park, Tae Sub ; Han, Jae Yong ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 87~92\nDOI : 10.5536/KJPS.2014.41.2.87\nThe avian embryos have been used as a good model to study embryonic development. Due to its unique development in the eggshell, avian embryos can be cultured and hatch in the surrogate eggshell system. In this study, we examined the viability, normal development and hatchability of Korean Oge (KO) chicken embryos in White Leghorn (WL) surrogate eggshells. Donor KO embryos at 3-day and 4-day-old were transferred into recipient WL eggshells, incubated for further 18 days at\nwith 70% of humidity until hatching. The viability of 3-day-old KO embryos at 7, 14 and 21 day in surrogate eggshell were 70.0%, 43.8% and 23.1%, respectively. In contrast, the viability of 4-day-old KO embryos at 7, 14 and 21 day in surrogate eggshells were 87.1%, 55.6% and 36.0%, respectively. The hatchability of KO embryos transferred into surrogate eggshells at 3-day-old was 23.1%, whereas embryos transferred at 4-day-old was 36.0%. Furthermore, the development of all viable embryos from 3-day group and 4-day group were normal. Our results suggested that culture of KO embryos in WL surrogate eggshells is highly possible, and transfer of donor embryos at 4-day-old may yield higher percentage of hatchability. This study may provide potential knowledge for the conservation of wild and endangered birds through surrogate system.\nAnalysis of HACCP System Implementation on Productivity, Advantage and Disadvantage of Laying Hen Farm in Korea\nNam, In Sik ; Kim, Hyung Sik ; Seo, Kang Min ; Ahn, Jong Ho ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 93~98\nDOI : 10.5536/KJPS.2014.41.2.93\nThis study was conducted to analysis the reason for implementing HACCP system, advantage and disadvantage of HACCP system implemented laying hen farm. The study was carried out by randomly selected fifteen laying hen farms located in all around Korea. All data were collected from fifteen laying hen farms before and after the implementation of HACCP system. The results were as follows: The egg production rate, livability rate and monthly used animal medicine fee did not changed after HACCP system implementation. However, monthly used disinfectant fee tended to be higher in HACCP farm compared to non-HACCP farm. 26.92% of the laying hen farmer responded enhancement of their farm competitiveness as the major propose for implementing HACCP system. The advantages of HACCP implemented laying hen farms were methodical farm management (22.39%), improvement of awareness (21.18%), improvement of the farm sanitation management level (15.30%), safety egg production (15.05%), productivity enhancement (7.29%), reduction of mortality rate (6.82%), and improvement of labor`s welfare (5.89%). The disadvantages of HACCP implemented laying hen farms were HACCP recording (43.30%), alteration of consciousness (22.60%), HACCP monitoring (11.11%), HACCP education (9.97%), HACCP verification (6.90%), and A high turnover of labor (6.13%). In conclusion, implementation of HACCP system to laying hen farm did not affect on the productivity or the use of animal medicine. However, the HACCP system may enhance safety and sanitation of egg production for consumer.\nA Case Study on the Exogenous Factors affecting Extra-large Egg Production in a Layer Farm in Korea\nLee, Hyun-Chang ; Jang, Woo-Whan ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 99~104\nDOI : 10.5536/KJPS.2014.41.2.99\nThe objective of this study is to analyze the production of extra-large egg and assess the impacts of exogenous factors in feeding the layer chicken. The main results of this study are as follows; First, feeding rations on the basics of statistics, internal maximum and minimum temperature and, the age at first egg affect the production of extra-large egg. Second, implicating the standardized coefficients from the conclusion of regression model estimating suggest that the amount of feed has the greatest impact on production followed by the age at first egg. Third, by using the elasticity of output and the volatility in the production, the result suggest that among the independent variable factors in the external volatility, the biggest one goes to feed ration, and the age at first egg follows. In order to control the production volatility in the extra-large egg production of the farms, it is necessary to manage an efficient feeding based on feed ration, age at first egg and, the maximum and minimum temperature inside the farm. Taken together, the results demonstrates that it should be concentrated by controlling the exogenous factors affecting extra large egg production and the management system construct, to increase extra-large egg production and the income of farmers at the same time.\nEffects of Dietary Supplementation of Coffee Meal on Intestinal Enzyme Activity, Biochemical Profiles and Microbial Population in Broiler Chicks\nKo, Young-Hyun ; Yun, Seo-Hyun ; Song, Min-Hae ; Kim, Se-Yun ; Kim, Jong-Sun ; Kim, Hyoun-Wook ; Jang, In-Surk ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 105~113\nDOI : 10.5536/KJPS.2014.41.2.105\nThe current study was performed to investigate the effects of dietary supplementation of dried coffee meal (CM) on growth performance, intestinal and blood biochemical index, intestinal enzymes, and cecal microbial populations. A total of 162, 3-day-old male broiler chicks were randomly allocated into three dietary groups: control group (CON), basal diet added with 0.5% CM (CM I), and basal diet added with 1.0% CM (CM II). Dietary supplementation of CM did not change bird performance and the relative weight of intestinal mucosal tissues. The birds fed the diet supplemented with CM (0.5 and 1.0%) significantly decreased mucosal glucose concentration (P<0.05) without affecting blood glucose level compared with those fed control diet. The level of blood aspartate aminotransferase (AST) significantly increased in CM II group (P<0.05) without affecting\n-glutamyl transpeptidase (\n-GTP) compared with that in the CON group. The specific activity of intestinal maltase, leucine aminopeptidase (LAP) and alkaline phosphatase (ALP) were not affected by dietary supplementation of CM, whereas sucrase activity in birds fed the diet supplemented with CM was decreased (P<0.05) compared to that in the control birds. The colony forming units (CFU) of E. coli in the cecum of CM-fed birds was significantly decreased (P<0.05) compared with that of control birds without changing the CFU of Lactobacillus. In conclusion, dietary supplementation of lower level of CM (0.5%) can be used as a beneficial feed resource without liver toxicity in broiler chicks.\nComparison of Stress Response between Korean Native Chickens and Single Comb White Leghorns subjected to a High Stocking Density\nSohn, Sea Hwan ; Cho, Eun Jung ; Park, Dhan Bee ; Jang, In Surk ; Moon, Yang Soo ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 115~125\nDOI : 10.5536/KJPS.2014.41.2.115\nWith Single Comb White Leghorn (WL) and Korean Native Chicken (KNC) breeds, we compared the stress response with chicken breeds that were subjected to a high stocking density. Stress response was analyzed by the quantity of telomeric DNA, the rate of DNA damage and the expression levels of heat shock proteins (HSPs) and hydroxyl-3-methyl-glutaryl coenzyme A reductase (HMGCR) genes on tissues and blood. The telomere length and telomere shortening rates were analyzed by quantitative fluorescence in situ hybridization on the nuclei of lymphocytes and tissues. The DNA damage rate of lymphocytes was quantified by the comet assay. The expression levels of HSP70, HSP90-\nand HMGCR genes were measured by quantitative real-time polymerase chain reaction in lymphocytes. There was no significant difference between KNC and WL in body weight, weight gain, telomere shortening rate and DNA damage rate. However, the growth rate significantly decreased in chickens raised under high stocking density conditions, as compared to the control group. The telomere-shortening rate, DNA damage and HSPs expression of the lymphocytes were significantly higher in the high stocking density group than the control. The stress condition and breeds had a significant effect on the expressions of HSP70, HSP90-\nin lymphocytes, except HMGCR. The stress response of WL was higher than that of KNC, as analyzed to the expression of HSP70 and HSP90-\n. Therefore, we concluded that the chickens which were exposed to a high stocking density had increased the individual physiological stress response regardless of breeds, and White Leghorns are more susceptible to stress condition than Korean Native Chickens.\nEffect of Adding Lactobacillus-Fermented Solution on Characteristics of Chicken Breast Meat\nKim, Sun Hyo ; Jayasena, Dinesh D. ; Kim, Hyun-Joo ; Jo, Cheorun ; Jung, Samooel ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 127~133\nDOI : 10.5536/KJPS.2014.41.2.127\nThe effect of Lactobacillus-fermented solution (LFS) at a concentration of 0, 1 and 2% on shelf-life extension in terms of total aerobic bacteria and on color, lipid oxidation, and sensorial characteristics of injected chicken breast meat was tested during a 9-day storage period at\n. Throughout the whole storage days, addition of LFS showed the significant inhibition of total aerobic bacteria counts in chicken breast meat compared with that of control. However, the addition of LFS to chicken breast meat resulted in the decrease of pH, the increase of\nvalues, and the increase of lipid oxidation in chicken breast meat when compared with those of control at any given storage period (P<0.05). In addition, the chicken breast meat added with LFS was subjected to low scores in sensorial properties such as flavor, taste, tenderness, and overall acceptability. The results suggested that LFS can be used for improving the shelf-life of chicken meat processing product, however, further study to prevent the deterioration of quality such as lipid oxidation and sensorial property is needed.\nThe Dietary Effects of Marigold Extracts on Egg Production, Egg Quality and the Production of Lutein Fortified Chicken Eggs\nKim, Eun-Jib ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 135~142\nDOI : 10.5536/KJPS.2014.41.2.135\nThis study was conducted to evaluate dietary effects of Marigold extract on laying performance, egg quality, oxidative stability of egg yolk and lutein transfer into chicken eggs. A total of one-hundred eighty nine 55-wk-old Hy-Line Brown layers were divided into seven groups and fed control diet or each experimental diet containing 0.1, 0.3, 0.5, 1.0, 1.5 or 2.0% Marigold extract. Egg production, egg weight and daily egg mass were not affected by dietary treatments. The yolk colors in groups fed diets containing Marigold extract were significantly higher than that of control. The Haugh unit were tended to be improved by feeding of diets containing Marigold extract although there were no significant difference in egg shell strength and thickness. The MDA (malondialdehyde) contents in groups fed diets containing Marigold extract above 0.5% were significantly reduced than that of control. After 14d of storage, the Haugh unit values in groups feed diets containing 0.3 and 1.0% Marigold extract were significantly higher than that of control (p<0.05). The concentration of lutein in egg yolk increased by feeding of Marigold extract. When 2% Marigold extract was supplemented to the diet, lutein content of egg was increased as much as 1.71 mg/60 g. These results indicated that the use of Marigold extract in layer diets was effective in egg quality and for the production of lutein fortified eggs.\nEffects of Dietary Supplementation of Mixed Probiotics on Production Performance and Intestinal Environment in Broiler Chicken\nOh, Seong Taek ; Kang, Chang Won ; Kim, Eun Jib ;\nKorean Journal of Poultry Science, volume 41, issue 2, 2014, Pages 143~149\nDOI : 10.5536/KJPS.2014.41.2.143\nThis study was conducted to investigate the effects of dietary supplementation of the mixture of probiotics (MP) on growth performance, size of small intestine, cecal microflora and ammonia concentrations in broiler chicks. A total of 700, one-day-old male broiler chicks were randomly allotted to four treatments with seven replications having 25 birds per pen. The birds were fed one of the four experimental diets; containing no antibiotics nor MP (negative control, NC), containing antibiotics without MP(positive control, PC), negative control with MP 0.1% and negative control with MP 0.2% for 5 weeks. During overall experiment, birds in PC and MP treatments had higher final BW and daily BW gains than birds in NC treatments; however, the significance was not identified. The feed conversion ratio of the chicks fed the diet containing MP was significantly improved as compared to those fed the NC diets. The weights of jejunum were increased by the MP (p<0.05), but weights of duodenum, ileum and length of small intestine were similar among the groups. Birds in PC treatment showed lower populations of total microbes and lactic acid bacteria than other groups (p<0.05), cecal ammonia concentrations of the chicks fed the diet containing MP were significantly decreased as compared to those of NC and PC (p<0.01). In conclusion, MP added to the broiler diets improved the feed conversion rate and reduced cecal ammonia concentration.']	['<urn:uuid:317517fb-2443-4cb2-a198-aa825c46ed92>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	15	56	2538
95	I'm building a disease prediction model for a medical application, and I want to make sure the probability estimates are reliable. Why is model calibration especially important in healthcare applications?	Model calibration is particularly crucial in healthcare applications because the exact probability values directly affect medical decisions and patient health outcomes. For example, when a model predicts the probability of contracting a specific disease based on analysis results, these probabilities need to be well-calibrated because they influence doctors' decisions. A well-calibrated model means that when it predicts something with 70% confidence, it should be correct 70% of the time. This is different from applications like news recommendation systems, where you only need to know which option has the highest probability rather than the exact probability values.	"[""Do you ever encounter a storm when the probability of rain in your weather app is below 10%? Well, this shows perfectly how your plans can be destroyed with a not well-calibrated model (also known as an ill-calibrated model, or a model with a very high Brier score).\nWhen building a prediction model, you take into account its predictive power by calculating different evaluation metrics. Some of them are common, like accuracy and precision. But others, like the Brier score in the weather forecasting model above, are often neglected.\nIn this tutorial you’ll get a simple, introductory explanation of Brier Score and calibration – one of the most important concepts used to evaluate prediction performance in statistics.\nWhat is the Brier Score?\nBrier Score evaluates the accuracy of probabilistic predictions.\nSay we have two models that correctly predicted the sunny weather. One with the probability of 0.51 and the other with 0.93. They are both correct and have the same accuracy (assuming 0.5 threshold) but the second model feels better right? That is where Brier score comes in.\nIt is particularly useful when we are working with variables that can only take a finite number of values (we can call them categories or labels too).\nFor example, level of emergency (which takes four values: green, yellow, orange, and red), or whether tomorrow will be a rainy, cloudy or sunny day, or whether a threshold will be exceeded.\nThe Brier Score is more like a cost function. A lower value implies accurate predictions and vice versa. The primary goal of dealing with this concept is to decrease it.\nThe mathematical formulation of the Brier Score depends on the type of predicted variable. If we are developing a binary prediction, the score is given by:\nWhere p is the prediction probability of occurrence of the event, and the term oi is equal to 1 if the event occurred and 0 if not.\nLet’s take a very quick example to assimilate this concept. Let’s consider the event A=”Tomorrow is a sunny day”.\nIf you predict that the event A will occur with a probability of 100%, and the event occurs (the next is sunny which means o=1), the Brier score is equal to:\nThis is the lowest value possible. In other words: the best case we can achieve.\nIf we predicted the same event with the same probability, but the event doesn’t occur, the Brier score in this case is:\nSay you predicted that the event A will occur with another probability, let’s say 60%. In case the event doesn’t occur in reality, the Brier Score will be:\nAs you may have noticed, the Brier score is a distance in the probability domain. Which means: the lower the value of this score, the better the prediction.\nA perfect prediction will get a score of 0. The worst score is 1. It’s a synthetic criterion that provides combined information on the accuracy, robustness, and interpretability of the prediction model.\nDotted lines represent the worst cases (if the event occurs, the circle is equal to 1).\nWhat is probability calibration?\nProbability calibration is the post-processing of a model to improve its probability estimate. It helps us compare two models that have the same accuracy or other standard evaluation metrics.\nWe say that a model is well calibrated when a prediction of a class with confidence p is correct 100p % of the time. To illustrate this calibration effect, let’s consider that you have a model that predicts cancer with a score of 70% for each patient out of 100. If your model is well calibrated, we would have 70 patients with cancer, if it is ill-calibrated, we will have more (or less). Therefore, the difference between these two models:\n- A model has an accuracy of 70% with 0.7 confidence in each prediction = well calibrated.\n- A model who has an accuracy of 70% with 0.9 confidence in each prediction = ill-calibrated.\nFor a perfect calibration, the relationship between the predicted probability and the fraction of positives follows the given:\nThe expression of this relationship is given by:\nThe figure above represents the reliability diagram of a model. We can plot it using scikit-learn as below:\nimport sklearn from sklearn.calibration import calibration_curve import matplotlib.lines as line import matplotlib.pyplot as plt x, y=calibration_curve(y_true, y_prob) plt.plot(x,y) ref = line.Line2D([0, 1], [0, 1], color='black') transform = ax.transAxes line.set_transform(transform) ax.add_line(line) fig.suptitle('Calibration – Neptune.ai') ax.set_xlabel('Predicted probability') ax.set_ylabel('Fraction of positive') plt.legend() plt.show()\nPlotting the reliability curve for multiple models allows us to choose the best model not only based on its accuracy, but on its calibration too.\nIn the figure below, we can eliminate the SVC (0.163) model because it is far from being well calibrated.\nIf we want a numeric value to check the calibration of our models, we can use the calibration error given theoretically by:\nWhen should you use the Brier score?\nEvaluating the performance of a machine learning model is important, but it’s not enough to evaluate the real-world application predictions.\nWe often worry about:\n- the model’s confidence in its predictions,\n- its error distribution,\n- and how probability estimates are made.\nIn such cases, we need to use other performance factors. Brier score is an example.\nThis type of performance score is specifically used in high-risk applications. This score allows us to not treat the model results as real probabilities, but instead go beyond the raw results and check the model calibration, which is important for avoiding bad decision making or false interpretation.\nExample of needing well-calibrated probabilities/model calibration\nLet’s consider that you want to build a model that shows news pages to users by the chance of clicking on them. If the chance of the user clicking on a suggested item is high, the item is shown on the main page. Else, we show another item with a higher chance.\nIn this kind of problem, we don’t really care about how much the exact chance of clicking is, but only which item has the highest chance between all of the existing items. The model calibration is not really crucial here. What matters is which one of them has the highest probability (chance) of being clicked.\nOn the other hand, consider a problem in which we build a model that predicts the probability of contracting a specific disease based on the output of some analysis. The exact value of the probability is crucial here because it affects the decision of the doctor and the health of the patient.\nGenerally, calibration is used to improve a model when the results show that it has mistakes with high probabilities (or prediction score when the model doesn’t output the probability estimate e.g. random forest).\nOf course, you can also look at other metrics that take prediction scores as input, like ROC AUC score, but they usually don’t focus on correctly calibrated probabilities. For example, ROC AUC focuses on ranking predictions.\nOk, now we can tell when the model is not well calibrated, but what can we do about it? How can we calibrate it?\nProbability calibration methods\nPeople use a lot of calibration methods. We will focus on the two most popular approaches:\nPlatt Scaling is often used to calibrate a model that we have already built. The principle of this method is based on the transformation of the outputs of our classification model into probability distribution.\nOur model will not only give a categorical result (label or class), but also a degree of certainty about the result itself.\nInstead of returning the class 1 as a result, it will return the probability of the correctness of this class prediction. Unfortunately, some classification models (like SVM) do not return probability values, or give poor probability estimates.\nThat’s why we use specific transformations to calibrate our model and convert the results into probability.\nTo use the Platt scaling method, we train our model normally, and then train the parameters of an additional sigmoid function to map the model outputs into probabilities.\nYou can do it using Logistic Regression fitted on the output of the model.\nfrom sklearn.linear_model import LinearRegression model=LinearRegression() model.fit(p_out, y_out) calib_p=model.predict(p_test)[:,1]\nIsotonic Regression does the same thing as Platt Scaling – they both transform model output into probability, and therefore calibrate it.\nWhat’s the difference?\nPlatt Scaling uses a sigmoid shape to calibrate the model, which implies a sigmoid-shaped distortion in our probability distribution.\nIsotonic Regression is a more powerful calibration method that can correct any monotonic distortion. It projects a non parametric function into a set of increasing functions (monotonic).\nIt is not recommended to use the isotonic regression if you have a small dataset, because it can easily overfit.\nTo implement this approach, we will again use sklearn (we assume you have already built and trained your “uncalibrated model”):\nfrom sklearn.linear_model import IsotonicRegression model=IsotonicRegression() model.fit(p_out, y_out) calib_p=model.transform(p_test)\nUsing model calibration on a real example\nLet’s get some practice!\nIn order to keep it unique, we will generate two classes using\nmake_classification from sklearn.\nAfter that, we will train and fit an SVM classifier on the dataset.\nfrom sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC X, y = make_classification(n_samples=2500, n_classes=2) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nWe have just created our unique random data, and split it into a training set and test set.\nNow we build the SVC Classifier, and fit it to the training set:\nfrom sklearn.svm import SVC svc_model=SVC() svc_model.fit(X_train, y_train)\nLet’s predict the results:\nNext, let’s plot the calibration curve that we talked about earlier:\nfrom sklearn.calibration import calibration_curve x_p, y_p=calibration_curve(y_test, prob, n_bins=10, normalize=’True’) plt.plot([0, 1], [0, 1]) plt.plot(x_p, y_p) plt.show()\nThe result of the code above is the calibration curve or reliability curve:\nThis shows that our classifier is ill-calibrated (the calibration reference is the blue line).\nNow let’s calculate the Brier score of this ill-calibrated model:\nfrom sklearn.metrics import brier_score_loss, roc_auc_score y_pred = svc_model.predict(X_test) brier_score_loss(y_test, y_pred, pos_label=2) roc_auc_score(y_test, y_pred)\nFor this, we get a ROC AUC score equal to 0,89 (means a good classification) and a Brief score equal to 0,49. Pretty high!\nThat explains the fact that the model is ill-calibrated. What do we do?\nLet’s calibrate our model using sklearn again. We will apply Platt scaling (calibrate using sigmoid distribution). Sklearn offers a predefined function that does the job:\nfrom sklearn.calibration import CalibratedClassifierCV calib_model = CalibratedClassifierCV(svc_model, method='sigmoid', cv=5) calib_model.fit(X_train, y_train) prob = calib_model.predict_proba(X_test)[:, 1] #plot the calibration curve (see above to know what is it) x_p, y_p = calibration_curve(y_test, prob, n_bins=10, normalize='True') plt.plot([0, 1], [0, 1]) plt.plot(x_p, y_p) plt.show()\nThe calibration or reliability curve of the model after calibration is shown below:\nThe reliability curve shows a tendency towards the calibration reference (the perfect case). For more verification, we can use the same numerical metrics as before.\nIn the following code, we calculate the Brier score and ROC AUC score of the calibrated model:\nfrom sklearn.metrics import brier_score_loss, roc_auc_score y_pred = calib_model.predict(X_test) brier_score_loss(y_test, y_pred, pos_label=2) roc_auc_score(y_test, y_pred)\nThe Brier score gets decreased after calibration (passed from 0,495 to 0,35), and we gain in terms of the ROC AUC score, which gets increased from 0,89 to 0,91.\nWe note that you may want to calibrate your model on a held-out set. In this case, we split the dataset to three parts:\n- We fit the model on the training set (first part).\n- We calibrate the model on the calibration set (second part).\n- We test the model on the testing set (third part).\nCalibrating your model is a crucial step to increase its prediction performance, especially if you care about “good” probability predictions that have a low Brier score.\nHowever, you should keep in mind that it’s not obvious that improved calibrated probabilities will contribute to better predictions based on class or probability.\nThis potentially depends on the particular evaluation metric you use to test predictions. According to some papers, SVM, decision trees, and random forest are more likely to be improved after calibration (in our example we used a support vector classifier).\nSo as always proceed with caution.\nI hope that, after reading this article you have a good understanding of what Brier score and model calibration are and you’ll be able to use that in your ML projects.\nThanks for reading!\n Brier GW. “Verification of forecasts expressed in terms of probability”. Mon Weather Rev 1950.\n Gneiting T, Raftery AE. “Strictly proper scoring rules, prediction, and estimation”. J Am Stat Assoc 2007.\n Is your model ready for the real world? – Inbar Naor – PyCon Israel Conference 2018\n A guide to calibration plots in Python – Chang Hsin Lee, February 2018.\nThe Ultimate Guide to Evaluation and Selection of Models in Machine Learning\n10 mins read | Author Samadrita Ghosh | Updated July 16th, 2021\nOn a high level, Machine Learning is the union of statistics and computation. The crux of machine learning revolves around the concept of algorithms or models which are in fact statistical estimations on steroids.\nHowever, any given model has several limitations depending on the data distribution. None of them can be entirely accurate since they are just estimations (even if on steroids). These limitations are popularly known by the name of bias and variance.\nA model with high bias will oversimplify by not paying much attention to the training points (e.g.: in Linear Regression, irrespective of data distribution, the model will always assume a linear relationship).\nA model with high variance will restrict itself to the training data by not generalizing for test points that it hasn’t seen before (e.g.: Random Forest with max_depth = None).\nThe issue arises when the limitations are subtle, like when we have to choose between a random forest algorithm and a gradient boosting algorithm or between two variations of the same decision tree algorithm. Both will tend to have high variance and low bias.\nThis is where model selection and model evaluation come into play!\nIn this article we’ll talk about:\n- What are model selection and model evaluation?\n- Effective model selection methods (resampling and probabilistic approaches)\n- Popular model evaluation methods\n- Important Machine Learning model trade-offs""]"	['<urn:uuid:5691cbff-0812-45de-9e97-af1c27b2a42b>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:43:46.574121	30	96	2352
96	what happens if eye damage repair delayed	If eyelid trauma is treated within 12-24 hours of the injury, there are very high chances of treating it effectively without future complications. However, the total healing time for eyelid trauma wounds takes around a complete year, even after surgery. Proper care must be taken by the patient to avoid complications.	['Trauma to the any body parts can pose a lot of challenges to a surgeon. Specifically, trauma of the eyelids can be quite complex in nature and may become a test of skill of an ophthalmologist. An ophthalmologic surgeon not only needs expertise in ocular as well as orbital anatomy but also great precision and skill as far as the trauma surgery of the eyelids is concerned.\nBefore choosing any surgical approach for the management of an eyelid trauma surgery, it is important that the entire ocular system is thoroughly checked. Both minor as well as major trauma can cause a lot of ocular problems which may cause retinal detachment, angle recession etc. the globe of the eye structure is dealt with before attending to the condition of the eyelids so that no undue pressure is applied to the globe. It is easier to make a surgical evaluation of the globe before the eyelids are treated.\nTo begin treatment of the trauma of the eyelids, it is very important to know how the injury was caused in the first place. It is crucial to know the nature of the injury, the time when the injury happened, if the patient was wearing any safety goggles or contact lenses and if there could be any foreign debris in the eyes. If the injury involves any chemicals it is important that the eyes are flushed out immediately. To understand the extent and the type of injuries, some surgeons compare the old picture of the patient like the one that may be present on an ID with current condition. This can give the surgeon an idea about how far the injury has been sustained. Before beginning the treatment the medical history of the patient including any present medical conditions and allergies should be communicated to the surgeon,\nBroadly, eyelid trauma injuries can be classified as blunt trauma injuries and penetrating trauma injuries. Though the classification of the injuries is exclusive, many injuries can have common characteristics.\nPenetrating trauma injuries can run deeper and cause much more damage. The levator muscles and the canthal tendons may get detached or tissue oedema may be caused due to blunt trauma injuries. If the eyelid skin has been penetrated there are chances of the presence of foreign bodies in the eyes. In some cases, the foreign particles may be present in the outer orbital area but it may not be evident during the testing.\nTo test for any ocular injury, a detailed step by step examination is done. The eyes are checked for vision, reaction of pupils to light, pressure in the ocular area and ocular movements. For evaluating the condition of the eyelids, both the nervous and the muscular condition of the eyelids are analyzed. The functions of the nervous system may be disrupted because of the trauma. The condition of the cornea is affected by the functioning of the nerves. In many cases, if the levator muscle is injured due to trauma and is not repaired it can lead to ptosis after the trauma surgery. If there is swelling present in the eyelids but the lid seems to function properly, this could mean that the integrity of the levator muscle has not been compromised. If the injury is caused by massive blunt trauma and foreign debris has entered the eyes, it may lead the eyelid tissues unusable. Therefore, great care is taken by the surgeon to assess what type of tissue and how much of the tissue is usable.\nSurgical repair of the eyelid trauma\nIf the eyelid trauma is addressed within 12 – 24 hours of the injury, there are very high chances of the treating the eyelid trauma effectively and without any future complications. It is very important to completely remove any foreign debris from the eyes before beginning the surgical treatment to eliminate any possibility of infection. The process of removal of foreign debris from the eyes is often done along with saline irrigation. If the eyelid injury is caused due to animal bites it can be repaired well, due to good blood supply. Before the wounds are sutured, irrigation is done on them to clean them and excess of blood clot is also removed. It is necessary to perform this to allow better healing of the wound. The edges of the skin tissues should be handled with great care by the surgeon so that healing is accelerated. The types of sutures that are placed are chosen by the surgeon. The sutures that are placed on the skin of the eyelid are removed within four to five days of the surgery. The total times for the wounds of an eyelid trauma take around a complete year to heal. Even after the surgery great care should be taken by the patient to avoid any complication.']	['<urn:uuid:7960e105-5fa0-4c33-84c9-ea540ef22d0b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:43:46.574121	7	51	801
97	knelson concentrator gold recovery mechanism	Knelson Concentrators use a combination of centrifugal force and a patented water injection process to recover precious metals. The system uses enhanced gravitational forces while maintaining a fluidized bed process, allowing it to capture fine gold and other precious metals with high specific gravities that were previously considered too fine for gravity devices.	"[""The Knelson Concentrator is a compact hatch centrifugal separator with an active fluidized bed to capture heavy minerals. A centrifugal force up to 60 times that of gravity acts on the particles, trapping denser particles in a series of rings (riffles) located in the\nNew and Used Knelson & Falcon Gold & Mineral Recovery Centrifugal Concentrators for Sale Savona Equipment is your source for New, Used, and Reconditioned Gold Recovery Centrifugal Concentrators including Knelson and Falcon Concentrators for recovering fine gold\nHere is how a gravity gold concentrator (centrifuge) works as a separator: Make sure the concentrator has been correctly assembled and that all bolts have been tightened. If required, attach a funnel to the material feed inlet. If necessary, connect a hose to the tailings outlet. Place the concentrator on a raised structure such as blocks.\nKnelson concentrators are the most widely used semi-continuous centrifuge separators for the recovery of gold and platinum minerals by gravity methods. Bench scale characterization studies on these units provide information about the occurrence of gold\nKnelson Concentrators are the market leaders in gravity concentrators, and are worth more than their weight in gold. Using centrifugal force and water injection processes, Knelson Concentrators create enhanced gravitational forces of up to 200 g’s to recover free gold, fine gold and any other precious metals or minerals with high specific gravities.\nThe semi-continuous (batch) Knelson Concentrator is the most widely utilised centrifugal gravity concentration device in the industry. Knelson Concentrators combine centrifuge enhanced gravitational force with a patented bed fluidization process to provide unmatched performance in the recovery of precious metals such as gold and fine gold, platinum, silver, mercury and native copper.\nThe patented water injection process combined with centrifugal force allowed Knelson Concentrators to recover gold that was previously considered too fine for gravity devices. The concentration cone in Byron’s first ever Knelson Concentrator spun at 60 G force, and since then extensive research has proven that for 90% of gravity gold\nSep 13, 2012 Knelson Batch How It Works 360p crownmining. Small scale gold mining Knudsen bowl gold concentrator Duration: Knelson VS Falcon Gold Concentrators Comparison\nThe trials involved a side by side comparison of two enhanced gravity concentration devices, of approximately the same rated capacity, which were supplied by Falcon Concentrators and Knelson Concentrators. The performance characteristics of these units were compared on a process stream taken from within the primary grinding circuit\nSep 14, 2018 Our new and used Knelson and Falcon Gold Concentrators are supplied in full working condition. If you have a Knelson Concentrator for sale please contact us. #savonaequipment #mining #\nSep 26, 2013 We have a full range of fine gold recovery equipment in all brands of gold centrifugal concentrators including knelsons with enhanced gravity separation. Category Science & Technology\nIn Alaska, ore samples from mining properties were processed using Knelson concentrators. In one case, material originally evaluated at $5.00 a cubic yard yielded over $12 a yard in recoverable gold when tested with a Knelson concentrator. In a second case, the recoverable gold\nJun 19, 2015 Inside a Knelson concentrator during its wash cycle. This centrifugal machine (similar to the other Brand 'Falcon') separates gold from waste rock using the forces of gravity. https://www\nRecovery of Gold Combining Cyanidation and Knelson Concentrator Y. Olyaei 1,2,*, J. Mansouri 2, H. Haghi 2, M. Eftekhari3 1 Pars Kani Mineral Industries Research and Development Company, Zarshoran Gold Plant, Takab, Iran 2 School of Mining Engineering, College of Engineering, University of Tehran, Tehran, Iran 3 Material Science Department, Faculty of engineering, Tarbiat Modares University\nKnelson Concentrators. Consep has represented the Knelson Concentrator since 1990, servicing the Australian and South East Asian markets. With over 500 installations in this region and over 3000 worldwide, the Knelson Concentrator is proven as the most efficient unit for free gold recovery.\nEffect of Operating Variables in Knelson Concentrators: A Pilot-Scale Study Gravity, Gold Recovery, Gravity Recoverable Gold, Knelson Concentrator . GRG characterization studies, at\nDispelling the myth: There are abundant documented cases of Knelson Concentrators recovering extremely fine gold “paint” or “powder” gold that floats on water and in some trials and installations in Africa, Knelsons have recovered this very fine gold downstream of competitor products that claimed to be better suited for the job.\nMar 16, 2014 Gold Cyclone Gold Super Concentrator Gold Hog. Loading Unsubscribe from Gold Hog? Final updates and releases to the Gold Cyclone. Running in the commercial mode. Skip navigation Sign in.\nA typical application for a Falcon SB Gravity Concentrator is recovering liberated precious metals (Au, Ag, Pt, etc.) within a grinding circuit. Outside grinding circuits, Falcon SB Gravity Concentrators are also used for precious metal separation from aggregate or placer deposits.. Falcon SB Gravity Concentrators are known as “Semi-Batch” Gravity Concentrators because they continually\nKnelson concentrator cooperates with a gold shaker is a kind of advanced gravity beneficiation model with high efficiency and good environmental protection performance. The theory and examples of using a Nelson concentrator to recover gold and other precious metal with a gravity separation process are discussed in order to reduce the use of\nCentrifugal Gold Concentrator Working Principle Centrifugal gold concentrator is a relatively new type of gravity concentration apparatus. The machines utilise the principles of a centrifuge to enhance the gravitational force experienced by feed particles to effect separation based on particle density.\nhas been developing and optimizing a standard technology for gravity recoverable gold (GRG) with a 7.5 cm Knelson Concentrator manufactured by Knelson Gold Concentrators Inc, which is located in Langley, B.C. Canada [1, 2]. The standard laboratory procedure can simply be described as follows: a representative sample, with a\nconcentrator and a Knelson concentrator. No significant improvements were observed with these trials probably due to the nature of the unfriendly gold particles. Prior to these trials in the gravity plant, several trials were conducted in the flotation circuit using a Falcon concentrator a Knelson concentrator\nRecovery of gold from a free-milling ore by centrifugal gravity separator. Pr evious studies ha We attempted to recover the ground free gold particles using a knelson concentrator to\nWe have more CENTRIFUGAL CONCENTRATORS available also. Call us for more details. AN INDUSTRY LEADER SINCE 1992 RELY ON US FOR YOUR NEW AND USED EQUIPMENT NEEDS . KNELSON KC-CVD6 GOLD CONCENTRATOR WITH ICS AUTOMATED CONTROLS back to SUB-CATEGORY. Category: CONCENTRATORS. SubCategory: CENTRIFUGAL CONCENTRATORS.\nKnelson Centrifugal Concentrators for Sale Gravity Gold . Centrifugal Gold Concentrators New and Used Knelson Falcon Gold Mineral Recovery Centrifugal Concentrators for Sale Savona Equipment is your source for New, Used, and Reconditioned Gold Recovery Centrifugal Concentrators including Knelson and Falcon Concentrators for recovering fine gold and other precious metals from\nKnelson concentrator Wikipedia. A Knelson concentrator is a type of gravity concentration apparatus, predominantly used in the gold mining industry It is used for the recovery of fine particles of free gold, meaning gold that does not require gold cyanidation for recovery. 24/7 Online; nelson centrifuge for gold extraction plant\nAlibaba offers 2,793 gold centrifugal concentrator products. About 93% of these are mineral separator, 1% are separation equipment, and 1% are laboratory centrifuge. A wide variety of gold centrifugal concentrator options are available to you, such as gravity separator, sprial separator.\nThe simulated Knelson concentrator in this investigation was a 7.5-cm (3″) laboratory Knelson concentrator from IMPRC, Iran. The Knelson concentrator inner bowl is made of polyurethane and consists of five collecting rings with different sizes in diameter. This bowl is located concentrically within a cylindrical stainless steel outer bowl. 3.1.\nNewly Designed Knelson Type Gravity Gold Centrifugal Concentrator For Sale,Find Complete Details about Newly Designed Knelson Type Gravity Gold Centrifugal Concentrator For Sale,Gold Concentrator,Knelson Gold Concentrator,Gold Centrifugal Concentrator from Mineral Separator Supplier or Manufacturer-Jiangxi Jinshibao Mining Machinery Manufacturing Co., Ltd.\ncarried out on the Knelson Concentrator KC XD-20 at the High Pressure Leach (HPL) plant at Kansanshi Mining PLC (KMP), Zambia. The concentrate sent to HPL plant contains approximately 8 g/t of gold. This study involved the optimization of various parameters and performance determination of\n(GRG) can provide the missing data. Case studies are presented and used to illustrate the concepts of recovery effort and aggressive gravity recovery. INTRODUCTION Gold gravity recovery has evolved significantly over the past twenty years, largely because of the advent of the Knelson Concentrator.\nA wide variety of knelson concentrator price options are available to you, such as free samples. There are 129 knelson concentrator price suppliers, mainly located in Asia. The top supplying country or region is China, which supply 100% of knelson concentrator price respectively.\nCopyright © 2022 - All Rights Reserved""]"	['<urn:uuid:c7ff1587-44dd-4a92-98d3-388e01e70cc4>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:43:46.574121	5	53	1430
98	What can I spot on the Willow City Loop in Texas?	During spring months on the Willow City Loop, you'll likely encounter bluebonnets, coreopsis, sunflowers, firewheels, and many other wildflowers.	['Spring has sprung, and the wildflowers are here! Spectacular color displays are dotting hillsides and wilderness areas across the nation. It’s time to hop in the car and experience the blooms while they last.\nAlthough sightseers can spot wildflowers year-round, they’re most abundant during the spring due to the influx of sunlight after months of rain or snow. Road trips are a good way to get outside, see the sights, and cover a lot of ground.\nWhile there are plenty of options for locations to view the flowers, not all road trips are created equal. Here, we’ve rounded up five of the best U.S. road trips for viewing wildflowers as well as favorite places to stop, the types of flowers you can expect to see, and the best times to visit.\nKey Items to Bring\n- Camera for documenting your sightings (could be a smartphone camera or a DSLR)\n- Hiking boots for exploring on foot\n- Field guide for identifying flowers\n- Binoculars for birding and faraway views\n- Water bottle (gotta have the H2O)\n- Backpack for stashing all the essentials\nBest Spring Road Trips for Wildflower Viewing\nGeorge Washington Memorial Parkway: Virginia & Maryland\nSpanning approximately 27 miles across Virginia and Maryland, the Georgia Washington Memorial Parkway is a drive through history with scenic views along the way.\nThe drive follows the Potomac River and passes George Washington’s Mount Vernon home, the U.S. Capitol, the Arlington National Cemetery, Theodore Roosevelt Island, and many other historical locations.\nDuring the springtime, over 500 species of wildflowers have been documented along the highway, with the most spectacular showing in Turkey Run Park. In the park, the Potomac Heritage Trail offers a variety of popular hikes, with an assortment of flowers such as the Virginia bluebells, purple spring cress, and blue phlox along the way.\nFor the best chance of viewing the wildflowers, visit between mid-to-late March to mid-to-late April.\nNewfound Gap Road: North Carolina & Tennessee\nBeginning at the Sugarlands Visitor Center and finishing at the Mountain Farm Museum, the 34-mile road traverses through the famous Great Smoky Mountains National Park.\nThe drive takes anywhere from 60 to 90 minutes depending on how many detours you make. Popular stopping points include the Campbell Overlook, Chimney Tops Picnic Area, Newfound Gap, Clingmans Dome Road, Smokemont Campground, Mingus Mill, and Oconaluftee Visitor Center.\nIf you visit during the springtime, you’ll be treated to views of mountain laurels, catawba rhododendron, flame azalea, and an assortment of other wildflowers along the way.\nFlowers can be spotted anytime between February and September, but to maximize your chances, visit between early spring and early summer.\nJoshua Tree National Park: California\nLocated in Southern California, Joshua Tree National Park is known for its stark desert landscape, rugged rock formations, and the Joshua trees from which it gets its name.\nTo make the most of a day at the park, begin at the Cottonwood Visitor Center and drive the 50-mile road through the park to the town of Joshua Tree. Popular stopping points along the way include Cottonwood Spring, Ocotillo Patch, Chollo Cactus Garden, Arch Rock, Skull Rock, and Barker Dam. The timing and expanse of the wildflower blooms depend on fall and winter precipitation levels and spring temperatures.\nFlowers typically begin to bloom at lower elevations in February and at higher elevations in March and April. Popular sightings include the desert paintbrush, Utah firecracker, evening primrose, Mojave aster, grizzly bear prickly pear cactus, and many others.\nTexas Hill Country: Texas\nThe Texas Hill Country is home to a variety of landscapes such as rolling hills, rocky canyons, grasslands, woodlands, and savanna that is dotted with small towns, secluded swimming holes, and, of course, wildflowers.\nWhile there are many options for road trips in the Hill Country, a popular choice is to begin in Austin and work your way the 45 miles across the countryside to Gruene Hall in New Braunfels. Must-see stops along the way are the Hamilton Pool Preserve, LBJ State Park and Historic Site, Enchanted Rock State National Area, Cave Without a Name, and Guadalupe River State Park.\nAlthough wildflowers can be seen along the entire drive in the spring, the Willow City Loop is a big hit during the spring months, as there are wildflowers aplenty. On the hike, you’ll likely encounter bluebonnets, coreopsis, sunflowers, firewheels, and many more.\nPrimetime for blooms tends to be during March, April, and May.\nColumbia River Highway: Oregon\nCompleted in 1921, the Columbia River Highway was one of the first U.S. highways designed for scenic touring. The 350-mile highway begins in Astoria and ends in Pendleton, passing through temperate rainforest and by maples, conifers, ferns, and a collection of wildflowers.\nThere is much to see along the route, but highlights include a collection of jaw-dropping waterfalls: LaTourell Falls, Shepperd’s Dell Falls, Bridal Veil Falls, Wahkeena Falls, Multnomah Falls, Oneonta Falls, and Horsetail Falls.\nYou can spot flowers all along the route, but an especially good place to see them is in Hood River, specifically along the Mosier Plateau Trail.\nHundreds of species of wildflowers have made the Columbia River Gorge their home, including the Columbia kittentail, long-beard hawkweed, balsamroot, lupine, Howell’s daisy, northern wormwood, smooth desert parsley, and many more.']	['<urn:uuid:2fcad682-f829-433c-acc0-d69ea6954d52>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:43:46.574121	11	19	872
99	bypass emergency functions alcohol lock system frankfurt drunk driving consequences	The alcohol lock system offers two bypass options: the Bypass function, which can be activated multiple times by pressing the left-hand stalk switch OK button and hazard warning button for 5 seconds, and the Emergency function, which can only be used once before requiring workshop reset. In Frankfurt, the consequences for drunk driving are severe: first-time offenders with levels between 0.5‰ and 1.1‰ face €500 fines, second offenses result in €1000 fines and 3-month license suspension, and third offenses lead to €1500 fines and license suspension, while levels above 1.1‰ result in six months jail time and 5-year license revocation.	"['- Avoid eating or drinking approx. 5 minutes before the breath test.\n- Avoid excess windscreen washing - the alcohol in the washer fluid may result in an incorrect measurement result.\nChange of driver\nIn order to ensure that a new breath test is carried out in the event of a change of driver - depress the switch (2) and the send button (3) simultaneously for approx. 3 seconds. At which point the car returns to start inhibition mode and a new approved breath test is required before starting the engine.\nCalibration and service\nThe alcohol lock must be checked and calibrated at a workshopAn authorised Volvo workshop is recommended. every 12 months.\n30 days before recalibration is necessary the combined instrument panel shows the message Alcoguard Calibration required See manual. If calibration is not carried out within these 30 days then normal engine starting will be blocked - only starting with the Bypass function will then be possible, see the following heading ""Emergency situation"".\nThe message can be cleared by pressing the send button (3) once. Otherwise it goes out on its own after approx. 2 minutes but then reappears each time the engine is started - only recalibration at a workshopAn authorised Volvo workshop is recommended. can clear the message permanently.\nCold or hot weather\nThe colder the weather the longer it takes before the alcohol lock is ready for use:\nMaximum heating time (seconds)\n+10 to +85\n-5 to +10\n-40 to -5\nAt temperatures below -20 ºC or above +60 ºC the alcohol lock requires additional power supply. The combined instrument panel shows Alcoguard Please insert power cable. In which case, connect the power supply cable from the glovebox and wait until indicator lamp (6) is green.\nIn extremely cold weather the heating time can be reduced by taking the alcohol lock indoors.\nIn the event of an emergency situation or the alcohol lock is out of order, it is possible to bypass the alcohol lock in order to drive the car.\nAll Bypass activation is logged and saved in memory, see Recording data.\nAfter the Bypass function has been activated the combined instrument panel shows Alcoguard Bypass enabled the whole time while driving and can only be reset by a workshopAn authorised Volvo workshop is recommended..\nThe Bypass function can be tested without the error message being logged - in which case, carry out all the steps without starting the car. The error message is cleared when the car is locked.\nWhen the alcohol lock is installed, either the Bypass or Emergency function is selected as the bypassing option. This setting can be changed afterwards at a workshopAn authorised Volvo workshop is recommended..\nActivating the Bypass function\n- Depress and hold the left-hand stalk switch OK button and the button for hazard warning flashers simultaneously for approx. 5 seconds - the combined instrument panel first shows Bypass activated Please wait for 1 minute and then Alcoguard Bypass enabled - after which the engine can be started.\nThis function can be activated several times. The error message shown during driving can only be cleared at a workshopAn authorised Volvo workshop is recommended..\nActivating the Emergency function\n- Depress and hold the left-hand stalk switch OK button and the button for hazard warning flashers simultaneously for approx. 5 seconds - the combined instrument panel shows Alcoguard Bypass enabled and the engine can be started.\nThis function can be used once, after which a reset must be made at a workshopAn authorised Volvo workshop is recommended..', 'Penalties for the violation of traffic regulation, performed while driving hired cars in Frankfurt\nIf you have violated any traffic rule in Frankfurt, you should mind that your refusal to pay the penalty in place may lead to more deplorable consequences. After the police officers apply to the court, you will be obliged to pay significantly higher amount of money than initially. That is why it is recommended not to do beyond the law in any case. Here is the list of penalty figures, which are due for payment by the traffic rules breachers:\n- Parking in the pedestrian zone will cost you € 15;\n- Tailgate, depending on the speed rate, will cost you from € 25 to € 400;\n- For the wrongly performed overtaking you will need to pay € 70;\n- Having gone through a red traffic light you will pay the penalty, depending on the time period during which the prohibited light was flashing. For less than one second you should pay € 90, and for more than one second you should pay € 200;\n- For the overtaking performed in the prohibited place, i.e. the place marked with the «overtaking prohibited» sign, your wallet will become easier for € 150.\nIn short, if you are going to book a car in Frankfurt, you should better follow the local traffic regulations.\nIf you have decided to park in the forbidden place, you are more likely to receive the penalty for this. A «forbidden place» means, for example, a place located against the traffic direction. Or a parking lot intended only for the local residents. Then again, the law-abiding Frankfurters will be very dissatisfied if you exceed the prepaid parking timeline. Special automated devices are used for controlling such «rebels». These devices record the parking entrance and exit time. Here are the sums you will need to pay in case of untimely exit from the parking zone:\n- Less than 30 minutes — € 10;\n- One hour — € 15;\n- Two hours — € 20;\n- Three hours — € 25;\n- More than three hours — € 30.\nThe admissible blood alcohol content limit fixed for the people driving rented cars in Frankfurt\nOf course it would be more pleasant to hope that if a car is rented, especially in such a law-abiding city as Frankfurt, nobody will drink alcohol while being at the wheel. However, if you have still been tempted by the famous German beer, then you should mind that the blood alcohol content level should not exceed 0,3‰. At the same time, if you have not committed any violation, while the tester shows the 0,5‰ level, but not higher, then the things are going to be alright for you, even if you are not a local resident, but just a client of a car rental agency in Frankfurt.\nBlood alcohol level exceeding:\n- The level higher than 0,5‰, but lower than 1,1‰ is punished by the penalty of € 500. But this is just for the beginning;\n- For the second time the penalty will be € 1000, and the driver’s license will be deprived for the term of 3 months;\n- The third case of the blood alcohol level exceeding will be punished by the penalty of € 1500 and driver’s license deprivation for the term of 3 months;\n- If the level is higher than 1,1‰, it will result in a six months jail term and driver’s license deprivation for the period of 5 years;\n- If your driving experience is less than 2 years, or the age is under 21, you are forbidden to drink alcohol while being at the wheel. That means that the blood alcohol content level should be equal to 0‰.\nThere are no toll roads in Frankfurt and its surroundings. Во Франкфурте и его окрестностях нет платных автодорог. The two toll tunnels and a beautiful panoramic motorway for the tourists are the only exceptions. However, they are located pretty far away from this city. The panoramic road, for example, is found on the border with Austria. So, all motorways in Germany, including Frankfurt, are open for your perceptual car trip!']"	['<urn:uuid:45fc2382-51a3-4827-8858-0a5c89cd91f2>', '<urn:uuid:4f9c7916-041f-4c87-b3a1-8f4602aebdbb>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:43:46.574121	10	100	1285
100	atomic clocks quantum computing risks impact	Atomic clocks in Galileo satellites are extremely precise, keeping time to one second in three million years, which is crucial for accurate navigation. However, quantum computing poses a significant threat to these systems. As Quantum as a Service (QaaS) becomes available, it could enable adversaries to decrypt secure satellite communications and potentially compromise the timing systems, undermining the security of satellite-based services.	"[""Europe’s Galileo satellite navigation system – already serving users globally – has now provided a historic service to the physics community worldwide, enabling the most accurate measurement ever made of how shifts in gravity alter the passing of time, a key element of Einstein’s Theory of General Relativity.\nTwo European fundamental physics teams working in parallel have independently achieved about a fivefold improvement in measuring accuracy of the gravity-driven time dilation effect known as ‘gravitational redshift’.\nThe prestigious Physical Review Letters journal has just published the independent results obtained from both consortiums, gathered from more than a thousand days of data obtained from the pair of Galileo satellites in elongated orbits.\n“It is hugely satisfying for ESA to see that our original expectation that such results might be theoretically possible have now been borne out in practical terms, providing the first reported improvement of the gravitational redshift test for more than 40 years,” comments Javier Ventura-Traveset, Head of ESA’s Galileo Navigation Science Office.\n“These extraordinary results have been made possible thanks to the unique features of the Galileo satellites, notably the very high stabilities of their onboard atomic clocks, the accuracies attainable in their orbit determination and the presence of laser-retroreflectors, which allow for the performance of independent and very precise orbit measurements from the ground, key to disentangle clock and orbit errors.”\nThese parallel research activities, known as GREAT (Galileo gravitational Redshift Experiment with eccentric sATellites), were led respectively by the SYRTE Observatoire de Paris in France and Germany’s ZARM Center of Applied Space Technology and Microgravity, coordinated by ESA’s Galileo Navigation Science Office and supported through its Basic Activities.\nHappy results from an unhappy accident\nThese findings are the happy outcome of an unhappy accident: back in 2014 Galileo satellites 5 and 6 were stranded in incorrect orbits by a malfunctioning Soyuz upper stage, blocking their use for navigation. ESA flight controllers moved into action, performing a daring salvage in space to raise the low points of the satellites’ orbits and make them more circular.\nOnce the satellites achieved views of the whole Earth disc their antennas could be locked on their homeworld and their navigation payloads could indeed be switched on. The satellites are today in use as part of Galileo search and rescue services while their integration as part of nominal Galileo operations is currently under final assessment by ESA and the European Commission.\nHowever, their orbits remain elliptical, with each satellite climbing and falling some 8500 km twice per day. It was these regular shifts in height, and therefore gravity levels, which made the satellites so valuable to the research teams.\nReenacting Einstein’s prediction\nAlbert Einstein predicted a century ago that time would pass more slowly close to a massive object, a finding that has since been verified experimentally several times – most significantly in 1976 when a hydrogen maser atomic clock on the Gravity Probe-A suborbital rocket was launched 10 000 km into space, confirming Einstein’s prediction to within 140 parts per million.\nIn fact, atomic clocks aboard navigation satellites must already take into account the fact that they run faster up in orbit than down on the ground – amounting to a few tenths of a microsecond per day, which would result in navigation errors of around 10 km daily, if uncorrected.\nThe two teams relied upon the stable timekeeping of the passive hydrogen maser (PHM) clocks aboard each Galileo – stable to one second in three million years – and kept from drifting by the worldwide Galileo ground segment.\n“The fact that the Galileo satellites carry passive hydrogen maser clocks, was essential for the attainable accuracy of these tests,” noted Sven Hermann at the University of Bremen’s ZARM Center of Applied Space Technology and Microgravity.\n“While every Galileo satellite carries two rubidium and two hydrogen maser clocks, only one of them is the active transmission clock. During our period of observation, we focus then on the periods of time when the satellites were transmitting with PHM clocks and assess the quality of these precious data very carefully. Ongoing improvements in the processing and in particular in the modelling of the clocks, might lead to tightened results in the future.”\nRefining the results\nA key challenge over three years of work was to refine the gravitational redshift measurements by eliminating systematic effects such as clock error and orbital drift due to factors such as Earth’s equatorial bulge, the influence of Earth’s magnetic field, temperature variations and even the subtle but persistent push of sunlight itself, known as ‘solar radiation pressure’.\n“Careful and conservative modelling and control of these systematic errors has been essential, with stabilities down to four picoseconds over the 13 hours orbital period of the satellites; this is four millionth of one millionth of a second,” Pacôme Delva of SYRTE Observatoire de Paris.\n“This required the support of many experts, with notably the expertise of ESA thanks to their knowledge of the Galileo system.”\nPrecise satellite tracking was enabled by the International Laser Ranging Service, shining lasers up to the Galileos’ retro-reflectors for centimetre-scale orbital checks.\nMajor support was also received from the Navigation Support Office based at ESA's ESOC operations centre in Germany, whose experts generated the reference stable clock and orbit products for the two Galileo eccentric satellites and also determined the residual errors of the orbits after the laser measurements.\nFor more information:\nDr Pacôme Delva\nSYRTE, Observatoire de Paris, Université PSL, CNRS, Sorbonne Université, LNE\nDr Sven Hermann\nZARM Center of Applied Space Technology and Microgravity\nDr Javier Ventura-Traveset\nHead of Galileo Navigation Science Office, ESA\nDr Erik Schoenemann\nESOC Navigation Support Office, ESA"", 'One of the most surprising findings from this year’s Nash Squared Digital Leadership Report is that the number of major cyberattacks reported is falling.\nIn its 25th year of publication, the report found that across more than 2,100 technology/digital leaders surveyed globally, the proportion experiencing a major cyberattack in the last two years has fallen to 23% – down from 28% in 2022. A higher proportion – 44% – of large organisations reported a major incident this year, down 56% last year.\nOur perspectives on cyberattacks are changing. There are so many attacks now that cyber and technology professionals have become hardened to them, and what they class as ‘major’ has changed. A few years ago, a short DDoS (Distributed Denial of Service) or small data breach might have sparked a midnight call to the CEO, now it’s seen as ‘routine’.\nMore organisations are strengthening their cyber defences. Some sectors like financial services and central government continue to raise their game, investing in sophisticated protection systems, threat detection and response capabilities, and continually reminding the whole workforce of the importance of good cybersecurity protocols. We can’t say that businesses are ‘winning’ the cyber battle. It’s more nuanced and complicated than that.\nRaising the stakes: new threats from generative AI\nNew threats are coming that could dwarf anything we have seen to date. Generative AI has become today’s hot ticket. But while generative AI holds enormous positive potential, it could also be a gift to the cyber criminals. A communique signed by attendees in advance of the UK government’s AI summit of world leaders warned that AI systems could be used to launch cyberattacks and create bioweapons, and that it is “especially urgent” to address the risks.\nThe success of cyberattacks is often dependent on their ability to scale – swamping and overwhelming an organisation’s defences – and their ability to mimic real humans (as in a phishing campaign). We are already seeing instances of incredibly convincing, tailored phishing emails that appear to have been generated with AI. In time, some predict that the success rate of phishing campaigns could leap exponentially, from the present level of about 0.1% to anywhere around 20%.\nPhishing would only be the start. Cyber criminals are also engaging in what’s been termed as ‘AI poisoning’, infecting the content that is subsumed into the learning process of an AI algorithm so that it becomes untrue, biased or downright malicious. This could then be replicated and multiplied across systems and networks with terrible consequences.\nThere is also malware. So far, generative AI’s coding abilities have been relatively basic. But AI is improving at exponential rates – much faster than a human can learn. It may not be long before generative AI can develop malicious code that is almost impossible to block. Malware potency could hit new levels, and the cyber industry will need all its skill and investment (and some help from ‘good’ AI) to combat it.\nThe quantum risk\nWe are already beginning to see Quantum as a Service (QaaS) being offered where quantum mainframes are made available to users. It may not be long before the networking challenges of quantum being solved so that quantum computing becomes available on a mass scale. Users, including cyber criminals, could have access to thousands if not tens of thousands of qubits. This will put almost unimaginable computing and processing power into peoples’ hands.\nQuantum computing poses a risk to encryption. Customers’ secure transactions to their bank or all the data transmitted over a VPN may no longer be protected. Adversaries may be able to go back and decrypt historic financial communications. The underlying basis of blockchain could be undermined, permitting the ability to rewrite financial records.\nThat kind of quantum scenario may be a little way off – and hopefully defences and mitigations would be invented at the same kind of speed.\nThere is no doubt that, with the new and emerging technologies that are coming, the cyber challenge could be massively amplified.\nThat’s why organisations have to keep on investing in their defences and get used to the thought that the battle is never over, and never won.\nJim Tiller is CISO, Nash Squared. The Nash Squared Digital Leadership Report 2023 is based on the world’s largest and longest running annual survey of technology/digital leadership']"	['<urn:uuid:661a6c54-86b1-46da-9574-20f52f39125f>', '<urn:uuid:8015c11a-f376-4016-a133-40158705ce29>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:43:46.574121	6	62	1655
