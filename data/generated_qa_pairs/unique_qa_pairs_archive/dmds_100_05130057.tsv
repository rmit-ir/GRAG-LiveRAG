qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	How many gallons of water can the largest tilapia pond in rural Malawi hold, and what is the maximum depth of these ponds?	The largest tilapia pond holds 3 million gallons of water, and these ponds reach a maximum depth of approximately three meters.	['Agriculture and Aquaculture\nLike many developing countries, most of Malawi’s population survives on subsistence farming. In rural areas of the country, over 90% of the population exists on this small-share farming alone — yet only 2% of the land is irrigated. Because traditional practices have been used for decades, the soil in Malawi is depleted and yields are becoming smaller every year. Not only does this prevent rural farmers from participating in their local economies, but there is not enough food for their families. Today, over 47% of Malawian children are stunted from severe malnutrition.\nSince food is a necessity of life, we research and teach the local population the best agricultural methods and crop choices for the climate and soil of each specific rural community in which we serve.\nIn addition, our vocational training schools provide an in-depth education on large and small scale farming, including crop rotation, fertilization, and irrigation methods powered by renewable energy.\nChild Legacy’s rural Malawi site has a rich diversity of grains, fruits, vegetables, and protein sources, which feed our staff, visiting guests, and hospital patients, all while supporting the economic sustainability of the project. On nearly 100 acres of cultivated and irrigated land, Child Legacy researches and teaches the local population the best agricultural methods and crop choices for the climate and soil of each specific rural community we serve. Our research findings are passed on to the local people to improve their harvests, as well as that of our own. Local farmers who implement what they’ve learned at Child Legacy report larger yields within a single harvest cycle, providing nutrition for their family and participation in the markets.\nOur aquaculture program continues to set the standard for excellence, showcasing the marketable benefits of sustainable design. After feeding our employees and project site guests, the fish raised in our ponds are sold in the local markets. Child Legacy currently has eight tilapia ponds- two holding 300,000 gallons, three holding 1 million gallons, two holding 1.5 million gallons, and the largest holding 3 million gallons. Each pond is sloped with the epicenter depth reaching approximately three meters. Although tilapia only inhabit the top meter of water — where warmth and oxygen are plentiful — we use the 2 meters of depth below as a reservoir to store water that carries the agriculture department through the dry season in Malawi. This fish farming project is becoming contagious in the surrounding villages. One of our employees has already dammed off part of a lake in his village and begun raising a small pool of Tilapia to sell for additional personal profit for his family, and additional nutrient-rich food for his community.\n- Over 100 local Malawians employed by CLI’s Agriculture and Aquaculture departments\n- 100% organic methods practiced on site\n- Over 200 acres owned in rural Malawi\n- Over 75 acres cultivated and irrigated for agriculture and farming\n- Eight on-site ponds used for aquaculture also serve as water reservoirs during dry season\n- Ponds are stocked with over 2 million tilapia\n- Adult fish are sold every week at the local markets, in an effort to move toward economic sustainability\n- Staff, Hospital Patients, and Guests are served nutritious organic meals with on-site meat, produce, and dairy products\n- Weekly demonstrations held to educate small-share farmers in irrigation and other methods that greatly increase their harvest yields\n- On-site maize mill equips rural farmers to give their harvests greater leverage in larger markets']	['<urn:uuid:61be5a16-6c20-486d-a24b-c69684b429fe>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	23	21	579
2	I keep seeing these natural wine trends everywhere - what are winemakers doing differently now compared to regular wine production?	Modern winemakers who are focusing on environmentally friendly methods are moving away from conventional winemaking techniques. The most radical ones are abandoning stainless steel tanks, refrigeration, and added yeast - all typically considered safe methods of sound winemaking. Instead, they're adopting ancient techniques, particularly with white wines. While most white wines are typically pressed before fermentation to create bright, clear, light-bodied wines, these natural winemakers are leaving white wines on their grape skins during fermentation, which results in more color, flavor, and tannins.	['An Ancient Style of Winemaking is All the Rage\nA group from Lot18 recently dined at a bring-your-own-bottle restaurant in Chinatown, sharing wines from our cellars and samples from the office. We had an eclectic collection to choose from, but the one everyone insisted on tasting first was the “funky white”: a 2004 Gravner Anfora Ribolla Gialla, which goes live on our site tomorrow.\nWines made in amphorae (spelled “anfora” in Italy), large, human-sized clay vessels, like this Gravner, are getting a lot of buzz. They’re showing up on restaurant wine lists, blogs and mainstream wine media, but I had never tasted one.\nAs more producers turn toward environmentally friendly methods – organic and biodynamic chief among them – they’re seeking ways to make wines completely au naturel. The most radical winemakers are eschewing stainless steel tanks, refrigeration and added yeast – all the safe methods of sound winemaking – and fermenting wines as their ancient Greek and Roman forefathers did in these beeswax-lined amphora vessels (also called “dolium” at the time) buried in the ground for primitive temperature control.\nWhile the majority of red wines are made with the grape skins in contact with the juice, most white grapes are pressed before fermentation to create bright, clear, light-bodied wines. Many amphora winemakers are forgoing the press and leaving white wines on their skins during fermentation for more color, flavor and tannins. Amphora-made wines are darker in color, often cloudy and much more labor intensive to make, but it seems they’re worth the effort: critics are raving. The 2004 Gravner we’re offering scored 92 points and received a Tre Bicchieri from Gambero Rosso (Italy’s highest wine award).\nWhat will the average wine consumer think though? That was the question we asked at our Chinatown tasting.\nIt poured, as we heard it would, intensely amber and slightly cloudy. Jancis Robinson described it well in her review of orange wines, as a “rather worrying colour,” similar to oxidized, spoiled wine. But the wine was surprisingly vivacious. Most striking was its lively acidity, and assertive tannins – so unusual in whites – providing texture and chewiness. The aromas and flavors were complex. Apricot and sultana mingled with chamomile, resin, almond and a slight brininess. It paired well with an appetizer of salted nuts and a first course of steamed clams in a cilantro-infused broth.\nWe were impressed, and I think most adventurous wine drinkers will be. (The less daring might run screaming back to Pinot Grigio). More of a sipper, than a thirst quencher, it’s similar to fino sherry but with brighter acidity, less alcohol, more texture and layers. Besides our nut and clam pairing, it would match well with olives, anchovies, sardines and hard cheeses.\nI’m eager to try more, and there’s a range of styles to explore. The Scholium Project out of California produces a skin-fermented Sonoma Sauvignon Blanc with the name “The Prince in His Caves.” A winemaker out of Slovenia, Ales Kristancic at Movia, who is always experimenting, also ferments Ribolla in amphora, leaving it on its skins for nine months. C.O.S. in Sicily is making wines in a combo of amphora and oak barrel from native grapes like Nero d’Avola, Greciano and Inzoli. Each of these unique methods allows the wines to evolve in different ways, creating different flavors and textures.\nI hope you’ll seek some of these wacky wines out. When you pop your first open, raise a glass to our ancient winemaking forefathers and the modern mavericks.']	['<urn:uuid:f1c3ca98-0c4d-4c0e-b574-74295969d00d>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	20	83	581
3	gold value as investment versus practical recovery from waste	As an investment, gold has shown significant limitations - it historically delivered only half the returns of stocks, with returns of 3.8% versus 8.4% for stocks from 1975-2017, while experiencing even higher price volatility. Gold prices tend to spike during crises but then fall significantly, as seen in 1980 and 2011. Regarding practical recovery, extracting gold from waste materials like electronics and ores is technically challenging - gold particles are often locked within host materials making gravity or chemical recovery difficult. The recovery process requires specialized methods depending on the source material type, with microorganisms sometimes being used to reduce gold ions to metallic form. The difficulty of extraction contributes to gold's value.	"['The Ring on my Finger is golden and he is dear to me. My dentist has recommended to me, my Golden tooth filling, to repeal, after they had fallen out. Is, after all, what value. However, a proper money system is not of course both. Even if the price of gold these days, rises again, in the appropriate View, again, crisis scenarios are evoked, and the so-called Prepper fill your basement in front of the Brexit and the European elections, with the canned goods.\nGold is, and remains, not a really good investment. My fellow “Finanztip” last year, calculated that Gold historically has only brought half as much return as stocks. The gold has fluctuated in price by even more than the of stock, so he was even more volatile, as it is called in the jargon.\nReading the can also be used in the two most important observations to investment in Gold in the last 50 years:\nThe first story begins in the middle of the Iran crisis of 1979 and 1980 . The world is restless, the price of Oil has increased dramatically, the Inflation also, particularly in the United States. Finally, the Iranian revolution, take garden of the Embassy staff, the U.S. mission in Tehran as hostages. The price of gold climbs and climbs from 500 dollars per fine ounce at the beginning of the year, 1979 to $ 850 in early 1980.\nAnd then the bubble bursts. After the stock market operator and state actors make this more difficult side of the Atlantic are Speculating with non-existing Gold and silver rushes of the course to the bottom.\nDetails are to be found in the standard work “Gold” by Rae Weston. 21. January 1980, continued for example, the Federal government limits for reserves, the banks in Gold were allowed to keep. The then head of the U.S. Central Bank, Paul Volcker, later explained: “the Gold and the speculators, that were my opponents.”\nWho had bought an ounce in 1980, to the high prices, had to wait until January 2008 (i.e. up to the beginning of the great financial crisis) to achieve a nominal price from 1980.\nabout The author Finanztip Hermann-Josef Tenhagen (Born 1963) is editor-in-chief of the “Finanztip”. The consumer guide is a non-profit. “Finanztip” refinanced through what is called Affiliate Links. More on this here.\nTenhagen has previously led as editor-in-chief for 15 years, the magazine “financial test”. After his studies of politics and Economics, he began his journalistic career at the “daily newspaper”. He is now the honorary Supervisory Board of the cooperative. In the case of MIRROR ONLINE Tenhagen writes weekly about the right way to deal with your own money.\nSubsequently, the price rose actually again – up in 2011, the peak of the Euro crisis . Again political and economic turmoil were the starting point. The global financial crisis and then the Euro crisis and the particularly difficult situation in Greece. And again, the state intervened and hampered the trade with gold papers. The collateral had to put speculators on the table, to buy so-called Gold Futures rose in August and September 2011, within one month smooth by half. The Speculate was much more expensive. And the prices plummeted. The Declaration of the heads of the European Central Bank, Mario Draghi, did, months later, the Rest of the. Draghi said in 2012, you will do whatever is necessary to preserve the Euro – and thus the speculator the Wind out of the sails.\nWho has bought in 2011 for up to 1900 dollars per ounce of Gold that is waiting since then to a rate of return. Still, the price is around a third below the peak prices of 2011 and he has moved in the past two years, a little bit up and a bit down, but rather sideways.\nOnly those who has bought in the beginning of this Millennium, Gold, for the Gold is still a good Investment in the long term, worse than stock, but still.\nreturn and volatility of MSCI World vs. Gold\nMSCI World¹ gold 2 period return Schwankung3 yield fluctuation 1980 – 1990 18,8% 15,4% -1,2% 21,8% 1990 – 2000 12,6% 16% -2,6% 12,3% 2000 – 2017 1,8% 14,9% 7,7% 12,7% 1975 – 2017 8,4% 15,1% 3,8% 15,9%\n¹Die performance was calculated on the MSCI World net index, the offset, the dividends, after deduction of taxes and costs, and the index values were converted into Euro.\n2The price of gold was converted into the Euro.\n3Schwankung referred to the average standard deviation of monthly returns (annualized).\nFrom the height of the Internet stock market hype in the year 2000, the Bursting of the Dotcom bubble in 2001 and the financial crisis of 2007-2009, shareholders who had placed their money on the world stock index MSCI World had, until 2017, only a return of 1.7 per cent in the year, when Gold it were smooth and 7.7 percent. Over the whole period from 1975 to 2017, the annual rate of return on Gold, in contrast, was 3.8 percent and the MSCI Stock portfolio is twice as high, at 8.4 per cent in the year.\nAnd now you say: I’d rather have bars than stock. I just feel safer with it and I don’t care about the warnings against price fluctuations, speculators and government intervention. What then?\nMy response: Then make it at least a craft correctly.\nGold has been good in the past at least, to lose in the months of tough financial crises less money. You create a part of their free assets in Gold, you can mitigate the fluctuations of the total net assets , at the expense of the long-term rate of return. For the control, the following rule applies: After a one-year holding period, you will not have to pay tax on any winnings. You buy, if it’s supposed to be physical Gold, the best bars and well-known coins . Especially in the case of the coins, you should prefer to take larger coins, the small, the purchase costs are quite high. More than ten percent for a tenth-ounce, it’s common to a whole ounce of just 3.5 percent. Use Online platforms, in order to good and reputable precious metal or coin dealer to find out. The price differences can be significant. To create a small Safe or a locker . One kilo Gold bars is barely larger than a phone, but more than 30,000 euros. Update your household insurance . Good household contents insurance to insure bullion and coins even in the Safe of the Bank. If you want to sell Gold , is the well-known coins and bars easier. Jewelry, gold, or unknown coins you should let the professional to appreciate and not easy to sell online. Gold is of use only in very limited as a currency in times of Crisis. For a couple of months of unrest in the financial markets, it is okay. But then? Gold you can take, but not to eat. On the go, you need to keep your valuable possession safe and are so reliant on well-functioning state structures. The allotment garden with potatoes, strawberries and Apples remains in the great crisis, therefore, is usually the better answer.\nWhat follows from this: As the last sentence of the first sentence, my colleague, writes in her standard guide to the topic of Gold: “Gold is very likely to be never worthless: Who’s afraid of a currency crash, you can think about buying Gold.” Think. .', ""Precious Metals Guide - Meierotto Jewelers,Traditional metals used in jewelry are the noble metals: silver, gold and the ... This is partly due to the use of new types of metals and alloys used to make...\nRecovery of precious metals from waste streams - NCBI,13 Jul 2017 ... Gold particles that are locked up in host rocks make the recovery by gravity or cyanidation methods difficult or even unfeasible (Marsden and ... Some microorganisms can reduce precious metal ions to their metallic forms.\nYour Trusted Source for Jewelry Education ... - Reflections In Gold,Gold is widely dispersed through the Earth's crust and is found in two types of .... This grayish white to silver gray metal is harder than gold and very durable with a .... a paste, liquid polish or a treated polishing cloth to its original luster.\nThe Ups and Downs of Gold Recycling - BCG,5 Mar 2015 ... Because of gold's high value, manufacturers strive to recover and reuse .... There are two types of source materials for gold recycling, each with ... Extracting valuable metals from these materials safely and efficiently is difficult.\nGold extraction - Wikipedia,Gold extraction refers to the processes required to extract gold from its ores. This may require a ... 2 Types of ore; 3 Concentration; 4 Leaching; 5 Refractory gold processes; 6 Gold .... Sulfide minerals are impermeable minerals that occlude gold particles, making it difficult for the leach solution to form a complex with the gold.\ngold recovery from printed circuit boards of mobile phones ... - SciELO,recycling in small and medium industries difficult, ... for gold recovery from PCBs and other types of scrap. ... form (Cu+2), influencing the gold dissolution rates.\nTurning old jewelry into pure gold bars - YouTube,29 May 2019 ... Turning old jewelry into pure gold bars ... Note: There have been some comments about the gold being stolen, but just to clear, I have verified that it wasn't. .... Solving THE HARDEST Lock Puzzle in HISTORY!! ... How To Recover Gold From Computer Scrap with Household Chemicals - Duration: 21:57.\nDid you know? Gold Recovery and Recycling | Knowledge of ...,The solution (pregnant solution) is placed in an electrolytic bath where difficult-to-ionize gold, platinum, silver and other elements are deposited out of the...\nAbout gold jewellery | World Gold Council,11 Sep 2019 ... In addition it is usually plated with rhodium to create a harder surface with a ... Nickel-containing white gold alloys form a small/very small...\nGold processing | Britannica.com,Gold processing, preparation of the ore for use in various products. ... it forms with oxygen and watera characteristic that allows gold to be refined from ... made it amenable to recovery by placer mining and gravity concentration techniques.\nThe recovery of gold from the aqua regia leachate of electronic parts ...,For this reason, a number of methods for recovery of gold from aqua regia leachates have ... In the present work, the coreshell type anion exchange resins were .... The created tin acid made filtration of the remaining PINs much harder, as it...\nGold vs Platinum | What's the Best Metal for Your Ring - Do Amore,29 Mar 2015 ... The good thing is that re-polishing your ring will help the ring to near its original look for both platinum and gold. All Do Amore Rings...\n(PDF) A Review on Alternative Gold Recovery Reagents to Cyanide,24 Aug 2018 ... PDF | The importance of gold recovery from low grade ores and flotation ... depending on type and efficiency of the processes . .... Another disadvantage is the difficult handling of the strongly corrosive chlorine solution and.\nOrigin and effects of impurities in high purity gold - Springer Link,Chemical specifications for high purity gold have grown increasingly stringent as ... elements would be difficult and costly and serve no .... precipitation agents, solvent extraction has been ... metal impurities such as iron and zinc to form volatile.\nGold and Platinum Refining & Recovery Systems / Equipment,There are many kinds of materials that yield pure gold, platinum, or silver when refined. ... So one of the hardest things for us is when new refiners recount horror...\nSelective dissolution of elemental gold from multi-metal sources in ...,13 Dec 2018 ... Recycling of precious metals in smartphones, computers, and other electronic gadgets is much harder, and the recovery quote is still low.\nMetal Guide - Hannon Gemologist and Master Jewelers,gold Pure gold: (fine gold) is softer than pure silver but harder than tin. .... Depending of the type of alloys used in the fabrication of white gold, white gold jewelry ... In these cases, white gold can be rhodium-plated to the bright white...\nHow Gold Is Refined: A Step-By-Step Guide - Pease Curren,To make it harder and stronger, manufacturers use other metals such as ... The resulting gold alloy can be used to make all kind of things such as coins and...\nGold & Platinum Recovery Methods - Sciencing,Platinum recovery from catalytic converters and electronics components is a much more expensive and difficult process than gold recovery.\nR.C. Wahl Jewelers: Metal Types,Pure 24 karat gold is rarely used in jewelry because it is too soft for frequent wear. ... It is the hardest natural metal on earth and therefore offers more scratch...\nPMPC - Gold Recovery, Electrowinning of Heavy and Precious Metals,Reclaim gold, palladium, silver w/ electrolytic recovery of heavy & precious ... and patented a revolutionary type of electrolytic recovery system, the Gold Bug®. Gold recovery and platinum recovery does not have to be expensive or difficult.\nThe 5 C's of White Gold vs. Platinum | HuffPost Life,29 May 2013 ... Readers of my diamond education blog write to me with all kinds of ... Platinum can also be shined professionally to its original luster,...\nGold recovery from used electrical contactors - Core,widely spread gold-containing type of electronic device. Used Electronic Devices as a Source of Gold Recycling. It is very difficult to say how much of the...\nGold Recovery Correctly: 12 Steps,Gold Recovery Correctly: I will detail a SAFE process of turning old electronic parts into little round lumps of pure gold. There are many tutorials that claim to offer...\nSelective Recovery of Gold from Electronic Waste Using 3D-Printed ...,27 Oct 2017 ... Currently, used gold recovery methods include physical, ... (31) The scavenger material is printed in a form of a column or a .... of the flow channels, it is difficult to give a simple figure for the capacity of the printed scavenger.\nGold Alloys - an overview | ScienceDirect Topics,Extraction of Gold from Platinum Group Metal Ores ... Alloys of this type are high-grade jewelry scrap, typically karat gold alloys and fillings. .... large scale: silver can be added to a difficult-to-treat doré to increase the silver content, followed by...\nPrecious Metal Refining Questions | Refining & Recycling FAQs,We process and recycle almost all forms of scrap metal containg Gold, Silver, ... It would be difficult to say one sampling method is better than another when it comes ... Complete recovery of the Fine Gold and other Precious Metals is achieved...\nProperties of gold - OpenLearn - Open University,20 Jul 2006 ... Gold is valuable simply because it's scarce and difficult to extract. ... By the time it reaches the sea, any gold is in the form of very small specks...\nRecovery and Concentration of Precious Metals from Strong Acidic ...,25 May 2011 ... A recovery of 99% Copper (Cu), 96% Palladium (Pd), and 85% Gold (Au) by cementation using. Iron (Fe) ... is usually difficult due to the formation of metal ammine complex .... the type of the cementation agent being used.\n- roll crusher secondary type 36 48 rb\n- limestone crusher type selection\n- crusher type dealers uae\n- coal crusher model and type ethiopia crusher\n- crushing type paddy moisture tester china\n- different type of machines for crusheing stone\n- cone type stone crushers in ethiopia\n- crusher plant use which type of stone stone quarry plant india\n- zenith type lt impact crusher\n- cone crusher type thr\n- fergus type crocodile crusher\n- what type of bearing used in coal crusher\n- crushing machine production line cost in uk\n- rock breaker machine manufacturer\n- gold mining equipments from nigeria\n- vertical shaft hammer crusher principle\n- sbm sbm cone crusherstone crusher machinery for sale\n- used crusher related\n- grinding machine koyo\n- alat pemotong crusher\n- sand mapane ki machine\n- kolkata clay brick machine plant for sale\n- limestone crushing for cfb power plant\n- industrial metal crusher and separator separator""]"	['<urn:uuid:100f6a28-151c-46d8-b29e-0e0d83b86587>', '<urn:uuid:24af551d-b939-4580-9377-152a1c755d00>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	9	113	2679
4	How long is a divorce mediation process, and what happens in sessions?	A complete divorce mediation typically takes three to six two-hour sessions, depending on property, assets, and disagreements. During these sessions, the mediator listens to both parties, helps work out what needs to be handled, reviews options, ensures both sides can speak and be heard, provides necessary information, and ensures decisions are made jointly and fairly. The mediator can see parties together or separately if needed.	['Overview of the Family Mediation Process\nWelcome to my Mediation Practice. The purpose of this overview is to orient you to the process of mediation so that you can be thoughtful and better prepared as you enter this process.\nWhat is mediation?\nMediation is a voluntary process in which two or more parties involved in a dispute work with an impartial professional, the mediator, to generate their own solutions to resolve their dispute. Unlike a judge or an arbitrator whose decisions subject one party to win and the other party to lose, mediation is about finding a solution that works for both parties.\nWhat are the benefits of mediation?\nMediation offers parties an alternative to taking your dispute to court and can be considered before, during and after formal court proceedings. Prior to contacting an attorney, some couples or families seek mediation to contain legal costs and have more control over the decisions to divide property, determine child support and/or alimony, develop a co-parenting plan, develop an elder care or placement plan and settle other financial matters. Some couples or families seek mediation if the dispute has been ongoing and the process of legal advocacy is not reaching a settlement facilitated by the attorneys involved. Others seek mediation to preserve a relationship that is being affected by the unresolved conflict, if the dispute is upsetting and affecting daily life, and when one party or both would like professional assistance to facilitate effective communication so that each party can hear and appreciate the other’s concerns. Mediation also remains an option after legal settlement for parties wishing to renegotiate or further clarify an agreement.\nWhat does divorce mediation cover?\nDivorce mediation covers division of joint assets and debt, and when children are involved, child custody and child support and a parenting plan.\nWhat does family or elder care mediation cover?\nFamily or elder care mediation assists family members in crafting a joint agreement regarding which party assumes what specific area(s) of responsibility and/or care for the family member in need of assistance. Typical responsibilities and types of care negotiated in the mediation include property power of attorney, medical power of attorney, bill payment, management of finances, supervision, transportation, attendance to medical appointments, personal care assistance, house keeping, medication distribution, repairs to the dwelling, visits and holidays, and other daily life needs such as grocery shopping.\nWho participates in the mediation?\nThe participants in the mediation are the actual parties considering divorce or planning to develop an elder care plan. Usually attorneys do not attend the mediation sessions. The mediator usually asks for consent to consult with any attorney representing the parties to explain the mediation process, learn what aspects of the divorce or estate planning the attorney thinks is important to include in mediation and confirm that the client is encouraged to bring any mediated agreement to the attorney for review prior to signing it.\nHow long does it take and what is the cost?\nMediation of a complete divorce generally takes three to six two hour sessions, depending on the extent of the shared property and assets, the degree of disagreement and the need for a co-parenting plan. The rate for mediation is based on the anticipate number of mediation sessions, emails, phone calls, consultation (with the parties, their attorneys and/or their financial advisors or other parties they choose to include in the process) and preparation of the written agreement called the Memorandum of Understanding or MOU. Mediation is considered a forensic service and so is not reimbursed by medical insurance. Payment is reviewed at the free orientation meeting during which the scope of the mediation is discussed in conjunction with the projected fee. The mediator will update the parties regarding any unanticipated fees for consultation and report preparation. Feel free to contact one of us if you have any questions.', 'If the very first meeting (MIAM) goes well and you all accept attempt mediation, you will certainly schedule mediation sessions. It normally takes in between three as well as 5 conferences ahead to contract, relying on what you require to iron out.\nThe mediator will typically see you and also your ex partner with each other, although you must be provided the selection to see the mediator individually if you need to.\nWhat does the mediator do?\nMediators are trained to:\n- Pay attention as well as assist you both to work out what needs to be handled\n- Review what your choices could be and what may work best for the future.\n- Ensure you both have chance to speak and be heard\n- Offer any kind of info needed to aid your conversations\n- Inform you when you could need additional independent guidance on issues such as pensions\n- Guarantee choices are made collectively, are fair for both of you, for any children involved, and for your family scenarios\nWhen you get to agreement, the mediator will certainly put it in writing as well as ensure you’re green light regarding what it implies.\nAre the decisions we make lawfully binding?\nThe choices you reach aren’t legally binding by themselves. However you can ask a court to make what you’ve decided into a lawfully binding permission order. Your mediator can describe what this is as well as exactly how you can obtain an approval order.\nThere is an expense for this court application and your mediator will be able to supply information regarding this. , if you obtain legal help you may qualify for totally free legal guidance as well as assistance with this.\nCan I obtain lawful suggestions on whether the proposed arrangement is reasonable for me?\nMost individuals going via mediation locate it valuable to have lawful guidance to support them. If you are speaking concerning points that associate to a lawful problem, you can prepare this at any time and your mediator might likewise recommend you do. The mediator can offer you details regarding neighborhood family solicitors and just how to select one. If you get lawful help for mediation, you may additionally obtain cost-free legal suggestions throughout mediation.\nWhat if things don’t go as prepared later on?\nYou can go back to the mediator to change the original contract if your scenario modifications as well as the arrangements aren’t functioning.\nIf you’ve made an agreement lawfully binding and also someone does not follow it, you must consider whether it can be ironed out with the aid of a mediator. Otherwise, it can be implemented through the courts.\nCan I see what takes place in mediation?\nMediation is confidential, so we can not reveal you an actual session. However, we have videotaped a mock mediation session with an FMCA mediator and two actors, to show you what actually takes place in a mediation. This video is about a hr long.\nWe are an expert all concerns family mediation service devoted to helping dividing pairs function out future plans for kids, residential property as well as financial resources for Private as well as Legal Aid clients. We analyze for Legal Aid– analysis totally free. Ask regarding totally free meetings for private clients.\nFamily Mediation Organisation UK helps you make you very own decisions regarding what is best for you and your family in future without going to court. We will certainly help you improve communication, fix your disputes and also get to a workable, long-lasting service rapidly, compassionately as well as cost-effectively.\nOur outstanding group of family mediators are trained to direct you with the procedure to lessen the price, distress and hold-up so frequently related to separation and divorce.\nFamily Mediation Organisation UK has actually conveniently-located offices (see Get in touch with Us). We are certified to take on Mediation Information and also Evaluation Conferences (MIAMS), offer forms A & C100, and also direct examination with youngsters if you feel their voice is essential.\nThis web page clarifies the procedure of family mediation, when mediation is needed and also the anticipated requirements of a family mediator.\nWhat is mediation?\nMediation is the process through which households can work out concerning future arrangements for children with the assistance of a neutral third event. The mediator does not inform celebrations what to do, yet can help the events to reach their own contracts agreeably, whilst trying to enhance interaction between them.\nWhat are the advantages of mediation?\nMediation is recommended when parents locate it tough to concur on making ideal arrangements for kids after a family failure. There are numerous advantages to participating in mediation, such as:\n- providing you more control over what decisions are made in regard to children, as opposed to relating to the courts;.\n- providing a less stressful means of handling sensitive matters;.\n- improving interaction and also helping you to figure out future setups;.\n- allowing plans to be evaluated as well as altered much easier, as long as they are mutually agreed by both events; and also.\n- supplying a quicker as well as more affordable method of dealing with disagreements.\nAre any type of contracts made via mediation legally binding?\nAny kind of agreements made throughout mediation are not legally binding in the sense of being enforceable in a court. Some individuals do determine to obtain a solicitor to examine the contract, and also the agreement can be utilized in court at a later phase in order to produce an Authorization Order. See our page on Consent Orders to learn more.\nWhat is a Mediation Information and Evaluation Meeting (MIAM)?\nA Mediation Info Analysis Fulfilling is the first meeting which will aid develop whether mediation will certainly appropriate in your situations, and also whether it will help you to get to an arrangement.\nWhat will occur at mediation?\nThe mediator will attempt to discover commonalities between you. If you’re not comfy with being in the very same area as your ex-partner, the mediator can organize ‘shuttle’ mediation. This is where the mediator talks with you alone and then talks with your ex-partner with your propositions individually. It may take more than one session to reach an agreement.\nUpon an arrangement being gotten to in between you as well as your ex-partner, a “memorandum of understanding” will certainly be produced by the mediator so every person understands what has been concurred.\nDo I have to go to mediation?\nFrom April 2014, any person applying to the courts for aid in resolving disputes about children or financial resources will certainly be required to go to a meeting Mediation Details Analysis Satisfying. This consists of any kind of applications for:.\n- Youngster Arrangements Order.\n- Particular Problem Order.\n- Prohibited Tips Order.\n- Parental Duty Order.\n- An order designating a Child’s Guardian.\n- Elimination from Jurisdiction Order.\n- Unique Guardianship Order.\nYou will not require to go to mediation for the above applications if you are getting a Consent Order, or if there are ongoing emergency proceedings, care procedures or guidance process for a kid or there is an Emergency Situation Security Order, Care Order or Guidance Order in area.\nYou can additionally be exempt from needing to participate in a MIAM, if you meet among the exceptions detailed in paragraph 3 of the C100 application kind, which can be downloaded from www.justice.gov.uk. A few of the major exemptions include:.\n- where there has been any type of form of domestic physical violence between you and your ex-partner and also it has been reported to the police, courts, wellness experts or specialist agency;.\n- where the child is the topic of a Youngster Protection Plan or an area 47 enquiry;.\n- where the circumstance refers urgency, i.e. a danger of damage to the child’s safety and security;.\n- where mediation has been attempted within the last four months; or.\n- where the individual looking for to make the application does not have enough call information of the other person to which the application connects.\nFrom April 2014, it is required to participate in a Mediation Details Evaluation Fulfilling before making an application for court.\nWhat can I anticipate from my mediator?\nA family mediator have to act impartially and stay clear of any dispute of interest. This indicates that a mediator has to not moderate on a conflict where they have acquired pertinent details about the parties. A mediator must continue to be neutral on the end result of the mediation. They have to not seek to implement their preferred result or influence on any one of the events.\nYou have to also expect the mediator to keep private all details gotten throughout the program of mediation. The mediator can not even divulge info to the court, without the consent of both individuals. The mediators may only disclose info where there are serious accusations of harm to a kid or grownup.\nMediation is a voluntary process and also any type of session for mediation can be put on hold or terminated, if it is really felt that the events hesitate to completely participate in the procedure. Mediators must also urge the individuals to take into consideration the wishes and feelings of the children.\nThe length of time can mediation take?\nMediation can proceed while it fulfills the demands of the specific events involved. The initial conference lasts about 45 minutes. Full mediation sessions will typically last between 1 to 2 hrs, depending upon the intricacy of the circumstance.\nWhat is the cost of mediation?\nIf you get on a reduced revenue or in receipt of specific advantages, you could be able to get Legal Help to assist with the expenses. If only one celebration is eligible for lawful aid, Legal Aid can cover the initial MIAM session for both of you. The mediator ought to be able examine whether you are eligible for lawful aid or you can contact Civil Legal Suggestions on 0345 345 4345.\nFor precise expenses, get in touch with your mediation provider.\nSuppose we can not reach an agreement with mediation?\nIf you can not get to a contract with the various other individual, or mediation falls short for any other factor, as an example the various other celebration will certainly not attend or the mediator feels that mediation is unfeasible, you may wage your disagreement to the courts. You should ensure that the mediator signs and accredits your application kind.\nThat we can assist\nWedded and cohabiting pairs (still living together or divided), consisting of same-sex pairs.\nVarious other member of the family associated with disputes worrying kids, such as grandparents, new companions as well as siblings.\n, if the various other parent desires you to go to mediation\nYou may have been called by a mediator or the various other parent may have asked you to attempt mediation. It is essential to comprehend what mediation is and how it can assist your circumstance.\nThe other parent might have approached a specialist mediator because they want help to reach a choice about arrangements for your kids. Or you might have been invited to a ‘Mediation Details and Analysis Meeting’ or ‘MIAM’, which is a lawful demand prior to the various other parent can put on court.\nShould I go to mediation?\nIt might have come as a shock to you that your ex intends to change something pertaining to the arrangements for your child or youngsters. A letter from a mediator may have been completely unexpected. If you’re great with the plans as they are, why should you speak to the mediator?\nMediation can aid you as well as the various other parent resolve troubles without going to court. Getting a court order frequently takes longer and also might be much more stressful and also costly. Mediation might simply be a chance to speak with somebody independent about your parenting circumstance.\nYou ought to contact the mediator to discover much more. Do not fret that the other parent has actually selected the mediator, they’ll constantly be entirely independent. You might agree and locate on one together if you ‘d prefer to use an additional mediator.\nIf you have actually been welcomed to a MIAM, you’re expected to go – unless you’re exempt. If you don’t attend, the mediator can sign a file allowing the various other parent to look for a court order regarding your kid plans.\nIn mediation, both you as well as the various other parent will have the ability to elevate points that are necessary to you and also function to agree a service. A court will determine for you as well as it will certainly be legitimately binding if an instance goes to court.\nFamily mediation works for many people however it is not ideal for everybody. It will certainly not appropriate if, for instance, there’s been residential abuse or you’re bothered with the security of the children. The mediator will aid you recognize if it might be suitable for your situation.\nIf there are no security problems, nonetheless, there are other ways you can try and also get to an arrangement if you don’t believe mediation is best for you. The cheapest and most convenient way to make setups is to negotiate with the various other parent. There are complimentary tools and also solutions that can help you.\nWhat happens in mediation?\nYou’ll locate out what the various other parent wants to talk about if you go to mediation. You’ll get the possibility to speak about the situation from your factor of view too. You don’t have to see the other parent if you don’t want to (you can be in separate spaces), as well as the mediator can prepare for you to reach different times.\nThe mediator will certainly be independent, also if they operate from a solicitor’s office, and won’t take sides. They’ll sustain you both.\nWhatever you claim in mediation is personal. Information will not be shown anybody else without your approval (unless as an example, there’s a securing danger or a criminal offence is revealed).\nYou and your ex-spouse will have the ability to elevate things that are very important to you as well as the mediator can help develop a checklist of subjects to speak with.\nMediation can help you remain in control. It’s volunteer and no-one will certainly make you do anything versus your desires.\nThe mediator will certainly assist you and also the various other parent (separately or with each other) experience all your concerns, believe of your alternatives, make a decision whether they would certainly work well in technique as well as pertain to an agreement about what’s ideal. They can additionally describe how you can make your arrangement legitimately binding, if you intend to.\nHow a lot does it cost?\nMediation can be more affordable than going to court.\nSuggestions Currently has a guide to using mediation adhering to separation, which contains info regarding prices.\nTypical cost of MIAM per person.\nIf you attend together, it might be less expensive.\nYou might not have to pay for the MIAM or mediation if you or the various other parent are eligible for legal aid. The mediator can help you check.\nFirst you have a MIAM, which is the initial conference. The various other parent sees the mediator also. You can have your MIAM by yourself with the mediator so you can speak about any worries you have.\nIt normally takes area with the other parent over several sessions if you proceed with mediation. If you like, you can be in separate spaces. Each session lasts concerning 90 mins.\nSome of the concerns that may be gone over:\n- Exactly how to connect regarding the kids.\n- What the youngsters are assuming.\n- Making crucial decisions concerning the youngsters.\n- Child upkeep.\nFinances and also Property\n- Who lives where.\n- Cash money flow and that spends for what.\n- Figuring out properties, financial obligations as well as pensions.\n- How to pay for life after separation.\n- Just how to reduce conflict.\n- Exactly how to be good parents apart.\n- The participation of brand-new companions.']	['<urn:uuid:35b00848-0501-45e3-a1f1-0c5daa5fe6e0>', '<urn:uuid:fc53f922-6176-4f57-b6bb-4c700dc67d78>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	12	65	3332
5	When did both Armenia and Kazakhstan gain their modern independence?	Armenia restored its independence on September 21, 1991, while Kazakhstan gained independence after the Soviet Union's collapse following the failed coup attempt of August 19, 1991.	"['Territory of Kazakhstan has come to be mastered by man nearly a million years ago. As early as the age of Lower Paleolith the ancient man settled down on these Karatau lands fit for normal life, rich with game and wild fruit. It is there that they have found ancient settlements of Stone Age. By and by, in the centuries of Middle and Upper Paleolith the man came to master Central and Eastern Kazakhstan and Mangyshlak area.\nAs have been shown by excavations of the neolythic settlement Botay in the North Kazakhstan, Kazakhstan constitutes the area of horse-mastering (breeding) and that of formation of nomad civilizations. Archeologists revealed dwellings, numerous hand-made articles of stone and ivory which present the ancient history and archeology of Kazakhstan in the Stone epoch in an altogether new way.\nAs early as the Bronze Age, some four millenia ago, the territory of Kazakhstan was inhabited by tribes of the so called Andron and Begazy-Dandybay culture. They were engaged in farming and cattle-breeding, they were fine warriors who handled combat chariots marvellously. To this day we can see images of chariots drawn on rocks where ancient people would arrange their tribal temples and sanctuaries with the firmament as their natural cover. On the surfaces of black cliffs burnt with the sun people would chisel out scenes of dances, images of sun-headed deities, mighty camels and bulls as impersonations of ancient gods. Burial mounds of noble warriors scattered all throughout Kazakh steppes are known for magnificient size of mounds and burial vaults proper. Particularly famous are such necropoles in the steppes of Sary-Arka and Tagiskent in the Transaral area. People of that epoch were not only fine warriors, shepherds and farmers but also skilled metallurgists. They would take bronze and manufacture axes, knives, daggers and various decorations thereof.\nIt were they who initiated development of copper which is being practiced to this day — they are Zhezkazgan and Sayak copper quarries of today. Ancient people lived in large settlements and ancient towns surrounded with walls and fosses. These towns were inhabited with warriors and craftsmen, priests and farmers. These tribes lived on the territory of Kazakhstan for about a thousand years — from the XVIIth century BC to IX-VIII centuries AD.\nLater on they were ousted by Saks. Such was the name given to this tribe by ancient Persians. The Chinese called them «se» whereas Greeks chose to call them Scythians. They were essentially nomads, semi-nomads and farmers. Yet, first and foremost, they were excellent horsemen. In fact Saks were the first ever horsemen in the world to master arrow-shooting at full tilt.\nIn VI-II centuries BC Saks set up their first state with its centre in the Zhetysu (Semirechje) in South-East Kazakhstan. Kings of Saks were at the same time high priests. Saks had written language and mythology of their own, they were known for their well developed art of world standard labelled in research papers as «animal-styled art». Respective subjects were represented by predators and herbivourous animals and the struggle there between. Sheer masterpieces made of gold and bronze serve as worthy exhibits of best museums of the world.\nIn the Issyk burial mound that harboured the world-famous «Golden Man» they have found a silver bowl whose bottom bore an incription consisting of 26 characters. They have failed to read it to this day. Some think that the incription is made in one of the Iranian languages, others insist on its prothoturkic origin.\nThe middle of the first millenium AD is a fairly important stage in the history of all Turks in general and Kazakhs in particular. The period is marked with manifest changes in ethnic media: predominant now there become Turkic tribes which chose Altai as their natural centre. Written sources of the VIth century register the term «Tyurk» which is pronounced as «Tutszyue» by the Chinese and as «Turk» by Sogdians.\nStarting from the IV-th century up to the beginning of the XIII-th century the territory of Kazakhstan was the seat of West-Turkic, Tyurgesh, Karluk Kaganates, of the state made by Oguzes, Karakhanides, Kimeks and Kypchaks. All of them successively replaced one another up to the very Mongol invasion. After the invasion, i.e. in the beginning of the XIIIth century, there have shaped up uluses of the Mongol Empire of Zhuchi-Khan and Zhagatai which later gave birth to Ak-Orda, Mongolistan and finally to Kazakh Khanate.\nEssentially all these states were mixed economies. Tribes of cattle-breeders had farming tribes as their neighbours, steppes and cities supplemented each other. Such cities as Taraz, Otrar, Ispijab, Talkhir were set up right in the way of the Grand Silk Route which served as a reliable link joining antiquity and Middle Ages, the West and the East: Japan, Korea and China with Central Asia, Iran, the State of Seldzhuks, Rus, Byzantium, France and Italy.\nOn the whole the process of formation of the Kazakh ethnic national group came to completion in XIV-XV centuries.\nBy virtue of many ethnopolitical and economic factors on the territory of Kazakhstan there formed three ethnoterritorial associations — Senior, Middle and Junior Juzes. Division into juzes was the actual reflection of the complex process — formation of the Kazakh ethnic national group, it left its characteristic stamp on local cultural and every-day specifics of its development.\nKazakh Khanate shaped up in the middle of the XVth century. Extension of estates, consolidation of statehood, active foreign policy pursued by the First Khans, particularly by Kasymkhan in early XVIth century, later on gave way to ever growing intestine wars and mass roamings-off of people.\nThe aggravated economic and political situation generated the issue of joining Russia, a process that lasted nearly 150 years. Abulkhair, Khan of the Junior Juz exasperated by the ongoing weakening of the Kazakh Khanate as a result of the exhausting war had to accept the terms of the Junior Juz’s joining Russia. Efforts of the «opposition» headed by Sultan Baraka to impede the negotiations and frustrate formalization of the act of the Junior Juz’ joining Russia ended in a failure and on October 10, 1731 a part of the assembly of Kazakh elders supported the decision.\nPolitical and economic status of Kazakhstan in the middle and the end of the XVIIIth century features the following: aggravation of internal discord in the Junior juz; expansion of economic relations with Russia; development of barter trade; aggravation of the land issue; first moves in colonial policies of tsarism in the agrarian issue.\nTo step up colonization of the territory they introduced a Russian system of administration which resulted in a protest on the part of the Kazakh population which — subsequently — expressed itself in a national-liberation struggle of Kazakhs within the Russian empire. In sum, the Kazakh people raised the banner of freedom more than 300 times. Starting from the second half of the XIXth century Kazakhstan represents a completely shaped up colony of the Russian Empire.\nLate in the XIXth century colonial specifics of the agrarian policy of tsarism brought about a gradual shift in the proportion of nomadic and settled population. There have shaped up new forms of economies: a settled cattle-breeding one and a settled farming one. Social differentiation of the Kazakh society becomes ever more pronounced. A part of economies comes to be envolved in market relations. Now quite typical is the engagement of impoverished Kazakhs in various industries that emerged in Kazakhstan in the last quarter of the XIXth century. Local merchants initiated a new practice of trade — they started arranging fairs. Over the last decade of the XIXth century they built some 482 km of railway lines. Development of transit trade is also underway. In context of the Kuldzha Treaty of 1851 trade links with China come to be more intensive. In commercial systems there appear elements of usury and private entrepreneurship.\nColonial oppression weighed hard on the living standards of the Kazakh people. Ever growing taxes and duties, land withdrawal generate aggravation of internal contradictions in the Kazakh countryside, a crisis in cattlebreeding economies.\nThe war of 1918 crowns the national-liberation movement of Kazakhs within the Russian Empire. Victorious February revolution in Russia brought about an overthrow of the tsarist government. It was a period of dual power in the country: the power of the proletariat and peasantry represented by the Soviets of Workers’ and Soldiers’ Deputies and the power of bourgeoisie and land-lords represented by the Provisional Government.\nUpon the overthrow of tsarism, in Kazakhstan too, they started organizing Soviets of Workers’ and Soldiers’ Deputies. It went on hand in hand with the emergence of bodies of bourgeois power: all sorts of «executive committees», «civil committees», regional or district commissars appointed by the Provisional Government.\nThe end of the 20s-30s is marked with an upsurge of peasant uprisings to counter forced collectivization which brought about mass deaths of people. «Kazakhstani tragedy» — such is the name of this man-made disaster in history. The 30s is the period of establishment of totalitarianism in Kazakhstan which entailed massive political repressions, ever-present and omnipotent in all spheres of social and political life.\nIn 1939-1941 Kazakhstan is transformed into a major basis for production of non-ferrous metals, coal, oil, it becomes a region of developed agriculture.\nIn 1941-1945 Kazakhstan comes to be envolved in the WWII, more particularly in the Great Patriotic war against the invasion of fascist Germany. In this period practically all Kazakhstani enterprises put out only defence produce. Major industrial facilities and cultural establishments are being evacuated to Kazakhstan from western and central areas of the USSR. The war years are filled with the selfless heroic labour of millions of Soviet people in industries and agriculture. In WWII Kazakhstan lost nearly 425,000 people. The Republic played host to hundreds of thousands of evacuated people;\nIn 1954 top leaders of the country launch a campaign of massive development of virgin and fallow lands which entails a new influx of people arriving from other republics of the USSR, they boost up housing construction, that of cultural and mass facilities.\nThe years of the 50s — 80s are those coloured with the struggle between the party-and-state bureaucracy and the people. This 30-year span is characterized by an offensive against the bureaucracy (so called «Khruschev’s thaw») and the bureaucratic retaliation (L.I. Brezhnev, K.U. Chernenko) which brought about a stagnation in the Kazakhstani society.\nThe course of social and economic transformations announced by Mikhail Gorbachov when he became the topmost leader of the USSR, betrayed, initially, the same tendency as the efforts of his predecessors who did their utmost — when ascending to power — to present themselves as exceedingly active democratic reformers. The system of social, economic and political actions aimed at reforming the society remained in history as a notorious «perestroika».\nElections of the USSR people’s deputies held in spring 1989 proved to be very important in context of eventual transition to genuine democracy. For the first ever time they were conducted on the basis of alternative nomination and voting accompanied by political competition.\nThe day of August 19, 1991 — the day of the abortive attempt of a coup. President Nursultan Nazarbayev of Kazakhstan addresses the people of Kazakhstan. They issue Presidential Decrees «On the formation of the Security Council of the Kazakh SSR», «On the transition of state-owned enterprises and organizations of Union subordination into that of the Government of the Kazakh SSR», «On establishing the gold and diamond fund of the Kazakh SSR», «On closing of the Semipalatinsk nuclear testing grounds», etc. In September 1991 there has been adopted a\nAt the moment major world companies open their representations in the Republic, we witness an intensive increase in the number of joint ventures, they elaborate and implement projects of joint development of natural resources and processing raws and materials, introduction of advanced technologies in the agrarian sphere, etc.\nTremendous natural resources of Kazakhstan, social and political stability in the Republic make this region one of the most attractive objects of capital investments among other republics of the late Soviet Union.\nRepublic of Kazakhstan has become a full-fledged equal member of the world community to enjoy universal recognition and support of absolute majority of the states of the world. As of today Kazakhstan has been recognized by 113 states including 105 ones that maintain diplomatic relations with Kazakhstan.\nToday we witness consolidation of interaction, extension of cooperation with most authoritative international agencies. Among them is the UNO, the European Community, the International and European Banks for Reconstruction and Development, the International Monetary Fund, the IAEA, the IFRCRCS, the UNICEF, the UNESCO, the WHO, etc. The Republic has acceeded to more than 40 international conventions, it has signed almost 400 multilateral and over 700 bilateral agreements and treaties.\nIn 1997 there has been adopted a decision on transferring the capital of the country from Almaty to Akmola, the circumstance conditioned by geopolitical and economic considerations of Kazakhstan’s development.\nFinally, there has been elaborated and adopted for realization a Programme «On measures for realization of Kazakhstan’s development up to 2030″.', 'The History of Armenia\nMap of Armenia (through the centuries)\nAncient Armenia (3500 BC - 520 BC)\nArmenia is one of the oldest countries in the world with a recorded history\nof about 3500 years. The oldest known ancestors of modern Armenians, the\nHayasa-Azzi tribes, also known as Proto-Armenians, were indigenous to the\nArmenian Highland in Eastern Anatolia. These tribes formed the Nairi tribal\nunion, which existed until late 13th century BC. The legendary forefather of\nArmenians, Hayk, famous for his battles with Babylonian ruler Bel, most likely\nwas one of the Hayasa tribal leaders. The words \'Nairi\' and \'Nairian\' are still\nused by Armenians as poetic synonyms of the words \'Armenia\' and \'Armenian\'.\nAt the end of the second millennium BC, another Indo-European ethnic group, closely related to Thracians and Phrygians and referred to by the Greeks as Armens, migrated to the Armenian Highland from Northern Balkans. According to a Greek myth, which actually reflects this tribal migration, the forefather of Armenians - Armenios - was one of the Argonauts, accompanying Jason in his quest for the Golden Fleece. In the year 1115 BC, king Tiglath Pileser I of Assyria reports a battle with a force of 20,000 Armens in the Gadmokh province of Assyria.\nThe mixture of Armens with the indigenous Hayasa eventually produced the Armenian people as it is known today. The existence of two major segments in the Armenian people is best of all illustrated by the fact that Armenians call themselves ""Hay"" and their country ""Hayastan"" after Hayasa, while other peoples call them Armenians and their country Armenia after the Armens. The Armenian language is basically the language of Armens, which is the only survivor of the now extinct Thraco-Phrygian group. It incorporated a large number of Hayasa words and grammatical features, as well as a significant number of non-Indo-European words from minor ethnic groups, which also took part in the ethnogenesis of Armenians.\nThe first significant state of the Armenian Highland was the highly advanced Kingdom of Ararat (with the capital in Tushpa, today\'s Van), better known under its Assyrian name Urartu (Ararat). This state was formed in the XI century BC and existed until VII century BC. Although populated mostly by Armenians, Urartu was ruled (at least during the first centuries) by a non-Armenian and non-Indo-European dynasty. In 782 BC the Urartian king Argishti I founded the fortified city of Erebuni, which is today\'s Yerevan, the capital of Armenia. Another major city in the Valley of Ararat was Argishti-khinili, also founded by Argishti I in the year 775 BC.\nIn the late VII century BC Urartu, weakened by Scythian invasions, fell, but after several decades was revived under the Armenian Yervanduni (the Orontides) dynasty with the capital in Armavir, former Argishti-khinili. The revived kingdom was already called Armenia by its neighbours, but in some languages the older name, Urartu, was still in use. In the famous tri-lingual Behistun inscription of Persian king Darius the Great (522-486) the same country is referred to as \'Armenia\' in the Persian and Elamite versions, and \'Urartu\' in the Akkadian version.\nArtashisian dynasty, First Armenian KingdomArmenia under the Yervanduni dynasty soon became a satrapy of the mighty Achemenide Persia, and later part of the Seleucid Empire. It restored its full independence in 190 BC under the king Artashes I, founder of the Artashesian dynasty (the Artaxiads).\nThe kingdom started to expand and reached its peak during the reign of Tigran II, also called Tigran the Great (95-55 BC). Under Tigran, Armenia ascended to a pinnacle of power unique in its history and became the strongest state in Asia Minor. Extensive territories were taken from Parthia, which was compelled to sign a treaty of alliance. Iberia (Georgia), Caucasian Albania, and Atropatene had already accepted Tigran\' suzerainty when the Syrians offered him their crown (83 BC). Tigran penetrated as far south as Ptolemais (modern Akko in Israel). As a result, the empire of Tigran II stretched from the Caspian Sea in the East to the Mediterranean Sea in West, and from Mesopotamia in the South to the river Kura in North. Political strengthening and territorial expansion of Armenia was accompanied also by unprecedented cultural development, with rich cultural heritage of Urartu intermixing with Hellenistic features. As a result Armenia during the Artashesian period became one of the most Hellenized and culturally advanced countries of Asia Minor.\nAfter the death of Tigran II, Armenia was reduced back to its ethnic Armenian territory and found itself in the middle of a long war campaign between Rome and Persia, with each superpower trying to have Armenia as its ally, as the military assistance with Armenia was crucial for gaining political superiority in Asia Minor.\nArshakunian dynasty, Second Armenian KingdomIn the middle of the I century AD a new royal dynasty - the Arshakuni (the Arsacids) - was established in Armenia. This dynasty was related to the royal family of Persia, which bared the same family name. At this period Armenia and Persia enjoyed a long period of peace and cooperation, until in 251 AD the Sassanid dynasty came to power in Persia. Regarding Armenia as the ally of the overthrown dynasty, the Sassanids adopted anti-Armenian policy, trying to eliminate the Armenian state and to assimilate the Armenian nation. Since the Armenian religion of that period bared similarities to both Zoroastrianism and Greco-Roman polytheism, in the realization of their anti-Armenian policy the Sassanids were trying to capitalize on the religious closeness. In order to deprive the Persians of this advantage, the Armenian king Trdat III in 301 AD declared Christianity the state religion of Armenia, thus making Armenia the first Christian state in the world, with Gregory the Illuminator as the first head (Catholicos) of the Armenian Apostolic Church. Christianity was officially legalized in the Roman Empire 12 years after Armenia became officially Christian.\nArab invasion and Byzantine EmpireBy the end of the IV century the Byzantine Empire and Sassanid Persia officially established their spheres of influence in Armenia. The Arshakuni dynasty was dissolved in the year 428, and eastern part of Armenia was annexed to Persia, while the western part was put under Byzantine rule. The Sassanids were forcing Armenians to convert to Zoroastrianism, causing the Armenian revolt of 451 under the leadership of prince Vartan Mamikonian, commander-in-chief of the Armenian army. Although the Armenian forces, outnumbered by the Persians, actually lost the legendary battle of Avarayr, and Vartan Mamikonian himself was killed, this turned out to be a significant victory for Armenians, as Persians eventually gave up their efforts to convert and assimilate Armenians, and were forced to agree to much higher level of autonomy for Armenia.\nThe spiritual independence of Armenia was further asserted in 554, when the second Council of Dvin (capital of Armenia of that period) rejected the dyophysite formula of the Council of Chalcedon (451), a decisive step that cut Armenians off from the Roman and Greek churches as surely as they were already ideologically severed from the East.\nBy the time of Arab invasion in 634 Armenia, ruled by prince Theodore Rshtuni, was virtually independent. After conquering Persia, the Arabs started to concentrate their armies against Armenia, but didn\'t manage to conquer the country until 654.\nBagratunian dynasty, Third Armenian KingdomAfter more than two centuries of struggle with the Arab Caliphate, Armenia regained its independence in 886, and both the Caliphate and Constantinople recognized prince Ashot Bagratuni as the king of Armenia. During the rule of the Bagratuni dynasty Armenia reached its peak in political, social and cultural development. The capital of Armenia of that period, Ani, was a magnificent city, known as ""a city of one thousand and one churches"". The Armenian architecture of the Bagratuni period, especially the dome laying techniques, for which Armenian architects were notorious, significantly influenced the Byzantine and European architectural styles.\nAt the end of the 10th century the Byzantine Empire, although ruled by an imperial dynasty of Armenian origin, adopted a near-sighted policy of weakening Armenia and eventually annexed it in 1045, thus depriving itself of an effective shield against disastrous invasion of Turkic nomads from Central Asia.\nRubinian dynasty, Fourth Armenian KingdomBefore the fall of the Bagratuni kingdom a number of Armenian princes managed to escape from Armenia and found refuge in Cilicia, a region at the north-eastern corner of the Mediterranean Sea, where Armenians were the majority of population. In 1080 their leader, prince Ruben, founded in Cilicia a new kingdom, which became known as Cilician Armenia, or Armenia Minor (Little Armenia). The new Armenian state established very close relations with European countries and played a very important role during the Crusades, providing the Christian armies a safe heaven and provision on their way towards Jerusalem. Intermarriage with European crusading families was common, and European religious, political, and cultural influence was strong. The royal court of Cilicia and the kingdom itself were reformed on Western models, and many French terms entered the Armenian language. Cilician Armenia also played an important role in the trade of the Venetians and Genoese with the East.\nEnduring constant attacks by the Turks, Mongols, Egyptians and Byzantines, Cilician Armenia survived for three centuries and fell to Egyptian Mameluks in 1375. The last Armenian king of Cilicia, Levon VI Lousinian, emigrated to France, where his grave still can be seen in the St. Denis Cathedral of Paris. The title ""King of Armenia"" passed to the kings of Cyprus, thence to the Venetians, and was later claimed by the house of Savoy.\nArmenia under turkish ruleAfter the fall of the Cilician Armenia, the historical Armenian homeland, or Greater Armenia, was subject to various Muslim warlords, and eventually was divided between the Ottoman Empire (Western Armenia) and Persia (Eastern Armenia). Several Armenian principalities managed to preserve their independence or autonomy. The most significant among those was the Federation of Khamsa in Artsakh (today\'s Nagorno-Karabakh), which consisted of five allied principalities. De facto independent Armenian principalities existed also in the regions of Sasun and Zeytun in Western Armenia.\nBeing for centuries at the edge of physical annihilation, Armenians nevertheless managed to preserve and develop their national, religious and cultural identity. Apart from architecture, Armenians successfully manifested themselves in literature, painting, sculpture and music. Armenians were the 10th nation in the world to put their language in print.\nArmenian QuestionIn 1828 the Russian Empire captured Eastern Armenia from Persia. Contact with liberal thought in Russia and Western Europe was a factor in the Armenian cultural renaissance of the 19th century. In the Ottoman Empire, the Armenians initially benefited with the rest of the population from the measures of reform known as the Tanzimat, and in 1863 a special Armenian constitution was recognized by the Ottoman government. These liberties were however unknown outside Constantinople, and the condition of Armenians in Anatolia was unbearable. A so-called ""Armenia Question"" emerged in the relations between the Ottoman Empire - ""the sick man of Europe"" - and European superpowers. After the Russo-Turkish War of 1877-78, in which Eastern Armenians had taken part, Russia insisted in the Treaty of San Stefano that reforms be carried out among the sultan\'s Armenian subjects and that their protection against the Kurds be guaranteed. This demand was softened at the Congress of Berlin, but the ""Armenian Question"" remained a factor in international politics, with Great Britain taking on the role of Turkey\'s protector until the end of the century.\nHaving lost most of its territory in the Balkans, the Ottoman Empire was afraid of losing Western Armenia as well, which would mean the end of the Ottoman dream of creating a pan-Turkic empire, stretching from Balkans to the Yellow Sea. A new state policy was formed, aiming at the final resolution of the ""Armenian Question"" through total annihilation of Armenians in their historic homeland of 3500 years. During the reign of sultan Abdulhamid Armenian massacres became a common phenomenon. In 1895, after Abdulhamid had felt compelled to promise Britain, France, and Russia that he would carry out reforms, large-scale systematic massacres took place in the Armenian provinces. In 1896 more massacres broke out in the capital and in Cilicia.\nArmenian GenocideAfter coming to power in Constantinople, the Young Turks made the policy of ""No Armenians - no Armenian Question"" their main priority. Taking advantage of the favourable political conditions created by the World War I, they began the ""final resolution of the Armenian question"" on April 24th, 1915, by executing hundreds of Armenian intellectuals of Constantinople without trial. In Armenian provinces of Eastern Anatolia all Armenian males aged 15-62 have been conscripted, disarmed and executed. Defenceless Armenian women, children and the elderly were deported to the Syrian desert Der-el-Zor; most of them were brutally murdered on the way by Turkish soldiers or Kurdish nomads, or died of starvation and exhaustion. More than one and half million Armenians, i.e. 80% of the Armenian population of Western Armenia, perished in this first Genocide of the twentieth century. Several hundred thousand survivors of the Genocide found refuge in neighbouring counties, laying the foundation of the worldwide Armenian Diaspora. By the year 1923 Western Armenia was completely de-Armenized, and successfully incorporated into the newly formed Turkish Republic.\nNagorno-Karabakh movementThe 71 years of Soviet rule in Armenia were a period of relative security, of great economic development, and of cultural and educational achievements. But during the same period the government of Soviet Azerbaijan was conducting a systematic policy of removing the Armenians from Nakhidjevan, which today has no Armenian population whatsoever. The same policy was less effective in Nagorno-Karabakh, where Armenians remained the overwhelming majority. In February 1988 a peaceful, democratic movement for the reunification with Armenia began in Nagorno-Karabakh, and the regional Assembly of the Nagorno-Karabakh Autonomous Region adopted a resolution seeking transfer of Karabakh from Azerbaijan to Armenia, as a realization of the right of the peoples under alien domination to self-determination. The Azeri side responded by Armenian massacres in the Azeri cities of Sumgait, Kirovabad and Baku, transforming the peaceful movement into a violent conflict, and lately committing an act of military aggression against the Armenian population of Nagorno-Karabakh. The newly proclaimed Republic of Nagorno-Karabakh managed to defeat the invading Azeri forces and to create a security zone around its territory and a humanitarian corridor to Armenia. The negotiations on the future status of Nagorno-Karabakh are being conducted within the Organization for Security and Cooperation in Europe, through so-called Minsk Group, co-chaired by USA, Russian Federation and France\nRestoration of Independent ArmeniaArmenia restored its full independence on September 21, 1991, and became a member of the United Nations on March 2, 1992. On January 25, 2001, Armenia also became a member of the Council of Europe.\nSource: United Nations (Official Site)']"	['<urn:uuid:7ecf9864-53ce-4c75-90c5-12938f09ab6c>', '<urn:uuid:40d7fb63-79ab-4c06-b0bd-a309a2c92c48>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T00:57:18.859842	10	26	4612
6	What's the history behind cigars and how did they become popular around the world?	Cigars originated with the Mayan Indians and other indigenous peoples of the Caribbean Islands, Mexico, and Mesoamerica, who were smoking them as far back as the 10th century. Christopher Columbus is credited with introducing tobacco to Europe after encountering it during his explorations. Interestingly, the word 'cigar' comes from the Mayan word for tobacco, 'siyar'. In places like San Andrés Tuxtla, Mexico, the tobacco industry began in the 1830s when Cuban and European immigrants recognized that the climate was suitable for growing fine tobacco. They established cigar production for export to Europe, and today regions like San Andrés Tuxtla produce some of the finest cigars in the world.	['San Andrés Tuxtla\nSan Andrés Tuxtla is a city and municipality is the south of the Mexican state of Veracruz.\nThe city is the largest in the Los Tuxtlas region. It was founded after an eruption of the San Martín Volcano pushed people into this valley during the early colonial period.\nSince the 19th century, the area has been the center of Mexico’s production of fine tobacco, mostly used in the making of cigars. The best known cigar producer here, Te Amo, is considered to be one of the best brands in the world.\nThe coastline of San Andrés Tuxtla municipality has a number of beaches which are promoted under the name of Costa de Oro, along with Puerto de Balzapote, a fishing village.\nThe city of San Andrés Tuxtla is the largest city in the municipality of the same name and in the entire Los Tuxtlas regions. It is located in a small valley with the San Martín Volcano towering above it.\nThe city is centered on a square called Parque Lerdo, which has a circular kiosk in the center. Every Sunday the city sponsors cultural events such as art exhibitions and traditional music and dance, such and fandango and zapateado dance. Much of this reflects Afro-Mexican influence.\nThe main structure is the Cathedral of Saint Joseph and Saint Andrew, built in 1870 with sleek bell towers and a sober facade with Neoclassical influence. The patron saint of the city is Saint Andrew, who is celebrated on November 29 and 30. On the 29th, the main event is the running a bull figure made of reeds with the local youth.\nThis is followed by a parade of mojigangas, large figures of hollow cartonería on a frame, worn on the shoulders of a dancer. These figures were brought to the area by the Spanish and over time have been modified to a unique Tuxtla style. On the 30th, there is mass and a procession with a large image of the saint pulled by tractor and surrounded by mojigangas. There are also releases of sky lanterns. Another important annual event is Las Ramas on December 16, which begin Las Posadas, a type of caroling event carrying branches.\nThe main museum is the Regional Museum of San Andrés Tuxtla, which contains objects from the region, many of which were donated by residents. It was opened in 2008, and features artifacts from is pre Hispanic history as far back as the Olmecs and the development of the tobacco industry, especially in the 19th century. One important piece is a crucified Christ image called El Señor de Matacapan, whose pieces were found in Matacapan and reconstructed by the Instituto Nacional de Antropología e Historia.\nThe Casa de Cultura is on Hernandez y Hernandez Street and hold cultural events mostly attended by city residents. It also has a collection of pre Hispanic artifacts.\nThe city has a number of old houses with gardens and patios from the city’s heyday and Parque Chichipilco is popular with families and hold cultural events.\nLa Maquina Vieja is an old industrial structure dating from 1840, located just outside the city.\nSan Andrés Tuxtla is best known for the production of tobacco and cigars and cigarettes with their cigars considered the best in Mexico, and Winston Churchill is said to have favored cigars from the area. The area’s tobacco history began in the 19th century when Cuban and German immigrants and decided that they could grow fine tobacco and make cigars here for export to Europe.\nToday, the area produces some of the finest cigars in the world. All steps from the growing of tobacco, to curing to wrapping are all done in the municipality with San Andres’s own experts to assure quality. Torcedores, or those who create the cigars from cured tobacco leaves, apprentice for two years. Many families have worked in the industry for generations, with children learning the work as children in family workshops which make both cigars and cigarettes.\nThe best known local producer of cigars is Te Amo in Sihuapan, ranked as among the best cigar producers in the world. The enterprise employs and average of 350 people although times of peak demand can raise this number to 1,200.\nTe Amo and other producers have large “houses” in which tobacco leaves are dried and fermented using firewood and steam over a course of fifteen days. The dried and fermented leaves are rehydrated for deveining and rolling. After this, the cigars are stored for one or two years before going to market. The flavor of the cigar depends on the selected leaves. Te Amo produces five kinds of cigars which vary in diameter and length.\nOther notable tobacco producers include Matacapan, Sihuapan, La Constancia and El Salto.\n156.5 km of main highway, mostly state-maintained roads. The main highway through the area is Federal Highway 180.\nThe municipality is located in the south of Veracruz in the San Martin Mountain Range in the Los Tuxtlas region. It has rugged terrain of volcanic origin that extends to the Gulf of Mexico and an average altitude of 300 meters above sea level. The main elevations are San Martín, Mastagaga, Vigía and El Chilar. There are caverns at Roca Partida, De la Encantas, the Cerro del Borrego and Arrecife de la Poza.\nAlong the Gulf shore there are beaches such as Salinas Roca Partida, Toro Prieto, Arroyo de Lisa, Costa de Oro, Playa Hermosa, Los Organos, Dos de Abril, Monte Pio and Balzapote (which is also a fishing port). There are cliffs at Roca Partida.\nMost of the natural vegetation of the area has been destroyed with 410.1km2 used for agriculture, 398.3km2 for pasture and 7.2km2 is urban. Ab out 36km2 contains secondary vegetation only 10.8% is forest and 94.1km2 is rainforest. The soil is poor in nutrients and is susceptible to erosion. Despite this, much of the municipality is part of the second most important biosphere reserve in Mexico, the San Andres Tuxtla Reserve. The remaining dominant natural foliage is mostly high perennial rainforest, with dunes and secondary vegetation near the coast.\nWildlife consists of armadillos, rabbits, opossums, skunks and anteaters.\nMost of the municipality’s surface water is in the San Andrés River, streams such as the Sihuapan and San Juan and various small lakes. Laguna Encantada (Enchanted Lake) is an isolated lake, off the highway between the city and Sontecomapan in the neighboring Catemaco municipality. The road does not reach the lake, so it is necessary to walk about a kilometer over a rise. The name comes from the fact that its water level rises in the dry season and lowers during the rainy season. Its circumference is 1,500 meters . Other lakes include De Cuyapan, De Tizatal, Los Manantiales, Delicia and Carrizal.\nOn the San Andrés River is the Eyipantla Falls, located twelve km from the municipal seat. The name comes from Nahuatl and means “three furrows of water.” Legend has it that Tlaloc, Aztec god of rain, governed the Los Tuxtlas area and the waterfall was his home. The falls are part of the San Andres River, which drains Lake Catemaco on its way to the Gulf of Mexico. The falls itself is forty meters wide and fifty meters high. It is popular with tourists, to descent 400 stairs to the foot of the falls. At the top of the falls, there is a lookout point which gives view of the surrounding mountains along with the falls. The entrance to the falls is crowded with restaurants and tourist shops. Another important waterfall in the municipality is Los Organos, located between Playa Hermosa and Costa de Oro. Other waterfalls include Del Río Revolución, Salto Maquina Vieja and Tres Chorros.\nThe climate of the municipality varies depending on location. It is warm in all parts but the amount of rain varies to areas with most rain in the summer to those that receive abundant rainfall year-round. Average annual temperature varies from 18 to 28C and average annual rainfall varies from 1,100 to 4,600mm.\nThe name “Tuxtla” is derived from Nahuatl and means “place of the rabbit.” San Andrés refers to the patron saint, Saint Andrew.\nIn the 16th century, the area belonged to the Santiago Tuxtla province. In 1530 there was an eruption of the San Martín Volcano, which prompted the movement of people from Ixtlán to what is now the city of San Andrés Tuxtla, then called Tuzacoalco (land between the mountains). The settlement was recognized in 1580 by the head of the Tlacotalpan, Tuztla and Cotaxtla region.\nIts tobacco history began in 1830 when a slow influx of Cubans and Europeans made their way here and decided that the climate was apt for the growing of fine tobacco. Today, the area produces some of the finest cigars in the world. Just prior to the Mexican Revolution the industry experienced strikes by workers, with the first occurring at the El Destino factory in 1896.\nThe municipality was defined in 1825 and in 1826 the city was declared its seat. In 1830 the seat was declared a town, and in 1893 a city.\nIn 1932, the city and municipality’s name was changed to Los Tuxtlas, but this was changed back to San Andrés Tuxtla in 1938.\nYou can dial 078 from any phone, where you can find free information about tourist attractions, airports, travel agencies, car rental companies, embassies and consulates, fairs and exhibitions, hotels, hospitals, financial services, migratory and other issues.\nOr dial the toll-free (in Mexico) number 01-800-006-8839.\nYou can also request information to the email firstname.lastname@example.org\nMORE EMERGENCY NUMBERS:\nGeneral Information: 040 (not free)\nNational Emergency Service: 911', 'Cao cigars are a specialty brand cigars especially made in Honduras at Nestor Plasencias de Tabacas Oriente factory. Some of the ingredients are made elsewhere: such as the fillers are made of Nicaraguan and Mexican tobacco. The binders come from Honduras with wrappers made in Connecticut giving them a unique quality and diverse image as compared to other tobaccos. These cigars are well constructed and offer a mild smoke. The Cao brand has a good amount of diverse tobaccos to suit the cigar connoisseur. Furthermore, the Cao brand is most popular for its superb quality, its innovative product development, and the specialty packaging. The premium hand-rolled cigars are most enjoyed by connoisseurs from around world.\nCigars in general are rolled in a circular form, which provides ease for smoking. The bundle of tobacco is dried and fermented, in which it is ignited so that its smoke is drawn into the smoker’s mouth. The roll consists of cut tobacco filler which is formed in a binder leaf and with a wrapper leaf that is rolled in a spiral around the bunch of leaf. The wrapper leaf has to have the quality of strength, elasticity, silky in texture, and even in color. The pleasant flavor and good burning properties gives it an ease for processing. Compared to cigarettes, cigars produce a stronger odor and smoke. The origin of the cigar dates back to the 10th century where they were being smoked by Mayan Indians among other indigenous inhabitants of the Caribbean Sea islands, Mexico, and Mesoamerica. One of the famous and well-known explorers Christopher Columbus reported back to Spain his encountering of the tobacco and he is credited for introducing it to Europe. The word ‘cigar’ in itself is actually derived from the Mayan word for tobacco, ‘siyar’.\nThe cigar tobacco is grown in great quantities in countries such as Brazil, Cameroon, Cuba, Nicaragua, and the United States to name a few. When the tobacco leaves are harvested and aged, there is a process that is used to combine the use of heat and shade to reduce sugar and water content without causing the large leaves to deteriorate. This first step is called curing, which takes between 25 to 45 days. The curing process varies substantially based on the climatic conditions and the construction of the storage units used to store harvested tobacco. Therefore, the areas mentioned above tend to have the best climate for growing and harvesting tobacco. The second part of the process, which is called fermentation, is carried out under conditions that are designed to aid the leaf to die slowly. To ensure the leaf continues to ferment, without deteriorating, the temperature and the humidity are kept under control. This process allows the leaf to be more robust with aromatic richness. Then with the aging process being done properly, the tobacco are sorted for use as a filler or wrapper, which is based upon appearance and overall quality. The leaves go through an aging process. Then after the aging process is complete, the leaf matures to the specifications of the manufacturer it is ready to be used in the production of a cigar.\nQuality cigars are usually hand-made, where experienced cigar rollers are able to produce hundreds of good quality, almost identical, cigars on a daily basis. These rollers keep the tobacco moist, which includes the wrapper. Once rolled, the cigars are stored in wooden forms as they dry, in which, their uncapped ends are cut to a formal size.\nThe different Cao Cigars have fillers, wrappers, and binders originating in diverse areas:\n• The Cao America combines Connecticut Broadleaf Maduro and the Connecticut Shade wrappers with a unique designed pinstripe effect. It features a full-flavored, rich, and bold taste; that marks the legacy of our country. The tobacco grown and produced in the USA in the excellence of conditions preserving the authenticity of the cigar. The outward packaging is significantly designed to show the emblem and symbol of the USA.\n• The Cao Brazilia. Brazil known for its sun that sets gently on Rio de Janeiro, above a towering statue of Christ. The air currents give way to those that enjoy the atmosphere. The people enjoy the beaches by swimming day and night. The finest Brazilian tobacco grows to be the product of Cao Brazilian cigar.\n• The Cao Cameroon. It provides an escape to Africa. The cigar is filled and bound with rich, earthly Nicaraguan tobacco and wrapped with Grand One African Cameroon wrapper.\n• The Cao Criollo. The seed of the tobacco used originated in Cuba and grown in Nicaragua. This produced a rich and sweet Nicaraguan tobacco. The previous seeds that were grown, when they grew into tobacco would grow mold. The new seed from Cuba is immune to disease keeping them robust and fresh. Therefore, the best qualities of cigars are produced.\nThe Cao cigars are unique and diverse in that they bring the highest quality in tobacco to the consumer. Bringing the finest cigar is the goal of Cao cigars.\n- CAO OSA Sol Cigar: CAO OSA Sol Size: 5 x 50 (Lot 50/Robusto)...\n- CAO Brazilia Wrapper: Brazil Filler: Nicaragua Size: Lambada (6×50) Strength: Medium Quick...\n- CAO Flavours Wrapper: Cameroon Filler: Dominican Size: Petite Corona (4 x 38)...\n- CAO LX2 PRE-Review CAO’s LX2 cigar line had much buzz at the 76th...\n- CAO Gold Vintage Cremant Wrapper: 2004 Ecuadorian Connecticut Filler: Nicaragua & Honduras Size: Cremant...']	['<urn:uuid:1322bba5-a134-4836-b2d5-f9bdd06957ca>', '<urn:uuid:add27f10-bb1c-4a0b-b327-a675ba301a9c>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T00:57:18.859842	14	108	2509
7	What improves soil health and prevents pollution?	Soil health can be improved through vermicomposting and starting compost piles, while pollution can be prevented by using bio-fertilizers instead of chemical ones, properly treating industrial waste, recycling solid waste, and increasing tree plantation.	"[""Soil is the most important resource we in agriculture have. There are different classifications of soil, but at the end of the day all farmers are concerned with not only maintaining the soil properties, but also improving them over time. Yes indeed, the farmer is the epitome of conservation. After all, their livelihood depends on it!\nDirt is more mineral based with pebbles and rocks. Soil is mineral, plant, fungi, and animal based; an ecosystem in and of itself. Without soil, there is no agriculture. Period. So next time you look around at all the dirt in plentiful amounts, remember that it is just dirt, a filler if you will. And while dirt may be plentiful, soil certainly is not. It is precious and finite. We all should do what we can to take care of the wonder that is soil.\nThere are 4 main aspects of soil sampling:\n- Collect samples from the proper depth - For example, a depth of 7 inches is used to obtain data on pH, Phosphorous, and Potassium\n- Collect the proper amount of samples based in the land area in question - This is always a balancing act between information (what you want done) and cost (what can actually be done).\n- Collect samples from precisely the sames areas of the field - Thanks to GPS (Global Positioning System) and GIS (Geographic Information System), this can easily be accomplished.\n- Collect samples at the proper time - It is suggested samples be taken at a minimum of every 4 years; however, the farmers I work with usually sample every 3 years. The optimal time of the year to take samples is late summer and early fall.\nSounds easy enough no? Well it isn't really easy to do. You can't just walk out into a field and just start taking cores with a soil probe/auger. There are timing issues, consideration of past fertility application(s), tillage, and field conditions. It is indeed a maze a farmer must navigate and if the 7 P's (Proper Prior Planning Prevents Piss Poor Performance) were ever applicable, it is with soil management. Rightly so, the soil sampling activity is usually outsourced to a company whose specializes in this aspect of agriculture.\nThis is just one example of what farmers do to protect, manage, and improve their soil.\nSo what can you do if you are not a farmer? Here are some actions you can take today to make a difference.\nVermicomposting - Start a worm farm and use the end result, known as castings, as fertilizer for your garden. It is relatively inexpensive and will help create actual soil, a living breathing being. In conjunction with creating a bountiful crop you and your family can enjoy, all the food waste you either put down the garbage disposal or garbage can can now be used to feed those hungry worms! A win-win for everybody all around.\nStart a compost pile - Gasp! You are asking me to start piling up my food waste in a pile in my yard? Well actually no, not exactly. If you have the land area and can create a compost bin, go for it! But you don't have to have a ton of room to start composting. As a matter of fact you can compost using 2 five gallon buckets as seen here"", 'Soil pollution can be defined as the contamination of soil bringing adverse affects on the organisms in it. It can also be defined as the addition of unwanted substances in odd proportion. Soil pollution is also called land pollution.\nHere pollutants remain in direct contact with the soil for a long period of time, than that of water and air. Hence its problems are different from that of water and air pollution.\nThe widespread industrialization and increasing consumption have changed the composition and complexion of earth strata. It can be said that soil is being heavily polluted day by day by the toxic materials coming out of industries and domestic circles.\nSources of Soil Pollution\n(i) Industrial waste\n(ii) Urban waste\n(iii) Chemical and metallic pollutants\n(iv) Biological agents\n(v) Radioactive pollutants\n(vi) Agricultural practices\nDisposal of industrial waste is one of the major reasons for soil pollution. Industrial waste mainly consists of organic; inorganic and non-biodegradable materials. These pollutants change the physiochemical and biological aspects of the soil. Such as change in texture, mineral, bacterial and fungal colonies in the soil.\nBecause of the consumerism, our life style and food habits have been changing and are dumping us towards the point of danger. It includes both commercial and domestic waste, which is commonly referred to as refuse. This refuse along with solid waste contribute to soil pollution.\nChemical and metallic pollutant:\nVarious industries such as textile, dyes, soap and synthetic, detergents, drugs and metal industries dump their hazardous waste in soil and water, directly creating harmful impact on living organism.\nHarmful organism such as bacteria, fungi, algae, protozoan and microorganisms such as nematodes, earthworms, millipedes, snail etc. alter the physiochemical and biological atmosphere of the soil leading to soil pollution.\nVarious radioactive substances are introduced to the environment by different sources such as nuclear reactors, scientific laboratories, explosions, hospitals etc. They penetrate into the soil and accumulate there for many years creating soil pollution.\nAdvance agro-technology, huge quantities of fertilizers, pesticides, herbicides and weedicides are used to increase the fertility of the soil but their excessive use disrupts the soil’s physiochemical and biological property.\n1. Municipal garbage and chemical composition\n2. Mining practices\n3. Food processing waste\n4. Sugarcane trash in the field\nEffects of soil pollution\n1. Hazardous chemical entered into the food chain from soil, causes disruption of biochemical process.\n2. Soil becomes infertile because of water logging and salinity.\n3. Toxic chemicals affect plant growth and animal life.\nControl Measures for Soil Pollution\n1. The waste from various industries should be properly treated before dumping into the soil and water.\n2. Use of bio-fertilizers should be promoted instead of chemical fertilizers.\n3. Use of toxic and non-biodegradable materials should be discouraged.\n4. Education programmes regarding soil pollution and prevention measures should be promoted.\n5. There should be strict enforcement of environment protection law.\n6. Recycling and reuse of solid waste generated from various sources should be done.\n7. Maximum tree plantation should be initiated to minimize soil pollution.']"	['<urn:uuid:55c0bc09-f143-4982-a1fb-f80ce1a13e7b>', '<urn:uuid:aa4b4e03-3be6-41ae-b509-94a640e7edb4>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	7	34	1060
8	laser treatment applications veterinary medicine	The Companion Therapy Class IV Laser is used to accelerate natural healing through photo-bio-modulation. Laser therapy has been proven to relieve pain and inflammation, decrease swelling, stimulate nerve regeneration and promote tissue repair. It can treat both acute conditions (cuts, bites, tooth extraction pain, sprains, strains, fractures, post-surgical healing) and chronic conditions (degenerative joint disease, inflammatory bowel disease, periodontal disease, lick granulomas, hip dysplasia, feline acne, anal sacculitis, perianal fistulas).	['Our goal at the Integrative Veterinary Center is to provide clients with all available options for pet care. This is achieved by using conventional medicine, diagnostics, and all other reasonable therapies or systems of medicine to obtain the best therapeutic results. Every system of medicine has its strong and weak points and each is utilized according to what tool is best suited for an individual pet’s problem. The goal is to try to resolve or cure disease and not to suppress symptoms alone. In cases where the animal is at the end of its lifespan or has a terminal condition, the goal is to provide our patients with as high a quality of life as possible.\nAcupuncture is a branch of Traditional Chinese Medical (TCM) that has been practiced for over 5000 years. Traditional Chinese Medicine theory holds that Qi (vital energy) flows throughout the body along energetic channels called meridians. Each meridian is also associated with a specific internal organ that reflects the physiological and pathological conditions of that organ. The meridians connect the exterior of the body with the interior.\nAcupuncture points are located along these meridians, and when stimulated can relieve pain and restore normal body functions. Acupuncture points have specific locations and effects and have been documented over millennia. From a Western perspective, acupuncture has been shown to stimulate a variety of sensory nerves in the body that transmit the signal through the nervous system to the brain, which then releases various chemicals that produce multiple physiologic effects that activate the body’s homeostatic regulatory mechanisms.\nAt IVC, we always say that anyone, human or pet, can benefit from acupuncture. Acupuncture can be used to treat numerous ailments from diarrhea to kidney failure. There are multiple acupuncture techniques: dry needle acupuncture, moxibustion, aquapuncture, alectroacupuncture, laser acupuncture, and gold wire implantation into acupoints. These various methods allow for flexibility in animal acupuncture because different pets respond to the various techniques in different ways.\nAcupuncture and herbal medicine are typically used together because clinical results can be obtained more quickly that way. The combined effects are also stronger, this is especially important in the treatment of difficult cases.\nFor more information on the different types of acupuncture practiced at IVC please go to the Pain Management page of our website.\nBotanical (Herbal) Medicine\nTraditional Chinese Medicine (TCM) has been used in China for over 2500 years to maintain health in humans and animals. TCM includes herbs, acupuncture and massage. Herbal therapy and acupuncture are typically used together, as the combined effect is greater than using one of them alone. Chinese herbal medicine is the primary type of botanical medicine practiced at IVC. However, Western, Aryuvedic, Native American, South American and Hawaiian herbs may also be used depending on the case.\nThe Chinese herbal prescriptions used at IVC are not available over the counter; they are prescribed the same as conventional pharmaceutical drugs. The herbs used in these prescriptions are from the same companies as those used to treat humans. They have been tested for purity and quality. TCM herbs are safe and effective and can be used for long periods of time without side effects when prescribed by a qualified herbalist and used appropriately.\nChinese herbs can be used to treat most conditions recognized by conventional medicine. They can be used alone or combined with other therapies for an enhanced or synergistic effect. Often, a Chinese herbal and a conventional prescription will be used together.\nChinese herbs are especially helpful in the treatment of internal medicine disorders, failing organs, chronic diseases and diseases of the geriatric animal. They can be used to relieve pain, help improve and restore organ function, strengthen and support the immune system. TCM herbal prescriptions are specific for the individual patient and are directed at the root cause of an illness to correct it, and are not given to disease present and are not used to control symptoms alone.\nIn general, TCM herbal prescriptions must be given for longer periods of time than pharmaceutical drugs, but the benefit lies in their natural ability to gradually return the body to a state of balance and health, without adverse effects. Herbs must be given regularly two to three times a day just like conventional medications to see a therapeutic effect. Modern botanical medicine research is currently working to identify and document the active chemical constituents of many of the herbal formulas used today. Clinical results from TCM herbs can be seen in as little as 3 days but some take up to 2 months depending on the formulation for full effect.\nHerbal medications come in a variety of formulations: pills, tablets, capsules, liquids, topical tinctures, washes or pastes, and concentrated herbal extracts in powder or granule form. The formulation used depends on the type of condition treated and the species of animal.\nFor best results, our veterinarians will periodically monitor your pet’s condition, utilizing both conventional diagnostics and Chinese veterinary medical examination (tongue and pulse diagnosis) to determine if the formula they are being given is effective and still appropriate. In general, for acute conditions an animal may need to be rechecked once every 3-7 days; for chronic cases, once a month or longer may be adequate.\nAnimal chiropractic is a medical therapy that is used to maintain the health and normal functioning of the nervous and musculoskeletal systems. It follows the same principles and practice as chiropractic medicine in human medicine.\nA chiropractic adjustment is a very specific treatment that corrects subluxations. Subluxations are defined as misaligned vertebrae that are stuck or unable to move correctly causing pain, stiffness and/or neurological deficits. When the movement of the vertebrae or bones of the spine are restricted, the animal will not have normal function or flexibility of the spine and/or limbs.\nAnimal chiropractic is a medical therapy that is used to maintain the health and normal functioning of the nervous and musculoskeletal systems. It follows the same principles and practice as chiropractic medicine in human medicine.In general, chiropractic treatments or adjustments correct subluxations.\nA subluxation is defined as a partial dislocation of the joints where the articular surfaces are still in contact with each other, but are misaligned. Subluxations commonly occur in the spine secondary to acute injury, over exercise and chronic orthopedic disease, but can affect other joints as well. Subluxations can cause poor performance, stiffness, pain, decreased flexibility and function of the muscles and nerves.\nWhat is a Chiropractic Adjustment?\nA chiropractic adjustment or spinal manipulation is a specific high velocity controlled thrust on the specific joint that is being manipulated to correct subluxations or misalignment of the spinal column to relieve pain and restore normal range of motion and neurologic function to the area being treated. Chiropractic medicine is designed to stimulate a natural healing response and return the body to homeostasis as quickly as possible without harmful side effect.\nChiropractic adjustments can treat neck pain, back pain, sacroiliac pain, and tail pain that occur secondary to hip and elbow dysplasia, arthritis, disc prolapse, cruciate ligament tears, cancer, and others. Early chiropractic treatment can prevent the development of more serious musculoskeletal conditions in the future.\nProlotherapy, also known as nonsurgical ligament reconstruction, is a medical treatment for the repair of torn or weak tendons and ligaments and for chronic pain.\n“Prolo” is short for proliferation, because the treatment causes the proliferation (growth, formation) of new connective tissue in areas where it has become weak. Clinical results using prolotherapy in dogs and cats appear to indicate the same response. Many elite human athletes use prolotherapy to strengthen their weak ligamentous tissues to prevent against future tears and injury.\nProlotherapy is helpful for many different types of chronic musculoskeletal pain including: chronic osteoarthritis, intervertebral disk disease, chronic back and neck pain, degenerative joint disease, strengthen weak tendon and ligaments and repair torn joint ligaments, particularly the cranial cruciate ligament. Not all animals are candidates for prolotherapy. Each pet is evaluated on an individual case by case basis, and an examination is required to determine if prolotherapy is an appropriate therapy for your pet.\nThe type of laser used at IVC is the Companion Therapy Class IV Laser. Laser therapy has been shown to accelerate the body’s natural healing process through photo-bio-modulation. Laser clinical studies over several decades have shown that laser therapy relieves pain and inflammation, decreases swelling, stimulates nerve regeneration and promotes tissue repair. Therapy Lasers have been scientifically proven and successful in treating post-surgical pain and inflammation and many acute and chronic pain conditions.\nConditions that can be treated with Laser Therapy\n→ Acute Conditions\n→ Cuts/ Bites\n→ Tooth Extraction Pain Relief\n→ Sprains, Strains & Fractures\n→ Post-Surgical Healing / Pain Relief\n→ Chronic Conditions\n→ Degenerative Joint Disease\n→ Inflammatory Bowel Disease\n→ Periodontal Disease\n→ Lick Granulomas\n→ Geriatric Care\n→ Hip Dysplasia\n→ Feline Acne\n→ Anal Sacculitis\n→ Perianal Fistulas\n“Let food be your medicine, and your medicine be your food.”\nHippocrates – The Father of Medicine\nAt the Integrative Veterinary Center we believe that many of our pet’s health problems can be traced to feeding species inappropriate diets. We do not follow or advocate a single dietary program for animals with a “one size fits all” approach. Instead, our specially trained veterinarians can prescribe a nutritional therapy plan that is tailored to your animal’s specific needs.\nOur dietary recommendations are formulated taking into account a variety of factors such as the species, age, breed, weight, work the animals is expected to perform and disease condition. Individual food therapy programs suitable for your lifestyle that range from complete and balanced homemade diets to commercially available pet foods can be formulated for your pet.\nFor more information please visit bensbarketplace.com\nChinese Food Therapy\nChinese food therapy is one of the five branches of Traditional Chinese Medicine (TCM) and is the study of food as therapeutic agents for the preservation of health and the treatment and prevention of disease. For over 2000 years, doctors in China evaluated and recorded the properties of foods and their effects on the body. Our doctors can formulate Chinese food therapy programs to treat your pet’s specific disease condition.\nChinese food therapy follows the same diagnostic and treatment principles as acupuncture and Chinese herbal medicine. Chinese food therapy can be thought of as food used as medicine. Diets are designed to bring the body back into balance and work synergistically with the other TCM modalities.\nFood is considered to have the same energetic actions as herbs (cooling, warming etc) for treating disease and its application is based on similar principles but with a much broader range of applications. Food energetics refers to the effects food has on the digestion, physiological processes and metabolism of the body.\nFood is grouped into 4 basic energetic classifications:\n- Thermal nature (hot, warm, neutral, cool, cold)\n- Flavor (sour, bitter, sweet, pungent, salty)\n- Organ association (a food can affect specific internal organs)\n- Channel affiliation (a food has a definite effect on a particular acupuncture channel more than any others)\nNutritional consultations are 30-45 minutes in duration and can be scheduled at the center or via the telephone by calling 916-454-1825\nConventional (Western) Medicine\nAt IVC all of our doctors are trained in conventional medicine and we provide the same veterinary services as those in a strictly conventional veterinary medical practice.\nThe goal of an integrative approach to veterinary medicine is to provide clients with all the available options for pet care. This is achieved by using conventional medicine, diagnostics and procedures together with all other reasonable systems of medicine (Chinese medicine, chiropractic) or therapies (prolotherapy, gold bead implants) to obtain the best possible results.\nEvery system of medicine has its strong and weak points and each is utilized according to what tool is best needed for a particular animal’s problem. The goal is to try and cure disease and not to suppress symptoms alone. In cases where the animal is at the end of its lifespan or has a terminal condition, the goal is to provide as high a quality of life as possible for that animal.\nWe carry a full line of conventional pharmaceutical drugs and offer the following services:\nIVC offers non-anesthetic dental cleaning through the Animal Dental Care. Non-anesthetic dental cleaning is performed without general anesthesia or sedation using the same cleaning techniques and instruments as dental cleanings performed under general anesthesia. This type of dental is indicated for animals that require routine dental cleaning more often than once per year, those that are sensitive to general anesthesia and for pets whose age and condition will not allow for general anesthesia.\nYour pet will have their mouth examined by one of our veterinarians and then the dental technician with assess the mouth by examining and probing each tooth. Any visible abnormalities of the teeth and or mouth will be recorded on a customized patient dental chart. Most dental cleanings take 20-30 minutes to complete. Due to limited kennel space, we kindly ask that you wait for your pet during this procedure. If your pet has a large amount of tartar or inflamed gums, our doctors may prescribe antibiotics to be given 48 hours before the dental cleaning and continued for at least 5 days after the procedure. They are not required in all patients.\nYour pet’s teeth will be scaled and polished and the oral cavity rinsed. Minor tooth extractions can be performed by our doctors under light sedation and or injectable anesthesia with appropriate monitoring. Major tooth extractions or other dental work such as root canals, fillings, etc, can’t be performed with this dental technique. If major dental work is required you will be referred to have your animal evaluated and treated for dental disease under general anesthesia.\nPost-dental, the doctor and dental technician will make recommendations as to how to keep your pets teeth clean and healthy and how often they will need to be cleaned in the future.\nFor more information about anesthesia-free dental services including an instructional video, please visit: animaldentalcare.info.\nIntravenous Vitamin C\nIntravenous (IV) Vitamin C Therapy involves the administration of Vitamin C directly into the bloodstream. This delivery system is very powerful because it allows the plasma concentration of Vitamin C to reach levels high enough to kill cancer cells.\nIV Vitamin C Therapy is a frequent treatment option for cancer, but it can treat much more than that. Vitamin C Therapy can also treat atopic dermatitis, gingivitis, chronic nasal discharge, chronic upper respiratory infections, chronic infections, autoimmune disease and immune deficiency conditions.\nFor more information on IV Vitamin C go to our Cancer Treatment page.\nWounds, both surgical and non-surgical can be difficult. At IVC we have a number of unique techniques to promote wound healing in addition to antibiotics and surgery.\nWe understand that many people live far away, and the ability to talk to a veterinarian regarding integrative treatment options for your pet is often requested. A phone consultation can be the first step in deciding what direction to pursue for your pets condition. For this reason, we offer phone consultations with Dr. Signe Beebe. This option provides flexibility for individuals that live long distances from our center, those out of state and internationally who are interested in obtaining information on an Integrative and or Traditional Chinese Medicine approach to healthcare for their pets.\nWe can provide dietary recommendations, herbal therapy and other integrative therapies after a review of your pet’s medical records. We may also be able to refer you to a veterinarian closer to your area that practices Integrative and or Chinese veterinary medicine.\nAs no physical examination of the pet was performed during a phone consult, we can’t prescribe any medication. Instead, you will need to follow up with a primary care veterinarian or schedule an examination of your pet with our office so that we may legally prescribe any recommended medications.']	['<urn:uuid:3cbff71c-1a55-478f-a542-273f3594a981>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	5	70	2650
9	Why do athletes take bicarbonate and what affects it?	Athletes take sodium bicarbonate to help offset exercise-induced fatigue caused by metabolic by-products and ion imbalance during high-intensity exercise. The effectiveness of bicarbonate supplementation is influenced by amino acid metabolism, as amino groups from glutamate and aspartate contribute to acid-base balance. Studies show bicarbonate's ergogenic effects are enhanced when combined with other supplements like beta-alanine.	"['BIOC13Winter2013 Lecture 12: Amino acid metabolism and metabolic integration\nAmino acid Metabolism\no Plants and bacteria synthesize all twenty amino acids, humans can only synthesize about half of the twenty amino acids\n(non-essential AAs). Rest must be acquired from dietary sources\no In general, the more complex amino acids are essential amino acids in humans as they require enzymes that have been lost\nfrom the human genome over evolutionary time.\no In animals, proteases present in stomach and intestines cleave the peptide bond to yield amino acids and small\no The carbon skeletons of all twenty amino acids are derived from just seven metabolic intermediates:\no three glycolytic pathway intermediates: 3-phosphoglycerate, phosphoenolypyruvate, and pyruvate\no two pentose phosphate pathway intermediates: ribose 5-phosphate and erythrose 4-phosphate\no two citrate cycle intermediates: -ketoglutarate and oxaloacetate\no Free amino acids in the body can be generated by degradation of cellular proteins which occurs continuously in all cells.\nMost eukaryotic cellular proteins are degraded by one of two pathways:\no an ATP-independent process that degrades proteins inside cellular vesicles called lysosomes\no an ATP-dependent pathway that targets specific proteins for degradation in proteasomes if they contain a polymer of\nubiquitin protein covalently attached to lysine residues.\no In order for proteins to be degraded by the proteasome, they are first ""tagged"" on lysine residues by covalent linkage of\nubiquitin. Ubiquitin is a 76 amino acid protein found in all eukaryotic cells that is specifically attached to proteins by\nubiquitin ligating enzymes.\no The proteins are cleaved by endo and exo-proteases/peptidases to individual amino acids\nAmino acid degradation\no Most AAs are de-aminated first by a process called transamination which basically transfers the amino group to an a-keto\nacid to yield an a-keto acid of the original AA and a new AA\no a-ketoglutarate is the main amino acid acceptor and yields glutamate and a keto acid\no Glutamate’s amino group can be transferred to oxaloacetate in a second transamination reaction, yielding Aspartate and\no Transamination is catalyzed by enzymes called aminotransferases or transaminases which require a coenzyme called\nThe Urea cycle\no Glutamate and glutamine function as the primary nitrogen carriers in most organisms. In mammals, this nitrogen ends up in\nthe liver where it is converted to urea.\no The two nitrogens in urea are derived from the 4H released when glutamate or glutamine are deaminated, and from\naspartate which is formed when oxaloacetate is transaminated by aspartate aminotransferase.\no The carbon atom in urea comes from CO2(HCO 3 that is produced in the mitochondrial matrix by the citrate cycle (the\noxygen atom is derived from 2 O in the final reaction of the cycle).\no Urea synthesis provides an efficient mechanism for land animals to remove excess nitrogen from the body. Urea is\nsynthesized in the liver and exported to the kidneys where it enters the bladder.\nAmino acid biosynthesis\no Arginine is listed as an essential amino acid because humans require arginine in their diet to support rapid growth during\nchildhood and pregnancy, even though it is made by urea cycle.\no Tyrosine is also highlighted because this conditional nonessential amino acid is made in humans from the essential amino\no In general, the structures of the essential amino acids are more complex than the nonessential amino acids which is\nreflected in the number of enzymatic reactions required for synthesis\no Metabolic flux through various amino acid biosynthetic pathways is tightly regulated by feedback inhibition to provide the\nrequired proportions of each amino acid in response to cellular needs.\no amino acids are precursors for a number of biomolecules\no heme comes from glycine and acetate\no purines and pyrimidines\no hormones and NTs synthesized by decarboxylation and hydroxylation of histidine, glutamate, tryptophan and tyrosine\no oxidation of arginine = NO which is signaling molecule in cells\no Numerous diseases are caused by defects in amino acid metabolic pathways; some of these diseases are genetic\ndiseases and can be due to recessive or dominant mutations of key enzymes in the pathways.\n2 o Metabolic homeostasis describes steady-state conditions in the body and can apply to a wide variety of physiological\no These include glucose, lipid, and amino acid levels in the blood, electrolyte concentrations, blood pressure and pulse rate.\no During times of physical activity, psychological stress, or feeding, biochemical processes are altered to counteract the effects\nof these environmental stimuli in an attempt to return the body to metabolic homeostasis.\no Regulation of metabolic homeostasis requires both neuronal signaling from the brain and the release of small molecules\ninto the blood that function as ligands for receptor-mediated cell signaling pathways.\no Disturbances in metabolic homeostasis leads to metabolic diseases such as diabetes.\no Flux through pathways depend on presence of appropriate enzymes and organismal/cell needs liver can carry out all the\nCellular Regulation of Metabolism\no Substrate availability\no Supply and demand\no Allosteric and feedback control\no 5’ AMP kinase (AMPK)\no Intracellular control in response to energy demands\no Hormonal/cytokine regulation\no Multi-organ control\no Response to serum levels of metabolites and metabolic state of body\nMetabolic integration through tissue specialization\no Metabolic pathways are specialized in the tissues\no Brain transports ions to maintain membrane potentials, inegrates inputs from body and surrounding, sends signals to\no Liver processes fats, carbs, proteins ; synthesizes and distributes lipids, ketone bodies and glucose; converts excess N to\no Skeletal muscle uses ATP to do mechanical work\no Cardiac muscle supplies body with blood (i.e oxygen and nutrients)\no Adipose Tissue synthesizs and stores and mobilizes triacylglycerols\nMetabolism in Brain\no Fuel reserves: Very little to none\no Strictly aerobic\no Very high metabolism (consumes 20% of total energy)\no Needs a constant supply of glucose from the blood (120g/day)\no Fuel: Glucose\no Fasting conditions: the brain can use Ketone bodies, but still requires carbohydrates (may also use lactic acid)\no Long chain FAs cannot cross the blood brain barrier because they are bound to the carrier proteins\no Fuel exported: None\nMetabolism in Muscle Cells\no Fuel reserve: glycogen (P-creatine), some FAs\no at rest – aerobic\no vigorous activity – anaerobic\no Preferred Fuel: Fatty acids, glucose when active\no Muscles must be prepared for rapid provision of energy\no Creatine kinase and phosphocreatine act as a buffer system, providing additional ATP for contraction\no Glycogen provides additional energy, releasing glucose for glycolysis\no Glycolysis rapidly lowers pH, causing muscle fatigue\no Fuel exported: Lactate, alanine, glutamine\no Hormones: Insulin, adrenaline\nEffects of exercise\no Short, vigorous (eg 100 m sprint)\no Free ATP P-creatine glycolytic ATP\no blood lactate increases and blood pH decreases. Acidosis causes fatigue.\no Longer duration(eg, 1000 m run)\no aerobic oxidation of muscle glycogen - energy produced at a slower rate.\no Very long periods of exercise (eg. marathon)\no liver glycogen supply – (even slower rate of supply)\n3 o Glycogen stores are insufficient to provide fuel required for marathon (require 150 mols ATP, glycogen ≈ 105 mols).\no Fat reserves – (slowest rate of energy production)\no Ketogenesis (generally only in conditioned athletes)\no Protein breakdown - This is a fuel of last resort for the fasting or exhausted organism\nMetabolism in Cardiac Muscles\no Due to continuous contractions, the cardiac\no muscles rely on aerobic metabolism\no Cells have high number of mitochondria (~40% of the cytoplasmic space)\no Heart can metabolize a number of energy sources: fatty acids, ketone bodies, glucose, pyruvate and lactate\no Under resting conditions fatty acids are the fuel of choice\no Under exercise conditions (increased heart rate), glucose derived from limited glycogen stores in the cells is used as a\nprimary source for the energy.\nMetabolism in Adipose tissue\no Fuel reserve: TAGs, some glycogen\no Metabolism: aerobic\no Preferred fuel: fatty acids, glucose\no Fuel exported: fatty acids, glycerol\no Hormones: insulin, Glucagon, adrenalin\no TAGs may account for as much as 65% of weight of fat cell.\no Receives exogenous TAGs in chylomicron from intestinal system (via lymphatic system and bypass liver)\no High blood glucose - glucose used for FA and TAG synthesis (FFAs from liver)\no Cells require a source of glucose to make TAGs (lacks glycerol kinase)\no Active player in metabolic integration serving as an endocrine organ t', 'Many sports involve repeated bouts of high-intensity exercise. High-intensity exercise is compromised, however, by the early onset of exercise-induced fatigue. Metabolic by-products, ion dysbalance and amount of phosphocreatine are considered the main peripheral causes of fatigue during high-intensity exercise. Intake of nutritional ergogenic aids is commonplace to enhance performance of high-intensity exercise by offsetting the potential mechanisms of fatigue. Creatine, probably one of the best known nutritional aids to enhance performance of high-intensity exercise, has convincingly substantiated its ergogenic potential. Although multi-ingredient supplements are now common, the justification for effectiveness is mostly based on observations with single intake of those ingredients. In this narrative review, the main focus is on the evidence of the effect of co-ingestion of ergogenic aids on performance of high intensity exercise for which the single intake has shown beneficial effects on high-intensity performance.\nThis is a preview of subscription content, access via your institution.\nBuy single article\nInstant access to the full article PDF.\nPrice excludes VAT (USA)\nTax calculation will be finalised during checkout.\nSahlin K. Muscle energetics during explosive activities and potential effects of nutrition and training. Sports Med. 2014;44(Suppl 2):S167–73.\nLanham-New SA, Stear S, Shirreffs S, et al, editors. Sport and exercise nutrition. Chichester: Wiley; 2011.\nEnoka RM, Duchateau J. Muscle fatigue: what, why and how it influences muscle function. J Physiol. 2008;586(1):11–23.\nMaclaren D, Morton JP, editors. Biochemistry for sport and exercise metabolism. Chichester: Wiley; 2011.\nBagchi D, Nair S, Sen CH. Nutrition and enhanced sports performance: muscle building, endurance and strength. New York: Academic; 2013.\nMargolis LM, Pasiakos SM. Optimizing intramuscular adaptations to aerobic exercise: effects of carbohydrate restriction and protein supplementation on mitochondrial biogenesis. Adv Nutr. 2013;4(6):657–64.\nHarris RC, Tallon MJ, Dunnett M, et al. The absorption of orally supplied beta-alanine and its effect on muscle carnosine synthesis in human vastus lateralis. Amino Acids. 2006;30(3):279–89.\nStephens TJ, McKenna MJ, Canny BJ, et al. Effect of sodium bicarbonate on muscle metabolism during intense endurance cycling. Med Sci Sports Exerc. 2002;34(4):614–21.\nHarris RC, Söderlund K, Hultman E. Elevation of creatine in resting and exercised muscle of normal subjects by creatine supplementation. Clin Sci (Lond). 1992;83(3):367–73.\nCrowe MJ, Leicht AS, Spinks WL. Physiological and cognitive responses to caffeine during repeated, high-intensity exercise. Int J Sport Nutr Exerc Metab. 2006;16(5):528–44.\nTobias G, Benatti FB, de Salles Painelli V, et al. Additive effects of beta-alanine and sodium bicarbonate on upper-body intermittent performance. Amino Acids. 2013;45(2):309–17.\nKilding AE, Overton C, Gleave J. Effects of caffeine, sodium bicarbonate, and their combined ingestion on high-intensity cycling performance. Int J Sports Nutr Exerc Metab. 2012;22:175–83.\nNewsholme EA. Basic aspects of metabolic regulation and their application to provision of energy in exercise. In: Poortmans JR, editor. Principles of exercise biochemistry, vol. 38. Basel: Karger; 1993. p. 52–88.\nNewsholme EA. Application of knowledge of metabolic integration to the problem of metabolic limitations in sprints, middle distance, and marathon running. In: Poortmans JR, editor. Principles of exercise biochemistry, vol. 38. Basel: Karger; 1993. p. 230–47.\nHirvonen J, Rehunen S, Rusko H, et al. Breakdown of high energy phosphate compounds and lactate accumulation during short supramaximal exercise. Eur J Appl Physiol. 1987;56(3):253–9.\nHirvonen J, Nummela A, Rusko H, et al. Fatigue and changes of ATP, creatine phosphate, and lactate during 400-m sprint. Can J Sports Sci. 1992;17(2):141–4.\nSahlin K. Metabolic changes limiting muscle performance. In Saltin B, editor. Biochemistry of Exercise: Vol. 6. Champaign; Human Kinetics; 1986. pp 323–43.\nBlomstrand E, Ekblom B, Newsholme EA. Maximum activities of key glycolytic and oxidative enzymes in human muscle from differently trained individuals. J Physiol. 1986;381:111–8.\nGreenhaff PL, Casey A, Short AH, et al. Influence of oral creatine supplementation of muscle torque during repeated bouts of maximal voluntary lifting exercise in man. Clin Sci (Lond). 1993;84(5):565–71.\nLehninger AL, Nelson DL, Cox MM. Principles of Biochemistry. 2nd ed. New York: Worth; 1995.\nMeyer RA, Sweeney HL, Kushmerick MJ. A simple analysis of the “phosphocreatine shuttle”. Am J Physiol. 1984;246(5 Pt 1):C365–77.\nSavabi F, Carpenter CL, Mohan C, et al. The polysome as a terminal for the creatine phosphate energy shuttle. Biochem Med Metab Biol. 1988;40(3):291–8.\nBoobis LH. Metabolic aspects of fatigue during sprinting. In: Macleod R, Maughan M, Nimmo M, Reilly T, et al., editors. Exercise: Benefits, Limitations, and Adaptations. London: E&FN spon; 1987. p. 116–43.\nHultman E, Bergstrom M, Spriet LL, et al. Energy metabolism and fatigue. In: Taylor AW, Gollnick PD, Green HJ, et al., editors. Biochemistry of Exercise, vol. 7. Champaign, Il: Human Kinetics; 1990. p. 73–92.\nHultman E, Greenhaff PL. Skeletal muscle energy metabolism and fatigue during intense exercise in man. Sci Prog. 1991;75(298 Pt 3–4):361–70.\nStorey KB, Hochachka PW. Activation of muscle glycolysis: a role of creatine phosphate in phosphofructokinase regulation. FEBS Lett. 1974;46(1):337–9.\nCheetham ME, Boobis LH, Brooks S, et al. Human muscle metabolism during sprint running. J Appl Physiol. 1986;61(1):54–60.\nMcCartney N, Spriet LL, Heigenhauser GJ, et al. Muscle power and metabolism in maximal intermittent exercise. J Appl Physiol. 1986;60:1164–9.\nSpriet LL, Söderlund K, Bergstrom M, et al. Anaerobic energy release in skeletal muscle during electrical stimulation in men. J Appl Physiol. 1987;62:611–5.\nSöderlund K, Hultman E. ATP and phosphocreatine changes in single human muscle fibers after intense electrical stimulation. Am J Physiol. 1991;261(6 Pt 1):E737–41.\nHultman E, Bergström J, Anderson NM. Breakdown and resynthesis of phosphorylcreatine and adenosine triphosphate in connection with muscular work in man. Scand J Clin Lab Invest. 1967;19(1):56–66.\nInfante AA, Klaupiks D, Davies RE. Phosphorylcreatine consumption during single working contraction of isolated muscle. Biochim Biophys Acta. 1965;94:504–15.\nSpande JI, Schottelius BA. Chemical basis of fatigue in isolated mouse soleus muscle. Am J Physiol. 1970;219(5):1490–5.\nEdström L, Hultman E, Sahlin K, et al. The contents of high-energy phosphates in different fiber types in skeletal muscles from rat, guinea-pig and man. J Physiol. 1982;332:47–58.\nGreenhaff PL, Nevill ME, Söderlund K, et al. The metabolic responses of human type I and II muscle fibres during maximal treadmill sprinting. J Physiol. 1994;478:149–55.\nSöderlund K, Greenhaff P, Hultman E. Energy metabolism in type I and type II human muscle fibers during short term electrical stimulation at different frequencies. Acta Physiol Scand. 1992;144(1):15–22.\nTesch PA, Thorsson A, Fujitsuka N. Creatine phosphate in fiber types of skeletal muscle before and after exhaustive exercise. J Appl Physiol (1985). 1989;66(4):1756–9.\nBalsom PD, Seger JY, Sjödin B, et al. Maximal intensity intermittent exercise: effect of recovery duration. Int J Sports Med. 1992;13(7):528–33.\nBalsom PD, Seger JY, Sjödin B, et al. Physiological responses to maximal intensity intermittent exercise. Eur J Appl Physiol. 1992;65(2):144–9.\nBogdanis GC, Nevill ME, Lakomy HKA, et al. Human muscle metabolism during repeated maximal sprint cycling. J Physiol. 1993;467:77P.\nGreenhaff PL, Hultman E, Harris RC. In: Poortmans JR (editor). Carbohydrate metabolism. Medicine and Sport Science Vol 46. Principles of Exercise Biochemistry. Basel: Karger 2004. pp. 101–51\nDonaldson SKB, Hermansen L, Bolles L. Differential direct effects of H+ on Ca2+- activated force of skinned fiber from the soleus, cardiac, and adductor magnus muscles of rabbits. Pflüg Archiv. 1978;376(1):55–65.\nSahlin K, Edström L, Sjöholm H. Fatigue and phosphocreatine depletion during carbon dioxide induced acidosis in rat muscle. Am J Physiol. 1983;245(1):C15–20.\nDanforth WH. Activation of the glycolytic pathway in muscle. In: Chance B, Estabrook RW, editors. Control of energy metabolism. New York: Academic; 1965. p. 287–98.\nGoldstein ER, Ziegenfuss T, Kalman D, et al. International society of sports nutrition position stand: caffeine and performance. J Int Soc Sports Nutr. 2010;7(1):5.\nStuart GR, Hopkins WG, Cook C, et al. Multiple effects of caffeine on simulated high-intensity team-sport performance. Med Sci Sports Exerc. 2005;37(11):1998–2005.\nCarr AJ, Hopkins WG, Gore CJ. Effects of acute alkalosis and acidosis on performance: a meta-analysis. Sports Med. 2011;41:801–14.\nChristensen PM, Petersen NH, Friis SN, et al. Caffeine, but not bicarbonate, improves 6 min maximal performance in elite rowers. Appl Physiol Nutr Metab. 2014;39(9):1058–63.\nJuel C. Regulation of pH in human skeletal muscle: adaptations to physical activity. Acta Physiol (Oxf). 2008;193(1):17–24.\nPruscino LC, Ross MLR, Gregory JR, et al. Effects of sodium bicarbonate, caffeine, and their combination on repeated 200-m freestyle performance. Int J Sports Nutr Exerc Metab. 2008;18(2):116–30.\nCarr AJ, Christopher JG, Dawson B. Induced alkalosis and caffeine supplementation: effects on 2,000-m rowing performance. Int J Sports Nutr Exerc Metab. 2011;21(5):357–64.\nFelippe LC, Lopes-Silva JP, Bertuzzi R, et al. Separate and combined effects of caffeine and sodium bicarbonate intake on judo performance. Int J Sports Physiol Perform. 2016;11(2):221–6.\nMarriott M, Krustrup P, Mohr M. Ergogenic effects of caffeine and sodium bicarbonate supplementation on intermittent exercise performance preceded by intense arm cranking exercise. J Int Soc Sports Nutr. 2015;12:13.\nDanaher J, Gerber T, Wellard RM, et al. The effect of β-alanine and NaHCO3 co-ingestion on buffering capacity and exercise performance with high-intensity exercise in healthy males. Eur J Appl Physiol. 2014;114(8):1715–24.\nBellinger PM, Howe S, Shing C, et al. The effect of combined β-alanine and NaHCO3 supplementation on cycling performance. Med Sci Sports Exerc. 2012;44(8):1545–51.\nDe Salles Painelli V, Roschel H, De Jesus F, et al. The ergogenic effect of beta-alanine combined with sodium bicarbonate on high-intensity swimming performance. Appl Physiol Nutr Metab. 2013;38(5):525–32.\nDucker KJ, Dawson B, Wallman KE. Effect of β-alanine and sodium bicarbonate supplementation on repeated-sprint performance. J Strength Cond Res. 2013;27(12):3450–60.\nMero AA, Hirvonen P, Saarela J, et al. Effect of sodium bicarbonate and beta-alanine supplementation on maximal sprint swimming. J Int Soc Sports Nut. 2013;10:52.\nBellinger PM. β-alanine supplementation for athletic performance: an update. J Strength Cond Res. 2014;28(6):1751–70.\nHobson RM, Harris RC, Martin D, et al. Effect of β-Alanine with and without sodium bicarbonate, on 2000 m rowing performance. Int J Sport Nutr Exerc Metab. 2013;23(5):480–7.\nSale C, Saunders B, Hudson S, et al. Effect of β-alanine plus sodium bicarbonate on high-intensity cycling capacity. Med Sci Sport Exerc. 2011;43(10):1972–8.\nBrosnan JT, da Silva RP, Brosnan ME. The metabolic burden of creatine synthesis. Amino Acids. 2011;40:1325–31.\nVolek JS, Duncan ND, Mazzetti SA, et al. Performance and muscle fiber adaptations to creatine supplementation and heavy resistance training. Med Sci Sports Exerc. 1999;31(8):1147–56.\nVandenberghe K, Gillis N, Van Leemputte M, et al. Caffeine counteracts the ergogenic action of muscle creatine loading. J Appl Physiol. 1996;80(2):452–7.\nVanakoski J, Kosunen V, Meririnne E, et al. Creatine and caffeine in anaerobic and aerobic exercise: effects on physical performance and pharmacokinetic considerations. Int J Clin Pharmacol Ther. 1998;36:258–62.\nHespel P, Op’t Eijnde B, Van Leemputte M. Opposite actions of caffeine and creatine on muscle relaxation time in humans. J Appl Physiol. 2002;92(2):513–8\nHarris RC, Sale C, Delves SK. Modification of the ergogenic effects of creatine loading by caffeine. Med Sci Sports Exerc. 2005;37:S348–S349\nDoherty M, Smith PM, Davison RC, et al. Caffeine is ergogenic after supplementation of oral creatine monohydrate. Med Sci Sports Exerc. 2002;34(11):1785–92.\nLee CL, Lin JC, Cheng CF. Effect of caffeine ingestion after creatine supplementation on intermittent high-intensity sprint performance. Eur J Appl Physiol. 2011;111(8):1669–77.\nTrexler ET, Smith-Ryan AE, Roelofs EJ, et al. Effects of coffee and caffeine anhydrous intake during creatine loading. J Strength Cond Res. (in press).\nTrexler ET, Smith-Ryan AE. Creatine and caffeine: considerations for concurrent supplementation. Int J Sport Nutr Exerc Metab. (in press).\nSnow RJ, McKenna MJ, Selig SE, et al. Effect of creatine supplementation on sprint exercise performance and muscle metabolism. J Appl Physiol (1985). 1998;84(5):1667–73.\nBarber JJ, McDermott AY, McGaughey KJ, et al. Effects of combined creatine and sodium bicarbonate supplementation on repeated sprint performance in trained men. J Strength Cond Res. 2013;27(1):252–8.\nMero AA, Keskinen KL, Malvela MT, et al. Combined creatine and sodium bicarbonate supplementation enhances interval swimming. J Strength Cond Res. 2004;18(2):306–10.\nGriffen C, Rogerson D, Ranchordas M, et al. Effects of creatine and sodium bicarbonate co-ingestion on multiple indices of mechanical power output during repeated Wingate test in trained men. Int J Sport Nutr Exerc Metab. 2015;25(3):298–306.\nBuford TW, Kreider RB, Stout JR, et al. International society of sports nutrition position stand: creatine supplementation and exercise. J Int Soc Sports Nutr. 2007;4:6.\nEarnest CP, Snell PG, Rodriguez R, et al. The effect of creatine monohydrate ingestion on anaerobic power indices, muscular strength and body composition. Acta Physiol Scand. 1995;153(2):207–9.\nDangott B, Schultz E, Mozdziak PE. Dietary creatine monohydrate supplementation increases satellite cell mitotic activity during compensatory hypertrophy. Int J Sports Med. 2000;21(1):13–6.\nBlancquaert L, Everaert I, Derave W. Beta-alanine supplementation, muscle carnosine and exercise performance. Curr Opin Clin Nutr Metab Care. 2015;18(1):63–70.\nDerave W, Ozdemir MS, Harris RC, et al. beta-Alanine supplementation augments muscle carnosine content and attenuates fatigue during repeated isokinetic contraction bouts in trained sprinters. J Appl Physiol. 2007;103(5):1736–43.\nHoffman JR, Landau G, Stout JR, et al. beta-Alanine ingestion increases muscle carnosine content and combat specific performance in soldiers. Amino Acids. 2015;47(3):627–36.\nArtioli GG, Gualano B, Smith A, et al. Role of beta-alanine supplementation on muscle carnosine and exercise performance. Med Sci Sports Exerc. 2010;42(6):1162–73.\nSmith AE, Walter AA, Graef JL, et al. Effects of beta-alanine supplementation and high-intensity interval training on endurance performance and body composition in men; a double-blind trial. J Int Soc Sports Nutr. 2009;6:5.\nHoffman J, Ratamess N, Kang J, et al. Effect of creatine and beta-alanine supplementation on performance and endocrine responses in strength/power athletes. Int J Sport Nutr Exerc Metab. 2006;16(4):430–46.\nKresta JY, Oliver JM, Jagim AR, et al. Effects of 28 days of beta-alanine and creatine supplementation on muscle carnosine, body composition and exercise performance in recreationally active females. J Int Soc Sports Nutr. 2014;11(1):55.\nLowery RP, Joy JM, Dudeck JE, et al. Effects of 8 weeks of Xpand(R) 2X pre workout supplementation on skeletal muscle hypertrophy, lean body mass, and strength in resistance trained males. J Int Soc Sports Nutr. 2013;10(1):44.\nSpradley BD, Crowley KR, Tai CY, et al. Ingesting a pre-workout supplement containing caffeine, B-vitamins, amino acids, creatine, and beta-alanine before exercise delays fatigue while improving reaction time and muscular endurance. Nutr Metab (Lond). 2012;9:28.\nStout JR, Cramer JT, Mielke M, et al. Effects of twenty-eight days of beta-alanine and creatine monohydrate supplementation on the physical working capacity at neuromuscular fatigue threshold. J Strength Cond Res. 2006;20(4):928–31.\nZoeller RF, Stout JR, O’Kroy JA, et al. Effects of 28 days of beta-alanine and creatine monohydrate supplementation on aerobic power, ventilatory and lactate thresholds, and time to exhaustion. Amino Acids. 2007;33(3):505–10.\nGreen JM, McLester JR, Smith JE, et al. The effects of creatine supplementation on repeated upper- and lower-body Wingate performance. J Strength Cond Res. 2001;15(1):36–41.\nHobson RM, Saunders B, Ball G, et al. Effects of beta-alanine supplementation on exercise performance: a meta-analysis. Amino Acids. 2012;43(1):25–37.\nNo sources of funding were used to assist in the preparation of this article.\nConflict of interest\nConrad Earnest is the Director of Research for Nutrabolt International and a research scientist at Texas A&M University. Alireza Naderi, Ryan Lowery, Jacob Wilson and Mark Willems declare that they have no conflicts of interest relevant to the content of this review.\nRights and permissions\nAbout this article\nCite this article\nNaderi, A., Earnest, C.P., Lowery, R.P. et al. Co-ingestion of Nutritional Ergogenic Aids and High-Intensity Exercise Performance. Sports Med 46, 1407–1418 (2016). https://doi.org/10.1007/s40279-016-0525-x\n- Creatine Supplementation\n- Ergogenic Effect\n- Creatine Monohydrate']"	['<urn:uuid:da229e2d-11f7-43e0-8209-9bbea5aa44e1>', '<urn:uuid:483cf07a-fe42-4b97-84f8-f913efe2a1d5>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:57:18.859842	9	55	3877
10	rural water supply challenges implementation solutions worldwide	Rural water supply faces several key challenges and solutions worldwide. The challenges include higher per capita construction costs due to scattered populations, institutional vacuums in system management, and poor fee collection. Solutions have emerged through various approaches: In Vietnam, Rural Water Supply and Sanitation Enterprises were established with local government and community ownership. In Senegal, Water Users' Associations (ASUFORs) manage systems through private operators with institutional oversight. In Haiti, professional management models involving local entrepreneurial water operators were introduced. The Rural Water Supply Network (RWSN), with 10,000 members, serves as a global platform for sharing expertise and scaling up innovation, helping achieve universal safe water access in rural areas through knowledge exchange and technical guidance.	"[""Rural Water Supply Network (Core contribution)\nUniversal access to safe, affordable water is at the core of the Sustainable Development Goals (SDGs), and the realisation of human rights. Achieving this in rural areas presents some particular challenges linked to poverty, discrimination and poor infrastructure. The Rural Water Supply Network (RWSN) is the only global network to offer powerful, effective mechanisms to share expertise and collaborate with local and global partners to trial and scale up innovation and to raise the quality and professionalism in rural water supplies.\n- SKAT Foundation\n|Background||Major progress was achieved during the Millennium Development Goal (MDG) period in improving access to water in rural areas. The challenge for the SDGs will be not only to increase access to basic water services to reach the unserved, but also to raise service levels by improving the availability, accessibility and quality of the water provided, particularly in rural areas, and sustain existing and future water services.|\n|Objectives||All rural people have access to sustainable and reliable water supplies which can be effectively managed to provide sufficient, affordable and safe water within a reasonable distance of the home.|\nThe ultimate target groups are the populations that suffer from insufficient, poor water supply in rural areas, in particular those living in water stressed areas.\nThe direct target groups are (i) practitioners and professionals and (ii) organisations that are actively engaged in improving rural water supplies, or have a strong interest in the subject, from different stakeholder groups including national and local government, international organisations, the private sector, civil society organisations, donors, research and training institutions and water users.\nOutcome 1: As a dynamic and trusted network, RWSN grows and evolves to meet the demands of its members and makes coherent contribution of knowledge exchange and learning to support the achievement of universal safe water access in rural areas worldwide.\nOutcome 2: Women and Young Professionals\nExpected results: Flagship outputs include:\nResults from previous phases:\nRWSN as a network has shown dynamic growth over the past years, both in terms of members (now reaching 10’000) and in term of scope. It is considered the leading network for the rural water sector allowing key actors to convene, exchange information, and advance strategic discussions. It is a trusted source of technical and strategic guidance with over 25,000 document downloads per year. From 2015 to 2017, RWSN hosted 68 webinars, in English, French and Spanish, with 2,437 participants and 169 different speakers from across all global regions.\nAn external evaluation of the network over the period 2012-2017 found that RWSN is ‘a highly competent, advanced network which disseminates valuable knowledge to its members, which can be applied to practice’. To name a few, previous activities have led to: (i) The 2014 United Nations Handbook on human rights to water and sanitation being mainstreamed into the working practices of key WASH organisations; (ii) on Guidance note on cost-effective boreholes developed and institutionalised in countries and organisations in partnership with United Nations Children’s Fund (UNICEF) and (iii) convening a reflection and call to action on designing and institutionalising sustainability indicators for rural water services in partnership with key actors in the sector.\n|Directorate/federal office responsible||\nSwiss Non-profit Organisation\nSkat Foundation (host organisation of RWSN) – see Annex 3\n|Coordination with other projects and actors||RWSN is well aligned with the overall SDC’s strategy of policy influencing, innovation and knowledge management as well as leveraging partnership. RWSN collaborations with the SDC portfolio are detailed in Annex 10.|\n|Budget||Current phase Swiss budget CHF 750'000 Swiss disbursement to date CHF 450'000|\n|Project phases||Phase 99 01.02.2018 - 31.12.2030 (Current phase) Phase 1 15.03.2018 - 31.12.2020 (Current phase)|"", ""With 70% of the world's extreme poor living in rural areas , and improved water access still lacking for close to 768 million  people around the world, investing in safe and sustainable drinking water for rural populations is important to our goal of eradicating extreme poverty within our generation.\nWhen compared to urban water supply, rural areas present a different set of challenges:\nOften, the cost per capita of constructing water systems is higher in rural than in urban areas, due to a smaller population which is scattered over a large area. This, in turn, leads to high operating costs, to be recovered by fewer users.\nMost importantly, there may not always be an obvious institution to take the responsibility of managing and operating the system after construction. This institutional vacuum leads to poor collection of water fees, and ultimately to poor operation and maintenance of the rural water systems.\nKeeping rural water systems working is perhaps even more important than building them . Leaving it up to the community to manage complex rural water systems has often been used to remedy the institutional challenge of operating and maintaining rural water systems, albeit with mitigated results . While there are no silver institutional bullets, three approaches from recent World Bank projects used innovative concepts to establish sustainable rural water supply:\n1. In the Red River Delta Rural Water Supply and Sanitation  project in Vietnam, four provinces piloted the establishment of Rural Water Supply and Sanitation Enterprises to manage the schemes built under the project. The Enterprises are accountable to, and part-owned by, local government and the local communities served by the schemes. The pilot aimed at recovering direct operating costs within the first 3 years of operation in order to guarantee reliable supply beyond the life-cycle of the project. This necessitated strong support from the local communities and the local government, and willingness for both to commit to higher tariffs for water. Already in its first year, the Enterprises are able to recover the direct costs of producing water and are on track to start repaying the investment loan in the next 3 years. The same project also successfully scaled up a revolving fund for low-income families to improve household sanitation , managed by the Women’s Union, which effectively combined microfinance, demand generation for sanitation and hygiene promotion activities.\n2. In Ndieng Diaw in Senegal, a water supply scheme servicing over 100 villages (initially funded by Saudi Arabia) is managed by a Water Users’ Association , or ASUFOR in French. ASUFOR, upon obtaining a license from the Directorate of Operations and Maintenance, signs a management contract with a private operator for daily operations of the network, and sets water tariffs. The Directorate of Operations and Maintenance is responsible for overall Exploitation and maintenance of assets, validation of technical specifications of pumping systems, rehabilitation, renewal and extension of pumping systems, and oversight of the ASUFORs.\nThis system of institutional checks and balances has led to a significant increase in coverage, production and population served since the project started in 2004; community participation is crucial to affordable tariff-setting, though network extension remains a challenge as it is not covered by the tariff and funds come from central government. Strong capacity building for planning of maintenance, rehabilitation and extension of network, strong regulation, oversight and control of management contracts; and a sound asset management policy and investment planning are 3 key ingredients to ensuring the sustainability of this model.\n3. The Rural Water Supply and Sanitation Project in Haiti  introduced a professional management model involving local entrepreneurial water operators selected by the community to operate, maintain and manage the water supply systems, based on a contract signed with the communities. Professional operators were trained to manage piped water connections and water kiosks built under the project, including billing, expansion of network and daily operations, and they are supported at the local government level by the National Water Directorate’s (DINEPA) decentralized technical Rural Units. The Project promoted cost recovery, metering, and the use of household connections and water kiosks, as well as gravity-fed piped systems, and where pumping was necessary and cost-effective, the project gave a strong preference to renewable energy, such as solar. Most professional operators are currently able to cover their operating costs with the current tariff structure and some are able to generate net benefits. While the delegated professional management model for water systems was lengthy to implement and still needs to be refined, the DINEPA has received requests from communities outside the project area who have heard about the model.\nWhat is your experience in sustainable management of rural water supply? What do you think is essential to sustainable rural water management? Please share in the comments’ section below!\nWith special thanks to Hoa Thi Hoang, Parameswaran Iyer, Jean-Martin Brault, Sylvestre Bea, Miguel Vargas-Ramirez and Lilian Pena Peireira Weiss for their contributions to this blog.""]"	['<urn:uuid:bfbe1da2-c135-4d19-9c71-5465fa734c8c>', '<urn:uuid:6b3ba589-6b70-47a3-8624-a9eb500430e5>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:57:18.859842	7	115	1419
11	chlorophyll photosynthesis function light intensity effects	Chlorophyll catalyzes the conversion of solar energy to chemical energy via photosynthesis, converting light energy, CO2 and water into chemical energy. Regarding light intensity effects, while only about one percent of light energy is converted into potential chemical energy, very low intensity reduces photosynthesis and may cause stomata closure, reducing plant growth. High light intensity can be detrimental as it increases respiration rate and can cause solarization where cell contents are oxidized by atmospheric oxygen.	['Chlorophyll Biosynthesis and Technological Applications\nISBN/ASIN: 9789400771338,9789400771345 | 2014 | English | pdf | 469/495 pages | 5.84 Mb\nPublisher: Springer Netherlands | Author: Constantin A. Rebeiz (auth.) | Edition: 1\nHeme and chlorophyll (Chl) are porphyrins. Porphyrins (also referred to as tetrapyrroles) are essential for life in the biosphere. Chlorophyll catalyzes the conversion of solar energy to chemical energy via the process of photosynthesis. Organic life in the biosphere is made possible by consumption of the chemical energy generated by photosynthesis. Hemes are the prosthetic groups of cytochromes which are involved in electron transport during oxidative phosphorylation and photosynthetic phosphorylation which generate ATP and NADPH. The latter are essential for many cellular functions. Chlorophyll on the other hands catalyzes the process of photosynthesis. Indeed, life in the biosphere depends on the process of photosynthesis which converts light energy, carbon dioxide and water into the chemical energy, required for the formation of food and fiber. Photosynthetic efficiency is controlled by extrinsic factors such as the availability of water, CO2, inorganic nutrients, ambient temperature and the metabolic and developmental state of the plant, as well as by intrinsic factors (Lien and San Pietro, 1975). The most important intrinsic factor is the efficiency of the photosynthetic electron transport system (PETS). Conventional agriculture is one of the few human activities that have not undergone a revolution to join other activities such as overcoming gravity by flying, and landing on the moon, crossing underwater the polar cap, and communicating wirelessly over long distances via electromagnetic waves. We now feel that enough biochemical and molecular biological knowledge has accumulated to render this dream amenable to experimentation. We believe that the time has come to bioengineer chloroplasts capable of synthesizing a short chain carbohydrate such as glycerol at rates that approach the upper theoretical limits of photosynthesis [Rebeiz, C. A. (2010) Investigations of possible relationships between the chlorophyll biosynthetic pathway and the assembly of chlorophyll-protein complexes and photosynthetic efficiency. In: Rebeiz, C. A. Benning, C., Bohnert, H.J., Daniell, H., Hoober J. K., Lichtenthaler, H. K., Portis , A. R., and Tripathy, B. C. eds. The chloroplast: Basics and Applications. Springer. The Netherlands, p 1-24]. In order to achieve this goal a thorough knowledge of the Chl biosynthetic pathway is needed along with knowledge in other domains (Rebeiz 2010). In this context, this monograph is devoted to an in depth discussion of our present knowledge of the Chl biosynthetic pathway. The complexity and biochemical heterogeneity of the Chl biosynthetic pathway and the relationship of this complexity to the structural and biosynthetic complexity of photosynthetic membranes will be emphasized. We will also emphasize in historical perspective, key stages in our understanding of the Chl biosynthetic heterogeneity. The reader should keep in mind that a complex biosynthetic process is only fully understood when it becomes possible to reconstitute in vitro every step of the process. We are not yet at this stage of understanding of thylakoid membrane biogenesis. Considerable progress has been achieved however, in the understanding of numerous facets of the Chl biosynthetic pathway, namely (a) detection and identification of various major and minor metabolic intermediates (b) precursor-product relationships between various intermediates, (c) structure and regulation of many enzymes of the pathway, and (d) the relationship of the Chl biosynthetic heterogeneity to the structural and functional heterogeneity of thylakoid membranes. In addition topics related to the development of Analytical techniques, Cell-free systems, Herbicides, Insecticides, and Cancericides are also discussed.', 'EFFECT OF RADIATION ON CROP PRODUCTION\nSolar energy provides two essential needs to plants-\na. light, required for photosynthesis and for many other functions of the plant- including seed germination, leaf expansion, growth of stem and shoot, flowering, fruiting and even dormancy, and\nb. thermal conditions required for the normal physiological functions of the plant.\nLight is indispensable for the synthesis of the most important pigment of the plant, i.e. the green chlorophyll. The chlorophyll is capable of absorbing radiant energy and converting it into potential chemical energy of carbohydrates.\n6 CO2 + 12 H2O\nCCH12O6 + 6 H2O + 6O2\nThe carbohydrates manufactured by the plants during photosynthesis is the only link between the solar energy and the living world.\nLight affects the plants in four ways :\na. Light intensity\nb. Light quality\nc. Light duration\nd. Light direction\na. Light intensity :\nAlthough about one percent of the light energy is converted into potential chemical energy, very low intensity reduces photosynthesis, and may even results in the closing of the somata, and consequently reduced vegetative growth of the plants. Very high light intensity is detrimental to plants in many ways. It increases the rate of respiration and thus disrupts the photosynthesis-respiration balance. The most harmful effect of high light intensity is the phenomenon of solarization in which all the cell contents are oxidized by atmospheric oxygen. However, the rate of photosynthesis is directly proportional to light intensity up to certain limit.\nFig 1. Effect of increasing light intensity on the rate of photosynthesis is expressed in terms of rate of oxygen evaluation.\nPlants have been divided into sun (heliophytes) and shade (sciophytes) loving species on the basis of their marked differences in the saturation high intensity as shown in Fig.2. At a certain light intensity the amount of carbon dioxide used in photosynthesis and amount of carbon dioxide produced in respiration are volumetrically equal. This point is known as compensation point.\n2. Light Quality :\nFor photosynthesis the visible of spectrum (between 350 to 750 mm) is essential. The maximum photosynthesis is shown to occur in the red part of the spectrum with the next peak in blue part and minimum in the green region. The region between 570 to 750 mm (yellow to red) is quite congenial for photosynthesis (Fig.3.) The red light promotes germination and far-red light inhibits germination. The pigment (phytochrome) occurs in two reversible forms-Per and Prf which develop under red and far-red light. The germination depends upon the balance between two forms:\nThere are three broad classes of germination response which are :\n- Seed germination promoted by white light e.g. Tobacco, Latuce\n- Seed germination inhibited by white light e.g. Allium Sp. and\n- Seed germination not influenced by white light e.g. Rice, Wheat.\n3. Light duration :\nDuration of light has consideration importance from the farmers point of view in selecting the crop species or variety of a crop, as it in fluencies the flowering of crops. The response of plants to the relative lengths, and alternations of light and dark periods with regard to the initiation of flowering is called photoperiodism.\nBased on flowering in response to different photoperiod, Garner and Allard divided plants into three groups.\na. Long Day Plants :\nPlants which flower in response to days longer than critical day lengths. In other word, long day plants flower when the dark period is shorter than the critical dark period.\nExamples: Gram, Barley, Wheat, Pea, Sugar beet.\n4. Direct of Light :\nShoots, roots and leaves show different orientation to the direction of light. Shoots elongate towards the source of light (positively phototrophic) while roots towards soil (negative phototrophic). Again, in temperate regions, the southern slopes show better growth of crops than the northern slops due to direction of light.']	['<urn:uuid:67ab998f-1b43-46b6-b03b-2ef46a58549f>', '<urn:uuid:24a63335-6562-4892-bd08-8a6131c54f75>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	6	75	1204
12	hunting weapon training requirements different game	Different kinds of animals require different weapons and ammunition. For hunting, it's essential to take weapons training courses to learn about guns. You wouldn't use a shotgun meant for ducks to hunt a bear, as this would be dangerous. It's recommended to take classes to learn proper shooting, talk to other hunters about their preferred guns and ammunition, and practice extensively, especially if using a bow. Understanding both your prey and weapon capabilities is crucial.	['10 Tips Guaranteed to Turn a Beginner Hunter into an All-Star Hunter\nNew to hunting? Then you’ve come to the right place. Every type of game and terrain requires a different strategy, and a different set of skills. There is an awful lot of knowledge and advice out there about this sport, and plenty of people who love to hunt. You can improve you hunting abilities, and learn how to stay safe, by joining a hunting club (just look online), making friends with hunters, and learning from the seasoned pros. With that in mind, here are 10 general tips to help you get the most out of your time in the wild.\nKnow Your Weapon and Your Game\nDifferent kinds of animals require different kinds of weapons and ammunition. If you are new to hunting, one of the best ways to learn about guns is to take a weapons training course. You wouldn’t hunt a bear with a shotgun meant for ducks, loaded with a #2 shot, just like you wouldn’t want to go after a large bear with a scatter gun. That would only make the bear very mad and ruin your day, if not your life. Take a class and learn how to shoot. Talk to other hunters and learn what kind of gun and ammunition they prefer, how they pattern their guns, and any tricks they are willing to share. If you plan on using a bow, then practice and practice some more. Know your prey, your weapon, and what you can and can’t do with both.\nGet a Good Knife for Field Dressing Your Kill\nAssuming you have landed your mark and taken an animal down, you now have to deal with what’s left. A good field knife, and knowing how to use it, is paramount. This is even truer for bigger game. You’ll want to drain the blood from the animal as quickly as possible, to keep bacteria at bay, and make the game easier to transport. You’ll have to decide if you want a fixed or folding baled. A good quality blade, around three-to-four inches in length, will require less maintenance, and keep its edge longer. If you intend to use your knife for breaking large bones (instead of a small hatchet), you’ll want a very durable blade as well. A comfortable knife grip with finger grooves will let you hold onto the blade in the rain, and when slippery blood from the animal you’re dressing runs down the handle. Some knives come with gutting hooks, which can be convenient, but can also become a little cumbersome while working inside the animal. A small and separate gutting hook is a good item to carry along with you, in addition to a solid knife.\nAlways Get an Early Start\nA lot of game tends to be active in the early morning and at dusk. Hunters who get out into the forest, the marshes, or wherever they happen to be ranging early, will reap all kinds of benefits. Yes, it’s hard to get up before dawn for some people, but if you do, you’ll be one of the first hunters on the spot. You’ll also have plenty of daylight ahead of you, in case your luck isn’t that great, and you need some more time to bag an animal. There will be time for a longer rest in the afternoon, as well. By getting up early, you’re giving yourself the entire day, which can make all the difference in a successful hunt. Of course weather, and specific animal behavioral patterns also play into the time of day you’ll most likely find some prey, but an early start is usually advisable, although opinions do vary some.\nMask Your Scent and Watch the Wind\nMost animals smell a heck of a lot better than people do. There are several ways your can mask you scent, so your prey doesn’t smell you coming from miles away. Before you even venture out into the wild, make sure you use a scent-free soap and shampoo when you shower. Don’t put on cologne (not even a drop), or any scented deodorant. Scent-control clothing, which makes use of carbon activated by heat, is also another way to mask you smell. Fields sprays can eliminate the ‘human’ scent that tends to scare off animals. Be sure to spray your clothes and your gear before you set out on a hunt.\nStop the Chatter\nBe as still and as quiet as possible. While hunting can be a wonderful social activity, and a great way to bond with your friends, you need to keep fairly silent while in the forest, meadow or other natural areas where your game resides. Lots of movement and chatter will scare away your prey, or at the very least alert them to your presence. You can swap stories with your buddies in the bar later, after a good day of hunting.\nShot Placement and Vital Zones\nDifferent animals have different anatomy. After you’ve practiced with your weapon, and have improved your targeting, you need to know what parts of the animal to aim for in order to bring about a clean and successful kill. Wounding game and not bringing it down is a terrible outcome. You’ll feel bad about it, not to mention how the suffering animal will feel. Study charts and the vital zones of the prey you’re planning to go after, and know the physical structure of the animal intimately, which is also important when dressing your kill.\nThe Weather and the Time of Day\nOne of our tips was to get an early start. While this is true in many cases, this wouldn’t make sense, of course, if you happen to be hunting a nocturnal animal. As the seasons turn, animals take advantage of the change in weather, just like people do, and alter their feeding habits. Moonlit nights, rain, baking sun, snow and other weather patterns affect animal (and human) behavior. There are too many variables to list here, but weather and time of day are things you’ll need to take into account when planning a hunt. When in doubt, ask the local hunters about their opinion on the matter. They’ll know more about the intricacies of the flora and fauna than you.\nLearn About Various Animals Calls and State Regulations\nSometimes you can bring the prey to you. Hunting calls are the sounds that you make to attract animals closer to your vicinity. If you have a talent for voices and odd sounds, you might be able to learn how to make some hunting calls with your mouth, hands, a reed or a special mouthpiece. There are also devices you can buy that mimic various animal sounds and mating calls. Turkeys and other birds are famous for responding to calls, if the calls are done just right. There are also digitally recorded calls you can buy and use, although many states prohibit the use of electronic hunting devices. Best check up on the local regulations before you make use of electronic hunting calls. Many hunters believe prerecorded calls are unsportsmanlike. You’ve got to give the animals a fighting chance.\nRead Up On Game, Equipment, and Trends\nThe hunting community is amazingly active, online and in your town, putting on conferences, seminars, training courses and other events. New technology is always becoming available, and hunting techniques and rules, as well as various hunting licenses, are often in flux, or at least up for some debate. Talk to as many people as you can, subscribe to news groups and head out to the shooting range and practice. Chat with folks about game, weather conditions and what’s working for them and what’s not. You’ll learn more that way than by staying home and reading guides and how-to books. It doesn’t matter if you’re wrangling fish, or after a big, fat moose. Get out there and make this a social activity.\nAttend a Hunter Safety Course\nThere are dangers in the wilderness. Some come from the elements and the rugged terrain, while others come from the animals and even some of the hunters themselves. First aid, proper gear and hunting technique are all very important things to know and have when you’re away from civilization stalking your prey. A hunting safety course can teach you the basics of hunting, as well as the local legislation you’ll have to respect, and most importantly, how to avoid trouble, or deal with trouble if and when it arises.']	['<urn:uuid:37852533-0a0f-4b72-bac5-931a69371ea7>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	6	75	1415
13	Which facility handles more water daily - Carrie Furnaces' cooling system or Edward C. Little recycling?	The Carrie Furnaces' cooling system required over 5,000,000 gallons (approximately 18,927,000 m3) of water per day from the Monongahela River, which is significantly more than the Edward C. Little Water Recycling Facility's production of 170,000 m3 of recycled water per day.	"['Almost 100 feet tall and constructed of 2.5-inch-thick plate steel lined with refractory brick, the iron cupolas at the Carrie Furnaces National Historic Landmark in Swissvale and Rankin, Pennsylvania, are extremely rare examples of pre-World War II iron-making technology. Since the collapse of the Pittsburgh region’s steel industry and the dismantling of most of the mills along the waterways of southwestern Pennsylvania, the Carrie Furnaces are the only non-operative blast furnaces in the area that remain standing. A visit to the Carrie Furnaces is like stepping into a cathedral of iron and steel. The architecture is both overwhelming and inspiring. This is a sculptor’s paradise, and it also happens to be one of the sites for the 26th International Sculpture Conference, “Sculpture in Context: Tradition and Innovation.”\nPlace matters, and tradition and innovation surround this National Historic Landmark. Built in the late 1880s, the Carrie Furnaces produced iron for the Homestead Works steel operation (after 1898), which was owned by Andrew Carnegie, and then US Steel (Henry Clay Frick). Iron flowed from its seven cupola furnaces for close to 100 years. Furnaces 6 and 7 are the only ones that still remain on the site. During the 1920s, ’30s, and ’40s, Carrie 6 and 7 consumed approximately four tons of raw iron ore, coke, and limestone for every ton of iron produced. The cooling system for the blast furnace required more than 5,000,000 gallons of water a day, all supplied by the Monongahela River. The furnaces reached their peak production in the 1950s and ’60s, when they were producing 1,000–1,250 tons of iron a day. It was here, just outside Pittsburgh, that steel became synonymous with American progress and industrial revolution. This is the birthplace of American big steel. From these furnaces flowed the molten alloy that was formed into many monumental icons of the American built environment, including the Empire State Building, the Golden Gate Bridge, and the Brooklyn Bridge.\nUS Steel ceased operations at the Carrie Furnaces in 1984. When the site was sold to the Park Corporation in 1988, the company started to dismantle and salvage the steel structures and machinery. During that time, many local artists would jump the fence and explore the derelict site, including the locally famous Industrial Arts Cooperative (IAC), a rogue group of guerrilla artists led by sculptor Tim Kaulen. The group created monumental site-specific installations from materials found there and spent several seasons of dedicated effort creating the massive Carrie Deer that is still on the site today. Many sculptors and graffiti artists created passionate works among the rusting giants of the Carrie Furnaces.\nThe Steel Industry Heritage Task Force was also founded in 1988, the same year that the Park Corporation bought the Carrie Furnaces. In 1991, the organization morphed into the Rivers of Steel Heritage Corporation (RSHC). A visionary nonprofit, the RSHC was formed through a collaborative effort between the National Parks Service and the Pennsylvania County and State Departments of Conservation and Recreation with the intention to secure the Carrie Furnaces site. The RSHC now owns and stewards many sites of cultural importance in the Pittsburgh region, repurposing and preserving them for future generations. Its mission includes historic preservation, cultural conservation, education, recreation, and resource development. Through the tireless efforts of the RSHC and its main historian and hyper-passionate site manager, Ron Baraff, the Carrie Furnaces now offers a dynamic platform for the arts. Baraff and the RSHC have approached the site in an extremely open manner. Realizing that the site had more potential than just a monument to the region’s past and that it could be accessed through multiple forms of interpretation, they welcomed art as a catalyst.\nAnyone who has a foundry practice or understands the steel and iron heritage of the area will marvel at the majestic array of buildings and cupolas that make up this unique campus. In the mid-2000s, local sculptor Ed Parrish Jr. started running iron pours at the Carrie Furnaces as part of his Hot Metal Happenings (funded by the Sprout Fund), whose purpose was to share with the regional community the fact that iron casting in Pittsburgh is still happening (at a smaller scale)—this time, for making contemporary art.\nIn 2013, two art professors from Indiana University of Pennsylvania (IUP)—Chris McGinnis, a curator, and fellow colleague/ sculptor Sean Derry—formed Alloy PGH. Artists accepted to this biennial residency, education, and exhibition program receive the opportunity to respond to the Carrie Furnaces site. Funded by local arts organizations such as the Sprout Fund, IUP’s Kipp Gallery, and Radiant Hall Studios, Alloy PGH has brought in artists, historians, ecologists, and curators to help the artists-in-residence learn more about what it means to work contextually. Luminaries such as Ann Hamilton and Mary Jane Jacob helped younger artists, including Ricardo Robinson, Oreen Cohen, Carl Bajandas, Ryan Keen, and Edith Abeyta, to create fantastic installations on site during the 2015 program. McGinnis and Derry had the vision and the drive to bring artists to this dynamic location to demonstrate how the visual and performing arts can transform neglected sites into gathering places. The 2013 and 2015 iterations of the program have been extremely successful, establishing the model as a regional gem of an art exhibition that challenges visitors to experience the Carrie Furnaces in a whole new way.\nAt the same time that iron casting events and Alloy PGH were beginning to take shape at Carrie, landscape ethicist and photographer Rick Darke had been documenting the site’s ecological fabric. Darke’s passion for place and the site led him, in consultation with the RSHC and Baraff, to invite the Penn State Master Gardener Program of Allegheny County to develop a comprehensive plant survey in and around Carrie.\nIn 2014, the Master Gardeners—led by Susan Marquesen, Joanne van Linden, Addy Smith-Reiman, and Anna Johnson—put together a team of artists, ecologists, historians, and students to develop a multi-disciplinary approach to interpreting the native and invasive plant species that are surviving, and even thriving, in the disturbed soils of the Carrie Furnaces site. As part of their research, they addressed best practices in environmental stewardship and responded to the site itself with the Iron Garden Walk, which is open to the public and features a series of 10 interpretive iron plaques. First, the Master Gardeners consolidated their research to develop the content, which includes botanical illustrations from the Hunt Institute of Botanical Documentation at Carnegie Mellon and descriptions of the various plants growing around the foundry, and Smith-Reiman created the layout of the plaque designs. The designs were translated and milled in high-density urethane (HDU) on the CNC in the Carnegie Mellon Sculpture Department by art student Lauren Valley. The patterns were molded by myself and Ed Parrish Jr. and cast on site with the Master Gardeners, CMU students, a large contingent of local artists, and Casey Westbrook of Carbon Arts. After the pour, the plaques were installed for future generations of visitors to discover through the seasons.\nThe Iron Garden Walk is a perfect example of collaboration between multiple organizations, using art and cast iron to bring people with related ideas together. The Master Gardeners, with the help of historians and artists, were able to realize their concept of how the site might be seen. By supporting projects like the Iron Garden Walk and Alloy PGH, the Rivers of Steel Heritage Corporation has been forward-thinking in its approach to how this site can benefit from its context, heritage, and public engagement.\nThe RSHC continues to diversify, bringing more artists to engage with the Carrie Furnaces. It recently unveiled a newly formed program, Rivers of Steel Arts (RoSA), headed by artist Chris McGinnis. Through RoSA’s multifaceted programing, the Carrie Furnaces is quickly becoming one of the region’s most exciting venues for the arts, education, and entertainment. RoSA has seen the benefits of multiple forms of community outreach and has teamed up with Parrish Jr. and Westbrook to develop Carbon Arts at Carrie, a metal arts program that will offer educational opportunities for mold-making, casting, and welding in the coming years. The groups are in the process of putting together infrastructure for a permanent foundry on the site, outfitted with a gantry, material storage, classrooms, and eventually multiple iron cupolas of various sizes. RoSA is dedicated to artistic programming that reimagines the future of familiar places, builds pride in community, and attracts renewed public interest in Pittsburgh’s Monongahela River Valley. It continues to evolve and hone its program offerings to attract new visitors from the local, regional, and global community to the site, while knowing the past and embracing the future. RoSA has already generated a substantial and eclectic program, including artist residencies, eco-arts, photo arts, urban arts (graffiti), film, dance, theater, festivals, and heritage arts consisting of a regional folklife center that represents eight counties in southwestern Pennsylvania.\nDuring the 26th annual International Sculpture Conference, conference-goers will have a chance to visit the Carrie Furnaces for panel discussions, cast iron workshops, tours of the Iron Garden Walk, and an exhibition of contemporary cast iron sculpture curated by Westbrook and Parrish Jr. For more on the arts at Carrie, visit http://rosarts.org.\nJoshua Reiman is an artist living in Portland, Maine, where he is also an assistant professor in the MFA in Studio Art + Sculpture programs at the Maine College of Art.\nWatch these videos on the Carrie Furnaces', ""SUEZ announces the signature of three contracts in the United States for a total turnover of more than €180 million. The Group will contribute to improve the drinking water services of Jersey City, New Jersey, and help preserve the water resources of Southern California through its partnership with West Basin Municipal Water District. The Group is also pursuing its development in the industrial sector: the Conemaugh Generating Station in Pennsylvania has chosen the solutions provided by SUEZ to reduce its operating costs and environmental footprint.\nThe Jersey City Municipal Utility Authority (JCMUA) and SUEZ have renewed the public-private partnership contract for the management of the city's drinking water services. This 9-year contract worth around €155 million covers the maintenance and operation of the infrastructures that ensure the distribution of drinking water to the 262,000 inhabitants of Jersey City. These infrastructures include the Boonton plant with a capacity of 300,000 m3 per day, two dams, a 310 km2 watershed, a 37 km aqueduct and 520 km of drinking water network. The Group will also be responsible for customer service and billing. This contract extends the initial 1996 agreement between Jersey City and SUEZ, which was the largest public-private partnership for the management of water services in the United States at the time.\nThe new contract includes the modernization of the drinking water distribution infrastructure with the aim of installing 36,000 connected water meters. This solution will improve service quality for users by providing them with more control over their water consumption, alerting them in the event of a leak at their home, and billing them on their actual rather than estimated consumption. For the JCMUA, this solution will make it possible to monitor, in real time, the performance of the drinking water network, preserve the water resource, and achieve the associated savings.\nSUEZ has renewed the contract with West Basin Municipal Water District for the operation and maintenance of the Edward C. Little Water Recycling Facility in El Segundo (California) and four satellite plants. For a duration of two years and nearly €26 million, this contract extends the initial 1994 partnership to meet the challenge of preserving potable water supplies in South California.\nThe Edward C. Little facility is one of the largest water recycling operation of its kind in the United States. It produces approximately 170,000 m3 of water every day, conserving water resources in one of the nation’s most drought-prone regions. The plant also produces 45,000 m3 of barrier water every day, thus protecting South Bay’s coastal groundwater reservoirs from seawater intrusion from the Pacific Ocean. The plant supplies up to 90% percent of the water injected into the West Coast Groundwater Barrier, preserving 17 million m3 of water each year.\nThe facility produces five different qualities of custom-made recycled water to meet the specific needs of West Basin’s municipal, commercial and industrial customers (irrigation, supply of low and high-pressure boiler feed water, cooling tower water and indirect drinking water).\nThe collaboration of SUEZ and West Basin Municipal Water District will help the District increase recycled water production capacity to 265 000 m3 per day as downstream demand continues to grow.\nThese two contracts strengthen SUEZ's presence in the United States where it provides drinking water and sanitation services to 6.4 million people. The Group also supports industries in the management of their water resources, including the Oil & Gas, Power and mining sectors. The acquisition of GE Water in October 2017 and the creation of the Business Unit “Water Technologies & Solutions” enable it to offer a unique full-service offering to industrial customers with a network of 45 manufacturing and service centers and 10,000 employees worldwide. The Conemaugh Generating Station in New Florence, Pennsylvania, has chosen SUEZ's mobile water solutions to produce boiler feedwater. By using the mobile demineralizer with InSight asset performance management, the coal-fired power plant will gain $220,000 in annual operating cost savings, reduce chemicals associated with the make-up demineralized (DI) systems, and optimize water consumption, which subsequently reduces treated wastewater discharge into the Conemaugh River.""]"	['<urn:uuid:eef00725-c4d0-4ba0-8f28-fcef60e8f74c>', '<urn:uuid:0990a1d8-93cf-442c-a8f2-589dfeaec1de>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T00:57:18.859842	16	41	2229
14	How do zooplankton respond to oxygen changes and what are acidification's effects on plankton?	Zooplankton are extremely sensitive to oxygen changes, responding to even slight variations of 1% or less in oxygen levels, despite having high hypoxia tolerance. They live near their physiological limits in oxygen minimum zones. Regarding acidification's effects, planktonic organisms that build calcium carbonate shells are particularly vulnerable. Ocean acidification impairs the ability of calcifying plankton to build and maintain their shells, as the changing water chemistry reduces available carbonate ions needed for shell construction. These changes affect plankton's role in marine food webs and their contribution to carbon cycling in the ocean.	"['Small zooplankton rings the alarm for oxygen loss in big oceans\n""Hypoxia, a low level of oxygen that limits the physiological functions of animals, is a topic that fascinates many biologists. As climate change progresses, the frequency of hypoxic episodes in aquatic environments is increasing, putting fish species under stress and even reducing populations in some cases. But it is not only fish that suffer the ill effects of hypoxia. [...]""\nSource: Journal of Experimental Biology\nAuthor: Yangfan Zhang\nPhytoplankton calcifiers control nitrate cycling and the pace of transition in warming icehouse and cooling greenhouse climates\n""Phytoplankton calcifiers contribute to global carbon cycling through their dual formation of calcium carbonate and particulate organic carbon (POC). The carbonate might provide an efficient export pathway for the associated POC to the deep ocean, reducing the particles\' exposure to biological degradation in the upper ocean and increasing the particle settling rate. Previous work has suggested ballasting of POC by carbonate might increase in a warming climate, in spite of increasing carbonate dissolution rates, because calcifiers benefit from the widespread nutrient limitation arising from stratification. [...]""\nAuthors: Karin F. Kvale et al.\nPlanktonic food web structure and trophic transfer efficiency along a productivity gradient in the tropical and subtropical Atlantic Ocean\n""Oligotrophic and productive areas of the ocean differ in plankton community composition and biomass transfer efficiency. Here, we describe the plankton community along a latitudinal transect in the tropical and subtropical Atlantic Ocean. Prochlorococcus dominated the autotrophic community at the surface and mixed layer of oligotrophic stations, replaced by phototrophic picoeukaryotes and Synechococcus in productive waters. [...]""\nSource: Scientific Reports\nAuthors: Laia Armengol et al.\nMuch of the surface ocean will shift in color by end of 21st century\n""Climate change is causing significant changes to phytoplankton in the world\'s oceans, and a new MIT study finds that over the coming decades these changes will affect the ocean\'s color, intensifying its blue regions and its green ones. Satellites should detect these changes in hue, providing early warning of wide-scale changes to marine ecosystems. [...]""\nOcean colour signature of climate change\n""Marine calcifiers are considered to be among the most vulnerable taxa to climate-forced environmental changes occurring on continental margins with effects hypothesized to occur on microstructural, biomechanical, and geochemical properties of carbonate structures. Natural gradients in temperature, salinity, oxygen, and pH on an upwelling margin combined with the broad depth distribution (100–1,100 m) of the pink fragile sea urchin, Strongylocentrotus (formerly Allocentrotus) fragilis, along the southern California shelf and slope provide an ideal system to evaluate potential effects of multiple climate variables on carbonate structures in situ. [...]""\nSource: Nature Communications\nAuthors: Stephanie Dutkiewicz et al.\nOcean deoxygenation and zooplankton: Very small oxygen differences matter\n""Oxygen minimum zones (OMZs), large midwater regions of very low oxygen, are expected to expand as a result of climate change. While oxygen is known to be important in structuring midwater ecosystems, a precise and mechanistic understanding of the effects of oxygen on zooplankton is lacking. Zooplankton are important components of midwater food webs and biogeochemical cycles. Here, we show that, in the eastern tropical North Pacific OMZ, previously undescribed submesoscale oxygen variability has a direct effect on the distribution of many major zooplankton groups. Despite extraordinary hypoxia tolerance, many zooplankton live near their physiological limits and respond to slight (≤1%) changes in oxygen. [...]""\nSource: Science Advances\nAuthors: K. F. Wishner et al.\nURI researchers: Small changes in oxygen levels have big implications for ocean life\nOceanographers at the University of Rhode Island have found that even slight levels of ocean oxygen loss, or deoxygenation, have big consequences for tiny marine organisms called zooplankton.\nZooplankton are important components of the food web in the expanse of deep, open ocean called the midwater. Within this slice of ocean below the surface and above the seafloor are oxygen minimum zones (OMZs), large regions of very low oxygen. Unlike coastal “dead zones” where oxygen levels can suddenly plummet and kill marine life not acclimated to the conditions, zooplankton in OMZs are specially adapted to live where other organisms – especially predators – cannot.\nSource: Whats up newp\nParticulate matter flux interception in oceanic mesoscale eddies by the polychaete Poeobius sp.\n""Gelatinous zooplankton hold key functions in the ocean and have been shown to significantly influence the transport of organic carbon to the deep sea. We discovered a gelatinous, flux‐feeding polychaete of the genus Poeobius in very high abundances in a mesoscale eddy in the tropical Atlantic Ocean, where it co‐occurred with extremely low particle concentrations. Subsequent analysis of an extensive in situ imaging dataset revealed that Poeobius sp. occurred sporadically between 5°S–20°N and 16°W–46°W in the upper 1000 m. [...]""\nSource: Limnology and Oceanography\nAuthors: Svenja Christiansen et al.\nAnnual plankton community metabolism in estuarine and coastal waters in Perth (Western Australia)\n""The planktonic metabolic balance that is the balance between gross primary production (GPP) and community respiration (CR) was determined in Matilda Bay (estuarine) and Woodman Point (coastal) in Perth, Western Australia. The rates of net community production (NCP = GPP – CR) and the ratio between GPP and CR (P/R) were assessed to evaluate whether the metabolic balance in the two coastal locations tends to be net autotrophic (production exceeding community respiration) or net heterotrophic (respiration exceeding production). [...]""\nAuthors: Susana Agusti, Lorena Vigoya, Carlos Manuel Duarte\nDistribution of planktonic biogenic carbonate organisms in the Southern Ocean south of Australia: a baseline for ocean acidification impact assessment\n""The Southern Ocean provides a vital service by absorbing about one-sixth of humankind\'s annual emissions of CO2. This comes with a cost – an increase in ocean acidity that is expected to have negative impacts on ocean ecosystems. The reduced ability of phytoplankton and zooplankton to precipitate carbonate shells is a clearly identified risk. The impact depends on the significance of these organisms in Southern Ocean ecosystems, but there is very little information on their abundance or distribution.""\nAuthors: Thomas W. Trull et al.\nRead the full article here.', 'What is Ocean Acidification?\nSince the beginning of the Industrial Revolution, when humans began burning coal in large quantities, the world’s ocean water has gradually become more acidic. Like global warming, this phenomenon, which is known as ocean acidification, is a direct consequence of increasing levels of carbon dioxide (CO2) in Earth’s atmosphere.\nPrior to industrialization, the concentration of carbon dioxide in the atmosphere was 280 parts per million (ppm). With increased use of fossil fuels, that number is now approaching 400 ppm and the growth rate is accelerating. Scientists calculate that the ocean is currently absorbing about one quarter of the carbon dioxide that humans are emitting. When carbon dioxide combines with seawater, chemical reactions occur that reduce the seawater pH, hence the term ocean acidification.\nCurrently, about half of the anthropogenic (human-caused) carbon dioxide in the ocean is found in the upper 400 meters (1,200 feet) of the water column, while the other half has penetrated into the lower thermocline and deep ocean. Density- and wind-driven circulation help mix the surface and deep waters in some high latitude and coastal regions, but for much of the open ocean, deep pH changes are expected to lag surface pH changes by a few centuries.\nOcean acidification and global warming are different problems, but are closely linked because they share the same root cause—human emissions of carbon dioxide. The atmospheric concentration of carbon dioxide is now higher than it has been for the last 800,000 years and possibly higher than any time in the last 20 million years. Humans have thus far benefited from the ocean’s capacity to hold enormous amounts of carbon, including a large portion of this excess carbon dioxide. Had the ocean not absorbed such vast quantities of carbon dioxide, the atmospheric concentration would be even higher, and the environmental consequences of global warming (sea level rise, shifting weather patterns, more extreme weather events, etc.) and their associated socioeconomic impacts would likely be even more pronounced. However, the oceans cannot continue to absorb carbon dioxide at the current rate without undergoing significant changes in chemistry, biology, and ecosystem structure.\nMeasuring ocean acidification: Past and present\nScientists know that the oceans are absorbing carbon dioxide and subsequently becoming more acidic from measurements made on seawater collected during research cruises, which provide wide spatial coverage over a short time period, and from automated ocean carbon measurements on stationary moorings, which provide long-term, high-resolution data from a single location.\nThese records can be extended back through time using what are known as chemical proxies to provide an indirect measurement of seawater carbonate chemistry. A proxy is a measurement from a natural archive (ice cores, corals, tree rings, marine sediments, etc.) that is used to infer past environmental conditions. For example, by analyzing the chemical composition of tiny fossil shells found in deep ocean sediments, scientists have developed ocean pH records from ancient times when there were no pH meters. Furthermore, because the ocean surface water is in approximate chemical balance, or equilibrium, with the atmosphere above it, a record of historical ocean pH can be inferred from atmospheric carbon dioxide records derived from Greenland and Antarctic ice cores, which contain air bubbles from the ancient atmosphere. Such evidence indicates that current atmospheric carbon dioxideconcentrations and ocean pH levels are at unprecedented for at least the last 800,000 years.\nGoing back deeper in Earth history to the Paleocene-Eocene boundary about 55 million years ago, scientists have found geochemical evidence of a massive release of carbon dioxide accompanied by substantial warming and dissolution of shallow carbonate sediments in the ocean. Although somewhat analogous to what we are observing today, this carbon dioxide release occurred over several thousand years, much more slowly than what we are witnessing today, thus providing time for the oceans partially to buffer the change. In the geologic record, during periods of rapid environmental change, species have acclimated, adapted or gone extinct. Corals have undergone large extinction events in the past (such the Permian extinction 250 million years ago), and new coral species evolved to take their place, but it took millions of years to recover previous levels of biodiversity.\nHow is ocean acidification affecting ocean chemistry?\nSeawater has a pH of 8.2 on average because it contains naturally occurring alkaline ions that come primarily from weathering of continental rocks. When seawater absorbs carbon dioxide from the atmosphere, carbonic acid is produced (see Box 1), reducing the water’s pH. Since the dawn of industrialization, average surface ocean pH has decreased to about 8.1.\nBecause the pH scale is logarithmic (a change of 1 pH unit represents a tenfold change in acidity), this change represents a 26 percent increase in acidity over roughly 250 years, a rate that is 100 times faster than anything the ocean and its inhabitants have experienced in tens of millions of years.\nAcidification can affect many marine organisms, but especially those that build their shells and skeletons from calcium carbonate, such as corals, oysters, clams, mussels, snails, and phytoplankton and zooplankton, the tiny plants and animals that form the base of the marine food web.\nThese “marine calcifiers” face two potential threats associated with ocean acidification: 1) Their shells and skeletons may dissolve more readily as ocean pH decreases and seawater becomes more corrosive; and 2) When CO2 dissolves in seawater, the water chemistry changes such that fewer carbonate ions, the primary building blocks for shells and skeletons, are available for uptake by marine organisms. Marine organisms that build shells or skeletons usually do so through an internal chemical process that converts bicarbonate to carbonate in order to form calcium carbonate.\nExactly how ocean acidification slows calcification rates, or shell formation, is not yet fully understood, but several mechanisms are being studied. Most hypotheses focus on the additional energy an organism must expend to build and maintain its calcium carbonate shells and skeletons in an increasingly corrosive environment. In the face of this extra energy expenditure, exposure to additional environmental stressors (increasing ocean temperatures, decreasing oxygen availability, disease, loss of habitat, etc.) will likely compound the problem.\nThese effects are already being documented in many marine organisms, particularly in tropical and deep-sea corals, which exhibit slower calcification rates under more acidic conditions. The impact on corals is of great concern because they produce massive calcium carbonate structures called reefs that provide habitat for many marine animals, including commercially important fish and shellfish species that use the reefs as nursery grounds. Coral reefs are vital to humans as sources of food and medicine, protection from storms, and the focus of eco-tourism. In addition to corals, studies have shown that acidification impairs the ability of some calcifying plankton, tiny floating plants and animals at the base of the food web, to build and maintain their shells. Scientists have also observed increased larval mortality rates of several commercially important fish and shellfish.\nWhat can we expect in the future?\nOcean acidification is occurring at a rate 30 to100 times faster than at any time during the last several million years driven by the rapid growth rate atmospheric CO2 that is almost unprecedented over geologic history. According to the Intergovernmental Panel on Climate Change (IPCC), economic and population scenarios predict that atmospheric CO2 levels could reach 500 ppm by 2050 and 800 ppm or more by the end of the century. This will not only lead to significant temperature increases in the atmosphere and ocean, but will further acidify ocean water, reducing the pH an estimated 0.3 to 0.4 units by 2100, a 150 percent increase in acidity over preindustrial times. Assuming a “business-as-usual” IPCC CO2 emission scenario, predictive models of ocean biogeochemistry project that surface waters of the Arctic and Southern Oceans will become undersaturated with aragonite (a more soluble form of calcium carbonate) within a few decades, meaning that these waters will become highly corrosive to the shells and skeletons of aragonite-producing marine calcifiers like planktonic marine snails known as pteropods.\nAlthough ocean acidification has only recently emerged as a scientific issue, it has quickly raised serious concerns about the short-term impacts on marine organisms and the long-term health of the ocean. Scientists estimate that over the next few thousand years, 90 percent of anthropogenic CO2 emissions will be absorbed by the ocean. This may potentially affect biological and geochemical processes such as photosynthesis and nutrient cycling that are vital to marine ecosystems on which human society and many natural systems rely. At the same time, marine organisms will face the enormous challenge of adapting to ocean acidification, warming water, and declining subsurface-ocean oxygen concentrations.\nNews & Insights\nWHOI working to address ocean acidification; protect region’s vital shellfish industry\nA new report addresses the impacts of ocean acidification in Massachusetts and New England coastal waters on the region’s vital seafood industry.\nOcean acidification gets a watchful eye in New England aquaculture ‘hot spot’\nShellfish aquaculture is thriving in New England, but future growth in the industry could be stunted as coastal waters in the region become more acidic. Researchers at WHOI have developed…\nOcean acidification causing coral ‘osteoporosis’ on iconic reefs\nScientists Pinpoint How Ocean Acidification Weakens Coral Skeletons\nClimate Change Will Irreversibly Force Key Ocean Bacteria into Overdrive\n[ ALL ]\nWHOI in the News\nThe Top Eight Ocean Stories of 2022\nThe $500 Billion Question: What’s the Value of Studying the Ocean’s Biological Carbon Pump?\nEcology Research: Ocean acidification causing coral ‘osteoporosis’ on iconic reefs\nDisentangling influences on coral health\n[ ALL ]\nFrom Oceanus Magazine\nOcean acidification is no big deal, right?\nWHOI’s Jennie Rheuban discusses the very real phenomenon of an increasingly acidic ocean and the toll it’s taking on marine life.\nTo Tag a Squid\nHow do you design a tag that can attach to a soft-bodied swimming animal and track its movements? Very thoughtfully.\nHow Do Corals Build Their Skeletons?\nWHOI scientists discovered precisely how ocean acidification affects coral skeletons’ a factor that will help scientists predict how corals throughout the world will fare as the oceans become more acidic.\nSearching for ‘Super Reefs’\nSome corals are less vulnerable to ocean acidification. Can the offspring from these more resilient corals travel to other reefs to help sustain more vulnerable coral populations there?\nGraduate student Hannah Barkley is on a mission to investigate how warming ocean temperatures, ocean acidification, and other impacts of climate change are affecting corals in an effort to find…']"	['<urn:uuid:2ff4dc08-788e-45e3-865f-c119df2cde67>', '<urn:uuid:51ca6f21-f0f4-4343-8b14-a7f059f0fe69>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	14	92	2728
15	fume hood installation complexity vs range hood setup	Ducted fume hood installation is much more complex than range hood setup. Fume hoods require extensive engineering studies, proper rooftop ventilation planning, ductwork requirements, and make-up air system calculations, with significant costs for ductwork and fans. In contrast, range hoods can be simply plugged into a standard 120V outlet with a three-pronged connector, or hardwired to the electrical system, and only require basic wall/roof venting installation if using a ducted model.	"[""Following on from our previous article on horizontal vs vertical LAF cabinets, it's time to look at another crucial decision. What is the best option—a ductless filtering fume cabinet or a ducted fume hood?\nTruthfully, it is impossible to give an answer in favour of either concept without objectively analysing the disadvantages and advantages of both systems. It is also impossible to give an answer to the ducted vs ductless fume hood question without prior knowledge of the intended usage. Therefore, to help make a qualified decision, it is important to examine how and where the hood will be used.\nEqually as important is to determine what will happen within the hood and how each hood type affects its environment. Objectively analysing the disadvantages and advantages of both systems requires exact knowledge of chemical usage and project type.\nOnly once the usage scenario has been clearly established can a decision be made in favour of using either system. Aside from helping make the right decision, the goal of this article is also to eliminate any preconceived negative notions against one system or the other so as to allow users to find the best solution in terms of environmental impact, cost, and adaptability of use, without losing sight of the fact that the solution should ultimately ensure their safety.\nMost, if not all laboratory personnel are knowledgeable about ducted fume hoods. In fact, the basic concept dates back to the Middle Ages, when the alchemists used the chimneys of their own homes to perform experiments. Since then this concept, which never ceased to evolve, has finally turned into the modern ducted fume hoods of today's laboratories.\nClick here to browse our range of ducted fume cupboards.\nIn contrast, recirculating filtered fume hoods have only been around for about 50 years. The concept can be linked back to the first gas mask (created in 1847), whereby air is filtered before being recirculated, and the first activated charcoal gas mask created in 1915.\nDuctless cupboards were first created around 1968. Today, ductless filtering fume enclosures use the properties of activated carbon combined with catalysts and certain neutralisers in order to filter toxic molecules and HEPA or ULPA filters are used to filter out dust and particles, therefore eliminating their dependency on HVAC and the need to be connected to external ductwork. Filter technology has advanced to the stage where a large percentage of experiments or processes can be carried out safely in a ductless cupboard. However, you still need to make sure you regularly clean and take care of your filters.\nClick here to browse our range of recirculating fume cupboards.\nThe installation of a ducted fume hood is complex. Extensive engineering studies are needed beforehand to determine proper rooftop ventilation and ductwork requirements along with the make-up air system that will be necessary to compensate for the air consumed by one or more fume hoods. The costs of ductwork and fans (and the installation of) is significant.\nSince a ducted cupboard consumes more energy per year than an average house, the consumption of heated or cooled air is high and can represent for a 1500mm modern fume hood an average of 1076 m3 per hour (9,425,760 m3 per year [assuming 24/7/365]). Recently updated calculations show that this energy consumption translates to an estimated minimum of £1017.00 per year.\nLastly, their working principle forces ducted fume cupboards to eject toxic substances directly into the atmosphere, and their fixed nature and tie-in to the HVAC system render them immobile. Even if scrubbing media is used there is still the issue of waste chemicals to dispose of, and if a charcoal bed is used, the waste carbon.\nToday, fume hoods that entail ducting have reached a high level of sophistication and, if a Variable Air Volume system is present, with regard to saving the air exhausted into the atmosphere and containing the fumes around the work area. They do not require such a detailed pre-verification of the chemicals intended for use, aside from those prohibited by laws and regulations, which as of today do not limit the amount of chemicals they can exhaust into the atmosphere. The only consideration is the work surface required and whether a scrubber is needed (for example with Perchloric acids).\nTraditional fume hoods have been long recognised as an effective means of protection for laboratory personnel, although recently they have been identified as a large consumer of energy in comparison to those that use filters, which can’t be ignored.\nThe frame of use of a ductless filtering fume enclosure is limited to certain chemicals. This limitation can vary between manufacturers, which is why it is important to consult each manufacturer's guide of retained chemicals or chemical listing to determine the TWA (time-weighted average over an 8 hour period) maximum exposure rates. Filters are only able to remove chemicals down to a set ppm and some chemicals can be as low as 2ppm TWA!.\nAnother disadvantage can be the amount of chemicals being used – especially if boiling off large quantities of acids where a lot of moisture is generated as fume cupboard filters have a finite adsorption capacity. It is recommended to monitor filter saturation in any situation (for example a built-in filter alarm). It is necessary as well to periodically replace the filters (recommended annually or 600 hours use, whichever occurs sooner).\nRecirculating fume cupboards do not require external fans/ductwork/exhaust ducts or make-up air systems since they filter and recycle the laboratory air. They don't exhaust chemical pollutants into the atmosphere and they are mobile, so can be moved to suit changing laboratory environments. Fume hoods that are built with filters instead of ducting are immediately available for use without lengthy or costly installation requirements. They can be temporarily stored and dispatched where needed by a safety officer for a scheduled class or project and also be placed anywhere at a moment's notice to protect users in cases in which the need suddenly arises.\nThe fact is that when an application falls within the intended frame of usage, as set by the manufacturer, one cannot help but be drawn to the advantages of a ductless filtering fume hood. However, it is very important at this moment to verify that the manufacturer has answered all of the necessary questions that must be asked prior to making a final decision, such as:\nIf each and every one of these questions receives a positive response, one's decision is documented and should now be easy to make. However, attention must be given to the fact that certain ductless enclosure manufacturers do not hesitate to provide positive answers to all of these questions for the sole purpose of making a sale. It is therefore advisable to deal strictly with manufacturers that are able to show official proof of compliance with the performance standard BS EN 14175 (NOT just stating 'designed to comply with').\nThis indispensable precaution will help protect the buyer from certain companies involved in this emerging profession, which need more rigorous product design and sales approaches. Complying with a stringent safety standard can only benefit all parties involved and primarily guarantees the safety of end-users.\nIn the event that satisfactory responses cannot be provided to ensure the safe usage of a recirculating fume hood, a decision should certainly be made in favour of selecting a ducted fume hood. Without a doubt, safety must remain the first priority when making this decision. Whether the lab is in a school or a blue-chip organisation, this is vital. However, while ducted enclosures have been the industry standard for containment of toxic gases for years, their tremendous infrastructure, usage, and environmental costs cannot be ignored any longer.\nAt the same time, some ductless fume hood manufacturers have recently emerged with safety services and product evolutions, which, under the right circumstances, can provide a viable solution in the laboratory. In today's world, the decision between the two styles needs to be made with a careful eye on safety, the environment, and energy consumption, and a conclusion can be made that it is worthwhile to try and determine where both products have their place. Given the extreme rise in energy costs and the benefits of using green technologies, taking the time to figure out which system is best where, and determining the applications most suitable to each can have a significant impact on one's budget and on the environment."", '- Simple as plugging the hood into a power socket, and if required, using an extension cord, Unless the hood came with a three-pronged connector or if the plug was damaged, it will need to be connected to the electrical system. The Hurricane insert from Proline Range Hoods is supplied with a power cord that is fastened to the hood.\nCan range hoods be plugged in?\nRange hoods are available in two configurations: hardwired and plug-in. If your hood comes with a plug, you won’t have to worry about hardwiring it. If there is no plug, it indicates that it is intended to be hardwired. In accordance with the manufacturer’s specifications, hoods that come with a plug can be hardwired, and hardwired hoods can be converted to plug-in versions.\nHow do range hoods get power?\nRange hoods and vent hoods are typically operated with only a single motor. Some people have two or three. These blower motors draw in fumes and cooking exhaust from above your cooktop and transport it via ducting and outside your home to the outside (or through charcoal filters with ductless models).\nDo vent hoods have plugs?\nWhen installing a range hood, you have two options for connecting the hood to the electrical system. The first and most straightforward alternative is to connect your range hood to a conventional 120V outlet, provided that it has a three-pronged socket. To reach the outlet if your chord isn’t long enough, you may use an extension to extend it.\nCan you put a plug on a cooker hood?\nPlug and socket: You may install a new socket in the chimney part of the cooker hood that will be hidden from view. It is possible to spur this plug off of an existing circuit by using a junction box. After that, you may plug in your cooker hood using a plug with a 3amp fuse, just as you would any other electrical device.\nAre cooker hoods hard wired?\nMany stove hoods are available for installation as either a chord and plug connected appliance or as a hardwired appliance; however, hardwiring is the recommended technique in the majority of cases. It is the way that has been approved by all building regulations, and it is the one that is used by all licensed and certified installers.\nDo range hoods use a lot of electricity?\nYour range hood consumes only 25 kWh per year, which is extremely efficient. Range hoods consume between 70 and 150 watts of electricity. When used as frequently as a standard electric range, a range hood consumes just around 25 kWh of electricity per year when used on a regular basis.\nDo range hoods have to be vented outside?\nRange hoods do not have to be vented to the outside in order to function properly. Range hoods with ducts, on the other hand, are virtually usually favored over ductless range hoods. Diffuser hoods completely eliminate all of the smoke and cooking exhaust from your kitchen, whereas ductless range hoods recycle the cooking exhaust back into the kitchen.\nHow do you install a range hood vent through the wall?\nAn outside wall or roof cap should be attached to the wall or roof.\n- Identify the location where you wish to vent your range hood. Make sure that your ductwork is installed in the most direct and clear manner possible to the outside of your property. Remove the hood and create an opening 1-2 inches bigger than your ductwork where you will run the duct from your hood. Connect the range hood to the ducting in your home.\nHow do I know if my range hood is ducted?\nRange hoods are vented either upward through the roof or outward via an outside wall, depending on the design. Look for a metal duct running through the cabinets above, or an exhaust cap on the outside wall behind the range hood, which will indicate the presence of an exhaust system. When the fan is functioning, you should be able to feel air being blasted out of the exhaust cap located on the roof or wall.\nDoes a range hood need a separate circuit?\nIs it necessary to have a separate circuit for a range hood? No. A range hood can be connected to a circuit that contains more than one appliance. Putting your range hood on the same circuit as an electric stove is not a good idea when you have a 15 to 20 amp breaker in place.']"	['<urn:uuid:b8f89714-b218-4c21-b3a4-1d8980042435>', '<urn:uuid:c05a08e0-2677-426a-b708-0f4a10c55a82>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	8	71	2149
16	outdoor fireplace porch chimney safety requirements	Outdoor fireplaces and chimneys must meet specific safety requirements. For outdoor structures, chimneys must maintain a minimum 1-inch clearance from combustible materials, while indoor chimneys require 2 inches. Through-wall thimbles need at least 8 inches of masonry and 12 inches total distance from combustible materials due to high heat. In residential construction like porches, proper foundation is crucial - concrete footings with steel rebar reinforcement must be installed first, followed by concrete-filled blocks for a solid core. The FireRock precast masonry material ensures the firebox and chimney have the exact shape for proper drawing, and the brick covering should match the house while maintaining all safety clearances.	"['A beautiful back porch with ipê wood flooring, a massive fireplace, and adjoining patio was added to this two-story, historic home to turn the backyard into the perfect place for outdoor entertaining.\nSince the house was located in a historic district, the addition had to be approved and comply with local regulations. Architect Nick Holmes of Holmes and Holmes Architects incorporated many of the architectural features found on the existing home into the design of the porch.\nPorch and Patio FoundationAfter the footprint of the addition had been established, holes were dug for the concrete footings and steel rebar added for reinforcement. Once the footings had been poured, concrete blocks were laid, then filled with concrete to provide a solid core for the foundation.\nConcrete slabs were also poured for an adjoining patio, with wooden forms used to create troughs in the concrete for a brick border. A decorative diagonal pattern was cut into the concrete patio with a circular saw.\nOnce the foundation was complete, sheet metal termite shields were placed on top of the concrete block piers. Pressure treated floor joists were used to frame the 17’ wide by 24’ long porch. To make sure rainwater runs off the porch floor, the floor joists were installed with the center of the porch a bit higher than the edges.\nPosts to support the ceiling and roof were attached to the joists and notched at the top to accept beams. Ceiling joists were then installed across the porch between the beams.\nA decorative scroll pattern was cut into the bottom of the exposed ends of the rafters to match the design on the house. The roof framing was then covered with plywood decking followed by roofing.\nThe firebox and chimney were assembled from precast masonry material made by FireRock, so it would have exactly the right shape to draw properly. The fireplace and concrete block foundation piers were then covered with brick that had been chosen to match the house.\nPorch Columns and TrimTo match the recessed panel columns on the front of the house, the pressure treated posts were covered with:\n- A layer of plywood.\n- Trim boards to form the center panels.\n- Corner boards made from 1×4 lumber.\n- Plinth blocks on the column tops.\n- Crown molding around the capitals.\n- Bases on the bottom of the columns.\nThe ceiling on the porch was then covered with wood beadboard to mimic the ceiling found on the front porch of the house.\nIpê Porch Flooring\nThe flooring for the porch was made from a dense, tropical hardwood called ipê. Native to South America, ipê is naturally resistant to moisture, fungus, insects, and rot. The tongue and groove flooring was installed using stainless steel screws in predrilled holes. A small gap was left between every few rows of flooring to allow for expansion.\nAdded AmenitiesOther amenities on the porch included:\n- Cabinets for an outdoor kitchen were constructed.\n- Pressure treated lattice panels were installed around the foundation.\n- Mahogany door with glass was installed on the back of the house.\n- Mantel and flat screen TV were installed above the fireplace.\nOther Tips from This Episode\nSimple Solutions with Joe Truini:\nCleaning a Grill with Vinegar\nTo clean the cooking surface on an outdoor grill, spray the metal grate with white vinegar, allow it to sit for a few minutes, then rub the surface with crumpled up aluminum foil. The acid in the vinegar will cut through grease and food residue while the abrasive foil cleans it away.\nBest New Products with Jodi Marks:\nHampton Bay Solar Rope Light\nThe Hampton Bay Solar Rope Light is great for accenting porch and deck railings or as a border on outdoor walkways. The 16’ long strand has 55 white LED lights. Since it’s solar powered, the light doesn’t require wiring, making it easy to install. The Hampton Bay Solar Rope Light is available at The Home Depot.\nAsk Danny Lipford:\nWater Drainage Problems\nPoor drainage in your yard can cause fungus and disease problems in your lawn, as well as allowing water to seep under your house and into the crawlspace or basement. Fill in any low spots in your yard, and slope the ground next to the foundation away from your house, so water runs off.', 'Masonry Chimney Construction can be done by yourself but you need to follow a few guide lines. Virtually every building or home has at least one chimney.\nThey have the potential to provide warmth and heat to our families yet can also be destructive to our possessions and loved ones. During Chimney Construction,\nDuring Masonry Chimney Construction keep your eye open for combustible material close to the; through wall thimble. For the most part, they are one of the most overlooked parts of our structures, and rarely are they built according to the local building codes. Masonrworktools.com offers this article, and it takes a look at two major categories of chimneys, and provides some quick guidance and notes for concerned professionals or homeowners on what to look for when assessing the condition of the chimney during Masonry Chimney Construction. The purpose is not to make experts out of you - lets leave that to the properly certified, full time masonry chimney contractors. However, this Masonry Chimney Construction information may help raise your awareness of possible problems associated with chimneys.\nThe purpose of the different types of chimneys is to allow combustion, by-products to be vented away from the structure safely without causing damage to the structure they are attached to. These types of chimneys have many similarities. They are constructed of similar materials and both are subject to many of the same building code requirements. Even though chimneys serving fireplaces, and chimneys serving appliances may be venting different combustion by-products (for instance, a fireplace chimney may be venting solid fuel by-products), they should all be constructed to discharge these by-products at a rate that does not adversely affect the combustion process and to release the discharged material at a height and location that provides fire safety. Masonry Chimney Construction is quite simple if these guidlines are followed.\nIt is important to note that Masonry Chimney Construction building code requirements often vary on a local basis depending on what your city requires. Typically, the reference used for local building codes is based upon the National Fire Protection Agencies 211 standard for chimneys, fireplaces, vents, and solid fuel burning appliances. (The latest version was updated in 1996 and may be obtained by calling 1-800-344-3555). A few of the codes which are typically universally adopted include the following:\n1. The minimum chimney height for fire safety is the greater of 3 ft. above the highest point where the chimney penetrates the roof line, or 2 ft. higher than any portion of the structure or adjoining structures within 10 ft. of the chimney.\n2. A chimney flue shall not change in size or shape within 6 in. above or below any point where the chimney passes through a combustible floor, ceiling, or roof component.\n3. Chimney penetrating through the wall thimble shall have a minimum of 8 inches of masonry and 12 inches total distance away from anything combustible.\n4. Residential masonry chimney wall thickness should be a nominal 4 inches.\n5. Masonry chimneys should be lined. The selection of the lining material should be appropriate for the class of chimney service and the type of appliance to it in accordance with the terms of the appliance listing and the manufacturers instructions.\n6. Chimney clearance from combustible material is a minimum of 2 in. except where the chimney is located outside the structure, in which case 1 inch is acceptable. ""Do not misinterpret this as being by the thimble"" The through wall thimble is exceptionally important, it gets very hot at that location and needs to be addressed carefully. Refer to figure 3.\n7. According to most building codes, flues may be constructed to discharge their by-products with other flues as long as the following conditions are met:\n8. The flues do not slope more than 30 degrees from vertical.\n9. The flues are discharging by-products of similar fuels (for instance, you can\'t combine a flue venting gas by-products with one venting wood by-products).\n10. When combining flues, the main discharge flue should be sized for the maximum combined flow of both smaller flues.\nMetal chimneys, also called prefab, class A and Double or triple wall are usually made in three diameters as show below. Larger diameters which may apply to prefab fireplaces. Round flue tiles or single wall stainless chimney liner are also shown. Each diameter has a specific usable area, which is also called it\'s cross section. The capacity of a chimney to remove smoke and/or BTU\'s from an appliance is directly related to this area.\n|Chimney Size||Square Inches||Common Venting Uses|\n|6 Inch Inside Diameter||28.3||Stoves, Central Heaters|\n|7 Inch Inside Diameter||38.5||Stoves, Central Heaters|\n|8 Inch Inside Diameter||50.3||Stoves, Small Fireplaces, Central Heaters|\n|10 Inch Inside Dia.||78.6||Stoves*, Fireplaces, Central Heaters|\n|12 Inch Inside Dia.||113||Fireplaces, Central Heaters|\n*only rare older stoves use 10"" chimneys\nOne thing that we notice from this table is that a 6"" flue is almost 1/2 the size of an 8"" flue. The natural tendency might be to think that a 6"" flue is ""only"" two inches smaller that an 8"" one, but in truth you can see the difference is much larger.\n|Flue Size (O.D.)||Square Inches||Common Venting Uses|\n|7 1/2 X 7 1/2||31||Stoves, Central Heaters|\n|8 1/2 x 8 1/2||41||Stoves, Central Heaters|\n|8 1/2 X 13||70||Stoves, Fireplaces, Central Heaters|\n|13 X 13||99||Fireplaces|\n|13 X 18||156||Fireplaces|\nSince smoke rises in a circular motion, only a certain area of a square flue tile is actually being used effectively. The listings in the table below refer to the usable area.\nThe benefit of a square flue is it takes a little longer for creosote to build up, thus prolonging the need for a chimney sweep.\nThe chimney cap should be designed to keep moisture from entering the system. Caps should not be constructed as a mortar wash, which is simply parging the surface with mortar. If the masonry chimney cap is constructed with mortar only it will crack severely because mortar is designed to hold at a maximum of three quarters of an inch and not in mass amounts, it has no aggregate to hold the cement together, only sand which is not substantial enough to get any strength out of it. The chimney cap should be constructed with cast in place concrete, precast concrete or stone. The chimney cap should be sloped away from the flue to direct water out of harms way. If water gets between the flue and the structural masonry, this can cause efflorescence in high humidity areas and it can also freeze, simply prying your cap off your chimney. The cap should overhang the chimney wall at least two inches and should have a drip edge cast into the overhang. Rain water is now directed off of the top of the chimney and drips off of the overhang.\nBecause the concrete cap and clay flue liner are different materials having different thermal expansion characteristics, there should be a void left between the cap and the flue. This void should be filled with a compressible material and caulked. This allows both materials to move independently, while the sealant keeps moisture out.\nAll of the measures discussed above are focused on keeping water out of the chimney. Although these measures can protect the chimney from moisture, they are not foolproof. Some water will still find it\'s way past the cap. For this reason, through wall flashing should be placed under the cap as a second line of defense against moisture penetration. The flashing is adhered to the flue liner, extends horizontally under the cap and is extended beyond the face of the wall. If necessary provide weep vents at base of chimney weeping out on the roof using a # 10 cotton wick, laying it in the bed joint, you can get this at most any hardware store. (Technical Notes on Masonry Chimney Construction Construction, by masonryworktools.com an ever growing masonry information site to help you succeed in your masonry venture.)']"	['<urn:uuid:f2711435-efe1-46e2-be3f-3022c3ed7c2e>', '<urn:uuid:389f34c0-0a37-4730-af5f-31dcab0ada96>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	6	107	2040
17	disney company founding date compared to db cooper hijacking northwest airlines flight 305	The Disney Company began on October 16, 1923, when Walt Disney contracted with M.J. Winkler to distribute the Alice Comedies, while the D.B. Cooper hijacking of Northwest Airlines Flight 305 occurred much later, on November 24, 1971.	['On November 24, 1971, a man perpetrated an incredible crime that still has law-enforcement officials puzzled.\nThe man, who went by the name D.B. Cooper, boarded Northwest Airlines Flight 305, flying from Portland, Oregon, to Seattle, Washington. After the plane took off, Cooper told flight attendants his briefcase had a bomb in it. The plane, crew, and 36 passengers were hijacked.\nCooper requested the plane land in Seattle, where he was to receive $200,000 and a parachute. The airline complied with his demands. After Cooper received his ransom, he ordered the plane to fly slowly and at a low altitude toward Mexico.\nThen, somewhere over the forests of the Pacific Northwest, Cooper jumped into the ink-black night, never to be seen again.\nThe only evidence of the crime was discovered in 1980, when a boy found $5,800 of the ransom money on a sandbar in the middle of the Columbia River.\nThough the FBI investigation into the crime is still open, the agency is reluctant to spend resources on a decades-old mystery. So today, “citizen sleuths” with electron microscopes are trying to crack the case.\nThe sleuths began their investigation in 2007, when FBI Seattle Special Agent Larry Carr decided to release information about the crime to the public.\nTom Kaye was on the case. Kaye is not a detective or forensic investigator, however. He’s a paleontologist and associate researcher at the Burke Museum of Natural History and Culture in Seattle. Kaye’s team of citizen sleuths included metallurgist Alan Stone and scientific illustrator Carol Abraczinskas.\nThe key tool that Kaye and his team employed was an electron microscope, which uses electrons to create a higher magnification than a regular microscope. Electron microscopes had never been used to examine physical evidence from the Cooper case.\n“In 1971, when Cooper jumped out of the airplane, the FBI didn’t even have an electron microscope,” Kaye says.\nKaye and his team used electron microscopy to analyze the money found on the sandbar. Since its discovery, the money had turned black.\n“There was a debate about whether or not the money had been buried there since day one,” Kaye says. “Or had the money been tumbling down the river for four or five years and then got washed up on the beach by a dredge?”\nWhen Kaye looked at the bills with his electron microscope, he discovered something surprising.\n“We started investigating the black coloration of the bills, and the electron microscope told us it was made of silver,” he says. “This is very unusual to find silver on a bill like that.”\nBut the silver lining turned out to be a dead end for the citizen sleuths.\n“At the end of the investigation, what we found was the FBI used silver chloride to look for fingerprints on the bill,” Kaye says.\nThe FBI was impressed with the work of Kaye and his team, however.\nSo the agency allowed the citizen sleuths to examine a tie, found on the hijacked plane, that most likely belonged to the criminal. Kaye says the tie had been in an FBI office for 40 years.\n“What we were looking for was pollen, because pollen is everywhere,” he says. “The tie is the most ideal thing for him to leave on a plane, because you don’t wash a tie. So we knew that the tie had accumulated particles from everywhere he had been. If we could find pollen on his tie, depending on what pollen we found it could tell us what part of the country he was from.”\nOnce again, Kaye and his team used the electron microscope. Once again, they found something surprising.\n“What usually happens in these cases is that you find out the unexpected,” Kaye says. “We found these metal particles on the tie, and one or two of them ended up being titanium, which is very unusual because titanium was not very popular in ’71. And it’s not the type of thing that would have gotten on the tie since 1971 in the FBI lock-up.”\nTitanium was used in the aircraft industry at the time, and the discovery led Kaye and his team to think that D.B. Cooper may have worked somewhere where aircraft were built. But aircraft were built with titanium alloys, and the team had concluded that the substance on the tie was pure titanium.\n“That led us down the road of who uses pure titanium,” Kaye says. “That could come from one or two places. Either the plant that manufactures it, or at that time they were using titanium in chemistry plants.”\nUsing the electron microscope to further investigate the tie, Kaye and his team discovered a chip of aluminum and a piece of stainless steel. Those finds allowed the citizen sleuths to narrow their case.\n“There’s a second piece of evidence pointing toward a chemical plant,” Kaye says.\nThe team’s discoveries led them to think Cooper probably worked as a manager or engineer in a metalworking plant that processed titanium, or as an employee at a chemical plant.\nMore importantly, the discoveries made the team doubt that anyone on the FBI’s current list of suspects is D.B. Cooper.\n“We don’t feel that any of the current crop of suspects have anything to do with the crime,” Kaye says.\nIt’s difficult to know if the D.B. Cooper case will ever be solved. Many think Cooper could not have survived his parachute jump into the Pacific Northwest wilderness. If he did survive, the criminal is most likely in his eighties.\nKaye thinks science and time will help close the case.\n“Maybe a hydrologist can use the latest technology to trace the $5,800 in ransom money found in 1980 to where Cooper landed upstream,” he says in an FBI article. “Or maybe someone just remembers that odd uncle.”\n“We are hoping that when he dies,” Kaye says, “someone who knows something about him comes forward.”\nMost people don’t own electron microscopes. Many researchers get to use a university’s electron microscope for just an hour or two. Tom Kaye bought his electron microscope on eBay, and he said it has been a very worthwhile investment.\n“It’s been absolutely beyond words worth it,” he says. “You can’t imagine the world you can investigate with an electron microscope. You go to a place where no one has gone before.”\nto gather or collect.\nmixture of two or more metals.\nthe distance above sea level.\nstudy of the atoms and molecules that make up different substances.\nto put together.\nto collect and use work contributed by the public, usually through an internet-driven campaign, toward a specific project or goal.\nto argue or disagree in a formal setting.\nto remove sand, silt, or other material from the bottom of a body of water.\nnegatively charged subatomic particle.\npowerful device that uses electrons, not light, to magnify an image.\nperson who plans the building of things, such as structures (construction engineer) or substances (chemical engineer).\ndata that can be measured, observed, examined, and analyzed to support a conclusion.\nperson who works with law-enforcement agencies to solve crimes, often using science and technology.\necosystem filled with trees and underbrush.\nto steal a transportation vessel, such as a truck or plane, or the cargo it is carrying.\nperson who studies the distribution, circulation, and properties of water.\nto make or produce a good, usually for sale.\nperson who studies the properties of metals and the technologies used in extracting them from ores, refining them for use, and creating alloys and useful objects from them.\nperson who studies fossils and life from early geologic periods.\nsmall piece of material.\nto commit or engage in.\npowdery material produced by plants, each grain of which contains a male gamete capable of fertilizing a female ovule.\nfee associated with the release or return of property.\nunderwater or low-lying mound of sand formed by tides, waves, or currents.\nwhite, granular compound (AgCl) that darkens on exposure to light, often used in fingerprinting and photo-processing.\ndetective or investigator.\nmetal that is very resistant to rust.\nthe science of using tools and complex machines to make human life easier or more profitable.\nchemical element with the symbol Ti.\ntoward an elevated part of a flow of fluid, or place where the fluid passed earlier.\nenvironment that has remained essentially undisturbed by human activity.', 'History of 16 October\n1916 – Margaret Sanger opened the first birth control clinic in New York City, NY.\n1923 – Walt Disney contracted with M.J. Winkler to distribute the Alice Comedies. This event is recognized as the start of the Disney Company.\nDisney movies, music, and books\n1928 – Marvin Pipkin received a patent for the frosted electric light bulb.\n1939 – “Right To Happiness” debuted on the NBC-Blue network.\n1939 – “The Man Who Came to Dinner” opened on Broadway.\n1941 – The Nazis advanced to within 60 miles of Moscow. Romanians entered Odesa, USSR, and began exterminating 150,000 Jews.\n1942 – The ballet “Rodeo” premiered in New York City.\n1943 – Chicago’s new subway system was officially opened with a ribbon-cutting ceremony.\n1944 – “The Robe,” by Lloyd Douglas, was published for the first time.\n1945 – “His Honor the Barber” debuted on NBC Radio.\n1955 – Mrs. Jules Lederer replaced Ruth Crowley in newspapers using the name Ann Landers.\n1962 – U.S. President Kennedy was informed that there were missile bases in Cuba, beginning the Cuban missile crisis.\n1964 – China detonated its first atomic bomb becoming the world’s fifth nuclear power.\n1967 – NATO headquarters opened in Brussels.\n1970 – Anwar Sadat was elected president of Egypt to succeed in Gamal Abdel Nassar.\n1973 – Henry Kissinger and Le Duc Tho were named winners of the Nobel Peace Prize. The Vietnamese official declined the award.\n1978 – Poland’s Karol Josef Wojtyla was elected Pope John Paul II.\n1982 – China announced that it had successfully fired a ballistic missile from a submarine.\n1987 – Rescuers freed Jessica McClure from the abandoned well that she had fallen into in Midland, TX. She was trapped for 58 hours.\n1989 – U.S. President George H.W. Bush signed the Gramm-Rudman budget reduction law that ordered federal programs to be cut by $16.1 billion.\n1990 – Comedian Steve Martin and his wife Victoria Tennant visited U.S. soldiers in Saudi Arabia.\n1993 – The U.N. Security Council approved the deployment of U.S. warships to enforce a blockade on Haiti to increase pressure on the controlling military leaders.\n1994 – German Chancellor Helmut Kohl was re-elected to a fourth term.\n1995 – The “Million Man March” took place in Washington, DC.\n1997 – Charles M. Schulz and his wife Jeannie announced that they would give $1 million toward the construction of a D-Day memorial to be placed in Virginia.\n2000 – It was announced that Chevron Corp. would be buying Texaco Inc. for $35 billion. The combined company was called Chevron Texaco Corp. and became the 4th largest oil company in the world.\n2002 – It was reported that North Korea had told the U.S. that it had a secret nuclear weapons program in violation of a 1994 agreement with the U.S.\n2002 – The Arthur Andersen accounting firm was sentenced to five years probation and fined $500,000 for obstructing a federeal investigation of the energy company Enron.\n2008 – The iTunes Music Store reached 200 billion television episodes sold.\n2013 – Lao Airlines Flight 301 crashes on approach to Pakse International Airport in Laos, killing 49 people.\n2017 – Storm Ophelia strikes the U.K. and Ireland causing major damage and power loss.\nCelebrating Birthday Today\n- 1981 – Brea Grant, American actress, and writer\n- 1981 – Martin Halle, Danish footballer\n- 1981 – Boyd Melson, American boxer\n- 1981 – Anthony Reyes, American baseball player\n- 1982 – Frédéric Michalak, French rugby player\n- 1982 – Cristian Riveros, Paraguayan footballer\n- 1982 – Prithviraj Sukumaran, Indian actor, singer, and producer\n- 1983 – Philipp Kohlschreiber, a German tennis player\n- 1983 – Kenny Omega, Canadian wrestler\n- 1984 – François Pervis, French track cyclist\n- 1984 – Rachel Reilly, American talk show host, and actress\n- 1985 – Jay Beagle, Canadian ice hockey player\n- 1985 – Verena Sailer, German sprinter\n- 1985 – Casey Stoner, Australian motorcycle racer\n- 1985 – Peter Wallace, Australian rugby league player\n- 1986 – Nicky Adams, English-Welsh footballer\n- 1986 – Derk Boerrigter, Dutch footballer\n- 1988 – Zoltán Stieber, Hungarian footballer\n- 1989 – Dan Biggar, Welsh rugby player\n- 1992 – Bryce Harper, American baseball player\n- 1992 – Kostas Fortounis, Greek footballer\n- 1992 – Stuart Lightbody, Irish badminton player\n- 1993 – Caroline Garcia, a French tennis player\n- 1994 – Adam Elliott, Australian rugby league player\n- 1997 – Charles Leclerc, Monégasque Formula One driver\n- 1997 – Naomi Osaka, Haitian-Japanese tennis player']	['<urn:uuid:049cd102-ddff-4e39-9f43-9aa1a9507ba8>', '<urn:uuid:962c891e-2fe5-4c1b-864d-be540f49bbd8>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	13	37	2139
18	What are the characteristics of mediation in preserving business relationships, and how does this connect to different approaches for structuring a business divorce?	Mediation is designed to preserve or strengthen relationships between parties by finding mutually beneficial solutions through a mediator who helps identify common ground, making it ideal for businesses wanting to maintain future relationships. This aligns with various business divorce options, including voluntary sale of the business, buyout arrangements, or dissolution, where early non-litigation resolution allows owners to maintain control and potentially preserve business value, similar to mediation's goal of reaching magnanimous agreements.	"[""Arbitration vs. Mediation -- What's the Difference?\nThe legal definition of mediation does not differ far from the common colloquial understanding of the term.\nMediation, in many cases, is performed between two parties that are still on stable terms with one another. Mediation's primary purpose is to further enlighten each party of perceived injustices, and also allow a third party (the mediator) to gives it opinion on those injustices. The mediator then helps find common ground between both parties, and attempts to broker a deal between both parties that can then be willingly, and hopefully amicably, carried out. After mediation, both parties must agree to the results of the mediation and can only willingly fulfill the terms of the mediated agreement.\nArbitration is a process of conflict dispute that focuses again around a neutral third party, this time known as an arbitrator. Arbitration looks not to find a comfortable middle ground between the conflicted parties, but instead looks to lay find its own decision - known as a judgement - that will end the dispute between both parties. Arbitrators are often used in situations where the relationship between both parties is tenuous, and an amicable agreement seems out of reach.\nThey key portion of arbitration is that the arbitrator, the neutral third party that will make the judgement in a dispute, will give a decision that is considered legally binding. The arbitrator is more of a judge than a facilitator, utilizing their knowledge and position to effectively act as a judge. The major difference between court and arbitration is that the arbitrator is agreed upon by both parties, and in most cases, the process of arbitration is significantly less costly and time consuming than a court process.\nWhich to use?\nBoth systems of resolution have their merits, but are better suited to separate situations. Mediators are often called upon to find mutually beneficial and agreeable results, ending with a favorable decision for both parties that can preserve or even strengthen the relationship between the two parties being mediated. A business and a client who are having a disagreement, but would like to continue to work in the future, can utilize a mediator to try to find an acceptable solution - a solution that they can then reject if they find unfair.\nArbitration, on the other hand, has the same finality of court proceedings and can render decisions that feature the same animosity and disdain as losing a court case. The arbitrator lays down a decision that they think is fair, not that is necessarily mutually beneficial or believed fair by either party. If a business does not mind burning a bridge, has a strong case, and does not care about the negative effects of being known for ‘beating' clients, arbitration may be a viable option. However, businesses that are looking not just for future relationships with a disputed party but also for being known for reaching magnanimous and acceptable agreements with their clients - mediation may be the high road that ends with satisfaction (and future business) for all parties involved.\nBy Travis Lindsay"", 'Relationships between business partners can go sour, to the point where the co-owners of a business are unable to work together and need to separate. Like a marital divorce, sometimes this is a mutual breakup, while in other instances, one owner may feel slighted by his or her former partner or partners. In either case, it is generally preferred for the parties to settle their disputes without litigation, to avoid the expense of a drawn-out lawsuit. The best way to accomplish that is to start the resolution process early and understand the available options.\nWhat Can Cause Disputes Between Business Partners?\nThere are many reasons conflicts arise among business partners. Such disputes may involve the following:\n- Management of the company. Owners may disagree over day-to-day operations, investments, allocation of resources and decision-making authority.\n- Compensation. When owners are also employees of the business, compensation claims can arise based on an employment contract or other agreement.\n- Breach of fiduciary duty. Generally, owners owe fiduciary duties to each other and to the business. These include a duty of care, duty of loyalty and duty of candor. Breaches may involve allegations that an owner engaged in self-dealing, deception, or improperly usurped a corporate opportunity for himself or herself.\n- Breach of contract. The company’s governing document, such as an LLC’s operating agreement, sets forth how the business will be run and the rights and obligations of the owners. As a result, disagreements over capital contributions, voting rights, decision-making procedures, transferring and selling shares and how profits and losses will be divided, among other issues, often lead to breach of contract claims.\n- Lack of transparency and miscommunication. These disputes may fall under breach of contract or breach of fiduciary duty, where owners are intentionally hiding information. Depending on the relevant state’s corporate laws, interest holders typically have the right to inspect the company’s books and records and may be able to get an accounting.\n- Minority vs. majority shareholder rights. Minority shareholders often have some protection against oppressive treatment by majority shareholders under state law. However, this varies from state to state.\nHow Can You Structure a Business Divorce?\nVarious options exist for a business divorce, short of litigation. The first step is always to check the company’s ownership or operating agreement regarding procedures and alternatives for resolving disputes. Some common ones include:\nSale of the Business\nIf it seems likely that the business will not survive the conflict, it may be best to sell the company or its assets and split the proceeds from the sale. By taking this step early and without involving the courts, the owners remain in control of the sale. When owners resort to judicial dissolution, the court may appoint a receiver to manage the business and sale process, which may result in less favorable terms.\nThe company, shareholders or a third party can try to buy out the interests of shareholders involved in the dispute. Typically, the company’s governing document will set forth when owners or the business can buyout an owner’s interest or force out an owner, as well as how the owner’s interest should be valued.\nA dissolution is voluntary if all of the owners agree. However, if some do not want dissolution, it can be involuntary and the owner requesting dissolution must get a court order dissolving the business and dividing its assets and debts. In either case, owners must follow state law dissolution procedures applicable to their type of company (i.e., corporation, LLC, partnership) as well as any relevant provisions in the company’s ownership agreement. For example, New York’s Corporation Law and LLC law establish default procedures in the absence of any terms in the shareholder agreement or operating agreement. It is important to follow these rules to avoid further conflict and litigation with co-owners.\nOnce the process is begun, companies should also notify employees, creditors, lenders, suppliers, vendors and service providers that the company is dissolving and begin to wind up the business. Where a dissolution is involuntary, a receiver may be appointed by the court to manage the business until the dissolution is final.\nThe business may also be required to file with state, federal and local agencies. Even if the business does not have to file, it is a good practice as it puts creditors on notice and protects the company and owners from any debts or obligations that may be incurred after dissolution.\nWhat Issues Must Be Resolved in Settlement?\nSettlement terms will vary depending on whether the company will continue to operate. If the company is dissolving, the parties must decide how to split assets and liabilities among the owners. Where the business will continue to exist, the core issue is typically the buyout price paid to the departing owner(s). However, as noted above, the operating or ownership agreement may set forth procedures and mechanisms for determining these issues. Absent that, state law will govern.\nConflicts are inevitable in any business relationship. Where it results in a business divorce, the key concern of the owners should be to resolve the issues efficiently, so it does not damage the value of the business.\nLooking for other Business Law services?']"	['<urn:uuid:59aaea00-aebe-4ceb-9efc-e51957a550f9>', '<urn:uuid:afa9fdac-41cc-442b-8e05-30af040456f9>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	23	72	1376
19	I've been studying commercial vehicle powertrains and their efficiency - how do hybrid trucks manage power generation during operation, and what strategies can be employed to minimize their overall fuel consumption?	Hybrid trucks like the DAF CF utilize a smart energy management system where the electric motor functions as a generator during diesel operation to recharge the 85 kWh battery pack. The system captures energy from engine braking and Down Hill Speed Control to support the diesel engine, reducing fuel consumption and CO2 emissions. To minimize overall fuel consumption, several strategies can be implemented: avoiding unnecessary engine idling (which can consume up to 0.9 gallons per hour), using auxiliary power units (APUs) instead of PTO-driven systems (which use 34% more fuel), and reducing vehicle weight (every 100 pounds reduces miles per gallon by 1%). Additionally, equipment that automatically shuts off when not in use and matches engine output to load can further optimize fuel efficiency.	['DAF starts field test with CF Hybrid\nZero emissions in the city and long range outside urban areas\nDAF Trucks has started field testing the CF Hybrid with the aim of gaining experience in daily use. The Dutch transport operator Peter Appel is now using two of these innovative trucks to supply supermarkets in the heart of the Netherlands. The DAF CF Hybrid is 100% electric in urban areas and uses clean diesel technology out of town. The innovation truck combines best of the both worlds by driving with ‘zero emissions’ in town, thereby ensuring both long range and flexibility outside urban areas.\nThe DAF CF Hybrid innovation trucks are equipped with the extremely efficient 10.8 litre PACCAR MX-11 diesel engine (330 kW/450 hp), a ZF electric motor\n(75 kW/100 hp, peak power: 130 kW/175 hp) and a special ZF TraXon gearbox for hybrid powertrains.\nRecharging while driving or at charging station\nThe electric motor gets its energy from a 85 kWh battery pack, which recharges when the diesel engine is being used. During diesel operation the electric motor functions as a generator and delivers energy to the battery pack. In the future it will also be possible to charge the battery at a (fast) charging station.\nWhen the battery is fully charged, the DAF CF Hybrid –– has an electric range of 30 to 50 kilometres, depending on the total weight of the truck-trailer combination, which is more than enough to drive into and out of urban areas without producing any tailpipe emissions.\nSmart energy management\nOutside of town the CF Hybrid is powered by the clean and efficient PACCAR MX-11 diesel engine, which offers a long range. In addition, the hybrid technology delivers extra savings on fuel consumption thanks to smart energy management. The energy that is generated by the engine brake as well as Down Hill Speed Control is used by the electric motor to support the diesel engine. This is beneficial both in terms of fuel consumption and CO2 emissions.\nSuitable for longer journeys\n“Together with our client, Albert Heijn, we are always looking for ways to reduce our CO2 footprint,” says Marcel Pater, fleet manager at Peter Appel Transport, which boasts a total of 680 trucks. “While the electric trucks are ideal for urban distribution, the hybrid technology is preferable for longer routes. In town the DAF CF Hybrid operates fully electric, and the truck’s diesel engine means it can travel to and from the different distribution centres within the Netherlands and abroad without any problems.”\nDaily use by clients\n“With a cleaner future in mind, for DAF there is no one single technological solution for the broad spectrum of transport requirements,” according to Ron Borsboom, executive director of Product Development. “That’s why we are testing different technologies. Fully electric is a good alternative for urban distribution, clean diesel technology an excellent option for longer distances – partly due to new types of fuel – and for the long term we are having a closer look at hydrogen as well. With the field test of the CF Hybrid, we want to assess not only its electric/diesel technology performance but also how suitable it is in terms of daily use by our clients.”', 'Fleet fuel management\nThe cost of fuel is one of the most significant operating expenses for mobile service truck fleets, in some cases comprising more than half of the budget. The larger the fleet, the faster your fuel costs add up.\nYou can’t control the price of fuel, but as fleet manager you can make sure your service technicians and trucks use fuel more efficiently. Fleets of all types and sizes can reap the benefits when they choose solutions and technologies that reduce fuel usage. Stop wasting fuel with these five tips for improving efficiency and reducing spending in your service truck fleet.\n1. Pay attention to preventive maintenance\nMaintenance is an important part of keeping your truck running optimally and efficiently. Cleaning the air filters and diesel particulate filters (DPF) regularly, changing engine oil and keeping tire pressure at recommended levels all play a role in fuel economy. For example, under-inflated tires can reduce gas mileage by about 0.2% for every 1-psi drop in the average pressure of all tires. A dirty DPF increases the need for greater engine output that burns more fuel. DPF-related increases in fuel consumption can range from 4.5% to 7%. The bottom line? A well-maintained truck is a more efficient truck.\n2. Stop unnecessary truck engine idling\nTurning off truck engines on the jobsite can deliver significant fuel savings. Running these high-horsepower engines to drive your air, hydraulic or other power needs is extremely inefficient — decreasing the truck’s overall fuel economy and eating into profit. For example, engines in these typical truck configurations can burn up to 0.9 gallons of fuel per hour while idling. This may not seem like much, but the extra fuel adds up. A PTO-equipped system uses 34% more fuel on average to supply air, hydraulic and other power needs compared to an APU (auxiliary power unit) system.\n3. Choose truck equipment and technologies wisely\nSelect fuel-efficient equipment and technologies. Using APU-type equipment to run your tools, such as air and hydraulics, is one of the easiest ways to minimize fuel spend. But not all of these systems are created equal. Look for solutions with technologies that maximize their fuel efficiency, such as equipment that automatically shuts off when not in use. Some equipment also matches engine output to the load — so it’s not constantly running at the highest rpm and burning more fuel. These technologies can pay for themselves quickly, usually in less than one year. When specifying the truck, don’t just consider the acquisition cost; it’s also important to think about your long-term ROI.\n4. Lightweight and rightsize your fleet\nReducing vehicle weight does make a difference for fuel economy and savings; every additional 100 pounds on a vehicle reduces the average miles per gallon by 1%, according to the EPA. Using telematics on your fleet and the associated power equipment lets you analyze metrics and review equipment usage data to help ensure you have the right equipment as well as the right size equipment. You should also evaluate if smaller, lighter trucks that use less fuel will work for your fleet and still allow techs to carry the tools and equipment they need to get jobs done efficiently. In addition to using a lower class of truck — a class 5 versus a class 7, for example — consider bodies constructed of alternative materials, which can potentially reduce weight considerably and greatly increase fuel efficiency on the road. Choosing lightweight tools and equipment also helps lower your overall truck weight.\n5. Evaluate and optimize your truck equipment\nAnother way to reduce overall truck weight — and therefore increase fuel efficiency — is to audit the onboard equipment, remove any that is unnecessary or not frequently used, and rightsize what is necessary. Understand whether or not you need a crane and, if so, choose the appropriate size. You also need to know whether you need a welding solution and at what output. Once you have those answers, look for tools and equipment that can be combined to save space and weight on the truck, such as using a single APU-type unit rather than several separate components (like an air compressor, welder/generator and battery charge/jump packs). An all-in-one APU can save up to 550 pounds compared to using a PTO-driven air compressor and an engine-driven welder/generator.']	['<urn:uuid:16fe8cab-5591-4c8e-8560-27bdf9b4bdd7>', '<urn:uuid:7cbef819-06fc-449d-ab4b-6bf0a435be14>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	31	124	1256
20	I cook with turmeric a lot - should I be worried about using too much?	While curcumin (found in turmeric) has beneficial effects at low doses, including reduction of adipose tissue mass and improvement of fatty liver disease, high doses should be avoided. Higher doses of curcumin can increase liver enzymes in short-term studies and can interfere with heat shock proteins, potentially leading to toxicity. The clearance of curcumin from the body is poorly understood, so careful control of intake is recommended.	['Ian James Martins1-3*1Centre of Excellence in Alzheimer’s Disease Research and Care, Sarich Neuroscience Research Institute, Edith Cowan University, Verdun Street, Nedlands, 6009, Western Australia, Australia\n2School of Psychiatry and Clinical Neurosciences, the University of Western Australia, Nedlands, 6009, Australia\n3McCusker Alzheimer’s Research Foundation, Hollywood Medical Centre, 85 Monash Avenue, Suite 22, Nedlands, 6009, Australia\n*Address for Correspondence: Dr. Ian James Martins, School of Medical Sciences, Edith Cowan University, Western Australia 6009, Australia, Tel: +61863042574; Email: firstname.lastname@example.org\nDates: Submitted: 24 January 2018; Approved: 30 January 2018; Published: 31 January 2018\nHow to cite this article: Martins IJ. Indian spices and Caffeine treatment for Obesity and Cardiovascular disease. Ann Clin Endocrinol Metabol. 2018; 2: 010-014. DOI: 10.29328/journal.acem.1001005\nCopyright License: © 2018 Martins IJ. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nKeywords: Neuroendocrine system; Metabolism; Adipose tissue; Liver; Mitphagy; NAFLD; Obesity; Apelin; Sirtuin 1; Nitric oxide; Autoimmune disease; Caffeine; Cardiovascular disease; Curcumin; Cinnamon\nThe global obesity epidemic that was previously reported [1,2] is now to worsen with obesity to double in 73 countries around the world [3,4]. Improving the health of obese individuals by dietary restriction, anti-obese foods and increased physical activity  has not reduced the global obesity epidemic. Obesity is linked to nonalcoholic fatty liver disease (NAFLD) [5,6] with complications relevant to the metabolic syndrome and cardiovascular disease . Appetite control has become critical to endocrinology and metabolism with the apelinergic pathway and nuclear receptor Sirtuin 1 (Sirt 1) now connected to the endocrine system  and critical to metabolism. The apelin-Sirt 1 interaction involves nitric oxide (NO)  that is now considered as the defect  in the interaction between the peptide apelin and calorie sensitive gene Sirt 1 involved in NO imbalances in the adipose tissue, liver and the brain.\nA model proposed involves the central co-ordination of the neuroendocrine system by the suprachiasmatic system in the hypothalamus with the adipose tissue and liver in the periphery . The apelinergic pathway is linked to NO balance between the brain, adipose tissue and liver  with dietary composition (low calorie diets) and reduction of stress  essential to maintain the NO co-ordination of the neuroendocrine system between the brain and peripheral tissues. Nutrigenomic diets are now essential to activate Sirt 1  involved in NO regulation that activates the adipose tissue-liver interaction for the prevention of NAFLD. The crosstalk between the adipose tissue and liver has become of major concern in various communities with adipocyte dysfunction linked to the acceleration of NAFLD . Drugs that lower fat absorption and activate adipose tissue and liver Sirt 1 have been recommended for human use  but the use of other anti-obese drugs have raised concerns .\nThe composition of diets have now become important with adipogenesis a major disorder connected to NAFLD. The reports of obesity to become a pandemic is now related to accelerated aging with mitophagy  the major defect in NAFLD. The adipocyte and its transformation with relevance to apelin-Sirt 1 defects is related to autoimmune disease and linked to mitophagy and NAFLD [9,15-17].\nSirt 1 via peroxisome proliferator-activated receptor gamma coactivator 1-alpha releases apelin  with Sirt 1/ peroxisome proliferator-activated receptor gamma coactivator 1-alpha linked to mitochondrial biogenesis . Sirt 1 mutations have been reported in obesity, diabetes, cardiovascular disease and autoimmune disease [2,19-21]. Sirt 1 is the heat shock gene [22,23] with its repression involved with dysregulation of heat shock protein 70 (HSP70), natural killer cell activation and mitophagy . Defective adipose tissue-liver interaction has been treated with caffeine to prevent mitophagy [25,26] linked to adipocyte dysfunction and reversal of NAFLD . Nutrition therapy that allows co-ordination between the brain, adipose tissue and liver activates the apelinergic-Sirt 1 interaction essential for hepatic caffeine metabolism and caffeine’s critical role as modulation of Sirt 1 in the brain  important to the prevention of obesity.\nThe health promoting benefits and protective role of Indian spices  have been reported in obesity and NAFLD with their role as an antioxidant and antimicrobial agent important to the maintenance of the adipose tissue-liver interaction. Identification of spices such as five commonly used dietary spices include saffron, curcumin, pepper family, zingiber and cinnamon to inhibit amyloid beta aggregation. The curry spice curcumin has been extensively studied in animal models to reduce amyloid pathology with beneficial effects on the prevention of neurodegeneration in man. Curcumin (Figure 1) has been used in various studies to inhibit adipogenesis and decrease adipose tissue mass [30-33]. In the liver curcumin has been shown to stabilize or reverse NAFLD [34-36] but in specific controls curcumin has been shown to increase liver enzymes in short term studies. Indian spices now as novel therapeutic agents are potent in scavenging of NO  with relevance to apelin-Sirt 1 neuroendocrine regulation of NO balance. Curcumin has potent effects on NO synthase [38,39] with regulation of Sirt 1 control of NO synthase in endothelial cell in the liver and heart  (Figure 1). Curcumin is a potent inducer of the heat shock response [40,41] with complete interference of the heat shock gene Sirt 1 regulation of HSP 70. Low dose curcumin ingestion is indicated with other Indian spice consumption and their clearance from the body is poorly understood  with higher dose curcumin involved with increased HSPs associated with amyloid beta aggregation (Figure 1) induced toxicity [40-42].\nCinnamon has now been reported to activate the adipose tissue-liver interaction with cinnamon effects on increased adipocyte lipid and glucose metabolism [43-45]. In the liver cinnamon has been used to stabilize NAFLD [46-48] with its important role as a Sirt 1 activator  connected to insulin therapy  and its maintenance of the adipose-tissue liver interaction. Consumption of spice intake and its quantification (mg/day) in man  vary considerably between countries with cinnamon Sirt 1 activation related to curcumin content with higher doses of curcumin relevant as a Sirt 1 inhibitor. Consumption of Indian spices need to be carefully controlled to avoid induction of obesity by complete nullification of caffeine’s co-ordination of the neuroendocrine system by dysregulation of the brain, adipose tissue and liver interaction. Caffeine and its metabolism is critical to cardiovascular disease [50-52] with Indian spice intake (mg/day) now important with relevance to interference with caffeine’s therapeutic properties in man and various species. Delayed caffeine clearance leads to interference with caffeine’s beneficial effect on adipogenesis with increased transport to the brain relevant to mitochondrial induced apoptosis and the induction of Type 3 diabetes [26-28].\nThe global obesity epidemic that now affects many countries in the world has not improved in spite of dietary restriction, anti-obese foods and increased physical activity. The neuroendocrine system in many obese individuals is defective with an adipose tissue-liver defect that induces NAFLD and cardiovascular disease. Appetite control to reverse NAFLD and obesity has not been successful with revision of dietary composition that may allow activation of the adipose tissue-liver interaction. Indian spices are important to the adipose-tissue interaction but Indian spice intake should be carefully controlled to avoid curcumin toxicity (apelin-Sirt 1 inhibition) associated with defective fat and caffeine metabolism and associated with the induction of autoimmune and neuroendocrine disease.\nThis work was supported by grants from Edith Cowan University, the McCusker Alzheimer’s Research Foundation and the National Health and Medical Research Council.\n- MartinsIJ. Appetite dysregulation and obesity inWestern Countries. LAP LAMBERT Academic Publishing. 2013. Ref.: https://goo.gl/9Q9tnW\n- Martins IJ. Induction ofNAFLD with Increased Risk of Obesity and Chronic Diseases in DevelopedCountries. OJEM Diseases. 2014; 4: 90-110. Ref.: https://goo.gl/uHz7H6\n- An R, Ji M, Zhang S. Global warming and obesity: a systematicreview. Obes Rev. 2018; 19: 150-163. Ref.: https://goo.gl/i9GpEv\n- Friedrich MJ. Global Obesity Epidemic Worsening. JAMA.2017; 318:603. Ref.: https://goo.gl/79iEGw\n- Fabbrini E, Sullivan S,Klein S. Obesity and non-alcoholicfatty liver disease: biochemical, metabolic, and clinical implications. Hepatology.2010; 51: 679-689. Ref.: https://goo.gl/ebJjFZ\n- DietrichP, Hellerbrand C. Non-alcoholic fatty liver disease, obesity and themetabolic syndrome. Best Pract Res Clin Gastroenterol. 2014; 28: 637-653.Ref.: https://goo.gl/ecCzVn\n- Kim SH, Després JP, Koh KK. Obesity and cardiovascular disease:friend or foe? Eur Heart J. 2016; 37: 3560-3568. Ref.: https://goo.gl/4ZjohT\n- Martins IJ. Nutritionaldiets accelerate amyloid beta metabolism and prevent the induction ofchronic diseases and Alzheimer’s disease. J Endocrinol andMetab. Photon. 2015; 1-48.\n- Martins IJ. Apelin andSirtuin 1 dysregulation induce Endocrine and Metabolic Disorders inChronic disease. GJEM. 2017; 1:1. Ref.: https://goo.gl/Z3TmLM\n- MartinsIJ. Apelinergic System Defects with Relevance to Mental Disorders inDiabetes. World J Psychiatry Ment Health Res. 2017; 1: 1001. Ref.: https://goo.gl/q3yoRr\n- Martins IJ. UnhealthyNutrigenomic Diets Accelerate NAFLD and Adiposity in Global communities. JMol Genet Med. 2015; 9: 1-8. Ref.: https://goo.gl/mWnRMX\n- QureshiK, Abrams GA. Metabolic liver disease of obesity and role of adiposetissue in the pathogenesis of nonalcoholic fatty liver disease. World JGastroenterol. 2007; 13: 3540-3553. Ref.: https://goo.gl/56N5AG\n- Martins IJ. Avasimibe andSirt 1 Activators Reverse NAFLD and Obesity. Nov Appro Drug Des Dev. 2017;1: 1-2.\n- AdanRA. Mechanisms underlying current and future anti-obesity drugs. TrendsNeurosci. 2013; 36: 133-40. Ref.: https://goo.gl/1n2Ha4\n- García-RuizC, Baulies A, Mari M, García-Rovés PM, Fernandez-Checa JC. Mitochondrialdysfunction in non-alcoholic fatty liver disease and insulin resistance:cause or consequence?Free Radic Res. 2013; 47: 854-868.Ref.: https://goo.gl/iwfFy7\n- Martins IJ. Defective Interplaybetween Adipose Tissue and Immune System Induces Non Alcoholic Fatty LiverDisease. Updates Nutr Disorders Ther. 2017; 1: 3.1. Ref.: https://goo.gl/YQSnyd\n- Martins IJ. Autoimmunedisease and mitochondrial dysfunction in chronic diseases.Res Chron Dis.2017; 1: 10-12. Ref.: https://goo.gl/LXak5D\n- MazzucotelliA, Ribet C, Castan-Laurell I, Daviaud D, Guigné C, et al. Thetranscriptional co-activator PGC-1alpha up regulates apelin in human andmouse adipocytes. Regul Pept. 2008; 150: 33-37. Ref.: https://goo.gl/rJtdLa\n- KilicU, Gok O, Bacaksiz A, Izmirli M, Elibol-Can B, et al. SIRT1gene polymorphisms affect the protein expression in cardiovasculardiseases. PLoS One. 2014; 9: 90428. Ref.: https://goo.gl/3Y8B9Y\n- Biason-Lauber A, Böni-Schnetzler M, HubbardBP, Bouzakri K, Brunner A, etal. Identification of a SIRT1Mutation in a Family with Type 1 Diabetes. Cell Metab. 2013; 17: 448-455.Ref.: https://goo.gl/xjz2Qm\n- Hughes JW, Herold KC. Novel SIRT1 MutationLinked to Autoimmune Diabetes in Humans. Cell Metab. 2013; 17: 311-312. Ref.: https://goo.gl/NQ5rtv\n- MartinsIJ. Type 3 diabetes with links to NAFLD and Other Chronic Diseases in theWestern World. Int J Diab and Metab Disord. 2016; 1: 1-5. Ref.: https://goo.gl/hGqXFe\n- Martins IJ. Heat shock gene Sirtuin 1 regulatespost-prandial lipid metabolism with relevance to nutrition and appetiteregulation in diabetes. Int J Diab Clin Diagn. 2016; 3: 1-3. Ref.: https://goo.gl/WdzCgC\n- Martins IJ. Regulation ofCore Body Temperature and the Immune System Determines Species Longevity.Curr Updates Gerontol. 2017; 1: 6. Ref.: https://goo.gl/y3tCqE\n- Martins IJ. CaffeineConsumption and Induction of Obesity in the Developed World. Ann ObesDisord. 2017; 2: 1018. https://goo.gl/km1Unh\n- Martins IJ. NutritionTherapy Regulates Caffeine Metabolism with Relevance to NAFLD andInduction of Type 3 Diabetes. J Diabetes Metab Disord. 2017; 4: 019. Ref.: https://goo.gl/ukJA55\n- MartinsIJ. Caffeine with Links to NAFLDand Accelerated Brain Aging. In Tech Open. 2017. Ref.: https://goo.gl/8dZsGD\n- Martins IJ. Caffeineconsumption with Relevance to Type 3 diabetes and accelerated brain aging.Res Reveiws: Neurosci. 2016; 1: 1-5. Ref.: https://goo.gl/qbUzY8\n- Martins IJ. Indian Spicesand Unhealthy Diets interfere with Drug Therapy in Diabetes andNeurodegenerative Diseases. Nov Appro Drug Des Dev. 2018; 3: 1-4. Ref.: https://goo.gl/H5YZN4\n- EjazA, Wu D, Kwan P, Meydani M. Curcumin inhibits adipogenesis in 3T3-L1adipocytes and angiogenesis and obesity in C57/BL mice. J Nutr. 2009; 139:919-925. Ref.: https://goo.gl/XspCuq\n- TeichT, Pivovarov JA, Porras DP, Dunford EC, Riddell MC. Curcumin limits weightgain, adipose tissue growth, and glucose intolerance following thecessation of exercise and caloric restriction in rats. J Appl Physiol(1985). 2017; 123: 1625-1634. Ref.: https://goo.gl/djjGKY\n- WangS, Wang X, Ye Z, Xu C, Zhang M, et al. Curcumin promotes browning of whiteadipose tissue in a norepinephrine-dependent way. Biochem Biophys ResCommun. 2015; 466: 247-253. Ref.: https://goo.gl/SuiirT\n- BradfordPG. Curcumin and obesity.Biofactors. 2013; 39: 78-87. Ref.: https://goo.gl/wkqhdv\n- InzaugaratME, De Matteo E, Baz P, Lucero D, García CC, et al. New evidence for thetherapeutic potential of curcumin to treat nonalcoholic fatty liverdisease in humans. PLoS ONE. 2017; 12: e0172900. Ref.: https://goo.gl/C5CdXU\n- PanahiY, Kianpour P, Mohtashami R, Jafari R, Simental-Mendía LE, et al. Efficacyand Safety of Phytosomal Curcumin in Non-Alcoholic Fatty Liver Disease: ARandomized Controlled Trial. Drug Res (Stuttg). 2017; 67: 244-251. Ref.: https://goo.gl/QzbkiS\n- RahmaniS, Asgary S, Askari G, Keshvari M, Hatamipour M, et al.Treatment of Non-alcoholic Fatty Liver Disease with Curcumin: A RandomizedPlacebo-controlled Trial. Phytother Res. 2016; 30: 1540-1548. Ref.: https://goo.gl/oBQ7j7\n- BaligaMS, Jagetia GC, Rao SK, Babu K. Evaluation of nitric oxide scavengingactivity of certain spices in vitro: a preliminary study. Nahrung. 2003; 47:261-264. Ref.: https://goo.gl/97anfL\n- KalaivaniP, Saranya RB, Ramakrishnan G, Ranju V, Sathiya S, et al. Cuminum cyminum,a dietary spice, attenuates hypertension via endothelial nitric oxidesynthase and NO pathway in renovascular hypertensive rats. Clin ExpHypertens. 2013; 35: 534-542. Ref.: https://goo.gl/R5V2P9\n- Nakatake R, Hishikawa H,Matsuhima H, Nakamura Y, Ishizaki M, et al. Curcumin protects liverinflammation by suppressing expression of inducible nitric oxide synthasein primary cultured rat hepatocytes. Functional Foods in Health andDisease. 2017; 7: 716-734. Ref.: https://goo.gl/CyxUaA\n- CalabreseV, Scapagnini G, Colombrita C, Ravagna A, Pennisi G, et al. Redoxregulation of heat shock protein expression in aging and neurodegenerativedisorders associated with oxidative stress: a nutritional approach. AminoAcids. 2003; 25: 437-444. Ref.: https://goo.gl/kZ3LvS\n- MaitP, Manna J. Activation of Heat Shock Proteins by Nanocurcumin to PreventNeurodegenerative Diseases. Brain Disord Ther. 2014; 3:139. Ref.: https://goo.gl/vSsgkU\n- Martins IJ. CalorieSensitive Anti-Aging Gene Regulates Hepatic Amyloid Beta Clearance inDiabetes and Neurodegenerative Diseases. EC Nutrition ECO. 2017; 01: 30-32.Ref.: https://goo.gl/FswPeB\n- ShatwanIA, Ahmed LA, Badkook MM. Effect of barley flour, crude cinnamon, andtheir combination on glycemia, dyslipidemia, and adipose tissue hormonesin type 2 diabetic rats. J Med Food. 2013; 16: 656-662. Ref.: https://goo.gl/uvZt6j\n- LopesBP, Gaique TG, Souza LL, Paula GS, Kluck GE, et al. Cinnamon extractimproves the body composition and attenuates lipogenic processes in theliver and adipose tissue of rats. Food Funct. 2015; 6: 3257-3265. Ref.: https://goo.gl/5bCMBm\n- Kwan HY, Wu J, Su T, ChaoX-C, Liu B, et al. Cinnamon induces browning in subcutaneous adipocytes.Scientific Reports. 2017; 7: 2447. Ref.: https://goo.gl/NGzgYy\n- AskariF, Rashidkhani B, Hekmatdoost A. Cinnamon may have therapeutic benefits onlipid profile, liver enzymes, insulin resistance, and high-sensitivityC-reactive protein in nonalcoholic fatty liver disease patients. Nutr Res.2014; 34: 143-148. Ref.: https://goo.gl/hCkdx7\n- Hekmatdoost A. Prevention of Nonalcoholic FattyLiver Disease (NAFLD) Progression to Nonalcoholic Steatohepatitis (NASH)by Modification of Lifestyle and Dietary Supplements. J Clin Nutr Diet.2016; 2: 2. Ref.: https://goo.gl/rRi7VN\n- EslamparastT, Eghtesad S, Poustchi H, Hekmatdoost A. Recent advances in dietarysupplementation, in treating non-alcoholic fatty liver disease. World J Hepatol. 2015; 7:204-212. Ref.: https://goo.gl/C6dmCa\n- Martins IJ. Insulin TherapyInactivation is connected to NAFLD and Diabetes Severity Index. J DiabClin Stud. 2017; 1: 001-003. Ref.: https://goo.gl/zXAMW3\n- WilsonPW, Bloom HL. Caffeine Consumption and Cardiovascular Risks: Little Causefor Concern. J Am Heart Assoc. 2016; 5: 003089. Ref.: Ref.: https://goo.gl/mQymj7\n- DingM, Bhupathiraju SN, Satija A, van Dam RM, Hu FB. Long-term coffeeconsumption and risk of cardiovascular disease: a systematic review and adose-response meta-analysis of prospective cohort studies. Circulation.2014; 129: 643-659. Ref.: https://goo.gl/eY1FgP\n- Martins IJ. Genomic Medicine and Acute Cardiovascular Disease Progression. Res Chron Dis. 2018.']	['<urn:uuid:9bbb5555-cd0e-48c1-bf44-70c75df11770>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	15	67	2479
21	compare role significance religious faith cemetery locations new england church doctrine liberty politics 1800s	Religious faith played significant but different roles in cemeteries and politics in 1800s New England. In terms of cemeteries, they were seen as sacred spaces connecting earthly and heavenly realms, as shown by Nellie Smith's view of the Kinne Brook cemetery as hallowed ground for transitioning from earthly to heavenly state. The cemetery held deep religious meaning, with about a quarter of its 57 graves belonging to devout Protestant families. Regarding politics and liberty, religious faith was foundational but operated differently - it helped advance modern principles of liberty and self-government, with evangelical religion playing a large role in promoting these concepts. However, this did not create a theocracy, as by 1832 religious establishment had ended in American states, showing how liberty itself became emancipated from mere religious custom while still being influenced by Protestant dissent working alongside Enlightenment rationalism.	['Large sympathy must inform a reading of The Language of Liberty, for its heterodox reading of the substance of the American Revolution strains against its very helpful survey of the rhetoric of the Revolution. That the American revolutionaries were, in the main, a religious and not a secular people is obvious and beyond cavil, despite the impression conveyed by secular historians of later eras. Clark conveys this suitably and accurately, justifying throughout the book his conclusion that “democracy–in the sense of debates over the franchise, the distribution of seats, or the representative machinery in general–was not central to the conflict which rent the English-speaking world in the early-modern period, and was not at the heart of the self-image of any of the societies which made up that world. Its key term had been not ‘democracy’ but ‘liberty,’ and liberty was a term which had its ramifications chiefly in the vast intellectual territories then occupied by law and religion.” This largely accurate (leaving aside the inappropriate separation of law and politics), general view of the Anglo-American socio-political development veers off course, however, when Clark seeks to localize it in the United States. His argument presupposes Americans who could not think around more than one important question at a time. Thus, he jettisons their legitimate interests in constitutional reform in order to arrive at the view that the Revolution was centered in “utopian millennial expectations.”\nAbout this conclusion two things must be said. First, abundant evidence exists that the Americans were independently energized around the broad constitutional questions and the broad religious questions, however much the two came to be assimilated to a single set of contingent references once the need for social construction–as opposed to preservation–became unavoidable. Second, “utopian millennial expectations” were rooted in the “New Israel” wholly independent of the eventual political solution which addressed social contradictions that were evident–but not resolved or even systematically addressed–as early as the Massachusetts “Body of Liberties” (1648).\nNothing illustrates the first consideration so tangibly as the broadside found in Ezra Stiles’s papers, and which announced the formation of the “American Society for Promoting Religious Knowledge among the Poor, in the British Colonies.” This advertisement, declaiming against the “prevalence and increase of vice among us,” was published between 1772 and 1775, on the very eve of the Revolution. The names of leading members, those designated to receive subscriptions, include such revolutionary activists as William Smith, Elias Boudinot, and John Lathrop, as well as the evangelist, Jonathan Edwards and the lawyer, Tapping Reeves. No evidence exists that the society ever functioned, and I presume that its good work was swamped by the rising tide of Revolution. Thus, the constitutional question did not co-opt the religious question. Rather, the constitutional event displaced attention from the religious project, suggesting that “liberty” was far from a code word for religious awakening.\nThis argument may be sustained, I believe, even when Stiles resurfaces in his election sermon of 1783, “The United States Elevated to Glory and Honor,” and charts a millennial, providentially inspired course for the new polity. The millennial end, however, is nothing other than the triumph of religion on grounds of constitutional liberty much like those addressed to Charles II by Massachusetts in 1662. That is, the earlier view of the need for moral regeneration (under the old constitution) has been reformulated as an expectation of moral regeneration under a new constitution (“we have realized the capital ideas of Harrington’s Oceana“). Stiles discussed the equal franchise and an equable distribution of property as conditions of this unique opportunity: ”Religion may here receive its last, most liberal, and impartial examination. Religious liberty is peculiarly friendly to fair and generous disquisition. Here Deism will have its full chance; nor need libertines more to complain of being overcome by any weapons but the gentle, the powerful ones of argument and truth. Revelation will be found to stand the test...“ Thus, far from being ignorant of the war between secular constitutional principles and evangelical faith, it would be fairer to say that Stiles reveled in that war as an opportunity for faith (Sidney Mead to the contrary notwithstanding).\nMuch less need be written concerning the older–indeed original–roots of “utopian millennial expectations.” Pastor John Robinson in 1620 addressed the Pilgrims departing Delft Haven with an injunction to keep peace with God and man and a promise of God’s ordinances to sustain human innovations. Similarly, the General Court of Massachusetts answered Robert Child that “we account all our countrymen brethren by nation, and such as in charity we may judge to be believers are accounted also brethren in Christ,” doing which under “the rules of God’s word, the civill prudence of all nations, and our owne observation of the fruite of other mens follies” the General Court anticipated a “peace, unity, prosperity, &c.” Of course, this was also the case in which the General Court heralded the superiority of Massachusetts’s constitution to the English constitution after a detailed, side-by-side comparison of the two. Finally, none can read Mather’s Christi Magnalia Americana (his imitation of Plutarch), and its rich praise of religious devotion and secular knowledge (as in the life of William Bradford) without discerning the powerful belief among Americans that God’s grace would conduct their affairs, as a people, in this world as well as in the next.\nAccordingly, it is fair to say that Clark has exaggerated the transformation of American evangelism into political utopianism through the American Revolution. Unlike the French Revolution (however philosophically consanguine) the American Revolution never hazarded the Gnostic presumption. Human nature was relied upon rather than jettisoned in the United States.\nThis reckoning raises the interesting question, therefore, of exactly what bearing should inform the reading of Clark’s book. In a word, I believe its true bearing is to reconstruct the seriousness of the language of faith and what I call voiced differences, not as some Golden Age myth but as a continuing if infrequently resorted to resonance in Anglo-American social reflection. To understand this one must rather review the demonstrations than the conclusions of this book.\nPerhaps one can account for Clark’s misreading of the nature of the American identification with England by the opening observation of the work, which relies on David Fischer’s Albion’s Seed: Four British Folkways in America. There Clark recounted the ritual, religious, and political celebrations the colonists shared with their cousins, including “the birthday of the reigning monarch.” Clark omitted the significance, however, of New England’s celebration of “Fore-fathers Day” each December 22. These forbears were not ritually shared with Englishmen, and the rite emphasized the pre-existing sense of difference between Americans and Englishmen. Still more importantly, that combined religious and political holiday was replaced, most prominently, by “Independence Day,” the celebration of the “Declaration of Independence.” Thus, the colonists-turned-citizens of the United States symbolically provide witness of both the true nature and extent of their prior identification with their cousins and of their self-conscious political separation.\nClark’s belief that the two shared one history led him to ignore the most fundamental reason for the divergence of American and English common law (a point which William Nelson’s Americanization of the Common Law misses for different reasons). Though Blackstone spoke but briefly on the subject, the area in which he spoke directly addresses a central constitutional dilemma. English law did not “unify” the colonies for the sufficient reason that English law itself created the exception to the reach of English law in proportion as a colony was considered “conquered” or “discovered” (1 Blackstone, p. 46). The difference often served, in the colonies, to place the monarch in the room the common law would have occupied. The same problem, in a far more intensified degree, informed the trial of Warren Hastings twenty years after the American Revolution. The legal separation of Americans and Englishmen was an accomplished requirement of English legal practice long before the political separation of the American Revolution. It constituted the heart of Thomas Jefferson’s Summary View of the Rights of British America, in which Jefferson rejected Blackstone’s conquest theory for a discovery theory. Ironically, had the American constitutional theory been prevalent in Britain, Clark’s analysis would now be correct. Unfortunately, it did not prevail. Moreover, it serves to demonstrate precisely why Lord Bryce’s observation that “natural rights” was a “mass” of political dynamite in France (as Clark cites it) cannot be applied to the Constitution the same Lord Bryce celebrated as the “greatest work ever struck off by the mind of man.”\nIn a word: the political history of the United States unavoidably shapes the rhetorical context in which the “language of liberty” must be weighed, not the reverse. Clark’s survey, therefore, is the survey of a single set of meanings and symbols in the employ of two different people. Only the illusion of a single English-speaking political universe (which Churchill knew himself to be using when he spoke thus) permits the view that an inadvertent rupture resulted from little more than rhetorical excess.\nNow, this political account does not militate heavily against Clark’s claim: “In this study the American Revolution is analysed theologically as a rebellion by groups within Protestant Dissent against an Anglican hegemony... A rebellion of natural law against common law and a rebellion of Dissent against hegemonies Anglicanism were the same rebellion, since their target was the unified sovereign created by England’s unique constitutional and ecclesiastical” blend. For the truth is that evangelical religion plays a large role, such as he recounts, in advancing modern principles on both sides of the Atlantic. That very observation minimizes, for the Americans, the role of the monarch, since the ecclesiastical authority of the monarch was rejected in the colonies from the outset. Nothing highlights this so well as the appeal of the General Court of Massachusetts to Charles II that “we might enjoy divine worship without humane mixtures.” For the same reason it minimizes the claim that this was a revolt of natural law against common law, inasmuch as the monarch sought to maintain an authority in America based on a right of conquest. That leaves us with only the serious claim that we may analyze the public opinion of the Revolution in terms of the concerns of Protestant Dissent, and that those are similar for America and England.\nThe dimension of Clark’s analysis that benefits from his patient, if somewhat disorderly excavation appears in chapter one, “The Conflict of Laws.” He argues that “The sovereignty of the people, under God, was an idea which led away from the ancient constitution or from English liberties as a set of positive privileges and immunities, and towards a unified society whose fundamental laws ... mirrored and expressed the eternal principles of natural law.” The centuries-long religious war, then, became the pre-condition for the separation of “liberty” out of the merely contextual, blood-bound concept of nationality and into the status of a truth of nature and nature’s God. By this account one conceives of an American Revolution growing “naturally” from the soil of piety–perhaps even on account of the historical accident of internal British political disorder–and assimilating to such philosophical or Enlightenment concepts as strengthen the intrinsic tendency of the movement. This view contrasts sharply with the notion of a pious people upon whom steals unperceived a godless faith to create a new secular state.\nClark succeeds in drawing this picture rather more because he succeeds in revealing how extensive was the disorder and the extent of religious contest in England, which in turn enables the reader to imagine how far evangelical principles might advance in the absence of the Revolution. The answer is, “quite far indeed.” In fact, one may plausibly derive the democratic revolution in Britain, slow and incoherent as it has been, from the religious struggles chronicled here. Thus, when a people similarly engaged found themselves imbued with the ideas but liberated from the systems of political control (well before the military victory!), it brings no surprise that they concretize liberty, popular sovereignty, equality, and all such fundamental principles as came to find permanent residence in the Constitution of 1787.\nThe great mystery of The Language of Liberty is that Clark fails to realize that this is the story he has presented. He believes, for example, that the Constitution of 1787 “in a fundamental sense reversed the verdict of 1776.” One suspects that he is rather uncritically reading Wood’s Creation of the American Republic (which even Wood no longer reads uncritically). Clark seems to conceive that generative political discourse should be judged by the speed with which it generates clichés (hence the erroneous discussion of the first relevant uses of “socialism,” “capitalism,” “individualism,” “Americans,” etc.), when one should rather look to the speed with which generative political discourse changes or introduces ideas. Nothing can be clearer than, by the end of the eighteenth century, notions of “liberty” and “self-government” had prevailed powerfully over public opinion in the United States. The fact that such movement was advanced by evangelical dissent in company with enlightenment rationalism reveals well the sources and powers of political change. Indeed, to judge by the measure of religious establishment, the cardinal index for Clark, the change was wholly worked in the United States before even the political clichés associated with it came into general usage; for 1832 witnessed the end of establishment in the American states (though Clark is silent about this fact, it greatly amplifies his argument). The book is far less about the “language” of liberty than it is about liberty’s overthrow of establishment once liberty itself was emancipated from mere custom, or what Washington called in 1783 the “gloomy age of ignorance and superstition. ”\nBritain existed within the context of a species of political irony: its constitution was rooted in an Anglicanism to which relatively few adhered and which displayed little capacity for independent existence. “Even within England, the position of the Church was hegemonic not consensual...”(p. 203) As dissenting faiths challenged Anglicanism, having already (in company with Anglicanism) dislodged Catholicism, they served not only to undermine meaningful establishment. They also exposed ill-defined and ill-defended constitutional foundations. Thus Clark turns our attention away from the reification, “Revolution,” and towards the social condition, “the contingent features,” that invited fundamental change. This very framework makes it unnecessary to lean on such intellectual placebos as “paranoia” (p. 222) in order to explain these large events. Where there is room to dispute Clark’s fairly idiosyncratic reading of the ‘contingent features,’ it ought not to be denied that his recovery of the seriousness of voiced differences–the arguments people actually had–goes a long way to re-invest the period of “movement toward revolution” with historical significance.\nIn order that the judgment of Clark’s reading of historical contingencies as idiosyncratic should not be seen as an ad hominem, a postscript example should suffice. That is his rather quirky view that the term “America” had only a geographical meaning until the King conceded it a national meaning in the 1783 Treaty of Paris. William Gordon’s history urged a different construction before mid-century. Nathaniel Ames’s 1758 Almanac connected “utopian millennial expectations” with the term. George Washington’s first official address to the troops of the Revolutionary Army, not to mention his dramatic correspondence with General Gage (among many other and still more emphatic examples) sets forth a clearly national–if incomplete–meaning. Not only did various usages among Americans explicitly urge such meanings, but even speeches in Parliament sometimes made use of the separately cognizable political existence (and what else can he mean by a “national sense”?) of America. Clark’s ill-advised vendetta threatens, then, to undermine otherwise able scholarship. As Ames opened his paean to the growing America of the next two centuries in the 1758 Almanac, “America is a subject which daily becomes more and more interesting.”', 'by George H. Bresnick\n“Now I would ask you in the presence of the living, made solemn by the silence of the dead;- How could you! Oh!…How could you sit there and give uterance [sic] to such expressions as those which fel [sic] from your lips, under such circumstances, and your tongue not cleve [sic] to the roots of your mouth!”\nThis startling condemnation is contained in a letter dated May 28, 1866, from North Chester, Massachusetts, addressed to Edward L. Higgins, Esq., also of North Chester, and written by Nellie W. Smith, an aggrieved woman who could no longer hold her tongue or stay her hand in the face of an awful prior affront. I discovered the letter in a trove of documents kept in a trunk in the attic of the Old Methodist Episcopal Parsonage in South Worthington, Massachusetts.\nThough the letter never specifies what Edward did to warrant this condemnation, the following article investigates the main characters and content in this explosive missive. The original letter is pictured below.\nStanding on a small hillside cemetery near Kinne Brook Road at the eastern edge of Chester, Massachusetts, in May of 1861, Ellen (Nellie) Wise Smith was shaking to the core from what she had just witnessed. Her neighbor and primary school friend, Edward L. Higgins, had just buried his mother, Phebe, and turning to the nearby grave of Nellie’s recently interred little sister Addie, he uttered a falsehood that wrenched at Nellie’s heart. She could not bring herself to speak of this, and only years later, in May of 1866, did she finally write a letter to Edward, filled with anger and pathos, condemning him for his dastardly words, and beseeching him to repent for his duplicity. “I now commence the important, although unpleasant task of addressing you,” she wrote. “Receive it not as an Instrument of retaliation, but rather, as a subject of contemplation…for I have meditated long & fervently on the efficacy of informing you, in this manner, of your duplicity.”\nIndeed five years had passed before Nellie, then 23 years of age, summoned up the courage to address a matter that traumatized her so deeply. Much had transpired in that five-year interim. A bloody war had been fought across the southern and western parts of the country, and virtually every New England family had young men in the War, losing lives and limbs in the pursuit of Union and justice. Edward Higgins was among the enlistees, serving in Company K, Massachusetts 46th Infantry Regiment, from October 1862 to July 1863.\nThe 46th Regiment Massachusetts Volunteer Militia was raised largely in Hampden and Hampshire Counties in response to President Lincoln’s call for short-term troops in August 1862. Company K was sent to New Bern, North Carolina, along with Company A, which was under the command of Russell H. Conwell of South Worthington. The 46th saw limited action during its assignment, losing 36 men, all but one to disease. Edward spent two weeks in the military hospital at Bern in the spring of 1863, most likely due to disease rather than war wounds. By the summer of 1863 the Regiment was back in Massachusetts, where Edward mustered out on July 29, 1863, at Hampden Park, Springfield.\nThe Smith and Higgins families lived on nearby farms along Kinney Brook (as it was spelled on an 1870 map) north of Chester Center. Nellie’s father, Amok Clinton Smith, came from a long line of Chester farmers. Her great-great-grandfather Captain Abner Smith emigrated from New Haven to Chester before the Revolutionary War, probably in the 1770s. Her mother, Sarah L. Belden, was also born in Chester and came from similar stock. Amok and Sarah married in 1839 and had a son, Henry, in 1841, two years before Nellie was born. Addie, Mary E., Marshall C., and Jennie followed over the next twenty years.\nThe setting for the drama in the letter of May 1861 centers on Nellie’s little sister Addie, who died from diphtheria in March 1861 at the age of 12. Just two months later, on May 22, Edward Higgins’ mother Phebe died. His father Barney predeceased her by six years. Both the Higgins and Smith families maintained burial plots in a cemetery off Kinne Brook Road in Littleville, Chester Township.\nNellie’s 1866 letter to Edward recalls the terrible events in the cemetery five years earlier. “How could you! Oh! How could you, sit there, where you were so recently seated beside the remains of that Dear Sister, whom memory made, & still makes, dear to us all;- How could you sit there and give uterance [sic] to such expressions as those that fel [sic] from your lips, under such circumstances, and your tongue not cleve [sic] to the roots of your mouth! …Standing by the grave of her [Edward’s mother], whom we all loved so well, and watching clod, after clod, falling into the narrow house, hiding her forever from mortal view – Then turning from the resting place, of Father,- mother, & Sisters, to speak premeditated falsehoods…” [underlining in original]\nThe “Dear Sister” is Addie Smith, and during her March burial Edward was seated close to her remains. Now, in May, he was standing next to the coffin of his mother Phebe, who was being buried in the family plot, which also contained his father Barney; his older sister, Martha Maria Higgins, who died at the age of 19 in 1848; and an infant sister, Nancy, who died in 1843. For Nellie, the cemetery was hallowed ground, from which both families would ultimately enter their eternal rewards or punishments. Her sense of shared fate was heightened by the intimacy of the cemetery, which had only 57 graves, of which one quarter belonged to Smith or Higgins family members. A place of transition from the earthly to the heavenly state was no place for “duplicity.”\nWhy did it take Nellie five years to come forward and confront Edward? Perhaps it was partly her youth at the time of the affront (18 years old) and her gender. Perhaps the intense pressures of the lead-up to the Civil War, followed by the disruptions of the War itself, also figured in. Edward’s having served in the War clearly earned her respect, and perhaps even heightened her concern for him and his eternal fate. She repeatedly affirms that her letter is inspired by concern, not vengeance. “Know, therefore, that I would not cause you that anguish of soul, more bitter than the grave, even, were it in my power;… Nevertheless, I am compelled by the imperative calling of duty, to perform this act. Thinking, perchance, you may yet listen to the exortations [sic] of a friend of former years;- that you may yet reflect on your double-dealings; that you may yet repent, & turn from the error of your ways, ere the star of your honor, sits in blackness of darkness forever.”\nAs an ardent churchgoer, Nellie believed in the salvation of repentant souls. Her family were prominent members of the First Chester Congregational Church. In 1772, her great-great-grandfather Abner Smith and his family held a pew in the front row next to the pulpit, a position reserved by tradition for congregants of the highest “dignity.” Repentance was the only route to salvation for sinners. Nellie ends her accusations thus: “A young man of your years, of your attachments, & your refined sensibilities, which, in your situation, God most generously bestowed upon you: Taking into consideration all these qualities, together with love of Character, which no one doubts, you in common with every true son of America, passes [sic]…to speak premeditated falsehoods, such as no villain would dare to speak, unles [sic] his honor was trampled in the dust, his tongue the avowed instrument of deceit – his heart the abode of universal wicked, while his dark, contaminate, feindis [sic] opperations [sic] were preparing him for an honorable situation, if not a crown in Ston’s [Satan’s?] infernal kingdom. Then, and not till then, let the act be forgotten and forgiven.”\nThere is a distinctly biblical tone to Nellie’s letter, suggesting inspiration or even borrowing from Bible passages. Consider the underlined in the following segment:\nthat you may yet repent, & turn from the error of your ways, ere the star of your honor, sits in blackness of darkness forever.\nand compare to Jude 1:13 in the King James Bible:\nRaging waves of the sea, foaming out their own shame; wandering stars, to whom is reserved the blackness of darkness forever.\nAnother excerpt from the letter:\nHow could you sit there and give uterance [sic] to such expressions as those that fel [sic] from your lips, under such circumstances, and your tongue not cleve [sic] to the roots of your mouth!\ncan be compared to Ezekiel 3:2:\nAnd I will make thy tongue cleave to the roof of thy mouth, that thou shalt be dumb, and shalt not be to them a reprover: for they are a rebellious house.\nPerhaps the borrowing was not conscious, but simply reflected the vernacular of the day, colored by the Bible’s ever-presence in the people’s daily lives and ministers’ exhortations.\nBy contrast, another passage in the letter suggests Nellie’s schooling in classic poetry. She writes of Edward observing his mother’s coffin as it was buried in the ground:\nwatching clod, after clod,\nfalling into the narrow house, hiding her forever from mortal view.\nThese lines evoke one of the most famous and revered poems of the English language: Thomas Gray’s Elegy in a Country Churchyard, first published in England in 1771:\nEach in his narrow cell forever laid,\nThe rude forefathers of the hamlet sleep.\nThe Elegy was included among other poems and classical texts in the Sixth McGuffey Reader, part of a series of textbooks widely used by grammar and secondary schools in New England and elsewhere during the 19th Century. It is likely that both Nellie and Edward read the poem in their little Chester schoolhouse. The rural setting of the Kinne Brook cemetery, although not attached to a church, may also have evoked Gray’s country churchyard burial ground for Nellie as she penned her jeremiad.\nAnother couplet in the Elegy reminds one of the context of this affair:\nSome village Hampden, that, with\nThe little tyrant of his fields withstood.\nThis reference is to John Hampden (ca. 1595-1643), one of the leading Puritan parliamentarians in England who challenged the authority of King Charles I. He and four other Opposition members of Parliament were unconstitutionally designated for arrest by the King, but the Commons refused to hand them over to the monarch’s henchmen. This was one of the signal acts that led directly to the English Civil War, and ultimately to the trial and execution of Charles I. (Hampden’s cousin Oliver Cromwell became the Lord Protector of England after the overthrow of the monarchy.) Hampden was killed during the English Civil War, and his life so inspired the Puritans of New England that they named a county of Western Massachusetts County after him – and the Town of Chester belongs to Hampden County.\nNellie Smith’s Puritan forbears were among those early settlers of the Massachusetts Bay Colony, most of whom migrated from England in the wake of the Puritan upheavals in the 1630s and 1640s. One of those migrants, George Smith, by 1644 had settled in New Haven, Connecticut, as part of a religious splinter group that moved from the Bay Colony up the Connecticut River to establish new communities. George Smith’s great-grandson, Captain Abner Smith, as we saw earlier, left New Haven for Chester, Massachusetts, in the 1770s, and Nellie Smith was a scion of that line.\nBefore writing the letter, Nellie was for a time a factory worker in Chester, where industries in the 1860s included the manufacture of bedsteads and emery grinding wheels, as well as mills for cotton and a carding factory for local wool. At the wool factory Nellie probably met the overseer, Robert Billings, who had moved to Chester with his wife, Hannah Gorton Billings, and their children around 1865 from East Providence, Rhode Island. Presumably Robert was recruited because of his experience in the mills of Rhode Island. Apparently the Billings moved back to the Providence area (Rehoboth, MA) a year or two later. In 1868 Hannah Billings died, leaving Robert a widower with three young children. One year later Robert married Nellie Smith, 12 years his junior, in Rehoboth where the family remained. Robert worked in a nearby factory as a wool carder. Robert and Nellie had no children together.\nRobert Billings died in 1910, and Nellie moved back to Hampden County, living with her younger sister, Mary E. Smith, in West Springfield by 1920. Nellie died in South Worthington in 1927, apparently at the home of her youngest sister, Jennie Smith Freeman, who reported Nellie’s death for the town records. Anson and Jennie Freeman lived in what is now the Schrade/James house at 17 Ireland Street behind the Conwell Academy building in South Worthington.\nMary Smith had married Ptolemy Smith (a cousin) of West Chesterfield in 1866, and they lived for many years on what is now Ireland Street in South Worthington. Ptolemy and Mary were active members of the South Worthington Methodist Church, as were Ptolemy’s parents, Lucy Cole Smith and Warham Smith. Ptolemy and Mary had a daughter, Idella, and a son, Howard Clayton. Idella married Wilbur T. Hale, a minister of the Methodist Episcopal New England Conference, in 1896. They lived in many places around Massachusetts, settling finally in West Springfield after Wilbur’s retirement. Wilbur and Idella died in 1955 and 1959, respectively, leaving no immediate heirs. Howard Clayton Smith also moved to West Springfield as a young man, and had two sons: Rexford and Wayne C. Smith. In Idella’s obituary, printed in the Springfield Union newspaper, Wayne C. Smith is listed as executor of her estate. Nellie Smith’s letter must have come into his possession at that time. Wayne C. Smith bought the Old Parsonage in South Worthington from the Trustees of the New England Conference of the Methodist Church in 1960 for $1. In 1968 he sold the old Parsonage to Beatrice Mercer, who kept the letter along with other Smith/Cole family papers in a trunk in her attic. I acquired the papers from a local antique dealer who had recently purchased them from Bea Mercer’s estate.\nEdward Higgins remained in Chester and farmed his father Barney’s land. He raised a family of four children, and, as far as we know, led a respectable life. Yet for some reason Nellie kept the letter for the rest of her life, and passed it on to her sister Mary. There is no proof she sent the letter, unless another copy turns up in the possession of Edward’s family. I suspect the letter below is Nellie’s original copy, with corrections and insertions, and from this original she copied a clean draft to send to Edward. In any case, one could imagine she was saving the evidence for some final Day of Judgment. Perhaps Mary felt the same, as did Mary’s daughter, Idella, who also retained the letter. I suspect that by the time it came into the hands of Nellie’s grand-nephew Wayne Smith, it had become part of the family legend, too memory-laden to discard.\nThe other retained papers in the Smith/Cole trove, aside from legal papers (deeds, wills, and a fire insurance policy), consisted of a Cole family genealogy; a copied treatise entitled The Political Struggle, originally published by Horace Greeley just prior to the presidency of Abraham Lincoln; two 18th-century documents apparently stolen during the Civil War from the courthouse in Union-occupied Stafford, Virginia; and a somewhat bawdy poem, entitled “The character of a young gentleman,” about a man from “Chestertown” who goes on a tryst to Sandersfield, losing his pants in the process. Nothing else in the collection compared in drama and gravity to Nellie’s letter to Edward Higgins.\nWe will probably never know the content of Nellie’s accusation, nor its veracity. One can only assume that Edward’s “premeditated falsehoods” were directed at either Nellie or her dear departed sister Addie. Did Edward ever respond to the letter, in writing or in person? Given the proximity of their properties, they must have run into each other on occasion. Did Nellie leave Chester to marry Robert Billings a few years after writing the letter in part to escape the discomfort of her surroundings? Did Edward spread rumors about Nellie that could have been grist for the likes of a Hawthorne short story concerning the mores of a small New England town? Any suggested answers – or further research – would be welcome in the comments section below.\nNellie’s gravestone in Ringville Cemetery, Worthington, is shared with three of her siblings: Henry (H. H.), Fitch (who died in infancy), and Adda. Their other siblings Marshall, Mary, and Jennie are also buried at Ringville. Thus Adda and Fitch Smith each have two gravestones: one by Kinne Brook Road in Chester, and another at Ringville Cemetery, where their names remain united with their fiercely protective sister.\nABOUT THE CONTRIBUTOR\nGeorge Bresnick has been researching Worthington history since moving to the village of South Worthington in 1999, and has continued his interest in the area even after relocating to St. Paul, Minnesota, in 2010. During his tenure as Chairman of the Worthington Historical Commission, the South Worthington Historic District was added to the National Register of Historic Places. As founding director of the H. Stanley Bresnick Foundation, George reconnects material objects of historical significance with people or organizations closely associated with those objects. In recent years he has returned stolen Civil War papers (found in the attic of the former Methodist Parsonage in South Worthington) to the Stafford, Virginia, Courthouse; an 1886 letter (written by a young Yankee steamboat traveler cum patent medicine salesman on Florida’s longest inland waterway) to the Florida Historical Society; and a Boston lady’s diary (1887-1893) to the Massachusetts Historical Society.']	['<urn:uuid:a8a068cc-a23c-4581-a468-5842f0012223>', '<urn:uuid:80e6c08c-55ca-4496-a9b2-a6f7aa53162a>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	14	140	5622
22	How did people study different faiths 100 years ago?	The study of different faiths in the early 20th century was approached through both academic and experiential methods. Rudolf Otto undertook extensive travels in 1911-12, visiting North Africa, Egypt, Palestine, India, China, Japan, and the United States to explore diverse religious responses worldwide. He mastered multiple languages, including Sanskrit, to translate Hindu texts and compare Indian and Christian religious thought. Meanwhile, scholars like Maurice Blondel focused on deeply examining Christian theology and its philosophical foundations, contributing to a broader understanding of religious thought. Otto particularly emphasized taking all religions seriously as occasions to experience the holy, while respecting their distinctive characteristics rather than reducing them to common denominators.	['“Blondel sought to ‘open up a position in philosophy through which the light of Christian revelation could pour in.’”\nThe second of November, 2011 will mark the 150th anniversary of Maurice Blondel’s birth. He was born at Dijon into an old, landed Burgundian Catholic family. His father and his uncle were both lawyers. He studied philosophy at the École Normale Supérieure from 1881 to 1884 and defended his controversial dissertation, L’Action, at the Sorbonne on 7 June 1893. A year and a half later, on 12 December 1894, Maurice Blondel and Rose Royer were married. They had three children. Rose Royer Blondel died on 7 March 1919 in the twenty-fifth year of their marriage. After a year at the University of Lille, Blondel taught philosophy at the University of Aix en Provence from 1896 to 1927, when blindness forced him into retirement. From 1931 onward, thanks to the help of his secretary and former student, Nathalie Panis, he continued to write at an extraordinary pace until his death in 1949 at the age of eighty-eight. After his death, Panis retained the care of his archives at Aix en Provence.2\nA professional philosopher, Blondel (1861–1949) had a decisive impact on twentieth-century Catholic theology. Often noted, his impact is as often left unexamined. After the briefest sketch of Blondel’s philosophy of action, this essay addresses a more historical question: how did this philosopher, important in his own right, come to have such a deep and abiding impact on twentiethcentury Catholic theology?3\n1. Blondel’s impact on twentieth-century Catholic theology\nFive years after the Second Vatican Council ended in 1965, Canadian theologian Gregory Baum published a book-length attempt to re-vision Catholic theology. He entitled the opening chapter “The Blondelian Shift.”4 Almost two decades later, Hans Urs von Balthasar called Blondel “the greatest Catholic philosopher of modern times.” He credited L’Action (1893) with giving to Catholic thought “a decisive new beginning.”5 Anglican theologian John Milbank called Blondel’s philosophy, “reunderstood as theology, . . . perhaps, the boldest exercise in Christian thought of modern times.”6\n1. This essay is based on the second annual Michael J. Kerlin Lecture given at La Salle University on 22 March 2010 and dedicated to the memory of my friend Michael Kerlin (1936–2007).\n2. On Blondel’s life, see Oliva Blanchette, Maurice Blondel: A Philosophical Life (Grand Rapids: William B. Eerdmans Publishing Co., 2010). Blanchette dedicates his book to Nathalie Panis. See also the chronology in Henri Bouillard, Blondel and Christianity, trans. James M. Somerville (French edition 1961; Washington/Cleveland: Corpus Books, 1969), 218–19 and Jean Lacroix, Maurice Blondel: An Introduction to the Man and His Philosophy, trans. John C. Guinness (New York: Sheed & Ward, 1968), chapter 1. Lacroix knew Blondel personally and belonged to the network Étienne Fouilloux describes as the second “Lyon School.”\n3. On Blondel’s philosophical importance, see Maurice Blondel et la Philosophie Française, Colloque tenu à Lyon, ed. Emmanuel Gabellieri and Pierre de Cointet (Lyon: Éditions Parole et Silence, 2007).\n4. Baum credits Blondel with initiating in the Catholic Church “a new style of thinking about that transcendent, redemptive mystery in human history which we call God” (Gregory Baum, Man Becoming: God in Secular Experience [New York: Seabury Press, 1970], 1).\n5. Hans Urs von Balthasar, Dare We Hope ‘That All Men Be Saved’? With a Short Discourse on Hell (German edition 1986/1987; San Francisco: Ignatius Press, 1988), 81, 114. The quote from 114 begins chapter 7, entitled “Blondel’s Dilemma.”\n6. John Milbank, Theology and Social Theory: Beyond Secular Reason, 2nd ed. (Malden, Mass.: Blackwell Publishers, 2006), 319. This quote comes near the end of a ten-page excursus on the theological significance of Blondel’s philosophy.\nPage 1 of 2 pages', 'Rudolf Otto, (born Sept. 25, 1869, Peine, Prussia—died March 6, 1937, Marburg, Ger.), German theologian, philosopher, and historian of religion, who exerted worldwide influence through his investigation of man’s experience of the holy. Das Heilige (1917; The Idea of the Holy, 1923) is his most important work.\nEarly life and academic career.\nOtto was the son of William Otto, a manufacturer. Little is known of Otto’s early life, except that he was educated at the gymnasium in Hildesheim before becoming a student of theology and philosophy at the University of Erlangen and, later, at the University of Göttingen, where he was made a Privatdozent (“lecturer”) in 1897, teaching theology, history of religions, and history of philosophy. In 1904 he was appointed professor of systematic theology at Göttingen, a post he held until 1914, when he became professor of theology at the University of Breslau. In 1917 he became professor of systematic theology at the University of Marburg and for one year (1926–27) served as rector of the university. He retired from his university post in 1929, though he continued to live in Marburg the rest of his life.\nOtto took time from his scholarly pursuits, more out of a sense of duty than of preference, to participate in community and public affairs. He was a member of the Prussian Parliament from 1913 to 1918 and a member of the Constituent Chamber in 1918, where he asserted a liberal and progressive influence. And he was later to concern himself with the political questions of the Weimar Republic. Otto also participated widely in Christian ecumenical activities, both as they related to divisions within the Christian community and as they concerned relations between Christianity and other religions of the world.\nWhat initially prompted Otto’s inquiry into man’s experience of the holy was a specifically Christian, even Protestant, concern that had awakened in him while studying the life and thought of Martin Luther. This concern—to elucidate the distinctive character of the religious interpretation of the world—is reflected in his first book, Die Anschauung vom heiligen Geiste bei Luther (1898; “The Perception of the Holy Spirit by Luther”). He was to expand his inquiry in his book, Naturalistische und religiöse Weltansicht (1904; Naturalism and Religion, 1907), in which he contrasted the naturalistic and the religious ways of interpreting the world, first indicating their antitheses and then raising the question of whether the contradictions can be or should be reconciled.\nOtto resisted an easy reconciliation between the world view offered by the sciences and the religious interpretation but opposed equally the religionist’s hostility toward science and the scientist’s disregard of religion. The two perspectives, he insisted, are to be embraced and heeded for what they purport to disclose concerning the world in which men live. It was clear, however, that Otto’s principal concern was to justify and to clarify what it is that the religious interpretation of the world, even within its rational aspect, conveys to man as a distinctive dimension of understanding beyond the discoveries of the sciences and the generalized knowledge following from them. Five years later came his work, Kantische-Fries’sche Religionsphilosophie (1909; The Philosophy of Religion Based on Kant and Fries, 1931), a discussion of the religious thought of the German philosophers Immanuel Kant and Jacob Friedrich Fries, in which he sought to specify the kind of rationality that is appropriate to religious inquiry.\nDuring 1911–12 Otto undertook an extended journey, visiting many countries of the world, beginning with North Africa, Egypt, and Palestine, continuing to India, China, and Japan, and returning by way of the United States. These experiences were to set his problem in a worldwide context, turning him to an extended and searching exploration of the diverse ways in which the religious response had manifested itself among various religions of the world. He proved to be remarkably well equipped for such an exploration, both in his mastery of languages and his knowledge of the history of world religions. In addition to being at home with the languages of Near Eastern religions, he had mastered Sanskrit sufficiently to translate many ancient Hindu texts into German as well as to write several volumes comparing Indian and Christian religious thought.\nInfluence of Schleiermacher\nOtto’s initial mentor guiding his inquiry into the specific character of the religious response was the eminent German philosopher and theologian Friedrich Schleiermacher. It was Schleiermacher’s early work, specifically his book Über die Religion. Reden an die Gebilden unter ihren Verächtern (1799; On Religion: Speeches to Its Cultured Despisers, 1893), to which Otto gave particular attention. What appealed to him in this work was Schleiermacher’s fresh way of perceiving religion as a unique feeling or awareness, distinct from ethical and rational modes of perception, though not exclusive of them. Schleiermacher was later to speak of this unique feeling as man’s “feeling of absolute dependence.” Otto was deeply impressed by this formulation and credited Schleiermacher with having rediscovered the sense of the holy in the post-Enlightenment age. Yet he later criticized the formulation on the grounds that what Schleiermacher had pointed up here was no more than a close analogy with ordinary, or “natural,” feelings of dependence. For “absolute dependence” Otto substituted “creature-feeling.” Creature-feeling, he said,\nis itself a first subjective concomitant and effect of another feeling element, which casts it like a shadow, but which in itself indubitably has immediate and primary reference to an object outside of the self.\nThe Idea of the Holy.\nVarious influences had played upon Otto’s reflections through the years, aiding him in reformulating the religious category that was to carry him beyond Schleiermacher. His early teacher at Göttingen, Albrecht Ritschl, had located religion in the realm of value judgments, whereas, more significantly, his theological colleague at Göttingen, Ernst Troeltsch, sought for a religious a priori as the ground of religious interpretation and judgment. Otto was impressed by William James’s shrewd insights in The Varieties of Religious Experience (1902), yet he found James’s empirical method inadequate for interpreting such phenomena. Otto was particularly attracted to the thought of J.F. Fries, already mentioned, whose notion of Ahndung (obsolete form of Ahnung; literally, “presentiment,” or “intuition”), a yearning that yields the feeling of truth, opened up to him a way of dealing with religious phenomena sensitively and appropriately. These “feelings of truth” Otto sought to schematize in his The Idea of the Holy.\nIn that work, however, Otto was conscious of moving beyond his previous efforts, exploring more specifically the nonrational aspect of the religious dimension, for which he coined the term numinous, from the Latin numen (“god,” “spirit,” or “divine”), on the analogy of “ominous” from “omen.” The numinous, the awe-inspiring element of religious experience, Otto contended,\nThus, The Idea of the Holy, while benefiting from earlier studies, represented for Otto a new venture and a radical shift in the nature and ground of his inquiry. The concern here was to attend to that elemental experience of apprehending the numinous itself. In such moments of apprehension, said Otto,\nwe are dealing with something for which there is only one appropriate expression, mysterium tremendum. . . . The feeling of it may at times come sweeping like a gentle tide pervading the mind with a tranquil mood of deepest worship. It may pass over into a more set and lasting attitude of the soul, continuing, as it were, thrillingly vibrant and resonant, until at last it dies away and the soul resumes its “profane,” non-religious mood of everyday experience. . . . It has its crude, barbaric antecedents and early manifestations, and again it may be developed into something beautiful and pure and glorious. It may become the hushed, trembling, and speechless humility of the creature in the presence of—whom or what? In the presence of that which is a Mystery inexpressible and above all creatures.\nAlthough the mysterium, which Otto represents as the form of the numinous experience, is beyond conception, what is meant by the term, he insists, is something intensely positive. Mysterium can be experienced in feelings that convey the qualitative content of the numinous experience. This content presents itself under two aspects: (1) that of “daunting awfulness and majesty,” and (2) “as something uniquely attractive and fascinating.” From the former comes the sense of the uncanny, of divine wrath and judgment; from the latter, the reassuring and heightening experiences of grace and divine love. This dual impact of awesome mystery and fascination was Otto’s characteristic way of expressing man’s encounter with the holy.\nOtto employed the method he had developed in The Idea of the Holy in three major publications that followed: West-Östliche Mystik (1926; Mysticism East and West, 1932); Die Gnadenreligion Indiens und das Christentum (1930; India’s Religion of Grace and Christianity, 1930); and Reich Gottes und Menschensohn (1934; The Kingdom of God and Son of Man, 1938). Of the three books, the latter is especially important for glimpses of new insight that seem to point beyond the earlier, more widely acclaimed volume; it renders the hint of ultimacy that appears in present history.\nOtto’s concern with experiencing the numinous also gave rise to experimenting with new forms of liturgy designed to give urgency and vividness to such experiences in Protestant services of worship under critically controlled conditions. Here he employed a “Sacrament of Silence” as a culminating phase, a time of waiting comparable to the Quaker moment of silence, which he acknowledged to have been the stimulus to his own innovation.\nOtto took all religions seriously as occasions to experience the holy and thus pressed beyond involvement in his own historical faith as a Christian to engage in frequent encounter with people of other religious traditions. He had much respect for the distinctive characteristics of the various religions and thus resisted universalizing religion in the sense of reducing all to the lowest common denominator. Yet he strongly argued for a lively exchange between representatives of the various religions. It was this concern that led him to create in Marburg the Religious Collection of religious symbols, rituals, and apparatus on a worldwide basis for purposes of inspection and study and to advocate establishing an Inter-Religious League as “a cultural exchange in which the noblest . . . of our art and science and of our whole spiritual heritage would be mutually interpreted and shared.”Bernard E. MelandThe Editors of Encyclopaedia Britannica\nLearn More in these related Britannica articles:\nChristianity: Christian mysticism…mysticism developed by the comparativist Rudolf Otto.…\nChristianity: God the FatherRudolf Otto, an early 20th-century German theologian, attempted to describe to some extent the basic ways of experiencing the transcendence of the “holy.” He called these the experience of the “numinous” (the spiritual dimension), the utterly ineffable, the holy, and the overwhelming. The “holy” is…\nstudy of religion: Theories concerning the origins of religion…the German theologian and philosopher Rudolf Otto (1869–1937) in distinguishing between the unique holiness (separateness) of God and his ethical characteristics. Unfortunately, much of his information was unreliable, and his schematism was open to question; he foreshadowed, nevertheless, other forms of evolutionism, which were to become popular both in sociology…\nmysticism: Understanding the spiritual…theologian and historian of religion Rudolf Otto called “numinous.”…\nTheologyTheology, philosophically oriented discipline of religious speculation and apologetics that is traditionally restricted, because of its origins and format, to Christianity but that may also encompass, because of its themes, other religions, including especially Islam and Judaism. The themes of…\nMore About Rudolf Otto10 references found in Britannica articles\n- Christian mysticism\n- concept of the sacred\n- emphasis on religious experience\n- experience of holiness\n- psychological Neo-Kantianism\n- study of religion\n- transcendence of the holy']	['<urn:uuid:bb0c5354-c70b-4e03-a167-e8222dbe9b6e>', '<urn:uuid:395f12e7-d2c9-4387-80ec-e099bb37b50e>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-13T00:57:18.859842	9	108	2542
23	working data storage reliability ssd protection durability comparison ssd vs traditional storage	SSDs offer data protection through various write protection mechanisms. The first uses software settings where host software sends vendor commands to enable write protection. The second method uses hardware switches connected to GPIO pins. The third is fully hardware-based using switches connected to NAND Flash write protection pins. SSDs also have automatic firmware-based protection when NAND Flash approaches end-of-life. Regarding durability, different flash types offer varying endurance levels: SLC (25 DWPD), MLC (10 DWPD), TLC (3 DWPD), and QLC (1 DWPD). While SSDs provide benefits like lower latency, higher IOP rating, lower energy use, and higher shock rating compared to hard disks, their endurance is limited by DWPD restrictions.	"['This article illustrates the mechanisms adopted in SSD write protection and compares their disadvantages and complexity.\nProduct reliability is the key element in its industrial applications while product quality and reliable improvements the focus of every manufacturer. SSD ""write protection"" technology is designed to protect data integrity. This write protection function is enabled by hardware or software settings. When SSD firmware finds that the function is enabled, it continues to accept the write-in command from the host without writing data into NAND Flash. That is, it maintains the integrity of data in SSD by setting disks into read-only and disabling the write-in command by the host.\nAs shown in the above figure (Figure 1), the first common practice is to set the write protection function by software: users set up host software to send the vendor command to SSD, the latter then logs the write protection to maintain its validity even after power outage and notify the FTL (Flash Translation Layer) that the write protection write function is now enabled. The FTL then discontinues any NAND write-in functions for write-in relevant commands, including TRIM and security erase, received later. This is not the case with the background write-in function of SSD including Data Scan, Refresh, and Flush Log. The merits of this are that the write protection function can be exercised by settings in software and firmware without any change in hardware.\nAs shown in the figure above (Figure 2), another write protection method is initiated by hardware settings. The SSD PCB features one hardware switch connecting to the GPIO pin on the SSD controller. Users turn on the switch to enable the write protection function: the SSD notifies FTL that write protection is active once the GPIO pin signal is detected. The remaining steps of the process are the same as method 1. The merits of this method are in its simplicity. What is required to enable the write protection function is turning on the hardware switch without any software settings. This is the write protection method adopted by scores of SD cards.\nThe third method, as shown in the figure above (Figure 3), is completely hardware based. The SSD comes with a switch connecting to the write protection pin on the NAND Flash and no changes are required on the SSD firmware. The merits of this method are that the hardware maker alone can enable the write protection function, at the price of failures of some SSD firmware, including Prevent Read Disturb, Data Retention, and Flush Log, as NAND is now totally write-in disabled.\nThe last write protection method is to have the function enabled by SSD firmware automatically: due to the limited service life of NAND Flash, the SSD firmware may auto enable write protection when it detects the NAND Flash usage period is about to expire or when number of bad blocks overrun predefined upper limits. This prevents users from continuing to write in critical data in the SSD leading to data damage. This write protection is auto enabled without any user intervention and available on almost every market available SSD.', 'We provide all flash storage arrays for a variety of different workloads including OLTP, Databases, Virtualization for Server or Desktop, VOD, VDI, VR, 3D modelling, video editing or molecular modelling. Many of the newer advanced flash arrays utilize data deduplication and this dramatically increases the amount of flash storage capacity available. Please consider that not all data is compressible, and tests would need to be carried out before specifying an all flash design.\nAll flash storage provides outstanding performance compared to hard disk, but in order to achieve the desired performance, we also consider potential bottlenecks in the network in order to maximize the performance and alleviate any unforeseen issues. By deploying an enterprise flash array, you will typically see a 10x or more performance increase in your system environment which in turn leads to increased productivity. Although the cost of flash storage is coming down the cost per Terabyte is still 2-3x that of a comparable disk storage solution.\nAll Flash Storage vs Hard Disk\nEnterprise flash storage capacities are slowly increasing and at present stand at 8TB for an SSD drive, although larger capacity drives have been announced by Seagate and Samsung they are not yet commercially available. With hard disk capacities of 20TB and rising and likely to reach 100TB by 2025. An SSD flash drive has many benefits over hard disks and these are:\nFar Lower Latency\nHigher IOP Rating\nLower Energy Use\nHigher Shock Rating\nWhilst flash provides super-fast performance, another important thing to consider is the application and workload you are intending to run on your flash storage. All flash has a limit on the number of DWPD (Drive Writes per Day) permitted by each SSD. If you specify a flash array with a small number of drives and it is being heavily used, it is likely your flash storage will fail sooner than anticipated. Here is an article on what to look out for when buying a flash array.\nIt is always a useful question to ask as to the type of flash memory deployed in the array, below shows the main types of flash used today.\nSLC stores one-bit-per cell has longer endurance but is significantly costlier to produce with higher capacities. Enterprise Class – 25 DWPD\nMLC uses two bits per cell, the most common type of SSD used by the flash storage vendors – Enterprise Class – 10 DWPD\nTLC uses three bits per cell. These flash technologies have lower endurance but hold larger capacities and can be produced at lower costs. Consumer Class – 3 DWPD\nQLC uses four bits per cell. These flash technologies have the lowest endurance, highest capacities and lowest cost. Consumer Class – 1 DWPD\nThe differing types of flash\nAll of the systems below contain flash storage in one form or another.\nHybrid Storage - Designed as an intermediary between spinning disks and flash. Introduced over 5 years ago, these were the first storage platforms to take advantage of flash, could provide significant performance improvements, albeit at far less cost than an All-Flash Array.\nSSD Storage - Based on 2.5″ SSD drives, these are primarily designed to replace 1 or 2 drives in an existing disk sub-system to enhance performance. The issue is that a normal RAID storage system cannot handle the massive performance SSD’s provide, basically they overwhelm the data bus. So there is little or no point in replacing 16x spinning disk with SSD as it probably won’t be able to handle it.\nPCIe Storage - Now flash operates at the speed of the bus rather than the SAS/SATA controller. The primary issue with PCIe is the number of controllers that can handle the extra bus speed and the lack of standard compatibility between storage vendors which could cause interoperability issues. Over time these issues will all be incorporated in future versions of PCIe, but for now, it’s a path that hasn’t been tried, tested or approved by standards committees.\nAll Flash - Designed to take full advantage of SSD drive technology, allied to high-speed controllers, advanced data analytics and a relatively future-proof upgrade path, these systems are designed to provide the simplest and easiest out of box uses with a number of host interfaces allowing you to connect and go.\nNVMe Flash - NVMe is a high-performance, NUMA (Non Uniform Memory Access) optimised, and highly scalable storage protocol, that connects the host to the memory subsystem. The protocol is relatively new, feature-rich, and designed from the ground up for non-volatile memory media (NAND and Persistent Memory) directly connected to CPU via PCIe interface. The protocol is built on high speed PCIe lanes. PCIe Gen 3.0 link can offer transfer speed more than 2x than that of SATA interface.\nAll Flash Storage Solutions\nBuying an all flash storage solution it is a high investment and an unknown risk for many companies as they have always bought disk-based systems. Flash storage today is as reliable as hard disk systems and can scale to provide PB’s of usable space. They can have multiple flash controllers, support the latest flash technology developments including NVMe, clustered and provide scalability for performance and capacity.\nHow can we help?\nWe work with the world’s leading providers of flash storage to deliver solutions to enable your organisation to grow, deliver data faster and reduce unnecessary expenditure.']"	['<urn:uuid:8c554d6a-0907-41ed-ac0f-2dd2cabf4525>', '<urn:uuid:e4c62636-8ffe-42a2-a7ff-a088b0c5acdb>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	12	109	1402
24	Why do delivery drivers need fast charging stations?	Delivery vehicles, carshare services, and rideshare drivers can travel 50,000 miles annually - five times more than average commuters. Since time is money for these drivers and they need to charge multiple times daily, they require fast charging that is centrally located and easily accessed.	['Why Cities Should Support Right-of-Way Charging\nThroughout history, cities have invested in programs and infrastructure to support the efficient movement of people and goods. Once upon a time, this meant regularly cleaning up after horses and providing tethering rings. Over time, streetcar tracks ran down city streets. Then those were paved over to make way for cars to drive and park. As cities began focusing more on livability and embraced “new urbanism” approaches to revival, streetcar tracks began to return alongside dedicated bus lanes, bike racks, and dedicated bike lanes.\nCities now face a number of new demands on their public rights of way. It will take new forms of urban design and management to accommodate the next generation of mobility services like bike share systems, car sharing, micro-mobility services, and rideshare services. Broader smart city technologies like parking management, way-finding, and citizen engagement technologies will also factor into the design process.\nCities that are serious about tackling climate change are also recognizing that they need to electrify everything that moves. This includes private passenger cars, and requires making room for electric vehicle charging infrastructure in the public right of way.\nCities that have been most successful in promoting cycling, like my home town of Portland, have invested in bike racks, dedicated bike lanes, and other infrastructure that is ubiquitous, convenient, and highly visible. Likewise, cities that want to reduce air pollution and meet carbon goals by accelerating transportation electrification need to invest in electric vehicle charging infrastructure that is ubiquitous, convenient, and highly visible, using public rights of way.\nWhy Cities Should Support Right-of-Way Charging\nTo meet clean air and carbon goals.\nCities that are serious about reducing carbon pollution from transportation need to promote walking and biking, expand transit and micro-mobility services, manage development, and use pricing to reduce traffic and parking congestion.\nMany of these steps are designed to reduce the use of single occupancy cars. At the same time, though, cities will also need to electrify everything that moves, including those passenger cars. Just as our approach to solid waste requires a “reduce, reuse, recycle” approach, city transportation policy needs to pursue a “both-and” strategy. Making it easier to use an electric car does not conflict with encouraging alternative transportation options, any more than making it easier to recycle conflicts with discouraging single-use packaging.\nOut of sight, out of mind.\nResearch by UC Davis in California shows that most consumers do not realize that electric vehicles are available, and that public awareness has barely budged over the last six years. Highly visible charging on the street can be a very effective way to increase awareness. Cities that are serious about promoting cycling paint colorful bike lanes, replace car parking with large bike racks, and encourage drivers to “start seeing bicycles.” We need to help gas-car drivers “start seeing electric cars.”\nCare for your “garage orphans.”\nCharging at home is simple if you own a home with a dedicated parking spot and electrical service. However, some 20% of Americans, and an even higher percentage of low- and moderate-income drivers, are “garage orphans” who live in apartments or condominiums. Urban policy to promote sustainability and density are steadily increasing this percentage, while also steadily reducing the amount of parking provided in new construction. Public charging for these drivers will be increasingly important in cities.\nElectrify shared mobility.\nDelivery vehicles, carshare services, and drivers for rideshare services like Uber and Lyft can easily travel 50,000 miles a year or more, some five times more than an average commuter. If cities want these services to electrify, it is important to recognize that time is money for these drivers, that they will need to charge multiple times in a day, and will want to be able to do so as quickly as possible. They will need fast charging that is centrally located and easily accessed.\nRecommendations for Cities\nAs part of our work with multiple cities through the Bloomberg American Cities Climate Challenge, Forth has recently published a report that offers a toolkit for cities seeking to provide ubiquitous, convenient, highly visible, and affordable charging options in busy urban areas. Forth’s report includes a wealth of additional detail, but our recommendations begin with a few basic suggestions.\nStart with high level policy.\nTo meet our climate goals, all new cars sold must be electric within the next decade. Cities should start by adopting a policy recognizing this imperative and committing the city to support this transition. City departments can then develop specific policies and programs to meet this goal, but with clear direction and support from political leadership.\nThink bike racks, not gas stations.\nPublic electric vehicle charging generally won’t be profitable until there are far more electric cars using it, yet cities sometimes approach charging companies as if they were entering franchise negotiations with profitable technology companies, or regulating undesirable gas stations. To create the best environment for creative problem solving, cities should think of public charging as an important public service that they provide their citizens through public-private partnerships.\nTo make charging easier, cities will need to collaborate across internal departments, but will also need to work closely with their local electric utility and with private charging companies. Mobility technologies and needs are evolving quickly, creating new challenges and opportunities. Convening a cross-functional stakeholder group can be a great way to brainstorm ideas and to shape and evaluate programs.\nCharging can be integrated with high-efficiency lighting, mounted on power poles, combined with parking control systems, integrated into multimodal “smart mobility hubs,” or used to offer Wi-Fi and other smart city services. Some companies offer free charging services supported by advertising, others operate smart networks that generate valuable data, and others simply sell non-networked hardware. In short, there are a variety of technologies and business models that should enable cities to develop a strategy that supports their other mobility priorities.\nWhen a city is ready to proceed, it should create clear and simple parameters and processes to minimize the transaction costs for participants. Small pilot programs that are clear, fast, and well defined are nearly always better than sweeping plans that are vague and overly ambitious without implementation strategies behind them. Simple applications with the potential to request additional information are generally better than asking participants for comprehensive information up front. Sometimes, the most impactful thing a city can do is simply get out of the way by reducing permitting or planning barriers, or identifying specific locations for innovation.\nThink big, but start small.\nMeeting our carbon reduction goals will require all new cars sold in the US to be electric within the next decade or two. On the one hand, this means that cities need to be prepared to accommodate and charge a fleet that is 100% electric. On the other hand, it means that there is time to plan. Cities do not need to rush to tear up streets just to install charging. They can begin to incorporate charging into their long-term plans for the urban streetscape, incorporating charging naturally as streets are renovated along with lighting, sidewalks, bike lanes, and other aspects of modern urban mobility.\nIn many progressive cities, it is increasingly easy to find a well-marked bike lanes and convenient bike parking corrals in place of car parking spots. To address the climate crisis, cities need to make it just as easy to find electric car charging. A key element in that strategy will be making that charging available at street parking spaces. We hope our report provides helpful case studies and recommendations to cities as they innovate towards that goal.\nLeave your comment below, or reply to others.\nPlease note that this comment section is for thoughtful, on-topic discussions. Admin approval is required for all comments. Your comment may be edited if it contains grammatical errors. Low effort, self-promotional, or impolite comments will be deleted.\nRead more from MeetingoftheMinds.org\nSpotlighting innovations in urban sustainability and connected technology\nDedicated anti-trafficking actors across the nation are trying to build better systems in big jurisdictions like New York, San Francisco, and Los Angeles, and in smaller but scrappy jurisdictions like Waco, Texas and Boaz, Alabama. They all share the same need, for stronger interconnectedness as an anti-trafficking field, and more collaboration.\nThe Forging Freedom Portal is a one-stop shop where a police officer planning a victim-centered operation can connect with their law enforcement counterparts, and the right service providers ahead of time, collaborating to make sure they’re planning for the language skills, social services, and legal support that victims may need. The portal is a place where the people who care most about ending human trafficking, who are doing the hard work every day on the ground, can learn from each other and share best practices to raise the collective standard of this work.\nMaximizing both the mobility and safety of road users at urban and suburban intersections is of utmost importance to city leaders and citizens today. Trends such as micromobility, connected and automated vehicles, and an explosion of available data, coupled with increasing numbers of bikes and pedestrians on our streets, result in both challenges and opportunities.\nThe increasing ability to provide intersection connectivity, edge computing and cloud storage, along with growing tool sets, such as Signal Performance Measures (SPM) and advanced video detection, provide new and exciting opportunities to traffic engineers. Possible combinations of Vision Zero intersection solutions, Near-Miss analyses, and the ability to make real-time operating decisions at our intersections can be overwhelming. Still, they must be embraced to ensure public officials are accountable to the traveling public.\nI caught up with Joe Bergera – CEO of Iteris – recently and we discussed a cloud-first strategy for cities and the benefits, particularly during the pandemic. Organizations, cities and companies that have replicated some of their business processes in the cloud have navigated the pandemic quite well. We discuss why that is, and what can be done to help other cities during this time.']	['<urn:uuid:5daf1e4d-e779-4953-aafd-50012b3593cc>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	8	45	1660
25	I'm studying voltaic cells and I'm curious about the salt bridge - what is its function in the galvanic cell and what is it composed of?	The salt bridge helps to balance the charges in the galvanic cell. It is composed of Na+Cl- and two cotton plugs that prevent the Na+Cl- solution from pouring into the cathode and anode sites.	['What are they?\nA voltaic cell is\nalso known as a galvanic cell, it was named/ created by\nAlessandro Volta (voltaic) and Luigi Galvani (galvanic). It is a type\nof cell that makes electrical energy from the chemical reactions that\noccur inside itself. The chemical reaction that occur inside of the\ncell is a redox reaction. Examples: batteries inside your TV\ncontroller, batteries inside flashlight. Examples of types voltaic\ncells: 1-Alkaline cell 2-Daniell cell 3-Simple cell 4-Lead-acid\naccumulator 5-Dry cell.\nThe fact that a voltaic cell can produce energy/ electricity is by\nusing Zinc (anode) and Copper (cathode) that are connected by a wire\nand that are put in a solution: CuSO4. When the wire and\nthe salt bridge (helps to balance the charges in the galvanic cell,\nit is composed of Na+Cl- and 2 cotton plugs\nthat won’t let the solution of Na+Cl- of\npouring into the cathode and anode sites) are there, they will allow\nelectrons to move through the wire(s) that connect(s) Cu (with a\nstrong pull of electrons) to Zn (with a weak pull of electrons) with\na LED between the two. When electrons are in a movement position,\nthey produce or make electricity which will turn the LED on.\nWe said that there was two chemical reactions: reduction and\noxidation. The cathode (Cu) part is the site where reduction occurs\nand the anode is where the oxidation occurs. That because the\nelectrons are moving from Zn to Cu and the Zn2+ that are\npresent on the anode site come across to the cathode site and react\nwith the Cu2+ to make only Cu ? what happens in the\nCopper site is a reduction process:\n(aqueous = dissolved in water) + 2e- ? Cu (solid metal)\nAnd in the Zinc site is an oxidation process:\n(solid)? Zn2+ (aqueous) + 2e-\ntwo half cells\ncan be called: the reduction half cell and the oxidation half cell.\nWe saw in TP that if you only put 1 Zn and 1 Cu, it won’t provide\nenough electricity to turn a LED on. So what we did is we took more\nhalf lemons and connected the 3 Zn and 3 Cu with 4 more wires and it\nmade the LED turn on with enough electricity that was moving within\nthe 4 wires.\nAn electrolytic cell is also happening with redox reaction. It\nis where an electric current will make the transfer of the electrons\nduring/in the redox reaction. Here, the anode is positively charged\nand the cathode is negatively charged. The flow of the electrons\nduring the redox goes from the positive (anode) to the negative\ncharge (cathode). The electrolytic cell will convert electrical\nenergy to chemical energy. A battery is an electrical energy. It is\nthe total opposite of a galvanic cell.\nIt is a type of cell that needs energy and is non-spontaneous so\nforced. An example of electrolytic cell could be electroplating (it\nis the process of coating one metal with another.\nHere, the redox occurs in the one and only section, which is where\nthe anode and the cathode are together so the equations are:\nAnode ? oxidation = X- ? X + e\nAnd Cathode ? oxidation = M+ + e ? M\nDifference between a voltaic and electrolytic\nThe Galvanic cell has a salt bridge and the electrolytic doesn’t\n2. The Electrolytic cell uses a battery to produce energy and the\nVoltaic cell uses a light/ LED.\n3. They have a different flow of the electrons: Voltaic ? from\nnegative (anode) to positive charge (cathode) and Electrolytic ?\nform positive (anode) to negative charge (cathode).\n4. In Galvanic, the energy goes from the Zinc and the Copper to the\nlight or LED and in Electrolytic, the energy goes from the battery to\nthe Zinc and Copper.\n5. The voltaic has two sections in two different containers (the\nreduction half cell and the oxidation half cell) and the electrolytic\nhas only one section in one container (redox).\nconverts chemical energy to electrical power/energy and electrolytic\nconverts electrical power/energy to chemical energy.\n7. Galvanic is a spontaneous and electrolytic is a forced\nreaction generates current (galvanic) and a current drives a reaction']	['<urn:uuid:50dd989e-2ec0-4487-9fe9-99b5a7043f7a>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	26	34	696
26	what see national theatre panama city	The National Theatre is the ultimate home of the arts in Panama. It was constructed following Law 52 in 1904, designed by Italian architect Genaro Ruggieri, and inaugurated on October 1, 1908, coinciding with the inauguration of President Jose Domingo de Obaldía. The theatre was built by the Duke and Arias Company under the supervision of engineer Harmodios Florencio Arosemena.	['PANAMA TOURISM SITES BY PROVINCE\nTHE PANAMA CANAL – MIRAFLORES VISITORS CENTER\nThis is undoubtedly the biggest tourist attraction in the city of Panama .\nFrom the terraces and balconies of the Miraflores Visitor Center you can watch the boats pass just meters away.\nThere is a restaurant and a museum within the center.\nRecognized as the eighth wonder of the world , is the narrowest point of union between the Caribbean and the Pacific Ocean.\nThis is possible through three gates and three artificial lakes.\nIs much more than an incredible feat of engineering , it is a complex union between man’s ability and force of nature .\nThe ecosystem around it is just wonderful . 52 million gallons of fresh water each boat to mobilize needed , annually about 14,000 ships pass .\nSmall boats leave public docks Gamboa to lead you to an ecological expedition or an exclusive fishing trip on Lake Gatun , can also snorkel and explore the railway, trains and other equipment remain submerged in the canal since its opening in 1914.\nCASCO ANTIGUO OR CASCO VIEJO\nIt is the second Panama City founded in 1673 after the destruction of the first city now called ” Panama La Vieja” , which occurred in 1671 with the pirate Henry Morgan.\nIt is for this reason that the new building was a fortified city with a thick stone walls and seven bastions to protect it from any attempted pirate attack.\nInside, religious, military and civil buildings which remain today and can be visited by private car or by going on guided tours were erected.\nAMADOR CAUSEWAY AND CAUSEWAY\nThe Causeway of Amador Causeway in Panama is a tourist place where you can spend some quality time with family or friends.\nThe island has amador causeway that once were a favorite anchorage of Spanish galleons arrived in Panama City with treasures which snatched the Inca empire. In 1913 , the islands became part of Fort Grant, this fort was used to defend the entrance of the Panama Canal against possible attacks from Germany or Japan.\nThe armed forces of the United States built the road connecting the three islands using rocks digging the Panama Canal .\nThere is an ancient secret tunnel “Since military command ” dug deep into the rock of the Flamenco Island .\nThe amador causeway called in Castilian ” Causeway ” , is composed by a road from the mainland which connects four small islands in the Bay of Panama : Naos , Culebra, Perico and Flamenco.\n” MUSEUMS “\nAmong the most important museums in Panama City include:\n• Museum of Contemporary Art (MAC ) is the only Museum of Contemporary Art that exists in Panama . It is a non – governmental organization ( NGO) non-profit , which has legal personality .\nIts mission is to be the spearhead of the visual arts in the acquisition, conservation, research and dissemination of the arts of Latin America, with emphasis on the national.\nRecovering our artistic heritage and promote it through the sensory and spiritual experience of contact with the Art Museum . Sensitize people of all ages , with emphasis on children , through formal education in art .\nThe Museum of Contemporary Art is much more than an art collection , is a place where everyone can enjoy , learn and research on art in general and especially Art Panama .\n• Explore : Centre for Science and Art : EXPLORE is the first Interactive Center of Science and Art in Panama . It is a place where children and teens can learn, encourage and foster the spirit of research using as main tool the game a fun and innovative way.\n” THEATRES “\nNational Theatre at the Old Town of the city of Panama .\nThe National Theatre is the ultimate home of the arts in Panama .\nIn 1904 , Law 52, the new Republic of Panama ordered the construction of the National Theatre .\nHe commissioned the Italian architect Genaro Ruggieri design Theatre and the Palace of Government .\nThe Duke and Arias Company was responsible for the construction and the supervisor of the work was the engineer Harmodios Florencio Arosemena .\nIt was inaugurated on October 1, 1908 , coinciding with the inauguration of Don Jose Domingo de Obaldía as President of the Republic.\n” PARKS “\n• Omar Torrijos Recreational Park\nKnown simply as Omar Park , is located in the center of the metropolis and is declared a national green.\nThe park has facilities for public use, such as gym, pool , tennis courts , baseball , basketball and football , a outdoor auditorium , meeting room , library, restaurants.\nThis is a place where different activities take place either cultural, religious , social type and promoting public health.\nThe route along its perimeter is about 5km . The Omar Recreational Park , 56.5 ha, is the most visited country as a natural attraction and recreation space\nin the city of Panama . Receives an average of 25,000 people monthly.\n• Metropolitan Park of Panama : It has an area of 232 ha. Is located in Ancon and is a protected area that is within the limits of a metropolis.\nIts goal is to preserve in the city of Panama a natural area to help maintain the balance between the natural environment and human habitat ,\nwhile protecting biodiversity , providing a suitable habitat for species that require large areas .\nBOCAS DEL TORO\nIt is a spectacular place to enjoy the sun, sea and sand tourist province .\nIt has beautiful beaches where you can practice SCUBA diving , snorkeling, surfing or just enjoy the weather and calm waters .\nTourist Places to visit: 1. Isla Colón is located in the town of Bocas del Toro.\nTwo . Parque Nacional Marino Isla Bastimentos .\nThree . Keys trainers .\nApril . Lazy Bear Trail in Bay Wave.\nMay . Cristóbal Island.\n6. Punta Caracol.\n7. Punta Hospital.\n8. Dolphin Bay .\n9. Path of the Red Frogs .\n10. Bird Island .\nYou can enjoy a pleasant climate in its highlands , extreme sports, observing animals and exotic birds, fair flowers ,\nfresh fruit, large hotels and excellent access to various tourist attractions .\nTourist Places to visit:\n1. City of David – capital of the province of Chiriqui.\nTwo . Boquete – city of flowers and coffee.\nThree . Cerro Punta – nice weather , beautiful flowers and amazing tourist sights .\nApril . Playa las Lajas – white sand, palm trees and a great big sun.\nMay . Rio Chiriqui and Chiriqui Viejo – strong currents , waterfalls and rapids excellent for walks in the raft.\n6. The boiler – located in mind Boquete with hot springs .\n7. Volcano – located on the slopes of Volcan Baru .\n8. La Amistad International Park – PILA was for World Heritage Site by UNESCO\nIt is a great tourist spot where you can enjoy the warmth of its people, beautiful handicrafts ,\nbeautiful beaches where you can enjoy the sun, sand and surf. Visit Veraguas and you will not regret .\nTourist Places to visit:\n1. Coiba National Park\nTwo . watchtower\nThree . Thermal Wells in Cañazas\nApril . Craft Market at the Peña de Veraguas\nMay . In Montijo : beach Beach Queen and Torio ( good for surfing ) , Malena beach .\n6. In Sona : Santa Catalina beach (used for surfing ) .\nWith the warmth of its people , spectacular surfing beaches , paradise islands , beautiful rivers ,\ninternational fairs like the Azuero and enviable carnivals.\nTourist attractions to visit :\n1. National Festival in Guararé Marjoram .\nTwo . Festival of the Pollera .\nThree . Azuero International Fair .\nApril . Packs of skirts in San Jose de Las Tablas.\nMay . Playa Venado excellent surf .\n6. Playa El Uverito in Tables .\n7. Isla Iguana Pedasi\nIt has beautiful green beaches, forest parks , clay crafts and friendly people who make you feel like in your country.\nTourist Places to visit: 1. Festival of Sugarcane in Pesé .\nTwo . Manito Festival in Ocú .\nThree . The Sarigua National Park.\nApril . Parita and Chitre where you can stay , eat local dishes and buy handicrafts .\nYou can enjoy its rich culture , folk , cool climate in Valle de Antón or tropical in its beautiful beaches. Visit Abroad is a great tourist choice .\nTourist places to visit: 1. Minor Basilica of St. James , located in the city of Nata de Los Caballeros.\nTwo . Archaeological Park in the Caño .\nThree . Anton Valley where you can enjoy a pleasant and cool climate.\nApril . Beautiful beaches like Santa Clara, Farallon and Rio Hato, located on the Pacific coast.\nMay . The salt of Aguadulce\n6. Museum Penonomé\n7. Las Mendozas\nIt is a province with great tourism boom thanks to its beautiful beaches, resorts, shopping center for tourists such as the Colon Free Zone ,\nharbor cruises , islands and many more tourist attractions .\nTourist places to visit :\n1. Isla Grande.\nTwo . Beach at Nombre de Dios .\nThree . Playa Langosta.\nApril . Playa Palenque.\nMay . Portobelo .\n6. Turtle Cay .\n7. Fuerte San Lorenzo .\n8. Sierra Llorona.\n9. Colon Free Zone .\nYou can relax, enjoy life , beautiful beaches, eat lobster or shrimp , see exotic animals, visit a rainforest and discover a hidden paradise.\nTourist Places to visit:\n1. San Blas.\nTwo . El Porvenir.\nThree . Nargana .\nApril . Darien National Park .\nINDIAN DISTRICTS OF PANAMA\nIndigenous regions of Panama are wonderful to visit as they have beautiful beaches , exotic rainforests places, the food is very good ,\nthe warmth of its people and above all is an experience you will never forget .\nYalala Kuna Kuna Yala islands formed by about 360 extends from the border with the Province of Colón to the border with the Republic of Colombia .\nThe Kuna Yala is divided into four districts :\n1. Township of Narganá : composed of 28 communities , the most outstanding are: Sector Cartí , Sugar River communities ,\nand Nargana Heart, Tiger and Ticantiqui community .\nTwo . Township of Ailigandi : made up of 10 communities , the most outstanding are: Playon Chico community .\nThree . Township of Tubuala : composed of 11 communities.\nApril . Township of Puerto Obaldia : with 3 communities.']	['<urn:uuid:ae596868-a763-4666-b717-14568ec13f5e>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	6	60	1767
27	What's a good way to make multiple parts at once, and what's the problem with family molds?	Multi-cavity molds can produce several identical parts per cycle, reducing costs for high-volume production. However, family molds (which make different parts simultaneously) face challenges with uneven filling when parts are different sizes, as large imbalances lead to quality issues due to unequal pressure distribution.	"['5 design considerations to improve your success rate of multi-cavity molds\nBy Gus Breiland, Customer Service Engineering Manager, Proto Labs\nMoving from a single-cavity mold to one that produces two, four, or eight parts at once seems like an easy way to increase production volume and reduce part costs. This can be true in many cases, but only if the right steps are taken and the requisite homework done first. Designing a part for multi-cavity molding is not as simple as copying the CAD file for a single-cavity mold multiple times.\nThe physics encountered when forcing molten plastic through a mold\'s sprues, runners, and gates change as molds become larger and more complex, something that can impact molding performance and part quality. Also, thermal variations within a multi-cavity mold body become more of a concern, and plastic must travel longer distances to reach the finish line, both of which increase the risk of partially filled cavities and sink as well as part deformation after ejection.\nWhen moving from single- to multi-cavity tooling, it\'s important to recognize that parts that behave perfectly in a single-cavity mold might not play well with others, at least not without first making some tweaks to the part, the process, or even the material.\nA single 3D CAD model of a lens is shown here ...\nbefore it moves into this multi-cavity mold.\n1. Hold the gate\nOne of these tweaks is the gate. Cattle gates, child gates, Gate E24 at the airport -- each is designed to control traffic. The gates used in plastic injection molding are no different. They allow molten plastic to flow into the mold at the beginning of the injection cycle, and then hold it under pressure until the mold cools, the plastic has solidified, and the part is subsequently ejected.\nIn moldmaking, there are more types of gates than there are players on a baseball team. Proto Labs relies on three. Pin-style and hot tip gates are often employed on single-cavity molds to solve challenges with complex part geometries and to reduce gate vestige (the small remnant of runner material that must be trimmed from the finished workpiece) but these are rarely if ever used on multi-cavity molds. Here, tab gates (also known as edge gates) are the rule. Not only is gate placement far more flexible -- something very important when trying to squeeze multiple parts into a mold -- but the larger vestige that comes with tab gating works well to absorb residual flow stress around that section of the mold.\nAnother example where costly part redesign can be avoided when making the jump to multi-cavity is gate placement. Consider a mold for a plastic water bottle lid. Initial limited-production expectations might dictate placement of the gate in a single-cavity mold at a certain mold location. But when production ramps up and the head of supply chain decides it\'s time to invest in a multi-cavity tool, the original gate location may be impossible to achieve due to the changes in part orientation required for multi-cavity molding. By discussing product expectations with Proto Labs\' customer service engineers early in the design cycle, hiccups like this can potentially be avoided.\nNote that we also advise against designing your own runner and gate system to create your own multi-cavity tool -- Proto Labs can help facilitate the design of your multi-cavity mold.\n2. Side-actions and pick-outs\nSimilar situations can occur with a side-action. Let\'s say that you\'ve designed a lightweight bobbin with holes like Swiss cheese running through each flange to reduce mass. This is a perfect use for side actions, which slide into place prior to the plastic entering the mold -- thus restricting material flow -- and pop back out before part ejection. While side-actions can work great for a single-cavity mold, this sewing machine accessory most likely wouldn\'t qualify for multi-cavity tooling due to that very requirement.\nManually loaded inserts, or pick-outs, should also be given careful consideration. Placing a small block of metal into a mold cavity to create a cutout on an internal feature, for example, is fairly straightforward in single-cavity molding. That same approach on an eight-cavity tool, however, is time consuming and should be avoided if large quantities are in your product\'s future. If this is the case, let us know and we\'ll help you design a more efficient mold from the start.\n3. Family style\nCustomers at Proto Labs occasionally employ family molds for low-volume production needs. This type of mold is used to manufacture different components of a multi-part assembly, or multiple variations of a single component, in a single shot -- if you\'ve ever assembled a plastic model of a Ford Pinto as a kid (or a ""Star Wars"" TIE-Fighter replica as an adult), you\'ve handled family-molded parts. If this is the approach you want to take for your project, be prepared for some additional work and a greater tooling investment.\nTwo halves of a handheld medical device are designed into a family mold.\nDue to the different size and geometry of parts, family molds bring added design challenges, and it might make sense to investigate alternative molding options (e.g., multiple single-cavity tools) until larger production volumes warrant higher tooling costs. That said, Proto Labs has successfully tackled many projects involving family molds, part geometry, and family size permitting. Don\'t throw in the towel before giving us a shot.\n4. Easy flow\nMaterials that flow well, like liquid silicone rubber (LSR), are typically well suited for a multi-cavity situation. Because silicone flows more easily than nickels at a Las Vegas casino, most of the challenges faced with multi-cavity and family molds are greatly reduced. If your project requires a flexible material with excellent strength and dimensional stability, good performance, at extreme temperatures, chemical resistance, and biocompatibility, LSR is a solid option.\n5. For your consideration\nIn addition to the design elements unique to multi-cavity tooling, there are, of course, common injection molding guidelines to keep in mind like draft, wall thickness, material choice, and surface finish. The likelihood of success increases -- especially as the number of cavities increases -- with part designs that account for these variables and others. See our recent Design Tip on leveraging low-volume injection molding for a more thorough examination of design considerations for plastic injection molding.\nThe bottom line is this: Efficient injection-molded part design is about reducing part costs. This is true whether you\'re making 5,000 parts or 5 million. Some designers and manufacturers aim to shortcut the mold development cycle by jumping feet first into multi-cavity tooling, skipping the critical prototyping phase. Proto Labs advises against this approach -- not because we want to sell you another mold, but because testing parts using a single-cavity mold is an excellent way to vet design, improve quality, and likely save some money in the long run. Some customers test multiple iterations of the same molded part in parallel with multiple single-cavity molds, select the winner, and then move into a multi-cavity mold. This can increase your overall speed to market by helping you avoid development speed bumps along the way.\nAll of the aforementioned design considerations for multi-cavity molds can be discussed with a customer service engineer at Proto Labs, who can help direct you to the right tooling.\nPublished July 2015\nRate this article', ""The mold is one of the most important parts of injection molding. A mold is made up of two halves that have a hollow space between them into which a melted material is injected. This hollow space is shaped by both the core and the cavity when the mold is closed.\nMany people confuse the cavity with the core. In injection molding, the cavity is the female portion of the mold that forms a product’s external shape. The core is the male part that is responsible for forming the product’s internal shape.\nThis article explores cavities of injection molding in more detail, with a specific focus on multi-cavity injection molding.\nSingle Cavity, Multiple Cavity, and Family Cavity Injection Molding\nThere are three main types of cavity injection molding a manufacturer may leverage:\nNo. 1 – Single Cavity Molds\nThe simplest and most cost-effective option, single cavity molds produce one molded part per production cycle. These molds are easier to produce than multi-cavity molds, which reduces lead times and allows the manufacturer to start production faster. However, they’re not appropriate for parts in high demand as the tool reaches its production limit quickly.\nSingle cavity injection molding may be a good choice for low-volume production runs. This type of injection molding is often called 1 x 1 to reference the fact that there is one cavity inside the mold producing one part per cycle.\nNo. 2 – Multiple Cavity Molds\nAs the name implies, multiple cavity injection molding uses a mold that produces several parts per production cycle. In addition to reducing the cost per part, this speeds up the production process. Creating the mold itself is more expensive. But for high-volume production runs, that expense is quickly accounted for with the faster and more cost-effective production times.\nMultiple molds are often denoted as multiplication sums, such as 1 x 2, In this case, the mold produces two identical parts per production cycle. This can extend to 1 x 4, 1 x 8, and so on based on the manufacturer’s needs.\nNo. 3 – Family Molds\nExpanding on the concept of 1 x 1 molds, there are family molds too. Though these tools are more expensive to produce, they can create multiple different molded parts with a single injection mold. Though 1 +1 is the most common, it’s possible to create 1 + 1 + n + ... molds, which would create 'n' different parts per cycle. Examples include:\n1 + 1 – The mold produces two different parts per cycle.\n2 + 2 –The mold produces 2 different parts with 2 cavities each (4 parts in total). There is also a more modern form of injection molding that involves injection of two different materials into a mold that produces two parts, but this requires valves inside the runners to control polymer flow.\nFamily molds offer challenges when the parts are of different sizes. The manufacturer needs to balance filling each cavity evenly. Large imbalances lead to product quality issues because of not equal pressure distribution.\nArrangement of the Cavities\nAfter understanding that there are different types of cavity injection molding, a manufacturer needs to determine how to arrange the cavities in the mold. This includes figuring out how many cavities the mold should have. Several factors influence these decisions.\nWhen a mold is closed, it needs to be clamped together to prevent leakage and ensure equal pressure is exerted to each side of the mold. This can be a challenge with larger molds. If a manufacturer clamps the mold at the top and bottom, the center of the mold may not have the appropriate pressure applied to it to produce consistent parts (without flash).\nCentric clamping must be used in these cases. This type of clamping ensures pressure is placed on the center of the mold. Centric clamping is fairly simple to accomplish with a single-cavity mold. However, multiple and family molds require forethought in clamping mechanisms due to each having several cavities.\nEasy Tool or Multiple Tool\nEasy tool is another term for single cavity injection molding. Often, a manufacturer will use a large tool wherein the mold cavity is slightly off-center. This gives the tool larger dimensions but allows for the injection of melt via a side gate. Some manufacturers place the gate directly on the part, allowing a more direct injection that leads to smaller tool sizes.\nWith multiple and family tools, the manufacturer has to consider sprue distance. This distance must be the same for each part or the mold cavities fill unevenly. The article explores different sprue types later.\nAccording to a Clemson University paper that examined the concept of cavity pressure as a quality indicator in injection molding, cavity pressure is a reliable indicator for part quality and process monitoring.\nThe research found that creating a cavity pressure curve illustrates the molding cycle’s progression, even in the case of micro-injection molding:\nAs demonstrated in the graph, the peak cavity pressure line matches the part weight line. It increases as the weight increases, and decreases accordingly. As such, a manufacturer can track pressure increases and decreases in line with part weight. If pressure decreases substantially for a heavier part, this suggests an issue in the mold that could lead to quality problems.\nIt also found no significant difference in the curves created by different types of materials, leading to cavity pressure being a suitable indicator of product quality.\nCavity Surface Finish\nAll injection molded parts have a surface finish applied by the tools used to create them. The type of surface finish applied affects the time and effort required to create a part. The finish must be produced, with each step of refinement adding time to this production effort.\nSandblasting and EDM are two common methods for ensuring a stable cavity surface finish. EDM is a subtractive method that uses electrical discharges to create features on a mold. Sandblasting involves forcing solid particles across the part’s surface using compressed air. This serves to clean and smoothen the surface.\nExamples of Gates and Sprues\nA sprue is a channel through which molten material flows during the injection molding process. Sprues guide the material from the hopper to the desired location inside the mold.\nIn multiple cavity injection molding, the distance of the sprue is crucial to the production of quality parts. Variances in sprue distances lead to uneven cavity fills that compromise part quality. There are several types of sprue a manufacturer may use in injection molding.\nUsed for large-area parts, film section sprues and gating involve filling the molded part using a rectangular cross-section. This prevents internal tension and warping, though the sprue has to be mechanically separated from the part after demolding.\nThe molded part is filled using a cylindrical and conical cross-section. This is also known as a side injection. Tunnel cuts allow automatic separation of the mold and sprue during demolding, though it also leads to pressure loss and material shearing.\nBanana cut sprues are curved to enable access to more difficult injection points. It requires the use of easily deformable melt materials, though it does allow automatic ejection of the sprue during demolding.\nUsed specifically for ring-shaped parts, this sprue prevents weld and flow lines from appearing on the part. It evenly distributes the melt as it flows through, allowing the production of high-quality parts. But this sprue must be manually removed during the rework process, leaving marks behind in the process.\nCone or Bar\nCone and bar sprues offer very little resistance to the melt, which flows straight down into the cavity. This makes it ideal for filling difficult cavities. However, the mold has to be removed manually, again creating visible sprue marks.\nRing sprues are ideal for long tubular parts. They solve the problem of the pressure of inflowing melt leading to bends by providing support to the mold on all sides.\nUnderstanding Cavity Injection Molding\nNoting that there are several types of cavity injection molding allows a manufacturer to determine which types of molds serve their products best. For low-volume runs of a single product, a single mold, or easy tool is preferable. In high-volume runs, it’s often best to use a family or multiple mold to produce multiple parts per injection cycle.""]"	['<urn:uuid:069583f2-e57a-485f-ae8d-7277796fa905>', '<urn:uuid:ae2fc53a-9452-4dfc-97e4-23963af93af0>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	17	44	2595
28	What options does this audio system provide for playing different types of vinyl records from various music labels?	The system includes three types of equalizer curve switches: RIAA for stereo recordings, Mono 1 (specifically for DECCA label recordings), and Mono 2 (specifically for Columbia label recordings). The Mono1 and Mono2 settings are designed for monaural use only, enabling higher fidelity playback specifically for DECCA and Columbia label recordings.	['A tube type LCR non-feedback phono amplifier of independent Lch & Rch and completely monaural composed of 6 units\nDivided into six units on the left and right and supported by a powerful power supply, the played sound with its high S/N ratio and high resolution delivers the highest degree of realism of the performer with an overwhelming sense of liveliness and good visibility of the performer on stage.\nAn MC step-up transformer composed of 2 monaural blocks\nTransformers are made pursuing high sound quality by reviewing the wire/core material and the winding structure. The monaural unit is composed of a chassis base made of solid steel plates with copper plating and a case cover with an induction ham shield. In addition, magnetic shields were added making a double-layered structure which made the T-2000 successful to completely eliminate sound quality interference between the left and right. Setting this as the base, we have tuned it specifically for the EA-2000.\nNon-feedback LCR equalizer due to tube type unit amplifiers composed with two monaural blocks\n1) The non-feedback SRPP-type amplifying circuit of ECC-803S which has an excellent transient response and an outstanding reputation for sound quality plus the ultra-low impedance output cathode follower due to 6922 (6DJ8 high reliability tube) are together adopted as a unit amplifier. You can enjoy the large dynamic range and its glossy and lively sound quality that is possible only in vacuum tubes.\n2) The LCR type is adopted for the equalizing circuit. Power loss and inductive noise in the signal line can be minimized and sound quality deterioration can be suppressed by adopting the LCR type which can reduce the impedance of the signal system. The information obtained from the step-up transformer is outputted from the output transformer of the last stage without any losses by adopting our own winding coil and high quality film capacitors and so forth.\n3) Three types of equalizer curve switches are installed. The RIAA for stereo, Mono 1 (for use with DECCA label and so forth) and Mono 2 (for use with Columbia label and so forth) can be selected. Mono1 and Mono2 are for monaural use only. This enables a higher fidelity playback for both the DECCA and Columbia labels.\nPower supply with direct heat rectifier tubes 5U4G composed of 2 monaural blocks using a high-capacity R-core transformer.\nFor the rectification circuit, an R-core power transformer with excessive capacity is used and a rich rectification circuit is used for each independent left and right power supplies by using two choke coils in parallel and rectifier tubes (5U4G) which in principle do not generate switching noise. Furthermore, the power supply transformer and choke coils are magnetically shielded to provide a cleaner and stronger power to the amplifier. In addition, a 3-terminal regulator DC ignition system is used for the heater power supply of the amplifier achieving a high S/N ratio.\nA tough chassis structure consisting of machined aluminum control knobs, a 20mm-thick aluminum slant front panel, a 2mm-thick copper-plated steel plate chassis base, and a 2mm-thick aluminum cover with a magnetic shield are used to ensure rigidity and reduce magnetic distortion.\nThe bottom of the chassis is made of walnut board, which effectively suppresses unwanted resonance in the metal chassis. The foot is equipped with a TAOC high carbon insulator to isolate external vibrations. The RCA terminals are made by WBT, renowned for their high quality and high sound quality, and the XLR terminals are rhodium-plated by Furutech to aim for total balance in the sound quality. The highly universal and elegant polishing does not abandon the pleasure for owning it.']	['<urn:uuid:a2da5930-aa47-4e79-9056-017716ad353a>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	18	50	601
29	Are modern cots riskier than portable ones?	Regular cots are actually safer for long-term use compared to portable cots. While portable cots can be handy for traveling, they are not recommended as a long-term substitute for regular cots. Regular cots manufactured after June 2011 must meet strict safety standards and undergo testing and verification. However, cribs remain responsible for more infant deaths than any other nursery product, with over 12,200 children rushed to emergency rooms with injuries in 2011.	"[""When you're buying a cot for your new arrival you need to consider safety, ease of use, size, features and the option of buying second-hand. You'll also want one that suits your style and budget.\nIf you're also considering a portacot, see our portable cot buying guide. We explain why they are handy when travelling, but aren't a good long-term substitute for a regular cot.\nCots should be certified under the mandatory Australian safety standard AS/NZS 2172, but CHOICE has found cots being sold that don't comply. So take a tape measure with you to ensure it meets the safety guidelines.\nIs the cot deep enough to stop a child from falling out?\nThe distance from the top of the mattress to the top of the lowest side when the dropside is closed should be at least 50cm when the base is set in the lowest position. The depth should be 30cm when the mattress base is in the upper position. The depth should also be at least 15cm when the dropside is down.\nThere shouldn't be any footholds in the cot that your child could use to climb out.\nDoes the mattress fit snugly around all sides?\nWhen you choose a mattress, make sure there is no more than a 4cm gap between the edge of the mattress and the adjacent cot side when the mattress is pushed to the opposite side, or 2cm on each side when the mattress is centred. Gaps at the sides are a suffocation risk – your baby could roll face-first into them. And mattresses should be firm; a soft or saggy mattress can also be a suffocation hazard.\nAre there any head entrapment hazards?\nAny large space or opening must be between 5cm and 9.5cm to stop your baby from either getting caught or falling out.\nCan you see any limb or finger entrapment hazards?\nSmaller openings should not be between 3cm and 5cm wide – which is wide enough for a child's limb to get stuck – or be between 5mm and 12mm wide, so little fingers don't get caught.\nAre there any sharp edges or protruding parts?\nCheck for any sharp edges or anything sticking out or pointing up that could hit a child's head or snag on their clothing. Snagging clothing may not sound serious, but can be distressing for the child and in the worst cases can end in strangulation.\nThis is really important – any seemingly small annoyance can become a major headache for a sleep-deprived parent!\n- The dropside (the side of the cot that moves up and down) should be secure and smooth to operate and not too heavy for you.\n- It should be low enough for you to bend comfortably over into the cot or your back might suffer.\n- It should be impossible for a child to open, but easy for an adult to operate – can you open it with one hand?\n- Make sure the side is at least 5cm off the floor when lowered or feet will bang against the side or be crushed when it comes down.\nConsidering they are a bed for a small baby, some cots can be surprisingly big.\n- Check the dimensions of the fully-assembled cot and how well it will fit into your nursery.\n- Will it be too heavy to lift if need be?\n- Will it fit through doorways if you need to move it to a different room?\n- If the cot will be in your own bedroom for the first few months, space can be a serious issue.\n- You might also want space for other things, like a change table or a seat for feeding.\nSome cots take a long time to assemble and are fiddly with lots of parts; we note these in our cot reviews. While you may only need to assemble the cot once, it's still a consideration.\nCHOICE tip: For safety, all components of the cot should be permanently fixed or require the use of a tool to take apart.\nThese are plastic strips on the wooden edges of the cot, such as the top of the dropside, so that neither the baby nor the cot is damaged if it's chewed on (and it probably will be!).\nWheels make the cot easier to move around, but there should be lockable brakes on at least two wheels.\nLook for claims of sustainable manufacture, such as use of plantation or FSC-certified timber.\nMaking the change from cot to bed at the right time will help to prevent injuries associated with the child falling out of the cot. Kids are ready to move onto a bed when:\n- they can stand confidently in a cot,\n- shake the sides of the cot and\n- make a serious effort at climbing out.\nA cot that allows you to take the sides off and convert it into a small bed will give you more use over time. Some need the addition of bed rails at the sides for strength, which is fine as long as the mattress is firm and fits snugly against the rails and bed ends so there are no gaps (which are suffocation hazards).\nSome cots can also be converted to a small lounge chair or desk, giving them even longer-term use.\nFrom an environmental point of view, it's good to reuse an old cot rather than buy a new one, and it can certainly save money. However, it might not be the safest option. In Australia, cots have been subject to a mandatory standard only since 1998, and our tests show that even since then, not every cot meets the standard. So before accepting a used cot, there are several questions you should consider.\nCheck that all parts are in working order, in particular whether the catches are too easy to undo by a child. Make sure it meets modern safety standards and whether there are instructions for assembly and safe use.\nAlways buy a new, correctly-fitting cot mattress; an old mattress may be a SIDS or breathability risk if it's too soft, or it could just be dirty!\nIf it was made or repainted before 1970, a cot might be painted with lead paint, which children can chew on and swallow when they're teething. If in doubt, strip the cot completely and repaint it. The standard for new cots requires that any paint used is safe. In any case, a cot that old is unlikely to meet current standards and may have other hazards such as unsafe gaps or strangulation hazards. You're better off with a more recent model."", 'Read First: A new federal safety standard went into effect on June 28th, 2011. All cribs manufactured after that date must comply with 5 new standards, and cribs manufactured before that date should not be bought, sold, or used. See the CPSC website for more information.\nCribs are responsible for more infant deaths than from any other nursery product. In the last two decades, 1,100 children have died from crib related injuries. In 2011, over 12,200 children were rushed to emergency rooms with injuries. Since then, crib recalls have decreased a great deal. In 2015 cribs were recalled three times, but this is still three too many.\nThe types of hazards found in recalled cribs include drop-side and mattress support hardware failures that allowed portions of mattresses to fall or a gap to form, posing suffocation hazards to young children. Crib slats separating from headboards and gaps between side rails and crib mattress supports created additional entrapment hazards. Other recalls were due to spindles loosening and detaching, posing dangers of falls and entrapment; paint containing high levels of lead; and chipping paint posing a choking hazard.\nRecalls and corrective actions for cribs have been issued for various reasons including:\n- non-compliance with safety standards\n- strangulation hazards\n- risk of head entrapment when side rails, spindles, and slats in side rails become loose\n- risk of suffocation\n- choking hazards\n- risk of falling\n- danger of laceration when fingers become trapped in folding drop gates\nAlthough mandatory standards exist for cribs, only recently has an effort been made to strengthen those standards and require testing and verification of new cribs. All cribs must include proper assembly instructions and diagrams as well as cautionary and warning labels as required by federal law.\nWhen choosing a crib, be sure:\n- The crib was manufactured AFTER June 28th, 2011, when new federal safety standards were put into place\n- A soda can cannot pass between any of the slats of the crib\n- There are no corner post extensions or decorative cut-outs on the crib\n- Hardware (screws and bolts) which secure crib components are not loose or missing\n- Crib is free of protruding rivets, metal nuts or bolts, knobs, and wing nuts\n- Mattress is tight-fitting; 2 fingers cannot fit between the mattress and crib’s side\n- Joints and parts fit tightly, and wood is smooth and free of splinters\n- There is no cracked or peeling paint, and all painted surfaces are lead-free\n- Lowered crib sides are at least 9 inches above the mattress support\n- Raised crib sides are at least 26 inches above the mattress support in the lowest position\n- To avoid drop-side cribs – they do not meet mandatory standards\nWhen using a crib, be sure to:\n- Read directions for set up, use, and care of the crib. Replacement instructions can be obtained from the manufacturer.\n- Remove all soft bedding such as pillows, sleep positioners, blankets, bumper pads, and stuffed toys\n- Use a fitted sheet that is tight-fitting\n- Check that the mattress support system is connected on all four sides and all hardware is tight and in good condition\n- Position mobiles or hanging crib toys out of child’s reach and remove these items when child starts to push up into a sitting or standing position\n- Never hang anything on or above the crib with ribbon or string\n- Drop mattress to the lowest position when child can pull themselves up\n- Stop using a crib with loose or missing attachments or support hardware and contact the manufacturer for replacement hardware. Do not attempt to repair a broken crib\nCrib recalls began to rise in 2005. Since 2007, over 9 million cribs have been recalled.\nIn Memory of Danny Lineweaver\nThis page is dedicated to the memory of Danny Lineweaver and the efforts of the Danny Foundation, named in his honor. From 1984 through 2005, his parents and the Danny Foundation worked to improve crib safety. Thanks to their efforts, fewer children die from corner posts and other unsafe crib designs.\nSafe Sleep Tips\nCPSC Safe Sleep Poster (Spanish)\nCPSC: Check your crib for safety video\nDangers of Drop-side Cribs\nDangers of Drop-side Cribs (Spanish)\nDangers of Drop-side Cribs (Polish)']"	['<urn:uuid:feb74df4-4e1f-4dca-9f4d-7001c0626f6c>', '<urn:uuid:eb1318d4-4b9c-4eac-9f90-75a3e747921a>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T00:57:18.859842	7	72	1815
30	How much natural nitrogen is available in soil vs irrigation?	Soils with 3-5% organic matter naturally contain about 3,000-5,000 pounds of nitrogen per acre, with 1-3% converting to plant-available nitrogen yearly. In comparison, irrigation water typically contains 2-45 milligrams per liter of nitrate-nitrogen, which can provide a substantial portion of crop needs when considering the total volume of irrigation water applied.	['You are here\nOne of the amazing things about your soils is that many of them already have enough nitrogen to grow an optimal corn crop this year – and for years to come – without applying any more.\nSo why do you have to pay for commercial nitrogen (N) to secure top-notch yields?\nBecause most of what is there isn’t available to be taken up by the roots. For that to happen, the process of N mineralization (the natural conversion of organic nitrogen in the soil to the ammonium form) has to take place.\nFabian Fernandez, a professor and Extension educator in nutrient management and environmental protection at the University of Minnesota, is as perplexed about this as you are.\n“The right amount of applied nitrogen can vary from year to year,” he says, “because the amount that’s mineralized varies.”\nExactly why that is and what you can do to get the soil to give up more of its stored N is a head-scratcher even to the experts. What they do know is, if you can get more mineralization, you might be able to reduce the amount and expense of commercial N. Meanwhile, you might also improve the timing of commercial applications if N applications are needed. This can reduce leaching and runoff into water sources in addition to securing enough N for high yields.\n- About 5% of the organic matter in soils is nitrogen, says Fernandez.\n- About 1% to 3% of that N converts to plant-available nitrogen every year.\n- Soils with 3% to 5% organic matter have about 3,000 pounds to 5,000 pounds of N per acre in the top few inches. If 3% converts to plant-available N in a given year, that’s 90 to 150 pounds of N that you get free!\nThe problem is, you don’t know which year or which set of circumstances will see that much mineralization or if there will be none at all.\n“Lots of people are trying to recommend rates of applied nitrogen based on standards of mineralization,” says Fernandez. “We have very little information on that. For example, in one of our studies in several soils across Minnesota, soil organic matter varies from as low as 1.5% to as high as 5.6%. That’s a huge range of nitrogen available to be mineralized.”\nThen, there’s the issue of what makes mineralization happen. At least two things stand out, but neither is crystal clear.\nOne is the application of commercial N. “In our field and laboratory studies, we see that as we apply commercial nitrogen, we get more mineralization,” Fernandez says. “It’s like we sort of prime the pump for mineralization.”\nFernandez speculates that it’s because the applied N enhances the microbes and soil chemistry that allows for more mineralization in the organic matter.\n“The amount of nitrogen we get mineralized far exceeds the amount we get when no supplemental nitrogen is applied,” he says. “We don’t have enough data yet to really get to the reason.\n“Another question is how much nitrogen is needed to get the full priming effect,” he adds. “At this point, I don’t think it is linear – it won’t increase indefinitely with increasing N rates. I’m not sure how much is needed to maximize mineralization in different soils.”\nThe second issue affecting N mineralization is soil saturation with water. “We know mineralization is an aerobic process; it requires oxygen,” he says. “So in theory, well-drained soils should have more mineralization.”\nThat theory holds up in lab experiments. Yet, in preliminary and limited field trials, Fernandez has documented more mineralization in undrained soils.\n“We’re asking why that would be,” he says. “It makes the point that sometimes there are book values generated in the lab that may or may not relate to what is actually happening in the field.\n“A lot of things impact mineralization, but we just have low understanding of them,” Fernandez summarizes. “We are going to figure this out. If we can predict more closely how much nitrogen mineralizes, then we can apply the right amount of fertilizer more precisely than we apply it today. That will translate into greater efficiency.”\nWhat if you don’t apply N?\nFabian Fernandez has reviewed dozens of corn yield studies with varying rates of nitrogen. In virtually all of those trials, there’s a check plot that gets no N at all. In the recent studies, corn yields in the check plots have varied from a low of 52 bushels an acre to a high of 218 bushels.\n“The average is 116 bushels,” says Fernandez. “We get about half a yield with no N fertilizer.”\nThere is that odd year or field where the check plot yields as well as the plot that got a full rate of nitrogen. Fernandez admits that can be baffling.\n“We’ve just been looking at this seriously for two years,” he says. “I’m optimistic we will find answers and make progress on this.”\nWritten by Gene Johnston', 'Field trials show the fertilizer value of nitrogen in irrigation water\nCalifornia Agriculture 71(2):62-67. https://doi.org/10.3733/ca.2017a0010\nPublished online April 12, 2017\nIncreased regulatory activity designed to protect groundwater from degradation by nitrate-nitrogen (NO3-N) is focusing attention on the efficiency of agricultural use of nitrogen (N). One area drawing scrutiny is the way in which growers consider the NO3-N concentration of irrigation water when determining N fertilizer rates. Four drip-irrigated field studies were conducted in the Salinas Valley evaluating the impact of irrigation water NO3-N concentration and irrigation efficiency on the N uptake efficiency of lettuce and broccoli crops. Irrigation with water NO3-N concentrations from 2 to 45 milligrams per liter were compared with periodic fertigation of N fertilizer. The effect of irrigation efficiency was determined by comparing an efficient (110% to 120% of crop evapotranspiration, ETc) and an inefficient (160% to 200% of ETc) irrigation treatment. Across these trials, NO3-N from irrigation water was at least as efficiently used as fertilizer N; the uptake efficiency of irrigation water NO3-N averaged approximately 80%, and it was not affected by NO3-N concentration or irrigation efficiency.\nCalifornia agriculture faces increasing regulatory pressure to improve nitrogen (N) management to protect groundwater quality. Groundwater in agricultural regions, such as the Salinas Valley and the Tulare Lake Basin, has been adversely impacted by agricultural practices, with nitrate-N (NO3-N) in many wells exceeding the federal drinking water standard of 10 mg/L (Harter et al. 2012). The threat to groundwater is particularly acute in the Salinas Valley, where the intensive production of vegetable crops has resulted in an estimated net loading (fertilizer N application – N removal with crop harvest) of > 100 lb/ac (> 112 kg/ha) of N annually (Rosenstock et al. 2014).\nLevels of NO3-N in irrigation wells in the Salinas Valley commonly range from 10 to 40 mg/L. Given the typical volume of irrigation water applied to vegetable fields, NO3-N in irrigation water could represent a substantial fraction of crop N requirements, provided that crops can efficiently use this N source. Indeed, the concept of “pump and fertilize” (substituting irrigation water NO3-N for fertilizer N) has been suggested as a remediation technique to improve groundwater quality in agricultural regions (Harter et al. 2012).\nInexpensive nitrate test strips allow on-farm estimation of irrigation water NO3-N concentration. In Salinas Valley irrigation wells, levels of NO3-N commonly range from 10 to 40 mg/L, which could supply a substantial portion of crop N requirements.\nCooperative Extension publications from around the country (Bauder et al. 2011; DeLaune and Trostle 2012; Hopkins et al. 2007) agree that the fertilizer value of irrigation water NO3-N can be significant, but they differ as to what fraction of water NO3-N should be credited against the fertilizer N recommendation. There is a paucity of field data documenting the efficiency of crop utilization of irrigation water N. Francis and Schepers (1994) documented that corn could use irrigation water NO3-N, but in their study N uptake efficiency from irrigation water was low, which they attributed to the timing of irrigation relative to crop N demand and the availability of N from other sources. Martin et al. (1982) suggested that uptake efficiency of irrigation water NO3-N could actually be higher than from fertilizer N, but their conclusion was based on a computer simulation, not on field trials.\nWith this near total lack of relevant field data, California growers have legitimate concerns about the degree to which irrigation water NO3-N can substitute for fertilizer N. Two questions commonly asked by growers are whether plants can effectively use N at the low concentrations common in irrigation water, and to what degree irrigation inefficiency reduces water NO3-N availability. We undertook this study to document the agronomic value of irrigation water NO3-N in the production of vegetable crops under field conditions representative of the Salinas Valley.\nIrrigation water NO3-N trials\nFour field trials were conducted at the U.S. Department of Agriculture Agricultural Research Service (USDA-ARS) facility near Salinas between 2013 and 2015. The soil was a Chualar sandy loam. Before planting, fields were sprinkler-irrigated to leach residual soil NO3-N so that all trials were conducted with low background soil N availability. The well water used for pre-plant leaching as well as for all in-season irrigation ranged between 2 and 4 mg/L NO3-N over the course of this study. The experimental design for each trial was a randomized complete block, with four replications. Individual plots consisted of four beds, each 40 inches (1 meter) wide and 40 feet (12.2 meters) long, with all data collected from the middle two beds.\nCrisphead lettuce ‘Telluride’ was seeded on May 16, 2013, in two rows per bed and germinated using sprinklers. A soil anticrustant solution containing 17 lb/ac (19 kg/ha) of N was applied to all treatments at planting to improve germination. After plants were thinned to a final in-row spacing of approximately 12 inches (30 centimeters), drip tape was installed on top of the beds and the field was drip-irrigated for the rest of the season.\nCrop growth and N uptake were compared across a range of treatments simulating different irrigation water NO3-N concentrations during the drip-irrigated phase of the crop. The different NO3-N concentrations were achieved by using water-powered proportional injectors to enrich all drip-applied water to 12, 25 or 45 mg/L NO3-N. Injected NO3-N was a blend of Ca(NO3)2 and NaNO3 to maintain a cation balance similar to groundwater (Ca:Na milliequivalent ratio of 1.0). A water sample was collected from each treatment during each irrigation to confirm that the target NO3-N concentrations were achieved. Additionally, an unfertilized control and a fertilized control treatment were included; both were irrigated using water containing only 2 mg/L NO3-N. The fertilized control received five fertigations of ammonium nitrate solution (AN-20) totaling 150 lb/ac (168 kg/ha) of N. Also, all treatments were fertilized with potassium thiosulfate (KTS) in two fertigations of 30 lb/ac (34 kg/ha) of K each.\nCalculating the N in irrigation water\nCalculation of the amount of nitrogen in irrigation water requires knowledge of both the N concentration and the volume of water applied. Laboratory analysis for nitrate in water is commonly reported as milligrams per liter (mg/L) or parts per million (ppm); these units are numerically the same: 1 mg/L equals 1 ppm. Labs may report concentration either as nitrate (NO3−) or nitrate-N (NO3-N); the conversion between the two is\nTo convert NO3-N concentration to mass of N applied, this equation can be used:\nNitrate is usually the only form of N present in irrigation water in an agronomically significant amount, so it is the only N form reported on the typical water test. However, recycled municipal wastewater, which is increasingly being used for irrigation in California, can contain more ammonium N (NH4-N) than NO3-N, as well as some organic forms of N that become relatively quickly available in soil. Wastewater treatment plants routinely test for these other N sources in addition to NO3-N, and this information is publicly available. One should consider all forms of N when estimating the amount of plant-available N in recycled water.\nEach N treatment was evaluated at two levels of irrigation to observe the interaction between irrigation efficiency and crop uptake of irrigation water NO3-N. The lower level of irrigation, 110% of crop evapotranspiration (ETc), was chosen to represent efficient management with minimal leaching. The higher level of irrigation, 160% of ETc, was chosen to represent less efficient irrigation management; we have observed a number of Salinas Valley vegetable fields in which irrigation reached as high as 200% of ETc (Smith et al. 2016). Applying 160% of ETc generated an estimated leaching fraction of 37% (Cahn and Bali 2015). ETc was estimated by multiplying reference evapotranspiration (ETo) values obtained from the CIMIS weather station located on the USDA-ARS facility by crop coefficients calculated by the method described by Johnson et al. (2016). Irrigation was applied twice weekly. Data on ETc and irrigation volume are given in table 1. Precipitation was an insignificant factor, with < 0.2 inches (< 0.5 cm) received in any trial.\nTABLE 1. Inches of crop evapotranspiration (ETc) and irrigation applied during the drip-irrigated portion of the field trials\nA second trial of the same structure was conducted in 2014. Broccoli ‘Patron’ was seeded on Aug. 18 in two rows per bed and germinated with sprinkler irrigation following an anticrustant application containing 23 lb/ac (26 kg/ha) of N. After crop establishment and bed cultivation, the trial was converted to surface drip irrigation. The irrigation levels evaluated were 110% and 190% of ETc. The fertilized control treatment received three fertigations of AN-20 totaling 220 lb/ac (246 kg/ha) of N. All treatments were also fertigated with KTS in two applications of 25 lb/ac (28 kg/ha) of K.\nAn on-site CIMIS weather station provided accurate evapotranspiration data. Each N treatment was evaluated at two levels of irrigation — 110% and 160% of ETc.\nTwo trials were conducted in 2015 to directly compare the uptake efficiency of irrigation water NO3-N to that of fertilizer N. In the spring trial, crisphead lettuce ‘Telluride’ was seeded and germinated as previously described. After converting the field to drip irrigation, four levels of fertigation (a seasonal total of 0, 20, 60 and 150 lb/ac [0, 22, 67 and 168 kg/ha] of N from AN-20, applied in three equal fertigations) were compared at each of two irrigation levels (110% and 180% of ETc). In each irrigation treatment, three concentrations of irrigation water NO3-N (14, 25 and 45 mg/L) without any AN-20 fertigation were also evaluated. In the fall trial, broccoli ‘Patron’ was grown. The treatments were similar to the lettuce trial, with the exception that the seasonal AN-20 fertigation levels were 0, 40, 80 and 200 lb/ac (0, 45, 90 and 224 kg/ha) of N. The irrigation levels evaluated were 120% and 200% of ETc.\nIn all trials, plots were harvested when the highest fertilizer N rate treatment reached commercial maturity. Aboveground fresh and dry biomass and whole-plant N concentration were determined. From these data, crop N uptake was calculated. Uptake efficiency of irrigation water NO3-N was calculated as the increase in crop N uptake above the unfertilized control divided by the amount of NO3-N in the applied water.\nUptake efficiency of NO3-N\nLettuce biomass and crop N uptake increased linearly with increasing irrigation water NO3-N concentration in the 2013 trial (fig. 1). Across the NO3-N enrichment levels, uptake efficiency of irrigation water NO3-N was 85%, and it was similar between the levels of irrigation (which received 7.0 and 10.1 inches [18 and 26 centimeters] of drip irrigation in the 110% and 160% ETc treatments, respectively). The amount of N applied in the 45 mg/L water treatment at 160% of ETc (91 lb/ac, or 102 kg/ha) was sufficient to maximize crop productivity, producing fresh biomass equivalent to the biomass of the fertilized control receiving 150 lb/ac (168 kg/ha) of N from AN-20.\nFig. 1. Effect of irrigation water NO3-N on lettuce biomass and aboveground N uptake, 2013 trial; water NO3-N concentrations were 2, 12, 25 and 45 mg/L.\nResults of the 2014 broccoli trial were similar, with crop biomass and N uptake increasing linearly with increasing irrigation water NO3-N concentration (fig. 2). Uptake efficiency of irrigation water NO3-N was again high (78%) across NO3-N concentrations and irrigation levels. However, given the much higher N requirement of broccoli compared to lettuce, even the 45 mg/L NO3-N water treatment was insufficient to maximize crop productivity.\nFig. 2. Effect of irrigation water NO3-N on broccoli biomass and aboveground N uptake, 2014 trial; water NO3-N concentrations were 2, 12, 25 and 45 mg/L.\nThe 2015 trials clearly demonstrated that irrigation water NO3-N was at least as effectively used by the crop as fertilizer N. The regression lines in figures 3 and 4 indicate the crop response to fertigation with AN-20 at the two levels of irrigation; all regressions were highly significant (p < 0.001). The fact that the irrigation water NO3-N treatments generally placed above the fertilizer response line for their respective irrigation regimes suggested that a higher N uptake efficiency was achieved with irrigation water NO3-N than with N from fertigated AN-20. This was most pronounced in the broccoli trial (fig. 4), where the N uptake efficiency for fertilizer was substantially lower under the high irrigation level (200% of ETc).\nFig. 3. Comparison of lettuce response to N fertilizer (solid and dashed lines) with crop response to irrigation water NO3-N, 2015 trial; water NO3-N concentrations were 14, 25 and 45 mg/L.\nFig. 4. Comparison of broccoli response to N fertilizer (solid and dashed lines) with crop response to irrigation water NO3-N, 2015 trial; water NO3-N concentrations were 14, 25 and 45 mg/L.\nAveraged across all field trials, the N uptake efficiency of irrigation water NO3-N was remarkably high, averaging approximately 80% (fig. 5). Neither NO3-N concentration nor irrigation level significantly influenced N uptake efficiency. It must be noted that the high N uptake efficiency in these trials was attributable to the fact that residual soil NO3-N in these fields had been deliberately minimized by heavy preplant leaching in order to maximize the uptake efficiency of both fertilizer N and water NO3-N. In typical production fields, higher levels of residual soil NO3-N are common, and N uptake efficiency of applied N, whether from irrigation water or fertilizer, would likely be lower.\nFig. 5. Influence of irrigation water NO3-N concentration (mg/L, across irrigation levels) and irrigation level (across water NO3-N concentrations) on the mean N uptake efficiency of irrigation water NO3-N across the four field trials. Bars represent the 95% confidence interval of the measurement.\nCalculating “fertilizer credits”\nThese field trials unequivocally demonstrated that vegetable crops can effectively use NO3-N from irrigation water, even at relatively low concentration. The important question is how can growers safely estimate an appropriate fertilizer credit for irrigation water NO3-N. In answering that question, it is important to distinguish between N uptake efficiency and a fertilizer credit. N uptake efficiency refers to the fraction of applied N taken up by the crop. N uptake efficiency from either fertilizer or irrigation water is affected by overall soil N availability (all sources, including residual soil NO3-N and soil N mineralization); as total N availability increases, N uptake efficiency from either fertilizer or irrigation water will decline. A fertilizer credit is the comparison of the relative availability of N from irrigation water and from fertilizer N.\nDrip irrigation increases irrigation efficiency and simplifies the determination of the “fertilizer credit” for irrigation water NO3-N.\nSeveral factors need to be considered in calculating a fertilizer credit. First, the stability of the irrigation water NO3-N concentration over time is important. In general, surface water sources have reasonably low but stable NO3-N, typically < 5 mg/L. Water districts usually have historical records that provide good estimates of NO3-N concentration for the current season. Nitrate concentration in irrigation wells may be more variable, so periodic monitoring within a growing season may be appropriate. Growers who use several wells of differing NO3-N concentration to irrigate a field would need to monitor the NO3-N concentration of the blended water. This can be accomplished by collecting water in a covered bucket using a drip emitter connected to the irrigation main line; this sample can be tested using nitrate-sensitive colorimetric test strips.\nSecond, it may be necessary to consider irrigation inefficiency when calculating a fertilizer credit, depending on the details of the irrigation management. In this study, drip irrigation was used, with frequent irrigation at relatively low volume, typically < 0.6 inches (1.5 centimeters) per application; even in the high irrigation treatment (160% to 200% of ETc), the volume of leachate from individual irrigations was small. Under these conditions, N uptake efficiency was similar in the high and low irrigation regimes, indicating that the crops were able to remove a substantial amount of NO3-N even from the fraction of applied water that eventually leached. This phenomenon may relate to the residence time of applied water within the active root zone. With low volume leaching events, it may take several irrigation cycles before water moves below the root zone, giving the crop the opportunity to take up applied NO3-N. In a fertigation trial with bell pepper, Scholberg et al. (2009) found that increasing fertilizer retention time from 1 to just 3 days quadrupled fertilizer N uptake efficiency.\nConversely, when irrigation management features large leaching events, particularly early in the season when crop N uptake is slow and before a substantial root system has developed, crop access to and use of irrigation water NO3-N would be limited, and this should be considered in the fertilizer credit calculation. In the context of vegetable production, irrigation to germinate seeded crops or to establish transplants would be particularly vulnerable to inefficiency. It may be appropriate not to credit any of the irrigation water NO3-N applied during crop establishment. Presidedress soil nitrate testing (PSNT, a valuable practice to evaluate N fertilizer requirements; Hartz 2003) would capture any N contribution from irrigation water still in the root zone following establishment. From that point forward, crediting 100% of irrigation water NO3-N against the assumed fertilizer N requirement would be a reasonable practice if in-season irrigation were managed efficiently. Where in-season irrigation results in large leaching events, a smaller fertilizer credit could be justified. However, it should be acknowledged that large leaching events may similarly restrict crop recovery of fertilizer N.\nThese field trials documented that NO3-N in irrigation water is effectively used by crops. Growers can confidently adjust their fertilization practices to reflect the agronomic value of this N source. In doing so they will reduce the potential for N loading to groundwater.']	['<urn:uuid:40de5eee-d16b-4e39-b530-a563e900f4e3>', '<urn:uuid:d2775de1-7603-44a1-bf72-d1c2327f6e10>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T00:57:18.859842	10	51	3753
31	How are mixed alcohols produced industrially, and what dangers do they pose?	Mixed alcohols are produced industrially using a sulfided, nanosized transition metal catalyst suspended in a slurry. The process involves contacting this slurry with carbon monoxide and hydrogen at temperatures of 200-325°C and pressures of 500-3000 psig. However, these industrial alcohols can be extremely dangerous if consumed. They often contain toxic additives and chemicals like methanol that can cause severe illness, permanent blindness, or death. Industrial spirits should never be ingested as they are designed to be unpalatable and are frequently used in manufacturing products like adhesives, cosmetics, and pharmaceuticals.	"['Easy To Use Patents Search & Patent Lawyer Directory\nAt Patents you can conduct a Patent Search, File a Patent Application, find a Patent Attorney, or search available technology through our Patent Exchange. Patents are available using simple keyword or date criteria. If you are looking to hire a patent attorney, you\'ve come to the right place. Protect your idea and hire a patent lawyer.\nMethod for production of mixed alcohols from synthesis gas\nA method for production of mixed alcohols by using a sulfided transition\nmetal catalyst selected from Group VI metals; nano-sizing the metal\ncatalyst during its synthesis; suspending the catalyst in solvents to form\na slurry; adding, a sulfur containing material to extend catalyst life;\nand contacting this slurry with carbon monoxide and hydrogen at\n200-325.degree. C. and 500-3000 psig pressure.\nJackson; Gene R. (Arvada, CO), Mahajan; Devinder (South Setauket, NY)\nPrimary Examiner: Parsa; J.\nAttorney, Agent or Firm:Margolis; Donald W.\nParent Case Text\nRELATED U.S. APPLICATION DATA\nThis application is a division of Ser. No. 09/438,333 Nov. 13, 1999 U.S.\nPat. No. 6,248,796 which claims benefit of Ser. No. 60/108,364 Nov. 13,\n1998 for A NOVEL METHOD FOR PRODUCTION OF MIXED ALCOHOLS FROM SYNTHESIS\nWhat is claimed is:\n1. Methods for the production of mixed alcohols including the steps of: using a sulfided, nanosized transition metal catalyst selected from Group VI metals; nanosizing the\nGroup VI transition metal catalyst by selecting Group VI metals, and mixtures thereof, and then nanosizing said Group VI metals and mixtures thereof to a mean particle diameter of about 100 nm; suspending the Group VI transition metal catalyst in a\nliquid to form a slurry; and contacting said slurry with gases including carbon monoxide and hydrogen at a temperature in the range of about 250.degree. C. to about 325.degree. C. and at a pressure in the range of about 500 psig to about 3000 psig, to\nthereby produce mixed alcohols.\n2. The method of claim 1 wherein the nanosized Group VI transition metal catalysts is sulfided prior to its use in producing mixed alcohols from gases including carbon monoxide and hydrogen.\n3. The method of claim 1 wherein the nanosized Group VI transition metal catalysts are selected from Cr, Mo and W, and mixtures thereof.\n4. The method of claim 2 wherein the nanosized Group VI transition metal catalysts, and mixtures thereof are produced including the step of sulfiding said nanosized Group VI transition metal catalysts, and mixtures thereof.\n5. The method of claim 4 wherein the nanosized Group VI transition metal catalysts, and mixtures thereof, are selected from Cr, Mo and W, and mixtures thereof.\nBACKGROUND OF THE\n1. Field of the Invention\nThe present invention relates to a novel slurry-phase method to produce mixed alcohols from synthesis gas by utilizing a nanosized catalyst. The catalyst is activated by nanosizing and sulfiding during catalyst preparation.\n2. Discussion of the Prior Art\nSynthesis gas, hereinafter ""syngas"" is produced from any organic/carbonaceous source, such as, but not limited to municipal solid waste (MSW), refuse derived fuel (RDF), biogas from a digester, sewage sludge, chicken manure, turkey manure, other\nanimal and agricultural waste, corn stover, switch grass, timber, grass clippings, construction demolition materials, cotton gin waste, biomass, landfill gas, natural gas and the like. The catalytic production of mixed alcohols from synthesis gas is a\nwell established route and the literature contains numerous examples pertaining to this transformation. Of particular interest is a method described in U.S. Pat. Nos. 4,675,344; 4,749,724; 4,752,622; 4,752,623; and 4,762,858, all originally assigned\nto Dow Chemical Company. These patents describe, in general, a micron-size supported catalyst based on molybdenum disulfide (MoS.sub.2). Mixed alcohols, primarily C.sub.1 -C.sub.4, i.e. methanol-butanol, are produced in good yields when the Dow\ncatalyst is used in a packed column or fluidized bed. The best yield of oxygenates fraction is approximately 20%, on a CO.sub.2 -free basis, with up to 85% selectivity to mixed alcohols. The rate of 0.1-0.4 grams product/gram catalyst/hour is claimed\nby the use of the Dow catalysts at 240-325.degree. C. reaction temperature and 700-3000 psig. The above Dow patents and the references discussed and cited therein are incorporated by reference in this application.\nReview of the above noted prior art and references will show that a process and catalyst that improves upon conditions of high temperature and high pressure conversion of synthesis gas to mixed alcohols, and which provides a higher conversion\nrate of synthesis gas to mixed alcohols per pass over/through the catalyst are highly desirable, especially for commercial applications.\nFor commercial application, a process that improves upon conditions of high temperature and high pressure and allows higher conversion per pass are highly desirable. To make a commercially significant alcohol process the catalyst must be highly\nefficient as well as the conditions in which the catalyst operates. The efficient catalyst must yield a high ratio of mass of product per given mass of catalyst in a given period of time. The catalyst must be stable and active over long periods of time\nbefore regeneration or replacement of the catalyst is required. When the feed gas has a low ratio, ideally when the H.sub.2 /CO ratio is less than 2 to 1, the catalyst will be highly selective to produce a commercial product to avoid purification or\nremoval and disposal of by-products with the addition of a distillation tower that will split the product into two or more product streams.\nWhen the mixture is used as a neat fuel for automobiles the presence of C.sub.1 alcohol, i.e. methyl alcohol, is more beneficial than when the alcohols are used as a commercial blend in gasoline. As used in this application, the weight ratio of\nmethanol or C.sub.1 alcohol to C.sub.2 +alcohols means the higher alcohols, such as ethanol, propanols, butanols, etc., taken as a whole for calculation purposes. This number may be calculated by determining the weight fraction of methanol in the mixed\nalcohols. The esters or ethers portion of the alcohol mix are not included in either the C.sub.1 to C.sub.2 + numbers. It is therefore understood that it would be beneficial to prepare mixed alcohols, primarily C.sub.1 -C.sub.4, from synthesis gas\nderived from any carbonaceous source. It would also be beneficial to produce mixed alcohols in a highly efficiently manner by a catalytic method, i.e. in high yield per pass to avoid gas recycle under mild conditions of temperature and pressure.\nSUMMARY OF THE INVENTION\nIt is therefore an object of the present invention to prepare mixed C.sub.1 -C.sub.4, alcohols, primarily from synthesis gas derived from any carbonaceous source.\nIt is therefore an object of the present invention to produce mixed alcohols in a highly efficient manner by a catalytic method, having a high yield per pass to avoid gas recycle under mild conditions of temperature and pressure.\nThe present invention provides a novel method for producing mixed alcohols by combining one or more of the following steps. First a catalyst is selected from the Group VI metals, namely Cr, MO, W and mixtures thereof Next, the selected metal is\nnanosized to a mean particle diameter (ND) of less than about 100 nm. Nanosizing the metal is an especially important feature of the present invention, in that nonosizing provides more surface area per unit volume of the metal, thereby enhancing the\nreaction rates. The nanosized metal catalyst is then sulfided to enhance its resistance to the catalyst poisons that are normally present in syngas.\nNanosizing of the metal catalyst can be achieved by a variety of methods. One preferred method of nanosizing is sonication of a carbonyl precursor of the metal. An example of nanosizing is found in the literature reference (Mdleleni er al. J.\nAmer. Chem. Soc. 120 6189-6190 (1998)). Catalyst-sulfiding can be achieved during or after the nanosizing procedure.\nDuring the alcohol production from syngas production, the nanosized, sulfided Group VI metal catalyst may be unsupported, or it may be supported on a high surface area support such as carbon, alumina, silica or the like. In either arrangement\nthe nanosized catalyst is suspended in an inert solvent, such as a high molecular weight hydrocarbon solvent such as ethylflo-164, to form a slurry. Suspension of the nanosized catalyst allows excellent heat management during the thermal operations, and\nthis in turn increases mixed alcohol product yield.\nThe syngas is then passed through the catalyst slurry to produce alcohols in the product stream. The input syngas composition varies from H.sub.2 /CO of 1/4 to 3/1 though other gaseous impurities may be present. In order to enhance reaction\nrates, additives that make the Group VI metal catalyst more susceptible to initial carbon monoxide attack are preferred. The operating temperature range is from about 200.degree. C. to less than about 300.degree. C. The operating pressure ranges is\nfrom about 500 to about 3000 psig. The space-time-yield (STY) of product mixed alcohols is better than those claimed by any known commercial methods, that is greater than about 0.4 gram product/gram catalyst/hr. In order to enhance catalyst life, a\nsmall amount of sulfur source is added either directly to the reaction vessel in which alcohols are being continuously produced, or to the incoming syngas stream.\nBy combining these steps, using a novel catalyst in a novel process efficiently produces mixed alcohols. In one embodiment of this invention, nanosized particles (MPD<100 nm) of molybdenum are produced by the sonication method and are\nsuspended in a hydrocarbon solvent having a carbon chain length of 30. A sulfur source, being elemental sulfur itself, is added to the slurry. By contacting the slurry with a stream of carbon monoxide and hydrogen in 1/2 ratio at temperature in the\nrange of about 250 to about 280.degree. C. and pressure in the range of about 500 to about 200 psig pressure, mixed alcohols are produced in the product stream with STY surpassing 0.4 grams product/gram catalyst/hour reported with the known prior art\nThese and other objects of the present invention will become apparent to those skilled in the art from the following detailed description and accompanying drawings, showing the contemplated novel construction, combination, and elements as herein\ndescribed, and more particularly defined by the appended claims, it being understood that changes in the precise embodiments to the herein disclosed invention are meant to be included as coming within the scope of the claims, except insofar as they may\nbe precluded by the prior art.\nBRIEF DESCRIPTION OF THE DRAWINGS\nThe accompanying drawing which is incorporated in and form a part of this specification illustrate complete preferred embodiments of the present invention according to the best modes presently devised for the practical application of the\nprinciples thereof and in which:\nFIG. 1 is a conceptual diagram of the entire process from syngas feed to mixed alcohol storage.\nDETAILED DESCRIPTION OF THE PROCESS\nDuring the municipal solid waste treatment process the waste material is sorted to be free of all metals including aluminum, and glass. Plastics may or may not be separated depending upon the value of the recycled plastics at the time. The\nmaterial is then gasified, cooled and cleaned. The synthesis gas that is produced by this process will be at a ratio that will vary from H.sub.2 /CO of 1:1.2 to 1:2 although other gaseous impurities may be present. The gaseous material will then be\ncompressed at approximately 100.degree. F. at a pressure from about 500 to about 1000 psig and passed through the novel nanosized suspended Group VI metal to produce mixed alcohols with STY surpassing 0.4 grams product/gram catalyst/hour.\nThe digestion of manure from all types of animals produces syngas, although manure from diaries and hog farms and feedlots have been targeted. The digestion process will yield several more moles of methane than of carbon dioxide. A commercially\navailable partial oxidation unit then disassociates the methane gas. The syngas from the partial oxidation unit is expected to yield a 1:1 ratio of H.sub.2 to CO. The gaseous material will then be compressed at approximately 100.degree. F. at a\npressure from about 500 to about 1000 psig and passed through the novel nanosized suspended Group VI metal to produce mixed alcohols with STY surpassing 0.4 grams product/gram catalyst/hour.\nWhere waste rubber, such as tires and or autofluff become plentiful and needs to be processed, a pyrolyzer will be utilized to produce syngas. The syngas ratio can vary widely with these processes and the material being processed will also vary. The syngas from the pyrolyzer unit is expected to yield a 1:1 or 1.1.4 ratio of H.sub.2 to CO.\nA steam reformer, such as those found on a typical methanol plant, may be utilized with a recirculation of the hydrogen back into the process to be utilized for makeup heat as well as additional carbon monoxide being manufactured from the carbon\ndioxide through the hydrogen burner unit furnished by others in the process.\nTo provide a commercially significant alcohol process, the present invention uses a catalyst and conditions which are highly efficient. To be efficient the catalyst must yield a high ratio of mass product per given mass of catalyst in a given\nperiod of time. The catalyst must be stable and highly active for long periods of time between regenerations. This is particularly difficult to accomplish when the H.sub.2 /CO ratio of feed gas is low, such as less than about 2 to 1. Ideally the\ncatalyst is highly selective to commercial product to avoid purification or removal and disposal of by-products and to avoid separation into two or more product streams. The use of a partial an oxidation unit before placing the syngas stream into the\ncatalyst slurry bed, or other catalyst presentation method, is chosen to make the mixed alcohol and greatly enhances the ability of the catalyst to select the desired ratio of alcohols.\nThe replacement or the use of the alcohol as an additive to gasoline the ratio of the C.sub.1 to C.sub.2 + alcohols should be no greater than a certain amount. As used in this application, the ratio of C.sub.1 to C.sub.2 + means the weight ratio\nof methanol to higher alcohols such as ethanol, propanols, butanols, and the like, taken as a whole. This number may be easily calculated by determining the weight fraction of methanol in the mixed alcohols with the desired mixture for mixing with\ngasoline to be almost zero on the C.sub.1, alcohols.\nThrough all of these processes it is desired that ethyl alcohol be a major product constituent, with the yield of methanol at a very small portion of the overall product. While this process is an advance over the art it would be advantageous if\nit were possible to increase the C.sub.2 and other alcohols and decrease the percentage of methanol in the mixes made when using the mix as a blend in gasoline. Under preferred conditions, alcohols may be obtained in about 95 percent per pass of the\nH.sub.2 /CO syngas at any preferred ratio. The selectivity of the C.sub.2 and other higher alcohols are preferred and should be obtained with this invention. The space velocity of the hourly rate that the H.sub.2 /CO gas passes a given volume of\ncatalyst in an hour\'s time (GHSV) is a measure of the volume of the hydrogen plus carbon monoxide gas at a standard temperature and pressure. The selectivity of the alcohols generally increase as the space velocity increases, however conversion of the\ncarbon monoxide decreases as the space velocity increases. Some of these gases may be recycled in the reaction; however, the recycle ratio of zero is within the scope of the invention because of the highly active catalyst.\nWhere shut-in natural gas, pipeline natural gas or landfill gas is plentiful it needs to be processed by other means than cleaning it to enter the market place via the pipeline. The syngas ratio will be fairly stable in all of these applications\nbut should yield a very stable syngas. An autotherm reformer or partial oxidation unit will be utilized to produce syngas that is expected to yield a 1:1 or 1:1.2 ratio of H.sub.2 to CO.\nFIG. 1 is a conceptual diagram of the entire process from syngas feed to mixed alcohol storage. The mixed alcohol is referred to be its trademark ""ECALENE"".\nIt will therefore be appreciated that in the practice of the processes and in the use of the catalysts of the present invention, a novel process results that efficiently produces mixed alcohols from syngas.', 'Dangers of Industrial Spirits\nIndustrial alcohol is distilled ethyl alcohol (C2H5OH), normally of high proof, produced and sold for other than beverage purposes. It is usually distributed in the form of pure ethyl alcohol, completely denatured alcohol, specially denatured alcohol and solvent blends. Pure ethyl alcohol is used in laboratories and in industry for its sanitizing, cleaning and solvent properties. Many medicines,\nfood products, flavorings and cosmetics are made with ethyl alcohol. It is also used to process vaccines, syrups, tinctures, liniments and antiseptics and in the manufacture of pharmaceuticals such as chloroform and barbiturates. It may be used in the production of adhesives, cosmetics, detergents, explosives, inks, hand cream, plastics and textiles.\nIndustrial spirits often have similar properties to alcohol and are chemically similar to drinking alcohol. However, these other types of alcohol may cause a person to become violently ill, damage vital organs or become unconscious when consumed. Industrial spirits should never be ingested under any circumstance. Many industrial spirits have colors or bitter tasting additives that are designed to make them unpalatable or unappealing.\nIndustrial Spirits are Deadly Chemicals\nDeterrents do not stop some people from trying these chemicals as a way to get drunk. The risks associated with drinking these chemicals are outweighed by the potential for getting high or euphoric. Some may believe that the adverse reactions that the chemicals cause to the body will not affect them. Or they may be living in a country where access to alcohol is difficult or too expensive so they will use these other spirits to feed their addiction. In some instances, industrial spirits are used to manufacture illicit alcohol and the end consumer is not aware of the presence of deadly chemicals. Spirits such as gin and vodka are often made with these spirits as a counterfeit product or sold as a home-made vodka. South Africa has had a problem with the sale of industrial spirits as a legitimate alcohol product that contains toxic chemicals. In some case counterfeit products are made from recycled bottles of alcohol filled with cheaply made vodka or gin concoctions. Advice from the South African authorities is to avoid purchasing very cheap products or purchasing alcohol from non-authorized avenues.\nIn Russia, authorities have had to deal with a number of deaths that were found to be caused by the consumption of deadly alcoholic concoctions that were made with industrial spirits. Due to increasing poverty issues, young men were found to be drinking cleaning agents, perfumes and other industrial chemicals as a way to become intoxicated. A study published in The Lancet found that over 1,500 men died prematurely due to the consumption of these deadly chemicals\nMethanol & Methylated Spirits\nMethanol is an incredibly dangerous and potentially life-threatening spirit that causes high toxicity in humans if ingested. Methanol can cause damage to the optic nerve and may cause permanent blindness. This chemical is very similar to ethanol (drinking alcohol) and is light, colorless and flammable. When first ingested methanol causes a range of adverse symptoms which include headaches, nausea, lack of coordination, depression, confusion and unconsciousness. In large doses methanol can cause death. Secondary symptoms can take up to 30 hours after ingesting to occur and include blurring and loss of vision, respiratory depression and acidosis.\nMethylated spirits are another dangerous chemical that may be consumed by some people as a substitute spirit. Also known as denatured alcohol, methylated spirits is pure alcohol with additives that are bad tasting, poisonous or change the color of the alcohol. It is not a chemically altered ethanol molecule rather a mixture of chemicals. Methylated spirits are used as a solvent, for fuel and other industrial uses. It can cause blindness, severe nausea and death if it contains methanol.\nHome Made Alcohol\nHome made alcohol can be distilled through a process of using industrial spirits along with other fermented fruits or grains. Inexperience can place brewers and drinkers at risk of unknowingly using harmful and toxic chemicals. These chemical combinations can cause blindness or death to those who drink them, especially if they contain high amounts of methanol.\nThe dangers of home brewed or home made alcohols are most severe in developing nations where people who consume the alcoholic mixes are unable to drink high quality beverages. In these regions, alcohol is often unregulated or sold on the black market and is deadly. High octane fuel or other chemicals are used in these drinks to increase the potency so that the drinker will have a strong euphoric reaction.']"	['<urn:uuid:37f74aad-3581-4fbb-ac38-66531c8a19a3>', '<urn:uuid:4d1e8e05-0740-46e2-9b5b-922e94289b95>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	12	89	3482
32	best method measure control industrial liquids	For industrial liquid measurement and control, there are two main approaches to consider. The first is an integrated iris-diaphragm valve system that combines flow control and measurement in a single device, using differential pressure sensing across the valve to both measure and regulate flow. The second approach uses vortex meters, which are particularly suitable for clean, low-viscosity liquids and provide stable long-term accuracy without moving parts. Vortex meters offer advantages like low maintenance requirements and good performance across different process conditions, while iris-valve systems provide the benefit of combined control and measurement in a single unit. Both systems require consideration of pressure drops and potential cavitation in their application.	"['|Publication number||US7255012 B2|\n|Application number||US 11/000,564|\n|Publication date||Aug 14, 2007|\n|Filing date||Dec 1, 2004|\n|Priority date||Dec 1, 2004|\n|Also published as||CN101061338A, CN101061338B, EP1834116A1, EP1834116B1, US20060112773, WO2006060185A1, WO2006060185A8|\n|Publication number||000564, 11000564, US 7255012 B2, US 7255012B2, US-B2-7255012, US7255012 B2, US7255012B2|\n|Inventors||Robert C. Hedtke|\n|Original Assignee||Rosemount Inc.|\n|Export Citation||BiBTeX, EndNote, RefMan|\n|Patent Citations (10), Non-Patent Citations (3), Referenced by (18), Classifications (6), Legal Events (4)|\n|External Links: USPTO, USPTO Assignment, Espacenet|\nThe present invention is related to industrial process control and measurement devices. More particularly, the present invention is related to an integrated device that controls fluid flow therethrough.\nField devices, such as process variable transmitters, are used by a number of industries to remotely sense or control a process variable. Such variables are generally associated with fluids such as slurries, liquids, vapors, gasses, chemicals, pulp, petroleum, pharmaceuticals, food, and other fluid processing plants. Process variables may include pressure, temperature, flow, turbidity, density, concentration, chemical compensation, and other properties. Other examples of field devices include valves, actuators, heaters, and controllers.\nIn the past, an industrial process fluid flow control system generally required multiple components. For example, a first field device, such as a process variable fluid flow transmitter would have a fluid obstruction device, such as an orifice plate, disposed in the fluid flow. The flow transmitter would then measure differential pressure across the fluid flow device and calculate the mass flow of the fluid passing therethrough. The flow transmitter would then convey the fluid flow information to a process controller, which may be a computer located in a control room, or even another field device mounted in the field. The controller would then apply a control algorithm to both the flow measurement information received from the process fluid flow transmitter and a known flow set point provided to, or otherwise known by, the controller. The controller then generates a fluid flow output that is conveyed to yet another field device, generally a valve, to modify the fluid flow through the system based upon the applied control algorithm. Such a closed-loop fluid flow control system is known in the art.\nRecently, iris-diaphragm control valves have been used in conjunction with flow measurement to provide systems that can regulate fluid flow. Iris-diaphragm control valves are similar in concept to the optical shutter of a camera. The iris is generally comprised of three or more fingers that extend into the flow area and obstruct fluid flow through the flow area. One such iris-diaphragm control valve is sold by the Emile Eggar and Ciesa Pump and Machine Manufacturers in Switzerland. While work has been done to provide closed-loop fluid flow control systems using iris-diaphragm control valves, there is significant need for improvements. For example, each different device in the flow control system introduces additional costs as well as additional process fluid seals that may fail. Further, each process device used in a flow control system will also require additional technician time for installation and/or maintenance.\nA process device providing total fluid flow control is provided. The device includes a closure mechanism disposed in a flow conduit. The closure mechanism, which is preferably an iris-type diaphragm, provides a variable internal diameter. The device includes a differential pressure sensor for sensing the differential pressure on opposite sides of the diaphragm. A controller receives an indication of differential pressure and generates a control signal to an actuator that actuates the closure mechanism. The closure mechanism, differential pressure sensor and controller create a closed-loop flow controller in a single process device.\nElectronics compartment 18 includes suitable electronic circuitry to couple to and communicate over process communication loop 24 in accordance with an industry standard process communication protocol. Examples of such protocols include the HART® protocol as well as the all-digital FOUNDATION™ Fieldbus protocol. However any other suitable process industry standard communication protocols can be employed in accordance with embodiments of the present invention. Preferably, the process industry standard protocol is one that can provide power to device 10. Thus, device 10 may receive all of its electrical power over the same conductors that it uses for communication.\nPreferably, the fluid flow control mechanism 12 is an iris-diaphragm type valve that generally provides a variable orifice substantially coaxial with conduit 16. The coaxial orifice allows device 10 to have flow characteristics that approximate an orifice plate. Descriptions of various orifice plates and nozzle plates and the calculations and equations associated therewith are described in Liptak, Beto, Instrument Engineer\'s Handbook: Process Measurement and Analysis, Third Edition, Chilton Book Company (1995) and Miller, Richard, Flow Measurement Engineering Handbook, Third Edition, McGraw-Hill, Inc. (1996).\nIn order to approximate flow through device 10, differential pressure is measured across flow control mechanism 12. Typically, orifice plates are characterized by their beta ratio that is a function of, among other things, the internal diameter of the orifice plate. Accordingly, embodiments of the present invention that provide a variable orifice, naturally have varying beta ratios. The beta ratio is required for solving for flow rate through device 10 based upon the differential pressure. Accordingly, some embodiments of the present invention include characterizing or otherwise calibrating the device with known fluid flow (measured from a device external to device 10) at varying internal diameters. At each internal diameter, the known flow can be used in combination with the known diameter to calculate the beta ratio for that diameter. A plurality of diameter/beta ratio pairs, can then be stored in a look up table for use during operation. Additionally, if the function of internal diameter and beta ratios can be approximated by an equation, the equation itself could be used instead of a look-up table. In order to better conform the varying beta ratios to the varying internal diameter of mechanism 12, embodiments of the present invention also include modifying the configuration of edges 26 such that the beta ratio is easier to estimate accurately. Calibration can also include providing the device with an indication that there is zero flow, which allows the device to be zeroed. Additionally, the device can check its span by varying the orifice size during a constant flow. In addition to this calibration, the device can periodically, or as requested, perform a system diagnostic. For example, having the ability to both affect flow and measure flow allows the device to perform a check by momentarily changing the closure level, and monitoring the associated change in differential pressure. If the expected change is not observed, an alarm, or other appropriate indication, can be provided.\nActuator 108 may include any suitable type of actuating mechanism including, but not limited to, electric motors, suitable gearing reductions, a pneumatic actuator, as well as any other appropriate device capable of transforming a source of potential energy, such as electricity or compressed air, into physical displacement suitable for driving closure mechanism 106.\nDevice 100 also includes a differential pressure sensor in order to sense differential pressure of the process fluid across closure mechanism 106. The differential pressure sensor may be formed of a pair of impulse lines disposed on opposite sides of closure mechanism 106 and coupled to a differential pressure cell. However, in one embodiment, the differential pressure sensor is formed by two discrete pressure sensors 120, 122 that are positioned on different sides of mechanism 106. Preferably, pressure sensors 120, 122 are formed of a semiconductor material. These types of pressure sensors are taught in the U.S. Pat. No. 5,637,802, assigned to the assignee of the present invention. Such semiconductor-based pressure sensors generally provide a capacitance that varies with deflection of a portion of the semiconductor sensor. The deflection is in response to an applied pressure. The use of semiconductors, and in particular, sapphire, provides a number of advantages. Sapphire is an example of a single-crystal material that when properly fusion-bonded has no material interface between the two bonded portions. Thus, the resulting structure is exceptionally robust. Additionally, semiconductor-based sensors have extremely good hysteresis as well as an extremely high frequency response. Additional information related to semiconductor-based pressure sensors can be found in U.S. Pat. Nos. 6,079,276; 6,082,199; 6,089,907; 6,485,585; and 6,520,020, all of which are assigned to the assignee of the present invention. The use of a sapphire-based pressure sensor is particularly beneficial in embodiments, such as that illustrated in\nMeasurement circuitry 124 is coupled to each of pressure sensors 120, 122 and is adapted to drive a suitable energization signal through pressure sensors 120, 122 in order to generate measurements of the pressure-variable electrical response. For example, where pressure sensors 120, 122 are capacitance-type pressure sensors measurement circuitry 124 is able to drive a suitable signal through sensors 120, 122 in order to measure the relative capacitances of sensors 120, 122. Measurement circuitry 124 is coupled to controller 112 and provides an indication of the measured pressures to controller 112 over line 126. Preferably, measurement circuitry 124 provides the measured pressures in digital form, but any suitable form can be used.\nController 112 is preferably a microprocessor and able to execute a suitable control algorithm to relate the energization output signal 114, provided to actuator 108, to the differential pressure measured across closure mechanism 106. The control algorithm can be a proportional, derivative, integral control algorithm, or any combination thereof.\nLoop communication module 130 couples controller 112 to loop communication terminals 132, 134. Loop communication module 130 is adapted to communicate over a process communication loop via terminals 132, 134 in accordance with a process industry standard communication protocol. This communication allows device 100 to receive a process fluid flow set point from the process communication loop. Additionally, the communication, which is generally bi-directional, allows device 100 to report information relative to its operation, or any sub-components thereof. Device 100 also includes power module 140 that powers device 100 with energy received via terminals 132, 134. Accordingly, device 100 can be powered by the same conductors over which it communicates. However, in some embodiments, power module 140 may receive power from one or more sources that are not the communication media. Thus, power module 140 may simply couple to 110 volt 60 hertz power, or any other suitable power source.\nIn accordance with an embodiment of the present invention, device 100 may advantageously utilize a secondary indication of differential pressure of the process fluid passing therethrough. For example, if closure mechanism 106 is an iris-diaphragm valve, the pressure exerted upon the iris surface by the differential pressure of the fluid may cause the iris fingers to bind slightly. The degree of binding can be related to the differential pressure. Accordingly, the amount of energy required to actuate closure mechanism 106 by actuator 108 may provide a secondary indication of differential pressure. Thus, sensing the back EMF of an electric actuator mechanism, or the differential pressure across a pneumatic motor would provide this secondary indication of process fluid differential pressure. While not necessarily as precise as using sensors 120, 122, the secondary indication of differential pressure may provide an added element of redundancy and/or the ability to provide enhanced diagnostics for the device.\nIn accordance with another embodiment of the present invention, the device can act as a variable orifice differential pressure flowmeter. Thus, instead of controlling fluid flow, orifice changes can be employed in order to better measure flow. For example, the device may operate at a first closure level and sense fluid flow therethrough. As the sense fluid flow rate drops to a low threshold, where measurement accuracy may be lessened, the device can increase the closure level thus increasing the detectable differential pressure and overall accuracy of the flow measurement. This automatic ranging can occur at selected low and high thresholds. For example, whenever a reading is within 10% of the low or high DP measurement limit, the orifice closure level can be increased or decreased, respectively, as desired.\nAlthough the present invention has been described with reference to preferred embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention.\n|Cited Patent||Filing date||Publication date||Applicant||Title|\n|US4353500 *||Feb 26, 1981||Oct 12, 1982||George Brown||Thermostat|\n|US5327790 *||Jun 19, 1992||Jul 12, 1994||Massachusetts Institute Of Technology||Reaction sensing torque actuator|\n|US5971015 *||May 15, 1998||Oct 26, 1999||Xomox International Gmbh & Co.||Check valve|\n|US6154686||Mar 27, 1998||Nov 28, 2000||Innovex Technologies||Distributed architecture|\n|US6311568||Sep 13, 1999||Nov 6, 2001||Rosemount, Inc.||Process flow device with improved pressure measurement feature|\n|US6539968 *||Sep 20, 2000||Apr 1, 2003||Fugasity Corporation||Fluid flow controller and method of operation|\n|US20040177703||Mar 12, 2003||Sep 16, 2004||Schumacher Mark S.||Flow instrument with multisensors|\n|EP0565485A1||Mar 24, 1993||Oct 13, 1993||EMILE EGGER & CO. AG||Method and device to measure the flow of a medium and application of the method|\n|GB1379772A||Title not available|\n|WO1997028390A1||Jan 28, 1997||Aug 7, 1997||ABB Fläkt Oy||Flow rate control element for ducts|\n|1||""Iris Diaphragm Valves,"" Chemical Equipment, Kemutec Group, Inc. Bristol, PA, 1 Page.|\n|2||""Sensor, actuator and loop validation,"" IEEE Colloquium on Advances in Control Technology, May 25, 1999, pp. 1-10.|\n|3||International Search Report and Written Opinion from Appln No. PCT/US2005/041807, filed Nov. 18, 2005.|\n|Citing Patent||Filing date||Publication date||Applicant||Title|\n|US8066559 *||Jun 13, 2006||Nov 29, 2011||Stealthdrive, Inc.||In-plane airflow circulation contingency and control system|\n|US8132783||Sep 12, 2007||Mar 13, 2012||Lumec Control Products, Inc||Dynamic orifice valve apparatus and method|\n|US9175786||Aug 30, 2013||Nov 3, 2015||Lumec Control Products, Inc.||Valve apparatus|\n|US9181343||Jul 15, 2013||Nov 10, 2015||Redwood Bioscience Inc.||Antibody specific for CD22 and methods of use thereof|\n|US9238878||Feb 16, 2010||Jan 19, 2016||Redwood Bioscience, Inc.||Aldehyde-tagged protein-based drug carriers and methods of use|\n|US9476744||Sep 25, 2015||Oct 25, 2016||Dieterich Standard, Inc.||Integrated orifice plate assembly|\n|US9540438||Jan 13, 2012||Jan 10, 2017||Redwood Bioscience, Inc.||Aldehyde-tagged immunoglobulin polypeptides and methods of use thereof|\n|US9568111||Feb 27, 2015||Feb 14, 2017||Clarke Industrial Engineering, Inc.||Shutter valve|\n|US9568387 *||Aug 8, 2012||Feb 14, 2017||Rosemount Inc.||Thermal diagnostic for single-crystal process fluid pressure sensor|\n|US9651410||Mar 31, 2015||May 16, 2017||Dieterich Standard, Inc.||Paddle style orifice plate with integral pressure ports|\n|US20080160896 *||Jun 13, 2006||Jul 3, 2008||Stealthdrive, Inc.||In-Plane Airflow Circulation Contingency and Control System|\n|US20090065081 *||Sep 12, 2007||Mar 12, 2009||Paul Luebbers||Dynamic Orifice Valve Apparatus And Method|\n|US20140041437 *||Aug 8, 2012||Feb 13, 2014||Robert C. Hedtke||Thermal diagnostic for single-crystal process fluid pressure sensor|\n|US20140124061 *||Nov 8, 2012||May 8, 2014||Kyle Patrick Daniels||Shutter Valve for Pressure Regulation|\n|DE102009045372A1||Oct 6, 2009||Apr 7, 2011||Endress + Hauser Gmbh + Co. Kg||Durchflussmessanordnung und Verfahren zu deren Funktionsüberwachung|\n|DE102009046758A1||Nov 17, 2009||May 19, 2011||Endress + Hauser Process Solutions Ag||Sich selbst überwachende Durchflussmessanordnung und Verfahren zu deren Betrieb|\n|WO2011061005A1||Oct 7, 2010||May 26, 2011||Endress+Hauser Process Solutions Ag||Self-monitoring flow measurement assembly and method for the operation thereof|\n|WO2016090438A1 *||Dec 11, 2014||Jun 16, 2016||Zammi Instrumental Ltda||Membrane valve for controlling flow and relieving pressure|\n|Cooperative Classification||F16K3/03, G05D7/0635|\n|European Classification||G05D7/06F4B, F16K3/03|\n|Dec 1, 2004||AS||Assignment|\nOwner name: ROSEMOUNT INC., MINNESOTA\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HEDTKE, ROBERT C.;REEL/FRAME:016043/0298\nEffective date: 20041123\n|Nov 27, 2007||CC||Certificate of correction|\n|Aug 17, 2010||FPAY||Fee payment|\nYear of fee payment: 4\n|Feb 16, 2015||FPAY||Fee payment|\nYear of fee payment: 8', 'Vortex Meters can be used for a wide range of fluids, i.e. liquids, gases and steam. They are to be seen as first choice, subject to verification to cover the requirements of a particular application.\nVortex meters are essentially frequency meters, since they measure the frequency of vortices generated by a “bluff body” or”shedder bar”. Vortices will only occur from a certain velocity (Re-number) on-wards, consequently vortex meters will have an elevated zero referred to as the “cut-off” point. Before the velocity becomes nil, the meter output will be cut to zero.\nAt a certain back-flow (above cut off point) some vortex meters could produce an output signal, which could lead to a false interpretation.\nAlso See: Vortex Flow Meter Animation\nVortex meters are actual volume flow meters, like orifice meters. These being intrusive meters like orifice meters, will cause the pressure drop as flow is increased, resulting in a permanent loss. consequently, liquids near their boiling point, could introduce cavitation as the pressure across the meter drops below the vapour pressure of the liquid. As soon as the pressure recovers above the vapour pressure the bubbles will impode. cavitation causes the meter to malfunction and should be avoided at all times.\nPrincipal – A fluid flowing with a certain velocity and passing a fixed obstruction generates vortices. The generation of vortices is known as Karman’s Vortices and culmination point of vortices will be approx. 1.2D downstream of bluff body. Strouhal discovered that as soon as a stretched wire starts vibrating in an air flow, frequency will be directly proportional to air velocity,\nSt= f*d/V0 (without dimention)\nSt= Strouhal’s number\nf=frequency of wire\nd=diameter of wire\nThis phenomena is called “vortex shedding” and the train of vortices is known as “Karman’s Vortex street”.\nThe frequency of vortex shedding is a direct linear function of fluid velocity and frequency depends upon the shape and face width of bluff body. Since the width of obstruction and inner diameter of the pipe will be more or less constant, the frequency is given by the expression-\nf= vortex frequency, Hz\nSt=strouhal’s number, dimention less\nV=Fluid velocity at the sheddar bar, m/s\nD=Inner diameter of the pipe, m\nc=constant (ratio d/D)\nd= Face width of sheddar bar, m\nThe pressure loss gradient across the vortex meter will have a similar shape to that of an orifice meter. the lowest point in pressure will be at the sheddar bar (comparable to vena contracta for orifice meter). downstream of this point of pressure will recover gradually, finally resulting in permanent pressure loss. To avoid cavitation, the pressure loss at vena-contracta is of interest.\nThe minimum back pressure required to ensure cavitation doesn’t occur is:\nPmin=3.2*Pdel + 1.25*Pv\nPmin= minimum required pressure at five pipe diameters downstream of the flow meter in bar\nPdel= calculated permanent pressure loss in bar\nPv= vapour pressure at operating temperature in bar\nRemember- for most vortex meters d/D will have range, 0.22 – 0.26, & frequency od vortices will depend on sizre of meter, larger the meter, lower the frequency. So the maximum diameter of vortex meter is restricted, because resolution of meter could become a problem.for control purposes. To overcome this problem, on-board digital multipliers are used which will multiply the vortex frequency without additional error.\nFrequency Sensing Principle –\nPiezo-electrical Sensors- a pair of piezo-electrical crystals is built into the sheddar bar. as the sheddar bar will be subject to alternating forces caused by shedding frequency, so will the piezo-crystals.\nVariable capacitance Sensors- a pair of variable capacitance sensors is built into the sheddar bar. As the sheddar bar will be subject to alternating micro movements caused by forces as a result of the shedding frequency, the capacitors will change their capacitance accordingly.\nPerformance of Vortex meters is influenced by-\nchange in sheddar bar geometry owning to erosion\nchange in sheddar bar geometry owning to deposits, i.e. Wax\ncorrosion of upstream piping\nchange in position of sheddar bar if not properly secured\nIn-general votex meter will consist of following electonics part-\npick-up elements, AC-pre amplifiers, AC-amplifier with filters, Noise abatement features, Schmitt Trigger, Microprocessor\nThe vortex shedding meter provides a linear digital (or analog) output signal without the use of separate transmitters or converters, simplifying equipment installation. Meter accuracy is good over a potentially wide flow range, although this range is dependent upon operating conditions. The shedding frequency is a function of the dimensions of the bluff body and, being a natural phenomenon, ensures good long term stability of calibration and repeatability of better than ±0.15% of rate. There is no drift because this is a frequency system.\nThe meter does have any moving or wearing components, providing improved reliability and reduced maintenance. Maintenance is further reduced by the fact that there are no valves or manifolds to cause leakage problems. The absence of valves or manifolds results in a particularly safe installation, an important consideration when the process fluid is hazardous or toxic.\nIf the sensor utilized is sufficiently sensitive, the same vortex shedding meter can be used on both gas and liquid. In addition, the calibration of the meter is virtually independent of the operating conditions (viscosity, density, pressure, temperature, and so on) whether the meter is being used on gas or liquid.\nThe vortex shedding meter also offers a low installed cost, particularly in pipe sizes below 6 in. (152 mm) diameter, which compares competitively with the installed cost of an orifice plate and differential pressure transmitter.\nThe limitation include meter size range. Meters below 0.5 in. (12 mm) diameter are not practical, and meters above 12 in. (300 mm) have limited application due their high cost compared to an orifice system and their limited output pulse resolution. The number of pulses generated per unit volume decreases on a cube law with increasing pipe diameter. Consequently, a 24 in. (610 mm) diameter vortex shedding meter with a typical blockage ratio of 0.3 would only have a full scale frequency output of approximately 5 Hz at 10 ft/s (3 m/s) fluid velocity.\nSelection and Sizing :\nAs the first step in the selection process, the operating conditions (process fluid temperature, ambient temperature, line pressure, and so on) should be compared with the meter specification. The meter wetted materials (including bonding agents) and sensors should then be checked for compatibility with the process fluid both with regard to chemical attack and safety. On oxygen, for example, non ferrous material should be used avoided or approached with extreme caution. The meter minimum and maximum flow rates for the given application should then be established.\nThe meter minimum flow rate is established by a Reynolds number of 10,000 to 10,500, the fluid density, and a minimum acceptable shedding frequency for the electronics. The maximum flow rate is governed by the meter pressure loss (typically two velocity heads), the onset of cavitation with liquids, and sonic velocity flow (choking) with gases. Consequently, the flow range for any application depends totally upon the operating fluid viscosity, density, and the vapour pressure, and the applications maximum flow rate and line pressure. On low viscosity products such as water, gasoline, and liquid ammonia, and with application maximum velocity of 15 ft/s (4.6 m/s), vortex shedding meters can have a rangeability of about 20:1 with a pressure loss of approximately 4 PSIG (27.4 kPa).\nThe meter’s good (“of rate”) accuracy and digital linear output signal make its application over wide flow ranges a practical proposition. The rangeability declines proportionally with increase in viscosity, decrease in density, or reductions in the maximum flow velocity of the process. Vortex shedding meters are therefore unsuitable for use on high viscosity liquids.\nVortex Meter Advantages\n• Vortex meters can be used for liquids, gases and steam\n• Low wear (relative to turbine flow meters)\n• Relatively low cost of installation and maintenance\n• Low sensitivity to variations in process conditions\n• Stable long term accuracy and repeatability\n• Applicable to a wide range of process temperatures\n• Available for a wide variety of pipe sizes\nVortex Flow Meter Limitations\n• Not suitable for very low flow rates\n• Minimum length of straight pipe is required upstream and downstream of the vortex meter\nVortex flow meters are suitable for a variety of applications and industries but work best with clean, low-viscosity, medium to high speed fluids.\nSome of the main uses include:\n• Custody transfer of natural gas metering\n• Steam measurement\n• Flow of liquid suspensions\n• General water applications\n• Liquid chemicals & pharmaceuticals\nAlso Read: Turbine Flow Meter Working Principle\n- Vortex Flow meter Working Animation\n- Difference between Different Types of Flow Meters\n- Types of Vortex Flow Meter Sensors\n- How to Select a Flow Meter\n- What is a Vortex flowmeter ?\n- V Cone Flow Meter Working Principle\n- Comparison of Venturi and Orifice flow meter\n- Swirl Flow Meters Working Principle\n- Flow Transmitter Rangeability\n- Flow Measurement Multiple Choice Questions']"	['<urn:uuid:4d7725ad-5141-4551-8fab-ad8be2c90c2b>', '<urn:uuid:62cb8005-b1b0-4130-923e-fcdbf2907b07>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-13T00:57:18.859842	6	109	3945
33	what kind storage solutions maximize space in narrow corridor kitchen	In a Corridor (Galley) kitchen, you can maximize space by stacking storage solutions. You can also add a pass-through or remove a wall to open the kitchen while maintaining base cabinetry and countertop space.	['What is a kitchen layout?\nThe kitchen layout is the shape that is made by the arrangement of the countertop, major appliances, and storage areas. This floor plan helps define the kitchen’s work triangle – the path that you make when moving from the refrigerator to the sink, to the range to prepare a meal. When designing your new kitchen and choosing the best cabinetry solutions for your home, one of the first considerations is the overall layout of the kitchen.\nThe 5 fundamental kitchen layouts\nThere are five fundamental layouts for most kitchens – the L-Shape, Galley, U-Shape, Single-Wall, and G-Shaped layouts. While certain floor plans create a more spacious, efficient kitchen, each kitchen layout can be enhanced with the right cabinetry and decorative accents. Depending on the overall size of your kitchen, a kitchen island can be added to many of these layouts to expand storage and create additional counter space. The width of walkways between cabinetry and the kitchen island is important and your kitchen designer will help determine if you have enough space to accommodate a kitchen island. Kitchen islands can be created in all shapes and sizes and you are not limited to a standard square or rectangular shape.\nIn an L-Shaped kitchen layout, a natural work triangle is created from continuous counter space and work zones (also known as work stations) on two adjacent walls. The benefit of this kitchen floor plan is that it not only provides the cook with an efficient work area, but it typically opens to a nearby room, making it easy for the cook to interact with guests.\nHow to enhance the L-Shaped kitchen:\nIf space allows for it, adding an island to your L-Shaped kitchen can increase and maximize countertop work surface and storage for your kitchen. Incorporating a seating area at the kitchen island can also be a great addition to allow guests to interact with you in the kitchen without intruding into the primary work area of the space.\nThe Galley (Corridor):\nHow to enhance the Corridor kitchen:\nSimilar to the Single-Wall kitchen floor plan, stack storage solutions to maximize space. If possible, add a pass-through or remove a wall to open the kitchen, but still allow for base cabinetry and countertop space.\nThe U-Shaped kitchen is the most versatile layout for kitchens large and small because the design offers continuous countertops and ample storage, which surround the cook on three sides. In larger kitchens, this floor plan is spacious enough to be divided into multiple work zones for cooks to easily prepare a meal together without getting in each other’s way.\nHow to enhance the U-Shaped kitchen layout:\nTo maximize storage in a U-Shaped floor plan and keep countertops clutter-free, conceal the microwave in a base or wall cabinet and store large cooking essentials in a corner storage solution like a Lazy Susan.\nThe Single-Wall kitchen floor plan is ideal for smaller homes and lofty spaces. The work triangle in this kitchen layout is less like a triangle and more of a work line with all three kitchen zones along one singular wall. Sometimes an island will run parallel to the Single-Wall kitchen to offer additional storage and countertop space, but if a sink, cooktop, oven, or refrigerator are added to the kitchen island space it’s no longer considered a Single-Wall kitchen, but becomes a Galley kitchen.\nHow to enhance the Single-Wall kitchen:\nAdd additional storage and maximize the space by stacking cabinetry such as the wall pantry pull-out above the base pantry pull-out. If space allows, incorporate a kitchen island for additional pantry and supply storage.\nThe G-Shaped kitchen is a version of the U-Shaped kitchen layout, with the same amount of counter space and storage options that surround the cook on three sides. However, the difference with the G-Shaped kitchen floor plan is it has a kitchen peninsula or a partial fourth wall of additional cabinets.\nHow to enhance the G-Shaped kitchen layout:\nDepending on the size of the kitchen, G-Shaped kitchen designs can seem cramped. To help the room feel more spacious, open up the wall in a nearby room, and create a pass-through or breakfast bar for the family.\nWhich Kitchen Layout is best for me?\nFirst, identify which kitchen layout is the most similar to your current kitchen design. You and your kitchen designer may need to work within the space of your current kitchen, OR you may decide to remove or reconfigure walls to expand the space which would give you additional options for your kitchen layout. Keep in mind – even if you’re not making significant structural changes to the kitchen, you can still enhance the layout with the right cabinetry.\nWhatever you decide, we recommend working with a kitchen designer to select the cabinetry and create a kitchen that works best for you.']	['<urn:uuid:7812cd1a-4ebb-4b91-b305-875b9051f6c5>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	10	34	806
34	professional development coaching mentoring difference benefits	While both support professional development, coaching and mentoring have distinct benefits. Coaching provides increased learning, confidence, motivation, autonomy, and self-reliance for employees, while creating higher performing teams and more time for managers. Mentoring specifically helps junior faculty compete for NIH funding, increases the pool of successful translational scientists, and provides assistance in identifying mentors, supporting career development through mentor/mentee relationships, and connecting mentees to institutional resources.	"['How to Coach\nHow good are you at coaching your team? Asked this question, many managers will say ‘pretty good’. However, a study published in the Harvard Business Review showed that managers consistently over-estimated their own ability to coach This is mainly because most managers and leaders misinterpret what is meant by coaching. Coaching is often an over-used word within organisations and can be used to mean everything from training someone to giving advice or even telling them what to do.\nSo if you want to know how you, your team, and your organisation can benefit from you really coaching at work, read on.\nWhat is Coaching?\nThe ‘father’ of coaching, Sir John Whitmore, describes coaching as ‘unlocking a person’s potential to maximise their own performance’. But what does this really mean? And how do you do that?\nCoaching is at one end of a spectrum of ways in which we can help others – whether that’s about learning something new, solving a problem or achieving a goal. At the other end of this spectrum is the most direct form of helping: simply telling someone what to do.\nIn between telling and coaching, is a myriad of ways of helping others which we use every day without even thinking about it: instructing, training, guiding, advising, suggesting. With all of these options, you are helping to solve the other person’s problem for them. The biggest difference, and benefit, with coaching is that you are helping the other person to solve their own problem.\nWe do this by asking powerful open questions and listening deeply to the coachee. In doing so, we are encouraging them to explore their issue or goal and look at it from a variety of angles, to help them consider options they hadn’t before, and to take responsibility for the choice they make and the outcome of that choice.\nThis might sound anything but helpful. Why would we leave someone to sort their own problem out? Surely they’ve come to us because they need help, and it sounds harsh to sit back and let them figure it out. But consider how many times you have given your advice or opinion to someone and they haven’t taken it? Or they’ve done it, and it hasn’t worked out well, so they still have their issue?\nThe challenge with trying to solve others’ problems is that as human beings, we are coming at that problem from our own perspective, with our own experiences, values and biases. Which invariably is not the only way of looking at or solving an issue. By using coaching instead, we can enable our employees to develop new insights, challenge their own thinking and consider options they hadn’t previously.\nWhen Should You Coach?\nWhat makes coaching so powerful is that you are enabling your team member to learn, become more confident and develop their resilience. There is absolutely a place for telling, suggesting and advising – the skill is to know when this is, and when to coach.\nManagers will need to be more directive when they have someone who is very inexperienced or brand new to their role. One of the golden rules of coaching is that the coachee needs a degree of ability and will to resolve their own issue. However, you can still use coaching questions to ascertain the team member’s level of knowledge and experience before giving further direction. Indeed, you can use coaching more often than you think. As a professional coach, I remain in the coaching space for the entire session with my clients.\nAs a manager, you can move up and down the spectrum between coaching and telling even in the same conversation. The reason we spend a lot of time developing the coaching skill set with managers is because no-one I’ve ever come across has needed help giving advice or making suggestions! We spend most of our lives in this part of the spectrum of helping, even when we don’t realise it. Coaching requires practise and skills development, but even with very little training, managers can start to use coaching questions in their everyday conversations which will make those conversations more fruitful for both parties.\nWhat are the Benefits of Coaching?\nThere is a myriad of benefits of coaching at work – for employees, for the manager and also for the organisation:\nBenefits for Employees:\n- Increased learning and development of knowledge and skills\n- Increased confidence and motivation\n- Feeling empowered and engaged\n- Increased autonomy\n- Increased self-reliance and agility\n- Improved performance\nBenefits for the Manager:\n- Higher performing team members\n- Reduced pressure to ‘have all the answers’\n- Able to delegate more often to team members\n- New ideas and creativity within the team\n- Team more open to change\n- More time\nBenefits for the Organisation:\n- Organisational performance improves as individuals’ performance improves\n- Creates a learning organisation – learning is quicker and spreads throughout the organisation without relying purely on formal methods\n- Teams work more effectively together\n- Increased engagement and retention\n- Increased innovation and creativity\n- Teams are more open to change\n- Leaders’ time can be utilised more for business improvement and future-focussed thinking\nOne of my favourite examples of the power of coaching in action is by David Marquet in his book ‘Turn the ship around!’. Watch this video to see how he used coaching on a US Nuclear Submarine with brilliant results…\nSo How Do You Coach?\nYou may think that you need certain qualities to coach others. The good news is, everything you need to be a successful coach can be learned. You may have heard that there are different styles of coaching, but ultimately whatever the specific approach that might be taken, the principles of coaching is the same. Effective coaching is a combination of skill, emotional intelligence and good judgement.\nMost of the time we listen selectively. We have so much else going on in our heads that we only hear part of what someone is saying. Often, this is the part we either want to hear or the part we are looking for in order to respond and move the conversation on. We’re so keen to resolve the issue, we don’t pick up on the undercurrent of what’s really going on – and so we often offer a solution which doesn’t solve the real problem.\nWhen we coach, we need to actively listen by giving our whole, undivided attention. We are listening not just to the words someone is saying, but how they are saying it – their tone, their body language, the pauses. Active listening tells us so much more about what is going on with someone than when we are doing our normal every day listening. Active listening also helps us to build empathy – a valuable and key element of effective coaching. It takes practice – so start today by really attentively listening to someone.\nAsking Open Questions\nOpen questions (any question which begins with What, Where, How, When, Who or Why) are the main types of questions we use in coaching. Using open questions enables the team member to consider and explore fully the issue. Consider the difference between these questions:\n“Have you spoken to your line manager?” and “Who have you spoken to about this?”\nThe first question is a closed question, leading to a yes or no answer and it’s also an implied suggestion. If the answer is “No,” the obvious response would then be: “Well why don’t you speak to them and see what they say?”. This is one possible route to the solution.\nThe second question not only establishes what the team member has done so far, but it also elicits the follow up question: “Who else could you speak to?”. This leads to multiple routes to the solution.\nAnother effective way of asking questions is a slightly softer “Tell me…”. For example:\n“Tell me what you know about this already.”\n“Tell me what your thoughts are about this.”\n“Tell me how you think that went.”\nPractice asking open questions in your everyday conversations and notice how the conversation alters as a result. As a note, I would personally avoid asking a ‘Why..?’ question as it can sound accusatory even when you don’t intend it to.\nStructuring the Conversation\nA coaching conversation is a conversation with a purpose – to explore, clarify and come to an action or conclusion. In order to manage the direction of the conversation, many coaches use a structure to help them to keep track of the conversation and manage the coaching process.\nThe most well-known process for this is called the GROW model. GROW stands for:\nGoal – what is the outcome or goal you want to achieve?\nReality – what is the current situation? What have you tried already?\nOptions – what options are there for achieving the goal or outcome?\nWill/ Way forward – what will you do now as a result of this conversation? How motivated are you to do this?\nAs a manager you can use this coaching process in your conversations. It can feel uncomfortable at first, but with practice, it comes more naturally and your team member won’t notice that you are using this process to keep the conversation structured and as useful as possible.\nAs well as key skills, successful coaching also requires a high degree of emotional intelligence. The main thing that gets in the way of effective coaching is ourselves.\nIf we are not aware of how we are listening, asking questions or impacting our team member, we will not be able to coach effectively. We’ll slip into asking closed questions; we won’t pick up on the subtleties of what is being said, or not said; and our team member may realise that we’re not fully focussed on them. So self-awareness is key. We can develop our self-awareness by becoming conscious of how we are holding conversations. Gathering feedback from others is also invaluable.\nIf we are tired, stressed or feeling overwhelmed, we will find it very difficult to coach. Coaching requires emotional head space. So managing our emotions and stress levels is crucial. Realising that you aren’t currently in a place to coach effectively is one of the most important things you can do as a manager. Some simple tactics such as taking a break, getting some fresh air or even getting something to eat can help. Sometimes, you might want to delay your coaching conversation for a few hours until you are able to give your team member your full attention – and they will appreciate your honesty more than if you rush through a conversation with them just to get onto the next thing on your list.\nBuilding rapport and trust between ourselves and our coachee is crucial for effective coaching, and developing empathy is a key way of doing this. If your team member senses that we don’t (or won’t) understand where they are coming from, they will feel less willing to be open and honest and the coaching conversation will be less effective. Empathy is the ability to be able to understand how someone feels in a given situation – even if we don’t or wouldn’t feel the same.\nRemembering that everyone is unique and respecting how they feel is therefore key to developing empathy. Tapping into your own emotions and recalling how it feels to be angry, distressed, indignant, frustrated, delighted, proud etc also helps us to empathise with others.\nKnowing when and when not to use coaching is an important judgement call. Coaching is only effective if the person wants to be coached. If the team member doesn’t have the knowledge, experience or confidence to develop their own solutions, then using another way of helping them might be more appropriate. Mentoring is a good solution for this, because you can use a coaching approach (i.e., active listening, asking open questions). But also offering advice and suggestions based on your own experience.\nIn genuine emergency situations or where there is a clear important policy or procedure, coaching isn’t appropriate, as either time or protocol determines a particular path to be taken. However, once the situation is over, coaching can be used to help reflect on and learn from the situation.\nTake These 5 Actions to Learn How to Coach:\nYou can start learning to coach straight away, by taking these 5 actions:\n- Become aware of how you are listening in your day-to-day conversations.\n- Start practising active listening.\n- Listen to your own questions during conversations – are they closed or open?\n- Start to convert your current closed questions into open questions.\n- Encourage your team member to explore their own options to address their issue before offering your own suggestions.\nWhy Don’t Managers Coach More Often?\nManagers often think they are coaching when they’re not, and even when they have received training to understand what coaching is and improve their coaching skills, it’s still really hard to do. Coaching takes practice and creating new habits because we’re using under-utilised skills. Managers and leaders are incredibly busy, and one of the things they don’t have a lot of is time. They believe that coaching someone takes more time than simply telling them what to do. In the immediate term, they are often right. However, in the medium to long term, coaching saves them time and energy as they equip their team members to become more self-sufficient.\nAnother common (but less visible barrier) is that many people enjoy helping others by imparting their own knowledge and experience. We like to see ourselves as being helpful and knowledgeable, and coaching can feel like these attributes aren’t being used. If this sounds like you, I would encourage you to consider the benefits of coaching outlined above, and to start small. Simply begin by noticing how you are listening, how you are asking questions and how your typical conversations go. Then start to develop your active listening and questioning skills. Remember, helping someone by coaching them is much more effective in the longer term. Taking these small steps will make you a better coach in the workplace and bring countless benefits to you and your team.', 'What is Mentoring?\nMentoring can be defined as ""faculty with useful experience, knowledge, skills, and/or wisdom offers advice, information, guidance, support, or opportunity to another faculty member for that individual’s professional development."" Berk RA, Berg J, Mortimer R, Walton-Moss B, Yeo TP. Measuring the effectiveness of faculty mentoring relationships. Acad Med. 2005;80(1):66-71.\nSYNERGY’s Research Mentoring and Education is a critical component of SYNERGY’s overall Education, Training and Career Development Program. Its specific purpose is to provide research mentoring to junior faculty members who aspire to become independent investigators. Its aim is to help early-career investigators compete for NIH (and other extramural) research funding and increase the Geisel/Dartmouth-Hitchcock (D-H) pool of successful translational scientists.\nResearch Mentoring and Education can:\n- Provide assistance in identifying mentors for eligible junior faculty\n- Support and facilitate career development through mentor/mentee relationships\n- Provide training opportunities for successful mentoring\n- Connect mentees to existing SYNERGY services and institutional resources.\n- Track progress of developing investigators\nFor assistance in choosing and contacting a mentor, or for more information, contact: Synergy.Mentoring@Dartmouth.edu\n- Research Mentoring and Education Steering Committee\n- Research Mentoring and Education Departmental Liaisons\nIn addition to mentoring, SYNERGY has valuable Research Consultation Services to assist you in your research development or initiation. See the listing here at: https://synergy.dartmouth.edu/research-consultation-cores\nMentor Training Resources\n- Office of Development and Alumni Relations, Geisel School of Medicine/Dartmouth-Hitchcock\n- Finding foundation funding: Kathleen J. Bryar, Senior Associate Director of Institutional Giving\nGrant Writing and Editing Support\n- GrantGPS is an initiative funded by the Dartmouth Offices of the Provost and President, and directed by Dean Madden, PhD, that supports grant-writing activities across Dartmouth. GrantGPS provides opportunities to assist all faculty, postdocs, and graduate students in developing and writing effective and successful grant proposals. This group facilitates and covers all costs for professional pre-submission editing and outside review of grant proposals. The group also organizes workshops which provide training on specific grant mechanisms, including NIH and NSF research proposals, grant funding in the Humanities and Social Sciences, and more. Read More: GrantGPS\n- Dartmouth College Office of Sponsored Projects (OSP)\n- The Office of Sponsored Projects serves as a central resource to support the research at Dartmouth by providing guidance and stewardship for the research community and the College. OSP provides both pre-award and post-award services and has primary responsibility for assuring Dartmouth\'s compliance with the regulations of a range of funding agencies, for negotiating Dartmouth\'s indirect cost rate, and monitoring college-wide implications of these negotiations on a regular basis.\n- Dartmouth Biomedical Libraries\n- The Biomedical Libraries consist of the Dana Biomedical Library on the Medical School\'s Hanover campus and the Matthews-Fuller Health Sciences Library at the Dartmouth-Hitchcock Medical Center. Services include: literature searching, systematic review support, instruction in the use of information resources (MEDLINE, evidence-based medicine tools, EndNote/RefWorks, PowerPoint, and more), and assistance with scholarly publishing and copyright.\nAbedin Z, Rebello TJ, Richards BF, Pincus HA. Mentor training within academic health centers with Clinical and Translational Science Awards. Clin Transl Sci. 2013 Oct;6(5):376-80.\nBrandt DS, Bosch M, Bayless M, Sinkey CA, Bodeker K, Sprenger K, Johnson K, Gilmore JM. A CTSA-sponsored program for clinical research coordination: networking, education, and mentoring. Clin Transl Sci. 2011 Feb;4(1):42-7.\nBurnham EL, Schiro S, Fleming M. Mentoring K scholars: strategies to support research mentors. Clin Transl Sci. 2011 Jun;4(3):199-203.\nFeldman MD, Steinauer JE, Khalili M, Huang L, Kahn JS, Lee KA, Creasman J,Brown JS. A mentor development program for clinical translational science faculty leads to sustained, improved confidence in mentoring skills. Clin Transl Sci.2012 Aug;5(4):362-7.\nKeller TE, Collier PJ, Blakeslee JE, Logan K, McCracken K, Morris C. Early career mentoring for translational researchers: mentee perspectives on challenges and issues. Teach Learn Med. 2014;26(3):211-6.\nPfund C, House S, Spencer K, Asquith P, Carney P, Masters KS, McGee R, Shanedling J, Vecchiarelli S, Fleming M. A research mentor training curriculum for clinical and translational researchers. Clin Transl Sci. 2013 Feb;6(1):26-33.\nPfund C, House SC, Asquith P, Fleming MF, Buhr KA, Burnham EL, Eichenberger Gilmore JM, Huskins WC, McGee R, Schurr K, Shapiro ED, Spencer KC, Sorkness CA. Training mentors of clinical and translational research scholars: a randomized controlled trial. Acad Med. 2014 May;89(5):774-82.\nSorkness CA, Pfund C, Asquith P, Drezner MK. Research mentor training: initiatives of the University of Wisconsin Institute for Clinical and Translational Research. Clin Transl Sci. 2013 Aug;6(4):256-8.']"	['<urn:uuid:b1131e0a-8b9c-4bfa-a26b-598d579abf95>', '<urn:uuid:2301a6b8-7cc5-4d04-8e07-9d7209f874d3>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	6	66	3095
35	visual characteristics grey dark material compare lead clouds appearance	Lead and clouds share notable visual characteristics in terms of their grey and dark appearances. Lead, as used in artistic works, presents a predominantly grey, monochromatic palette and creates cloudy, veiled images. Similarly, clouds can appear grey or black when they contain larger water drops, and both lead and clouds can obscure or darken what's behind them. While lead's darkness is permanent and physical, clouds' darkness is temporary and varies based on their water content and atmospheric conditions, ranging from white when formed with warm air to dark grey or black when containing larger water droplets.	"['“For me, art is the only possibility to establish a connection between things that make no sense and those that have a meaning. I see history as something synchronized, both if it refers to the Sumerians or to German mythology. As far as I am concerned, old sagas are not old at all. Neither is the Bible. When you look into it, the majority of things have already been formulated.” 1\nAnselm Kiefer, The Land of the Two Rivers(Zweistromland), 1995. Emulsion, acrylic, lead, salt through electrolysis and zinc plates-condenser, on canvas, 416 x 710 cm.\nGuggenheim Bilbao Museoa\nBorn just months before the final European battle of World War II, Anselm Kiefer (b. 1945, Donaueschingen, Germany) grew up witnessing the consequences of modern warfare and the division of his homeland. He experienced the rebuilding of a fragmented nation and its struggle for renewal. The artist dedicated himself to investigating the interwoven patterns of German mythology and history and the way they contributed to the rise of Fascism. Many of his paintings—immense landscapes and architectural interiors, often encrusted with sand and straw—invoke Germany\'s literary and political heritage; references abound to the Song of the Nibelung, a German epic poem from the Middle Ages or to Nazi dictator Adolf Hitler (1889–1945). In one of his earliest projects, his 1969 Occupations (Besetzungen) series, Kiefer photographed himself mimicking the Nazi salute at various sites in France, Italy, and Switzerland. Following his move to southern France in the early 1990s, Kiefer\'s iconography expanded to encompass more universal themes of civilization, culture, and spirituality, drawing upon such sources as alchemy, ancient myths, and the Kabbalah.\nKiefer became one of the foremost representatives of Neo-Expressionism, an approach characterized by violent, gestural brushwork. Bright color and strong light are not usually present in his works: images are cloudy, veiled, and show twilight scenes, painted with gray as the dominating color. His large-scale works combine a nearly monochromatic palette with mixed media, including materials such as ash, plaster, seeds, soil, straw, and strips of lead. Experimenting with materials is of great importance to Kiefer’s creative process. The chosen material acquires a symbolic meaning when understood in combination with the subject matter. The objects that are gathered in his works transcend their physical identities and speak for themselves, showing the artist’s obsessions through rich association and metaphors. Sand, flowers, dry branches, straw, and the iron objects all show Kiefer’s fascination with metamorphosis. Lead becomes a key material, both for its physical properties and great transformation capacity, as for its relationship with alchemy and the Kabbalah.\nThe Land of the Two Rivers (Zweistromland) (1995) refers to the land delimited by the Euphrates and Tigris rivers in whose banks settled some of the most ancient peoples in humankind, the Mesopotamian civilization, and which is the site of ancient Babylonia, where the Sumerians invented writing in the fourth millennium BCE. Kiefer\'s painting evokes that land, its civilizations, and the establishment of written culture. The inscribed title at the upper right of the canvas might be seen as an allusion to the written word itself, which leaves a lasting mark that transcends civilizations and eras. Records have gone beyond the decay and ruins of these cultures and have made their past a present in this painting. The Land of the Two Rivers was preceded by a sculpture with the same title comprising several lead books that convey a sense of the enduring nature of the written word and history. Continuing with the use of lead books, Kiefer created For Paul Celan (2006), lead books pierced by flowers, ancient symbols of both fertility and the transience of life, which refers to the Jewish Romanian poet and essayist Paul Celan (1920–1970), who miraculously survived the Holocaust. Celan shares many of the same themes and concerns as Kiefer—a sense of mourning and melancholy, and the importance of preserving memory as a means of coming to terms with the traumas of human history.\n1- Anselm Kiefer quoted in Ein Gespräch: Joseph Beuys, Jannis Kounellis, Anselm Kiefer, Enzo Cucchi. Edited by Jacqueline Burckhardt. Ed. Parkett-Verlag, Zurich, 1986, p. 40.\nLópez-Remiro, Miguel. ""Anselm Kiefer."" In Guggenheim Museum Bilbao Collection. Guggenheim Museum Bilbao; TF Editores, Bilbao, 2009, p. 142-147\nSpector, Nancy. ""Anselm Kiefer."" In Guggenheim Museum Collection: A to Z, edited by Nancy Spector. 3rd rev. ed. Guggenheim Museum, New York, 2009, p.164\nBrainstorm a list of the words that best describe this work.\nHow would you describe the atmosphere that Kiefer has created in this work? How do you feel when you look at this work? Why? Imagine you could walk in this landscape. How would you feel? Why?\nWhat materials does Kiefer use? Make a list of the materials and describe the characteristics that you associate with each one of them. Why do you think Kiefer chose these materials? How do you think they are related to the subject matter?\nWhat words would you use to describe the surface of the canvas? Discuss how some materials transform with the passing of time. Why might Kiefer be interested in the transformation of materials?\nLead is a very lasting and malleable material, with a great capacity of transformation. Kiefer bought lead from the roof of the Cathedral of Cologne, Germany, when it was removed for restoration after having been devastated during World War II. Talk about the qualities of this material. What is lead used for nowadays? What are your associations with lead? Why do you think Kiefer is attracted to it?\nDiscuss the meaning or meanings that this material has in his work. Kiefer comments:\n“I really like the paradoxical idea that this Gothic cathedral was covered by lead (a very impermeable and resistant material for light rays), when it was supposed to be looking for its union with heaven. It’s a lead that I like because it suffered transformations, it holds many stories, footprints and you can fold it: it’s malleable. Time is also a material that you can work on, the same way that it works us.”2\nFor Kiefer, reconstructing history as a healing and regenerating tool against the wounds of the past becomes extremely important. Why do you think re-evoking the past is so important for Kiefer? Organize a debate about the consequences of the war in Germany or other countries and comment on how memories of war affect citizens. Discuss why learning about the past is so important.\nWhat does the title suggest to you? What alternative title would you give to this work? Why? How does knowing the title change your perception of the work?\nHow does the size of this painting affect its impact? Imagine this work much smaller. How would that change its impact? Why do you think Kiefer has chosen such a large scale?\nInterview by Anselm Kiefer with Bernarad Comment “Cette Obscure claret qui tombe des ètoiles”, Art Press, Paris, September, 1998 published in Anselm Kiefer. Exh. cat. Guggenheim Bilbao Museoa, Ed. Milano: Skira; Bilbao, 2007, p. 294-295\nDivide the class in groups. Each group will choose three materials which could transform and undergo a metamorphosis with the passing of time. You can use clay, flowers, grass, pieces of wood, plastic, soil, stones, water, etc. Think about the chosen materials—their properties, characteristics, uses—and write about them. Experiment with the materials: leave some of them outdoors, to observe the effect of climate on them, and keep some indoors. Look for changes and record them in writing. Take photographs of the process. What materials show greater transformation?\nCreate a memoir book gathering images, objects, and materials that symbolize your memories. Try to make your book reflect the passage of time. You may use some of the weathered and transformed materials from the first activity in your book to imply the passage of time.', 'White clouds against a blue sky.\nThe definition of a cloud is a visible mass of small water droplets or ice particles which are suspended in the sky.\nFacts About Clouds\nWhen a cloud forms in space, it is known as an interstellar cloud, or a nebula. Three components are necessary for clouds to form: water vapor, cool or cold air, and condensation nuclei. A white cloud means that the cloud was formed with warm air. A grey, or a black cloud, means that the cloud contains larger drops of water.\nThere are four main types of clouds:\n- Cumulus - the white and puffy clouds in the sky, generally under 7,000 feet in altitude. Made up almost entirely of water drops.\n- Stratus - the flatter clouds that stretch over greater lengths of distance in the sky, generally under 7,000 feet in altitude. Made up almost entirely of water drops.\n- Cirrus - clouds which are extremely high in the sky, generally over 18,000 feet, and are very thin and feathery looking. Composed primarily of ice crystals.\n- Cumulonimbus - clouds which go the highest of all clouds, up to 60,000 feet. They produce storms, thunder and lightening.\nAn example of a cloud is what makes the sky dark on a rainy day.\n- Cloud is defined as a mass of smoke, dust, steam or a mass of things suspended in the air.\n- An example of a cloud is what forms in the air just after a large explosion.\n- An example of a cloud is the formation of steam above a factory.\n- An example of a cloud is a large group of locusts traveling through the air.\n- To cloud is defined as to darken, cover or make gloomy.\nAn example of to cloud is fog covering a view of the landscape.\n- a visible mass of tiny, condensed water droplets or ice crystals suspended in the atmosphere: clouds are commonly classified in four groups: A (high clouds above 6,096 m or 20,000 ft) cirrus, cirrostratus, cirrocumulus; B (intermediate clouds, 1,981 m to 6,096 m or 6,500 to 20,000 ft) altostratus, altocumulus; C (low clouds, below 1,981 m or 6,500 ft) stratus, stratocumulus, nimbostratus; D (clouds of great vertical continuity) cumulus, cumulonimbus\n- a mass of smoke, dust, steam, etc.\n- a great number of things close together and in motion: a cloud of locusts\n- an appearance of murkiness or dimness, as in a liquid\n- a dark marking, as in marble\n- anything that darkens, obscures, threatens, or makes gloomy\nOrigin of cloudMiddle English cloude, clude, origin, originally , mass of rock, hence, mass of cloud ; from Old English clud, mass of rock: for Indo-European base see climb\n- to cover or make dark as with clouds\n- to make muddy or foggy\n- to darken; obscure; threaten\n- to make gloomy or troubled\n- to cast slurs on; sully (a reputation, etc.)\n- to become cloudy\n- to become gloomy or troubled\nin the clouds\n- high up in the sky\n- fanciful; impractical\n- in a reverie or daydream\nunder a cloud\n- under suspicion of wrongdoing\n- in a depressed or troubled state of mind\n- a. A visible body of very fine water droplets or ice particles suspended in the atmosphere at altitudes ranging up to several miles above sea level.b. A mass of particles or droplets, as of dust, smoke, or steam, suspended in the atmosphere or existing in outer space.\n- a. A large moving body of things in the air or on the ground; a swarm: a cloud of locusts.b. A collection of particles or other small entities: an electron cloud; a cloud of spores.c. An opaque mass of particles suspended in water: a cloud of silt in the pond.\n- A dark region or blemish, as on a polished stone.\n- A state or cause of sadness, worry, or anger: At the bad news a cloud fell over the celebration.\n- A state or cause of confusion or misunderstanding: writing made difficult by a cloud of jargon.\n- A state or cause of suspicion or disgrace: A cloud of mistrust lingers among the signers of the treaty.\n- a. A large area of coordinated wireless Internet service.b. The collection of data and services available through the Internet: stored company data in the cloud.\nverbcloud·ed, cloud·ing, clouds\n- a. To cover or obscure with clouds: We could not see the moon because the sky was clouded over.b. To darken, obscure, or make less transparent: Smoke clouded the sky. Steam clouded the windows.\n- a. To make sorrowful, troubled, or angry: a bad memory that clouded his spirits.b. To cause to appear sorrowful, troubled, or angry: Worry clouded her face.\n- a. To make difficult to know or understand; make obscure or uncertain: The economic downturn clouded the future of the project.b. To confuse: Don\'t let your resentments cloud your judgment.\n- To cast aspersions on; sully: Scandal clouded the officer\'s reputation.\n- a. To become cloudy or overcast: The sky clouded over.b. To become dark, obscure, or less transparent: The water in the tank clouded up.\n- To show sorrow, worry, or anger: His face clouded at the news.\nOrigin of cloudMiddle English, hill, cloud, from Old English clūd, rock, hill.\n- A surname.\ncloud - Computer Definition\nA wide area network (WAN) commonly is depicted as a cloud, which serves to obscure its complex inner workings from view. Data just pops in on one side of the cloud and pops out on the other side, so to speak.\nA communications network. The word ""cloud"" often refers to the Internet, and more precisely to some datacenter full of servers that is connected to the Internet. However, the term ""cloud computing"" refers to the services that have enabled the Internet cloud to become so prominent in everyday life (see cloud computing). See private cloud and personal cloud. A Cloud May Refer to Any Network A cloud can be a wide area network (WAN) like the public Internet or a private, national or global network. The term can also refer to a local area network (LAN) within an organization. For decades, network diagrams have used a cloud-like symbol to reduce the entire infrastructure of a network into simple entry and exit points when the specific network architecture is not material to the illustration. Inside the cloud, there may be any number of cables, routers and switches that handle the forwarding of data from one point to another. The cloud diagram may also include the servers that perform the required data processing.']"	['<urn:uuid:63a737f2-9c90-4180-a0ef-8e904bbba5f0>', '<urn:uuid:526253a6-0123-4aef-a316-34ac7806f42a>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	9	96	2393
36	what makes marine insurance different from regular insurance policies characteristics	Marine insurance has several distinct characteristics that set it apart from regular insurance: it can be purchased at market value or agreed value, it can be assigned (transferring insurable interest in the consignment), it is offered as an unvalued product where the insured sum is calculated by agreed-upon methods, and it comes with a warranty guaranteeing proper voyage conduct and legal compliance of all conditions.	['With the increase in intercontinental trade, it is unwise and makes no business sense to opt out of bringing in goods from another continent where they are far cheaper than in your locality. This, after all, guarantees a healthy profit margin. Marine transport is currently the leading choice for hauling bulky cargo across continents. Though this is typically a very safe means of transport, several things can go wrong and affect your entire shipment and consequently, the profits you stand to make.\nFreight demurrage and defense insurance is one of the key forms of insurance all marine charterers and ship owners should get. This is a discretionary insurance category that will cover your assets and interests and help you recover compensation in the event of any form of loss. Moreover, it covers your legal costs for defending you against any legal claims brought against you by different parties. This form of marine insurance and others might seem inconsequential and an unnecessary expense against issues you can cover with your standard business and personal insurance covers. The following characteristics distinguish marine insurance from other forms of insurance.\nIt Is Available At ‘Market Value’ And ‘Agreed Value.’\nWhen buying marine insurance, you can opt to insure the vessel for its market value. The value of your vessel will depreciate over time. You might, therefore, be compensated for less than its worth when taking the policy under market value should anything happen to it.\nWith the agreed value, you and the insurer agree on an amount upfront. Should anything happen to your vessel afterwards, you will be compensated as per this agreed value that will not factor in depreciation.\nYou Can Assign It\nUnless explicitly prohibited before or after your loss, you can assign your marine insurance policy. In this instance, however, you only transfer your insurable interest in the consignment and cannot transfer your entire interest in the insurance policy. Moreover, you cannot transfer your rights in the insurance policy unless you explicitly mention this during your transfer.\nIt Is Offered As an Unvalued Product\nAn unvalued insurance policy does not mention the price of the item or cargo to be insured. The policy leaves this sum to be calculated according to agreed-upon methods during the buying of the policy. You and the insurer will decide on the method of calculation and explicitly include it in your policy.\nYour Policy Comes With a Warranty\nThe warranty of your marine insurance policy is a significant agreement between you and the insurance company. In this instance, it means the voyage will be conducted properly. The warranty also guarantees that all conditions, content or objects attached to your marine insurance policy are legal.\nThe above elements of a marine insurance policy make it virtually impossible to replace with a regular insurance policy. While several insurers have come up over time, they will not all be appropriate for sea transportation. The ideal choice to give you peace of mind when investing in marine insurance is an insurer dedicated to this insurance field. With this alternative, you are guaranteed a customized product that will suit the issues you are exposed to in marine transportation.']	['<urn:uuid:a354e352-7ec7-4efd-9774-3fdd0817eadd>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	10	65	527
37	I'm planning a long wilderness trip. What's the max time off-grid possible?	The vehicle can remain self contained in the bush with no facilities for periods of at least one month but probably up to three months.	"['the ""Design Criteria"" section describes\nthe basic rational behind the major design decisions,\nthis section covers the actual specifications of the\nACCO Mk 5 ex-Army vehicle. The chassis has been extended\nby 1 metre.\n6354 diesel. 6 cyl, 6 litres (354ci), turbo charged.\nSMA 475, 5 speed synchromesh.\nrange transfer case. Six wheel drive. Permanent four\nwheel drive on rear bogie and selectable drive to\nrims with 1200/20 tyres, 1 spare wheel/tyre, 1 spare\nair over hydraulic split system enhanced with Jacobs\ntruck, 24v & 12v house. Total re-wire of\ntruck harness performed.\n2.35m (7\'8""), height 3.4m (11\'), length 8.1m\n(raw, no tanks etc): 6 tonnes (13,200lbs)\nChassis (including tanks, spare wheels, storage\nbins etc): 6.5 tonnes (14,300lbs)\nTotal: 12,500 kgs (27,500lbs)\n14,020 kgs (30,844lbs)\n(14"") under diffs, 500mm (20"") under most\n- 47 degrees\nExit - 47 degrees\nRamp over - 140 degrees.\nlitres (130gals) diesel, 100 litres (22gals) petrol\n(for motor bike & generator).\n(1125 miles) on-road. Off-road, depends on terrain.\n(9 tonnes) PTO driven winch, pulls to front or\nrear. 2x ground anchors, assorted shovels, chains,\nstraps, snatch blocks etc.\nlitres (130 gals) drinking/fresh in seven tanks, 55\nlitres (12 gals) grey. Serious filtration on drinking\nwater, use water from anywhere.\nx 9kg bottles.\n||4 x 100w, 2 x 125w and 4 x\n250w solar panels giving a total of 1650 watts nominal of panels generating\n51 amps @ 24 volts. 3300 watt inverter is also a 100 amp 24v battery\neu20i, 2000-watt 4-stroke inverter style generator.\nUsed only for backing up the solar system.\n||14 x 215Ah 12v deep cycle AGM batteries\nmakes a 36,120Wh battery bank or 1,500Ah @ 24v.\nburner Highland gas cook top, 240 litre 24v Fisher\n& Paykel fridge (120 fridge, 120 freezer), 30\nlitre Endel drawer fridge, 15 litre Engel chest fridge/freezer.\nAtwood gas hot water system. Spinflo gas oven/grill.\nDometic (aka Eiberspächer) diesel heater.\naluminium tread plate over all of body. No protrusions\nor visible windows/doors.\nbody is made entirely from steel. It is mounted to\nthe chassis at four points (diamond pattern) using\nspecial rubber mounts. This system allows the chassis\nto flex without affecting the body and also provides\ngood vibration isolation.\nProvision for 65 long neck beer bottles and three\nand AM CBs, hand-held UHF, NextG mobile phone.\nand refillable water fire extinguishers accessible\nfrom outside. Powder extinguishers and fire blanket\nheavy items below chassis and most storage as low\nroof provides shade, also well insulated with\nThermaSheild paint and closed-cell foam.\nsmall compressor for running air tools and pumping\ntyres but also used as a backup for the engine\'s\nwindows in living room fully open for ventilation.\nhas full-length ""windows"" (just openings,\nno glass) on three sides at bed level to provide\nfor one Honda SL230 motor bike in lounge room.\ntoilet opens out for use then returns to allow\npassage through entrance.\ndeck folds down to provide outdoor living space.\ncan be pressurised while driving to help keep\ndust from entering.\nself contained in the bush with no facilities\nfor periods of at least one month but probably\nup to three months.']"	['<urn:uuid:215a64db-cd37-4cfe-9a88-1143c6f16e93>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	12	25	517
38	How is the ship stabilized while unloading fish feed?	Water is pumped into specially designed chambers to provide more stability in rough seas while the fish feed is unloaded.	"['Fish feed weighing at sea\nWeighing solution offers maximum reliability and optimum protection against frost and salt\nAqua Shipping supplies its Norwegian fish farms with special feed by sea. In the past, the feed was filled on shore in big bags, weighed and then transported by ship to the floating fish farm. New silo ships now go directly to the farm and fill up there.\nApplication & Product\nFor precise traceable weighing of the feed, load cells were installed under the silos. For each silo, a weighing indicator transmits the measurement signals to a PC, which simultaneously corrects the gravitational constant via GPS\nAqua Shipping achieved the desired process optimisation with the weighing solution by Minebea Intec:\n- maximum reliability with German Quality\n- precise measurement results for an exact account of the load\n- optimum protection against waterlogging, frost and salt\n""Minebea Intec is a reliable partner for us in industrial weighing – especially because of the quality of technical advice. The products are reliable, precise and distinguish themselves by their long service life. Perfect for complex applications in challenging environments.""\nCEO Global Weighing A/S\nThe Norwegian supplier of fish and seafood Aqua Shipping sought to optimise its processes in the field of fish farm logistics. In the past, the storage, filling of and accounting for fish feed took place on land. The fish feed was transported by a freighter to a logistics base on land, unloaded on conveyor belts and filled into silos with the aid of a nozzle, before being filled into big bags, weighed and transported on a transport ship to the fish farms on the high seas.\nThe plan to fill and deliver fish feed directly on the spot – on the high seas – and not via land, was a challenge in many respects. The Danish company Fjordvejs Maskinfabrik A/S designed a special freighter with several silos. A large number of security aspects were taken into account in the process. Among other things, wishbones and hydraulic cylinders stabilise the silos during the voyage to prevent the cargo from slipping at sea. For more stability in rough seas, water is pumped into chambers specially designed for this purpose while the fish feed is unloaded.\nHigh demands were also placed on the weighing solution. Fjordvejs Maskinfabrik brought the Minebea Intec Sales Partner Global Weighing A/S into the project, which opted for load cells and weighing electronics by Minebea Intec: the PR 6201 load cell distinguishes itself particularly through its reliability and precise measurement results. Together with the X3 weighing indicator, the load cells provide a durable solution – even in challenging environments.\nAn important parameter of the project was the calibration of the silo scale by the Norwegian Metrology Insititute. The Norwegian Metrology Insititute was concerned that the position of the ship – at alternating latitudes – could influence the measurement result of the weighing solution. Global Weighing solved the problem by using a PC with GPS to correct the gravitational constant on board and thus ensure the reliability of the measurement results. The silo scale was then calibrated on the ship with weights of up to 300 tonnes and then successfully approved by the Norwegian Metrology Insititute.\n|Category||File Name||File Size||File Type|\n|Best Practices||Best Practice // Precise fish feed weighing on the high sea||1,82 MB|']"	['<urn:uuid:58728bde-b140-464e-adf1-781e984b5408>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	9	20	549
39	As someone interested in personal growth, I'm curious about how our unconscious side affects us. What does our 'shadow self' represent, and how does it impact our development?	The shadow represents the dark or unknown aspects of our personality - qualities we often dislike in others but fail to recognize in ourselves. When ignored or misunderstood, the shadow can become hostile, yet it often contains valuable forces that need to be integrated rather than repressed. In literary works like 'The Shadow,' this concept is demonstrated through the learned man's shadow becoming an autonomous entity that ultimately dominates him, representing suppressed aspects of his personality. The shadow's integration is crucial for personal growth, though confronting it often begins with a wounding of the personality and accompanying suffering.	"['""En mand, et ord - H. C. Andersens \'Skyggen\'"".\nThis paper has been published in Andersen og Verden, Odense 1993.\nA Man, a Word - Hans Christian Andersen\'s ""The Shadow""Hans Henrik Møller\n(summary for pages 303-10)\nAndersen\'s ""The Shadow"" is the story about a learned man and his shadow, his attempt to visualize and fulfil his own writings about Beauty, Truth and Goodness, and it is the story of the act of writing itself: the storyteller\'s attempt to maintain the potential for Poetry in a reality dominated by appearances.\nThe learned man is a Romantic and his aspirations for Truth are doomed to fail when confronted with the increasingly persistent reality represented by the shadow; the fact that the shadow originates out of his own longings reduces the learned man, not only to being the servant of the shadow, but even to being a shadow himself of what he might have been. The final irony of the story is merciless: There is no escape from that illusion into which the world has turned, the illusion itself being a perversion of the original Romantic claim for unity in truth.\nAnd yet the story has more to it. The actual narration of the story, the course of events, reveals certain gaps - gaps that challenge our final understanding of the story. Is the death of the learned man a final judgement on Romantic claims as such? Is it identical with the storyteller\'s intention, not only in presenting us with the story as told but by enclosing it in the very act of telling, or writing?\nThe narrative gaps produce a certain indecisiveness, a hesitation on the part of the reader as to how to understand the fundamental ambiguity of the text. Why does the learned man simply accept the loss of his old shadow and its replacement by another, thereby producing the initial shift from a more or less realistic to a fantastic narrative? Why cannot the entrance to the house of Poetry be found in the first place, within the realistic setting?\nThe realistic and the fantastic, the dream turning into a nightmare and the nightmare turning into reality: the basic structural levels of the text point to the narration itself as the place where the nightmare as well as the new, dreamt-of potential are contained. The very act of narration, then, is ambiguous: as the writings of the learned man aim at a re-gained, Romantic unity, so does the act of writing implicate the desire of the storyteller to come to terms with the world as described.\nWriting is an attempt to relate an unbearable reality to a new opportunity, but reality itself also produces a further distance from the goal. It reveals its own price.\nAs for the learned man, the dream turns into a nightmare when he is confronted with the return of his shadow. This return is ""unheimlich"" (Freud): the shadow comes from a place where the self is not at home, not at ease with itself; the return of the shadow is the return of something formerly suppressed - the subconscious knowledge of the learned man that Beauty and Truth have no place in a world ruled by appearances, his unwillingness to admit the sexual implications of his desire for Poetry personified as a young maiden. The return of the shadow even appears as a bargain with the devil: the shadow presents the learned man with the price of his desire for Poetry, the learned man accepts his debts and agrees to act as a shadow for the shadow. He even promises never to expose the true identity of the shadow; ""a man a word"", he says, receiving the seemingly obvious but yet elusive reply ""a word a shadow"". What does this signify?\nTo the learned man, the return of the shadow is the dream turning into a nightmare. To the storyteller, however, the return of the shadow is the very condition upon which the narration is based: the return of the shadow is the result of the act of writing, it is the emergence of writing as a problem per se. The learned man is ""a word"", only truly existing in his own writings about, and longings for, Truth, the writings and longings already being incorporated in the story as told. And a word is \'\'a shadow"", itself without real substance, a mere shadow of what might have been. The act of writing establishes its own story: it is a draft of Poetry, based upon the death of Truth.\nThe act of writing is the attempt of the storyteller to connect himself with what he is telling, to place himself at the centre of the world as described. In Andersen\'s ""The Shadow"", as in many other works of European fantastic or ""gothic"" literature, this attempt is haunted by its own shadow: to re-write the world as a place of regained opportunities is also to expose oneself to the unreal within writing itself.\nThe search of the learned man for Truth and Beauty reaches its peak in his vision of Poetry as a young maiden. Yet there seems to be no entrance to the house in which she is hiding; only by letting his shadow go astray can he hope for another glimpse. Poetry itself, then, becomes a shadowy experience, only accessible to a writer who has already crossed the borderlines of sanity.', 'Part 1 – Approaching the unconscious – Carl G. Jung\nThe sign is always less than the concept it represents, while the symbol always stands for something more than its obvious and immediate meaning. p.41\nThere are many symbols, however (among them the most important), that are not individual but collective in their nature and origin. p.41\nI gave up hypnotic treatment for this very reason, because I did not want to impose my will on other. p45\nThe individual is the only reality. p45\nOne can, for instance, make a relatively simple distinction between individuals who have “extraverted” personalities and others who are “introverted.” p47\nThese four functional types correspond to the obvious means by which consciousness obtains its orientation to experience. Sensation (i.e. sense perception) tells you that something exists; thinking tells you what it is; feeling tells you whether it is agreeable or not; and intuition tells you whence it comes and where it is going. p49\nIn our efforts to interpret the dream symbols of another person, we are almost invariably hampered by our tendency to fill in the unavoidable gaps in our understanding by projection–that is, by the assumption that what the analyst perceives or things is equally perceived or thought by the dreamer. p50\nThus, a dream cannot produce a definite thought. If it begins to do so, it ceases to be a dream becasue it crosses the threshold of consciousness. p53\nThere is no difference in principle between organic and psychic growth. As a plant produces its flower, so the psyche creates its symbol. Every dream is evidence of this process. p53\nI have already said that dreams serve the purpose of compensation… But when it is a matter of obsessive dreaming or of highly emotional dreams, the personal associations produced by the dreamer do not usually suffice for a satisfactory interpretation. p56\nJust as the biologist needs the science of comparative anatomy, however, the psychologist cannot do without a “comparative anatomy of the psyche.” In practice, to put it differently, the psychologist must not only have a sufficient experience of dreams and other products of the unconscious activity, but also of mythology in its widest sense. Without this equipment, nobody can spot the important analogies; for instance, to see the analogy between a case of compulsion neurosis and that of a classic demonic possession without a working knowledge of both. p57\nThe archetype is a tendency to form such representations of a motif–representations that can very a great deal in detail without losing their basic pattern. p58\nHere I must clarify the relation between instincts and archetypes: what we are properly calling instincts are psychological urges, and are perceived by the senses. But at the same time, they are also manifestations themselves in fantasies and often reveal their presence only by symbolic images. These manifestations are what I call the archetypes. p58\nThe general idea of Christ the Redeemer belongs to the world-wide and pre-Christ theme of the hero and rescuer who, although he has been devoured by a monster, appears again in a miraculous way, having overcome whatever monster it was that swallowed him…. The hero figure is an archetype, which has existed since time immemorial. p.61\nEmotional manifestations, to which such thought patterns belong, are recognizably the same all over the earth. p64\nThese unconscious factors owe their existence to the autonomy of the archetypes. Modern man protects himself against seeing his own split by a system of compartments. Certain areas of the outer life and of his won behavior are kept, as it were, in separate drawers and are never confronted with one another. p.72\nIf we could see our shadow (the dark side of our nature), we should be immune to any moral and mental infection and insinuations. p73\nA sense of a wider meaning to one’s existence is what raises a man beyond mere getting and spending. If he lacks this sense, he is lost and miserable. p.78\nThe interpretation of dreams and symbols demands intelligence. It cannot be turned into a mechanical system and then crammed into unimaginative brains. p.81\nIntuition is almost indispensable in the interpretation of the symbols, and it can often ensure that they are immediately understood by the dreamer. p82\nThis argument illustrates the way in which archetypes appear in practical experiences: They are, at the same time, both images and emotions. One can speak of an archetype only when these two aspects are simultaneous. p87\nThey (words) gain life and meaning only when you try to take into account their numinosity–i.e., their relationship to the living individual. p.88\nIt is this psychic energy that gives them (unconscious matters) such vital importance. p89\nFor the symbols are natural attempts to reconcile and reunite opposites within the psyche. p90\nPsychology is often accused of not being scientific on this account; but its critics fail to understand the scientific and practical necessity to giving due consideration to feeling. p90\nPart 3 – The Process of Individuation – M.-L. von Franz\nHe found that, on the whole, they (dreams) seem to follow an arrangement or pattern. This pattern Jung called “the process of individuation.” p.159\nSince this psychic growth cannot be brought about by a conscious effort of will power, but happens involuntarily and naturally, it is in dreams frequently symbolized by the tree, whose slow, powerful, involuntary growth fulfills a definite pattern. p161\nThe organizing center from whitch the regulatory effect stems seems to be a soft of “nucular atom” in our psychic system… Jung called this center the “Self” and described it as the totality of the whole psyche, in order to distinguish if rom the ‘ego,’ which constitutes only a small part of the total psyche. p.162\nThe Self can be defined as an inner guiding factor that is different from the conscious personality and that can be grasped only through the investigation of one’s own dreams. p163\nAgain, the realization of this uniqueness in the individual man is the goal of the process of individuation. p163\nStrictly speaking, however, the process of individuation is real only if the individual is aware of it and consciously makes a living connection with it.\nThe individuation process is more than a coming to terms between the inborn germ of wholeness and the outer acts of fate. Its subjective experience conveys the feeling that some supra-personal force is actively interfering in a creative way.\nThe ego must be able to listen attentively and to give itself, without any further design or purpose, to that inner urge toward growth. p.164\nCarpenter tree dream, p165\nLike the tree we should give into this almost imperceptible, yet powerful dominating, impulse–an impulse that comes from the urge toward unique, creative self-realization. And this is a process in which one must repeatedly seek out and find something that is not yet known to anyone. The guiding hints or impulses come from, not from the ego, but from the totality of the psyche: the Self. p167\nIf the development of consciousness is disturbed in its normal unfolding, children frequently retire from outer or inner difficulties into an inner “fortress”; and when that happens, their dreams and symbolic drawings of unconscious material often revel to an unusual degree a type of circular, quadrangular, and “nuclear” motif.\nThe actual process of individuation– the conscious coming-to-terms with one’s own inner center (psychic nucleus) or Self–generally begins with a wounding of the personality and the suffering the accompanies it. – p169\nThus is seems as if the initial encounter with the Self casts a dark shadow ahead of time, or as if the “inner friend” comes at first like a trapper to catch the helplessly struggling ego in her snare.\nThere is only one thing that seems to work; and that is to turn directly toward the approaching darkness without prejudice and totally naively, and to try to find out what its secret aim is and what it want from you. p170\nIt (shadow) represents unknown or little-known attributes and qualities of the ego–aspects that mostly belong to the personal sphere and that could just as well be conscious. In some aspects, the shadow can also consist of the collective factors that stem from a source outside the individual’s personal life. p.174\nThe shadow usually contains values that are needed by consciousness, but that exist in a form that makes if difficult to integrate them into one’s life. p178\nIf people observe their own unconscious tendencies in other people, this is called a ‘Projection.” p181\nThe shadow becomes hostile only when it is ignored or misunderstood.\nSo, whatever form it takes, the function of the shadow is to represent the opposite side of the ego and to embody just those qualities that one dislikes most in other people. p182\nIf the shadow figure contains valuable, vital forces, they ought to be assimilate into actual experience and not repressed. It is up to the ego to give up its pride and priggishness and to live out something that seems to be dark, but actually may not be. p183\nDivining in advance whether our dark partner symbolizes a shortcoming that we should overcome or a meaningful bit of life that we should accept–this is one of the most difficult problems that we encounter of the way to individuation. p184\nWhen it comes to such difficult ethical problems, no one can truly judge the deeds of others. p185\nThe anima is the personification of all feminine psychological tendencies in a man’s psyche, such as vague feelings and moods, prophetic hunches, receptiveness to the irrational, capacity for personal love, feeling for nature, and – last but not least–his relation to the unconscious. p186\nThe anima can be an inner guide. p193\n..(the Self) normally expresses itself in some kind of fourfold structure. The number four is also connected with the anima becasue, as Jung noted, there are four stages in tis development. (Eve-Helen-Mary-Sapientia) p195\nThe male personification of the unconscious in woman–the animus–exhibits both good and bad aspects, as does the anima in man. p198\nJust as the character of a man’s anima is shaped by his mother, so the animus is basically shaped by a woman’s father. p199\nHe (animus) personifies a cocoon of dreamy thoughts, filled with desire and judgements about how things “ought to be,” which cut a woman off from the reality of life. p202\nHe (animus) too has a very positive and valuable side: he too can build a bridge to the Self through his creative activity. p203\n.. the Self can appear as a supernaturally gifted girl. p209\nThe whole inner psychic reality of each individual is ultimately oriented toward this architypal symbol of the Self. p215\n(dream) When the stone is polished, it will begin to shine like a mirror so that the bear can see herself in it; this means that only by accepting earthly contact and suffering can the human soul be transformed into a mirror which the divine powers can perceive themselves. p217\nA round stone is the symbol of the Self. p218\nIn fact, passion that goes beyond the natural measure of love ultimately aims at the mystery of becoming whole, and this is why one feels, when one had fallen passionately in love, that becoming one with the other person in the only worthwhile goal of one’s life. p219\nThe Self also appears as a crystal. p221\nBut some profound inner experience of the Self does occur to most people at least once in a lifetime.\nDr.Jung put forward a new concept that he called sychronicity. This term means a “meaningful coincidence” of outer and inner events that are not themselves casually connected. p226\nThe classical Chinese tests did not ask what causes what, but rather what “likes” to occur with what.\nSynchronistic events, moreover, almost invariably accompany the crucial phases of the process of individuation. p227\n… the obligation attached to the process of individuation is often felt to be a burden rather than an immediate blessing.\nThis miraculous child is a symbol of the Self that literally “depresses” the ordinary human being, even though it is the only thing that can redeem him” p237\nThrough the unconscious ties those who belong together come together. That is one of the reasons why attempts to influence people by advertisements and political propaganda are destructive, even when inspired by idealistic motives.\nif a man who want to influence public opinion misuses symbols for this purpose, they will naturally impress the masses in so far as they are true symbols, but whether of that the mass unconscious will be emotionally gripped by them is something that cannot be calculated in advance, something that remains completely irrational. p240\nAs far as we at present understand the process of individuation, the Self apparently tends to produce such small groups by creating at the same time sharply defined ties of feeling between certain individuals and feelings of relatedness to all people. p241\nOnly if he can use his freedom to create something meaningful is it relevant that he should be free. That is why finding the inner meaning of life is more important to the individual than anything else, and why the process of individuation must be given priority.\nBut is a single individual devotes himself to individuation, he frequently has a positive contagious effect of the people around him. It as as if a spark leaps from one to another.And this usually occurs when one had no intention of influencing other and often when one uses no words. p245\nThe mandala serves a conservative purpose–namely, to restore a previously existing order. But it also serves the creative purpose of giving expression and form to something that does not yet exist, something new and unique. The second aspect is perhaps even more important than the first, but does not contradict it. For in most cases, what restores the old order simultaneously involved some element of new creation. The process is that of the ascending spiral, which grows upward while simultaneously returning again and again to the same point. p247']"	['<urn:uuid:8340a17b-956d-4cae-9e2c-16cae2b515ff>', '<urn:uuid:293e1888-d7bf-4501-82b2-a3b6057afb1f>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:57:18.859842	28	98	3244
40	want to become guidance counselor what personality traits and skills needed	To become a good Guidance Counselor, you need specific personality traits beyond just the required graduate degree. Key traits include being knowledgeable, professional, and patient. You must also have finely tuned communication skills. The role requires strong active listening abilities, problem-solving skills, and relationship-building capabilities. Additionally, counselors need to establish credibility in areas like child abuse, special needs, and intervention strategies.	['Salaries For Degrees in Guidance Counseling\nThose who majored in Guidance Counseling can work in a variety of jobs. It is impossible to say what you personally will do with a degree in Guidance Counseling, our survey panel picked the following jobs as likely options:\nStudents with a degree in Guidance Counseling are considered well prepared for\nbecoming educational, vocational, and school counselors.\nThe median salary for people with online bachelors guidance counseling degrees is $44,686.60. The lifetime value of this degree is approximately $873,303.00.\nSalaries are highly dependent on individual negotiating skill, experience, your employer, region, and a host of other factors. The estimates we show on these pages are just that: estimates. Your individual experience will likely vary.\nWhere does this come from?\nThe Bureau of Labor Statistics, a unit of the US government, classifies all workers into some 800-odd occupational categories. We paid a team of freelancers to get their view on what type of degree a holder of each type of job would likely have majored in. For pairs which had a high degree of consensus, we created a link between the degree and the job.\nFrom this, we calculated the average salary for Guidance Counseling degrees and converted it into a lifetime value. We then compared it against other degrees at the same level of schooling (such as online associate guidance counseling degrees or online bachelors guidance counselors degrees), so that you can make informed educational and employment decisions.\nWhat Can a Guidance Counseling Student Expect to Learn?\nGuidance Counselors get a lot of criticism from impatient children, confused parents, and disgruntled teachers. If you want to become a Guidance Counselor you will need a graduate degree but to become a good Guidance Counselor you will need to possess certain personality traits to back up that graduate degree. Guiding children and helping parents is a noble and satisfying profession which will lead to a very fulfilling career. Guidance Counselors need to be knowledgeable, professional, patient and have finely tuned communication skills. In order to practice as a professional Guidance Counselor you will need to have a graduate degree from an accredited program and there are many Guidance Counseling Degree programs across the country, both online and in a traditional classroom setting.\nAlthough there are many different schools which offer Guidance Counseling Degrees, most of them offer very similar course options. Some of the popular Guidance Counseling course options include but are not limited to the following:\nProfessional Orientation in School Counseling\nThis is a survey class which will explain what it is that school counselors can expect to encounter in the workplace. This gives the student a very broad overview of their work environment, clientele, salary ranges, and job duties.\nCounseling Theories and Practice\nThis class discusses the major theories behind counseling practices throughout decades of history such as: behavioral, social, behavioral, cognitive, and psychoanalytic. The student learns the importance of active listening, communication skills, and problem solving. Relevant topics of study include: confidentiality training, relationship skills building, evaluation and referral.\nThere are many psychology courses which will be a part of your curriculum as you study for your Guidance Counselor Degree. Some of these courses include Developmental Psychology, Psychology of Learning, Research Methods and Educational Psychology.\nAlong with a background in psychology, counseling courses make up the bulk of your curriculum. Some of the actual counseling courses which will make up your curriculum will include: School and Family Consultations, Group Counseling and Leadership, Multicultural Family Counseling, Vocational and College Counseling.\nAlong with the many standard courses you will need to take to earn your Guidance Counseling Degree, you will also need to establish credibility in the areas of child abuse, special needs, and intervention strategies.\nClinicals and Practicums\nIn addition to course work, all students will need to successfully complete several semesters’ worth of actual counseling practice under the supervision of a licensed counselor. These can range from two to five days per week in an actual learning center, working with the children and reporting to a guidance counselor. These professional evaluations of your skills are a critical part of the learning process and it is important to apply all you have learned in these situations. This is also an invaluable part of your education as you will learn actual techniques from professionals in the field.\nOnline Schools Offering Accredited Guidance Counseling Degree Programs\nThere are many successful online Graduate Counseling Degree programs! This is a perfect option for you if you already have a full time job in another field and want to obtain a graduate degree in Guidance Counseling. Some of the top online colleges and universities which offer a graduate degree include:\n1) Argosy University. This university offers a Doctor of Education in Counseling Psychology, all from the comfort and convenience of your own home.\n2) Liberty University. This university offers a Masters of Education in School Counseling.\n3) University of North Dakota. This university offers a Masters of Art in Counseling with an emphasis upon school counseling.\n4) Seton Hall University. This university offers a Masters of Art in School Counseling and has a very prestigious program.\nTop Colleges & Universities Offering Campus-based Guidance Counseling Degrees\nThere are many colleges and universities across the nation which offer Graduate Counseling Degrees. Some of these establishments award Graduate Degrees from their School of Psychology while some offer them from their School of Education so be careful to check before you apply, if you have a preference. Depending upon your location and willingness to travel or relocate; you may be able to choose from several notable establishments. Some of the highest ranking universities which offer Graduate Counseling Degrees include:\n1) Northwestern University. This university is ranked in the top 25 psychology schools in the nation. Upon completion of their program you will receive a Masters of Arts in Counseling Psychology.\n2) University of Florida. This university specializes in scientific research which is very important for advancement in this field of study. Upon completion of their program you will receive a Masters of Science in Counseling Psychology.\n3) University of Missouri. This university boasts a specialization in multicultural studies which is valuable if you intend to serve in impoverished communities or internationally. Upon completion of their program you will receive a Masters of Education in Counseling Psychology.\nFamous Students of Guidance Counseling\nCareers in Counseling have been a chosen profession and hobby for many famous people in American History. Some of the noble men and women who have practiced in the field of counseling are:\n- Presidents Martin Van Buren and Jimmy Carter (pictured right)\n- Famous psychologist Carl Jung\n- Renowned writer Nathaniel Hawthorn\n- Actresses Shirley McClain and Shirley Temple Black\n- Mother Theresa\nThe salaries for Guidance Counselors vary greatly depending on the length of time you have been working, the grade level you are trained to work with and the location of your job. Counselors who work at the primary and secondary school levels typically earn the highest salaries with median earnings around $54,000 per year. Areas with a high demand and high turnover for teachers often pay their Guidance Counselors higher salaries than those with small populations.\nChoosing to earn your Graduate Degree in Guidance Counseling is a smart decision for your future. Currently, the field has more job openings than graduates to fill them. The field is expected to continue growing and thriving.']	['<urn:uuid:8f147897-5269-40a6-baa3-65dc16e847ec>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	11	61	1235
41	security expert compare malware infection symptoms incident management preparation strategies	Malware infections show clear symptoms like increased Internet traffic, slower computer performance, unusual router port activity, and nonsensical email attachments. For preparation against such threats, organizations must develop an Incident Response Plan (IRP) covering organization, processes, and procedures, design resilient IT infrastructure, and conduct proactive response team exercises. This preparation helps limit potential harm to finances, operations, and reputation when attacks occur.	"[""Intrusion detection systems and firewalls that detect and prevent network compromise are critical elements in any secure network environment. But, let’s get real: Many people lack working backups and decent password controls, let alone dedicated defensive gear.\nMost small to medium enterprise organisations have limited investment in security. They also lack the capacity to maintain security systems: Intrusion detection signatures and firewall rules need regular updates; log files need analysis; and policies need updating.\nSo, are you compromised and how would you know?\nWhen I have responded to cyber attacks, frequently it's the client and their staff who have noticed “something odd going on”. Maybe they’ve noticed their Internet traffic increasing, a computer running slower than normal, or the light on a specific port of their Internet router flashing like crazy.\nThere are two types of companies: those that have been hacked, and those who don't know they have been hacked.\nIt’s quite a testament to everyday users just how much they notice changes in their computer’s behaviour. Unfortunately it often takes too long to join the dots, by which time the damage is done.\nThings behaving badly\nLet’s take a brief look at some ‘everyday’ indicators of compromise. I’m not talking about formal malware signatures or technical descriptions of file system detritus. More generally, we’ll look at the anomalous behaviour of computers and networks under a few common attacks.\n· General malicious software\nGeneral malware infections can exhibit a range of behaviours. They can be a prelude to installation of ransomware, credential stealers, back-doors, bot-net clients and much more.\nMost malicious software arrives in an email as an attachment, or infects devices through a compromised website.\nEmail attachments from malware often have interesting subject lines. They are crafted to appeal to a wide audience and may even seem relevant to your business. E.g. “Hi, here’s the resume we discussed” or “Invoice for work”.\nIf you open an attachment accidentally, you’ll notice pretty quickly that the content is nonsense, it’s not relevant to you, the application crashes straight away, or nothing seems to happen.\nIf you’re running up-to-date systems, network malware installers from compromised sites may cause a pop-up security warning that users might accidentally approve.\nDigital muggings from ransomware are pretty noisy. Once you’re infected, your files are scrambled and you are presented with a ransom message.\nBecause file encryption can be a slow process, there’s a small window of time between infection and complete encryption where you may notice your computer running slowly.\nCurrent ransomware strains usually scan external hard disks and network shares. This means network and hard disk lights will be thrashing even when you’re doing nothing at all.\n· Botnet clients\nBotnet software (clients) can infect a range of computers and devices. These clients help spammers and hackers by recruiting your equipment to do their bidding.\nBotnets use your computer and network connection to participate in further attacks, spam email or malware distribution, or network scanning. These attacks are usually very active on the network; they’ll hammer your link and chew through your data cap, or result in additional Internet charges.\n· Automated and scripted attacks\nHackers use scripted attacks for reconnaissance, and to get their foot in the door. They use automated attacks to cast a wide net and find vulnerable servers or software.\nBecause hacking scripts work by trial-and-error, they’ll test your system to see if it’s vulnerable to hundreds of exploits. An up-to-date system will be safe from most and will simply block the attempt, or create an error message. Often these errors will be visible in server logs.\nIf a scripted attack succeeds in identifying vulnerabilities, your attack might turn from automated to directed. Instead of dealing with a random script flailing about, you’ll have a hacker directing their efforts at your network.\n· Directed attacks\nDirected attack behaviours vary greatly as they’re being staffed by hackers instead of automatic scripts. They’re less noisy than malware and scripted attacks because the hackers can be more judicious in how they attack your network.\nA hacker moving through your network will still leave a trail of evidence. Their initial goal will be to establish a presence in your network. To do this they’ll often modify configurations, install software, download malware, and create or modify user and system accounts.\nYou might notice new software installed on your computer. I’ve seen common desktop sharing tools, remote presentation and meeting software installed on compromised PCs. On windows you can check for ‘recently added’ programs, and on Mac you can check the ‘System Report’ for software applications and installations.\nIf you’re familiar with task-manager on Windows or Activity Monitor on your Mac, you might see unexpected processes. A Google search of the process name can help identify real tasks from malicious ones.\nBecause these types of attackers are seeking to maintain access, they may install hard to detect root-kits and backdoors. But, more often they’ll use less sophisticated ways of maintaining access such as adding new users, and modifying router and firewall rules.\n· Online, webmail and the cloud\nBreaches of online systems and cloud services will leave different footprints depending on the service.\nMany online services have the ability to check your login history, reviewing these could provide evidence of online breaches - hackers will leave evidence of strange connection times and weird IP addresses.\nRead more: Chatbots: Opportunity and threat\nWeb based email clients may also indicate the presence of a compromise through other activities like sent or deleted email messages, as well as emails marked as read unexpectedly.\nAll common security incidents leave a trail of evidence. By being familiar with your device configuration, active users, and Internet data usage you’ll be better placed to use your dual-shoulder-mounted anomaly detectors.\nBe aware of the threats and how to recognise some when something’s not right. If something seems out of place, it might be an indication of compromise and warrant further investigation.\n[Takes steps to improve your security: Quick and dirty guide to security policy creation]\nIf you’ve been breached, you need to move quickly and decisively. You can significantly minimise damage if you disconnect suspected equipment, shut down networks, change passwords and freeze bank accounts.\nIn an ideal world secure operating systems, applications and networks will stop you from being compromised in the first place. But seeing as that’s fantasyland, you’ll just have to keep your wits about you.\nNikolai Hampton holds a Master's Degree in Cyber Security and is a director of Impression Research. He consults on matters of privacy, security, digital forensics, and incident response. His focus is on the correct application of cryptography. He is passionate about educating business on complex security issues. Follow Nikolai on Twitter: @NikolaiHampton"", 'Preparing for cyber incidents involves more than merely being ready to react\nto (and neutralize) a one-off cyber attack. It involves the ability to respond effectively, plan proactively, and to defend your critical systems and data assets. To get ahead of evolving threats, and to recover thoroughly when attacks do occur, you need to be familiar with the Cyber Incident Management Life Cycle.\nCyber incidents can run the gamut, from a simple email phishing attack to sophisticated malware or ransomware. Organizations now are investing more than ever in cyber-incident and attack preparedness, with 74% of companies saying Best Practices for incident prevention are their number one cybersecurity priority, followed by compliance mandates at a close second. A major part of this investment in readiness is the Incident Management Lifecycle, which lays out a framework of event management and how companies should respond in the event of an attack, hack, or breach.\nBut what exactly is the incident response lifecycle? What are the various stages in the life cycle of incident management, and what specific elements, steps, and processes do they entail? Read on to learn about the incident management lifecycle process, and how it can be used to protect your business.\nWhat is the NIST Incident Lifecycle?\nAll 4 phases of the incident response lifecycle stem from standards and best practices set forth by the National Institute of Standards and Technology. By law, Organizations must create and operate a formal incident response capabilities in alignment with NIST’s incident management and response lifecycle framework for incident handling. That’s primarily because, in the event of an actual cyber breach or incident, organizations will need to report to (and coordinate with) appropriate government agencies who will expect that the issue is being handled in accordance with NIST’s incident response lifecycle.\nPer NIST, organizations should document their guidelines for interactions with government agencies and any other organizations throughout the entire process of the incident management lifecycle. This could include response teams, law enforcement, media, vendors, and any potential victims affected by a hack. Because these communications need to occur quickly at various stages of the lifecycle, organizations need to have predetermined communications guidelines so that the appropriate information is shared with the correct entities.\nIn general, having written guidelines for how incidents will be responded to, and prioritized throughout the organization, is a point of emphasis in the NIST cybersecurity framework. Prioritizing the handling of individual incidents is a critical decision point in the incident response process. Effective information sharing and communication throughout the lifecycle can help organizations identify situations that are of greater severity and demand immediate attention, and coordinate teams, parties, and departments throughout all four stages of the incident management lifecycle.\nFinally, one of the key functions of the NIST incident management lifecycle is for organizations to use lessons learned throughout the process to gain valuable lessons for future use. After any major incident has been handled, organizations should hold a debrief and review to make necessary process improvements, and proactively identify systematic weaknesses to be remedied. While we’ll discuss this stage in greater detail later on, before diving into the four stages it’s important to remember that the NIST incident management lifecycle isn’t just about taking steps to limit the damage of an attack, but about analyzing what exactly happened to prevent similar attacks in the future.\nPhase 1: Preparation\nThe nature of your business, data types, and critical systems will determine how you approach the first phase of the incident management lifecycle, which is Preparedness. Defenses against potential hackers and attacks should be formulated based on the potential impact on your company, the likelihood of such an occurrence, and exactly how critical the systems or data affected might be. This is typically determined by a formal risk assessment (with your cybersecurity partner), designed to identify potential systems vulnerabilities so that your organizations can implement proper protective (and preventative) countermeasures.\nIn short, the preparedness phase is designed to determine (and quantify) the potential risk to your systems and data. You’ll work with your cybersecurity partner to pinpoint your risk appetite, and then begin developing an effective Incident Response Plan (IRP) in accordance with the NIST lifecycle guidelines. Your IRP will cover not only preparedness but also the other three phases of the incident management lifecycle. You’ll want to periodically review your IRP, and keep it up to date as potential threats and risks to your systems and data evolve. The preparedness phase is vital because it ensures that, if and when an attack does occur, the harm caused to your finances, operations, and reputation is limited as much as possible.\nThe basic components of your phase one preparation plan should include:\n- Design and development of an IRP covering organization, processes, and procedures.\n- Design and implementation of a resilient IT infrastructure to sustain business operations in the event of an incident.\n- Proactive response and incident management team exercises to test incident response processes, procedures, and personnel.\nPhase 2: Detection and Analysis\nHopefully, your organization never moves beyond phase one of the incident management lifecycle, meaning that hackers aren’t able to break into your systems in the first place. However, if they do manage to breach your defenses, you’ll need to be ready for what’s going to take place in phase two of the lifecycle, which is threat Detection and Analysis. On a high level, the detection part of phase two includes setting up alerts and notification for any suspicious activity that might take place within your systems. But this also includes periodic monitoring and follow-ups of suspicious activity, even if it’s deemed harmless upon initial analysis.\nSurprisingly, far too many organizations actually fall flat when it comes to phase two of the incident management lifecycle. That’s because, all too often, management comes to the conclusion that the expense and effort of proactive threat monitoring, detection, and analysis far outweigh the risk. Maybe the company has never had a breach, and there are seemingly more pressing projects or initiatives that demand those financial resources. While this type of thinking makes some logical sense, it’s akin to driving a car without insurance. Experiences show that there are far too many instances when an enterprise becomes aware of a data breach or attack, only to find out later that it’s actually been an ongoing attack for several weeks, months, or even longer.\nIn last year’s Target cyber attack, for example, it was found that hackers had gained access to critical customer information months before the actual breach was identified. Therefore, the importance of proactive threat detection and incident analysis can’t be overemphasized. Effective implementation of phase two will help identify the source, extent, impact, and details of any breach before it metastasizes too far. And without proper analysis, managing the next two phases of the lifecycle will prove far more difficult.\nWork with your cybersecurity partner to create a phase two plan that includes:\n- Leveraging of cyber threat intelligence (CTI) capabilities and other methods to formulate a comprehensive monitoring program to support ongoing monitoring and detection.\n- A cyber compromise assessment to detect unknown compromises and validate the ongoing health of your network environment.\n- Information gathering (and prioritizing) of individual incidents and concrete steps for incident response.\n- Methods of forensic preservation and analysis of threat detection data to determine the extent and impact of any potential malicious actors within your systems.\nPhase 3: Containment, Eradication, and Recovery\nFor organizations that haven’t effectively implemented steps for all four phases of the incident management lifecycle, phase three is all too often the first phase that’s actually acted upon. Due to whatever reason, companies don’t adequately prepare or monitor for threats and are then left reacting to a specific incident in an effort to contain the problem, eliminate the issue, and attempt to restore the system to its state prior to the incident. Needless to say, this can be time-consuming, disruptive, and costly. Phase three activities, while necessary, will be much more effective if phases one and two are carried out in close accordance with the NIST framework.\nFor example, your organization will need to take the time and resources necessary to identify the type of incident (malware, ransomware, phishing attack, etc.), in order to take the right steps to contain and eradicate the threat, as well as recover critical systems and data. And as your incident response team works towards these ends, many of your users may not be able to conduct business as usual. The result is not only lost man hours, but potentially revenue losses and damage to your reputation.\nThat being said, the focus of phase three should be containment and eradication of any and all threats. This will require a certain amount of downtime, which you should plan for along with your cybersecurity partner. After the threat has been eliminated, during remediation all affected systems need to be restored to where they were before the incident took place. Proper phase one and two planning will substantially reduce the time, financial cost, and organizational effort required for all phase three activities.\nBut in a nutshell, your phase three planning should cover the following:\n- Taking risk-mitigating actions to prevent further impact and damage to your organization.\n- Removing any known existing threats from the network completely.\n- Plan for near-term incident remediation, remediation strategy, and roadmap for recovery.\n- Resuming normal business operations, as well as developing long-term risk mitigation based on documentation of lessons learned.\nPhase 4: Post Incident Activity\nOnce a cyber incident has been contained and remediated, and operations normalized, your phase four post-incident activity should focus on what lessons you’ve learned. Be sure to ask some of the following questions:\n- How did the incident occur in the first place?\n- How can similar incidents be prevented from reoccurring in the future?\n- What existing preventive measures can be strengthened, or additional ones that can be put into place?\n- How can monitoring and alerting processes be improved to ensure more timely and accurate notifications?\n- How can containment, remediation, and recovery processes be better streamlined to minimize overall downtime and disruptive activities?\n- How can management ensure that the incident (and others like it) have not negatively impacted the overall business?\nYou’ll also want to review incident logs to answer some (or all) of the questions above, especially to map out any soft spots in your security configurations. Subsequently, work with your cybersecurity partner to tweak any policies or procedures needed to eliminate any of the weaknesses you’ve spotted. Finally, begin testing any new processes, rules, or systems configurations to validate their effectiveness (while bearing the potential of false positives in mind). A well-equipped cybersecurity partner can offer services like breach assessments, forensic analysis, and penetration testing to help optimize your approach and implementation of phase four of the incident management lifecycle. Any claims management, legal, or regulatory compliance issues should also be taken into account in phase four.\nBy now you should be able to recognize the basic four phases of the incident management lifecycle and realize that adhering to those phases as set forth by NIST is truly a team, organization-wide effort. Moreover, all four phases need to be implemented in concert for optimal protection, elimination, and remediation of any cyber attack that could potentially affect your critical systems or data.\nYou’ll want to work with your cybersecurity partner to understand the how’s and why’s of the NIST framework, and how they apply specifically to your business. And while you might think that a cyber attack is something that simply won’t (or can’t) happen to your organization, investing in early-stage phase one preparedness is simply not an option.\nOnce you’ve formulated an IRP in phase one of the incident management process, it’s also critical to invest the proper amount of resources and tools into phase two early detection and analysis. Many malicious actors can exist within a system for long periods of time, stealing data or doing damage to your systems, if adequate threat detection and incident management tools aren’t implemented with your cybersecurity partner. And once an attack or cyber crime does occur, having a plan for phase three response, containment, and elimination is essential to making sure the damage is limited and that workflow can continue. Once all systems have been restored, conduct a thorough post-mortem to bolster your defenses and lessen the odds of a similar attack occurring in the future.\nDownload Our Breach Response Checklist\nWhether you’re in the midst of a breach or preparing a plan for the future – this checklist will give a good starting point for responding to a breach. Upon filling out this brief form you will receive the checklist via email.']"	['<urn:uuid:5c31ca17-7239-4de9-a357-957ee5c12df2>', '<urn:uuid:da9b1aa7-ec51-4e25-a327-7184ec4a9f1b>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	10	62	3238
42	seaplane lagoon contamination cleanup dredging radioluminescent paint items found details	The Seaplane Lagoon (IR Site 17) had sediment contamination from metals, PCBs, and pesticides, primarily from industrial wastewater disposed into storm drain lines before the mid-1970s. During dredging of contaminated sediment, 51 items with small amounts of radioluminescent paint were found, most smaller than a quarter. These items were found very sparsely, at a rate of about one item per 2,000 cubic yards of sediment or four per acre, and did not appear to originate from the outfalls. The BCT is discussing appropriate land-use controls for this low-level radiological contamination.	"[""Memo from City Manager to City Council Regarding Status Report of Environmental Conditions and Clean-up at Alameda Point, January 20, 2015\nPresentation on Status Report of Environmental Conditions and Clean-up at Alameda Point. (Base Reuse 819099)\nTo: Honorable Mayor and Members of the City Council\nFrom: John A. Russo, City Manager\nRe: Presentation on Status Report of Environmental Conditions and Clean-up at Alameda Point.\nAlameda Point was an active United States Navy (Navy) base from 1940 to 1997. While active, as many as 16,000 to 18,000 personnel worked at the Naval Air Station (NAS Alameda). It was selected for closure as part of the 1993 Base Realignment and Closure (BRAC). Operational closure round, which occurred in April 1997.\nOperations at the base included aircraft maintenance, automotive repair, storage tanks and piping (diesel, gasoline, and jet fuel), painting, paint stripping, machine shops, and a skeet range. Localized areas across Alameda Point became contaminated with residues from these activities. The contaminants include fuels, metals, solvents, PCBs, pesticides, polynuclear aromatic hydrocarbons, and, in a few instances, radioluminescent paint residues. The Navy is responsible for the clean-up of contamination associated with its former activities at Alameda Point and has been actively investigating and remediating the property for the last 15+ years, during which time it has spent over $500 million on these efforts. The City's zoning, infrastructure and development plans have been, and will continue to be, coordinated with the Navy's clean-up levels and schedule. The Navy can only transfer the property to the City once the environmental regulators agree that the property has been cleaned-up to federal standards.\nThis evening's staff report provides an overview of the environmental clean-up process at Alameda Point and highlights issues with a few key sites that are undergoing clean-up and/or are scheduled for transfer to the City. The City's environmental consultant, Peter Russell of Russell Resources, who has been the City's independent environmental consultant and advisor at Alameda Point for the last 17 years, will make the presentation this evening. Dr. Russell reviews all pertinent environmental documents and actively participates with the Navy and environmental regulators in decision-making discussions for the remediation process. Dr. Russell also attends the Navy's community advisory committee, the Restoration Advisory Board meetings, which include discussions among the Navy, environmental regulators, and community residents and stakeholders.\nTo evaluate the nature and extent of the contaminated areas, and to complete any needed remediation, the Navy conducts two parallel environmental programs at Alameda Point: (1) the Petroleum Program, and (2) the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) Program. At locations where petroleum and CERCLA contamination is co-located, both are sometimes addressed in the CERCLA Program.\nThe Navy's cleanup of petroleum at Alameda Point is overseen by the San Francisco Bay Regional Water Quality Control Board (Water Board). The Petroleum Program includes 160 sites with almost 300 individual environmental units (fuel storage tanks, piping, sumps, etc.). About 115 of the Petroleum Program sites have completed investigation with remediation as needed, and have been closed. Roughly 45 of the remaining sites are currently being evaluated for closure. The Navy plans to close all of the Petroleum Program sites, although some closures will not be completed until after transfer to the City. The City participates in the Navy's and Water Board's work on the Petroleum Program as an observer, providing comments on proposed actions as appropriate.\nThe federal CERCLA Program, commonly known as Superfund, addresses most other types of contamination in soil, groundwater, and soil gas. Petroleum and petroleum products are generally excluded. The CERCLA Program's BRAC Cleanup Team (BCT) is made up of the US Environmental Protection Agency (EPA), the California Department of Toxic Substances Control (DTSC), the Water Board, and the Navy. In certain instances, the California Department of Public Health provides technical support to DTSC's participation in the BCT. The City participates in the BCT's work as an observer, providing comments on proposed actions as appropriate.\nThirty-four Installation Restoration (IR) Sites (sites designated by the Department of Defense for cleanup of contamination from past operations) are in Alameda Point's CERCLA Program: 16 IR Sites are now closed. Active remediation is complete at two more sites, but full closure will occur after natural processes lower contaminants to target levels. All of the IR Sites within the land owned by the City are closed or in the natural attenuation phase of remediation. Exhibit 1 depicts the IR sites and Exhibit 2 presents a map of the Navy's conveyance phases. Eight IR Sites are currently in remediation and the BCT is completing clean-up plans for the remaining eight sites. Most IR Sites are grouped into Operable Units (OUs) for collective investigation and cleanup. Seven of the eight sites that are still in the planning stage are clustered in two OUs (OU-2B and OU-2C). Four key CERCLA sites (IR Site 1, IR Site 17, OU-2B, and OU-2C) are discussed in more detail below.\nIR Site 1\nIR Site 1 is the former landfill in the northwest corner of Alameda Point. It operated from 1943 until 1956, when the base began using the former landfill in the southwest corner of Alameda Point (IR Site 2). Active remediation of the IR Site 2 has been completed, but work on IR Site 1 is currently in progress. The cleanup of IR Site 1 consists of groundwater treatment (active phase appears complete) and containment of the waste and contaminated soil. Excavation with off-site disposal of the landfill contents would have been prohibitively expensive. The landfill's containment involves relocation of contamination present along a portion of the shoreline to the interior of the landfill (complete), a waste isolation barrier along another stretch of the shoreline (complete), a 3-foot thick soil cover, and wetlands restoration. Land-use controls will allow passive recreational use of the former landfill site once remediation is complete. Remediation of IR Site 1 is expected to be complete in 2015.\nIR Site 17\nIR Site 17 is the Seaplane Lagoon. Most of the sediment contamination (metals, PCBs, and pesticides) at IR Site 17 came from industrial wastewater that the Navy routinely disposed into storm drain lines prior to the mid-1970s. The Navy has successfully dredged and off-hauled this contaminated sediment, which was located around storm drain outfalls at Seaplane Lagoon's two northern corners. During the dredging process, the Navy encountered 51 items (most smaller than a quarter), which had small amounts of radioluminescent paint on them. These items, which appear not to have originated from the outfalls, were found at the rate of about one item per 2,000 cubic yards of sediment (or four per acre), making them very sparse. They potentially are present elsewhere in Seaplane Lagoon sediment. The BCT is currently discussing what, if any, land-use controls are appropriate for this low-level radiological contamination. There may need to be a limited amount of dredging of the Seaplane Lagoon to facilitate plans for a ferry terminal, although it is possible that the ferry terminal could be constructed without any dredging. If dredging is required, the cost of dredging and disposing of the sediment would be borne by the City and/or its developers. Prior to title transfer, the City will be seeking an exemption from any need for a radioactive materials license with the State due to the minimal risk of any residual contamination left in place and the extensive protections afforded by the federal CERCLA process.\nOU-2B is the formerly industrialized area along Atlantic Avenue and includes IR Sites 3 (Abandoned Fuel Storage Area), 4 (Aircraft Engine Facility), 11 (Engine Test Cell), and 21 (Ship Fitting and Engine Repair). The planned excavation of contaminated soil (metals and solvents) in OU-2B is complete. Two small areas of OU-2B, totaling about an acre, are expected to have land-use controls restricting ground-floor residential use. These areas were not excavated because they are in soil beneath Buildings 360 (IR Site 4) and 398 (IR Site 3). The Navy has completed initial efforts to remediate widespread solvent contamination in OU-2B groundwater, and the BCT is currently planning the additional groundwater treatment that is needed. Land-use restrictions will be applied to the portions of OU-2B over the solvent plume. Ground-floor residential use likely will be prohibited, and buildings will need vapor-intrusion control measures. The most significant area of petroleum contamination at Alameda Point was within OU-2B. Most of the petroleum in OU-2B soil and groundwater has been successfully cleaned up under the Petroleum Program. Active remediation of OU-2B is expected to be complete in 2021.\nOU-2C is the formerly industrialized area along West Tower Avenue and includes IR Sites 5 (Air Rework Facility), 10 (Missile Rework Facility), and 12 (Power Plant). The most significant impact is solvents in groundwater beneath and in the general vicinity of Building 5. The Navy has been successful with initial efforts to remediate the groundwater. Additional groundwater treatment is being designed to achieve groundwater quality that is acceptable for unrestricted commercial and industrial use. A land-use restriction will be imposed to prevent residential use in certain areas and to require maintenance of Building 5's floor slab to limit access to underlying contaminated soil.\nWithin OU-2C, Buildings 5 and 400 formerly housed shops that worked with radioluminescent paint, primarily to refurbish instrument dials, buttons, switches, and other items. Cleanup of the low-level radiological residues on floors, walls, etc. is nearing completion. Wastewater originating in these shops left traces of radioluminescent paint in some storm drain lines and an industrial waste line beneath and outside of these buildings. The Navy has cleaned some of the storm drain lines already. The BCT is in discussions about what the ultimate disposition of storm drain and industrial waste lines will be. Active remediation of OU-2C is expected to be complete in 2021. Again, prior to title transfer, the City will be seeking an exemption from any need for a radioactive materials license with the State due to the minimal risk of any residual contamination left in place and the extensive protections afforded by the federal CERCLA process.\nIn spite of the complexity of the environmental issues at Alameda Point, the City and its own independent experts work closely and carefully with the numerous State and Federal regulators and the Navy on the clean-up of Alameda Point to ensure that, once transferred to the City, new development at Alameda Point neither poses a threat to human health and the environment, nor creates a long-term liability for the City.\nIn 2013, Phase 1 of the Alameda Point conveyance (510 acres) was transferred to the City subject to approval by all State and Federal environmental regulators. The Navy, with approval from those same regulators, plans on transferring the remaining 370 acres to the City in three subsequent phases over the next five to seven years.\nThis report is for information only. There is no financial impact to the General Fund or Base Reuse Department budgets.\nThis report is for information only.\nJennifer Ott, Chief Operating Officer - Alameda Point\nFinancial Impact section reviewed,\nJuelle-Ann Boyer, Interim Finance Director\n1. Map of Installation Restoration Sites at Alameda Point\n2. Map of Navy Conveyance Phases\n3. PowerPoint Presentation on Status Report of Environmental Conditions and Clean-up at Alameda Point""]"	['<urn:uuid:84c86219-89a7-420e-9e9e-17faa167bb64>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	10	90	1859
43	how does light contribute to amphibian nutrition besides water	Light is described as being equally important as water in amphibian nutrition. Specifically, UV-B light is converted to usable vitamin D3, which is a crucial part of their nutritional needs.	['Books on Reptile and Amphibian Care\nby John Courteney-Smith\nThe Arcadia Guide to MBD and its elimination in captivity\nCode: RBK01 / Paperback 118 pages\nMetabolic bone disease (MBD) is a killer of captive reptiles. These three letters strike fear into the heart of every reptile keeper and rightly so. MBD is a painful but avoidable condition that can effect captive reptiles and amphibians. In this full colour guide you will not only learn what MBD is and its causes but you will also find a clear and up to date solution that can be applied in the home.\nThis title covers MBD as a disease, diet, hydration and new technology. Written by the experts from Arcadia reptile the worlds number name in reptile science this is a practical and easy to understand guide to MBD and our ultimate goal of its total eradication in captive animals.\nThe Arcadia Guide to Reptile & Amphibian Nutrition\nCode: RBK02 / Hardback 272 Pages\nReptiles and Amphibians have been kept by man for thousands of years. Over the last 20 years, we have seen a boom of interest into the keeping of these incredible animals as pets and home display species. More and more research into the wild habitats of these animals is becoming easily available to keepers, and with this research comes a responsibility for reptile keepers to provide their captive animals with the essential elements needed to cause a captive animal to thrive and reproduce and not simply to survive.\nOne of the most worrying aspects of captive reptile care has been the food items that we have had to offer them. Do these mass-produced insects contain all the nutrition a reptile needs to thrive? Do defrosted frozen rodents really contain all of the vitamins and minerals that certain species need to survive? And how do these various vitamins and food sources interact with each other?\nIn this highly practical guide, we explain reptile nutrition, hydration, the vitamin groups, wild diets, supplementation, the D3 cycle, gutloading,\nand much, much more\nBio-Activity and the Theory of Wild Re-Creation™\nCode: RBK03 / Hardback 374 Pages\nExplore the wonderful world of Reptile, Amphibian and Invertebrate keeping for the modern age. Reptile keeping expert John Courteney-Smith will guide you through the critical processes that allow for a more effective and altogether more ethical form of exotic pet care.\nBy mimicking the wild environments of the animals that we keep we can be sure that we will be providing everything that they need to live a happy, healthy and reproductively positive life.\nIncluding: nutrition, plants, water, soil, hygiene, UVB, the correct provision of light for plant growth and much, much more.\nThe Elements Series, Part 1. Fire, the Sun & Its Replication in Reptile Keeping\nCode: RBK04 / Hardback 200 Pages\nDid you know that light is as much a part of amphibian nutrition as water?\nUnderstand the importance of heating and how UV-B is converted to usable vitamin D3.\nDiscover how a lamp actually works and how to get the very best out of it.\nSee your own animals thrive in your care.\nThis easy-to-understand guide, the first in a brand new series will explain everything that you need to know to go on to further investigate and understand ‘overall nutrition’. while expanding on the ‘Theory of Wild Re-Creation™’\nTopics covered include: how the sun provides us with energy; how lamps work, UV, infrared, light meters, LEDs, PAR, PUR, CRI, Kelvin, habitats, ‘Light & Shade’, MBD, basking and much more.\nRegistered in England & Wales with company number 5476225\nVAT Number: GB 857 7424 85']	['<urn:uuid:2cd154bd-a134-4fb9-bdaf-e3390cc60634>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	9	30	600
44	what does hokushin ittoryu name mean curious about japanese martial arts history	The name Hokushin Ittoryu has specific meaning in Japanese martial arts. Hokushin refers to the North Star, and was taken from 'Hokushin Musoryu.' The North Star appears immovable in the center of the celestial sky, so the name 'Hokushin Ittoryu' was chosen to imply the physical center where strength gathered, making the swordsman immovable and invincible. The school itself was called 'Genbukan,' named after Genbu, an imaginary beast that protects the northern sky and appears as a snake entangled by a turtle - with the snake symbolizing prosperity and the turtle symbolizing Martial Arts.	['A true purpose and quality of martial art discipline – Budo\nA purpose to train in a martial art discipline… is to strengthen oneself. But what does that really mean? We should not draw a quick conclusion on the meaning of strength. True strength is not a physical attribute to defeat opponents in combat, but rather a quality to overcome difficulties in our life itself.\nThe strength sought by the past Samurai warriors, was the strength to overcome fear of death.\nThis typically appears in “Hagakure” as the concept of Bushido\n- A practical and spiritual guide for a warrior, drawn from a collection of commentaries by the clerk Yamamoto\nTsunetomo from the Kyushu Saga (Nabeshima) clan during the latter 17 th century “Bushido, The Way of the Samurai is found in death.\nIn another words, if you are afraid of death, you cannot claim to be a samurai warrior.\nDeath should not be a result from an act of desperation. Rather it should be met with honor by understanding ones true thoughts and value in life.\nHowever, even so “Death” stays the most ominous fear for any individual(s).\nThus during the old days in Japan, such strength to overcome that fear was achieved thru the intense practice of fencing (Kenjutsu) disciplines.\nBecause “true” strength was achieved by practicing the way of the sword (Katana), the sword also became considered as the “the spirit of a samurai”.\nUnfortunately, few people remain today who understand such wonderful value inherent in the training of martial arts – Budo.\nThe quality of true swordsmanship\nContinue reading “Purpose of Martial Arts”\nHokushin is another description for the North Star.\nThis naming is taken from “Hokushin Musoryu”, a style practiced by the founder of the style, Master Chiba Shusaku Narimasa during his adolesence.\nIttoryu was taken from the “Nakanishi Ittoryu” school which he trained as a youth.\nSince the North Star can be seen immovable in the center of the celestial sky the name “Hokushin Ittoryu” was used to imply the physical center where the strength gathered making the swordsman immovable and invincible.\nThe school (dojo) of Hokushin Ittoryu was located at, Kanda Otamaga Ike in today’s Chiyoda-ward, Tokyo and was named “Genbukan”. Genbu is an imaginary beast that protects the northern sky and is represented in a chimerical form of a snake entangled by a turtle. The snake symbolizes the prosperity, while the turtle is said to symbolize the Martial Arts (Budo).\nContinue reading “Introduction to Hokushin Ittoryu”\nRestoration of Hokushin Ittoryu school of swordsmanship\nDuring the Meiji period it was the Tobu Hall at Mito that preserved the technical transmission of the style.\nThe Hokushin Ittoryu techniques were handed down from Master Chiba Eijiro to Ozawa Toraichi, then to his son Ichiro and finally to the family’s adopted son Toyokichi.\nAt other training halls, the practice method of using the Oni Gote (a large-sized leather gloves used for striking when practicing the kata) ceased to be transmitted.\nWhen Master. Ichiro ‘s real daughter grew up and got married, the Ozawa family’s son – in – law (Takeshi Ozawa) took over the running of the school. This arrangement ended in Master Toyokichi’s abrupt departure from the Tobu Hall where he was eventually invited to become the instructor at the newly opened “Kodo Gikai Society”* in Tokyo .\nAs a result there was no transmission and exchange of techniques between Master Toyokichi and the Mito contingent, there after making the practice of Hokushin Ittoryu at Tobu Hall merely in name only.\nThe actual transmission of the Hokushin Ittoryu sword techniques got inherited at the fencing classes held at Kodo Gikai Society.\n*a politically backed non-profit organization established in 1918 to preserve various Japanese traditions and ideology.\nContinue reading “Restoration of Hokushin Ittoryu”\nThe ultimate technique of all sword schools is essentially the same, and it is named “Kiriotoshi” in the Hokushin Ittoryu school. In physical terms it is when an incoming sword is deflected by an intercepting cut executed simultaneously. Anyone, can be taught the physical movement. However, lacking the psychological and emotional content it will not work.\nThe technique itself cannot be committed successfully when counteracting and deflecting the incoming sword if there is a slightest distraction that crosses one’s mind, resulting in tensing the body. Our mind (or the spirit) can only manage to “free” itself for an instance from the spontaneous thoughts and emotions. It is very difficult to control your mind by reigning in the various psychological activities to stay calm and relaxed. That’s why the “Kiriotoshi” is considered the innermost technique requiring a disciplined “mind” along with\nthe proper physical execution.\nBecause it concerns the mind, when performing the technique, it will manifest the individual’s personality. Your ego will reveal your inner-most fear while your conventional wisdom and past efforts will become irrelevant. At that very moment, you will require to literally “cut down” your anxiety and obsession, discover your true inner-self and render yourself to the “divine existence”.\nAn instructor can only demonstrate to you how it is done, but will not be able to teach you how to perform the actual feat. Every individual(s) will need to strive on their own to attain such ability and skill.\nOn the other hand, even a novice can be immediately certified as a “master” swordsman if the individual can perform the proper “Kiriotoshi” on their first day of joining the school.']	['<urn:uuid:6d41749d-80d1-43b8-8d05-c435134a5686>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	12	94	900
45	I need to know what happens to the placenta during birth, and what problems can occur?	The placenta usually separates from the uterus after birth. In normal cases, this should occur within 30-60 minutes after childbirth. However, problems can occur. Retained placenta happens if the placenta isn't delivered within this timeframe, which can cause severe infection or life-threatening blood loss. The placenta might become trapped behind a partially closed cervix or remain attached to the uterine wall. Another serious condition is placental abruption, where the placenta separates from the uterine wall during pregnancy, which can cause abdominal pain, back pain, uterine bleeding, and put the baby at risk due to decreased oxygen and nutrients.	"[""Hysteroscopy is a diagnostic test that makes use of a thin telescope-like hysterescope to view and operate upon the endometrial cavity. Carbon dioxide is filled into the cavity to aid this process. While often hysteroscopy can be done as an outpatient procedure, some women may need local anesthesia. In some cases, hysterescopy is done along with a resectoscope. But this procedure destroys the uterine lining and is not a viable alternative for women who wish to have children. A laparoscope may be used to view the uterine exteriors.\nDiagnostic hysteroscopy involves observation of the endometrial cavity for any abnormalities. This procedure is often used in cases where there has been abnormal uterine bleeding or repeated miscarriage. Diagnostic hysteroscopy may also be used to confirm the results of HSG. Hysterescopy may be used to check for causes of heavy or irregular menstrual cycle or fit IUD.\nOperative hysteroscopy involves use of hysteroscope to remove polyps, cut adhesions or treat fibroids and septums. This can be used as an alternative to open abdominal surgery. This involves use of operative hysteroscope that allows the physician to insert operating tools. In rare cases, hysterescopy may lead to infection and heavy bleeding or injury to the cervix or uterus.\nOviduct blockage is the blockage in one or both microscopic fallopian tubes that allow a woman's egg to pass from her ovaries to her uterus. The blockage is an impediment for the egg to migrate, implant and begin pregnancy. The result is infertility. In rare cases, even if pregnancy occurs it can be dangerous, if not treated immediately.\nDiagnosing oviduct blockage\nLaparoscopic chromotubation: Usually done after sedating with a mild anesthetic, laparoscopy involves making a very small incision in the belly button and near the pubic bone area; a small camera helps view the tubes. Laparoscopy helps detect endometriosis, adhesions, and ovarian cysts and also check the tubes.\nChromotubation involves infusing diluted methylene blue dye solution into the uterine and tubal lumen. If the tubes are not blocked, the dye should come out of the ends of the tubes into the peritoneal (abdomen) cavity. This test is considered the most reliable way to determine oviduct blockages.\nBoth laparoscopic chromotubation and Hysterosalpingogram can sometimes open a blocked tube; hence these procedures can be both diagnostic and therapeutic. The rate of fertility is likely to improve after the procedure.\nFalloscopy: The newest form of endoscopic examination, falloscopy is helpful to look inside the fallopian tubes. Using a catheter-based system, a flexible tube is inserted through the vagina and cervix which threads through one of the fallopian tube. Very similar to hysterosalpingogram, it allows viewing of tubal walls and checks if it is healthy and also detects obstruction, if any.\nSonohysterography: This is a non-invasive procedure wherein fluid is injected through the cervix into the uterus, and ultrasound imaging is used to determine if any abnormality is present. The procedure is extremely helpful in detecting underlying cause of many problems such as abnormal uterine bleeding, infertility and repeated miscarriage. Like the other diagnostic procedures, sonohysterography is done when the woman is not having her menstrual cycle.\nOviduct blockage treatment\nThe goal of treating oviduct blockages is to unblock fallopian tubes and increase the chances for a successful pregnancy. Surgical procedures are the primary treatment option to open blocked oviducts. If needed, doctors will use more than one procedure to treat oviduct blockage. Take a look at the various surgical and non-surgical options available today to treat this particular condition.\nLaparoscopy: To treat oviduct blockages, laparoscopy is widely preferred. It involves inserting the scope into the abdomen and cutting away scar tissue which blocks the tubes and is a result of infection and/or endometriosis, primary causes for oviduct blockages. It helps in unblocking the oviducts and allows the eggs and sperm to meet and facilitates the egg to become fertilized.\nRecanalization: A tiny wire is inserted into the tube to remove the blockage.\nSalpingectomy: It involves removing a blocked fallopian tube or sealing it in order to maximize the functioning of the second, unblocked tube.\nTubal Reanastomosis: This is also a laparoscopic procedure in which small incisions are made through the abdomen. The blocked portion of the oviducts or fallopian tube is cut away and the healthy sections of the tube are connected. It is followed by a procedure called Salpingostomy to create a new opening in the tube close to the ovary.\nFimbrioplasty: This plastic surgery that facilitates to reshape ends of fallopian tubes closed off by scar tissue or some other blockage.\nTubal Cannulation: Is a non-surgical option which involves clearing blockages with the use of a catheter, or Cannula that is inserted through the uterus into the fallopian tube.\nThe placenta usually separates from the uterus after the birth of the baby. Placental abruption is a condition where the placenta separates from the uterine wall during the pregnancy. Placental abruption is a serious condition and can put the baby at risk. This is a medical emergency. Hypertension can sometimes lead to placental abruption. Women who have blood-clotting disorders may experience placental abruption. Women suffering from diabetes or abusing drugs are also at higher risk for placental abruption. Those women who have had multiple pregnancies are also at higher risk for placental abruption.\nA pregnant woman suffering from placental abruption is likely to have abdominal and back pain. There might be rapid uterine contractions. There is tenderness in the abdomen. Uterine bleeding is often noticed. Placental abruption can occur anytime after the 20th week of pregnancy.\nAn ultrasound can help in locating any possible blood clot behind the placenta. It can also help in checking for any signs of fetal distress. Fetal monitoring is essential since the placenta supplies nutrients to the growing fetus. There may be decreased fetal movements. The extent of placental abruption decides the course of treatment to be followed. Partial placental abruption needs adequate bed rest and close monitoring. In cases of total placental abruption, delivery of the infant is undertaken. This is either with vaginal delivery or cesarian section. But there is a risk of premature birth and fetal death. The newborn baby could suffer brain damage due to low levels of oxygen in the blood.Tags: #Hysteroscopy #Oviduct Blockage #Placental Abruption\nEnter your health or medical queries in our Artificial Intelligence powered Application here. Our Natural Language Navigational engine knows that words form only the outer superficial layer. The real meaning of the words are deduced from the collection of words, their proximity to each other and the context.\nDiseases, Symptoms, Tests and Treatment arranged in alphabetical order:\nBibliography / Reference\nCollection of Pages - Last revised Date: September 24, 2022"", ""Placenta: How it works, what's normal\nThe placenta plays a crucial role during pregnancy. Find out what the placenta does, issues that might affect the placenta and how the placenta is delivered.\nBy Mayo Clinic Staff\nIf you're pregnant, you might wonder what the placenta does and what factors can affect the placenta. Get the facts about this important fetal organ.\nWhat does the placenta do?\nThe placenta is an organ that develops in your uterus during pregnancy. This structure provides oxygen and nutrients to your growing baby and removes waste products from your baby's blood. The placenta attaches to the wall of your uterus, and your baby's umbilical cord arises from it. In most pregnancies, the placenta attaches at the top or side of the uterus.\nWhat affects placental health?\nVarious factors can affect the health of the placenta during pregnancy, some modifiable and some not. For example:\n- Maternal age. Certain placental problems are more common in older women, especially after age 40.\n- Premature rupture of the membranes. During pregnancy, your baby is surrounded and cushioned by a fluid-filled membrane called the amniotic sac. If the sac leaks or breaks before labor begins, the risk of certain placental problems increases.\n- High blood pressure. High blood pressure can affect your placenta.\n- Twin or other multiple pregnancy. If you're pregnant with more than one baby, you might be at increased risk of certain placental problems.\n- Blood-clotting disorders. Any condition that either impairs your blood's ability to clot or increases its likelihood of clotting increases the risk of certain placental problems.\n- Previous uterine surgery. If you've had a previous surgery on your uterus, such as a C-section or surgery to remove fibroids, you're at increased risk of certain placental problems.\n- Previous placental problems. If you've had a placental problem during a previous pregnancy, you might be at increased risk of experiencing it again.\n- Substance abuse. Certain placental problems are more common in women who smoke or use illegal drugs, such as cocaine, during pregnancy.\n- Abdominal trauma. Trauma to your abdomen — such as from a fall or other type of blow to the abdomen — increases the risk of certain placental problems.\nWhat are the most common placental problems?\nDuring pregnancy, the most common placental problems include placental abruption, placenta previa and placenta accreta. These conditions can cause potentially heavy vaginal bleeding. After delivery, retained placenta is also sometimes a concern.\nMarch 13, 2015\n- Placental abruption (abruptio placentae). If the placenta peels away from the inner wall of the uterus before delivery — either partially or completely — it's known as placental abruption. Placental abruption can cause varying degrees of vaginal bleeding and pain or cramping. It might also deprive the baby of oxygen and nutrients. In some cases, early delivery is needed.\n- Placenta previa. This condition occurs when the placenta partially or totally covers the cervix — the outlet for the uterus. Placenta previa is more common early in pregnancy and might resolve as the uterus grows. Placenta previa can cause severe vaginal bleeding before or during delivery. A C-section delivery usually is required if the placenta previa is present at the time of delivery.\n- Placenta accreta. This condition occurs when the blood vessels of the placenta grow too deeply into the uterine wall. Placenta accreta can cause vaginal bleeding during the third trimester of pregnancy and severe blood loss after delivery. Treatment might require a C-section delivery followed by surgical removal of the uterus (abdominal hysterectomy). More-aggressive forms of this problem can also occur if the placenta invades the muscles of the uterus (placenta increta) or if the placenta grows through the uterine wall (placenta percreta).\n- Retained placenta. If the placenta isn't delivered within 30 to 60 minutes after childbirth, it's known as retained placenta. Retained placenta might occur because the placenta becomes trapped behind a partially closed cervix or because the placenta is still attached to the uterine wall — either loosely (adherent placenta) or deeply (placenta accreta). Left untreated, a retained placenta can cause severe infection or life-threatening blood loss in the mother.\nSee more In-depth\n- Roberts V, et al. Placental development and physiology. http://www.uptodate.com/home.\n- Lockwood CJ, et al. Clinical features, diagnosis and course of placenta previa. http://www.uptodate.com/home. Accessed Jan. 3, 2015.\n- Moore KL, et al. The Developing Human. 9th ed. Philadelphia, Pa: Saunders Elsevier; 2013. http://www.clinicalkey.com. Accessed Jan. 27, 2015.\n- Ananth CV, et al. Placental abruption: Clinical features and diagnosis. http://www.uptodate.com/home. Accessed Jan. 3, 2015.\n- Lockwood CJ, et al. Management of placenta previa. http://www.uptodate.com/home. Accessed Jan. 27, 2015.\n- Weeks A. Retained placenta after vaginal birth. http://www.uptodate.com/home. Accessed Jan. 3, 2015.\n- Gabbe SG, et al. Obstetrics: Normal and Problem Pregnancies. 6th ed. Philadelphia, Pa.: Saunders Elsevier; 2012. http://www.clinicalkey.com. Accessed Jan. 27, 2015.\n- Cremers GE, et al. Attitudes toward placentophagy: A brief report. Health Care for Women International. 2015;35:113.\n- Resnik R. Clinical features and diagnosis of placenta accreta, increta and percreta. http://www.uptodate.com/home. Accessed Jan. 3, 2015.\n- Creasy RK, et al. Creasy and Resnik's Maternal-Fetal Medicine: Principles and Practice. 7th ed. Philadelphia, Pa: Saunders Elsevier; 2014. http://www.clinicalkey.com. Accessed Jan. 27, 2015.\n- Laughlin-Tommaso SK (expert opinion). Mayo Clinic, Rochester, Minn. Jan. 15, 2015.""]"	['<urn:uuid:10969c6b-fd4a-4128-b17d-b071e7651181>', '<urn:uuid:ec84c217-8317-4d87-b895-c6b1f774f108>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:57:18.859842	16	98	1984
46	I'm currently managing both web and game development projects - what's the difference between planning web development versus game development in terms of the initial documentation and design approach?	Web development planning typically starts with coding and database requirements, while maintaining consistent styling through tools like CSS. In contrast, game development begins with high-level conceptualization and specific design documentation that covers interface design, play mechanics, platform modifications and performance testing. Web development focuses on technical specifications, while game development requires additional documentation of elements like game flow, choice mechanics and entertainment value.	['Web development has been the buzzword of Information Technology industry for over two decades now, ever since the ‘dot com’ boom created an entirely new industry called ecommerce. As the internet and computers gained popularity, organizations around the world realized the need for an online presence. All of a sudden, there was a way of sharing information with the entire world at an affordable cost. It also created the demand for a new set of skills – collectively known as web development. In this article we look into what front end web development really is about, what are the skills required to become a successful developer and the resources available to learn web development. Given that web development technologies are continuously evolving; we wrap up the article by mentioning the additional skills that one needs to catapult his career as a front end developer.\nDownloadable solution code | Explanatory videos | Tech SupportStart Project\nWeb development is a broad term used for a set of skills required to build a website hosted over the internet or intranet. It includes designing web pages, programming, database management, client side/server side scripting, security configurations, ecommerce development, writing markup and coding.\nIn the developer ecosystem, development is synonymous to the non – design work involved in websites such as coding. Having said that, while coding forms the foundation of web development, a successful front end web developer needs strong knowledge of associated skills such as user interface/ user experience (UI/UX) design, database administration (DBA), client side and server side scripting and search engine optimization (SEO).\nWebsite development has moved from the traditional static websites to dynamic ones that update in real time. Being an expert programmer is no longer sufficient, though it is still necessary. Modern organizations demand custom web development solutions. When a development project kicks off, it is common for clients to not know what they really want as the end product. This is the time where front end developers need to think beyond programming and visualize possible solutions that can solve the problem at hand.\nIn the following list, we mention background skills as well as tips that a successful front end web developer must have in his web development\nCascading Style Sheets is a widely used styling language. Developers often need to maintain a consistent look and feel of websites and web applications. Instead of writing the same code for each and every HTML page, it is convenient to write a master style sheet and then keep referring to it from different pages to achieve desired design. Style sheets isolate the document’s content from its presentation, which comes handy for complex web pages. They allow developers to have more control over layout, fonts, colors and accessibility. Also, it saves bandwidth and your site loads more easily because fewer lines of codes are transferred to the browser every time your websites receives a hit.\nIt is a high level programming language that is one of the three building blocks (HTML and CSS being the other two) for developing web content. It supports a number of programming styles such as object – oriented, imperative and functional programming. It is useful for both client side and server side scripting of web applications. It is also useful for environments that aren’t web based, such as pdf documents, site specific browsers, etc. It is an excellent companion for browsers, as it allows just-in-time compilation.\nEver since Microsoft released its first server-side scripting engine ASP (Active Server Pages) for dynamically generated web pages - web development has never been the same. Modern websites have come a long way from the static websites of the 1990s. They have become dynamic, interactive and responsive and ASP has become a popular tool for their development. ASP.NET is the improved version of classic ASP, with a bigger library support and faster processing.\nOriginally, PHP stood for Personal Home Page but it now stands for PHP: Hypertext Preprocessor. It is a server side scripting language suited for web development, though it is also used for general programming. It provides clear design, integration with other platforms and languages and well organized modules which gives developers better control. The biggest advantage of PHP is that it is open source, which means it is available for free. It is similar to the popular programming languages such as C and Java. It has a big developer community across the world and works on all major operating systems such as Linux, UNIX and Windows. It is easy to embed PHP into an existing HTML code, which is a boon when you are looking to convert a static website into a dynamic one.\nDatabase is an integral part of website and application development. Although Database Administration (DBA) has emerged as a specialized position, developers need to know how data is stored and how to retrieve it. Structured Query Language (SQL) is one of the most popular database languages familiar to developers around the world. It can efficiently retrieve data from huge database. SQL databases adhere to a well-established standard (adopted by ANSI and ISO) and it doesn’t require extensive coding skills.\nMost organizations hire a team of developers spread across geographies to get their development work done. Team members constantly provide updated versions of code and it is important to be familiar with a collaborative, version control tool which helps you keep track of changes. Github is one such open source tool that allows version control functionality for free. It is widely used and has a large user community to resort to for help.\nOther than the technical skills required to be a successful web developer, there are host of other skills that one should have to stand out in the crowd. Here are few non-technical skills that employers and clients would always appreciate and help you be different from others:\nBefore you get down to coding, you need to draw up a plan and estimation plays a vital role in it. You won’t get them right from day one but the important point is to keep learning from each project and improve. You might be required to collaborate with a number of other developers across different teams. The key is to be inclusive and not think only about your pace of work. You don’t want to be in a situation where you develop into an expert coder but your estimations are so wild that clients start avoiding you. There is a huge crowd of developers out there who bring a degree of sameness to the table. Once you brush up on your estimations, you will bring something different that will set you apart from the rest.\nMost Watched Projects\nWhen you start out as a developer, keep in mind that you are looking for a platform to develop yourself. It is better to focus on doing quality work rather than worrying about monetary aspect. There may be that department store in your locality which is looking to go online but doesn’t have the funds to hire top notch developers. When you work with them, you get valuable experience and build a portfolio for yourself which will stand you in good stead in future.\nThere are plenty of online resources that allow you free hosting. This is a perfect stage to present your knowledge to the rest of the world. It serves as an online portfolio which is easily accessible to your future interviewers and employers. Unlike in projects, where your creativity is restricted by client requirements, a blog allows you to use your imagination to the fullest.\nThe good thing about learning the tricks of the trade in the internet age is that there are plenty of open source platforms and languages available to learn. Effort is the only thing you need to spend. There are some excellent tutorials on YouTube and Web Development Tutorials that would teach you all you need to know about programming, scripting and databases.\nWish you a happy journey on the road to web development!\nWhat Users are saying..\nThis is one of the best of investments you can make with regards to career progression and growth in technological knowledge. I was pointed in this direction by a mentor in the IT world who I highly... Read More\nYou might also like\nTop 15 Latest Recipes', 'Information Technology 108\nITEC 108: Introduction to Game Development.\nCredit Hours: (3) Two hours lecture; two hours laboratory\nAn introduction to problem solving and programming in the context of game development. Topics follow the framework laid out by the International Game Developers Association (IDGA) and will touch on each of the core topics: Games and Society, Game Design, Game Programming, Visual Design, Audio Design, Interactive Storytelling, Game Production, and Business of Gaming.\nDetailed Description of Content of Course\n1. Critical Game Studies - Criticism, Analysis & History of electronic and non-electronic games.\n2. Games and Society - Understanding how games reflect and construct individuals and groups, as well as how games reflect and are constructed by individuals and groups.\n3. Game Design - Principles and methodologies behind the rules and play of games.\n4. Game Programming - Aspects of traditional computer science and software engineering – modified to address the technical aspects of gaming.\n5. Visual Design - Designing, creating and analyzing the visual components of games.\n6. Audio Design - Designing and creating sound and sound environments.\n7. Interactive Storytelling - Traditional storytelling and the challenges of interactive narrative.\n8. Game Production - Practical challenges of managing the development of games.\n9. Business of Gaming - Economic, legal and policy aspects of games.\nDetailed Description of Conduct of Course\nConcepts of game development are covered in the lecture portion of the course. These concepts are reinforced by the programs and critiques that the students write. A significant portion of the lab time is spent in the microcomputer lab. This provides the opportunity for close contact between the student and the instructor while the student is designing and implementing games and writing critiques. The lab time may also be used to cover specific design details.\nGoals and Objectives of the Course\nStudents who complete the course will be able to:\n1. Articulate a brief history of video games, covering the evolution of the major genres, platforms, publishers, and developers of the last several decades.\n2. Describe the sociology of games, including audience demographics, societal reactions to sex and violence, and the emergence of online communities.\n3. Explain ludology – the academic study of games – including the issues and individuals central to this rapidly growing field.\n4. Explain the theories, processes, and design considerations that form the foundation of game creation.\n5. Describe the nature of fun, including ideas such as game structure, game flow, and the role of choice in generating an entertaining interactive experience.\n6. Discuss the specifics of actually designing a game, from high-level conceptualization and design documentation, to specific topics such as interface design, play mechanics, platform modifications, and performance testing.\n7. Explain programming teams and processes, and common game programming languages.\n8. Explain fundamental concepts in computer programming including variables, looping, and branching.\n9. Describe how to debug games and explore their use of system resources.\n10. Articulate the basics of math and physics used in game development – geometry, applied trigonometry, vectors and matrices, transformations, general physical concepts, real-time game physics, rigid body simulations, and particle systems.\n11. Explain general visual design principles, such as graphic design, color theory, and user interfaces.\n12. Describe the “real-world” aspects of the game industry, including the domains of producers, attorneys, and game business professionals.\nCourse will include both individual and group development assignments as well as individual game critique assignments. Multiple exams will also be used to assess student progress.\nOther Course Information\nReview and Approval\nFebruary 2010 Initial Course Approval Art Carter, Chair\nRevised: June 1, 2012']	['<urn:uuid:e2acfffb-f9ce-4be2-8ab2-77bbc77453e8>', '<urn:uuid:56f5f24b-4be7-4ec9-812f-b5f25458a148>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	29	63	1976
47	bhutan tourism visa cost requirements restrictions	To visit Bhutan, tourists must arrange their trip through a licensed Bhutanese tour agency. The daily fee is $200 which covers food, lodging, and vehicle costs. Visitors must be accompanied by a Bhutanese guide and driver at all times. Independent travel is not allowed - all visas must be obtained through approved tour operators. The country maintains strict controls on who can enter, how long they stay, and where they can go to preserve its culture and environment.	"[""IN THE LAND OF THE THUNDER DRAGON\nA Journey to Bhutan\nby Bruce Meade\nThe small passenger jet dropped out of the clouds, darted through a heart stoppingly narrow pass, then settled onto the single landing strip of the Paro Airport. I stepped out into the sweet smelling spring air. I was in Bhutan, the last Buddhist Kingdom.\nThe main reason for my journey was to experience the culture of this isolated Himalayan realm, but I also had a paper trail to follow.For by coincidence no sooner had I made my travel arrangements than we began to carry handmade paper made in Bhutan here at Hiromi Paper International.\nBhutan is a small Kingdom about the size of Switzerland, with Tibet as its neighbor to the north and India to the south. The current king is a very enlightened ruler, more concerned with his countries Gross National Happiness than its Gross National Product. By his decree the protection of Bhutan's pristine natural environment is the stated policy of the government.\nTo enter Bhutan, whether in a group or by yourself you need to be with a Bhutanese guide and a driver. This, as well as your visa, must be arranged through a licensed Bhutanese tour agency and you must pay $200.00 per day while in the country, which includes food, lodging, and vehicle.\nFor me the opportunity to visit an isolated Buddhist country high in the Himalayas far outweighed any of the aforementioned limitations. As far as the $200.00 per day I figured, hey, that's why credit cards were invented.\nAnd that how I came to be standing in a field of blooming Daphne at a 10,000 foot high pass in Bhutan! To touch this shrub whose bark would eventually become a sheet of paper selling in the store where I worked halfway across the world was an incredible feeling of being part of a full circle.\nI was en route to the Paro festival, an important rite of spring in Bhutan. The festival is part county fair, part religious ceremony. Vendors from all over the Himalayas set up to sell their wares. Colorful masks, beautiful jewelry, antique prayer wheels, and exotic foods were all on display.\nOn its final day the Festival is capped off by the unfurling at dawn of a four story high banner depicting Guru Rimpoche, who brought Buddism to Bhutan in 650 A.D.\nSo blessed is the huge banner (called a Thondrol) that it is believed one achieves liberation simply by viewing it. So there I stood in the cool light of dawn, staring up at the massive image of Rimpoche seated upon his lotus throne. Bhutanese people had come from all over, many walking for days to see the Thondrol. It remains up for only a few hours, on this one day a year, as the monks do not let the 300 year old work of devotional art be touched by direct rays of the sun, least it fade.\nI then made my way to the booth of Mangala Paper, who make the paper we import, and introduced myself to Kezang Udon. Her Eco Friendly hand made paper was doing a brisk business, with many western tourists checking out the beautiful sheets, as well as handsome photo albums and journals crafted from Bhutan paper. I told Kezang how proud I was to be part of the introduction of Bhutan paper to the United States and she graced me with the warm smile that is so typical of the Bhutanese people.\nI promised to stop by her factory on my way back from my journey to the remote central valley, and began an arduous day and a half drive on the only paved road in the country.\nI traveled over passes towering above the clouds, through dense forests punctuated by flaming red rhodendrums. I visited the cave where Guru Rimpoche first meditated when he arrived in Bhutan from Tibet. A temple has been built around the cave and a huge cypress tree stands beside it, supposedly having sprung up from Rimpoche's walking stick. I touched the tree with deep respect for the man who is believed to be the second incarnation of Buddha.\nOn the return journey it began to snow. We were heading for a monastery at the top of a pass, and arrived to the sound of deep guttural chanting of monks, accompanied by the plaintive cry of conch shell horns and thudding bass of drums. A healing ceremony for the Lama (Teacher) was in progress and we were allowed to observe.\nThe interior of the monastery was a treasure trove of wall paintings depicting various spiritual heroes battling demons. Bhutan is a Tantric Buddhist nation, thus visualizations of inner states of mind are constantly on view, serving as signposts on the path towards enlightenment.\nLeaving the monastery I noticed the snow had stopped, and the sky was now a dazzling blue. I stood at the top of the pass gazing out at the Himalayan range in all its monumental glory. Massive snow crowned peaks stood as silent sentinels between the border of Bhutan and Tibet. Bathed in the clear light the mountains seemed to be the liar of Bhutans legendary Thunder Dragons. It was a sight that will linger forever in my memory.\nContinuing westward I entered Thimphu, the only world capitol without a traffic light. I made my way to the paper making facility, which sits above the river at the southern end of the city. Colorful prayer flags fluttered by the entrance, sending hopes for peace out to the world on the breeze.\nOver tea Kezang shared with me the story of how she came to be involved in the making of handmade paper. It was not a family business by any means. She learned about the long tradition of paper making in Bhutan, and was most impressed with the rich character of the paper. Realizing the art was slowly dying she decided to take it up despite the objections raised by her family.\nSince she didn't know very much about papermaking she went to Japan to study their techniques. After returning to Bhutan she set up the Mangala House of Handmade Paper, knowing full well she was bucking all the modern trends. The labor intensive, time consuming handmade methods were being fazed out in search of higher profit margins, but she did not want to see the handmade paper of Bhutan disappear. With lots of imagination and hard work Kezang Udon is helping keep an ancient tradition alive. As I told her, we at Hiromi Paper are happy to be part of this effort, introducing these beautiful sheets to artists all over the world.\nAll too soon it was time for me to climb onto a Druk Airlines jet to fly up and over the Himalayas towards home. I knew how fortunate I was to have been able to visit this magical realm of beauty and insight. And someday I hope to return. Until then I can content myself showing our customers gorgeous sheets of paper that come from the Land of the Thunder Dragon."", 'Bhutan Travel Guide: Overview\nIt’s easy to see why Bhutan is known as the “Last Shangri-La."" Its culture—deeply influenced by a unique form of Buddhism and rich with traditions of spirituality, art, music, dance and food—has not only been preserved but treasured by its citizens. This is, in fact, a foundational tenet of Bhutan. Until only a couple of decades ago, the country chose to isolate itself from the rest of the world and refused the influences of other societies and customs. Televisions, smartphones and the Internet exist in Bhutan, but their ubiquity and impact on the local culture is much less than that seen with its neighbors. Further differentiating itself from other nations, Bhutan measures its economic and national success with a rather unique self-created methodology: Gross National Happiness (GNH).\nBhutan cares deeply about preserving its culture, traditions and environment. Located deep in the Himalayas, its natural beauty is abundant and timeless. Its charm and appeal also lie in its refusal to let tourism run wild. There’s no overdevelopment, no mass land sales and no exploitation of its people solely to make Bhutan a popular destination with travelers. Bhutan has strict controls regarding who can enter the country, how long they stay and where they go. For these reasons, it\'s not a budget travel destination, and it takes some planning to visit here, but as anyone who takes on the challenge can attest, once you arrive, you immediately realize, appreciate and celebrate just how truly special it is.\nArchaeological evidence suggests this area was inhabited as far back as 2000 BC. Tibetan king Songtsen Gampo introduced Buddhism to Bhutan in the seventh century AD, extending the Tibetan Empire into the region and ordering the construction of two Buddhist temples in the Paro area.\nUntil the early 17th century, Bhutan was essentially a patchwork of small, warring fiefdoms, and then the Tibetan lama and military leader Ngawang Namgyal, who had fled religious persecution in Tibet, unified the area. To defend the country against periodic Tibetan advances, Namgyal built a network of impregnable dzongs (fortresses) and promulgated the Tsa Yig, a legal code that helped to bring local lords under centralized control. Many dzongs still exist and are active centers of district administration and religion.\nThrough the British East India Company, the British entered Bhutan in the 18th century as part of its colonial expansion into India, China and the region as a whole. The British remained involved in the country\'s affairs for the ensuing two hundred-plus years. After centuries of internal struggles, civil wars and conflicts with neighboring peoples, during which the British backed certain leaders and factions, Bhutan chose a king in 1907. The nation settled into an agreement that gave the British control of Bhutan\'s foreign affairs.\nIn 1953, the king established the 130-member National Assembly in order to promote a more democratic form of governance, and the country eventually changed from an absolute to a constitutional monarchy.\nIt was only in 1999 that the country lifted its ban on television and the Internet and began to open up more to the outside world. In 2005, Bhutan began to implement a new constitution, and in 2007 and 2008, it held its first national parliamentary elections. After opening its country to commercial tourism in 1974, Bhutan welcomed 287 tourists that year. Every year, as the draw of this vibrant, unspoiled country increased, more and more people made the journey. Today, over a quarter-million people visit Bhutan annually.\nDue to its relative isolation from the rest of the world and strong adherence to its traditions and religion, Bhutan has maintained its customs over the last several centuries. Experiencing the nation’s unique culture is one of the strongest draws for Western visitors.\nBhutan is officially the only Buddhist kingdom still in existence, and the large majority of the country adheres to the state religion of Vajrayana Buddhism. (By and large, Hindus make up the remaining people.) The religious leaders have always had a strong say in Bhutan’s political affairs and have been political leaders in the past as well. The lifestyle of the people, therefore, largely conforms to these religious beliefs. The country controls the influence of foreign culture, which is also why the local traditions have remained so strong. However, this is slowly changing with the continued presence of tourism, as well as the curiosity of much of Bhutan\'s younger population. The juxtaposition of young Bhutanese tapping their smartphones while traditionally dressed Buddhist monks walk by is becoming a more common sight in Thimpu, the capital city.\nNo rigid clan systems exist in the nation, and men and women share equal rights. Most of the country relies on subsistence farming.\nThe Bhutanese inherit some truly unique customs that have their origin in the mythological legends and folktales of the country. For example, visitors will likely see giant phalluses painted outside the homes of otherwise reserved Bhutanese people. Theses images represent the Divine Madman, a figure believed to ward off evil spirits. Don’t be alarmed when you see this on a person\'s home.\nIt’s compulsory for all Bhutanese citizens to adhere to a national dress code when in public. Men wear robes called gho, while women wear brightly colored kira with ornate brooches and wraparound skirts.\nVisitors should take advantage of the opportunities to witness cultural experiences, such as dance, music, archery (a very popular sport) and religious ceremonies.\nGood to Know\nWhen traveling in Bhutan, keep some of the following in mind:\nThe official currency is the ngultrum (BTN), which is pegged to the Indian rupee (INR). US dollars are widely accepted. Bhutanese currency is only needed for personal expenses and small souvenir items. For larger expenses, travelers can use US Dollars and other foreign currency. Most ATMs in Bhutan (mostly concentrated in Thimphu and Paro) are compatible with international credit cards, such as Visa, Mastercard and Maestro. However, as the service is not overly reliable, it’s good practice to have a supply of ngultrum or US dollars on hand.\nTap water is unsafe in Bhutan. Stick to bottled water or the purified water provided by your hotel.\nTipping is not obligatory. Hotels and restaurant bills include service charges of 20 percent. There’s no need to add anything further to these bills, but do feel free to tip your guides, drivers and other crew members accompanying you throughout your trip.\nBhutan is a modest country that adheres to its Buddhist principles. Dress conservatively, and be respectful of the local culture and customs. Never touch someone on the head, and don\'t use your feet to point at anything. A formal greeting is to bow with outstretched, open hands (palms up). Always show the utmost respect in temples and holy places.\nThe standard voltage is 230 volt, and the frequency is 50 hertz. If you\'re coming from North America, you’ll need a voltage converter and plug adapter to fit your devices.\nMost hotels have Western toilets and provide toilet paper, though some exceptions exist, particularly in local hotels in eastern Bhutan. There are very few public toilets, so take full advantage of hotel and restaurant facilities before any long drive. Most public toilets are of the Asian squat variety, and toilet paper isn\'t provided, but a container of water should be present.\nWhen To Visit Bhutan\nOctober to December is the ideal time to visit Bhutan as the air is clear, and the skies are sunny. January and February are colder, but in March and April, the climate remains dry and pleasant. In late spring, the famous rhododendrons bloom spectacularly and flood the valleys with color. Heat and humidity increase in May, and from June to September, monsoon rains cover the mountains.\n- Thimpu is one of just two Asian capital cities that doesn’t have a single traffic light. (The other is Pyongyang, North Korea.)\n- “Bhutan” translates roughly to “Land of the Thunder Dragon.” It earned this nickname due to the fierce storms that often roll in from the Himalayas.\n- In Bhutan, killing a black-necked crane, which is highly endangered and culturally sacred, could carry a lifetime sentence in prison.\nFull article coming soon.\nRegions and Cities\nJakar: This administrative town in the north is the birthplace of Buddhism in Bhutan.\nPunakha: A former winter capital of Bhutan, the city still hosts the monastic body in winter.\nThimpu: The capital and largest, most developed city, Thimpu contains many monasteries, museums and landmarks to visit.\nWorth a Visit\nJigme Dorji National Park\nBhutan\'s largest national park, this stunning place provides sanctuary for many wild animals. Keep a keen eye out for any number of species, including snow leopard, Himalayan black bear, red panda, musk deer, sambar deer and barking deer.\nRoyal Manas National Park\nThis park and the adjoining Manas National Park in Assam provide an opportunity to spot many kinds of wild and exotic animals, including tigers, leopards, rhinos, bears, pangolins and elephants.\nNo Bhutan travel guide would be complete without mentioning the hike to this monastery perched atop the side of a cliff. It\'s said Guru Rinpoche (an eighth-century Tibetan Buddhist master) was carried from Tibet to this spot on the back of the tigress, hence the name. First constructed in 1692 near the cave where Guru Rinpoche first meditated, this monastery (still functioning amid daily visits from tourists) consists of four temples with residential accommodations for the monks.\nThings to See and to Do\nFull Article Coming Soon!\nHow to Get to Bhutan\nBhutan has one international airport (Paro) and two airlines (government-owned Druk Air and the privately run Bhutan Airlines). Although schedules change by season, usually several flights a week arrive from Kathmandu, New Delhi, Singapore and Bangkok. These are either direct or have a stopover. Only a few aircraft can operate on a runway as short and high as Paro\'s, and because all landings and takeoffs in Paro are by visual flight rules—the pilot must be able to see the runway and surrounding hills before landing—no flights can be operated at night or in poor visibility. When Paro’s valley is clouded in, flights are potentially delayed for days. When this happens, your tour program must be changed and everything rebooked.\nSince independent travel to Bhutan is not allowed (you must apply and go through an approved tour operator), your tour operator will apply for and receive your visa for you when you book your trip, and you’ll normally have already paid the visa fee directly to that operator. When you arrive at one of the ports of entry for tourists, your visa endorsement is stamped in your passport at that time. You’ll receive a visa for the exact period you have arranged to be in Bhutan. If extenuating circumstances require you to obtain a visa extension, your tour operator will arrange it.\nBhutan doesn’t have a railway system, and because there’s only one main road through the country, the only way to see Bhutan is by foot, car or the very limited domestic air service, which is restricted to Paro, Bumthang and Gelephu, as of 2018. Travelers should discuss their desired routes and itineraries with their tour operators since tourists aren’t able to book their own flights or travel unaccompanied through the country.\nThe national highway, a stretch of tarmac that winds its way up and down mountains, across clattering bridges, along the side of cliffs and over high mountain passes, is in the process of being widened to double lanes. This is a massive undertaking because of the rivers, mudflows and landslides that currently present hazards—especially when it rains. Roads can easily become blocked due to snow or landslides and can take anywhere from an hour to several days to clear, and these conditions won\'t change with a wider road. Public buses are crowded, uncomfortable and generally not recommended for travelers. Point begin, however you travel, bring plenty of reading material for your journeys!\nTaxis frequent Thimpu, Paro and Jakar, and drivers usually don\'t use their meters. If you’re traveling on a tourist visa, transport costs are likely included in the price of your trip, and you\'ll have a vehicle available for both short- and long-distance travel.\nBhutan is a multilingual society with 19 different languages and dialects spoken throughout. That said, Dzongkha is the national language. Widely spoken in the western region, this became the state language in 1971. Besides Dzongkha, you’ll likely encounter three other dominant languages: Tshanglakha (or Sharchokpa), spoken in eastern Bhutan; Lhotshamkha (or Nepali), spoken in the southern region and Bumthangkha, spoken in central Bhutan. As the medium of instruction in schools, English is widely spoken. Hindi is also common and understood by most Bhutanese because of the influence of Bollywood (Indian cinema).\nIf you want to practice the local language, here are some key phrases in Dzongkha:\n- Hello: Kuzu zangpo la\n- How are you?: Gaday bay zhui?\n- Yes: Inn\n- No: Men\n- Nice to meet you: Ga choe da je di sem ga yi\n- Excuse me: Atsi zur nang\n- Thank you: Kadrin chhe la\nFood and Drink\nAlthough Indian, Tibetan and Nepali influence can be felt in the local cuisine, Bhutanese food is quite unique and strongly reflects the local ingredients. Datshi (cheese) is ubiquitous throughout Bhutan and is combined with potatoes, meat and other vegetables in a number of dishes. Ema datshi (chilies and cheese) is the most famous dish in Bhutan, and it’s present in nearly every meal. The chilies, which can either be fresh green chilies or dry red chilies, are sliced lengthwise and cooked with Bhutanese cheese and plenty of butter. Chilies are used in lots of Bhutanese dishes, and they can pack a punch! Meat is eaten more commonly in Bhutan than in other neighboring Buddhist regions, which is largely due to the mountainous geography and relative lack of many fresh ingredients beyond chilies, potatoes and a few other vegetables. As a result, it\'s common to see beef, yak and pork meat used in Bhutan.\nFood in Bhutan: What to Know and to Eat—Full Article Coming Soon!\nBhutan is generally one of the safest places in the world for tourists. Tourism is a huge part of the Bhutanese economy, and daily tariffs and strong government control ensure there’s little to no crime committed against visitors. Thimphu police are quite active and do rounds around the city at night to ensure the safety of those enjoying Bhutan travel.\nBhutan has a universal health care system, and hospitals and clinics are located throughout the country, even in the remotest areas. Travelers, however, should not expect high-tech facilities.\nIn terms of the natural environment, bears are a threat in remote mountainous regions.\nThe US State Department advises LGBTQ travelers that homosexuality is illegal in Bhutan, and the law is enforced. Punishment can include prison sentences of one month to one year. Tobacco users should know that the sale of tobacco (cigarettes, chewing tobacco or other tobacco products) is effectively banned throughout Bhutan. (Bhutan remains the only country in the world to enforce this policy.) Penalties for possession or use of tobacco might be severe. That said, due to a parliamentary amendment in 2012, the country now allows visitors to import a limited quantity of cigarettes and tobacco products into the country for personal use only. To do so without facing fines, one must have receipts proving the payment of import duties.\nWHAT IS BACKROADS\nEstablished in 1979, Backroads is a pioneer in active, immersive and off-the-beaten-path travel. Now operating adventure tours in over 50 countries, our passion for discovery and our desire to experience the world in original ways continue to inspire our pursuit of new adventures. We hope this guide will be enlightening to you as you plan your next great Bhutan adventure!']"	['<urn:uuid:d0478ab1-06df-4a29-ab44-293c94d109fc>', '<urn:uuid:5b97c28f-2c23-4d9a-b4a3-e020742eb8bf>']	open-ended	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:57:18.859842	6	78	3808
48	brazil study location serra do mar state park	The study was conducted at the Serra do Mar State Park, Nucleo Picinguaba, on the northern coast of the state of São Paulo, Brazil, from April to July 2014.	['Warming and top predator loss drive direct and indirect effects on multiple trophic groups within and across ecosystems\nAntiqueira, Pablo et al. (2021), Warming and top predator loss drive direct and indirect effects on multiple trophic groups within and across ecosystems, Dryad, Dataset, https://doi.org/10.5061/dryad.ns1rn8pv8\n1. The interspecific interactions within and between adjacent ecosystems strongly depend on the changes in their abiotic and biotic components. However, little is known about how climate change and biodiversity loss in a specific ecosystem can impact the multiple trophic interactions of different biological groups within and across ecosystems.\n2. We used natural micro-ecosystems (tank-bromeliads) as a model system to investigate the main and interactive effects of aquatic warming and aquatic top predator loss (i.e., trophic downgrading) on trophic relationships in three integrated food web compartments: i) aquatic microorganisms, ii) aquatic macroorganisms, and iii) terrestrial predators (i.e., via cross ecosystem effects).\n3. The aquatic top predator loss substantially impacted the three food web compartments. In the aquatic macrofauna compartment, trophic downgrading increased the filter-feeder richness and abundance directly and indirectly via an increase of detritivore richness, likely through a facilitative interaction. For the microbiota compartment, aquatic top predator loss had a negative effect on algae richness, probably via decreasing the input of nutrients from predator biological activities. Furthermore, the more active terrestrial predators responded more to aquatic top predator loss, via an increase of some components of aquatic macrofauna, than more stationary terrestrial predators. The aquatic trophic downgrading indirectly altered the richness and abundance of cursorial terrestrial predators, but these effects had different direction according to the aquatic functional group, filter-feeder or other detritivores. The web-building predators were indirectly affected by aquatic trophic downgrading due to increased filter-feeder richness. Aquatic warming did not affect the aquatic micro- or macro-organisms but did positively affect the abundance of web-building terrestrial predators.\n4. These results allow us to raise a predictive framework of how different anthropogenic changes predicted for the next decades, such as aquatic warming and top predator loss, could differentially affect multiple biological groups through interactions within and across ecosystems.\nWe conducted the study at the Serra do Mar State Park, Nucleo Picinguaba, on the northern coast of the state of São Paulo, Brazil, from April to July 2014. The bromeliad species Neoregelia johannis (Carriere) LB Smith. (Bromeliaceae) were employed as a model system in the experiment. Our study investigated how aquatic warming and the loss of top aquatic predators affect the food web structure of aquatic macrofauna and microfauna, as well as multiple groups of terrestrial predators. Temperature manipulations here predicted warming for the next decades in the study site, i.e., an average projected for 2040 (2°C above ambient), and 2100 (4°C above ambient). To sample the aquatic microbiota organisms, we homogenized the bromeliad water, collected 2 ml, and fixed it with 5% Lugol for counting and identifying microscopic algae, protozoa, and metazoa (hereafter referred to as microbiota). To sample macrofauna organisms at the end of the experiment, we first sampled the terrestrial organisms foraging upon the experimental tank-bromeliads (e.g., spiders, centipeds, ants). After that, we dissected and washed the bromeliads. Then, we separated terrestrial and aquatic organisms found in the trays, counted, fixed, and later identified the organisms to the lowest possible taxonomic level and functional group (Antiqueira et al., 2018a). The details of the sampling and characterization of macrofauna and microfauna are described in Appendix 1 of the paper. The consumer-resource relationships between all the macrofauna and microbiota were determined according to specific literature and field observation.\n|ID = experimental unity identification|\n|block = block identification|\n|pred = predator treatment (1spp, 4spp, no=absence)|\n|temp = warming treatment (amb=ambient; Two = +2ºC; Four= +4ºC)|\n|media.t = average temperature of each experimental bromeliad|\n|pred.1 = predator treatment (Presence/Absence)|\n|filt.feed.abund = filter-feeder abundance|\n|filt.feed.riq = filter-feeder richness|\n|mesopred.riq = aquatic mesopredator richness|\n|mesopred.abund = aquatic mesopredator abundance|\n|detritivore.abund = detritivore abundance|\n|detririvore.riq = detritivore richness|\n|curs.pred.t.rich = cursorial terrestrial predator richness|\n|curs.pred.t.abd = cursorial terrestrial predator abundance|\n|web.pred.rich = web-building predator richness|\n|web.pred.abd = web-building predator abundance|\n|mesopred.t.ric = terrestrial mesopredator richness|\n|mesopred.t.ab = terrestrial mesopredator abundance|\n|ciliate.abund = ciliates abundance|\n|ciliate.richness = ciliates richness|\n|ameb.abund = testate amoebae abundance|\n|ameb.richness = testate amoebae richness|\n|zoop.abundance = zooplankton abundance|\n|zoop.richness = zooplankton richness|\n|flage.abund = flagellates abundance|\n|flage.richness = flagellates richness|\n|phyto.rich = algae richness|\n|phyto.abund = algae abundance|\nFundação de Amparo à Pesquisa do Estado de São Paulo, Award: 2017/26243-8\nFundação de Amparo à Pesquisa do Estado de São Paulo, Award: 2018/12225-0']	['<urn:uuid:9919bcd6-4701-46ae-89cd-c2894716e6db>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	8	29	746
49	What economic and ecological threats face the Upper Mississippi region?	The Upper Mississippi region faces dual threats to its ecosystem and economy. The river supports $24.6 billion in economic activity and 421,000 jobs through fishing, hunting and boating. However, illegal levee raises between Muscatine, Iowa, and Hamburg, Illinois are destroying critical riverside habitat for fish and wildlife while increasing flood heights downstream. Climate change is exacerbating these problems, with the three highest-volume rain storms ever recorded occurring in the last three years, and projections showing extreme downpours could increase by 400% by century's end. The economic impact is severe - the 2018-2019 flood alone caused $9.2 billion in losses.	['Climate change and illegal levee development threaten public safety, river health\nEileen Shader, American Rivers, (570) 856-1128\nElliot Brinkman, Prairie Rivers Network, (217) 344-2371, x202\nChristine Favilla, Sierra Club, (618) 462-6802\nDavid Stokes, Great River Habitat Alliance, (314) 918-1007\nMelissa Samet, National Wildlife Federation, (415) 762-8264\nRob Moore, Natural Resources Defense Council, (312) 651-7923\nMaisah Khan, Missouri Coalition for the Env., (314) 727-0600, x113\nWashington, D.C. – American Rivers today named the Upper Mississippi River among America’s Most Endangered Rivers® of 2019, citing the grave threat that climate change and illegal levee raises pose to public safety and river health. American Rivers and its partners called on state and federal agencies to prohibit the reckless raising of levees and promote better flood protection solutions.\n“The America’s Most Endangered Rivers report is a call to action to save rivers that face a critical decision in the coming year,” said Eileen Shader with American Rivers. “It’s time to stop the illegal levee development on the Upper Mississippi that is putting people and river health at serious risk.”\n“We are already feeling the impacts of climate change in the Midwest, including more frequent and severe flooding, and it’s only going to get worse. Abusing and degrading the Mississippi River will make us more vulnerable to these threats. Protecting and restoring the river will make us better prepared to face future floods and safeguard communities. It’s our choice to make.”\nThe Upper Mississippi River is threatened by levees that are being raised (i.e., made taller) without required permits and approvals. Eighty miles of levees between Muscatine, Iowa, and Hamburg, Illinois, have been raised without obtaining the required state or federal approvals. These levees not only destroy critical riverside and floodplain habitat for fish and wildlife, they also make flood heights higher and increase flooding downstream. For example, during the most extreme flood events, Hannibal, Missouri, is projected to experience an additional foot or more of floodwaters because their neighbors have raised their levees without regard to the impacts.\nAmerican Rivers and its partners called on the U.S. Army Corps of Engineers, the Federal Emergency Management Agency, and the states of Illinois, Iowa and Missouri to take corrective action to stop and resolve these levee violations. Further, the groups called on the agencies to advance 21st century flood protection solutions that deliver multiple benefits to people and nature.\n“We have to stop the circle of absurdity where we spend enormous sums of money to build larger levees which make the next flood even worse and costs us millions more in emergency funds, only to have the entire cycle repeat year after year,” said David Stokes with the Great River Habitat Alliance.\n“We must move beyond this outdated vision of flood control that foolishly relies on bigger and higher levees and floodwalls to a new vision that makes room for rivers and allows nature-based solutions to protect us,” said Elliot Brinkman with the Prairie Rivers Network.\nThe threats posed by these unlawful changes are real and getting worse as climate change is leading to more frequent floods and intense storms in the Upper Mississippi River Basin and across the country. The three highest-volume rain storms ever recorded in the U.S. have occurred in the last three years, in line with climate scientists’ projections that extreme downpours in the U.S. could increase by 400 percent by the end of this century. Munoz et al. (2018) determined that the magnitude of 100-year flood events in the Mississippi Basin has increased by 20 percent over the past 500 years, with much of that increase being caused by the combination of river engineering and climate change. The increased risk of flooding is the reason some levee districts have pursued higher levees, but their actions are intensifying the impacts of flooding for their neighbors.\nThe Mississippi River is a globally significant flyway used by hundreds of species of birds and provides unique habitat for fish, mussels, reptiles and mammals. This significant ecosystem supports commercial and recreational fishing, hunting and boating, which contribute $24.6 billion to the region’s economy and an estimated 421,000 jobs. The river has been the lifeblood of many cultures throughout history and has served as the inspiration for a rich heritage of American music, art and literature.\nThe annual America’s Most Endangered Rivers® report is a list of rivers at a crossroads, where key decisions in the coming months will determine the rivers’ fates. Over the years, the report has helped spur many successes including the removal of outdated dams, the protection of rivers with Wild and Scenic designations, and the prevention of harmful development and pollution.\nSome portion of the Mississippi River was previously included on this list in 1991-1992, 1994-1996, 2000-2001, 2004, 2011, 2014 and 2018. Other rivers in the region listed as most endangered in recent years include the Buffalo National River (2019, 2017) and Middle Fork Vermilion River (2018).\nAmerica’s Most Endangered Rivers® of 2019\n#1 Gila River, New Mexico\nGov. Grisham must choose a healthier, more cost-effective way to provide water to agriculture than by drying up the state’s last major free-flowing river.\n#2 Hudson River, New York\nThe U.S. Army Corps of Engineers must consider effective, nature-based alternatives to storm-surge barriers that would choke off this biologically rich tidal estuary.\n#3 Upper Mississippi River, Illinois, Iowa, Missouri\nState and federal agencies must enforce laws that prohibit illegal levees, which increase flood risk for communities and degrade vital fish and wildlife habitat.\n#4 Green-Duwamish River, Washington\nLocal leaders must produce a flood protection plan that safeguards communities and restores habitat for chinook salmon — fish that are essential to the diet of Puget Sound’s endangered orca whales.\n#5 Willamette River, Oregon\nThe U.S. Army Corps of Engineers must immediately improve 13 dams to save wild chinook salmon and steelhead from going extinct.\n#6 Chilkat River, Alaska\nThe Japanese investment firm, DOWA, must do the responsible thing and back out of a mining project that could decimate native salmon.\n#7 South Fork Salmon River, Idaho\nThe U.S. Forest Service must safeguard endangered fish by denying a mining proposal that could pollute this tributary of the Wild and Scenic Salmon River.\n#8 Buffalo National River, Arkansas\nGov. Hutchinson must demand closure of an industrial hog-farming facility that pollutes groundwater and threatens endangered species.\n#9 Big Darby Creek, Ohio\nLocal leaders must use state-of-the-art science to craft a responsible development plan that protects this pristine stream.\n#10 Stikine River, Alaska\nThe International Joint Commission of the United States and Canada must protect the river’s clean water, fish and wildlife, and indigenous communities by stopping harmful, polluting mines.\n2019’s “River of the Year”: Cuyahoga River, Ohio\nAmerican Rivers celebrates the progress Cleveland has made in cleaning up the Cuyahoga River, fifty years since the river’s famous fire that sparked the nation’s environmental movement.\nABOUT AMERICAN RIVERS\nAmerican Rivers believes every community in our country should have clean water and a healthy river. Since 1973, we have been protecting wild rivers, restoring damaged rivers and conserving clean water for people and nature. With headquarters in Washington, D.C., and offices across the country, we are the most effective river conservation organization in the United States, delivering solutions that will last for generations to come.', 'The Mississippi River exceeded its banks from December 2018 until August 2019, becoming one of the longest floods in the history of the U.S. According to NOAA National Centers for Environmental Information, the event cost the country a total of $9.2 billion. The impact of this economic loss was felt by communities and governments across the Mississippi basin as the insured loss estimate was minimal. Munich Re estimates the insured losses for this event to be approximately $200 million. Events like these highlight the importance of creating communities resilient to natural disasters, both from a risk transfer and a risk mitigation standpoint. Within disaster risk management, investment in risk-reducing measures is considered wholly separate from investment in risk transfer measures. But the two can actually be combined to create more efficient solutions.\nRecently, a group of researchers from University of California, Santa Cruz; The Nature Conservancy; and Munich Re collaborated on a study that focuses on a combined risk transfer/risk reduction solution. The study examines coral reef restoration and its risk reduction impact for property exposed to storm surges along the coastline. While the scope of the study is limited to the benefits of coral reef restoration, we are confident that this combined solution is transferable to other regions focusing on nature-based and/or green solutions. We term this solution broadly as a Resilience Risk Transfer (RRT). Munich Re has been actively pursuing potential proof of concept for testing the RRT solution. Our intent is to apply the RRT solution to the nature-based solutions within the Mississippi River basin, e.g., wetlands, bioswales, etc.\nOne of the major challenges of developing a specific RRT solution is the unavailability of a comprehensive vulnerability study for a potential region. The issue is further complicated with changing views of risk that the regions experience with a warming climate. In recent times, Insurtech companies have offered the unique capabilities of using machine learning and high-resolution climate modeling to address the above gap. These models will be crucial in highlighting and identifying the most vulnerable communities which, in turn, will be useful in devising risk reduction solutions.\nWith the work we’ve done with The Nature Conservancy, we could structure a resilience risk transfer offering by quantifying the impact of mitigation using nature-based solutions, and thus the reduction in premium over the time frame of the mitigation project. We are working toward developing a potentially novel/visionary solution, i.e. Resilience Risk Transfer, that can help communities recover financially from a flood event (and, potentially, other natural disasters) as well as focus on the risk mitigation requirements for the region. The RRT solution has the potential to restore the natural infrastructure that provides one of the most important protections from flood disasters.\nAny descriptions of coverage contained in this document are meant to be general in nature and do not include nor are intended to include all of the actual terms, benefits and limitations found in a reinsurance/insurance policy. The reinsurance/insurance policy and not this document will form the contract between the parties thereto, and governs in all cases.']	['<urn:uuid:5946720d-7051-47b0-a1fd-3eb3da363daa>', '<urn:uuid:8b6429c2-2868-4249-aecb-f7b95f8a6486>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	10	99	1714
50	How do the construction considerations for a Viking bench's durability relate to the necessary tool maintenance and safety precautions for long-term woodworking?	The Viking bench's durability relies on using properly maintained tools and correct construction techniques. The bench requires stout Douglas fir lumber and waterproof polyurethane glue, with tightly glued seams to prevent moisture damage. To maintain safety and effectiveness during construction, power tools must be kept moisture-free, regularly checked for damaged wiring or loose plugs, and properly stored away from unauthorized users. Tools should only be used as designed, with appropriate safety gear including eye protection, ear plugs, work gloves, and protective clothing. Additionally, maintaining a clean, well-lit workspace is crucial for both construction quality and safety.	['The Viking Bench\nThis heavy-duty Viking bench looks right at home with the massive built-to-last Viking long table. And rightly so—it shares most of the same construction techniques. If you’re thinking of building both the table and the bench, consider starting here; it’s a great warm-up for the larger project.\nThis Viking bench can also serve as a great stand-alone piece for your deck, patio or garden. It was designed to be just the right length to tuck neatly beneath the Viking table, but you can make it shorter if you like. Also, be sure to check out our top 10 woodworking projects once you are done building the Viking bench and table of course!\nIf you’ve already built the Viking table, the “leg” part of this Viking bench project is going to look mighty familiar. Just as with the table, we made our benches from stout Douglas fir lumber. For other wood options, see “Pick the Right Wood” in the Viking table story.\nStart with flat, straight boards—free of splits, twists, cupping and loose knots—and you’ll spare yourself a lot of head scratching and extra work down the road. If you have trouble finding perfect 2x12s for leg material, purchase extra lumber so you can cut around the defects.\nTo create a single Viking bench, cut the four 22-in. leg blanks (Labeled “A” in Figure B in Project PDFs below) to length; the ends need to be square, so cut carefully. Pair up your boards so when one is laid atop the other, there is little or no gap along the ends and edges. If you flip or rotate the boards, you might find the perfect fit. Try to have any defects fall in the areas of the wood you’ll be cutting away as you form the legs.\n1. Make the Leg Template\nMark out your leg template on 1/4-in. plywood as shown in the photo above. Set a scrap of plywood against the template. Drive a screw 9-1/4 in. from the end of the scrap and use that screw as a pivot point for your tape measure. Then swing the two arcs to create the leg shape. Cut just outside the line with a fine-tooth jigsaw blade, then use a belt sander to sand right up to the line\n2. Make a Glue Sandwich\nUse your template to mark the leg shape on all four leg parts (A). With the marks facing up, lightly dampen one board—polyurethane glue needs moisture to work—then apply the glue in squiggles across the main body of the leg. Use a putty knife to spread it slightly beyond the edges of your template marks. Polyurethane glue is waterproof, and with tightly glued seams there’s less chance of moisture working its way between the boards. Place a second board—marked side up—over your glued board and align the edges.\n3. Clamp the Leg Blanks Together\nLine up the mortise edges of the boards, then drive two or three drywall screws into the waste wood to keep the boards aligned. Fasten clamps—more is better—around the perimeter to force the boards tight together. Add more screws as needed. The glue will foam as it goes to work. Keep your boards clamped together for at least two hours; Let it dry overnight for good measure. Repeat this procedure for the other leg blank.\n4. Cut the Leg Blanks\nCut the legs to shape. A jigsaw with a long, coarse blade works fine, but a band saw means less sanding. Whichever tool you use, make a series of relief cuts along both the concave side of the leg and the mortise cutout area as shown. These allow you to remove waste material as you cut. They also allow your blade to get back on track if it wanders and begins making angled cuts. If you have a jigsaw with reciprocating action, set it at zero; it will cut slower, but your blade will wander less.\n5. Sand the Leg Blanks and Rout the Edges\nNext, use a belt sander to smooth and true up the curved sides. Begin with a coarse belt, then progress to finer grits. If you have access to a bench-top sander of some sort, use it; you’ll get better results.\nUse a router with a 1/2-in. round-over bit to soften the edges of the curved parts. DON’T rout the tops and bottoms of the legs or the flat area where the mortise cutout will be.\n6. Cut the Leg Blanks in Half\nMark the exact center of the leg blanks and use a circular saw—cutting from both sides—to cut them in half. They’re 3 in. thick, so you’ll need to cut from both sides. Dry-fit the pieces to be sure the parts fit tightly together. Butt the tops of the legs together, then place the seat brace (B) and bottom leg plate (C) in their respective positions. Make sure the stretcher fits into the opening between the legs. Also, be sure the ends of the legs sit flat against the seat brace and bottom plate; you may need to do a little sanding or trimming until the pieces fit tight.\n7. Screw the Leg Assemblies Together\nClamp the leg halves together and check the size of the stretcher opening. Adjust if needed. Once all systems are a go, apply construction adhesive to the ends of the legs, cinch the tops together with a clamp, position the seat brace and bottom leg plate (B and C) and then secure them with 3-1/2-in. exterior screws. Repeat for the other leg assembly.\n8. Secure the Stretcher to the Legs\nHaving built the leg assemblies, you’ve done the hardest part. Cut, shape and install the bench stretcher (E) as shown in Figure A and the photo above. Use clamps to pull the stretcher shoulders tight against the legs, and drive in wedges or shims to snug the tenon tight against the seat brace. Drive screws through the seat brace into the tenon.\n9. Complete the Seat Braces\nFasten the seat braces (B) to the stretcher, then position the two 2×8 top boards (F); they should run past the outer seat braces by about 6 in. Fasten the top boards with 2-1/2-in. washer-head screws. Install the 2-1/4-in. edge boards (G and H) along the four edges of the Viking bench. Drill pilot holes to avoid splitting the edge boards.\n10. Apply Finish and Attach Viking Bench Feet\nApply two coats of exterior finish; we used a semitransparent deck stain. Finally, screw on the Viking bench feet (D). We recommend using white oak for the feet because it’s rot resistant. To keep your benches in tiptop shape, set them up on 2×4 blocks and cover them with a tarp before winter strikes.\nWant more wood working projects? Check out these 15 amazing woodworking projects to try.\nClick the links below to download the materials and cutting lists for this project, as well as Figure A and B.', 'Most people will tell you that some sort of tool is a must for any diy project around the house. Knowledgeable trades individuals who constantly need to use tools for their job could hardly live without them. Nowadays, power tools are definitely more efficient but additionally they bring an increased risk of operator injury. Accidents when using power tools are often on account of negligence, apathy or ignorance. With this in mind, here are some pointers to keep you protected when working with power tools.\nYour eyes are actually extremely vulnerable to injury when using power tools so eye protection is a must have. If you will probably use safety goggles you’ll be able to prevent a lot of things from getting into yours eyes, like dust, debris, wood shavings, and fiberglass, to name a few things. Eye safeguards is of the foremost importance when working in the area of power tools. Your ears must be protected also, making use of earplugs. Doing this, you will have a better chance of minimizing ear damage due to the loud noises from power tools.\nProtecting against injury can be done by using the appropriate tool for the job. When you use a tool for something that it wasn’t designed for, you increase, not only the odds of damaging the tool, but also the chance of causing injury. You ought to browse through the instruction manual to make sure you know the proper way to use your new tool. Tools are meant to be used in a certain way, and that is how you should use them. Almost any power tool not utilized should be separated from it’s power source by disconnecting it, and never carry it around dangling from it’s power cord. Take into account the clothes you might be wearing, and avoid loose clothing. Ideally, your whole body should be covered with your work clothes, and work gloves would help protect your hands. Wear masks as required, steel-toed work footwear, and hard hats are also good things for safety.\nPower tools should not be used in the rain and always keep them free of moisture. Frequently look at the tools for any wiring having loose, damaged plugs, or exposed wiring. Taping a nicked cord is okay, but if a cut is deep, the cord should really be replaced. A thoroughly clean work area clear of accumulated dust and flammable liquids won’t be a potential fire hazard. A tidy and organized work environment will probably be a lot safer and easier to work in. Don’t forget that power tools such as circular saws, nail guns and table saws tend to be potentially more dangerous than other tools so take more care when using them. Keep tools in their spot, where kids or unauthorized people can get to them.\nMake sure that there is lots of light if you are working. Whenever you pay attention to what you are doing, and understand all of the safety precautions, you should have no problems working without any accidents.']	['<urn:uuid:df5f7900-9bba-4bcf-9619-0ef7d9fa1067>', '<urn:uuid:40e030e8-bf31-4d78-984b-cb8e721856ab>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	22	96	1659
51	marketing expenses home kitchen business restaurant difference	Home-based food businesses primarily rely on social media platforms like Facebook, Instagram, and Shopify for marketing, along with local fairs, events, and word-of-mouth through friends and family reviews. Restaurants, however, typically spend 3-6% of their sales on marketing, which may include professional marketing agencies, especially for fine dining establishments in saturated markets. Restaurant marketing often requires more substantial investment and can include loyalty programs and various advertising channels.	"['Every talented chef and baker has likely thought about earning a profit from making the foods they love. Choosing to start a food business from home is a great way to turn your hobby into a business opportunity. However, there is a lot more to it than simply preparing your delicious delicacies. Once you have a viable idea, there are also laws, business plans, and marketing strategies to consider. A new food business needs more than a good recipe to become a successful enterprise.\nLearn the Laws about Starting a Food Business from Home\nAs with any investment, you should do your research. Know the local regulatory laws before you sink your money into a new venture. If you plan ahead and anticipate your next steps, it could save you a lot of time and frustration as you set up. Before you can start running your home business, you must get the required permits and inspections to keep everything above board. Since laws vary from state to state, it is a good idea to get familiar with local legislation governing food businesses.\nCottage Food Laws\nEvery state establishes its own laws that regulate the sale and manufacturing of home food businesses. But, if you are not looking to get into commercial production, cottage food laws create more opportunities for residents to earn income. Working under these laws makes it simpler to start a food business from home.\nThe cottage food laws eliminate some of the bureaucratic red-tape associated with commercial food production. However, they also limit which types of food you can sell. In the name of public health, small home-based food business cannot sell products that are susceptible to food-borne illnesses. In short, this usually translates that you can’t sell products that require refrigeration. Most home-based food businesses stick to low-risk foods to generate income. For more specific information, you can find your state’s cottage food laws here.\nAdditionally, some states will also require that you have business or food-handlers permits. Be aware that some permits can only be obtained after you complete a training course. However, the cost of the course usually includes the content and paperwork. If you bypass this step and fail to get the permits, there could be steep penalties or fines.\nSome states also limit how much money you can make without crossing the threshold into commercial production. If you overlook any stipulations of the laws, it could be disastrous to your new business. So, don’t be afraid of asking for professional advice to ensure you are operating a legal business. If you are not sure where to begin, you can find out more from the U.S. Small Business Administration. You could also speak with local business owners and legal experts familiar with food regulation laws in your area.\nMake a Business Plan\nIf you are considering a new business venture, then you probably already have a product idea in mind. So, the next step is to evaluate the market and decide which niche you want to focus on. Observing your competition is a good way to gauge the market and determine if you have a viable idea. It can also help you get a lay of the land and understand how your food business can fit into the local commercial landscape.\nFinances and Recordkeeping\nAs you launch your new business, you want to maintain clear separation between your personal and professional finances. Distinguishing clear boundaries between the two safeguards both your personal accounts and your business from risks. Creating this separation will not only protect you, but also any future assets you accumulate.\nFrom my experience, it is much easier if you establish separate bank accounts, records, and tax numbers from the very beginning. Otherwise, you may be left sorting through financial documents and invoices when taxes come due.\nCreate a Marketing Strategy for Your Food Business\nOnce you have all the pieces in place, it is time to get to work. You have to adopt a marketing and branding strategy to get your name out there. However, the name you choose is almost as important as the product you serve. It impacts your entire public perception. This is why you want something that sets you apart. When choosing the right name, you want to pick something memorable; something unique and relative to your food business. It is best not to choose anything too long or difficult to remember. You should also avoid names that could easily be misspelled or confused for another business.\nYou also want to project a professional image to all prospective clients and customers. Creating a logo and official website are easy ways to achieve this. If you are not comfortable setting it up yourself, you can enlist the help of graphic designers for reasonable fees. Attend local fairs and events to start networking and looking for venues where you could sell your product.\nExpanding Your Food Business\nSocial media will also be another important tool for you to promote your food business. Platforms like Facebook, Instagram, and Shopify can help you reach a much larger audience. Post tantalizing pictures and tap into your friends list to draw in new customers. You can also ask for help from your friends and family. Ask your loved ones to try your food and leave reviews to help create an internet presence. Utilizing social media platforms will get your business off the ground.\nLastly, you have to perfect your sales pitch. You know how good your product is, but now it is time to ensure everyone else does as well. Whether you are trying to get your products on the shelves in local supermarkets or launching an e-commerce takeover, you need a focused plan. With a little preparation and the right business model, you can certainly start a successful food business from home.', ""Opening a restaurant and becoming a restaurant owner is part of many people’s dreams. Turning that dream into a reality can be challenging, but also rewarding. It’s a big decision and with every big decision comes the question of, “How much does it cost to open a restaurant?”\nThe startup cost to open a restaurant is significant. There are many factors to consider which can make it a tough question to answer. However, this article will provide you with the information you need to start budgeting for your restaurant.\nSo, how much will it cost to start a restaurant? Let's find out!\nAverage Cost to Open a Restaurant\nThe average restaurant opening cost can be hard to come by. The answer will vary based on the type of restaurant you want to open, the location, and the size of the business. Other factors include whether you’ll want to offer a dine-in experience or operate a ghost or commissary kitchen and offer takeout and delivery options.\nOnce you narrow down these factors, you’ll get a better idea of the normal expenses of running a restaurant. This can also help you understand how to increase restaurant sales.\nRestaurant costs can vary, but they range from $170,000 to $750,000. However, these numbers shouldn’t discourage you from bringing your dream restaurant to reality. There are several ways to reduce restaurant expenses and plan ahead.\nTypical Restaurant Startup Costs\nNot all restaurants will have the same startup costs. However, you should consider the different areas where you plan on spending your money in order to get clarity on your final expected costs and what to expect to pay.\nProperly planning out your restaurant business can help you avoid losing thousands in additional labor cost and potential customers. Poor restaurant equipment choices and construction can end up costing you twice as much as time goes on. Below are some essential startup costs and expenses for opening a restaurant.\nBreakdown of Restaurant Startup Costs\nThe following restaurant opening costs can help you budget for your potential new business project:\nEven though your restaurant won’t be fully operational during the building stage, you’ll still have utility expenses. These include water, gas, and electricity.\nIt’s common for these expenses to add up quickly. Average costs can range from $1,000 to $1,200 per month depending on the size of the property.\nPicking a location for your restaurant is one of the most crucial decisions. It’s beneficial to pick a location that has good foot traffic, but these spots can be pricey. When choosing the location, you’ll have to consider the following options:\n- Investing in a new construction property\n- Converting an existing commercial property or space per sqaure-foot into a restaurant\n- Purchasing an existing and/or operating restaurant\nEach of the options above has pros and cons. Choosing the right option will require thorough research.\nFinishing Touches and Equipment\nIt’s true that the little expenses start to add up. These include interior finishing touches and your kitchen equipment. From kitchen tools to restaurant inventory software, it’s all something to consider. The back of house equipment like the lighting, signage, phone systems, and music systems are also crucial.\nFinishing touches like decor and furniture are what help bring life to your restaurant and make the space appealing to guests. Don’t forget that everything needs to be ready in terms of restaurant inventory such as food, plates, cups, utensils, and cookware. All in all, these costs can range from $20,000 to $400,000.\nMarketing is a restaurant opening cost that can be minimal or expensive, depending on your needs. Franchise locations might have the advertising part taken care of. However, new restaurants might need to be advertised more, especially in a saturated market.\nThe costs for this can vary based on the channel or niche market you’re targeting. A new brick and mortar fine dining restaurant might benefit from professional marketing from an agency. Smaller establishments might only require some social media marketing efforts to achieve O2O success.\nMarketing and PR costs can depend on your competition and restaurant concept. In most cases, similar costs will equal about 3-6% of your sales. Continuous marketing is necessary in order to keep business moving. You may also want to consider incorporating a loyalty program.\nDevelopment and Organizational Costs\nObtaining the necessary licenses and permits is one aspect of opening a restaurant that people dread. However, it’s essential.\nYou’ll have to file and pay your restaurant licenses and permits as well as pay for all utilities. The average licensing cost of this can range from $2,500 to $200,000.\nRestaurant technology will help your business run smoothly. You’ll need to have a point of sale system, like a bar POS system for your restaurant, and you’ll likely want to incorporate restaurant tech. These include restaurant QR code menus, ERP accounting system (see ERP meaning), and restaurant inventory management software. It's also crucial for restaurants to understand the benefits of ERP in this case.\nNo restaurant is complete without food. A successful restaurant opening will call for fully stocked pantries, refrigerators, and freezers. If you run out of food items, especially at the beginning of your business venture, it can give guests the wrong impression.\nKeeping your inventory stocked can guarantee that you have all you need to provide seamless customer service. Some restaurants consider having safety stock on hand. Your average food cost per month will likely vary based on the menu offerings and the type of cuisine you serve.\nCommon Restaurant Startup Mistakes\nBeing aware of potential mistakes or problems before they arise can help you better plan and prepare to avoid them. Even some of the best business owners and best coffee roasters can make mistakes. Below are three common mistakes that can occur when opening a restaurant:\nNot Having a Solid Business Plan\nA solid business plan is essential for a restaurant startup. Anyone can come up with a good idea, but you should make sure that your idea and plan are feasible. A business plan can help you outline your business and understand how it will operate. It's also easy to seek out investors or eCommerce banks for small business loans with a business plan. This will also help you avoid sticky situations in the future.\nNot Focusing On the Location\nLocation is a crucial factor when opening a hospitality business. You should always choose the location that is first on your list, given that it’s within your budget to do so.\nBe sure to consider your target market, where they tend to spend time, and a location with decent foot traffic. If you find a location that meets all three characteristics, strongly consider it.\nNo Financial Planning\nIf you don’t plan out your finances, you might underestimate the amount of money that is required. This can result in a lot of problems down the road. Unexpected expenses can come up, and it’s good to be prepared for this by having money set aside.\nFrequently Asked Questions About the Cost of Opening a Restaurant\nWhat Is the Average Cost of Opening a Restaurant?\nThe average cost of opening a restaurant can vary depending on numerous factors, but it tends to range from $170,000 to $750,000. These factors can include the type of restaurant you open, the location, and the size of the restaurant.\nWhat Costs Should Be Considered Before Opening a Restaurant?\nThe costs that you should consider before opening a restaurant include:\n- Finishing touches and equipment\n- Development and organization\n- Food costs\nWhat Are Common Restaurant Startup Mistakes?\nThe common restaurant startup mistakes that people run into include:\n- Not having a solid business plan\n- Not focusing on the location of the restaurant\n- No financial planning""]"	['<urn:uuid:45643f83-9ae9-49e5-a4f0-9947d41078b1>', '<urn:uuid:3658a8c6-8e02-4dff-899a-d50af0df1a8d>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	7	68	2260
52	white house funding civil war financial plan	According to the Treasury's announcement in December 1861, they could manage the war if it ended by mid-1862, but if it continued longer, the Treasury's expenditures would exceed its income, necessitating increased taxes on most items to fund the war.	['December 1861 saw a continuing fraught relationship between the president, Abraham Lincoln, and the man he appointed as overall commander of the North’s army, General McClellan. Lincoln continued to question the timidity of McClellan’s approach while McClellan continued with his assertion that if got it wrong, the whole of the North could suffer as a consequence.\nDecember 1st: President Lincoln expressed his concern to General McClellan that the Unionist armies did not seem to be doing anything substantial.\nDecember 2nd: Congress gave its permission for the suspension of habeas corpus in Missouri.\nDecember 3rd: Lincoln gave his State of the Union address to Congress. The Union started its move against New Orleans when ‘USS Constitution’ arrived at Ship Island at the mouth of the Mississippi River carrying the 26th Massachusetts Regiment.\nDecember 4th: Great Britain announced an embargo on all exports to the US\nDecember 5th: The Secretary of War announced that Unionist strength stood at 660,971 men of whom 640,637 were volunteers.\nDecember 6th: It was announced that the Treasury could cope with a war that ended by mid-1862 but if it lasted longer than this then the Treasury’s income would be far outweighed by its outgoings and taxes on most things would have to be increased to fund the war.\nDecember 7th: In a scene that mirrored the ‘Trent’ incident, the ‘USS Santiago de Cuba’ stopped a British ship, the ‘Eugenia Smith’ and a Southerner called J W Zacharie was taken off. Zacherie was a purchasing agent for the Confederacy.\nDecember 9th: The Senate approved the setting up of the Joint Committee on the Conduct of the War. This recognised that previous comments made to the Confederacy, that states rights would not be interfered with once the war was over, was no longer the case and that the internal affairs of the rebel states would be reformed regardless and that the Union would be upheld.\nDecember 10th: The Confederacy admitted Kentucky to its membership despite the overwhelming evidence that the state was about to fall to Unionist forces.\nDecember 11th: Charleston was damaged as a result of a fire that swept through the city. Charleston was the most important port in South Carolina.\nDecember 12th: The success of the Union’s navy along the South’s coastline was such that cotton farmers started to burn their crop in fear that it might fall into the hands of the Union.\nDecember 15th: Congress expressed its view that the use of slavery in the South was becoming more and more an issue. The original cause of the war was state’s rights but greater knowledge in the Union about slavery put it at the forefront of why the war was being fought. Over the next few months Congress passed a number of laws such as the military could no longer return to the South fugitive slaves; that slavery was to be outlawed in the District of Columbia; that any slave state that offered to give up slavery would be given financial assistance from the Union.\nDecember 18th: Lord Lyons, the British ambassador in Washington DC, received a message from the British government that he was to demand the release of Mason and Slidell. If the Union failed to do this within 10 days, he had instructions to break off diplomatic relations.\nDecember 19th: Lyons met the US Secretary of State, Seward.\nDecember 20th: Two British warships arrived in Canada as a result of the ‘Trent’ affair.\nDecember 21st: The meetings between Lyons and Seward continued. Both Seward and Lincoln recognised that there was a real risk of war with the British if their demands went unheeded.\nDecember 23rd: The Cabinet was advised by Seward that Captain Wilkes made an error in taking off Mason and Slidell and that he should have brought in the ‘Trent’ and its ‘contents’ as the ship had violated its neutral status. Seward made it clear that the seizure of the Confederate commissioners was unlawful whereas the seizure of the ‘Trent’ as an entity would have been lawful.\nDecember 24th: Congress passed a series of duties that were to be added to tea, coffee, sugar and what were classed as “luxury goods”.\nDecember 25th: Despite it being Christmas Day, the Cabinet and the President were in discussions on what to do with Mason and Slidell. Fighting was reported at Fort Frederick in Maryland and Cherry, western Virginia.\nDecember 26th: It was announced that Mason and Slidell would be released because their arrest was illegal. It was further announced that Captain Wilkes had acted without the knowledge of the government.\nDecember 30th: Mason and Slidell were handed over to Lord Lyons. They were immediately put on a ship to England. Lyons then released his own interpretation of the law regarding “neutral nations” and it was at odds with Seward’s and, ironically, found support among many Americans. However, with the issue resolved, relations between the North and Great Britain improved.\nDecember 31st: President Lincoln pressed his army commanders for more action. However, McClellan did not hear his message as he was ill.']	['<urn:uuid:e3493ebf-8eab-42d3-bc8a-da2a32726db1>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	7	40	841
53	What are the similarities between how the National Historical Museums in Sweden and BENTU handle the reuse and transformation of cultural materials?	The National Historical Museums in Sweden openly shares 3D models of cultural heritage sites like the Hallwyl House, allowing worldwide creative reuse including VR tours and virtual community meetings. Similarly, BENTU works with existing materials by repurposing waste materials and pre-industrial relics, transforming them through modern reconstruction to give them new life and meaning.	['A review of the 2019 Stockholm edition of the Sharing is Caring conference.\nSharing is Caring is a conference series on open cultural heritage data. Since the first event brought together professionals from all across Europe in Copenhagen in 2011, extensions have taken place in several European cities. On 16 and 17 September 2019, the Nationalmuseum and the Royal Armoury organised the event in Stockholm with the theme “Open Data – now what? Applying principles of openness and collaboration in strategy and practice“. The learning and insights gained during these two days can be valuable pieces of advice for the sector, shared and discussed by colleagues from around the world. Let me share my personal top 5 with you.\nYour data is your core value\nWhatever you or others want to create from your data, from curated content on your institution’s website or social media channels to Virtual Reality experiences based on your 3D models: It all depends on the quality and licensing of your data. You can start small and learn as you go – but prepare for working continuously on your data. As Loic Tallon, former CDO of the Metropolitan Museum, put it: “Do the fundamentals brilliantly.“ Your data will never be perfect (it simply can’t with technology evolving and standards shifting) – but if you prioritise your data quality and sharing with open licenses over a growing number of platforms and media, your overall success in reaching your defined goals will increase.\nSharing can lead to your data’s success where you didn’t suspect it\nSeveral speakers shared amazing examples on success and creative re-use of their data where they had not expected it at all. Erik Lernestål from the National Historical Museums in Sweden showed how they got started with basic equipment in creating 3D models of the Hallwyl House. They uploaded them to Sketchfab and shared it openly – and experienced the data being reused all around the world with users re-creating the whole house based on original plans, designing VR tours and hosting virtual community meetings of their avatars in the rooms. Another example by Sandra Fauconnier from the Wikimedia Foundation was the massive increase in views of digitised content after uploading it to Wikimedia Commons: If your object’s digital representation is being used in a Wikipedia article, so many more people will see it and interact with it compared to when you keep it to yourself and on your own website or online collection.\nDon’t shy away from interacting at eye-level\nSharing cultural heritage data with an open license in a high quality goes with letting go of some control (if institutions ever had any is another question). But it is not enough to put your data out there – build partnerships at eye-level so your data is reused. Both Jill Cousins from the Hunt Museum and Loic Tallon shared great insights in how the success of their projects and initiatives is based on relationships to partners outside the institution, ranging from private companies to volunteers from their communities. Going open can be easier when you have partners by your side who cherish your data, give you new perspectives, and enrich your data.\nBe critical and reflect upon your own data\nYour data will not only never reach perfection in terms of data (and metadata) quality – as most cultural heritage collections also include differing amounts of problematic data, reflection on biases and issues like cultural appropriation and alienation is a must. Dr Andrea Wallace from the University of Exeter explained how open licensing can in some cases be harmful: Sharing objects online with an open license that came to your collection under difficult circumstances (for example colonialism or theft) might violate the intentions and wishes of original owners or communities (again). And Sandra Fauconnier, Wikimedia Foundation, explained how movements and projects such as Wikimedia Commons or Wikidata deal with biases that arise from predominantly Western data contributions to for example “Sum of all paintings“ that is meant to display cultural heritage worldwide.\nCreate, fail, learn, repeat\nSeveral speakers pointed out that there needs to be more space for experimenting in open GLAM projects. Trying things out and reiterating when necessary is crucial when you want to discover new ways of adapting technologies and reaching your goals. Although most project funding does not encourage failure and finding new, unexpected ways – being generous with mistakes and product ideas that failed is vital. Be open about the things that did not work out as you expected, so others can learn from them – and don’t feel alone in moments of unexpected need for reorientation.\nIn a nutshell: The bigger revolution lays ahead\nOpenness is not enough – that is my conclusion from the Sharing is Caring conference in Stockholm. Or as Dr Karin Glasemann, one of the organisers and Digital Coordinator at the Nationalmuseum, summed it up: “Choosing an open license and releasing your collection is only the start for a much bigger revolution that comes afterwards.“ You have to continue working on and with your data, build sustainable partnerships, reach out to your communities and audiences, enable re-use and be critical on the biases in your collection. The good thing is: The community around the open GLAM movement is there to help; reach out to us and ask for help or feedback.\nFurther information on the conference page (this is also where you are going to find recordings of the conference).\nFollow the discussion under the hashtag #ShareCareX on Twitter, including my more detailed notes in this thread.', 'Samsara of All Things: Boundless in Art and Design between East and West\nThe theme comes from two thoughts: one is “All things are equal”, originated from Zhang Zai’s ideology, who was a thinker from Song Dynasty, thinking that all people and all things between heaven and earth are our fellows and we should treat other people like brothers, and love things just as we love people; and the other comes from samsara, the transmigration idea from Indian Buddism, which holds the belief that the subject of life take turns in different spheres of existence, and the wheel of death and rebirth is almost endless.\nBased on the above two ideas, we believe that samsara is not only meant for living things, but also for non-living things. Human beings and things are equal, and we should be kind to all things, which could be anything in the material world, and the nonmaterial including cultures, concepts, and skills, etc. Art and design can achieve the reconstruction and transformation of ancient cultures, concepts and skills, etc. That is what we call samsara of all things, and the theme for the 2019 Arts Bridge forum at Frankfurt Book Fair.\nAll the explorations and practices by Xu Bing (Artist of the Year), Jamy Yang (Designer of the Year), Zhu Zheqin’s KANJIAN (Honour of the Year), and Xu Gang (Manufacturer of the Year) verify such a theme.\nXu Bing’s “new” Chinese characters and “new” English characters are the art recreation of characters, and such spirit is also embodied in his exploration of 3D books.\nJamy Yang has integrated Chinese philosophical thinking and humanistic concepts into the industrial design of buildings, furniture, daily objects, etc. His design expresses his goodwill to human nature.\nMs. Zhu Zheqin has been devoted to the field investigation of music and folk art in China’s remote and minority areas for many years. She uses KANJIAN, the brand she has founded, to retain the traditional folk art in a modern design way and make it presented in a brand new way.\nThe BENTU team, led by Xu Gang, treat all things equally, and make true the modern reconstruction of waste materials by walking among ruins and wastes of agricultural civilisation and pre-industrial relics, thus realising samsara of all things in the secular world.\nAll these Chinese positions on art and design will be presented, and linked to Western traditions.\nOrganisers: ACC Art Books, Guangxi Normal University Press (Shanghai) Co., Ltd., Museum Angewandte Kunst, Frankfurt a.M. Hochschule für Gestaltung Offenbach a.M., Institute for Design Exchange (IDEe) and Northwestern Polytechnical University Mingde College Moderators: Wang Zijian (from China) and Prof. Dr. Klaus Klemp (from Germany)\n(The conference language is English)\nIn the first half\n- Speech by Prof. Xu Bing (artist) 10 mins\n- Speech and performance by Axel Malik (artist) 10 mins\n- Dialogue between Xu Bing and Axel Malik 10 mins\n- Speech by Ms. Zhu Zheqin (crossover artist) 10 mins\n- Speech by Prof. Matthias Wagner K (experimental curator) 10 mins\n- Dialogue between Zhu Zheqin and Matthias Wagner K 10 mins\n- Tea break with brief cold buffet (cakes, tea, coffee, fruits, etc.)\nIn the second half\n- Speech by Mr. Jamy Yang (industrial designer) 10 mins\n- Speech by Mr. Xu Gang (product designer) 10 mins\n- Speech by Prof. Dr. Klaus Klemp (design theorist) 10 mins\n- Dialogue between Mr. Jamy Yang, Mr. Xu Gang and Prof. Dr. Klaus Klemp\n- Resume by Prof. Dr. Klaus Klemp\n- Come together\nTotal time: max. 3 hours\n17th October, 2019, 2 p.m. - 5 p.m.\nMuseum Angewandte Kunst, Frankfurt am Main']	['<urn:uuid:8f6468cd-1971-4df4-a5f2-b4b56b1f2cca>', '<urn:uuid:a0d1bb7e-24e7-46a6-b2c7-56e35d2465b4>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	22	54	1525
54	What material covers tin-free steel surfaces?	Tin-free steel (TFS) surfaces are covered by a very thin mixture of metallic chromium and chromium oxide in a single layer.	['Hello, everyone. Last month we had Part II of a five-part series on the metals used to fabricate spray packaging. This month, we continue with a discussion of tin-free-steel, more commonly referred to as TFS.\nSheets of TFS have a similar chemical composition to that for the steel used to make ETP (tinplated steel). However, the surface treatment for TFS is much simpler, as shown by the diagram in Figure 1. Notice that the surface of TFS is essentially only one layer of a very thin mixture of metallic chromium and chromium oxide.\nTFS is commonly used to fabricate food containers, but there are steel aerosol containers that are also fabricated with TFS.\nThere is an old axiom that says polymer coatings bond more strongly to chromium than to tin. However, I’ve not found any referenced technical publications that either prove or disprove this saying.\nThe corrosion rate-pH behavior for both TFS and ETP are determined by the base steel and chemical composition of a formula. Figure 2 provides a corrosion rate graph for steel as a function of pH magnitude. This graph is a very old one and was generated for a steel alloy similar to that used for aerosol containers. The solution for this graph is water.\nThe interpretation of the graph in Figure 2 is complex and often leads to the incorrect conclusion that corrosion is prevented by high pH. Indeed, the curve in Figure 1 is the basis for the corrosion myth that increasing pH stops TFS and ETP aerosol container corrosion.\nFor example, notice in Figure 2 that when the pH is 13 the corrosion rate is approximately 50 millimeters per year—a very high corrosion rate. The service lifetime for a 2Q aerosol container with this high corrosion rate would be approximately two days. In other words, even though increasing pH does decrease the corrosion rate, it might not decrease the rate to a low enough magnitude to result in a commercially acceptable container service life.\nOther ingredients in a formula could change the actual pH range that produces low corrosion, produce significantly higher or lower corrosion rates from those in Figure 2 at any given pH, and change the shape of the pH-corrosion rate curve altogether. In other words, Figure 2 only illustrates that steel corrosion rates are a complex function of pH\nConsequently, changing a formula’s pH should be accompanied with corrosion tests to determine if the pH-change will increase or decrease TFS container and ETP container steel corrosion.\nNext month we’ll continue the series with a discussion of stainless steel.\nPair O Docs has a state-of-the art electrochemical corrosion testing laboratory; please contact me if you would like to know more about our faster and predictive corrosion testing. You can also visit our new website which has a short Vision Video that discusses all our corrosion prevention and control services. Please also contact me if you would like to a have our Elements of Spray Package Corrosion short course taught at your R&D facility. Back issues of Corrosion Corner are available on CD-Rom. Thanks for reading and I’ll see you in December. SPRAY']	['<urn:uuid:15953c93-2cc9-4948-8370-ce947573b69e>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	6	21	521
55	I'm curious about wedding rings - what do they mean?	Wedding rings symbolize the love between the two people getting married. They are significant because they have no beginning and no end. They also serve as a concrete symbol of the promises the couples have made to each other on their wedding day.	['Necessarily, a pledge is a “solemn promise”. Conventional wedding celebration vows, after that are the set of solemn promises that each participant of a couple makes to the various other on their special day. But what do the standard wedding event swears include? Please continue reviewing to discover extra.\nNon-Denominational Conventional Wedding Celebration Swears\nThe words that a lot of us know with that start off a conventional wedding ceremony, being, “” A lot precious, we are gathered together here in the view of God, as well as despite this congregation, to join together this Guy as well as this Female in holy matrimony” can be discovered in the Anglican Book of Common Prayer. This source of really typical wedding event pledges go back to 1662.\nA few of the words spoken on a pair’s big day are very comparable to the Christian wedding ceremony from the Middle ages Period. The portion of the ceremony where the couple answers what is referred to as the “Inquiry of Objective” falls within this group.\nThe Question of Intention is the section of the ceremony where everyone (the groom being asked initially) is asked whether they take the various other to be his or her legal joined other half or wife. The Inquiry proceeds with each party to the marital relationship being asked whether they guarantee to love, convenience, honor, and keep the various other for better or even worse, for richer or poorer, in health issues and in health.\nThis is also the part of the event where the couple are asked whether they will be faithful to every various other so long as they both will live. The typical response is to state “I do”. The couple is then pronounced husband and wife.\nIn addition to the standard wedding pledges, a wedding typically consists of either the giving of a ring to the bride-to-be or the exchange of rings by the couple. The ring or rings are blessed and after that exchanged. It is throughout this part of the ceremony that the visitors will hear the typical wedding celebration vow, “With this ring, I Thee wed”.\nThe practice of the wedding celebration ring is based on the concept that the wedding celebration ring is the icon of the love in between both people. The ring has no beginning and also no end. On top of that, they are a concrete sign of the promises the couples have made to every other on their wedding day.\nThe Concern of “Obey”.\nThe standard wedding pledges for the bridegroom consist of the phrase “to love and cherish” until parted by fatality. For ladies, the typical wedding pledges were a little various and specified that the bride-to-be assured to “enjoy, cherish, and comply with” her husband.\nThis is now considered outdated by many individuals therefore in the majority of typical wedding, both the couple make the same swears to every various other, and also the concept of the new bride “following” her hubby is deserted.\nOne remarkable exception to this practice was the Duchess of York. When Sarah Ferguson wed Britain’s Prince Andrew, she chose to assure to obey her spouse. This part of the marriage ceremony would appear to be part of an older practice in which the hubby became the new owner of any property or belongings possessed by the bride before her marital relationship. He would likewise have single custodianship of any kind of kids they would certainly have in the occasion that the marital relationship broke down in the future.\nThe Civil Wedding Ceremony.\nIn a civil wedding ceremony, the vows are comparable but rather than being asked questions, each person makes statements for themselves. Everyone assures to like and take care of the various other so long as they both shall live.']	['<urn:uuid:8a9b8bd3-fa1f-4008-83d1-c1b8374fe10c>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	10	43	632
56	financial data breach compliance penalty impact	Data breaches involving financial information can have severe consequences. As shown in the AMP Global Clearing case, unauthorized access to customer financial data resulted in a $100,000 fine from the CFTC. Beyond regulatory fines, organizations face multiple impacts including: increased transaction costs, damage to customer trust and reputation, potential legal action from affected consumers and regulators, account closure by payment processors, mandatory security remediation procedures, increased monitoring and auditing requirements, and higher cybersecurity insurance premiums. Additionally, organizations may face ongoing costs from remediation activities, legal complexities, and lost business opportunities due to customer dissatisfaction.	"[""Last week, a futures commission merchant settled an enforcement action brought by the Commodity Futures Trading Commission, claiming that it failed to supervise a third-party technology provider it engaged to implement “critical” elements of the FCM’s information system security program. As a result of the breakdown, claimed the CFTC, an unauthorized individual improperly infiltrated the FCM’s technology system and copied files containing customers’ records and private information. Unrelatedly, a Chicago-based trader was criminally charged in a federal court in Chicago with fraud for misappropriating his employer’s cryptocurrencies.\nOn February 12, AMP Global Clearing LLC, a CFTC-registered FCM, agreed to pay a fine of US $100,000 to resolve an enforcement action brought by the Commission claiming that it failed to supervise a third party’s implementation of “critical” provisions of its information system security program (ISSP). As a result of this failure, said the Commission, AMP’s technology system was compromised by an unauthorized individual (Infiltrator) who impermissibly copied approximately 97,000 files, including many files that contained confidential personal information.\nAccording to the CFTC, in June 2016, an unnamed IT provider engaged by AMP installed a storage device – known as a network attached storage device (NASD) – on the firm’s computer network to store back-up data. However, the IT provider failed to alert AMP that the NASD had a feature to copy data to and from other NASDs over the Internet and that a data port used by AMP’s NASD to effectuate this functionality was left open by default. This feature could potentially permit permissionless access to AMP’s data from the Internet.\nAMP apparently maintained an ISSP that required assessment of potential vulnerabilities in its computer systems and engaged the IT provider to maintain strict firewall rules and to conduct regular assessments, including of access routes into AMP's network. However, alleged the CFTC, the IT provider “did not identify or perform a risk assessment… in accord with the ISSP” and the potential vulnerability was not detected. Moreover, the IT provider failed to detect this vulnerability during September 2016, December 2016 and March 2017 quarterly network penetration tests, vulnerability scans and firewall audits.\nIn March 2017, the Infiltrator detected AMP’s open data port and the following month, he copied the 97,000 files without detection by AMP. Later in April, the Infiltrator advised AMP of the security breach and the firm reported it to the firm’s customers, the CFTC and the National Futures Association. At about the same time, the Infiltrator alerted federal authorities regarding its unauthorized access and that the information it copied “had been secured, and was no longer in the [Infiltrator’s] possession.”\nPreviously, from December 2016 through March 2017, the Infiltrator “and his colleagues” publicized on blog posts about their unauthorized access to NASDs used by entities other than AMP through data ports also left open by default. At least three of these incidents were reported in the media. However, despite this publicity, the IT provider failed to identify any vulnerability in AMP’s NASD during its March 2017 network security tests or risk assessments.\nNo third party other than the Infiltrator accessed AMP’s customer files through the open data port.\nAccording to the CFTC, AMP’s failure to diligently supervise how its ISSP policies and procedures were implemented and how its customers’ records and information were electronically protected constituted a regulatory breach (click here to access CFTC Regulation 166.3). The Infiltrator was not named as a defendant in the CFTC's action.\nThe CFTC said that AMP’s substantial cooperation in this enforcement action was rewarded by a reduced fine. In addition to paying a fine to resolve this matter, AMP agreed to provide written reports after six months and one year summarizing its efforts to improve the integrity of its computer network and confirming its adherence to the requirements of its ISSP.\n(Click here for a copy of the CFTC’s settlement order in this matter.)\nMy View: Huh? First off, the facts of this CFTC enforcement action read like the plot of a bad cliché television show where the purported hero may have been the villain all along. Apparently, prior to compromising AMP’s data files, the Infiltrator may have alerted AMP regarding its system’s vulnerabilities. Why? What was going on? Was the Infiltrator making a bid to be hired? Was the Infiltrator a non-hired vendor scorned? There are many questions not answered by the Commission’s settlement order. It appears, however, at a minimum, AMP may not have acted on the Infiltrator’s tip.\nThe oddity of this enforcement action aside, the message of this case is quite disturbing. Even when a registrant develops and institutes a reasonably sound ISSP and employs a responsible third party to administer it in recognition of its own lack of technical acumen, it may be held liable by the CFTC if the third party fails to detect a system flaw and act on it promptly.\nThis standard imposes an incredibly harsh burden on registrants where they may not be technologically savvy and must (and should) rely on the assistance of a qualified third party.\nMoreover, the CFTC’s approach seems to run directly counter to a 2016 guidance issued by the National Futures Association requiring members to develop and maintain ISSPs. Although members must maintain ISSPs “reasonably designed to diligently supervise the risks of unauthorized access to or attack of their information technology systems, and to respond appropriately should unauthorized access or attack occur,” the NFA recognized that one size does not fit all. According to the self-regulatory organization,\nNFA recognizes that given the differences in the type, size and complexity of operations of Members’ businesses including but not limited to their customers and counterparties, markets and products traded, and the access provided to trading venues and other industry participants, Members must have an appropriate degree of flexibility to determine how best to diligently supervise information security risks.\n(Emphasis added. Click here to access NFA Interpretive Guidance 9070, Information Systems Security Programs.)\nThis NFA approach is consistent with guidance provided by the CFTC’s own Division of Swap Dealer and Intermediary Oversight in 2014 that likewise recognized that\nEach covered entity should develop, implement and maintain a written information security and privacy program that is appropriate to its size and complexity [and] the nature and scope of its activities, and which requires it to, at a minimum [address certain enumerated elements].\n(Emphasis added. Click here to access CFTC Staff Advisory 14-21, Graham-Leach Bliley Act Security Safeguards.)\nHowever, through this enforcement action and settlement, the CFTC seems to be suggesting that there may be only one way for a registrant to manage the risk to its data infrastructure: hands-on, by itself, no matter how unqualified it assesses itself to be for such a task. As a result, that one way may be impractical for all but the largest organizations with the deepest technology staff.\nThis is now the second enforcement action brought and settled by the CFTC within the past six months where a registrant was held liable for failure to supervise when the registrant expressly engaged a third party to assist it to detect potential regulatory problems when it believed it lacked expertise, and the third party apparently did not fulfill its objective. (Click here for details of this other enforcement action in the article “Two Commodity Pool Operators Charged by the CFTC With Failure to Supervise “ in the October 1, 2017 edition of Bridging the Week.)\nCompliance Weeds: Since March 1, 2016, every NFA member FCM, retail foreign exchange dealer, commodity trading advisor, commodity pool operator and introducing broker is required to maintain a formal written ISSP that, among other things, establishes a government framework “that supports informed decision making and escalation within the firm to identify and manage information security risks.”\nISSPs must also require assessment and prioritization of the risks associated with the use of information technology systems; the deployment of safeguards against identified threats and vulnerabilities; and implementation of a formal incident response plan to respond and recover from cyber-breaches.\nEmployee training and the risks posed by critical third-party service providers that access a member’s system or provide outsourcing must also be addressed in an ISSP.\nA relevant member’s chief executive officer, chief technology officer or other executive-level officer should approve its ISSP. Moreover, “sufficient information” should be provided about the ISSP to a relevant member’s board or governing body (or delegated committee) “to enable it to monitor the Member’s information security efforts.” NFA contemplates that a member that is part of a group may comply with its ISSP requirements through participation in a consolidated entity ISSP. An NFA member must retain all records related to its adoption and implementation of an ISSP in accordance with ordinary CFTC recordkeeping requirements.\nISSPs should be regularly monitored by NFA members, and ISSPs’ effectiveness should be reviewed at least once every 12 months by either in-house staff with appropriate knowledge or an independent third-party specialist.\nA criminal complaint was filed against Joseph Kim on February 15 for allegedly misappropriating Bitcoin and Litecoin – two virtual currencies – from his former employer, Consolidated Trading, LLC, a proprietary trading firm; Franklin & Wacker, LLC, an affiliate; and the two firms’ principals. Mr. Kim was charged with committing wire fraud.\nAccording to the Complaint, which was filed in a federal court in Chicago, Mr. Kim was hired by Consolidated as an assistant trader in July 2016. In September 2017, Mr. Kim was transferred by Consolidated to a newly established Cryptocurrency Group at Franklin.\nShortly after this move, alleged the Complaint, Mr. Kim transferred 980 Litecoins from Consolidated’s account at Bitfinex – a non-US spot virtual currency exchange – to his own account. When this transfer was discovered by a Consolidated director, Mr. Kim indicated that “he moved these funds to his personal digital wallet for safety reasons.” Mr. Kim purportedly made similar misleading comments to other of Consolidated’s management regarding the location of the Litecoin until Mr. Kim’s alleged misappropriation was uncovered on approximately November 28, said the Complaint.\nSimilarly, the Complaint claimed that, on November 17, 2017, the same Consolidated director discovered that 55 Bitcoin were missing from a Consolidated account at Bithumb – another non-US cryptocurrency exchange. In response to the same director’s inquiry, Mr. Kim claimed that he was taking steps to unlock the virtual currencies that had been blocked by Bithumb. Later in November, Mr. Kim returned 27 Bitcoin to Consolidated’s Bithumb account. Within a few days, however, Mr. Kim transferred more Bitcoin from his company’s to his own account, returned some, and lost some Bitcoin through personal trading.\nOverall, the Complaint alleged that Mr. Kim withdrew from company accounts and transferred to his own accounts without authorization Bitcoin and Litecoin, such that Consolidated sustained an overall US dollar loss in excess of US $600,000.\nAlthough the Complaint indicated that Consolidated maintained written policies regarding employee trading of securities and futures, these policies did not address cryptocurrencies. However, the Complaint indicated that Mr. Kim was expressly told by a Consolidated director that he could not engage in personal trading in cryptocurrencies consistent with the firm’s policy for all traders for other financial instruments. Mr. Kim supposedly agreed to comply with the instruction but, in fact, he did not comply.\nAccording to the Complaint, after his alleged misappropriation was discovered, Mr. Kim wrote, “It was not my intention to steal for myself from [Consolidated] and until the end I was perversely trying to fix what I had already done.” The Complaint also alleged that Mr. Kim told another trader at Consolidated that he was a “degen,” a slang term the trader understood to mean a degenerative gambler.\nIf convicted, Mr. Kim faces imprisonment of up to 20 years.\n(Click here for a copy of the criminal complaint against Mr. Kim.)\nCompliance Weeds: If they have not already done so, registered financial services firms and proprietary-trading entities should consider whether they should amend existing employee personal trading polices to expressly address cryptocurrencies. This may be appropriate even if such firms are not engaged in cryptocurrency activities today.\nThe easiest approach would be for firms to ban all personal cryptocurrency trading by employees because of reputational or other perceived risks. However, such a policy may impede hiring or retention of some employees, especially so-called “millennials.”\nAlternatively, if firms already have policies addressing employees’ trading of securities, including participation in new offerings of securities, it might be appropriate to consider extending these policies to digital tokens issued as part of initial coin offerings that the Securities and Exchange Commission has said are likely securities. (Click here for background regarding the SEC’s views in the article “SEC Chairman Warns Lawyers Providing ‘It Depends’ Advice on ICOs” in the January 28, 2018 edition of Bridging the Week.)\nMoreover, to the extent firms have existing polices addressing employees’ trading of gold or similar commodities, they may wish to extend such policies to employees’ trading of virtual currencies like Bitcoin or Litecoin.\nHowever, because of the SEC’s views, it is not definitively clear today what is the bright line between virtual currencies and security tokens.\nFirms that engage in cryptocurrency activities should consider the potential impact of employees front-running firm or a firm’s customers’ trading or engaging in other wrongful conduct. Firms not engaged in cryptocurrency activities but contemplating engagement should consider the potential implications of employees purchasing virtual currencies in advance of any firm announcement with the expectation that the announcement might cause prices of relevant cryptocurrencies to rise.\nThe monitoring of employee cryptocurrency activity may also be difficult as cryptocurrency exchanges may not be willing or able to provide statements of employee activity to employers automatically. At best, it may be up to an employee to authorize such third-party transmissions that he or she could activate or deactivate at his/her discretion.\nThe information in this article is for informational purposes only and is derived from sources believed to be reliable as of February 10, 2018. No representation or warranty is made regarding the accuracy of any statement or information in this article. Also, the information in this article is not intended as a substitute for legal counsel, and is not intended to create, and receipt of it does not constitute, a lawyer-client relationship. The impact of the law for any particular situation depends on a variety of factors; therefore, readers of this article should not act upon any information in the article without seeking professional legal counsel. Katten Muchin Rosenman LLP may represent one or more entities mentioned in this article. Quotations attributable to speeches are from published remarks and may not reflect statements actually made."", ""IDC Names Securiti a Worldwide Leader in Data PrivacyView\nPayment Card Industry Data Security Standard Compliance, better known as PCI DSS Compliance, a term which at first glance may seem daunting, is a critical framework that ensures the security of sensitive financial data. In a landscape rife with cyber threats and data breaches, understanding PCI DSS Compliance is paramount for organizations that handle credit card transactions.\nPayment Card Industry Data Security Standard, commonly known as PCI DSS, is a set of security standards and guidelines designed to ensure the secure processing, storage, and transfer of payment card data, including debit and credit card data. The two key objectives of PCI DSS compliance are protecting sensitive cardholder data and minimizing the likelihood of financial fraud and data breaches.\nInstead of being a one-size-fits-all strategy, PCI DSS Compliance is a flexible framework that organizations can customize to meet their unique requirements. PCI DSS is developed and upheld by the Payment Card Industry Security Standards Council (PCI SSC) and applies to all entities that handle credit card data, ranging from small online retailers to multinational organizations.\nPCI DSS certification refers to complying with several specific requirements and standards designed to ensure the secure handling of payment card data. The current version of the PCI DSS is PCI DSS v4.0.\nCybercriminals actively target digital transactions containing credit card data, and PCI DSS attempts to reduce such evolving security risks. These threats include:\nThreat: Unauthorized access to cardholder data while it's being processed, stored, or transferred.\nPCI DSS Requirement: When transmitting cardholder data over open, public networks, ensure data is encrypted.\nThreat: Malicious software with the ability to steal cardholder data and jeopardize system security.\nPCI DSS Requirement: Utilize antivirus software and update it regularly.\nThreat: Attempts made fraudulently to assume the identity of reliable organizations to obtain sensitive data.\nPCI DSS Requirement: Establish policies in place to protect against phishing attempts, such as training and awareness campaigns for employees.\nThreat: Inadequate authentication methods that may lead to unauthorized access.\nPCI DSS Requirement: Use strong authentication methods, such as multi-factor authentication, to protect system access.\nThreat: Weaknesses in network security that can be exploited to gain unauthorized access.\nPCI DSS Requirement: Regularly monitor and test networks and implement security measures like firewalls to protect cardholder data.\nThreat: Inability to promptly detect and respond to security incidents.\nPCI DSS Requirement: Implement robust logging and monitoring systems to track and alert security events.\nTokenization can be used to reduce the risk involved with maintaining actual card data by substituting non-sensitive tokens for sensitive cardholder data.\nFrom the point of interaction, such as a card swipe, until it reaches the payment processor, use P2PE to encrypt card data.\nConduct security audits and risk assessments to identify vulnerabilities and ensure PCI DSS compliance.\nThe PCI DSS security standards are implemented by an alliance of major credit card organizations, including Visa, Mastercard, American Express, JCB, and Discover, to ensure that every organization that accepts credit cards does so in a secure environment. Your organization may be classified into one of four PCI categories based on the annual volume of card transactions you process:\nPCI Level 1: Businesses processing over 6 million transactions per year\nPCI Level 2: Businesses processing 1 million to 6 million transactions per year\nPCI Level 3: Businesses processing 20,000 to 1 million transactions per year\nPCI Level 4: Businesses processing less than 20,000 transactions per year\nOrganizations must go through several phases and activities in the PCI DSS assessment process to ensure PCI DSS compliance. The following essential components are usually included in the process:\nEach system and procedure that handles, transmits, stores, or processes sensitive authentication data (SAD) and cardholder data (CHD) must be identified and recorded. Additionally, the cardholder data environment (CDE) must be determined to accurately assess the scope of PCI DSS compliance.\nGet familiar with the PCI DSS standard's 12 core requirements and accompanying sub-requirements, and recognize the specific security controls and practices that are essential to meet each requirement.\nSAQ types differ according to the processing, storing, and sending of cardholder data methods. Identify the relevant Self-Assessment Questionnaire (SAQ) or, in the case of Level 1 merchants, submit a Report on Compliance (ROC) evaluation.\nExamine your organization’s security controls with PCI DSS requirements to identify any vulnerabilities in the current security framework.\nTo comply with PCI DSS requirements, patch identified gaps by implementing security controls and measures. Develop and implement a remediation strategy to resolve vulnerabilities and enhance security.\nCreate security policies and procedures that align with PCI DSS requirements, document them, and ensure employees receive adequate training.\nEstablish a system for routine security testing and vulnerability assessments, as well as methods for continuous monitoring to identify and resolve security events swiftly.\nConduct the SAQ or ROC assessment in compliance with the specified parameters and relevant laws.\nProvide the acquiring bank and card brands with the completed SAQ or ROC and any necessary supporting documentation. Assure prompt submission and adherence to reporting requirements.\nEstablish and execute plans to identify and resolve any non-compliance concerns and collaborate with the QSA or Internal Security Assessor (ISA) to validate remediation efforts.\nMaintain accurate records of all security assessments, processes, policies, and remediation activities. These records would also come in handy for audit purposes and demonstrating ongoing compliance.\nEstablish procedures for continuous security control testing, monitoring, and evaluation. Additionally, to handle new threats and landscape changes, evaluate and update security measures on a regular basis.\nKeep track of compliance status by communicating with acquiring banks and payment card companies and respond to the credit card companies' requirements for more details or actions.\nReevaluate and validate compliance with PCI DSS requirements to renew the compliance status.\nInstall and maintain network security controls by employing strong firewalls, intrusion detection systems, and encryption methods to prevent data breaches and cyberattacks.\nApply secure configurations to all system components by changing default passwords, eliminating unnecessary software, functionalities, and accounts, and deactivating or uninstalling unnecessary services to reduce the possibility of compromising the system.\nProtect stored account data using encryption, truncation, masking, and hashing. Employ risk-reduction strategies such as avoiding holding account information unless absolutely essential, truncating cardholder data when the entire PAN is not required, and refraining from providing unprotected PANs via end-user messaging platforms like email and instant messaging.\nProtect cardholder data using strong cryptography keys during transmission over open and public networks. This increases the likelihood of data secrecy, integrity, and non-repudiation. Any transmissions of cardholder data through a network that stores, processes, or transmits cardholder data are immediately subject to PCI DSS. Such networks must be evaluated and assessed to comply with the applicable PCI DSS regulations.\nTo protect all systems and networks from malicious software, malicious software or firmware must be identified and eliminated. Examples of malicious software include viruses, worms, Trojans, spyware, ransomware, keyloggers, rootkits, malicious code, scripts, and links.\nDevelop and maintain secure systems and software to prevent security vulnerabilities that can be exploited to gain privileged access to systems. Organizations must routinely update their software components via the necessary software patches to ensure no software intrusion.\nRestrict access to system components and cardholder data by business need-to-know to ensure that only authorized individuals gain access to data. These requirements apply to user accounts and access for employees, contractors, consultants, internal and external vendors, and other third parties.\nTwo fundamental principles of identifying and authenticating users are to establish the identity of an individual or process on a computer system and prove or verify the user associated with the identity is who the user claims to be.\nThe element used to prove or verify the identity is known as the authentication factor. Authentication factors include something you know, such as a password or passphrase; something you have, such as a token device or smart card; or something you are, such as a biometric element.\nRestrict physical access to systems that store, process, or transmit cardholder data since it enables individuals to access and/or remove systems or hardcopies containing cardholder data.\nLog and monitor all access to system components and cardholder data to prevent, identify, or mitigate the effects of a data compromise. Logs are present on every system component and in the Cardholder Data Environment (CDE), enabling full monitoring, notification, and analysis if something goes wrong. Without system activity logs, it is difficult, if not impossible, to identify the cause of a compromise.\nTo ensure that security policies continue to take into account the ever-evolving environment, system components, processes, and customized and custom software should all undergo regular testing.\nThe overall information security policy of the organization establishes the tone for the entire organization and specifies what is expected of the employees. Every employee should understand the sensitivity of cardholder data and the need for protection.\nOrganizations face several obstacles in their quest for PCI DSS compliance, including:\nTo overcome these challenges, organizations must actively engage in strategic planning, demonstrate PCI DSS compliance commitment by implementing required security measures, and utilize PCI DSS compliance to negotiate the complex landscape of credit card security regulations.\nOrganizations that violate PCI DSS requirements may face dire repercussions, including:\nThere could be serious penalties from card companies and regulatory agencies. The fine amount depends on how serious the infraction was and how many cardholder data records were stolen.\nOrganizations that do not comply may be subject to higher transaction costs and extra scrutiny by payment processors and acquiring banks. This could result in processing payments with higher operational costs.\nA non-compliance-related data breach may damage customer trust. Consumers may become less confident in the organization’s ability to protect their personal data, which could harm its reputation and result in lost revenue.\nNon-compliance may initiate legal action, such as a lawsuit from impacted consumers, regulatory authorities, and payment card brands. Settling such lawsuits can drain an organization’s financial resources and dent its reputation.\nThe merchant accounts of organizations that do not comply may be closed by acquiring banks and payment processors. Revenue streams may be impacted if this interferes with the company's capacity to accept credit card payments.\nRegulatory agencies or payment card brands may require certain security remediation procedures. Organizations that don't comply may have to spend additional financial resources on security enhancements.\nIf there is a data breach, the organization can be held liable for the expenses associated with investigating and remediating the incident. This entails conducting legal investigations, informing those impacted, and implementing remedial action plans.\nRegulatory agencies and credit card companies may beef up their monitoring and auditing of non-compliant organizations. This might strain internal resources and interfere with regular corporate operations.\nOrganizations that do not comply with PCI DSS compliance risk missing out on opportunities to work with other organizations that prioritize security since compliance is frequently a requirement for collaboration.\nOrganizations may find it difficult to obtain cybersecurity insurance or may see higher premiums as a result of greater perceived risk if they do not comply with PCI DSS.\nFor non-compliant organizations, regulatory agencies and payment card companies may mandate more frequent and stringent compliance assessments, imposing a continuous cost on the firm.\nThe costs and efficiency of an organization can be negatively impacted by remediation activities and the repercussions from non-compliance, which include legal complexities and customer resentment towards the organization.\nPCI DSS compliance significantly benefits organizations that process credit card transactions. These include:\nOrganizations can implement strong security measures designed to reduce vulnerabilities to safeguard sensitive payment card data, reducing data breaches, fraud, and other security risks.\nIn the event of a data breach, non-compliance with PCI DSS may result in fines and penalties. Businesses can prevent these financial implications by achieving compliance.\nPCI DSS compliance may be required by law in certain regions. Being PCI DSS compliant ensures that the company is in good legal standing, preventing unforeseen legal complexities.\nOptimizing data security practices is frequently necessary to ensure PCI DSS compliance, ultimately resulting in more cost-effective and effective operations.\nMajor card networks like Visa and MasterCard require PCI DSS compliance to process payments, enabling companies to process payments swiftly without interruptions.\nPCI DSS compliance demonstrates a commitment to security, which protects and enhances an organization’s reputation and improves customer trust.\nCompliance with PCI DSS gives your company a competitive edge over competitors who might not be compliant.\nIndustry leaders embrace and mandate merchant compliance as failure to comply with the PCI DSS can lead to security lapses and the loss of sensitive credit card data, which can result in severe penalties and other legal consequences.\nIt is imperative for organizations to determine what type and level of encryption exists in their systems today. With a lack of visibility, it is essential for organizations to embrace automation and gain a holistic view of their data.\nSecuriti Data Command Center can scan, classify and keep encryption types in a data graph that is continuously updated and auditable. That visibility generates a prioritized work list for risk inventories and remediation plans.\nRequest a demo now to learn how Securiti can help improve compliance with PCI DSS v4.0.""]"	['<urn:uuid:183eae06-da8b-4af0-9bc3-8365d7a9571e>', '<urn:uuid:a9acb6e1-37c4-4bad-b932-4578a90edb0e>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T00:57:18.859842	6	94	4597
57	need info timeline first europe settlements brazil development major cities from discovery until today	Brazil was discovered by Portuguese admiral Pedro Álvares Cabral in 1500 when his fleet landed in Porto Seguro. The first European settlements were established in two captaincies: Pernambuco and São Vicente. In 1514, the Portuguese landed on Santa Catarina Island, though permanent settlement didn't occur until Spain established a colony there in 1542. Olinda became the de facto capital initially, followed by Recife which grew due to its deeper port. Salvador was later declared the colonial capital and flourished in the 17th century. The discovery of sugar, coffee, and gold led to expansion into cities like Paraty, Rio de Janeiro, and mountain cities like Ouro Preto. São Paulo, despite being the current financial capital, only developed significantly in the early 20th century, marked by the construction of mansions on Avenida Paulista and later architectural developments in the 1950s and 60s.	"['Thursday, March 1st, 2012\nDivers have discovered a Spanish shipwreck from 1583 off the coast of Brazil’s southern Santa Catarina state. Although they haven’t found any remains including the name of the ship, they did find a triangular plaque engraved with the date 1582 and King Philip II of Spain’s name. They also discovered a stone shield bearing the heraldic symbol of the Iberian Union, the dynastic union of the monarchies of Spain and Portugal that united the entire Iberian peninsula south of the Pyrenees under a single crown (the Spanish one).\nArchaeologists believe that the triangular plaque would have been used as a marker to claim any newly discovered lands. The Iberian Union was established in 1580. Contemporary documents reference a ship going down on January 7, 1583 off the coast of Santa Catarina, so even without the ship name there is solid evidence that the wreck dates to the early 1580s. If the 1583 date is confirmed, this will be the oldest shipwreck ever found in Brazil.\nThe recovered pieces and the documentary review indicate the wreck was a supply ship for a fleet that left Spain in 1581 on a mission to build two forts on the Strait of Magellan to stymie the advance of English pirates menacing Madrid’s territories in the New World.\nStarting March 14th, divers will return to the site to bring back as many artifacts as they can find, visibility and currents depending. On previous dives they saw a cannon, ballast stones, cannonballs and ceramic fragments which the team hopes to recover on the upcoming underwater expedition.\nThe Portuguese had landed on the Island of Santa Catarina, the island which today contains most of state capital Florianópolis, in 1514, but they didn’t colonize it until the next century. It was Spain who first began to use it as a base for trade and further exploration just a decade later.\nIn 1525, Charles I of Spain ordered the friar García Jofre de Loaísa to take a fleet of seven ships and 450 men, sailors plus tradesmen and clerical staff, westward to reach the far east. The expedition’s immodest brief was to cross the Atlantic Ocean, the Strait of Magellan and the Pacific Ocean to reach the Spice Islands (today part of Indonesia). Don Rodrigo de Acuña, captain of the fourth ship, stopped at Santa Catarina Island and left 17 of his crew behind before continuing on towards the Strait.\nI don’t know what their fate was, but it can’t have gone much worse than the Loaísa expedition which shed ships and people like a husky sheds undercoat in the summer. Only four ships reached the Pacific, and only one, the Santa Maria de Parral, reached the Spice Islands in 1526 carrying the captain, Andrés de Urdaneta, and 24 of his men. They were captured by the Portuguese for their trouble but eventually made it back to Spain in 1528, thus completing the second circumnavigation of the globe in history.\nThe first permanent European settlement on Santa Catarina Island was established by Spanish colonists in 1542. The island was used as a supply port by Spanish ships headed towards the Rio de la Plata estuary that today lies on the border between Uruguay and Argentina. The Portuguese took over in 1673, gradually colonizing the island and the mainland coastline. By the late 1670s, the settlement that would become Florianópolis was beginning to grow.', ""|Recife, in Northeastern Brazil|\nby Alexandra Forbes\nThe Portuguese admiral Pedro Álvares Cabral discovered Brazil by accident in 1500. His fleet had set out in search of Indian spices, but favorable winds pushed them towards an exuberant and unknown land. They dropped anchor in the palm-lined bay of Porto Seguro, in what is today the southern part of the state of Bahia. There are still signs of their arrival: the first whitewashed churches in Porto Seguro and in the neighboring Arraial d’Ajuda attract curious tourists to this day.\nThe Portuguese organized their new territory into two captaincies (the administrative order of the time) – Pernambuco, north of Bahia, and São Vicente, in what is now the state of São Paulo – and they flourished. Pernambuco in particular grew and prospered thanks to the sugar trade, and soon the colony’s de facto capital was Olinda – Pernambuco’s main city, perched on a hill for protection. Its highest point is home to the São Salvador do Mundo church, better known as Alto da Sé. Built in 1537 as a testament to the wealth of the sugar magnates, this ornate church was adorned with an intricate design of gold leaf and painted tiles. Even today, the church and its surroundings are the nucleus of Olinda’s tourist hub.\n|Recife's Capela Dourada|\nThe nearby village of Recife boasted deeper waters and a better port, so it soon surpassed Olinda as Brazil’s most prominent city. As Recife’s prosperity grew, the local churches became more luxurious. The Capela Dourada (Golden Chapel) was opened to the public in 1697, taking its name from the tremendous quantity of gold that covers its complex wooden engravings. The chapel is part of the Santo Antônio convent and remains one of the city’s most glorious legacies from this period.\n|Salvador's Elevador Lacerda|\nBut back to Bahia, where a Portuguese king eventually declared the region’s principal city, Salvador, to be capital of the colony. Salvador da Bahia flourished as a result, becoming the most important city in Brazil. Many of its buildings and churches date back to the 17th century and are concentrated in the historic Cidade Alta and Cidade Baixa, the two levels of the city that since 1872 have been connected by the famous Lacerda Elevator.\nAfter Brazil had been established as a major exporter of sugar, and later coffee and gold, the Portuguese colonists began to expand through the rest of the country. Much of the country’s architectural riches from these days can be found in cities like Paraty and Rio de Janeiro, as well as in the mountain cities of Ouro Preto, Tiradentes and Mariana, heart of the gold rush.\n|Paraty, near Rio|\nStrangely, São Paulo, the undisputed financial and cultural capital of the country, has its origins in poverty and neglect. Wealthy industrialists didn’t build large mansions on Avenida Paulista until the early 20th century. Only a few of these palaces have survived, but the avenue remains a symbol of the city’s economic dominance. The most iconic structure is the Museu de Arte de São Paulo (MASP). A large box of glass and cement, held aloft by red pillars, this building was designed by renowned architect Lina Bo Bardi and is an essential stop for any tourist.\n|MASP Museum in São Paulo|\nAround the time when the MASP was constructed, in the 1950s and 60s, São Paulo also enjoyed an architectural boom thanks to the work of architect Oscar Niemeyer. Known primarily for Brasilia, the current capital of the country, Niemeyer’s buildings in São Paulo are just as extraordinary and, most importantly, are easily accessible to most travelers. They include the spectacular Auditório Ibirapuera, the OCA exhibition center and the Museu de Arte Moderna (MAM), which are all located in Ibirapuera Park, as well as the city’s architectural landmark, the COPAN, in the city’s downtown.\nIt’s ironic that São Paulo, once known as a gray and industrial city, now possesses some of Brazil’s greatest architectural gems.\nIn fact, the country’s 500 years of history are most evident the largest cities, like São Paulo and Rio de Janeiro. Each chapter, from the sugar and coffee eras to the discovery of gold, from independence to industrialization and now globalization, has left indelible marks on the urban tapestry. Each new wave of visitors – the Portuguese, African slaves and immigrants from Europe and Japan – has led to new neighborhoods and new architectural styles. The Portuguese may have imposed their tastes and customs over the first two centuries, but today the sheer variety of Brazil’s cities reveals the diversity of its people. In\nIglesia de São Salvador do MundoAlto da Séwww.embratur.gov.br\nSão PauloMuseu de Arte de São Paulo, MaspAv. Paulista 1578Admission: US$8.50www.masp.art.br\nAuditório IbirapueraAv. Pedro Álvares Cabral, (no address number) – Portão 2Tel. 55-11-3629-1075www.auditorioibirapuera.com.br\nOcaParque do Ibirapuera, portão 1 y 2 – (no address number)MAM\nParque do Ibirapuera, portão 3 – (no address number)Admission: US$3.50www.mam.org.br\nCOPANAv. Ipiranga 200, downtownwww.copansp.com.br\nRecifeCapela DouradaRua Imperador Dom Pedro II, 206www.embratur.gov.brSalvador da Bahia""]"	['<urn:uuid:7d73c994-2ef4-4c05-8cff-a5dfde07f7ea>', '<urn:uuid:8af1bc46-7391-44ce-84af-e5583d7a1f0c>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T00:57:18.859842	14	140	1395
58	printing technology differences maintenance requirements commercial home printer comparison	Commercial photo printers primarily use thermal dye technology, where the printer's head evaporates colors from a color band and deposits them on paper, with darker colors resulting from more deposition. Home printers are mostly inkjet, which spray thousands of droplets of ink. Regarding maintenance, inkjet printers typically have more moving parts than thermal printers, increasing the likelihood of breakdowns requiring repair or replacement. Thermal printers, having fewer moving parts, tend to last longer, are easier to maintain, and run more reliably. While home inkjet printers can now compete with commercial lab printers in quality, commercial prints are still recommended for bulk orders or large sizes that home printers cannot handle.	['In our previous article “Post Production For Printing (And Web)” of this series, we went through the techniques involved in post-processing. Once the post-processing is complete, it’s time to prepare your photographs for the printer. Lets go through the various steps involved in printing and also how you can prepare your photographs for the final print.\nDifference Between The Lab & Home Printers\nPrimarily, all commercial photo printers work on the thermal dye technology. This means that the printer’s head evaporates colors from the color band and deposits them on the paper to create prints. More deposition at one place means a darker color. Home printers are mostly inkjet printers. Due to the advent of technology it is possible to get quality prints from your inkjet printer at home which can compete with the commercial lab printers. But it is still advisable to go in for commercial prints when you want to order in bulk or want large sizes that your home printer cannot deliver.\nA calibrated monitor is a primary requirement of the post-processing workflow. This is especially required if you are processing the photographs for prints. This is due to the difference in display characteristics of the two media – the monitor and the paper. The monitor is an artificial source of light whereas the paper is only a reflective surface. Thus the luminance (vaguely brightness) characteristics of the monitor must be closely calibrated to simulate the paper accurately. Professional photographers use monitor calibration equipment to calibrate and profile their monitor. A good start is to set your monitor’s color temperature (white point) to 6500k (or D65).\nMonitor resolution Vs Print resolution\nThe image size displayed on the monitor is not an accurate representation of the final print size that you get. This is because printers resolution is calculated in dots-per-inch whereas monitors resolution is measured in pixels. The size of the pixel varies from monitor to monitor and needs to be calculated into pixels-per-inch. In Photoshop you set the ppi in Edit > Preferences > Units & Rulers > Screen Resolution. Thereafter you can go to View > Print Size to get a pretty accurate representation of the print size. To size of the image is visible in teh info palette or by going to Edit > Image Size > Document Size. Make sure the size of prints you are ordering is close to the document size else it may be scaled or cropped in the lab.\nProfessional photographer insist on asking the lab for their printer profile. This is important because, prints do not come out exactly as they appear on the monitor. That’s because monitors essentially use the RGB color space and the printers use the CMYK color space. Thus a mapping or transformation of image data takes place inside the printer. With the lab printers color profile, photographers are able to soft-proof their photographs (which is a very close representation of the actual print that will come from the lab).\nThough we covered sharpening under post-processing in the last post, sharpening for print is different from the sharpening for web. Sharpening for print is more aggressive because the ink is absorbed by the paper. Also the ink dots projected on the paper ofter bleed into the dots nearby thereby killing the sharpness.\nIdeally TIFF is a desirable file format as it is loss-less. However a fine JPG will do just fine and the differences are so negligible that they can’t be noticed by the bare eye.\nRGB Or CMYK\nWhen it’s time to send the photographs for print, it’s time to decide whether your lab requires the CMYK or the RGB images. Most of the dedicated photo printers have RGB as their working space. To be sure, check with the lab.\nPrinting is a complex process — one which needs to be carefully synced between various data formats, output devices and different color profiles. While this is not an exhaustive step-by-step article to walk you through the entire process, we hope is it a good pointer to get you going in the right direction. In the next and final article in this series we’ll see how to get prints at home with your very own photo printer.', 'Businesses around the world rely on printers every day to print all manner of items, including contracts, receipts, asset management tags, and more. It’s essential that businesses understand and choose the right printer for their specific needs. An inkjet printer, for instance, may print high-quality photos but lack the capabilities to quickly and easily print barcodes. This article will examine inkjet and thermal printers, two of the most common types of printers currently available, with information on the pros and cons of each, typical applications, and resources for choosing your next printer.\nWhat is an Inkjet Printer?\nInkjet printers, as the name implies, use ink to print text, graphics, and images onto various types of paper. These printers are most common in homes and small offices, though many commercial printing companies use inkjet printers to produce high-quality brochures, flyers, and other pieces.\nHow do Inkjet Printers Work?\nInkjet printers spray thousands of droplets of ink onto a piece of paper, where the colors are combined to form an image or text. The colors and sizes of these droplets can be altered to produce sharp, photo-realistic images. Users can look at the advertised dots per inch (DPI) to gauge the printer’s resolution. The DPI measurement tells users how many tiny droplets can fit along a single inch on the page. A higher DPI means that the printer makes smaller individual dots, and a sharper image.\nWhat are the Advantages of an Inkjet Printer?\nThere are many reasons a company might opt for an inkjet printer. Here are some common benefits and advantages of choosing an inkjet:\n- Cost: Inkjet printers, cartridges, and supplies are more widely available—and therefore cheaper—than their thermal counterparts.\n- Image quality: Even low-end inkjet printers can produce nearly photo-quality images.\nWhat are the Disadvantages of an Inkjet Printer?\nInkjet printers aren’t without drawbacks and disadvantages. Here are a few:\n- Image durability: Ink-based images may smudge, stain, and run when exposed to moisture, sunlight, humidity, and routine use.\n- Moving parts: Inkjet printers typically use more moving parts than thermal printers, increasing the chance of breakdowns that require repair or replacement.\n- Reliability: Stickers, signs, labels, and other printed material may tear and peel too often for reliable long-term use.\nWhat is a Thermal Printer?\nUnlike inkjet printers, thermal printers don’t spray liquid ink through a nozzle to produce images. Rather, thermal printers use tiny heating elements to activate or transfer pigments. Thermal printers are used most commonly to create labels, safety signs, way-finding markers, bar-codes, shipping labels, and other heavily-used items.\nHow do Thermal Printers Work?\nThere are two distinct categories of thermal printers: Direct thermal printers and thermal transfer printers. Here’s a quick breakdown of how each works:\n- Direct thermal printers use chemically-treated paper that darkens when heated by a thermal print head. Direct thermal printers do not use separate ink, toner, or ribbon supplies. Direct thermal printers are most commonly used to print items such as receipts and shipping labels.\n- Thermal transfer printers use a thermal print head to transfer a solid ink from a ribbon onto a label supply (usually made from vinyl, polyester, nylon, or other thicker materials) to produce a permanent print. Ribbon is usually made from wax, resin, or a combination of the two, and is bonded to the label supply surface with heat and pressure.\nWhat are the Advantages of a Thermal Printer?\nThere are several reasons to consider a thermal printer for your facility. Here are a few advantages of investing in a thermal printer:\n- Durable: Labels and signs created with thermal printers last longer and stand up to a wider variety of weather conditions than those printed with ink.\n- Versatile: Thermal transfer printers can print documents, labels, and signs that meet various needs, including durable arc flash labels, text- and graphics-based floor marking, and labels meant for extreme conditions such as cold storage.\n- Less maintenance: With fewer moving parts, thermal printers tend to last longer, are easier to maintain, and run more reliably than inkjet printers.\nWhat are the Disadvantages of a Thermal Printer?\nFor all the good that comes with using thermal printers, they aren’t without disadvantages and drawbacks. Here are a few complications that may arise from using thermal printers.\n- Cost: The specialty materials used in the thermal printing process are considerably more expensive than their inkjet counterparts; even the most basic thermal printer may cost hundreds of dollars.\n- Color choice: Thermal printers print fewer colors, and the high heat limits wax and resin choices; both factors limit color and application options. (For these reasons, users cannot print high-quality photographs with thermal printers.) Despite these disadvantages, Graphic Products offers a Custom Label Service, which allows employers to design unique labels with up to four spot colors.\nWhen Should You Use One Printer Over the Other?\nWith so many printer choices and label standards, it can be tough to know when one type of printer is better suited than the other.\nUse an Inkjet Printer for:\n- Office printing: Use an inkjet printer for basic office print jobs, like contracts, manuals, and emails.\n- Photographs: Thermal printers cannot print high-quality photographs, making inkjet printers the ideal solution.\n- Temporary signage: Print short-term notices, announcements, and reminders designed for short-term use (such as a sign informing workers that a meeting has moved rooms).\nUse a Thermal Transfer Printer for:\n- Durable signs and labels: Thermal transfer printers can print signs and labels that stand up to extreme environments and meet OSHA and NFPA requirements, including arc flash labels, marine signage, safety signs, and more.\n- Specialized visual communication: Print magnetized shelving labels, wire wraps, low-light visual communication, and more for applications unique to your facility’s needs.\n- Floor marking: Boost efficiency and safety with custom floor marking signs, which can be modified to create pathways, cordon off areas, and alert pedestrians to forklift traffic.']	['<urn:uuid:2952935a-2458-435a-898f-c152264ec915>', '<urn:uuid:4c5eba40-ea88-46e5-b3b4-ed4cbece3d8e>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:57:18.859842	9	110	1687
59	I'm in the process of buying a new fiberglass boat without bottom paint - what's the most effective type of primer I should use before applying the protective coating?	The best marine primer for bare fiberglass is a 2-part epoxy primer. This coating has been formulated as a hard, strong base for its associated epoxy and polyurethane paint system.	['Primers are the unsung heroes of the boat painting world. Some primers adhere to the underlying hull surface, enabling the topcoat to achieve a sound mechanical bond with the primer. In other cases, these unseen paint coatings separate the old topside paint from the new, preventing an adverse chemical reaction.\nOther marine primers are designed as build-up coatings that seal and hide repairs. Higher solids primers can even fill small scratches. These primers can be sanded for a smooth, paintable surface.\nAlthough the actual boat primer paint application is a straightforward process, thorough preparation is key to outstanding results. Performing an in-depth examination of the current paints condition is an essential first step.\nIf the existing coating is peeling, cracking, or shows large-scale separation, remove the entire coating down to the bare surface. Sanding, scraping, grinding, or paint stripping are good options for paint removal. Once you have a clean bare surface, proceed using the steps below.\nAs with topside paint, you can use a roller, brush, or sprayer to apply primer to your boats hull. Add thinner as needed to create the right viscosity. This is especially important for spraying applications.\nYes, you should prime your boats gelcoat before painting it. If the gelcoat is in generally good shape, and shows no major crazing or cracking, the process is relatively simple.\nRemove all debris and contaminants from the gelcoats surface. Next, apply a coat of primer. Carefully sand the primer, and then remove the sanding residue. Finally, apply the topcoat.\nIf your boats gelcoat shows heavy areas of crazing or cracking, that calls for extra preparation. If you see signs of damage, repair those areas before proceeding with the previous steps.\nLets say you just purchased a brand-new boat, and it arrived from the factory without bottom paint. Or, maybe you own a boat that has never seen antifouling bottom paint before. In either case, you should prime the bare fiberglass before applying the appropriate coating.\nFirst, clean the hull thoroughly to remove accumulated dirt and debris. Next, wipe the hull with a solvent to get rid of wax remnants from the factory mold. Now, you are ready to apply a quality primer designed for use with a compatible bottom paint.\nThe best marine primer for bare fiberglass is a 2-part epoxy primer. This coating has been formulated as a hard, strong base for its associated epoxy and polyurethane paint system.\nHardworking epoxy primers are known for their outstanding adhesion and filling capabilities. Epoxy primers are compatible with fiberglass, steel, and aluminum substrates. These versatile primers can be used below or above the waterline.\nAn epoxy primer for boats is a versatile compound that provides outstanding filling and paint adhesion benefits. These two-part systems are often utilized for their maximum anti-corrosion properties and their ability to keep moisture out. Together, these attributes are important in the prevention of osmosis and blistering.\nMarine epoxy primer can be used on all substrates, including fiberglass, steel, and aluminum. They are designed to be used above or below a boats waterline.\nDesigned for fiberglass boats, a barrier coat is a special-purpose protective epoxy undercoat. When properly applied, it creates a moisture-resistant surface that greatly decreases the chances of gelcoat blisters. A barrier coat can also be a key component of a vessels hull repair system. If your boat experienced gelcoat blisters as a result of water permeation into the hull, you must resolve the blisters before doing anything else.\nOnce you have done that, applying a barrier coat will help to ensure that the blisters do not return and potentially cause serious damage to your boat. After you have finished the barrier coat application, you can roll on the antifouling paint.\nA 1-part primer has been formulated for use with 1-part paint systems. Examples include yacht enamels and one-part polyurethane paints. Do-it-yourselfers find it easy to work with 1-part paint systems. Use these versatile primers on above-the-waterline surfaces on fiberglass, wood, aluminum, and steel boats.\nA 2-part epoxy primer provides a super-hard base that easily accommodates epoxy and polyurethane paints and components. These painting systems are trickier to use compared to 1-part paint systems. When applied correctly, however, a 2-part paint system yields a deep, rich gloss and a long-lasting finish.\nAlthough the majority of topside primers are intended only for above-the-waterline use, a few primers are also approved for below-the-waterline application. Carefully review the product can, or view the manufacturers website, to confirm the usage instructions.\nEpoxy primers are designed for use below and above the waterline. Regardless of the location, these versatile compounds are known for their excellent filling and adhesion properties. Although frequently used on fiberglass vessels, epoxy primers work equally well on substrates such as aluminum and steel. Epoxy primers include TotalBoat 2-part Epoxy Primer and Awlgrip 545 Epoxy Primer.\nSpecifically, 2-part epoxy primers are designed to provide a durable base for epoxy and polyurethane paint systems. These 2-part paint systems provide a deeper gloss and more durable finish compared to 1-part paint systems. However, 2-part application techniques are more complex than those of 1-part paint products.\nA well-applied topside primer performs three important functions prior to the application of topside paint. Designed for above-the-waterline applications, the primer bonds to the underlying hull surface. This bonding enables a compatible topside finish coating to mechanically bond to the primer.\nTopside primer also seals and hides previous hull repairs. A higher solids primer can even fill tiny scratches. Finally, the primer creates a wall between your old and new topside paint, thus prohibiting adverse chemical reactions.\nMost topside primers are 1-part primers that are designed for use with a matching 1-part paint system. Examples include yacht enamels, Monourethane-alkyd paints, and 1-part Polyurethane paints.\n1-part paint systems are typically used on above-the-waterline surfaces on fiberglass, wood, steel, and aluminum boats. Do-it-yourselfers will find 1-part paint systems easy to work with.\nTopside primer cure times vary by product, and temperature and humidity also come into play. Always consult the manufacturers literature for specific instructions. Use only in a well-ventilated area, and avoid repeated or prolonged skin contact.\nSome 2-part universal epoxy primers, such as Awlgrip 545 and TotalBoat 2-part epoxy primer, may be used on topsides or for below-the-waterline applications. Epoxy primers are known for their superior adhesion and filling properties. These versatile compounds can be used on fiberglass, aluminum, and steel vessels.\nA metal primer allows a finishing paint to better adhere to a bare metal surface. At the same time, the primer fills in imperfections on the substrate materials surface layer. If the metal object will have considerable exposure to moisture, especially saltwater, using a primer is strongly advised.\nMarine wood primers and sealers are similar in function to fiberglass primers. Specifically, they prepare the wood for the application of paint or varnish. Cure times vary with the product and environmental factors. Use these chemicals only in well-ventilated areas, and minimize skin contact.\nGenerally speaking, a wooden boat primer is designed to penetrate and bond with the wood. Some primers will hide previous coatings, and will also mask the emerging wood grain. Properly applied wood primer enables the paint to properly cure, and improves the final paint coatings waterproofing abilities.\nA correctly applied wood sealer gives the wood a uniform finish before the paint or varnish application. Wood sealers dry very quickly, and are easy to sand to a very smooth finish.']	['<urn:uuid:9724c2b7-06bf-4958-b52b-1b1507711a58>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	29	30	1231
60	What happens if one company can't pay in a lawsuit?	Under joint and several liability, which exists in some form in 36 states, if one defendant cannot pay their portion of damages (for example due to bankruptcy), the other defendants must pay the entire amount regardless of their proportional fault. For instance, if two companies are found liable for $1 million in damages with an 80-20 split of fault, but one company goes bankrupt, the remaining company must pay the full $1 million.	['Medmarc Insurance Group\nThe United States is widely regarded as the most litigious country in the world―and for good reason. The U.S. has the most lawyers per capita, and spends the most money—both in dollars and as a share of gross domestic product—on litigation. Doing business in a litigious society can be difficult for any business, but life sciences companies face some unique challenges related to litigation. The Litigation 360 series will describe these challenges and provide some general guidance to assist life sciences companies minimize the likelihood of litigation, and maximize the chances of success should litigation occur.\nIn this first article of the series, we begin by discussing two factors―jury bias and joint and several liability―that can negatively impact a life sciences company involved in products liability litigation.\nOne challenge that life sciences companies must grapple with is adverse jury biases. In general, there is a strong undercurrent of anti-corporate sentiment. This undercurrent has been percolating in the U.S. at least since the savings and loan crisis of the 1980s and is regarded by many to have intensified over the last fifteen years or so. (The Institute for Legal Reform (2012).) There have been several watershed events in the popular movement against “corporate America” in recent history, and each of these probably contributed to the difficulty corporations now face in overcoming jury bias that assumes they are solely motivated by profit and greed at the expense of consumer safety. Such events include the revelations over the last several decades that tobacco companies knew about the harmful effects of tobacco for a long time while advertising cigarettes as safe, the Enron scandal of 2001, and movies like Erin Brockovich (and the real-life experience it was based on). The “Occupy Wall Street” movement was the latest manifestation of this anti-corporate undercurrent. Recent polling of jury-eligible individuals confirms that such biases against companies are still a tangible obstacle for corporate defendants.\nA nationwide juror survey in 2008 indicated the following of the jury-eligible public:\nBeyond the overt backlash against corporations, there is also a more basic juror perception that can be detrimental to companies. This perception is that companies have “deep pockets,” and thus will be relatively unharmed by having to pay substantial monetary awards. Where there is a harmed plaintiff—a victim—jurors may look to make them whole by utilizing the “deep pockets” of the corporation, whether or not the jury really believes the company is at fault for the plaintiff’s harm.\nBeyond facing the general anti-corporate bias of juries, life sciences companies—and particularly pharmaceutical companies—have an especially uphill battle. Juror sentiment and popular distrust surrounding pharmaceutical companies seems to be especially strong. Though a majority of respondents to a nationwide poll held the belief that most suits against companies (in general) probably have merit, an even greater proportion of respondents felt that suits against pharmaceutical companies and oil and gas companies (notoriously mistrusted by the public) likely had merit.\nThough it probably comes as a surprise to companies that fall under the oversight purview of the Food & Drug Administration (FDA), a majority of polled jurors actually felt that the FDA was too lenient a watchdog. Indeed, in one such survey, almost 80% of respondents felt that there was insufficient government regulation of pharmaceutical companies. (DecisionQuest (2008).) Much like the few, widely publicized recent events that probably underlie the anti-corporate sentiment currently permeating the national conscience, this view of the FDA as merely a “rubber-stamp” organization can be largely attributed to a few significant incidents that received dramatic media and popular attention.\nThe 1980s products liability litigation surrounding rupturing silicone breast implants that resulted in a $4.25 billion class-action settlement certainly could have contributed to this view. The Fen Phen disaster of the 1990s, in which the FDA-approved diet drug caused severe heart valve damage in a substantial portion of users is another event that likely painted the FDA as too lax a guardian. Though in the context of the thousands of safe drug and devices on the market these disasters seem all but inevitable, such stories resonated with the public and probably underlie the popularly-held opinion that medical products are not as rigorously tested prior to approval as they should be. Indeed, in another comprehensive set of juror surveys performed by DecisionQuest in 2006, the firm determined that 84% of jury-eligible individuals felt that that companies today need to do a better job of testing products before they sell them to consumers. The same proportion also said product manufacturers had a duty to make their goods 100% safe for all consumers.\nAt the heart of the anti-corporate sentiment are the beliefs that corporations are greedy, apt to put profits over people and the safety of consumers, and are willing to lie or cover up bad facts in order to be more profitable. To combat this, it becomes paramount that your company create and maintain a positive “story” and demonstrate your commitment to patient safety and transparency as often as possible. This can be accomplished through careful document creation and incorporating an appropriate document retention policy. Including dedication to patient safety in the company’s mission statement is also an easy but effective way to integrate a regard for patient safety into the company’s “story.” (Document creation and retention policies will be discussed in greater detail in the next installment of the Litigation 360 series.)\nWhen an adverse event occurs in a healthcare setting, the injured party—or more accurately, the injured party’s attorney—is unlikely to “zero in” on only one defendant. Instead, plaintiffs’ attorneys generally cast a wide net and bring suit against any and all persons and entities that were involved in the plaintiff’s care when the adverse event occurred. This generally includes the healthcare facility where the injury took place, the doctors and/or other healthcare professionals treating the patient, and the manufacturer and/or distributor of the product(s) being used to treat the patient. This frequently places life sciences companies alongside healthcare providers (doctors and hospitals) and their own vendors, suppliers, contract manufacturers, or distributors, as defendants in the plaintiff’s case. Though technically on the same side, things may quickly become adversarial between the drug or device company and other defendant(s) as the success of each often depends on imputing liability to the other.\nJoint and several liability is the doctrine that makes each defendant liable for the entire amount of a plaintiff’s award, regardless of each defendants’ proportional fault. The doctrine is intended to ensure that a plaintiff obtains their whole damages award against multiple defendants, even if one or more of the defendants goes bankrupt or is otherwise unable to pay. It is retained today in some form (pure or modified) by the majority of (36) states. The application of joint and several liability can be more or less onerous depending on the liquidity, location, and, occasionally, legal protection of each co-defendant independently and relative to one another. There are a few situations in which the application of joint and several liability can be an especial hardship for the drug or device company.\nThe most common scenario in which joint and several liability can result in one defendant paying a disproportionately large percentage (perhaps all) of a plaintiff’s award is when a co-defendant has declared bankruptcy or no longer exists (e.g., is deceased or has gone out of business). It is this situation, in fact, for which joint and several liability was devised to address. Rather than the wronged party—the plaintiff—not receiving its due compensation, joint and several liability shifts the burden of a co-defendant’s inability to pay to the other co-defendant(s) on the grounds that, in sharing some part of responsibility for the plaintiff’s harm, other co-defendants are the just bearers of the payment or non-payment burden.\nExamples are most effective at illustrating how joint and several liability operates in practice. Imagine an adverse event involving a metal-on-metal hip implant for which the plaintiff brings suit against the original equipment manufacturer (OEM) of the implant as well as the contract manufacturer that made a component part in the implant. If the jury awards the plaintiff $1 million and determines that the proportional fault of the OEM and contract manufacturer is 80% and 20%, respectively, the OEM would be responsible for $800,000 and the contract manufacturer, $200,000. Now, in a state with joint and several liability, if the contract manufacturer has gone bankrupt by the time of the trial, the OEM is responsible for the entire $1,000,000 award to the plaintiff.\nBesides a co-defendant’s bankruptcy or other inability to pay, there are two other types of co-defendants—both of which are particularly common in the life sciences products liability context—that can result in the drug or device company defendant paying a disproportionate amount, relative to fault, where joint and several liability exists. The first occurs when the co-defendant is foreign, such as a supplier or manufacture based outside the United States. There is often substantial difficulty in asserting jurisdiction over foreign companies in order to bring them into court and even more in enforcing a judgment against such a foreign operation. Again, joint and several liability may dictate that the burden of finding the foreign party and enforcing a judgment against them be shifted from the plaintiff to the available defendant. Because so many medical device OEMs utilize foreign suppliers and contract manufacturers, this can be a common scenario in the context of medical device litigation.\nThe second situation that is of particular significance for life sciences companies arises when there is a statutory cap on the damages a particular defendant must pay. Many states, through tort reform efforts, have enacted such caps for doctors and other health care providers. When a statutory cap limits the damages a doctor must pay to an amount less than the plaintiff’s award, the co-defendants must pay the difference (regardless or proportion of fault) in a pure joint-and-several liability state.\nTo illustrate how this works, imagine a physician and manufacturer are co-defendants in a joint-and-several-liability state. A plaintiff is injured after the failure of her hip implant and brings suit against the surgeon and the implant manufacturer. The jury finds that the surgeon’s own negligence in implanting the hip implant was responsible for 80% of the plaintiff’s adverse outcome, and a design defect in the implant was responsible for the other 20%. The jury awards the plaintiff $1 million. Though the proportion of damages attributable to the surgeon is $800,000, the state has enacted a statute limiting physician liability to $200,000. This means that, being liable for the entire award under joint and several liability, the manufacturer must pay the entire difference—$800,000.\nTwenty-seven more states retain what’s known as “modified” joint and several liability, where each defendant is liable for the entire verdict only if they are found to be above a specific percentage at fault (often 50%). These are:\nLife sciences companies facing claims should consider whether the state in which the claim is brought has joint and several liability and what tort reform efforts have been passed there. These considerations may be one of many factors to consider in deciding whether it is more prudent to settle a case or proceed with trial and risk an adverse jury award.\nAlthough the challenges discussed are inherent in the environment in which life sciences companies operate, their effect can be mitigated by the creation of and dedication to the company’s positive story that regards patient safety as paramount, and careful consideration of the various factors at play in each claim in devising the corresponding defense strategy.\nThe second installment of the Litigation 360 series will explore how the transparency mandated by the FDA can serve to increase the likelihood of claims and be a hindrance during litigation for life sciences companies, as well as how appropriate documentation practices can be the most successful means of nullifying the weight and effect of the environmental challenges faced by life sciences companies.\nFor additional resources contact the Marketing department\nCopyright © 2022 - Medmarc\nMedmarc is a member of ProAssurance Group. The product material is for informational purposes only. In the event any of the information presented conflicts with the terms and conditions of any policy of insurance offered from ProAssurance, its subsidiaries, and its affiliates, the terms and conditions of the actual policy will apply.']	['<urn:uuid:971873fd-b12c-436c-9bc3-af44b31021cc>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	10	73	2050
61	What problems do Indigenous communities face during COVID-19?	Indigenous people have been disproportionately affected by COVID-19 due to a lack of data on their needs and experiences, limited resources like PPE and running water, and historical discrimination and trauma.	['Congressional Briefing & Interviews:\nAddressing Disparities in Black, Indigenous & Rural Communities\nVirtual Congressional Briefing\nIndividuals in Black, Indigenous, and rural communities have been disproportionately affected by the COVID-19 pandemic. Bipartisan efforts to reduce health disparities in these communities cannot succeed without addressing economic and social drivers of health, often called “social determinants of health”. For example, poverty affects access to quality housing, food, and the ability to withstand economic shocks like unemployment due to the pandemic. Education and economic opportunities affect income and health decision-making. Inadequate transportation, environmental contaminants (e.g., mold or lead in substandard housing), historical inequities, and discrimination also contribute to health disparities, which are exacerbated during the COVID-19 pandemic. A disconnect between community needs and the disproportionate availability of health care services with adequate capacities further widens these disparities.\nCurrent policy efforts related to COVID-19 provide an opportunity for reducing disparities as well as improving economic mobility, health care, and the criminal justice system. Issues related to disparities in this context include, but are not limited to, housing, violence and crime (e.g., due to stress, economic instability), workplace issues (e.g., access to PPE, low wages, racism), access to resources (e.g., food, technology), and healthcare (e.g., pre-existing conditions, insurance, infrastructure). Access to key supports are particularly diminished in these racialized and marginalized communities. Promising practices in relaxing restrictions on healthcare providers, improving work and home environments, and digital innovations have the potential to enhance access points. This briefing will cover a few issues related to racial and health disparities: (i) identifying complex needs that contribute to disparities, (ii) promoting training for individuals in marginalized communities (iii) expanding access to care and information through technology.\nAgendA & Materials:\nOpening Remarks by: Congresswoman Yvette Clarke\nHealth Inequities Experienced by Black Americans During COVID-19\nKamila Alexander, John Hopkins University, School of Nursing\nMeeting the Needs of Indian Country and Indigenous Populations\nAnn Michelle Daniels, South Dakota State University\nThe Impact of COVID-19 on Rural Communities: Problems and Policy\nDara Whalen, The College of New Jersey\nMargaret Holland McDuff, Family Service of Rhode Island\nModerated by: Tracy Vozar, Graduate School of Professional Psychology at the University of Denver\nPanelist Interviews and Fact Sheets\nIn addition to participating in our virtual congressional briefing, Ann Michelle Daniels, Dara Whalen, and Kamila Alexander participated in short interviews with RPC interns Nicolyn Charlot and Brianna Baker. These interviews go into more depth about the disparities Black, Indigenous, and rural communities are facing, and what policymakers can do to address these issues.\nHealth Disparities in Black Communities During COVID-19\nBlack individuals are at increased risk for contracting COVID-19 and experiencing negative outcomes from the virus. Reasons this is occurring include poor work and living conditions and racial bias in healthcare and the workplace.\nSolutions to these issues include: update the minimum wage, improve low income housing, and create policies to address discrimination.\nHealth Disparities in Indigenous Communities During COVID-19\nIndigenous people have been disproportionately affected by COVID-19. Some of the reasons for these disparities include a lack of data on Indigenous’ needs and experiences, limited resources (e.g., PPE, running water, and historical discrimination and trauma.\nSolutions to these issues include: streamlining fund distribution, improving data collection, and promoting collaborations between tribal and non-tribal health centers.\nHealth Disparities in Rural Communities During COVID-19\nRural Americans are at increased risk for infection and mortality from COVID-19. This is happening due to factors such as increased rates of chronic illness, limited health care options, and poor working conditions.\nSolutions to these issues include: Expanding public health infrastructure, investing in job reallocation and training programs, and relaxing regulations on healthcare providers.']	['<urn:uuid:3c5d8f70-5a17-4f62-8797-ff582e0d14f5>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	8	31	601
62	I'm curious about science teams. Who needs to work together on microbe research?	Integrated approaches involving interdisciplinary teams of researchers are needed, specifically combining experts in statistics, comparative genomics, evolutionary biology, and ecological theory to properly analyze metagenomic data.	"[""Metagenomics1 – the study of the genomes of many microbes in an environment simultaneously -has the potential to revolutionize our understanding of the hidden yet incredibly important world of microorganisms. This potential has been highlighted by a series of recent metagenomic-based studies [1-8] as well as multiple government reports  including in particular the recent National Academy of Sciences report “The New Science of Metagenomics – Revealing the Secrets of our Microbial Planet.” The great potential of metagenomics comes with enormous challenges in the analysis of the data2. These challenges include the fragmentary nature of sequence data, the sparse sampling of genomes, populations and communities and the unknown phylogenetic diversity and ecological structure of the communities being sampled . Methods designed for analysis of single organism genomes simply do not work well on data sets sampled from complex ecological communities. To develop new methods, the NAS report suggested (and we agree) that integrated approaches involving interdisciplinary teams of researchers are needed in which the researchers both ask scientific questions and develop new data analysis tools.\nHere we propose building exactly such an integrated, interdisciplinary effort, bringing together three labs with different relevant areas of expertise including statistics (to deal with the sparse sampling), comparative genomics (because the data is genomic in nature), evolutionary biology (to assess phylogenetic and genomic diversity), and ecological theory (to examine community structure).\nThe research we propose covers three major topics considered of fundamental importance in metagenomic studies3: biodiversity, evolutionary dynamics, and statistical measures. Our proposed work should lead to novel insights into microbial ecology and evolution. In addition, at the core of all of our work is the development of novel mathematical, statistical and computational methods for analyzing metagenomic data. Since these methods will be of use to the research community at large, we propose to work closely with the CAMERA team to the methods broadly available through the CAMERA database.\n1 We use the term metagenomics to refer to shotgun sequencing DNA from environmental samples.\n2 The NAS report identifies five challenges in metagenomics: need for interdisciplinary teams, role of government, methods development, complexities of data analysis and need for databases.\n3 In the NAS report they identify four key questions: how can we find new functions; how diverse is life; how do microbes evolve; and what role do microbes play in the health of their hosts.\nJonathan Eisen Evolutionary and comparative genomics, metagenomics, phylogenetics\nKatherine Pollard Statistical and computational genomics\nJessica Green Applied and theoretical ecology, microbial community structure\nProposed research areas\n- Metagenomic based characterization of microbial biodiversity\n- Guidelines and weightings for using different gene families in metagenomic based diversity assays\n- Searching for novel phylogenetic types in metagneomic data\n- Metagenomic analysis of community phylogenetic structure\n- Estimating biodiversity from metagenomic samples\n- Metagenomic studies of microbial evolutionary dynamics\n- Molecular evolutionary dynamics of gene families\n- Population genomics\n- Statistical metagenomics: Correlation analysis of sequence data and metadata\nThough each of these projects can be considered separate activities, they are highly interdependent and the interdisciplinary nature of the labs involved is critical for the success of the project. For example, we propose to use phylogenetic analysis to search for new types of organisms and genes (project 1.2 led by Eisen). The results of this phylogenetic analysis will also be used to assess the phylogenetic structure of communities (project 1.3 led by Green) and to study evolution of gene families across environments (project 2.2 led by Pollard). Similarly, the statistical methods developed for comparative metagenomics (project 3 led by Pollard) will be used in the population genomic studies (project 2.1 led by Eisen) as well as in the development of biodiversity estimators (project 1.4 led by Green). To achieve this integration we plan to have active feedback between each of the PIs and each of the projects. To do this we propose a management plan that will help guide these interactions. We believe that by taking this integrated approach – both in terms of the research topics and by combining separate fields of study, we will not only make important scientific discoveries about microbial communities but we will also build and develop novel methods and approaches of great utility to the metagenomics community.\n1. Yooseph, S., et al., The Sorcerer II Global Ocean Sampling Expedition: Expanding the Universe of Protein Families. PLoS Biol, 2007. 5(3): p. e16.\n2. Rusch, D.B., et al., The Sorcerer II Global Ocean Sampling Expedition: Northwest Atlantic through Eastern Tropical Pacific. PLoS Biol, 2007. 5(3): p. e77.\n3. Denef, V.J., et al., Implications of Strain- and Species-Level Sequence Divergence for Community and Isolate Shotgun Proteomic Analysis. J Proteome Res, 2007.\n4. Tyson, G.W., et al., Community structure and metabolism through reconstruction of microbial genomes from the environment. Nature, 2004. 428(6978): p. 37-43.\n5. DeLong, E.F., et al., Community genomics among stratified microbial assemblages in the ocean's interior. Science, 2006. 311(5760): p. 496-503.\n6. DeLong, E.F., Microbial community genomics in the ocean. Nat Rev Microbiol, 2005. 3(6): p. 459-69.\n7. Casas, V. and F. Rohwer, Phage metagenomics. Methods Enzymol, 2007. 421: p. 259-68.\n8. Delwart, E.L., Viral metagenomics. Rev Med Virol, 2007. 17(2): p. 115-31.\n9. Pennisi, E., Metagenomics. Massive microbial sequence project proposed. Science, 2007. 315(5820): p. 1781.""]"	['<urn:uuid:1ab12b9e-b43f-4abe-9980-fe97b00fe0eb>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	13	26	871
63	does potassium help blood pressure research findings	Studies show potassium has significant effects on blood pressure. Research found those who consumed 4069 mg of potassium daily had a 49% lower risk of death from ischemic heart disease compared to those who consumed around 1000 mg per day. Having a diet high in potassium and low in sodium has been shown to reduce the risk of high blood pressure and stroke, as demonstrated in the DASH diet trials where participants consuming high amounts of fresh fruits and vegetables experienced significant drops in blood pressure levels.	"['The ways to consume grapes are endless - from red, green and purple grapes to seedless grapes, grape jelly, grape jam and grape juice. And don\'t forget wine and raisins! This popular fruit\'s history goes back as far as 8,000 years, when grape vines were first cultivated in what is now the Middle East.\nIt may surprise you to hear that the humble grape is actually considered to be a berry. Seventy-two million tons of these berries are grown each year worldwide, with most of them being used to produce wine - 7.2 trillion gallons of wine per year, to be exact.7\nThe portability, texture, flavor and variety of grapes have made them a popular finger food in countries all over the world. The potential health benefits of consuming grapes are numerous, with past studies associating them with prevention of cancer, heart disease, high blood pressure and constipation.\nThis MNT Knowledge Center feature is part of a collection of articles about the health benefits of popular foods.\nContents of this article:\nThe average serving size for grapes is about one cup, or 32 grapes. As a useful measure, you can use the size of your fist to estimate the proper portion size for grapes.\nOne cup of red or green grapes contains 104 calories, 1.09 grams of protein, 0.24 grams of fat, 1.4 grams of fiber, 4.8 milligrams of vitamin C, 10 micrograms of vitamin A, 288 milligrams of potassium, 0.54 milligrams of iron and 3 micrograms of folate.3\nGrapes are high in water content and good for hydration. High water-content fruits and vegetables are nutrient dense, meaning they provide a large amount of essential nutrients while containing few calories. Grapes contain 70 milliliters of fluid per cup.2\nGrapes are high in antioxidants important for eye health such as lutein and zeaxanthin, and red grapes contain the phytochemical resveratrol in their skins, the antioxidant synonymous with wine known to lend protection from several chronic diseases and conditions. Grapes also boast the power of the flavonoids myricetin and quercetin which help the body to counter-act harmful free radical formation.\nPossible health benefits\nConsuming fruits and vegetables of all kinds has long been associated with a reduced risk of heart disease, diabetes, cancer and other conditions. Many studies have shown that increasing consumption of plant foods like grapes decreases the risk of obesity and overall mortality. Grapes also have some special components that make them even more essential to our health, giving them ""super food"" status and reducing the risk of the following conditions:\nCancer: Grapes contain powerful antioxidants known as polyphenols, which may slow or prevent many types of cancer, including esophageal, lung, mouth, pharynx, endometrial, pancreatic, prostate and colon.1\nThe resveratrol found in red wine famous for heart health is a type of polyphenol found in the skins of red grapes.\nHeart Disease: The flavonoid quercetin is a natural anti-inflammatory that appears to reduce the risk of atherosclerosis and protect against the damage caused by low-density lipoprotein (LDL) cholesterol in animal studies. Quercetin may have the additional bonus of anti-cancer effects; however more studies are needed using human subjects before these results can be confirmed.6\nThe high polyphenol content in grapes may also reduce the risk of cardiovascular disease (CVD) by preventing platelet build-up and reducing blood pressure via anti-inflammatory mechanisms.\nThe fiber and potassium in grapes also support heart health. An increase in potassium intake along with a decrease in sodium intake is the most important dietary change that a person can make to reduce their risk of cardiovascular disease, according to Mark Houston, MD, MS, an associate clinical professor of medicine at Vanderbilt Medical School and director of the Hypertension Institute at St Thomas Hospital in Tennessee.5\nIn one study, those who consumed 4069 mg of potassium per day had a 49% lower risk of death from ischemic heart disease compared with those who consumed less potassium (about 1000 mg per day).5\nHigh potassium intakes are also associated with a reduced risk of stroke, protection against loss of muscle mass, preservation of bone mineral density and reduction in the formation of kidney stones.5\nHigh Blood Pressure: As noted above, potassium has many benefits for the body. It may be that a low potassium intake is just as big of a risk factor in developing high blood pressure as a high sodium intake.4 Because of their high potassium content, grapes are recommended to those with high blood pressure to help negate the effects of sodium in the body.\nAccording to the National Health and Nutrition Examination Survey, fewer than 2% of US adults meet the daily 4700 mg recommendation for potassium.5\nAlso of note, a high potassium intake is associated with a 20% decreased risk of dying from all causes.5\nConstipation: Eating foods that are high in water content like grapes, watermelon and cantaloupe can help to keep you hydrated and your bowel movements regular. Grapes also contain fiber, which is essential for minimizing constipation.\nAllergies: Because of the anti-inflammatory effects of quercetin, consuming grapes may help to alleviate symptoms of allergies including runny nose, watery eyes and hives. There have been no human studies done to prove this theory.6\nDiabetic neuropathy and retinopathy: A few studies have shown promise that resveratrol can protect against diabetic neuropathy and retinopathy, conditions caused by poorly controlled diabetes where vision is severely affected. One study in which diabetic rats were treated with resveratrol for two weeks found that it reduced the effects of neural changes and damage associated with diabetic neuropathy.\nResearchers have also found resveratrol to be beneficial for treating Alzheimer\'s disease, relieving hot flashes and mood swings associated with menopause and improving blood glucose control, however large studies using human subjects are still needed to confirm these findings.\nIncorporating more grapes into your diet\nThe Environmental Working Group produces a list each year of fruits and vegetables with the highest levels of pesticide residue, known as the Dirty Dozen. Grapes are high on the list of produce that the EWG suggests that you buy in the organic version to ensure a lower risk of pesticide exposure.\nGrapes are available year round. Select grapes that are tight to the touch and free of wrinkles. Grapes are best stored in the refrigerator and should be washed before eaten.\nThe best way to consume grapes is fresh right out of your hand! Most grape jellies/spreads, juices and wine have added sugars, which tack on additional calories.\nHere are some handy tips to incorporate more grapes into your diet:\n- Slice grapes in half and add them to your chicken salad.\n- Make your own fruit cocktail with fresh fruit and include grapes, pineapple, sliced peaches and strawberries. Drizzle a small amount of honey on top of the fruit mixture for an extra sweet treat.\n- Freeze grapes and eat them as a snack or dessert on a hot day.\n- Have a quick 100-calorie snack with 1 cup of fresh grapes.\nRisks and precautions\nBeta-blockers, a type of medication most commonly prescribed for heart disease, can cause potassium levels to increase in the blood. High potassium foods such as bananas should be consumed in moderation when taking beta-blockers.\nConsuming too much potassium can be harmful for those whose kidneys are not fully functional. If your kidneys are unable to remove excess potassium from the blood, it could be fatal.When drinking wine to reap it\'s health benefits drink no more than one drink per day for women or two drinks per day for men.\nRecent developments on grapes from MNT news\nNew research presented at the Experimental Biology conference in San Diego, California, suggests that regular grape consumption may help alleviate pain associated with symptomatic osteoarthritis of the knee, and improve joint flexibility and overall mobility. Researchers attribute these potential benefits to the polyphenols found in grapes.\nIn an analysis of 446 compounds for their ability to boost the innate immune system in humans, researchers in the Linus Pauling Institute at Oregon State University discovered just two that stood out from the crowd - the resveratrol found in red grapes and a compound called pterostilbene from blueberries.\nWritten by Megan Ware, RDN, LD, registered dietitian and nutritionist', ""Potassium: The overlooked nutrient in crop production\nUnderstanding potassium's role as an essential plant and animal nutrient is critical to alleviating some emerging crop production and human health issues.\nAlthough potassium (K) is listed among the top three macronutrients (N-P-K) needed for crop production, nitrogen (N) and phosphorus (P) receive most of our attention. This is partly because N and P have potential to cause long-term environmental implications. Also, N and P get incorporated into key complex molecules within the cell such as DNA, proteins, enzymes, etc. In contrast, K rarely poses a threat to the environment. It remains in the plant and animal tissues in its ionic form K+. When crop and animal residues decompose on the soil surface, the soluble K will seep into the soil.\nSoil minerals such as feldspars and micas are high in K and will release their K into the soil upon weathering. Some K is bonded in the interlayer position of clay particles. Many global K deposits for mining purposes were formed millions of years ago when early seas evaporated and marine salt was concentrated and covered with deep layers of sediment. Large reserves of buried K deposits occur around the world, with the largest reserves and fertilizer production coming from Canada, Belarus and Russia. Currently, the global supply of K remains stable.\nRecently, K has received much needed awareness with the realization of its critical role in plant and human nutrition. In January 2017, the International Plant Nutrition Institute held a three-day conference devoted to K, titled “Frontiers of Potassium.” Some of the recent advances on sustainable intensification, 4R stewardship practices and linking science and technology to frontier practices were highlighted. The current literature indicates that K, at a global level, is as limiting as N and P for plant productivity.\nRole of K in plant nutrition\nPotassium is associated with many metabolic processes and functions within the plant. It is known that K activates as many as 60 enzymatic and plant hormonal reactions. It is vital to photosynthesis and protein synthesis. It has a fundamental role in regulating leaf stomata openings and controlling water use, particularly under dry conditions (Read the Michigan State University Extension article “Drought and potassium deficiency in corn and soybeans” for more information.)\nFoliar K deficiency symptoms were observed on many Michigan corn and soybean fields during the drought of 2016. When the potassium supply is limited, plants have reduced yields, poor quality and are more susceptible to pest damage. A local example would be the increased soybean aphid damage in K deficient fields. Potassium also helps in winter hardiness and overcoming stress situations, which may be vital in a changing climate.\nVegetables, fruits and legumes remove large qualities of K from soils. Harvesting 9 tons of alfalfa per acre will remove over 450 pounds K2O; a potato crop of 450 cwt per acre removes 500 pounds K2O; and harvesting 40 tons per acre of tomatoes will take over 450 pounds K2O/A. Read MSU Extension bulletin E2934, “Nutrient Recommendations for Vegetable Crops in Michigan,” for vegetable K removal rates.\nMaintenance of an adequate K supply has become essential for sustaining food production. Even though K is widespread in soil minerals and rocks, many soils need additional K fertilizer to meet the plant removal rates. In Michigan, about 65 to 70 percent of the soil samples tested in the past would indicate a need for supplemental K as fertilizer, whereas only about 35 percent of the soils would require supplemental P.\nRole of K in human nutrition\nPotassium is an essential nutrient in animal health. It is the most abundant cation in the intracellular fluid where it plays a key role in maintaining salt balance between cells and body fluids. Adequate K is essential for cardiovascular, nerve and renal functions and preventing muscle cramps. Potassium plays a role in the synthesis of proteins and in the biochemical transformations required for carbohydrate metabolism.\nGovernment agencies dealing with human health state that diets high in potassium and low in sodium have shown to reduce the risk of high blood pressure and stroke. In the Dietary Approaches to Stop Hypertension (DASH) diet trials, participants with high blood pressure who consumed an average of eight to 10 total servings of fresh fruits and vegetables per day experienced significant drops in their blood pressure level.\nSince potassium is not stored in the body, it is necessary to continually replace this nutrient on a regular basis with potassium-rich foods. K is routinely supplemented to animal feeds and rations. About 90 percent of the K consumed is lost in the urine fraction. Hence, it is important to combine the urine fraction with solid fraction of the manure to recover most of the K in livestock waste.\nThe recommended dietary allowance of K for adults is 4,700 milligrams a day. Only 2-3 percent of American population meets this requirement. The good news is that it’s easy to get enough K by simply eating a healthy balanced diet. The U.S. Department of Agriculture lists good sources of K, which are all fruits, vegetables, fruit juices, legumes, fish, yogurt and milk. Surprisingly, bananas don’t top the list—that spot goes to potatoes, which pack over 900 milligrams per serving.\nBased on the critical role of K in plant and animal nutrition, MSU Extension recommends crop producers and nutrient managers pay more attention to the accuracy of K recommendations. Some of it can be achieved by frequent soil testing, using realistic yield goals and understanding the critical soil levels. In Michigan, K is certainly not a nutrient to discard in bad economic times.\nFood consumers should pay more attention to K from a healthy diet point of view. Meeting the daily dietary K requirements by eating foods rich in K will go a long way in alleviating some of the emerging heath issues in our society.""]"	['<urn:uuid:f3ff65e4-a4b2-4b0d-b0f3-0907897b2c8f>', '<urn:uuid:bd1b6ae0-49fd-40a9-984b-61122dc31236>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T00:57:18.859842	7	87	2339
64	cookies website store how long	Different cookies have varying storage durations. Some examples include: '_ga' which lasts for 2 years and is used for statistical data, 'fr' which is stored for 3 months for advertising purposes, 'Dynamicweb.VisitDate' and 'Dynamicweb.VisitorID' which last for one year, and several session-based cookies that are only temporary. Additionally, login-related cookies like 'DW_Extranet' last for one month after your last use of the login function.	"['What is the General Data Protection Regulation?\nIn legal terms, the GDPR is a European Union law intended to strengthen and unify data protection rules and rights for the benefit of EU citizens. Put in other words, to give control of personal data back to the user. While great news for individuals, it presents complex problems for companies.\nThe new rules apply to all organizations (of any size) that provide goods and services to the EU or that utilize tracking technologies (like cookies or tracking pixels) to monitor EU users’ behavior.\nWhile simple in theory, the law is dense and complex and requires a comprehensive understanding of the data you collect, whether it\'s personal or not, how you store and use it, and how you expose or remove data upon request. To do so, you must look at every single process and line of software code to outline your data processes.\nIn most cases, your data collection will require valid consent following May 25 - not just going forward, but also from all your existing users. Under the GDPR, consent can\'t be implied or inferred from someone\'s actions. Instead, valid consent must be specific to the data being collected, by an affirmative action that is unambiguous.\nLearn more about the General Data Protection Regulation (GDPR)\n1. Personal Information - What we collect and why\nWe offer certain site features, services, applications, and tools that are available only through the use of the following tracking technologies. You are always free to block, delete, or disable these technologies if your browser, installed application, or device so permits. However, if you decline cookies or other similar technologies, you may not be able to take advantage of certain site features, services, applications, or tools.\nGenerally, these technologies allow our sites, services, applications, and tools to store relevant information in your browser or device and later read that information in order to identify you to our servers or internal systems. Where applicable, we protect our cookies and other similar technologies to help ensure that only we and/or our authorized service providers can interpret them by assigning them a unique identifier that is designed for interpretation only by us.\nThis website collects and uses personal information for the following reasons:\n1.1. Google Analytics\nLike most websites, this site uses Google Analytics (GA) to track user interaction. We use this data to determine the number of people using our site, to better understand how they find and use our web pages and to see their journey through the website.\nAlthough GA records data such as your geographical location, device, internet browser and operating system, none of this information personally identifies you to us. GA also records your computer’s IP address which could be used to personally identify you but Google do not grant us access to this. We consider Google to be a third party data processor (see section 2 below).\nDisabling cookies on your internet browser will stop GA from tracking any part of your visit to pages within this website.\nLearn more about Google Analytics.\n1.2. Google Tag Manager\nWe are constantly trying to improve the customer experience on our website by providing visitors with more personalized and targeted campaigns and offerings. To do so, we use Google Tag Manager (GTM).\nGTM is a little snippet of code that helps us track user behavior across our sites and then pushes the data to our Google Analytics account. Then, all the data is perfectly organized and ready for us to assess and review for potential site improvements and remarketing campaigns.\nWe consider Google to be a third party data processor (see section 2 below).\nLearn more about Google Tag Manager.\n1.3. Contact, signup and download forms etc.\nShould you choose to contact us using i.e. the contact/signup form on our pages or download gated content from our website, your information will be stored in our backend and collated into an email and sent to us over the Simple Mail Transfer Protocol (SMTP).\n1.4. E-mail newsletter\nOn the website of Migatronic A/S, users are given the opportunity to subscribe to our enterprise\'s newsletter. A confirmation e-mail will be sent to the e-mail address registered by a data subject for the first time for newsletter shipping, for legal reasons, in the double opt-in procedure. This confirmation e-mail is used to prove whether the owner of the e-mail address as the data subject is authorized to receive the newsletter.\nThe personal data collected as part of a registration for the newsletter will only be used to send our newsletter. In addition, subscribers to the newsletter may be informed by e-mail, as long as this is necessary for the operation of the newsletter service or a registration in question, as this could be the case in the event of modifications to the newsletter offer, or in the event of a change in technical circumstances.\nThe subscription to our newsletter may be terminated by the data subject at any time. The consent to the storage of personal data, which the data subject has given for shipping the newsletter, may be revoked at any time. For the purpose of revocation of consent, a corresponding link is found in each newsletter. It is also possible to unsubscribe from the newsletter at any time by contacting us.\nThe newsletter of Migatronic A/S contains so-called tracking pixels. A tracking pixel is a miniature graphic embedded in such e-mails, which are sent in HTML format to enable log file recording and analysis. This allows a statistical analysis of the success or failure of online marketing campaigns. Based on the embedded tracking pixel, Migatronic A/S may see if and when an e-mail was opened by a data subject, and which links in the e-mail were called up by data subjects.\nSuch personal data collected in the tracking pixels contained in the newsletters are stored and analyzed by the controller in order to optimize the shipping of the newsletter, as well as to adapt the content of future newsletters even better to the interests of the data subject. These personal data will not be passed on to third parties. Data subjects are at any time entitled to revoke the respective separate declaration of consent issued by means of the double-opt-in procedure. After a revocation, these personal data will be deleted by the controller. Migatronic A/S automatically regards a withdrawal from the receipt of the newsletter as a revocation.\n1.5. Website cookies\n- In connection with log-in information: ""DW_Extranet"" (only if the solution contains Extranet)\n”DW_Extranet” is a ”persistent cookie”. It contains encrypted information about username and password to the extent you are using a log-in function on the Website. The lifespan of the cookie is one month, and it will therefore be deleted one month after the last time you have used the Website\'s log-in function. This cookie is used to remember you when you return to the Website, so that you won\'t have to log in again.\n- In connection with the date of your last visit: ”Dynamicweb.VisitDate”\n”Dynamicweb.VisitDate” is a persistent cookie. it contains information about the date of your last visit to the Website and is used in connection with statistics. The cookie has a one year lifespan and it is therefore deleted one year after your last visit to the Website.\n- In connection with previous visits: ”Dynamicweb.VisitorID”\n”Dynamicweb.VisitorID” is a persistent cookie. It contains a unique ID which you have been given upon your visiting the Website. It is used in connection with statistics. The cookie has a one year lifespan and it is therefore deleted one year after your last visit to the Website.\n- In connection with current session: ”Dynamicweb.SessionVisitor”\n""Dynamicweb.SessionVisitor"" is a session based cookie. It preserves users states across page requests.\n- In connection with each session: ""ASP.NET_SessionID""\nThis cookie preserves the visitor\'s session state across page requests.\n- In conncection with statistics: ""_ga""\n""_ga"" is a 2 year HTTP cookie. Registers a unique ID that is used to generate statistical data on how the visitor uses the website.\n- In conncection with each session: ""_gat""\n""_gat"" is a session cookie, used by Google Analytics to throttle request rate.\n- In connection with statistics: ""_gid""\n""_gid"" is a session cookie. Registers a unique ID that is used to generate statistical data on how the visitor uses the website.\n- In connection with marketing: ""ads/ga-audiences""\n""ads/ga-audiences"" is a session cookie. Used by Google AdWords to re-engage visitors that are likely to convert to costumers based on the visitor\'s online behaviour across websites.\n- In connection with marketing: ""fr""\n""fr"" is a HTTP cookie stored for 3 months. Used by Facebook to deliver a series of advertisement products such as real time bidding from third party advertisers.\n- In connection with marketing: ""tr""\n""tr"" is a session based tracking pixel. It tracks the actions that users take on the website using both standard events and custom events.\n2. Our Third Party Data Processors\nWe use a number of third parties to process personal data on our behalf. These third parties have been carefully chosen and all of them comply with the legislation set out in the EU General Data Protection Regulation 2018 (GDPR).\n- Microsoft Office 365 (Privacy Statement)\n- Microsoft Dynamics 365 (Security and Compliance)\n3. How Long do We Store Your Information\nWe will retain your personal information for the period necessary to fulfill the purposes outlined on our \'Consent page\'. You can always contact us, if you want your personal data \'erased\' from our databases (see Section 6). We will respond without undue delay (and in any event within one month, although this can be extended in difficult cases).\n4. Data Breaches\nWe will report any unlawful data breach of this website’s database or the database(s) of any of our third party data processors to any and all relevant persons and authorities within 72 hours of the breach if it is apparent that personal data stored in an identifiable manner has been stolen.\n5. Data Controller\nThe data controller of this website is: Svejsemaskinefabrikken Migatronic A/S, with company number (CVR): 34485216 whose registered office is:\nPhone: +45 96 500 600\n6. Right to Access and To Be Forgotten\nPlease contact us, if you want to request access to your personal information stored in our databases, or if you want your personal data \'erased\' from our databases. We will respond without undue delay (and in any event within one month from receiving your request, although this can be extended in difficult cases).\nPhone: +45 96 500 600\n7. Privacy Questions\nPhone: +45 96 500 600\nWhen a privacy question or access/download request is received we have a dedicated team which triages the contacts and seeks to address the specific concern or query which you are seeking to raise. Where your issue may be more substantive in nature, more information may be sought from you.']"	['<urn:uuid:991c2fe9-1627-484b-9ccd-be7bab67b8ae>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	5	64	1815
65	what happens if fishing without rod licence england wales maximum fine	Getting caught fishing without a licence could land a fine of up to £2,500.	['How much is a UK fishing Licence?\nBuy a rod fishing licence for England and Wales Licence type Trout and coarse up to 2-rod Salmon and sea trout 1-day £6 £12 8-day £12 £27 12-month £30 £82 12-month – over 65 or disabled £20 £54\nWhich rod licence do I need?\nIf you are not targeting Salmon or Sea Trout but wish to fish in freshwater, then you should buy a Trout & Coarse licence The standard licence covers you to fish with up to 2 rods at a time, however if you are a specimen angler and wish to fish with three rods, you will need to buy a three rod licence\nHow do I get a fishing license in South Africa?\nFreshwater fishing licenses can be obtained from CapeNature, who is responsible for biodiversity conservation in the Western Cape, and are valid for 12 months from the date of issue To apply for one, you can contact the head office at 087 087 9262 or one of the CapeNature regional offices\nDo you need a fishing license to fish in the sea NZ?\nYou do not need a licence to fish in the sea around New Zealand However, you do need a licence for freshwater fishing (in lakes and rivers)5 days ago\nCan you fish without a license UK?\nYou must have a rod fishing licence for England and Wales if you’re fishing for salmon, trout, freshwater fish, smelt or eel with a rod and line in: England (except the River Tweed)\nIs it free fishing in the UK?\nIn the strict legal sense, the public only has a right to fish for free in the officially recognised fully tidal parts of rivers and in the sea, except where an individual owns a private right of fishery\nDo you need a fishing license to fish in the sea?\nAnglers who are fishing from the shore into the sea will not require a fishing license, however, it is still worth checking with other local anglers just in case If you are fishing saltwater stretches of the inland river close to the river mouth then there is a chance you may require a fishing license\nDo I need a fishing Licence for a private lake UK?\nYou must seek permission from the landowner to fish on private land and sort an additional licence to fish in locks or weirs on the River Thames 2 You must follow national and local rules (byelaws) when freshwater fishing with a rod and line in England and Wales\nHow many rods can you fish with?\nA maximum of four rods or lines can be used by any one person at any one time If any rods or lines are left unattended, they must be clearly marked with the fisher’s name and address or name and boat registration number A maximum of three hooks or three gangs of hooks can be attached per line\nDo you need a license to fish in South Africa?\nBe it bait collection, rod & reel fishing, boat fishing, spearfishing, or cast-netting, each fishing activity requires the correct permit which are available at any local Post Office No permit is valid for species during their closed seasons, or for the listed prohibited species\nHow long does it take to get a fishing license in South Africa?\nIt will take one working day to obtain a licence\nDo you need a fishing license in Gauteng?\nThe following points are important to know regarding angling within Gauteng A person needs a fishing license as explained in point 2 Only two fishing rods with no more than two hooks per rod are allowed No person shall, while angling, employ a method to hook fish other than in the mouth\nHow many Kina are you allowed?\nThe daily limit of kina is 50 per person per day\nCan you fly fish all year round?\nFly fishing season peaks between the months of April and October Though, for fly fishermen willing to brave the cold weather, it is technically a year-round sport Additionally, with an abundance of warm weather destinations, fly fishing can be enjoyed all year long\nCan I fillet my fish at sea?\nYes you can fillet at sea,but you must be able to prove it was a legal sizeSo only fillet say 30cm snaps nothing near the minimin size If you are over nighting clearly date the bags ! You must prove to the fishery officer that the fish you have filleted is of a legal size\nHow much does a fishing license cost?\nEvery state sets it own price for fishing licenses On average, an annual state resident fishing license costs around $25 while non-resident licenses cost an average of $60 to $70\nWhat happens if you get caught without a fishing license UK?\nAs well as cheating other anglers, fishing illegally can carry a hefty penalty Getting caught without a licence could land a fine of up to £2,500 Anglers who wish to buy their licence online should buy it directly from GOVUK as they will receive a reference number allowing them to go fishing on the same day\nWhat fish you can take home in UK?\nCoarse (freshwater) fish Each day you can only take from rivers: 1 pike (up to 65cm) 2 grayling (30cm to 38cm) 15 small fish (up to 20cm) including barbel, chub, common bream, common carp, crucian carp, dace, perch, rudd, silver bream, roach, smelt and tench\nCan I fish in any river UK?\nYou will, of course, need an Environment Agency fishing licence to fish on any river, stillwater or canal(if in doubt, see our fishing licence blog post for more on this topic) Occasionally you might find “free fishing”, but most stretches of fishable rivers are run by angling clubs\nCan you fish anywhere in UK?\nPut simply, there are three ways to go fishing in the UK You can fish on public waters, go to privately owned waters, or fish on the sea – and needing a licence depends on what you fish and where\nCan I fish anywhere?\nFISHING is still allowed in NSW and is considered “exercise” Under the rules for exercise, you must remain in your Local Government Area (LGA) or if outside your LGA, stay within 5km from home It’s best to check this website to clarify the borders of your LGA In some areas, LGAs don’t extend past waterways\nWhat is the difference between coarse fishing and fly fishing?\nFly fishing is the technique usually used for freshwater game fishing, while other angling techniques are usually used for coarse fishing Coarse fish have scales that are generally larger than the scales of game fish, and they tend to inhabit warmer and stiller waters']	['<urn:uuid:1f0248c0-f974-44e5-9d80-eedb71b98af2>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	11	14	1126
66	where is the louvre museum located what district in paris	The Louvre Museum is located in the 1st arrondissement (district) of Paris, on the Right Bank of the Seine. It is a central landmark of the city.	"['Daniel Mayer. Mroue, Haas\nConvergence de drames\nISBN , OCLC [ A 2 ]. Japanese architects SANAA were selected to design the Lens project in Retrieved 19 October\n6/12/ · En mars , les ouvriers de Longwy ont trouvé une voix. Née libre au sein des combats contre le démantèlement de la sidérurgie en Lorraine, sous l’égide de la CGT, Radio Lorraine Cœur d.\nThe Louvre (English: / ˈ l uː v (r ə)/ LOOV(-rə)), or the Louvre Museum (French: Musée du Louvre [myze dy luvʁ] ()), is the world\'s second-largest art museum and a historic monument in Paris, France, and is best known for being the home of the Mona Lisa.A central landmark of the city, it is located on the Right Bank of the Seine in the city\'s 1st arrondissement (district or ward).\nAfrik.com - L\'actualité de l\'Afrique noire et du Maghreb\nAfrik.com offre un journal quotidien sur l\'actualité, la culture, l\'économie, la santé, le sport de 56 pays d\'Afrique et du Maghreb\nElle tourna un film en , intitulé Liliom, film de Fritz Lang. C\'est la femme dans le carousel avec un chapeau noir et une robe à carreaux. elle a un petit rôle. Charles Girard (né le 9 .\nAnna et l\'apocalypse Charlie\'s Angels Sonic le film Negative La maison biscornue The Snow Queen : La reine des neiges 2 Star Wars : L\'Ascension de Skywalker Waves Uncut Gems Baby boss 2 : Une affaire de famille. After - Chapitre 3. Venom : Let There Be Carnage. Cry Macho. Prisoners of the Ghostland. Mourir peut attendre. Halloween Kills. Cher Evan Hansen. The Many Saints of Newark.\nBest Sellers. En juin , il signe le Programme commun de gouvernement avec le Parti communiste de Georges Marchais et le Mouvement des radicaux de gauche de Robert Fabre. Il adopte les « propositions pour la France » comme programme, recyclant bon nombre de propositions du Programme commun. Enfin, la loi Quilliot modifie les droits et les devoirs des bailleurs et locataires. Bien qu\'il se sache atteint d\'un cancer de la prostate depuis le 16 novembre [ ] , il annonce sa candidature pour un second septennat le 22 mars au journal d\' Antenne 2.\nMais les communistes refusent une alliance gouvernementale. Toutefois, les auteurs ne parlent pas d\' euthanasie. Dans les relations franco-africaines, l\'immobilisme avait pris le dessus sur la novation. Il obtient une multitude de doctorats honoris causa , dont 6 pendant ses 14 ans de mandat [ ] :.\nWikimedia Commons Wikiquote Wikisource. Espaces de noms Article Discussion. Cette section contient une ou plusieurs listes. Article connexe : Affaire Habache. Laffont, ISBN et , OCLC , lire en ligne. Fayard, , p. Mitterrand, montrant que F. ISBN , p.\nPortrait d\'un ambigu [« Mitterrand. A Study in Ambiguity »], Paris, nouveau monde, , p. A Study in Ambiguity »], Paris, nouveau monde , , p. Michel, , p. La Table ronde, , p. ISBN , lire en ligne , p. Michel, ISBN , lire en ligne , p. De l\'intime au politique , Paris, Robert Laffont, septembre , p. ISBN L\'heure des choix, pp. ISBN , OCLC ISBN , OCLC , lire en ligne. Serge Berstein dir. Gaspard-Hubert Lonsi Koko, Mitterrand l\'Africain?\nMazarine Pingeot, postface Jack Lang , Mitterrand, une affaire d\'amitie , Paris, L\'Archipel, , p. ISBN Philip Short trad. A Study in Ambiguity »], Paris, Nouveau Monde , , p. Jacques Attali , Verbatim , Paris, Fayard, - , 3 vol. Grasset, , p. Fayard, ISBN , OCLC Pierre Joxe , Pourquoi Mitterrand?\nRobert Laffont, Charles Moulin , Mitterrand intime , Paris, Albin Michel, coll. La Table ronde, [dernier chapitre].\nISBN , OCLC Hugh Verity , Nous atterrissions de nuit Filipacchi, ISBN et , OCLC ISBN et ISBN , OCLC [ A 2 ]. Michelle Cotta , Le monde selon Mitterrand. Catherine Demangeat et Florence Muracciole , « Dieu » et les siens : Jacques, Jean-Louis, Georgina et les autres , Paris, P.\nBelfond, , p. Alain Duhamel , De Gaulle-Mitterrand : la marque et la trace , Paris, Flammarion, , p. Les ruptures - 2. Catherine Nay , Le Noir et le Rouge , Paris, Grasset et Fasquelle, Ramsay ISBN Citations Photographies. Parti socialiste. Gouvernements Paul Ramadier I et II 22 janvier - 19 novembre Gouvernement Robert Schuman I 24 novembre - 19 juillet Gouvernement Robert Schuman II 5 septembre - 7 septembre Gouvernement Henri Queuille I 11 septembre - 5 octobre Gouvernement Henri Queuille III 10 mars - 10 juillet Gouvernements Joseph Laniel I et II 28 juin — 12 juin Laval V Flandin II Darlan Laval VI.\nSOC FGDS PSRG SOC Joseph Laniel I. Pierre Pflimlin indirectement. Maurice Faure indirectement. Edgar Faure I. Robert Schuman I.\nPont des Arts - Wikipedia\nBetween 1802 and 1804, under the government of Napoleon Bonaparte, a nine-arch metallic bridge for pedestrians was constructed at the location of the present day Pont des Arts: this was the first metal bridge in Paris.The engineers Louis-Alexandre de Cessart and Jacques Dillon initially conceived of a bridge which would resemble a suspended garden, with trees, banks of flowers, and benches.\nottavianelli.eu 46, likes · 55 talking about this. Portail des communes de France: nos coups de coeur sur les routes de France. Les infos, chiffres, immobilier, hotels & le Mag ottavianelli.eu Between and , under the government of Napoleon Bonaparte, a nine-arch metallic bridge for pedestrians was constructed at the location of the present day Pont des Arts: this was the first metal bridge in ottavianelli.eu engineers Louis-Alexandre de Cessart and Jacques Dillon initially conceived of a bridge which would resemble a suspended garden, with trees, banks of flowers, and benches. Le Monde de Narnia, chapitre 3: L\'Odyssée du Passeur d\'Aurore DVDrip, p. 8,,\nA central landmark Frivole Bar St Pauli the city, it is located on the Right Bank of the Seine in the city\'s 1st arrondissement district or ward. Approximately 38, objects from prehistory to the 21st century are exhibited over an area of 72, square meterssquare feet.\nDue to the COVID pandemicthe museum was closed for days inand attendance plunged by 72 percent to 2. The museum is housed in the Louvre Palaceoriginally built in the late 12th to 13th century under Philip II. Remnants of the Medieval Louvre fortress are visible in the basement of the museum.\nDue to urban expansion, the fortress eventually lost its defensive function, and in Francis I converted it into the primary residence of the French Kings. InLouis XIV chose the Palace of Versailles for his household, Le Chapeau De Mitterrand Film the Louvre primarily as a place to display the royal collection, including, froma collection of ancient Greek and Roman sculpture. The museum opened on 10 August with an exhibition of paintings, the majority of the works being royal and confiscated church property.\nBecause of structural problems with the building, the museum was closed in until The collection was further increased during the reigns of Louis XVIII and Charles Xand during the Second French Empire the museum Russian Mother Tube 20, pieces.\nHoldings have grown steadily through donations and bequests since the Third Republic. The collection is divided among eight curatorial departments: Egyptian Antiquities ; Near Eastern Antiquities ; GreekEtruscanand Roman Antiquities ; Islamic Art ; Sculpture ; Decorative Arts ; Paintings; Prints and Drawings.\nThe Louvre museum is located inside the Louvre Palacein the center of Paris, adjacent to the Tuileries Gardens. A secondary entrance at the Porte des Lionsnear the western end of the Denon Wing, was Le Chapeau De Mitterrand Film in but is not permanently open.\nThe museum\'s entrance conditions have varied over time. Initially, artists and foreign visitors had privileged access, a feature that only disappeared in the s. The Louvre Palacewhich houses the museum, was begun by King Philip II in the late 12th century to protect the city from the attack from the West, as the Kingdom of England still held Normandy at the time. Remnants of the Medieval Louvre are still visible in the crypt. The origins of the name ""Louvre"" are somewhat disputed. The Louvre Palace changed a lot over the centuries.\nIn the 14th century, Charles V converted the building from its military role into a residence. InFrancis I started its rebuilding in French Renaissance style.\nThe royal move away from 2. 1 11 119 resulted in the Louvre being used as a residence for artists, under Royal patronage. Meanwhile, the collections of the Louvre originated in Iserv Molitorisschule acquisitions of paintings and Le Chapeau De Mitterrand Film artworks by the monarchs of the House of France.\nFrancis acquired what would become the nucleus of the Louvre\'s holdings, his acquisitions including Leonardo da Vinci \'s Mona Lisa. The Cabinet Escort Boy Amsterdam Roi consisted of seven rooms west of the Galerie d\'Apollon on the upper floor of the remodeled Petite Galerie. Many of the king\'s paintings were placed in these rooms inwhen it became an art gallery, accessible to certain art lovers as a kind of museum.\nInafter the court moved to Versailles, 26 of the paintings were transferred there, somewhat diminishing the collection, but it is mentioned in Paris guide books from on, and was shown to ambassadors from Siam in By the midth century there were an increasing number of proposals to create a public gallery in the Louvre.\nOn 14 OctoberLouis XV decided on a display of 96 pieces from the royal collection, mounted in the Galerie royale de peinture of the Luxembourg Palace. A hall was opened by Le Normant de Tournehem and the Marquis de Marigny for public viewing of the ""king\'s paintings"" Tableaux du Roy on Wednesdays and Saturdays.\nThe Luxembourg gallery included Andrea del Sarto \'s Charity and works by Raphael ; Titian ; Veronese ; Rembrandt ; Poussin or Van Dyck. It closed in as a result of the royal gift of the Luxembourg palace to the Count of Provence the future king, Louis XVIII by the king in Many design proposals were offered for the Louvre\'s renovation into Le Chapeau De Mitterrand Film museum, without a final decision being made on them.\nHence the museum remained incomplete until the French Revolution. The Louvre finally became a public museum during the French Revolution. In Maythe National Constituent Assembly declared that the Louvre would be ""a place for bringing together monuments Horler Playboy all the sciences and Le Chapeau De Mitterrand Film. Because of fear of vandalism or theft, on 19 August, the National Assembly pronounced the museum\'s preparation as urgent.\nIn October, a committee to ""preserve the national memory"" began assembling the collection for display. The public was given free accessibility on three days per week, which was ""perceived as a major accomplishment and was generally appreciated"".\nThe early days were hectic. Privileged artists continued to live in residence, and the unlabeled paintings hung ""frame to frame from floor to ceiling"". It reopened on 14 Julyarranged chronologically and with new lighting and columns.\nThe collection grew through successful military campaigns. This treaty marked the completion of Napoleon\'s conquest of Italy and the end of the first phase of the French Revolutionary Wars. The Nile was later returned to Rome,  whereas the Tiber has remained in the Louvre to this day. The despoilment of Italian churches and palaces outraged the Italians and their artistic and cultural sensibilities.\nAfter the French defeat at Waterloothe looted works\' former owners sought their return. The Louvre\'s administrator Denon was loath to comply in absence of a treaty of restitution. In response, foreign states sent emissaries to London to seek help, and many pieces were returned, though far from all.\nDuring the Bourbon Restoration —Louis XVIII and Charles X added to the collections. Following the July RevolutionKing Louis Philippe focused his interest on the repurposing of the Palace of Versailles into a Museum of Le Chapeau De Mitterrand Film History conceived as a project of national reconciliation, and the Louvre was kept in comparative neglect.\nThe Assyrian Museum opened on 1 Dreammovis As a consequence, the works were removed after Louis-Philippe was deposed inand were eventually auctioned away in The rule of Napoleon III was transformational for the Louvre, both the building and the museum.\nInhe Collectif Catastrophe 11, artworks including paintings, Le Chapeau De Mitterrand Film gold and Le Chapeau De Mitterrand Film antiquities of the Campana collection. Between andthe museum added 20, new artefacts to its collections. The main change of that period was to the building itself. The Louvre narrowly escaped serious damage during the suppression of the Paris Commune.\nOn 23 Mayas the French Army advanced into Paris, a force of Communards led by Jules Bergeret [ fr ] set Le Chapeau De Mitterrand Film to the adjoining Tuileries Palace. The fire burned for forty-eight hours, entirely destroying the interior of the Tuileries and spreading to the north west wing of the museum next to it.\nBut the museum was saved by the efforts of Paris firemen and museum employees led by curator Henry Barbet de Jouy [ fr ]. Following the end of the monarchy, several spaces in the Louvre\'s South Wing went to the museum. Meanwhile, during the Third Republic Naked Mature Men the Louvre acquired new artefacts mainly via donations, gifts, and sharing arrangements on excavations abroad.\nThe item Collection La Cazedonated in by Louis La Cazeincluded works by Chardin ; FragonardRembrandt and Watteau. The expansion of the museum and its collections slowed after World War I, however, despite some prominent acquisitions such as Georges de La Tour \'s Saint Thomas and Baron Edmond de Rothschild \'s donation of 4, prints, 3, drawings, and illustrated books.\nThe Louvre\'s extensive collections of Asian art were moved to the Guimet Museum in Nevertheless, the Louvre\'s first gallery of Islamic art opened in In the late s, Louvre Director Henri Verne [ fr ] devised a master plan for the rationalization of the museum\'s exhibitions, which was partly implemented in the following decade. The Cour du Sphinx in the South Wing was covered by a glass roof in During World War IIthe Louvre conducted an elaborate plan of evacuation of its art collection.\nNew arrangements after the war revealed the further evolution of taste away from the lavish decorative practices of the late 19th century. InEdmond Guillaume\'s ceiling ornaments were removed from the Salle des Etats where the Mona Lisa was first displayed in New exhibition spaces of sculptures ground floor and paintings first floor opened there later in the s, on a design by government architect Olivier Lahalle.\nIn I. The open spaces surrounding the pyramid were inaugurated on 15 Octoberand its underground lobby was opened on 30 March Further rooms in the same sequence, designed by Italo Rotaopened on 15 December On 18 NovemberMitterrand inaugurated the next major phase of the Grand Louvre plan: the renovated North Richelieu Wing in the former Finance Ministry site, the museum\'s largest single expansion in its entire history, designed by Pei, his French associate Michel Macary, and Jean-Michel Wilmotte.\nFurther underground spaces known as the Carrousel du Louvrecentered on the Inverted Pyramid and designed by Pei and Macary, had opened in October Other refurbished galleries, of Italian sculptures and Egyptian antiquities, opened in The third and last main phase of the plan unfolded mainly inwith new renovated rooms in the Sully and Denon wings.\nA new entrance at the porte des Lions opened inleading on the first floor to new rooms of Spanish paintings. As ofthe Louvre\'s visitor count had doubled from its pre-Grand-Louvre levels. President Jacques Chiracwho had succeeded Mitterrand ininsisted on the Le Chapeau De Mitterrand Film of non-Western art to the Louvre, upon a recommendation from his friend the art collector and dealer Jacques Kerchache [ fr ].\nThat new section opened on 22 September Anita Badeanzug Aileen, together with collections from the Roman-era Eastern Mediterranean, with financial support from the Al Waleed bin Talal Foundation and on Lustige Dildos design by Mario Bellini and Rudy Ricciotti. InAmerican painter Cy Twombly completed a new ceiling for the Salle des Bronzes the former Salle La Cazea counterpoint to that of Braque installed in in the adjacent Salle Henri II.\nThe room\'s floor and walls were redesigned in by Louvre architect Michel Goutal to revert the changes made by his predecessor Albert Ferran in the late s, triggering protests from the Cy Twombly Foundation on grounds that the then-deceased painter\'s work had been created to fit with the room\'s prior decoration.\nIt was closed for six months during French coronavirus lockdowns and saw visitor numbers plunge to 2. The Cour Visconti \'s ground floor covered to host the new Islamic Art Department in Islamic art display in the covered Cour Visconti 3dxxx, The department\'s origins lie in the royal collection, but it was augmented by Napoleon\'s expeditionary trip with Dominique Vivantthe future director of the Louvre.\nGrowth continued via acquisitions by Auguste Mariettefounder of the Egyptian Museum in Cairo. Mariette, after excavations at Memphissent back crates of archaeological finds including The Seated Scribe. Holdings include art, papyrus scrolls, mummies, tools, clothing, jewelry, games, musical instruments, and weapons.\nMiddle Kingdom art, ""known for its gold work and statues"", moved from realism to idealization; this is exemplified by the schist statue of Amenemhatankh and the wooden Offering Bearer. The New Kingdom and Coptic Egyptian sections are deep, but the statue of the goddess Nephthys and the limestone depiction of the goddess Hathor demonstrate New Kingdom sentiment and wealth. The Gebel el-Arak Knife ; BC; handle: elephant ivory, blade: flint; length: The Seated Scribe ; — BC; painted limestone and inlaid quartz ; height: The Great Sphinx of Tanis ; circa BC; rose granite; height: cm, width: cm, thickness: cm.']"	['<urn:uuid:87327cf7-2ae5-4ffb-875c-e32dac8a335e>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	10	27	2942
67	I'm looking at a property in Shadow Wood Preserve and want to know more about the environmental work there - what kind of environmental protection measures are in place on the property?	Shadow Wood Preserve's 422-acre development includes extensive environmental mitigation measures: 85 acres of wetland preservation, 13 acres of upland preservation, 30 acres of wetland enhancement, two acres of wetlands restoration and creation, and one acre of littoral shelves around the western storm water management lakes planted with native wetland vegetation.	['Florida’s ecology is delicate and irreplaceable, which makes it a critical element in many engineering projects. The services that our environmental team offers is ecological assessments, threatened and endangered species surveys habitat restoration, preparation of permit applications, mitigation design, digital image mapping and wetland determinations. Through careful planning, a balance between nature and man can be achieved, preserving the environment for future generations.\nMidfield Terminal Expansion – Project Millennium\nOur environmental team was responsible for providing the environmental services required at the project site and the 7,000-acre offsite mitigation area. The mitigation park was the recipient of the Environmental Achievement Award in 2004 presented by the Airport Council North America and the Florida Airports Council Environmental Award for 2008. The extensive habitat mapping, species surveys, water level data and habitat evaluations formed the input for the overall mitigation park management plan. Our team collected and analyzed all of the relevant data to prepare the mitigation park plan that consists of 500-acres of wetland creation, monitoring, controlled burns and maintenance. Southwest Florida International Airport’s Project Millennium is the largest capital improvement project in the history of Southwest Florida.\nFlorida Gulf Coast University\nWe provide ecological monitoring, environmental planning and environmental permitting services at FGCU. This includes implementation of an intensive mitigation plan that includes wetland creation, wetland restoration, and upland and wetland enhancement. Our team regularly conducts wildlife surveying for the eastern indigo snake, and has developed and implemented a bird flight path monitoring protocol.\nLee County Port Authority\nAs the Lee County Port Authority’s general environmental and planning consultant, we service both the Southwest Florida International Airport and Page Field General Aviation Airport. We are responsible for all mitigation monitoring, compliance and land management oversight for 7,400 acres of onsite and offsite conservation lands, water quality monitoring, NPDES monitoring, listed species surveys, and preparation of a Wildlife Hazard Assessment. Johnson Engineering also provides planning and environmental services for master plan updates and comprehensive plan amendments.\nPost-Restoration Monitoring – Kissimmee River Restoration Project\nThe South Florida Water Management District is working to restore a portion of the historic flood plain of the Kissimmee River corridor. By doing this, areas that are now appropriate habitat for the crested caracara may become flooded. The SFWMD selected our environmental team to conduct post-restoration monitoring of the caracara population within the Kissimmee River corridor to study how they adapt to large habitat changes across this significant part of the population’s range. Post-restoration monitoring focused on providing additional data on the use of the Kissimmee River corridor by both breeding and non-breeding crested caracaras.\nTreviso Bay Mitigation Monitoring\nWe provided services for the environmental permitting and mitigation monitoring required for Treviso Bay, a 1,000 acre mixed-use community in Naples, Florida. Mitigation was required for wetland impacts, and impacts to gopher tortoise and panther habitat. A combination of onsite and offsite mitigation was utilized on this project. Offsite mitigation includes 110 acres within the Florida Department of Environmental Protection owned Rookery Bay, purchase of mitigation credits from two mitigation banks, and 300 acres for panther mitigation in Hendry County, Florida. The mitigation program includes prescribed burning, water level monitoring, chloride monitoring, wildlife utilization and vegetation. The project required approvals from Collier County, the U.S. Army Corps of Engineers (USACE), the South Florida Water Management District (SFWMD), the Environmental Protection Agency, the U.S. Fish and Wildlife Service and the Florida Fish and Wildlife Conservation Commission.\nShadow Wood Preserve Mitigation Monitoring\nWe performed wetland mitigation monitoring as required by the U.S. Army Corps of Engineers Permit (USACE) and SFWMD environmental resource permit for the 422-acre development of Shadow Wood Preserve. The mitigation plan consists of a combination of onsite and offsite mitigation. Onsite mitigation includes 85 acres of wetland preservation, 13 acres of upland preservation, 30 acres of wetland enhancement and two acres of wetlands restoration and creation, as well as one acre of littoral shelves created around the perimeter of the western most storm water management lakes and planted with native wetland vegetation. Offsite mitigation was conducted on state lands and included wetland enhancement and restoration, and wetland creation totaling 60 acres.']	['<urn:uuid:0c1a3da4-d1a8-46bf-9d40-070fcd7f1637>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	32	50	684
68	As a quality control engineer, I'm interested in understanding how electron beam welding quality assurance works and what potential processing difficulties might affect the inspection process. Can you explain both the tell-tale marking system for weld inspection and the visual perception challenges that could impact quality control?	The tell-tale system for electron beam welding quality assurance works by machining witness lines on each side of the joint. During welding, the beam is deflected to leave intermittent marks on these lines, requiring only a fraction of a Joule of energy and taking microseconds to complete. If the tell-tale marks align with the witness lines, it confirms accurate beam positioning. However, visual inspection can be complicated by visual perception difficulties, where an inspector might have trouble distinguishing subtle differences in shapes or experience figure-ground problems when focusing on specific details among many visual elements. They might skip lines or read the same line twice, which could impact the effectiveness of the visual inspection process. These visual processing issues could affect the ability to accurately detect weld inconsistencies, even when the tell-tale system is properly implemented.	"[""Frequently Asked Questions\nTechnologists at TWI have developed a tell-tale system\nto reveal any potentially detrimental inconsistencies in electron beam welding\nIn the last 30 years, electron beam welding has developed from a technique used only in the aerospace and nuclear industries, to a versatile production tool used to join a wide variety of materials, in thicknesses from hundredths of a millimetre to hundreds of millimetres.\nThe range of industries in which electron beam welding is exploited in similarly impressive, and includes high-volume operations such as the automotive industry. The vast majority of electron beam welds are produced for this type of industry; hardly surprising when many electron beam machines (each capable of producing hundreds of parts per hour) are frequently operated on three shifts.\nOne of the prime considerations for any welding process is quality assurance. Fortunately electron beam welding is a highly reproducible process owing in part to the quality and durability of the equipment. Many of the hundreds of electron beam machines operating worldwide are at least 10 and sometimes 20 years old, but are still capable of producing excellent results. Most parameters defining the weld are electrical; consequently some electron beam machines can be fitted with automatic systems for monitoring them. Although this does not provide an absolute guarantee, it can still promote a higher degree of weld reproducibility.\nAlternatively, some kind of non-destructive examination of the welded joints can be undertaken. However, this is not always either possible or desirable, particularly if the part throughput is high. In production where neither of the above techniques is practised, weld quality rests on the consistency of the components in terms of fit-up, jigging and residual magnetism. A cursory visual inspection of the welded joint is unlikely to reveal any hidden defects. It is even possible for the beam to miss the joint entirely while the weld top bead still lies centrally in the joint preparation. Unfortunately, the first indication that such a component is defective may well be when it fails in service.\nThe principle of operation of the tell-tale system is simple: In addition to the normal features machined on the components, a witness line is machined on each side of, and equidistant from, the joint. During welding, the line is marked intermittently by the beam itself, which is deflected out of the weld pool to leave a tell-tale mark. The energy required to leave a tell-tale mark is generally a fraction of a Joule. Marking is thus accomplished in a few tens or hundreds of microseconds, i.e. so quickly that the normal welding operation is not affected. A straightforward visual inspection of the component after welding will reveal any variation in weld quality. So long as the tell-tale marks lie over the witness lines, it is certain that the beam has hit the joint accurately. Defects, are easily identified and the component immediately rejected or reworked.\nApplication of this new technique can result in an end product of consistently higher quality. It may also be used to supplement or replace existing quality assurance procedures. With the potential benefit of reducing production losses due to stoppages, it can help identify problems before defective components are produced.\nThe tell-tale marking is just one of a number of techniques that can be used to improve the quality and reliability of electron beam welded products. TWI's Electron Beam Group can help with both the expertise and the hardware required to implement any of these techniques.\nFor further information see Joining Technologies or please contact us."", 'Specific learning disability means a disorder in one or more of the basic psychological processes involved in understanding or in using language, spoken or written, which may manifest itself in an imperfect ability to listen, think, speak, read, write, spell, or to do mathematical calculations. The term includes such conditions as perceptual handicaps, brain injury, minimal brain dysfunction, dyslexia, and developmental aphasia. The term does not include children who have problems that are primarily the result of visual, hearing, or motor disabilities, or mental retardation, emotional disturbance, or of environmental, cultural, or economic disadvantage.\nThe types of LD are identified by the specific processing problem. They might relate to getting information into the brain (Input), making sense of this information (Organization), storing and later retrieving this information (Memory), or getting this information back out (Output). Thus, the specific types of processing problems that result in LD might be in one or more of these four areas.\nInformation is primarily brought into the brain through the eyes (visual perception) and ears (auditory perception). An individual might have difficulty in one or both areas.\nAuditory Perception. (Also called Receptive Language) The individual might have difficulty distinguishing subtle differences in sound (called phonemes) or might have difficulty distinguishing individual phonemes as quickly as normal. Either problem can result in difficulty processing and understanding what is said. Individuals might have difficulty with what is called auditory figure-ground. They have difficulty identifying what sound(s) to listen to when there is more than one sound.\nVisual Perception. One might have difficulty distinguishing subtle differences in shapes (called graphemes). They might rotate or reverse letters or numbers (d, b, p, q, 6, 9); thus misreading the symbol. Some might have a figure-ground problem, confusing what figure(s) to focus on from the page covered with many words and lines. They might skip words, skip lines, or read the same line twice. Others might have difficulty blending information from both eyes to have depth perception. They might misjudge depth or distance, bumping into things or having difficulty with tasks where this information is needed to tell the hands or body what to do. If there is difficulty with visual perception, there could be problems with tasks that require eye-hand coordination (visual motor skills) such as catching a ball, doing a puzzle, or picking up a glass.\nOnce information is recorded in the brain (input), three tasks must be carried out in order to make sense or integrate this information. First, the information must be placed in the right order or sequenced. Then, the information must be understood beyond the literal meaning, abstraction. Finally, each unit of information must be integrated into complete thoughts or concepts, organization.\nSequencing. The individual might have difficulty learning information in the proper sequence. Thus, he might get math sequences wrong, have difficulty remembering sequences such as the months of the year, the alphabet, or the times table. Or, she might write a report with all of the important facts but not in the proper order.\nAbstraction. A person might have difficulty inferring the meaning of individual words or concepts. Jokes, idioms, or puns are often not understood. He might have problems with words that might have different meanings depending on how they are used. For example, ""the dog"" refers to a pet. ""You dog"" is an insult.\nOrganization. An individual might have difficulty organizing materials, losing, forgetting, or misplacing papers, notebooks, or homework assignments. She might have difficulty organizing her environment, such as her bedroom. Some might have problems organizing time. They have difficulty with projects due at a certain time or with being on time. (Organization over time is referred to as Executive Function.)\nThree types of memory are important to learning. ""Working memory"" refers to the ability to hold on to pieces of information until the pieces blend into a full thought or concept. For example, reading each word until the end of a sentence or paragraph and then understanding the full content. ""Short-term memory"" is the active process of storing and retaining information for a limited period of time. The information is temporarily available but not yet stored for long-term retention. ""Long-term memory"" refers to information that has been stored and that is available over a long period of time. Individuals might have difficulty with auditory memory or visual memory.\nOne reads a sentence and hold on to it. Then the next and the next. By the end of the paragraph, he pulls together the meaning of the full paragraph. This is working memory. He continues to read the full chapter and study it. Information is retained long enough to take a test and do well. This is short-term memory. But, unless the information is reviewed and studied over a longer period of time, it is not retained. With more effort over time, the information might become part of a general body of knowledge. It is long-term memory.\nInformation is communicated by means of words (language output) or though muscle activity such as writing, drawing, gesturing (motor output). An individual might have a language disability (also called expressive language disability) or a motor disability.\nLanguage Disability. It is possible to think of language output as being spontaneous or on demand. Spontaneous means that the person initiates the conversation. Thoughts have been organized and words found before speaking. Demand language means that one is asked a question or asked to explain something. Now, she must organize his thoughts, find the right words, and speak at the same time. Most people with a language disability have little difficulty with spontaneous language. However, in a demand situation, the same person might struggle to organize her thoughts or to find the right words.\nMotor Disability. One might have difficulty coordinating teams of small muscles, called a fine motor disability. He might have problems with coloring, cutting, writing, buttoning, or tying shoes. Others might have difficulty coordinating teams of large muscles, called a gross motor disability. She is awkward when running or jumping.\nEach individual will have his or her unique pattern of LD. This pattern might cluster around specific common difficulties. For example, the pattern might primarily reflect a problem with language processing: auditory perception, auditory sequencing/abstraction/organization, auditory memory, and a language disability. Or the problem might be more in the visual input to motor output areas. Some people with LD will have a mixture of both.\nSymptoms of Learning Disabilities\nThe symptoms of learning disabilities are a diverse set of characteristics which affect development and achievement. Some of these symptoms can be found in all children at some time during their development. However, a person with learning disabilities has a cluster of these symptoms which do not disappear as s/he grows older.\nMost frequently displayed symptoms:\n- Short attention span\n- Poor memory\n- Difficulty following directions\n- Inability to discriminate between/among letters, numerals, or sounds\n- Poor reading and/or writing ability\n- Eye-hand coordination problems; poorly coordinated\n- Difficulties with sequencing\n- Disorganization and other sensory difficulties\nOther characteristics that may be present:\n- Performs differently from day to day\n- Responds inappropriately in many instances· Distractible, restless, impulsive\n- Says one thing, means another\n- Difficult to discipline\n- Doesn\'t adjust well to change\n- Difficulty listening and remembering\n- Difficulty telling time and knowing right from left\n- Difficulty sounding out words\n- Reverses letters\n- Places letters in incorrect sequence\n- Difficulty understanding words or concepts\n- Delayed speech development; immature speech']"	['<urn:uuid:c6dc1c01-3dc3-4d4c-83e5-3dae7a7cd10a>', '<urn:uuid:e1b7bc04-72bd-406b-9644-d678fa782a79>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	47	136	1828
69	size galatheid squat lobster species	The Galatheid crab, also called a squat lobster, is often only about four inches long according to the University of Delaware.	"['From Crabs to Faultlines, 7 Ways Updated Google Earth Journeys to the Ocean Floor\nThe same downloadable Google Earth program that lets you check out the mountains of Tuscany where your grandfather grew up and scope out the backyard of that house you\'re considering buying can take you underwater, too: The recently updated Google Earth includes maps of underwater topography that show off massive mountain ranges and volcanoes, while scientists at Ridge 2000 added a tour that takes you around the ocean to hydrothermal vents, sealife, and fields of plants.\n1. East Pacific Rise\nScreenshot: Google Earth\nThe Pacific Ocean\'s East Pacific Rise is a chain of volcanoes that run parallel to South America with peaks as high as 9,000 feet.\nThis point on Google Seafloor includes a look at one specific area of the rise, where tectonic plates are slowly separating -- transforming faults that run across the edge of the rise.\n2. Galatheid CrabScreenshot: Google Earth\nThis Galatheid crab -- also called a squat lobster -- makes its home on the East Pacific Rise, although the species is found all over the world.\nSince they thrive on bacteria, they congregate near the hydrothermal vents and, though it looks large here, they often are only about four inches long according to the University of Delaware.\n3. Hydrothermal VentScreenshot: Google Seafloor\nSome of the marked points on the seafloor trade high-quality images for fascinating videos, like this one of a hydrothermal vent.\nHydrothermal vents, once believed to be silent, are fissures that appear in areas near ocean basins or plate movements, and which release geothermally-heated water into the ocean. The minerals that build up when the water meets the ocean can solidify and stack on top of each other, reaching up to 200 feet high.\n4. SeastarsScreenshot: Google Seafloor\nThe closeups can also give viewers a unique look at the plants and animals that make their home on the seafloor -- like these seastars spotted in the Lau Back-Arc Basin in the Western Pacific Ocean.\nYou\'re more likely to find these creatures on vents that are no longer active.\n5. Kilo Moana Vent FieldScreenshot: Google Seafloor\nThese stacked barnacles were photographed on the Kilo Moana vent field -- part of the Lau Bain -- and are just one more of the amazing examples of underwater life found near the vents (though this batch is growing on a cooled vent rather than a live one).\nAccording to the Sylvia Earle Alliance, these barnacles don\'t move, so they ""extend arm-like appendages out of their shells to catch food.""\n6. Mariner Vent FieldScreenshot: Google Seafloor\nAnother close-up of a hydrothermal vents -- this time in the Lau Basin\'s Mariner field -- offers an even better look at these natural wonders. In this area, the vents grow as high as 78 feet and, the caption points out, are unique among other fields because the surrounding sea life is much less diverse than in other parts of the ocean.\n7. Sully at Juan De Fuca RidgeScreenshot: Google Seafloor\nThe Juan De Fuca Ridge, a 300-mile long stretch of underwater volcanoes found off the coasts of Washington and Oregon, allows for a world of thriving bacteria and lifeforms -- like these colorful tubeworms.\nThe website for the Marine Conservation Biology Institute says this about the Sully vent, shown here:\nCreated by the separation of the Juan de Fuca Plate and the Pacific Plate, the Juan de Fuca Ridge is home to an extraordinary community of life whose source of energy is not the sun, but sulfur-rich chemicals. Deriving energy through chemosynthesis, bacteria form the basis of a deep-sea food chain that supports unusual creatures such as red-and-white tubeworms, deep-sea crabs, and mussels.\nMore on Ocean Life\n10 Real Life Sea Monsters\nStunning Underwater Plants and Sea Life on the Ocean Floor\nKnow Your Marine Life']"	['<urn:uuid:74d35b46-a0b3-46aa-a01c-6394b8177817>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	5	21	639
70	Which process stores energy and which releases it?	Photosynthesis stores energy by producing sugar (glucose) molecules, while cellular respiration releases energy by breaking down those organic molecules. During photosynthesis, plants trap the sun's light energy and convert it to chemical energy in sugar, while in respiration, this stored energy is released in the form of ATP.	"['The Carbon Cycle: Sources and Sinks\nCompounds that contain the element carbon are referred to as ""organic."" They are present in all living things. Carbon is continually moving among Earth\'s lithosphere, hydrosphere, biosphere, and atmosphere in various forms: as carbon dioxide (CO2) in the atmosphere, sugars or carbohydrates (CnH2nOn) in living organisms, and calcium carbonate (CaCO3) in rocks and minerals, to name just a few. The movement of carbon among Earth\'s spheres, as diagrammed below, is known as the carbon cycle.\nThe black numbers in the diagram indicate how much carbon is stored in carbon sinks (areas of storage) in billions of tons (gigatons—GtC). The arrows show how carbon moves among Earth’s spheres.\nGreen plants play a very important role in the carbon cycle. They absorb carbon dioxide (CO2) from the atmosphere and produce carbon-containing sugars. This process is called photosynthesis. There are two main steps in photosynthesis. First, plants trap the sun\'s light energy in a compound called chlorophyll. This energy is converted to a chemical form called adenosine triphosphate (ATP). In the second step plants use the energy from ATP to produce sugar (C6H1206). The process of photosynthesis requires water (H2O). It also produces water as well as oxygen (O2). The net chemical reaction for the process of photosynthesis is\n6 CO2 + 6 H2O + sunlight ------> C6H12O6 + 6 O2.\nAnimals eat plants to obtain the energy trapped during photosynthesis. As the animals\' bodies break down the carbohydrates in the plant tissue, CO2 is released to the atmosphere. This process is called respiration. The net chemical reaction for the process of respiration is the exact opposite of photosynthesis: 6 O2 + C6H12O6 ------> 6 H2O + 6 CO2. Plants respire also as they break down the organic molecules in themselves in order to release the stored energy. Plants and animals also release CO2 to the atmosphere when they decay.\nWhen dead plants and animals slowly decay under high pressure and high temperatures, they may eventually form pools of energy known as fossil fuels. Coal, oil, and natural gas are fossil fuels. People burn fossil fuels to release the energy stored in them. The energy is used for heat, operating automobiles, etc.\nSince the Industrial Revolution humans have burned increasingly greater amounts of fossil fuels in order to produce more energy. As the practice of burning fossil fuels grows, so does the amount of carbon dioxide emitted to the atmosphere. Historical data from ice cores and modern data collected from the Mauna Loa Observatory in Hawaii support this.\nAs a greenhouse gas, CO2 increases the rate of global climate change. This is because CO2 contributes so greatly to the greenhouse effect. The sets of graphs below show carbon dioxide variations over thousands of years. Data was collected from various sources, such as ice core sampling. Further below is a graph comparing CO2 and temperature levels over thousands of years. Do you see a distinctive pattern across the data visualizations presented?\nThis figure of temperature and carbon dioxide is from the Marian Koshland Science Museum: Variations of Northern Hemisphere Surface Temperature and Carbon Dioxide. Additional information: The image and text excerpt below are from a website by RickyRoad, a faculty member at U Michigan and who leads a course on climate change problem solving.\n""In this figure there are variations in the temperature field that are not present in the carbon dioxide field. Scientific rigor requires that we investigate this variability, and with some confidence we can attribute the variability prior to about 1800 to volcanic activity and changes in the sun. There is no doubt that the further back we go from the present time, the greater the uncertainty. We, today, have much better observations of temperature, of solar variability, of volcanic activity, of carbon dioxide; we have better measurements of every quantity. Does the fact that we have better measurements today devalue the knowledge from the past? Change it?"" --Ricky Road\nThere are many cooperative programs around the world that monitor carbon dioxide emissions, sources, and sinks.\nSource: Earth System Research Laboratory—Global Monitoring Division, http://www.esrl.noaa.gov/gmd/Photo_Gallery/GMD_Figures/ccgg_figures/tn/ccggmap.png.html\nResearch stations and observatories measure atmospheric carbon dioxide as well as other greenhouse gases, such as methane, carbon monoxide, hydrogen, nitrous oxide, and sulfur hexafluoride. The data obtained from these stations and sites are invaluable contributions to the science of global climate change.\nBarrow, AK, Observatory (BRW) is located on the northernmost point of the United States. Source: http://www.esrl.noaa.gov/gmd/obop/brw/\nU.S. Climate Reference Network climate observing station on Mauna Loa in Hawaii (Mauna Kea in the background).\nImage source: NOAA Satellite and Information Service: http://www.ncdc.noaa.gov/oa/wdc/index.php\nThe South Pole Observatory (SPO) is located at the geographic South Pole on the Antarctic plateau. Source: http://www.esrl.noaa.gov/gmd/obop/spo/\nThe Mauna Loa Observatory in Hawaii, run by the National Oceanic and Atmospheric Administration (NOAA) is known for its research on atmospheric carbon dioxide concentration.', 'To emphasize this point even more, the equation for photosynthesis is the opposite of cellular respiration. If they stop respiring, they will die. In addition to glucose, this reaction produces oxygen that is released by the plants into the atmosphere. On the contrary, photosynthesis occurs in green plants, which contain chlorophyll and uses sunlight and water to convert it into energy. Photosynthesis and respiration in plants. 5.\nOxygen and glucose are both reactants in the process of cellular respiration. 57% average accuracy. Make sure you are able to answer the Essential questions at the end of the PowerPoint after you have moved through the PowerPoint.\nWithout photosynthesis, there would be no plant and animal life. During cellular respiration, one glucose molecule combines with six oxygen molecules to produce water, carbon dioxide and 38 units of ATP. 1. mmaeda. Biology.\nApart from plants, cellular respiration also takes place in animals and is the process through which energy is released from organic compounds.\nNonetheless, fundamentally they are quite different from one another. The interchange of oxygen and carbon dioxide by cellular respiration and photosynthesis helps in maintaining both … Plants use the sunlight along with carbon dioxide and water to form glucose, a simple sugar. Humans, animals and plants depend on the cycle of cellular respiration and photosynthesis for survival. Through these processes, plants obtain the carbon dioxide they need and living organisms obtain the oxygen … Respiration is a metabolic process common to all living things. 8 months ago. Photosynthesis contains the manufacturing of NADPH whereas cellular respiration contains the manufacturing of every FADH and NADH. Cellular respiration takes place in the every living organisms, as it is the simple process of converting oxygen and glucose into carbon dioxide and water back, therefore producing energy for the cells of the body. It is important to understand the differences between the two.\n3. Edit. Plant cells respire, just as animal cells do. At least Flash Player 8 required to run this simulation.\nPhotosynthesis and cellular respiration are two biochemical reactions which have a crucial role to play when it comes to life on the planet Earth. Photosynthesis and cellular respiration are essentially the reverses of one another where photosynthesis is an anabolic process resulting in the formation of organic molecules. Unlike photosynthesis cellular respiration occurs in.\nWhich of the following is the correct equation for PHOTOSYNTHESIS? In respiration energy is released in the form of ATP’s, on the other hand, in photosynthesis, the energy is stored in the form of Respiration is the catabolic process, while photosynthesis is the anabolic process. Without photosynthesis, there would be no sugar, without which there could be no cellular respiration. 182 times. Read through the PowerPoint slides on Photosynthesis and Cellular Respiration.\nPhotosynthesis is an anabolic process that ends in the production of carbohydrate molecules.\n9th grade. 6O 2 + C 6 H 12 O 6-> 6CO 2 + 6H 2 O + Energy. When you go through the concepts of cellular respiration and photosynthesis, you are bound to find some striking similarities.\nPhotosynthesis vs. Cellular respiration - - Photosynthesis and cellular respiration are complementary processes by which living things obtain needed substances. The following are general representations (formulae) for both photosynthesis and cellular respiration: Cellular Respiration (aerobic) C6H12O6 + 6O2 → 6CO2 + 6H2O + 32 ATP.\nIn photosynthesis, the source of energy while in cellular respiration, chemical bonds are the source of energy. Drag each tile to the correct box. The processes of photosynthesis and cellular respiration are linked to each other.\nanswer choices Respiration is different than cellular respiration, which produces energy in cells. Photosynthesis vs Cellular Respiration DRAFT. Photosynthesis occurs in the presence of light while cellular respiration is a continuous activity that likes to happen all the time.\nLearn vocabulary, terms, and more with flashcards, games, and other study tools. Title: Photosynthesis and Cellular Respiration 4.']"	['<urn:uuid:83e9af7a-3698-4a0b-bc9e-a6d41e61ac99>', '<urn:uuid:f0788de7-e230-4156-bef8-3bc7412edd18>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	8	48	1451
71	quality standards price differences artisan chocolate mass production	Artisanal bean-to-bar chocolates command higher prices due to their superior quality and production methods. The best cacao represents only the top 5% of the world's crop, as seen in Fossa Chocolate's pricing of SG$8-10 for a 200-gram bar. These makers, including Dandelion Chocolate and Amano, work directly with cacao producers to ensure sustainable methods and high-quality beans. In contrast, store-bought mass-produced brands use chocolate compounds and higher sugar levels to compensate for lower quality ingredients. The artisanal makers maintain quality by producing in small batches, with companies like Fossa making only 500-600 bars daily.	"['The Best Cities For Bean-to-Bar Chocolate\nLike wines grown from different kinds of grapes, chocolates made from cacao beans sourced from Ecuador, Bolivia, and other countries offer deliciously different tastes. Visit any of these craft chocolatiers to sample their melt-in-your-mouth treats.\nXocolatl Small Batch Chocolate Micro-factory, Atlanta, Georgia\nThis jewel of Atlanta\'s chocolate scene was born in the steamy jungles of Puerto Viejo, Costa Rica, after founders Elaine Read and Matt Weyandt moved there with their children and began meeting local cacao growers and artisanal chocolate makers. Today, the company’s micro-factory, located in the city\'s Krog Street Market, grinds whole beans from small farms in the Americas and East Africa and turns them into chocolate liquor. You won’t find cocoa butter used at Xocolatl; the makers craft their single-origin, dark chocolate with only cocoa and cane sugar, and add ingredients like dried apple, sea salt, and peppermint to make scrumptious flavored bars.\nBook a tour and tasting to learn about the bean-to-bar process; a wine pairing is included. And don’t miss Xocolatl\'s Frozen Drinking Chocolate, a “slushy” beverage made with Nicaraguan chocolate in coconut milk, topped with coconut whipped cream and cacao nibs (nibs are like the ""chocolate chips"" of chocolate). For the holidays, the makers will release a new 60 percent dark milk chocolate bar.\nDandelion Chocolate, San Francisco, California\nVisit San Francisco’s Mission District to experience Dandelion Chocolate, where beans are roasted, cracked, ground and otherwise prepared before being molded into bars by hand. Dandelion, which has won multiple awards for its chocolate, works directly with cacao producers to buy high-quality beans grown by sustainable methods. Each year, the company leads trips to visit some of its bean producers. They’ve taken groups to Hawaii; Maya Mountain Cacao, in Belize; Ecuador; and the Dominican Republic. Contact them for details, or just treat yourself to a Kokoa Kamili Bar from Tanzania, with flavors of ripe mango and caramelized red berries, or a bar made from Venezuelan beans that tastes of roasted almond, dulce de leche and chocolate fudge. Dandelion\'s Madagascar Chocolate Bar offers a punch of raspberry and Meyer lemonade flavors.\nChocoVivo, Los Angeles, California\nLike the ancient Mayans and Aztecs, ChocoVivo grinds its beans with a mano and metate, tools that are used like a mortar and pestle. The only ingredients in ChocoVivo\'s products are whole cacao nibs and spices, so its dark chocolate – the only kind of chocolate the company makes—has a pure, stone-ground, traditional taste. “Chocolate is truly food, not a candy bar,” says owner and chocolatier Patricia Tsai. Look for bars made with 100 percent, 85 percent, 75 percent or 65 percent cacao, or try flavored chocolates like Cherries + Almonds + Black Peppercorns (crumble the bars to make a yummy trail mix).\nYou can also snack on Shangri-La Bars or melt them for a drinking chocolate. They’re made with beans from Tabasco, Mexico; toasted black sesame; unrefined cane sugar; and bits of chewy, organic Goji berries. They make for great pairings with white or light red wines. ChocoVivo hosts a variety of events so you can meet their growers, sample café beverages, and more.\nSolstice Chocolate, Salt Lake City, Utah and other locations\nRich, delicious Solstice small-batch chocolate is made from exotic, slow-roasted cacao beans. All the ingredients are 100 percent organic—and even its zero impact, zero waste facility embraces “green” initiatives, using 100 percent solar power and fully recyclable, resealable packaging. Try the 70 percent Ugandan Bundibugyo Dark Chocolate; its mildly bitter taste comes from cocoa powder made from fine Bundibugyo cacao beans, combined with bits of berries and tree fruit. Palos Blancos Bars are produced from beans harvested in a rainy area in Bolivia; they have a deep, chocolatey taste with hints of fresh cream and nuts. Check the website for a list and map of store locations.\nMadre Chocolate, Honolulu and Kailua, Hawaii and other locations\nTalk about paradise: Madre’s organic, fair-trade chocolates and the tropical beauty of these Hawaiian destinations make an unbeatable combination. The makers lightly process their beans to retain healthy antioxidants, and flavor some bars with fruits and spices traditionally used by the Aztec, Maya and Olmec tribes that invented chocolate.\nMadre has won 23 Hawaiian, U.S. and international awards for its chocolates, which include such treats as the Lili’koi Passionfruit Hawaii Chocolate Bar, Chipotle Allspice Bar, and Hibiscus Bar. Sign up for events or classes to make your own chocolate, or to enjoy a whiskey and chocolate pairing. Madre also offers farm to factory tours, wine and chocolate pairings, and an experience with craft coffee and chocolate. Madre products are available in Hawaii and other locations around the world.\nAmano Chocolate, Orem, Utah\nAward-winning Amano chocolatiers say they’re passionate about their craft; in fact, “amano” is Italian for both “by hand” and “they love.” They\'ve won international awards for products like their Raspberry Rose Bar, made with ground rose petals and raspberries mixed into smooth chocolate, and the Mango Chili Bar, which combines the creamy, fruity flavor of mangos with the warmth of chiles. One of the company’s original bars, the Madagascar 70 percent Dark Chocolate Bar, is packed with fruity citrus and berry tastes—and it earned a gold medal from London’s respected Academy of Chocolate. If you’re into making your own chocolate—and you have a generous budget—opt for ChocoVision\'s $2,250 Revolation Delta, sold by Amano, to melt and temper up to ten pounds of chocolate in an hour. (""Temper"" refers to making smooth, glossy chocolate.) The Revolation Delta is suitable for use in your home or a lab setting.', 'Dried shrimp and fish flakes are probably the last things that you’d expect to find in your chocolate. But Fossa Chocolate, a craft chocolatier based in Singapore, is all about pushing the boundaries of taste, conjuring new varieties every one to two months that surprise even the most passionate chocolate lovers.\nTheir shrimp and bonito dark chocolate was inspired by a trip to the famed Tsukiji fish market in Tokyo. Throwing seafood into chocolate provided an unexpectedly tasty umami-flavoured treat, reminiscent of Japanese cuisine is so well-loved for.\nThe trio behind Fossa is founder and chief chocolate maker Jay Chua, chocolatier Charis Chia, and business and marketing director Yilina Leong. They make what’s known as bean-to-bar chocolate, whereby the same company controls every stage of the production process.\nAll of Fossa’s chocolate is made by hand using the finest quality ingredients, producing 500 to 600 bars a day that are sold through its online store and selected retailers. It offers 18 varieties, including favourites such as sea salt, lychee rose dark milk chocolate, candied ginger and Uji matcha.\n“The Uji matcha chocolate is made from ‘watakake’-grade matcha – the highest grade of culinary matcha,” says Leong.\nWhile its products have naturally found popularity with foodies and chocolate lovers, the barista community is also a fan of Fossa’s single-origin chocolates. Like roasting coffee beans, roasting cacao bring out the beans’ complex, natural flavour notes. “We are able to draw these flavours out from the cacao through careful curation of the beans we bring in and refined roasting techniques,” she explains.\nStore-bought brands are made using chocolate compounds, and often have high sugar levels to make up for the loss in taste.\n“Health-conscious chocolate lovers also love this range because they are made with only two ingredients (cacao beans and sugar) which make them vegan, and free of gluten, additives or substitutes. Some of the cacao we use are also organic,” Leong says.\nFrom cocoa pod to chocolate bar\nThe journey from bean to bar typically spans continents and for Fossa begins with sourcing cacao from growers and fermentaries. The best cacao accounts for just the top 5 per cent of the world’s cacao crop and this is a major factor in the higher price of Fossa’s chocolate, which sells for SG$8 to $10 for a 200-gram bar.\nThe quality is most evident in Fossa’s single-origin bars, which are made only with cacao beans from the world’s top-producing regions, such as Tanzania, Indonesia and Madagascar.\nFarmers in these fertile regions have cultivated cocoa for several generations. They work with cacao fermenters, who have perfected their techniques over many years.\nLeong says the fermentation process is key – the correct technique will release the optimal level of taste required by chocolate makers – and it usually takes five to seven days. That is why bean-to-bar chocolatiers are picky about who they work with because poorly fermented beans make for an unsatisfactory product.\nFossa names their single-origin products after the farmers or estates from where the cacao has been sourced in order to educate consumers on where their chocolate comes from. For instance, Kokoa Kamili is named after the Kokoa Kamili Fermentary in Tanzania.\nAfter the beans have been fermented and sorted, they are brought into Fossa’s factory where flawed beans and debris are removed.\nThe beans are carefully roasted to further develop the flavours. Leong describes the roasting process as a crucial flavour-determining step, with each batch of cacao beans roasted just enough to bring out the right taste profiles.\nThe roasted beans are then cooled before they are passed through a machine to crack and remove the shells. Broken down into cocoa nibs, it is grounded into a paste called cocoa liqueur, which has a texture similar to nut butter. Sugar is added to smoothen the chocolate’s texture.\nThe cocoa liqueur is left to churn in a cacao bean grinder (known as a melangeur) for 48 hours. After this, the liqueur is tempered just enough to give it gloss and a rigidity for a “clean snap”. It is poured into moulds, cooled further into bars and the final product is hand-wrapped.\nPutting Singapore on the chocolate map\nThe founders’ love for bean-to-bar chocolate began when a friend brought them a single-origin chocolate bar from Madagascar as a Christmas gift. They had not tasted such fine chocolates before and were “blown away” by its taste, says Leong.\n“With Singapore being renowned for our high food quality and safety standards, we were appalled by the difficulty to find fine, real chocolates that are not made from mass-produced covertures or compounds,” she says.\n“Since then, we’ve embarked on our chocolate discovery journey – sourcing beans directly from farms and experimenting with micro-batch making in our home kitchen.”\nAll three were making chocolate at home just for fun, hand-peeling cacao beans and roasting them in their own ovens. But as word of their chocolates spread they decided it was time for them to make a career switch.\nWith some savings in tow, they gave up their comfortable office jobs and channelled their energies into Fossa. It was a leap of faith, says Leong.\n“We never thought it’d become a business. As more people tried our chocolates and there was demand, we reached the turning point where there was no way we could fulfil the orders without dedicating more time and resource. That was when we decided to pursue this full time,” she says.\nThey named their new outfit Fossa, after a cat-like creature native to Madagascar. “The fossa is also nimble, agile and well-known for its fierce and wild personality. This is similar to our approach in chocolate making, where we constantly challenge ourselves to create bold yet delicious and memorable flavours,” she says.\nTaking the leap into the unknown has paid off for the trio, with the brand garnering recognition both in Singapore and abroad.\nLast year, the salted egg cereal blond chocolate won a Bronze Award at the Asia Pacific International Chocolate Awards and Leong says it is “an all-time favourite”. “Surprisingly, even our customers in the US and Canada love it,” she adds.\nFossa has moved production from a shared kitchen into its very own facility and now exports to the Maldives and Hong Kong, as well as the US and Canada. But quality is something they will not compromise in spite of growing demand, so each batch of chocolate will be kept “small”, says Leong.\n“We now have a small team of chocolate makers and chocolatiers working very passionately to produce fine authentic chocolate from cacao beans. From roasting the beans, grinding and tempering in small batches and finishing them in hand packaged chocolate bars, the process is time-consuming but rewarding.”']"	['<urn:uuid:c702f64e-0613-4a2c-8ec2-2beab38b5bd9>', '<urn:uuid:e0b43dfc-e6ac-4d2f-acbb-c92fc67a2106>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-13T00:57:18.859842	8	94	2043
72	compare python libraries arcgis functions geospatial analysis visualization	Python libraries like GeoPandas, Shapely, and Fiona allow users to read, manipulate and analyze geospatial datasets in various formats like GeoJSON, KML and ESRI shapefiles. These tools enable basic operations like calculating distances between geographic entities and finding nearest neighbors. Meanwhile, ArcGIS provides a more comprehensive system that includes professional tools for advanced analysis, robust data management in 2D/3D/4D, creation of geographic databases, solving spatial analysis problems, and building map-based applications. ArcGIS also offers additional capabilities like publishing and sharing data through ArcGIS Online and Enterprise using web GIS, along with ready-to-use basemaps and templates.	['What is Geospatial Data? Types, Python code, Analytics & Visualization\nWelcome to my blog, where we explore the fascinating world of geospatial data!\nHave you ever wondered how analysts use Python to make sense of complex geographical datasets for visualizing and predicting trends in analytics? In this blog, I’ll break down the different types of geospatial data, discuss the advantages and disadvantages of using each type, and show you how Python can be used to make data analysis a breeze. So let’s get started on our journey through the world.\nGeospatial data is time-based data related to a specific location on the Earth’s surface. It can provide insights into relationships between variables and reveal patterns and trends.\nIntroduction to Geospatial Data\nGeospatial data is information that is tied to a specific location on the Earth. It can be used to represent things like roads, buildings, and geographical features like mountains and rivers. This data is often shown on maps, and it can be used for a variety of purposes, such as helping people find their way around a city, understanding the layout of a region, and studying how different parts of the Earth are connected and interact with each other. Geospatial data is collected using tools like satellites, GPS devices, and aerial photographs, and it is often stored in computer systems that can help us analyze and understand it in more detail.\nGeospatial data, also known as geographic data, refers to any data with a location element. It is usually associated with mapping data and can be used to create maps or analyze geographic trends. Geospatial data includes geography and spatial analysis aspects such as coordinates, elevation and shapefiles.\nPython is a popular language for working with geospatial data due to its libraries such as GeoPandas, Shapely, and Fiona. With these tools, users can read in existing geospatial datasets in various formats such as GeoJSON, KML and ESRI shapefiles. Users can also create new geospatial datasets from scratch using powerful Python tools such as geopandas.\nAnalytics of these datasets typically takes the form of calculating distances between different geographic entities such as cities or points on the earth surface or finding nearest neighbors for a given point. Visualization of this information is one of the key tasks for understanding any trends or patterns in the spatial dataset being examined. Common types of visualization include choropleth maps that display boundaries filled with color gradients based on certain attributes associated with those boundaries (such as population density).\nIn summary, Geospatial Data encompasses location-based information that can be analyzed and visualized to gain insight into any trends or patterns present within its related area. Python provides powerful resources for working with spatial information that make its manipulation relatively easy while analytics and visualization provide the best way to understand results derived from those manipulations.\nTypes of Geospatial Data\nGeospatial data refers to any data that has an associated geographic or spatial element, including points, lines and polygons. This type of location-based information is created by combining geographical information systems (GIS), remote sensing, and global positioning systems (GPS). Geospatial data may be collected from a variety of sources, including both digital and non-digital sources, such as aerial imagery, GPS receivers and surveyors.\nExamples of geospatial data include:\n- Point data: This type of geospatial data contains information about a specific point in the 3D space, such as the exact location of a tree or an airplane. It’s commonly stored in vector files with simple X, Y coordinates to represent the points on a map.\n- Line data: This type of geospatial data consists of a set of points connected by straight or curved lines. It’s typically used to show linear features such as roads or rivers in GIS mapping projects.\n- Polygonal/Area data: This type of geospatial information describes geographical areas with boundaries that are represented by adjacent polygonal shapes. It’s often used to represent land parcels such as tracts or counties when creating GIS maps.\nGeospatial analytics enable organizations collecting and using this type of location-based information to gain insights from their datasets through processing techniques like machine learning algorithms and advanced visualization tools for two dimensional (2D) or three dimensional (3D) output. Python libraries are also widely used for programming related to geospatial analysis tasks because they provide dedicated packages for working with GIS datasets like GeoPandas and Rasterio modules.\nGeospatial Data in Python\nPython is a popular language for data science and Machine Learning, and it has gained immense popularity due to its flexibility, scalability, and robust set of libraries. One of the most significant advantages of Python is its use in geospatial data processing. Geospatial data (GIS) is a term that describes information about geographical features. This data can be used for a variety of applications, from aerospace engineering to urban planning.\nGeospatial data in Python makes use of specific libraries built specifically for processing, analyzing, and visualizing GIS datasets. Some popular tools used for geospatial analysis in Python are GeoPandas, geopandasGF (which builds on geopandas), PySAL, Folium, PyQGIS (the official library for interacting with QGIS), Shapely, PyProj and more. These tools provide powerful mapping capabilities that allow users to integrate existing GIS datasets with their own analysis tools into user-friendly web or desktop applications.\nGeospatial analytics in Python includes working with both vector and raster data – such as geometries like points, lines, polygons , or pixels generated from aerial imagery – often further probed through linear regression algorithms such as Ordinary Least Squares or variograms born out of interpolation methods like Kriging . Integrating these analytical approaches with other types of statistical modelling can provide insights into new types of problems which cannot be solved by traditional GIS approaches alone.\nOther important topics include cartographic visualization techniques – ranging from basic plotting libraries up to crafting complex geographic charts – as well as pattern recognition through kernel density estimations or building spatially-constrained models with the help of spatial autocorrelation functions . All these topics require detailed knowledge about the underlying concepts that drive the respective task at hand. However due to their direct application into ‘real world’ problems they might prove invaluable for solving problems at hand with an added spatial element attached\nGeospatial Analytics is a Trend\nGeospatial data, or geographic information systems (GIS) data, are the foundation of geospatial analysis. This type of data includes digitized maps, aerial photos and satellite imagery that provide geographical information about a particular area. It also includes other types of non-spatial data such as address points, relationships between features and metadata.\nGeospatial Analytics is the process of using GIS data to generate insights into a particular area. It is an important tool for decision makers in a variety of fields including urban planning, public health and business analytics. In Geospatial Analytics, the GIS data is analyzed to uncover trends and patterns in the area being studied. The findings from geospatial analysis can then be used to create more efficient solutions for businesses or guide policy decisions for government agencies.\nGeospatial analytics uses advanced software such as Python in conjunction with specialized libraries like geopandas to blend GIS datasets and perform powerful spatial analysis. This software makes it easy to visualize complex trends within GIS datasets with interactive maps, graphs and charts. Data can be further explored with advanced analytics techniques like machine learning to predict future outcomes for areas being studied. With geospatial analysis, professionals have better tools than ever before to solve big problems using accurate geographic information systems data.\nGeospatial Visualization in 2023\nGeospatial visualization is an important tool for analyzing and interpreting data related to a geographic area. This method of data analysis involves using software programs like Python, analytics, and visualization tools to construct detailed visuals of the physical characteristics within a given region. By creating digital maps, researchers can gain insight into the patterns of population density, land use, environmental features, natural resources, and other data parameters in an area. By leveraging such information within industry sectors such as agriculture, transportation & logistics, healthcare services, manufacturing operations – businesses can better plan for future operations with accurate insights into their surrounding environments.\nUsing geospatial visualization allows stakeholders to examine different types of data on a granular level such as weather disturbances, disease outbreaks among agricultural products or asset management. With this approach it is possible to represent fields in subtle color gradations against other features that may lend understanding towards more acutely targeted explanations. Visualizing geospatial data also enables researchers to identify key patterns and correlations between geographic features and targeted progress points across large areas or even worldwide regions.\nTOP tools for visualization of your GIS Data\n- ArcGIS Online\n- GRASS GIS\nGeospatial Data Sources\nGeospatial data, also known as geographic information or spatial data, is a highly structured form of data used to represent real-world physical and cultural features on a two- and three-dimensional map. It includes the coordinates of points, lines and areas; attributes about the features; relationships between them; and geometric arrangements for efficient visualization of large data sets.\nUse this links to find digital geospatial data sites.\nPrimary Sources of FREE USGS Geospatial Data\n- The National Map – Go to The National Map Download Client to download US Topo and historical topographic maps, and data for the United States for various themes to include:\n2) Elevation Products (3DEP);\n3) Elevation Source Data (3DEP);\n4) Hydrography (NHD) and Watersheds (WBD);\n5) Imagery from High Resolution Orthoimagery and National Agriculture Imagery Program;\n6) Map Indices;\n7) Geographic Names;\n8) Land Cover;\nContours are under Elevation Products, and lidar point clouds are under Elevation Source Data.\n- Small-scale data, formerly on The National Atlas, includes Printable Maps, Small-scale Framework data, and Global Map data (1:1,000,000 scale).\n- The National Map Services provide a complete list of service endpoints, which contain REST, WMS, WMTS (cached base maps only), WFS, WCS, and other service links.\n- Earth Explorer*- Download data from more than 300 data collections that span over 75 years of coverage. Includes USGS satellite imagery, historical and recent USGS aerial photography, elevation data, land cover products, Lidar point cloud data, Digital Line Graphs (DLGs), Digital Orthophoto Quads (DOQs), and much more. How to use Landsat Spectral Bands can be helpful.\n- GloVis*- Just looking for satellite imagery? Don’t really need a robust and complex search engine? GloVis is a browse-based viewer that’s easier to search and more user-friendly than EarthExplorer. Products include Landsat, ASTER, EO-1, MODIS, and TerraLook imagery, NAPP and NHAPimagery. How to use Landsat Spectral Bands can be helpful.\nAdditional FREE Geospatial Data from the USGS & Our Partners\n- LandsatLook*-Full-resolution JPEGs of Landsat satellite images that are best used for image selection and simple visual interpretation. Available as thermal or natural color images with optional geographic reference. Not recommended for digital analysis.\n- Emergency Response Portal*-Geospatial imagery for national and international disasters.\n- Science Data Catalog*-Metadata and links to all USGS data. Search capabilities are somewhat limited due to the size of certain datasets.\n- National Geologic Map Database-Search over 90,000 maps and reports, including free digital maps and GIS map data. Primary themes are geology, geophysics, marine, energy resources, and natural hazards.\n- Hazards Data Distribution System (HDDS)*-A dynamic online map interface that can be used to view and download USGS datasets for national or international disasters including floods, hurricanes, tsunamis, and earthquakes.\n- Water Spatial Data-Maps and GIS data for water related topics such as water use maps and data, watershed boundaries, NHD, streamflow records, and more.\n- Quaternary Fault and Fold Database of the United States– Geologic, geomorphic, and geographic information for over 2,000 Quaternary (active in the last 1,600,000 years) faults in the U.S. Download shape files and Google Earth files.\n- Mineral Resources On-Line Spatial Data*- A source for regional and global geology, geochemistry, geophysics, and mineral resource maps and data. Includes map services.\n- Global Mineral Resource Assessments*- Most of these reports contain GIS data. The USGS is in the process of compiling all the data in a single location, but for now users must review each link.\n- Energy Data Finder*- Geospatial data related to oil & gas, coal, geochemistry, and geophysics. This is also a good place to look for base data in international locations (used in global energy assessments).\n- Maps of America’s Submerged Lands– A portal to published USGS maps, reports, and digital data that depict the bathymetry, surficial geology, and/or subsurface structure of selected submerged U.S. areas. This is mostly coastal locations, but some lakes are included.\n- Coastal & Marine Interactive Map Server– GIS data and metadata for past and current projects.\n- Astrogeology MCTR GIS Lab– Graphical, statistical, and spatial GIS tools for analyses of planetary data. All downloads are free. Links to Interactive GIS maps of Mars, the Moon, and Venus.\nGeospatial data comes in a variety of forms. Traditional sources include aerial photography, satellite imagery, paper maps and census records. Modern geospatial technologies increasingly use ever larger datasets collected through sensors ranging from GPS systems to mobile phones to automated vehicle Recognition (AVR) systems.\nThe rise in geospatial technology has drastically increased the availability and sophistication of geospatial datasets from both public sector (e.g., government) sources as well as an exploding number of private sector organizations actively collecting their own location-based commercial data (e.g., web traffic). Consequently, most sectors now rely on geospatial technology solutions including navigation systems, environmental monitoring systems, analytics engines and visualization software applications such as Python’s geopandas library for geographic analytics or QGIS for visualization tasks.\nBy understanding its sources and capabilities, businesses can leverage the power of geospatial technology to optimize product delivery updates , enable customer targeting programs based on locationarlytics , offer engagement mobile solutions (like Walmart Inc.’s Walmart Pay system), or improve their disaster risk management programs .\nChallenges of Working with Geospatial Data\nThe use of GIS data has become increasingly popular in recent years; it provides a powerful tool for analyzing spatial data related to locations and activities. However, working with large datasets of this type can pose a challenge. Geospatial data often has different formats and fields than other types of information, and some users lack the technical skills or resources required to understand and work with them efficiently.\nPython is an excellent programming language for handling geospatial data, as it is designed specifically for manipulating such large amounts of spatial information. Geoprocessing tools available in Python provide an effective means of manipulating geospatial data, enabling the user to format the data correctly, filter it according to different criteria and complete analytics tasks quickly.\nGeoanalytics refers to using geospatial technology to create actionable insights from location-based data. Advanced analytics tools can be used to explore patterns within geo-data sets and identify trends across varying spatial dimensions. For instance, businesses can use geo-analytics software to gain insights into customers’ purchasing habits by analyzing location information gathered from sales transactions.\nVisualization is also a fundamental part of working with geospatial data. By plotting the results from geo-analytics on a map, users can gain valuable insights into their datasets that would otherwise be difficult or time-consuming to uncover within the raw numbers or text alone. Whether using existing visualization tools such as ArcGIS Story Maps or building custom maps using GeoJSON files, visualizing your results brings your analysis process full circle.\nDespite its complexities and differences from other forms of datasets, working with geospatial information can open up new opportunities for businesses when used correctly in combination with analytics and visualization techniques. Understanding the challenges associated with this type of data is key to maximizing its potential benefits for any organization looking maximize results from their location-based studies.\nGeospatial data is a specialized type of data representing information related to our planet’s physical locations. It can include variables such as latitude, longitude, terrain elevation, and population density, among many others. These complex datasets are used in activities such as mapping and analytics to analyze different patterns and trends across the world. Geospatial data offer key insights in order to answer complex geographical questions and figure out the best ways to use physical space for a variety of applications.\nPython is one of the most popular programming languages for working with geospatial data due to its wide range of tools available for importing, manipulating, and visualizing this type of data. Furthermore, Python’s libraries make it easy to create powerful analytics dashboards or interactive maps with dynamic features. Additionally, geospatial data visualization has become an essential tool for presentations in recent years as it enables communicators to more easily understand patterns in large datasets and key pieces of information when it comes to making decisions related to land management or policy initiatives.\nIn conclusion, geospatial data is critical for businesses seeking geographic insight into their activities or operations when developing products or making strategic decisions. With the range of open source libraries available that enable easy manipulation of this type of information using Python coding language skillsets have become highly sought after professionals across many industries today.', 'A GIS map today is a necessity when it comes to change monitoring, environmental disasters tracking, and making effective decisions based on reliable data. But what is GIS mapping?\nGIS map is a dynamic and interactive visualization of quantifiable data. This type of mapping allows for uncovering previously unseen features by not only highlighting them but also showcasing the change that has happened to these features over time. There are different types of mGIS maps, from category maps to heat maps, depending on the purpose of their use.\nBasically, GIS maps connect a geospatial point on the ground to a particular data element. This technology enables geographers and other GIS experts to visualize different kinds of data, including demographic facts, client base distribution, and more. Each of this data is contained in gis mapping layers, each representing a particular type of real-world entities, be it streets, customers, or anything else. In this piece, we’ve gathered today’s top GIS mapping software to describe what each of them can offer.\nThis is an online GIS mapping assistant developed by EOS Data Analytics enabling on-the-fly search, visualization, analytics, and download of satellite imagery with the purpose of helping to tackle real business tasks and challenges. The tool is basically a digital satellite images catalog where users can access medium-resolution images for free and purchase high and ultra high-resolution imagery from a variety of different satellites.\nUsing this platform, you can search for images of your particular area of interest (AOI) and process them right there in one tool. Processed imagery can be stored in a cloud, downloaded, and shared inside and outside the platform whenever needed, allowing for saving storage space on the device and enabling data exchange. The tool can also be integrated into a third party GIS software to ensure a geographic data workflow and extraction of valuable insights.\nWith the Split view feature, users can easily identify change by comparing different satellite images of the same area taken by different satellites, at a different time of day, and on different days. The results can be visualized via a time-lapse animation to showcase how the change occurred over a particular time span in a particular place. The tool offers various default indices for imagery analytics, but users can create their own by applying different combinations of available spectral bands.\nArcGIS is a system that allows you to collect, organize, manage, analyze, share, and distribute geographic information. The ArcGIS platform allows geographic information to be published for anyone to access and use. The system is accessible from anywhere where web browsers, mobile devices such as smartphones, and desktop computers can be used.\nThis system includes software, an interactive cloud infrastructure, professional tools, customizable resources such as application templates, ready-to-use web and mobile applications, ready-to-use basemaps, and trusted content that is distributed by a user community.\nArcGIS allows you to do the following:\n- Create, share and use mind maps\n- Compiling geographic information\n- Create and manage geographic databases\n- Solving problems with spatial analysis\n- Building applications based on maps\n- Connect and share information using the power of geographic data visualization\nArcGIS Pro is designed for visualization, advanced analysis, and robust data management in 2D, 3D, and 4D. It supports publishing and sharing data through ArcGIS Online and ArcGIS Enterprise using web GIS.\nThe application is a GIS mapping program that allows you to quickly and easily access map information and map changes. The application is divided into two large blocks. The first is a browser with information about maps, supports spatial tables, vector and raster files, various databases and documents in MS Word format. The second block serves as an editor. It contains a well-chosen set of tools and functions for interacting with maps and data. With it, users can overlay vector and raster images on top of each other without converting them into a single accepted format. The editor has the function of structuring and creating maps based on the materials obtained from tables, files, and other documents. All received information can be edited and sent to other computers.\nGeoMedia Hexagon Geospatial\nGeoMedia is a GIS mapping software by Hexagon Geospatial is a modern GIS platform built on global industry standards. The GeoMedia platform includes instrumental GIS and a wide range of specialized modules, which provides a solution to any problems related to providing multi-user access to spatial data, their display, editing, analysis, and distribution.\nGeoMedia technology fully complies with the principles and standards of the world OpenGIS consortium. With the help of Data Servers technology, users can work with geodata directly, without prior conversion and conversion.\nGeoMedia technology provides the following features:\n- Combining data of different formats from various sources (databases, maps, plans, technical diagrams and drawings) into a single corporate repository\n- Support for all industrial vector and raster data formats\n- Integration of data into a single information work environment without long-term transfers into a single format\n- Differentiation of access rights to information, privileges of working with data in accordance with the authorities.\nThis user-friendly and affordable GIS app supports an unparalleled diversity of spatial data and provides a set of features needed to satisfy both experienced GIS mapping professionals and novice users.\nAmong the main features of the Global Mapper program are:\n- Support of viewing main data formats\n- Direct access to Digital Globe, TerraServer-USA, WMS, and more\n- Support for viewing 3D landscapes\n- Crop, redesign, and merge any combination of raster and elevation data\n- Compatibility with GPS devices\n- Creation of outlines for any combination of elevation data\n- Loading and displaying JPG images with embedded EXIF position data\n- Support for NASA World, Google Maps, Virtual Earth, and many more features.']	['<urn:uuid:441baed1-e9fb-400f-be79-3c8027e27c20>', '<urn:uuid:8f3c970f-5b38-45f9-9f24-f66443875c98>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	8	95	3794
73	newly pregnant woman with diabetes here what puts me at risk for womb infections	Women with diabetes are among those at highest risk of contracting maternal sepsis, a bacterial infection in the uterus during pregnancy. Other high-risk groups include women with previous multiple miscarriages, those who undergo invasive conception treatments, and those who have invasive tests during pregnancy.	['Maternal sepsis is a serious bacterial infection that occurs during pregnancy, inside the uterus or womb. It is an unnatural development inside the body, which involves the nerves, cells, and the pregnancy hormones. The bacteria commonly involved in triggering this disease are Group A Streptococcus (GAS) and E. Coli. Maternal sepsis causes injuries to the internal organs and tissues. This can develop from infections in any part of a pregnant woman’s body. However, it mainly develops from urinary tract infections, influenza, and pneumonia. Maternal sepsis is a deadly disease and can affect up to nearly 1/3 of the world’s expecting women. This disease is most common in the developing countries, as compared to the developed ones, due to inferior healthcare systems and lack of clean sanitary options. Maternal sepsis continues to be one of the leading factors for deaths among pregnant women.\nRead More: Best Treatment for Neonatal Sepsis\nPregnant women who are at the highest risk of contracting maternal sepsis are:\n- Women who have diabetes\n- Women with a previous history of multiple miscarriages\n- Women who undergo invasive treatments to conceive\n- Women who go through invasive tests during pregnancy\n- Women who are in labour for particularly prolonged periods\n- Women who carry the foetus inside the womb for long hours after the breaking of their water sac\n- Women who linger in polluted areas during the middle and advanced stages of their pregnancy\n- Women who consume large quantities of unprocessed meat\nThe major signs and symptoms of maternal sepsis are:\nMajor Signs and Symptoms of Maternal Sepsis\n1. Acute pain in the lower abdomen\nMaternal sepsis, as we know, is a serious bacterial development inside the uterus or the womb. This naturally leads to extremely painful cramps inside the uterus and throughout the uterine and vaginal area. If you notice a sudden spike in the levels of your regular abdominal cramps, consult your gynaecologist for immediate diagnosis and treatment.\n2. Increased vaginal discharge with a foul smell\nVaginal discharge is a common occurrence throughout pregnancy, but an unnatural increase in the amount of discharge, accompanied by a foul odour or a yellowish colour, can be a sign of serious complications. As soon as you sense a distortion in your daily dose of vaginal discharge during pregnancy, consult your doctor for clarity on the situation.\nWhile a moderate amount of vaginal discharge is common during pregnancy, it is particularly worrying to witness traces of blood in your underwear. One of the most common symptoms of maternal sepsis is a bleeding vagina. Make sure that you get your abdomen tested on a regular basis, to avoid this infection or get yourself diagnosed at an early stage.\n4. Chills in the body\nAnother very common sign of maternal sepsis is a chilling sensation and numbness throughout the body. If you experience a sudden freezing of your limbs and dizziness, inform your doctor and get yourself tested. Since this does not sound like an extreme symptom, most women tend to ignore the same and take it as normal pregnancy occurrence. It is always better to get yourself tested whenever you feel that something is out of place. In this way, you can save yourself and the baby from a life-threatening disease.\nMaternal sepsis, if detected at the right time, can be treated through medication, admitting the patients to intensive care units. In 2012, in the US, almost 30% pregnant women were admitted to ICUs with maternal sepsis. However, the rate is reducing, now that many senior researchers are coming up with new medications. Whenever it occurs, maternal sepsis must be treated as a medical emergency and the patient(s) must be admitted to the hospital immediately after diagnosis of the same.\nThe most common and important treatment processes involved here are:\nDepending on the severity of the infection, your doctor will initially provide you with an array of oral antibiotics followed by intra venous antibiotics. This is done to ensure that the antibiotics are administered equally throughout the body, and that they are absorbed by the blood for the quickest possible reaction.\nIntra venous fluids\nAlthough antibiotics are administered as a first level treatment process, it is vital to inject the patient with extra fluids that will help their blood pressure from dropping to extreme low levels, which might lead to a coma. One of the most common fluids administered is the normal saline, which is crystalloid in form. This contains sodium, which is added to the circulatory system for the cleansing of the harmful bacteria. Another type of fluid, which is thicker, administered to women facing maternal sepsis, is colloid. This is also an intra venous fluid that includes albumin and dextran.\nWomen with maternal sepsis are usually admitted to the ICU, in which their arteries are pierced to insert arterial lines that help in obtaining direct blood samples and measuring the blood pressure of the patients. This enables the doctors to monitor the gravity of the situation and provide appropriate medication or surgical treatment.\nCentral venous catheter\nIn serious cases, when quick flow of strong antibiotics is required to restore the patient’s condition, doctors opt for a central line or passage for administering the fluids so that the body absorbs the medicines quicker.\nThis is a form of steroids that are injected into the patients’ bodies to cleanse the infected blood and reduce inflammation. Corticosteroids are a form of intra venous medicines that help in strengthening the immune system to ease the process of recovery.\nPulmonary artery catheter\nA needle or catheter is inserted into the pulmonary artery of the patient. This blood vessel pumps the blood from the heart to the lungs so that there can be ample oxygen supply in the body. This helps the doctor analyse the situation and recommend the required amount of antibiotic for healing.\nThis process is used on patients who are in severe shock and are not even able to breathe on their own. In this type of ventilation, the affected person is intubated and then the tube is attached to a ventilator to facilitate rhythmic breathing.\nThis is a process whereby, while the patient is in the ICU, their oxygen levels are measured on a regular basis. In this way, doctors can trace the improvement rate or deterioration rate and administer medicated fluids accordingly.\nAlthough maternal sepsis can be treated if diagnosed at the right time, there are certain precautions that must be followed.\nAccording the WHO, in a recent publication, the general precautions to prevent sepsis are:\n- Quality care during pregnancy\n- Complete hygiene to be maintained throughout pregnancy and after childbirth\n- Timely tests and medications\n- Proper and balanced diet\nMaternal sepsis is a deadly disease but not as long as you are careful about your surroundings. Follow your doctor’s instructions and know the symptoms of the disease so that there is no delay in treatment.']	['<urn:uuid:e2f16ebf-9549-4d21-880c-cb01296130bd>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	14	44	1151
74	how edge AI machine learning monitoring systems help reduce rail maintenance costs	Machine learning powered monitoring systems retrofitted onto existing rail tracks monitor the condition of tracks and rolling stock, providing a reduction in maintenance costs of around 25%.	['Both edge computing and artificial intelligence (AI) have continued to gather attention over the past few years as mobile and Internet of Things (IoT) technologies become increasingly adopted by a vast number of industries around the world.\nThese new technologies have also created new problems and challenges for those looking to implement and benefit from the advances and developments of the fourth industrial revolution.\nEdge computing is a computing technique used to move decision making closer to the source of data (the edge) and artificial intelligence is an area of computer science that looks to create intelligent machines and also includes sub-fields such as machine learning.\nIndividually, these two technologies have been shown to work incredibly efficiently and both show huge potential for future development, however, combining the two for use within IoT systems could result in a match made in IoT heaven. In this article, we’ll explain how edge computing and artificial intelligence are being developed to work together and put forwards some possible use cases that could be applied to AI at the edge.\nSo, let’s jump straight in.\nEdge Computing & Artificial Intelligence\nThere are various ways in which edge computing and artificial intelligence could be used together, however, certain limitations with each of these technologies will need to be overcome in order for this partnership to be effective.\nTraditionally, artificial intelligence technologies require a vast amount of computational and processing power to run some of the more complex machine learning algorithms, for example, making them impractical for deployment in locations distant from the cloud servers they are mostly based in.\nConversely, most edge computing technologies are currently being designed to operate in low-power environments with little to no connectivity, making the integration of artificial intelligence systems into their architectures unpractical.\nHowever, there are a selection of proposed methods that could finally see artificial intelligence brought to the edge. Top industry players such as Google and Microsoft, as well as organisations such as Movidius and iEx.ec are working on solutions to bring intelligence closer to the edge.\nTwo potential solutions showing promise are distributed computing using blockchain technologies and portable AI co-processors. Using blockchain and smart contracts technologies, computing resources can be shared among IoT devices and cooperate without the need for a central broker. This could then allow for the running of computation-heavy AI algorithms within the edge devices themselves.\nAlternatively, portable AI coprocessors could be integrated into boards in order to create devices capable of deep learning. Movidius is one of a few companies that have been developing edge neural networks for a while and have even produced their Myriad 2 vision processing unit (VPU), which can be used to achieve computer vision and image signalling capabilities in low power edge environments.\nPotential Use Cases for Edge AI\nSo, as we’ve seen, there are indeed ways in which edge computing and artificial intelligence are being developed so as to meet the needs of other technologies such as the Internet of Things. But how could they improve IoT systems? Let’s now take a look at a few potential use cases for AI at the edge.\nAutonomous vehicles are looking extremely likely to be the next big step for intelligent transportation systems of the not-too-distant future. Self-driving cars and intelligent traffic management systems are already being tried and tested today and the integration of edge AI could very well be the way forward.\nWhen it comes to autonomous systems, safety is paramount. Any delay, malfunction, or anomaly within the system can be devastating, and in the case of autonomous vehicles fatal. Using both edge computing and artificial intelligence capable of providing intelligent decision making within edge devices could ensure that these challenges are overcome.\nWhile the robots many of us grew up seeing and reading about in science fiction aren’t quite here, robotics are no doubt a fundamental part of many industrial processes and, given the advances in wireless communications, automation, and IoT technologies, will likely grow in both their necessity and the scope of their applications.\nAnd the excitement surrounding these new innovations within robotics is certainly growing. According to an interview with David Schatsky, a managing director at Deloitte who specializes in emerging technologies, “The new generation of robots can use AI technologies such as computer vision, speech recognition, and more sophisticated analytics of the sensors that they have.”\nMaintenance, Monitoring & Security\nMaintenance, monitoring and security are three areas in which IoT devices are already thriving. Their use in video surveillance, motion detectors, environmental sensors, and other monitoring and security devices could be greatly enhanced through the integration of edge AI and could allow for further automation of monitoring and security processes.\nPredicative maintenance is also an area where edge AI could greatly improve the output of IoT systems. Rail is one of the industries that has started to look into solutions based on edge computing and AI to reduce maintenance costs and unplanned downtime. Machine learning powered monitoring systems are being retrofitted onto existing rail tracks in order to monitor the condition of the tracks and rolling stock. These systems claim to provide a reduction in maintenance costs of around 25%. For multi-billion dollar transit authorities, that’s a huge opportunity.']	['<urn:uuid:c8265890-9dc1-43d0-90b5-11a98badea93>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	12	27	867
75	beginners tips drawing face lessons	If you're drawing a portrait for the first time, it's important to break down the process into manageable steps rather than trying to tackle everything at once. Three key ideas need to be understood for making a decent portrait: proportion and perspective, drawing accurate contours, and creating accurate shading. Taking the process one step at a time makes it significantly less daunting than trying to capture all details simultaneously.	"['HOW TO DRAW & PAINT PORTRAITS WITH EASY DRAWING LESSONS IN PORTRAITURE : Learn How to Draw the Human Face\nLearn how to draw people\'s faces and heads as well as their facial features with the following drawing lesson - including eyes, noses, ears, lips, mouth, etc.\nIf you have never drawn a portrait of a human face, then this is a good starting point.\nLearn how to draw the human head, neck, and should in the correct proportion to each other.\nHere you will learn about drawing children\'s face and head from the front view with these tutorials to create great drawings.\nHere you will learn about drawing children\'s face and head from the side profile view with these tutorials to create great drawings\nThis is a Drawing Tutorial for learning how to draw people\'s faces and heads for beginners. How? By drawing the casts of human faces and heads.\nPortraits and general subject drawings made out of many ink dots laid carefully side by side are often called pointillism or stipple. Pointillism is the color version of a stipple portrait. As colored dots are placed adjacent to each other, when the viewer moves further away, the colors seem to blend to the eye, creating a new color.\nLearn how to draw this adorable boy\'s face with step by step guide.\nDrawing with colored pencils is dramatically different than drawing with graphite pencils, in several ways. In this tutorial, I will introduce and discuss some general concepts, tips, and techniques you\'ll need to learn in order to work well with colored pencils.\nIf you take the image all at once and dwell on how many details you have to draw and how difficult it will be, trying to tackle drawing a portrait would be intimidating for anyone! You can make the process significantly less daunting by taking it one step at a time.\nFrom Photo to Sketch in MinutesWhen photography first came onto the scene many artists thought it would signal the death of painting, but the reality is that photography plays a vital part in painting. Many artists paint from photographs and this is where a projector can be of significant help too.Some artists mistakenly believe that using a projector is cheating. Nothing could be further from the truth...\nHere you will learn how to draw adults, infants, toddlers, and children from the side / profile view.\nLesson on drawing the profile. Sketching the face from side view. Portrait tutorial.\nColored pencil seems like an easy medium to use, but to achieve any kind of mastery, it can become exceedingly difficult and unforgiving. There is only so much erasing you can do before the board or paper no longer yields the pigments without damage, and blending and layering can easily result in ""mud.""\nThis is a step by step tutorial on how to draw a realistic portrait. I would suggest that you have a basic understanding of drawing and shading before attempting to draw a serious portrait.\nI hope this post will be kinda helpful for people who want to know more about how to draw portraits. I used my favorite cross-hatching technique to draw this portrait.\nLearn portrait drawing skills including face proportions, features, eyes and hair. Learn about portraiture with lessons on sketching children and adults. Find a portrait artist or gallery.\nThe ability to draw the human figure, while a great accomplishment, is, however, of little avail unless one also can give portrait values—expression and character—to his subjects. The following pages, while in the nature of the case they cannot be made exhaustive, yet will give the student valuable hints that, with constant practice and study, will enable him to gain facility in this direction.\nThere are numerous methods to portrait drawing. But they can all be categorized into two approaches: drawing from the inside out and drawing from the outside in. The former is the purview of the untrained artist who, inevitably will stumble their way through art.\nDrawing portraits has always been, and will continue to be one of the greatest aspects of drawing.\nI think there are three ideas that need to be understood in order to make a decent portrait: Proportion and perspective, drawing accurate contours, and creating accurate shading. We\'ll look all three of these ideas in this drawing people section.\nStep by step baby portrait pencil drawing\nI put grid coordinates across the top and right side of my picture area and my photo reference, then measured from the top and one side to where a dark spot meets light, like the corner of one eye. I made a tick mark at the same measured place on my art paper -- a very small dot put on the shadow side of where it belonged.\nPortraiture is a specialized aspect of art. It involves the ability to maintain a likeness to the subject while still incorporating the technical elements of art\nIf you are struggling to come up with the perfect, hand made gift for your friends or family, a hand drawn pencil portrait is a wonderful gift and is extremely affordable. I am going to give away the secret here in this blog post so dive right in to learn exactly how to draw a professional looking portrait.\nLearn how to draw beautiful young children with this step by step drawing tutorial demonstration.\nDifferent cat breeds demand different treatment in pastel and Conte drawing. This lesson covers how to draw a realistic Siamese cat portrait, and covers fur texture, color mixing, tracing and improving a photo reference. Siamese are exotic, mysterious, blue-eyed and hard to capture with their dark masks and pale shaded fur. White whiskers against dark needs a fine white line over dark, so we\'ll use colored Conte crayons.\nThere are two schools of thought about whether an artist should use an art projector or other similar devices to help create art. Art projectors, when use correctly, can help an artist lay out a piece of art. When creating a portrait, it is very helpful to get the placement of the eyes, ears, and nose. An art projector can help with that, and also help get an accurate shape of the face.\nThere are few things more appealing than a child\'s face. However, many artists struggle with trying to create portraits of children. There are some techniques that can make learning how to draw a child\'s face a more manageable.\nA step-by-step demo of how to paint a background for a portrait or still life.\nHere are some simple guidelines to drawing portraits.\nBut what’s the setting? To help you answer that, first make sure you are painting a portrait, and not a figure in a landscape. To understand the difference, take a look at the following two images. The one on the left is a figure in a landscape, and the one on the right is a portrait.\nHere\'s the story behind these video tutorials and the 5-pencil method. This guy is the best, but it will cost you to learn.\nBelow you\'ll find a collection of tutorials focused on portraiture. If you are looking for tutorials on figure drawing, check this page. The media and technique may vary so the approach will also vary in each demonstration.\nThe human face is an endlessly fascinating subject, and you\'ll find everything you need to get started with portrait drawing right here. If you are new to drawing, its a good idea to start with the basics, learning the skills that you\'ll need to apply when you tackle portraits. Many of us learn by copying photographs, and that\'s fine, but to get a good sense of the head as a three-dimensional form, nothing beats drawing from life.\nn this guide about portrait drawind, I would like to give some tips and hints about what you should look at when you are drawing or painting a portrait or a face. If you want to learn to draw people, you will find here hopefully some useful tips.\nSketching the portrait with charcoal using brush techniques\nImprove your drawing skills by following our video drawing tutorials.\nCreativeSpotlight has gathered a nice selection of free lessons on the following pages that you are sure to learn a great deal from. The art instruction on the following pages has been generously donated by professional artists from all over the world. We are most grateful for their generosity and we hope you enjoy!\nThis is a step by step tutorial on how to draw a realistic portrait. I would suggest that you have a basic understanding of drawing and shading before attempting to draw a serious portrait. For this lesson I am drawing on 11""x14"" Fabriano Hotpressed Watercolor Paper .\nPracticing Three Ways to Draw a figure. Each day, after a self-portrait practice, practice figures drawing by the three methods below.\nI know many beginning artists who want to start right from scratch and draw a portrait of someone they know or someone popular. Drawing great portraits is like the holy grail of drawing. Creating realistic and living portraits needs a high skill level near to mastery. Fortunately you can learn these skills much easier, if you learn them separately.\nThere isn\'t really much of a ""secret"" to drawing in this style. I was first introduced to this technique by one of my teachers, L.A. Times illustrator, Richard Milholland. He explained it very simply—you just get a sharp pencil, smooth Bristol board, etc']"	['<urn:uuid:edfc1628-964a-472c-926f-d248e5d24c4d>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	5	69	1579
76	havel playwright president transformation compare roles	As a playwright in the 1960s, Havel established himself as a leading figure in Czechoslovak theatre of the absurd, writing plays that criticized the communist system, like 'Garden Party.' After being banned from theatres following the 1968 Soviet invasion, he became a dissident, co-founded Charter 77, and spent five years in prison. His transformation into a political leader came during the 1989 Velvet Revolution, when he united opposition groups into Civic Forum and became the first post-communist president. As president, he faced new challenges, including the peaceful split of Czechoslovakia and the struggle to maintain his idealistic vision amid growing corruption. Even while serving as president, he continued his artistic work, writing his play 'Leaving' in 2007, which reflected on political power.	['Source: New Europe / www.neweurope.eu / By Sasha Borovik /\nFormer First Deputy Minister of Economy of Ukraine responsible for the international donor coordination, and Former Acting Deputy Governor at Odessa Regional State Administration.\nThe year 1968 remains one of the most turbulent years in modern history, marked by geopolitical shocks that include the Robert Kennedy and Martin Luther King’s assassinations, student protests around the world. and the first humans orbiting the moon. That year also saw the Soviet Union lead a group of fellow Warsaw Pact nations across the border into what was then Czechoslovakia to crush a popular anti-Communist revolt in Prague.\nIn the decades that followed, Communist rule in Eastern Europe continued. As I was growing up in the Soviet Union in the 1980s, there was no mention in my textbooks of the 1968 Prague Spring. Speaking about it was also taboo. When pressed for answers, grownups were reluctantly explaining that small groups staged a violent pro-capitalist uprising and the Soviets rushed in to fulfil their international duty to help a brotherly Socialist state, and the majority of ‘the people’, preserve the peace. Kids were told this whenever they were asking about the Soviet regime intervening in post-war Eastern Europe.\nI moved to Prague with little knowledge of the Prague Spring shortly after the Velvet Revolution began 30 years ago on November 17, 1989. The Prague Spring set the stage for the revolution’s success and the Berlin Wall, which had only just fallen, made it clear that the changes that were happening across Eastern Europe were irreversible.\nTogether, the momentous events of 1989 put the final nails into the coffin of the Soviet Empire.\nThe Velvet Revolution was the culmination of a long struggle in many corners of the Soviet-led Eastern Bloc. It was called ‘Velvet’ because the transition of power was considered non-violent, but it was peaceful only in relative terms. The day the uprising began, riot police brutally attacked a group of students in the centre of Prague. In the fall of 1956, a similar gathering of a few thousand students in Budapest resulted in millions of ordinary people trying to overthrow the hated Stalinist regime of Matyas Rakosi. In June of that same year, workers in Poznan, Poland had risen up against the Communist regime.\nFor many years, tens of thousands of ordinary people were killed and wounded by tanks, infantry, and security police as uprisings against Communism were brutally oppressed and in the repressions that followed.\nThe police violence in Prague in November 1989 was unprecedented and had not been seen since the days of the Prague Spring. But unlike in 1956 and 1968, the Soviet military did not intervene.\nSensing the momentum that was building, Civic Forum, a dissident movement led by famed play write Vaclav Havel, was established as an opposition group. Encouraged, the people of Czechoslovakia started to lose their fear. A series of demonstrations from November 17 to late December turned into a revolution.\nOn November 20, the number of protesters in Prague grew from 200,000 the previous day to an estimated 500,000. That show of force was followed on November 27 when the entire nation held a two-hour general strike. Disoriented, the Communist government announced that it would relinquish power and end the one-party state. A new government, the members of which included several individuals from the opposition who had served prison terms, was formed. These fateful events gave birth to the new democracy in the heart of the rapidly crumbling Communist bloc.\nThe new country that emerged was led by Havel. No one captured the moral high ground of this historical moment better than him. Havel was a poet who long opposed to Communist totalitarianism in his homeland as well as a dissident and co-founder of the movement known as Charter 77, for which he was imprisoned.\nIn 1989, Havel united diverse opposition groups into Civic Forum, which then nominated him to become president once the Communist government had fallen. On December 29, the Federal Assembly unanimously elected Havel and in 1990, Czechoslovakia held its first free elections in 44 years, resulting in a sweeping victory for Civic Forum and its Slovak counterpart, Public Against Violence.\nOn a personal level, Havel left a deep impression on me, both as a human and as a politician. I had the great fortune to meet him in the early 1990s in Prague. I encountered a small man, wearing a rumpled sweater and jeans – he was, after all, a playwright, not a politician. He was shy, yet resilient, overly polite, yet deceptively decisive, and a true nonconformist.\nAt right around that same time, and by pure chance, I also met one of his friends, Pavel Tigrid. He was a leading writer and journalist from the Czech émigré community who later led a ministry in the Czech Cabinet. He and his wife gave me a ride in their old car when I was hitchhiking in the summer of 1991 to Tabor, a town south of Prague.\nThose days everything seemed possible and everything was simple. People were celebrating the victory of openness. It felt that everything was about to arrive and the truth had won forever. This new political class of writers and intellectuals seem genuinely committed to shaping their country into a model of 21st century economic, political. and cultural renewal.\nThey had successfully completed their primary objective—the overthrow of the hated Communist regime. Some of the views of that political group later became controversial domestically – they reluctantly criticised the consumerism of voters and the greed of corporations. They were determined to project moral authority and were not afraid of taking political risks for important principles.\nThey were a political class of their own.\nHowever, intellectually Civic Forum was too diverse. For some, their spiritual heroes were John Lennon and the Rolling Stones. For others, it was Margaret Thatcher and Milton Friedman. As a governing party, Civic Platform was too ineffective. It’s not surprising that in 1990, the year after the Velvet Revolution, Civic Forum split into several political groups.\nBy 1992. Slovakia’s calls for greater autonomy effectively blocked the daily functioning of the government. People like Havel were unable to contain the trend toward breaking up Czechoslovakia, and in 1993 the Czechs and Slovaks went their separate ways. Many former dissidents left the government. After an unsuccessful campaign for election to the Czech Senate, Tigrid retired back to France where he died in 2003. Havel stayed in power until that year and fought many political battles. Vaclav Klaus, one of his greatest political adversaries, was later elected as his successor.\nThe Czech Republic is probably not exactly what the idealists like Havel and his dissident friends were hoping for. People are allowed to speak while the country continues to maintain a developed economy and a European social welfare system. The country is a member of NATO and fully participates in the European Single Market as a member of the EU. It also ranks high in World Bank Human Capital Index, ahead of countries such as the US and Denmark.\nToday’s Czech Republic is, however, also plagued by widespread corruption.\nFor that reason, mass demonstrations have returned to the streets of Prague. This time people are protesting against Andrej Babis, a billionaire who has been the Czech prime minister since October 2017. He allegedly committed fraud using a €2 million European Union grant for one of his businesses. He is also accused of having collaborated with the Communist regime and maintains close ties to Russian President Vladimir Putin.\nPresident Milos Zeman is also a target of protests for having shown a blatant disregard for the Czech constitution. Among European leaders, Zeman is also known as one of most ardent supporters of Russia, which is again becoming more assertive in Eastern Europe. For now, the polls suggest that both Babis and Zeman remain popular and are unlikely to leave.\nModern protesters should not be discouraged though. From the days of the Prague Spring and right through to the Velvet Revolution, they should know that revolutions come from a country’s youth. Bringing about fundamental change is often a lengthy process. Over time responsible leaders do appear and so does the power of civil society to use nonviolent means to bring down any government, especially a corrupt one.', 'By Jan Flemr\nPRAGUE – Former Czech president and hero of the Velvet Revolution Vaclav Havel, who steered his country to independence from Soviet rule in 1989, died on Sunday at the age of 75, his office said.\nHavel died in his sleep at dawn after a lengthy illness, his secretary Sabina Tancevova said.\n“In his last moment, his wife Dagmar was with him, together with one of the nuns who have been taking care of him in recent months,” Tancevova said.\nHavel, president of Czechoslovakia from 1989 to 1992 and of the successor Czech Republic from 1993 to 2003, had long battled poor health, partly caused by the five years he spent in communist jails.\n“He was the soul of the Czech revolution. Without him and his courageous words, the democratic awakening in Central and Eastern Europe would have been unthinkable,” German Foreign Minister Guido Westerwelle said on Sunday.\nAustrian Foreign Minister Michael Spindeleger also described him as an “exceptional figure, an incorruptible intellectual.”\n“His homeland owes him a great deal, as does Europe,” he said. “Vaclav Havel was a moral authority well beyond Czech borders. With his commitment and courage, he pulled the Czech Republic out of communism and led it towards Europe.”\nThe playwright and former dissident had grappled with breathing problems since he had part of his lung removed in 1996 to stop cancer.\nCzech Prime Minister Petr Necas paid tribute to Havel.\n“President Havel was a symbol of what happened here in 1989, he did a great deal for this country, for its peaceful switch to democracy. He was the symbol and face of our republic abroad.”\nGerman Foreign Minister Guido Westerwelle hailed him as “the soul of the Czech revolution”.\nHavel’s health woes stemmed from a poorly treated case of pneumonia he suffered while he was jailed by the communist regime in the 1980s for dissident activity.\nPart of his right lung was removed in December 1996 after cancer was detected. The former chain smoker has also suffered repeated lung and heart problems and underwent surgery for a pulmonary inflammation in 2009.\nEarlier this year, Havel was taken to hospital with acute bronchitis, from which he had never seemed to fully recover.\nThe illness also caused “a loss of balance, memory loss and weight loss,” Havel said in an interview.\nThis summer, Havel retreated to his house in the countryside to convalesce and last returned to Prague to meet Tibetan spiritual leader the Dalai Lama on December 10.\nHavel was born in Prague on October 5, 1936 into a wealthy family which lost its assets as the communists took power in 1948.\nHe established himself as a leading figure on the scene of the Czechoslovak theatre of the absurd in the 1960s, before being banned from theatres after the Soviet-led invasion of Czechoslovakia in 1968.\nHe was responsible for drawing up Charter 77, a 1977 manifesto challenging the communists to live up to their international promises to respect human rights, and he kept fighting the regime which earned him five years in prison.\nAs communism was toppled in the peaceful Velvet Revolution, Havel was the first-choice man to take the top job in Czechoslovakia, which split peacefully into the Czech Republic and Slovakia in 1993.\nHavel married actress Dagmar Veskrnova, 20 years his junior, in 1997, following the death of his first wife Olga a year earlier. He had no children.\nTIMELINE: Following are key events in the life of Vaclav Havel:\nOct. 5, 1936 – Born into a wealthy Prague family that made its money in construction and has strong influence in the artistic community.\nDec. 3, 1963 – His first play, “Garden Party” premieres at a Prague theatre, lampooning the communist system.\n1969 – Havel is barred by communist leaders from his job as a writer/editor after the suppression of the Prague Spring reforms of 1968. He is forced to work as a manual labourer.\nJan. 1, 1977 – Becomes first spokesman for the Charter 77 dissident group that strongly criticises communist officials.\nOct. 18, 1977 – He is given a suspended sentence of 14 months for hurting the interests of the country, a phrase commonly used at the time to denote criticism of the government.\n1978 – He publishes one of his most important works, the “Power of the Powerless”, an analysis of how a totalitarian regime sticks to power by morally corrupting and manipulating individuals.\nOct. 23, 1979 – Sentenced to four and a half years in prison for “subversion” against the state.\nFeb. 7, 1983 – Released from prison amid immense foreign diplomatic pressure after falling seriously ill with pneumonia.\nJan. 1989 – Imprisoned again after meeting dissidents and French President Francois Mitterrand several weeks earlier when Mitterrand visited Prague. Havel was sentenced to nine months, but released in May after protests from both home and abroad.\nDec. 29, 1989 – Chosen as Czechoslovak president following the November collapse of the communist regime.\nJuly 3, 1992, – Leaves office ahead of the breakup of Czechoslovakia. On January 26, 1993, he is elected president of the newly-emerged and independent Czech Republic.\nDec. 2, 1996 – A heavy smoker, Havel is forced into hospital with recurring pneumonia. Doctors remove part of his cancerous right lung.\nJan. 20, 1998 – Havel becomes president for a second five-year term after elections. In April, he nearly dies and has emergency surgery on his large intestine in Austria.\nFeb. 2, 2003 – Havel steps down as Czech president.\nNov. 2007 – Havel publishes “Leaving”, his first play since 1988. The satire is about a political leader who resigns but refuses to leave his government residence. Some reviewers say it is his best work.\nJan. 2008 – Suffers heart arrhythmia and is released from hospital after two weeks.\nMay 23, 2008 – Havel wins a standing ovation at the premiere of his new play “Leaving”, marking a successful return to theatre after two decades. The play, inspired by Shakespeare’s “King Lear” and Chekhov’s “The Cherry Orchard”, depicts a former ruler whose world falls apart after he leaves office.\nJan. 14, 2009 – Havel is in serious but stable condition after throat surgery and a battle to halt the spread of an inflammation in his chronically ill lungs.\nMarch 8, 2011 – Havel is taken to hospital and will spend several days there due to a fresh bout of his chronic respiratory illness, the Prague Central Military Hospital says.\nMarch 22 – “Leaving” a film version of the play directed by Havel, has its premiere in Prague.\n— Starring Josef Abrham, Atiana Vilhemora and Oldrich Kaier, Havel’s movie also includes turns from Karlovy Vary film festival director Jiri Bartoska and Havel’s wife, the actress Dagmar Veskrnova.\n— Havel’s movie details the story of a government chancellor who faces a crisis after being removed from political power.\nDec. 10 – Tibetan spiritual leader, Dalai Lama, arrives in Prague for a three-day visit at Havel’s invitation and the two meet for an hour.\nDec. 18 – Vaclav Havel dies.']	['<urn:uuid:fc91ac37-3b2b-44fc-b139-919eb4c941bf>', '<urn:uuid:917485b1-a997-4aad-aebc-a47e271c6388>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T00:57:18.859842	6	122	2556
77	thailand muslim silk weaving where located	The Muslim silk weavers were located in Bangkok's Ban Khrua quarter, situated along the banks of a canal that provided fresh water for rinsing and dyeing silks.	"['Uncertain future looms for ancient Thai silk\nThe last of Thailand\'s muslim handweavers produce fabric for jet-set fashion.\nBangkok, thailand — Deep in the steamy thicket of low-rise wooden houses of this city\'s Ban Khrua Muslim quarter, the din of teak hand looms thudding and clacking fills the air. Here in the canal-side workshop of Nipon Manuthas, three women surrounded by the kaleidoscope of vibrantly dyed silk spools pump the pedals of the looms with bare feet. The weavers manage to carry on an animated conversation over the racket as they pass spools back and forth across the looms.\nIt\'s a scene straight out of another century, but the lustrous swaths of fabric that Mr. Nipon\'s little workshop produces in this humble old neighborhood are in demand from bespoke tailors for a jet-setting clientele of European diplomats and dignitaries.\nThough Mr. Nipon flicks through a slick European fashion magazine to explain where he gets the inspiration for the stacked rainbow of folded silk fabrics in a glass case behind him, he\'s clear that creative control rests right here in a family whose ancestors have been weaving silk as far back as anyone can remember. ""We choose all the dye to get the right color, he says over the shop clatter, holding out a spool of bottle green thread as an example.\nThis was a typical household scene in this Muslim neighborhood of Nipon\'s 1950s boyhood. Silk weaving was the lifeblood of the community, located along the banks of a canal that provided fresh water for rinsing and dyeing silks. It was a successful cottage industry in Bangkok, a city that was starting to find its economic feet after a post-World War II lull.\nToday, sullied by unchecked urban expansion that has ringed Ban Khrua with office blocks and shopping malls, the canal waters no longer run clear, and Thailand\'s silk trade has shifted toward factory-made fabrics. Most of Ban Khrua\'s old looms were sold off when demand dropped in the 1970s.\nNipon\'s is one of the last of the original Muslim weaving families. As the city has changed around him, he has stuck it out in a modest house behind his two small workshops, while most of his nine siblings have moved out of the old neighborhood. ""I want to stay here in the community. Silk is my life,"" he says, a smile creasing his face.\nNipon inherited the business from his mother. In 1947, she was one of the weavers whose checkered sarongs caught the attention of James Thompson, the storied American spy-turned-entrepreneur. At the time, Ban Khrua was the only place in Bangkok where silk was still woven by hand.\n""Jim saw the silk in the market and said, \'where does this come from?\' I told him, and off he went,"" recalls William Warren, a friend of Mr. Thompson and his biographer. At the time, Thailand\'s silk industry was in decline due to changing tastes and competition from imported man-made textiles. Many silk weavers had given up the practice.\nBut in Ban Khrua, Thompson found a community where most families had a loom or two. The secret to Ban Khrua\'s enduring tradition was its unusual history. Its founders were Cham Muslims from Cambodia and Vietnam who fought for the Thai monarchy in the late 18th century. In return, they were given a plot of land east of the new capital, Bangkok, where they built a mosque and dug a canal to the river.\nAs Muslims in an overwhelmingly Buddhist nation, residents held fast to their traditions, including silk weaving. Thrilled at his discovery, Thompson set off for New York with some Ban Khrua samples. The revival of Thailand\'s silk trade had begun. But when Thompson disappeared mysteriously in 1967, while on vacationing in Malaysia, the thread between Ban Khrua and his company was broken. Over the next decade, new management switched production to other locations and eventually opened a factory in northeast Thailand that now produces 1.5 million meters of silk, much of it for home furnishings. The company is no longer the bridge between the humble homespun of Ban Khrua and the catwalks of Paris and New York.\n* * *\nAs a child, Nipon watched Thompson on his daily trips to check on the dyeing and weaving of his silks. When the soft-spoken American bought land beside the canal in the 1950s, Nipon\'s family donated their teak house to him. Dismantled and floated across by boat, it was incorporated into Thompson\'s grand Thai-style house that today draws hundreds of tourists. Across a footbridge from the Thompson estate lies Ban Khrua: a jumble of two- and three-story houses under a blistering sun. Some have seen better days: Tin roofs are flaking rust, and walls are missing wooden slats. The walkways are so narrow that you can reach out and touch the houses on both sides.\nNipon left to study at a university in Germany. When he returned in 1976, the silk trade was in decline, and its survival was uncertain. Weavers blamed the crash on Thompson\'s company for cutting orders, saying that if the ""big boss"" were still around, it never would have happened. That\'s partly true, admits Surindr Supasavasdebhandu, a Thompson company director: ""When he disappeared, the confidence was lost. Nobody else could build this kind of faith.""\nBut many weavers were already looking for new buyers, or selling out and investing in land and other trades. Nipon bears no grudge against Thompson\'s company. He and other original weaving families are shareholders. Thompson believed in sharing profits with his workforce, and gave shares to his weavers. Last year, Nipon and his siblings shared around $30,000 in dividends (the average annual household income is $4,500, according to government data).\nHaving survived the slump, the next challenge came from the city government. In 1988, a construction firm was contracted to build a new highway with an exit road through Ban Khrua to a nearby luxury mall. Up to 5,000 households were to be cleared for the road. Infuriated, Nipon and others in the community used every trick in the book – public rallies, legal appeals, civil disobedience – to stop the road. Muslim residents insisted that their land was royally bestowed and that their ancestors\' graves there couldn\'t be disturbed. Finally, in 2001, after a 13-year fight, the planners conceded defeat.\nThese days, Nipon is looking to the future. He plans to retire in five years and hand over the business to his British-educated daughter. He hopes she can find a way to stay competitive in a changing marketplace.\nIt\'s 100 degrees outside as he leaves the workshop, a bundle of silks tucked under his arm. He crosses the muddy graveyard where his grandparents are buried and climbs into his silver Toyota pickup truck.\nCrawling in stop-start afternoon traffic, Nipon takes a cellphone call from an Italian customer who wants a sample for a range of ladies shoes. Today\'s sales call, though, is to a tailor in a shopping plaza by the venerable Oriental Hotel, where Thompson lived and ran his business in the early years.\nInside the air-conditioned plaza, Nipon ducks into the tailor shop of Wilaiwan Anusapa and hands over his bundle. He doesn\'t linger long, but pauses to point out the shelves where his latest silks are displayed.\nAsked why she buys from weavers like Nipon, Ms. Anusapa\'s eyes widen as she reaches for a length of crimson-red silk that shimmers richly under the shop lights. Its tiny, bobbly imperfections and the fine lines of the weave show that it\'s a genuine handicraft, not a mass-market factory fabric, she explains. That\'s what she needs to reel in her customers, who include European diplomats and senior Thai bureaucrats.\nMany of her other suppliers – tiny, family-run companies – also trace their roots to the old Muslim weaving clans. Ban Khrua weavers have the best silk, she says. While other weavers supply lengths of single colors or subtle weaves, the final product is as distinct as the weavers themselves, she says, and Nipon\'s silks stand out on the shelves because of their bold designs.']"	['<urn:uuid:2429ed1a-1775-48a6-af08-3a6d82be4175>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	6	27	1347
78	What kinds of health problems can AI help doctors spot?	The Food and Drug Administration has approved at least 130 AI-powered medical devices, with applications across various medical conditions. These AI tools can help detect several health issues: they can spot and alert doctors to suspected blood clots in the lungs, analyze mammograms and ultrasound images for signs of breast cancer, examine brain scans for signs of hemorrhage, and flag various hidden heart problems. Half of these FDA-approved AI medical devices were approved in just the last year, indicating rapid growth in this field.	"['Image source: Unsplash/ThisisEngineering RAEng\nThe Food and Drug Administration has approved at least 130 AI-powered medical devices, half of them in the last year alone, and the numbers are certain to surge far higher in the next few years. Several AI devices aim at spotting and alerting doctors to suspected blood clots in the lungs. Some analyze mammograms and ultrasound images for signs of breast cancer, while others examine brain scans for signs of hemorrhage. Cardiac AI devices can now flag a wide range of hidden heart problems. But how much do either regulators or doctors really know about the accuracy of these tools? A new study led by researchers at Stanford, some of whom are themselves developing devices, suggests that the evidence isn’t as comprehensive as it should be and may miss some of the peculiar challenges posed by artificial intelligence.\nQuite surprisingly, a lot of the AI algorithms weren’t evaluated very thoroughlyJames Zou\nMany devices were tested solely on historical — and potentially outdated — patient data. Few were tested in actual clinical settings, in which doctors were comparing their own assessments with the AI-generated recommendations. And many devices were tested at only one or two sites, which can limit the racial and demographic diversity of patients and create unintended biases. ""Quite surprisingly, a lot of the AI algorithms weren’t evaluated very thoroughly,"" says James Zou, the study’s co-author, who is an assistant professor of biomedical data science at Stanford University as well as a faculty member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI).\nIn the study, the Stanford researchers analyzed the evidence submitted for every AI medical device that the FDA approved from 2015 through 2020. In addition to Zou, the study was conducted by Eric Wu and Kevin Wu, PhD candidates at Stanford; Roxana Daneshjou, a clinical scholar in dermatology and a postdoctoral fellow in biomedical data science; David Ouyang, a cardiologist at Cedars-Sinai Hospital in Los Angeles; and Daniel E. Ho, a professor of law at Stanford as well as associate director of Stanford HAI.\nReport • Finding solutions for radiology tasks\nWith a growing number of Artificial Intelligence (AI) algorithms to support medical imaging analysis, finding the best solution for specific radiology tasks is not always straightforward. Many are cloaked in secrecy and commercial sensitivity and while an algorithm may perform well in one area, evidence of performance and adaptability in another environment is not always available.\nIn sharp contrast to the extensive clinical trials required for new pharmaceuticals, the researchers found, most of the AI-based medical devices were tested against “retrospective” data — meaning that their predictions and recommendations weren’t tested on how well they assessed live patients in real situations but rather on how they might have performed if they had been used in historical cases. One big problem with that approach, says Zou, is that it fails to capture how health care providers use the AI information in actual clinical practice. Predictive algorithms are primarily intended to be a tool to assist doctors — and not to substitute for their judgment. But their effectiveness depends heavily on the ways in which doctors actually use them.\nThe researchers also found that many of the new AI devices were tested in only one or two geographic locations, which can severely limit how well they work in different demographic groups. “It’s a well-known challenge for artificial intelligence that an algorithm may work well for one population group and not for another,” says Zou.\nThe researchers offered concrete evidence of that risk by conducting a case study of a deep learning model that analyzes chest X-rays for signs of collapsed lungs. The system was trained and tested on patient data from Stanford Health Center, but Zou and his colleagues tested it against patient data from two other sites — the National Institute of Health in Bethesda, Md., and Beth Israel Deaconess Medical Center in Boston. Sure enough, the algorithms were almost 10 percent less accurate at the other sites. In Boston, moreover, they found that their accuracy was higher for white patients than for Black patients.\nAI systems have been famously vulnerable to built-in racial and gender biases, Zou notes. Facial- and voice-recognition systems, for example, have been found to be much more accurate for white people than people of color. Those biases can actually become worse if they aren’t identified and corrected.\nReport • AI does not discriminate\nAI can help tackle inequities and bias in healthcare but it also brings partiality issues of its own, experts explained in a Hot Topic session entitled ""Artificial Intelligence and Implications for Health Equity: Will AI Improve Equity or Increase Disparities?"" at RSNA.\nZou says AI poses other novel challenges that don’t come up with conventional medical devices. For one thing, the datasets on which AI algorithms are trained can easily become outdated. The health characteristics of Americans may be quite different after the Covid-19 pandemic, for example.\nWe don’t want things to be overregulated. At the same time, we want to make sure there is rigorous evaluation especially for high-risk medical applicationsJames Zou\nPerhaps more startling, AI systems often evolve on their own as they incorporate additional experience into their algorithms. “The biggest difference between AI and traditional medical devices is that these are learning algorithms, and they keep learning,” Zou says. “They’re also prone to biases. If we don’t rigorously monitor these devices, the biases could get worse. The patient population could also evolve.”\n“We’re extremely excited about the overall promise of AI in medicine,” Zou adds. Indeed, his research group is developing AI medical algorithms of its own. “We don’t want things to be overregulated. At the same time, we want to make sure there is rigorous evaluation especially for high-risk medical applications. You want to make sure the drugs you are taking are thoroughly vetted. It’s the same thing here.”\nSource: Stanford University']"	['<urn:uuid:08a89bb2-0146-44d8-a4fd-9a80d06d80b5>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	10	84	982
79	psychological debriefing effectiveness ptsd treatment	According to systematic reviews, single-session psychological debriefing interventions following trauma are not effective for preventing or treating PTSD. Multiple studies have demonstrated no evidence supporting its use.	['1. Vojdani A, Thrasher JD. Cellular and humoral immune abnormalities in Gulf War veterans. Environ Health Perspect. 2004;112:840–6.[PMC free article][PubMed]\n2. McKeown-Eyssen G, Baines C, Cole DE, Riley N, Tyndale RF, et al. Case-control study of genotypes in multiple chemical sensitivity: CYP2D6, NAT1, NAT2, PON1, PON2 and MTHFR. Int J Epidemiol. 2004;33:971–8.[PubMed]\n3. Da Costa JM. On irritable heart: a clinical study of a form of functional cardiac disorder and its consequences. Am J Med Sci. 1871;61:17–52.\n4. Myers ABR. On the Etiology and Prevalence of Diseases of the Heart among. Soldiers. London: John Churchill & Sons; 1870.\n5. American Psychiatric Association . Diagnostic and Statistical Manual of Mental Disorders. 4th Edition. Washington, DC: American Psychiatric Press; 1994.\n6. First MB, Spitzer RL, Gibbon M, et al. Structured Clinical Interview for DSM-IV. Washington, DC: American Psychiatric Press; 1997.\n7. Lyons JA, Keane TM. Keane PTSD scale: MMPI and MMPI-2 update. J Traum Stress. 1992;5:111–7.\n8. Davidson JRT, Book SW, Colket JT, Tupler LA, Roth S, David D, et al. Assessment of a new self-rating scale for post-traumatic stress disorder. Psychol Med. 1997;27:153–60.[PubMed]\n9. Kubany ES, Leisen MB, Kaplan AS, Kelly MP. Validation of a brief measure of posttraumatic stress disorder: the Distressing Event Questionnaire (DEQ) Psychol Assess. 2000;12:197–209.[PubMed]\n10. Weiss DS, Marmar CR. The Impact of Event Scale-Revised. In: Wilson J, Keane TM, editors. Assessing Psychological Trauma and PTSD. NY: Guilford; 1996. pp. 399–411.\n11. Briere J. Psychometric review of the trauma symptom checklist-40. In: Stamm BH, editor. Measurement of Stress, Trauma, and Adaptation. Lutherville, MD: Sidran Press; 1996.\n12. Blanchard EB, Jones-Alexander J, Buckley TC, Forneris CA. Psychometric properties of the PTSD Checklist (PCL) Behav Res Therap. 1996;34:669–73.[PubMed]\n13. Norris FH, Perilla JL. The revised civilian Mississippi scale for PTSD: reliability, validity, and cross-language stability. J Traum Stress. 1996;9:285–98.[PubMed]\n14. Foa E, Cashman L, Jaycox L, Perry K. The validation of a self-report measure of posttraumatic stress disorder: the posttraumatic diagnostic scale. Psychol Assess. 1997;9:445–51.\n15. King LA, King DW, Leskin GA, Foy DW. The Los Angeles symptom checklist: a self-report measure of posttraumatic stress disorder. Psychol Assess. 1995;2:1–17.\n16. Blake DD, Wheathers FW, Nagy LM, Kaloupeu DG, Klaumizer G, Charney DS, et al. A clinician rating scale for assessing current and lifetime PTSD: the CAPS. Behav Ther. 1990;13:187–8.\n17. Bremmer JD, Vermetten E, Vythilingam M, Afzal N, Schmahl C, Elzinga B, et al. Neural correlates of the classic color and emotional Stroop in women-related posttraumatic stress disorder. Biol Psychiatry. 2004;55:612–20.[PubMed]\n18. Bremmer JD, Soufer R, McCarthy G, Delaney R, Staib LH, Duncan JS, et al. Gender differences in cognitive and neural correlates of remembrance of emotional words. Psychopharmacol Bull. 2001;35:55–87.[PubMed]\n19. Shin LM, Whalen PJ, Pitman RK, Bush G, Macklin ML, Lasko NB. An fMRI study of anterior cingulate function in posttraumatic stress disorder. Biol Psychiat. 2001;50:932–42.[PubMed]\n20. Keane TM, Caddell JM, Taylor KL. Mississippi scale for combat-related posttraumatic stress disorder: three studies in reliability and validity. J Consult Clin Psychol. 1988;56:85–90.[PubMed]\n21. Keane T, Wolf J, Taylor KL. Posttraumatic stress disorder: evidence for diagnostic validity and methods of psychological assessment. J Clin Psychol. 1987;43:32–43.[PubMed]\n22. Kulka RA, Schlenger WE, Fairbank JA, Hough RL, Jordan CB, Marmar CR, et al. Trauma and the Vietnam war generation: Report of findings from the National Veterans Readjustment Study. NY: Brunner/Mazel; 1990.\n23. Keane T, Fairbank J, Caddell J, Zimering R, Taylor K, Mora C. Clinical evaluation of a measure to assess combat exposure. Psychol Assess. 1989;1:53–5.\n24. Spitzer RL, Williams JB. Structured Clinical Interviews for DSM-III-R- Non-Patient Version Modified for the Vietnam Veterans Readjustment Study. NY: New York State Psychiatric Institute, Biometrics Research; 1987.\n25. Beals J, Manson SM, Shore JH, Friedman M, Ashcraft M, Fairbank JA, et al. The prevalence of posttraumatic stress disorder among American Indian Vietnam veterans: disparities and context. J Trauma Stress. 2002;15:89–97.[PubMed]\n26. Monnelly EP, Ciraulo DA, Knapp C, Keane T. Low-dose risperidone as adjunctive therapy for irritable aggression in posttraumatic stress disorder. J Clin Psychopharmacol. 2003;23:193–6.[PubMed]\n27. Barrett DH, Doebbeling CC, Schwartz DA, Voelker MD, Falter KH, Woolson RF, et al. Posttraumatic stress disorder and self-reported physical health status among US Military personnel serving during the Gulf War period: a population-based study. Psychosomatics. 2002;43:195–205.[PubMed]\n28. Conrad KJ, Wright BD, McKnight P, McFall M, Fontana A, Rosenheck R. Comparing traditional and Rasch analyses of the Mississippi PTSD scale: revealing limitations of reverse-scored items. J Appl Meas. 2004;5:15–30.[PubMed]\n29. Elhai JD, Frueh BC, Davis JL, Jacobs GA, Hamner MB. Clinical presentations in combat veterans diagnosed with posttraumatic stress disorder. J Clin Psychol. 2003;59:385–97.[PubMed]\n30. Trent CR, Jr, Rushlau MG, Munley PH, Bloem W, Driesenga S. An ethnocultural study of posttraumatic stress disorder in African-American and white American Vietnam War veterans. Psychol Rep. 2000;87:585–9.[PubMed]\n31. Beckham JC, Braxton LE, Kudler HS, Feldman ME, Lytle BL, Palmer S. Minnesota multiphasic personality inventory profiles of Vietnam combat veterans with posttraumatic stress disorder and their children. J Clin Psychol. 1997;53:847–52.[PubMed]\n32. Betemps EJ, Smith RM, Baker DG, Rounds-Kugler BA. Measurement precision of the clinician administered PTSD scale (CAPS): a RASCH model analysis. J Appl Meas. 2003;4:59–69.[PubMed]\n33. Engdahl B, Eberly R. Assessing PTSD among veterans exposed to war trauma 40–50 years ago. NCP Clinical Quarterly. 1994;4:13–14.\n34. Hovens JE, Falger PR, Op den Velde W, Schouten EG, de Groen JH, Van Duijn H. Occurrence of current post traumatic stress disorder among Dutch World War II resistance veterans according to the SCID. J Anxiety Dis. 1992;6:147–57.\n35. Chiappelli F, Prolo P, Cajulis E, Harper S, Sunga E, Concepcion E. Consciousness, emotional self-regulation, and the psychosomatic network: relevance to oral biology and medicine. In: Beauregard M, editor. Consciousness, Emotional Self-regulation and the Brain, Advances in Consciousness Research. John Benjamins Publishing Company; 2004a. pp. 253–74.\n36. Chiappelli F, Abanomy A, Hodgson D, Mazey KA, Messadi DV, Mito RS, Nishimura I, Spigleman I. Clinical, experimental and translational psychoneuroimmunology research models in oral biology and medicine. In: Ader R, Falter D, Cohen R., editors. Psychoneuroimmunology, III. Academic Press; 2001. pp. 645–70.\n37. Chiappelli F, Cajulis OS. Psychobiological views on ‘stress-related oral ulcers’ Quintess Intern. 2004;35:223–7.[PubMed]\n38. Sterling P, Eyer J. Allostasis: a new paradigm to explain arousal pathology. In: Fisher S, Reason J, editors. Handbook of Life Stress, Cognition, and Health. NY: Wiley; 1988.\n39. McEwen B, Wingfield JC. The concept of allostasis in biology and biomedicine. Hormones Behav. 2003;43:2–15.[PubMed]\n40. Schulkin J. Allostasis: a neural behavioral perspective. Hormones Behav. 2003;43:21–7.[PubMed]\n41. Foa E, Keane T, Friedman M. Effective Treatments for PTSD. NY: The Guilford Press; 2000.\n42. Bradley R, Greene J, Russ E, Dutra L, Westen D. A multidimensional meta-analysis of psychotherapy for PTSD. Am J Pschiatr. 2005;162:214–27.[PubMed]\n43. Pivac N, Kozaric-Kovacic D, Muck-Seler D. Olanzapine versus fluphenacine in an open trial in patients with psychotic combat-related post-traumatic stress disorder. Psychopharmacol. 2004;175:451–6.[PubMed]\n44. Kozaric-Kovacic D, Kocijan-Hercigonja D, Grubisic-Ilic M. Posttraumatic stress disorder and depression in soldiers with combat experience. Croat Med J. 2001;42:165–70.[PubMed]\n45. Kozaric-Kovacic D, Kocijan-Hercigonja D. Assessment of posttraumatic stress disorder and comorbidity. Mil Med. 2001;166:78–83.\n46. Hamner MB, Frueh C, Ulmer HG, Arana GW. Psychotic features and illness severity in combat veterans with chronic posttraumatic stress disorder. Biol Psychiat. 1999;45:846–52.[PubMed]\n47. Bisson JI, McFarlane AC, Rose S. Psychological debriefing. In: Foa E, et al., editors. Effective Treatments for PTSD. The Guilford Press; 2000. pp. 39–59.\n48. Deahl M, Gillham A, Thomas J, Searle M, Srinivasan M. Psychological sequelae following the Gulf war: factors associated with subsequent morbidity and the effectiveness of psychological debriefing. Br J Psychiatry. 1994;165:60–5.[PubMed]\n49. Turner S, Beidel D, Frueh B. Multicomponent behavioral treatment for chronic combat-related posttraumatic stress disorder: trauma management therapy. Behavioral Mod. 2005;29:39–69.[PubMed]\n50. Sherman J. Effects of psychotherapeutic treatments for PTSD: a meta analysis of controlled clinical trials. J Trauma Stress. 1998;11:413–35.[PubMed]\n51. Van Etten ML, Taylor S. Comparative efficacy of treatments for post-traumatic stress disorder: a meta analysis. Clin Psychol Psychother. 1998;5:126–44.\n52. Hembree EA, Foa EB. Posttraumatic stress disorder: psychological factors and psychosocial interventions. J Clin Psychiatry. 2000;61S:33–9.[PubMed]\n53. Davidson P, Parker K. Eye movement desensitization and reprocessing (EMDR): a meta analysis. J Consult Clin Psychol. 2001;69:305–16.[PubMed]\n54. Foa E, Rothbaum B, Furr J. Augmenting exposure therapy with other CBT procedures. Psychiatr Ann. 2003;33:47–53.\n55. Pitman R, Orr S, Altman B, Longpre R, Poire R, Macklin M, et al. Emotional processing and outcome of imaginal flooding therapy in Vietnam veterans with chronic posttraumatic stress disorder. Compr Psychiat. 1996;37:409–18.[PubMed]\n56. Frueh BC, Turner SM, Beidel DC, Mirabella RF, Janes WJ. Trauma management therapy: a preliminary evaluation of a multicomponent behavioral treatment for chronic combat-related PTSD. Behav Res Ther. 1996;34:533–43.[PubMed]\n57. Boudewyns G, Hyer L. Physiological response to combat memories and preliminary treatment outcome in Vietnam veterans: PTSD patients treated with direct therapeutic exposure. Behav Ther. 1990;21:63–87.\n58. Cooper N, Clum G. Imaginal flooding as a supplementary treatment for PTSD in combat veterans: a controlled study. Beh Ther. 1989;3:381–91.\n59. Hamner M, Robert S, Frueh B. Treatment resistant posttraumatic stress disorder: strategies for intervention. CNS Spectr. 2004;9:740–52.[PubMed]\n60. Stein DJ, Zungu-Dirwayi N, van Der Linden GJ, Seedat S. Pharmacotherapy for posttraumatic stress disorder. Cochrane Database Syst Rev. 2000;4:CD002795.[PubMed]\n61. Brady K, Pearlstein T, Asnis G, Baker D, Rothbaum B, Sikes C, et al. Double-blind placebo-controlled study of the efficacy and safety of sertraline treatment of posttraumatic stress disorder. J Am Med Assoc. 2000;283:1837–44.[PubMed]\n62. Davidson J, Landburg P, Pearlstein T, Weisler R, Sikes C, Farfel G. Double-blind comparison of sertraline and placebo in patients with posttraumatic stress disorder (PTSD). Abstracts of the American College of Neuropsychopharmacology 36th Annual Meeting; San Juan, Puerto Rico. 1997.\n63. Davidson J, Malik M, Sutherland S. Response characteristics to antidepressants and placebo in post-traumatic stress disorder. Int Clin Psychopharmacol. 1996;12:291–6.[PubMed]\n64. Chung M, Min K, Jun Y, Kim S, Kim W, Jun E. Efficacy and tolerability of mirtazapine and sertraline in Korean veterans with posttraumatic stress disorder: a randomized open label trial. Hum Psychopharmacol. 2004;19:489–94.[PubMed]\n65. Chiappelli F, Prolo P, Negoatis N, Lee A, Milkus V, Bedair D, et al. Tools and methods for evidence-based research in dental practice: preparing the future. J Evid Based Dent Pract. 2004b;4:16–23.\n66. Rose S, Bisson J, Wessely S. A systematic review of single-session psychological interventions (‘debriefing’) following trauma. Psychother Psychosom. 2003;72:171–5.[PubMed]\n67. Wessely S, Rose S, Bisson J. Brief psychological interventions (‘debriefing’) for trauma-related symptoms and the prevention of post traumatic stress disorder. Cochrane Database Syst Rev. 2001;3:CD000560.[PubMed]\n68. Suzanna RO, Jonathan BI, Simon WE. Psychological debriefing for preventing post traumatic stress disorder (PTSD) Cochrane Database Syst Rev. 2002;2:CD000560.[PubMed]\n69. Albucher RC, Liberzon I. Psychopharmacological treatment in PTSD: a critical review. J Psychiatr Res. 2002;36:355–67.[PubMed]\n70. Levine EG, Eckhardt J, Targ E. Change in post-traumatic stress symptoms following psychosocial treatment for breast cancer. Psychooncol. 2005 Jan 13 [Epub] [PubMed]\n71. McPherson F, Schwenka MA. Use of complementary and alternative therapies among active duty soldiers, military retirees, and family members at a military hospital. Mil Med. 2004;169:354–7.[PubMed]\n72. Westermeyer J, Canive J, Thuras P, Chesness D, Thompson J. Perceived barriers to VA mental health care among Upper Midwest American Indian veterans: description and associations. Med Care. 2002;40S:I62–71.[PubMed]\n73. Sommers E, Porter K, DeGurski S. Providers of complementary and alternative health services in Boston respond to September 11. Am J Public Health. 2002;92:1597–8.[\nPTSD Introduction. In October of 2001, America launched new military action in Afghanistan which would come to be known as Operation Enduring Freedom. Within the scope of America’s War on Terrorism we soon found ourselves in another war officially called Operation Iraqi Freedom. As these and other similar conflicts became the household names and terms within the last decayed, so have the visualization of battlefield injuries and enormous physical and psychological impact on the military personal. Post Traumatic Stress Disorder, commonly known as PTSD has embraced itself as a major medical and psychological malfunction among American soldiers. PTSD does not limit itself to any one demographic or country but is a universal complex disorder that has made headlines throughout various countries and has become high in conversation here in America. Books, journals, college term papers, etc. have recently been published in the quest to bring this subject to a fore front of the public due to its severity and increasingly commonality among combat veterans. As PTSD’s complexity on many levels has become learning process in which its healing process requires constant situational awareness by all those involved. i.e. injured personal, medical and psychological community, family, friends and group therapy, etc. Cisell-Brown is a daughter of a Viet Nam veteran and the wife of an Iraqi war veteran. In 2009 as published in the American Journal of Nursing , she says that of the many obstacles veterans face while trying to obtain care, one main reason is the fact that health care professionals fail to recognize the illness. She goes on to say that by the time their illness is validated, a more serious or chronic form of PTSD has emerged. The purpose of this paper is to help continue to bring Post Traumatic Stress Disorder to the recognizable terms and to discuss and highlight some of the significant professional literature published on this issue. 2']	['<urn:uuid:b8d0796c-a307-40bf-a5bf-f59d82af7157>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	5	27	2183
80	storing duration comparison walnuts amarone wine shelf life tips	Walnuts can be stored for up to 1 year in the refrigerator and 2+ years in the freezer. Amarone wine, particularly those made in the traditional method, can age for up to 40 years, while modern method Amarone wines typically last 8-20 years when stored properly.	['Walnuts are my hands down favorite nuts with the added benefit of not only being high in protein but omega-3 fatty acids as well. Omega-3 fatty acids are touted as extremely beneficial for the heart but beyond that, they are delicious! What better reason to grow your own? The question is, when are walnuts ready to pick and what is the best way to pick walnuts?\nWhen are Walnuts Ready to Pick?\nWalnuts may be either English or the black walnut varieties, with the latter having a thicker shell and more intense flavor. Both types are fruiting, deciduous trees that are fairly easy to grow and lacking in few serious issues especially once mature.\nThey can grow to 100 feet (30 m.) tall and 50 feet (15 m.) across, which makes the tree a bit unmanageable for some landscapes. Luckily, young trees can be trained via pruning. Walnut trees can be grown with a central leader or remove the leader which will encourage side shoot growth and restrict the tree’s size.\nA pitted shell encases a fibrous, leather sheath that splits as the nuts begin to ripen in the fall and indicates that walnut tree harvesting is nigh. Once you are done harvesting the walnuts, you can eat them right away, but keep in mind they won’t be quite like those purchased ones at the grocers.\nThe nuts will be rubbery in texture and are, thus, usually dried which also extends their shelf life. Think your nuts are ready for harvesting but don’t know the best way to pick walnuts? Keep reading to find out how to harvest walnuts.\nHow to Harvest Walnuts\nDepending upon the variety and region they are grown in, walnut tree harvesting starts from early September to early November. At this point, the kernels are light in color and the membrane between the halves has turned brown.\nTo determine if your nuts are ready for harvest, crack a few open. The nuts should show browning of the membrane and loosening of the hull. Take your nut samples from as high up in the tree as possible since those that are at this height ripen latest. Also, if your tree is water stressed, harvesting walnuts will be delayed. To speed things up, be sure to keep the tree well watered through harvest.\nBegin harvesting when you estimate that at least 85% of the nuts can be easily removed from the tree. Delay too long and insects and birds may get to the nuts before you do. Additionally, if you delay too long, the outer husks become soft and black and the resulting nut has a bitter, rancid flavor.\nTo begin harvesting walnuts, you will need a pole or a pole combined with a hook for larger trees. Shake the nuts loose using the pole. Immediately pick the walnuts up from the ground. If they lie there too long, they will either begin to mold or become over run with ants, or both. The hulls of walnuts contain phenols, chemical compounds that cannot only stain hands but for some people cause skin irritation, so when handling walnuts, wear rubber gloves.\nOnce you have harvested the walnuts, hull the nuts using a pocket knife. Wash the hulled nuts and then dry them in a single layer on a smooth, flat, shaded area. Stir the nuts around on a daily basis to promote drying. If drying outdoors, cover the nuts with plastic netting to deter birds. The length of time until complete drying depends on temperature but, generally, will be dry in three to four days. At this point, the kernels should be brittle as well as the membrane separating the two halves.\nStore the cured walnuts in a cool, dry area or to extend their shelf life, in the refrigerator or freezer. They can be stored for up to a year in the fridge and for two or more years in the freezer; that is, of course, if you can stay out of them that long.', 'Amarone wine or as it’s officially named, Amarone della Valpolicella, is one of those wines that you buy and sit on and pray your marriage stays together long enough so that you can drink it on your 20th anniversary. It’s one of those holy-jesus-I-may-now-die-complete wines that, if you’re lucky, you can pick up for around $100. No, Amarone della Valpolicella is not cheap, but it shouldn’t be, it’s just too difficult to make–and too scarce.\nLet’s take a detailed look into what Amarone wine is all about and why it’s special, from a taste profile of great Amarone to its defining features so you can find great wines on your own. This is an advanced guide, so open up a bottle of Ripasso and start sipping!\nGuide to Amarone Wine\nThe Taste of Amarone Wine\nExpect bold aromas of cherry liqueur, black fig, carob, cinnamon and plum sauce along with subtle notes of green peppercorn, chocolate and crushed gravel dust. Sound intriguing? On the palate Amarone wines often have medium-plus to high acidity balanced with high alcohol and flavors of black cherry, brown sugar and chocolate. By the way, the older the wine the more it will offer flavors of brown sugar, molasses and fig. What might surprise you about this wine is the presence of a touch of natural residual sugar (RS) in the wine, usually around 3–7 g/L (or about a 1/4 teaspoon per serving). The RS helps compliment the wine’s natural high acidity and adds to its boldness–if you didn’t know Amarone had residual sugar, you’d think it was dry.\nThe Styles of Amarone della Valpolicella\nTime for a quick history lesson: In 1963 the Italian government adopted a system of quality assurance labels for its food products, especially wines and cheeses. The labels rate how authentic and regional the methods of a food’s production are, and increase in strictness from IGT, to DOC, to DOCG. Only a few wines get to become DOCGs (Denominazione di Origine Controllata e Garantita), and Amarone della Valpolicella officially became one of them in 1968. With the designation came many rules about planting, vine production and vineyard location; but more important to us is the production method called apassimento, and the styles of Amarone and Amarone Riserva. You can read more about wine designations here.\nAmarone vs. Amarone Riserva\nThe major difference between Amarone “normale” and Amarone Riserva is time. Amarone is aged 2 years following the vintage, whereas Amarone Riserva is required to age 4 years. Now, you’ll find that in reality great producers tend to age the wine longer than the minimums and release when they believe it to be ready. This is good considering that some Amarone wines should really continue to age for 10 or 15 years longer to develop those keen flavors of fig, carob and mexican chocolate. So, “the older the better” tends to reign true with this particular wine. There is one thing you should really pay attention to when picking an Amarone wine, and it’s the methodology used to make them.\nThe Apassimento Method and Traditional vs. Modern Winemaking\nCorvina grapes are laid out in drying lofts where they will lose 40% of their moisture. Image courtesy of Bertani\nTechnically, there is only one way to make Amarone wine:\n- pick grapes\n- dry grapes until there is 40% less liquid (called apassimento and can take as long as 120 days)\n- slowly press dried grapes\n- slowly ferment grapes into wine over a period of 35–50 days (this is a long time for wine!)\nHowever, due to modern technology two distinct styles have emerged. There are those who practice the traditional method of naturally drying their grapes and using neutral oak or chestnut barrels to age them and there are those who use a modern method of quickly drying grapes using temperature and humidity-controlled rooms and aging their wines in new oak barrels. Both methods can make excellent tasting wines but they will taste a bit different on first release and also tend to age differently.\nAmarone della Valpolicella made in the traditional method tend to maintain their acidity longer and, thus, will also potentially age quite a bit longer too. In taste trials it appeared that traditional method Amarone could easily last 40 years! As great as this is, these wines also take a bit longer to come around, meaning you’ll want to be sure to hold them for around 20 years to really let the wine shine. It’s common to see producers practicing the traditional technique to only use the regional grapes of Corvina, Corvinone and Rondinella in the blend. In terms of flavors on release, tasting notes for this style often have flavors of red cherry, cinnamon and green peppercorn. If you drink them sooner (and let’s be honest, this happens) be sure to decant them for a couple of hours and they’ll still be awesome.\nExamples of traditional producers\nAmarone della Valpolicella made in the modern method tend to be quite a bit bolder upon release because of the help of new oak aging which adds flavors of chocolate, molasses and vanilla along with cherry liqueur. It’s also more common to see non-indigenous varieties blended into the modern-styled wines. Legally, it can be up to 25% of other grapes including Cabernet Sauvignon, Merlot and Sangiovese. The wines taste awesome right out of the gates but aging-wise tend to taper off a bit faster. Some will only last 8–10 years, whereas others with bolder red fruit characteristics will go 20 or so years. As always, a bit of decanting is great for any Amarone wine.\nExamples of modern producers\nThe Grapes of Amarone Wine\nThere are less than 12,000 acres in the world of the most important Amarone grapes–Corvina and Corvinone, and they only grow in Valpolicella. To make the situation a little more intense, Valpolicella has a regulatory committee that protects the historic nature of the land around Verona. It means if a winery wants to plant a new vineyard, they’ll have to rip out an old vineyard to allocate space. There are 4 main grapes of Amarone and a total of 20,000 acres (8,200 ha).\n- Corvina (technically Corvina Veronese)\nWinemakers in the region will tell you that the best Valpolicella wines come from the Corvina (and more rare Corvinone) grapes. Historically, Rondinella and Molinara were very dominant in the region, however they tend to produce lower quality grapes due to their high productivity. Thus, the wines made with primarily Corvina grapes offer up the heady aromas of rose, cherry liqueur and cinnamon and they also consistently get the highest ratings.\nThe Region of Amarone Wine\nThe Valpolicella wine region lies in the lowest foothills of the Alps just north of Verona and has 3 primary zones: Classico, Valpantena, and Est (meaning “East”). Most of us will hone our focus on the Classico zone for quality (which does contain 5 notable sub-areas), but within each of the 3 main zones there are many excellent wines.\nWithin the Classico zone there are 6 designations within the 3 valleys of Negrar, Marano and Fumane. This is the “Original Gansta” area for Amarone and Recioto della Valpolicella wines. The Classico zone even has an order of knights called SNODAR (Sovereign Noble Order of Ancient Recioto) established in 1969 –a year after the DOC was first officiated, to promote and protect Valpolicella’s wines. Within the Classico region you’ll find many of the largest producers–save for a few.\n- San Pietro in Cariano (lower valley)\nHeading east from the Classico zone is Valpantena. The best vineyards have been noted around Grezzana and Cerro Veronese which is midway up the valley.\n- Cerro Veronese\nThis region is next to the Soave wine region (a Veronese white wine made with Garganega grapes) and is considered a newer region for producing Amarone wines. The best vineyards have been noted in the mid-point up the valley around Illasi, Cazzano di Tramigna, Mezzane and Tregnago.\n- Cazzano di Tramigna\n- San Mauro di Saline\n- Colognola ai Colli (low valley)\n- Montecchia di Crosara (far east, next to Soave)\n- San Martino Buon Albergo (low valley)\nTasting Amarone Wine\nTasting Amarone is a fascinating experience. You’ll want to be sure and decant the wine and serve it in oversized glasses to collect its aromas. Younger wines can typically be served just below room temperature and older wines slightly cooler. Hopefully this guide will get you into the perfect bottle. Salut!']	['<urn:uuid:4af4de5f-a843-406d-82d0-6486ce3bc644>', '<urn:uuid:458df5a0-a5b8-4646-a3a0-033e5b8ec859>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T00:57:18.859842	9	46	2080
81	structural adaptations pressure burj al arab building versus human body stress	The Burj Al Arab and human body both have specialized adaptations to handle pressure. The hotel uses a double curve membrane design with PTFE-coated fiberglass containing air gaps, plus expansion joints to handle wind pressures. Similarly, the human body has built-in adaptations to pressure in the form of stress responses - the heart pounds faster and breathing increases to prepare for challenges. While the building's adaptations are permanent physical features, the body's stress response involves temporary physiological changes that can either help or harm based on how the person perceives them.	['Aerialist Nik Wallenda looks down and sees the 1,500-foot drop to the bottom of the Grand Canyon below him. All that stands between him and a lethal landing is the 2-inch tightrope that he has decided to traverse on camera, the moment being broadcast around the world on live television. If most people were to rank the most stressful events of their life, this would very likely be near the top of the list. But Wallenda thrives on stress of this magnitude.\nIn June, Wallenda balanced his way across a quarter-mile gap in the Grand Canyon. And with his feet firmly back on the ground, he shared how he is able to perform stunts like this: by seeing the physical manifestations of stress as positives. Yes, the body will start to shake on the tightrope. But this is not a sign of weakness; it is instead a natural response that is preparing him for what is to come. In moderate amounts — and perhaps this comes with decades of training — stress can be helpful in willing strength and focus when it is needed most.\nKelly McGonigal: How to make stress your friendIn today’s talk, given at TED2013, health psychologist Kelly McGonigal reveals a similar approach to stress. She shares a fascinating idea: that the harmful effects of stress may be a consequence of our perception that it is bad for our health.\n“Can changing how you think about stress make you healthier? Here the science says yes,” says McGonigal. “Your heart might be pounding, you may be breathing faster… but what if you viewed them as signs that your body was energized and it’s preparing you to meet this challenge.”\nMcGonigal says that a paradigm shift when it comes to stress could literally be life saving. In this talk, she shares some of the research behind her conclusion. Below, we’ve rounded up some of the studies she mentioned, as well as some further research that hints at some surprising upsides of stress.\nStress may actually be correlated with longevity—if a person doesn’t view it as a negative\nThe study: Researchers at the University of Wisconsin-Madison asked almost 29,000 people to rate their level of stress over the past year as well as how much they believed this stress influenced their health — a little, a moderate amount or a lot. Over the next eight years, public death records were used to record the passing of any subjects.\nThe findings: People who reported having high levels of stress and who believed stress had a large impact on their health had a whopping 43% increased risk of death. On the other hand, those that experienced a lot of stress but did not perceive its effects as negative were amongst the least likely to die as compared to all other participants in the study.\nFurther reading: Check out this paper, “Does the perception that stress affects health matter?” And these two similar studies back up the idea: (1) “Increased risk of coronary heart disease among individuals reporting adverse impact of stress on their health: the Whitehall II prospective cohort study” (2) “Meta-analysis of perceived stress and its association with incident coronary heart disease.”\nA possible antidote to negative effects of stress: giving to others\nThe study: Lead author Michael J. Poulin of the University of Buffalo and his team interviewed almost 850 people, ages 34-93, living in Detroit, Michigan. Participants were asked to report stressful events they had encountered in the past year and how much, in the same time period, that they had assisted others. Deaths that occurred within the group in the next five years were tracked using obituaries and public death records.\nThe findings: Every major stress event increased an individual’s risk of death by 30%. But, overall, this increase was erased for those who reported high rates of helping others, even if they additionally dealt with a lot of stress. The evidence suggests that giving to others significantly reduces stress-induced mortality.\nFurther reading: “Giving to others and the association between stress and mortality”\nModerate stress can lead to cell growth in the brain’s learning centers\nThe study: In this 2013 study out of University of California, Berkeley, adult rats were immobilized in a small space for three hours or left unchanged. Two days and then two weeks later, the rats were given a fear-conditioning test, which evaluates the rats freezing or avoidance behavior when in a context that last lead to a negative consequence such as a shock. The measure is known as ‘fear extinction memory.’\nThe findings: The immobilized rats showed an increased level of the stress hormone corticosterone (the rat equivalent of cortisol in humans) as well as an increased growth of neural stem cells in their hippocampus, an important learning center of the brain. As compared to the control group, these rats had similar results on the fear extinction memory test given two days after the initial stressor, but showed significant improvement on the test after two weeks. The authors put fourth that these newly proliferated cells, after taking multiple days to integrate to fullest capacity, helped in learning at this improved level. Overall, it suggests that moderate levels of stress enhanced neural function and learning.\nStress can summon helper hormones to vulnerable areas\nThe study: A small amount of stress has previously been found to initiate the redistribution of immune cells, which is thought to aid in survival by sending protection where stress is occurring. Researchers at Stanford University set out to learn more about the shifting levels of these cells, like white blood cells, over time and how different stress hormones trigger the response. Rats were either restrained for a random period from two minutes to two hours or injected with stress hormones (epinephrine and corticoterone) to mimic the effects of such an event. Blood samples were collected throughout.\nThe findings: The restraint experience and the injection brought on consistent patterns of mobilization in the rats. For example, after the actual stressor, the concentration of a majority of immune cell types monitored followed a pattern of increase and then subsequent decrease in the rat’s blood. Further, specific hormones stimulated unique reactions in a subpopulation of immune cells. By understanding this system, researchers hope to crack the code that would predict which hormone to administer in order to stimulate immune cell aggregation in a specific vulnerable region, just as the body does naturally in response to acute stress.\nFurther reading: Read the study, “Stress-induced redistribution of immune cells–from barracks to boulevards to battlefields: a tale of three hormones.” And watch the TEDx Talk from the lead author called “The positive effects of stress.”\nStress can induce both good and bad habits\nThe study: Do you bite your nails, change your eating habits or fall back into addictive dependencies when you are stressed? It is commonly thought that the pressures and anxieties of life are a major initiator of such bad habits. Scientists from the University of California, in the first of five experiments, asked 65 students to record what section of the newspaper they read and what they ate for breakfast over a period of several weeks. Reports for the first three weeks were used to determine habits to then compare data from the next four, a time when the presence of exams lead to depleted willpower. A separate group of students rated breakfast and newspaper options on a scale based on desirability (with the most desirable being the goal), specifically based on which were healthier or more educational, respectively.\nThe findings: By comparing the students’ choices in more and less stressful periods to the more desirable actions, it was observed that when willpower is depleted, people often return to their habits regardless of its effect on reaching a goal. Importantly, however, the action that became repeated for each individual could be either harmful or helpful –it simply depended on whether it sincerely is a natural tendency. This suggests that breaking a bad habit is most successful when more willpower is maintained, but that times of less control may not be so bad if there are some good habits to fall back on.\nA stress-is-enhancing mindset may have lasting effects\nThe study: Employees at a financial institution were asked to take a test on their stress mindset before and after watching three videos over the course of a week that either presented stress as enhancing or harmful. In a second study, students who had previously taken a survey on their stress mindset were told in class that five of them would be randomly selected to give a speech that would also be videotaped. For each student, mouth swabs were taken to measure cortisol levels. Each was also asked to decide, if chosen to speak, whether or not they would receive feedback from their peers and business school experts who watched the footage.\nThe findings: In the first study, not only were many people influenced by the message of the videos. Those that viewed the video that approached stress as enhancing reported better work performance as well as less psychological complications. As for the students, those who naturally saw stress as helpful had a more moderate cortisol response upon hearing about the speech possibility — and they were more likely to request feedback.', 'Burj Al Arab is the world’s tallest iconic and most luxurious hotel. This building has received numerous recognitions around the world, and is mainly known by its design which resembles the shape of a sail boats mast.\nThis 321 meter high building was built on a man-made island only 280 meters from the coast, giving all visitors a 360o view of the bay. Construction began in 1993. Engineers created a surface with a layer of rocks, which is circled with a honey-comb pattern which protects the surface and foundation from erosion. The building contains more than 70 000 m2 of concrete and 9 000 tons of steel. It took only two years to construct the building and three years to construct the foundation on beach sand, making this a geotechnical wonder.8\nGeotechnical Engineering entails to obtain information of the physical properties of the soil and rock on a proposed site known as site exploration. This information combined with the mechanics of soil can assess the risks presented by the site conditions that must be concluded in the design process of foundations, earthworks and retaining walls.\nBurj Al Arab has one of the rarest and most interesting foundations, earthworks and retaining wall phenomena’s. This include building an island 280 meter off coast as a foundation for this 321 meter high sea shore wonder.1 Throughout this report we will look at which soils are found in Dubai, as well as the properties thereof including the construction of this hotel foundation.\nIf you need assistance with writing your essay, our professional essay writing service is here to help!Essay Writing Service\nConstructing the Burj Al Arab hotel\nThe design of the Burj Al Arab has been constructed with a specific geometry which supports the sail boat like design by protecting the building from changing wind loads. The outermost wall of the building has been constructed by the use of PTFE coated fibreglass which contains air gaps at regular intervals. This double curve membrane design is able to withstand wind pressures easily.3\nAdditional cables have been provided onto the structure to prevent any deflection in materials. On the full height of the building expansion joints were provided on the right side of the building to ensure the building can withstand the wind load pressures as well as the horizontal stresses that may occur during construction and operation. The material that was used for this sail boat like structure is not only robust but it also protects the buildings’ interior from the sun by using light defusion.3\nAfter seventy thousand cubic meters of concrete and ninety thousand tons of steel, this great engineering wonder is noted as the heart of Dubai. This spectacular feature of the Burj Al Arabhotel, with its two hundred and two rooms, is located 280 meter off shore from the coast of Dubai and recognised as the best in the world. 2\nConcept Architect: Tom Willis-WrightimagesCAHJ41BO.jpg\nConstruction Engineer: WS Atkins and Partners Overseas – Multidisciplinary Consultancy.\nInterior designers: Khuan Chew, Design Principal of KCA International (London).\nLocation: Jumeirah Beach Road, Jumeirah, Dubai, United Arab Emirates.\nType/Structure: Luxury 7 stars* rating hotel/resort\nSize: 321m x 280m (1,053 ft)\nMedium: steel, glass, cement, steel cables, piles\nFrom: Antonino Terranova. The Great Skyscrapers of the World. Special gatefold edition, page 269-279.\nFigure 1: Burj Al Arab hotel\nDubai’s Soil Profile\nThe topography of Dubai (which lies within the Arabian Desert) is different from the southern portion of the UAE. Its landscape consists of sandy desert patterns consisting mostly out of crushed shell and coral and is clean and white, whereas gravel dominates in the southern regions of the country. 2 When looking at the soil properties of Dubai, it’s weak and will most probably move outwards in the case of any construction on it. See figure 1 Dubai soil map. 3\nStudies also show that Dubai’s possibility of a tsunami is minimal, due to the Persian Gulf water that is not deep enough to trigger a tsunami. Thus Dubai is classified as a stable zone, whereas the nearest seismic fault line is 120 km from the UAE, making it unlikely for Dubai to be hit by a seismic impact.2\nFigure 2: Dubai soil map.\nThe moment when Dubai laid focus on the development of this world wonder, they knew it would be an engineering challenge. Many elements must be taken in consideration in geotechnical engineering to build the world’s 15th tallest building on seabed, where its properties are known as a collapsible soil due to a lack of silt and clay.\nThe collapse phenomenon can be defined as a soil which can withstand somewhat large stresses, with little settlement at low in situ moisture content which will show signs of a decrease in volume and associated settlement with no increase of load if the moisture content rises. Therefore the change in volume goes hand in hand with the change in the soil structure.\nIt is thus evident that a number of conditions need to be met before collapsing begins: 6\nThe soil must have a collapsible fabric in its structure. This is where the specified soil has a high void ratio and yet has relatively high shear strength at low moisture content due to a coating (Colloidal) around each grain.\nPartial saturation is essential. This is where collapse settlement will not occur in soils which are located under the water table.\nIncrease of moisture content. This could be seen as the cause for the collapse to take place. With the increase of moisture the colloidal coating loses its strength and thus forces the grains to a denser state with reduction in void ratio.\nSubjected to an imposed load greater than their overburden pressure before collapse can take place. This is only applied to certain collapsible soils.\nThe typical problem associated with a collapsible soil to a building is that although it is dependent on the increase of the moisture content, collapse can take place years after construction has taken place. 6 Large magnitude settlements can occur beneath lightly loaded structures as well as collapse settlement is regularly localised due to defects in foundation, drainpipe leakage and where ponding occur during rainfall.\nThe engineering properties which most affect the cost of a construction are strengthening compressibility. Both can be enhanced by reducing the voids in the soil. Water must be displaced from the saturated soils in order to reduce the void volume. This can take months if the permeability of the type of soil present is low.\nThe following engineering solutions to the mitigation of the collapse problem are listed below: 6\nAvoidance by stopping the triggering mechanism (increase in moisture). This can be made possible by ensuring that water does not penetrate the collapsing soil horizons.\nDesign for collapse. This could be possible in certain scenarios to design a structure which could withstand the predicted collapse settlement.\nChemical stabilisation. This is to make use of a stabilising agent which could reduce the settlement.\nPiled or pier foundation. This is used only when the soil comes from a transported origin which means that the bedrock is covered with a shallow layer, making it possible to rather build on piers or piles.\nRemoval and compaction. This could be done by removing the collapsible soil to a certain depth and replace it through compacting the removed soil in layers.\nIn situ densification by surface rolling. Surface rolling can be done by making use of an impact or vibrating roller for compaction.\nBeach sand is one of many soils that have a collapsible grain structure, where its surface contains large quantities of calcium carbonate which in more defined terms are remains of microscopic plants and animals that thrive on nutrients in the water surface, where it ultimately settles to the floor.\nThe strength and the behaviour of this soil are thus dependent on the calcareous particles which it contains. These grain particles’ is well rounded due to it being rotated and shaped by the waves and is poorly graded (i.e. having a narrow particle distribution). This contributes to the high void ratio, meaning that the soil is very loose and can be seen as not a good bonding material. 6\nTable 1: Transported soil and possible engineering problem.\nTransported Soil Type\nAgency of transportation\nProblem to anticipate\nCollapsible grain structure\nTests to be performed on beach sand\nA large amount of data can be generated from soil, but it can all be wasted if the most important step of sampling is not carried out properly. Thus, in order for an analysis to be of significance to a proposed project, it should represent the bulk material of the site. Additionally, soil samples must be taken in abundance and at random, to ensure that the overall characteristics of the soil are effectively represented. See table 2 for properties beach sand.\nThe following tests were used in classifying beach sand (collapsing soil structure):3\nParticle size distribution. This test is performed to measure the particle size distribution of the soil sample by passing it through a set of sieves. This is in order to produce a grading curve for the soil, which is used to find out its classification. The solid particles in a soil can have different shapes and sizes, and these characteristics thus have a significant effect on its engineering behavior. By making use of this test one can clearly note whether the soil is well or poorly graded. As for beach sand it is known to be a collapsible soil due to it having a poorly graded grain structure and affected by an increase in moisture.\nOur academic experts are ready and waiting to assist with any writing project you may have. From simple essay plans, through to full dissertations, you can guarantee we have a service perfectly matched to your needs.View our services\nAtterburg limits. This test makes use of three separate tests namely Liquid limit test, Plastic limit test and Shrinkage limit test. This test is used to determine a relationship between the soils consistency and its moisture content. If the soil has low moisture content, it would aim to break before deformation takes place, whereas if the moisture content is too high, the soil will deform more easily. This test is of great importance due to it having an impact on settlement underneath a proposed structure. The test can be used to distinguish between the presence of silts and clays. This is important as silt has much less cohesion than clay.\nDry density. This can easily be determined in a laboratory by measuring its physical dimensions and by weighing them. The dry density of a collapsible soil lies between 900-1600 kg/m3.\nOedometer test. When a structure is build on a soil it produces settlement due to compression within the soil profile, which depends on the soil’s properties such as self-weight and also the type of load the soil is experiencing. This test makes use of a series of loads in order to measure the corresponding settlement of the soil. By knowing the soil’s stress and strain properties will allow the prediction in settlement and swelling of the soil.\nCollapse Potential test. This test is used to determine the collapse parameters in order to design accordingly. The CP (Collapse Parameters) is given in percentage, to determine the level of severity.\nTriaxial test. This test is similar to the unconfined compression test, except that the sample is surrounded by a waterproof membrane and installed in a pressure chamber (cell). This test is thus performed to estimate the stress and strain parameters of the specific soil.\nPermeability. This test is used to determine the ease of which water can flow through a soil profile, which is important for geotechnical engineers in projects.\nTable 2: Soil properties of silt sand.9\nBulk Density (Mg/ M3)\nDry Density (Mg/ m3)\nLiquid limit (%)\nPlastic limit (%)\nEffective cohesion (kPa)\nAngle of friction (deg)\nConstruction of Burj Al Arab Foundation\nConstructing a building on sea, an artificial island is needed to design and build the foundations. As many elements need to be taken into account to build an off shore structure, it is therefore important to ensure the protection of the foundation. This can only be done by evaluating all apposed loads to a structure. Seafloor stability regards to the bearing capacity and the sliding resistance thereof must be evaluated for static and combined static, operational and environmental (Like horizontal, vertical loading and overturning moments of the environment which have a return period of up to 15 seconds) loads. Structures with more or less a 150 meter depth could experience horizontal loads of 15-35% of the vertical loading, whereas the overturning moment can be ranging from 100 to 500 million kN/m.8 The change in vertical load during a storm can range from 10 – 40 % of the static vertical load. This means that the foundation needs to be strong to be able to obtain these loadings. Luckily these loadings were much less when Burj Al Arab was constructed, due to it only being 7 meters in depth.8IslandConstruction1.jpg\nFigure 3: Piling of the Burj Al Arab hotel\nThe first step in constructing the island was to place 230 concrete piles (see Figure 3), each one 40 meter in length, which was drilled into the sea bed. The foundation is therefore held in place by the friction of the sand and silts along the length of the piles, rather than the conventional bedrock. The surface was then made by using large rocks that were put together in a specific concrete pattern (honey-comb pattern) which serves as a shield to minimise erosion to the foundation.\nMaking the platform on which the building would be constructed, tube files and sheet files were drilled deep into the sea to support various boundary rocks. Once this was completed the sea water was displaced to fill the inside with concrete slabs as seen on table 3. IslandConstruction2.jpg\nThe structure was then surrounded with a temporary concrete structure to protect the island and the base of the structure, which was filled with a concrete plug slab. Lastly the concrete walls was made where the main basement floor of the building was build (See Figure 4).3\nFigure 4: Burj Al Arab Excavated Basement\nTable 3: Foundation of the Burj Al Arab\nThe Burj Al Arab being a geotechnical wonder is due to its size as well as its ability to withstand the environment and the impacts thereof. This building is only carried by a sandy soil which contains broken shells. Its ability under shear strength and pressure is very low, especially since it is located 280 meters of shore. Thus the building stands only on piles which are held into place by just the friction between the soil and the piles, making this project one of the most remarkable foundation types in history. The design of this foundation on this type of soil is breaking barriers in the building industry, making Dubai a leading country in development.\nDue to South Africa not being a first world country it is impossible for us to be compared to a city which encourages ultimate engineering in structural, hydraulic and geotechnical engineering. Therefore we cannot compare the infrastructures of their country with ours. South Africa could always strive to be a first world country by focusing on infrastructure as well as the inequality of societies. This would encourage tourism and affect the economy positively.\nCite This Work\nTo export a reference to this article please select a referencing stye below:\nRelated ServicesView all\nDMCA / Removal Request\nIf you are the original writer of this essay and no longer wish to have your work published on UKEssays.com then please:']	['<urn:uuid:e9e0959b-ad46-41ce-9039-8d8856bd05e6>', '<urn:uuid:c03a22c6-b8cb-4901-a631-3e39e83ec2a6>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T00:57:18.859842	11	91	4161
82	grief types causes treatment methods	Grief can be triggered by both death-related losses (death of person/pet) and non-death losses (divorce, job loss, health issues). Normal grieving involves adapting to the loss and finding appropriate emotional place for the lost person/object. Treatment methods include talking with therapists, writing in journals, maintaining routine activities, using relaxation techniques, and seeking professional help when needed.	['Coping with the loss of someone or something you love is one of life’s biggest challenges. Often, the pain of loss can feel overwhelming. You may experience all kinds of difficult and unexpected emotions, from shock or anger to disbelief, guilt, and profound sadness. The pain of grief can also disrupt your physical health, making it difficult to sleep, eat, or even think straight. These are normal reactions to significant loss. But while there is no right or wrong way to grieve, there are healthy ways to cope with the pain that, in time, can ease your sadness and help you come to terms with your loss, find new meaning, and move on with your life.\nWhat is grief?\nGrief is a natural response to loss. It’s the emotional suffering you feel when something or someone you love is taken away. The more significant the loss, the more intense your grief will be. You may associate grieving with the death of a loved one—which is often the cause of the most intense type of grief—but any loss can cause grief, including:\n- Divorce or relationship breakup\n- Loss of health\n- Losing a job\n- Loss of financial stability\n- A miscarriage\n- Death of a pet\n- Loss of a cherished dream\n- A loved one’s serious illness\n- Loss of a friendship\n- Loss of safety after a trauma\n- Selling the family home\nEven subtle losses in life can trigger a sense of grief. For example, you might grieve after moving away from home, graduating from college, or changing jobs. Whatever your loss, it’s personal to you, so don’t feel ashamed about how you feel, or believe that it’s somehow only appropriate to grieve for certain things. If the person, animal, relationship, or situation was significant to you, it’s normal to grieve the loss you’re experiencing.\nHow to cope with grief\nWhile experiencing loss is an inevitable part of life, there are ways to help cope with the pain, come to terms with your grief, and eventually, find a way to pick up the pieces and move on with your life.\n- Acknowledge your pain.\n- Accept that grief can trigger many different and unexpected emotions.\n- Understand that your grieving process will be unique to you.\n- Seek out face-to-face support from people who care about you.\n- Support yourself emotionally by taking care of yourself physically.\n- Recognize the difference between grief and depression.\nThe grieving process\nGrieving is a highly individual experience; there’s no right or wrong way to grieve. How you grieve depends on many factors, including your personality and coping style, your life experience, your faith, and how significant the loss was to you.\nInevitably, the grieving process takes time. Healing happens gradually; it can’t be forced or hurried—and there is no “normal” timetable for grieving. Some people start to feel better in weeks or months. For others, the grieving process is measured in years. Whatever your grief experience, it’s important to be patient with yourself and allow the process to naturally unfold.', 'I was asked to speak on Radio FIVEaa recently about loss, grief and trauma following the death of a well-known South Australian. The community was hit hard by his loss, and having some knowledge about these areas might assist. We may experience loss which is death-related (death of a person or pet), or non-death-related (divorce, loss of one’s job or health). ‘Attachment’ between individuals develops to maintain a state of balance in life and loss and grief disturbs this balance – people often describe a sense of their ‘whole world being thrown upside down’ (Bowlby, 1980). There can be a roller-coaster of emotions, from sadness to numbness, anger, guilt or anxiety.\nGrief is the response to loss, and affects many aspects of the individual: physical, emotional, behavioural, cognitive (such as memory and concentration), social and spiritual. It involves adaptation to the loss, and as loss threatens our inner assumptions about the world, it takes time to re-adjust (Corr, 1998; Parkes, 1988). Loss may be hidden by individuals, particularly if there is stigma or shame involved. Loss may be gradual, such as adapting to dementia in a parent.\nAdjusting to loss takes time and effort. It can be very useful to talk with your GP, therapist or a friend. It is important to find someone who is a good listener and someone whom you trust. There has been a lot written about grief work, and there are a number of different approaches to grief therapy. This (Worden, 2008) involves understanding the process of grief, and that it is normal to have positive as well as negative feelings about the lost person or object. It is important to share thoughts and feelings about the loss and reviewing what it means to the individual. We eventually move forward – but this does not mean giving up on the lost person or object, but rather ‘finding an appropriate place’ for them in our emotional lives.\nDr Sheila Clark, a GP with expertise on grief and author of ‘After Suicide Help for the Bereaved’, advises:\n1. Allocating grief time each day – say 15 to 20 minutes in which to have a cry or write about the loss (such as in a journal).\n2. Naming the problems – emotional or practical.\n3. Looking after general health – endeavouring to eat regularly, and avoiding\noverindulgence in alcohol or smoking.\n4. Taking time out to walk in the nature.\n5. Sometimes spoiling yourself, for example, having a coffee with a friend, or\nrelaxing in a hot bath.\n6. Not making any major decisions before at least one year has passed.\n7. Continuing existing relationships, seeking support.\n8. Getting some advice on dealing with practical issues, or dealing with special occasions such as Christmas or the anniversary of the loss.\n9. Understanding that your ability to think and remember is reduced – don’t be too hard on yourself, and use reminder lists.\n10. Considering whether you need some time off work or to negotiate reduced working hours (Clark, 1995).\nSometimes thoughts can be intrusive in grief. Sometimes you need to challenge them, but at other times you may not want to deal with them at the time. Work on imagining putting the thought aside, perhaps into a box on a shelf, or letting them float away down a stream. You can then come back to the thoughts later, such as in your grief time. If possible, focus on finding some meaning out of the loss and grief. Narrative therapy speaks of ‘saying hullo again’ to the deceased, rather than saying goodbye (White and Denborough, 1998). This refers to incorporating what has been lost into the present, for example, holding on to the influence (or some other aspect) of that person that is meaningful. Robert Neimeyer has done some research and writing on ‘continuing bonds’ with loved ones who have died. This involves continuing the attachment with the person who has died in some ways.\nWhen there is trauma as well as loss, it is important to remember that the effects, even though they can be very distressing, are normal, and they can last for days or weeks. Here are some tips to help coping:\n1. Recognise that you have been through a very distressing experience, and focus on feeling safe and secure in the first instance.\n2. Avoid overusing alcohol or other drugs to cope.\n3. Don’t bottle up your feelings – share them with people close to you if you are able to or write them down. If you are a person who expresses yourself through action (e.g. gardening), then do these things.\n4. Let key family and friends know about what you need, whether practical or emotional support.\n5. Maintain a normal routine, and do some things you usually enjoy.\n6. Rest when you can, and use relaxation techniques (e.g. relaxing each muscle group in the body, breathing effectively – you can download phone apps or borrow CDs from a library).\n7. Eat well and do some exercise.\n8. Problem-solve what you need to be doing – seek help with this from family and friends.\n9. Be aware that a recent trauma may stir up memories from past traumas. Focus on keeping these memories separate in your mind – you can only process so much at one time.\n10. Seek professional help if need be.\nFurther information is available at www.grieflink.org.au or if you wish to speak with someone about your grief, you can contact Lifeline on 131114 or Beyondblue on 1300224636. You may choose to seek assistance from your GP or a grief counsellor.You can find out more information on PTSD on the Beyondblue website (www.beyondblue.org.au), or The Australian Centre for Posttraumatic Mental health (www.acpmh.unimelb.edu.au). Again, please seek help from your GP.\n• Bowlby J. Attachment and Loss. Vol. 3: loss, sadness and depression. London, UK: Hogarth Press; 1980.\n• Clark S. After Suicide: help for the bereaved. Melbourne, Vic: Hill of Content; 1995.\n• Corr C. Enhancing the concept of disenfranchised grief. Omega: J Death Dying. 1998; 38: 1–20.\n• Howell, C. (2010). Keeping the blues away: The ten step guide to reducing the relapse of depression. Radcliffe, Oxford.\n• Neimeyer, R. A., Baldwin, S. & Gillies, J. (2006). Continuing bonds and reconstructing Meaning: Mitigating complications in bereavement. Death Studies, 715-738.\n• Parkes CM. Bereavement as a psychosocial transition: processes of adaptation to change. J Soc Issues. 1988; 44(3): 53–65.\n• Parkes CM. Coping with loss. BMJ. 1998; 316: 1521–4.\n• White C, Denborough D. Introducing Narrative Therapy: a collection of practice-based writings. Adelaide, SA: Dulwich Centre Publications; 1998.\n• Worden J. Grief Counselling and Grief Therapy: a handbook for the mental health practitioner. 4th ed. New York, NY: Springer Publicati']	['<urn:uuid:d371c023-6504-4b47-b2bb-487c46e53aae>', '<urn:uuid:5b8321a5-66c1-4092-af0d-291361a84df5>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	5	56	1628
83	How do raster and vector systems handle map transformations and what challenges exist in map creation?	In raster systems, cell sizes and extents remain fixed during transformations, though feature shapes may change when combining layers (e.g., multiplying slope and soil erodability maps). Vector systems allow feature geometry to change when combining layers, producing new shapes different from input layers. However, map creation faces several challenges - data access is often limited to partial areas or inconsistent sampling methodologies, generating and updating maps requires costly specialized technology and expertise, and keeping up with rapidly advancing technology demands frequent training updates. These limitations particularly affect developing countries, resulting in data gaps and inaccuracies.	"['More Discussion on Map Algebra (including\n- that Map Algebra allows one to use GIS data layers as variables in algebraic/mathematical expressions\n- why there is a need for transformation of data layers to provide proper input to a model (e.g., by scale or range-of-data-values conversion) or for applying a model to analyze data.\n-that combination of data layers is a powerful tool for derivation of new data layers.\n- the difference between applying a scalar, a function, or another data\nlayer to a data layer to create a new output layer.\n-how to perform each of the three types of data layer transformation: data layer with scalar, data layer with function, and data layer with data layer.\n-how a mathematical equation can be implemented using a sequence of\nMap Algebra steps to derive an analytical result.\n- working back from a desired outcome or given equation to determine\nthe sequence of data layer manipulation steps required to derive a desired\ndata layer result.\n(S * C * P)/160 = R\n* indicates multiplication, / indicates division,\nS = the surface slope, categorized into values of 1 (0 to 3 degrees), 2 (3 to 6 degrees), 3 (6 to 9 degrees), or 4 (greater than 9 degrees),\nC = ground cover coefficient, a value of 10 for dense, broad leaf cover, 20 for grass or mixed coniferous forest, 30 for sparse canopy forest, and 40 for bare ground,\nP = Precipitation in millimeters, and\nR = Runoff volume of water, in liters per square meter\nWe start with the following data layers, each has 201 rows by 201 columns; each cell covers a 20-meter by 20-meter portion of the study area:\nCategorical slope image, containing values from 1 to 4\nGround Cover for our study area, containing values of 10, 20, 30, and 40:\nPrecipitation for our study area for a particular rainfall event, values range from 12 mm to 22 mm:\nIn order to perform our analysis, we first overlay-multiply our slope category image times our ground cover category image. This overlay-multiplication step creates a new image in which each cell contains an attribute value equal to the multiplication product of the two corresponding cells in the input images:\nWe take this image, which resulted from the first overlay step and overlay-multiply by our precipitation image to produce this image:\nNext we perform a scalar operation. We divide this image by 160 to produce this result:\nFor each cell in our raster data layer, we have estimated the amount of runoff in liters per square meter. Each cell covers an area of 400 square meters. We can transform this data layer from representing liters per square meter to liters per cell by performing a simple scalar multiplication of this data layer times 400.\nFor example, if a particular cell in our resulting image above has an attribute value of 15.2 liters per square meter, then the total runoff for that cell is:\n15.2 liters per square meter X 400 square meters = 6080 liters.\nAn image is not provided to show the result of the scalar multiplication of the raster result above (Runoff in liters per square meter) times 400 because the image looks identical. The only difference is that the attribute value in each cell in the new image (Runoff in liters per cell) is 400 times larger than the value in the image above.\nIf we wish to compute the total runoff in our study area, we need to\nsum the attribute values in all of the cells in our raster (Runoff in liters\nper cell). This is a fundamental type of database query operation, and\nthe capacity to perform this analysis is likely to be found in any robust\nGIS. In Idrisi, an easy way to sum up all of the attribute values in an\nimage is to create a ""blank"" raster - an image containing one value, zero,\nfor instance. This blank raster has the same number of rows and columns,\nspatial extent, and reference system as the rasters we have been working\nwith. Next we use the EXTRACT module specifying that the ""blank"" raster\nis our Feature Definition Image, and the image we wish to sum is our Image\nTo Be Processed. Specify the SUM option of EXTRACT to add up the attribute\nvalues for all of the cells in the raster. The EXTRACT module sums (if\nwe choose the SUM option) the values in all of the cells which underlie\neach value category in the Feature Definition Image when it is placed over\nthe Image To Be Processed. In this case we have only one value category\n(zero) throughout our blank raster, the Feature Definition Image. Therefore\nthe attribute values for the entire image are summed to produce a value\nof 81,378,400 liters for the entire study area for this precipitation event.\n2. Unit 22: Merging Tabular Data with Spatial Data\n3. Unit 34: Types of Overlay Operators\n4. Unit 41: Using Boolean Search Techniques\n2. Unit 28: Editing Raster Data\n3. Unit 43: Using Derivative Surface Operators\n4. Unit 44: Using Hydrologic Models\n2. Student can explain the differences between mathematical application of a scalar, a function, and a data layer.\n3. Student can describe why Map Algebra operations are performed on the actual data values in a raster or in a database for a vector GIS.\n4. If software will be used, students can describe the software steps used to conduct Map Algebra using a scalar, a data layer and a function.\n5. Student can give an example of each of the three types of operation.\n6. Student understands the special case when two data layers are multiplied\nand the second contains only zeroes and ones. This has the effect of ""masking\nout"", i.e., converting to zero, the areas in the first data layer which\ncorrespond to zero areas in the second, while leaving attributes in the\nareas corresponding to ones unchanged.\n- The spatial extent of the data layers is not altered in Map Algebra.\n- In a vector system, in which attributes are stored in a database, Map Algebra using a scalar or a function involves transformation of the attribute values in the database. These are commonly written to a new field in the database.\n- In a vector system, Map Algebra in which a data layer is combined with another data layer can produce a new data layer in which derived features may have different sizes and or shapes than in either input layer. For example, a data layer in which polygons represent areas suitable for development, with attribute values ranging from 0 (unsuitable) to 1 (most suitable) could be multiplied by a data layer in which preference for development was also depicted as polygons with attribute values between 0 and 1. By multiplying these two data layers together, we can produce some composite probability of development image. It is important to note that we might find that our output polygons differed in size and shape from the polygons in either of our input images (Figure 5).\n- In a raster system, sizes and extents of individual cells are not altered by application of a scalar, a function, or another data layer, but the sizes and shapes of features (collections of cells, generally with the same single value) depicted in the result may differ from the sizes and shapes of features in the input. For example, one could have a raster image in which each pixel value is the average slope for that pixel, and one could have another image where the attribute value of each pixel is the soil erodability of that pixel. If the two images are multiplied together one can derive a simple erosion risk map. In this resulting image, the features, i.e., the areas of high erosion risk, intermediate levels of risk, relatively lower risk, or whatever way the output values are categorized, will have shapes and sizes which differ from features (areas of constant erodability or areas of constant slope) in the input images.\n- If software is being used, the student should know what Map Algebra\noperations are available, and how they can be invoked.\n2. Student will demonstrate the capability to perform a sequence of Map Algebra steps to conduct an analysis described by a mathematical equation.\n3. Students will be able to explain why combination of data layers is\nlikely to produce a resulting data layer in which the sizes and shapes\nof features differ from the input layers, whereas when a scalar or function\nis applied to a data layer, the resulting features have the same geometry\nas the input layer.\nIn a raster system, multiply the elevation values in feet in the input layer by 0.3048 to produce a new data layer containing elevation values in meters (or transform from meters to feet by multiplying by 3.28).\nIn a vector system, transform the line attributes for an elevation contour layer, by multiplying by 0.3048 to transform the line attributes from elevation units of feet to units of meters.\n2. Apply a simple mathematical function to a data layer, e.g., square the attribute values in a raster or vector.\n3. Combine two data layers to produce a result:\nRaster: multiply urbanization factor map by travel time per cell\nVector: multiply property values by appreciation coefficient map; public properties have one value of appreciation, private properties have a different value.\n4. Perform a sequence of Map Algebra steps to interpret an equation,\ne.g., raster: estimate precipitation given a temperature map (degrees f)\nand an elevation map (m) using an equation such as precip(mm) = (ln(temp(deg\nK)) * (elev(m)/250))\n2. Student will understand how layer multiplication by second layer containing only attributes of zero and one can be used to discriminate features of interest or to mask features to be omitted from further analysis.\n3. If vector software is used, student will understand how combination\nof two data layers containing features whose geometry differs between the\ntwo layers can produce a new data layer whose features differ in shape\nfrom features contained in both input layers.\nRohatgi, Janardan S., Vaughn Nelson, 1994, Wind Characteristics: An\nAnalysis for the Generation of Wind Power, Alternative Energy Institute,\nWest Texas A & M University, Canyon, Texas\n2. Unit 22: Merging Tabular Data with Spatial Data\n3. Unit 31: Managing Database Files\n4. Unit 33: Using Buffering Operators\n5. Unit 34: Using Overlay Operators; Pre/Post-Overlay Tasks', 'Maps and spatial information technologies (Geographical Information Systems) in health and environment decision-making\nA map is primarily a means of display; it cannot predict the patterns of distribution or relationships between resources. In Figure 1, for example, the map does not infer a causal relationship, it merely points out that there are some spatial coincidences that are worth exploring, to see if a causal relationship exists. Likewise, to show how changes in one resource may impact distribution of another resource, the relationship must be known and put into the model creating the map.\nMost maps, unless they use data that are recorded in real time or near-real time (see Figure 5), represent only a snapshot of the situation at a particular time in history. As the environment is extremely dynamic, and conditions (many of which impact on health) are constantly changing, maps can quickly become outdated. Updating maps can be expensive in terms of the time required to make the maps and the cost of the information inputs required. In cases where current information is required (for example, monitoring of pollution dispersal from accidents or leaks), obtaining spatial information may be difficult or costly.\nThe Centre for International Earth Science Information Network (CIESIN) has used satellite-derived data to compile an Ultraviolet Index (UVI) and animated the maps to show the year to date. This can be used to issue health warnings to residents and travellers on a daily basis.\nAnother limitation of spatially-referenced environmental information is that access is often limited. For example, data may be available for only a portion of the required area, or for the whole area but taken from two or more different sampling exercises which may have used different sampling methodologies, scales, or accuracy levels.\nConnected to this are costs involved in generating maps, and in printing, disseminating, and updating them. This requires specialized hardware and software, trained personnel, and often expensive and time-consuming means of acquiring, checking, interpreting, and inputting information.\nFurthermore, the technology is rapidly advancing, and thus new applications and training courses are required on an almost annual basis. It is difficult for any non-commercial enterprise to keep up with these costs, but those in developing countries are most affected. The obvious result is that there are frequently data gaps or inaccuracies.\nFinally, not all people can readily relate to information in a two-dimensional spatial format, especially if the map is of an unfamiliar area or is presented in an unusual projection. Furthermore, different cultures place different importance or meaning on symbols and colours. For example, western cultures may use the colour red to symbolize danger or an area where conditions are bad, but in China this colour would symbolize luck or a favourable area.\nIn summary, maps are useful communication tools if they have been developed according to the best available information (about both the environmental conditions and the targeted users). However, they should be used together with – not in lieu of – other information, such as documents, discussions, and models.']"	['<urn:uuid:fb9ac420-2f94-467d-9b32-25159892e13e>', '<urn:uuid:88c436f7-8ecb-433c-bb85-d88ab3152d1c>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	16	95	2242
84	What's the rough cost to put epoxy on a floor?	Epoxy flooring can cost between $2.50 to $8 per square foot, with prices varying based on project size. For a 400-square-foot basement, costs typically range from $3,200 to $3,400. Larger projects of over 2,500 square feet can receive a 25% discount, while projects of 600, 900, and 1,200 square feet may qualify for a 10% discount.	['If you’re considering installing an epoxy floor in your kitchen or bathroom, consider some of the pros and cons of this type of flooring. Here’s a look at stain resistance, durability, color options, and cost. It’s best to speak with a professional before making the final decision. Once you’ve made the decision, you can start planning the installation process. Listed below are the main benefits of installing this type of floor.\nFor a great stain-resistance flooring solution, you should have the floor thoroughly prepared before the epoxy coating application begins. This preparation process involves the use of media blasting, diamond grinding, jackhammers, chipping guns, and other materials. In order to ensure good adhesion, de-greasing may be required before the epoxy can be applied properly. Installation procedures vary according to the type of system you choose, but they commonly include screeding, troweling, broadcasting, and grinding. All of these procedures are discussed before the installation process begins.\nEpoxy floor installation is a cost-effective alternative to traditional materials like carpets. Because of the seamless nature of the epoxy surface, it requires less ventilation than other types of flooring. Traditional materials release particulates into the air. Moreover, a quality epoxy provides a stain-resistant surface that’s both smooth and glossy. These qualities make it an excellent choice for high-traffic industrial areas. In addition to enhancing aesthetics, stain-resistant epoxy floors are perfect for areas where people are prone to spills and liquids.\nMoreover, epoxy floors are highly durable and scratch-resistant. These floors can hold heavy items without losing their luster. Aside from that, epoxy flooring is also chemical and shock-resistant. It won’t absorb chemicals like antifreeze and acid, and it won’t spread electricity. Despite the many benefits of epoxy floors, it is important to note that regular maintenance and cleaning are still necessary to keep them looking new.\nCompared to other types of flooring, epoxy floors are easy to maintain. They can withstand heavy items being dropped on them. However, it’s important to keep epoxy floors free of dust and debris, and it’s best to keep them spotless for a few weeks. You can deep clean if the floor is heavily stained or soiled. This is particularly important in areas where heavy objects are dropped. This ensures that your floors will be clean for years to come.\nWhen you install an epoxy coating, you can easily connect your walls and floors. It will create a seamless transition between the two. You’ll also get a more hygienic environment in these areas. And if you have a cafeteria or a gym, you can install a seamless wall system around the facility. If you have a gym or a spa, you can choose epoxy coatings for the walls.\nAn epoxy floor installation is among the toughest commercial and industrial floors. Only pure concrete can compete with epoxy. Untreated concrete is porous and invites microbial infestation. An epoxy floor coating is also highly durable and long-lasting, lasting as much as 30 years. Over time, this means that an epoxy coating is a more cost-effective solution. And while epoxy may initially seem to be expensive, the long-term savings are worth it.\nThe durability of an epoxy floor installation depends on how much traffic and to use it receives. In general, a daily sweep is sufficient, but a deeper cleaning should be performed at least once a week. A floor deteriorates if it develops cracks or localized breakdown of the coating. Eventually, the floor will need a new coat of epoxy to prevent damage from occurring. For this reason, it is advisable to wear stable shoes when walking on a concrete floor.\nAfter epoxy flooring is installed, it takes several days for the surface to fully harden. The curing process should be undertaken in conditions that will not cause excessive moisture, as this will result in a stronger finish. The curing process will take up to 30 days. However, it is important to note that the drying time is critical as the epoxy will wear down and scratch easily if it is exposed to high humidity levels. If a floor is not dry enough for 24 hours, the floor can be damaged by moisture trapped beneath the floor finish.\nThe preparation work associated with epoxy floor installation is extensive. The concrete slab must be thoroughly cleaned, with oil, grease, and solvents removed from it. Properly applied epoxy will allow for the desired bond between the concrete slab and the sealant. It may take several attempts to clean the floor, but the process can be done relatively easily with hand tools. If any imperfections are present, the contractor can repair them. So, hiring a professional to install an epoxy floor is important; check out the Precision Epoxy Fort Myers listing.\nAlthough the longevity of an epoxy floor installation is generally uncompromising, daily traffic can cause it to chip or crack over time. A floor that receives daily traffic is likely to require reapplication in a few years if it is heavily used. If it does crack and requires reapplication, it may be time to look for another solution. Fortunately, Colorado has some of the best quality epoxy coatings available. We specialize in basement, garage, and commercial floor coatings.\nIf you want to create the ideal environment, choosing the right epoxy floor color is critical. Choose a color that blends in with the environment. If you are a woodworker, select a color that resembles sawdust to hide any dust that may accumulate on the floor. This way, your garage will appear cleaner and less cluttered. Light-colored soil should be white, or a light color that matches it.\nMetallic-colored concrete sealers reflect light, giving off blinding rays. Light colors make the space more prominent and reduce the need for additional lighting. Depending on the type of color, you can select a variety of different colors for your epoxy floor. However, you should know that darker colors can show dirt and require more frequent cleaning. However, light colors are easier to maintain. Metallic pearl effect epoxy is an excellent choice for creating dynamic colors.\nIn addition to industrial applications, epoxy paint is a great choice for garages and basement floors. Basements can get quite humid, so epoxy paint is a sensible choice instead of carpet. In addition to a beautiful floor, a washable area rug will also provide warmth, softness, and sound dampening. If you’re considering remodeling your basement, choosing a color-rich epoxy floor is an excellent way to begin. You can even choose to make the space radon-proof!\nDecorative color flakes are another option for enhancing your space. These are available in an endless number of sizes and textures, and you can customize the design and color scheme to enhance the environment. Flake color flakes are usually installed in conjunction with resinous flooring systems, so you can achieve the optimal appearance. Furthermore, they will blend in with the surrounding decor. With so many colors available, you’ll have no trouble finding the right match for your space.\nPolycuramine is another option. This is a material that is twenty times stronger than epoxy and dries much faster. It can be driven on within 24 hours. Another option is to paint over the old epoxy. A polyamine coating can be used as a top coat over a different color. If you don’t have time to hire an epoxy coating company, you can also opt for a DIY version.\nThe cost of epoxy floor installation can vary widely, depending on the area of the floor you want to be covered. For a 400-square-foot basement, for example, you may spend anywhere from $3,200 to $3,400. A discount of up to 10% will be given for projects of 600, 900, and 1,200 square feet. For large projects of over 2,500 square feet, you will receive a 25% discount. You can use the following tips to minimize the cost of epoxy flooring installation.\nBefore determining the cost, it’s essential to consider the four main variables. By doing this, you will be better prepared to answer the contractor’s questions and understand the range of prices for different Epoxy Flooring options. By preparing yourself, you’ll know exactly what you should expect from your new floor. It’s also important to understand the process of installation and the different materials used. When choosing a flooring system, you should consider the type of floor you’ll have in mind – basic, medium, and superior – and your budget.\nThe types of epoxy floor installation available vary in cost and quality. If you’re looking for a low-cost option, consider choosing a metallic epoxy floor system. It’s a more affordable option and can deliver outstanding performance for many years. It’s also important to check the Better Business Bureau for any complaints. DIY projects can also be a good idea, and you can often purchase do-it-yourself epoxy floor kits at paint stores.\nAmong the least expensive flooring options, an epoxy floor can cost as little as $2.50 per square foot, and the price can go as high as $8 per square foot for a smaller job. However, it’s worth considering that epoxy flooring is among the best all-around value for the money when it comes to resinous flooring. However, it is important to note that epoxy is durable and resistant to stains, but it’s not impervious to objects falling on it. If anything does happen to your epoxy floor, you can simply lay another layer of epoxy to fix the damage.\nThe cost of epoxy floor installation is directly proportional to the level of preparation work required by the concrete floor. The surface must be completely smooth and clean before the application of epoxy. In Portland, the prep work will take a day. If the concrete is in poor condition, the job will require more time and materials to ensure the best possible result. The installation process can last between eight and ten years, but proper care is essential to avoid costly repair costs.']	['<urn:uuid:57680291-20c4-43ab-ae13-9d0104fd7c53>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	10	56	1653
85	difference pulse compression grating raman amplifier gain fiber length	Raman amplifiers require a longer gain fiber compared to pulse compression gratings which are compact devices. While pulse compression gratings are thin optical elements (typically 6-20mm thick), Raman amplifiers need a longer gain fiber to achieve amplification.	"['An optical amplifier is a device that amplifies an optical signal directly, without the need to first The most common example is the Erbium Doped Fiber Amplifier ( EDFA), where the core of a silica The amplification window of an optical amplifier is the range of optical wavelengths for which the amplifier yields a usable gain. My sincerest thanks also to all the members of Centre d’Optique, Photonique et . constmction of an EDFA and its amplification principles in sections and Amélioration de la dynamique de stabilisation des EDFA grâce à l’insertion d’un amplificateur optique à semiconducteur. Conference Paper · January with.\n|Published (Last):||23 April 2010|\n|PDF File Size:||4.55 Mb|\n|ePub File Size:||14.14 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nModule d’amplificateur haute puissance. Such amplifiers are commonly used to produce high power laser systems. First, Raman gain exists in every fiber, which provides a cost-effective means of upgrading from the terminal ends. Parametric amplifiers use parametric amplification. Fibres de Plastique Doubles. Different sites expose ions to different local electric fields, which shifts the energy levels via the Stark effect.\nAchromat Dispersion Gradient-index optics Hydrogen darkening Optical amplifier Optical fiber Optical lens design Photochromic lens Photosensitive glass Refraction Transparent materials.\nPlaques Murales en Fibre Optique. Doped fiber amplifiers DFAs are optical amplifiers that use a doped optical fiber as a gain medium to amplify an optical signal. Becker, High-gain erbium-doped traveling-wave fiber amplifier,” Optics Letters, vol.\nManchons de Protection pour Connecteur RJ A relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler WSC. This effect is optiqus as gain saturation — as the signal level increases, the amplifier saturates and cannot produce any amplifictaeur output power, and amplficateur the gain ampliflcateur.\nIn addition to boosting the total signal gain, the use of the resonant cavity structure results in a very narrow gain bandwidth; coupled with the large FSR of the optical cavity, this effectively limits operation of the VCSOA to single-channel amplification.\nA significant point is that the erbium gives up its energy in the form of additional photons which are exactly in the same phase and direction as the signal being amplified. Lumped amplifiers, where the pump light can be safely contained to avoid safety implications of high optical powers, may use over 1 W of optical power.\nSecond, Raman amplifiers require a longer gain fiber. Since this creates a loss of power from the cavity which is greater than the gain, it prevents the amplifier from acting as a laser. Systems meeting these specifications, have steadily progressed in the last few years from a few Watts of output power, initially to the 10s of Watts and now into the s of Watts power level.\nFrom Wikipedia, the free encyclopedia. The erbium doped amplifier is a high gain amplifier. Another advantage of operating the DFA optlque the gain saturation region is that small fluctuations in the input signal power are reduced in the output amplified signal: Optics and Photonics Letters. A typical DFA has several tens of meters, long enough to already show this randomness of the birefringence axes.\nCapteur de fibre optique. Titulaire et D’installations de Polissage.\nHowever, Ytterbium doped fiber lasers and amplifiers, operating near 1 micrometre wavelength, have many applications in industrial processing of materials, as these devices can be made with extremely high output power tens of kilowatts. The absorption and emission cross sections optiquue the ions can be modeled ampluficateur ellipsoids with the major axes aligned at random in all directions in different glass sites.\nThe random distribution of the orientation of the ellipsoids in a glass produces a macroscopically isotropic medium, but a strong pump laser induces an anisotropic distribution by selectively exciting those ions that are more aligned with the optical field vector of the pump.\nPanneaux D’adaptateur de Fibre SC. Point est en stock. Adaptateur de Fibre nu. The amplification window is determined by the spectroscopic properties of the dopant ions, the glass structure of the optical fiber, and the wavelength and power of the pump laser.\nAmplicicateur, in optical fibers small amounts of birefringence are always present and, furthermore, the fast and slow axes vary randomly along the fiber length. This broadening is both homogeneous all ions exhibit the same broadened spectrum and inhomogeneous different ions in optiqeu glass locations exhibit different spectra.\nSuch reflections disrupt amplifier operation and in the extreme case can cause the amplifier to become a laser. Sonde de Fibre Portative.\nSa and different geometries disk, slab, rod to amplify optical signals. In semiconductor optical amplifiers SOAselectron – hole recombination occurs. These devices are similar in structure to, and share many features with, vertical-cavity surface-emitting lasers VCSELs. Finally, there are concerns of nonlinear penalty in the amplifier apmlificateur the WDM signal channels. Panneaux de Brassage Cat5e. They are related to fiber lasers. The amplification bandwidth of Raman amplifiers is defined by the pump wavelengths utilised and so amplification can be provided over wider, and different, regions than may be possible with other amplifier types which rely on dopants and device design to define the amplification ‘window’.\nThe broad gain-bandwidth of fiber amplifiers make them particularly useful in wavelength-division multiplexed communications systems as a single amplifier can be utilized to amplify all signals being carried on a fiber and whose wavelengths fall within the gain window.', 'Del Mar Photonics\nPulse Compression Gratings - Instructions for ordering -\nWhen ordering a grating, please use the following example format below, or choose from the standard PC grating stock list...\nPC 1200 W x H x Thk 800 nm (TM/-1) constant deviation 8°\n1. PC stands for pulse compression\n2. 1200 is the groove density (groove frequency) in grooves/mm\n3. W is the blank dimension in mm parallell with the grating grooves\n4. H is the blank dimension in mm perpendicular to the grating grooves\n5. Thk is the blank thickness in mm\n6. 800 nm is the desired optimized wavelength. A range or range with peaked wavelength can also be specified\n7. (TM/-1) is desired polarization state and diffraction order the grating should be optimized for. TE and average (TM+TE)/2 can also be specified\n8. Constant deviation 8° is the configuration the grating should be optimized for. Constant incidence angle can also be specified\nStandard tolerances on W, H, Dia: ± 0.2 mm Thk ± 0.5 mm\nCA > 90 % of blank size\nStandard Pulse Compression gratings Type PC, specified by combining from:\n25 x 25 x 6 mm\n30 x 30 x 6 mm\n30 x 64 x 10 mm\n30 x 75 x 16 mm\n30 x 110 x 16 mm\n50 x 50 x 10 mm\n50 x 110 x 16 mm\n58 x 58 x 10 mm\n64 x 64 x 10 mm\n90 x 90 x 16 mm\n110 x 110 x 16 mm\n120 x 140 x 20 mm\nNIR 750-1064 nm\n@ 790 nm\n@ 800 nm\n@ 920 nm\n@ 1032 nm\n@ 1053 nm\n@ 1064 nm\nTypical near Littrow\nOther specifications available on request.\nNote: The absolute efficiency curves shown are only representative for the stated geometry and wavelength(s) and can vary depending on use geometry and measurement technique *)\nPulse Compression Gratings\nThese gratings are especially suited for use in laser pulse compression experiments. High diffraction efficiency, in combination with good spectral quality and high damage resistance makes these gratings useful in all kinds of laser pulse applications; both pulse compression using optical fibre grating pairs, and for amplification of pulses with chirped pulse amplification.\nFibre grating compressor\nWhen a short laser pulse is transmitted through an optical fibre, the pulse will be stretched, or ""chirped"" due to nonlinear effects (selfphase modulation). The group velocity dispersion in the fibre results in that the front of the pulse will have a longer wavelength than the tail. By using a pair of gratings one can arrange so that the long wavelength pulse will travel a longer path than the short wavelength pulse, with the result that, after the grating pair, they arrive at the same time. The grating pair not only compensates for the pulse broadening in the fibre, but makes the pulse even shorter than the input; up to 90 times compression can be achieved.\nChirped pulse amplification\nVery short pulses (100 femtoseconds) can be produced by some types of mode locked lasers. For many applications, these pulses have too low peak power. The technique of chirped pulse amplification (CPA) can be used for amplifying such pulses, to achieve peak powers in the order of Terawatts.\nThe amplifier is basically a laser crystal inside a resonator. To avoid strong nonlinear effects which would destroy the crystal, the input pulse is stretched in time, so that the peak power is decreased. This chirped pulse is then amplified, and subsequently compressed to obtain a high power pulse with a duration nearly equal to the input pulse.\nStretching and compression with grating pairs\nBoth stretching and compressing utilize grating pairs, arranged in subtractive dispersive mode; so that the angular dispersion of the first grating is cancelled by the second grating. Two parallel beams of different wavelengths, incident on the first grating, are still parallel when they leave the second grating, but they have travelled different distances.\nA grating pair arranged parallel as in fig.A, will introduce a negative group velocity dispersion, i.e. pulses of long wavelength arrive later than short wave pulses.\nIn order to achieve a positive dispersive delay, a more complicated arrangement is necessary. Fig.B shows such an arrangement, normally used in the stretcher stage. An afocal lens system (telescope) is inserted between the gratings. The telescope reverses the sign of the angles so that the beams will hit the second grating at the same angle as they leave the first one.\nBoth stretcher and compressor are normally used in double pass. The advantages are twofold: the dispersion is doubled, and all wavelength components of the beam emerge colinear, not linearly translated as shown in the figure for single pass.\nExtremely low stray light\nThe gratings are holographically recorded with two highly collimated, clean and homogeneous beams, which give straight and equispaced grooves. The diffracted light is free from ""ghosts"" and give very low levels of randomly scattered light.\nHigh efficiency and low loss\nThe groove profile of the pulse compression gratings is optimized to give maximum efficiency for light polarized perpendicularly to the grooves (TM polarization).\nAbsolute efficiency of a pulse compresslon grating, 1800 gr/mm, gold coated, in Littrow configuration\nThe efficiency depends on the wavelength and configuration, but frequently an absolute efficiency of more than 90% for each grating is achieved. The figure shows the efficiency of a gold coated grating with 1800 grooves/mm, in a Littrow configuration.\nNot only the diffraction efficiency is important. In order to avoid thermal effects, the absorption in the grating surface should be as small as possible. Therefore, a coating material with high reflectance should be used. In the NIR region, gold coated gratings have the best performance.\nFlat diffracted wavefront\nThe combination of a flat grating surface and a holographic exposure setup of high optical quality, gives a flat diffracted wavefront. This gives the possibility to focus the laser pulse to high intensities.\nPulse compression grating for Yb-based femtosecond laser\nProduct news and updates - Training Workshops - Featured Customer - Other News\n| Trestles LH Ti:Sapphire\nTrestles LH is a new series of high quality femtosecond Ti:Sapphire lasers for applications in scientific research, biological imaging, life sciences and precision material processing. Trestles LH includes integrated sealed, turn-key, cost-effective, diode-pumped solid-state (DPSS). Trestles LH lasers offer the most attractive pricing on the market combined with excellent performance and reliability. DPSS LH is a state-of-the-art laser designed for today’s applications. It combines superb performance and tremendous value for today’s market and has numerous advantages over all other DPSS lasers suitable for Ti:Sapphire pumping. Trestles LH can be customized to fit customer requirements and budget.\nDPSS DMPLH lasers\n|New laser spectrometer\nT&D-scan for research that\ndemands high resolution and high spectral\ndensity in UV-VIS-NIR spectral domains - now available with\nnew pump option!\nThe T&D-scan includes a CW ultra-wide-tunable narrow-line laser, high-precision wavelength meter, an electronic control unit driven through USB interface as well as a software package. Novel advanced design of the fundamental laser component implements efficient intra-cavity frequency doubling as well as provides a state-of-the-art combined ultra-wide-tunable Ti:Sapphire & Dye laser capable of covering together a super-broad spectral range between 275 and 1100 nm. Wavelength selection components as well as the position of the non-linear crystal are precisely tuned by a closed-loop control system, which incorporates highly accurate wavelength meter.\nAOTF Infrared Spectrometer\nOpen Microchannel Plate Detector\nnow in stock!\n|Hummingbird EMCCD camera\nThe digital Hummingbird EMCCD camera combines high sensitivity, speed and high resolution.\nIt uses Texas Instruments\' 1MegaPixel Frame Transfer Impactron device which provides QE up to 65%.\nHummingbird comes with a standard CameraLink output.\nIt is the smallest and most rugged 1MP EMCCD camera in the world.\nIt is ideally suited for any low imaging application such as hyperspectral imaging, X-ray imaging, Astronomy and low light surveillance.\nIt is small, lightweight, low power and is therefore the ideal camera for OEM and integrators.\nfemtosecond transient absorption data acquisition system\nFuture nanostructures and biological nanosystems will take advantage not only of the small dimensions of the objects but of the specific way of interaction between nano-objects. The interactions of building blocks within these nanosystems will be studied and optimized on the femtosecond time scale - says Sergey Egorov, President and CEO of Del Mar Photonics, Inc. Thus we put a lot of our efforts and resources into the development of new Ultrafast Dynamics Tools such as our Femtosecond Transient Absorption Measurements system Hatteras. Whether you want to create a new photovoltaic system that will efficiently convert photon energy in charge separation, or build a molecular complex that will dump photon energy into local heat to kill cancer cells, or create a new fluorescent probe for FRET microscopy, understanding of internal dynamics on femtosecond time scale is utterly important and requires advanced measurement techniques.\nspot in our Ultrafast Dynamics Tools\ntraining workshop in San Diego, California.\nspot in our Ultrafast Dynamics Tools\ntraining workshop in San Diego, California.\nTerahertz systems, set ups and components New band pass and long pass THz optical filters based on porous silicon and metal mesh technologies. Band pass filters with center wavelengths from 30 THz into GHz range and transmissions up to 80% or better. Standard designs with clear aperture diameters from 12.5 to 37.5 mm. Long pass filters with standard rejection edge wavelengths from 60 THz into GHz range. Maximum transmission up to 80% or better, standard designs at 19.0 and 25.4 mm diameters. Excellent thermal (from cryogenic to 600 K) and mechanical properties THz products: THz Spectrometer kit with Antenna THz transmission setup THz time domain spectrometer Pacifica fs1060pca THz time domain spectrometer Pacifica fs780pca THz detectors: Golay cell and LiTaO3 piroelectric detectors PCA - Photoconductive Antenna as THz photomixer Pacifica THz Time Domain Spectrometer - Trestles Pacifica Holographic Fourier Transform Spectrometer for THz Region Wedge TiSapphire Multipass Amplifier System - THz pulses generation Terahertz Spectroscopic Radar Mobile System for Detection of Concealed Explosives Band pass filters with center wavelengths from 30 THz into GHz range Long pass filters with standard rejection edge wavelengths from 60 THz into GHz range Generation of THz radiation using lithium niobate Terahertz crystals (THz): ZnTe, GaP, LiNbO3 - Wedge ZnTe\niPCA - interdigital Photoconductive Antenna for terahertz waves\nLarge area broadband antenna with lens array and high emitter conversion efficiency\niPCA with LT-GaAs absorber, microlens array for laser excitation wavelengths l £ 850 nm, adjusted hyperhemispherical silicon lens with a high power conversion efficiency of 0.2 mW THz power / W optical power. The iPCA can be used also as large area THz detector. The two types iPCAp and iPCAs have the same active interdigital antenna area but different contact pad directions with respect to the electrical THz field.\nInterdigital Photoconductive Antenna for terahertz waves generation using femtosecond Ti:Sapphire laser\n|IntraStage lowers the cost\nof test data management!\nStruggling with gigabytes or terabytes of test data?\nIntraStage easily transforms test data from disparate sources into web-based quality metrics and engineering intelligence you can use.\nus today to discuss your test management requirements and specifications of your\n|Come to San Diego next summer! Attend one of our training workshops in San Diego, California\nduring summer 2011\nDel Mar Photonics has presented training workshops for customers and potential customers in the past 3 years.\nOur workshops cover scientific basics, technical details and provide generous time for hands-on training.\nEach workshop is a three-day seminar conducted by professional lecturer from 10am to 4pm. It includes lunch, as well as a training materials. We have also reserved two days for Q&A sessions, one-on-one system integration discussions, social networking, and San Diego sightseeing.\nThe following training workshops will be offered during this\nMar Photonics offers new\nTrestles fs/CW laser system which can be easily\nswitched from femtosecond mode to CW and back. Having both modes of operation in one system dramatically increase a\nnumber of applications that the laser can be used for, and makes it an ideal\ntool for scientific lab involved in multiple research projects.\nFrequency-stabilized CW single-frequency ring Dye laser DYE-SF-007 pumped by DPSS DMPLH laser installed in the brand new group of Dr. Dajun Wang at the The Chinese University of Hong Kong.\nDYE-SF-077 features exceptionally narrow generation line width, which amounts to less than 100 kHz. DYE-SF-077 sets new standard for generation line width of commercial lasers. Prior to this model, the narrowest line-width of commercial dye lasers was as broad as 500 kHz - 1 MHz. It is necessary to note that the 100-kHz line-width is achieved in DYE-SF-077 without the use of an acousto-optical modulator, which, as a rule, complicates the design and introduces additional losses. A specially designed ultra-fast PZT is used for efficient suppression of radiation frequency fluctuations in a broad frequency range. DYE-SF-077 will be used in resaerch of Ultracold polar molecules, Bose-Einstein condensate and quantum degenerate Fermi gas and High resolution spectroscopy\nOptical Society of Southern California meeting at UCSD OSSC 2011-04-27\nNd:YAG laser ordered by the University of Leon, UANL, Mexico\nWedge 50 Multipass Amplifier pumped with a Darwin-527-30-M DPSS Laser ordered by Hong Kong customer\nNew Trestles LH10-fs/CW femtosecond+CW laser ready for delivery to the University of California Santa Cruz\nTrestles femtosecond Ti:Sapphire laser delivered to North Carolina State University\nDel Mar Photonics sponsor IONS (International OSA Network of Students) conference IONS-NA-2 in Tucson, Arizona IONS-NA-2 website\nBest talk and best poster awards at IONS-Moscow 2010 conference sponsored by Del Mar Photonics\nWatch Del Mar Photonics videos!\nDel Mar Photonics is now on Twitter!\nDel Mar Photonics featured components\nDel Mar Photonics continuously expands its components portfolio.\nPrisms for Concentrating Photovoltaic Systems (CPV)\nSolar cells made of compound semiconductors such as gallium arsenide are very expensive. Usually very small cells are installed and various means such as mirrors, lenses, prisms, etc..are used to concentrate sunlight on the cells. Concentration photovoltaic technology (CPV) uses the solar radiation with an efficiency of 40%, double that of conventional solar cells\nDel Mar Photonics design custom Concentrating Photovoltaic Systems (CPV) and supply variety of the optical components for CPV such as solar prisms shown in the picture.\nAxicon lens also known as conical lens or rotationally symmetric prism is widely used in different scientific research and application. Axicon can be used to convert a parallel laser beam into a ring, to create a non diffractive Bessel beam or to focus a parallel beam into long focus depth.\nDel Mar Photonics supplies axicons with cone angles range from 130° to 179.5° for use with virtually any laser radiation. We manufacture and supply axicons made from BK7 glass, fused silica and other materials.\ndownload brochure - request a quote\nRutile (TiO2) coupling\nDel Mar Photonics offers optical elements made of high quality synthetically grown Rutile Titanium Dioxide crystals. Rutile’s strong birefringency, wide transmission range and good mechanical properties make it suitable for fabrication of polarizing cubes, prisms and optical isolators. Boules having high optical transmission and homogeneity are grown by proprietary method. Typical boules have 10 - 15 mm in dia. and up to 25 mm length. Optical elements sizes - from 2 x 2 x 1 mm to 12.7 x 12.7 x 12.7 mm. Laser grade polish quality is available for finished elements. So far we the largest elements that we manufactured are 12 x15 x 5 mm, in which optical axis is parallel to 15 mm edge, 5 mm is along beam path, 12 x 15 mm faces polished 20/10 S/D, one wave flatness, parallelism < 3 arc.min. (better specs. available on request).\nmore details - download brochure - request a quote\nDel Mar Photonics offer a range of competitively priced UHV viewports , Conflat, ISO or KF including a variety of coatings to enhance performance. Del Mar Photonics viewports are manufactured using advanced techniques for control of special and critical processes, including 100 percent helium leak testing and x-ray measurements for metallization control. Windows Materials include: Fused silica, Quartz , Sapphire , MgF2, BaF2, CaF2, ZnSe, ZnS, Ge, Si, Pyrex. Standard Viewing diameters from .55"" to 1.94 "".\nCoating - a range of custom coatings can applied - which include\n- Single QWOT\n- Broad Band AR\n- V coatings\n- DLC (Diamond like coating)\nmore details - request a quote\nThyratrons are used in\nsuch devices as radars with different power levels, high-power pulsed\ntechnical, electrophysical, medical devices and lasers. Sophisticated\ndesign and high quality ceramic-metal envelope determines long lifetime\nand very accurate and reliable operation of hydrogen thyratrons under wide range of environmental\n- pulsed lasers power supplies\n- medical apparatus\n- electrophysical instrumentation\nTriggered Three-Electrode Spark Gap Switches are ceramic-metal sealed off gas discharge trigatron-type devices with a co-axial trigger electrode. These Gas Discharge Tubes contain no mercury and, due to an advanced design, feature high reliability and a long lifetime being operating under wide range of environmental conditions.\n- pulsed installation for processing materials\n- installations with plasma focus\n- pulse power supplies for lasers and other pulse equipment\n- medical apparatus such as lithotriptors and defibrillators\n- processing systems for petroleum wells\n|We are looking forward to hear from you and help you with your optical and crystal components requirements. Need time to think about it? Drop us a line and we\'ll send you beautiful Del Mar Photonics mug (or two) so you can have a tea party with your colleagues and discuss your potential needs.|\nDel Mar Photonics, Inc.\n4119 Twilight Ridge\nSan Diego, CA 92130\ntel: (858) 876-3133\nfax: (858) 630-2376']"	['<urn:uuid:bbe0202c-c8b9-448d-8c6b-958b75228cc9>', '<urn:uuid:e929712d-21b6-45ef-9fd2-52f5c6c01726>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T00:57:18.859842	9	37	3792
86	enduring power of attorney property vs personal care differences requirements and limitations	There are key differences between the two types of enduring power of attorney (EPA). For property EPA, you can appoint multiple attorneys to manage your bank accounts, shares, businesses, and debts. It can take effect while you are mentally capable if you choose. For personal care and welfare EPA, you can only appoint one attorney who makes decisions about medical treatment and rest home care, and it only takes effect when you become mentally incapable. Personal care attorneys cannot make decisions about marriage, divorce, or refuse standard medical treatment. Both types require the attorney to be at least 20 years old, not bankrupt, and to act in your best interests at all times.	"['Property owners should get all their ducks in a row\nMake sure you have a Will and EPAs\nYou work hard to build up your assets over your lifetime, so you should decide what happens to them if you become incapacitated and when you die. If you own property, having a Will and Enduring Powers of Attorney (EPAs) will ensure all your affairs are in order and make things easier for your family and business colleagues.\nYou will all no doubt know the purpose of a Will. A Will records your wishes as to what happens with your property when you die, and who is to take care of it.\nWe strongly recommend reviewing your Will every five years, and when there are significant changes in a relationship, such as marriage, entering or ending a significant relationship.\nIf you don’t have a Will?\nThis is known as dying intestate, and means your property will be distributed in accordance with the Administration Act 1969. Your property is likely to go to your spouse/immediate family, but that may not be exactly what you or other family members wish. Dying intestate is also likely to increase the chances of a claim being made against your estate, and therefore incurring significant costs.\nWhat is an EPA?\nHaving an EPA allows someone (an attorney) to help look after your affairs, manage your property and make decisions about your welfare if you are unable to do this. Your attorney doesn’t have to be your lawyer; it could be a family member, a trusted advisor or a close personal friend. An EPA differs from a general power of attorney as it allows the attorney to act for you if you become too unwell to make decisions for yourself, or what’s called ‘mentally incapable’ in the professional opinion of a health practitioner.\nThere are two types of EPAs – a property EPA, and a personal care and welfare EPA.\nEPA in relation to property\nA property EPA gives the attorney the power to manage not only your property (as in real estate), but also your bank accounts, shares, businesses, debts and so on. You don’t necessarily have to give the attorney unlimited power; you can restrict your attorney’s ability to only deal with certain matters.\nYou also have the choice whether your property EPA takes effect while you are mentally capable (as well as mentally incapable). This can be handy if you need someone to deal with your affairs while you are out of the country, or you just no longer want the day-to-day hassle of paying bills, etc. You also have the right to appoint more than one property attorney.\nEPA in relation to personal care and welfare\nA personal care and welfare EPA gives your attorney the power to make decisions on matters such as ones concerning your medical treatment or whether you need to go into a rest home.\nUnlike the property EPA, the personal care and welfare EPA will only take effect when you are mentally incapable. As well, you can only appoint one attorney. You can, however, state that your attorney has the duty to consult with family members, but the final decision will rest with your attorney.\nWe recommend appointing a ‘successor attorney’ to act as a substitute in case your attorney is unable or unwilling to act for any reason.\nWho to ask?\nYour attorney/s must act in your best interests and protect your welfare. As you are giving considerable power to them, it pays to think carefully about who you appoint beforehand (as well as discussing it with them first). You can change attorneys at any time while you have mental capacity and appoint replacements.\nYour EPAs cease when you die, and those powers then vest in the executors named under your will.\nIf you don’t have EPAs?\nNot appointing attorneys before you are considered mentally incapable can be a costly and lengthy process. An application is made to the Family Court, but that doesn’t always mean the court will appoint who you would have wanted to look after your affairs.\nWe recommend everyone over the age of 18 should have a Will and both EPAs, so it’s never too early (but it can be too late) to get these organised. Having these ducks in a row will save your loved ones the hassle and the cost at what will be a difficult time.', ""What is a power of attorney?\nWhen you give someone (or a company) power of attorney, you give them the legal right to act on your behalf in relation to one or more aspects of your life e.g. your finances, property, or healthcare.\nThere are two types of power of attorney:\n1. Ordinary or general power of attorney: is where you give a person (or more than one person) the authority to act on your behalf in relation to either:\n(a) all of your affairs or\n(b) only a specific issue(s).\nIt can be for a fixed period of time or ongoing. If you lose your ability to make your own decisions (such as through illness or accident) then the ordinary power of attorney becomes invalid (is cancelled). An example of the kind of situation where you might give someone this kind of authority is if you were going overseas for a long time and wanted a trusted person to look after your finances until your return.\n2. Enduring power of attorney: is created under the Protection of Personal and Property Rights Act 1988, and continues on even after you lose legal capacity (the ability to understand the nature and consequences of decisions and/or the ability to communicate these decisions).\nFor example, someone with an illness that will eventually affect their mental capacity, arranges for a family member to have enduring power of attorney so that they can make decisions on the sick person's behalf.\nChoosing someone to give power of attorney to is a very important decision. Think carefully about who you want to choose as your attorney, as the role can be misunderstood or abused. The ideal attorney is someone you really trust, someone who will keep your best interests at heart and who cannot benefit financially from the decisions they may have to make.\nBack to top\nWhat are the two types of enduring power of attorney?\nThere are two types of enduring power of attorney:\n- Enduring power of attorney for personal care and welfare: usually a close friend or family member (there can only be one at a time and it has to be an individual – not a trustee corporation) who makes decisions about your care e.g. selecting a rest home or deciding on medical treatment.\nThey can’t make decisions about marriage or divorce, refuse standard or life-saving medical treatment, or consent to medical experimentation. This kind of enduring power of attorney comes into effect only when you lose your mental capacity.\n- Enduring power of attorney for property: you can pick one or more individuals or a trustee corporation to make decisions about how your property and finances should be managed. You can decide whether you want this to come into effect immediately or only when you lose your capacity.\nIt's possible to have one person who has enduring power of attorney for your personal care and welfare, and a different person who has enduring power of attorney for your property and finances.\nBack to top\nWhat should I look for when choosing to give someone power of attorney?\nWhether they are to have ordinary power of attorney or enduring power of attorney, your attorney’s responsibilities are:\n- To act in your best interests at all times and not abuse the trust you place in them\n- To involve you in the decision-making as much as they can - they have to consult you about decisions, and you should try to make decisions as much as you can\nTheir specific responsibilities depend on whether they’ll have ordinary power of attorney or enduring power of attorney (and with the latter, what type of enduring power of attorney). You can require your attorney to consult with people named in your agreement, and you can specify people you don’t want to look after you.\nWhen choosing an attorney, you should pick someone:\n- whom you trust and who will act in your best interests\n- at least 20 years old\n- who is not bankrupt or subject to a personal or property order (for an enduring power of attorney)\n- who understands their role as an attorney, and agrees to it\nFor enduring power of attorney, you may wish to choose different attorneys for personal care and for property as the two roles require different skills and criteria.\nBack to top\nHow is 'loss of mental capacity' defined?\nThe meaning of this term varies depending on whether it’s do with an Enduring Power of Attorney (EPA) and what kind of EPA it is.\nFor an EPA for personal care and welfare, this means that the person is unable to make decisions about their personal care and welfare or understand decisions; they are unable to understand the consequences of those decisions (or of not making those decisions) and aren’t able to communicate their decisions to others.\nFor an EPA for property, it means that the person is mentally incapable of managing their property affairs.\nThe decision about whether a person is mentally incapable is made by a court or by a qualified health provider. Their mental capacity is assumed to be competent unless it is proved otherwise.\nThere is more information about this on the Community Law website.\nBack to top\nDo I have to go to a lawyer to set up a power of attorney?\nIf you are giving someone an ordinary power of attorney (see What is a power of attorney?), all you need to do is fill out the appropriate forms, have them signed by the attorney and yourself, and get both signatures witnessed (not necessarily by a lawyer). You can purchase an Ordinary power of attorney form from an online legal site or obtain one from a law office.\nFor an enduring power of attorney, you (the donor i.e. the person giving power of attorney) must receive legal advice from the person who will be your witness. The donor's witness can be:\n- a lawyer, or\n- a qualified legal executive, or\n- an authorised officer or employee of a trustee corporation.\nThe attorney’s signature can be witnessed by anyone except:\n- the donor and\n- the donor’s witness.\nThe person who witnesses the donor’s signature must also be independent from the person who witnesses the attorney’s signature. The signatures must be witnessed by different people. For example, your lawyer can witness your signature but not the attorney’s, unless two people are appointing each other to be their attorney (e.g. between husband and wife). Also, if the attorney is a trustee corporation then a representative of the corporation can witness the donor’s signature.\nYou can get power of attorney forms from:\nIt’s a good idea to talk to your lawyer before you set up your power of attorney because they can tell you about:\n- the information you need to fill out the form\n- whether you should restrict your attorney’s powers to particular issues or types of decisions\n- optional provisions such as;\n- requiring your attorney to consult with people you choose\n- requiring your attorney to provide information to people you choose\n- appointing a replacement attorney if your first pick can’t act for you anymore\nIt’s worth asking around to get an idea of how much you’ll be charged for the legal advice and for drawing up the power of attorney for you (if you don’t want to do this part yourself). Fees are likely to vary widely depending on the provider.\nYou can find out more from the Community Law Centre website or the New Zealand Law Society factsheet.\nBack to top\nWhen does a power of attorney begin and end?\nThis depends on whether it's an ordinary power of attorney or an enduring power of attorney.\nOrdinary power of attorney\nWhen you fill out a form to give someone an ordinary power of attorney, you can specify when the power of attorney begins and ends. For example, you can specify that it begins on the date you leave New Zealand and ends on the expected date of your return.\nAn ordinary power of attorney is revoked (cancelled) automatically if you (the donor i.e. the person who is giving someone power of attorney) lose the mental capacity to make decisions, or die.\nEnduring power of attorney\nAn enduring power of attorney for property can begin before or after you lose the mental capacity to make your own decisions (this is something you can specify when you set it up).\nAn enduring power of attorney for personal care and welfare can only begin when you lose the mental capacity to make your own decisions.\nThere are several ways for the EPA to end or be revoked:\n- It is revoked automatically when you, the donor (the person who has given someone power of attorney), dies.\n- The Family Court can revoke the appointment of an enduring power of attorney if it decides that the attorney is not acting in the donor’s (the person who gave power of attorney) best interests or failing to comply with their responsibilities. See the next question for more about this.\n- If the attorney becomes bankrupt, loses their mental capacity or dies, they will lose their role of enduring power of attorney. Another one may be appointed in their place.\n- The attorney may also decide they no longer wish to act as attorney, by giving notice of disclaimer.\nBack to top\nWhat can I do if I don’t agree with how the attorney is managing my mother’s financial affairs?\nYour mother can cancel or revoke the power of attorney at any time while she is mentally capable. Because different procedures are needed depending on whether she is changing, cancelling or replacing a power of attorney, it’s a good idea for her to talk to her lawyer to get some advice about how to go about it.\nIf the attorney isn’t carrying out their responsibilities properly, you can apply to the Family Court to step in.\nThe people who can make a complaint to the Family Court about an attorney’s decision include:\n- a relative\n- another of your mother’s attorneys\n- the manager of the hospital, rest home or residential care facility (if she is a patient of one)\n- a medical practitioner\n- a trustee corporation\n- a welfare guardian that has been appointed to you\n- an authorised person from an elder abuse and neglect prevention service, such as Age Concern\n- any other person, with the leave of the court\nThe Family Court is able to do a number of things in relation to a person's attorney if necessary, such as:\n- monitor an attorney's performance,\n- change the terms of the attorney's role,\n- give directions to an attorney to do certain things, or\n- cancel an attorney's appointment.\nBack to top\nWhat can I do if my EPA attorneys are not getting on?\nIf you have two attorneys – one for personal care and one for property – then sometimes they can have different opinions about what is best for you and your life. Either attorney can apply to the Family Court to ask for direction about what to do.\nHow can I help someone who is already losing their mental capacity and doesn’t have an EPA?\nIf someone you care about won’t or can’t give someone enduring power of attorney, you can still protect them by applying to the Family Court for:\n- a personal order. This is a court order which decides on a person’s mental capacity and, if the judge decides an order is necessary, requires that a particular action is taken to look after the person’s welfare. For example, a personal order can specify where the person should live or that they must receive medical advice or treatment.\n- the appointment of a property manager. This is someone to manage the person’s property e.g. pay bills, arrange for repairs. A property order will specify what rights the property manager will have.\nThere’s more about this on the Family Justice website.""]"	['<urn:uuid:35f95f31-60c0-456e-bc51-35efeef423fd>', '<urn:uuid:3c153628-954e-4f63-bd5f-b5c8e8f1c2ca>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:57:18.859842	12	113	2740
87	gold standard carbon credits redd which prevents deforestation	Both Gold Standard carbon credits and REDD+ focus on preventing deforestation, but they operate differently. Gold Standard carbon credits must include gender-sensitive design, environmental safeguards, and contribute to at least three SDGs, while REDD+ specifically focuses on reducing emissions from deforestation and forest degradation in developing countries through conservation and sustainable forest management.	['Environmental integrity + sustainable development\nHigh-impact Gold Standard assets -- from carbon credits to renewable energy certificates -- allow the power of markets to drive change, accelerating progress toward the Paris Agreement and the Sustainable Development Goals.\nGold Standard was founded to ensure carbon markets feature the highest levels of environmental integrity and contribute towards sustainable development. We continue to strengthen and scale efforts across a range of environmental markets, focused on making it simpler for high-impact climate protection projects to achieve certification and access markets in the context of the new Paris Agreement Framework.\nVoluntary + Compliance Carbon Markets\nThe latest IPCC report highlights the urgency to fully decarbonize our global economy. Carbon markets help accelerate and finance the transition to net-zero emissions by enabling businesses and individuals to take accountability for their unavoidable emissions through the purchase of carbon credits.\nThis helps close the emissions gap left by countries commitments under the Paris Agreement, the finance gap where urgently needed climate action is not yet funded, and the time gap that favours action today over waiting for sufficient policy measures from governments.\nDRIVING FINANCE TO HIGH IMPACT CLIMATE PROTECTION ACTIVITIES\nHowever much we try to reduce our footprint, in our current world some emissions are still unavoidable. Financing emission reductions by purchasing high impact carbon credits is a way to take responsibility for your climate impact and to take-action beyond your own footprint.\nCarbon credits represent the reduction or removal of one tonne of CO2 equivalent (tCO2e). Gold Standard carbon credits also include gender-sensitive and inclusive stakeholder design, environmental and social safeguards, and a contribution towards at least three Sustainable Development Goals (SDGs) - ensuring the greatest impact delivered for every dollar channeled to projects.\nGold Standard Verified Emission Reductions (VERs) for voluntary climate action\nFor more information\nRENEWABLE ENERGY MARKETS\nRenewable energy markets\nAt least 42% of electricity must be supplied by renewable sources by 2030 to reach net zero emissions by midcentury -- the ambition of the Paris Agreement -- which requires doubling renewable power capacity. The private sector is key to this transition, yet some companies are limited in their ability to source renewable energy directly. Renewable energy markets allow these companies to purchase clean energy from their grid.\nBy purchasing clean energy -- whether directly or through renewable energy markets -- companies demonstrate a commitment to being more environmentally friendly, something clients, consumers + civil society are increasingly demanding.\nACCELERATING THE CLEAN ENERGY TRANSITION\nMore and more businesses are setting ambitious renewable energy targets in an effort to spur the global transitions to renewables and purchasing renewable electricity through energy attribute certificates is a convenient, affordable and accessible option.\nGold Standard labelled Renewable Energy Attribute Certificates represent a megawatt-hour (MWh) of electricity generated and delivered to the grid from a renewable source. They are issued on top of I-RECs, internationally recognised and reliable electricity attribute tracking certificates (RECs). And most importantly, they follow Gold Standard principles to ensure that a buyer’s purchase results in real-world emissions reductions and the addition of new renewable energy capacity to the grid, helping to drive the clean energy transition.\nFor more information\nWater Benefit Certificates\nThe World Economic Forum names water crises as the #1 greatest risk facing the world.\nFreshwater sustains life – neither people nor nature can exist without it. But many of the world’s water systems are dangerously stressed and experts predict that ‘business as usual’ water use will expose 4.8 billion people to severe water scarcity by 2050.\nINCREASING ACCESS TO SAFE WATER FOR ALL\nWater is the most fundamental human need and many organisations are committed to water stewardship, working collaboratively to ensure that social, environmental and economic benefits are achieved.\nGold Standard has developed requirements to certify the positive socio-economic impacts of water projects, enabling companies to support high-impact water projects through the purchase of Water Benefit Certificates (WBCs).\nA Water Benefit Certificate represents a volume of water that has been sustainably supplied, purified or conserved. There are different volumes for different project types, depending on size and impacts provided. Each Gold Standard certified water project must deliver impact toward at least three SDGs and demonstrate a clear financial need for additional finance from selling Water Benefit Certificates – delivering development impacts that would not have otherwise happened.', 'How Carbonhalo Works\nOffsetting your carbon footprint through verified carbon credits\nWe should all try to reduce our carbon emissions where we can. For the unavoidable emissions leftover, Carbonhalo is here to offset them with verified carbon credits that fund emissions reductions projects across the globe.\nThese projects include replanting forests, preventing deforestation, and investing in technology that reduces or removes CO2 emissions from the atmosphere.\nOur carbon credit certification\nWhat is a carbon credit?\nA carbon credit is a digital tradable certificate that confirms a tonne of CO2 has been averted through environmental projects or businesses. Because we use the Blockchain ecosystem across the Ethereum and Polygon network and tradable exchanges, all purchase transactions and disposal of carbon credits are visible to everyone and instantly auditable for complete transparency. When we retire the token, it’s impossible to be used again, meaning we can certify it as single-use.\nWe only invest in projects or purchase tokens that are verified by leading industry bodies/agencies including VERRA, Gold Standard, REDD+ and UNFCCC. This ensures that your contributions are only going towards projects that meet all the requirements to make a positive impact.\nWhat is a VERRA - Voluntary Carbon Standard?\nThe Voluntary Carbon Standard (VCS) Program is the world’s most widely used voluntary GHG program. They ensure the credibility of the emission reductions generated by offset programs. Nearly 1,700 certified VCS projects have collectively reduced or removed more than 630 million tonnes of carbon and other GHG emissions from the atmosphere. Verra’s role is to develop and administer the program.\nWhat is REDD+ and what are REDD+ carbon credits?\nREDD – the Reduction of Emissions from Deforestation and Forest Degradation programs – is a United Nations initiative that focuses on preserving and reforesting the worlds’ rainforests to fight climate change.\nREDD+ (or REDD-plus) refers to projects that reduce emissions from deforestation and forest degradation, incorporating the role of conservation, sustainable management of forests, and enhancement of forest carbon stocks in developing countries.\nREDD+ carbon credits are credits verified and linked to REDD+ specific projects.\nYour carbon credit purchases\nAcross the globe, carbon pricing is irregular and confusing. So we’ve adopted a universal approach that keeps carbon offsets at the same pricing structure, no matter where in the world you live. These tokenised carbon credits are offered as fractional credits, for ease of payment.\nWhat are tokenised carbon credits?\nA tokenised carbon credit is a certified carbon credit, that is placed on the Blockchain ecosystem like Ethereum or Polygon. Blockchain tokens offer full traceability, auditability, instant settlement, and the flexibility of fractional carbon credits.\nEach token or carbon credit that is burned or retired immediately and permanently offsets 1 tonne of CO2, mapped directly to the retirement of the carbon credit. This is captured within the blockchain ledger, and the transaction ID information is lodged and made publically available.\nWhat are fractional carbon credits?\nA fractional carbon credit is a portion of a credit that can be bought at an exact price, rather than having to round up or down to the nearest complete carbon credit, making it easier to offset your carbon footprint.']	['<urn:uuid:64e7b0b6-1149-4982-bade-00bdbfc0dbe5>', '<urn:uuid:043ce89c-9343-428a-a756-e6f364a81453>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T00:57:18.859842	8	53	1237
88	How do consumer protection laws interact with arbitration agreements, and what role do non-compete clauses play in protecting business interests?	Consumer protection laws take precedence over arbitration agreements, as established by the National Consumer Disputes Redressal Commission. The Commission held that disputes governed by special statutory enactments serving public policy are not arbitrable, and an arbitration clause cannot circumscribe the jurisdiction of Consumer Forums, even with amendments to the Arbitration Act. Regarding non-compete clauses, they are contractual provisions that prohibit employees from working for competitors or starting competing businesses during and after employment. These clauses are enforceable in many jurisdictions but must be reasonable in geographic scope and time duration, typically not exceeding one year. However, they are unenforceable in some states like California, where employers requiring such clauses can be sued.	['Statutory remedy vs. Arbitration: what survives? Interplay between special statutes and the Arbitration Act on applicability of non-obstante clauses\nThe NCDRC has held that:\nA three-member panel of the National Consumer Disputes Redressal Commission (“NCDRC”) in Aftab Singh and Others (“Complainants”) v. Emaar MGF Land Limited and Another (“Builders”)1 has held that an arbitration clause in agreements between Builders and Purchasers cannot circumscribe the jurisdiction of a Consumer Forum, notwithstanding the amendments made to Section 82 of the Arbitration & Conciliation Act, 1996 (“A&C Act”).\nRecognizing that the Consumer Act was envisaged as a special social legislation to protect consumer rights and provide a special dispute redressal mechanism, the NCDRC held that disputes governed by statutory enactments established to serve a particular public policy are not arbitrable.\nThis case involves batch matters. The complainants in these matters had booked residential villas/flats/plots in Projects of the Builders and accordingly executed Buyers’ Agreements (Agreements). Every agreement contained an arbitration clause. Upon the Builders’ failure to deliver possession of properties by the agreed date, the Complainants filed complaints before Single Member of the NCDRC. On the other hand, the Builders filed applications before the NCDRC under Section 8 of the Arbitration Act - praying for reference to arbitration as per the Agreements.\nConsidering the potential wide impact of the issue involving interplay between dispute redressal mechanism under the Consumer Protection Act (“CPA”) and the A&C Act (“A&C Act”), the Learned Single Member referred the issue to a larger bench.\nCONTENTIONS OF THE COMPLAINANTS\nCONTENTIONS OF THE OPPOSING PARTY\nThe NCDRC rejected the contentions of the Builders and held that an arbitration clause in the Agreements cannot circumscribe the jurisdiction of a Consumer Fora, notwithstanding the amendments to Section 8 of the A&C Act. The reasoning of the NCDRC is as follows:\n‘Arbitrability’ of certain disputes\nThe NCDRC analysed principles on arbitrability of certain types of disputes, as culled out in landmark cases of the Supreme Court viz. Booz Allen Hamilton,5 Vimal Kishore Shah,6 Natraj Studios7 and Ayyaswamy.8 In particular, it relied upon jurisprudence on non-arbitrability of consumer disputes,9 or to be accurate, on recourse to special remedy under the CPA - rather than arbitration under the A&C or civil suits under the Code of Civil Procedure.\nIn these decisions, rendered prior to amendment of Section 8 of the A&C Act, the Supreme Court had placed reliance on Section 3 of the CPA and held that the legislature intended to provide a remedy in addition to the consentient arbitration or the civil action in a suit. Hence, the remedy of approaching a consumer forum was not in derogation of the A&C Act by mere presence of an arbitration clause under the Agreements.\nAmendment to Section 8, A&C Act vs. law on arbitrability\nThe amended Section 8(1) states that: the judicial authority shall refer parties to arbitration, inter alia “notwithstanding any judgment, decree or order of the Supreme Court or any Court…” The NCDRC held that the Non-obstante clause under the amended provisions was not intended to, and in fact cannot, undo existing jurisprudence under the CPA on adjudication of consumer disputes before consumer fora.\nThe NCDRC looked into the legislative history of the amendments to Section 8 and held that the Non-obstante clause was intended solely to curb continued application of judgments where excessive judicial power was usurped by courts while enquiring into issues of existence or validity of arbitration agreements - in the matter of appointing arbitrators under Section 11 (as in SBP10) as also in making reference to arbitration under Section 8. The NCDRC held that interpretation of the Non-obstante clause to ignore the entire body of pre-amendment case law would also unfasten jurisprudence related to other non-arbitrable areas of disputes - such as tenancy, trusts, crimes, insolvency etc. Such an interpretation would also tantamount to over-ruling by the Parliament of law made by the Supreme Court.\nThe NCDRC considered the principle of exclusion of jurisdiction of a forum in the presence of a special forum under special legislations. In the context of consumer disputes, it relied on Skypack Couriers, National Seeds Corporation and Rosedale Developers.\nConsidering the specific case of the Indian Trusts Act, 1882, the NCDRC relied on the Supreme Court ruling in Vimal Kishore where it was held that, “since sufficient and adequate remedy is provided under the Trusts Act, 1882 for deciding the disputes in relation to trust deed, trustees and beneficiaries, the remedy provided under the Arbitration Act for deciding such disputes is barred by implication”. The NCDRC reiterated that public policy requires that parties cannot also be permitted to contract out of the legislative mandate which requires certain kind of disputes to be settled by Special Courts constituted by the Act.11\nObject of CPA vs. A&C Act\nThe NCDRC further observed the importance of remedies available under the CPA and the special object and purpose of a beneficial legislation such as the CPA in protecting interests of consumers. The NCDRC reasoned that allowing Section 8 to oust the jurisdiction of consumer fora would set at naught the entire purpose and object of the Consumer Act, which was to ensure speedy, just and expeditious resolution and disposal of consumer disputes. Exposure of such disputes to the Arbitration Act could invite application of portions of the A&C Act that are enforceable only through Civil Courts. This would be repugnant to the manifest purpose underlying the enactment of the CPA. The NCDRC also briefly relied on Section 2(3)12 of the A&C Act which recognizes schemes under other legislations that make disputes non-arbitrable.\nThe NCDRC finally held that in light of the overall architecture of the CPA and Court-evolved jurisprudence, amended sub-section (1) of Section 8 cannot be construed as a mandate to the Consumer Fora, constituted under the CPA, to refer the parties to arbitration in terms of the Arbitration Agreement.\nThis judgment is a welcome reiteration of the principle that - jurisdiction of specially constituted judicial authorities cannot be ousted by a blanket mandate to refer disputes to arbitration under the umbrella of Section 8, A&C Act. On the ground level, it provides relief to a party to avail special remedies available under a statute, despite presence of an arbitration clause. This serves to protect parties from lop-sided and sparely-negotiable standard form agreements. From another angle, parties will no longer be mandated to adopt the relatively long and expensive process of arbitration - in disputes arising under specific statutes having specific dispute redressal mechanisms.\nThis judgment could also be used to interpret Section 79 of the Real Estate (Regulation and Development) Act 2016 (“Real Estate Act”) which bars the jurisdiction of civil courts in matters involving real estate. The NCDRC applied the reasoning in Ayyaswamy to conclude that the same principles could apply to cases under the Real Estate Act, which provides a special mechanism for resolution of disputes between buyers and builders.\nThe reasoning of this judgment is simple and well-founded on object of beneficial legislations, coupled with recourse to special remedies provided there-under. However, what is of utmost importance is interpretation of the Non-obstante clause under the amended Section 8, A&C Act. Delving into the legislative intent and mischief attempted to be remedied by this clause, it is clear that the Non-obstante clause was added to undo the effect of judgments like SBP & Co. which opened floodgates of enquiry by courts into existence / validity of arbitration agreements - at pre-arbitration stages of Section 8 and Section 11. This power could also be sufficiently exercised by arbitral tribunals under Section 16 of the A&C Act, except in certain circumstances (as laid out in Ayyasamy).13\nTherefore, the Non-obstante Clause is likely to assist in ignoring pre-amendment case law that widens court-intervention. However, it certainly cannot be interpreted to upset the long-standing and carefully examined body of case law that determines types of disputes that are or are not arbitrable. In this regard, a combined reading of Section 5 and Section 8 - alongwith Section 2(3) of the A&C Act - is crucial to acknowledge laws under which certain disputes cannot be submitted to arbitration.\nThe Supreme Court considered this issue of arbitrability in the context of Section 8, A&C Act in the landmark case of Booz Allen. It held that all aspects of arbitrability have to be decided by the court seized of the suit, and cannot be left to the decision of the Arbitrator. Even if an arbitration agreement exists and the dispute is covered therein, the court will refuse to refer the parties to arbitration - if the subject matter of the suit is capable of adjudication only by a public forum or the relief claimed can only be granted by a special court or Tribunal.\nThe NCDRC is a special forum constituted under the CPA for adjudication of consumer disputes. It is therefore, needless to mention that the NCDRC will be the proper adjudicatory forum for any dispute falling under the CPA, despite existence of an arbitration agreement. This reasoning also finds place in rulings of the Supreme Court – notable ones being Natraj Studios14 for disputes relating to leave and license under the then existing Bombay Rent Act,15 and Vimal Kishore for disputes arising under the Indian Trusts Act, 1882.\nDoes this imply that arbitration cannot be resorted to for adjudication of disputes falling under specific statutes? If a party submits to arbitration by filing its statement of defence, or files its statement of claim followed by a corresponding statement of defence by the opposing party, would it constitute submission to arbitration? Would such submission, once crystallized, preclude a party from adopting the statutory remedy? These issues call for scrutiny and purposive interpretation of the statute vis-à-vis the A&C Act.\n1 Consumer Case No. 701 of 2015\n2 Section 8(1) under A&C Act, 1996:\n8. Power to refer parties to arbitration where there is an arbitration agreement.—\n(1) A judicial authority before which an action is brought in a matter which is the subject of an arbitration agreement shall, if a party so applies not later than when submitting his first statement on the substance of the dispute, refer the parties to arbitration.\nAmended Section 8(1) under A&C Act, 2015:\n8(1). “(1) A judicial authority, before which an action is brought in a matter which is the subject of an arbitration agreement shall, if a party to the arbitration agreement or any person claiming through or under him, so applies not later than the date of submitting his first statement on the substance of the dispute, then, notwithstanding any judgment, decree or order of the Supreme Court or any Court, refer the parties to arbitration unless it finds that prima facie no valid arbitration agreement exists.”\n3 National Seeds Corporation Limited v. M. Madhusudhan Reddy & Anr. (2012) 2 SCC 506\n4 Section 5. Extent of judicial intervention.—Notwithstanding anything contained in any other law for the time being in force, in matters governed by this Part, no judicial authority shall intervene except where so provided in this Part.\n5 Booz Allen and Hamilton Inc. v. SBI Home Finance Limited & Ors., (2011) 5 SCC 532\n6 Vimal Kishor Shah & Ors. v. Jayesh Dinesh Shah, (2016) 8 SCC 788\n7 Natraj Studios (P) Ltd. v. Navrang Studios, (1981) 1 SCC 523\n8 A Ayyasamy Vs. A Paramasivam & Ors., (2016) 10 SCC 386\n9 Skypak Couriers Ltd. v. Tata Chemicals Ltd., AIR 2000 SC 2008; Secretary, Thirumugugan Cooperative Agricultural Credit Society Vs. M. Lalitha (through LRs) & Ors., (2004) 1 SCC 305; Fair Air Engineers Pvt. Ltd. & Anr. Vs. N.K. Modi, (1996) 6 SCC 385; National Seeds Corporation Limited; Rosedale Developers Pvt. Ltd. Vs. Aghore Bhattacharya, (2015) 1 WBLR 385 (SC)\n10 SBP v Patel Engineering, (2005) 8 SCC 618\n12 “2. Definitions…\n(3) This Part shall not affect any other law for the time being in force by virtue of which certain disputes may not be submitted to arbitration.”\n13Please see out hotline on Ayyasamy case here.\n14Natraj Studios (P) Ltd vs Navrang Studios & Anr., AIR 1981 SC 537\n15Bombay Rents, Hotel and Lodging House Rates Control Act, 1947', 'What is a non competition clause in an employment contract?\nNon-Competition Clause. What is a non-competition clause? A non-competition (or non-compete) clause is a passage in an employment contract which purports to prohibit employees from working for another employer or starting their own business which competes with their employer during and after employment.\nCan a non-compete clause extend the reasonable notice period?\nAbacus Management, 2014 BCSC 938 (CanLII), the court considered the presence of a non-competition clause in extending the reasonable notice period. Accordingly, employers should not just blindly by boiler-plate insert into their employment contracts a non-compete clause for each and every employee.\nWhat happens if you violate a non compete clause?\nLike all legal agreements, a non-compete clause is only as meaningful as a court’s willingness to enforce it. If you violate your non-compete clause, your employer can take the matter to court (although not all do so).\nIs the non-compete clause unenforceable in California?\nNoncompete clauses are unenforceable in some states such as Oklahoma and North Dakota, whereas in California, for example, employers who require a non-compete clause can even be sued. Before signing, research the laws in your state.\nWhat does no compete mean?\nNon Compete Meaning: Everything You Need to Know. The non-compete meaning is a contractual agreement that exists between employer and employee that states that employee agrees not to use any information gained.3 min read.\nWhat is Employment Non compete?\nEmployment Non Compete Agreement Law and Legal Definition. A non-compete agreement is a promise by an employee not to compete with his or her employer for a specified time in a particular place. The agreement may cover such actions, among others, as opening a competing business or using customer information for business leads.\nWhat is a non competition clause?\nIn contract law, a non-compete clause (often NCC), or covenant not to compete (CNC), is a clause under which one party (usually an employee) agrees not to enter into or start a similar profession or trade in competition against another party (usually the employer). Some courts refer to these as “restrictive covenants.”.\nWhat is a non compete agreement?\nNon-Compete Agreement. What is a Non-Compete Agreement? A non-compete agreement is a contract between two parties, usually two individuals or one company and one individual, in which one of the individuals promises not to compete with the other individual or company once their relationship with the company has ended.\nWhat happens when you sign a non-compete agreement?\nNon-compete agreements can prevent workers from getting a job in their field if they leave a position after signing such an agreement. Non-compete agreements are signed when the relationship between employer and employee begins. They give the employer control over specific actions of the former employee—even after that relationship ends.\nAre there non-compete agreements in the state of Florida?\nThe enforceability of non-compete agreements in the state of Florida is quite common. Some law firms build their law practice around these agreements and represent employees, employers and potential new employers of an employee currently bound by a non-compete agreement.\nHow long can a non compete clause be enforced?\nGenerally, it should not be longer than one year. Most courts refuse to enforce any non-compete clause longer than a year. It is submitted that to be on the safe side, a non-compete clause should be six months. That is not to say that certain very long non-compete clauses will not be enforced.\nWhat does non compete clause mean?\nAre non compete agreements enforceable?\nNon-competition agreements are only enforceable to the extent they are reasonable in both geographic scope and time duration.']	['<urn:uuid:f7f4f83e-e1a6-4782-8390-60e105a74979>', '<urn:uuid:eb8fb404-ed77-41ed-994f-0f8f7f44b0f6>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:57:18.859842	20	112	2621
89	What were the key turning points that helped change the Continental Army's fortune in both Valley Forge and the Battle of Princeton?	In Valley Forge, two key developments helped turn things around: Baron Friedrich von Steuben's new training regimen and hygiene standards implementation, and France's commitment to the Revolutionary cause. At Princeton, Washington's superior tactical decisions and familiarity with the terrain were crucial - he organized night attacks and led his army to safe highlands in New Jersey, successfully encircling British forces. Both events proved to be significant turning points: Valley Forge produced a revitalized army that would soon force the British from the field at Monmouth, while the Princeton victory (along with Trenton) convinced many doubters that the war was actually winnable for the revolutionary cause.	['By December 1777, Washington was well aware that some members of the Continental Congress were questioning his leadership abilities. The Valley Forge site—located along trade routes and near farm supplies—was an attempt to balance Congress’ demands for a winter campaign against Philadelphia with the needs of his troops. It was common for armies at the time to withdraw to fixed camps during the winter, as the harsh weather made transportation of troops, arms and supplies extremely difficult.\nThe soldiers who marched to Valley Forge on December 19, 1777 were not downtrodden or desperate. Though they had been defeated in two key battles, and had lost Philadelphia to the British, Continental troops had often put themselves on the offensive, and proved themselves as skilled fighters against professional soldiers with superior numbers. They were certainly tired, and lacking in supplies, but these were not unusual circumstances in the life of a Continental soldier. Once the troops arrived at their winter camp site, military engineers directed the construction of some 2,000 huts laid out in parallel lines, forming a kind of city, along with miles of trenches, five earthen redoubts and a bridge over the Schuylkill River.\nRaw winter weather made things difficult for the tired troops, while a mismanaged commissary and Congress’ failure to provide the army with sufficient funds for fresh supplies led to widespread hunger and lack of clothing, shoes and other supplies among the men. Yet cold and starvation were not the most dangerous threats to soldiers at Valley Forge: Diseases like influenza, dysentery, typhoid and typhus killed two-thirds of the nearly 2,000 soldiers who died during the encampment.\nTo keep his battered army together, Washington instituted policies of lashing as punishment and threatened to shoot deserters on sight. Taking rumors of his imminent replacement in stride, he decided to regroup rather than give up. In February 1778, Washington welcomed to camp Baron Friedrich von Steuben, a Prussian officer who put Continental troops through a new training regimen, including the precision marching that was so important to Revolutionary-era warfare. Von Steuben also instituted new hygiene standards in the slovenly American camp, introducing the first latrines (and ensuring they were placed far from the kitchens). In thanks, a grateful Washington had Congress appoint Von Steuben inspector general of the Continental Army. Another officer, Nathanael Greene, took charge of procuring supplies through the commissary in March, leaving Washington free to concentrate on developing strategies to confront the British. By April, the general’s critics in Congress were silenced, and news of France’s commitment to the Revolutionary cause in May brought celebration to Valley Forge.\nOn June 19, 1778, exactly six months after they arrived, a revitalized Continental Army left Valley Forge and headed towards New Jersey. Barely a week later, they forced the British from the field in the Battle of Monmouth. The Valley Forge encampment proved to be a turning point in the Revolutionary War, testing the mettle of George Washington and his troops and paving the way for their ultimate victory in the war for American independence.', 'The Battle of Princeton took place on January 3rd, 1777 at Princeton, in what the British still considered their colonial Province of New Jersey. It was a military conflict between the British and their hired German Hessian soldiers on one hand, and revolutionaries from the 13 British colonies in North America on the other. The battle would come to be known as one of the most important in the American Revolutionary War (1775-1783). American general and leader of the Continental forces, George Washington, led his troops into this famous victory against the British colonialists. On December 26, 1776, Washington had managed to defeat Johann Rall’s Hessians, in service to the British, and evade a conflict with General Charles Cornwallis, who had been sent to Trenton to capture him. Washington had additionally managed to carry out successful assaults against the British rear guards as they departed to Trenton from Princeton, New Jersey.\nBrigadier General Hugh Mercer and Continental Army Commander-In-Chief George Washington led their forces against an out-gunned and outmanned opposition. The British were meanwhile led by General Charles Cornwallis and Lieutenant Colonel Charles Mawhood. The combatants on both sides used big guns as well as rifled and smoot-barrel muskets during the battle. Washington’s troops were made up of both Continentals and militiamen, while the opposition was mainly comprised by British Regulars. Continental troops boasted around 4,500 men, while the British forces was numbered at roughly 1,200.\nThe British were angered by Washington’s victory at Trenton, and they had sent Cornwallis to capture Washington, whom they had nicknamed “The Fox”. Cornwallis went to Trenton on January 2, 1777, accompanied by 8,000 British soldiers, to combat Washington’s army, which numbered 5,000 men. On the night of January 2, 1777, Washington left his position and moved to circle the British army with his continental forces. He obtained assistance from General Hugh Mercer who was also a leader of the continental forces. Mercer led his troops to a battle with a British army led by Lieutenant colonel Charles Mawhood. The British proved too strong for Mercer and his troops. Washington was compelled to send reinforcements using a militia that was led by John Cadwalader. The militia also attempted to flee from the advancing British troops, and it was then that Washington summoned up all of the available regular American forces to attack Cornwallis and his troops. Washington used his familiarity with the terrain to gain an edge over the British. He organized night attacks and led his army to safe highlands in New Jersey by setting up a camp at Morristown. While organizing on how to attack the British, the Americans led the enemies into territories where they could easily encircle them without there being opportunities for escape as they did along Post Road, a critical intersection between Trenton and Princeton. Cornwallis, fearing that American forces would escape, sent troops to take guard of the Delaware River. However, Washington and his troops managed to defeat the British forces, owing to their familiarity with the land and superior tactics.\nThe battle led to the deaths of 100 British soldiers and 25 American ones. 40 American and 70 British soldiers were wounded while the Americans managed to capture 280 British soldiers at the end of the battle. Following the defeat, British army leaders General William Howe, General James Grant and Admiral Richard chose to leave New Jersey to Washington and the American troops. The two concentrated their efforts between the Atlantic Coast and New Brunswick. As such, the British Hessian mercenaries were compelled to end years of their invasion of New Jersey, which reverted to the control of American Continental militias.\nThe victories the Americans obtained at Trenton and Princeton gave them hope and morale that they could go on and win the Revolutionary War. The Battle at Princeton was in a line of early conflicts in the war. Indeed, this victory and that at nearby Trenton led many former doubters to believe this war was a winnable one for the revolutionary cause. Part of the battlefield lies today in Princeton Battlefield State Park, which is maintained by the state of New Jersey. The battlefield is both a US National Historic Landmark and a US Historic district. It lists among both the New Jersey State and US National Registers of Historic Places. Of the 30 Army units with colonial roots still active today, eight were involved in the battle.']	['<urn:uuid:935846d2-3cf5-44aa-a629-ef202e37d7ca>', '<urn:uuid:d40576b7-46de-40b8-98d5-82293f7ff910>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T00:57:18.859842	22	105	1240
90	What are the recommended ways to care for a sprained finger in terms of using ice and elevation to reduce pain and swelling?	For a sprained finger, you should apply crushed ice wrapped in a towel to the injured finger every hour for 15 to 20 minutes, at least 4 to 8 times each day. Additionally, you should keep your injured finger raised above the level of your heart as often as possible by resting it on a pillow to help reduce swelling.	"[""WHAT YOU SHOULD KNOW:\nA finger sprain happens when ligaments in your finger or thumb are stretched or torn. Ligaments are the tough tissues that connect bones. Ligaments allow your hands to grasp and pinch.\nCare for your finger:\n- Ice your finger: Ice your finger to help decrease pain and swelling. Put crushed ice in a plastic bag and cover it with a towel. Put the ice on your injured finger or thumb every hour for 15 to 20 minutes at a time. You may need to ice the area at least 4 to 8 times each day. Ice your finger for as many days as directed.\n- Elevate your finger: Keep your injured finger raised above the level of your heart as often as you can. This will help reduce swelling. You can elevate your hand by resting it on a pillow.\n- Compress your finger: Compression (tight hold) helps support your finger or thumb as it heals. Your injured finger may be taped to the finger beside it. Severe sprains may be treated with a splint. Ask how long you must wear the splint or tape, and how to apply them.\n- Pain medicine: You may be given medicine to take away or decrease pain. Do not wait until the pain is too bad before taking your medicine.\n- Take your medicine as directed: Call your primary healthcare provider if you think your medicines are not helping or if you have side effects. Tell him if you take vitamins, herbs, or any other medicines. Keep a written list of your medicines. Include the amounts, and when and why you take them. Bring the list or the pill bottles to follow-up visits.\nExercise your finger:\nYou may be given gentle exercises to begin in a few days. Exercises can help decrease stiffness in your finger or thumb. Exercises also help decrease pain and swelling and improve the movement of your finger or thumb. Check with your primary healthcare provider before you return to your normal activities or sports.\nFollow up with your primary healthcare provider as directed:\nWrite down any questions you may have to ask at your follow-up visits.\nContact your primary healthcare provider if:\n- You have new or increased swelling or pain in your finger.\n- You have new or increased stiffness when you move your injured finger.\n- You have questions or concerns about your injury or treatment.\nReturn to the emergency department if:\n- The skin on your injured finger looks bluish or pale (less color than normal).\n- You have new weakness or numbness in your finger or thumb. It may tingle or burn.\n- You have a splint that you cannot adjust and it feels too tight.\n© 2014 Truven Health Analytics Inc. Information is for End User's use only and may not be sold, redistributed or otherwise used for commercial purposes. All illustrations and images included in CareNotes® are the copyrighted property of A.D.A.M., Inc. or Truven Health Analytics.\nThe above information is an educational aid only. It is not intended as medical advice for individual conditions or treatments. Talk to your doctor, nurse or pharmacist before following any medical regimen to see if it is safe and effective for you.""]"	['<urn:uuid:c58e3ce5-d0de-4b35-80b0-f2f91ed4a884>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	23	60	541
91	What are the recommended strategies for effectively embedding risk management into project planning, particularly when dealing with high-tech projects that often face time and resource constraints?	The key to effectively managing project risk involves several strategies. First, recognize that much of the work, even in high-tech projects, is not new and use lessons learned from previous projects as a roadmap for identifying and avoiding potential problems. Second, plan project work thoroughly, especially innovative portions, to understand challenges ahead and anticipate risks. It's important to adopt a systematic process that deals with uncertain events and conditions. Additionally, avoid fuzzy or poorly defined deliverables - if you cannot define everything, break the project into smaller, definable sequences. Use tools like process flowcharts, work breakdown structures, and brainstorming to identify risks. Finally, maintain consistent risk communication by making project risks a regular part of team meeting agendas.	['If you feel like you need to issue nonflammable suits to your entire team because they always seem to be fighting fires when it comes to projects, it’s time to step back and reassess why the risk is always so high.\nAs Tom Kendrick points out in his book, “Identifying and Managing Project Risk,” there is always risk involved in any project, and some level of uncertainty. In fact, high-tech projects are particularly risky, he points out, because of the number of variables. In addition, such projects are often pushed to move faster than others even when budget, staff and time are scarce.\nBut projects that succeed, Kendrick argues, do so because their leaders do two things well.\n“First, leaders recognize that much of the work on any project, even a high-tech project, is not new. For this work, the notes, records and lessons learned on earlier projects can be a road map for identifying, and in many cases avoiding, many potential problems,” he explains. “Second, they plan project work thoroughly, especially the portions that require innovation, to understand the challenges ahead and to anticipate many of the risks.”\nIf you’re ready to retire the daily use of flame-retardant suits for your team, here are some ways to effectively manage the risk in projects:\n- Embed risk management into projects. Charles Bosler, a risk management expert, says that risk is “simple” because it is “anything that requires you to make choices about the future.” While risk can never be entirely eliminated from projects, Kendrick contends, it can be reduced – often with “minor incremental effort.” Michael Taylor, an experienced project risk manager with more than 30 years of experience, suggests that the best way to manage risk is to adopt a process that systemically deals with the overall problem of uncertain events and conditions.\n- Be specific. Don’t allow “fuzzy” or poorly defined deliverables, because that can lead to failure, Kendrick explains. “If you do not know enough to define everything, convert the project in a sequence of smaller efforts that you can define, one after the other, and perform reviews and testing,” he suggests. Taylor suggests using process flowcharts, work breakdown structures and even brainstorming to identify risks.\n- Communicate. Bart Jutte, a business analyst and project risk management expert, explains that project managers who experienced failure “were frequently unaware of the big hammer that was about to hit them.” The unsettling reality, he adds, it that someone did see the hammer but failed to tell the project manager. That’s why risk communication must be consistent, he argues. “If you have a team meeting, make project risks part of the default agenda (and not the final item on the list!),” he explains. “This shows risks are important to the project manager and gives team members a ‘natural moment’ to discuss them and report new ones.”\n- Identify bottlenecks. Kendrick explains that when reviewing past projects and problems, look for the bumps in the road likely to occur again and then develop plans to avoid them. For example, budget and staffing shortfalls can often cause problems, so negotiate a budget reserve or extra bodies.\n- Analyze. Taylor explains that a qualitative analysis should look at the probability of a risk condition, along with the impact from that risk. Once that is done, then the magnitude of those risks can be assessed. This “weighted risk factor” technique is calculated as: WRF=W1*RFTECH+W2*RFSCHED+W3*RFCOST\n- “RF” means Risk Factor = (P+C) – (P x C).\n- The value for weight (W) is dependent upon its project priority within the triple constraint.\n- W1, W2, and W3 are valued 0 through 1.0 depending on the priorities of the project, and together must sum to 1.0.\nJutte notes that when project managers are examining the entire project, they can do a simulation “to show your project sponsor how likely it is that you finish on a given date or within a certain time frame,” along with a similar exercise for project costs.\nFinally, risk experts advise that when assessing risks in the project, managers should also keep an eye out for opportunities.\n“Make sure you create some time to deal with the opportunities in your project, even if it is only half an hour. Chances are that you see a couple of opportunities with a high pay-off that don’t require a big investment in time or resources,” Jutte suggests.Posted in Project Management, Team & Project Management | Tagged project management, risk, risk management']	['<urn:uuid:b3895e0f-c43c-4459-a899-4819100cfdc5>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	26	118	747
92	which gene works switch controlling single versus multiple dendrites fly neurons	The zinc finger-containing protein Hamlet functions as a binary switch between the elaborate multiple-dendrite morphology of the da neuron and the single, unbranched dendrite morphology of the external sensory (es) neuron.	"['Genetic Control of Dendrite Development in Drosophila\nDendrite arborization patterns are critical determinants of neural circuit formation and influence the type of synaptic or sensory inputs a neuron is able to receive. Moreover, dendrite defects are associated with a variety of known human mental disorders, such as autism. Relatively little is known about the molecular mechanisms that control dendrite development. To use Drosophila genetics to identify core programs that control dendrite development, we developed a simple assay system. We use the fly transgenic technique to express green fluorescent protein (GFP) in the dendritic arborization (da) neurons, a group of sensory neurons with a stereotyped dendritic branching pattern. This allows us to visualize the development of the dendrites of da neurons in the living fly embryos and to use them as an assay system for a genetic dissection of dendrite development. We have made some recent progress:\n1. Regulation of dendritic field size and complexity by transcription factors. Transcription factors are important regulators of the size and complexity of dendritic fields, and the logic of their usage is beginning to emerge. In some cases, the ""dendritic fate"" of a particular neuron might be specified by a single transcription factor. For example, in the Drosophila PNS, the zinc finger–containing protein Hamlet functions as a binary switch between the elaborate multiple-dendrite morphology of the da neuron and the single, unbranched dendrite morphology of the external sensory (es) neuron. In most cases, however, the dendritic fate is determined by the combined action of multiple transcription factors. Drosophila da neurons fall into four distinct morphological classes (I–IV). The selector gene cut is expressed at different levels in the da neurons. Neurons with small and simple dendritic arbors either do not express Cut (class I neurons) or express low levels of Cut (class II). Neurons with more complex dendritic branching patterns and larger dendritic fields (classes III and IV) express higher levels of Cut. Cut levels are a critical determinant of da neuron class-specific dendritic morphologies.\nIn contrast to Cut, Spineless (Ss), the homolog of the mammalian dioxin receptor, is expressed at similar levels in all da neurons. In ss mutants, different classes of da neurons elaborate dendrites with similar branch numbers and complexities, suggesting that da neurons might reside in a common ""ground"" state in the absence of ss function. Studies of the epistatic relationship between Cut and Ss indicate that these transcription factors are likely acting in independent pathways to regulate morphogenesis of da neuron dendrites. These findings, together with an analysis of an RNAi (RNA interference) screen that revealed more than 70 transcription factors regulate dendritic arbor development of class I neurons in fly, suggest that complicated networks of transcriptional regulators likely regulate type-specific dendrite arborization patterns.\n2. The molecular mechanism for dendritic self-avoidance and tiling. Dendrite-dendrite repulsion can have a profound influence on the size and shape of the dendritic field, as well as the spatial relationship between different dendritic fields. The dendrites of each da neuron show self-avoidance and tend to spread out without crossing over. Class III and class IV da neurons show tiling; i.e., there is little overlap between the dendritic fields of adjacent neurons of the same class because their dendrites show homotypic repulsion. Recently we found that Dscam (Down syndrome cell adhesion molecule), originally identified as axon guidance receptors by Lawrence Zipursky (HHMI, University of California, Los Angeles) and his colleagues, is needed for self-avoidance and contribute to the spreading of dendrites. Without Dscam, the dendrites of each da neuron bundle together or cross over. For the dendritic fields of different neurons to coexist in the same space, they need to express different Dscam isoforms. In contrast, tiling requires some cell surface recognition molecules other than Dscam to mediate the homotypic repulsion. Although the signals that mediate tiling behavior remain elusive, the evolutionarily conserved protein kinase Tricornered (Trc) and the putative adapter protein Furry (Fry) have been identified as important components of the intracellular signaling cascade involved in tiling. In trc or fry mutants, dendrites no longer show their characteristic turning or retracting response when they encounter same-type dendrites.\n3. Dendrite-specific developmental regulators. From our mutant screen, we identified a group of dar (dendritic arborization reduction) genes. Mutations of any of the dar genes lead to defective dendritic arbors but normal axonal projections. Thus, studies of dar genes should reveal how axons and dendrites are made differently. We estimate that there may be a total of about 20 dar genes in Drosophila. All five of the dar genes that we have cloned so far have mammalian homologs. Remarkably, three encode components of the secretory pathway. These results reveal the preferential role of endoplasmic reticulum–Golgi trafficking and Golgi outposts in dendrite arborization.\n4. The maintenance of dendritic fields. Our genetic screen revealed specific mechanisms that ensure maintenance of dendritic arbors. We found that the tumor-suppressor Warts (Wts), one of two NDR (nuclear Dbf2-related) family kinases in Drosophila (the other being Trc), and the Polycomb group of genes are required for the maintenance of class IV da dendrites. Loss-of-function mutants of any of those genes cause a progressive defect in the maintenance of dendritic tiling, resulting in large gaps in the receptive field. How are establishment and maintenance of dendritic fields coordinated? In Drosophila class IV neurons, the Ste-20–related tumor-suppressor kinase Hippo (Hpo) can directly phosphorylate and regulate both Trc, which functions in the establishment of dendritic tiling, and Wts, which functions in the maintenance of dendritic tiling. How Hpo regulates the transition from establishment to maintenance of dendritic fields remains to be determined.\n5. The remodeling of dendritic fields. Drosophila class IV da neurons undergo dramatic remodeling during metamorphosis. Early in the pupal stage, those neurons prune all their dendrites. Later each neuron grows a completely new dendrite for adult function. While the dendrites are being remodeled, the axons stay largely intact. We have begun to identify the molecular mechanisms that control this large-scale, dendrite-specific remodeling.\nExtension of Our Drosophila Work to Dendrite Development of Mammalian Central Neurons\nThe great majority of the genes found to affect Drosophila dendrite development have a mammalian homolog(s). In several cases, we have shown that those homologs (for example, Dasm1, Dar3/Sar1) have similar function in regulating dendrite development in the mammalian central nervous system. We will continue to extend our findings about dendrite development in Drosophila to the mammalian nervous system by studying cultured hippocampal neurons or knockout mice.\nGrants from the National Institutes of Health provided partial support for our research.\nMoore, A.W., Jan, L.Y., and Jan, Y.N. (2002). hamlet, a binary genetic switch between single and multiple dendrite neuron morphology. Science 297: 1355-1358.\nShi, S.-H., Jan, L.Y., and Jan, Y.-N. (2003). Hippocampal neuronal polarity specified by spatially localized mPar3/mPar6 and PI 3-kinase activity. Cell 112:63-75.\nGrueber, W.B., Jan, L.Y., and Jan, Y.N. (2003). Different levels of the homeodomain protein Cut regulate distinct dendrite branching patterns of Drosophila multidendritic neurons. Cell 112:805-818.\nEmoto, K., He, Y., Ye, B., Grueber, W.B., Adler, P.N., Jan, L.Y., and Jan, Y.N. (2004). Control of dendritic branching and tiling by the Tricornered-kinase/Furry signaling pathway in Drosophila sensory neurons. Cell 119:245-256.\nParrish, J.Z., Kim, M.D., Jan, L.Y., and Jan, Y.N. (2006). Genome-wide analyses identify transcription factors required for proper morphogenesis of Drosophila sensory neuron dendrites. Genes Dev. 20:820-835.\nKuo, C.T., Zhu, S., Younger, S, Jan, L.Y., and Jan, Y.N. (2006). Identification of e2/e3 ubiquitinating enzymes and caspase activity regulating Drosophila sensory neuron dendrite pruning. Neuron 51:283-290.\nEmoto, K., Parrish, J.Z., Jan, L.Y., and Jan, Y.N. (2006). The tumour suppressor Hippo acts with the NDR kinases in dendritic tiling and maintenance. Nature 14:210-213.\nKuo, C.T., Mirzadeh, Z., Soriano-Navarro, M., Rasin, M., Wang, D., Shen, J., Sestan, N., Garcia-Verdugo, J., Alvarez-Buylla, A., Jan, L.Y., and Jan, Y.N. (2006). Postnatal deletion of Numb/Numblike reveals repair and remodeling capacity in the subventricular neurogenic niche. Cell 127:1253-1264.\nParrish, J.Z., Emoto, K., Jan, L.Y., and Jan, Y.N. (2007). Polycomb genes interact with the tumor suppressor genes hippo and warts in the maintenance of Drosophila sensory neuron dendrites. Genes Dev. 21:956-972.\nYuh-Nung Jan earned his undergraduate degree in Physics from National Taiwan University before pursuing a PhD in Biology at California Institute of Technology. He completed subsequent postdoctoral research at both Caltech and Harvard Medical School before joining the Physiology Department and Neuroscience program at UCSF.\nDr. Jan is an elected member of the National Academy of Sciences, Acedemia Sinica Taiwan, and the American Academy of Arts and Sciences, as well as earning a Distinguished Alumni Award from Caltech and a Javits Neuroscience Investigator Award from the National Institutes of Health.']"	['<urn:uuid:b3971ec3-0d03-4f09-a273-ab77efb8cdcc>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	11	31	1427
93	first black poetry book major publisher	Countee Cullen's book 'Color' (1925) was the first book of poetry written by an African American to be published by a major American publishing house (Harper & Brothers) since Paul Laurence Dunbar.	['Welcome to Week 2 of BlaPoWriMo!\nFor the uninitiated, Black Poetry Writing Month (BlaPoWriMo) is a month-long writing challenge that combines the ambition of National Poetry Writing Month (NaPoWriMo) with the history, education, and self-reflection of Black History Month.\nThis year, we are going on a journey through the eras of black history and poetry.\nHow did you enjoy your first week of writing slavery inspired poems? I’ll admit some days were harder to write than others. Having to open those wounds that run deeper than the generations can be troubling. The fact that human beings could commit such atrocities against other human beings, often using Christianity as a defense of their actions, is still baffling to me.\nIf you want to continue writing poetry inspired by the era slavery, feel free to do so! The themes I provide each week are only optional. Just remember to tag your posts BlaPoWriMo, so I can find them and give you a shoutout!\nNow, let’s skip ahead to the next era: The Harlem Renaissance!\nI know. I’m brushing over about 50 years worth of great literature from the post-Civil War / Reconstruction era, but I need to leave something to be desired for next February, right? 😉\nIf you want to do your own research on the poets of this era, one of the most well-known was Paul Laurence Dunbar. (His poem “Sympathy” inspired Maya Angelou’s I Know Why the Caged Bird Sings.) Another poet from this era was James Weldon Johnson, who wrote “Lift Every Voice and Sing,” also known as the Black National Anthem.\nThe Harlem Renaissance took place during the 1920s and lasted until about 1940.\nThe Harlem Renaissance was like a great awakening of art and literature for Black Americans. During this period, Black America saw a cultural explosion of creativity. It’s center was in Harlem, a district in New York City, though it wasn’t limited to black writers and artists living in New York. In fact, it inspired a similar movement known as Négritude among French-speaking blacks across the African diaspora, including in France, the Caribbean, and the West Indies.\nWhile the era of slavery can be described as a time when most whites questioned the intelligence of blacks (remember those authentication papers written for Phillis Wheatley), the Harlem Renaissance was the confirmation of black intelligence, black excellence, black achievement, black genius, and all the above. There was an outpouring of publications not only in poetry, but also in the genres of fiction, drama, personal essay, music, dance, and visual art.\nThe Harlem Renaissance laid the ground work for black expression.\nThe creativity of black writers and artists of this period was driven by a sense of purpose, the artists using their craft to express a response to social conditions and to proclaim their dignity and humanity in the face of poverty and racism.\nThe most famous poet of this time period is of course Langston Hughes. Others include Countee Cullen—whose Color (1925) was the first book of poetry written by an African American to be published by a major American publishing house (Harper & Brothers) since Paul Laurence Dunbar—Jamaica-born Claude McKay (though he wasn’t the first “African American,” his Harlem Shadows was published in 1922 by Harcourt, Brace), Anne Spencer, Jean Toomer, Sterling A. Brown, and many more.\nTo get you started with this week’s Harlem Renaissance-themed poems, I won’t share a poem by Langston Hughes. No, that would be too easy. Everyone knows Langston Hughes! Instead, I’m giving you this poem by the lesser-known Anne Spencer to inspire you for this next week of writing. Of course, if you need further inspiration, feel free to look up the other poets mentioned in this post.\nMost things are colorful things—the sky, earth, and sea.\nBlack men are most men; but the white are free!\nWhite things are rare things; so rare, so rare\nThey stole from out a silvered world—somewhere.\nFinding earth-plains fair plains, save greenly grassed,\nThey strewed white feathers of cowardice, as they passed;\nThe golden stars with lances fine\nThe hills all red and darkened pine,\nThey blanced with their wand of power;\nAnd turned the blood in a ruby rose\nTo a poor white poppy-flower.\nThere are many different versions of this poem online. The above version came from the Vintage Book of African American Poetry (2000), edited by Michael S. Harper & Anthony Walton\nSo, are you ready for BlaPoWriMo?\nYou don’t have to be black to participate. This is not a space for discrimination but education. As long as you write a poem every day this month and your poem aligns with the theme for the week or focuses on blackness/race in general, there’s no reason not to join!\nBe sure to add your links to the prompt posts for the week (ex. link your “Harlem Renaissance” poems to this post) so others can read your poem. You can also tag your posts BlaPoWriMo so we can find you in the WordPress Reader.\nBy the way, I’m on Twitter! I previously created a separate account for BlaPoWriMo, but that became too much of a hassle, so follow me @Nortina_Mariela and tweet the hashtag #BlaPoWriMo. I’ll be retweeting your tweets all month long!\nWill you join the challenge this month? I’m excited to see the poems you create!\nHappy Black Poetry Writing Month!']	['<urn:uuid:f5ab1160-f749-4790-8b20-69a7e357a87b>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	6	32	892
94	I'm a new charity trustee trying to understand the balance between oversight and management. How can trustees properly discharge their duties without becoming too involved in operational management?	Trustees must properly discharge their duties without simply rubber stamping decisions, while also not becoming over-involved in operational management which could harm their critical perspective. The correct balance is achieved when individuals or teams have a clear understanding of responsibilities, the necessary authority to fulfill these responsibilities, and accountability for the consequences of their actions or inactions.	"['The Financial Reporting Council’s new UK Corporate Governance Code [PDF] was published earlier this week. Reading it I think there are matters that resonate for charities.\nThere is obvious common ground between the roles of the board in all sectors, but charity boards are invariably made up entirely of non-executives who are not usually involved with management. There are also different perspectives of ownership, contribution and management. For example, trustees of a charity do not own what they work for; individuals guide, support and influence with the key aim of acting in the best interests of the organisation overall, rather than a constituency or grouping. Importantly, this should not fetter those who work for the organisation or those for whom it is working, nor should it deny the flexibility that is needed if activities and services are to adapt to changing needs.\nSection 177 of the Charities Act 2011 defines charity trustees as ""the persons having the general control and management of the administration of the charity"". This definition sometimes causes confusion because of the concept of it involving management and administration. It is important to recognise that what is intended is that trustees need not actually carry out the management and administration, but be responsible for them. Boards are also required to make the appropriate level of challenge and make decisions on key issues.\nWith most large non-profits, the board has had the foresight to appoint professional management to manage the organisation. This sometimes causes a dilemma. A charity trustee’s position stands in contrast to that of the non-executive director of a company in the private sector, who shares their legal responsibilities with their executive colleagues on a unitary board. In some ways, the greater the competence and professionalism of management, the greater the challenge in understanding the issues of governance and management. Trustees must properly discharge their duties without being seen to simply rubber stamp, but at the same time they should not become over-involved in operational management to the detriment of their critical perspective.\nI have carried out many governance reviews and getting the balance right can be challenging. The correct balance will be achieved only when individuals or teams have a clear understanding of responsibilities, the authority necessary to fulfil these responsibilities and the accountability for the consequences of what they have done or failed to do. This accountability is required not just from management but also from boards. A key question is how the board and management are held to account for ensuring that they foster and indeed enforce the right culture and behaviour.\nThere is much more to it than ensuring compliance with standard policies and procedures, and the FRC’s updated corporate governance code now places much emphasis on culture and behaviour. It explains that ""the board should assess and monitor culture. Where it is not satisfied that policy, practices or behaviour throughout the business are aligned with the company’s purpose, values and strategy, it should seek assurance that management has taken corrective action."" The charity governance code espouses similar requirements.\nAt a Crowe charity breakfast briefing called ""Culture and Behaviour Trumps Policies and Procedures"" there was widespread agreement that board members often wrongly take for granted that the right culture and behaviour permeate through the organisation. Boards need to take time to ensure that organisational culture or values are discussed as part of the formal board agenda. Ethical dilemmas need to be discussed and the decisions should be carefully reviewed. Sometimes this needs to be a theoretical discussion in anticipation of what might go wrong – for example, the risk that organisational or personal performance indicators can lead to or incentivise inappropriate behaviours. An important question for boards is how incidents of inappropriate behaviours or unwanted culture are recorded, monitored and dealt with. The position in the past was that proper accountability and transparency led to unwanted publicity that should be avoided. This approach is no longer seen as appropriate and can lead to damaging consequences.\nThis article first appeared in Third Sector.']"	['<urn:uuid:ce153572-ab9b-4e97-932a-0110522224e8>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:57:18.859842	28	57	670
95	need program industrial tank water level monitoring	The tank control system uses level sensors (LS1 for top tank high level, bottom tank high level, and LS2 for bottom tank low level) along with start/stop pushbuttons. The pump only operates when two conditions are met: the level in the bottom tank must be above the low level, and the level in the top tank must be below high level. There are indicator lamps to show pump operation status and water level errors.	['Assignments on PLC Ladder Programming\nA) Simple PLC ON-OFF Instructions:\n- Develop OR, AND, NOT, NOR, NAND, and EX-OR logic using two toggle switches and output is given to a pilot lamp.\n- A fan is started and stopped from three different locations. Each location has a start and stop pushbutton.\n- Develop a ladder program for a motor with START and STOP pushbuttons. Green lamp should be ON, when motor is running and motor should stop suddenly when bearing temperature is high, with red lamp indication.\n- In a certain bank, each of three bank officers has a unique key to the vault. The bank rules require that two out of the three officers be present when the vault is opened. Develop the ladder program that will unlatch the door and turn on the light when two of the three keys are inserted.\n- Draw the ladder program for following application. There are four hazardous inputs to the alarm system that goes ON when a fault occurs. The sequence is,\n- If any one fault occurs, nothing will happen.\n- If any two faults occur, Red lamp will be ON.\n- If any three faults occur, Buzzer will be ON.\n- If all four faults occur, send alarm to fire department.\n- On the automatic lathe machine, the carriage ( tool-post ) should move in between headstock and tailstock continuously. ( i.e. reciprocating motion of carriage). Use limit switches to detect the positions.\n- The elevator system employs a platform to move object up and down. The objective is that, when ‘up’ pushbutton is pressed, the platform carries a material to ‘up’ position and when ‘down’ pushbutton is pressed, the platform moves to down position. Use limit switches for highest and lowest position and two relays to change the direction of motor.\n- Develop ladder program for bottle filling station having sequence as follows:\na)Start the conveyor.\nb)When bottle is in position, stop the conveyor and open solenoid valve.\nc)When bottle is full that is detected by optical proximity switch, close the solenoid valve.\nd) Go to step 1 for next bottle.\n9. Simple on – off program.\nRequirements: there are two buttons & three lamps. (Red, Blue, Green)\n- When PB 1 is pressed then only Blue & Green lamp on.\n- When PB 1& PB 2 pressed simultaneously, then only Red lamp on.\n- When PB 2 pressed then only Green lamp on.\n10. Pump on / off programme.\nThere are four push buttons\nPB 1: Pump one start.\nPB 2: Pump one stop.\nPB 3: Pump two start.\nPB 4: Pump two stop.\n- Two feedbacks (for pump one & pump 2) from contactor, which in turn makes pump on.\n- Two lamps (Green lamp for both pumps on, red lamp for one or both pump off / trip) and two relays (ready for pump 1, relay for pump 2)\n- When PB 1(pump 1 start push button) is pressed then Pump one start. When PB 2 (pump 1 stop push button) pressed then pump1 stop.\n- Similarly When PB 3 (pump 2 start push button) is pressed then Pump 2 starts. When PB 4 (pump 2 stop push button) pressed then pump 2 stop.\n- When both pumps are on the green lamp is on. If pump 1 or pump 2 ON but after 1 sec. Feedback not ON then red lamp starts flashing.\n11.Staircase lamp ON / OFF\n- There are two push buttons PB1 & PB2. PB1 is fixed at ground floor and PB2 is fixed at upper floor of staircase. A lamp is fixed at middle position.\n- When PB1 is pressed, lamp should get ‘ON’, if lamp was in the OFF state otherwise the lamp should go OFF, if the lamp was in the ON state.\n- The same thing must happen when the PB 2 is actuated.\n- In brief, ON & OFF the states of the lamp must be achieved alternatively using any push button whenever they are actuated.\n12. Tank controller\n- There are two pushbuttons (PB1 pump start, PB 2 pump stop), level sensors (LS1 top tank high level, bottom tank high level, LS 2 bottom tank low level). Two lamps (L1 pump on indicator, L2 water level error) & 1 relay for pump on.\n- When pump start PB is pressed then gets ON & pump on indication glows. If pump stop PB is pressed then pump is off.\n- There are two conditions for starting of pump.\n- Level in bottom tank is above low level\n- Level in top tank is below high level.\n13.There is one push button (PB 1) & one lamp (L1). First time when push button (PB1) is pressed then lamp becomes on. Next time when the same push button is pressed then lamp goes off.\n14. Star –delta connection.\nThere are only 2 push buttons. Start push button & Stop push button. System operates only in Auto Mode.\nWhen start push button pressed main contactor will get energized timer also get started. Initially star connections will come into picture as motor fetches extracts current. Then rate current limit for its smooth functioning. This is only for the first few seconds. Once motors starts. On the done bit of timer circuit switches the delta connection.\n15. Direct ON line start\nThere are push buttons, start PB, Stop PB System operates in only in one mode.\n1. Auto mode\nA) In initial condition for Direct on line, stop button is not pressed.\nB) Start button is also not pressed.\nC) When start push button is pressed. Corresponding output gets ON.\nD) In this step we used to latch the output so that respective action of start push button can b e avoid.\n16. Pump Automation.\nThere are two push button start PB, Stop PB. There are one switch Auto / manual switch.\nSystem operates in 2 modes\n1) Auto mode\n2) Manual mode\n- Initially selector switch is ON. Auto mode is selected; both push buttons are not pressed. Cycle is not going to start.\n- as start push button is pressed & system is on auto mode cycle starts, if tanks to be filled are less than 5 only pump 1will be ON. If tanks increase up to 5 or grater than 5 than pump 2 will starts. Fault lamp blinks when any problem is their during a run of pump 1 or both (pump 1 & pump 2) in response to action to fault lamp is given by starting of pump 3 which is stand by pump.\n- Initially all three pumps are off. Usually this mode is used to isolate a problem in any of part of the system.\n- In this mode we can start or stop all the pumps (including stand by pump) or in any of the combination. There are no any restriction of system interlock while making pump ON or OFF in manual mode.\n- I.e. user can run a system according to the time & requirement.\nB) Assignments on Timer instructions :\n- A small heating furnace has two heating elements that are energized in two stages, 3 minutes apart. That is when the furnace is turned ON, the first heating element comes on right away, and the second element comes on 3 minutes later. A temperature switch will shut down the furnace if it is too hot. Draw the ladder diagram for control circuit.\n- Develop a ladder program for sequencing for three motors of 0.5 HP, 1 HP and 3 HP. The expected sequence is 3-1-2. While starting these motors, provide delay of 60 seconds. For the safety of the systems, every motor has to satisfy the following interlocks,\n- Bearing temperature high for all three motors.\n- Winding temperature high for all three motors.\n- Over current protection high for all three motors.\n- A batch process, which involves filling a vat with a liquid, mixing the liquid, and draining the vat is automated with a PLC. The specific sequence is as follows, when start pushbutton is pressed,\n- An inlet valve opens and lets a liquid into vat until it is full.\n- The liquid in the vat is mixed for 3 minutes.\n- A drain valve opens and drains the tank.\n- We are provided with two pushbuttons and one lamp. Develop ladder program to flash a lamp, when start pushbutton is pressed (flashing rate: 2 sec.) .the lamp should be OFF upon pressing stop pushbutton.\n- Explain the working of CSTR and develop ladder diagram for CSTR. Draw the process diagram. Indicate clearly inputs and outputs. Write down the sequence of events. Mention the assumptions made if any.\n- You are required to develop a ladder diagram for pick and place type bottles on conveyor belt system.\n- Draw process flow diagram\n- List inputs and outputs\n- List the instructions.\nC) Assignments on Timer-Counter Instructions:\n- Develop a physical ladder for an automatic washing machine with front-loading system. Assume suitable washing sequence.\n- There is one selector switch. (Cycle starts) one push button, one pump & two lamps (red & green)\n- When selector switch is ON then pump gets on & after 6 seconds, pump becomes OFF, after two seconds pump again becomes on, which continues.\n- If pump gets on, and Off 5 times then cycle stops & cycle complete indication is on.\n- When cycle starts then cycle start indication is ON.\n- To restart operation cycle first press reset push button & then makes selector switch on.']	['<urn:uuid:83abd18d-cc2c-48c9-b540-6a0783b83276>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:57:18.859842	7	74	1595
96	difference between true knowledge and half knowledge	True knowledge represents your actual area of expertise - things you thoroughly understand and can successfully work with. Half-knowledge refers to areas where you think you know something but lack deep understanding, making it risky to operate there. For example, Rose Blumkin deeply understood furniture retail and cash (true knowledge) but avoided stock trading since she only had surface-level understanding of it (half-knowledge). Operating with half-knowledge carries more risks than benefits and is not sustainable long-term.	['Nobody can beat you at being you.Naval\nWhat would you do if money wasn’t an issue? Do what you love rather than what’s profitable? Work less and play more? Quit the rat race and take more control of your life? The Circle of Competence and the philosophy to escape competition through authenticity are two ideas that suggest we don’t have to make that choice. Because we wouldn’t want to.\nWhile the Circle of Competence was developed by investor legend Warren Buffett in the 1990s, the principle of escaping competition through authenticity was proposed by angel investor Naval Ravikant almost thirty years later. Let’s explore and combine the two philosophies to figure out how we might transcend the daily grind.\nWhat Is the Circle of Competence?\nThe Circle of Competence is the unique area of expertise each of us has built through study and experience. An individual’s core competency so to speak. It was coined by Warren Buffett alongside his long-time partner Charlie Munger. The investor explained the concept best in one of his shareholder letters from 1996:\nWhat an investor needs is the ability to correctly evaluate selected businesses. Note that word “selected”: You don’t have to be an expert on every company, or even many. You only have to be able to evaluate companies within your circle of competence. The size of that circle is not very important; knowing its boundaries, however, is vital.Warren Buffett\nPut differently, the idea is to never invest in a business we know we don’t understand. Or, framed positively, to only invest in a business we’re positive we do understand. What doesn’t matter so much is what we’re competent in. Arguably, this credo is part of what made his investment firm Berkshire Hathaway so successful.\nUnsurprisingly, Buffett’s mental model has its origins in his very own Circle of Competence, the world of stock trading. Though, it can certainly be applied beyond his field of expertise, as the ‘Oracle of Omaha’ illustrates in an often-cited example.\nBetween Knowledge & Half-Knowledge\nMeet Rose Blumkin, a Nebraskan retail legend who used to be one of Buffett’s business managers. In a lecture, Buffett recalls “Mrs B’s” immigrant success story as a case in point for the Circle of Competence, a model similar to the framework of knowing and not knowing:\nI couldn’t have given her $200 million worth of Berkshire Hathaway stock when I bought the business because she doesn’t understand stock. She understands cash. She understands furniture. She understands real estate. She doesn’t understand stocks, so she doesn’t have anything to do with them. If you deal with Mrs. B in what I would call her circle of competence… She is going to buy 5,000 end tables this afternoon (if the price is right). She is going to buy 20 different carpets in odd lots, and everything else like that [snaps fingers] because she understands carpet. She wouldn’t buy 100 shares of General Motors if it was at 50 cents a share.Warren Buffett\nWhat We Know\nBuffett attributes Mrs Blumkin’s success to her understanding of her field of expertise (What We Know). Coming to the United States, Mrs B specialised in what she had come to be most knowledgeable about. Furniture retail may lack prestige or be considered dull by some (or many). Though, that didn’t keep Madam Blumkin from building a successful business. There was no need for her to start from square one by becoming knowledgeable in seemingly more profitable markets.\nEssentially, we can define competence as not only understanding relevant facts or having experience. But knowing how to do something well and successfully so we achieve the desired outcome. We’re competent in many mundane fields, such as riding a bike. But we’ve also built a unique set of skills through our individual circumstances, experiences and personal choices. Whether it’s playing an instrument, a passion for martial arts or catching orbs.\nWhat We Think We Know\nThose choices seem to be where the wheat is separated from the chaff. Choosing to buy General Motors stocks would be an example of Mrs B acting on half-knowledge (What We Think We Know). That is not to say that her Circle of Competence could not organically broaden over time. But it is to say that forcing yourself into the speculative circle carries more risks than benefits. While Mrs B might have made a good deal once or twice by sheer luck, it wouldn’t have been a sustainable long-term strategy for her.\nWhat We Don’t Know\nThe risk increases when we stray even further from our Circle of Competence. We end up in the territory of What We Don’t Know and act on unconscious incompetence. Sure, we could reinvent ourselves completely; redefining our Circle of Competence. However, in Buffett’s reading that would be an unnecessary and risky process. There’s an aspect of control to it, too. Staying in our wheelhouse means we’re less surprised by how a decision might turn out. And if we are, our skills and experience enable us to problem-solve and get it under control.\nStaying Within Our Circle of Competence\nKnowing the boundaries is one thing, respecting them is another. Mrs B seemed to have the foresight and discipline not to concern herself with opportunities outside of her abilities. No matter how profitable they seemed. So yes, we should expand our expertise to seize on opportunities. We just should do so gradually.\nThe Peter Principle, the semi-satirical idea that in a hierarchy everyone rises to their level of incompetence, illustrates this point quite clearly. Success inevitably tempts us outside of our competence circle, eventually compromising our careers. According to Laurence J. Peter, it requires character and chutzpah not to be promoted to a position for which we’re utterly unqualified.\nThe smart thing to do — so the author’s tongue-in-cheek suggestion — is to avoid this form of Mission Creep by clinging to what we’re good at while pretending we’ve already reached our full potential. In order to not be pushed outside of our Circle of Competence, we must act against our impulses, resist outside pressure and game the system. What makes it so difficult then?\nThe Circle of Incompetence\nThere’s good reason to dismiss Buffet’s mental model as luxury. We often don’t seem to have a choice but to work in fields where we don’t really know what we’re doing. Perhaps we were looking for a challenge, or it was a necessity. Working for an organisation can be quite performative and require us to act against our very nature. (I’ve previously written about how some jobs can easily feel like a pointless game of Chmess.)\nBut playing outside our Circles of Competence artificially inflates our choices. At the same time, we lack the necessary information to make good decisions. Decision fatigue sets in, which leads to even more bad calls. Plus, there’s a good chance we compete with people whose job has become their second nature. This brings us to the question of how exactly we get to know and understand our Circle of Competence.\nEscaping Competition Through Authenticity\nCompared to Buffett, Naval’s approach seems very similar. Although with his thoughts on competition and authenticity, Ravikant takes a more general approach to life and dives deeper into the question of how we might find out who we’re supposed to be. It complements the Circle of Competence, taking it into the 21st century.\nThe philosopher starts with the premise that we tend to become what people around us do for a living. We emulate others. Our parents, our peers, or any successful role model for that matter. (If Mrs B was so successful with furniture, maybe I should try that, too?) The result is similar to where we end up when operating outside our Circle of Competence. Our chances of winning competitive games diminish. Here’s what Naval suggests instead:\nThe best way to escape competition — to get away from the specter of competition, which is not just stressful and nerve-wracking but also will drive you to the wrong answer — is to be authentic to yourself.\nIf you are building and marketing something that’s an extension of who you are, no one can compete with you.Naval\nAccording to Naval, a prime example of this dynamic is Joe Rogan. The comic and podcaster has transcended any competition by building his personal brand. Everyone can create a podcast, but no one can recreate The Joe Rogan Experience. The rise of the internet is what has turned this previously “useless advice” into a viable career option for many. The web has massively expanded the potential audience and made products more scalable.\nIn sum, the angel investor argues that one of the keys to creating wealth is to refrain from copying others and turn our unique authentic selves into the product.\nHow to Be Authentic\nAdmittedly, authenticity is a bit of a nebulous concept. We all seem to strive for it, but it’s hard to grasp what it actually is. Philosophers such as Alan Watts have long pointed out its self-contradictory nature. The whole of Zen, for example, is built around the idea of being in an authentic state by acting as if we and the environment were one being. This can be the case when two people are dancing and it’s impossible to tell who leads and who follows.\nParadoxically, so the Zen way goes, mastering life in such a way can only be achieved by showing that the real you is a mere abstraction. This is demonstrated by asking self-conscious students who they were before their mother and father conceived them or by simply telling them to be themselves. Usually, the request has the effect of alienating them from the group rather than making them more at ease.\nInterestingly enough, the scientific study of authenticity has arrived at a similar point. In his article Authenticity under Fire, humanistic psychologist Scott Barry Kaufman explains the importance of social integration for feeling true to ourselves. It’s not when we feel uniquely different that we think we’re most authentic. It’s when others see us the way we want to be seen and our reputation aligns with the one we aspire to.\nObviously, authenticity is not something others can decide for us. But the difficulty of measuring authenticity scientifically through self-reporting also suggests that being authentic is not something we can decide all by ourselves either. We just don’t know ourselves that well, which ties in nicely with how Naval suggests we can find our true selves.\nIdentifying and Calibrating Our Niche\nAccording to the investor, there are two aspects worth paying attention to:\nAs you go through your career, you’ll find you gravitate towards the things you’re good at, which by definition are the things you enjoy doing. Otherwise, you wouldn’t be good at them. You wouldn’t have put in the time.\nOther people will push you towards the things you’re good at, too. Because your smart bosses, co-workers and investors will realize you’re world-class in this one thing. And you can recruit people to help you with other things.Naval\nNaval’s first point has to do with ourselves. While who we are is not readily available knowledge, we have to start somewhere. Initially, this somewhere may very well be the wrong place. But we can keep track of life’s error messages and the things our minds keep circling back to. This fits in with the immigrant story of Mrs Blumkin who ended up building on her knowledge about cash and retail.\nThere’s also a collaborative aspect to it. Remember how Buffett describes recognising Mrs B’s skills, talent and potential. He didn’t try to force his own Circle of Competence on her or otherwise put her on the spot. Instead, both seemed to have an understanding of what she knows best. It’s worth listening to family, friends and teachers to clue in on where we should be heading. From an outside perspective, they can often better recognise what we’re drawn to.\nOn a final note, Naval makes clear that we all have more than one set of skills and proclivities. The goal is to identify those we gravitate towards and then calibrate them to find a viable product-market fit. We might also specialise even further within a field of expertise, finding that niche within our niche. The closer we come to the core of our Circle of Competence, the higher resolution the picture of our authentic selves becomes.\nWhat part of your job would you still do if you weren’t paid? If only money wasn’t an issue, we might think, we could find our Circle of Competence and finally be ourselves. However, consolidating the Circle of Competence with Naval’s philosophy suggests that authenticity is not a fixed state. It’s the pursuit of a long-term goal.\nThis is reminiscent of the Ship of Theseus paradox and the idea that our identities are more akin to a transformative spirit. The more we gravitate towards authenticity as the core of our Circle of Competence, the less we have to choose between doing what we love and what’s profitable. Granted, there’s no guarantee for success. But it can make success more likely.\nSo in a sense, authenticity is an escape route toward the familiar. Even though I don’t think we can escape competition entirely. We need some form of competitive struggle. But with the two philosophies in mind, the only person who can beat you at being you is yourself.']	['<urn:uuid:778a4321-4de2-4cb2-a1b4-71e223dc9777>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:57:18.859842	7	76	2235
97	How did the early settlers change the desert landscape when they first arrived to what is now known as Alpine, Texas?	The people who came to settle transformed the harsh desert into a small forest oasis. Using well water and rainfall, they were able to grow many species including fruit trees, non-natives and desert species that usually are found near rivers, creeks, or springs.	['Historic photos of Alpine from the late 1800s, when the town was still known as Murphyville, show a dusty desert outpost on the newly constructed railroad. Before the railroad, except for a few springs in the vicinity and spots along the creek bed, Alpine was another sunbaked piece of Earth in a wide desert expanse.\nAs the town became the county seat and an economic center for the Big Bend region, the people who came to settle here transformed the harsh desert into a small forest oasis. Supported by well water in addition to rainfall, many species—fruit trees, non-natives and desert species usually found near rivers, creeks, or springs—thrive in the high altitude yet temperate climate of the Texas “Alps.”\nHigher altitudes often receive more rainfall released from clouds building and breaking against the mountainous land masses reaching into the sky. In addition, a recently developed theory of a “biotic pump” argues that the level of rainfall in a region is influenced by trees and forests. Where previously it was believed wind currents and rainfall patterns are influenced entirely by changes in temperature, the biotic pump theory proposes that forests and trees also influence wind currents and attract rainfall. The atmospheric implications from the moisture generated and held by forests are possibly a driving force behind the winds directing precipitation.\nRegardless of whether or not the increasing tree cover of Alpine has attracted more rainfall, it is certain that these trees are catalysts for life. What moisture is in the air, they absorb. The shade they provide cools the earth and slows evaporation. Their leaves and the birds, insects, and reptiles that trees attract add nutrients to the soil. A tree’s roots and canopy form a matrix where rainwater is absorbed instead of running off as often occurs in the open desert.\nHere is a look at some of the trees of Alpine, the town’s long standing residents whose presence are part of the beauty, refuge, and quality of life found in this quiet mountain town.\nAn Elm tree has taken up residence in a narrow alley between two downtown businesses on Holland Ave. This opportunist tree is found throughout Alpine, popping up in areas where rainfall is diverted. In the late summer months, a parasitic insect devours the foliage, but these trees are quick-growing and leaf out again each spring.\nRio Grande Cottonwood\nAt its base, the trunk of this giant is 26 feet around. While most species of cottonwood have an average life span of around 50 years, some Rio Grande Cottonwoods are known to be older than 200, and still growing. In an area of Alpine where there existed an historic spring and the ground water is close to the surface, this ancient tree’s roots have been wet for likely over a century.\nAlso known as the Naked Indian, Lady’s leg, Texas Arbutus or Madrono, this evergreen is native to the Trans-Pecos region. In its youth the Texas Madrone dons a papery, dark brown bark which as it ages is shed to reveal a smooth and pale bark which ranges from white to orange, and apricot to crimson. This tree is typically found in canyons and rocky slopes between 4,500 and 6,500 feet.\nAlso known as Retama, Jerusalem Thorn, and Tree of Life. Palo Verde means “ green stick” in Spanish and refers to the tree’s distinctive bark. The seeds are edible, and its leaves have been used by Native people for medicinal purposes. The Palo Verde is native to the Big Bend region, but is usually found below 4,000 feet.\nA native drought-resistant tree, the Mesquite has for thousands of years provided a source of nutrition for indigenous people in the American Southwest and Northern Mexico. The sweet pulp of the seed pod can be dried and ground, or eaten fresh. Some cultures grind the seeds into a flour. Mesquite beans reach maturity in late summer/ early fall, when they can be collected, dried, and processed.\nA native of the southwest, it is not a true willow but rather a relative of the trumpet vine. Every part of the desert willow was utilized by native peoples. The flowers and seed pods can be used for food. The leaves have medicinal qualities and the bark is useful for its fiber in making cloth and twine.\nThe earliest settlers to Alpine brought fruit trees, and these food-bearing trees continue to be planted by residents. Except for occasional late freezes where crops are destroyed, it is usual to have an abundance of fruit ripening on trees around town as summer approaches. Nectarine, Apricot.Plum, Fig, Mulberry, and Apple are among varieties found in the Big Bend region.\nNot native to the Big Bend region, this tree is naturally found on the streams and river beds of East Texas to the Florida coast. This evergreen is adorned with large lemon-citronella scented flowers that can measure up to almost a foot wide. Magnolias are a part of one of the oldest flowering plant families, Magnoliaceae, which may have exited on earth for up to 100 million years. It is believed Earth’s first flowers would have resembled the Magnolia flower.\n– by Kleo Belay']	['<urn:uuid:803acb34-2521-46d8-9a66-c2d2374d39a2>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	21	43	861
98	what did ancient celts do with cows during samhain festival	During Samhain, the Celts would bring their cows back home from the summer-time pastures in the hills. The cows would then remain in enclosures for the duration of the winter, unless they were slaughtered for food.	['”Many cultures believe that on a certain day – Halloween, the Irish Samhain Eve, Mexico’s ‘Dia de los Muertos’ – the veil between this world and the next is especially thin.”\n– Michael Dirda\nOne of the most familiar Sabbats is what some might consider the Halloween pagan holiday. Of course, there are some significant differences between the Pagan celebration and the commercialized event. But, there are also some traditions the commercialized holiday shares with the way pagans celebrate Halloween today.\nOne main difference is the heavy spending occurring each year. In the United States alone, 179 billion people celebrated Halloween in 2017. On average, each person spends over $80.00 on holiday needs. The total amount of money spent for the year? (Can I hear a drumroll, please… Would you believe people in the US spent over $191 billion in 2017 alone? It’s true! But, you can rest assured knowing the ancient cultures didn’t have that kind of holiday budget!\nAncient Roots of the Pagan Halloween Holiday\nThe way Pagans celebrate Halloween varies, but many of the traditions stem from ancient practices. So, what was Halloween like in ancient times? What cultures honor the holiday and why?\nEver wonder what would happen if there were only two seasons for the whole year? The Wheel of the Year still turns. It’s true! In fact, the ancient Celts saw the yearly division in two seasons, not four. The division of the year is simple. One half of the year was the light half. The other remaining six months was dark. The existing seasons were summer and winter. The celebrations corresponding to the seasonal divisions were Samhain and Beltane. These two days are holy dates in the year where the days shift from light into darkness and vice versa.\nSamhain: The End of Summer\nThe Samhain holiday was the “marker” or “time milestone” indicating the conclusion of the summer season. Samhain marks the time when the daylight hours shorten, and night becomes longer. Samhain marks the midpoint between Autumnal Equinox and Winter Solstice. The people residing in the Isle of Man, Scotland, and Ireland celebrated the holiday. There’s some historical evidence the holiday was something people honored all the way back to Neolithic times. Some landmarks align with the sunrise and sunset on Samhain.\nSamhain is the final harvest for the year. It is the Festival of the Dead and the New Year for some Pagans. The holiday’s pronunciation is “SOW-en” or SAH-when.” The holiday, being a fire festival is a two-day celebration. It runs from sunset on Oct 31 until sunset November 1st.\nThe ancient Celts not only acknowledge the changes of light and dark during the year. The time of year was important because of agriculture and farming responsibilities. The Celts understood when it was light it was the time to prepare for the when the dark half of the year rules. Harvesting foods and preparing for winter was imperative for surviving the brutal winter.\nSamhain and Sacred Cows\nFor the Celts, there were two high fire festivals. The ancients saw Samhain and Beltane as the major Sabbats and Imbolc and Lammas as the minor Sabbats. The two high Sabbats have traditions surrounding the care of cattle. During the summer, Celts brought the cows to the hills where they had summer-time pastures. When the dark half of the year begins at Samhain, they brought the cows back home. It is here in enclosures the cows remain for the duration of the winter if the Celts did not slaughter them for food.\nThe Celts had ritualized the practice of bringing the cows out to pasture and home again. When Beltane time arrives, the cows receive a blessing through ritual practices. They would have the cows pass through two fires. But, why did they do this ritual?\nThe cows are an animal the Celts consider as lunar creatures. Cows offer nourishment through milk, cheese, and butter. The nourishment the animal gives aligns it with the Mother Goddess and water element.\nThe Celts also saw the elements of water and fire as magical. They held the belief the two elements were what created the entire world. By passing the cows through two sacred fires, the Celts believe magick is afoot. It is the act of bringing fire and water together. Doing so was to work magick for change, creation, and transformation.\nAncient Samhain Traditions & Beliefs\nIn ancient times Halloween is known as Samhain. Other names for the holiday include Sauin, Samhainn, and Samhuinn. There is a connection between modern-day Halloween and Samhain. The event is one the Celts consider a sacred day as it marks the division of the year. The Celts had several traditions NeoPagans include in their Sabbat celebrations too. They would honor the day with ritual practices, feasting, divination, and guising.\nAos Sí: Visiting Spirits from the Otherworld\nAncient myth tells us that Samhain is a time when Spirits roam the world free. They can cross over from one plane of existence to the other because the veil thins or lifts. The Celts believed that both fairies and spirits can crossover from the other side. These spirits are the Aos Sí. It was an appropriate time to satisfy the Aos Sí and pagan deities. It was necessary to ensure the health of the livestock during the cold winter season. It was a normal practice to leave out offerings for the spirits.\nThe Dumb Supper\nPagans would place beverages and food outside to satisfy the Aos Sí. NeoPagans celebrating Samhain have revived many of the old practices. There was a ritual to honor patron deities and to acknowledge the change of the seasons. Then there is often a feast. Once the circle closes, remaining libations is for spirits. An invitation to spirits is open this night. A practice called “The Dumb Supper” has practitioners putting out a dinner plate. For who is this place setting, you ask? For the deceased. During the celebratory meal, the practitioner will include some quiet time. The stillness is necessary in hopes of hearing the ancestor’s voice.\nModern Day Translation for Honoring the Ancestors Today, some pagans have ritual and feasts: here, the practice of putting out a place setting for spirit ancestors is still in practice. Others might stage a special altar dedicated to ancestors and those who have passed on. On top of the altar, images of ancestors, and perhaps small tokens or items once belonging to the ancestors can be placed.\nDuring Samhain, the mini-altar can be decorated with appropriate seasonal colors, flowers, fruits, and vegetables from the harvest, nuts, stones, crystals, and the like. Candles are lit to light the way for ancestors. The idea is to invite the spirit of ancestors to join in the festivities of the evening and to spend time remembering those who have gone before us. It’s a time when we can thank our ancestors for all they’ve done while they were alive, and all they continue to do in the way of offering protection and guidance from the other side.\nSince Samhain is the time of the thinning of the veil, it is one where modern pagans practice divination. Some feel the thinner veil makes it easier to get results from divinatory methods at this time. Other practices include spell casting and meditation, with the idea of using the powerful energies of the time to empower one’s magickal workings.\nMumming and Guising\nAs a part of ancient festivals, the Celts engaged in a practice called “mumming and guising.” It is from this practice that modern-day trick-or-treating originates. The practice involves people putting on costumes. Once in disguise, they go about reciting verses as they go from one home to another. In return, they receive food. The idea was to wear a costume to confuse the Spirits roaming free. Games and divination were also common during this Samhain celebration. Neopagans also play games. It is also a time to conduct divination sessions with an array of divinatory tools. Such tools include the Tarot, pendulum, and scrying are among the most popular.\nThe Sacred Samhain Bonfire\nThe Celts celebrated Samhain with bonfires serving as imitations of the Sun. As mentioned before, they saw the two forces of water and fire as magickal. Bonfires were lit to perform sympathetic magick. Ancient cultures belief it intensifies the power to fend off the darkness. The magick empowers the Sun and helps grow excellent crops. The fire is a purifying element seen as burning away all negativity in a person’s life. Fire scrying was not unusual on this holiday. Many NeoPagans have or enjoy a bonfire on Samhain. If not, candles are something to enjoy while invoking the power of the fire element on a smaller scale.\nThe Witches’ New Year\nIf we base our division of the year on the way the ancient Celts did, then Samhain is not only the end of summer but the beginning of the New Year. Modern Pagans and Witches consider Samhain a lot like January 1, of the Gregorian Calendar. It’s, therefore, an evening for contemplating all we’ve accomplished, and what we want for the year ahead. As we ready ourselves for the colder season, we also consider the “seeds” we want to plant when the appropriate time comes to do so.\nSamhain versus Halloween: Is There a Difference?\nOver the course of time, many of the practices stemming from Samhain rituals and traditions have melded into Halloween customs. But, make no mistake about it, there is a HUGE difference between Samhain and Halloween. The distinction is identified in the “meaningfulness” of the day.\nYes, Samhain is fun with ritual, feasts, and the present-day witches’ balls are a blast! Yes, Halloween is fun too, with trick or treating, costume parties, haunted hayrides, and the works. There’s plenty of fans of Halloween; that’s why it is widely celebrated, highly commercialized, and why there are so many fans of Jack Skellington from Tim Burton’s famed “Nightmare Before Christmas!” And, let us not forget the magic of a little peanut butter and chocolate: whoever came up with that idea definitely had some knowledge of the magical arts (wink)!\nUltimately, the big difference between Samhain and Halloween is the significance of the day. Halloween is a fantastic celebration, but Samhain is a day where Pagan’s celebrate things both religious and sacred: It is one where Pagans honor the dead, celebrate the living, and look with joy to the year ahead!']	['<urn:uuid:4b11ce29-d92e-499b-be2c-e1294ee38a67>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:57:18.859842	10	36	1741
99	what causes equipment malfunction and wood discoloration during processing lumber	Band saw malfunctions occur when wheels become misaligned, causing the blade to move during cuts and produce inaccurate measurements. Wood discoloration can be caused by two types of stains: biological (microbial) stains from fungi and chemical (non-microbial) stains, with both types being linked to slow or poor drying conditions.	"[""How to Align Your Band Saw Wheel How to Align Your Band Saw Wheel\nWith a misaligned band saw wheel, the blade will move during the cut, giving you a product that will not reach the proper measurement you need for your project. However, with a little maintenance and alignment, you will be able to fix your band saw and once again create accurate cuts.\nStep 1: Recognizing a Problem\nThe most common way to figure out if your band saw is cutting incorrectly is by inspecting the cut products themselves. Make sure that double-check each product so that you know if the piece is usable or not. If the piece varies in size at different points, your band saw wheels probably need to be cleaned, oiled or replaced.\nIf your band saw wheels are dirty, you may hear a strange noise as you run your equipment. In this case, you will need to also clean or oil your wheels.\nStep 2: Make Sure You Follow Safety First\nAs you will be removing both a blade and small parts, it is important to look after your safety. Make sure that you wear safety goggles and thick gloves so that you can avoid injury.\nAlso, it is important to make sure that the saw is disconnected from any electricity so that you do not accidentally turn on the saw while you are doing your maintenance check.\nStep 3: Initial Examination\nOnce the power has been disconnected, you must open the saw's doors to get access to the blade inside. Carefully tilt the saw so that you are able to slip your ruler or straight edge across both the lower and top wheels of your device, making sure that it is long enough to lay across both wheels at the same time.\nIt is important to note that certain models may need to have the table completely disconnected from the band saw for this maintenance step.\nStep 4: Check Your Alignment Between the Wheels and the Blade\nMaking sure that the blade is installed correctly, check the alignment between bot the top and bottom wheels and the installed blade. Double check your measurements to make sure you data is correct. It is important that there is a blade during this process, as the measurements will be off because there is no blade tension available.\nMake sure you note if there is any misalignment between the wheels and the blade. Once you know which wheel is misaligned, remove the blade and the misaligned wheel.\nStep 5: Fixing the Wheel\nMake sure that you add and attach the right amount of shims behind the wheel to make sure that both the top and bottom of the wheels are aligned correctly. Make sure that you double-check your work, then re-attach the wheel to the band saw.\nAfter the wheel has been attached, reinstall the blade and check the measurements again. If the measurements are correct, your work is done. If they are incorrect, you will need to repeat the process until it is correct."", 'Preventing Wood Stains During Kiln Drying\nDiscolourations (stains) in wood have the potential to seriously downgrade and reduce the value of lumber. Even in cases where stain is admitted within grading rules, the presence of stain can have an impact on the “perceived quality” and acceptance by the customer.\nJune 29, 2012 By Peter Garrahan FP Innovations\n|Sticker stain is related to slow drying conditions sometime after piling the green lumber and is often associated with slow drying conditions in the kiln.|\nThere are many types of stain and many points in the lumber manufacturing process where stain can develop. To cover all of the potential types, causes, and preventive measures for stain would require far more than a journal article. Most stains develop as a result of the exposure conditions of logs or green lumber prior to drying and, for that reason, there is a considerable amount of material already available on that subject. A recent special publication by FPInnovations deals extensively with the processes of stain development and identifies prevention/remediation treatments that address most of the pre-kiln drying stages. The publication is entitled “Wood Discolourations and Their Prevention” – Special Publication Number SP-50. For further information on this publication and how to order a copy visit our website at www.fpinnovations.ca and follow the link for “Publications”. This article will deal strictly with the subject of potential for stain development due to conditions present within the dry kiln.\nTypes of Stain\nThere are two basic types of stain; those we classify as biological and those that are a result of chemical reactions between naturally present compounds. These are often referred to as microbial versus non-microbial. Microbial stains include sap-stain caused by fungi that attack the sapwood portion of a log or board, decay-related discolourations (also caused by fungi), and mould-related discolourations. Chemical or non-microbial stains include brown stain in pines or hemlock, sticker stain, and interior greying or darkening in light-coloured hardwoods. Regardless of which type of stain you are dealing with, one of the common factors in virtually all stain formations is the link with slow or poor drying conditions. When slow drying conditions exist in a kiln, stain can develop quickly. This is because many of the processes associated with stain development are enhanced with the temperatures typically employed in the early parts of kiln drying.\nStain Development in a Kiln\nStain is often not detected until lumber reaches the planer mill. For that reason, the kiln-drying operation (including the kiln operator) is often the first area that is questioned. As mentioned previously, most, but not all, stain develops as a result of conditions prior to kiln drying. Therefore, it is good practice to closely monitor for the presence of incoming stain either in the logs or the green lumber. Sap-stain and mould will be readily visible on green lumber or logs. Quality-control personnel and kiln-operating staff should inspect for and document the presence of stain in incoming logs and green lumber packages. If it is found, then obviously the problem needs to be addressed at the supply end. Unfortunately, chemical discolourations such as brown stain in white pine may not be visible until after drying and therefore it may be more difficult to determine if its presence is related to the kiln-drying operations or the pre-kiln-drying operations. However, if sap-stain has been detected prior to kiln drying, there is a good possibility that some chemical discolourations will also be apparent after drying (in species that are susceptible to such discolourations).\nIf it is determined that new or additional stain has developed during the kiln-drying process, it then becomes necessary to determine the factors that have contributed to it and what can be done about it. The following is a list of possible causes of stain development in a kiln and what can be done about each.\nAs mentioned in the table below, the pattern of stain development can often help to identify the possible cause(s). For example, stain found only near the ends of boards is usually related to log handling and storage practices and NOT something related to the green lumber or kiln-drying operations. Stain found only in specific packages coming from specific areas of the kiln, however, is usually related to one or more of the issues identified below.\nIf everything is operating well, a dry kiln is usually the safest place to have your lumber. In fact, the stain prevention strategy for many mills is to minimize or entirely eliminate any storage of green lumber between the sawmill and kilns. This gets the wood into an environment where you have complete control. Following the points listed below will help make sure that your kilns are indeed a safe refuge for your lumber.\nPrint this page']"	['<urn:uuid:bec130a3-ecb0-49e3-b1ef-86582aaa6fbc>', '<urn:uuid:96441e17-8520-442b-a951-27cd37bb86f1>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:57:18.859842	10	49	1301
100	power requirements Dremel 4000 vs Polaris P2000i	The Dremel 4000 has a 1.6 amp motor which translates to 192 watts (1.6 amps x 120V), while the Polaris P2000i generator has a much higher power output with 2,000 watts maximum and 1,600 watts rated power.	"[""I teach techniques and principles for skull carving and skull art\nCourses + e-BookS\nSkull Staining Course\nThis is a 5 lesson course on how to stain animal skulls using a variety of staining techniques. I use my personal experience working with skulls to show you how you can get consistent results with your own skulls. The course will cover the following:\n- Types of Stains\n- Preparing the Skull\n- Application of Stains\n- Sealing the Skull\n- Stain Recipes\nSkull Carving Course\nSkull Carving Revealed course. Through this course you will be able to master many skull carving techniques in order to create top-notch carved skulls. This is a comprehensive course covering everything from your carving setup and tools to skull carving techniques and tricks. Whether you use a high speed rotary tool or a Dremel, the content of this course is designed to take your skill set to the next level.\nSKULL MONEY E-BOOK\nHow to set up your skull business so that you get noticed and get paid. If you are going to try and make money from skull carving or skull art, you’d better do it right! I cover the business side of selling skulls and how to set yourself apart so that you actually sell skulls and make money.\nComplete Guide to Staining Skulls e-Book\nThe Ultimate Guide to Staining Skulls + Recipes! Get step by step directions on 15+ skull-staining recipes. You will not only learn the basics of staining skulls but will be able to master staining using a wide variety of natural and commercial stains!\n- Basic skull staining techniques\n- Making natural stains\n- Typical skull staining problems\n- 15+ step-by-step examples of different stains\nHere are the tools and resources I personally use and recommend.\nSome links below are affiliate links\nThe number one tool you will need to carve skulls is some type of rotary tool. I much prefer the Dremel 4000 for carving for several reasons. The Dremel 4000 has a speed range of 5,000-35,000 rpm and a 1.6 amp motor, making it both great for carving out large areas of tough bone as well as carving finer work at the higher speeds.\nUsing some type of flexible shaft attachment to your rotary tool is really going to make carving skulls easier, as well as allowing you to do finer work. Holding a heavy rotary tool can be hard on your hands and simply does not allow for the same free movement that a flexible shaft will. I used the Dremel brand Flex Shaft with my Dremel 4000.\nAn adjustable chuck is kind of an undiscovered genie of a tool as it opens up so many opportunities for different burrs that regular collets just can’t do. Using an adjustable chuck allows you to use larger shafted burrs as well as small dentist burrs. The adjustable chuck ensures a good fit for each burr, which cuts down on any wobble you might get with collets.\nSkull Carving Designs\n4 Easy steps to carve a realistic feather\nWant to carve feathers on a skull that look like these? This is a one page instruction guide that shows you what you will need and the proper steps to carve a feather. The process is not complicated and is quite fun. You can do this!\n9 Western-themed skull carving designs\n9 Free Western Designs for Skull Carving! Included are 9 different pictogram style western-themed designs you can use freely to practice carving. Designs are beginner to advanced as any level of detail can be added to the basic design. I commissioned Anna Ježková (artist) to specifically make these for carving skulls. Designs can be transferred to a skull using my transfer method here.\nIn this article I am going to cover the steps to metalizing a skull. A lot of people are looking for something different for their European mounts and a metal finish is just the thing to give a skull a very classy look. The following is not a fake metal lookalike;...\nCleaning an antlered deer skull using maceration. Maceration is the process in which bacteria produces enzymes which break down organic matter. Putting a skull in a bucket of water and letting the bacteria grow and macerate a skull is actually the same thing that is...\nI have had to reconsider if I should publish an income report as that implies I am making an income, but I'm not! Well, I should clarify; I am not making any profit. My costs have been more than my income so far this year from my skull carving business U.S. Skull...\nMaking a deer antler ring is fun, and I will warn you, very addicting! Not to mention pretty easy! So grab an antler and let’s get started. Oh, and Mulletman is going to be helping demonstrate the steps 😉 Step 1. Cut out ring blank. I use a scroll saw to cut out...\nIn January of 2017 I set up a display table at a small local event (wild game dinner) hosted by a church. The evening was typical with people asking questions and the typical “oh can you carve … on a skull if I bring it to you?” type of stuff. I made no sales and...\nSkull Carving Income Report 1st Quarter 2018 1st quarter is in the books! Both my skull carving as well as my day job (real estate appraisal) have been slow due to baby #3 coming on March 6th! The transition of going from having 2 kids to 3 has been a challenge to...\nAs a skull carver there seems to be a gazillion different skull carving burrs to choose from. So which should you use? I will go over some burrs that have greatly improved my skull carving that I use on a daily basis. If you have not already you can check out the...\nWhere to Find Animal Skulls So you have a Dremel and you want to carve some skulls, but how do you get animal skulls? Well, you could simply go online and buy some off of eBay, or like me you can find...\nSo here's a natural alternative to staining skulls and bones, use walnut husks! Depending on where you are in the country (or world for that matter) walnut trees are a common sight. It doesn't matter if they are still green or dried up, they will make a dark dye..."", 'Choosing the correct Polaris Power generator means knowing how much power you require. Polaris offers four models: the P2000i, the P3200iE, the P5500 and the P6500. The following information can help you decide which one is the best fit for you.\nTo calculate your power needs correctly, you need to know which kind of load you are dealing with. A load is defined as the device that you are powering. There are two kinds of loads:\nResistive loads require the same amount of power to both start and run. Many resistive loads are involved in heating or creating heat of some kind. Examples include light bulbs, coffee makers and toasters.\nReactive loads contain an electric motor, which requires additional power to start but significantly less power to run once it gets going. Typically, starting power is three times the amount of power required to run the application. Examples of reactive loads include refrigerators and freezers, furnace fans, well pumps, air conditioners, bench grinders, air compressors and power tools.\nSome household appliances, like furnaces or refrigerators, have internal fans that come on intermittently. Extra wattage/power is needed to start the fan each time. Refrigerators also have a defrost cycle that requires power in addition to the compressor and fans.\nReactive loads also may require additional power when the electric motor begins to work. For example, when a saw begins cutting wood, its power requirement will increase. This is not applicable for most household appliances.\nTranslating Appliance Data Tags\nFor some appliances, you can determine the power needed by looking at the data tag supplied by the electric motor manufacturer. All electric motors should have a data tag attached to them that lists voltage, amps, phase and cycles.\n- Volts (V): The volts must be either 120 (110-120) or 120/240. 120/240 means that the motor can be wired to operate on 120V or 240V. Polaris generators all are 120V.\n- Amps (A): Indicates the amps required to RUN the electric motor but doesn’t consider STARTING or LOADED power requirements.\n- Phase (PH): Polaris generators can power only single phase motors.\n- Cycles (Hz): All U.S. electrical appliances run at 60 cycles per second.\nOther locations to find power requirements on appliances or devices include stamps on the bottom or side and nameplates.\nAmps to Watts\nWhile appliances frequently list their power requirements in amps, most generators list their output in watts. To determine the wattage needed, multiply amps by volts.\n- Amps x Volts = Watts\nMaximum vs. Rated Power\nGenerators are often advertised at the maximum wattage they can produce, but you’ll also see the rated power listed.\n- Maximum power is the maximum output that a generator can produce. Maximum power is usually available for up to 30 minutes.\n- Rated power is the power that a generator can produce for a long period of time. It’s typically 90 percent of the maximum power.\nIn general, use rated power to determine if a generator will be able to adequately power your applications continuously. Keep in mind that older appliances may require more power than what is listed because they become less efficient over time.\nThe P2000i has a maximum output of 2,000 watts and a rated output of 1600W. The P3200iE has a maximum output of 3,200W and a rated output of 2,800W. The P5500 has a maximum output of 5,500W and a rated output of 5,000W. The P6500 has a maximum output of 6,500W and a rated output of 6,000W.\nWattage Reference Chart\n|DEVICE||RUNNING (RATED) WATTS||ADDITIONAL STARTING (SURGE) WATTS|\n|Table Saw / Radial Arm Saw - 10""||2000||2000|\n|Central Air Conditioning||1500||4500|\n|Circular Saw - 7 1/4""||1400||2300|\n|Sump Pump - 1/2 HP||1050||2200|\n|Microwave Oven - 100 Watt||1000||1400|\n|Garage Door Opener 1/2 HP||875||2350|\n|Personal Computer w/ 17"" Monitor||800||0|\n|Color Television - 27""||500||0|\n|Electric Drill - 3/8"", 4 amps||440||600|\n|Light Bulb - 75 Watt||75||0|\nTo calculate the total generator output required, add the rated watts and surge watts for all appliances. For example, let\'s say you want to run a coffee maker and a radio. That would be 1000 + 300 = 1300 rated watts and 0 + 300 = 300 surge watts. The rated and surge watts together would be 1600 watts.\nFor more information, see your authorized Polaris Dealer. You can find a dealer near you with the Dealer Locator.\nMore operation and maintenance procedures can be found in your Owner\'s Manual: P5500 and P6500 Owner\'s Manual, P3200iE Owner\'s Manual and P2000i Owner\'s Manual.\nFor replacement part numbers, use the online parts catalog.']"	['<urn:uuid:452bcc3f-2faa-48c6-a8cf-eacd79be6c1b>', '<urn:uuid:b33ab9ba-1da5-415c-bd07-ea5836b9d0ed>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:57:18.859842	7	37	1825
