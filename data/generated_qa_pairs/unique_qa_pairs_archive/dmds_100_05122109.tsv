qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Does the ozone hole make Earth warmer?	No, the ozone hole does not cause global warming. While the ozone hole allows more UV light than usual to reach the surface, the additional energy added to the Earth system from the ozone hole is so small that it couldn't be responsible for the warming trend.	"['Antarctica\'s long months of polar darkness are a key ingredient to the making of the yearly ozone hole, but that doesn\'t diminish their unique beauty. As Michael Crichton put it at Caltech in Within the stratosphere, there lies a layer, made of a specialized form of oxygen, known as the ozone layer.\nDuring the continuously dark polar winter, the air inside the polar vortices becomes extremely cold, a necessary condition for polar stratospheric cloud formation. In addition to this global motion, strong winter polar vortices are also important to concentrating ozone at the poles.\nOzone-depleting gases reached their peak concentrations in the mid-latitudes purple in the mids and over Antarctica blue in the early s. As these chemicals reach the stratosphere, they destroy the ozone layer. In the meantime, the size of the ozone hole in any given year is influenced by natural variability in global atmospheric circulation.\nByincreases in ozone depletion stopped. As political leaders try to find ways to reduce greenhouse-gas concentrations with anticipated costs running in the trillions of dollars, they need to understand that this may have zero effect on reducing global warming.\nStratospheric ozone is constantly produced by the action of the sun\'s ultraviolet radiation on oxygen molecules known as photochemical reactions. Byincreases of CFCs in the atmosphere stopped. The atmosphere acts like a multi-layer shield that protects Earth from dangerous solar radiation.\nAt the poles, CFCs attach to ice particles in clouds. Observations show that as greenhouse gases increase and result in heating in the lower atmosphere tropospherea cooling is occurring in the upper atmosphere stratosphere.\nThe Montreal Protocol set an important precedent but more needs to be done. NASA Earth Observatory Human activities cause ozone depletion and global warming Ozone O3 depletion does not cause global warming, but both of these environmental problems have a common cause: The ozone hole The ozone hole.\nSir Robert Bob Watson played an important role in both cases There are both links and major differences between ozone depletion and global warming and the way the two challenges have been handled. Ozone-rich air from lower latitudes mixes back into the polar stratosphere, and the ozone hole disappears until the next spring.\nCarbon dioxide and other heat-trapping gases rise into the atmosphere and spread around the globe, like a blanket wrapping Earth. The largest ozone holes are often associated with the coldest winters Thanks to the Montreal Protocolthe production and use of ozone-damaging chemicals has declined.\nAntarctica is the only place on Earth where it\'s reliably cold enough for an ozone hole to form The ozone hole is an annual thin spot that forms in the stratospheric ozone layer over Antarctica in mid-September and October. As the temperature in the stratosphere falls, the ozone molecules in this layer become vulnerable to destruction by the harmful emissions.\nThe rate at which this layer depletes increases with the decrease in the temperature. In other words, heat-trapping gases contribute to creating the cooling conditions in the atmosphere that lead to ozone depletion.\nPublic opinion on climate change The two atmospheric problems have achieved significantly different levels of understanding by the public, including both the basic science and policy issues.\nThe chlorine comes from chlorofluorocarbons CFCs, for shortwhich were widely used in early refrigeration and cooling systems. The atmosphere of the Earth plays a crucial role in making it the only planet with life.\nNASA Earth Observatory Human activities cause ozone depletion and global warming Ozone O3 depletion does not cause global warming, but both of these environmental problems have a common cause: Stratospheric ozone also has natural processes that remove it from the atmosphere.\nThe Moon hovering over Earth\'s atmosphere. This time fueled by one of the most hazardous environmental issues threatening the planet, global warming.\nDoes global warming have an impact on the stratospheric ozone layer?The ozone layer is a belt of the naturally occurring gas ""ozone."" It sits to miles (15 to 30 kilometers) above Earth, and serves as a shield from the harmful ultraviolet B (UVB) radiation.\nThe ozone hole and global warming are not the same thing, and neither is the main cause of the other. The ozone hole is an area in the stratosphere above Antarctica where chlorine and bromine gases from human-produced chlorofluorocarbons (CFCs) and halons have destroyed ozone molecules.\nGlobal. The ozone hole did not cause global warming. Because the ozone layer normally blocks ultraviolet (UV) light, an ozone hole allows more UV light than usual to reach the surface. However, the additional energy added to the Earth system from the ozone hole is so small that it couldn’t be responsible for the warming trend that’s been occurring.\nThe ozone layer in the stratosphere is the earth’s sunscreen, and it protects living things from too much ultraviolet radiation. The manufacture and use of substances that deplete ozone in the stratosphere are controlled under the Montreal Protocol.\nDoes global warming have an impact on the stratospheric ozone layer? IPCC AR 5 Figure Temperature change in both the lower stratosphere and the lower troposphere go in opposite directions--a telling sign of excess carbon dioxide in the troposphere.\nThe ozone layer remains depleted, the ocean continues to warm, ice continues to melt, and sea-level continues to rise, but global temperatures did not change significantly from through They also had not changed much from toDownload']"	['<urn:uuid:5450b5dc-c93a-4bbc-8f07-ef8a2dace1a1>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	7	47	886
2	I've been analyzing digital retail performance, and I'm wondering about the relationship between cart abandonment and delivery services. What causes customers to abandon their carts, and how can delivery improvements help reduce this problem?	Cart Abandonment Rate (CAR) shows that users add items but don't complete purchases, calculated as: 1- (Completed purchases / Number of created carts) * 100%. To address this, enhanced delivery services can help - offering faster delivery times, providing flexible delivery options, and ensuring timely delivery updates can improve customer satisfaction and reduce abandonment. Additionally, providing options like same-day delivery, in-store pickup, and secure drop-off locations can meet customer expectations and encourage purchase completion.	['17 E-Commerce KPIs or How to Manage an Online Store to Make Profits\nTo achieve success in the e-commerce field one should be guided by such inviolable rules of trade as the measurement of key performance indicators (KPIs).\nSimply put, measure everything that can be measured and you will eventually understand the indicators influencing capital formation and be able to correct them in time.\nThere is a huge number of indicators that can be analyzed and taken into account when developing a business strategy or when assessing its economic components. Which of them should be prioritized is up to you, but you need to learn key positions by heart for building successful sales.\nIn this article, we will talk about how to properly monitor the effectiveness of e-commerce sales.\nRevenue is one of the basic indicators used in calculations of other KPIs usually known as the “top line” of any business. The indicator itself is calculated by the following formula:\nRevenue = Quantity of goods sold * Price\nProfit of sales demonstrates the success of a business. Note that sometimes, $100 payed today might be more profitable than $110 paid the day after tomorrow when the goods are shipped. It all depends on the environment, way of accounting, logistics, frequency of purchases, organization of the warehouse, etc.\nProfit = Income – Expenses\nAverage Revenue per User, ARPU\nThe indicator shows how much profit a user brings in a selected period of time. The indicator is closely related to the total web store traffic and shows its monetary effectiveness. It vividly explains the idea that sometimes it is more profitable to increase the sales volume through additional sales, including via cross-selling for example, rather than to expand the client base. To calculate APRU, use the following formula:\nARPU = Total revenue / Number of users\nAverage Revenue per Paying User, ARPPU\nSince it is usually more expensive to attract a new customer than to sell to the old one, one should also track and improve the Average Revenue per Paying User indicator. How can it be done? By remarketing, loyalty programs, personalized mailings, etc.\nARPPU = Total revenue / Number of customers\nYou can adjust the formula considering specific features of your business, including selling and operational costs and other factors.\nReturn on Marketing Investments, ROMI\nIn order to pay off marketing expenses, the indicator should be higher than zero. It is considered both in the context of the whole marketing activity and in the context of individual campaigns and channels.\nROMI = (Income – Marketing costs) / Marketing costs\nIf you want to make your marketing activities more efficient, work not only from the outside involving target advertisements, content of ads, etc., but also from the inside improving the contents of landing pages of the store. It means that you should focus on all dynamic elements adjusting the store for each advertising channel.\nCustomer Acquisition Cost, CAC\nThis indicator is considered in conjunction with another indicator: LTV (presented below). The lower the CAC compared to LTV, the more cost-effective your expenses for attracting new customers. Use the following formula:\nCAC = MCC / CA,\nwhere MCC is the cost of all marketing costs for attracting customers, CA – the total number of customers involved\nImproving this indicator, you can divide communication channels into three groups: positive, neutral (where the difference with LTV approaches to zero), and negative. So, negative value should be immediately disabled and you need to focus only on positive and neutral performers.\nCart Abandonment Rate, CAR\nCart Abandonment Rate is one of the most annoying indicators demonstrating that users added the goods to the cart, but for some reason, changed their mind to make the purchase. There can be various reasons for this: they was distracted or found a better offer from your competitor, or something else.\nCAR = 1- (Completed purchases / Number of created carts) * 100%\nIn this case, a user can be returned to the site via various remarketing mechanisms or follow-ups. You should not also forget about the shopping cart page. Check if the call-to-action is correct. Are the service conditions reported properly and positioned as USPs, etc.?\nIn addition, create a map of customer flows displaying exit points to understand the pitfalls or information missed . On the basis of the obtained analysis you can remove interface hurdles or add the necessary content to the basket.\nLifetime Value, LTV or Customer Lifetime Value, CLTV\nThe minimal suggested ratio between CAC and LTV is 1 / 3. If the second part of the proportion is considerably higher, then it makes sense to increase the cost of attraction, as it might indicate that marketing incentives do not work in full force. If CAC is higher than LTV, it means that your marketing is unprofitable. So, how to calculate LTV?\nLTV = Income from 1 customer for a planned life cycle – Cost of the customer’s attraction and retention\nHere’s how you can improve this indicator. After analyzing customer groups and establishing the most numerous ones, you need to determine how many orders a user makes (2-3-4, etc.) during the life cycle. Now, when creating a loyalty program, implement the motivating mechanics for this number of orders so that a user has the motive to make the first order, an additional motive to make the second one , and so on, just according to the potential number of repeated sales available in your market.\nFor a group with high loyalty, whose number of orders exceeds the average, you should develop special conditions. For the groups with relatively low loyalty, you need to re-activate programs and surveys to chase the pain points preventing sales. Also, try to increase the average customer’s paycheck.\nConversion Rate, CR\nConversion rates is one of the main indicators of the e-commerce efficiency. The higher is the ratio, the better is the performance.\nCR = Number of targeted visits / Total number of visits.\nA target visit is the one that ended with a purchase or any other measurable target action\nA variety of ways can be used to increase the indicator from improving website usability to making discounts.\nThe Number of Orders\nThe basic indicator which shows the number of orders made for the selected period. It is not very significant in itself, as it is used to calculate other metrics. Besides, it is highly dependable on traffic value and conversion rates of your store.\nTotal Amount of Orders\nThis indicator shows the amount of purchases for the selected period. It can be improved in two ways. The first way is “extensive”, which implies increasing the number of orders by increasing the cost of attracted traffic, remarketing, advertising mailings, etc. The second way – “intensive” – means increasing the average check through cross-sales, up-sales, and user motivations. The indicator is calculated by the following formula:\nTotal amount of orders = Quantity of orders * Average check\nAverage Order Value, AOV\nThis is one of the key indicators in any business, still it is easy to count:\nAOV = Total revenue / Number of orders\nThe result is going to explain the average paycheck in your web store. With the help of this parameter, you can judge the effectiveness of the sales department, relevance of the discount system, and work of the online store in general.\nThe average purchase check can be increased with free shipping, discounts, and incentive bonuses. You can also optimize commodity prices, increase cross-sells and up-sells (for example, through relevant product recommendations) develop and implement your loyalty program in order to further improve the AOV. The increase of it has a positive effect on ROI and other business metrics.\nCost per Order, CPO\nCPO shows how much you need to pay to obtain one purchase. It should be considered in comparison with the average check. If the cost per order exceeds the average check – you are operating at a total loss.\nCPO = Cost of attracting visitors / Number of orders\nWhat can you do to improve this indicator? First, determine the maximum limit of CPO for each channel. Then, conduct a Life Time Value analysis for each channel and determine the expected outcome. This will give you the insight that you can provide higher CPOs for some channels or have to reduce it for other ones.\nExecution of Orders\nThe success of any business also highly depends on the accuracy of everyday routine operations. With the Order Execution indicator, you can measure this activity properly. The ideal value of the indicator is 100%. Experts in the e-commerce field recommend online startups to strive for a level of at least 90%. The indicator is calculated in the following way:\nExecution of orders = Number of orders made on the site / Number of orders shipped successfully\nYou can improve the indicator by working on the quality of logistics and warehouses conduction. If the indicator is very low and shows no essential improvements, it might be worthwhile to think about the services of outsource fulfillment centers.\nThe Speed of Order Processing\nThe faster you process customer requests, the higher they appraisal is going to be. If you react to the request quickly, the customers will be satisfied. Thus, they will automatically mark you as a reliable supplier of goods and services.\nIt is very important to establish feedback from customers in order to understand whether you meet their expectations or not.\nRapid Reaction to Negative\nIf your customers are not happy with products or services, solve their problems quickly showing them that you admit your mistakes and tend to rise up the quality of service.\nInternet business begins with setting of goals and objectives. How you want to see it in the future and how you plan to achieve this. So, key performance indicators are the measures of results and costs you need to track and consider regularly, and the success will not keep you waiting.', 'In today’s fast-paced world of e-commerce and online shopping, last-mile delivery has become a critical component of the supply chain.\nThe final leg of the delivery journey, from the warehouse or distribution center to the customer’s doorstep, can make or break a customer’s experience and influence their decision to return for future purchases.\nAs online shopping continues to grow in popularity, last-mile delivery has become increasingly complex, with higher volumes of packages, tighter delivery windows, and rising customer expectations. In this blog post, we will explore the importance of last-mile delivery in the modern supply chain, and provide insights and strategies to help businesses optimize their operations, improve efficiency, and enhance customer experience.\nBenefits of Last-Mile Delivery:\nLast-mile delivery has several benefits for businesses, logistics providers, and end-users. Here are some of the key advantages of last-mile delivery:\nFaster delivery times for consumers: With the rise of e-commerce and on-demand delivery services, consumers now expect fast and reliable delivery times. Last-mile delivery helps to meet these expectations by ensuring that products are delivered quickly and efficiently.\nEnhanced customer experience and satisfaction: The quality of the last-mile delivery experience can have a significant impact on customer satisfaction and loyalty. By providing timely and accurate delivery updates, offering flexible delivery options, and ensuring the safety and security of products, businesses can improve the overall customer experience and build brand trust.\nIncreased sales and revenue for businesses: Effective last-mile delivery can lead to higher sales and revenue for businesses by reducing cart abandonment rates and encouraging repeat purchases. By offering free or low-cost shipping and same-day or next-day delivery options, businesses can incentivize customers to complete their purchases and build brand loyalty.\nImproved operational efficiency for logistics providers: Last-mile delivery can also help logistics providers to optimize their operations by reducing delivery costs, improving route planning, and increasing driver productivity. By leveraging technologies such as GPS tracking, real-time analytics, and automation, logistics providers can improve the accuracy and efficiency of their delivery operations.\nChallenges of Last-Mile Delivery:\nWhile last-mile delivery has many benefits, it also poses several challenges for businesses and logistics providers. Some of the key challenges of last-mile delivery include:\nTraffic congestion: Last-mile delivery often takes place in densely populated urban areas, where traffic congestion can be a significant issue. This can lead to delays, increased delivery costs, and reduced efficiency.\nCost: Last-mile delivery is often the most expensive part of the supply chain, accounting for up to 50% of total delivery costs. This is due to factors such as labor costs, fuel costs, and the need for specialized equipment.\nCustomer expectations: As mentioned earlier, customers have high expectations when it comes to delivery times and options. Meeting these expectations can be challenging for businesses, especially those that operate in multiple locations with varying delivery requirements.\nSecurity and safety: Last-mile delivery can be vulnerable to theft, damage, and other security and safety issues. Ensuring the safety and security of products during last-mile delivery is critical to maintaining customer trust and protecting brand reputation.\nEnvironmental impact: Last-mile delivery can have a significant environmental impact due to factors such as increased traffic congestion, carbon emissions, and waste. Reducing the environmental impact of last-mile delivery is becoming increasingly important as businesses and consumers become more environmentally conscious.\nSolutions for Last-Mile Delivery Challenges:\nWhile last-mile delivery poses several challenges for businesses and logistics providers, there are several solutions available to address these challenges. Some of the key solutions for last-mile delivery challenges include:\nRoute optimization: By using technology such as GPS tracking and real-time traffic data, businesses and logistics providers can optimize their delivery routes to reduce travel time, improve efficiency, and reduce costs.\nAlternative transportation methods: In addition to traditional delivery methods such as trucks and vans, businesses can also use alternative transportation methods such as bicycles, electric vehicles, and drones for last-mile delivery. These methods can reduce traffic congestion, emissions, and costs, while also providing faster delivery times.\nDelivery options: Offering customers a range of delivery options such as same-day delivery, in-store pickup, and lockers can help businesses meet customer expectations while also reducing last-mile delivery costs.\nSafety and security measures: To address security and safety concerns, businesses can use measures such as tamper-evident packaging, real-time tracking, and secure drop-off locations to ensure that products are delivered safely and securely.\nGreen delivery options: To reduce the environmental impact of last-mile delivery, businesses can use green delivery options such as electric vehicles, bicycles, and delivery on foot. These options can also help businesses improve their brand reputation and appeal to environmentally conscious consumers.\nFuture Trends in Last-Mile Delivery:\nAs the e-commerce industry continues to grow and evolve, the importance of last-mile delivery will only continue to increase. Here are some of the future trends that are likely to shape the last-mile delivery landscape:\nDelivery by autonomous vehicles: Self-driving cars and drones are becoming increasingly popular for last-mile delivery, as they can reduce delivery times and costs while also providing safer and more efficient delivery.\nSmart lockers and pickup points: With the rise of online shopping, smart lockers and pickup points are becoming more common. These allow customers to pick up their deliveries at a convenient location, reducing the need for last-mile delivery and providing a more flexible and convenient delivery option.\nBlockchain technology: Blockchain technology has the potential to revolutionize last-mile delivery by providing a secure and transparent system for tracking deliveries and managing the supply chain. This can help businesses reduce fraud, improve security, and increase customer trust.\nCollaboration between businesses and logistics providers: Collaboration between businesses and logistics providers can help to reduce the cost and complexity of last-mile delivery. This includes partnerships between retailers and logistics providers, as well as the sharing of data and resources to optimize delivery routes and reduce delivery times.\nAs these trends continue to develop, it is essential for businesses and logistics providers to stay ahead of the curve and adopt new technologies and strategies to meet the evolving needs of customers and the industry. By doing so, they can create a more efficient and effective last-mile delivery system that drives growth, profitability, and customer satisfaction.\nLast-mile delivery is a critical component of the supply chain that has become increasingly important in the age of e-commerce and online shopping.\nThe final leg of the delivery journey can make or break a customer’s experience, and businesses that neglect last-mile delivery risk losing customers and hurting their reputation.\nTo succeed in last-mile delivery, businesses need to focus on optimizing their operations, leveraging technology and data analytics, and partnering with reliable logistics providers. By doing so, they can improve efficiency, reduce costs, and enhance customer experience, ultimately driving business success. As the world continues to shift towards e-commerce and online shopping, last-mile delivery will only become more important. Businesses that prioritize and invest in their last-mile delivery operations will be better positioned to succeed in the fast-paced and competitive world of e-commerce.']	['<urn:uuid:5181fe7f-99c0-4b67-ae1e-3bc3302b04fe>', '<urn:uuid:0ff20579-237f-42c9-94a7-9028cc2a0096>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	34	74	2812
3	chemical synthetic clothes dye process drawbacks environmental friendly alternatives	Synthetic clothes dyes, which emerged during the industrial revolution, are among the top ten worst polluting industries, with millions of tons being dumped into waterways annually. While there are modern 'low-impact' synthetic dyes, their environmental safety is debated. A more eco-friendly alternative is natural dyeing, which uses plants, shellfish, and insects for coloring. However, the synthetic dyeing process can also be problematic on a practical level - as evidenced by issues like machine residue, disappointing color results, and the need for careful handling to avoid staining. Natural dyes, when used with safe mordants like alum and iron, can be disposed of safely and offer a more sustainable solution.	"[""“ Brand: Dylon / Clothes Dye „\nLast year I harboured the idea that I could be some sort of domestic goddess (no sniggering at the back please) and I bought myself a sewing machine and all sorts of additional expensive tat. The sewing machine has been reduced to making nothing more interesting than the occasional costume for school and fitted bedsheets for a cot bed. Everything else languishes at the bottom of a big box and gets a half-hearted examination every few months. When I did a recent examination I rediscovered about 5 of these little dyes and as this coincided with an empty house, a grubby white cotton bedsheet and a plastic toy box I had recently emptied I decided to give them a whirl.\n***What are they?***\nClothes dyes that you use in cold water, which means you don't have to use your washing machine or fiddle around with pans of boiling hot water. Each little tub is about 3 centimetres across and comes with a tiny little set of folded instructions inside the shrink wrap. The dye is a powder which is sealed in the small metal disk and it comes in a variety of colours. Each tub will dye about 250g/8oz of fabric (weighed when dry) and its recommended for natural fabrics e.g. cotton and linen only. You can use it on silk and wool and blended materials but it will give a lighter result than on the natural fabrics. The package also claims that it is colourfast, permanent and 'excellent for tie-dye, batik and any natural-fabric craft project'.\nYou should keep it away from anything that might want to eat it/smear it over your soft furnishings or their own bodies, so no children or pets.\n***How do you use it?***\nYou need a large container, small container, cold water, a knife, the dye, the material to be dyed, some salt and lots of patience. Oh yes and RUBBER GLOVES!\nMaterial goes in the container and is covered with water. Stick the knife in the dye pot and put it in the small container - add hot water and the salt until its all dissolved. Put the dye liquid in the large container and agitate the water and the material until its all mixed. Agitate at 15 minute intervals for an hour, then rinse until the the water runs clear. Hooray, dyed clothes.....in theory.\n***How much do they cost?***\nI paid 25p each for mine as a shop nearby was closing down. Normal retail price is around £2.70 per tub.\nThis was my first experience of dyeing anything so I was eager to read the instructions to make sure I was doing it all correctly. However the instructions are a string of folding hexagons which are tucked into the plastic wrap surrounding the dye tub. Given that the tub only measures around 3-3.5 centimetres across you can imagine how small the writing on the instructions is. In addition there was quite a bit of 'if you are using this fabric then do this' scattered throughout so its hard to work out a clear string of instructions- I had to write them down in the end. However one thing that I should have taken more notice of was the bit that says..'WEAR RUBBER GLOVES'. My husband had used the last pair, but as I had an unaccustomed day to myself with no children I was loathe to walk out and get a pair, or wait until another child free day. This was a mistake. Listen to the instructions. They know what they are talking about. Anyway, I had a roll of swing bin liners and thought I could wrap one of those around my hands so I was all set and ready to go.\nThe tub has a little metal lid that you stab with something sharp in order to release the powder within. I stabbed it with a knife and shook the powder out into the small bowl I was using to mix. Its quite a light powder and I found it flew around the sink a bit rather than going straight into the bowl. Turns out it was landing on my hand too and it doesn't need water to activate, my skin was enough-hooray, green speckled skin. The salt helps fix the dye in the fabric and all the next few stages of the process went without incident, very easy to do. It was no problem soaking the material, the dye seemed to take very quickly and very well and the bin bags worked quite effectively at first to protect my green speckled hands.\nBy the third agitation the bin bags sprang a leak and when I withdrew my hands I found that they were a rich dark green to the wrist, a similar colour to the bedsheet I was dyeing (a similar colour to surgical scrubs). As I wanted to finish the dyeing asap I hoped that when it was rinsing outside most of the dye would come off my hands. I used the outside tap as I thought it would reduce the amount of green dye going over myself, the kitchen or the bathroom (I have since heard that it can dye bath tubs so wouldn't recommend using the bath as a dyeing vat). An hour of freezing cold water and 15 thorough rinses later and the water was coming up clear, so I took it to the washing machine to get rid of any last chemicals. The sheet was going on my son's bed and I didn't want to take any risks as he is not quite 2 years old. The sheet went into the 30 degree wash a beautiful forest green and came out about twenty shades paler, I think I could probably have had a better effect if I had sneezed on it. In addition everything that has gone in the machine since has had an occasional splot of dark green dye on it, even now 3 weeks and many many washes later (including a 90 degree full clean of the machine). Ironically the dye came off the patio paving slabs outside and all the containers etc used with no residue or staining at all.\n***Would I recommend this***\nThe short answer is no.\nThe long answer is no because:\n-it's a long process that had disappointing results for me\n-the cold water is not pleasant even on a hot day when rinsing/agitating\n-I dyed my hands green and it took two hours of concerted scrubbing to get them clean (OK this is my fault)\n-residue has lingered in my washing machine (again perhaps I should have waited for it to dry before washing, but I had already been doing this for two hours plus and had had enough)\n-the instructions are very tiny and hard to read\n-its much easier to go and buy a green bedsheet. Buying the material and making one would have taken\nabout the same time as dyeing the original if you have a sewing machine and some elastic.\n-life is too short.\nDoes anyone want any dyes?"", 'By Guest Contributors Elissa Meyers and Mira Adornetto\nAfter pulling out green cotton fabric from a naturally fermenting indigo vat, our workshop group watches excitedly as the green transitions into a dark indigo blue. This incredible process, which occurs as the indigo dye oxidizes has been used for thousands of years in numerous places and cultures. Throughout human history, color has been applied to fibers on every continent, starting as far back as 2,600 BCE. Plants, shellfish, and insects: wildflowers, trees, mollusks and bugs, have been used to dye fibers.\nSince the industrial revolution dyeing went from natural colorants like plants and insects to synthesized petroleum based dyes, which are fairly damaging to the environment. The argument that modern “low-impact” dyes are less harmful to the environment than natural dyes has become a highly debated topic in the textile industry. Millions of tons of synthetic dye are still being dumped annually into waterways leaving textiles as one of the top ten worst polluted industries. Natural dyes, however, offer an alternative.\nNatural dyeing usually includes the use of a metallic salt, known as a “mordant,” from the French word “mordre” meaning “to bite.” Mordants include alum, iron, tin, chrome, copper, and also tannic acid. Tannins are readily available in the environment: they’re in black tea, as well as in oak trees and many other plants. Alum (sometimes used in food and cosmetics, though less often these days) is the most prevalent metal in the earth’s crust, followed by iron. These mordants can be used and disposed of safely. Although some of the other metals, tin and chrome for example, can produce more intense colors, they have a larger safety risk and environmental impact. These metals tend to give natural dyes a bad reputation as they are toxic to the environment. In fact, at BLUEREDYELLOW, we don’t even consider using them in our work.\nOur society’s awareness of global environmental issues has increased substantially since the 60s. In today’s DIY culture more people want to use accessible and sustainable materials. People now are more concerned with where manufactured goods come from and what to use them for. When synthetic dyes hit the textile industry during the Industrial Revolution, natural dyes became nearly obsolete. However, craftspeople like William Morris, (known for his textile patterns) continued to work using the older, more craft-based, processes including natural dyeing. With today’s advancements in science and technology, we have a better understanding of the world around us. Yet there is still so much to learn in the world of natural dyes. Philadelphia was once a powerhouse of the textile industry. Currently, it is a region with an active and abundant arts community, with an emphasis on craftsmanship.\nAside from the arts, Philadelphia is abundant with urban gardens and is a region where a long list of dye plants can be grown and found. Some of these include mugwort, goldenrod, sunflowers, black-eyed Susans, mint, dock, and sorrel. There are several tannin bearing plants including black walnut and sumac. It is also possible to grow historical dye plants such as the Japanese variety of indigo dye (vibrant blue) as well as madder root (alizarin red). It is incredible how many options there are to produce color in this area and we hope the craft will continue to be explored in a local, as well as global, way.\nAbout the Authors\nElissa Meyers and Mira Adornetto founded BLUEREDYELLOW, a design and natural dye house producing comfortable, chemical free textiles. BLUEREDYELLOW was founded with support from The Corzo Center for the Creative Economy. Currently, they work with businesses offering an alternative dye service and do piece dyeing as well as dyeing by the yard. Elissa and Mira also lead natural dyeing workshops around the Philadelphia area. They will be leading a two-day workshop at the Schuylkill Center on July 17 and 18.']"	['<urn:uuid:5c31571e-37fd-4257-a439-bcf177f9771e>', '<urn:uuid:a9798bac-4e66-42c1-bfcc-1b23dd84ce0f>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	9	108	1825
4	What accessibility accommodations have been implemented to ensure visitors with mobility challenges can explore this renowned archaeological complex?	According to European regulations, disabled access to the Acropolis is provided through special paths and a purpose-built lift installed on the north face of the hill. However, this lift facility is specifically restricted to wheelchair users only.	['The Acropolis hill, also called the «Sacred Rock» of Athens, is the most important site of the city and constitutes one of the most recognizable monuments of the world. It is the most significant reference point of ancient Greek culture, as well as the symbol of the city of Athens itself as it represent the apogee of artistic development in the 5th century BC.\nA visit to Athens is not complete without visiting the Acropolis, hundreds of tourists each day accordingly make the pilgrimage. Acropolis from Filopappou Hill.\nAcropolis at dusk from Lycabettus Hill.Acropolis fromAcropolis of Athens is an ancient citadel located on a high rocky outcrop above the city of Athens and contains the remains of several ancient buildings of great architectural and historic significance, the most famous being the Parthenon.\nAlthough there are many other acropoleis in Greece, the significance of the Acropolis of Athens is such that it is commonly known as «The Acropolis» withoutthere is evidence that the hill was inhabited as far back as the fourth millennium BC, it was Pericles (c. 495 – 429 BC) in the fifth century BC who coordinated the construction of the site’s most important buildings including the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike.\nThe Parthenon and the other buildings were seriously damaged during the 1687 siege by the Venetians in the Morean War when the Parthenon was being used for gunpowder storage and was hit by a cannonball.\nThe Acropolis was formally proclaimed as the preeminent monument on the European Cultural Heritage list of monuments on 26 March 2007.\n|From acropolis to Pireaus, Athens, Greece.|\n|Amphitheater in Acropolis, Athens, Greece.|\n|The Caryatid Porch of the Erechtheion, Acropolis, Athens, Greece.|\n|Erechtheum, Acropolis of Athens, Greece.|\n|The Parthenon on the top of the Acropolis Hill, Athens, Greece.|\n|The Propylaea of the Acropolis from inside, Athens, Greece.|\nOn the Acropolis\nThe Parthenon: The largest temple on the Acropolis, originally dedicated to the goddess of the city, Athena. Built between 447 and 438 BCE at the height of the Classical period.\nThe Temple of Athena Nike: First temple on the Acropolis to be built in the Ionic style, and one of the few exemplars of an amphiprostyle temple in all of Greece: what made it truly unique was the unit by which it was planned, which turns out to be the Egyptian foot of 300 mm.\nThe Erectheion: Dedicated to the worship of the two principal gods of Attica, Athena The Propylea: The ancient monumental gateway to the Acropolis.\nOn the South Side\nThe Odeon of Herodes Atticus: This ancient theatre is still used today for concerts and plays.\nThe Theatre of Dionysus.\nNew Acropolis Museum. Designed by Swiss architect Bernard Tschumi at a site south of the Acropolis, opened in June 2009. Located in Makryanni just below the Acropolis, it’s easily accessed from the Acropolis station of the Metro.\n|Acropolis of Athens.|\n|Acropolis of Athens.|\n|Odeon of Herodes Atticus, Acropolis of Athens.|\nFollowing European regulations, disabled access to the Acropolis can be gained by means of special paths and a purpose-built lift on the north face of the hill. Apparently this is only for the use of those in wheelchairs.']	['<urn:uuid:9321309d-5026-48de-989f-bf7625696d7c>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	18	37	533
5	As a business owner, I want to prevent legal problems before they happen. What are the important steps for reviewing my business practices to reduce risks, and what kinds of insurance should I regularly evaluate?	To prevent legal problems, you should regularly take a legal inventory to identify vulnerable areas, review and improve policies to ensure compliance with regulations, and maintain written contracts with all business relationships. For insurance, you should evaluate your coverage annually as your business changes, reviewing whether you need additional protection beyond general liability insurance, such as workers compensation, professional liability coverage, business interruption insurance, or data breach coverage. Working with both legal counsel and insurance agents as business partners is crucial for maintaining appropriate protection.	['Small Business Law\nWhether starting a business or growing an established enterprise, liability and risk are facts of life. Keeping those risks to a minimum is a never-ending endeavor that requires constant vigilance and effort. Our firm can minimize liability for the small business enterprise.\nPotential Areas Of Risk In A Business\nLiability takes numerous forms, ranging from risks with customers and employees to government regulations and market conditions. Some of the most common risk areas businesses face include:\n- Customers: While you want to offer your customers the best service possible, your relationship with them can sour in many ways, including those that involve legal matters. These can take various forms, including information protection and privacy, misinformation (false advertising), breach of contract and even civil lawsuits for personal injury.\n- Employees: Employees relationships can also pose a legal risk. Making sure you hire people who are right for your enterprise is key, but so too is avoiding issues with discrimination, unlawful termination, and workplace safety.\n- Market Conditions: Market conditions often present a great deal of financial risk to companies, but there are some legal considerations here too. The simplest form these could take is if current conditions limit your ability to deliver on promises and pay back investors. Keeping up with these responsibilities may be difficult if the market swings the wrong way.\n- Real Property And Leases: Whether you’re renting it, selling it or using it to conduct business, real estate is rife with liabilities and obligations like the Americans with Disabilities Act. You typically owe a duty of care to those who visit a property you own, and if someone gets injured, it can be costly for your business. Purchases and leases can be risky in that they can have numerous uncertainties, moving parts and regulations.\n- Executives And Leadership: Senior executives and leaders have numerous legal duties. Accountability is a major factor here, particularly where finances are concerned. Upholding ethics and complying with regulations is also important for minimizing liability.\n- Business Partners. Relationships with other companies are frequently a source of risk. Liability could stem from disputes over agreements, failure to deliver on promises or even perceived slights. Knowing how to prevent these disputes — and resolving them as elegantly as possible when they do arise — is vital to effective risk management.\n- Taxes. Tax liability is an ever-present matter for most businesses. As important as it is to keep tax exposure down, it’s more important to stay within the bounds of the law when doing so. Knowing how your organization fits within the tax code is vital to sustaining profitability.\nBest Practices For Minimizing Business Liability\nWhen you’re striving to keep your risk down, there’s a wide range of best practices to implement. The following tips are just a few ways to minimize liability for your organization, whether you’re a small family business or a large corporation.\n- Treating Legal Counsel As A Business Partner. First, make sure you don’t try to do it all by yourself. Let our attorneys provide legal counsel. Treat us as a business partner. Involve us in the numerous decision-making processes that are part of running your company, whether developing and revising policies, negotiating contracts or navigating a dispute. Our attorneys can provide insight into the legal implications of everything the business encounters. An ounce of prevention is worth a tons of court.\n- Choosing The Right Corporate Structure. Choosing the right corporate structure is key to minimizing liability for both the business and its stakeholders. Having the best corporate structure, like a Limited Liability Company (LLC) form, can provide the business owners’ assets with a shield from lawsuits. In addition, the corporate structure can impact other legal areas, such as what regulations pertain to the business or how it is taxed. Carefully consider all of these items when choosing an entity type for the business.\n- Taking A Legal Inventory And Identifying Areas Of Risk. To safeguard against lawsuits and other legal risks, a business owner should take stock of where the business is vulnerable. With legal counsel, take a careful, thorough inventory of the business’s current legal standing, and identify areas of risk. This legal inventory should take laws, practices, contracts, employees, ethics, environmental matters and even your current marketplace climate into consideration.\n- Written contracts protect. We don’t live in the days of a handshake anymore. Contracts can seem like a pain, but they are a great deal easier than expensive, drawn-out legal fights that can occur in their absence. No matter who the business is working with, how long they have been known, or what role they’re filling with the company, contracts are an assurance against any future legal issues.\n- Reviewing Policies And Practices. With areas of risk in mind, review policies and practices and make improvements where needed. The policies should support compliance with all pertinent regulations, but that won’t accomplish much if the current practices and corporate culture fail to do the same. Find ways to bring both in line with all relevant legal standards and best practices.\n- Solidifying Business Contracts. Business contracts are complex agreements with many moving parts. They’re all worth reviewing on a routine basis, particularly if the business team notices that some business relationships are becoming less profitable. As the business develops, the contracts will often need to change. In addition, if there are any weak points in the agreements, those should address and renegotiate those when necessary.\nMinimizing Liability — Never “One and Done”\nThe most important item to remember when it comes to minimizing liability for a business is that it’s never a one-and-done endeavor. Continuous improvement is key, so the modern business leader should keep revisiting their policies and practices on a routine basis.', 'What Is (And Isn’t) Protected Under Business Liability Insurance?\nBusiness Liability Insurance covers the basic risks associated with conducting business. Liability coverage refers to general items and not specific to unique business activities. Every business needs coverage but it is equally necessary to understand what coverage each plan offers and whether it is enough for your specific industry. Your Lyndhurst NJ insurance agent will be invaluable in determining the individual needs of your business. Not having the right kind of coverage, or lack of coverage may leave your business vulnerable to lawsuits and potential bankruptcy.\nWhat Is Covered By A General Business Insurance Policy?\nA general business insurance policy covers your business, your employees and yourself from lawsuits involving the business. Whether your business is being sued for an injury sustained in your place of business, caused by an employee or as a result of your product or service, Business Liability Insurance will cover it. Business Liability Insurance can also cover things like copyright infringement.\nBusiness liability insurance helps to mitigate the risk of losing money, or your business, due to legal action, whether that be from personal injury, property damage or intellectual damages. However, as with all other policy types, Business Liability Insurance has caps that may not cover the entire amount due. This is why it is important to have a risk assessment performed prior to choosing an insurance policy.\nWhat Is Not Covered By Your Business Liability Insurance Policy?\nA common misconception is that General Business Liability Insurance covers employees injured on the job… it doesn’t. New Jersey law requires business owners to have Workers Compensation coverage. Additionally, professional errors such as negligence, malpractice or misrepresentation will not be covered by a general Business Liability Insurance policy. Lastly, events such as car accidents, intentional acts and product failures that would be typically covered by a warranty are not covered by general business liability insurance. As a result, you may want additional coverage for any of the items not covered by your policy.\nIn addition to circumstance-specific policies like Workers Compensation and Car Insurance, it is important to evaluate the potential costs of damages following an accident. Business Umbrella Insurance is a policy that helps to pay any leftover amount in a lawsuit that is not covered by Business Liability, Workers Comp or Commercial Auto Insurance. This extra coverage helps to prevent major loses and even bankruptcy in the event of a large, substantial insurance claim.\nAdditional Insurance Coverage For Your Business May Include:\n- Automobile Insurance\n- Equipment Breakdown Coverage\n- Business Interruption Insurance\n- Electronic Data Loss Insurance\n- Data Breach Coverage\n- Professional Liability Coverage\nBusiness owners should have a solid understanding of what is included and not included with insurance policies. An experienced NJ insurance agent is instrumental in ensuring that you have the information and the coverage your business needs to be protected. Furthermore, take the time to evaluate your insurance annually. Your business changes over time, ensure your coverage is keeping up with these changes.\nSchedule A Meeting With Your NJ Insurance Agent Today\nContact our team at Bogle Agency Insurance today and make an appointment to evaluate your New Jersey business insurance needs. We welcome the opportunity to look at your current policies and determine whether there are any holes. Let our knowledge and experience serve as your guide to New Jersey business insurance, helping you understand what is protected under business liability insurance as well as what isn’t.']	['<urn:uuid:335b8ad5-5068-4544-95b3-652bacd0c49a>', '<urn:uuid:72a34e89-8db1-4ed3-b888-c215c78692a1>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:09:04.591601	35	85	1540
6	How many trials from London's Central Criminal Court can be accessed through the Old Bailey Proceedings, and what time period do they cover?	The Old Bailey Proceedings contains over 210,000 trials from London's Central Criminal Court, covering the years 1674 to 1913. It also includes biographical details of approximately 3,000 men and women who were executed at Tyburn.	"['Major Full-Text Collections Online\nResources marked with this symbol are restricted to Columbia affiliates.\n- British and Irish Women\'s Letters and Diaries\n""Includes approximately 100,000 pages of published letters and diaries from individuals writing from 1500 to 1900, including several thousand pages of previously unpublished materials. Drawn from 290 sources, including journal articles, pamphlets, newsletters, monographs, and conference proceedings, much of the material is in copyright. Represented are all age groups and life stages, all ethnicities, many geographical regions, the famous and the not so famous.""\n- British History Online\nBritish History Online is a digital library containing some of the core printed primary and secondary sources for the medieval and modern history of the British Isles. Created by the Institute of Historical Research and the History of Parliament Trust.\n- Chatham House Online Archive (Gale Cengage Learning)\nContains the publications and archives of the Royal Institute of International Affairs (Chatham House), the world-leading independent international affairs policy institute founded in 1920 following the Paris Peace Conference. The Institute\'s analysis and research, as well as debates and speeches it has hosted, can be found in this online archive, subject-indexed and fully searchable.\n- Early English Books Online\nA digitized version of the Early English Books microfilm collection, this database contains full text page images of over 96,000 titles listed in Pollard and Redgrave\'s Short Title Catalogue (1475-1640) and Wing\'s Short Title Catalogue (1641-1700) and their revised editions, as well as the Thomason Tracts (1640-1661) collection. A subset of more than 6,000 EEBO titles is now searchable as keyed full text.\n- Early English Books Online Text Creation Partnership (EEBO TCP)\nA full-text subset of items from Early English Books Online, created by a consortium of academic libraries. While the full text is eventually added to the main Proquest EEBO database, at any given time EEBO TCP is likely to contain a larger number of fully-keyed full-text searchable texts than is EEBO.\n- Eighteenth Century Collections Online (ECCO)\nA comprehensive digital edition of The Eighteenth Century microfilm set, which has aimed to include every significant English-language and foreign-language title printed in the United Kingdom, along with thousands of important works from the Americas, between 1701 and 1800.\n- Empire Online (Adam Matthew)\n""This resource brings together manuscript, printed and visual primary source materials for the study of \'Empire\' and its theories, practices and consequences. The materials span across the last five centuries and are accompanied by a host of secondary learning resources including scholarly essays, maps and an interactive chronology.""\n- Gale Artemis : primary sources\nProvides an interactive research environment that allows researchers to cross-search: Eighteenth Century Collections Online;Nineteenth Century Collections Online;Indigenous Peoples: North America;Sabin Americana, 1500-1926; The Making of the Modern World;The Making of Modern Law: Legal Treatises, 1800-1926;The Making of Modern Law: Trials, 1600-1926; and Associated Press Collections Online.\n- Global Commodities : Trade, Exploration & Cultural Exchange (Adam Matthew Digital)\nProvides a vast range of visual, manuscript and printed materials sourced from over twenty key libraries and more than a dozen companies and trade organisations around the world. These original sources will help scholars to explore the history of fifteen major commodities and to examine the ways that these have changed the world.\n- The Grand Tour (Adam Matthew Digital)\nThese accounts of the English abroad, c1550-1850, highlight the influence of continental travel on British art, architecture, urban planning, literature and philosophy. This collection of manuscript, visual and printed works allows scholars to compare a range of sources on the history of travel for the first time, including many from private or neglected collections. Includes letters; diaries and journals; account books; printed guidebooks; published travel writing; paintings and sketches; architectural drawings and maps.\n- India, Raj & Empire (Adam Matthew Digital)\nDrawing upon the manuscript collections of the National Library of Scotland, this searchable online resource provides access to digital facsimiles of diaries and journals, official and private papers, letters, sketches, paintings and original Indian documents containing histories and literary works. The collection documents the relationship between Britain and India in an empire where the Scots played a central role as traders, generals, missionaries, viceroys, governor-generals and East India Company officials. The dates of the documents range from 1710 to 1937.\n- Literature Online: The Home of English and American Literature on the World Wide Web (LION)\nA fully searchable library of more than 350,000 works of English and American poetry, drama, and prose.\n- London low life : street culture, social reform and the Victorian underworld. (Adam Matthew Digital)\nFull-text searchable resource, containing colour digital images of rare books, ephemera, maps and other materials relating to 19th and early 20th century London; designed for both teaching and study, from undergraduate to research students and beyond. Will be of interest to students and scholars from a wide range of disciplines, including literature, cultural studies, urban studies, social history and the study of leisure and tourism. There is a strong emphasis on rare or unique material, particularly in the range of ephemera and street literature available.There is also an emphasis on visual material. The documents are drawn from the holdings of the Lilly Library, the rare books, manuscripts, and special collections library of the Indiana University Libraries, Bloomington.\n- The Making of Modern Law : Legal Treatises 1800-1926\nDerived from two essential reference collections for historical legal studies, the Nineteenth Century and Twentieth Century Legal Treatises microfilm collections, The Making of Modern Law features a fully searchable database of more than 21,000 Anglo-American legal works including casebooks, local practice manuals, form books, works for lay readers, pamphlets, letters, speeches and more.\n- The Making of the Modern World: the Goldsmiths\'-Kress Library of Economic Literature, 1450-1850\nDigital facsimiles of literature on economics and business published from the last half of the 15th century to the mid-19th century from the collections of the Goldsmiths\' Library of Economic Literature at the University of London, England, and the Kress Collection of Business and Economics at the Harvard Business School; with supplementary materials from the Seligman Collection in the Butler Library at Columbia University, and from Sterling Library at Yale University. Collection contains materials on commerce, finance, social conditions, politics, trade and transport; documents the dynamics of Western trade and wealth; focuses on economics interpreted in the widest sense, including political science, history, sociology; special collections on banking, finance, transportation and manufacturing. Includes facsimiles of rare books and primary source materials such as political pamphlets and broadsides, government publications, proclamations, and a wide range of ephemera.\n- Mass Observation Online: British Social History, 1937-1972\n(Adam Matthew Digital)\nProvides integrated access to approximately 115,000 digital images of social observation materials from the Mass Observation Archive (MOA). In addition, it functions as a finding aid for all MOA material held on Adam Matthew Publications microfilm, and in the MOA itself. The Archive holds all the material generated by Mass Observation (MO) between 1937 and 1949, with a few later additions from the 1950s and 1960s. The materials fall into three main categories: file reports and publications; materials collected by investigators; and materials submitted by volunteers.\n- Medieval and Early Modern Sources Online\nA collection of digitized editions of texts concerning economic, political, legal, and ecclesiastical history, such as treasury accounts, chronicles, papal registers, etc. Most are from England, Ireland, and Scotland, although some are from Milan and the New World.\n- Nineteenth Century collections online (Gale Cengage Learning)\n""A multi-year global digitization and publishing program focusing on primary source collections of the long nineteenth century. . . . The content is sourced from the world\'s preeminent libraries and archives. It consists of monographs, newspapers, pamphlets, manuscripts, ephemera, maps, photographs, statistics, and other kinds of documents in both Western and non-Western languages"" (from publisher\'s description). Includes twelve in-depth thematic collections, sucha as Asia and the West; British Politics and Society; British Theatre, Music, and Literature; Europe and Africa; European Literature, 1790-1840: The Corvey Collection; Photography; Science, Technology, and Medicine: 1780-1925, Parts I & II; and Women: Transnational Networks.\n- Past Masters\nProvides fully searchable authoritative editions of philosophical, literary, scientific, personal, political, and theological writings by a wide range of British, American, and continental authors. Included among the British and American texts are: collected editions of the letters of Thomas Becket, Thomas Hobbes, Jane Austen, Samuel Taylor Coleridge, Charlotte Bronte, Charles Dickens and many others; the journals of Frances Burney, John Henry Newman and others; the works of Aphra Behn, Robert Boyle, Charles Darwin, John Dewey, Eliza Haywood, John Locke, Charles Sanders Pierce, Mary Shelley, Herbert Spencer, Mary Wollstonecraft, and others.\n- Perdita manuscripts : women writers, 1500-1700 (Adam Matthew Digital)\nComplete facsimile images of over 230 manuscripts written or compiled by women living in the British Isle during the 16th and 17th centuries. Contents include account books, advice, culinary writing, meditation, travel writing, and verse. Perdita manuscripts can be indexed by name, place, genre, and first lines of both poetry and prose.\n- The Proceedings of the Old Bailey: London\'s Central Criminal Court 1674 to 1913\nThe Old Bailey Proceedings Online makes available a fully searchable, digitised collection of all surviving editions of the Old Bailey Proceedings from 1674 to 1913, and of the Ordinary of Newgate\'s Accounts, 1690 to 1772. It allows access to over 210,000 trials and biographical details of approximately 3,000 men and women executed at Tyburn, free of charge for non-commercial use.\n- Queen Victoria\'s journals (ProQuest)\n""Makes available online digital images of every page in the entire sequence of Queen Victoria\'s diaries, and provides full transcriptions and keyword searching of all journal entries. The Queen Victoria\'s Journals resource is the product of a unique partnership between the Bodleian Libraries and the Royal Archives, working in collaboration with the online publisher ProQuest.This website reproduces as high-resolution colour images every page of the surviving volumes of Queen Victoria\'s journals, along with separate photographs of the many illustrations and inserts within the pages."" (from publisher\'s description)\n- State Papers Online : The Government of Britain, 1509-1714\nA collection of English government documents originating from the sixteenth and seventeenth centuries--when the King or Queen acted as Prime Minister as well as Monarch. The office archives and correspondence of the secretaries of state serving the Monarch are included as facsimile manuscript documents accessible through fully searchable Calendar entries. This collection contains information on every facet of English government, including social and economic affairs, law and order, religious policy, crown possessions and intelligence gathering as well as Britain\'s international relations and foreign policy.\n- Victorian popular culture (Adam Matthew Digital)\nAn essential resource for the study of popular entertainment in the nineteenth and early twentieth centuries. Consists of four components: Spiritualism, Sensation and Magic; Circuses, Sideshows and Freaks; Music Hall, Theatre and Popular Entertainment; and Moving Pictures, Optical Entertainments & the Advent of Cinema. Includes full-text, full-color reproductions of books, ephemera, handbills, pamphlets, photos, posters, programs, scripts, and other types of materials. Coverage is most extensive for Great Britain; but there is also a fair range of materials for the U.S.A.\n- Women Writers Online\nMore than two hundred full-text works in English, or in English translation, by women, covering a period from 1400 to 1850.']"	['<urn:uuid:34315f49-7eb1-4a4e-825a-73b6cf88eb2e>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	23	35	1860
7	surgery risks blood clots prevention treatment	Surgery carries risks of blood clots, particularly in the legs, which can cause swelling and pain. If a clot breaks off and travels to the lungs (pulmonary embolism), it can be dangerous or deadly. To prevent blood clots, doctors prescribe daily Aspirin for the first month after surgery, get patients moving as soon as possible after operation, may prescribe blood-thinning medication, and recommend using compression stockings on both legs.	"[""A single copy of this article may be reprinted for personal, noncommercial use only.\nCancer surgery: Physically removing cancerBy Mayo Clinic staff\nOriginal Article: http://www.mayoclinic.com/health/cancer-surgery/CA00033\nAlternative medicine (4)\n- Alternative cancer treatments: 11 options to consider\n- Mindfulness exercises: How to get started\n- Pet therapy: Man's best friend as healer\n- see all in Alternative medicine\n- Paraneoplastic syndromes of the nervous system\nCoping and support (8)\n- Cancer survivors: Care for your body after treatment\n- Cancer survivors: Managing your emotions after cancer treatment\n- Cancer survivors: Reconnecting with loved ones after treatment\n- see all in Coping and support\n- Cancer prevention: 7 tips to reduce your risk\nTests and diagnosis (8)\n- Cancer survival rate: What it means for your prognosis\n- Biopsy: Types of biopsy procedures used to diagnose cancer\n- Cancer blood tests: Lab tests used in cancer diagnosis\n- see all in Tests and diagnosis\nTreatments and drugs (26)\n- Cancer treatment decisions: 5 steps to help you decide\n- Eating during cancer treatment: Tips to make food tastier\n- Low blood cell counts: Side effect of cancer treatment\n- see all in Treatments and drugs\nCancer surgery: Physically removing cancer\nThe prospect of cancer surgery may make you feel anxious. Help put your mind at ease by learning more about cancer surgery and how and why it's used.By Mayo Clinic staff\nCancer surgery — an operation to repair or remove part of your body to diagnose or treat cancer — remains the foundation of cancer treatment. Your doctor may use cancer surgery to achieve any number of goals, from diagnosing and treating your cancer to relieving the symptoms it causes. Cancer surgery may be your only treatment, or it may be supplemented with other treatments, such as radiation, chemotherapy, hormone therapy and biological therapy.\nHow is cancer surgery used in treatment?\nCancer surgery may be used to achieve one or more goals. Common reasons you might undergo cancer surgery include:\n- Cancer prevention. If there's reason to believe that you have a high risk of developing cancer in certain tissues or organs, your doctor may recommend removing those tissues or organs before cancer develops. For example, if you have a genetic condition called familial adenomatous polyposis, your doctor may use cancer surgery to remove your colon and rectum because you have a high risk of developing colon cancer.\n- Diagnosis. Your doctor may use a form of cancer surgery to remove all or part of a tumor — allowing the tumor to be studied under a microscope — to determine whether the growth is cancerous (malignant) or noncancerous (benign).\n- Staging. Cancer surgery helps your doctor define how advanced your cancer is, called its stage. Surgery allows your doctor to evaluate the size of your tumor and determine whether it's traveled to your lymph nodes. Additional tests might be necessary to gauge your cancer's stage.\n- Primary treatment. For many tumors, cancer surgery is the best chance for a cure, especially if the cancer is localized and hasn't spread. If there's evidence that your cancer hasn't spread, your doctor may recommend surgery to remove the cancerous tumor as your primary treatment.\n- Debulking. When it's not possible to remove all of a cancerous tumor — for example, because doing so may severely harm an organ — your doctor may remove as much as possible (debulking) in order to make chemotherapy or radiation more effective.\n- Relieving symptoms or side effects. Sometimes surgery is used to improve your quality of life rather than to treat the cancer itself — for example, to relieve pain caused by a tumor that's pressing on a nerve or bone or to remove a tumor that's blocking your intestine.\nSurgery is often combined with other cancer treatments, such as chemotherapy and radiation. Whether you opt to undergo additional cancer treatment depends on your type of cancer and its stage.\nHow is cancer surgery traditionally performed?\nTraditionally, the primary purpose of cancer surgery is to cure your cancer by removing all of it from your body. The surgeon usually does this by cutting into your body and removing the cancer along with some surrounding healthy tissue to ensure that all of the cancer is removed. Your surgeon may also remove some lymph nodes in the area to determine if the cancer has spread. This helps your doctor assess the chance of your being cured, as well as the need for further treatment.\nIn the case of breast cancer surgery, your doctor may remove the cancer by removing the whole breast (mastectomy) or by removing only the portion of your breast that contains the cancer and some of the surrounding tissue (lumpectomy). In the case of lung cancer surgery, your doctor may remove part of one lung (lobectomy) or the entire lung (pneumonectomy) in an attempt to ensure that all the cancer has been removed.\nWhat other techniques are used in cancer surgery?\nMany other types of surgical methods for treating cancer and precancerous conditions exist, and investigators continue to research new methods. Some common types of cancer surgery include:\n- Cryosurgery. During this type of surgery, your doctor uses very cold material, such as liquid nitrogen spray or a cold probe, to freeze and destroy cancer cells or cells that may become cancerous, such as irregular cells in a woman's cervix that could become cervical cancer.\n- Electrosurgery. By applying high-frequency electrical currents, your doctor can kill cancer cells, for example, in your mouth or on your skin.\n- Laser surgery. Laser surgery, used to treat many types of cancer, uses beams of high-intensity light to shrink or vaporize cancer cells.\n- Mohs surgery. Useful for removing cancer from certain sensitive areas of the skin, such as near the eye, and for assessing how deep a cancer goes, this method of surgery involves carefully removing cancer layer by layer with a scalpel. After removing a layer, your doctor evaluates it under a microscope, continuing in this manner until all the abnormal cells have been removed and the surrounding tissue shows no evidence of cancer.\n- Laparoscopic surgery. A surgeon uses a laparoscope to see inside your body without making large incisions. Instead, several small incisions are made and a tiny camera and surgical tools are inserted into your body. The surgeon watches a monitor that projects what the camera sees inside your body. The smaller incisions mean faster recovery and a reduced risk of complications. Laparoscopic surgery is used in cancer diagnosis, staging, treatment and symptom relief.\n- Robotic surgery. In robotic surgery, the surgeon sits away from the operating table and watches a screen that projects a 3-D image of the area being operated on. The surgeon uses hand controls that tell a robot how to maneuver surgical tools to perform the operation. Robotic surgery helps the surgeon operate in hard-to-reach areas.\n- Natural orifice surgery. Natural orifice surgery is currently being studied as a way to operate on organs in the abdomen without cutting through the skin. Instead, surgeons pass surgical tools through a natural body opening, such as your mouth, rectum or vagina. As an example, a surgeon might pass surgical tools down your throat and into your stomach during natural orifice surgery. A small incision is made in the wall of the stomach and surgical tools pass into the abdominal cavity in order to take a sample of liver tissue or remove your gallbladder. Natural orifice surgery is experimental, and few operations have been performed this way. Doctors hope it can reduce the risk of infection, pain and other complications of surgery.\nCancer surgery continues to evolve. Researchers are investigating other surgical techniques with a goal of less invasive procedures.\nWhat can you expect before and after cancer surgery?\nPreparation and healing from cancer surgery varies greatly based on the operation. But in general, you can expect certain similarities, including:\n- Preparation. In general, expect to undergo certain tests, such as blood tests, urine tests, X-rays and other imaging tests, in the days preceding your surgery. These tests will help your doctor assess your surgical needs, such as your blood type should you need a transfusion, and identify potential risks, such as infections, that may influence your surgery.\n- Anesthesia. If you're having surgery, you'll likely need some type of anesthetic - a medication that blocks the perception of pain. Your options for anesthesia will be based on what type of surgery you're receiving.\n- Recovery. Depending on your surgery, you may stay in the hospital for a time before going home. Your health care team will give you specific directions for your recovery, such as how to care for any wounds, what foods or activities to avoid and what medications to take.\nWhat are the risks of cancer surgery?\nAs with any surgery, cancer surgery does carry risks. What side effects you might experience after cancer surgery will depend on your specific surgery. In general, most cancer operations carry a risk of:\n- Pain. Pain is a common side effect of most operations. Some cause more pain than others do. Your health care team will tell you how to keep your pain to a minimum and will provide medications to reduce or eliminate the pain.\n- Infection. The site of your surgery can become infected. Your health care team will show you how to care for your wound after surgery. Follow this routine closely to avoid infection, which can lengthen your recovery time after surgery. Doctors treat infections most often with antibiotics.\n- Loss of organ function. In order to remove your cancer, the surgeon may need to remove an entire organ. For example, a kidney may need to be removed (nephrectomy) if you have kidney cancer. For some such operations, the remaining organ can function sufficiently to compensate for the loss, but in other situations you may be left with impairments. For instance, removal of a lung (pneumonectomy) may cause difficulty breathing.\n- Bleeding. All operations carry a risk of bleeding. Your surgeon will try to minimize this risk.\n- Blood clots. While you're recovering from surgery, you're at an increased risk of developing a blood clot. Though the risk is small, this complication can be serious. Blood clots most commonly occur in the legs and may cause some swelling and pain. A blood clot that breaks off and travels to a lung is called a pulmonary embolism, a dangerous and sometimes deadly condition. Your surgeon will take precautions to prevent blood clots from developing, such as getting you up and out of bed as soon as possible after your operation or prescribing a blood-thinning medication to reduce the risk of a clot.\n- Altered bowel and bladder function. Immediately after your surgery, you may experience difficulty having a bowel movement or emptying your bladder. This typically resolves in a few days, depending on your specific operation.\nWhatever cancer treatment your doctor recommends, you're likely to feel some anxiety about your condition and the treatment process. Knowing what to expect can help. Use this information to help you ask informed questions when you meet with your doctor.\n- Niederhuber JE. Surgical interventions in cancer. In: Abeloff MD, et al. Abeloff's Clinical Oncology. 4th ed. Philadelphia, Pa.: Churchill Livingstone Elsevier; 2008:407.\n- Surgery. American Cancer Society. http://www.cancer.org/Treatment/TreatmentsandSideEffects/TreatmentTypes/Surgery/index?sitearea=ETO&vie. Accessed June 9, 2011.\n- Khashab MA, et al. Natural orifice translumenal endoscopic surgery. Current Opinion in Gastroenterology. 2010;26:471.\n- Gomez G. Emerging technology in surgery: Informatics, electronics, robotics. In: Townsend CM Jr, et al. Sabiston Textbook of Surgery: The Biological Basis of Modern Surgical Practice. 18th ed. Philadelphia, Pa.: Saunders Elsevier; 2008. http://www.mdconsult.com/das/book/body/208746819-6/0/1565/0.html. Accessed June 10, 2011."", 'Nearly everyone that has surgery will have pain post-operatively. Location, type, and duration of the pain will vary from person to person. It is crucial to work with your surgeon to find the best pain management plan for you. And, it is even more important to follow through with this plan.\nSeveral different medications can be used after surgery to help with your pain. A multimodal approach works best to help avoid the overuse of opioids. Various medicines that may be used include:\n- Anesthetic Blocks\n- Different types of Analgesics\n- Anti-inflammatory Medications\nOther natural pain-relieving modalities that should be used in addition to medication include:\nMedications Used During Surgery to Decrease Pain\nHistorically, most significant surgeries require general anesthesia. But as we advance in medicine, we are finding that using an anesthetic block can improve postoperative pain and decrease nausea and drowsiness. Coming out of hip or knee joint replacement surgery more alert and with less pain can help start you off on an excellent path for healing. Your medical team, including your surgeon and your anesthesiologist, will work with you to decide if this is a good option for you.\nWhether you receive an anesthetic block or general anesthesia, you may also receive local analgesics towards the end of your surgery. These local, soft tissue injections are provided to help begin the process of reducing postoperative pain, even before you are out of surgery.\nThen, as you awaken from surgery and are awaiting discharge from the hospital, IV analgesics can also be administered to continue to help manage or decrease your pain.\nMedications Used After Surgery\nOnce you are discharged from the hospital, it is essential to continue your pain management protocol and schedule. Most importantly, Aspirin will be prescribed daily for the first month, not just for pain relief but to be used prophylactically for deep vein thrombosis or pulmonary embolism (blood clots).\nYou will be prescribed an oral pain reliever, including an opioid and Tylenol. To help supplement these pain medications, you will also take an anti-inflammatory medication such as ibuprofen. Together, these medications help decrease your pain, inflammation and improve your tolerance to Physical Therapy and your rehabilitative process. More importantly, the combination of these drugs reduces your reliance on opioids, also known as narcotics.\nOpioids can be addictive and can cause further problems down the road if they are overused after surgery. To help minimize this risk, your surgeon will prescribe certain medications to be taken orally daily. As a backup, other drugs will be prescribed on an as-needed basis.\nHere is an example of what works well for many of our patients:\n- Tylenol 650mg daily\n- Celebrex 200mg twice a day or Meloxicam 15mg daily\n- Aspirin 81mg daily\nAnd, as needed:\n- Tramadol 50mg for mild pain as needed\n- Oxycodone 5mg for moderate pain as needed\n- Oxycodone 10mg for severe pain as needed\nOther Pain-Relieving Modalities\nMedications are not the only thing that can reduce your pain after a joint replacement or other surgery. Cryotherapy, elevation, and compression can all provide pain control.\nAfter surgery, cryotherapy (or the use of cold/ ice) can be applied several times a day to decrease swelling and improve pain. This is especially true after going to Physical Therapy or working on your rehabilitation exercises at home.\nYou should also elevate your leg while at rest. Elevating your leg above your heart will help with circulation, decreasing swelling, all while reducing pain.\nLastly, compression stockings, wraps, or the use of a compressive device can further enhance your ability to decrease swelling and pain. It is standard to be prescribed compression stockings to both legs after surgery, not only for relief but to lower the risk of a blood clot. But you may find yourself wanting to wear the stockings for longer than prescribed due to the comfort that these can provide.\nWhen working with Dr. Morton, you can trust that we will address your pain and functional status when creating a tailored medication plan after your surgery.']"	['<urn:uuid:9760cda1-3507-428f-b2dd-eaa6dec38f9e>', '<urn:uuid:739c5099-84c6-4d2c-b402-bd5a5adc28a9>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:09:04.591601	6	69	2615
8	I'm confused about physics - do strange metals and superconductors behave similarly with their electrons?	No, they behave quite differently. In strange metals, electrons don't follow the usual quasiparticle theory and their behavior is very hard to predict. In superconductors, electrons move in a coordinated way, similar to people dancing in a conga line formation, allowing them to travel through the crystal lattice without any resistance.	['Taming Superconductors With String Theory\nString theory was devised as a way to unite the laws of quantum mechanics with those of gravity, with the goal of creating the vaunted “theory of everything.”\nSubir Sachdev is taking the “everything” literally. He’s applying the mathematics of string theory to a major problem at the other end of physics — the behavior of a potentially revolutionary class of materials known as high-temperature superconductors.\nThese materials are among the most promising and the most perplexing. Unlike regular superconductors, which need to be cooled almost to absolute zero (–273.15 degrees Celsius) to pass a frictionless current of electricity, high-temperature superconductors yield the same remarkable performance under more accommodating conditions. Since the first high-temperature superconductor was discovered in 1986, physicists have found other materials that exhibit superconductivity at successively higher temperatures, with the current record standing at –70 degrees Celsius.\nThis progress has occurred despite the fact that physicists don’t understand how these superconductors work. Broadly speaking, many condensed-matter physicists study how electrons — the carriers of electrical current — move through a given material. In an ordinary conductor like copper or gold, the electrons flow through a lattice formed by the copper or gold atoms. In an insulator like diamond, electrons tend to stay put. In superconductors, electrons move through the underlying atomic lattice with no energy loss at all. For three decades, physicists have been unable to develop a comprehensive theory that explains how electrons in high-temperature superconductors behave.\nA particularly interesting question is how the behavior of the material changes with temperature — in particular, how conductors transition from ordinary to super as the temperature drops. Scientists call this a “quantum phase change,” with the two phases being the property of the material on either side of the transition temperature.\nSachdev, a condensed-matter physicist at Harvard University, explains that the challenge is one of scale. A typical chunk of material has trillions upon trillions of electrons. When those electrons interact with one another — as they do in superconductors — they become impossible to keep track of. In some phases of matter, physicists have been able to overcome this scale issue by modeling swarms of electrons as “quasiparticles,” quantum excitations that behave a lot like individual particles. But the quasiparticle strategy doesn’t work in high-temperature superconductors, forcing physicists to look for another way to impose collective order on the behavior of electrons in these materials.\nIn 2007 Sachdev had a startling insight: He realized that certain features of string theory correspond to the electron soup found in high-temperature superconductors. In the years since, Sachdev has developed models in string theory that offer ways to think about the electron behavior in high-temperature superconductors. He’s used these ideas to design real-world experiments with materials like graphene — a flat sheet of carbon atoms — which have properties in common with the materials that interest him.\nIn a forthcoming paper in Science, he and his collaborators use methods borrowed from string theory to correctly predict experimental results related to the flow of heat and electrical charge in graphene. Now he hopes to apply his insights to high-temperature superconductors themselves.\nQuanta Magazine spoke with Sachdev about how the electrons in high-temperature superconductors are related to black holes, his recent success with graphene, and why the biggest name in condensed-matter physics is skeptical that the string-theory approach works at all. An edited and condensed version of the interview follows.\nQUANTA MAGAZINE: What’s going on inside a high-temperature superconductor?\nSUBIR SACHDEV: The difference between old materials and the new materials is that in older materials, electrons conduct electricity independent of one another. They obey the exclusion principle, which says electrons can’t occupy the same quantum state at the same time and that they move independently of one another. In the new materials that I, and many others, have been studying, it’s clear that this independent-electron model fails. The general picture is that they move cooperatively and, in particular, they’re entangled — their quantum properties are linked.\nThis entanglement makes high-temperature superconductors much more complicated to model than regular superconductors. How have you been looking at the problem?\nGenerally I approach this through the classification of the quantum phases of matter. Examples of simple quantum phases are simple metals like silver and gold, or simple insulators like diamonds. Many of these phases are well-understood and appear everywhere in our daily lives. Since we discovered high-temperature superconductors, and many other new materials, we’ve been trying to understand the other physical properties that can emerge when you have trillions of electrons obeying quantum principles and also interacting with each other. At the back of my mind is the hope that this broad attack on classifying quantum phases of matter will lead to a deeper understanding of high-temperature superconductors.\nHow far have you gotten?\nThere has been great progress in understanding the theory of quantum phase transitions, which involves taking two phases of quantum matter that are very different from each other and adjusting some parameter — say, pressure on a crystal — and asking what happens when the material goes from one phase to the other. There has been a huge amount of progress for a wide class of quantum phase transitions. We now understand many different kinds of phases we didn’t know existed before.\nBut a full theory of how electrons behave in high-temperature superconductors has been difficult to develop. Why?\nIf you have a single electron moving through a lattice, then you really only need to worry about the different positions that electron can occupy. Even though the number of positions is large, that pretty much is something you can handle on a computer.\nBut once you start talking about many electrons, you have to think about it very differently. One way to think about it is to imagine that each site on the lattice can be either empty or full. With N sites it’s 2N, so the possibilities are unimaginably vast. In this vast set of possibilities, you have to classify what are reasonable things an electron would tend to do. That in a nutshell is why it’s a difficult problem.\nReturning to phase transitions, you’ve spent a lot of time studying what happens to a high-temperature superconductor when it grows too warm. At this point, it becomes a so-called “strange metal.” Why would understanding strange metals help you to understand high-temperature superconductors?\nIf you start with a superconductor and raise the temperature, there’s a critical temperature at which the superconductivity disappears. Right above this temperature you get a type of metal that we call a strange metal because many of its properties are very different from ordinary metals. Now imagine reversing the path, so that the phase of a system is changing from a strange-metal state to a superconducting state as it goes below the critical temperature. If we’re going to determine the temperature at which this happens, we need to compare the energies of the quantum states on either side of the critical temperature. But strange metals look strange in every respect, and we have only the simplest models for their physical properties.\nWhat makes strange metals so different from other unique quantum phases?\nIn certain phases, [quantum] excitations generally behave like new emergent particles. They are quasiparticles. Their inner structure is very complicated, but from the outside they look like ordinary particles. The quasiparticle theory of many-body states pretty much applies to all states we’ve discovered in the older materials.\nStrange metals are one of the most prominent cases we know where quasiparticle theory fails. That’s why it’s so much harder to study them, because this basic tool of many-body theory doesn’t apply.\nYou had the idea that string theory might be useful for understanding quantum phases that lack quasiparticles, like strange metals. How is string theory useful in this setting?\nFrom my point of view, string theory was another powerful mathematical tool for understanding large numbers of quantum-entangled particles. In particular, there are certain phases of string theory in which you can imagine that the ends of strings are sticking to a surface. If you are an ant moving on the surface, you only see the ends of the string. To you, these ends look like particles, but really the particles are connected by a string that goes to an extra dimension. To you, these particles sitting on the surface will appear entangled, and it is the string in the extra dimension which is entangling the particles. It’s a different way of describing entanglement.\nNow you could imagine continuing that process, not just with two electrons, but with four, six, infinitely many electrons, looking at the different entangled states the electrons can form. This is closely connected to the classification of phases of matter. It’s a hierarchical description of entanglement, where each electron finds a partner, and then the pairs entangle with other pairs, and so on. You can build this hierarchical structure using the stringy description. So it is one approach to talking about the entanglement of trillions of electrons.\nThis application of string theory to strange metals has some interesting implications. For instance, it’s led you to draw connections between strange metals and the properties of black holes. How do you get from one to the other?\nIn the string-theory picture, [changing the density of electrons] corresponds to putting a charge on a black hole. Many people have been studying this in the last five years or so — trying to understand things about strange metals from the properties of charged black holes. I have a recent paper in which I actually found a certain artificial model of electrons moving on a lattice where many properties precisely match the properties of charged black holes.\nI’ve read that Philip Anderson, considered by many people to be the most-influential living condensed-matter physicist, is skeptical that string theory is really useful for understanding strange metals. Do you know if that’s true?\nI think that’s correct. He’s told me himself that he doesn’t believe any of this, but, you know, what can I say, he’s a brilliant man with his own point of view. I would say that when we first proposed the idea in 2007, it certainly sounded crazy. A lot of progress has been made since then. I have a new paper with Philip Kim and others where it turns out that with graphene, which is a slightly less-strange metal, many of the methods inspired by string theory have led to quantitative predictions that have been verified by experiments.\nI think that’s been one of the best successes of the string-theory methods so far. It literally works; you can get the numbers right. But graphene is a simple system, and whether these methods are going to work for high-temperature superconductors hasn’t yet been proven.\nCould you say more about why Anderson might be skeptical of the approach you’ve taken?\nIf you go back and actually look at string-theory models, on the surface they look very different from the kinds of models you need for high-temperature superconductors. You look at the stringy models and their constituents, and it appears absurd that these are connected to the constituents of the high-temperature superconductors. But if you take the point of view that, OK, I’m not literally saying this model is going to be found in [high-temperature superconductors], this is just a model that helps me make progress on difficult issues, like how do materials without quasiparticles behave, string theory gives you examples of one of these materials that’s reliably solvable.\nHow literally are you using string theory? Is it a direct application, or are you drawing inspiration from it?\nIt’s closer to the inspiration side of things. Once you’ve solved the model, it gives you a lot of insight into other models that you may not be able to solve. After six or seven years of work closer to the string-theory side, we think we’ve learned a lot. For us the next step appears to be working in more realistic systems using inspiration we got from more solvable models.\nHow might the string-theory models, plus the work on graphene, put you in a position to understand the properties of high-temperature superconductors?\nAs you change the density of electrons in high-temperature superconductors, there’s a much more dramatic change in which the electrons go from a regime where it seems only a few electrons are mobile to one where all electrons are mobile. We’re understanding that there’s a special point called the optimal density where there seems to be a dramatic change in the quantum state of electrons. And right near this point is where the strange metal is also observed. We’re trying to work out microscopic theories of this special point where the quantum state changes, and stringy models can teach us a lot about such quantum-critical points. Once we have the full framework, we’re hopeful and optimistic that we can take many of the insights from graphene and apply them to this more complicated model. That’s where we are.\nThis article was reprinted on ScientificAmerican.com.', 'In the 1960s the tide in physics flowed towards tackling the strong interactions, as those grappling with the weak interactions had come up against an apparently unshiftable block.\nThe technique that seemed to be the most powerful one was the use of symmetry, and the use of Lie groups enabled a classification to be made that brought the weak interaction into a possible unification with the electromagnetic one. That was truly amazing. The long-distance electromagnetic field that gives us light and radio waves was somehow linked at a deep level with the weak interaction which we only know through its by-products in processes at the quantum level like radioactive decay.\nA second application of the treatment involved another type of symmetry, gauge symmetry, which required the existence of a new type of field, the Yang-Mills-Shaw field, which brought the weak interactions even closer, by giving them a new particle of their own to complement the electromagnetic one (the photon).\nBut – damn and blast – the predicted ‘weak photon’ was massless, which could not be the case in reality. And – damn and blast three times over – Goldstone’s theorem showed that there was no way in which you could generate a mass-possessing weak particle without destroying the symmetry completely.\nFor those few people who refused to abandon the problem, if they had any request from Father Christmas it would be for a symmetry-breaking kit – something that would break the symmetry while preserving it.\nThe situation required the kind of ingenuity of an Orcadian student from Birsay who had happily stuck photos of his native islands on the wall of his Edinburgh digs with Copydex, which at the time was regarded as proof against just about anything – a problem which his landlady encountered when she decided that the pictures infringed the rules of the house. She tried to haul them off the wall and took off a quantity of wallpaper with them, and irately confronted him for his misdeeds. Without the slightest appearance of a pause for thought, he said with a kind of calm authority: ‘Oh, what a pity that you didn’t ask me for the Copydex remover.’\nSomething this was needed with the electroweak interaction – to remove some of the picture-equivalents off the wall without at the same time taking the wallpaper with them.\nThe equivalent of the Copydex remover came indeed not from looking at a wall but from things that are much bigger and solider than the world of very small particles. There were three situations in solid state physics that produced the clues.\nSituation number 1\nWhen atoms join up to form a crystal, their outer electrons loosen and begin to drift. Having lost these outer electrons, the previously electrically-neutral atoms have a net positive charge. The electrons stay in the crystal, not attached to any particular atom, and an electric field can start them moving in a particular direction – this is what forms an electric current. However, the effect of the positive lattice with the little pulls of the various atoms in the structure can be to slow down the electrons – and make them appear to be more sluggish and massive than they would otherwise have been.\nSo one way for a particle to behave as if it has more mass is to put it in some kind of external lattice like this.\nSituation number 2\nA magnetic substance is formed out of many little units which are themselves magnetic – they are called domains. When the substance is very hot, the domains will move around and point in a random mix of ways and from the outside we would never know that there was the potential for magnetism there. But if we let the temperature drop, there comes a point where suddenly the domains click together into pointing in a single direction. Somehow each lines up with its neighbour, but not in a one-by-one fashion but all an overall ‘decision’. We have no way of knowing in advance which direction they will happen to lie along.\nThe reason why they line up in this way is because this is the most stable form they can take. It is as if they have dropped down to the lowest point they can reach. It is a bit analogous to us filling a jar with sugar and tapping it from time to time – to see the level sink as the sugar grains start to fit together better and better. In the case of the magnetic substance, there is no tapping process, and the alignment happens almost in an instant. But there is a parallel in that in each case the little units – whether domains or grains – ‘find’ that by aligning themselves closely a stable ground-state is reached.\nThe magnetic substance when it was hot was very symmetric in that we could turn it around in as many ways as we would like, and in terms of magnetism each direction looked the same. But after we cooled it, we had this sudden transition of the domains into an overall direction. So the individual units still retained their own internal symmetry while the collective result was to make a break.\nSituation number 3\nThis is in a very strange world, the interior of a superconductor. Here electrons can travel through the crystal lattice so smoothly that nothing slows them down in any way and they can travel forever. Here something has happened which is a kind of parallel to the magnet in that there has been a process of internal ordering. The electrons have a kind of overall link between them that enables them to get through the lattice without bumping into any of the lattice atoms.\nIt is a bit like dancing the conga. If a group of people are in a crowded room and all try to make for the door on their own, their will be a lot of bumping into others and an ever-slowing rate of progress, but if they line up in conga formation they can negotiate their way round in a coordinated way and avoid touching anyone.\nWhere does this get us? Well, from situation 1 we see that some kind of background lattice can cause particles within it to slow down and behave as if they have become more massive. So maybe we need to find some kind of background lattice encompassing the whole of space, so that it can somehow interact with the particles we have been studying and cause them to behave ‘massively’. We need in fact a background lattice that consists purely of forces – and that is just a definition of something that we have already become familiar with, a field.\nSo we need an additional field to interact with the field that we already have, in particular the Yang-Mills field.\nBut how will this field get round the constraints of Goldstone’s theorem? – which the Yang-Mills field on its own could not get beyond.\nHere is where situations 2 and 3 come in. This new background field has to have the various symmetries, otherwise we have broken them completely, but since it is a brand-new field with only one task, to help us out of our problem, we do not need to allow it to have all the myriad of options that the more familiar fields do. Indeed, it only requires to have one option, just like the magnetic bar does, and just like the electrons in the superconductor do. There is one single position that the choice of options locks in to – so to that extent it has broken the symmetry – but the break is not so bad that we have lost the symmetry completely.\nIn 1963 Philip Anderson looked closely at what happens inside a superconductor and said that there was something important for particle physicists to be aware of. Anderson was a specialist in the solid state, and went on to share the Nobel Prize in 1977 for his work on the electron interactions in phenomena like magnetism and superconductivity. His studies at Harvard had been interrupted by the war, in which he had been deployed in research on building radio antennas, and after going back to Harvard to complete his training (and where his fellow-students included Tom Lehrer) he went to work at Bell Laboratories, joining a glittering array of researchers there. He found that the sophisticated mathematical techniques of quantum field theory that he had learned from Schwinger at Harvard had practical applications in the advanced radio work. In his background there are professors on both sides of the family, together with a love of the outdoors, and from a sabbatical year in Japan there is a deep interest in Japanese culture and he is a first-degree master of the game of Go.\nThe paper that Philip Anderson wrote in 1963 showed that in a superconductor the ordered cloud of electrons acts like a single quantum system which can be energised, and that the bursts of energy look like particles. The presence of an electromagnetic field affects these excitations in a way that makes them behave as if they had acquired a mass. So, he noted, symmetry was being broken but yet mass was being generated, and so the blocking effect of Goldstone’s theorem had been overcome. The Goldstone difficulty was, he said – ‘not a serious one’. It was this paper that opened the way for the breakthrough of Peter Higgs.']	['<urn:uuid:596d2beb-2f73-45b3-9e6e-a9f6482cacab>', '<urn:uuid:0f06a0f6-4911-4d04-897e-d5c80bfb89f1>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	15	51	3759
9	mom with sick infant compare blood in spit treatment vs kawasaki syndrome treatment	The treatment approaches are quite different. For blood in spit, treatment may be as simple as monitoring if it's from maternal blood, or providing comfort measures for throat irritation. More serious cases may require medical attention. In contrast, Kawasaki syndrome requires specific medical intervention with immunoglobulin infusions (2 g/kg body weight over 12 hours) and high-dose acetylsalicylic acid (30-100 mg/kg body weight daily). While blood in spit often doesn't need hospital care unless accompanied by serious symptoms, Kawasaki syndrome always requires immediate medical treatment to prevent potential heart complications.	['Newborn Spitting Up Blood: Facts You Need to Keep in Mind\nImagine a parent’s reaction when their newborn is spitting up blood. They would be absolutely terrified to imagine that scene let alone witness it.\nRelated Articles :\nJust think about it, it’s just like any other day, and you are a parent holding their beloved newborn inside of your arms, and then suddenly they start hacking and coughing until you find a glimpse of red spilling through their little lips. Horrifying.\nWhat are you supposed to do? Why is your newborn doing this? What caused it? Go a little further down, and you will find everything that you need to know.\nHemoptysis in Newborns\nWhat is Hemoptysis in newborns? Well, it is what we call when newborns are coughing up blood or bloody phlegm. We can’t really say that this is what your newborn has right now, after all, there are many possibilities, but you must never rule out something as dangerous as this.\nHemoptysis happens when your newborn has something wrong with something inside of their lungs. It could be a damaged bronchi, larynx or trachea. This could be associated with cancer if the newborn was older.\nTake caution !\nPossible Causes Why Your Newborn is Spitting Up Blood\nThere are many reasons as to why, but before you panic and start thinking that your newborn is in serious medical condition, you must first check if she is having a fever, a swollen belly, or lethargy. Here’s a small list of reason as to why your newborn would be spitting up blood.\n1. Swallowed Maternal Blood\nFirst, you must look closer and see if your newborn has red flecks that resemble freckles. Their saliva should also be pink tinged, including their spit or vomit.\nIf you see these symptoms, they must’ve swallowed maternal blood. This means that the blood came from you or perhaps from your breasts or nipples when you were breastfeeding your child.\n2. Forceful Spit Up\nNewborns vomit, that’s not something new. However, there are times when during vomiting or spit ups it would cause a small tear in one of the blood vessels within your newborn’s esophagus.\n3. Clotting Disorder\nDuring a home delivery, there are chances that your newborn didn’t get a vitamin K injection. It may be cheaper, but nothing beats the extra care that hospital birthing gives your newborn.\n4. Nasogastric Tube\nIf you have given your newborn the nasogastric tube procedure or maybe something else related to it, he or she might have gotten a trauma or an irritation that is continuously leaking blood.\n5. Pulmonary Arteriovenous Malformation\nThis is a rare type of malformation. It’s when there is an abnormal opening between your newborn’s lung artery and a lung vein, and this happens a lot when it hasn’t been oxygenated. This can be something very expensive to take care of since it requires surgery.\n6. Goodpasture’s Syndrome\nThis affects the kidney and the lungs. This means that there is something wrong inside of your newborn’s immune system, making it attack the healthy cells instead of the bad ones.\nThis is what happens when your newborn has an inflammation of the mucous membrane lining the larynx. The larynx is located in the upper part of the respiratory tract. It may be the reason why your newborn is coughing up blood.\nThis happens when there is an infection affecting the esophagus. The esophagus is important because it is a tube where the food passes until it reaches the stomach.\nBlood from the nose traveled down to the throat. The newborn will spit it up or cough it out. This is very common and not to worry about.\n10. Irritation of the throat from violent coughing\nWhen a newborn has a cough, frequent coughing can irritate the throat and may bleed. This is one minor cause of newborn spitting up blood. Nothing to get alarmed about. Once a cough is healed, so does the irritation.\nAn infection of the lungs is a serious cause of newborn spitting up blood. Your inborn might have gotten infected by the bacteria through the air when somebody infected by it cough or sneezes.\nIf you believe that there really is something wrong with your newborn, immediately go to the hospital and contact a pediatrician. Nothing beats mother’s instinct. Do not delay a treatment for your newborn longer than it’s supposed to; There are chances it could be fatal.\nIf your baby is having symptoms of abdominal distention, green vomiting, lethargy, or a fever, that’s when you must definitely go to the hospital. It’s at times like this when you should search for prompt medical attention.\nHere’s a video on how you are supposed to handle normal spit-up newborns and when they vomit.\nThings you need to remember when newborn starts spitting up blood!\nRead through all of these steps carefully, you don’t want something bad to happen to your newborn now, do you?\nSTEP 1 : Don’t Panic!\nYou should never panic at times like these. Keep your presence of mind. You might cause an accident that makes stress for your newborn, making the situation even worse. You work through the situation as calmly as you can and make sure that you have the rationality to think things through clearly.\nSTEP 2: Observe your Newborn\nGo back up and see the different symptoms and reasons that explain the possibilities of what’s happening to your newborn. If you find that the symptoms are just pint-tinged newborn spit-up or vomit, that means that the blood didn’t come from them, but from you.\nIf you find that there really is something wrong with your newborn and that things are more serious than they seem, you must take procedures to make sure that your newborn is as comfortable as they can be.\nSTEP 3 : Stay Calm and Contact a Pediatrician\nThere might be no need to go to the hospital just yet. Contact your preferred pediatrician and tell them the different symptoms of your newborn and ask them what’s wrong and if it’s serious.\nSTEP 4: Rush to the Hospital\nIf you can’t contact a pediatrician and it seems that you have no idea what’s going on with your baby, and you feel as if there is something really wrong, you have to go to the hospital immediately and give your newborn the care that they need.\nDuring the ride to the hospital, you must make sure that your newborn is comfortable because coughing up blood can be very painful.\nIt is essential in situations like this to never ever panic. If you panic things can result in something negative. Clear your mind and focus on the safety of your child.\nTips on how to minimize your newborn’s spitting:\nHere are natural tips that are easy for you to remember. Simple but essential to help control your newborn’s spitting-up.\n• Ensure that your baby burps after feeding- It will prevent air from building up inside her tummy and lessens spit-ups.\n• Lessen movements- every after feeding, lessen movements that would keep your baby in motion. Do not swing or rock her too hard.\n• Do not overfeed- give fewer portions of milk. Feed your newborn in short intervals. Never make her too full.\nDifferent ways in soothing your baby when in pain:\nYour newborn may not be able to complain because they aren’t able to talk, but they must be in a lot of pain due to that coughing and hacking. Especially if they are coughing up blood. They might even start crying.\nIf you are still unsure on how to soothe your baby when they are in pain, here are a few tips.\n• Swaddle your newborn\nA baby always needs warmth, and they are most comfortable when they are basking underneath their mother’s natural heat. Cover them up with a soft blanket and lay them down on your chest so that they can calm down.\nView more : Sleep Sacks for Baby\n• Distract your newborn\nDistract them from the pain. Try to do anything you can that would grab their attention away from what’s hurting them. You can do many things! Give them their favorite toy, make silly faces, give them soft music, the silence inside of a running car, and especially… the voice of their parents.\nTalk to them as much as you can, even though you know that they can’t understand you.\n• Caress your newborn softly\nSkin contact, that’s what your newborn needs. Give your baby as many cuddles as you possibly can. Mother’s warmth will calm the newborn down. Never stop caressing the baby because the constant rubbing will keep the baby focused on that feeling.\n• Try Tylenol\nIf your newborn starts crying and you aren’t able to stop them from doing so, that must mean that they are in severe pain.\nGive them something light like Tylenol that would take care of their pain for just a moment, in result giving them relief.\nIf you need more explanation and examples as to how you are supposed to soothe your newborn, here is a video that I provided that you must watch in order for you to further understand what it is that you are supposed to do in order for you to calm a crying newborn.\nAre you satisfied with the information that we have provided you? If the situation is already as bad as it can be, don’t delay getting your newborn to the hospital. Stop everything you are doing and go right away. Your newborn needs attention, and if you aren’t a new parent, then you would know this very well.\nIf you found this article useful, don’t forget to share this with your friends or on other social media. Share this to help somebody and maybe even save another newborn that needs it.', 'Kawasaki syndrome is an acute febrile illness characterized primarily by inflammatory reactions of the arterial blood vessels with multiple organ involvement and occurs in infancy (up to 5 years of age). Kawasaki syndrome is mainly found in Japan and with increasing frequency in Germany (about 9 out of 100,000 children).\nWhat is Kawasaki Syndrome?\nInfants and young children suffering from Kawasaki syndrome suffer from severe inflammation of the vessels throughout the body. The inflammation causes symptoms such as fever and a rash.\nKawasaki syndrome (also mucocutaneous lymph node syndrome) is an acute fever illness that primarily occurs in infancy, particularly between the ages of 2 and 5, and is associated with vasculitis, an inflammation of the small and medium-sized blood vessels. See AbbreviationFinder for abbreviations related to Kawasaki Syndrome.\nIn most cases, the Kawasaki syndrome, which is initially very similar to other infectious diseases such as scarlet fever or measles, also manifests itself as a systemic inflammation of the internal organs.\nIn addition to the suddenly occurring fever, there are swellings in the lymph nodes in the neck and throat (cervical lymphadenopathy), non-purulent conjunctivitis (conjunctivitis), stomatitis (inflammation of the oral mucosa) with strawberry or lacquer tongue, a polymorphic exanthema (non-itching, multi-shaped rash on the trunk) and scaly erythema (redness) on the hands and feet characteristic of Kawasaki syndrome.\nThe underlying causes of Kawasaki syndrome have not yet been clarified.\nIt is assumed that the etiology of Kawasaki syndrome is due to an infection with previously unidentified bacteria or viruses whose superantigens (toxic proteins of the pathogens) cause the affected children’s immune system to react with pronounced inflammatory reactions as a result of a genetic defect.\nThe significantly higher incidence (number of new cases) of Kawasaki syndrome in Japan compared to Europe or North America also speaks for a genetic predisposition (predisposition).\nSymptoms, Ailments & Signs\nInfants and young children suffering from Kawasaki syndrome suffer from severe inflammation of the vessels throughout the body. The inflammation causes symptoms such as fever and a rash. The disease is characterized by a gradual rise in body temperature, which is usually associated with chills, weakness and a general feeling of being unwell.\nThe rash presents as painful redness that can appear on any part of the body and occasionally develops into ulcers or cysts. It can also lead to conjunctivitis and swelling of the mucous membranes in the mouth and throat. Also characteristic are the red lips and the reddish tongue, also strawberry tongue. If treated early, affected children usually recover relatively quickly.\nIn severe cases, however, the rash can leave scars or lead to permanent sensory disturbances. If not treated, there is a risk of death. Then the skin lesions worsen and the fever increases. Eventually, there may be complications of the heart and the death of the child as a result. The symptoms usually appear between the ages of one and eight. However, adolescents and young adults can also be affected. Kawasaki syndrome is particularly common in boys.\nDiagnosis & History\nKawasaki syndrome can be diagnosed based on the characteristic symptoms. In addition to the acutely occurring high fever, at least four of the main symptoms of the disease must be detectable in order to assume Kawasaki syndrome.\nIn addition, similar infectious diseases such as measles or scarlet fever should be excluded in the differential diagnosis. The diagnosis is confirmed by a blood analysis. So-called autoantibodies (cANCA), antibodies directed against endogenous antigens, can be serologically detected in Kawasaki syndrome. An increased number of leukocytes (white blood cells) and an increased CRP value (C-reactive protein) also indicate a possible Kawasaki syndrome as inflammatory markers.\nIn order to rule out involvement of the coronary vessels (arterial coronary vessels) or a heart aneurysm (protrusion of the arterial coronary vessels), an echocardiography (heart ultrasound) should be performed.\nIf left untreated, Kawasaki syndrome can lead to an aneurysm of the heart, which can lead to life-threatening secondary diseases such as myocarditis (inflammation of the heart muscle), infarcts, pericarditis (inflammation of the heart sac) and cardiac arrhythmias, which have a significant impact on long-term mortality (mortality rate). With timely diagnosis and treatment, the prognosis is good and the disease usually heals without complications.\nIn most cases, the Kawasaki syndrome in babies and children leads to a very strong fever. Those affected also suffer from a red discoloration of the skin, lips and tongue. The mucous membranes in the throat and mouth also swell, making it difficult to eat and drink.\nIt is not uncommon for babies and children to have Kawasaki syndrome with a rash all over the body and inflammation of the conjunctiva. The patient’s quality of life is significantly restricted and reduced by this syndrome. If left untreated, the syndrome can also cause heart problems and inflammation, which can eventually lead to death.\nThe treatment itself usually does not lead to any particular complications and the course of the disease is always positive. This is done with the help of medication that significantly reduces the discomfort and symptoms. If the treatment is successful, there are no further complications or reduced life expectancy. Consequential damage does not occur with successful treatment of Kawasaki syndrome in babies and children.\nWhen should you go to the doctor?\nA doctor’s visit is necessary if the fever persists or increases. Redness of the skin, an increase in cardiac activity and changes in the complexion must be examined and examined by a doctor. Children up to the age of 5 who suffer from sudden acute symptoms that lead to serious health impairments must be treated by a doctor as soon as possible. Since Kawasaki syndrome in babies and children can lead to premature death in severe cases without medical care, a doctor’s visit is recommended at the first irregularities.\nRed discoloration of the lips and tongue resembling strawberry color should be evaluated. Swelling of the lymph, pain in the lymph nodes or a change in the mucous membranes in the mouth and throat should be presented to a doctor. If conjunctivitis occurs, you need to see a doctor. In the event of cardiac arrhythmias, a doctor must be consulted immediately. The heart muscle can be damaged, which can have serious consequences. If the child refuses to eat and drink enough fluids, there is a risk that the organism will be undersupplied. A visit to the doctor is required without delay so that a life-threatening condition does not develop. If the child shows behavioral problems or an increased need for sleep, a doctor should be consulted.\nTreatment & Therapy\nIn the case of Kawasaki syndrome, the therapeutic measures are primarily aimed at reducing the inflammatory reactions and avoiding aneurysms in the coronary arteries, for which there is an increased risk of manifesting in the second and third week. For this purpose, immunoglobulins, which play an important role in the body’s defenses and cannot be synthesized to a sufficient extent by the children affected by Kawasaki syndrome, are infused (2 g/kg body weight over 12 hours).\nIf necessary, the therapeutic measure must be repeated if the affected child does not respond sufficiently. In addition, high-dose acetylsalicylic acid (30 to 100 mg/kg body weight per day) is used to reduce acute inflammation and fever. As long as there is a risk of a coronary aneurysm, acetylsalicylic acid should continue to be taken at a lower dose (2 to 4 mg/kg body weight per day) because the substance has an anticoagulant effect.\nIf there is a pronounced aneurysm, drugs such as coumarins are recommended to inhibit blood clotting more. If necessary, cardiac surgery measures are indicated for Kawasaki syndrome. In addition, cortisone-like drugs are being researched as part of clinical tests, with at least one additional positive effect for the treatment of Kawasaki syndrome being demonstrated. After the successful end of therapy, regular check-ups by a cardiologist should take place in the case of Kawasaki syndrome in order to rule out long-term damage to the heart (e.g. stenoses).\nOutlook & Forecast\nKawasaki syndrome is a very serious infection that requires medical treatment to avoid complications and possible long-term effects. However, parents of the affected children can help to alleviate the symptoms with simple means.\nIn most cases, cold calf wraps are sufficient to reduce the fever. If these do not help and the fever is dangerously high, a shower can provide relief. Direct cold showers or cold baths should be avoided. The blood vessels in the extremities would constrict quickly and the heart would have to beat faster. This is too much of a strain on the already stressed circuit. If you instead regulate the temperature slowly and not too far, this leads to a moderate drop in body temperature and only minimal strain on the circulatory system.\nBecause children dehydrate very quickly, it is imperative that parents stay hydrated. Patients have to be slowed down in their physical activity in order to avoid possible heart damage and subsequent complications. Simply adding fresh or dried turmeric to food, thanks to its anti-inflammatory effect, helps with both fever and skin rashes, which can also be alleviated with creams.\nA regular cardiological check-up after the symptoms have subsided is advisable so that possible late effects can be treated at an early stage.\nDue to the unknown etiology (underlying cause) of Kawasaki syndrome, no preventive measures exist.\nIn most cases, those affected with Kawasaki syndrome in babies and children have no special or direct options for aftercare. With this disease, however, a doctor should be consulted very early on, so that there are no further complications and no further deterioration of the symptoms.\nAn early diagnosis usually always has a very positive effect on the further course of the disease, so that the parents concerned should contact a doctor as soon as the first symptoms and signs of Kawasaki syndrome appear in the baby and child. The treatment itself is usually carried out by taking certain medications.\nParents should ensure that the dosage is correct and that the medication is taken regularly. If anything is unclear or if you have any questions, you should always consult a doctor first so that no further problems arise. Regular check-ups and examinations by a doctor are also very important for babies and children with Kawasaki syndrome in order to monitor the condition of the affected person over the long term. The child should not exert himself unnecessarily, since the heart is generally weak. In many cases, this disease reduces the life expectancy of those affected.\nYou can do that yourself\nKawasaki syndrome is a serious infectious disease in young children. Medical treatment is necessary to avoid complications and long-term effects. However, there are a number of ways parents can reduce the symptoms.\nThe disease is accompanied by a high fever. Calf wraps or moderate full-body cooling usually lower the body temperature by one to two degrees. Due to the fever, parents should also ensure that their sick child is drinking enough fluids. Still mineral water or unsweetened herbal teas are ideal. In order to avoid complications in the heart, strict attention must be paid to physical protection. Cooling compresses and the application of anti-inflammatory ointments are recommended for treating the rash. Coconut oil – which also has an antiseptic effect – or olive oil are recommended as natural substances. Specially mixed preparations can also be purchased in pharmacies.\nNaturopathy recommends the homeopathic remedy Belladonna for high fever. Furthermore, the administration of vitamin C preparations is recommended, as this vital vitamin is consumed at an increased rate during an inflammatory process. The turmeric root also has a strong anti-inflammatory effect. Parents can buy these fresh in the supermarket or in powder or capsule form (nutritional supplements) in the pharmacy.\nAfter the illness has been overcome, parents should take their child to a cardiologist for regular check-ups in order to be able to promptly identify and treat the occurrence of long-term effects.']	['<urn:uuid:a5b95c67-99c9-4090-8f79-cfadc184cc4a>', '<urn:uuid:ca5fba62-d897-41cd-bc81-e71f712e5673>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T21:09:04.591601	13	89	3624
10	How can graph analytics be applied to optimize business operations and improve decision-making in supply chain and social network analysis?	Graph analytics can significantly optimize business operations through multiple approaches. In supply chain and logistics, graph models can represent hierarchical relationships (such as items per container, containers per pallet, and truck routes) with links containing attributes like distance and weight, enabling optimization of travel time and fuel efficiency. For social network analysis, algorithms like PageRank help estimate influence by evaluating a node's importance based on linked neighbors, useful for suggesting accounts to follow or conducting sentiment analysis. Label Propagation algorithms assist in understanding consensus formation in social communities, while Louvain Modularity helps evaluate social structures in platforms like Twitter and LinkedIn, particularly useful for fraud analytics by identifying unusual relationship densities that might indicate fraud rings.	"['Neo4j includes a growing, open library of high-performance graph algorithms that reveal the hidden patterns and structures in your connected data.\nIn this series on graph algorithms, we’ll discuss the value of graph algorithms and what they can do for you. Last week, we explored how data connections drive future discoveries and how to streamline those data discoveries with graph analytics.\nThis week, we’ll take a detailed look at the many graph algorithms available in Neo4j and what they do.\n15 Graph Algorithms Optimized for the Neo4j Graph Platform\nUsing Neo4j graph algorithms, you’ll have the means to understand, model and predict complicated dynamics such as the flow of resources or information, the pathways that contagions or network failures spread, and the influences on and resiliency of groups.\nAnd because Neo4j brings together analytics and transaction operations in a native graph platform, you’ll not only uncover the inner nature of real-world systems for new discoveries, but also develop and deploy graph-based solutions faster and have easy-to-use, streamlined workflows. That’s the power of an optimized approach.\nHere is a list of the many algorithms that Neo4j uses in its graph analytics platform, along with an explanation of what they do.\nTraversal & Pathfinding Algorithms\n1. Parallel Breadth-First Search (BFS)What It Does: Traverses a tree data structure by fanning out to explore the nearest neighbors and then their sub-level neighbors. It’s used to locate connections and is a precursor to many other graph algorithms.\nBFS is preferred when the tree is less balanced or the target is closer to the starting point. It can also be used to find the shortest path between nodes or avoid the recursive processes of depth-first search.\nHow It’s Used: Breadth-First Search can be used to locate neighbor nodes in peer-to-peer networks like BitTorrent, GPS systems to pinpoint nearby locations and social network services to find people within a specific distance.\n2. Parallel Depth-First Search (DFS)What It Does: Traverses a tree data structure by exploring as far as possible down each branch before backtracking. It’s used on deeply hierarchical data and is a precursor to many other graph algorithms. Depth-First Search is preferred when the tree is more balanced or the target is closer to an endpoint.\nHow It’s Used: Depth-First Search is often used in gaming simulations where each choice or action leads to another, expanding into a tree-shaped graph of possibilities. It will traverse the choice tree until it discovers an optimal solution path (e.g., win).\n3. Single-Source Shortest PathWhat It Does: Calculates a path between a node and all other nodes whose summed value (weight of relationships such as cost, distance, time or capacity) to all other nodes are minimal.\nHow It’s Used: Single-Source Shortest Path is often applied to automatically obtain directions between physical locations, such as driving directions via Google Maps. It’s also essential in logical routing such as telephone call routing (least-cost routing).\n4. All-Pairs Shortest PathWhat It Does: Calculates a shortest path forest (group) containing all shortest paths between the nodes in the graph. Commonly used for understanding alternate routing when the shortest route is blocked or becomes sub-optimal.\nHow It’s Used: All-Pairs Shortest Path is used to evaluate alternate routes for situations such as a freeway backup or network capacity. It’s also key in logical routing to offer multiple paths; for example, call routing alternatives.\n5. Minimum Weight Spanning Tree (MWST)What It Does: Calculates the paths along a connected tree structure with the smallest value (weight of the relationship such as cost, time or capacity) associated with visiting all nodes in the tree. It’s also employed to approximate some NP-hard problems such as the traveling salesman problem and randomized or iterative rounding.\nHow It’s Used: Minimum Weight Spanning Tree is widely used for network designs: least-cost logical or physical routing such as laying cable, fastest garbage collection routes, capacity for water systems, efficient circuit designs and much more. It also has real-time applications with rolling optimizations such as processes in a chemical refinery or driving route corrections.\n6. PageRankWhat It Does: Estimates a current node’s importance from its linked neighbors and then again from their neighbors. A node’s rank is derived from the number and quality of its transitive links to estimate influence. Although popularized by Google, it’s widely recognized as a way of detecting influential nodes in any network.\nHow It’s Used: PageRank is used in quite a few ways to estimate importance and influence. It’s used to suggest Twitter accounts to follow and for general sentiment analysis.\nPageRank is also used in machine learning to identify the most influential features for extraction. In biology, it’s been used to identify which species extinctions within a food web would lead to biggest chain-reaction of species death.\n7. Degree CentralityWhat It Does: Measures the number of relationships a node (or an entire graph) has. It’s broken into indegree (flowing in) and outdegree (flowing out) where relationships are directed.\nHow It’s Used: Degree Centrality looks at immediate connectedness for uses such as evaluating the near-term risk of a person catching a virus or hearing information. In social studies, indegree of friendship can be used to estimate popularity and outdegree as gregariousness.\n8. Closeness CentralityWhat It Does: Measures how central a node is to all its neighbors within its cluster. Nodes with the shortest paths to all other nodes are assumed to be able to reach the entire group the fastest.\nHow It’s Used: Closeness centrality is applicable in a number of resources, communication and behavioral analysis, especially when interaction speed is significant. It has been used to identifying the best location of new public services for maximum accessibility.\nIn social network analysis, it is used to find people with the ideal social network location for faster dissemination of information.\n9. Betweenness CentralityWhat It Does: Measures the number of shortest paths (first found with Breadth-First Search) that pass through a node. Nodes that most frequently lie on shortest paths have higher betweenness centrality scores and are the bridges between different clusters. It is often associated with the control over the flow of resources and information.\nHow It’s Used: Betweenness Centrality applies to a wide range of problems in network science and is used to pinpoint bottlenecks or likely attack targets in communication and transportation networks. In genomics, it has been used to understand the control certain genes have in protein networks for improvements such as better drug- disease targeting.\nBetweenness Centrality has also be used to evaluate information flows between multiplayer online gamers and expertise sharing communities of physicians.\nCommunity Detection Algorithms\nThis category is also known as clustering algorithms or partitioning algorithms.\n10. Label PropagationWhat It Does: Spreads labels based on neighborhood majorities as a means of inferring clusters. This extremely fast graph partitioning requires little prior information and is widely used in large-scale networks for community detection. It’s a key method for understanding the organization of a graph and is often a primary step in other analysis.\nHow It’s Used: Label Propagation has diverse applications from understanding consensus formation in social communities to identifying sets of proteins that are involved together in a process (functional modules) for biochemical networks. It’s also used in semi- and unsupervised machine learning as an initial preprocessing step.\n11. Strongly ConnectedWhat It Does: Locates groups of nodes where each node is reachable from every other node in the same group following the direction of relationships. It’s often applied from a depth-first search.\nHow It’s Used: Strongly Connected is often used to enable running other algorithms independently on an identified cluster. As a preprocessing step for directed graphs, it helps quickly identify disconnected groups. In retail recommendations, it helps identify groups with strong affinities that then are used for suggesting commonly preferred items to those within that group who have not yet purchased the item.\n12. Union-Find / Connected Components / Weakly ConnectedWhat It Does: Finds groups of nodes where each node is reachable from any other node in the same group, regardless of the direction of relationships. It provides near constant-time operations (independent of input size) to add new groups, merge existing groups and determine whether two nodes are in the same group\nHow It’s Used: Union-Find / Connected Components is often used in conjunction with other algorithms, especially for high-performance grouping. As a preprocessing step for undirected graphs, it helps quickly identify disconnected groups.\n13. Louvain ModularityWhat It Does: Measures the quality (i.e., presumed accuracy) of a community grouping by comparing its relationship density to a suitably defined random network. It’s often used to evaluate the organization of complex networks and community hierarchies in particular. It’s also useful for initial data preprocessing in unsupervised machine learning.\nHow It’s Used: Louvain is used to evaluate social structures in Twitter, LinkedIn and YouTube. It’s used in fraud analytics to evaluate whether a group has just a few bad behaviors or is acting as a fraud ring that would be indicated by a higher relationship density than average. Louvain revealed a six-level customer hierarchy in a Belgian telecom network.\n14. Local Clustering Coefficient / Node Clustering CoefficientWhat It Does: For a particular node, it quantifies how close its neighbors are to being a clique (every node is directly connected to every other node). For example, if all your friends knew each other directly, your local clustering coefficient would be 1. Small values for a cluster would indicate that although a grouping exists, the nodes are not tightly connected.\nHow It’s Used: Local Cluster Coefficient is important for estimating resilience by understanding the likelihood of group coherence or fragmentation. Analysis of a European power grid using this method found that clusters with sparsely connected nodes were more resilient against widespread failures.\n15. Triangle-Count and Average Clustering CoefficientWhat It Does: Measures how many nodes have triangles and the degree to which nodes tend to cluster together. The average clustering coefficient is 1 when there is a clique, and 0 when there are no connections. For the clustering coefficient to be meaningful, it should be significantly higher than a version of the network where all of the relationships have been shuffled randomly.\nHow It’s Used: The Average Clustering Coefficient is often used to estimate whether a network might exhibit “small-world” behaviors which are based on tightly knit clusters. It’s also a factor for cluster stability and resiliency. Epidemiologists have used the average clustering coefficient to help predict various infection rates for different communities.\nThe world is driven by connections. Neo4j graph analytics reveals the meaning of those connections using practical, optimized graph algorithms including the ones detailed above.\nThis concludes our series on graph algorithms in Neo4j. We hope these algorithms help you make sense of your connected data in more meaningful and effective ways.\nLearn about the power of graph algorithms in the O’Reilly book,\nGraph Algorithms: Practical Examples in Apache Spark and Neo4j by the authors of this article. Click below to get your free ebook copy.\nGet the O’Reilly Ebook\nCatch up with the rest of the graph algorithms in Neo4j blog series:\nAbout the Author\nAmy E. Hodler , Analytics & AI Program Manager\nIn her career, Amy has consistently helped teams break into new markets at startups and large companies including EDS, Microsoft and Hewlett-Packard (HP). She most recently comes from Cray Inc., where she was the analytics and artificial intelligence market manager.\nAmy has a love for science and art with an extreme fascination for complexity science and graph theory. When the weather is good, you’re likely to find her cycling the passes in beautiful Eastern Washington.', 'Graph Databases for Analytics (Part 2 of 4): Practical Applications\nIs a graph database the solution to your business problem? In this article we\'ll explore the common characteristics of practical applications for graph databases.\n- By David Loshin\n- July 26, 2016\nWe live in an ever-more-connected world. Smart mobile devices are ubiquitous, and many industries are implementing a wide array of sensors, controllers, and similar devices across different domains.\nThese devices are continuously broadcasting streams of data that contain important information about connections. Relational database systems can represent these links but can\'t capture the characteristics of the links, so this explosion of Internet-connected devices is an opportunity for graph-oriented applications.\nAs discussed in Part 1, graph data processing engines can ingest and represent the qualitative characteristics of both the entities and the links among them. The captured information is embedded in the connections between things, not just the characteristics of the things themselves.\nBusiness environments suited to a graph data processing solution share these general features:\n-- Connectivity: First and foremost, the environment involves documenting and understanding connected entities.\n-- Entity volume: There are a large number of entities that can possibly be connected, such as the number of e-commerce website visitors and the products they view.\n-- Entity variety: There are entities with different characteristics, such as individuals with different job skills using a recruiting application.\n-- Link attribution: There are relevant characteristics associated with the connections between entities. For example, a person may have an employment relationship with a company, and that relationship may have a title, a duration, a location, and a salary.\nWithin such environments, graph databases are best used for solving business problems that involve answering questions related to the topology, or shape, of the graph. Some examples include:\n-- Proximity and distance: How close are two entities to each other within the graph?\n-- Centrality: To what extent is an entity ""important"" in a network; for example, who is the most influential person in a social network?\n-- Density: What entities have the least or most connections?\n-- Communication paths: What are the best ways to propagate information between sets of entities?\n-- Similarity and differentiation: How do the characteristics of the relationships expose similarities or differences among the entities? Can clustering be done using network distance metrics?\nUses for Graph Databases\nIf your enterprise collects connected data elements and needs to answer these types of questions, you can probably think of some applications for graph processing. Uses for graph databases include:\nSocial network analytics for marketing and advertising: Identifying influential individuals within a social network helps target your advertising. If you understand the structure of the connections within self-organized communities, you can customize your promotions to maximize responses and increase revenues.\nCybersecurity: The graph paradigm is a good fit for modeling the connections among millions of Internet domains, sites, and servers. You can detect telltale signs of an attempted data breach by analyzing the connectivity patterns (e.g., similar registration details, geographic proximity, or routing paths), transaction patterns (e.g., sources of denial of service attacks or phishing attempts), and links to known malicious sites. Patterns of connectivity can also identify possibly malicious sites as well as behaviors indicative of criminal activity.\nLogistics: A supply chain demonstrates hierarchical relationships that can be graphed, such as items per container, containers per pallet, pallets that fit into trucks, the truck routes between delivery points, and the types and quantities of deliveries made at each location. The links contain relevant attributes such as the distance between origin and delivery or the aggregate weight of the pallets on a truck. You can use graph models to optimize travel time, improve fuel efficiency, and verify that each item was delivered to the right location.\nSmart buildings: Understanding the interoperation of connected sensors and controls (such as thermostats) within a facility is another opportunity for graph processing. Investigating these connections could lead to reducing energy costs, improving air quality, and using predictive models for preemptive maintenance.\nIn this case, the relevant information is associated with the proximity of devices (such as temperature, humidity, and CO2 sensors within a room, or rooms on a building floor). The graph approach is nicely suited to modeling the dynamic relationships embedded within the hierarchies of device connections among the different facilities within a complex.\nIn each of these example cases we see similar features -- each contains a variety of connected things in a context where understanding the connections can lead to business opportunities. In the next article in this series, we will look at some graph analytics and algorithm basics.\nRead Part 3 of the series here.\nAbout the Author\nDavid Loshin is a recognized thought leader in the areas of data quality and governance, master data management, and business intelligence. David is a prolific author regarding BI best practices via the expert channel at BeyeNETWORK and numerous books on BI and data quality. His valuable MDM insights can be found in his book, Master Data Management, which has been endorsed by data management industry leaders.']"	['<urn:uuid:c810dab5-06bf-4782-b51f-1523fbea7fe4>', '<urn:uuid:47b34771-3254-4c28-9ce3-f672fda963e5>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:09:04.591601	20	116	2766
11	What are some of the different reasons why people choose to volunteer as mentors according to the volunteering functions?	People volunteer as mentors for various functions: Values Function to help the less fortunate, Understanding Function to learn more about the world, Enhancement Function to grow psychologically, Career Function to gain career-related experience, Social Function to strengthen social relationships, and Protective Function to reduce negative feelings or address personal issues.	['- Identify the five areas of the mentoring pyramid\n- Develop skills in forming open-ended and quality questions to ask of the mentee\n- Be introduced to active listening techniques\nA mentor is a person or friend who guides a less experienced person by building trust and modeling positive behaviors. An effective mentor understands that his or her role is to be dependable, engaged, authentic, and tuned into the needs of the mentee.\nToday, most youth development organizations recognize the importance of a child having a caring responsible adult in their lives. For children who come from less than ideal circumstances, mentoring can be a critical ingredient towards positive youth outcomes. Developmental psychologist and co-founder of Head Start, Urie Bronfenbrenner said it best, “development, it turns out, occurs through this process of progressively more complex exchange between a child and somebody else—especially somebody who’s crazy about that child.”\nThe word mentor comes from the character “Mentor” in Homer’s epic tale, The Odyssey. Mentor was a trusted friend of Odysseus, the king of Ithaca. When Odysseus fought in the Trojan War, Mentor served as friend and counsel to Odysseus’ son Telemachus. Riverside Webster’s II New College Dictionary 1995 defines a mentor as “a wise and trusted teacher or counselor”. The act of mentoring is a series of ongoing and little successes. You will be able to make a real impact through consistent and ongoing relationship building.\n___ Review the Resources\n___ Complete Activities\n___ Complete the Module 3 assessment\nPDF – Mentor Fact Sheet: This document highlights some of the key aspects of the ChalleNGe program and the role of the mentor in the program.\nPDF – History of ChalleNGe: This website outlines the history of the ChalleNGe program.\nAudio – Radio Show about Wisconsin ChalleNGe: This section of the Bob Hague Radio Show discusses some of the successes of the various Cadets from the Wisconsin ChalleNGe Academy.\nAudio – Commercial for Alaska Academy: This is a radio commercial that outlines some of the skills a cadet will gain through the Alaska ChalleNGe Academy.\nAudio – Commercial for Tarheel ChalleNGe Academy: In this radio commercial, Mike Minter of the Carolina Panthers shares why it is important to never give up and how the ChalleNGe program can help.\nAudio – Commonwealth ChalleNGe: Col. Larry Apell and Cadet Michael Arroyo discuss the graduation rates and the role of the military approach to guiding youth.\n|Values Function||The person is volunteering in order to express or act on important values, such as humanism and helping the less fortunate||“I hear so much about the hard lives these kids have and feel I should do what I can to help.”|\n|Understanding Function||The volunteer is seeking to learn more about the world and/or exercise skills that are often unused||“I know I’ve lived a sheltered life, so I want to know what these kids are dealing with.”|\n|Enhancement Function||The individual is seeking to grow and develop psychologically through involvement in volunteering||“I get such a good feeling when I am helping others.”|\n|Career Function||The volunteer has the goal of gaining career-related experience through volunteering||“I’m considering getting into education and want to see how I get along with children.”|\n|Social Function||Volunteering allows the person to strengthen one’s social relationships||“Two of my good friends are mentors and say I’d be good at it.”|\n|Protective Function||The individual uses volunteering to reduce negative feelings, such as guilt, or to address personal issues||“I want to give a child the role model I never had growing up.”|\n- Remember back to why you agreed to be a mentor. In the text field below try to identify examples for yourself that match the various functions of volunteering. Reflecting on and identifying your reasons now will help you maintain your focus throughout your time as a mentor. Come back and review what you have listed when you need a reminder of why you choose to volunteer as a youth mentor.']	['<urn:uuid:c3e18e58-683c-4db1-a131-99d20de1dcb5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	19	50	652
12	What exercises help prepare for joint replacement surgery?	For joint replacement surgery, you should focus on strengthening your upper body if you're having a lower joint replaced, as this will help you use crutches or a walker after surgery. It's also beneficial to practice the recovery exercises your doctor prescribes before the surgery, as this will make you more comfortable doing them during recovery.	"['Preparing for Joint Replacement Surgery\nThe prospect of having surgery is worrying for everyone, but approaching a procedure that will affect your mobility can be particularly stressful. If you\'re one of the many people getting ready for hip or knee replacement surgery, preparing both yourself and your home can help reduce your pre-op anxiety and make your recovery safer and easier.\nMake sure you understand the procedure you\'re having and what your recovery will be like. Ask your doctor all the questions you need to about such issues as rehabiltation, pain management, your hospital stay and anesthesia, and the implant being used in your surgery. And experts stress that you shouldn\'t be afraid to let your doctor or the hospital staff know if there\'s something about the surgery or your recovery that you don\'t understand.\nSmoking can affect how well you recover from a surgical procedure because the chemicals in cigarettes affect blood flow patterns, delay healing, and slow your recovery.\nJoint replacement surgery is done to increase people\'s mobility and quality of life. If you smoke, this is a perfect opportunity to cut back or quit completely in order to help you move into this more comfortable and active stage of your life.\nOne of the best ways you can ready your body for surgery is by eating well and exercising. If it\'s a lower joint that\'s being replaced in your procedure, work on strengthening your upper body in preparation for crutches or a walker after surgery.\nAlso ask your doctor about the exercises you\'ll be doing during your recovery. If you practice them ahead of time, you\'ll be more comfortable doing them after your surgery.\nIf you don\'t already have a permanent handicapped parking permit, it may be possible for you to get a temporary disabled permit for use during your recovery. This can help you get out and about with more ease during your rehabilitation.\nContact your state\'s Department of Motor Vehicles for more information on these temporary permits. Your doctor\'s office may also have permit application forms.\nStarting a week or two before your surgery, make ""double batches"" of everything you cook and freeze half. That way you\'ll have a week or more of ready-made food that you can easily reheat and eat while you\'re recovering!\nYou can also stock your freezer with frozen meals and other easy-to-prepare foods so that you don\'t have to worry about shopping or cooking right away.\nIn the days before your surgery, go into the kitchen and move everything you use every day out of the cabinets. This way items such as coffee, tea, mugs, plates, and much-used pots and pans will be in a place that doesn\'t require you to bend or reach to get them.\nDo this for items in your bedroom and bathroom as well.\nMost of us are not accustomed to walking with a walker or on crutches. This means that something as simple as a throw rug or an electrical cord can turn into a major fall hazard for a person recovering from a joint replacement procedure.\nBefore your surgery, remove or tape down any rugs and move electrical cords around the edges of the room. You might even need to move or remove furniture to clear safe, unobstructed paths through your house.\nMake sure you have everything you need in one place. Choose a particularly comfortable place in your house and set it up as a ""recovery center"" with your phone, television, reading glasses, books, magazines, medications, facial tissues, and other necessary items close at hand. Don\'t forget to make sure your cell phone charger and television remote controls are with you there too!']"	['<urn:uuid:661feb94-a6cd-4002-a81a-c8dfbafcca32>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	8	56	610
13	compare history establishment siemens telegraph company henderson templar sword manufacturing business	Siemens and Henderson's businesses had different founding trajectories. Siemens was founded in Berlin in 1847 when Werner von Siemens partnered with Johann Georg Halske and secured funding from his cousin to establish the Telegraphen-Bauanstalt von Siemens & Halske, focusing on innovative telegraph technology. In contrast, Frank Henderson began his sword and regalia manufacturing business in Kalamazoo, Michigan in 1850, initially operating under his own name before forming various partnerships, including with Ames Sword Company, and eventually consolidating into the Henderson Ames Company in 1893, which continued until 1923.	"['One of the most constant concerns we discovered within our email field is inspired by individuals trying to learn on Templar swords that he’s often purchased at estate conversion or passed down off family participants who have been members of the order.\nThose who are inquiring questions is assured that they possess an effective sword on Dark ages if the Templars battled regarding crusades. He’s soon upset if they are told that embellished blade they today has actually did not possibly be real because the Templar Signal forbade every ornamentation toward guns. Inevitably this new sword they are inquiring about are a beneficial fraternal Knights Templar sword or “people sword” since they are entitled among loan companies.\nThat it dating proceeded for a few ages from which date, in the 1893, he consolidated his situations which have Ames and you can formed brand new Henderson Ames Team\nThe intention of these pages would be to describe, not just these types of fraternal swords but also, various firms that are produced her or him.\nThis won’t separate new blade since exclusively Templar due to the fact most other fraternal swords such as the Knight’s out of Pythias portray an excellent knight’s direct also.\nGrip: The new grip towards a great Templar sword often is black and frequently ivory. Quite often it will portray the latest get across and crown emblem regarding the order while the shown from the diagram to the left. Particular swords often screen a conventionalized Religious get across and others good triangle and you may get across. Appear to Templar swords try engraved to the customer’s initials towards traction.\nKnuckle Guard: Templar blade knuckle guards generally represent the artist dating mix and you can top but a number of the Yards. C. Liley and Co. swords tell you an excellent knight’s lead as shown in the diagram to help you the newest kept.\nBlade: The newest knife of your own Templar blade is particularly ornate and certainly will often show crusader views and lily functions. Just about any Templar blade produced in the last millennium and you will a 1 / 2 of have a tendency to hold new etched term of the proprietor.\nSheath: A nickel-plated sheath suggests the fresh new blade belonged to help you a good Sir Knight, when you’re a gold plated you’re compared to an effective Preceptor (Canada) otherwise Leader (All of us). Including the blade this type of sheaths are ornate as well as the marks are different regarding manufacturer so you can brand.\nFrank Henderson, a prominent citizen when you look at the Kalamazoo Michigan and you will whose domestic, the brand new Henderson – Royce Palace is actually a well known site even today, first started offering and you can production swords and you can regalia for the 1850. The guy stayed in the business for pretty much a half-century until their passing within the 1899. The company the guy began manage are active until it had been dissolved in 1923.\nFirst Henderson marketed swords less than his personal name Frank Henderson and carry out continue doing very up to he formed a collaboration which have Theron F Giddings during the 1871. New business, Henderson and you can Giddings Co. try an initial resided strategy and also in 1873, Henderson resumed trading because Honest Henderson.\nThe fresh new freshly acquired gadgets is gone out of Chicopee MA, so you’re able to Columbus Kansas\nFrom this big date Henderson first started a link for the Ames Blade Business so that as an agent delivered its type of fraternal swords and you will regalia.\nHenderson taken over the fresh E. Good. Armstrong Regalia Facility, in addition to all of the devices and inventories a year later. Immediately after Henderson’s death from inside the 1899 the firm continued for the on next twenty-couple of years in the event it is purchased and you may demolished by the Yards. C. Lilley and you can Business.\nBrand new Meters. C. Lilley and you will Business ordered this new Ames Sword Providers during the 1925 and you will in identical seasons alter the identity towards Lilley Organization.\nWhen you look at the 1931 they truly became the newest Lilley-Ames Providers proceeded towards the for another 20 years however, on account of insufficient range within products began to falter. C. Age. Ward bought him or her call at 1951 and at this time first started in order to make badges.', 'On this day in economic and business history...\nSiemens (NASDAQOTH: SIEGY ) was founded in Berlin on Oct. 1, 1847.\nFounder Werner von Siemens was determined to develop new ideas in telegraphy, which was still in its infancy -- Samuel Morse had first publicly demonstrated long-distance telegraphy in the United States only four years earlier. In the winter of 1846, Siemens successfully developed a new prototype device, which his namesake company explains in its historical archives:\nThe cornerstone of [Siemens\'] career was the idea of building a pointer telegraph out of cigar boxes, tin plate, pieces of iron and a bit of insulated copper wire. The device was far superior to the apparatus in common use until then, because it no longer worked like a clock mechanism. Instead it automatically controlled synchronization between the transmitter and the receiver -- an entirely new solution in electric communications.\nSiemens collaborated on the device\'s construction with his friend Johann Georg Halske, a mechanic he had met at the local Physics Society. Halske was won over by the new device\'s potential, but the two tinkerers lacked the capital to establish a venture of the proper scale to produce it. Siemens convinced his wealthy cousin, Johann Georg Siemens, to be his financial backer, and the three men established the ""Telegraphen-Bauanstalt von Siemens & Halske,"" or ""Siemens & Halske Telegraph Construction Company,"" on Oct. 1, 1847. Two weeks later, on October 12 -- now considered the company\'s ""Founding Day"" -- the company began operations in Berlin.\nWerner von Siemens led the company to fantastic success until his death in 1892. By this point, the start-up had become an international electrical conglomerate with 6,500 employees worldwide and 20 million marks in annual sales -- it was easily a multibillion-dollar company by modern standards, though accurate conversion records of pre-World-War-I German currencies are very difficult to find. Along the way, Siemens helped wire the world, and the company\'s accomplishments included laying the first Indo-European and transatlantic telegraph lines.\nWerner von Siemens himself invented the first working electrical dynamo (in 1867), the first electric locomotive (in 1879), and the first electric streetcar and elevator (in the early 1880s). His wide-ranging electrical achievements made him something of a Thomas Edison of Germany, a comparison made all the more apt by the fact that several of his key breakthroughs -- particularly the dynamo -- helped Edison build General Electric into an electrical-manufacturing giant of ultimately greater global scale. The electrical unit of conductance, the siemens, is named in honor of his contributions to human knowledge of electrical engineering.\nThe birth of a new metal market\nThe Pittsburgh Reduction Company, predecessor of Alcoa (NYSE: AA ) , was incorporated on Oct. 1, 1888 , roughly two years after founder Charles M. Hall\'s discovery of the first effective way to produce aluminum in large quantities. Thus Alcoa became the first true aluminum company ever created, and thanks to Hall\'s aluminum extraction patent -- granted half a year later -- it had a legal monopoly over this revolutionary new process for nearly two decades. This patent defensibility no doubt helped 24-year-old Hall assemble $20,000 in seed funding (equal to nearly $500,000 today) for the world\'s first aluminum plant in Pittsburgh, which was already becoming known as a major steelmaking town.\nAlcoa embodied the youthful high-tech mindset of Silicon Valley decades before anyone even considered building computers, with recent graduates Hall and Arthur V. Davis -- only 21 when he became Alcoa\'s first employee in 1888 -- guiding the company in its early years. Hall was an idealistic Steve Wozniak to the more business-minded Davis\' Steve Jobs, and the inventor continued his role as Alcoa\'s head of research while Davis stepped into the corner office. By the early 1890s demand for Alcoa\'s aluminum necessitated expansion, and within a decade of Alcoa\'s formation the company operated three plants and had managed to reduce the cost of aluminum by 95.5%. Aluminum, once so rare that it was prized above gold, was becoming a metal used by the whole world, rather than just the elites.\nThe Pittsburgh Reduction Company changed its name to the Aluminum Company of America in 1907. Hall continued to serve as the company\'s vice president of research until his death in 1914, by which point the price of aluminum had fallen to $0.18 a pound, a 99.4% reduction in price from the roughly $32-per-pound price it had commanded before Hall\'s birth. Alcoa\'s position remained so dominant for so long that the U.S. Justice Department filed an (unsuccessful) antitrust suit against the company in 1938. After fending off this suit, Alcoa joined the Dow Jones Industrial Average (DJINDICES: ^DJI ) in 1959. After the removal of Bethlehem Steel in 1997, Alcoa became the sole metals-industry representative on the index for over 15 years, a position it finally relinquished in 2013.\nWorldwide aluminum production, which was negligible before Alcoa\'s creation, reached a worldwide total of 43.9 million tons 120 years after the company was incorporated by one ambitious 24-year-old with one revolutionary idea.\nThe disc that launched a trillion free-trial mailers\nSony (NYSE: SNE ) launched the CDP-101, the world\'s first CD player, on Oct. 1, 1982. For the first time in a century, media had a digital format and a device from which to access it, three years after the first public demonstration of compact discs. The CD player and the CDs it played were both crazy expensive by modern standards -- the CDP-101 cost roughly $900 at the time, and the limited selection of 113 albums retailed for anywhere from $15 to $20 each, which would be equivalent to a price range of $35 to $45 today. But the CD nonetheless represented a way forward for the music industry, which was at the time limited to formats that were either portable but prone to wearing out, or durable but too large to carry.\nA CD\'s storage capacity was initially determined in part by the size of Beethoven\'s Symphony No. 9, which was one of the first albums released at launch. Only 20,000 CDP-101s sold in 1982, but the success of the 1985 Dire Straits album Brothers in Arms -- the first CD to sell more than a million copies -- paved the way for a widespread industry transition to digital formats. Some 400 million CDs were pressed in 1988, and by the 25th anniversary of the CDP-101\'s launch, some 200 billion CDs had been sold around the world.\nPutting a ceiling on American borrowing\nThe first U.S. debt ceiling was put into place on Oct. 1, 1917, with the passage of the Second Liberty Loan Act. This act, one of a series of revenue-raising measures to finance the U.S.\' entry into World War I, offered $3 billion worth of Liberty Bonds to the public at a 4% interest rate. More importantly for the nation\'s long-term fiscal picture, this act rolled up the unused borrowing capacity of previous revenue-raising efforts into one aggregate limit -- a debt limit, now commonly known as the debt ceiling.\nThis limit restricted the amount the U.S. could borrow only under this particular act, but for several decades, later revenue-raising legislation simply amended the Second Liberty Loan Act to increase its capacity. Existing bond offerings made under earlier legislative edicts retained their own separate borrowing limits until June 20, 1939, with the passage of the Public Debt Act, which placed all U.S. borrowing into the same aggregate pool, thus creating the debt ceiling as we now know it today.\nWant to learn more about the U.S. debt ceiling, from its origins in 1917 to the present day? Click here to read a complete history, or click here for an interactive history on the growth of the U.S. debt ceiling.\nThe Truth About the Debt and the Debt Ceiling\nThe U.S. government has piled on more than $10 trillion of new debt since 2000. Annual deficits topped $1 trillion after the financial crisis. Millions of Americans have asked: What the heck is going on? Is it really time to reconsider this whole ""debt ceiling"" thing? The Motley Fool\'s new free report, ""Everything You Need to Know About the National Debt,"" walks you through with step-by-step explanations about how the government spends your money, where it gets tax revenue from, the future of spending, and what a $16 trillion debt means for our future. Click here to read the full report!']"	['<urn:uuid:4cc50353-1438-4cd9-ac71-44940a3d8729>', '<urn:uuid:eb0f9ea4-1c96-4908-a857-60a31dfd9c72>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T21:09:04.591601	11	88	2142
14	how do sawfish use rostral saw hunting and what major threats face population decline	Sawfish use their rostral saw (which contains dense nerves) to locate food buried in the ocean floor, shuffle sediment to surprise prey, and slap schooling fish to stun them. The major threats causing their population decline include fishing (particularly gill nets which easily snare their saw), coastal habitat modification such as damming of rivers and loss of mangroves, and the high value of their fins and saws which means they are rarely released unharmed when caught.	['In order to protect the five species of endangered sawfishes, we need to know where they can still be found today. Colin will work with sawfish experts around the world to undertake a global sawfish survey using environmental DNA (eDNA).\nI grew up exploring tide pools, fishing and snorkelling along the west coast of Australia. At age nine I announced to my family that I was going to be a marine biologist. That started a life-long journey of discovery and adventure with the ocean. I studied at James Cook University in Queensland, first for my Bachelor’s degree and then PhD. Along the way, the opportunity to research sharks and rays presented itself Since then, elasmobranch have been the primary focus of my career for 30 years.\nIt was during my PhD in the late 1980s that I caught my first sawfish...\nTo resolve the current global distribution of sawfishes using eDNA survey techniques.\nSawfishes are considered the most threatened group of marine fishes, with all species on the IUCN Red List as Critically Endangered or Endangered. Sawfishes have disappeared from many countries that made up their historic range because of fishing and habitat loss. As a result of rapid population declines sawfishes are protected by many of their range states, and listed by CITES and CMS. The development of a global conservation strategy identified the need to clearly resolve where populations persist beyond the known refuges of northern Australia and Florida. The rarity of sawfishes, however, makes traditional fishing surveys largely ineffective, unreliable and cost-prohibitive, at the scales needed to locate now scarce individuals. Through a recent local test case in northern Australia eDNA has been demonstrated to effectively survey for sawfish. This successful proof of concept provides the opportunity to conduct broad-scale surveys to resolve the current distribution of all species.\nSawfishes (Family Pristidae) are considered to be globally the most threatened group of elasmobranchs with all five species listed either as Critically Endangered or Endangered by the IUCN Red List of Threatened Species. The decline of sawfish populations is largely a result of their high level of catchability in fishing gear, their occurrence in shallow coastal waters where they commonly interact with human activities, and the high value of their fins and saws (which mean they are rarely released unharmed). In addition, habitat modification such as damming of rivers and loss of mangroves have also played a role in the declines. At present sawfish populations appear to be relatively safe in only two locations – northern Australia and Florida. In these locations national protection, effective management of threats and ongoing monitoring have enabled populations to stabilise or even recover.\nA recent analysis of the distribution of sawfish species demonstrated that all are extinct in significant parts of their historic ranges (30 – 81% decline in area of occurrence, depending on species), with 20 countries having lost all of their sawfishes, and 43 countries having lost at least one sawfish species. This analysis also demonstrated that there are many countries where the status of sawfishes remains uncertain. The lack of data on their current occurrence and distribution makes their conservation significantly more difficult because it is hard to know where to focus limited conservation resources to ensure that sawfish are not lost from more countries and to start recovering them in areas where they remain.\nTraditional approaches to surveying and monitoring sawfish populations involve fishing surveys of potential habitats, and can be manually intensive and costly. This makes undertaking a global survey using these techniques prohibitively expensive and time consuming. New molecular approaches to surveying water bodies (rivers, estuaries and oceans), however, make broad global scale surveys more feasible. The most common approach currently being developed is environmental DNA (eDNA) where water samples are filtered and assayed for fragments of a specific species’ DNA. This technique is becoming more widely used for research and monitoring of pest, invasive and threatened species because of its cost- effectiveness and reliability. We recently demonstrated that this approach can be used to detect sawfish in the flood plain waterholes in northern Australia. This study has demonstrated the potential for this approach to be used to deliver greatly improved information on the occurrence and distribution of sawfish at large spatial scales.\nBy trawling the fish markets and landing sites of Ghana’s coastline, Issah is surveying the patterns in catch composition over time for sharks and rays in artisanal fisheries. In doing so, he is also raising awareness about the best fishing practices that safeguard sharks and rays and garnering fishers’ support for sharks and the conservation of ocean ecosystems in Ghana.\nJuan is collecting environmental DNA (eDNA) samples from the estuaries and mangroves of Colombia’s Chocó region. He is uncovering the presence and distribution of largetooth sawfishes on the Colombian Pacific coast by detecting traces of their DNA left behind as signatures in their environment. The Critically Endangered largetooth sawfish – known locally as ‘El Guacapa’ – is typically found in estuaries and thought to be resident in some of Central and South America’s freshwater systems. Knowing exactly where this sawfish occurs is critical to its conservation.', 'Globally, nearly all sawfish species are declining largely due to coastal habitat threats and over-fishing, but dwindling northern populations of smalltooth sawfish in the Atlantic may get a helping hand from Florida Museum researchers who plan to begin monitoring for the endangered elasmobranch in the Indian River Lagoon.\nAbout 100 million years ago, smalltooth sawfish emerged in the planet’s oceans, teeth splayed perpendicular from a long bill projecting from where their forehead should have been. Not much has changed in their evolutionary blueprint, though the continents have swung into new positions and different currents pump through the seas.\nBut today’s population of Pristis pectinata, an ancient member of the biological order Batoidea which also includes stingrays and skates, is dwindling and may have reached critically low levels. Smalltooth sawfish were once widely distributed along the shallow near-shore waters of the Gulf and Atlantic coasts, but today they are mostly restricted to the waters around Florida’s southern tip.\n“Sawfishes as a group are among the most threatened of marine fishes,” said George Burgess, director for the Florida Program for Shark Research at the Florida Museum of Natural History. “Their decline is emblematic of the threat facing inhabitants of the world’s seas. As Americans and Floridians we have to do our part in conserving a species greatly at risk because of our own fishing and habitat-altering activities.”\nThough their bodies look like that of a shark’s, sawfish are actually large, bottom-dwelling rays whose heads flatten into an elongated sawlike snout — called a “rostral saw” — spiked on each side with a single row of sharp, conical teeth. The saw’s tissue is dense with nerves that help it to locate food buried in the ocean floor where they use it to shuffle sediment and surprise their prey. When threatened, sawfish may sling their head side-to-side, brandishing their serrated projectile in a slicing motion. Using the same motion, they may slap schooling fish, stunning them. Smalltooth sawfishes rostrum’s are edged with 24 to 32 pairs of “teeth,” made from a calcified cartilage material. If damaged, their teeth will not regenerate.\nScientists estimate that the current North American population of about 2,000 is only 5 percent of their historic numbers. They are so uncommon that the species was listed under the Endangered Species Act in 2003. The International Union for the Conservation of Nature also categorizes the smalltooth sawfish as “critically endangered” under its Red List for Threatened Species. Other smalltooth sawfish populations exist off of South America’s Atlantic Coast as well as in the Pacific and Indian oceans.\nConserving Florida’s Atlantic Sawfish\nHistoric and scientific records combine to yield more than 300 instances of sawfish in the Indian River Lagoon on Florida’s Atlantic Coast since the pre-1900s, but their presence there today is not well understood and so Museum scientists plan to begin monitoring for smalltooth sawfish in the lagoon’s waters.\n“Historically, this was a very important place for sawfish, particularly for the adults,” said Burgess, who is planning the smalltooth sawfish-monitoring project. Previously, he also collaborated with other scientists in writing the sawfishes’ recovery plan, as well as in petitioning to list it under the Endangered Species Act. While sawfish are fairly well-studied on Florida’s southwest coast, scientists do not know as much about the populations living off Florida’s eastern coast.\n“Monitoring sawfish catches will give us data on their distribution and abundance in the Indian River Lagoon, as well as their seasonality and how they use specific habitat,” Burgess said. “We lack this basic life history information for smalltooth sawfish, so we need to collect baseline data in order to know what areas need protection to ensure their recovery. Such studies will allow us to monitor the progress of the species’ recovery.”\nIn the future, Burgess plans to use an existing complex of underwater listening devices in the Indian River Lagoon to passively track sawfish that may be present. The fixed listening devices pick up unique “pings” given off by tags and store the information in a downloadable format. Researchers can also use a second method, called active tracking, where they work from a boat and use a hydrophone to track their study species in real time. It’s time intensive and can be only be done for 24 to 48 hours at a stretch, but it covers species activity out of range of the fixed listening devices. Both methods are typically used together. A third method involves satellite tags which transmit data in real time and are programmed to pop off after a certain length of time.\nA Dwindling Species In U.S. Waters\nSmalltooth sawfish are a slow-growing coastal-dwelling species that were once known to range from New York to Brazil. And while nearly 2,000 historic records of sawfish exist for Florida’s waters, there are only about 175 known records from all other states.\nBurgess said sawfish likely declined rapidly in the United States, in large part because they prefer shallow waters near shore and because the people fishing these throughout the 20th Century primarily used gill nets, which were disastrously effective at snaring the sawfishes’ toothy bill.\n“I assume that practically every sawfish ever caught here either was killed or at least had their saws removed because they damaged fishing nets or made great curios,” Burgess said.\nSmalltooth sawfish grow to about 18 feet long, and live 25 to 30 years with most reaching maturity at about 10 years old. Each litter yields between 15 to 20 embryos. They are able to tolerate a wide range of salinities — a trait known to scientists as being “euryhaline” — including salt, brackish and fresh water.\nIn February 2008, a sports fisher in Tampa Bay caught a rare sawfish, though it was not immediately identifiable because its saw had been removed. And on July 18, 2002, a sawfish was caught off the coast of Savannah, Ga., – the first recorded north of Florida since 1963.\nThe Smalltooth Sawfish Recovery Team, tasked by the National Oceanic and Atmospheric Administration with protecting and recovering the species in U.S. waters, identified parts of the Indian River Lagoon habitat critical to nursing sawfish populations back to healthy levels. On Florida’s Gulf Coast, Colin Simpendorfer, formerly of Mote Marine Laboratory, and Greg Poulakis of the Fish and Wildlife Research Institute, have studied sawfish from the Caloosahatchee River to the Keys. This stretch is considered the heart of where smalltooth sawfish spend their early years.\nEncountering Sawfish Today\nRecovery team members established an “encounters database” which helps them better understand how smalltooth sawfish use habitat, and therefore how human activities along the coast may impact their recovery.\n“Sawfish habitat lies within footsteps of every person living or recreating along Florida’s coast,” Burgess said. “Every person who puts a foot in the ocean or builds a dock or boathouse is operating in sawfish habitat.”\nHe said the database helps scientists understand how sawfish have used habitat over time, and how they use it today, which aids the recovery team in determining what habitat is most critical for sawfish survival and recovery. It also provides essential data needed for environmental permitting processes.\nThe National Sawfish Encounter Database, supported by National Marine Fisheries Service funding, is stitching together four existing but disconnected databases that have recorded human U.S. encounters with sawfish since 1782. While the majority of encounters are with commercial or recreational fishers who accidentally catch sawfish, other people also may have chance encounters.\nBut the project is not only computer based. As with many conservation issues, Burgess recognizes that people play a key role in the species’ fate. So he plans to use the database as a tool to meet with fishing groups and individuals along the coast and inform them of the fate of sawfishes, and what to do if they see or hook a fish.\n“If hooked, a sawfish should be kept in the water at all times, any line or debris tangled around the ‘saw’ should be removed, and the fishing line cut as close to the hook as possible if the hook is not easily removable,” Burgess said. “It is better to leave the hook in than stress the sawfish by trying to remove a deeply embedded hook – the hook will rust away quite quickly.”\nIf an angler accidentally catches a sawfish, Burgess and his team ask that they record the date, location, depth and size of the sawfish and send this information, along with any photos or video, to the Florida Museum of Natural History, firstname.lastname@example.org, where it will be incorporated into the National Sawfish Encounter Database.\nBy DeLene Beeland | More articles by DeLene Beeland\n• Learn more about the International Sawfish Encounter Database at the Florida Museum.\n• Further reading: Sawfish Species Profiles\n• Saw a sawfish? Report a Sawfish Encounter']	['<urn:uuid:333b391e-1000-47bf-92c4-4f2d2242c3e8>', '<urn:uuid:9619feec-5de4-47f0-b637-7bc5c9adbba2>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	14	76	2313
15	how does creative commons organization help share educational content legally	Creative Commons is a non-profit organization that works to increase the amount of creativity (cultural, educational, and scientific content) in 'the commons' - which is content available to the public for free and legal sharing, use, repurposing, and remixing. Instead of having 'all' rights reserved, organizations and individuals can make their materials available with 'some' rights reserved.	['Document Publishing Tools\nCreate PDF Online Free – A free tool for converting any document to a PDF.\nCreate PDFs for Free – All downloads are free. Once on your computer, just click to install and you’re ready to start creating professional-quality PDF files from any application the fast affordable way. Pdf995 is compatible with Adobe Acrobat v9.0, Adobe Reader v9.0 and previous versions.\nCreative Commons – This non-profit organization works to increase the amount of creativity (cultural, educational, and scientific content) in “the commons” — the body of work that is available to the public for free and legal sharing, use, repurposing, and remixing. Instead of “all” rights reserved, organizations and individuals can make their materials available with “some” rights reserved.\nDIGG – A social news site. The site’s cornerstone function consists of letting people vote stories up or down, called digging and burying, respectively.\nGoogle Documents – Create and share your work online. Collaborate on others with documents, spreadsheets or presentations. Upload any document.\nOnline ConVert – Free, online converter for documents, e-books, images, video and audio. It also generates hash tags.\nwrite.fm – An easy way to create and share documents. When you go to the link, an unique url is created where you can share writing, pictures, articles, documents, etc.\nZamzar – Convert numerous file types online.\nPractice Writing Resources\nWriting Fun – A flash-based site for elementary students to improve their writing in a variety of forms – how to, examples and organizers.\nWriter’s Toolbox for Building Argument – Exercises for building better arguments in writing.\nTeaching Writing Resources\nElements of Literature – Writing resources for grades six to twelve.\nEvaluation Station – Houghton Mifflin English Grades 6-8 tool for assessing writing. A piece of writing can be uploaded and assessed giving feedback to the student before handing it in.\nFallacious Arguments, A List of – A list of different argumentation types with examples.\nGreat Source: I Write! – Tips and tools to teach, write and achieve writing success. Links for educators, students and teachers.\nGuide to Grammar and Writing – A variety of essay and research paper resources.\nOnline Writing Courses – The project is a free and comprehensive resource that is a collection of open college course that spans videos, audio lectures, and notes given by professors at Harvard, Princeton and MIT.\nTools for Teaching – Strategies for teachers and non-English teachers to help students to write better in all courses.\nWriting Guidelines for Engineering and Science Students – Pennsylvania State University’s expectations for writing proficiency.\nWriting Matters – (MsMkean’s English Country Garden) Plagiarism, Writing Tips, Thesis Statements, Paragraph Unity, Paragraph Sandwich and Final Exercise.\nWriting With Writers – Workshops with real authors and illustrators to teach writing and illustrating concepts.\nYou Can Teach Writing – Resources and strategies for teaching writing in middle school and beyond.\nVisual Publishing Tools\nFlip Book Maker – ReadWriteThink online application for making your own flip book. Print and assemble the final result.\nScratch – A program that makes it easy to create your own interactive stories, animations, games, music and art.\nCreative Writing Prompts – A site dedicated to writing prompts.\nFolding Story – One student starts a story and others collaborate on it. It a group storytelling game, like write and pass in real life.\nFutureMe.org – Students can write an email to their future selves. You could have them email them to a school or teacher address for ‘marking’ or publishing. (Write as a character as a writing-in-role.)\nLetter Generator – A Read.Write.Think. flash application for creating a properly formatted letter.\nPicLits – Create a word poem over an image that can be saved and downloaded or posted to a blog. Play with the effect of using different nouns, verbs, adjectives and adverbs.\nPrompts – A creative writing portal with ideas for writing.\nRandom Logline Generator – A random line is generated to be a starter writing prompt.\nStory Join – Start a story and other members add to it.\nVisual Story Starters – Images to use as a starting point for writing a story; each image has a starting statement from which the students can continue their stories.\nWrite for 10 – Write for 10 minutes about anything.']	['<urn:uuid:f7dec6ed-ed7b-4b81-85a3-ab15fa857463>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	10	57	705
16	initial un general assembly members count	When the first UN General Assembly meeting was held on 10 January 1946, there were 51 nations participating.	['Marking the 70th anniversary of the first meeting of the United Nations General Assembly Secretary-General Ban Ki-moon today said the body has truly become the “Parliament for all people.”\n“The resolutions adopted by the General Assembly may not all be acted on right away. Yet they stand as our common position on the most pressing issues of our times,” Mr. Ban told UN officials and delegates attending the commemoration.\n“These resolutions tell the story of our resolve. They reflect our conviction that the countries of the world coming together can do far more collectively than they ever could alone. Each delegate who speaks, each vote that is cast, every gavel that opens a new meeting adds a little more hope to the world,” said the Secretary-General.\nSeventy years ago on 10 January 1946, 51 nations came together at Westminster Central Hall in London, England, and called to order the first meeting of the UN General Assembly. Among the many people who participated in that meeting was Sir Brian Urquhart, a British Government official who went on to serve the United Nations over four decades. He was among the participants in today’s events.\n“Sir Brian did more than live history,” the UN chief stressed. “He shaped history. And he recorded that history as a brilliant writer and journalist. We are privileged to have him here today.”\nEmphasizing that the General Assembly is more important than ever, the Secretary-General called on the international community to “follow the example of giants like Sir Brian Urquhart – by saving more lives, advancing more progress and promoting ever greater respect for human rights in our world.”\nThe President of the 193-Member body’s current session, who himself turned 70 on Saturday, began his address by highlighting several features from the first ever meeting, such as it lasting just over one hour with only one decision taken – whom to elect as its first President.\n“Interestingly, but not surprisingly, not one of the Permanent Representatives (PR) that day were female – a matter which remains a concern today where only 18 per cent of PRs are female,” stressed Mogens Lykketoft.\n“These features and imbalances aside, the first meeting of the General Assembly was without any doubt the beginning of something special,” he added. “It was a major step forward by what is now commonly referred to as the international community.”\nMr. Lykketoft underlined that following a “horrific period” of war, destruction, genocide and nuclear bombings, nations of the world deliberately decided to come together.\n“They decided to choose the only genuine path to achieve global peace, security, justice, human rights and social advancement,” he declared. “And in the General Assembly, they created the one true space in which ‘We the peoples’ – voices both big and small – would be heard.”\nMr. Lykketoft recalled that today, with 193 members representing 99.5 per cent of the world’s population, the General Assembly has become the “single most representative, deliberative body in the world.”\nIn their remarks, both Secretary-General Ban Ki-moon and President Lykketoft highlighted some of the most notable General Assembly achievements in the past year, including agreeing the Addis Ababa Action Agenda, the 2030 Agenda for Sustainable Development, and the Paris Agreement.\n“We must also expand the spirit of cooperation of 2015 into every area of work of the General Assembly in 2016,” insisted the President of the 70 year-old Assembly.\nOther speakers at today’s commemoration included representatives from Sudan (on behalf of African States), Fiji (on behalf of Asia-Pacific States), Albania (on behalf of Eastern European States), Trinidad and Tobago (on behalf of the Latin American and Caribbean States), Italy (on behalf of Western European and other States) and the United States (on behalf of the host country).']	['<urn:uuid:f423f7ee-e68d-4c97-a3b6-74a7f3c09dd1>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	6	18	621
17	How high is the Virgin Mountain anticline above sea level?	The Proterozoic basement in the core of the Virgin Mountain anticline resides at elevations of more than 2 km above sea level.	"['The northeast-trending Virgin Mountain anticline (VMA) of the North Virgin Mountains straddles the boundary between the unextended Colorado Plateau and the highly extended crust of the central Basin and Range province. The anticline is 50 km long and 8 km wide, is doubly-plunging, and has overturned Paleozoic beds on both limbs. The crystalline core of the VMA is composed of Paleoproterozoic supracrustal and intrusive rocks that record four tectonic events from circa 1740 Ma to 1550 Ma (d1-D4) characterized by distinct structural fabrics and associated metamorphic assemblages and microstructures. The most prominent Proterozoic structure is the Virgin Mountains shear zone (VMSZ), which was initially defined as a crustal weak zone during D1 northeast-southwest contraction and may represent an important crustal suture, as it contains ""exotic lithologies"" commonly associated with ophiolites. A clockwise-rotating strain field during progressive east-west (D2) and northwest-southeast (D3) contractional events resulted in the prominent northeast-strike of this zone. Late Paleoproterozoic, east-west contraction across this zone (D4) resulted in a complex array of linked, dextral, transpressive shear zones which partitioned strain into strike-slip, pure-shear, reverse, and normal sense deformation zones. Deformation within the VMSZ continued to 1600 and 1550 Ma based on syn-tectonic monazite rim ages that are associated with sub 500 degree Celcius (greenschist facies) metamorphic assemblages and textures, and sub 500 degrees Celcius deformational microstructures. Development of these highly fissile shear zones later controlled the geometry of Laramide and Miocene brittle deformation. The Proterozoic basement in the core of the VMA now resides at elevations of more than 2 km above sea level, roughly 2.5 km higher than the elevation of basement in the adjacent Colorado Plateau. The northeast trend and vertical uplift of the anticline is a result of Miocene east-west extensional deformation superimposed on the pre-existing northeast- and north-trending structural grains created during Paleoproterozoic (D2, D3, D4) and Laramide contractional tectonic events. Outward-verging monoclinal reverse faults similar in style to Laramide-aged faults in the Colorado Plateau are present on the east and west limbs of the VMA, and we propose that much of the vertical uplift of the anticline occurred during this time. The geometry of Miocene deformation was both strongly partitioned and directed by these pre-existing structures, and also manifest as steeply dipping conjugate normal faults in the Mesozoic and Paleozoic section that soled in to basal detachments in the Cambrian Bright Angel Shale and at the Great Unconformity. Apatite-fission track (AFT) dates range from 21.7 ± 2.3 Ma directly below the Cambrian-Precambrian unconformity to 14.0 ± 2.5 Ma in the core of the VMA, and indicate that the anticline was unroofed in Miocene time. Short AFT lengths (<13m) with large standard deviations (>2.5) from the 22-20 Ma AFT ages suggest that Proterozoic rocks of the VMA cooled slowly through the AFT partial annealing zone at 22-20 Ma, prior to extension, while longer AFTs suggest rapid cooling and synextensional exhumation from 16-14 Ma. We interpret the former ages to represent pre-extensional erosional cooling of a regionally elevated terrain, contrary to most models which assume a peneplain pre-extensional surface. Our model is consistent with geophysical, sedimentological, and tectonic studies of the region. MOOTS/ASTER Airborne Simulator (MASTER) remote sensing data were acquired over the YMA in order to evaluate the utility of the data for geological mapping in a structurally complex area. These data proved invaluable in producing more than 60 km2 of new geologic mapping by the first author, and recognizing many of the key structures which led to our Proterozoic, Laramide, and Miocene tectonic interpretations.\nEarth and Planetary Sciences\nLevel of Degree\nDepartment of Earth and Planetary Sciences\nFirst Committee Member (Chair)\nSecond Committee Member\nTectonics, Tectonic Development, Proterozoic, Laramide, Miocene, North Virgin Mountains\nQuigley, Mark Cameron. ""Tectonic development of Proterozoic structures and their influence on Laramide and Miocene deformation, north Virgin Moutains, SE Nevada and NW Arizona."" (2002). http://digitalrepository.unm.edu/eps_etds/66']"	['<urn:uuid:041dd362-2848-4077-81d3-62b7502521fd>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	10	22	637
18	i am playing baseball what muscle weakness leads to more neck pain and shoulder problems comparing serratus anterior vs upper trapezius	Both serratus anterior weakness and upper trapezius dysfunction can contribute to shoulder and neck problems, but in different ways. Serratus anterior weakness leads to scapular winging, contributes to scapular dyskinesia and shoulder dysfunction during repetitive throwing. The upper trapezius, when dysfunctional, shows increased activation patterns in patients with shoulder pain and can contribute to impingement through abnormal rotation of the scapula. Excess upper trapezius activity often tries to compensate for weak serratus anterior, leading to abnormal scapular movement and potential injuries.	['Spring training has begun, and youth baseball players all over the country are starting to practice and prepare for their upcoming seasons. My very own 14 y/o son has started his 8th grade season, while having been working with his travel team on the weekends since mid December.\nAs a physical therapist, former player, father, and assistant coach on his 14U team, my first concern is always the health of a player. I see several baseball players in my sports medicine practice ranging in age from 9 year olds to MLB platers. Diagnoses include internal impingement, SLAP tears, little league elbow/shoulder, OCD, UCL sprains, rotator cuff tendinitis, instability, fractures and scapular dyskinesia.\nOne of the hardest things to do in my profession is get inside the head of a young athlete. Many will refrain from mentioning pain for fear of letting down a parent or coach, or out of concern for losing playing time. Society has become too focused on early specialization and winning from an early age. In addition, “travel baseball” has been somewhat diluted and water down by lots of dads who want their sons to play year-round. I often see kids being abused on terms of too little rest or improper recovery after they pitch and catch.\nThe biggest, most athletic and hardest throwing kids undergo the most strain as they are asked to shoulder the load at pitcher, catcher and shortstop early on. Many coaches are counting innings in tourneys and not pitches based on tournament guidelines. Too many kids are pitching on consecutive days without proper rest all in an attempt to win meaning less tournaments at a young age. Fortunately, we are making progress in the sports medicine world thanks to the efforts of Dr. James Andrews and others.\nHigh schools are adopting pitch count regulations this year, and MLB along with Dr. Andrews has developed their site, www.pitchsmart.org, to spread education about injury prevention in youth pitchers. Dr. Christopher Ahmad is on the advisory committee for PitchSmart.org, and he is also the lead author on a new paper detailing an injury assessment tool for young baseball players, The Youth Throwing Score.\nThis is a follow-up to my previous post with respect to scapular strengthening for optimal shoulder function. Serratus anterior weakness is a common issue with overhead athletes, especially throwers. It may present clinically as scapular winging, however, it often fatigues quickly with repetitive throwing and contributes to scapular dyskinesia and shoulder dysfunction. Since throwing is an asymmetrical activity, I like to include some unstable and progressive strengthening exercises for the dominant arm.\nIn the video below (as part of my ‘Functionally Fit’ column for PFP Magazine) I will demonstrate how to use a bottom up kettle bell approach to strengthen the serratus anterior muscle.\nTo see the full article with progressions and regressions check out the link below:\nIn the case of shoulder pain and dysfunction, the lower trapezius and serrates anterior are often implicated as part of the problem. Research has shown that these two muscles often fatigue and don’t contribute equally to the force couple between them and the upper trapezius that facilitates upward rotation of the scapula.\nBuilding scapular stabilization and dynamic stability is a must for those doing repetitive overhead activities such as throwing, swimming, serving, or work-related tasks. It is a given that asymmetries will exist, so optimizing the strength of the rotator cuff and scapular stabilizers is paramount to prevent injury and recover from overuse syndromes.\nTo strengthen the lower trapezius, one of my ‘go to’ exercises is the lower trap raise. It can be done with just the weight of your arms or using light dumbbells. The link below includes the full description for the exercise, and I also embedded the video below.\nIn the next column, I will include a serratus anterior exercise using a kettle bell as a follow-up to this post.\nI work with a lot of baseball players in my clinic. In many cases, I see similar issues and recurring problems in them, especially pitchers. Some of these issues include:\nI integrate routine mobility exercises for my throwers, as well as other overhead athletes (tennis, swimming, volleyball, etc) to better prepare them for training and their sport, as well as prevent poor mechanics and compensatory motion that may lead to aberrant stress on the rotator cuff and labrum.\nThe following video reveals five of my preferred exercises using a foam roller to improve thoracic spine mobility, decrease latissimus tightness, and stretch the pec major/minor while facilitating lower trapezius activation.\nIn order you will see:\nI advise performing 5-10 repetitions of each prior to training and sport. These will help improve performance, optimize overhead mechanics and reduce injury risk associated with overhead sports.\nThis post is dedicated to improving mobility in two areas I commonly find restrictions in among my clients - the ankle and thoracic spine. Specifically, I often find limitations in dorsiflexion and thoracic spine rotation that create undue stress on other parts of the kinetic chain.\nRecently, I wrote an article for the WeckMethod site on how to assess and improve ankle mobility. As a clinician and coach, I see this issue in many runners and athletes I work with. At times, it is joint restriction, while in other cases it is soft tissue limitations that impact mobility.\nThere are several potential reasons why one might possess less than optimal movement in the ankle. The most common causes include: joint stiffness following injury and/or immobilization, soft tissue tightness in the gastroc/soleus complex, scar tissue from a prior injury, anterior ankle impingement, chronic ankle instability and adaptive shortening of the Achilles tendon. Want to read more?\nDecreased mobility in the thoracic spine often creates dysfunction and stress on other parts of the kinetic chain, namely the shoulder and lumbar spine. In many cases, clients will demonstrate asymmetry based on their sport, activity level and injury history. In light of this, I often prescribe a simple, yet effective corrective exercise they can do at home to restore motion. The exercise below is taken from my ‘Functionally Fit’ column I recently did for PFP Magazine.\nIn the full online column, I reveal two ways to do this and the applications for it. This exercise also offers a way to assess your own range of motion, while teaching you a straightforward corrective exercise to improve mobility.', 'Understanding The Upper Trapezius\nDoctor of Chiropractic, Dr. Alex Jimenez explores the relevant anatomy and function of the UT, the role it has in musculoskeletal dysfunction and management ideas in the form of strengthening and loosening.\nThe upper trapezius (UT) is a major muscle that affects movement and stability of both the scapula and cervical spine. The coordination and interplay between the UT and other scapula muscles such as the lower and middle trapezius, serratus anterior, rhomboids, levator scapulae and pectoralis minor, have a large bearing on how the scapula moves and how stable it can become to allow proper gleno-humeral motion. With this in mind, dysfunction in the UT, either as weakness or hypertonicity, has been implicated in a host of shoulder pathologies and cervical spine syndromes.\nAnatomy & Biomechanics\nThe gross anatomy and function of the UT is comprehensively discussed in a landmark study from the 1990’s(1). Johnson and Bogduk (1994) dissected the individual fascicular anatomy of the UT from the superior nuchal line near the occiput, all the way down to the C7 vertebrae, and also the thoracic portion of the\nUT. What they found was that the UT was a multipennate muscle that had no osseous attachments between the occiput to the C7 spinous process. Instead the UT originated from the fascial structure known as the ‘ligamentum nuchae’.\nThe upper cervical fibres from C1 to C6 passed in a downward direction, and traversed to insert into the clavicle. Fibres from C7 and T1 passed horizontally to reach the acromion and spine of the scapula. Its thoracic fibres converged to the deltoid tubercle of the scapula (see Figure 1).\nFigure 1: Anatomy of the upper trapezius\nNote the thicker C7 and T1 fibres\nThe fascicle length of the UT fibres measured around 1cm from the occiput to C3, and from C3 to T12 the fibres were around 7-14cm. These fibres are primarily low-threshold type I fibres, suggesting that their role is not to produce power but instead act as a controller and stabiliser of the scapula.\nThe findings from the Johnson and Bogduk dissectionstudy paved the way for a different interpretation on the function of the UT in scapula movement. As only the upper fibres showed a vertical orientation, this suggested they may have only a slight role in scapula elevation. These fibres are\nrelatively small compared to the lower fibres; volumetric studies have demonstrated that the fibres from C7, T1, and the lower half of ligamentum nuchae are the largest.\nJohnson and Bodguk also found that the UT muscle has an angle and orientation, which are too small to be a significant elevator of the scapula when the arm is down by the side. They act primarily as an upward rotator of the scapula when co-ordinated with the lower and middle trapezius fibres.\nThey also work in a force couple along with serratus anterior to create upward rotation. Without the contribution from the lower trapezius and serratus anterior, the UT cannot create upward rotation in isolation. It needs to work in concert with the other upward rotators.\nThe mechanism of this force couple is via the action of the serratus anterior, which pulls the scapula into protraction. The lower trapezius contracts isometrically to fixate the middle medial border of the scapula so that the serratus anterior can then upwardly rotate the scapula.\nUT then contributes towards upward rotation as the scapula has already begun upward rotation. The UT also elevates the scapula at the top of the shoulder flexion/abduction movement (see Figure 2).\nFigure 2: Force couple of UT with middle/lower trapezius and serratus Anterior\nFurthermore, the majority of the UT fibres attach onto the distal third of the clavicle, and due to their horizontal arrangement, they also rotate the clavicle medially – this rotation compresses and stabilises the sternoclavicular (SC) joint.\nThis SC joint compression allows load to be transferred away from the cervical spine and instead is directed towards the sternum and axial skeleton.\nJohnson et al argue that the increased EMG activity of the UT does not occur to elevate the scapula but instead exists to draw the clavicle medially and upward at the same time to compress the SC joint(1). Therefore, the UT creates elevation by exerting an upward moment on the clavicle and increases the\ncompression loads at the sternoclavicular joint. This means that the cervical spine is not taking the load of the UT during shoulder elevation but the SC joint is.\nThe diagram below shows the angle of orientation of the fascicles, based on the work by Johnston and Boduk(1). A radiograph shows the direction of fibres. In this image the size and cross sectional area of the fibres are also shown based on the thickness of the lines. It can be seen that the fascicles of the lower half\nof the UT are much larger than the upper fibres. The largest fibres are the transverse C6 and C7 fascicles.\nIn summary, the UT works synergistically with the other trapezius muscles (middle and lower) to produce a force couple on the scapula; therefore the UT has both a functional role in movement and stabilisation of the scapula. To highlight the individual and synergistic role of the trapezius muscle, the individual heads work in the following ways;\nFigure 3: Orientation of fascicles(1)\n- UT draws the clavicle backwards, medially and upwards at the sternoclavicular joint. It helps to control the neutral positionof the scapula. It has some potential to produce movement to the cervical spine and contributes to the stability of the cervical spine.\n- Middle trapezius produces scapula retractionand upward rotation, as well as drawing the clavicle and scapula backwards and medially together with UT allowing scapula upward rotation. It controls or resists scapula protraction, and downward rotation.\n- Lower trapezius produces scapula upward rotation, external rotation, posterior tilt and depression. It controls or resists scapula elevation, abduction and downward rotation, as well as controlling the neutral position of the scapula.\nFinally, the UT working with other cervical muscles can also laterally flex the cervical spine to the same side, rotate the cervical spine contralaterally and extend the cervical spine.\nDysfunction Of The Upper Trapezius\nThe UT has an important role to play in both scapula position and stability. This in turn will influence the gleno-humeral joint in positions of shoulder elevation, since scapula mobility has been linked with shoulder range of movement(2,3). During arm elevation, the scapula moves toward internal or external rotation, upward rotation, and posterior tilt. These actions are influenced by the upper/middle/lower trapezius and the serratus anterior as a force couple (mentioned above)(4). Lack of control of these muscles will break down the optimal scapula position and stabilisation/orientation of the shoulder joint. This scapula imbalance has been linked to shoulder pain(5). It must not be forgotten that other factors such as thoracic spine posture, capsule tightness in the shoulder and tightness in pectoralis major/ minor and latissimus dorsi may also influence scapula position(6,7).\nFigure 4: Classic UT trigger points (from Travel and Simons(21))\nTurgot et al (2016) studied the isometric strength of the three heads of the trapezius and linked it to electromagnetic assessment of scapula mechanics(8). They found that shoulders with stronger UT muscles showed greater upward scapular rotation at 30°, 60°, 90°, and 120° of elevation in the frontal plane. Shoulders with stronger middle trapezius had greater scapular upward rotation at 90° of elevation in the frontal plane. Shoulders with stronger lower trapezius showed greater scapular posterior tilt at 90° of elevation in the sagittal plane.\nNumerous studies have identified an imbalance in activation between the UT and lower trapezius in patients with painful shoulders. Specifically, the imbalance identifies overactivity of the UT and underactivity in the lower trapezius(9-13). One study found that using a novel taping technique to inhibit the UT, reduced activation patterns of the UT in patients with shoulder pain(14). This aspect will be explained in detail later in the article.\nPeat and Grahame investigated trapezius, serratus anterior and deltoid EMG in people with and without shoulder pathology(15). They found that in those with shoulder pathology, UT showed increased activity during armelevationand lowering (between 40 to 100 degrees of arm elevation). Also, serratus anterior showed decreased activity at some humeral elevation angles (between 70 to 100 degrees) compared to healthy controls. This increase in UT activity has been found in numerous other studies on UT activationand shoulder pathology(9-13,15,16).\nThis finding may be associated with the increase in clavicular elevation or scapular elevation found in several clinical and kinematic studies(17).\nThe increased UT activation may be viewed as a common compensatory strategy used by people with shoulder pain and pathology to elevate their arm. However, the subsequent increase in clavicle elevation at the SC joint that may be produced by increased UT activity will result in scapular anterior tilt. This offsets the scapular posterior tilt produced at the AC joint by the lower trapezius, reducing the overall posterior tilt of the scapula on the thorax, which may be viewed as a potential mechanism to either cause or aggravate impingement symptoms.\nOther authors have suggested that excess UT activity may attempt to compensate for a weak serratus anterior, and is believed to contribute to impingement through abnormal rotation\nof the scapula(13). Evidence of increased UT activation combined with reduced SA activation (as well as evidence of increased superior translation of the scapula) has been demonstrated in persons with shoulder pain(13,15,18). Imbalances of force production of the serratus anterior and UT can result in a scapula elevation motion (or early shoulder shrugging). This may cause excess superior translation of the scapula, with less efficient upward rotation and reduced posterior tilting. Clinical consequences of these alterations can include subacromial impingement, subacromial bursitis, and rotator cuff or biceps tendinitis, which can progress to rotator cuff tears. There may also be altered acromioclavicular joint forces and possible predisposition to degenerative changes(19).\nIn a study by Leong et al (2016) (20) it was shown that athletes with rotator cuff tendinopathy exhibited higher UT shear modulus during active arm holding than the asymptomatic athletes(20) (higher shear modulus suggests an increase in active and/or passive muscle tension measured on a specialised ultrasound). UT shear modulus was also higher in athletes with rotator cuff tendinopathy than in the asymptomatic athletes during the resting arm position at 0°of shoulder abduction.\nThe findings from such a study cannot however be used to determine whether the increased tension in UT is a result of rotator cuff tendinopathy or if it is a cause of the problem. It is clinically appreciated that athletes with painful rotator cuff may have a scapula at rest that appears downwardly rotated and anterior tilted, and often this is attributed to an overactive pectoralis minor. This position would place extra ‘stretch’ on the UT thus creating a situation whereby the UT had to counterbalance the drag effect of the scapula position.\nUpper Trapezius Myofascial Trigger Points\nFigure 5: UT shrug exercise (note the wide hand placement)\n1. Wide Grip Shrug\nIn a study performed in Brazil, researchers attempted to correlate the anatomy of the trapezius and the accessory nerve entry points into the muscle with clinical locations of myofascial trigger points(21,22). What they found was that of the classic seven clinical points of myofascial trigger points, four correlated well to the actual anatomy of the accessory nerve and the trapezius muscle. These seven points with their characteristic referral patterns are shown in Figure 4.\nUpper Trapezius, Neck Pain, Headaches & Nerve Pain\nIt was suggested in a study by Australian researchers that tightness in the UT is correlated to limited neural extensibility in the brachial plexus(23). Although it is difficult to prove causation, the suggestion is made that those with sensitised neural tissue may have reactive muscle tone in the UT. The weight of the arm at rest would provide a reasonable traction force to the brachial plexus, and the UT may react to protect the brachial plexus by increasing tone and lifting the scapula in order to reduce the traction effect on the nerves.\nFigure 6: Single-arm overhead shrug start position (note angle of abduction)\n2. Single-arm overhead shrug\nWhat is also interesting are the morphological changes in the UT in the presence of neck pain and dysfunction.\nThe muscle fibre type changes from type I to type II, with fatty infiltration and higher percentage of grossly hypertrophied type I muscle mega-fibres, along with poor capillarisation(24-26). Finally, the famous ‘Janda’s Upper Crossed Syndrome’ has been implicated as a causative factor in cervicogenic headaches and patients; this condition is often present with tightness of the sternocleidomastoid, UT, levator, scalenes, suboccipitals, pectoralis minor, and pectoralis major(27-30).\nManagement Of Trapezius Dysfunction\nFigure 7: Single arm overhead shrug finish position (note external rotation of arm)\nA group of Australian researchers investigated the use of a standard shoulder shrug (scapular elevation) at 0 degrees compared to a modified shrug performed with the arm elevated to 30 degrees (with the scapula in slight upward rotation). They found that the arm-elevated position elicited greater activation in the lower and upper trapezius. Therefore, traditional shrug exercises are not as effective as the 30-degree abduction position(31). Below are shown some exercises that may be used to retrain the UT in the presence of weakness and dysfunction.\nHold a barbell with the hand placed as far away as comfortably possible. This will place the arm in some abduction and the scapula in upward rotation. Slowly elevate the scapula towards the ear. Perform three sets of 20 slow repetitions with a hold. This is a great starter exercise for a patient with shoulder pathology because the arm is in relative neutral abduction.\nHolding a dumbbell above the head and the body in a side sit position (this position opens up the shoulder and avoids impingement of the shoulder). Slowly elevate the scapula and rotate the dumbbell from internal to external rotation.\nPerform three sets of 20 reps.\nHold two dumbbells overhead with the arm fully elevated (you need good shoulders for this). This is the same process as exercise 2 above but the exercise is performed bilaterally, and with more abduction of the shoulder.\n3. Monkey Shrug\nFigure 8: Monkey shrug (start position)\nFigure 9: Monkey shrug (finish position)\nFigure 10: Upper trapezius stretch\nFigure 11: Upper trapezius trigger point (position for TrP 2 and 6)\nFigure 12: Upper trapezius trigger point (position for TrP 1)\n1. Upper trapezius stretch\nThe action of the UT is to upwardly rotate and retract the scapula; its role in elevation is questionable. The effect on the cervical spine is to laterally flex towards the same side, rotate away and finally to extend the cervical spine. Therefore, the best stretch position is one that encourages a combination of:\n2. Upper trapezius trigger points\n- Scapula downward rotation, protraction, depression\n- Cervical contralateral flexion, flexion and ipsilateral rotation.\nAbove is an example of stretch to target the UT in a stretch and also ball placement for trigger point releases.\nHold the arm behind the back. This will create downward rotation of the scapula. Protract the scapula and depress the scapula actively. Pull the head to the opposite side and look over the same shoulder. Hold for ten seconds and repeat five times.\nPlace a ball (tennis, spikey, trigger ball) into any of the seven mentioned trigger points. Hold against the trigger for 30 seconds, move off and find another spot.\nThe UT is an important muscle of the scapula, which has a major role in scapulohumeral rhythm. Its key roles are scapula upward rotation, elevation and retraction. Its role as a scapula elevator is not as important. It is a muscle that may become either tight and hypertonic as a compensatory mechanism for other scapula imbalances, or it may become weak as it is dragged into a stretch position by poor scapula posture. Exercises have been presented that are designed to either strengthen the muscle or to release tightness and hypertonicity.\n1. Clin Biomech (Bristol, Avon). ;9(1): 1994. 44-50\n2. Am J Sports Med, 1998, 26: 325–337\n3. Journal of the American Academy of Orthopaedic Surgeons. 2003, 11, 142–15\n4. Clin Orthop Relat Res, 1996, (330): 3–12\n5. Orthop Clin North Am, 2000, 31: 285–293\n6. J Orthop Sports Phys Ther, 2009, 39: 90–104\n7. J Phys Ther Sci, 2015, 27: 1739–1742\n8. J. Phys. Ther. Sci. 2016. 28: 1864–1867\n9. The Physician and Sportsmedicine, 2003. 31(7), 25–32\n10. Mottram, S. L. (1997). Dynamic stability of the scapula. Manual Therapy, 2,123–131\n11. Journal of Orthopaedic and Sports Physical Therapy, 1999. 29, 31–38\n12. Am. J. Sports Med. 2003. 31; 542-549\n13. Physical Therapy Mar. 2000. 80(3):276-291\n14. Phys Ther Sport 10 (2), 45-50. 2009\n15. Am J Phys Med 1977;56(5):223–40\n16. J Electromyogr Kinesiol 2005;15(6): 576–86\n17. Phys Ther 2006;86(8): 1075–90\n18. J Orthop Sports Phys Ther. 1999. 29: 574–586, 1999\n19. American Journal of Sports Med. 2004. 32(2); 484-493\n20. Leong et al (2016) Increased Upper Trapezius Muscle Stiffness in Overhead Athletes with Rotator Cuff Tendinopathy. PLoSONE 11(5)\n21. Travel and Simons (1999) Myofascial pain and dysfunction. Volume 1. Upper half of the body. Second edition. Williams and Wilkins. Baltimore.\n22. Akamatsu et al (2015) Biomed Research International 2015.\n23. Australian Journal of Physiotherapy. 1994. 40(2); 99-103\n24. Manual Therapy. 2008. 13: 258–265\n25. Spine. 2004. Vol 29(13) 1 July:1436-1440\n26. Journal of Electromyography and Kinesiology. 2008. 18:255–261\n27. Cephalalgia, 1994. 14(4): p. 273-9\n28. Cephalalgia, 1999. 19(3): p. 179-85\n29. Man Ther, 2006. 11(2): p. 118-29\n30. The International Journal of Sports Physical Therapy. 2011. 6(3); 254-266\n31. Clinical Biomechanics. 2014. 29(2): 201-205']	['<urn:uuid:03c58168-b65d-4fa7-a7e4-4b13a6d32799>', '<urn:uuid:687f27c4-e469-464d-a255-0dffc19d1d41>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T21:09:04.591601	21	81	3994
19	how does avian dna sexing determine bird gender through zw chromosome testing procedure	Avian DNA sexing determines bird gender by analyzing ZW chromosomes, which are different from the X and Y chromosomes found in mammals. In birds, females carry ZW chromosome pairs while males have ZZ chromosome pairs. Unlike mammals, the female determines the gender. By testing a bird's DNA, the sex can be confirmed by determining which chromosome pairs are present in the sample.	['Avian DNA Sexing\nTesting: 5 Bird sample\nTimeframe: Within 5 working days of receipt of DNA samples at our office.\nWe test a minimum of 5 blood samples and/or 5 feather samples.\nWe cannot test all bird breeds known. Kindly state the bird breed you are testing on the forms you send back to us.\nIs it a male or a female? Get a definitive answer to your question with our avian DNA sexing service.\nNote: The avian test will not detect the sex of the bird in all known bird species.\nThanks to our avian DNA sexing you can now determine with 100% accuracy the sex of a bird and have your results in just 5 working days from the moment we receive your samples. We can test feather samples and/or blood samples. Pricing starts from just £95 for testing 5 blood samples.\nBirds often lack obvious physical characteristics that help distinguish their gender accurately. Even the most experienced veterinarians and bird breeders may find tricky establishing the- Pluck around 5 feathers from the chest or breast of the bird (do not collect naturally moulted or shed feathers as these will most likely produce no viable DNA when examined) gender of certain bird species with absolute certainty.\nCollecting Samples: Feather Samples & Blood samples\nInternational Biosciences can test blood or feather samples. We strongly suggested you get an experience bird handler to collect your bird samples to avoid stressing the animal.\n• Pluck around 5 feathers from the chest or breast of the bird (do not collect naturally moulted or shed feathers as these will most likely produce no viable DNA when examined\n• Place feather samples in a clean, unused zip lock bag. Make sure to clearly label the bag and not to mix feather samples from different birds.\n• Fill out the submission forms and send the samples for testing\nNote: when collecting feathers, examine each one quickly to make sure that you have also plucked the feather tip (the white part of the feather attached to the skin).\nCollecting a blood sample from a bird can be more challenging than a collecting feather samples. The samples can be collected by following the below procedure:\n• Thoroughly wash your hands and also disinfect the bird’s nail (an alcohol wipe or surgical spirit can be used to do this)\n• Using a clean, sanitised toe nail clipper, clip the birds nail. You need to ensure to clip at the right point – clipping too high might cause pain or discomfort. If no blood ensues, try squeezing the nail.\n• Using what is known as a blood card, simply blot the blood drop onto it.\n• Apply some blood clotting powder to the nail that has been clipped – this will help sanitize the area and stop any more blood drops forming.\n• Allow the sample to dry and send to the laboratory.\nCost For Avian DNA Test\n|5 samples||6 or more samples|\nFor any questions or queries about our bird sexing service, do not hesitate to contact us.\nScience Behind The Test\nBird species have a chromosomal designation known as ZW chromosomes which share no genes with the X and Y chromosomes we find in mammals. Females carry the ZW chromosome pairs whilst males have the ZZ chromosome pairs. Unlike what happens in mammals, it is the female who determines the gender and not the male. By testing a bird’s DNA, we can confirm the sex by determining the chromosomes in the sample.\nIs avian DNA testing required in dimorphic species?\nNote that although many bird species do not have characteristics that help distinguish the sexes (such as size, colour of plumage, external appearance characteristics etc), other bird species have very physically distinct males and females. These differentiating characteristics, known as sexual dimorphism, are so obvious in some species that bird DNA testing will not be needed.\nAre there any precautions or advice for the bird DNA test?\nAlthough bird samples are easy to collect, we do suggest ensuring someone experienced does this. Birds can get very stressed by being handled and this can affect their health. Small birds such as budgies and canaries can experience such intense fear from being handled, that they may drop dead. A bird reaction to such situations is to peck or bite; depending on the specie, this can result in an injury to the person handling the bird, who might also release the bird as a reaction to the painful peck.']	['<urn:uuid:0824cbbb-20ac-4737-ac3d-2fdfe53a6b54>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	13	62	749
20	robot emergency response system safety challenges	Emergency response systems using coordinated robots combine unmanned aerial vehicles equipped with cameras and sensors with surface vehicles capable of taking physical samples. This integration allows for efficient assessment of emergencies like oil spills or radiation leaks, with aerial vehicles directing surface vehicles to optimal sampling locations while processing and transmitting data to first responders. However, these automated systems face significant safety challenges and security risks. They are vulnerable to cyber threats that could compromise their operation and safety, potentially causing system damage and environmental harm. To address these risks, comprehensive risk assessment processes are necessary, including hazard identification, risk analysis, and implementation of control measures. The assessment should involve competent personnel with working knowledge of the systems and must be regularly updated to address emerging threats.	['Funding for research to coordinate aerial and aquatic robots could improve emergency response\nOctober 4, 2016\nResearchers at Virginia Tech have been awarded nearly $900,000 by the National Science Foundation to improve emergency response by developing methods to coordinate teams of aerial and surface vehicles.\nDuring an aquatic emergency such as an oil spill or radiation leak, these vehicles could autonomously assess the extent of the contamination and transmit critical data to first responders.\nLeading the project are David Schmale, a professor of food safety and plant biosecurity in the College of Agriculture and Life Sciences, who has developed methods for using autonomous vehicles to detect contaminants, and Pratap Tokekar, an assistant professor of electrical and computer engineering in the College of Engineering who works on algorithms that enable robots to sense and gather data.\nThe research will require them to tackle a notoriously difficult problem in robotics: how to process and integrate information from different sources.\nUnmanned aerial vehicles equipped with cameras or other sensors can map a contaminated area and measure the extent of a spill; vehicles on the surface of the water can take physical samples in localized areas. Combining data from the two types of vehicles could yield information about both the scope and nature of the pollution.\n“You have a certain efficiency with the boat alone and with the aircraft alone, but when you couple them you achieve a whole new level of efficiency. And that’s the most important part,” Schmale said.\nThe aerial vehicles can act as scouts, flying higher for a broader view or lower for finer detail. Using this data, they can direct the surface vehicles to the best locations for sampling.\n“What we are really trying to do is come up with algorithmic tools so that the robots can share data among themselves and autonomously plan their paths,” Tokekar said. “And the ultimate hope is to have this in a loop with the emergency responders, to inform them of the extent and nature of the threat, so they can plan a better course of action.”\nIf the vehicles decide autonomously where to deploy and collect data, and how much information to exchange, the system can assemble a comprehensive picture of the contamination without endangering first responders, and freeing them to focus on other tasks.\nThe researchers will use commercially available aircraft modified with a small computer to allow on-board data processing and wireless communication with other vehicles. The surface vehicles will include a custom 3-D printed science package with a variety of sensors and sampling capabilities.\nSchmale and Tokekar have already conducted preliminary experiments, supported in part by funding from the Institute for Critical Technology and Applied Science; now, the team is gearing up for small-scale testing at local ponds this fall.\nThey want to ensure that the system is intuitive for first responders to use, providing clear, actionable information quickly. So the researchers will field-test their prototypes with the Virginia Tech Rescue Squad, a student group that handles around 1,200 calls every year. Their expertise will help inform the development of a system that would be useful in a real-world emergency situation.\n“The engagement of the Virginia Tech Rescue Squad is going to be key to the success of our project,” Schmale said.\nThe grant also includes funding to develop a robotics unit for a local high school. Using 3-D printers and the high school’s design bay, students will explore how to design an efficient sampling system for an autonomous surface vehicle.', 'Figure 1: Top 15 cyber threats (source: ENISA)\nAutomation and emergent cyber-security threats\nAutomation is the key factor that will be increasingly present in the factories of the future. By definition, automated systems are characterized by a set of software and hardware components that allows computer systems, network devices or machines to function without any manual intervention. Processes are completely automated with the help of control loops and special logic: these systems use several events as input and perform operations based on conditional decision-making and specific control logic. Therefore, operations can be performed without a human operator physically located at the site where the system is installed.\nThis type of systems is used in a wide range of applications, such as control and monitoring systems, data security applications, factory automation systems, automated message response systems, autonomous vehicles etc.\nThe rise in automation leads to significant advantages in economic and safety terms. Some of the benefits are the elimination of the risk of human errors, the improvement of user productivity, the development of standardized operations and a greater operations management. At the end of the processes, the accuracy and precision of the final job is even greater in most of the cases.\nA crucial issue of automated systems is without doubts the high security standards that these devices must guarantee; in fact, they are exposed to spam, virus and security threats, therefore they need a certain degree of protection. Regarding cyber security, threats against automation networks are indiscriminate, but they can carry potentially destructive results. Examples of attacks are network spoofing and denial of service which can cause the loss of process alarms and, indeed, potential serious safety issues which can cause damages to the system itself but also to the external environment (other systems, workers, etc.).\nAn example of a cyber threat against operation technologies that caused significant material, economic and reputational damages was the Stuxnet computer virus . It caused the destruction of the uranium enrichment centrifuges in IRAN through legitimate commands (syntactically and semantically correct) but incorrect for the functioning of the system. The purpose of the software was the sabotage of Iran’s Natanz nuclear power plant. In particular, the virus had to disable the centrifuges of the plant, preventing the detection of malfunctions and the presence of the virus itself.\nThe Stuxnet architecture is extremely complex with three large modules: a worm that damages PLCs and allows the software to self-replicate on other machines, a link that runs the copies created by the worm and a rootkit that hides the virus making it undetectable.\nTo be successful, a cyber-attack against an automated system must learn the information on the targeted plant and its operating methods. This information is acquired by the malware through sniffing operations that is passive interception of data passing through the network in a prolonged first phase in which operation is dormant. An example is Blackenergy3 which was found to be installed and dormant in some control systems of various American utilities for at least 5 years. Indeed, latency is a weakness for this type of attack as there is the possibility that the malware can be identified during normal verification operations, sometimes even fortuitously as recently occurred in Saudi Arabia where investigations to determine the causes of a fire caused the discovery of dormant malware (which had no implications for the fire).\nRisk Assessment as a countermeasure\nRisk Assessment is a methodology aimed at identifying hazards and risk factors that have the potential to cause harm (hazard identification), analyzing and evaluating the risk associated with that hazard (risk analysis, and risk evaluation), and determining appropriate ways to eliminate the hazard, or control the risk when the hazard cannot be eliminated (risk control). Indeed, Risk Assessment can represent a useful weapon against cyber-security threats as main tool for prevention.\nThe aim of the risk assessment process is to evaluate hazards, then remove that hazard or minimize the level of its risk by adding control measures, as necessary. By doing so, a safer and healthier workplace has been obtained. The goal is to try to answer the following questions: what can happen and under what circumstances? What are the possible consequences? How likely are the possible consequences to occur? Is the risk controlled effectively, or is further action required?\nAssessments should be done by a competent person or team of individuals who have a good working knowledge of the situation being studied. Include either on the team or as sources of information, the supervisors and workers who work with the process under review as these individuals are the most familiar with the operation.\nIn general, a risk assessment is developed following the five following steps :\nStep 1: Hazards identification\nHazards can be identified by using a high number of techniques: one of the most common remains interviewing operators and make safety audits and test on the examined systems. Moreover, database could be useful to carry out further analysis on external factors, e.g. identify most frequent cyber threats.\nStep 2: Definition of who may be harmed and how\nOnce hazards have been identified the hazards, the relative levels of risks must be evaluated. It is necessary to understand which component of the system can be damaged, which services can be affected and which level of business disruption could be caused by the cyberattack.\nStep 3: Evaluate the risks and decide on precautions\nOnce hazards have been identified, the step consists of the completely remotion of the associated risks. When it is not possible, then certain control measures should be put in place. A risk assessment matrix is one of the possible methods to quantitatively measure the level of risk and residual risks, but evaluation is growing more and more accurate through forefront methodologies, data analysis, modelling and AI algorithms.\nStep 4: Record findings and implement them\nSignificant findings should be recorded. These findings should include the hazards identified, the risk level estimated, the likelihood and potential impact of each threat, the potential mitigation effect of countermeasures etc.\nStep 5: Review risk assessment and update if necessary\nRisk assessment must be periodically reviewed and updated. Changes could be in terms of new equipment, substances, and or tasks, that have been introduced since the last assessment took place. In case of significant changes on past risk assessments, a new assessment is necessary to consider new hazards, as well as possible control measures that should be introduced.\nNew findings for Risk Assessment of Automated Systems\nThreats mitigation and the improvement of the security levels in the field of automated electronic systems is a widely discussed topic. As a consequence, several methodologies have been developed in order to implement a valid risk analysis in the best possible way. A first help for the identification and mitigation of risks affecting information and automated systems could be provided by software applications. For instance, applications could be designed for the use with a database containing historical data on errors and risks, as well as an approach based on risk assessment and neutralization. This allows excluding the most serious risks at the earliest stages of the life cycle of the information system, while focusing on those that are considered relevant for the examined system.\nBeing automated systems domain very broad, targeted methodologies have been developed to respond to needs of a certain sector. For what concerns Automated Driving (AD) systems, new concepts have been recently introduced, such as a state-of-the-art framework for cyber-security analysis, known as Threat Analysis and Risk Assessment (TARA). It quantifies the likelihood and the impact of attack and combines them in order to derive an attack risk value. A novelty that could be introduced is the bespoke integration of the impact calculation, which incorporates the notion of controllability of an attack by the AD system and/or by the driver . While, for instance, in the Robotic field researchers have proposed an “Attacker” model and they have confronted it with the minimal set of requirements that industrial robots should honour: precision in sensing the environment, correctness in execution of control logic, and safety for human operators. It has been demonstrated that the modelled attacker can subvert such requirements through the exploitation of software vulnerabilities, leading to severe consequences that are unique to the robotics domain .\nThe combination of all studies relating to risk assessment for the various automated systems are useful and crucial to the design of any system and the result will be a safer overall environment for the users to operate, as well as resilient and protected equipment to safeguard continuity of operations and economic value of assets.\n Josh Fruhlinger, “What is Stuxnet, who created it and how does it work? | CSO Online,” 2017. https://www.csoonline.com/article/3218104/what-is-stuxnet-who-created-it-and-how-does-it-work.html.\n Health and Safety Executive, “Risk Assessment: A Brief Guide To Controlling Risks In The Workplace,” Toxicol. Pathol., p. 5, 2014, [Online]. Available: www.hse.gov.uk/pubns/indg163.htm.\n A. S. Boranbayev, S. N. Boranbayev, A. M. Nurusheva, K. B. Yersakhanov, and Y. N. Seitkulov, “Development of Web Application for Detection and Mitigation of Risks of Information and Automated Systems,” Eurasian J. Math. Comput. Appl., vol. 7, no. 1, pp. 4–22, 2019, doi: 10.32523/2306-6172-2019-7-1-4-22.\n T. Stolte, G. Bagschik, A. Reschka, and M. Maurer, “Hazard analysis and risk assessment for an automated unmanned protective vehicle,” arXiv, no. Iv, 2017.\n D. Quarta, M. Pogliani, M. Polino, F. Maggi, A. M. Zanchettin, and S. Zanero, “An Experimental Security Analysis of an Industrial Robot Controller,” Proc. – IEEE Symp. Secur. Priv., pp. 268–285, 2017, doi: 10.1109/SP.2017.20.']	['<urn:uuid:edf8028e-6f1f-46a0-b5b0-cd0989580a7a>', '<urn:uuid:f1dc563a-8962-41ca-88bb-9e9d91226edf>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	6	127	2157
21	What's special about Scotland's national flag?	Scotland's national flag is the Saltire, which features a white St Andrew's cross on a blue background. This flag is incorporated into the Union Jack, which is the national flag of the United Kingdom. It's important to note that the Lion Rampant (a red lion on a yellow background), which people often mistakenly use, is not actually a national flag but is the personal flag of the Queen in her capacity as Queen of Scotland and can only be flown by her special representatives.	['1. What is heraldry?\nHeraldry is the science of coats of arms and armorial bearings. These “heraldic achievements” were originally designed to help identify knights in armor, on the battlefield or during medieval sporting events known as tournaments. In Scotland arms are recorded in the Public Register of All Arms and Bearings, which is kept in the offices of the Lord Lyon, King of Arms.\n2. Is heraldry just a relic of the medieval past?\nCertainly not. New grants of arms are still made by the Lord Lyon. Besides, whenever schools or sports teams pick a motto, an emblem, and colors, they are practicing heraldry, even if these are never officially granted or recorded.\n3. What is an armiger? An armiger is an individual who has been granted or has inherited the right to a specific heraldic achievement (shield, crest, mottos, supporters etc.). Only one individual has the right to his or her arms, undifferenced, though others may show their loyalty to a particular armiger (say, a clan chief) by wearing their crest surrounded by a strap and buckle. In addition to individual armigers, however, there are some corporate grants, to cities, universities and so on. The Use of Corporate Arms\n4. What are the orders of the peerage? In order of precedence there are five orders of peerage in Scotland: Duke, Marquess, Earl, Viscount and Lord of Parliament. The Lords of Parliament are called Barons in England. Confusingly there are also people called barons in Scotland, but they are not (at least by virtue of their baronies) peers. (See leaflet no. 21, Titles, a Primer.)\n5. What other people may be armigers in Scotland? In addition to peers and feudal barons, there are other men and women who hold Scottish arms. One category is people with a Territorial Designation (See leaflet 20). Others may simply inherit their arms. People can also apply for arms themselves to the Lord Lyon.\n6. How can I become an armiger? Detailed information on how to apply for arms may be found in the attached leaflet (See leaflet 4). Grants are not typically made to people who are not citizens of the United Kingdom (General Colin Powell was a notable exception), but non-citizens may apply for a cadet-matriculation, if they can show, for instance, that they descend from a Scottish armiger. (See leaflet no.8)\n7. What is a “laird”? Is it the same as a Lord? No, it is not the same at all. Most often the word “laird” is used very broadly to indicate a landowner, who may also be (but is frequently not) a member of the peerage. As could be inferred from the fact that companies market holdings as small as a square foot of land by saying that ownership would entitle the purchaser to call themselves “laird,” calling oneself “laird” in no way makes one a member of the peerage or entitles one to become an armiger. Territorial designations will only be considered for holdings on which a proper house has been built.\nSee leaflet 20 and note on lairds on Lyon Court Website.\n8. Can I become a Lord by buying a square foot of Scottish soil? No.\n9. What allows some women to be called “lady”? Traditionally the title “lady” is used for one of three categories of woman: a peeress in her own right, the wife of a peer and the wife of a knight. The wife of a chief, if not also a peer or knight, should be addressed as “Madam.”\n10. What are the flags of Scotland that it is OK for me to fly? The national flag of Scotland is the Saltire, a white St Andrew’s cross on a blue background. This also forms part of the composite flag known as the Union Jack, which is the national flag of the United Kingdom of Great Britain and Northern Ireland, of which Scotland is a constituent part. Similarly, the United Kingdom is part of the European Union, whose flag may also, therefore, be flown in Scotland. A common mistake is for people to fly the Lion Rampant (a red lion on a yellow background). This is not a national flag. It is the personal flag of Her Majesty the Queen in her capacity as Queen of Scotland. Only her special representatives may do this.\n11. Who can wear a feather in their cap, and how many? This is not a matter of law but of convention. The simple rule would be that only a chief of the name may wear three feathers and other armigers, even peers, may only wear one. If you are not an armiger, you should not wear any at all. A very few armigers may wear two feathers. To wear two they would need to be either the head of a branch of a clan, so recognized by their chief, or the chairman of a society like the Society of Scottish Armigers, where that privilege was recognized by the Lord Lyon.\n12. How do I address a Duke, an Earl or other peer? (See Leaflet 21)\n13. Who can fly a banner? Banners are governed by the rules of heraldry, whether personal or corporate.\nSee leaflet 3.\n14. Can I get my family or clan coat of arms? In general terms (there are a few “armigerous clans or families”) only individual armigers have coats of arms. Displaying another person’s arms is considered akin to identity theft and can be prosecuted by and before the Lyon court. As mentioned above, however, it is acceptable for a member of a clan to display their chief’s crest surrounded by a strap and buckle.\n15. What makes some extended families a “clan”? In the past the word “clan” was used only about extended family groups from the Gaidhealtachd, the traditionally Celtic, Gaelic-speaking Highlands and Islands. Families from the East of Scotland (like the Leslies) or from the south (the Scotts or Elliots, say,) many of whom had a Norman or Anglo-Saxon origin, did not use the term. This distinction has now largely disappeared to be replaced with another. In simple terms, and with a few exceptions, to be considered a clan you must have a chief so recognized as chief of the name by the Lord Lyon and eligible to sit on the Standing Council of Scottish Chiefs.\n16. Who is the Lord Lyon? The Lord Lyon, King of Arms, is one of the Great Officers of State in Scotland, and the person responsible for regulating heraldry in Scotland. He issues new grants of arms and presides over the oldest heraldic court in daily operation anywhere in the world. (See Lyon Court Website)\n17. How do you get to be a chief? Most often you inherit the position. In Scotland this can happen through the maternal line. If a clan or family has no chief, because the line has died out, a derbhfine of leading clansmen can be instituted to elect one of their number.\nSee leaflet 12. Their choice would still need to be ratified by the Lord Lyon as chief of the name, at which point he or she might expect to be invited to join the Standing Council of Scottish Chiefs. Certain individuals have attempted to short-circuit this process, simply proclaiming themselves chief, but their doing so carries no weight in the wider Scottish community. The title “chief” – like the wearing of three feathers – is understood to be the preserve of the Standing Council, and should not be used loosely by others in the Scottish community.\n18. What is the difference between a plaid and a tartan? Originally a plaid was a garment, something like a great cloak, which one could wrap around oneself or lay on the ground to sleep on. Tartan is the name for the pattern with which such garments were woven. (See leaflet no 20)\n19. Can I wear whatever tartan I like or are there rules? Tartans are not part of heraldry as such. They are not the personal property of a chief in the way other “clan” insignia typically are. While most tartans are identified with certain specific clans or families, or sometimes regiments or regions, there is no law against wearing whatever one likes.\n20. What is the relationship between Scotland, England and the United Kingdom? Who are the British? Scotland today is a country which forms part of the United Kingdom of Great Britain and Northern Ireland. In turn the UK forms a part of the European Union. Sovereignty is shared within and between these three layers of power and government. England is by far the largest part of the United Kingdom. Scotland accounts for about one twelfth of the population of the UK, roughly the same ratio that Texas represents within the United States. Unlike Scotland, Wales and Northern Ireland, England does not have its own government and is governed only by the UK and EU Governments. Originally the term British referred to a specific ethnic and linguistic group in these islands, who spoke P-Celtic or Brythonic Celtic dialects. Today these are mostly identified with Welsh and Breton. There was a large British kingdom in the land which came to be known as Scotland, around the area of Strathclyde and with its capital at Dumbarton, which means “fort of the Britons.” After England and Scotland signed their Act of Union the term was revived to indicate the people of the combined area, including England, Scotland and Wales. The term “Great Britain” is a translation of the French “Grande Bretagne,” which the French used to distinguish the area from the region known as Bretagne or Brittany in their own country. It is as absurd to distinguish between Scots and “Brits” as between Texans and Americans. If Scotland left the United Kingdom, those remaining would probably revert to the name “English” and the term “British” would disappear from the political lexicon the way “Yugoslav” or “Czechoslovak” have done.\n21. An explanation of Territorial\nDesignations is to be found in Hot News. The Hot News button is found in\nthe Website navigation bar on the left hand side of all pages.']	['<urn:uuid:cc48bcb8-6aa6-477c-a856-383ea8669a63>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	6	84	1687
22	What kinds of athletic or competitive activities did Joey Webber participate in before becoming the Portland Timbers' mascot?	Before becoming the Timbers' mascot, Joey Webber competed in various athletic activities. He participated in state forestry and timber competitions, excelling in events like pole climbing, jack double bucking, fire hose relays, axe throwing, log rolling and hot saw operation. He was also a competitive rugby player and was part of the U.S. National Rugby Team's U-19 pool from 1996-97. Additionally, he competed as a bare-back bronco rider in the Northwest Professional Rodeo Association from 1996-2000 and the Professional Rodeo Cowboys Association in 2000.	['Embedded in long-standing tradition, the Timbers feature a unique mascot. Free of costume, big feet or cartoon-like gimmicks, the Timbers’ mascot is a true outdoorsman. With his chainsaw in tow, Joey Webber, known by fans as “Timber Joey,” helps create a one-of-kind atmosphere at Providence Park.\nThe tradition of having an authentic lumberman as the club’s mascot began in the 1970s when the Timbers competed in the North American Soccer League. Leading the way was Timber Jim (Jim Serrill), who created and continued several fan-favorite traditions during his 12 seasons as the club’s mascot and icon (1978-82; 2001-07). Initially volunteering for the role, he captured the hearts of fans with his rope stunts, hanging from rafters and light poles with his revved up chainsaw in hand.\nFollowing Serrill’s retirement in January of 2008, Webber has since carried on the club’s match-day traditions and possesses an inspirational charisma, which can bring the crowd to its feet with a quick rev of the chainsaw. A beloved Timbers match-day tradition is the sawing of a slab off a log following every Timbers goal and presenting the slice to the goal-scorer in a post-match ritual.\nWebber was born and raised in the timber town of Philomath, Ore., where he attended the School of Forestry as a youth. He competed in state forestry and timber competitions and ranked in several events including pole climbing, jack double bucking, fire hose relays, axe throwing, log rolling and hot saw operation. Growing up, Webber was also a competitive rugby player and was a member of the U.S. National Rugby Team’s U-19 pool for two years (1996-97). He competed as a bare-back bronco rider in the Northwest Professional Rodeo Association from 1996-2000, and the Professional Rodeo Cowboys Association in 2000.\nWebber is active in the Timbers community outreach efforts, joining players and staff in visiting hospitals, participating in charity events and is a part of a variety of other key community initiatives. When not in his trademark silver hard hat or Timbers suspenders, he operates his own construction company that specializes in concrete finishing and also serves as the security manager at a local restaurant.\nAn official induction ceremony for Webber, along with Serrill, took place during halftime of a Timbers exhibition match against Italy’s Juventus Primavera on June 14, 2008.\nIf you would like to request that Timber Joey appear at your event, please fill out the attached Appearance Request Form.\nHistory of the Log Slab Tradition:\n“No, you can’t bring any chainsaws in the stands. Are you crazy?” That’s what former Timbers general manager Keith Williams said to Jim Serrill back in the late 1970s. At the time, little did Williams know that Serrill would become the team’s long-time icon as “Timber Jim” and that his saw would establish a unique and beloved Timbers tradition – the cutting of the “log slab.” After each goal scored by the Timbers at Providence Park, a slice is cut from the victory log behind the north goal and celebrated in a cloud of sawdust and cheered by the roar of a chainsaw and thousands of voices. The log slab is then paraded through the stands, and ultimately, presented to its rightful owner – the goal scorer – in a post-game ceremony. This time-honored tradition has helped make Timbers matches at Providence Park a one-of-a-kind soccer experience in the United States. Over the years, the players have changed, the uniforms have changed, and even the mascot has changed, but the tradition of cutting the log slab after each Timbers goal lives on, and the spirit of the “log slab” will continue to inspire Timbers fans for years to come.']	['<urn:uuid:d3357171-0e00-4b91-b5e4-eab7c3d77396>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	18	84	608
23	what difference between content marketing strategy content strategy content plan explain	Content marketing strategy is the 'why' - explaining why you're creating content, who you're helping, and how you'll help them uniquely. Content strategy is broader, focusing on the creation, publication, and governance of useful content. A content plan is tactical, documenting specific execution details like who will handle each task. The content plan should only be created after developing a content marketing strategy. It's similar to a marketing plan specifically for content, including details about key topics, content creation, sharing schedules, and calls to action.	['Developing a Content Marketing Strategy\nContent marketing strategy, content strategy, and content plan.\nPeople often use these terms interchangeably (which is understandable, as the lines are somewhat blurry), but each is a bit different:\nContent marketing strategy\nAt its core, your content marketing strategy is your “why.” Why you are creating content, who you are helping, and how you will help them in a way no one else can. Organizations typically use content marketing to build an audience and to achieve at least one of these profitable results: increased revenue, lower costs, or better customers.\nWANT MORE? Details on how to create your content marketing strategy are discussed below. You can also speed up the process by following the six steps in How To Write a 1-Page Content Marketing Strategy.\nOn the other hand, content strategy delves deeper into (in Kristina Halvorson’s words) the “creation, publication, and governance of useful, usable content.” Note that content strategy often goes beyond the scope of a content marketing strategy, as it helps businesses manage all of the content they have.\nWANT MORE? Learn about The 3 Strategic Pillars Behind Every Winning Content Strategy.\nIn contrast to the other two, a content plan is very tactical. It documents the specifics of how you will execute your strategy, and who on your team will be handling each task. It’s important to understand that you need a content marketing strategy BEFORE you build your content plan. Think of it as a marketing plan that specifically relates to content; thus, it should include details such as the key topic areas you will cover, what content you will create, when and how to share your content, and specific calls to action you will include.\nWANT MORE? If you are interested in planning, check out 5 Strategic Frameworks To Take the Guesswork Out of Content Planning.\nDo I really need to create a content marketing strategy?\nYes! As we’ve learned through our annual research, not only do you need a strategy, but you also need to document it. Those with a documented content marketing strategy:\n- Are far more likely to consider themselves effective at content marketing\n- Feel significantly less challenged with every aspect of content marketing\n- Generally consider themselves more effective in their use of all content marketing tactics and social media channels\n- Were able to justify spending a higher percentage of their marketing budget on content marketing\nWhat should my content marketing strategy include?\nThink of a content marketing strategy as an outline of your key business and customer needs, plus a detailed plan for how you will use content to address them.\nWhile there are no definitive “templates” for building a content marketing strategy — each one will be unique to the business that creates it — there are five components that they commonly include:\n- Your business case for innovating with content marketing: By communicating your reasons for creating content, the risks involved, and your vision of what success will look like, you are much more likely to gain executive support for your strategy — and to get permission to make a mistake here and there as you figure out what works best for your business.\nLearn more: Read How To Make a Better Content Marketing Case With ROI to find out how to calculate the value content adds to your business.\n- Your operations plan for content marketing: This covers how you’ll govern, manage, and measure the impact of your content efforts. Operations planning helps you clearly define your content purpose, mission, and objectives, so you can put the right team resources, processes, and systems in place to execute your program successfully.\nLearn more: To develop organized, efficient processes that empower your team to achieve their goals, follow the five steps for building a content operations workflow.\n- Your audience personas and customer journey maps: This is where you describe the specific audiences for whom you will create content, what their needs are, and what their content engagement cycle might look like. You may also want to map out content you can deliver throughout their buyer’s journey in order to move them closer to their goals.\nLearn more: Follow this five-step guide to learn how to mind-map your content and increase its reach.\n- Your brand story: Here, you characterize your content marketing in terms of what ideas and messages you want to communicate, how those messages differ from the competition, and how you see the landscape evolving once you have shared them with your audience.\nLearn more: Use these storytelling structures to build, strengthen, or repair your brand.\n- Your channel plan: This should include the platforms you will use to tell your story; what your criteria, processes, and objectives are for each one; and how you will connect them so that they create a cohesive brand conversation.\nLearn more: Wondering which social channels will help you strengthen your audience engagement? Discover the answers by building a social media content plan.\nDo I need to share our content marketing strategy with other teams/departments in my company?\nWe’ve found that it’s beneficial to give everyone in your organization access to your content marketing strategy — even those who may not be directly involved in the content marketing process.\nThis is particularly critical in large organizations, as it can help keep siloed teams on the same page, minimize duplicated efforts, and ensure that everyone is working toward the same content goals. But sharing your documented strategy is also good practice for businesses that are just starting out with content marketing, for content teams that rely on internal or external subject matter experts, or for companies that outsource any part of the content creation and distribution process.\nOf course, how you communicate your strategy depends on the structure and culture of your organization. In some cases, it may be appropriate to share your full documentation. In other cases, it may make more sense to create targeted summaries for certain stakeholders (for example, busy executives, or external agencies) based on how your content marketing strategy will impact their particular roles, processes, and objectives.\nIn short, consider this: How can you use the principles of content marketing to “sell” content marketing throughout your organization? What do people care about most? This should help you determine which components of your content marketing strategy are most appropriate to share with each team.\nHow often should I update my content marketing strategy?\nSome parts of your strategy should stay consistent even as your content marketing program grows and evolves — namely, your mission and business goals. In fact, these two things are so key that you may want to put them on a Post-it note so you can keep them in view whenever you are working on your content. (For example, at CMI, we use them as part of our acceptance criteria for every editorial content submission we receive.)\nHowever, other aspects of your content marketing strategy will likely benefit from being reviewed and updated periodically. To ensure that your content marketing program remains on target, consider revisiting your channel strategy, core topics, and team processes on an annual basis — or more often if you are just getting started.']	['<urn:uuid:eca3277f-e182-4b80-90ca-a109116d3c70>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	11	85	1202
24	What was discovered about the protein called LotP during citrus host infection, and how does it respond to high osmotic conditions?	LotP was found to be induced more than 7-fold during citrus host infection compared to the phsyllid vector. As a member of the LdtR regulon, it was shown to be induced under high osmotic conditions and to have specific interactions with chaperones.	['Huanglongbing (also known as citrus greening disease) is the major cause of crop and tree loss in many parts of Asia, Africa, and now nearly the entire American continent. Although this devastating disease has been known in China since the end of the 19th century, no sure has been found.\nThe gold standard in finding antimicrobial agents is the ability to culture the responsible microorganism under controlled laboratory conditions. One of the main obstacles in advancing the search for treatments against the citrus greening bacteria, Candidatus Liberibacter asiaticus (CLas), africanus (CLaf), and americanus (CLam), is the inability to culture CLas. This standard scientific approach is out of the research communities grasp; consequently, efforts have headed in numerous directions of disease management and control in order to understand these complex bacteria.\nSince 2013, our research team has been fully committed to finding rational and sustainable solutions for citrus greening disease. Our research strategy embraces short and long-term goals aimed to help the industry win the fight against citrus greening.\nOur lab has identified several key CLas proteins that have been deemed critical for bacterium survival within the citrus host. Since these proteins are involved in significant biological processes, a biochemical approach was used to identify chemicals that inhibit these critical proteins in CLas. The inactivation of these proteins resulted in effective treatment of the disease. Using this approach, several known chemicals were identified as capable of diminishing CLas in infected seedlings maintained in a greenhouse. We are seeing promising results since transferring these treatments to groves in production.\nNew citrus greening-tolerant varieties are being developed and propagated by Dr. José Chaparro at the University of Florida. These new hybrids represent an innovative advancement for the citrus industry.\nAn economic analysis will include two primary activities: First, a traditional cost-benefit analysis of the compounds will be conducted to determine which compound has the highest net benefit per tree. Secondly, the construction of a dynamic bioeconomic model to determine the optimal, profit-maximizing use of each treatment in the field.\nA novel and target-specific antimicrobial treatment will have an immediate positive impact on the economic and social aspects of the citrus industry. The use of broad-spectrum antibiotics in our citrus groves can have devastating effects; conferring antibiotic resistance to other bacteria as well as disrupting the native and beneficial microbial community. The citrus industry, directly or indirectly, involves a large network of companies and workforces currently operating in the state of Florida. Because the citrus industry represents a large portion of the Florida economy, the recovery of infected groves will have a substantial, positive impact on the economy in Florida as a whole.\nLdtR: A master regulator of gene expression. As a MarR family member of transcriptional regulation, LdtR can activate and repress transcription genome-wide and has been shown to be crucial in the mediation of osmotic stress tolerance. Benzbromarone was identified as an inhibitory of LdtR activity.\nPrbP: This transcriptional accessory protein regulates a small subset of ribosomal genes. Tolfenamic acid, which has been used to treat migraines, was also found to bind and inhibit PrbP.\nYbeY: The single-stranded endoribonuclease, YbeY, is found in most bacteria and has been shown to contribute to bacteria pathogenicity. We are working to characterize YbeY’s ligand binding site and enzymatic activity regulation in hopes of unearthing a pathogenicity determinant of CLas.\nLdtP: As a novel protein, LdtP has shown to be involved in an alternative pathway for the modification of bacterial cell wall. This periplasmic esterase can modify the lipid A moiety of the lipopolysaccharide, potentially affording CLas a means of survival within the citrus host.\nLotP: This previously uncharacterized protein was found to be induced >7-fold during the citrus host infection when compared to the phsyllid vector. As member of the LdtR regulon, LotP was shown to be induced under high osmotic conditions and to have specific interactions with chaperones. The significance and biological role of these interactions may be critical in CLas persistence within the host.\nThis project encompasses a trans-disciplinary team brought together by the University of Florida with funding from the Specialty Crop Research Initiative (Award number 2015- 70016-23029 to GLL and CFG), and the Plant Biotic Interactions Program (Award number 2017- 03060 to CFG) from the USDA National Institute of Food and Agriculture. The team includes microbiologists, plant pathologists, agricultural economists, statisticians, bioinformaticians, and others working together to aid and support citrus growers, consumers and local citizens have healthy trees and delicious, safe citrus products.\nDr. Claudio F. Gonzalez, Department of Microbiology and Cell Science\nMarcelo Merli, PhD\nDan Zhang, PhD\nFormer members: Janelle Coyle, PhD; Flavia Loto, PhD\nDr. Graciela L. Lorca, Department of Microbiology and Cell Science\nLei Pan, PhD\nRan Zuo, PhD\nDanilo da Silva\nFormer member: Fernando Pagliai, PhD; Aline de Olivieira\nDr. José X. Chaparro, Department of Horticultural Science\nDr. Ana Conesa, Department of Microbiology and Cell Science\nFormer member: Cecile Pereria, PhD\nDr. Salvador Gezan, Department of Forest Resources and Conservation\nFormer member: Paulo Moreno Maynard, PhD\nSidney Banack, Countryside Citrus\nLarry Black, Peace River Packing Company\nGriffin Greene, Greene Groves & Ranch, LLLP\nJames LaBate, HL Holdings, LLC\nWill Andrews, Dinner Lake Grove, LLC & O’Berry Grove, LLC\nWilliam Floyd, Lake County Citrus Sales, Inc.\nMongui Zekri, UF/IFAS Extension Office – Hendry County\nCoyle JF, Pagliai, FA, Lorca GL, Gonzalez CF. (2018) A novel cell envelope modification enzyme, LdtP, in Liberibacter asiaticus. (Submitted Front. Microbiol.).\nCoyle JF, Lorca GL, Gonzalez CF. (2018) Understanding the Physiology of Liberibacter asiaticus: An overview of the Demonstrated Molecular Mechanisms. J Mol Microbiol Biotechnol. 28(3):116-127\nPagliai FA, Pan L, Silva D, Gonzalez CF, Lorca GL. (2018) Zinc is an inhibitor of the LdtR transcriptional activator. PloS One. 13(4): e0195746.\nBlaustein RA, Silfa-Cifuentes J, Lorca GL, Teplitski M. (2017) Functional diversity of citrus microbiomes correlates with Huanglongbing disease symptom severity. Phytopathology. 107 (12), 7-\nBlaustein RA, Lorca GL, Teplitski M. (2017) Challenges for Managing Candidatus Liberibacter spp. (Huanglongbing disease pathogen): Current Control Measures and Future Directions. Phytopathology. 108(4):424-435.\nBlaustein RA, Lorca GL, Meyer JL, Gonzalez CF, Teplitski M. (2017) Defining the Core Citrus Leaf- and Root-Associated Microbiota: Factors Associated with Community Structure and Implications for Managing Huanlongbing (Citrus Greening) Disease. Appl Environ Microbiol. 17;83(11) pii: e00210-17.\nPan L, Gardner CL, Pagliai FA, Gonzalez CF, Lorca GL. (2017) Identification of the tolfenamic acid binding pocket in PrbP from Liberibacter asiaticus. Front Microbiol. 8:1591.\nLoto F, Coyle JF, Padgett KA, Pagliai FA, Gardner CL, Lorca GL, Gonzalez CF (2017). Functional characterization of LotP from Liberibacter asiaticus. Microbial Biotechnol.10(3):642-656.\nPagliai FA, Coyle JF, Kapoor S, Gonzalez CF, Lorca GL. (2017) LdtR is a master regulator of gene expression in Liberibacter asiaticus. Microb Biotechnol. 10(4):896-909.\nGardner CL, Pagliai FA, Pan L, Bojilova L, Torino M, Lorca GL, Gonzalez CF. (2016) Drug Repurposing: Tolfenamic Acid Inactivates PrbP, a Transcriptional Accessory Protein in Liberibacter asiaticus. Front Microbiol. 18;7:1630.\nPagliai FA, Gonzalez CF, Lorca GL. (2015) Identification of a Ligand Binding Pocked in LdtR from Liberibacter asiaticus. Front Microbiol. 25;6:1314.\nPagliai FA, Gardner CL, Bojilova L, Sarnegrim A, Tamayo C, Potts AH, Teplitski M, Folimonova SY, Gonzalez CF, Lorca GL. (2014) The transcriptional activator LdtR from ‘Candidatus Liberibacter asiaticus’ mediates osmotic stress tolerance. PloS Pathogens. 24;10(4):e1004101.']	['<urn:uuid:37727fac-204b-407e-9fbd-6734cdbd2948>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	21	42	1198
25	Were safety briefings effective in preventing deaths in both incidents?	No, in both cases safety briefings proved inadequate. In the balloon incident, despite two safety briefings warning about the fan hazard, a passenger died when her scarf got caught. Similarly, in the electrocution case, despite OSHA standards being 'familiar' to the employer regarding safe distances from power lines, a worker was killed. As noted in the balloon case, 'Toolbox talks and safety briefings are necessary but not enough on their own to prevent incidents.'	"['Hot Air Balloon Company convicted and fined $130,000 over NSW tourist death\nAlice Springs-based, Outback Ballooning Pty Ltd has been convicted and handed a combined penalty of $130,000 after pleading guilty last month to a breach of section 32 of the Work Health and Safety (National Uniform Legislation) Act 2011 (the Act) over the death of a NSW tourist.\nOn 13 July 2013, 35-year-old Stephanie Bernoth and her husband were preparing for a balloon ride with eight other passengers. Despite receiving two safety briefings which highlighted the inflation fan as a hazard with advice to stand clear, the passengers boarded the balloon’s basket from the side where the fan was located, requiring the passengers to walk past the fan.\nMrs Bernoth was the second passenger to board the basket. As she was boarding, the scarf she was wearing was drawn into the fan, becoming entangled in the fan’s blades and driveshaft, causing fatal injuries.\nNT WorkSafe charged Outback Ballooning the following year, which was challenged on the grounds that NT WorkSafe did not have jurisdiction to prosecute. This sparked a four-year legal battle and the matter was eventually appealed to the highest court in Australia.\nIn February 2019 the High Court ruled in favour of NT WorkSafe, reverting the charge back to the Alice Springs Local Court to finalise. NT WorkSafe has been firm from the beginning, and this case demonstrates its strong commitment to see through matters of importance on safety.\nThe Alice Springs Local Court imposed the following penalties:\n- Outback Ballooning was convicted of breaching section 32 of the Act and the conviction was recorded.\n- The company was fined $120,000, with an additional Victims Levy of $1000.\n- The company was also ordered to pay NT WorkSafe $10,000 to assist in the preparation, publication and distribution of written advice regarding precautions to be undertaken by tourists in Central Australia in respect to weather conditions.\nThe Northern Territory’s Work Health and Safety Regulator, Mr Bill Esteves offered condolences on behalf of NT WorkSafe to Mr Bernoth and to Mrs Bernoth’s family in the Philippines.\n“This prolonged legal case has had a big impact on Mr Bernoth’s family and Mrs Bernoth’s family and we hope they find some closure,” Mr Esteves said.\n“The main point is a young woman on holidays in the Northern Territory died because a business did not have appropriate systems to prevent injury from a well-known hazard in the workplace.”\n“Entanglement in machinery can cause fatal injuries and businesses must ensure they are not complacent about safety and not to normalise accepting risks.”\nMr Esteves said businesses must take a risk-based approach to continuously review their safe systems of work. This means reviewing hazards from incidents and near misses that have the potential to inflict serious and fatal injuries, including those in other like businesses.\n“Toolbox talks and safety briefings are necessary but not enough on their own to prevent incidents,” Mr Esteves said.\n“Businesses must objectively assess risks if employees change or modify a procedure. In this case, the crew should have assessed the risk associated with repositioning the balloon’s fan due to a change in the direction of the wind.”\n“On this occasion, the guard fitted to the fan did not stop Mrs Bernoth’s scarf being drawn into it. A risk assessment would have identified this and the crew would have implemented measures to keep passengers at a safe distance from the fan.”\nBusiness who are uncertain about their WHS obligations are encouraged to reach out to NT Worksafe’s Safety Assurance Team to obtain information 1800 019 115.', 'ELECTROCUTION DEATH LEADS TO $62,600 IN FINES AGAINST CONTRACTOR\nBOSTON -- The electrocution death of a worker who came in contact with energized power lines at a Blackstone, Mass., sewer installation project has resulted in $62,600 in proposed fines against his employer. The U.S. Labor Department\'s Occupational Safety and Health Administration has cited ODF Contractors, of Dorchester, Mass., for alleged willful and serious violations of the Occupational Safety and Health Act following the July 5 accident. The worker was standing in the elevated bucket of an excavator attempting to tie together overhead wiring when the arm of the excavator contacted an energized power line, killing him instantly. ""The inspection found that the excavator was operating within 10 feet of energized lines, a clear violation of OSHA standards with which this employer was certainly familiar,"" said Ronald E. Morin, OSHA area director for Central Massachusetts. ""As a result, we have issued a willful citation for failing to maintain a safe distance from the lines and proposed a fine of $56,000 for that hazard."" A willful violation is defined by OSHA as one committed with an intentional disregard of, or plain indifference to, the requirements of the Occupational Safety and Health Act and regulations. Three serious citations, totaling $6,600 in fines, were also issued. Two are for allegedly exposing employees to serious shock hazards by allowing the worker to use the excavator bucket without first deenergizing the power lines and for lack of ground fault protection for employees using a submersible pump to remove water from a trench. The third citation is for placing the excavator at the edge of an excavation, into which it could fall in the event of a collapse of the trench\'s walls. OSHA issues a serious citation when there is a substantial probability that death or serious physical harm could result from the cited condition, and the employer knew, or should have known, of the hazard. ODF Contractors has 15 working days from receipt of its citations and proposed penalties to either elect to comply with them, to request and participate in an informal conference with the OSHA area director, or to contest them before the independent Occupational Safety and Health Review Commission. The inspection was conducted by OSHA\'s Springfield, Mass., area office. Citations and Proposed Penalties to ODF Contractors\nOne alleged willful violation, with a proposed fine of $56,000, for:\n- employees exposed to electrocution hazards due to an excavator being operated within ten feet of energized power lines.\nThree alleged serious violations, with $6,600 in fines proposed, for:\nfailure to supply employment or a place of employment free from recognized hazards likely to cause death or serious physical harm in that employees were exposed to serious shock hazard from standing in the bucket of an excavator that was in contact with energized power lines;\n- no grounding protection or GFI protection for employees using a submersible pump to remove water from a trench;\n- an excavator was located at the edge of an excavation where there was a risk of its falling into the excavation in the event of a wall collapse. Total proposed penalties: $62,600']"	['<urn:uuid:c2ce4430-4c26-4c9b-8d0f-1b8cc1c85541>', '<urn:uuid:f0774d20-1eb3-4dd5-a279-614f42ed61ea>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	10	74	1117
26	Could you tell me about someone who managed to bring together Americans and Russians through music despite their countries being enemies?	Van Cliburn became a hero to music-loving Soviets who eagerly sought to see him perform, despite Cold War tensions. He maintained strong connections with Russia throughout his life - in 2004, he received the Order of Friendship of the Russian Federation from President Vladimir Putin. His connection with Russian leaders was particularly notable when he performed at a 1987 White House state dinner for Soviet leader Mikhail Gorbachev, who responded by giving Cliburn a bear hug and kisses on the cheeks.	"[""© The Financial Times Ltd 2014 FT and 'Financial Times' are trademarks of The Financial Times Ltd.\nFebruary 27, 2013 8:34 pm\nFORT WORTH, Texas – Van Cliburn, the American pianist whose triumph at a 1958 Moscow competition deep in the cold war launched a career that gave him almost rock-star status, has died. He was 78.\nCliburn died early on Wednesday at his Texas home surrounded by loved ones after a battle with bone cancer, said his publicist and longtime friend Mary Lou Falcone.\n“Van Cliburn was an international legend for over five decades, a great humanitarian and a brilliant musician whose light will continue to shine through his extraordinary legacy,” Ms Falcone said. “He will be missed by all who knew and admired him, and by countless people he never met.”\nCliburn made what would be his last public appearance in September at the 50th anniversary of the piano competition named after him. “Never forget: I love you all from the bottom of my heart, for ever,” he said to a standing ovation.\nCliburn shot to fame when he won the first International Tchaikovsky Competition in Moscow at the age of 23 in 1958, six months after the Soviets’ launch of Sputnik propelled the world into the space age.\nNikita Khrushchev, the Soviet premier, reportedly gave the approval for the Tchaikovsky contest judges to honour a foreigner: “Is Cliburn the best? Then give him first prize.”He returned to a New York City ticker tape parade – the first for a classical musician – and a Time magazine cover proclaimed him “The Texan Who Conquered Russia”.\nDespite the tension between the nations, Cliburn became a hero to music-loving Soviets who clamoured to see him perform.\nIn the years that followed Cliburn’s popularity soared and the young man sold out concerts, caused riots when spotted in public and even prompted an Elvis Presley fan club to change its name to his. His recording of Tchaikovsky’s piano concerto No. 1 with Russian conductor Kirill Kondrashin became the first classical album to reach platinum status.\nThe first man in history to be a Horowitz, Liberace and Presley all rolled into one\n- Friend of Cliburn\nTime magazine’s 1958 cover story quoted a friend as saying Cliburn could become “the first man in history to be a Horowitz, Liberace and Presley all rolled into one”. He performed for royalty, heads of state in Europe, Asia and South America and played for every US president since Harry Truman.\n“Since we know that classical music is timeless and everlasting, it is precisely the eternal verities inherent in classical music that remain a spiritual beacon for people all over the world,” he once said.\nThe pianist also used his skill and fame to help other young musicians through the Van Cliburn International Music Competition. Established in 1962, the competition, held every four years, remains a pre-eminent showcase for the world’s top pianists.\n“It is a forum for young artists to celebrate the great works of the piano literature and an opportunity to expose their talents to a wide-ranging international audience,” Cliburn said during the 10th competition in 1997.\nGeorge W. Bush presented Cliburn with the US presidential medal of freedom, the nation’s highest civilian honour, in 2003. In 2004, the pianist received the order of friendship of the Russian Federation from Russian President Vladimir Putin. “I still have lots of friends in Russia,” Cliburn said at the time. “It’s always a great pleasure to talk to older people in Russia, to hear their anecdotes.”\nAfter the death of his father in 1974, Cliburn announced he would soon retire to spend more time with his ailing mother. He stopped touring in 1978. The pianist told The New York Times in 2008 that among other things, touring robbed him of the chance to enjoy opera and other musical performances. “I said to myself, ‘life is too short’. I was missing so much.”\nIt was thrilling to be wanted. But it was pressure, too\nHe said of winning the Tchaikovsky contest: “It was thrilling to be wanted. But it was pressure, too.”\nCliburn emerged from his sabbatical in 1987, when he played at a state dinner at the White House during a visit to Washington by Soviet leader Mikhail Gorbachev. Mr Gorbachev leapt from his seat to give the pianist a bear hug and kisses on the cheeks.\nCliburn was born Harvey Lavan Cliburn Jr. on July 12, 1934, in Louisiana, the son of oilman Harvey Cliburn senior and Rildia Bee O’Bryan Cliburn. At three, he began studying piano with his mother, herself an accomplished pianist who had studied with a pupil of Franz Liszt.\nCliburn won his first Texas competition when he was 12 and two years later he played in Carnegie Hall as the winner of the National Music Festival Award.\nAt 17, Cliburn attended the Juilliard School in New York, where fellow students marvelled at his marathon practice sessions that stretched until 3am. He studied under the famed Russian-born pianist Rosina Lhevinne.\nBetween 1952 and 1958, he won all but one competition he entered. By age 20, he had played with the New York Philharmonic and the symphonies of most major cities.\nHis career seemed ready to take off until his name came up for the draft. Cliburn had to cancel all shows but was eventually excused from duty because of chronic nosebleeds.\nAfter 1990, Cliburn toured Japan numerous times and performed throughout the US. He was in the middle of a 16-city US tour in 1994 when his mother died at age 97.\nCopyright The Financial Times Limited 2014. You may share using our article tools.\nPlease don't cut articles from FT.com and redistribute by email or post to the web.""]"	['<urn:uuid:f78752b1-050c-474d-b0d2-fce49d28611f>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	21	81	956
27	growing coral success factors risks	Successfully growing coral fragments requires proper aquarium care and patience, as it can take about a year for a frag to grow into a colony. The process involves carefully mounting the coral on frag plugs in the correct position, typically facing up towards the light. However, there are significant risks to coral growth, particularly from agricultural pollutants. Research has shown that various pesticides and fungicides can severely impact coral development, with some chemicals causing tissue death and bleaching in adult corals at extremely low concentrations. The fungicide MEMC is particularly harmful, affecting corals at all life stages and causing bleaching at barely measurable levels.	"['What is a Coral Frag?\nA coral frag is simply a fragment of a coral. Instead of saying coral fragments we often refer to them as a frags. 10 years ago in the aquarium hobby the idea of buying, selling or trading frags was still relatively new. Now frags are the commonly accepted form of coral among aquarists.\nA collection of different coral frag species\nHow are coral frags made?\nCoral frags are made by cutting or breaking off pieces of larger coral colonies. A colony is the term used to describe a larger or unfragged coral. Fragging a colony would entail cutting fragments off of it or is some cases cutting up the entire colony into many small pieces. With proper aquarium care the frags will once again grow into colonies were the process can be repeated.\nIt took about 1 year to grow this Tenuis frag from a frag to a colony\nHow to make coral frags\nWe use a special bandsaw in order to safely make frags from our colonies. Some corals we frag we use simple tools and others we are even able to frag by hand. We often use razor blades aswell to cut soft corals. There is no rule book when it comes to making coral fragments and no two colonies would be fragged the same. Every piece is unique and approached differently and fragged with different techniques\nA special bandsaw with a diamond blade is used to safely and precisely cut the corals and make frags\nIn some cases bone cutters are the best option for cutting through the hard skeleton of corals\nWhat is a Frag Plug?\nA Frag plug is a small piece of specially designed ceramic that holds the coral in place. The coral pieces are glued with special adhesive to frag plugs. The frag plug is used to mount the coral in the correct position. Most coral we want facing up towards the light. Similar to planting a tree you want to plant it straight and not have it fall over. There are few corals that grow sideways and in those cases we would frag and glue them according to the way we know or think the will grow. The stem is designed to fit in coral frag racks, these rack allow people to hover or hold many coral frags in their aquarium until they can decide were they want to glue the frags. Once a coral is adhered to your rock work its not advisable to move it. These frag rack holders give people time to decide and have the corals held in limbo into they are ready to commit to a spot\nFrags plugs in many peoples opinion are unsightly. We are trying to recreate reef aquariums that mimic natural reefs and you wouldn’t find small ceramic frag plugs all over the reef. With time many coral frags will completely cover their respective frag plugs and you can no longer see them.\nThis coral is very close to covering the entire frag plug to the point where its not longer visible\nIts important to remember some corals contain poison. Corals are living animals and want to protect themselves. We need to respect them and be careful when fragging. We always recommend wearing gloves and goggles when make coral frags.\nIts important to safely frag corals and always use gloves and goggles\nWhy do people Buy Coral Frags ?\nPeople buy coral frags in order to one day grow them into colonies.\nThere are a couple of great advantages of purchasing frags over colonies\nIts fun to watch them grow\nThey are fun to collect\nYou can try many different corals for much less that buying a colony\nIf a single frag dies it may only cost $10-20 were colonies often sell for $100-200\nThey are sustainable and many corals are now being grown and traded in Canada resulting in less impact in our oceans\nThe downside is they can , depending on the coral species take a long time to grow into a colony. This hobby however rewards those who are patient\nThis beautiful watermelon chalice frag will one day grow into a showpiece colony', 'Pesticides compound climate risk to reef\nCorals already under pressure from global climate change are facing an additional threat in the form of pesticides running off from the land, shows a new scientific study published in the journal Marine Ecology Progress Series.\nCorals can be harmed by agricultural chemicals at levels so low as to be practically undetectable, a ground-breaking study by scientists at the Australian Institute of Marine Science (AIMS), the ARC Centre of Excellence for Reef Studies (ARC CoE) and James Cook University (JCU) concludes.\nReefs on a global scale are under threat from many sources; one of the most insidious is land based pollutants from agriculture – and the new research indicates that this threat may have been underestimated.\nThe study measured the sensitivity of the eggs, larvae and adults of the broadcast spawning coral, Acropora millepora, to a number of common pollutants including four classes of agricultural insecticides and a fungicide commonly used in Great Barrier Reef river catchments.\nAccording to AIMS scientist, Dr Andrew Negri the novelty of the study is that it explored the effects of insecticides on many different life stages of the coral.\nNormally developing coral embryos at the ""prawn chip"" stage, 12 hours after fertilisation. Image: Andrew Negri\n""Previous studies have focused only on the adults, which seem more robust to insecticides. Our study looked at fertilization, larval development, survival and metamorphosis and we found that some of these stages were very vulnerable to these chemicals at very low concentrations"".\nCoral embryos exposed to 1 µg/L of the fungicide MEMC. The embryos have not developed normally and will not make it to the larval stage. Image: Andrew Negri\nKathryn Markey, of JCU, continues ""Neither fertilization rates nor adult branches were affected by any of the insecticides. In contrast, coral settlement was reduced by between 50 and 100% following 18 hours exposure to very low concentrations of each insecticide"".\nDr Andrew Baird of the ARC CoE for Reef Studies says ""These developmental stages and events are critical points in the life history of corals. The failure of any one of these events could seriously reduce the ability of coral populations to replenish themselves"".\nClose up image of a coral branch exposed to 1 µg/L of the fungicide MEMC. The brown tissue is normal but the MEMC has caused tissue death, exposing the white skeleton underneath.\nImage: Andrew Negri\nIn addition, the study found that coral at all life stages was particularly sensitive to the agricultural fungicide MEMC, which caused bleaching in adult corals at levels so low as to be scarcely measurable.\nThese are some of the most sensitive biological responses to pesticide contamination in the marine environment yet demonstrated. The researchers say the real worry is that the effects of these chemicalswere found at such low levels. In addition, the high sensitivity of coral settlement also suggests current water quality guidelines may not adequately protect all coral life stages.\nThe team says that both state and federal governments have recognised the pesticide threat to the Great Barrier Reef where up to 80 per cent of the catchment contains some form of agriculture.\nThe Great Barrier Reef Marine Park Authority, with other government agencies, is currently overseeing the implementation of the ""Reef Water Quality Protection Plan"", a ten-year $40 million program to halt and reverse the declining quality of water entering the GBR Marine Park by improving land management practices.\nHowever such measures do not necessarily apply elsewhere in the world.\nDr Negri says that the latest IPCC climate change report provides a particular context and reasons for concern about the impact of pesticides on corals: ""Corals are already under pressure from rising sea temperatures and pesticides in runoff may be causing additional critical stresses on corals especially during the early life histories such as the larval phase,"" he says.\n*This story was jointly released by AIMS and the Australian Research Council Centre of Excellence for Coral Reef Studies.\nMore stories from AIMS Waypoint newsletter edition 4']"	['<urn:uuid:a2e1603d-030a-495c-9d98-036f2fe8a9f2>', '<urn:uuid:0b0f74f0-c3f2-4e13-a813-aa609760b245>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	5	104	1363
28	how do agglomeration benefits compare between manufacturing firms and business services firms in terms of economic growth impact	Agglomeration benefits have significant but different impacts on manufacturing firms and business services firms in terms of economic growth. For manufacturing firms, agglomeration measured through employment density has a significant positive impact on both the formation of new firms and their scale of operations, as shown in Punjab. For business services firms, agglomeration benefits come through knowledge spillovers and productivity effects - they contribute to economic growth by exploiting scale economies for human capital, enabling knowledge diffusion between firms, and reducing human capital indivisibilities at the firm level. This allows for positive external scale economies and reduces the impact of firm-level scale disadvantages.	"['Where do new firms locate? The effects of agglomeration on the formation and scale of operations of new firms in Punjab\nThe formation of new firms is an important determinant of economic development and the industrial organization literature highlights agglomeration as one of the main factors affecting the formation and scale of operations of new firms. This paper is one of the first to use developing country data to estimate the impact of localization (the benefits accruing to firms that choose to locate in a specific region within a specific industry) and urbanization (the benefits accruing to firms located close to each other regardless of the type of industry to which they belong) on new firms\' formation and scale of operations. Our findings reveal that agglomeration measured through density of employment has a significant impact on the formation of new firms and on their scale of operations in Punjab.\n|Date of creation:||2014|\n|Contact details of provider:|| Postal: Kiellinie 66, D-24105 Kiel|\nPhone: +49 431 8814-1\nFax: +49 431 8814528\nWeb page: http://www.economics-ejournal.org/\nMore information through EDIRC\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on ""citations"" and make appropriate adjustments.:\n- Theresa Thompson Chaudhry, 2011. ""Contracting and Efficiency in the Surgical Goods Cluster of Sialkot, Pakistan,"" South Asia Economic Journal, Institute of Policy Studies of Sri Lanka, vol. 12(1), pages 91-115, March.\n- Edward L. Glaeser & Glenn Ellison, 1999.\n""The Geographic Concentration of Industry: Does Natural Advantage Explain Agglomeration?,""\nAmerican Economic Review,\nAmerican Economic Association, vol. 89(2), pages 311-316, May.\n- Glenn Ellison & Edward L. Glaeser, 1999. ""The Geographic Concentration of Industry: Does Natural Advantage Explain Agglomeration?,"" Harvard Institute of Economic Research Working Papers 1862, Harvard - Institute of Economic Research.\n- Theresa Thompson Chaudhry, 2005. ""Industrial Clusters in Developing Countries: A Survey of the Literature,"" Lahore Journal of Economics, Department of Economics, The Lahore School of Economics, vol. 10(2), pages 15-34, Jul-Dec.\n- Theresa Thompson Chaudhry, 2010. ""Relational Contracting in Pakistan’s Surgical Instrument Cluster: An Empirical Study,"" The Pakistan Development Review, Pakistan Institute of Development Economics, vol. 49(3), pages 213-237.\n- Mercedes Delgado & Michael E. Porter & Scott Stern, 2010. ""Clusters and entrepreneurship,"" Journal of Economic Geography, Oxford University Press, vol. 10(4), pages 495-518, July.\n- Mercedes Delgado & Michael Porter & Scott Stern, 2010. ""Clusters and Entrepreneurship,"" Working Papers 10-31, Center for Economic Studies, U.S. Census Bureau.\n- Azam Chaudhry & Marjan Nasir & Maryiam Haroon, 2012. ""The Birth of Exporters: Entry and Scale of Firms in Punjab’s Export Sectors,"" Lahore Journal of Economics, Department of Economics, The Lahore School of Economics, vol. 17(Special E), pages 191-212, September. Full references (including those not matched with items on IDEAS)\nWhen requesting a correction, please mention this item\'s handle: RePEc:zbw:ifwedp:201421. See general information about how to correct material in RePEc.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (ZBW - German National Library of Economics)\nIf references are entirely missing, you can add them using this form.', ""Kox, Henk L.M. and Rubalcaba, Luis (2007): Business services and the changing structure of European economic growth. Published in: CPB Memorandum , Vol. 183, (4 July 2007)\nDownload (444kB) | Preview\nA pervasive trend that characterised the past two decades of European economic growth is that the share in the economy of commercial services, and particularly business services, grows monotonically, and this mainly to the expense of the manufacturing sector. The structural shift reflects a changing and increasingly complex social division of labour between economic sectors. The fabric of inter-industry relations is being woven in a new way due to the growing specialisation in knowledge services, the exploitation of scale economies for human capital, lowered costs of outsourcing in-house services, and the growing encapsulation of manufacturing products in a ‘service jacket’. Business services, which inter alia includes the software industry and other knowledge-intensive business services (KIBS), play a key role in many of these processes. We argue that in recent decades business services contributed heavily to European economic growth, in terms of employment, productivity and innovation. A direct growth contribution stems from the business-services sector’s own remarkably fast growth, while an indirect growth contribution was caused by the positive knowledge and productivity spill-overs from business services to other industries. The spill-overs come in three forms: from original innovations, from speeding up knowledge diffusion, and from the reduction of human capital indivisibilities at firm level. The external supply of knowledge and skill inputs exploits positive external scale economies and reduces the role of internal (firm-level) scale (dis)economies associated with these inputs. The relatively low productivity growth that characterises some business-services sectors may be a drag on the sector's direct contribution to overall economic growth. The paper argues that there is no reason to expect a “Baumol disease” effect as long as the productivity and growth spill-overs from KIBS to other economic sectors are large enough. Finally, the paper pinpoints some policy 'handles' that could be instrumental in boosting the future contribution of business services to overall European economic growth.\n|Item Type:||MPRA Paper|\n|Institution:||CPB Netherlands Bureau for Economic Policy Analysis|\n|Original Title:||Business services and the changing structure of European economic growth|\n|English Title:||Business services and the changing structure of European economic growth|\n|Keywords:||economic growth; human capital; specialisation; business services; Europe|\n|Subjects:||L - Industrial Organization > L8 - Industry Studies: Services\nO - Economic Development, Innovation, Technological Change, and Growth > O5 - Economywide Country Studies > O52 - Europe\nO - Economic Development, Innovation, Technological Change, and Growth > O4 - Economic Growth and Aggregate Productivity\nO - Economic Development, Innovation, Technological Change, and Growth > O3 - Innovation ; Research and Development ; Technological Change ; Intellectual Property Rights\n|Depositing User:||Henk L.M. Kox|\n|Date Deposited:||29 Jun 2007|\n|Last Modified:||13 Oct 2016 15:24|\nAiginger, K. (2001), “Speed of change and growth of manufacturing”, in: M. Peneder et al., Structural Change and Economic Growth, WIFO, Vienna. Antonelli, C., 1999. The Micro Dynamics of Technological Change. Routledge, London. Arnold, J., B.S. Javorcik and A. Mattoo, 2006, Does services liberalisation benefit manufacturing firms? CEPR Discussion Paper 5902, Centre for Economic Policy Research, London. Autor, D., F. Levy and R. Murnane, 2003, The skill content of recent technological change: an empirical exploration, Quarterly Journal of Economics, 118 (4), pp. 1279-1333 Baker (2007), The impact of business-services use on client industries: evidence from input-output data, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. Baldwin, R. (2006), Globalisation: the great unbundling(s), contributed paper to project Globalisation Challenges for Europe and Finland, Prime Minister’s Office, Helsinki, 20 September 2006. Baumol, W. (2007), On mechanisms underlying the growing share of service employment in the industrialized economies, in: M. Gregory et al., eds, Services and employment: explaining the U.S.-European gap, Princeton University Press (forthcoming), pp. 63-80. Baumol, W. et al. (1989), Productivity and American leadership. The long view. MIT Press, London. Baumol, W. (1967). ”Macroeconomics of unbalanced growth: The anatomy of urban crisis”. American Economic Review , June 1967 (57), 415-426. Beyers, W.B. and D.P. Lindahl, 1996, Explaining the demand for producer services: is costdriven externalisation the major factor?, Papers in Regional Science, 75 (3), 351-74. Blind, K., et al. (2003), Patents in the Service Industries – Final Report. Fraunhofer Institut für Systemtechnik und Innovationsforschung, Karlsruhe. Boden, M., and I. Miles, eds., (2000), Services and the Knowledge-Based Economy. Continuum, London. Burda, M. and B. Dluhosch (2000), Cost competition, fragmentation and globalization, CESifo Paper Nr. 393, Center for Economic Studies & IFO Institute for Economic Research, München. Camacho, J.A. and M. Rodriguez (2007), Integration and Diffusion of KIS for Industry Performance, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. Coe, N.M., 2000, The externalisation of producer services debate: the UK computer services sector, The Services Journal, 20 (2), 64-81. Clark, C. (1938), Australian economic progress against a world background, J. Fisher Lecture. Collechia, A. (2001), The impact of Information Communications Technology on output growth. STI Working Paper. OECD, Paris Crespi, F. (2007) IT services and productivity in European industries, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. CSES (2001), Barriers to trade in business services - Final report, Report commissioned by the European Commission, Centre for Strategy & Evaluation Services / European Commission, Brussels. De Bandt, J., 1999, The concept of labour and competence requirements in service economy, The Services Journal, 19 (1). De Bandt, J., 1995, Indirect productivity of business services, through non-material investments, E. Felli et al. (eds), The service sector: productivity and growth, Physica Verlag, Heidelberg, 157-173. Drejer, I. (1999), Technological change and interindustrial linkages: introducing knowledge flows in input-output studies, PhD dissertation, Aalborg University, Aalborg. Eaton, B.C. and R.G. Lipsey, 1989, Product differentiation, in: R. Schmalensee and R. Willig (eds), Handbook of industrial organization, vol. I, North Holland, Amsterdam. EC (2004), Innovation in Europe - results for the EU, Iceland and Norway, data 1998-2001, 2004 edition, European Commission / Eurostat. EC (2003), ‘The competitiveness of business-related services and their contribution to the performance of European enterprises’. Communication from the Commission, COM(2003) 747. European Commission, Brussels. EC (2003a), Third European Report on Science and Technology Indicators - Towards a Knowledge-Based Economy. European Commission, Luxembourg. EC (2002a), Europe's response to World Ageing: a contribution of the European Commission to the 2nd World Assembly on Ageing, COM(2002) 143 FIN, Brussels. EC (1998), The contribution of business services to industrial performance: a common policy framework, Communication from the Commission, COM(1998)534final, EU, Brussels. ECB Task Force of the Monetary Policy Committee of the European System of Central Banks (2006), Competition, productivity and prices in the euro area services sector, Occasional Paper No. 46, ECB, Frankfurt. ECORYS-NEI (2004), Business Services: Contribution to Growth and Productivity in the European Union, technical report prepared for the European Commission, Enterprise Directorate-General, Brussels Edwards, B. and R. Starr (1987), A note on indivisibilities, specialisation and economies of scale, American Economic Review, 77 (1), pp. 192-194. European Commission Services (1997), Industrial competitiveness and business services: report to the Industry Council. Working Document IIIA.0/OGD(97)pw//bs/workpap.EU, Brussels. Eustace, C. (2000), The intangible economy - impact and policy issues, Report European High Level Expert Group on the Intangible Economy, European Commission, Brussels. Feenstra, R.C. and G.H. Hanson, 1999, The impact of outsourcing and high-technology capital on wages: estimates for the United States, 1979-1990, Quarterly Journal of Economics, 114 (3), 907-940. Fisher, A.G.B. (1939), Production, primary secondary, tertiary, in: Economic Record, vol.15 , June, 24-38. Fixler, D. and Siegel, D. (1999), Outsourcing and productivity growth in services, Structural Change and Economic Dynamics, 10, 177-94 Francois, J. (1990), Producer services, scale and the division of labour, Oxford Economic Papers, 42, pp. 715-729. Francois, J. and K. Reinert (1995), The role of services in the structure of production and trade: stylised facts from a cross-country analysis, CEPR Discussion Paper No. 1228, CEPR, London. Fuchs, V. (1968), The service economy, Columbia University Press, New York. Gadrey, J. and Gallouj, F. (1998) The provider-customer interface in Business and Professional Services. The Services Industries Journal. Vol. 18 (2), pp. 2-15. Greenhalgh, C. and M. Gregory (2000), Labour productivity and product quality: their growth and inter-industry transmission in the UK 1979-90, in: Barrell, R., Mason, G., O’Mahoney, M., (Eds.), Productivity, Innovation and Economic Performance. Cambridge University Press, Cambridge. Gregory, M., W. Salverda and R. Schettkat, eds. (2007), Services and employment: explaining the U.S.-European gap, Princeton University Press (forthcoming). Griliches, Z. (1979), Issues in assessing the contribution of research and development to productivity growth. Bell Journal of Economics, 10, 92-116. Grossman, G. and E. Rossi-Hansberg (2006), The rise of offshoring: it's not wine and cloth anymore, mimeo Princeton University, Lecture at Jackson Hole, August 25, 2006. Grubel, H.G and M.A.Walker, 1991, The dominance of producers services in the US economy, Banca Nazionale del Lavoro Quarterly Review, vol. 1976, pp. 57-68. Grubel, H.G., 1995, Producer services: their important role on growing economies, in: E. Felli et al. (eds), The service sector: productivity and growth, Physica Verlag, Heidelberg, pp. 11-41. Guerrieri, P., B Maggi, V. Meliciani, and P.C. Padoan, 2005, Technology diffusion, Services, and Endogenous Growth in Europe: Is the Lisbon Strategy Useful?, IMF Working Paper WP/05/103, IMF Washington. Hempell, T. (2002), What’s spurious, what’s real? Measuring the productivity impacts of ICT at firm-level. ZEW Discussion Paper 02-42. Center for European Economic Research, Mannheim. Hicks, J. (1973), Capital and time, a neo-Austrian theory, Clarendon Press, Oxford. Hill, T.D. (1977) “On goods and services”. The Review of Income and Wealth, vol. 4. Hughes, A. and Wood, E. (1999), Rethinking innovation comparisons between manufacturing and services: the experience of the CBR SME surveys in the UK. Working paper, no. 140. ESRC Center for Business Research, Cambridge University, Cambridge. Katsoulacos, Y., Tsounis, N., 2000. Knowledge-intensive business services and productivity growth: the Greek evidence, in: Boden, M., Miles, I. (Eds.), Services and the Knowledge-Based Economy. Continuum, London. Klodt, H., R. Maurer and A. Schimmelpfennig, 1997, Tertiarisierung in der deutsche Wirtschaft: Schwerpunkstudie zur Strukturberichterstattung fur den Bundesminister fur Wirtschaft, Institut fur Weltwirtschaft, Kiel. Kok, W. (2004) Facing the challenge: The Lisbon strategy for growth and Employment. Report from the High Level Group chaired by Wim Kok. November 2004. European Commission Kox, H., G. van Leeuwen and H. van der Wiel (2007), Market structure, productivity and scale in European business services, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. Kox, H. (2004), The contribution of business services to aggregate productivity growth, in G. Gelauff et al. (eds), Fostering productivity growth – patterns, determinants and policy implications, Contributions to Economic Analysis Series No. 263, Elsevier Science, Boston. Kox, H., 2002, Growth challenges for the Dutch business services industry: international comparison and policy issues, De Swart / CPB (ISBN 90-5833-095-8), The Hague. Kox, H., 2001, Sources of structural growth in business services. CPB Research memorandum No. 12, The Hague. (http://www.cpb.nl/eng/pub/cpbreeksen/memorandum/12/memo12.pdf) Kutscher, R.E. (1988) Growth of services employment in the United States, in: Guile, B.R. and Quinn, J.B. (Eds.), Technology in services: policies for growth, trade and employment. National Academy Press, Washington DC. Leiponen, A., (2001), Knowledge Services in the Innovation System. ETLA / SITRA, Helsinki. Lesher, M. and H. Nordås (2006), Business services, trade and costs, TD/TC/WP(2006)32, OECD, Paris. Leeuwen, G., van, and H. Van der Wiel (2003), Do ICT spillovers matter? – Empirical evidence for the Netherlands, CPB Discussion Paper 26, CPB, The Hague. Lewis, J. (1988) Services post industrial transformation or flexible production, in: Marshall, J.N. (Ed.), Services and uneven development. Oxford University Press, Oxford. Maddison, A., 2004, Macromeasurement Before and After Colin Clark, Australian Economic History Review, vol. 44(1), pp. 1-34. Markusen, J. and B. Strand (2007), Trade in business services in general equilibrium, NBER Working Paper 12816, NBER, Washington. Marsh, P. (2007), A new twist to India IT outsourcing, Financial Times, January 31st, p. 7. Martini, M. (1990) Implicazioni statistiche dell'analisi dei servizi resi alle imprese. Criteri di identificazione e classificazioni. Internal Document of Eurostat, Luxembourg. McCarthy, J.C., 2002, 3.3 million US service jobs go offshore, Techstrategy Brief, Forrester Research Inc, Nov.2002. Meade, J.E., 1955, The theory of customs unions, North Holland, Amsterdam. Miles, I. (2007), Business Services and their Users: a literature review, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. Müller, E. and R. Zenker (2000), The contribution of knowledge-intensive business services to regional innovation capacities. Fraunhofer Institute, Karlsruhe. Nordhaus, W. (2002), Productivity growth and the new economy. Brookings Papers on Economic Activity, 2002(2). Brookings Institution, Washington. O’Mahony, M and B. van Ark (2003), EU productivity and competitiveness: an industry perspective. Can Europa resume the catching-up process? European Commission, Brussels. OECD (2003), Science, Technology and Industry Scoreboard, 2003, OECD Paris. OECD (1999a), OECD (1999a) Strategic business services, OECD, Paris. Oosterhaven, J. and A.R. Hoen, 1998, Preferences, technology, trade and real income changes in the European Union - an intercountry decomposition analysis for 1975-1985, The Annals of Regional Science, 32, 505-524. Oulton, N. (2001), Must the growth rate decline? Baumol’s unbalanced growth revisited. Oxford Economic Papers, 53, 605-627 Pain, N. and F. Jaumotte (2005), Innovation policies: innovation in the business sector - evidence on innovation determinants from the Community Innovation Survey, ECO/CPE/WP1(2005)2/ANN4, OECD, Paris Peneder, M., S. Kaniovski and B. Dachs, 2000, External services, structural change and industrial performance, Enterprise DG Working Paper, European Commission, Brussels. Perry, M., 1992, Externalisation and the interpretation of business sector growth, The Service Industries Journal, 12 (1), pp. 1-17. Perry, M. (1990) Business services specialization and regional economic change, Regional Studies, 24, 195-209. Pilat (2007), Productivity in business services, in: L.Rubalcaba and H.Kox, eds., Business services in European economic growth, Palgrave MacMillan, forthcoming. Pilat, D. and A. Wölfl (2005), Measuring the interaction between manufacturing and services, STI Working Paper 2005/5, OECD, Paris. Pilat, D. and F. Lee (2001), Productivity growth in ICT-producing and ICT-using industries: a source of growth differentials in the OECD?, STI Working Papers, 2001/14. OECD, Paris Rajan, A. (1987) Services: the second industrial revolution? Butterworths, London. Rubalcaba, L. and H. Kox, eds. (2007), Business services in European economic growth, Palgrave MacMillan, forthcoming. Rubalcaba, L. (2007), The new service economy: challenges and policy implications for Europe, Edward Elgar.. Rubalcaba, L. (1999), Business services in European industry. Growth, employment and competitiveness, European Commission, Brussels. Sapir, A., P. Buigues and P.Jacquemin, eds. (1995). The coherence of European Policies on Trade Industry and Competition, introduction Edward Elgar. Savona, M. and A. Lorentz, 2006, Demand and technology determinants of structural change and tertiarisation – an input-output structural decomposition analysis for four OECD countries, BEPA Document de Travail 2006-01, Université Louis Pasteur, Strasbourg. Schettkat, R. and L. Yocarini (2003), The shift to services: a review of the literature, IZA Discussion Paper no. 964, IZA, Bonn. Stigler, G.J, 1951, The division of labor is limited by the extent of the market, Journal of Political Economy, 59. Triplett, J. and B.Bosworth (2004), Productivity in the U.S. Services Sector, Brookings Institution, Washington. Tschetter, J., 1987, Producer services: why are they growing so rapidly?, Monthly Labor Review, 110 (Dec.), 31-40. Windrum, P. and M. Tomlinson (1999), Knowledge-intensive services and international competitiveness: a four country comparison, Technology Analysis and Strategic Management, 11 (3), pp.391-408. Vernon, R. (1966), International investment and international trade in the product cycle, Quarterly Journal of Economics, vol. 80, 190-207. Van Welsum, D. and G. Vickery (2006), “The share of employment potentially affected by offshoring – an empirical investigation,” OECD Working Paper DSTI/ICCP/IE(2005)8, Paris. Viner, J., 1950, The custom unions issue, Stevens and Son, London. Van Welsum, D. and G. Vickery (2006), “The share of employment potentially affected by offshoring – an empirical investigation,” OECD Working Party on the Information Economy Report DSTI/ICCP/IE(2005)8, Paris. Van Welsum, D. and X. Reif (2005), “Potential Offshoring: Evidence from Selected OECD Countries,” Brookings Trade Forum 2005, 165-194. Wölfl, A., 2004. The interaction between manufacturing and services: some preliminary findings. DSTI Working Paper DSTI/EAS/IND/SWP/AH(2004)7. OECD, Paris. Wölfl, A. (2003), Productivity growth in service industries: an assessment of recent patterns and the role of measurement, STI Working Paper 2003/7, OECD, Paris. Zambon, S. et al., 2003, Study on the measurement of intangible assets and associated reporting practices, prepared for the European Commission (DG Enterprise), Ferrara/Melbourne/New York.""]"	['<urn:uuid:61a135cc-523f-42ce-a9c3-317435a837f7>', '<urn:uuid:8aefdc52-5173-492f-94f9-024fd0c27cc2>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	18	103	3206
29	brain maps made medical purpose role research development	Brain maps serve dual purposes - in medical settings, they help identify abnormalities like tumors, seizures, and brain injuries, while in research, they are used to study the organization of visual, somatosensory, and motor maps using techniques like functional magnetic resonance imaging (fMRI) to understand brain function and organization.	['Constructing a Google-Earth-like functional brain atlas\nBrain, the final frontier. These are the voyages of the brain cartographers. Their continuing mission: To explore strange new maps, to seek out new areas and new functions, to BOLD-ly go where no one has gone before.\nMaking a map is the first step in exploring a new world, whether it be an uncharted territory on the Earth, a new planet in outer space, or the complexly folded brain (Fig. 1). The human brain contains a large set of topologically organized maps, interconnected with neighbor-preserving fiber bundles. These include multiple body representations (somatotopic ‘homunculi’) that sense and control different parts of our body as well as retinotopic maps that represent features of objects at different locations in our visual field. For more than 25 years, researchers have used functional magnetic resonance imaging (fMRI), a noninvasive technique that detects the change in blood oxygen-level dependent (BOLD) brain activity, to examine the organization and functions of these maps.\nIn a chapter published in The Parietal Lobe (part of the Handbook of Clinical Neurology), we present a new surface-based functional brain atlas containing visual, somatosensory, multisensory, motor, action, and motion maps constructed from a large set of fMRI studies from our lab. This atlas defines more than 40 retinotopic areas distributed across five visual streams (Fig. 2) — an update on the original division of the visual system into a dorsal “where” pathway and a ventral “what” pathway. Starting at the occipital pole, the frontiers of the five retinotopic visual streams extend anteriorly into the posterior lateral sulcus, superior postcentral sulcus, cingulate sulcus, retrosplenial cortex, and fusiform gyrus. In addition, there is a separated region of retinotopic maps located in premotor and prefrontal cortex. Maps of passive tactile stimulation and voluntary movement of different body parts clarified our picture of primary sensorimotor areas but also uncovered an intriguing new multisensory homunculus — the parietal face and body areas — located at the border between somatosensory and visual maps.\nMultiple layers of maps rendered on the same cortical surface—a map overlay method typically used in geographical information systems like “Google Earth”—provide insight into understanding how visual, somatosensory, and motor maps partially overlap each other to support actions such as eye movements, pointing, reaching, grasping, eating, ducking, walking, and navigation in daily life. We expect that the areal and functional definitions in this atlas will be refined and updated by future studies using higher-resolution brain imaging and more sophisticated stimuli and tasks tailored to areas with different specificities.\nOur ultimate goal is to construct an online surface-based atlas containing layered maps from different modalities such as topological mapping, myelin density, and diffusion-based architectonics that can be used as a guide map to understand the topological organization, functions, connectivity, and disorders of the human brain. This online atlas will be designed for searching and browsing brain areas and functions via interactive multi-layer features similar to “Google Earth.” These features include gyri/sulci, areal labels, brain functions, anatomical coordinates, fiber tracts, and boundaries of areas (cf. hills/valleys, landmarks, site information, GPS coordinates, roads, and borders on “Google Earth”), and embedded links to publications and figures (i.e., “Google Scholar” and “Google Images”), all on the same interface.\nRuey-Song Huang 1, Martin I. Sereno 2,3\n1Institute for Neural Computation, University of California, San Diego, La Jolla, CA, USA\n2Department of Psychology and Neuroimaging Center, San Diego State University, San Diego, CA, USA\n3Experimental Psychology, University College London, London, WC1H 0AP, United Kingdom\nMultisensory and sensorimotor maps.\nHuang RS, Sereno MI\nHandb Clin Neurol. 2018', 'Brain Mapping Instruments Market: Introduction\n- Brain mapping is an integral part of contemporary neurosurgery. Brain mapping provides essential information of the structure and function of a human brain.\n- Brain mapping instruments identify abnormalities such as tumors, seizures, and physical injuries to the brain such as concussion\n- Eloquent regions of the cortex such as motor, somatosensory, Wernicke’s, and Broca are mapped either preoperatively or intraoperative using brain mapping instruments. Significant technological advancements in brain mapping instruments is projected to boost the growth of the global brain mapping instruments market.\n- Extensive adoption of non-invasive techniques such as functional magnetic resonance imaging (fMRI) and magnetoencephalography is anticipated to drive the global market during the forecast period, as these techniques can perform multiple neuroimaging tests with minimal discomfort to patient.\nGet a Glimpse of the In-Depth Analysis through our Comprehensive Brain Mapping Instruments Market Brochure\nKey Drivers of Global Brain Mapping Instruments Market\n- The global brain mapping instruments market is likely to be driven by increase in the geriatric population. Rise in the size and proportion of population aged 65 and above increases the prevalence of Alzheimer’s or dementia. Hence, rise in prevalence of neurodegenerative diseases is anticipated to drive the global brain mapping instruments market during the forecast period.\n- According to Alzheimer’s Association estimates, population aged 65 and older in the U.S. is likely to increase from 55 million in 2019 to 88 million by 2050\n- Increase in research activity in brain research drives demand for modern research methods such as functional magnetic resonance imaging (fMRI) and transcranial magnetic stimulation (TMS). Moreover, different instruments are used for brain research to acquire comprehensive understanding. This in turn contributes to the growth of the global brain mapping instruments market.\n- Rise in trend of rental contract agreement among manufacturers fuels the growth of the global brain mapping instruments market during the forecast period. According to rental contract agreement, end-users pay certain amount per brain mapping performed using brain mapping instruments installed free of cost by manufacturers. This helps low budget hospital and diagnostic labs to adopt advanced technology in brain mapping instruments without the need for capital investment.\nNorth America to Capture Significant Share of Global Brain Mapping Instruments Market\n- North America was the leading market for brain mapping instruments in 2018, driven by rise in adoption of technologically advanced devices for brain mapping. Moreover, rise in prevalence of neurological disorders in the geriatric population is expected to propel the market during the forecast period. Favorable reimbursement policies of the U.S. Government for these instruments is anticipated to augment the market in the region during the forecast period.\n- According to the Alzheimer’s Association, an estimated 5.8 million people in the U.S. are suffering from Alzheimer’s dementia\n- Asia Pacific is expected to be the fastest growing market for brain mapping instruments during the forecast period due to improving health care infrastructure. Rise in the number of diagnostic centers is likely to fuel the growth of the global brain mapping instruments market in Asia Pacific.\nExpanding Operations in Future? To Get the Perfect Launch Ask for a Custom Brain Mapping Instruments Market Report\nKey Players Operating in Global Brain Mapping Instruments Market\nMajor players in the global brain mapping instruments market are:\n- General Electric Company\n- Koninklijke Philips N.V.\n- Natus Medical Incorporated\n- Advanced Brain Monitoring, Inc.\nGlobal Brain Mapping Instruments Market: Research Scope\nGlobal Brain Mapping Instruments Market, by Instrument Type\n- Functional Magnetic Resonance Imaging (fMRI)\n- Computed Axial Tomography (CAT)\n- Electroencephalography (EEG)\n- Functional Near-infrared Spectroscopy (fNIRS)\n- Positron Emission Tomography (PET)\nGlobal Brain Mapping Instruments Market, by End-user\n- Diagnostic Centers\nThe report offers a comprehensive evaluation of the market. It does so via in-depth qualitative insights, historical data, and verifiable projections about market size. The projections featured in the report have been derived using proven research methodologies and assumptions. By doing so, the research report serves as a repository of analysis and information for every facet of the market, including but not limited to: Regional markets, technology, types, and applications.\nThe study is a source of reliable data on:\n- Market segments and sub-segments\n- Market trends and dynamics\n- Supply and demand\n- Market size\n- Current trends/opportunities/challenges\n- Competitive landscape\n- Technological breakthroughs\n- Value chain and stakeholder analysis\nThe regional analysis covers:\n- North America (U.S. and Canada)\n- Latin America (Mexico, Brazil, Peru, Chile, and others)\n- Western Europe (Germany, U.K., France, Spain, Italy, Nordic countries, Belgium, Netherlands, and Luxembourg)\n- Eastern Europe (Poland and Russia)\n- Asia Pacific (China, India, Japan, ASEAN, Australia, and New Zealand)\n- Middle East and Africa (GCC, Southern Africa, and North Africa)\nThe report has been compiled through extensive primary research (through interviews, surveys, and observations of seasoned analysts) and secondary research (which entails reputable paid sources, trade journals, and industry body databases). The report also features a complete qualitative and quantitative assessment by analyzing data gathered from industry analysts and market participants across key points in the industry’s value chain.\nA separate analysis of prevailing trends in the parent market, macro- and micro-economic indicators, and regulations and mandates is included under the purview of the study. By doing so, the report projects the attractiveness of each major segment over the forecast period.\nHighlights of the report:\n- A complete backdrop analysis, which includes an assessment of the parent market\n- Important changes in market dynamics\n- Market segmentation up to the second or third level\n- Historical, current, and projected size of the market from the standpoint of both value and volume\n- Reporting and evaluation of recent industry developments\n- Market shares and strategies of key players\n- Emerging niche segments and regional markets\n- An objective assessment of the trajectory of the market\n- Recommendations to companies for strengthening their foothold in the market\nNote: Although care has been taken to maintain the highest levels of accuracy in TMR’s reports, recent market/vendor-specific changes may take time to reflect in the analysis.']	['<urn:uuid:146c387e-67b2-48be-9fa7-aae6157ed1d8>', '<urn:uuid:5c23c412-2018-496e-b3f0-63d9eb4fc061>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	8	49	1595
30	How did Dvořák's American experience influence his music compositions?	Dvořák's American experience significantly influenced his compositions, particularly during his time in Spillville, Iowa. He stated he would never have written works like his F major String Quartet 'just so' if he hadn't seen America. He was deeply impressed by American music, especially Negro spirituals, which he learned from his student Harry T. Burleigh. While composing in America, he incorporated elements like pentatonic scales, which are common in spirituals. He even included direct American influences in his work, such as transcribing the song of the scarlet tanager bird into the third movement of his quartet. However, some analysts argue that besides the pentatonic scales, there are few specifically identifiable American elements in his works from this period.	"['IF CVNC.org CALENDAR and REVIEWS are important to you:\nIf you use the CVNC Calendar to find a performance to attend\nIf you read a review of your favorite artist\nIf you quote from a CVNC review in a program or grant application or press release\nNow is the time to SUPPORT CVNC.org\nWill Ransom, the artistic director of the Highlands Cashiers Chamber Music Festival, has worked his magic once again to bring a roster of stellar chamber musicians to its 36th season. With two weeks yet to go, the festival is still in full swing with full-length concerts and related events to suit many tastes. This concert at the Albert Carlton Library, entitled ""Czech Please,"" was brilliantly programmed to include the sunny and ebullient String Quintet in E-flat, Op. 97 (""American"") by Antonín Dvořák, the elegiac Piano Trio in G minor, Op. 15 by Bedřich Smetana, and the arrestingly eclectic Three Madrigals for Violin and Viola, H. 313 by Bohuslav Martinů. The performers were William and Alexandra Preucil, violins; Yinzi Kong and Aloysia Friedmann, violas; Andrew Yee, cello, and Ransom on piano. The event was underwritten by Gay Graves and Boyd Letcher.\nThe program opened with the Martinů duets from 1949, performed by Alexandra Preucil and Kong. Inspired by Mozart\'s duos, as well as English madrigals, each movement is infused with rhythmic intricacies, flexible declamations, and an ever-changing polyphonic or chordal texture. Musically light on their feet, Preucil and Kong were completely immersed in this playful cat-and-mouse game and tossed off the intricate writing without it sounding worked. Their exquisite musicianship, which matched mood, phrasing, and intonation, brought a resounding ovation from the audience.\nJust before intermission came the Smetana Trio, performed by William Preucil, Yee, and Ransom. This was Smetana\'s only piano trio and was composed in 1855 after his beloved four-year old daughter Bedriška had died from scarlet fever. In her short life, she had displayed some musical talent, and the bereaved father dedicated the trio to her. The challenging piano part which shows the influence of Schumann and Liszt was played by Smetana at the work\'s premiere.\nKnowing the genesis of this elegiac piece made it a bit difficult to listen to. Yee remarked that he felt a bit uncomfortable with how much fun they were having playing it. And, one might marvel that such a tragic event could unleash such an outpouring of beautiful music, instead of silencing the composer altogether. Preucil\'s impassioned solo at the beginning set a mournful tone with its series of descending half-steps comprising a classical lament gesture. Yee had to be experienced on two levels, with his large physical gestures running a counterpart to the sounds he generated. Ransom, though charged with a difficult part, anchored the ensemble rather quietly from the keyboard. The scherzo-like initial feel of the second movement provided some relief from the first movement\'s intensity and afforded some gorgeous solos from each of the strings. But the funereal rumblings were neither completely absent here, nor in the rousing closing movement, which slowed into an especially slow and expansive dark passage before ending in the parallel major key. The artists played their hearts out and won the audience\'s standing ovation.\nAfter intermission, violist Friedmann joined the string group and introduced the Dvořák String Quintet. The work was written between June 26 and August 1, 1893 while he was living in the Czech settlement at Spillville, Iowa. The first performance was given in New York City on January 12, 1894. This quintet was the perfect foil to the anguish-ridden Smetana Trio we\'d just heard. Its moniker ""American"" was said by annotator Philip Huscher to refer as much to its general manner – ""openhearted, untroubled, straightforward, unlearned in the best sense of the word"" – as anything else. The ensemble dug into this work with a hearty enthusiasm, so much so that at times they overpowered the acoustics of the relatively small performance space. Each of the four movements was given expansive treatment with the consummate musicianship one would expect from such an august ensemble. The third movement Larghetto was a particular highlight with gorgeous solos from both the violas and cello, with the joyous Finale played with sheer abandon. Again, the audience rose in ovation in tribute to this magnificent performance.\nAs the Highlands Cashiers Chamber Music Festival continues, visit our calendar for what is in store through August 6.', 'String Quartet No. 12 (Dvořák)\nThe American string quartet, opus 96 in F major, is the 12th string quartet composed by Antonín Dvořák. It was written in 1893, during Dvořák\'s time in the United States. The quartet is one of the most popular in the chamber music repertoire.\nDvořák composed the Quartet in 1893 during a summer vacation from his position as Director (1892-1895) of the National Conservatory in New York. He spent his vacation in the town of Spillville, Iowa, which was home to a Czech immigrant community. He composed the quartet (his second attempt to write a quartet in F major; his first effort, 12 years earlier, produced only one movement) shortly after the New World Symphony, sketching the manuscript in three days and completing it in three weeks. ""As for my new Symphony, the F major String Quartet and the Quintet (composed here in Spillville) – I should never have written these works \'just so\' if I hadn\'t seen America,"" wrote Dvořák in a letter in 1893. In his description of the New World symphony, Dvořák was more specific: ""As to my opinion, I think that the influence of this country (it means the folk songs that are Negro, Indian, Irish, etc.) is to be seen, and that this [the symphony] and all other works written in America differ very much from my earlier works, as much in colour as in character..."".\nListeners have tried to identify specific American motifs in the quartet. Some have claimed that the theme of the second movement is based on a Negro spiritual, or perhaps on a Kickapoo Indian tune, which Dvořák heard during his sojourn at Spillville. Others have heard suggestions of a locomotive in the last movement, recalling Dvořák\'s love of railroads. One feature of the quartet is the extensive use of the pentatonic scale, common in the music of spirituals and other ethnic musics worldwide.\nMost analysts, however, fail to see specific American influences in the quartet, aside perhaps from its use of pentatonic scales. ""In fact the only American thing about the work is that it was written there,"" writes Paul Griffiths. ""The specific American qualities of the so-called ""American"" Quartet are not easily identifiable, writes Lucy Miller, ""...Better to look upon the subtitle as simply one assigned because of its composition during Dvořák\'s American tour."" Dvořák himself gave the subtitle ""From the New World"" to his symphony. To the Quartet he gave no subtitle himself, but there is the comment ""The second composition written in America.""\nThe lack of identifiably American motifs notwithstanding, there is no doubt that Dvořák was deeply impressed by the music he heard in America, and specifically spirituals. Harry T. Burleigh, a baritone and later a composer, who knew Dvořák while a student at the National Conservatory, said, ""I sang our Negro songs for him very often, and before he wrote his own themes, he filled himself with the spirit of the old Spirituals."" Dvořák said ""In the negro melodies of America I discover all that is needed for a great and noble school of music.""\nThe one confirmed musical reference in the quartet is to the song of the scarlet tanager. Dvořák was annoyed by this bird\'s insistent chattering, and transcribed its song in his notebook. The song appears as a high, interrupting strain in the first violin part in the third movement.\n- Allegro ma non troppo\n- Molto vivace\n- Finale : vivace ma non troppo\nA characteristic, unifying element throughout the quartet is the use of the pentatonic scale play (help·info). This scale gives the whole quartet its open, simple character, a character that is frequently identified with American folk music. The first several measures of each movement are shown in a thematic catalogue. The New World Symphony is much less pentatonic.\n|The opening theme of the quartet is purely pentatonic, played by the viola, with a rippling F major chord in the accompanying instruments. This same F major chord continues without harmonic change throughout the first 12 measures of the piece. The movement then goes into a bridge, developing harmonically, but still with the open, triadic sense of openness and simplicity.||\n|The second theme, in A major, is also primarily pentatonic, but ornamented with melismatic elements reminiscent of Gypsy or Czech music. The movement moves to a development section that is much denser harmonically and much more dramatic in tempo and color.||\n|The development ends with a fugato section that leads into the recapitulation.||\n|After the first theme is restated in the recapitulation, there is a cello solo that bridges to the second theme.||\n|The theme of the second movement is the one that interpreters have most tried to associate with a Negro spiritual or with an American Indian tune. The simple melody, with the pulsing accompaniment in second violin and viola, does indeed recall spirituals or Indian ritual music. It is written using the same pentatonic scale as the first movement, but in the minor (D minor) rather than the major. The theme is introduced in the first violin, and repeated in the cello. Dvořák develops this thematic material in an extended middle section, then repeats the theme in the cello with an even thinner accompaniment that is alternately bowed and pizzicato.||\n|The third movement is a variant of the traditional scherzo. It has the form ABABA: the A section is a sprightly, somewhat quirky tune, full of off-beats and cross-rhythms. The song of the scarlet tanager appears high in the first violin.||\n|The B section is actually a variation of the main scherzo theme, played in minor, at half tempo, and more lyrical. In its first appearance it is a legato line, while in the second appearance the lyrical theme is played in triplets, giving it a more pulsing character.||\n|The final movement is in a traditional rondo form, ABACABA. Again, the main melody is pentatonic.||\n|The B section is more lyrical, but continues in the spirit of the first theme.||\n|The C section is a chorale theme.||\nA typical performance lasts around 30 minutes.\nPerformance and influence\nDvořák first played through the quartet with his student Josef Jan Kovarik on second violin, and Kovarik\'s two children on viola and cello. The first public performance was by the Kneisel quartet in Boston in January 1894.\nWhile the influence of American folk song is not explicit in the quartet, the impact of Dvořák\'s quartet on later American compositions is clear. Following Dvořák, a number of American composers turned their hands to the string quartet genre, including John Knowles Paine, Horatio Parker, George Whitefield Chadwick, and Arthur Foote. ""The extensive use of folk-songs in 20th century American music and the \'wide-open-spaces\' atmosphere of \'Western\' film scores may have at least some of their origins"" in Dvořák\'s new American style, writes Butterworth.\n- Šourek. p.89\n- Letter to Emil Kozanek, September 15, 1893, translated in Letters of Composers, Gertrude Norman and Miriam Lubell Shrifte, editors (1946, Alfred A Knopf, Inc.)\n- Program notes written by Dvořák for the first London performance of the New World symphony, quoted in Neil Butterworth, Dvořák: his life and times (1980, Midas Books, ISBN 0-85936-142-X), p. 103\n- Butterworth, p. 107\n- Butterworth, p. 89\n- Paul Griffiths, The String Quartet, A History(1983, Thames and Hudson ISBN 0-500-27383-9)\n- Lucy Miller, Adams to Zemlinsky (2006, Concert Artists Guild, ISBN 1-892862-09-3), p. 123.\n- Burghauser, Jarmil. Antonin Dvořák: Thematic Catalogue, with a bibliography and a Survey of Life and Work, Bãrenreiter Supraphon, Prague, 1996, p. 302\n- Jean E. Snyder, `A great and noble school of music: Dvořák, Harry T. Burleigh, and the African American Spiritual.\' In Tibbetts, John C., Ed., Dvořák in America: 1892-1895, Amadeus Press, Portland, OR 1993, p. 131\n- Interviewed by James Creelman, New York Herald, May 21, 1893\n- Miller, p. 124\n- This analysis is based on analyses in Griffiths, Miller, and ""Antonin Dvořák"" in Walter Willson Cobbett, Cobbett\'s Cyclopedic Survey of Chamber Music(1923, Oxford University Press)\n- Burghauser, 1996, p. 302.\n- Burghauser 1996, pp. 299-300.\n- Butterworth, p. 110\n- Butterworth, p. 95\n- Otakar Šourek; Roberta Finlayson Samsour (Translator). The Chamber Music of Antonín Dvořák. Czechoslovakia: Artia.\n- Dvořák, Antonín: Quartetto XII. Fa maggiore. Score. Prague: Editio Supraphon, 1991. S 1304\n- String Quartet No. 12: Free scores at the International Music Score Library Project\n- Performance of the quartet by the Seraphina quartet (Caeli Smith and Sabrina Tabby, violins; Madeline Smith, viola; Genevieve Tabby, cello) at Wikimedia Commons:']"	['<urn:uuid:40c9d2fd-f01d-4fbe-b5f6-25f3f1006cbf>', '<urn:uuid:68f63c13-74fd-4332-b147-e3e84afe8d72>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:09:04.591601	9	117	2146
31	How does the tool handle low frequencies when mastering audio?	The tool has a low frequency cut (LFC) filter early in the mastering chain that removes rumble from the mix. It's recommended to clean between 28hz and 44hz, though care should be taken not to cut too high as fundamental frequencies can exist as low as 50hz. Additionally, there's a sub enhancer control that affects sub bass, acting as a gain control and multiplier for the multiband compressor's bass part, while also tuning the sub decimator to focus on fundamental bass frequencies.	"['Blackbox is an analog style mastering chain with sophisticated automation to give clarity, loudness, warmth and punch to music. Blackbox uses expressive controls which allow the user to make a suggestion on how they want their audio to sound rather than having to know the intricacies of an audio mastering chain. Blackbox DSP modules simulate mastering hardware and its internal devices loosely emulate classic mastering gear like the MLA-3 and MBP II. The combining these expressive controls with content aware AI, Blackbox is able to finely tune its mastering chain to the audio content to get the most out of the sound.\nWindows - download the zip file and extract the binary executable. Put the executable file where you desire and create a shortcut to it if you wish. The Blackbox.exe is standalone so should just run from any location.\nMac - download the dmg file and extract the binary application. Put the application in the applications folder on your mac. Run the application. If you get a security warning saying it can\'t be run as settings only allow applications from Mac App Store, don\'t worry. Go to Apple -> System Preferences -> Security & Privacy and you should see a notification about ""BlackBox was blocked"", click on ""Open Anyway"" which should allow the program to run!\nOnce setup you can drag and drop audio directly to the interface or use the browse icon in the top right hand corner. After selecting the audio file you wish to process you can either process a small section of the track by highlighting and clicking process or you can process the entire track. There are some presets available to select from to aid you getting the desired sound.\nThere are several keyboard shortcuts in Blackbox which include:\nBlackbox box uses 8 crafted expressive controls to achieve desired outcomes. It\'s useful to know the scope of each of these controls and how they affect the signal.\nThe LFC, low frequency cut is a digital brickwall style filter that happens early on in the mastering chain. It is responsible for clearing out the rumble within a mix and gives a cleaner signal to the rest of the chain. Be careful not to go too high depending on your audio source as some fundemental frequencies often reside as low as 50hz. As a general rule you want to clean between 28hz and 44hz (this is slightly subjective).\nThe sub enhancer control affects several elements of the sub bass. It is primeraly a gain control but it also serves as a multiplier for bass part of the multiband compressor and as a tuner control for the sub decimator which focuses the fundemental bass frequencies.\nThe body level plays a similair role to the sub enhancer and focuses on the first set of harmonics. This is a useful control and in general should be used in the opposite direction as the sub enhancer. If you want a focused sub bass for loud club speakers then it is recommended that you lower the body level to create more room for sub frequencies. If however you are mixing for CD, Radio and smaller playback devices then it is recommended turning this control up whilst lowering the sub so the bass is more prominent.\nThe clarity control is dictated by the content of the audio. If there is a singer or and instrument like a guitar or maybe some plucky synths that are low in the mix and you want to bring them out then you can bring the clarity up a little. If they are to prominent and you want to push them back or maybe bring out some more stringy or brassy sounds then you may want to turn the control down a little.\nThe brightness control does what it says on the tin and brightens the whole mix. The upper frequencies will get pushed into the saturator and tamed a little by a compressor so its often required to bring the brightness down a little if a track has been produced digitally. If on the other hand you want the mix to have a more crispy air frequency you can push the control quite hard as it maintains perfect phase and shouldnt cause any more siblance than what is already present in the mixdown. This control can also be use creatively as it acts a little bit like a tuned shelf eq, turning it quite far down will give a warm vintage feel to the mix.\nThis is how much the sound is driven into the DSPs within blackbox. It is important to understand the loudness clarity trade off when you really push this setting however, Blackbox has some nifty algorythms for maintaining transients even when pushed hard. So if you like it loud and crunchy then crank the loudness setting up.\nThere are many tuned compressors in Blackbox and it is one of the reasons why the mixdowns are well balanced. The compressor control will lerp between a range of subtle settings to give more or less glue to the mixdown. The compressors settings should all auto adjust to compensate for the different compression ratios and keep the overall presence the same whilst creating room for elements to shine.\nQuality is directly related to the quality of the filters used by blackbox and the time it takes to render audio. Quality ranges from Fast, Low, Medium, High and Best. For context it takes approximately 16 times longer to render audio on the highest setting. It is recommended that you test with medium and then set it to high to render a 44.1 khz track. If you plan on using higher sample rates e.g. 96khz then it is highly recommended you use the Best setting as the low end may suffer on lower quality settings at these sample rates.\nThere are several other controls in Blackbox that help fine tune the output. These include\nThe ceiling control dictates the maximum peak value in decibels and can be used to conform to whatever standard is required for your production. There is also a true peak control which conforms to BS.1770 standard. Note that true peak may considerably reduce the volume of the track depending on its content. This prevents any analog system playing the signal above unity.\nThe loudness release control is in the settings screen. This a hyper parameter that affects the limiters in blackbox. Reducing this control will create a more agressive modern limiting profile and increasing this will produce a smoother mix.\nThe stereo cut frequency control is in the settings screen. This a hyper parameter that affects mid side processing in blackbox. In most cases it is a good idea to centralise the bass. This frequency dictates the center of the role off frequency where you want the bass to become more mono. In some cases you may wish to leave more stereo field on the bass so reducing this frequency down to its lowest setting will achieve this.\nBlackbox now features a Fast Fourier Transform Analyser window in Real Time Mode. The FFT has has been augmented to show perceived sonic levels in each frequency band.\nBlackbox features professional broadcast metering. You can now monitor levels in RMS, LUFS, Integrated Loudness and Dynamic Range.\nBlackbox export options allow you to save to AIFF and WAV formats at whatever processed sample rate you pick. If you choose to export to 16bit or 24bit you can addtionally add dithering signal which reduces aliasing / banding in the bit rate domain and thus smoothes out potential artifacts. Dithering has several options, None, Rectangular, Triangular and POW-r2 which dictate what kind of signal is applied. Once you have selected the appropriate options just click save and a window will appear prompting you for a file name. The extension will be added automatically. Once selected the save operation will start and in most cases will take under 10 seconds to complete.']"	['<urn:uuid:613af0ac-61e5-4547-92e5-df4c68f2b133>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	10	82	1323
32	what distinguishes ecap from conventional metalworking	The retention of the same cross-sectional area when processing by ECAP, despite the introduction of very large strains, is what distinguishes it from conventional metal-working operations such as rolling, extrusion and drawing.	['ECAP is one of the most commonly employed severe plastic deformation process to drive nano or ultrafine grained microstructure at low homologous temperatures.\nECAP is most suitable for materials used in industrial scale application, the process works by introducing a large shear strain by repeated extrusion steps.\nAmong the different procedures in use for the production of nanostructured materials, only severe plastic deformation (SPD) processes exhibit a potential for producing relatively large samples suitable for industrial applications. Severe plastic deformation is metal forming processes in which an ultra-large plastic strain is introduced into a bulk metal in order to create nano- or ultrafine- grained structure at low homologous temperatures (typically below 0.3 of the melting temperature).\nIn order to obtain the smallest microstructural sizes plastic strains of more than 600 to 800% are necessary. Such high degrees of plastic deformation are possible because one sample can be subjected several times to SPD in order to accumulate the total amount of plastic strain.\nEqual Channel Angular Pressing (ECAP) is one of the most employed methods of Severe Plastic Deformation (SPD); it can be applied to a variety of metals and alloys in order to obtain ultrafine grains and good mechanical and physical properties. The recent literature shows intense and growing interest in some fundamental aspects of SPD techniques, viz., the generation of ultrafine grains and the mechanisms underlying the high levels of strength observed. Aluminum and its alloys, Cu and Ti are the most employed materials in SPD-ECAP studies, with Ti being seriously considered for orthopedic implants.\nThe current trend in research and development of Ti alloys for biomedical applications is the development of alloys with low elastic modulus, free of toxic and allergic elements such as Al and V, while maintaining the already good mechanical properties of Ti-6Al-4V.\nFor producing large size stock that is suitable for industrial application, the equal channel angular pressing (ECAP) process is most suitable. This process involves introducing large shear strain in the work piece by pushing it through a die that consists of two channels with the same cross sectional shape that meet at an angle to each other. Since the cross-sections of the two channels are the same, the extruded product can be re-inserted into the entrance channel and pushed again through the die. Repeated extrusion through the ECAP die accumulates sufficient strain to breakdown the microstructure and produce ultra-fine grain size.\nThe principle of ECAP is illustrated schematically in Figure 1. For the die shown in Figure 1, the internal channel is bent through an abrupt angle, Φ, and there is an additional angle, Ψ, which represents the outer arc of curvature where the two channels intersect. The sample, in the form of a rod or bar, is machined to fit within channel and the die is placed in some form of fuss so that the sample can be pressed through the die using a plunger.\nThe nature of the imposed deformation is simple shear which occurs as the billet passes through the die. The retention of the same cross-sectional area when processing by ECAP, despite the introduction of very large strains, is the important characteristic of SPD processing and it is this characteristic which distinguishes this type of processing from conventional metal-working operations such as rolling, extrusion and drawing. Since the cross-sectional area remains unchanged, the same billet may be pressed repetitively to attain exceptionally high strain.\nFigure 1: Principle of ECAP']	['<urn:uuid:30677b1a-fa9d-4cef-a9ba-d73cc28cceac>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	6	32	571
33	I've noticed my tap water has a metallic taste and leaves reddish stains everywhere. At what concentration does iron in drinking water become a health concern rather than just an aesthetic issue?	According to the Environmental Protection Agency, the acceptable secondary maximum contaminant level for iron in drinking water is 0.3 parts per million. At or above this level, iron primarily causes aesthetic issues like discoloration, rusty appearance, metallic taste, and reddish stains, but doesn't cause serious health problems. However, higher concentrations can be problematic, particularly for people with hemochromatosis, a condition where the body stores excess iron. Additionally, heavily iron-contaminated water can damage pipes through rust or clogging.	['Finding contaminants in your water can be frustrating, even worse when it’s a metal like iron. If you discover that your water contains iron, you’ll naturally become skeptical about drinking it. But should you be?\nThis article will let you know if you should be concerned about finding iron in your drinking water. It’ll also guide you on practical steps to take to make your water iron-free.\nWhen Iron Becomes Harmful\nWe all need to consume a certain amount of iron in order to stay healthy. Not taking in enough iron can cause fatigue, anemia, and slowed development in children. So if you notice that your drinking water contains iron, don’t panic—it’s healthy to consume small amounts of iron.\nHowever, if your water contains an excessive amount of iron, there’s a problem. According to the Environmental Protection Agency, the acceptable secondary maximum contaminant level for iron in drinking water is 0.3 parts per million.\nAt or above this level, iron affects your water’s look, taste, and smell. Water becomes discolored, giving off a rusty hint and leaving orange or reddish stains on surfaces. It also starts to taste metallic. But besides these “aesthetic” effects, having 0.3 ppm iron in your drinking water does not cause serious health problems.\nIf your drinking water contains more than that, you may have reason to worry. For one thing, the water’s rusty appearance gets worse, and you find more reddish stains sticking to surfaces. The water’s metallic taste also gets more noticeable.\nDrinking water that’s been heavily contaminated with iron can cause complications for people with hemochromatosis, a condition where the body stores up any iron it takes in. Iron-contaminated water also damages pipes that transport drinking water by causing rust or clogging.\nBathing also becomes a hassle. Excess iron in water prevents soap from lathering properly, and you’ll find that after a bath, you don’t feel clean enough. Unrinsed soap will form a layer on your skin, which can be very uncomfortable and clog your pores.\nTypes of Iron Contamination in Water\nIron may occur in water as ferrous, ferric, or bacterial iron.\n- Ferrous iron dissolves in water, meaning you can’t see it. But its presence becomes obvious when it leaves stains on surfaces after drying. You’ll mostly find high amounts of ferrous iron in deep wells.\n- Ferric iron, on the other hand, cannot dissolve in water. It’s easier to detect since it forms solid particles. Having ferric iron in water can cause clogging of water pipes.\n- Bacterial iron is found in water supplies when bacteria bind with iron in the water. The result is a slimy red residue. The presence of bacterial iron in water serves as an invitation for other harmful organic compounds to breed.\nHow to Remove Iron From Drinking Water\nIf your drinking water contains high amounts of iron, then it’s time to work towards making it safe for consumption.\nHere are four ways you can remove iron from your water supply.\nIron filters can filter out iron from drinking water. There are different kinds of iron filters. You should make your choice based on effectiveness and the budget in mind. Get more info on the different types of iron filters before making your buying decision.\nWater softeners are generally used to soften hard water. They work by exchanging hardness-causing ions with sodium. Because iron is negatively charged like calcium and magnesium, water softeners can also exchange its presence for sodium ions. You should use a water softener for iron removal if you have ferrous iron present in your water.\nOxidizing filters are used to remove ferrous iron from water. They convert ferrous iron to its solid form and then filter out the solid particles.\nThis method works for well water that contains bacterial iron. It involves adding high concentrations of chlorine to a well. The chlorine handles the iron bacteria. The other iron forms in the well can be removed using a regular iron filter.\nAny of these methods will help your home water become iron-free.\nRemember, as long as the iron content in your drinking water is not too high, there’s no cause for alarm. Iron in water only poses real danger in high amounts.']	['<urn:uuid:83c3b25b-ef95-4873-86fc-86f6dcd8333f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	32	77	701
34	different types bonds molecules structure overlap orbitals	There are three main types of bonds that determine molecular structure. In ionic bonding, atoms arrange in a crystal lattice. In covalent bonding, atoms share electrons through orbital overlap - this can occur as sigma bonds (orbital overlap between nuclei) or pi bonds (orbital overlap above/below nuclei). The type of orbital hybridization (sp, sp2, sp3) determines the geometry - sp gives linear molecules, sp2 gives trigonal planar structures, and sp3 gives tetrahedral arrangements. The molecular structure is also influenced by electron pair repulsion, as both bonding and non-bonding electron pairs arrange themselves to minimize repulsion.	['How do atoms arrange themselves in bonds?\nAtoms arrange themselves in a lattice to form a crystal because of a net attractive force between their constituent electrons and atomic nuclei. The crystals formed by the bonding of atoms belong to one of three categories, classified by their bonding: ionic, covalent, and metallic.\nWhy do electron pairs around a central atom stay as far apart as possible?\nAccording to VSEPR, the valence electron pairs surrounding an atom mutually repel each other; they adopt an arrangement that minimizes this repulsion, thus determining the molecular geometry. This means that the bonding (and non-bonding) electrons will repel each other as far away as geometrically possible.\nHow does the number of bonded atoms around a central?\nHow does the number of bonded atoms around a central atom determine its shape? … Counting the groups of electrons that are shared pairs and lone pairs determine the electron geometry and bond angle forming its shape.\nWhat is the electron pair geometry around the central atom?\nThe basic geometry for a molecule containing a central atom with two pairs of electrons is linear.\nHow do electrons arrange themselves?\nViewed simply, electrons are arranged in shells around an atom’s nucleus. Electrons closest to the nucleus will have the lowest energy. Electrons further away from the nucleus will have higher energy. … In a more realistic model, electrons move in atomic orbitals, or subshells.\nHow do the electrons in bonds bonding domains differ from lone pairs non bonding domains )?\nHow do you do electron pair geometry?\nVSEPR Rules:Identify the central atom.Count its valence electrons.Add one electron for each bonding atom.Add or subtract electrons for charge (see Top Tip)Divide the total of these by 2 to find the total.number of electron pairs.Use this number to predict the shape.\nHow do you find bonding electron groups?\nWhat is the electron geometry if you have two electron groups around the central atom?\nExample 10Number of Electron Groups on Central AtomNumber of Surrounding AtomsMolecular Shape22linear33trigonal planar32bent44tetrahedral\nWhat is electron pair arrangement?\nelectron-pair geometry: arrangement around a central atom of all regions of electron density (bonds, lone pairs, or unpaired electrons) linear: shape in which two outside groups are placed on opposite sides of a central atom. molecular structure: structure that includes only the placement of the atoms in the molecule.\nWhat do electron pairs do?\nThe pairing of spins is often energetically favorable, and electron pairs therefore play a large role in chemistry. They can form a chemical bond between two atoms, or they can occur as a lone pair of valence electrons. They also fill the core levels of an atom.\nWhat is electron pair geometry vs molecular geometry?\nRe: Difference between molecular and electron geometry? Electron geometry describes the arrangement of electron groups. Molecular geometry describes the arrangement of atoms, excluding lone pairs.\nHow do electron pairs guide the shape of a molecule?\nVSEPR is based upon minimising the extent of the electron-pair repulsion around the central atom being considered. VSEPR theory is based on the idea that the geometry (shape) of a molecule is mostly determined by repulsion among the pairs of electrons around a central atom.\nHow do you find the bond order?\n- Draw the Lewis structure.\n- Count the total number of bonds.\n- Count the number of bond groups between individual atoms.\n- Divide the number of bonds between atoms by the total number of bond groups in the molecule.\nHow do multiple bonds affect the shape of a molecule?\nDouble and triple bonds are more repulsive than single bonds Like lone pairs of electrons, multiple bonds occupy more space around the central atom than a single bond. The result is that bond angles are slightly distorted compared to the parent geometry.\nHow does electron pair repulsion determine the molecular geometry?\nValence-Shell Electron-Pair Repulsion Theory The valence-shell electron-pair repulsion (VSEPR) theory states that electron pairs repel each other whether or not they are in bond pairs or in lone pairs. … The shape of a molecule is determined by the location of the nuclei and its electrons.\nWhat is AX3?\nAX3 is a data logger. It features a state of the art MEMS 3-axis accelerometer and Flash based on-board memory. … The device is ideal for collecting longitudinal movement data.\nWhat holds combinations of atoms together in molecules?\nChemical bonds are forces that hold atoms together to make compounds or molecules. Chemical bonds include covalent, polar covalent, and ionic bonds. Atoms with relatively similar electronegativities share electrons between them and are connected by covalent bonds.\nWhat is the central idea behind valence shell electron pair repulsion VSEPR theory?\nThe simple idea behind VSEPR theory is that valence electrons around a central atom (either as lone pairs or involved in bonding pairs) will repel one another. When they do this, they will take on specific geometries that try to maximize the distance between these electron groups.\nWhat are the 2 reasons for electron pair repulsion?\ni) The bond angle decreases due to the presence of lone pairs, which cause more repulsion on the bond pairs and as a result the bond pairs tend to come closer. ii) The repulsion between electron pairs increases with increase in electronegativity of central atom and hence the bond angle increases.\nHow can the theory of electron pair repulsion be used to rationalize the shapes of simple molecules?\nThe Valence Shell Electron Pair Repulsion (VSEPR) theory is a simple and useful way to predict and rationalize the shapes of molecules. … This number (the steric number) defines the electronic shape of the molecule by minimizing repulsion. For example a steric number of three gives a trigonal planar electronic shape.\nHow will electron pairs behave towards one another according to Vsepr?\nHow will electron pairs behave towards one another according to VSEPR? They will arrange themselves to be far apart from each other. … The molecular shape depends only on the electronic geometry, not on the number of the shared electron pairs.\nWhich pair of electron domains generates the most repulsion?\nBonding electrons are further away from the nucleus, but they are more localized, so they are not as spread out. This is why two lone pairs will exhibit more repulsion that one lone pair and one bond pair, which in turn will exhibit more repulsion than two bond pairs.\nWhich of the following will be Stabilised by the removal of an electron?\nWhich is the correct order of bond angle?', 'Explanation of Valence Bond Theory\nValence bond theory states that overlap between two atomic orbitals forms a covalent bond between two atoms.\nDescribe valence bond theory\n- Valence bond and molecular orbital theories are used to explain chemical bonding.\n- Two atoms that have unpaired electrons in their orbitals can overlap to give rise to a chemical bond.\n- A sigma bond ([latex]\\sigma[/latex]) forms when two atomic orbitals overlap between the nuclei of two atoms (also known as the internuclear axis ).\n- Pi bonds ([latex]\\pi[/latex]) occur when two atomic orbitals overlap outside of the space between the nuclei (outside of the internuclear axis).\n- The strongest bonds form with maximum overlap between orbitals.\n- covalent bond: a chemical bond that is formed from the sharing of two electrons between two atoms.\n- internuclear axis: The space between the nuclei of two atoms bonded to each other.\n- atomic orbitals: the physical region or space where an electron may be present or found, with a probability calculated from the precise mathematical form of the orbital.\n- valence bond theory: a covalent bond forms between the two atoms by the overlap of half-filled valence atomic orbitals from each atom.\nIn chemistry, valence bond (VB) theory is one of two basic theories—along with molecular orbital (MO) theory—that use quantum mechanics to explain chemical bonding. According to VB theory, a covalent bond forms from the physical overlap of half-filled valence orbitals in two atoms.\nMechanism of Bonding in VB Theory\nThe VB theory describes the formation of covalent bonds from the overlap of atomic orbitals on two different atoms. Because of the overlap, it is highly probable that a pair of electrons are found in the physical region or space where the orbitals overlap.\nSigma ([latex]\\sigma[/latex]) and Pi ([latex]\\pi[/latex]) Bonds\nThere are two types of overlapping orbitals: sigma ([latex]\\sigma[/latex]) and pi ([latex]\\pi[/latex]). Both bonds are formed from the overlap of two orbitals, one on each atom. [latex]\\sigma[/latex] bonds occur when orbitals overlap between the nuclei of two atoms, also known as the internuclear axis.\n[latex]\\pi[/latex] bonds occur when two (unhybridized) p-orbitals overlap. The p-orbitals, in one [latex]\\pi[/latex] bond, are located above and below the nuclei of the atoms. By occupying the region of space that is above, below, and on the sides of an atom’s nuclei, two [latex]\\pi[/latex] bonds can form.\nBoth types of overlapping orbitals can be related to bond order. Single bonds have one sigma bond. Double bonds consist of one [latex]\\sigma[/latex] and one [latex]\\pi[/latex] bond, while triple bonds contain one [latex]\\sigma[/latex] and two [latex]\\pi[/latex] bonds.\nComparing VB and MO\nVB theory complements molecular orbital (MO) theory, which does not adhere to the VB concept that electron pairs are localized between two specific atoms in a molecule. MO theory states that electrons are distributed in sets of molecular orbitals that can extend over the entire molecule. MO theory can predict magnetic and ionization properties in a straightforward manner. VB theory produces similar results, but is more complicated.\nAn important aspect of the VB theory is the condition of maximum overlap which leads to the formation of the strongest possible bonds. This theory is used to explain the covalent bond formation in many molecules. In the F2 molecule, the F–F [latex]\\sigma[/latex] covalent bond is formed by the overlap of pz orbitals of the two F atoms, each containing an unpaired electron. Since the nature of the overlapping orbitals is different in H2 and F2 molecules, bond strength and bond lengths differ between H2 and F2 molecules.\nIn an HF molecule, the covalent [latex]\\sigma[/latex] bond forms from the overlap of the 1s orbital of H and the 2pz orbital of F, each containing an unpaired electron. Mutual sharing of electrons between H and F results in a covalent bond in HF.\nsp3 hybrid orbitals form when a single s and three p orbitals hybridize.\nExplain the process of hybridization as it applies to the formation of sp3 hybridized atoms.\n- The bonds in a methane (CH4) molecule are formed by four separate but equivalent orbitals; a single 2s and three 2p orbitals of the carbon hybridize into four sp3 orbitals.\n- In the ammonia molecule (NH3), 2s and 2p orbitals create four sp3 hybrid orbitals, one of which is occupied by a lone pair of electrons.\n- In a water molecule, two sp3 hybrid orbitals are occupied by the two lone pairs on the oxygen atom, while the other two bond with hydrogen.\n- tetravalent: having a valence of 4\nIn a tetravalent molecule, four outer atoms are bonded to a central atom. Perhaps the most common and important example of this bond type is methane, CH4. In the ground state of the free carbon atom, there are two unpaired electrons in separate 2p orbitals. To form four bonds, the atom must have four unpaired electrons; this requires that carbon’s valence 2s and 2p orbitals each contain an electron for bonding.\nThe single 2s orbital is spherical, different from the dumbbell-shaped 2p orbitals. This would indicate that one of the four bonds differs from the other three, but scientific tests have proven that all four bonds have equal length and energy; this is due to the hybridization of carbon’s 2s and 2p valence orbitals.\nIn hybridization, carbon’s 2s and three 2p orbitals combine into four identical orbitals, now called sp3 hybrids.\nThe bonds between carbon and hydrogen can form the backbone of very complicated and extensive chain hydrocarbon molecules. The simplest of these is ethane (C2H6), in which an sp3 orbital on each of the two carbon atoms joins (overlaps) to form a carbon-carbon bond; then, the remaining carbon sp3 orbital overlaps with six hydrogen 1s orbitals to form the ethane molecule.\nIf lone electron pairs are present on the central atom, thet can occupy one or more of the sp3 orbitals. For example, in the ammonia molecule, the fourth of the sp3 hybrid orbitals on the nitrogen contains the two remaining outer-shell electrons, which form a non-bonding lone pair.\nIn the water molecule, the oxygen atom can form four sp3 orbitals. Two of these are occupied by the two lone pairs on the oxygen atom, while the other two are used for bonding. The observed H-O-H bond angle in water (104.5°) is less than the tetrahedral angle (109.5°); one explanation for this is that the non-bonding electrons tend to remain closer to the central atom and thus exert greater repulsion on the other orbitals, pushing the two bonding orbitals closer together.\nsp2 hybridization occurs between one s-orbital and two p-orbitals.\nRecognize the role of sp2 hybridized atoms in sigma and pi bonding.\n- For boron to bond with three fluoride atoms in boron trifluoride (BF3), the atomic s- and p- orbitals in boron’s outer shell mix to form three equivalent sp2 hybrid orbitals.\n- In an ethene molecule, a double bond between carbons forms with one sigma and one pi bond.\n- The sigma bond in the C=C for ethene forms between two sp2 hybrid orbitals of two carbon atoms, and a pi bond for between two p orbitals.\n- sp2 hybridization: the 2s orbital mixes with only two of the three available 2p orbitals\n- hybridization: mixing atomic orbitals into new hybrid orbitals, suitable for pairing electrons to form chemical bonds in valence bond theory\nBoron trifluoride (BF3) has a boron atom with three outer-shell electrons in its normal or ground state, as well as three fluorine atoms, each with seven outer electrons. One of the three boron electrons is unpaired in the ground state. In order to explain the bonding, the 2s orbital and two of the 2p orbitals (called sp2 hybrids) hybridize; one empty p-orbital remains.\nsp2 Hybridization in Ethene and the Formation of a Double Bond\nEthene (C2H4) has a double bond between the carbons. In this case, carbon will sp2 hybridize; in sp2 hybridization, the 2s orbital mixes with only two of the three available 2p orbitals, forming a total of three sp hybrid orbitals with one p-orbital remaining. The three hybridized orbitals explain the three sigma bonds that each carbon forms.\nThe two carbon atoms form a sigma bond in the molecule by overlapping two sp2 orbitals. Each carbon atom forms two covalent bonds with hydrogen by s–sp2 overlap, all with 120° angles. The pi bond between the carbon atoms perpendicular to the molecular plane is formed by 2p–2p overlap.\nsp hybrid orbitals form from one s-orbital and one p-orbital.\nDescribe the bonding geometry of an sp hybridized atom.\n- In sp hybridization, the s orbital overlaps with only one p orbital.\n- Any central atom surrounded by just two regions of valence electron density in a molecule will exhibit sp hybridization.\n- sp orbitals are oriented at 180 degrees to each other.\n- hybrid orbital: formed by combining multiple atomic orbitals on the same atom\n- sp hybrid: an orbital formed between one s-orbital and one p-orbital\nIn sp hybridization, the s orbital overlaps with only one p orbital. Atoms that exhibit sp hybridization have sp orbitals that are linearly oriented; two sp orbitals will be at 180 degrees to each other.\nAny central atom surrounded by just two regions of valence electron density in a molecule will exhibit sp hybridization. Some examples include the mercury atom in the linear HgCl2 molecule, the zinc atom in Zn(CH3)2, which contains a linear C–Zn–C arrangement, the carbon atoms in HCCH and CO2, and the Be atom in BeCl2. This last example will be discussed in more detail below.\nsp Hybridization in BeCl2\nThe beryllium atom in a gaseous BeCl2 molecule is an example of a central atom with no lone pairs of electrons in a linear arrangement of three atoms. There are two regions of valence electron density in the BeCl2 molecule that correspond to the two covalent Be–Cl bonds. To accommodate these two electron domains, two of the Be atom’s four valence orbitals will mix to yield two hybrid orbitals.\nWhen atomic orbitals hybridize, the valence electrons occupy the newly created orbitals. The Be atom had two valence electrons, so each of the sp orbitals gets one of these electrons. Each of these electrons pairs up with the unpaired electron on a chlorine atom when a hybrid orbital and a chlorine orbital overlap during the formation of the Be–Cl bonds.\nThe hybridization process involves mixing of the valence s orbital with one of the valence p orbitals to yield two equivalent sp hybrid orbitals that are oriented in a linear geometry.\nThe set of sp orbitals appears similar in shape to the original p orbital, but there is an important difference. The number of atomic orbitals combined always equals the number of hybrid orbitals formed. The p orbital is one orbital that can hold up to two electrons. The sp set is two equivalent orbitals that point 180° from each other. The two electrons that were originally in the s orbital are now distributed to the two sp orbitals, which are half filled.\nIn gaseous BeCl2, these half-filled hybrid orbitals will overlap with orbitals from the chlorine atoms to form two identical σ bonds.\nEnergy Level Diagrams for sp Orbitals\nThe electronic differences in an isolated Be atom and in the bonded Be atom can be illustrated using an orbital energy-level diagram. These diagrams represent each orbital by a horizontal line (indicating its energy) and each electron by an arrow. Energy increases toward the top of the diagram. We use one upward arrow to indicate one electron in an orbital and two arrows (up and down) to indicate two electrons of opposite spin.\nHybridization in Molecules Containing Double and Triple Bonds\nsp2, sp hybridizations, and pi-bonding can be used to describe the chemical bonding in molecules with double and triple bonds.\nDescribe the role of hybridization in the formation of double and triple bonds.\n- A new hybrid orbital forms when atomic orbitals are mixed; this process is called hybridization.\n- The bonding in ethene (which contains a C=C) occurs due to sp2 hybridization in each of the carbon atoms.\n- Molecules with triple bonds, such as acetylene, have two pi bonds and one sigma bond.\n- hybridisation: mixing atomic orbitals to form new hybrid orbitals suitable for the qualitative description of atomic bonding properties\n- triple bond: a covalent bond in which three electron pairs (instead of the usual one) are shared between two atoms; most common between carbon atoms and carbon or nitrogen atoms; symbolized in formulae as ≡\n- pi bond: covalent chemical interactions where two lobes of one involved atomic (p) orbital overlap two lobes of the other involved atomic (p) orbital\n- double bond: a covalent bond in which two electron pairs (instead of the usual one) are shared between two atoms; most common between carbon atoms and carbon, oxygen, or nitrogen atoms\nIn chemistry, hybridization is the concept of mixing atomic orbitals to form new hybrid orbitals suitable for describing bonding properties. Hybridized orbitals are very useful in explaining of the shape of molecular orbitals for molecules, and are an integral part of valence bond theory.\nThe hybrids are named for the atomic orbitals involved in the hybridization. In methane (CH4) for example, a set of sp3 orbitals forms by mixing one s- and three p-orbitals on the carbon atom. The orbitals are directed toward the four hydrogen atoms, which are located at the vertices of a regular tetrahedron.\nEthene (C2H4) has a double bond between the carbons. For this molecule, carbon will sp2 hybridize. In sp2 hybridization, the 2s orbital mixes with only two of the three available 2p orbitals, forming a total of 3 sp2 orbitals with one p-orbital remaining. In ethylene (ethene), the two carbon atoms form a sigma bond by overlapping two sp2 orbitals; each carbon atom forms two covalent bonds with hydrogen by s–sp2 overlapping all with 120° angles. The pi bond between the carbon atoms forms by a 2p-2p overlap. The hydrogen-carbon bonds are all of equal strength and length, which agrees with experimental data.\nMultiple bonds can also occur between dissimilar atoms. When the two O-atoms are brought up to opposite sides of the carbon atom in carbon dioxide, one of the p orbitals on each oxygen forms a pi bond with one of the carbon p-orbitals. In this case, sp hybridization leads to two double bonds.\nsp hybridization explains the chemical bonding in compounds with triple bonds, such as alkynes; in this model, the 2s orbital mixes with only one of the three p-orbitals, resulting in two sp orbitals and two remaining p-orbitals. The chemical bonding in acetylene (ethyne) (C2H2) consists of sp-sp overlap between the two carbon atoms forming a sigma bond, as well as two additional pi bonds formed by p-p overlap. Each carbon also bonds to hydrogen in a sigma s-sp overlap at 180° angles.']	['<urn:uuid:e396bb6e-262c-497c-87ba-27331b7f7375>', '<urn:uuid:de31a7ad-e397-4625-9fa6-ba5e1c7d0247>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	7	95	3544
35	As a plant biochemist studying wound responses, I'd like to know what happens to jasmonic acid production when plants get injured, and what biological processes it affects?	Jasmonates (JAs) are synthesized through the octadecanoic pathway from linolenic acid in undamaged tissues, but apparently through a different pathway in wounded tissues. When plants are wounded, JAs are produced and have diverse effects including: inhibition of root growth, promotion of fruit ripening, senescence, pollen development, tuber formation, tendril coiling, and defense against insect pests and pathogens. JAs influence gene expression at multiple levels, including transcription, RNA processing, and translation.	['BSPP2000: Session 5: Local and systemic resistance\nThis is a sub page of our conference BSPP2000: Plant-pathogen Interactions: Understanding Mechanisms of Resistance and Pathogenicity for Disease Control\nThe molecular biology of extreme resistance against potato virus X\nDavid C. Baulcombe\nThe Sainsbury Laboratory, John Innes Centre, Norwich NR4 7UH, UK\nRx-mediated resistance against potato virus X (PVX) in potato involves a surveillance system detects a virus-encoded protein and a response that has broad spectrum antiviral activity. The Rx protein is a component of the surveillance system that recognizes the PVX coat protein. However, as we have been unable to demonstrate a direct interaction of Rx and the coat protein, it seems likely that the surveillance mechanism is a complex of several proteins. The antiviral response has a primary phase resulting in suppression of virus accumulation and a secondary phase resulting in death of the infected cell. This second phase is activated when the primary phase is too late or too weak to prevent continued virus accumulation.\nTo investigate the signaling components involved in Rx and other resistance responses we are using a system based on virus induced gene silencing. I will describe the basis of this system and its application in different examples of disease resistance system.\nAnalysis of the jasmonate signal pathway in Arabidopsis\nChristine Ellis, Manuela Nieto-Rostro, Daoxin Xie, Ioannis Karafyllidis, Liangying Dai, John Turner\nSchool of Biological Sciences, University of East Anglia, Norwich,England\nJasmonates (JAs), which include jasmonic acid and its cyclopentanone derivatives, are widely distributed throughout the plant kingdom. They are synthesised by the octadecanoic pathway from linolenic acid in undamaged tissues and apparently by a different pathway in wounded tissues. JAs are formed when plants are wounded, and have strikingly diverse effects: they inhibit root growth, promote fruit ripening, senescence, pollen development, tuber formation, tendril coiling, and defence against insect pests and pathogens. They are associated with altered gene expression at the level of transcription, RNA processing, and translation. However, the molecular basis of the JA signal pathway and its regulation by the wounding is only starting to be elucidated.\nWe have started to dissect the JA signal pathway through the isolation and characterisation of Arabidopsis mutants. Of the three mutant classes isolated one, named coil, exhibits no JA-induced responses. We isolated the COI1 gene by map-based cloning and showed that it encodes an F-Box protein. We have used yeast two-hybrid screens to show that COI1 links the JA response to the ubiquination pathway. Six COI1 ASSOCIATED PROTEIN (CAP) genes were isolated. CAP1, CAP2, and CAP3 are homologues of SKP1, which forms an essential component of the ubiquitination complex. CAP4, CAP5 and CAP6 are candidate repressor protein targets for COI1-mediated ubiquitination. Further evidence for the function of the CAP proteins in the JA pathway was by co-immunoprecipitation and by reverse genetics. The results indicate that selective proteolysis is required for the JA perception-response pathway in Arabidopsis. Other mutants with altered JA responses define negative regulators. These mutants were and were isolated in a screen for constitutive expression of the JA-responsive promoter from the vegetative storage protein (VSP) gene fused to the luciferase structural gene. They formed two distinct types: the CONSTITUTIVE EXPRESSION OF VSP1 (cevl) mutant had constitutive expression of JA-induced and ethylene-induced responses, and was generally resistant to pests and diseases. cev2–cev8 had JA-induced overexpression of JA responses. Characterisation of these three types of mutant has enabled us to predict a model for the JA signal pathway, and its interaction with other signal pathways in Arabidopsis during the wound response.\nVariable interactions between salicylic acid and jasmonic acid signalling pathways in Arabidopsis thaliana and tobacco\nLuis A. J. Mur1*, Paul Kenton1, Faye J. Sturgess1, John Draper1, Claus Wasternack2 and Rainer Atzorn2\n1Institute of Biological Science, University of Wales- Aberystwyth, Aberystwyth, UK\n2 Institut fur Pflanzenbiochimie, POB 110432 Halle/Saale, Germany\n*Principal investigator: Email email@example.com Tele: 01970 622981.\nSalicylic acid (SA), is a key signal in the establishment of systemic acquired resistance whilst jasmonic acid (JA) is associated with wound-activated events. A well established model suggests that SA and JA signalling is mutually exclusive with SA suppressing JA synthesis at a cyclo-oxygenase step in a manner which is analogous to the reduction of prostaglandin levels by analgesics. We have reported however, the simultaneous accumulation of SA and JA in developing hypersensitive response (HR) lesions (Kenton et al., 1999) and significantly JA levels were not elevated within the HR occurring in salicylate hydroxylase expressing tobacco plants were SA levels are reduced. Reciprocally dosing Arabidopsis with various concentrations of SA, JA and the JA precursor of linolenic acid revealed that the SA-mediated suppression of JA-associated expression of PDF1.2 and thionin only occurred at >250M whilst at lower levels expressed was in fact augmented. Addition of low concentrations of JA was also observed to increase SA-induced PR1a-expression. Furthermore, addition of higher levels of SA or JA resulted in cell-death, an effect that could be suppressed by the apoplastic application of catalase. Significantly, the H2O2-responsive transgene, AoPR1-GUS also exhibited SA/JA synergistic effects in tobacco and Arabidopsis and associated increases in apoplastic H2O2 levels were measured in planta. These data suggests that SA/JA effects are concentration dependent with lower levels probably augmenting the oxidative burst during the hypersensitive response.\nReference: Kenton et al., (1999) MPMI 12:74-78.']	['<urn:uuid:8d88a2ce-fe1a-4de4-9623-cb3e8239883e>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	27	70	875
36	What are the three key goods and services that are assessed in the Economic Valuation Tool used to quantify the value of Caribbean coral reefs?	The Economic Valuation Tool focuses on three key goods and services: coral reef-associated tourism, fisheries, and shoreline protection services.	"[""Coral reefs are a vital part of the Caribbean’s marine environment, and are integral to the economies of many of the region’s small island states. WRI's economic valuation methodology can help decision-makers in the region better understand the enormous economic value the reef provides and use this data to make better-informed coastal policy.\nClick here to view the full results.\nTourists flock to the region to see the beautiful beaches and marine life these biologically rich habitats support. Local fishermen rely on healthy reefs as a source of food and livelihood. In addition, reefs act as natural barriers to protect the islands from the worst ravages of tropical storms.\nThe proper management of the Caribbean’s Coastal Capital is essential for the region’s economic and environmental health.\nYet government officials, developers and the public often overlook or do not fully appreciate the harmful effects coral reef degradation, resulting in short-sighted decisions about coastal investment, development and land use. Knowledge of the economic values stemming from coral reefs supports sustainable planning which maximizes the long-term economic potential of coastal areas.\nBut a clear presentation of the economic value derived from coral reefs can guide the sustainable use of these resources.\nThe World Resources Institute, in collaboration with government and NGO partners in St. Lucia, Tobago and Belize, has developed an economic valuation methodology to quantify the value of coral reefs in the Caribbean more accurately. The methodology supports the evaluation of trade-offs, thereby highlighting the management and development paths which protect coral reefs, and maximize the economic contribution of coral reefs to the economy.\nOur Excel-based Economic Valuation Tool guides users through a simple method that does not assess Total Economic Value (TEV), but instead focuses on three key goods and services: coral reef-associated tourism, fisheries, and shoreline protection services. Read more about the methodology\nThe tool evaluates the economic impact of both coral reef-associated tourism and fisheries with a financial analysis method that tracks the financial flows and wider economic impact these two industries generate.\nShoreline protection is the third aspect of the Economic Valuation Tool. WRI developed an innovative method for evaluating the role of coral reefs in protecting the shoreline. Using a modified avoided damages approach, the tool estimates the total value of reduction in wave-induced erosion and property damage due to coral reefs.\nCountries across the Caribbean can use the methodology; and the hope is that its widespread use will support wise, forward-looking coastal policy throughout the region.\nWRI has made lower bound (partial) estimates of the economic contribution of coral reefs to the economy of two pilot sites in the Eastern Caribbean: Tobago and St. Lucia. On these two small islands, coral reef tourism alone accounted for direct and indirect economic impact totaling US$101–130 million in Tobago and US$ 160–194 million in St. Lucia.\nUltimately, WRI hopes the project will:\n- Increase local capacity to perform ecosystem valuation and use the valuation results in planning and decision-making;\n- Make the economic case for better coastal and land management, as well as for increased investment in Marine Protected Areas, so that these are viewed as investments for the economic and societal benefits of the country;\n- Arm NGOs and marginalized resource users with powerful information, giving them a greater voice in local decision-making.\nFor a quick summary of the results, click here. For the full Coastal Capital report, click here. To view maps of the shoreline protection analysis, click here. Click here to learn more about one WRI's project partners, Buccoo Reef Trust.""]"	['<urn:uuid:d2b1fe40-5e72-41ba-a6cb-71d77511104d>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	25	19	585
37	show preparation when start cleaning tack	You should do a deep clean and condition of your tack one week before the show. This includes polishing silver and cleaning stainless steel.	['Gearing up for the first show of the season can be a little overwhelming. Whether this is your first year out or you’re a seasoned competitor, you know that preparation is key to success. That’s why we’re giving you this handy guide to help you get ready for that blue-ribbon ride. Here’s a simple way to keep you on schedule, from three months prior to the competition to the day before the big event. Using our countdown guide for everything you need, you’ll be ready to go.\n- Three months before show: Many pro trainers advise showing at one level below your schooling level. This way, you will ride with confidence at a comfortable level, yet still be challenged in the face of competition. So if you’re riding dressage Second Level in practice, compete at First Level. If you’re jumping 3′ courses, stick to 2’6″ at the shows.\n- One month before show: Go over your show program and decide which classes/division you’re going to enter. Start tailoring your riding sessions with these in mind.\n- One week before show: Challenge yourself in final schooling sessions. Tune up any riding problems.\n- One day before show: Lightly hack your horse for exercise. You’ve already worked out any issues the week before, so you should be prepared for your classes.\n- Three months before show: Consider supplementing your horse’s diet so that he has beauty from within: a burnished coat, great hoof condition and an ideal energy level all come from good nutrition and supplements.\n- One month before show: Start getting your horse’s mane and tail in shape. Pull mane (if appropriate for your breed or event) to correct length over several days. Use detangler on tail to avoid unnecessary hair-pulling.\n- One week before show: Do a deep cleaning spa day with shampoo, conditioner, whitener, et cetera.\n- One day before show: Bathe your horse and spray him with coat polish (avoiding the saddle area). Clip his whiskers, bridle path, ears and other areas, (jowl, fetlock) as needed. Braid or band the mane and wrap the tail and any white socks, if necessary.\n- Three months before show: Inventory your show tack. If you are using your working tack for showing, determine if you need to purchase any extra items for top turnout.\n- One month before show: Try on your show apparel and replace anything that is uncomfortable, outdated or in disrepair. Inspect all your tack’s stitching and repair or replace where needed.\n- One week before show: Deep clean and condition your tack. Polish silver and clean stainless steel. Make sure all show clothes have been washed, dry cleaned or ironed as needed.\n- One day before show: Do a light cleaning. Polish boots and gather all show apparel.\n- Three months before show: Make sure your horse is comfortable with loading, unloading and hauling. Show day is not the time to find out that your horse refuses to trailer.\n- One month before show: Inspect your trailer for protruding objects, tire condition, rust, damage to floorboards, et cetera. Fix any electrical issues.\n- One week before show: Change your tow vehicle’s oil, check brakes and fluids.\n- One day before show: Pack equipment in trailer, including grooming items, stable equipment, feed, et cetera. Fill gas tank in rig.']	['<urn:uuid:3ab60249-34a4-4504-9614-a0464ef4ed7f>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	6	24	552
38	I have hemophilia - can animals help with treatment and what should I know about current medicines?	There are interesting connections between animals and hemophilia treatment. Manatees and Dugongs, which are endangered species, have poorly clotting blood that may prove useful in hemophilia research. As for current treatments, anti-inhibitor coagulant complex injections are available to help control bleeding in hemophilia patients. This medicine is injected into veins, usually in hospitals or clinics, though some patients can be taught to self-administer at home. Patients should carry identification cards with their medication details and watch for side effects like blood clots or allergic reactions. The medicine is made from human plasma, with small risks of containing viruses or bacteria, though processing eliminates most of these risks.	"[""Preservation of Genetic Diversity\nIn Europe and the Mediterranean region we have some 145 indigenous breeds of cattle, no fewer than 115 of which are threatened with extinction, yet, as with plants, many traditional strains are of great value for breeding purposes. In Wensleydale, the now rare Wensleydale sheep has been used for crossing to produce a heat tolerant breed of sheep able to produce good quality wool in subtropical areas. The Cornish hen, once only maintained as a curiosity is now the basis of our broiler industry.\nResearchers today are looking more and more towards animals to try and solve human problems. For example, in their search for a more effective crash helmet, they have turned to the woodpecker - how is it that it can hit a tree repeatedly with its beak at 1300mph without damaging itself, but at relatively low speeds people in motor cycle accidents often suffer fatal head injuries?\nOne of the world's most distressing diseases is leprosy. To date, research has been difficult because we have been unable to culture the disease, but it has now been discovered that the 9-banded armadillo from South America may hold the key because it too can contract the disease.\nManatees and Dugongs are on the list of endangered species - they have poorly clotting blood which may prove useful in the research into haemophilia. The black bear and other animals which hibernate possess hormonal mechanisms which enable them to sleep for five months during the winter. This type of metabolism may well supply information on the development of low protein, low fluid diets that may be of great importance to humans suffering from kidney failure. Many pharmaceutical industries rely for their raw materials on wild material. Pharmaceutical industries have recently been established in Asia, Africa and Latin America. More than 40 of the 90 species of plant used in modern medicine in these countries are available only from the wild and a further 20, though cultivated, are also taken from the wild. To preserve these pharmaceutical industries the plants on which they depend also need to be conserved.\nWe have looked at the way in which the researcher is turning more and more to the natural world to solve our problems. Hunger too is a problem that affects almost two thirds of the world's population. Yet those of us in the West apparently living in lands of plenty should not be too complacent. Of the many thousands of plants available only about 30 form most of the world's staple diets and within those 30 species the genetic base is often very narrow. 75% of the crop grown on the Canadian Prairies is devoted to a single variety of wheat. The U.S. soya bean industry is derived from a mere 6 plants.\nThe loss of any species, however insignificant, is a loss to science and mankind as we cannot be sure whether or not that species may prove to be valuable in the future. Therefore, there are good economic reasons for conserving populations of species, ie. Biodiversity.Read More: Extinction Is Forever!"", 'Anti-inhibitor Coagulant Complex injection\nWhat is this medicine?\nANTI-INHIBITOR COAGULANT COMPLEX (AN tahy-in HIB i ter koh AG yuh luhnt KOM pleks) is used in patients with hemophilia to help control bleeding.\nHow should I use this medicine?\nThis medicine is for injection into a vein. It is usually given by a health care professional in a hospital or clinic setting.\nIf you are given this medicine at home, you will be taught how to prepare and give this medicine. Take your medicine at regular intervals. Do not take your medicine more often than directed.\nTalk to your pediatrician regarding the use of this medicine in children. While this medicine may be prescribed for children for selected conditions, precautions do apply.\nWhat side effects may I notice from receiving this medicine?\nSide effects that you should report to your doctor or health care professional as soon as possible:\nallergic reactions like skin rash, itching or hives, swelling of the face, lips, or tongue\nsigns and symptoms of a blood clot such as breathing problems; changes in vision; chest pain; severe, sudden headache; pain, swelling, warmth in the leg; trouble speaking; sudden numbness or weakness of the face, arm, or leg\nSide effects that usually do not require medical attention (report to your doctor or health care professional if they continue or are bothersome):\nWhat may interact with this medicine?\nThis medicine may interact with the following medications:\nWhat if I miss a dose?\nKeep appointments for follow-up doses as directed. It is important not to miss your dose. Call your doctor or health care professional if you are unable to keep an appointment.\nIf you take this medicine at home and miss a dose, take it as soon as you can. If it is almost time for your next dose, take only that dose. Do not take double or extra doses.\nWhere should I keep my medicine?\nKeep out of the reach of children.\nIf you are using this medicine at home, you will be instructed on how to store this medicine. Throw away any unused medicine after the expiration date on the label.\nWhat should I tell my health care provider before I take this medicine?\nThey need to know if you have any of these conditions:\nhistory of blood clots\nhistory of blood diseases\nlow platelet counts\nan unusual or allergic reaction to human protein, other medicines, foods, dyes, or preservatives\npregnant or trying to get pregnant\nWhat should I watch for while using this medicine?\nThis medicine is made from human plasma, and there is a small risk that they may contain certain types of virus or bacteria. All products are processed to kill most viruses and bacteria. If you have questions concerning the risk of infections, discuss them with your doctor or health care professional.\nIf you are a hemophilia patient, carry an identification card with you at all times. The card should have your name, the name and dose of your medication(s), the name and phone number of your doctor or health care professional, and a contact person in case of emergency.']"	['<urn:uuid:6c21c0e2-8e25-470e-96e9-fa140ce16651>', '<urn:uuid:3b0f1b82-213d-4e05-a4f5-97e91ccf36ab>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	17	107	1030
39	What evidence shows objects in our solar system formed together?	The overwhelming majority of celestial objects within our solar system orbit in the same direction as the Sun's rotation, indicating they formed at about the same time from the same processes.	"['In continuation of the question Why is Earth\'s age given by dating meteorites rather than its own rocks?, what evidence do we have that the asteroids indeed formed at the same time as earth? Is there any physical evidence, or is it a conclusion reached mostly through simulation?\nwhat evidence do we have that the asteroids indeed formed at the same time as earth?\nIt depends on what is your definition as ""the same time"". The formation of the solar system and Earth did not happen at a particular second in time but was rather a continuous process. It also depends on what you define as ""asteroids"".\nI\'ll try to put some things in order.\n- The oldest known material from Earth is a zircon from the Jack Hills in Australia. Its age has been recently refined to be 4382 million years. We know that because we measured it.(1)\n- The moon forming event occurred sometime between 4348 and 4413 million years ago.(2) Now, that zircon should be younger than the moon (because the moon forming event would destroy any already-existing minerals on Earth). Yet, there is a slight overlap between the ages. This is due to the fact that analytical capabilities have their uncertainties and error.\nThis means that the Earth has been here at least ~4.4 billion years ago, because you need the Earth to have a moon. The Earth didn\'t just pop into existence, but it formed over a period of time, in which small planetesimals collided with each other and accreted to form a larger body.\nThe question now is, what is the age of the planetesimals? We can\'t know for sure because they are all integrated into Earth, but we can look at asteroids and see their age. We do that by dating the meteorites that fall on the Earth, which in some cases are blasted off asteroids by (even more) collisions and impacts.\nWhen you look inside a specific type of meteorites called chondrites, you can see objects that are called CAIs (calcium-aluminum inclusions). These things are solids condensed from vapourised gas that existed in the solar nebula before any planets and planetesimals formed. These are basically the first solid to form in the solar system, and they define the birth of the solar system. We know that they formed 4568(3,4) or 4569.5(5) million years ago.\nPlanetary bodies (which for this discussion will be considered as km-sized chucks or rock with the ability to melt and differentiate to mantle and crust, and accrete to form proper planets) began forming around 4566.2 to 4567 million years ago(5,6,7). We know that because we can date meteorites that we know originated in asteroids. This is just 2 million years after the formation of the first solid droplets in the solar system. Now, while 2 million may seem like a long time for your daily commute, it is not too long for planetary processes (considering Earth is 4.5-4.4 billion years old).\nNow there is a time gap - what happened between 4566 (formation of planetesimals and asteroids) and 4413 (formation of the moon)? There are about 150 million years that I haven\'t talked about yet. Was the Earth gradually growing in a linear fashion during that time or did it form rather quickly? Luckily, there are answers to that as well. Based on some measured geochemical data it was shown that most of Earth\'s mass actually accreted ~10 million years after the formation of the solar system(8). That\'s rather quick! So the Earth has been sitting there, all by itself, around 100 million years until the moon formed. As to how the Earth itself formed, it could be just amalgamation of small planetisimals, collision of larger bodies, or something completely else. Chemical evidence and physical models do not always agree, and it\'s all highly debated.\nThis is a field which is rapidly evolving and discoveries are made all the time due to better analytical capabilities and better models. Just look at the years of the papers below. This is all cutting edge and a highly exciting field of study. Everything that I wrote here can be inaccurate or even plain wrong, but that\'s the fun in science.\nIs there any physical evidence, or is it a conclusion reached mostly through simulation?\nSo it is a combination of both. You find physical evidence: rocks from Earth and parts of meteorites and their minerals. You measure their isotopic composition to find their age and other characteristics. Then you make a model (or a simulation) that tries to see what has to happen in order for the physical properties to agree.\nAn introduction to Meteorites and the origin of the Solar System - a very accessible and interesting read.\nChronometry of Meteorites and the Formation of the Earth and Moon - a more technical review, may be paywalled.\n1 Valley, J. W., Cavosie, A. J., Ushikubo, T., Reinhard, D. A., Lawrence, D. F., Larson, D. J., … Spicuzza, M. J. (2014). Hadean age for a post-magma-ocean zircon confirmed by atom-probe tomography. Nature Geoscience, 7(3), 219–223. doi:10.1038/ngeo2075\n2 Carlson, R. W., Borg, L. E., Gaffney, A. M., & Boyet, M. (2014). Rb-Sr, Sm-Nd and Lu-Hf isotope systematics of the lunar Mg-suite: the age of the lunar crust and its relation to the time of Moon formation. Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences, 372(2024), 20130246. doi:10.1098/rsta.2013.0246\n3 Bouvier, A., & Wadhwa, M. (2010). The age of the Solar System redefined by the oldest Pb–Pb age of a meteoritic inclusion. Nature Geoscience, 3(9), 637–641. doi:10.1038/ngeo941\n4 Burkhardt, C., Kleine, T., Bourdon, B., Palme, H., Zipfel, J., Friedrich, J. M., & Ebel, D. S. (2008). Hf–W mineral isochron for Ca,Al-rich inclusions: Age of the solar system and the timing of core formation in planetesimals. Geochimica et Cosmochimica Acta, 72(24), 6177–6197. doi:10.1016/j.gca.2008.10.023\n5 Baker, J., Bizzarro, M., Wittig, N., Connelly, J., & Haack, H. (2005). Early planetesimal melting from an age of 4.5662 Gyr for differentiated meteorites. Nature, 436(7054), 1127–31. doi:10.1038/nature038825\n6 Greenwood, R. C., Franchi, I. A., Jambon, A., & Buchanan, P. C. (2005). Widespread magma oceans on asteroidal bodies in the early Solar System. Nature, 435(7044), 916–8. doi:10.1038/nature03612\n7 Amelin, Y., Kaltenbach, A., Iizuka, T., Stirling, C. H., Ireland, T. R., Petaev, M., & Jacobsen, S. B. (2010). U–Pb chronology of the Solar System’s oldest solids with variable 238U/235U. Earth and Planetary Science Letters, 300(3-4), 343–350. doi:10.1016/j.epsl.2010.10.015\n8 Rudge, J. F., Kleine, T., & Bourdon, B. (2010). Broad bounds on Earth’s accretion and core formation constrained by geochemical models. Nature Geoscience, 3(6), 439–443. doi:10.1038/ngeo872\nThe formation of a T-Tauri star spells the beginning of the end of the protoplanetary disk from which planets and asteroids can form. The end is nigh when that star ignites. The large solar winds and solar radiation pressure sweep the disk clean of all small objects. Some spirals inward, some outward. There\'s no dust and no gas from which new planets and asteroids can form. All that can be done is a mopping-up of the not-so-small objects that escaped the solar wind and radiation pressure.\nThe age of 4.55Ga for the Earth is actually a date of ""hard stuff"" based on Iron-Nickel meteorites, dated using radio-isotope dating.\nIf you had enough meteorite samples it should be possible to identify those with isotopes that imply a different age. For example, when plotted, their Pb isotope ratios will have different gradients. We do not see this.\nThe fact that the overwhelming majority of celestial objects within our solar system orbit in a like manner (same direction as the Sun\'s rotation) is observational evidence that they formed at about the same time from the same processes. A rogue planet/proto-planet/asteroid captured by our solar system\'s gravitational footprint in space time would have a 50/50 chance of developing an orbit in the same direction. If that was a retrograde orbit, its chances of survival would be considerably less. Think of a single NASCAR driver deciding to race in the wrong direction.\nI\'m certainly not saying that odd things in orbital mechanics don\'t happen and while our solar system has calmed down quite a bit from its earlier catastrophically chaotic days, it is currently believed by a growing number of scientists that Neptune and Uranus swapped orbital paths due to a harmonic gravitational tug from Jupiter and Saturn and an exo-planet with a retrograde solar orbit (WASP-17b) was either a rogue planet captured by its solar system or its orbit was flipped into a counter solar spin direction from a near collision with another object.\nPerhaps a little step back to get a bigger point of view is required. Take note of this line in the answer provided by Michael - ""it might be plain wrong"".\nThe question uncovers some very glaring assumptions: 1. We know how the earth was formed. 2. We know when the earth was formed.\nThe current theory of accretion doesn\'t hold much water - both metaphorically and literally speaking. There just is no way that dust or rocks in space can accrete all by itself to form a body such as the earth. Even basic physics disagrees with this. The simulations have to ignore this basic fact and instead introduce fudge factors to make rocks and dust ""stick"" together when the collide. You just go out and try making rocks stick together when they smash into each other - confirm for yourself whether it will work in space where there\'s even less resistance to flying apart. Furthermore, even if one allows for some miraculous hot sludge of radioactive material to stick together, one immediately creates another problem: the exclusion of water because of heat. There is no acceptable way for all the water on earth to arrive here from outer space. Why would the water only strike earth [ and where would it come from in the first place ] in such a way that Mars, Venus and the moon do not have any evidence of a watery bombardment?\nThen there\'s the question of how does one measure the age of anything?\nAge is unfortunately not a physical property that we can measure directly like length, volume, density, etc.\nIn order to know the age of anything one needs to have been present when it came into existence and then to have some way by which to assign some kind of measure of passage of time to it. In short - age is an abstract construct that cannot be measured physically.\nWhat can be measured is the proportions of the number of atoms of mother and daughter products of radioactive material. But having established that proportion and applying a rate of decay to determine age doesn\'t give a true age. Have you ever found a record of calibration that confirms that the radioactive methodology does indeed deliver the correct age? Did someone witness and record the creation of basic radioactive rock and then track it\'s age using the radioactive decay method? To my mind, it hasn\'t been done and in fact where some measurements have been made of newly created basalt in St Helen\'s eruption, the ages produced are plain incorrect. The best that it can produce is an ""age"" based on some very questionable assumptions.\nSince we do not know the history of the earth we need to make some assumptions regarding its creation. We need some form of ""clock"" by which we can measure how old the earth is. That clock has to be independent of the earth itself, hence why people use meteoroids, under the assumption that it must be at least as old as if not older than the earth itself.\nThe main problem is that we need to know when that clock started ticking and we just don\'t have any means to establish an objective way to determine that. No one was there to witness and record that event. There-in lies the problem.\nEven if you assume that the radio-active decay happened at a constant rate[ which we have seen is now no longer true ], you still have to make assumptions about the quantities involved. But therein lies another problem - you don\'t know the origin and history of the original material - how long did IT exist before it started its radioactivity? One can go on ad-infinitum.\nSo in spite of the highly ""scientific"" answer provided by Michael, the fact is that one cannot determine the age of the earth without making some highly metaphysical assumptions about the origin of the universe. You have to make some questionable unsubstantiated assumptions as to how the whole universe came into being. It cannot be determined scientifically, in spite of using the latest and greatest technology in cosmology today. We simply weren\'t there. We have no eye-witness account. We have no record. The best we can do is make assumptions of the starting point.\nYou may not like this answer. You may even get angry at seeing it but unfortunately for you, you cannot push it\'s validity out of the way.']"	['<urn:uuid:77cf6e65-3e4a-4eee-9273-df895864d090>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	10	31	2168
40	What are the risks of working with chlorine in industrial settings, and what responsibilities do employers have to protect workers from exposure to such hazardous substances?	Working with chlorine poses several risks. In industrial settings, moisture contamination can lead to the formation of dendrites (crystal-like masses) that can cause valve leaks and equipment damage. Any amount of moisture can lead to rapid corrosion of moving parts and create potential leak paths. As for employer responsibilities regarding hazardous substances like chlorine, they must implement control measures to protect workers, prevent or adequately control exposure, provide suitable information and training, maintain control measures in working order, draw up emergency procedures, ensure proper health surveillance, and carry out COSHH risk assessments. They must also ensure that substance exposure doesn't exceed Workplace Exposure Limits and provide appropriate protective equipment where necessary.	"['Dry chlorine gas is found within chlorine production, storage and transfer facilities, as well as in downstream processes such as production of vinyl chloride monomer (used in making polymer polyvinyl chloride) or phosgene (used in applications such as pharmaceuticals and organic compounds). Dry chlorine is generally understood to have less than 150 parts per million (PPM) water containment.1 Moisture contamination in any of these dry chlorine service areas, however, can result in the formation of dendrites, crystal-like masses that can cause problems.\nFor example, research shows that dendrite formation on critical sealing surfaces, such as valve seats and packings exposed to extremely harsh operating conditions, leads to a significant increase in potential leak paths because of erosion of seal surfaces that come into contact with the dendrites. Dendrites can also form on contacting or sliding surfaces, causing valves to freeze or to become abrasive to the soft seat, which can lead to severe seat damage. In fact, any amount of moisture can lead to dendrite formation and rapid corrosion of moving valve parts.\nRecent collaboration between valve manufacturers and end users has led to a clearer understanding of the factors that cause valve failure under these circumstances and factors that can increase service life of those valves. As part of that collaboration, researchers investigated construction materials that could combat moisture contamination and provide enhanced performance in harsh operating conditions, looking at each component of the valve for dendrite susceptibility. Suitable materials were identified, leading to designs that incorporate the appropriate combination of super alloys and stainless steel to allow for a cost-effective solution to these issues.\nThis article discusses the relationship between dendrite formation and valve life, and provides an overview of shortcomings that are now being overcome with these new designs and materials. Coincidentally, the improvements in design that resulted, combined with best practices for specifying features, has contributed to lower total life-cycle costs in the critical applications in dry chlorine services.\nTHE ISSUES INVOLVED\nTo understand the newer solutions and how they were derived, it is important to look at critical valve performance and maintenance issues that end users encounter in applications involving dry chlorine. Understanding the factors that can contribute to valve failure enables valve manufacturers to make corresponding design improvements, and the end user can use this knowledge to extend valve service life.\nFor these dry chlorine applications, end users articulated that real world applications were less than ideal—that these applications posed challenges that were too often severe and unforeseen, and that the end users could save money if they had access to valves that would tolerate the varied and harsh conditions involved. The consensus vision for an optimal design was that it would take into account demanding material requirements, ease of cleaning and packaging, and that external seals would limit emissions under severe conditions. Double packing and a monitoring port also were considered preferable.\nBy working through both typical and more unusual critical pain points, it became evident that dendrites often formed on the surface of Monel discs, causing damage to the soft seats. When such crystals form on frictional surfaces, they cause binding and seizing of mating parts. An example is simple components such as disc spacers that keep the disc centered. The shaft turns inside the bearings and packing so any crystal formation quickly deteriorates these critical components. When these crystals form on hard surfaces that make contact with soft seals, they erode the seals (working like sandpaper).\nFurthermore, while Monel performs well in dry chlorine service (when temperatures stay high enough to preserve chlorine’s “dry” form), any amount of moisture can lead to rapid corrosion of moving parts. For these two reasons, engineers settled on HastelloyC as the preferred material for disc and stem construction.\nFor the valve body, CF8M stainless steel has been identified as the new standard, but all other valve components (pin, gland, bolts, etc.) were also studied individually to determine the optimal choice of material. Using Hastelloy for key parts enables the valve to tolerate moisture. Using CF8M stainless steel for the body, meanwhile, also makes the exterior visibly cleaner and provides visual confirmation that the device is robust.2\nPamphlet 6 of the Chlorine Institute (page 5 of the May 2005 edition), recommends the use of carbon steel for piping for handling dry chlorine. Monel bodies and internals are recommended for dry chlorine, but on page 20, it suggests that consideration be given to materials suited for both wet and dry applications if there is a chance that moisture contamination could be present. For wet chlorine, HastelloyC internals are specified by The Chlorine Institute (note that dry chlorine can become “wet” through moisture contamination, or by sufficiently reduced temperatures).\nResearch indicates that even in dry chlorine service, systems are prone to moisture contamination during installation or whenever connections are made. Such contamination reacts with the chlorine and forms hard, microscopic metallic chloride crystals on metal surfaces. This occurs even on Monel surfaces, which are considered acceptable with perfectly dry chlorine.\nTo combat corrosion, the use of HastelloyC is the better option for dynamic surfaces that form between metal and soft parts. This is a very hard, resilient material that is more costly and more expensive to machine. However, it is indispensable for longer product life and reduced total lifecycle costs—reduced in-service operating costs (replacement, maintenance, spill/leak cleanup, emission/environmental hazards, etc.) more than compensate for higher up-front costs.\nWCB valve bodies are normally recommended for dry chlorine since stainless steel is susceptible to chloride stress cracking. CF8M bodies have demonstrated exceptional endurance in corrosive environments, while being structurally resilient against stress-cracking.\nEnd users also showed interest in valves featuring live-loading, double-packing with monitoring ports and Inconel spring (Belleville washers). Ideally, the packing gland is made from HastelloyC to prevent freezing in the packing bore, which would disable live loading. The packing gland is a simple part on the outside of a valve that might not appear imperiled by possible contamination. Experience indicates, however, that it can be exposed to considerable moisture and minute traces of chlorine vapor emanating or permeating from the system, often causing the gland to seize. Once this occurs, it no longer serves its function of pressurizing the packing, and leakage may result.\nBecause of these risks, Alloy 20 bolting can be used for packing gland studs and nuts to minimize corrosion on these stressed parts. At the same time, this mitigates stress cracking concerns that arise when stainless steel is used.\nA lantern ring between the series of packing allows leakage from the flow path to accumulate in an area adjacent to the second packing. A small sampling valve can be threaded to a monitoring port channeled to this area so it can be periodically checked for early signs of weakening. The lantern ring should be made from HastelloyC to ensure long term operation.\nFor seating, the use of Teflon provides a compliant, inert seal that provides bubble-tight shutoff. An axially pliant seat allows the use of reinforced PTFE seats to maximize resistance to abrasion while maintaining the chemical compatibility of the Teflon. This axially pliant seat flexes but is not subject to hoop stretch. Since Teflon does not have “memory,” it is important to avoid stretching the material.\nChlorine systems often encounter rust or scale so valve seats must be impurity-tolerant. Molecularly enhanced PTFE (TFM) has the disadvantage of performing poorly at low temperatures whereas the R-PTFE ensures valves perform well during temperature cycling and in cold environments.\nSystems often require bidirectional sealing as well as the ability to be used in end-of-line service with pressure contained behind a closed valve without the concern of which side will remain exposed to the medium being processed (which is accompanied by pressure from the piping system).\nBecause minimizing the potential for leak paths is essential in controlling fugitive emissions, using a closed bottom design is a good choice for inhibiting leakage that might occur if the shaft bore protruded through the bottom of the valve. This is the acknowledged best design for valves 12 inches and under. For larger valves, where it is not practical to bore the bottom shaft hole from the top of the valve, a bolted plate with a static PTFE seal is preferred over a threaded plug because a threaded plug tends to grab a seal and twist it during assembly.\nWith the foregoing realities in mind, valve manufacturers are either developing solutions or have already succeeded. The culmination of those efforts is a valve made with materials designed to withstand and be impervious to the effects of dendrite formation. Such is the case with some high-performance butterfly valves, which have incorporated the advances mentioned here in efforts to meet the needs of dry chlorine service. Such valves represent a breakthrough in design, as they specifically address the requirement of preventing valve damage from both moisture contamination and abrasion or erosion.\n1 The qualifying reading may not always be 150 PPM because chlorine at this and lower readings can become “wet” in sufficiently cool or cold conditions.\n2 For reference about recommended materials for valve components, please see the Valve Materials Selection Guide provided by The Chlorine Institute: www.cl2.com/index.php/.../139-pamphlet-6-edition-15-may-2005 table 4.8).\n- The Chlorine Institute, Inc. Pamphlet 6 – Piping Systems for Dry Chlorine (Edition 15)\n- Xomox Manufacturing Control Manual – Level III Cleanliness Requirements: Chlorine, Food, Std. Oxygen, and Vacuum Service Valves', ""COSHH – A Guide to Employers’ and Employees’ Responsibilities\nEmployee Responsibilities under COSHH\nEmployee responsibilities within the COSHH (Control of Substances Hazardous to Health) Regulations of 2002 include:\n- Making use of control measures and facilities provided by the employer\n- Ensuring equipment is returned and stored properly\n- Reporting defects/insufficiencies in control measures\n- Wearing and storing personal protective equipment (PPE)\n- Removing PPE that could cause contamination before eating or drinking\n- Making proper use of washing, showering and bathing facilities when required\n- Maintaining a high level of personal hygiene\n- Complying with any information, instruction or training that is provided\nEmployer Responsibilities Under COSHH\nUnder COSHH regulations, employers’ responsibilities include:\n- Implementing control measures to protect workers from hazardous substances.\n- Preventing or adequately controlling exposure to hazardous substances.\n- Providing employees with suitable and sufficient information, instruction and training, and appropriate protective equipment where necessary.\n- Ensuring that control measures are maintained, kept in full working order, and in a clean condition where appropriate.\n- Drawing up plans and procedures to deal with accidents and emergencies involving hazardous substances.\n- Ensuring that any employees exposed to hazardous substances whilst at work are under suitable health surveillance.\n- Ensuring that substances do not exceed the Workplace Exposure Limit (WEL).\n- Carrying out a COSHH risk assessment.\nNaturally, workplaces with higher risks, such as catering or a hair salon, will require more action than, say, an office. But as an employer, you should be assessing what risks may be posed by hazardous substances, no matter where you work.\nThat way, you can identify if there are risks and if so take action to reduce them to a minimum.\nCOSHH Risk Assessment\nA COSHH risk assessment is essentially the same as a standard risk assessment in terms of the process, but your assessment of the workplace will focus solely on hazardous substances.\nIf you’re unfamiliar with risk assessments, here’s a breakdown of the main 5 steps:\n- Identify the hazards.\n- Decide who might be harmed and how.\n- Evaluate the risks and decide on precautions.\n- Record your findings and implement them.\n- Review your assessment and update if necessary.\nRisk assessments will also involve frequently monitoring the workplace’s processes and the level of exposure to substances.\nWorkplaces are active and constantly changing, so a one-off check won’t be sufficient in minimising the risks posed by hazardous substances. You have to remain constantly vigilant and alert to the dangers.\nRecap: What is COSHH?\nCOSHH stands for the Control of Substances Hazardous to Health Regulations (2002). It exists to ensure that both employers and employees do all they can in a workplace to minimise people’s exposure to hazardous substances and work in ways that are safe.\nThis means that all hazardous substances need to be identified and precautions need to be taken to ensure that workers know how to use and handle them safely.\nThe importance of controlling hazardous substances cannot be overstated. In 2012/13, around 35,000 workers reported that they had breathing or lung problems caused by work, and the most common type of reported skin disease was contact dermatitis.\nAnd it’s estimated that around 13,000 deaths occur each year due to occupational lung disease and cancer – fatal conditions that will have developed over a prolonged period of exposure to dusts and chemicals at work.\nAs an employee or employer, you can prevent statistics like this from increasing. If you fulfil your workplace duties, you can prevent dangerous levels of exposure and meet COSHH requirements.\nRecap: What is a Hazardous Substance?\nSimply put, a hazardous substance is any mixture or substance that is toxic, irritant, or corrosive – whether it’s a liquid, gas, vapour, fume, or dust.\nThey cause harm to the body via routes of entry:\n- By coming into contact with skin or eyes.\n- By being inhaled.\n- By being ingested through the mouth.\n- By entering the body through cuts or punctures in the skin.\nAlthough there are certain industries that will be at greater risk, hazardous substances could exist in any workplace. They are often used directly in work activities, produced by work activities, or already present in your workplace’s premises.\nExamples of hazardous substances include:\n- Chemicals, e.g. cleaning chemicals or bleach.\n- Fumes, e.g. from paint or vehicles that exhaust.\n- Gases, e.g. ammonia from refrigerators.\n- Dusts and powder, e.g. from flour.\nIt’s worth noting that even seemingly innocent substances can be harmful, and that includes natural materials like wood dust or flour.\nWhile many hazardous substances can cause immediate harm, such as a corrosive liquid being spilled onto someone’s skin, the main danger posed by hazardous substances is prolonged exposure. For example, if someone is in the presence of or uses a dangerous chemical for a long time, they could develop breathing difficulties or skin conditions.\nExamples of ill-health caused by hazardous substances includes:\n- Occupational asthma.\n- Occupational dermatitis.\n- Occupational cancers.\n- Skin irritation.\n- Infection from bacteria.\n- Injury or death as a result of exposure to toxic fumes.\nWhat to Read Next:\nSubscribe for the latest Hub updates! Tell us what you're interested in hearing about:""]"	['<urn:uuid:3239124a-92eb-421d-bd65-dcc5664a7002>', '<urn:uuid:e2c99559-4fd5-48cf-884f-dca2baae7e40>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:09:04.591601	26	111	2425
41	What are the main components of a synovial joint and their functions?	A synovial joint has 5 typical components: 1) Hyaline cartilage that reduces friction and provides shock absorption, 2) Synovial fluid that nourishes cartilage and lubricates the joint, 3) Synovial membrane that produces synovial fluid, 4) Joint capsule which is a fibrous layer outside synovial membrane, and 5) Collateral ligaments that hold bones together and reduce movement in unwanted directions.	['Bones and Joints of the Proximal Forelimb\nHome > Flashcards > Print Preview\nThe flashcards below were created by user\non FreezingBlue Flashcards\n. What would you like to do?\nWhere is the central nervous system located?\nThe brain is located in the skull and the spinal cord is located in the vertebral column\nWhere do cranial/spinal nerves emerge from?\nCranial nerves emerge from the brain and spinal nerves emerge from the spinal cord\nDescribe the direction of afferent/efferent pathways\n- Afferent - body to brain - sensory fibres and proprioception\n- Efferent - brain to body - voluntary movement\nWhat do spinal nerves pass through when they leave the spinal cord?\nThe intervertebral foramen\nWhich ventral branches of the spinal cord form the ____ plexus in the forelimb?\n- C6, C7, T1, T2\n- Brachial plexus\nWhich extrinsic forelimb muscles does the brachial plexus NOT supply?\nBrachiocephalic and Trapezius\nWhat type of bone is the humerus?\nWhat part of the humerus articulates with the scapula to form the shoulder joint?\nHead of the humerus\nWhat palpable feature of the humerus marks the location of the shoulder joint? What aspect of the humerus is it located on\n- Greater tubercle\n- Lateral (lesser tubercle is located on the medial aspect)\nWhat is the function of the inter tubercular groove?\nAllows passage of the biceps tendon\nWhat palpable landmarks are located at the distal end of the humerus?\nMedial and lateral epicondyles\nWhat structure is located between the medial and lateral epicondyles? What is its function?\nTrochlea - provides a smooth articular surface for the elbow joint\nWhich fossa is located at the distal end of the humerus?\nOlecranon fossa is located on the caudal aspect of the distal humerus\nWhat foramen is present in dogs?\nDoes anything pass through the supratrochlear foramen?\nIs the supratrochlear foramen present in cats and larger species?\nWhat foramen is present in cats? Does anything pass through it?\nSupracondyloid foramen - blood vessels and nerves pass through\nHow many centres of ossification does the humerus develop from?\nWhat is a weak point in the humerus?\nThe supratrochlear foramen\nWhat are the different types of joints?\nWhat are the 5 typical components of a synovial joint? What is the function of each?\n- Hyaline cartilage - reduces friction, flexible for shock absorption\n- Synovial fluid - nourished cartilage, lubricates joint\n- Synovial membrane - produces synovial fluid\n- Joint capsule - fibrous layer outside synovial membrane\n- Collateral ligaments - hold bones together, reduce movement in unwanted directions\nWhat are the two components of the shoulder joint?\nGlenoid cavity of scapula (concave) and head of humerus (convex)\nWhat are the palpable landmarks for location of the shoulder joint?\nAcromion process of scapula and greater tubercle of humerus\nWhich muscles provide lateral and medial support to the shoulder joint? What are their nerve supplies?\n- Lateral - infraspinatous and supraspinatous muscles. Suprascapular nerve.\n- Medial - subscapularis muscle. Subscapular nerve.\nWhat structure protects the biceps brachii tendon of insertion as it runs through the inter tubercle groove?\nThe bicipital bursa\nWhat holds the bicipital bursa in place?\nWhat is osteochondrosis?\nErosion of articular cartilage and subchondral bone\nWhat is degenerative joint disease?\nNew bone formation around the osteochondral junction\nWhat causes DJD?\nWear and tear. Also damage to joints.\nWhich is weight bearing: radius or ulna?\nWhich part of the ulna provides the articular surface for the a) elbow joint b) carpus?\nWhat is the function of the ulna?\nIt provides leverage for extension of the elbow and movement of the distal limb\nWhat palpable feature is present on the proximal end of the ulna?\nWhat part of the ulna fits into the olecranon fossa of the humerus?\nThe anconeal process\nWhat part of the ulna provides the articular surface for the elbow joint?\nWhere does the radius sit in relation to the ulna? What holds it in place?\nThe radius sits between the medial and lateral coronoid processes of the ulna. It is held in place by an annular ligament.\nWhat is the most medial projection a) a proximal b) distal: radius or ulna?\nHow many centres of ossification does the radius and ulna develop from?\n- Radius - three\n- Ulna - four\nWhy must the radius and ulna grow together?\nIf they do not grow together there will be angular limb deformities\nIn which animals are the radius and ulna fused?\nRuminants and horses\nWhat would you like to do?\nHome > Flashcards > Print Preview']	['<urn:uuid:813013c0-cb2b-4b61-a780-14f6b46b254a>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	12	59	752
42	Looking for shade stops between Meran and Burgstall - suggestions?	Near the Lana-Burgstall train station, there's a rest area with shade and water, featuring a specially-constructed green cupola. This is one of the few shady spots on this section, as other areas with picnic tables have only young trees offering little shade.	['Published on: 12 April 2019 | Last updated: 27 February 2020\nUsing this guide\nThis page is the introduction to a series of articles, it is intended to provide an overview of the route, together with information on how to get to and from the start and finishing points. The downloads section at the bottom of the page includes downloads of GPS files as well as maps in PDF format.\nThe route is described in more detail in the articles in the series. You can navigate between them using the Next/Previous arrows at the end of the main article, or the list of links at the bottom of the page (and in the sidebar if your screen is wide enough).\nAfter Meran the main Etschradroute follows the river as it flows south and joins with the Eisack (Isarco) near Bozen (Bolzano). The ride changes too: the cycleway continues on the top of the high flood-protection embankments and you ride looking over the vineyards and fields of apple trees that fill the wide, flat, floor of the Adige valley.\nAt a glance\nEasy — completely flat.\nAlmost all on traffic-free cycleways.\nEntirely on asphalt-surfaced cycleways or roads..\nThe main alternative option is to turn off the cycleway at Sigmundskron (Ponte d’Adige) and follow the Überetsch cycleway as it climbs to Kaltern (Caldaro).You then rejoin the main route again near Auer (Ora).\nBozen (Bolzano) is 5 kilometres off the route following the Eisack (Isarco) river. It’s well worth considering as a side-trip, or if you’re planning on heading north along the Brennerradroute.\nDistance: this guide isn’t divided into daily stages, as people differ in how fast and how far they want to travel each day.\n‘Traffic-free’: many cycle routes include sections with roads with restricted access for residents or people working on the adjoining land. You may, very occasionally, encounter an agricultural vehicle like a tractor pulling a trailer of hay, but most of the time there is no motorised traffic. They are often indistinguishable from the cycleways that are legally set aside for the exclusive use of cyclists and pedestrians.\nMap and altitude profile\nPowered by WP-GPX Maps\ntips for using the map\nRun your cursor over the graph to show the elevation, and distance from the start, for any given point on the route. (Note: the altitude graph is not shown where the route is flat).\nClick the little icon in the right-hand corner to see the map fullscreen\n|Meran to Terlan (Terlano)||19 kms|\n|Terlan to Sigmundskron (Ponte d’Adige)||7 kms|\n|Sigmundskron to turning for Bozen (Bolzano)||2 kms|\n|Sigmundskron to Auer (Ora)||17 kms|\n|Auer to Salurn (Salorno)||14 kms|\nAbout this table\nThe table doesn’t necessarily show the distances from one city centre to the centre of the next town — if a route skirts around a town the distances are measured to the nearest point on the route from the centre.\nMeran to Sigismundkron (Ponte d’Adige)\nThe route through Meran is well signposted. Look out for a sculpture with a bike wheel surrounded by padlocks. The padlocks were left on a nearby bridge by young soldiers at the end of their military service. When the bridge was refurbished the locks that left were moved to this the sculpture here. One typical inscription reads:\nFratelli per 1 anno, amici per la vita. E’ FINITA!!\n(Brothers for a year, friends for life. IT’S OVER!!).\nFrom Meran, the cycleway continues along the left bank of the river. This isn’t the most interesting part of the route as you have the river on your right-hand side and the rail line on the other. On the other side of the river is the SS38 road. The SS38 is off-limits to bikes, but the cycleway has good links to the villages along the way.\nUnfortunately, there are very few shady places on this section of the cycleway. There are some picnic tables that have been planted with trees, but these are still quite young and offer very little shade. However, near the Lana-Burgstall train station, there’s an old railway bridge that takes you over the river and to a rest area with shade and water and a specially-constructed green cupola which is about as shady as you can get. Next door to the bridge is an old electric locomotive, the Apfel Express (Apple Express) that used to transport apples on a branch line. There’s also a bridge over the railway line which gives you direct access from the cycleway to the train station.\nAt Gargazon (Gargazzone) there is an underpass that takes you under the railway line and to the open air swimming pool (naturbad) just on the other side of the tracks.\nOptions at Sigmundskron\nAt Sigmundskron (Ponte d’Adige) you have the option of taking a variant of the Etschradroute that follows the Überetscher Bahn railway as it heads for Kaltern and the heart of the Südtirol wine country around the Kalterer See (Lago di Caldaro).\nEven if you are planning on heading south, the Überetsch cycleway is well worth the effort. You could either do it going to Kaltern and then coming back the same way, or going to the Kalterer See and then rejoining the main Etschradroute near Auer (Ora).\nAs you come into Sigmundskron you come to a junction with the road, and a sign indicating that you need to turn right and cross over the bridge. My advice is to ignore the sign, and instead turn left following the sign for the Radstation Wiesl . There’s a short stretch on the road before you bear right. The road then takes you past the Radstation, over a level crossing, and to an old steel rail bridge. This is the start of the Überetscher cycleway. Go over the bridge if you want to take this variant, otherwise stay on the left bank of the river.\nIn front of you should see a castle on a hill — this is the Schloss Sigmundskron (Castello Firmiano).\nSigmundskron to Auer\nA kilometre or so further on, you come to the point where the Etschradroute meets the Brennerradroute. Turn left for Bozen (Bolzano) a bit more than 5 kilometres from here, or turn right to continue south.\nThe next stop is Auer 16 kilometres further on along the Etsch/Adige. For a little over 3 kilometres, the cycleway follows a narrow strip of land between the Etsch and the Eisack as they flow in parallel before they merge to become the Etsch. At the Bozen city boundary, the route crosses a bridge over the Etsch and picks up a quiet country lane. On the outskirts of Auer you make a short detour to go under the rail line. Turn left if you want to go into Auer, which is a little over a kilometre away, otherwise turn right.\nAuer to the Salurn (Salorno)\nFrom Auer, the cycleway continues towards the end at the border with the Trentino region. It’s an enjoyable cruise, but by this point, you may have decided that you’ve seen enough vineyards and apple trees. If you want a bit of a change of scenery or a stop for an ice cream, then the Neumarkt (Egna) with its attractive altstadt (old town) is a short detour off the route.\nYou could also stop off at the nearby Klösterle Sankt Florian once a hospice for pilgrims (including Albrecht Dürer on his way to Venezia).\nThe cycleway brings you to the bicigrill at Salurn (Salorno). A bicigrill is a café beside a cycleway — the name comes from the Autogrills on Italy’s autostrade. The bicigrill by the bridge at Salurn is, for me at least, always a welcome sight and an obligatory stop. It’s something of an institution, although it now has a rival for your affections a a few kilometres further on: the Bicigrill Faeda aka Bike Break.\nThe bicigrill is pretty much at the end of the Etschradroute (or at least before the cycleway crosses into the Trentino and becomes the Ciclabile Valle dell’Adige). Look out for the Schlöss Haderberg on a rocky spike above Salurn. In an area with so many castles, this has perhaps the most dramatic location.\nArticles in this series\n- italy-cycling-guide.info: The Etschradroute: Introduction\n- Etschradroute: Part 1: Reschensee to Algund (Lagundo)\n- Etschradroute: Part 2: Algund to Salurn (Salorno)\nPlaces to stay\nThis is a popular tourist area so that there are a lot of accommodation options — although in high season there is also a lot of demand. The main towns are Meran and Bozen, but almost all of the villages along the way will have at least one hotel or gästehaus .\nMerano (Meran) | Marlengo (Marling) | Bolzano (Bozen) | Appiano | Tesimo (Tisens) | Nals (Nalles) | Andrian (Andriano) | Leifers (Laives) | Kaltern (Caldaro) | Auer (Ora) | Egna (Neumarkt) | Salurn (Salorno)\nThere are hostels in Meran (Jugendherberge Meran) there’s also a jugendherberge in Bozen (Jugendherberge Bozen) and Salurn (the Jugendhaus Dr Josef Noldin ). There’s also the Gästehaus Castelfeder but it’s tricky to get to without going on the busy SS48.\nCampsites on this section of the main Etschradroute:\n- Schlosshof Ferienresort\n- Camping Zum Guten Tropfen near Nals (Nalles)\n- the Camping Tisens at Tisens near Nals\n- the Camping Steiner in Leifers\n- at Auer the Hotel Markushof also runs a campsite\n- Camping Ganthaler at Terlan (Vilpiano)\n- the Camping Obstgarten is a friendly site near\nCampsites at the Kalterersee\n- the Camping Sankt Josef and the [Camping Gretl am See at the Kalterersee. When I stayed at the Camping Sankt Josef, the main area of the campsite — for campers and caravans was absolutely crammed, but fortunately there is an area devoted to tents around two sides of the site, so there was enough space. At 13€ it was good value\nPlaces to eat and drink\nSouth from Meran, there are far fewer places along the cycleway itself and you may need to make a detour into the villages along the way.\n- Sinich (kilometre 89): there’s a bar on the corner of the OBI building (OBI are a German DIY/Home/Garden retailer). There are several other places along the main road.\n- Sigmundskron (kilometre 110): Radstation Wiesl (weekends only)\n- Branzoll (kilometre 119): there’s a café-restaurant (Zur Brücke) by the bridge, just after you cross over the river, a couple of kilometres from the village itself. I must admit it’s always seemed to be closed whenever I’ve come past. Alternatively, there’s the Pizzeria Vadena, which is 300 metres or so away on the road to Pfatten (Vadena). Turn right just before the bridge and take the cycleway beside the road.\n- Auer (kilometre 127): there are several places to eat and drink in Auer, a short detour from the route, just follow the signs. The closest option to the cycleway is Viktor’s Imbiss a short distance away, on the other side of the river, on the road to Kaltern (closed on Mondays).\n- Neumarkt (kilometre 131): if it’s Monday and Viktor’s Imbiss is closed, there’s another imbiss a bit under three kilometres further on, between Auer and Neumarkt (closed on Wednesdays). It’s OK, but located quite close to the main road, so if you may want to consider making the detour to the attractive centre of Neumarkt. There may also be a place a little further on along the radweg, but I can’t say for sure it’s still there. I would also recommend theFischerhof restaurant on the other side of the river in Kurtatsch.\n- Salurn (kilometre 141): the last time I passed by, there was the Bike Café at the bridge at Salurn, but I haven’t been able to confirm whether it is still there.\n- Meran (Merano): Busselli Bike | Fin Sports (Bike and More) | Flarer | McBike | Salvadori (Via IV Novembre 62A)\n- Bozen (Bolzano): Alpina Cicli (Claudia Augusta Straße) | Biciclette Fausto | Cicli Marchetti | ComfortBIKE | Sportler (Bozen) | Zagocicli | Zanolini\n- Nals (Nalles): Tobike Nals\n- Lana: Staffler Bike\n- Leifers (Laives): Neri Diego (Via Galizia 1) Unterkofler P (159 Via John F Kennedy)\n- Eppan: Sanvit Bike & Fitness\nDistances are from the start of the cycleway at the border with Austria. Distances are (roughly) to the nearest point on the route.\nIf you know of other bike shops, or you spot a mistake, please let me know.\nMeran, and the stations on the route after it, is served by trains from both the national Trenitalia, as well as the regional SAD company (and you may also see trains operated by Trentino Trasporti). Trenitalia tickets are valid for travel on services operated by the regional train companies.\nIf you catch a train from one of the smaller stations it may only have a ticket machine for the SAD company, and stations outside the Südtirol may not be listed. Don’t worry; you can sort it out with the train manager when you get on the train.\nGeneral tourist information\nThe Südtirol has excellent regional tourist information website: suedtirol.info (de/it/en/nl/cz/pl). Useful local sites include:\n- naturns.it (it/de/en)\n- suedtirols-sueden.info (bolzanosurroundings.info) is the main tourist information website for the southern Südtirol (de/it/en)\nPlaces and attractions\n- Messner Mountain Museum Firmian (Sigmundskron)\n- listing of swimming pools in the Südtirol\n- listing of outdoor pools/swimming facilities around Meran and Bozen.\n- Naturbad Gargazon\n- suedtiroler-weinstrasse.it has information about cycle routes in the Südtirol wine country']	['<urn:uuid:d9a44177-a4ba-420a-ada7-3dfd71e42115>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	10	42	2233
43	How can I create a natural garden environment that attracts wildlife, and why is it important to avoid using chemical pesticides for the ecosystem?	To create a natural wildlife-friendly garden, you need to provide four key elements: food, water, shelter, and space. You can plant a variety of native plants that seed, flower, and fruit throughout the year, create water sources like small piles of moist sand for butterflies, leave areas with dead flowers and grasses for insect shelter, and make comfy homes using rock or branch piles. Avoiding chemical pesticides is crucial for the ecosystem because these substances can harm beneficial insects like bees and ladybugs, contaminate waterways through runoff, and persist in the environment for long periods. Pesticides can kill bees directly when they land on treated plants, affect entire colonies when foraging bees carry pesticides back to their hives, and may be linked to honey bee colony collapse disorder. Additionally, pesticides can wash into streams and harm salmon and other aquatic organisms, potentially remaining in soil, sediment, or water for weeks to over a year.	['Bees and Butterflies add much to my garden excitement. Bee and butterfly gardeners reap many rewards considering they often enjoy the same colorful flowers the insects rely on. Insects can add color, beauty and grace to your garden. It’s delightful to discover the array of activity bouncing on each bloom and blossom, such a garden is sure to win compliments from your friends, family and neighbors.\nAs a naturalist, I offer interpretive walks and talks to visitors of all ages in an effort to educate, entertain and enrich their lives with the natural environment. While one can never guarantee wild nature observations, you can increase your odds of encounters through the art of turning your yard into a wildlife habitat. All you need to provide are the following four elements: food, water, shelter and space. It’s as easy as 1,2,3,4.\nLet’s Eat – We all need food. Wildlife eats a variety of food found in nature, some that we can provide. Tip: By planting a variety of plants that seed, flower and fruit throughout the year, you’ll provide a banquet for wildlife.\nPuddle Party – Butterflies will visit wet soil to sip water and obtain salt and mineral nutrients using their proboscis. This act is known as mud-puddling.Tip: Provide a small pile of moist sand to attract butterflies.\nLeave ‘Em Standing – Clumps of dead flowers, grasses and vines make cozy hideouts for insects to use as shelter. Tip: Leave inconspicuous areas of your yard wild for wildlife.\nPile it On – Pile up rocks or fallen branches lying in your yard. Put the biggest stuff on the bottom. Presto! A comfy home for hibernating butterflies, beetles, snakes, frogs and toads! Tip: Put the biggest stuff on the bottom.\nDelightful Decomposers – Rotting logs provide homes for pill bugs, centipedes, millipedes and more. Tip: Allowing rotting logs to decompose also helps improve the soil.\nGive Me Shelter – Many Native bees may nest in hollow stemmed plants or holes found in trees. Tip: Create a Native Bee Nesting Block to attract native pollinators to your garden. Drill holes between 3/32″ and 3/8″ diameter and 3″4″ deep in un-treated lumber.\nAll Native – Focus on native plants that are good for native wildlife. Remember native plants feed and attract native animals. Tip: Go native and avoid foreign, invasive plant species.\nFree Your Lawn – All Natural – Manicured Lawns are largely useless to wildlife, there’s no food, no place to hide and no shelter. Consider allowing native plants to takeover where mowing isn’t necessary. Tip: Allow for a low maintenance natural lawn and bees, butterflies and birds will be thankful. Mow your lawn less often.\nAll Natural – Pesticides and herbicides are bad for pollinators, and without pollination many plant species cannot thrive. Tip: Organic gardening can reduce the need for synthetic chemicals.\nBeneficial Bugs Kick Pests-a-side – Ladybugs and hoverflies are considered beneficial bugs because they eat aphids, a plant pest, which can destroy garden plants. Tip: Allowing these beneficial bugs to reside in your yard reduces the need to use pesticides.\nNo Yard? – Add wildlife friendly plants to containers, window boxes and balconies. Tip: Growing cooking herbs like dill, fennel and parsley will also attract pollinators.\nPlant Seeds – Share and collect native plant seeds with other gardeners. Create a garden that includes a mixture of the best host and nectaring plants for pollinators. Tip: One seed can make a difference. We need you to plant it!\nCreate a Pond – Pond habitats provide space for a variety of wildlife to survive and thrive. You will be dumbfounded by dragonflies and dazzled by damselflies. Tip: Add wetland plants to create habitat for aquatic insect larvae.\nUncover and Discover – Bees and butterflies may be most active during the middle of the day after many birds have begun to rest. Tip: After early morning birding, search for bees, butterflies and dragonflies in the middle for the best results.\nMost Importantly…Share Your Habitat\nThe best place to learn about the environment is in the environment. Creating backyard or schoolyard habitats provide places for children to explore and discover the natural world around them. Backyard habitats can beautify, entertain, educate and enrich our lives so make sure to invite neighbors and friends over to see what you have accomplished.\nFinally…Create a Naturalists Kit to Explore and Enjoy a Habitat!\nBring along tools that will help you explore, discover and appreciate the wonders found in your habitats. Tip: Include a magnifying glass, binoculars, note pad & pencil, field guide and camera.\nDavid Alexander is author of the Buzz Into Action & Hop Into Action Science Curricula. He specializes in making nature accessible to people and wildlife. You can follow him at www.natureintoaction.com', 'The Grow Smart, Grow Safe® consumer guide to lawn and garden products is a project of Thurston County Environmental Health in Olympia, Washington. The information below comes directly from the Grow Smart, Grow Safe website.\nHow can we reduce the danger of pesticides?\nIncreasing the demand for safer products will encourage manufacturers to make products that are safer and more environmentally sound. You can have an attractive landscape without using hazardous pesticides. Use effective non-chemical methods and less-hazardous products instead.\nLearn about pesticide ratings, how to choose safer products, and ways to care for your lawn and garden naturally at growsmartgrowsafe.org.\nPesticides, human health and the environment\nPesticides include some of the most hazardous chemicals commonly used in and around the home. Products that kill insects, weeds, bacteria and fungi may be hazardous to children, pets, birds, fish, other wildlife, and also to beneficial insects like bees and lady bugs. Pesticides used in the yard or at the workplace can be carried inside on shoes and work clothes and mix with house dust. Young children, who crawl on the ground and put objects in their mouths, can then ingest the chemicals. Rain and irrigation wash pesticides off of yards and carry them to streams, sometimes in amounts that can harm salmon or the aquatic organisms that are their food.\nPesticides and children\nIn 2009, United States poison centers had over 90,000 incidents related to acute exposures to pesticides - about 45% involved children under the age of six. Children are vulnerable to repeated, very small, unintended exposure to pesticides. Per pound of body weight, pesticides have a greater effect on children’s growing bodies and developing nervous and hormonal systems and organs.\nDanger to pets\nMany pesticide products are toxic to dogs, cats and other pets. The risks can be similar to the human health risks. Pets with access to treated landscapes may pick up pesticide residues on their paws and fur, licking it or tracking it into the house. Slug bait containing metaldehyde poses a special risk because dogs are attracted to it and may eat enough to be seriously injured or even die.\nToxicity to birds and bees\nMost insecticides are toxic to beneficial insects, bees and other pollinators, and some insecticides are toxic to birds. Insecticides can kill bees directly when they eat or even just land on treated plants. Foraging bees can carry pesticides back to their hives, threatening the entire colony. Some researchers believe that pesticide use may be implicated in honey bee colony collapse disorder.\nMany insecticides and herbicides are toxic to fish, amphibians and other aquatic organisms. Aquatic organisms come into contact with pesticides and fertilizers through irrigation or stormwater runoff from yards and other landscapes. Pesticide ingredients that don’t bind well to soil and are persistent in the environment pose the greatest risk to water pollution. Monitoring studies in the Puget Sound region have found common pesticides and fertilizer nutrients in many of our waterways.\nThe longer a pesticide remains in the environment, the more likely it is to do damage. Older products, like DDT, are still in the environment and in our bodies almost 40 years after their last uses were banned. Many modern pesticides breakdown faster, however most pesticides are not gone after a month or two, and may not be harmless just because the treated area has dried. Some pesticide residues may remain in soil, sediment or water from weeks to over a year or more, and sometimes they breakdown to chemicals that are toxic and persistent.\nHow do I know which products are safer?\nLearn how to read a pesticide label, which chemicals are lowest risk, how to handle and dispose of pesticides, and how to avoid the need for pesticides altogether at www.growsmartgrowsafe.org.']	['<urn:uuid:da94635a-b68d-4b7c-993f-4dea1a6c9422>', '<urn:uuid:ecd62e19-7b14-4024-93f6-ba091430d7c5>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:09:04.591601	24	154	1415
44	What's the best way to protect my winter clothes from getting damaged while they're stored away during the warm months?	To protect stored clothing, they should first be dry cleaned and laundered at high temperatures to eliminate all moth life stages. Items should then be stored in airtight containers with tight seals. You can also expose garments to temperatures below 0 degrees F for at least 72 hours, or to high temperatures between 110-120 degrees F for at least 30 minutes to kill moths. Additional preventive measures include maintaining low humidity levels, regular monitoring of fabrics for signs of damage, and thorough cleaning of closets before seasonal storage.	"[""- The geographical origin of the webbing clothes moth is not yet known; some experts believe it came from South Africa, while others argue it has European origins.The clothes moth is now worldwide in distribution.\n- Contrary to common belief, it is not the adult moths that eat holes in fabrics and carpets - it is their larvae that feed on organic fibers in order to obtain the proteins they need for growth.\n- Clothes moths are rarely seen during the day. They prefer dark, undisturbed areas such as basements and closets, where they live in dark corners or the folds of fabrics.\n- Clothes moths will generally avoid synthetic and cotton fabrics, as they don’t contain keratin, and focus on clothes made of wool, angora, cashmere, and other animal fibers.\n- Moths don’t need liquid water; they obtain their needed moisture by feeding on garments that contain traces of human sweat, urine, and other liquids that have been spilled on them.\nThe adult stage of the webbing clothes moth has a uniform pale golden color and measures approximately ¼ inch with the wings folded back on the body and ½ inch with the wings spread. The eyes are black, and the top of the head is covered in a reddish golden tuft of hairs. The larvae’s head capsule is brown to black, and the body creamy white; eyes are absent in this stage. As it feeds on the surface of a fabric, the larva spins long threads and builds tunnels made of silk, excrements, and scraps of whatever fabric is available for feeding during this stage. Larvae are commonly found in the hidden parts of clothing, such as under cuffs and collars.\nDue to the similarity in size and appearance, webbing clothes moths are often confused with casemaking and grain-infesting moths. It is, however, possible for homeowners to differentiate between them by looking closely at their size at rest (clothes moths are approximately 1/2 inch while food-infesting moths are 1/3-1/2 inch in length) and color (the casemaking clothes moths are brown with colored spots and have lighter colored hairs on their heads than webbing clothes moths).Unlike the webbing clothes moth larvae, the larvae of casemaking clothes moths aren't exposed - they live inside a tube made of silk and wool, where they feed until they reach maturity.\nDiet, Behavior, and Habits\n(Image source: Flickr Commons\nTypically, if conditions such as room temperature, humidity, and the quality of food are favorable, the clothes moth goes from egg to adult in approximately 45 days. The eggs are glued on fabrics so that they will not be easily dislodged. Within 1-2 days of pupation, moths are ready to mate and lay an average of 40-50 pinhead-sized eggs, usually on the surface of or near fabrics and furnishings. Adult moths do not possess a mouthpart and cannot feed, and so they die within one month.\nThe webbing clothes moths' primary diet contains woolens, fur, leather, hair, mohair, and even dead insects and dried animal carcasses. Their typical dwelling places include clothing, carpets, blankets, stored wool garments, upholstery, piano felts, fish meal, mounted animals, and other dark and isolated places where they can feed and mate. Synthetic and cotton fabrics are usually avoided, unless they are mixed with organic fibers, in which case the larvae may feed on them and use the remaining ones to build their tunnels. Fabrics that are not of animal origin cannot be digested.\nThe home provides the ideal environment for clothes moths to grow and reproduce. They especially thrive on dirty and stained fabrics, from which they can obtain the vitamins, salts, and essential nutrients they need for survival. Clean wool, which lacks vitamin B and salts required by the larvae, will delay and even stop their development into adults. Clothes damaged by the webbing clothes moth display furrows on the surface of the fabric, a direct result of the grazing activity of the larvae. The presence of long, differently-shaped holes is usually the sign of a heavy infestation.\nSigns of Infestation\n(Image source: Flickr Commons\nWhere to look for moth larvae and damage ?\n- Cracks and crevices near or on food sources\n- Wool sweaters, fur, mohair, cashmere, and other organic fabrics; carpets and rugs; upholstered furniture; fabrics with food, beverage, and perspiration stains; synthetic and cotton blends\n- Bristle brushes made from animal fibers\nWhat are the signs of an infestation ?\n- Damaged fabrics\n- The presence of silken webs and scattered patches of silk across the surface of the attacked fabric\n- The presence of webbing tubes and fecal pellets, which are often of the same color as the attacked material\n- Threadbare sports in rugs and carpets\nManagement and Control\n. A moth infestation typically begins when infested garments are brought inside and stored with other clothes made from natural fibers. Therefore, in order to eliminate current infestations and prevent future ones, all clothes should be inspected prior to storage. Woolen fabrics should be dry cleaned and laundered at high temperatures in order to eliminate all moth life stages. Hanging clothes in direct sunlight and brushing them thoroughly will expose larvae and cause them to drop from the fabrics in order to seek protection. Using airtight containers to store items for longer periods is also useful in preventing and controlling infestations.\n.Exposing infested garments to lethal temperatures (lower than 0 degrees F) will almost always kill pests after a minimum of 72-hour exposure. A sudden change of temperature is most effective in repelling moths because it prevents the insects from acclimating to the room and developing resistance to freezing. High temperatures between 110 degrees F and 120 degrees F maintained for at least 30 minutes will also eliminate moths from woolen materials.\n.There are several chemical options available for the management and control of moth larvae, including:\n- Moth crystals containing naphthalene or paradiclorobenzene work to repel moths when placed in airtight containers with the infected clothes. However, the substances are toxic and may also leave an unpleasant odor on clothes and inside closets.\n- Pheromone traps work by luring adult male moths inside a sticky trap, where they get stuck and eventually die. They are relatively easy to use and quite effective at reducing both webbing and casemaking clothes moth infestations.\n- Insecticide sprays should typically be used as a last resort on infested garments that cannot be dry cleaned, laundered, frozen, or fumigated. Insecticides are commonly based on a substance called pyrethrin, which provides a fast knockdown effect. Heavy infestations requiring the use of large quantities of insecticide should be handled by specialized pest control companies.\nPreventing a clothes moth infestation is always easier than eliminating an existing one. In order to minimize the risk of infestation, homeowners must ensure:\n- Periodic dry cleaning and laundering of clothes at high temperatures\n- Proper storage of woolen and fur garments in special pest-proof containers with tight seals\n- Maintaining low humidity levels to delay moth development\n- Good housekeeping practices\n- Regularly monitoring of fabrics and closets for moth presence and signs of damage\n- Regularly vacuuming and inspection of tapestries and woolen carpets\n- Cleaning closets and dressers prior to storing clothes for the season\n- Careful inspection of the area underneath baseboards in closets""]"	['<urn:uuid:b4455e3b-2058-4b94-b2d8-b163919c3967>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	20	88	1214
45	How do medical discoveries pose risks when implemented too early, and what role does individualized medicine play in understanding microbial health?	Medical discoveries pose risks when implemented before thorough testing and understanding, as demonstrated by radiation's harmful early uses in medical treatments and consumer products. We risk repeating this mistake with the microbiome, where we don't yet know what an optimal microbiome looks like or understand the long-term effects of manipulation. Regarding individualized medicine, it represents a paradigm shift that considers each person's unique genetics, microbiome, and environmental factors rather than applying the same treatment to all patients with similar diagnoses. This approach recognizes that different bacterial communities can trigger different immune responses in individuals.	"['New scientific discoveries are incredibly exciting, but often poorly understood and misused in dangerous ways.\nConsider the discovery of radioactivity. It profoundly changed everything from medical imaging to nuclear power to atomic warfare. But before it was completely understood (no one knew it could lead to genetic mutations that result in cancer) it was misused in a variety of ways that range from humorous to horrifying.\nAs the website Mental Floss explained in a its post 11 Ways We Used Radiation in Everyday Life:\n…Once upon a time, radiation in different forms was new and wondrous and had a million uses -medications, cosmetics, industrial applications, and even entertainment. It was only later that the danger became evident.\nThese uses ranged from radioactive toothpaste, advertised as providing a bright smile and freshening your breath by killing bacteria in the mouth, to radioactive cosmetics to make your complexion “glow” and prevent aging, to patent medicines, to radioactive suppositories designed to deliver healthful radiation directly to internal organs. There is no way to know how many cases of cancer and how many deaths might have been caused by the enthusiastic adoption of radiation as a “cure.”\nWe appear to have learned nothing from that debacle even though the message could not be clearer: Do not implement new medical discoveries until they are thoroughly tested and understood. Indeed, we are eagerly rushing to repeat our mistake, only this time the scientific discovery is the microbiome.\nThe microbiome will likely turn out to be an exceedingly important factor in human health and disease, just as radiation turned out to be exceedingly important in health and disease (think radiology and treating cancer with radiation). But we are only on the cusp of understanding the microbiome, and therefore, capable of doing terrible harm by turning our speculations into action. That’s the theoretical reason why you shouldn’t believe what you read about the microbiome in the mainstream media, but there are practical reasons as well.\nConsider that the micriobiome has its own microbiome, the virome, and it is the interaction between the bacteria and the viruses that prey on them and on human beings, that determines health and disease. There is a small, but growing body of evidence that the human body can harness or at least influence the composition of the virome in order to regulated the microbiome. A paper published last year in PNAS, Bacteriophage adhering to mucus provide a non–host-derived immunity, offers a tantalizing glimpse of the possibilities:\nMucosal surfaces are a main entry point for pathogens and the principal sites of defense against infection. Both bacteria and phage are associated with this mucus. Here we show that phage-to-bacteria ratios were increased, relative to the adjacent environment, on all mucosal surfaces sampled, ranging from cnidarians to humans. In vitro studies of tissue culture cells with and without surface mucus demonstrated that this increase in phage abundance is mucus dependent and protects the underlying epithelium from bacterial infection… Based on these observations, we present the bacteriophage adherence to mucus model that provides a ubiquitous, but non–host-derived, immunity applicable to mucosal surfaces. The model suggests that metazoan mucosal surfaces and phage coevolve to maintain phage adherence. This benefits the metazoan host by limiting mucosal bacteria, and benefits the phage through more frequent interactions with bacterial hosts. The relationships shown here suggest a symbiotic relationship between phage and metazoan hosts that provides a previously unrecognized antimicrobial defense that actively protects mucosal surfaces.\nIn other words, certain viruses help the body to fight off bacterial infections by killing the bacteria. Even more remarkable, it appears that the animal will change the composition of its mucous to recruit more of the helpful viruses and thereby make it easier to ward off infection by harmful bacteria. Moreover, research suggests that the virome differs far more from person to person than microbiome, suggesting that it may play a greater role in health and disease than the microbiome.\nThe bottom line is that the microbiome is extremely complex and interacts with the body and with both helpful and pathogenic bacteria in ways that we do not yet comprehend. That’s why any contemporary claims about the microbiome, including claims about possible differences in the microbiome of babies born by C-section vs. babies born by vaginal delivery are the intellectual equivalent of radioactive suppositories. We are dealing with something powerful, but we don’t know enough about it yet to make ANY recommendations since we have no idea of what the optimal microbiome looks like, of how the virome and the microbiome interact, of how the body uses the virome to manage the microbiome, whether individual differencess in the microbiome are clinically meaningful, and the long term effects of attempting to manipulate the microbiome.\nThe microbiome may well turn out to be as important as radioactivity. Only time, and much more study, will tell. Until then, we should be very careful about making any claims and promising any health benefits. As the example of radioactivity demonstrates, it is all too easy to do more harm then good by manipulating a system that is, as yet, poorly understood.', 'There are many factors that influence the health and diversity of our microbiome. Many medications alter our flora, especially antibiotics, but many others do too such as oral contraception. What we eat, what personal care products we use, and what we use to clean our homes are also major factors. Antibiotics can have a devastating effect on the microbiome, something which is only now being appreciated, and which may explain many of the side effects and unintended outcomes of widespread antibiotic use. A recent article about research being done on the role of the microbiome in autism and other health conditions called Autism\'s bacteria link gains credence provides a basic introduction to these factors:\n""But changes in the past century have altered the microbial balance as societies relied too heavily on antibiotics, disinfectants, C-section deliveries and a diet of refined carbohydrates. Those changes replace inner gardens of helpful bacteria with a harmful mix that makes compounds that in excess can damage the digestive system, brain, immune system, and the way cells metabolize energy. The damage is likely worse in babies whose brains and immune systems are developing, MacFabe says. But the damage also occurs throughout life.""\nThe Origins of the Microbiome\nA fetus growing in it\'s mother\'s uterus is sterile until birth, when it is colonized by bacteria. In a vaginal birth, the baby is colonized by the mother\'s vaginal and fecal flora. Babies born by c-section are instead colonized by the bacteria present in the room during birth. Touching and kissing the baby is another source of flora. To learn more about this read The infant gut microbiome: New studies on its origins and how it\'s knocked out of balance. Once the baby is colonized at birth, the gut flora is further influenced by whether the baby is breastfed or formula fed and when solid food is started and what foods are eaten.\nMaternal prenatal stress is associated with the infant intestinal microbiota.\nPsychoneuroendocrinology. 2015 Mar;53:233-45\n""Results showed that maternal prenatal stress, i.e., either reported stress or elevated basal maternal salivary cortisol concentrations or both, was strongly and persistently associated with the infants\' microbiota composition as determined by a phylogenetic microarray. Infants of mothers with high cumulative stress (i.e., high reported stress and high cortisol concentrations) during pregnancy had significantly higher relative abundances of Proteobacterial groups known to contain pathogens (related to Escherichia, Serratia, and Enterobacter), and lower relative abundances of lactic acid bacteria (i.e., Lactobacillus, Lactoccus, Aerococcus) and Bifidobacteria, altogether characteristics of a potentially increased level of inflammation. Furthermore, this aberrant colonization pattern was related to more maternally reported infant gastrointestinal symptoms and allergic reactions. In conclusion, clear links were found between maternal prenatal stress and the infant intestinal microbiota and health. Although causality cannot be concluded, the results suggest a possible mechanism by which maternal prenatal stress influences the offspring development. These results suggest a potential for bacterial interventions to enhance offspring health and development in pregnant women with stress.""\nHow Diet Affects the Microbiome\nStudy links common food additives to Crohn\'s disease, colitis\n""A key feature of inflammatory bowel diseases and metabolic syndrome is a change in the gut microbiota - the roughly 100 trillion bacteria that inhabit the intestinal tract - in ways that promote inflammation. In mice given emulsifiers, the bacteria were more apt to digest and infiltrate the dense mucus layer that lines and protects the intestines.""\nThe Microbiome in Health and Disease\nIn this study Foxp3 T Cells Regulate Immunoglobulin A Selection and Facilitate Diversification of Bacterial Species Responsible for Immune Homeostasis (you can read about what this study means here) it is found that:\n""They discovered that the immune system ""sees"" and responds differently to different bacterial communities. Rich and balanced bacterial communities seem to be perceived as ""self"" and induce a quick maturation of the immune system and gut responses (induction of regulatory T cells and IgA), while a poor and unbalanced bacterial community is apparently perceived as ""non-self"" and induces responses aimed at eliminating it (T cells with inflammatory properties and IgG or IgE responses).""\nUnderstanding an individual\'s microbiome is part of the emerging paradigm shift going on in medicine right now to individualized medicine, which is an approach to understanding and treating disease based on an individual\'s genetics, microbiome, environmental factors, and any other factors that influence the health of an individual. In this new paradigm, rather than classifying patients together by diagnosis and then treating each of them with the same methods it is a way to individualize treatment to the individual\'s specific needs. This brief interview from a medical conference held in 2015 on the cutting edge research in digestive disease gives a little more detail on how this paradigm shift relates to the gut and it\'s microbiome.\nRestoring Microbial Balance Key to Keeping Sinuses Healthy\nThe dormant blood microbiome in chronic, inflammatory diseases\nIn addition, a compromised immune system can alter the microbiome of the gut, leading to intestinal disease such as Inflammatory Bowel Disease.\nStudy Finds Gut Bacteria Help Prevent Post Traumatic Stress Disorder (PTSD)']"	['<urn:uuid:de51001d-905f-417b-a156-3831bc4ff919>', '<urn:uuid:652337ed-b7f4-4e03-b12c-9d64e053e6f3>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	21	94	1692
46	What happens if police find evidence without following proper rules?	If police collect evidence without following proper rules, that evidence can be suppressed and cannot be used in court under the exclusionary rule established by the Supreme Court in 1961. If the suppressed evidence is essential to prove the criminal charges and there is no other evidence available, this could result in the charges being dismissed or the person being found not guilty.	['If you are charged with committing a crime, you need the assistance of an experienced criminal defense attorney to help you fight the charges. An attorney should have strategies based on experience handling similar criminal cases—which may help in getting your charges dismissed or reduced to a crime with a less severe punishment. One important way that an attorney may be able to weaken or destroy the prosecution’s case against you is to file a motion to suppress evidence against you.\nHow Can a Motion to Suppress Evidence Help in Your Criminal Case?\nIt is critical to keep in mind that you are innocent until proven guilty in a criminal case—even if you know that you are guilty of committing the crime. The prosecution has the burden of proving your guilt beyond a reasonable doubt. This is a very high standard that the prosecutor cannot always meet, including cases where the accused is obviously guilty.\nA motion to suppress evidence is filed to prevent certain pieces of evidence from being used against you in your criminal case. It can help your case in the following ways:\n- If the suppressed evidence is essential to prove the criminal charges against you, its suppression could result in the criminal charges being dismissed.\n- If the prosecutor has other sufficient evidence against you, the charges against you will not be dismissed. However, the suppressed evidence can severely weaken the prosecutor’s case against you and create reasonable doubt—sufficient for you being found not guilty. It could also result in the prosecutor offering you a more favorable plea agreement due to the lack of evidence against you.\nCommon Grounds to File a Motion to Suppress Evidence\nIn order to successfully file a motion to suppress evidence, you need to have legal grounds to argue that the evidence should be suppressed. These grounds are generally based on police misconduct and violation of your constitutional rights. An experienced criminal defense attorney will be able to identify grounds to file a motion to suppress evidence in your case. Common reasons to file this motion include:\n- Unlawful searches and seizures. Under the Fourth Amendment, you are protected against illegal searches of your home, your property, and yourself. In general, police must have a valid search or arrest warrant or probable cause to believe that a crime was committed to stop, search, or arrest you. If the police did not have a proper warrant or probable cause, the evidence collected that supports the charges against you could be thrown out of court.\n- Failure to give Miranda warnings. Once the police take you into custody, they must inform you of your Miranda rights under the constitution before questioning you further. This includes being advised of the right to remain silent, that any statements can be used against you, and your right to an attorney. If they fail to give you these warnings and you make an incriminating statement or confess to the crime, these statements can be suppressed. In addition, the police could violate your rights by trying to continue to question you after you invoke your Miranda rights.\n- Coerced statements. If the police coerce you into confessing or making a statement, this may be grounds for suppression of your statement.\n- Chain of custody errors. The police are required to follow strict procedures for the handling of and storage of evidence against you. This is to ensure that it is not tampered with or mixed up with evidence in another criminal case. When the police mishandle the evidence or otherwise violate the rules, the evidence can be suppressed.\n- Witness identification. If you were identified in a police lineup, the police may have violated your rights by improperly suggesting that the witness identify you, failing to allow your attorney to attend the lineup, or failing to include other potential suspects in the lineup with a similar appearance. This can be grounds to suppress the witness’ identification—which could be crucial to proving you committed the crime.\n- DUI testing results. There can be many challenges to the tests performed by the police as part of a DUI arrest. This can include violation of the many rules regarding field sobriety tests, improper administration of breathalyzer tests, or calibration problems with testing machines. In addition, many successful challenges to evidence are based on the lack of probable cause to stop the accused in the first place.\nTalk to a Charlotte Criminal Defense Atttorney\nIf you are facing criminal charges, our experienced criminal defense attorneys are here to guide you through the criminal process. To learn more about how we can assist you, call our office today to schedule your free consultation.', 'POLICE LIMITATION ON THE POWER OF SEARCH AND ARREST 1\nPolice Limitation onthe Power of Search, Arrests, and Seizure\nWhile the United States’ Constitution enshrines the individuals’right to privacy and certain freedoms, the rights are not absolute.When justifiable, the federal and state law enforcement personnel areallowed to search one’s property, car, and premises for the purposeof finding and seizing any illegal, stolen goods as well as asubstance that constitutes evidence of the occurrence of a crime.However, while the law grants police officers the right to infringeon a person’s right to privacy and certain freedoms, it has a clearprovision on how this ought to be done. The Fourth Amendment securesindividuals in their persons, properties, and premises againstunreasonable seizures and searches. The Fourth Amendment provides thetwo conditions only under which the right to privacy may beinfringed. They include in the presence of a probable cause and insupport of affirmation or an oath that undertakes to describe theplace and the property to be searched and items to be seized. Thereare two types of searches or arrests with and without a warrant.\nOnly under two circumstances can an arrest, search and seizure berecognized as legal. The first case is when the police have probablecause. The presence of probable cause is a requisite that empowers apolice officer to make an arrest without a search warrant. Under theprobable cause rule, police officers must prove that it is morelikely than not that the individual or property to be searched isinvolved in one way or another in the commission of a crime. Onlyupon the fulfillment of this basic tenet can a search be termed asreasonable. According to the Legal Information Institute (2014), aprobable cause entails the police having a reasonable basis thatmakes them believe that either a crime has been or is about to becommitted. The Fourth Amendment enshrines the probable cause andrequires that police have reasonable grounds for conducting an arrestor search without a search warrant. According to the Legal StudiesInstitute (2014), the Supreme Court in the Illinois v. Gates made anattempt to clarify the definition of the term probable cause. Itdefined probable cause as the practical and factual consideration ofeveryday situations under which any prudent or reasonable man acts.It means that police officers can only utilize the probable causeexemption to the right to privacy as provided for in the FourthAmendment when an individual fails to act in the way any reasonableor prudent person ought to (United States Government PublishingOffice, 1992). The deviation from the normal by an individual givespolice officers the basis on which to arrest without a warrant. Under the Fourth Amendment to the Constitution, a search and anarrest warrant must be preceded by probable cause. The existence ornonexistence of probable cause is left entirely to the policeofficers` use of their discretionary powers. It is up to the policeofficers to interpret the current situation and make an informeddecision on whether it mandates them to make an arrest without anarrest warrant.\nPolice are limited when making arrests, searches, and seizures in theabsence of probable cause, as a warrantless arrest is invalid.Besides, the absence of a probable cause invalidating a search orarrest, it also results in the suppression of all evidence seizedfrom the suspect or their property. Under the exclusionary ruleestablished by the U.S Supreme Court in 1961 if a Court whenreviewing the circumstance under which the search was made comes to aconclusion that there was an unreasonable search involved, the seizedevidence under the same condition are rendered inadmissible in acourt of law (Legal information Institute, 2014). It means that ifthere is no other evidence available to sustain the case broughtagainst an individual whose person or property was searched, underthe exclusionary rule the person ought to be acquitted of all thecharges. According to Katz (1997), the exclusionary rule tends toelevate the protection of the individual`s privacy to a supreme levelwhile relegating the truth-determining role of a criminal trial tosecondary importance. The rationale behind the exclusionary rule isthat police officers are more likely to conduct reasonable searchesif they know that an improper search will result in an acquittal evenwhen the suspect is guilty. The doctrine of the probable causeapplies in the search warrant only when there is a high chance that asearch without a warrant will culminate in the discovery of pertinentevidence. However, even in a circumstance that may result in thepolice officers obtaining crucial evidence, they are required toconsider the option of getting a search warrant. It means that thereliance of a probable cause without a search warrant should only bea last resort when the latter cannot be sought due to timeconstraints.\nPolice officers are limited in the exercise of their power to arrest,search and seizure as they are required to obtain a search or arrestwarrant prior to them executing the above mandates. If there are noissues of the time constraints involved, police officers are requiredto present their suspicion of the existence of evidence or a crime ina certain property or premises to a judicial officer before they aregranted the right to search the same. According to Smith (2009), inthe Arizona v. Gant, the U.S Supreme Court held that police officersought to get a warrant to grant them the right to search a suspect’svehicle after detaining or arresting the suspect when a probablecause is missing. When seeking a search warrant, police officers areexpected to present an affidavit that supports their contention thatthey have sufficient and credible information that establish theexistence of a probable cause. However, unlike when civilians areinvolved, affidavit involving police are not elaborate as there is apresumption that law enforcement officers can be trusted to provideimpeccable information. Hence, the affidavit to support their requestfor a search warrant does not require the presence of an attorney asthe judicial officer can rely on their word of mouth. Additionally,police officer’s power to search is limited as they can only searchand seize items indicated on the search warrant. As previouslyindicated probable cause should precede a search warrant. Accordingto the United States Government Publishing Press (1992), the doctrineof particularity requires that a search warrant particularly describethe items to be seized. Thus, the rationale behind the policesearching and seizing only the things indicated in the search warrantis that they were unaware of the existence or involvement of otheritems in a crime before coming into contacts with them. It means thatpolice officers cannot go about collecting any items they find in theproperty in which they are executing a search warrant.\nIn conclusion, while police officers enjoy massive power when makingan arrest, search, and seizures, these powers are limited by theConstitution. By limiting, the Constitution states the scope or theextent in which law enforcement officers may execute a search,arrest, and seizure. For instance, the Fourth Amendment to the U.SConstitutions enshrines the right to privacy that forbids any searchon an individual’s person, property and premises unless in theexistence of a probable cause and oath or affirmation. It means thatthe police can only make an arrest or search when they have aprobable cause which is the belief that crime is more likely that notto have occurred and that the person or property being searched wasinvolved in one way or another. While the existence of a searchwarrant is the other acceptable way the police can execute a search,arrest, and seizure, it must be preceded by the probable cause. Therepercussion of infringement of the rules stipulated by theconstitution on what constitutes reasonable search results in thesuppression of the evidence collected under what is termed as theexclusionary rule.\nLegal Information Institute. (2014). “Probable Cause.” Accessedon May 4, 2016. https://www.law.cornell.edu/wex/probable_cause\nKatz, R. L. (1997). Reflection on Search and Seizure and IllegallySeized Evidence in Canada and the United States. Canada-United StatesLaw Journal, 3(103).\nSmith, P. (2009). Search and Seizure: Supreme Court Limits Police CarSearch Powers.” Accessed on May 4, 2016.http://stopthedrugwar.org/chronicle/2009/apr/24/search_and_seizure_supreme_court\nUnited States Government Publishing Office. (1992). Fourth Amendment:Search and Seizure. Accessed on May 4, 2016.https://www.gpo.gov/fdsys/pkg/GPO-CONAN-1992/pdf/GPO-CONAN-1992-10-5.pdf']	['<urn:uuid:80e6e23d-1b14-4a03-a9bb-e15fbdc836b6>', '<urn:uuid:c1985789-280a-4c91-981e-b5f1cb9a62f6>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:09:04.591601	10	63	2076
47	Which needs more permits - home business or regular store?	A home-based business requires a unique set of permits compared to commercial properties. Home businesses need specific permits like Home Occupation Permits (to ensure the neighborhood is properly zoned), while avoiding some commercial property permits. However, both types still need basic permits like Business Operation Licenses, Seller's Permits, and Sales Tax Licenses to operate legally.	"[""If you're wondering whether your online business needs a business license or other permits in order to operate legally, the answer is yes. Obtaining the proper ecommerce business licenses and permits is a crucial step in starting your online business.\nThe good news for ecommerce companies is that you don't need many business licenses, and the ones you need are fairly easy to acquire. Below are some common ecommerce business licenses that most online businesses need to stay compliant with regulations and tax laws. (Keep in mind that licensing requirements can vary by state, so check with your state's licensing boards to make sure you're getting the appropriate licenses.)\nChecklist of licenses and permits necessary for ecommerce businesses.\nBusiness Operation License\nA Business Operation License allows your ecommerce business to conduct business in your city, county, or state. Virtually every online business will need a version of this license to operate legally. The regulations surrounding these licenses and how to register for them vary by location, so contact your local City Hall or check your state's Secretary of State website to find and apply for the correct license. The requirements for obtaining these licenses are usually fairly easy to follow, and you'll probably have to pay a small fee. Make sure to check whether your Business Operation License needs to be renewed—operating a business with an expired license can result in fees or fines.\nAlmost every ecommerce business will need a Business Operation License to operate in their city, county, or state.\nEmployer Identification Number (EIN)\nAn Employer Identification Number (EIN) is a federal tax ID number issued by the IRS that identifies your business as a distinct tax entity. While not technically a license, an EIN ensures that your business collects payroll tax and stays in compliance, so it's a VERY good idea to get one so you can avoid business tax-related hassles down the road. If you plan on hiring employees, are incorporated as an LLC or corporation, or plan to open a business bank account, an EIN number is a necessity. Visit the IRS website to file an EIN.\nWhile not always necessary, setting up an EIN for your company is recommended to avoid tax problems later on.\nDoing Business As (DBA) License\nA Doing Business As (DBA) License gives you the opportunity to operate your online business under a name other than your own—for example, you may wish to operate your business as “Austin Homemade Candle Company” instead of “Jane Smith.” A DBA is not mandatory, but since it fulfills some requirements for opening a business bank account and enforcing any necessary contracts, it’s definitely something to consider getting. If you would like to register for a DBA, you must file with your county or state.\nDBAs allow you to run your company under a specific business name instead of your own name.\nA Seller's Permit (or Seller's License) is required for most businesses that sell goods or services online. Nearly all states require a Seller's Permit to legally sell products online, but the requirements, fees, and other details vary by state. Check with your state’s Licensing Office or Department of Revenue to make sure you get the appropriate permits. This business license is still required even if you're selling digital or other intangible products or services, so make sure to apply for the correct sellers' permits from your state.\nIf you're selling goods or services online, you'll need a Seller's Permit from your state.\nSales Tax License\nHome-based businesses selling taxable products and services must pay sales tax on the items sold. This will require getting a Sales Tax License. It’s important to note that sales tax differs from state to state, so you’ll need to check with the state and localities you do business in to make sure you’re following their rules. Outside of a Sales Tax License, make sure to research whether there are any other tax permits required by the state you do business in. One thing you should absolutely avoid doing is collecting sales tax when you're not licensed, as this can result in hefty fees.\nYour business will need a Sales Tax License from one (or more) states to avoid tax penalties.\nHome Occupation Permit\nMany ecommerce business owners operate their companies from their homes. This allows them to avoid licenses and permits that come with commercial properties, but it doesn’t exempt them from all licenses. Filing for a Home Occupation Permit ensures that your neighborhood is zoned for home business activity—that is, your business is not adding additional or unnecessary traffic or noise to the community. Check your city’s government website for instructions on how to apply for a Home Occupation Permit in your area.\nIf you're operating your business out of your home, you'll need a Home Occupation Permit.\nNot every online business will need an Occupational License—whether your ecommerce business needs this license depends on the type of industry you’re in and the work you do. If you’re an accountant, for example, you will need an Occupational License to conduct business and establish credibility. Tools like the National Occupational Licensing Database can help you determine whether you need an Occupational License. If your business requires special training, education, or licensing to operate legally, you'll need to get the corresponding occupational permits. If you're just selling merchandise online, it's unlikely you'll need an Occupational License.\nIf you're doing business in a regulated industry, you'll likely need an Occupational License.\nThere are even more licenses and permits, like Environmental Permits and Health Permits, that may pertain to your ecommerce business. Always check with your state and local jurisdictions to make sure you have every license and permit required to operate your company before you open for business so you can avoid unnecessary fines or notices. Find official websites and contact information for your local government entities here.\nHave any questions about ecommerce business licenses and permits? Let us know in the comments!"", 'Steps to Apply for Business Licenses and Permits in Mississippi\n- Search for any necessary Mississippi general business licenses\n- Obtain applicable federal licenses for your Mississippi business\n- Check for Mississippi permits and licenses\n- Check your city or county for local licensing in Mississippi\n- Search for applicable Mississippi professional licenses\n- Obtain any other necessary Mississippi business licenses and permits\n- Apply for Mississippi home-based business licenses\n- Maintain your Mississippi licensing\nStarting your own business can mean you have to get through a lot of preparation and red tape. Once you officially form your business, you’re likely going to need some kind of additional state licenses, permits, or other certifications to start running it. Figuring these things out can take a lot of time and searching. If you don’t know where to begin with getting the proper licenses for your newly-formed business, read on. We’ll discuss what kinds of licenses and permits you might need in Mississippi, and how we may be able to help.\nLet’s take a look at what kinds of permits or licenses your business might need and how our Business License Report service can help you get it done easily.\nWhat is a business license?\nA business license is what gives most businesses the permission or authority to operate after they form. Your business might need a license at the federal, state, or local level, and many licenses are based on your industry and your specific business needs. Some states require a general business license to engage in commerce, while also requiring licenses for specific business activities.\nThere are a lot of licenses and permits your business might need, but frustratingly enough, there isn’t one central place to search for all the licenses that apply to you. To open your business, you need to research licensing requirements at the federal, state, and local levels to determine what your business has to have for its grand opening.\nStep 1: Search for any necessary Mississippi general business licenses\nSome states require almost any business to have a general business license. In those states, businesses can’t take part in any kind of commercial activity without a general business license. The State of Mississippi doesn’t require general business licenses, but many Mississippi counties and municipalities require non-exempt businesses to have a general business license called a “privilege license” before they can operate in the area.\nAlthough some may call business registration “getting a business license,” registering a business and licensing a business are two different things. You bring your business into existence by registering it with the state, but you give your business the authority to run by getting the proper licenses and permits. Receiving business licenses and permits normally comes after you’ve registered your business.\nIf you’re not ready to get business licenses because you’re still trying to form a limited liability company (LLC) or corporation, we can help you. We provide both Mississippi LLC formation and Mississippi corporation formation services to help you get started.\nStep 2: Obtain applicable federal licenses for your Mississippi business\nAlthough a lot of business licensing happens at the state and local levels, there are a number of business activities that require licensing from the federal government. The U.S. Small Business Administration notes that the following business activities need licenses or permits from the following federal agencies:\n- U.S. Department of Agriculture: interstate importing or transporting of animals, biological products, animal products, or plants\n- Alcohol and Tobacco Tax and Trade Bureau (and the Local Alcohol Beverage Control Board): selling (wholesale or retail), manufacturing, or importing alcoholic beverages\n- Federal Aviation Administration: transporting people and/or goods by aircraft, maintaining aircrafts, or operating aircrafts\n- Bureau of Alcohol, Tobacco, Firearms and Explosives: importing, dealing in, or manufacturing ammunition, firearms, or explosives\n- U.S. Fish and Wildlife Service: businesses involving wildlife-related activities or importing/exporting wildlife-derived products\n- National Oceanic and Atmospheric Administration Fisheries Service: commercial fishing\n- Bureau of Safety and Environmental Enforcement: business activities involving drilling for oil, natural gas, or minerals on federal land\n- Federal Maritime Commission: businesses providing ocean transportation or helping with cargo shipment by sea might\n- U.S. Nuclear Regulatory Commission: businesses that are fuel cycling facilities or that deal with nuclear energy or nuclear materials\n- Federal Communications Commission: broadcasting information by cable, television, wire, satellite, or radio\n- U.S. Department of Transportation: businesses operating overweight or oversized vehicles\nConsider all the facets of your business to determine your need for federal licenses or permits. In some cases, you might need licensure from more than one government agency. The same business activity might also need licensing from federal, state, and local governing bodies at the same time.\nStep 3: Check for Mississippi permits and licenses\nLicensing requirements can change from state to state. You might need more than one Mississippi state business license before you can start making money. Typical state license requirements you can encounter when opening a Mississippi business include:\n- Liquor licenses\n- Food permits\n- Tobacco licenses\n- Environmental permits\n- Commercial fishing licenses\n- Construction licenses\n- Professional licenses\nThe above list isn’t exhaustive, and you might also need licenses at the federal and local levels to move forward with your business.\nStep 4: Check your city or county for local licensing in Mississippi\nYour county or municipal governments can have a lot of rules you have to follow before you can conduct business on their turf. Along with requiring a privilege license (i.e., a general business license), many counties and municipalities require businesses to have a plethora of other licenses and permits, including:\n- Alcohol and tobacco permits\n- Alarm permits\n- Occupational permits\n- Health permits\n- Trader’s licenses\n- Tax certificates\nNot all local license and permit requirements apply to all businesses, so you need to contact your county and municipality to determine your business licensing needs. Your county could have specific requirements, and so could your specific city or town.\nStep 5: Search for applicable Mississippi professional licenses\nSometimes using your specialized education and/or skills to provide services to your customers is a great way to make money. If your business offers professional services, you likely need a license from the state to perform those services. The state requires licenses for many professionals, including:\n- Childcare professionals\n- Hair braiders\n- Tattoo operators\n- Athletic trainers\n- Healthcare professionals\n- Professionals dealing with radioactive materials\n- Social workers\n- Massage therapists\n- Dealers of durable medical equipment\n- Funeral directors\n- Insurance professionals\nOnce again, this list isn’t exhaustive, and it’s important to check with the state about whether you need a professional license to provide certain business services. Different state boards and departments govern licensing for different professions.\nStep 6: Obtain any other necessary Mississippi business licenses and permits\nOther types of licenses you might need from the federal, state, or local government don’t necessarily have to do with your industry. These other licenses might have to do with the size, location, look, and revenue of your business. Other types of licenses and permits your business might need can include:\n- Zoning permits\n- Building permits\n- Signage permits\n- Sales tax permits or registration licenses\nMany zoning, building, and signage permits come from your county and municipal governments.\nIf your business sells tangible, personal property, you have to have a sales tax registration license or permit. Tax rates may vary, depending on what you sell. You apply for a sales tax license with the Mississippi Department of Revenue.\nSome Mississippi businesses also have to apply for employer identification numbers (EINs) with the IRS. The EIN is a federal tax identification number that many businesses need to properly pay their federal taxes. Businesses that need to apply for an EIN include:\n- Businesses with employees\n- Businesses run as partnerships or corporations\n- Businesses that file returns for employment, excise, or alcohol, tobacco and firearms taxes\n- Businesses that withhold taxes on non-wage income for non-resident immigrants\n- Businesses that have Keogh plans\n- Businesses involved with certain organizations, trusts, and entities\nBesides their necessity for tax purposes, EIN’s can also come in handy when applying for business bank accounts or certain business benefits.\nStep 7: Apply for Mississippi home-based business licenses\nRunning a business from your home can be convenient and cost-effective, but you might have to jump through some hoops before you can operate. Many home businesses need certain licenses and permits to legally run. Licenses required for a home-based business might include:\n- Building permits\n- Signage permits\n- Privilege licenses\n- Occupational permits\n- Sales tax licenses or permits\nMany local governments have specific laws and regulations regarding home-based businesses that you need to check before starting. You might also need to check with your homeowners’ association or residential covenants to see if your home-based business is a violation.\nStep 8: Maintain your Mississippi licensing\nRunning a successful business is often an exercise in constant monitoring and maintenance. In many cases, your business licenses will need to be renewed on a regular basis. If you need multiple business licenses and permits, you’re probably going to have multiple due dates for renewal. It’s crucial that you’re very clear about and aware of your different renewal deadlines, so you can timely fulfill them all and keep your business strong.\nRely on us to help with your Mississippi business’s compliance needs\nTrying to figure out all of your federal, local, and Mississippi business license and permit needs can be like going on a scavenger hunt. And how do you know you got all the clues right? Well, that’s where we come in. We offer Business License Reports that give you one simple report that outlines your licensing needs at all government levels. Our partners at Business Licenses, LLC, look at your industry, location, and activities to create a comprehensive list of your obligations. No longer do you have to feel overwhelmed trying to figure out if you’ve properly identified every business licensing obligation. We also offer a Worry-Free Compliance service to make sure you are up to date and keep your business legally compliant. Our services can do it for you so you have more time to focus on growing your business.\nDisclaimer: The content on this page is for information purposes only, and does not constitute legal, tax, or accounting advice. If you have specific questions about any of these topics, seek the counsel of a licensed professional.\n- Do all Mississippi businesses need a license?\nAlthough Mississippi business license requirements don’t include general business licenses at the state level, it’s unlikely that a Mississippi business won’t need some kind of licensing or permitting at the local, state, or federal level before it can operate.\n- Can you sell things without a license in Mississippi?\nIn most cases, no. Mississippi businesses that sell tangible, personal property must have a sales tax registration license or permit from the Department of Revenue. You also need permits to sell tobacco and alcoholic beverages.\n- Is it legal to run a business from home in Mississippi?\nIn many cases, yes. Your ability to legally run a business from your home in Mississippi depends on the nature of the business as well as relevant state, local, and federal laws. You might also need to review the rules from your homeowners’ association or contained in a residential covenant.\n- What kind of business license do I need for an online business in Mississippi?\nYour licensing needs for an online business in Mississippi depend on the nature of your business, your business needs, and your business location.\n- Do I need a license to collect sales tax in Mississippi\nYes. If you sell tangible personal property, you need a sales tax registration license or permit. You also need permits to sell tobacco and alcoholic beverages.']"	['<urn:uuid:2bb21eee-ebf8-4243-92ba-086e6b2ee9d4>', '<urn:uuid:6f9cad78-b05c-42bd-b587-19b0c66174c0>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:09:04.591601	10	55	2969
48	As a packaging manufacturer, I'm curious about the advantages of digital printing for folding boxes and its current growth in textile printing - what are the key benefits, and how fast is the textile digital printing market expanding?	For folding boxes, digital printing offers three main advantages: speedy production process, simple personalization possibilities, and low costs for short print runs. In the textile industry, digital printing is experiencing remarkable growth with a CAGR of up to 20%, although it currently only represents about 5% of all fabric printing globally (with around 31 billion sqm being printed in total).	['The required serial enumeration for drug packaging is no problem with digital printing. Photo: Edelmann\nFolding Boxes on Demand\nThere is a constant rise in demand for individually labelled and/or personalised packaging, short print runs and lead times. At the same time, state-of-the-art printing presses and folders open up new avenues to packaging producers for complete manufacturing processes from the initial design through packaging to shipping. Especially folding box producers benefit from digital printing, which at times is one step ahead of offset printing when it comes to fulfilling customer requests for personalisation and speed.\nAt present, over 90% of folding boxes are offset printed. in Germany. The printing quality is outstanding, the choice of materials diverse, and unit prices are dropping to a minimum as print volumes go up – this, however, is generally only the case from an absolute minimum quantity of 2,000 boxes with identical motifs. After all, the machines have to be changed over to the subsequent product each time, and therefore delivery times are, as a rule, correspondingly long due to the intricate manufacturing process.\nDigital printing offers sufficient flexibility for small runs. With a speed of 42 metres per minute the HP Indigo 20000 Digital Press prints on packaging, labels and shrink-wrap films. Photo: HP Indigo 20000 Digital Press HP\nAdvantages of Offset\nPerfect reproduction of special colours\nHigh precision, fine pixels\nGuaranteed quality for repeat jobs\nExtensive choice of substrates\nAdvantages of Digital\nSpeedy production proces\nSimple personalisation possibilities\nLow costs for short print runs\nEspecially for small and medium-sized print runs the HP T1100S delivers higher productivity and flexibility than analogue printing technology. Photo: KBA\nTrendsetting Digital Printing\nBe it pharmaceutical, cosmetic or food industries: digitally printed folding boxes generate added value for both the industry and end users, say experts.\nAdded value for the industry\nEven volume buyers of printed folding boxes are now looking beyond their own backyard and to the functional benefits of digital printing processes for their type of packaging. The drug sector, for example, can easily print tamper-proof features such as bar codes, consecutive numbers or batch numbers onto their tablet blisters even after production.\nDirect “just-in-time” carton printing permits the low-cost production of one-off pieces. This allows the advertising impact of each design on shoppers to be measured so that only the “winning box” ends up being mass-produced.\nAdded Value for End Users\nEnd users are given the low-cost opportunity to have customised product packaging produced – from one single unit to small and medium folding-box print runs. This is perfect for one-off give-aways for close friends or for promotional products featuring company logos presented for Christmas or company anniversaries.\nStill room for improvement in economic terms\nIn purely commercial terms, however, the increasing popularity of digital printing still offers room for improvement. Only optimised processes with a high degree of automation across the entire production process ultimately result in time and therefore cost savings – also thanks to online estimates and orders or direct digital networking. Insiders deem a coordination of IT with innovative folding box technology the smart packaging solution of the future.', 'Digitally printed fabric with disperse dye inks from Kothari Info Tech Ltd\nTansy Fall, Editor at World Textile Information Network (a great resource for news and analysis in the textile and nonwovens industries), shares her first exclusive piece with WhichPLM. She discusses digitalisation, paying particular attention to printing, fabrics and inks. Tansy studied at the University of Leeds, joining WTiN in 2014; she is responsible for the b2b Digital Textile magazine, as well as the newly launched IoTex magazine.\nIn many respects, the textile and fashion industries operate as two separate entities, with a wealth of apparel manufacturers bridging the gap, feeling cost pressures from below and margins being squeezed from above. This detachment stems from the globalisation trend of the ‘70s, which led to mass manufacturing in Asia, with vast amounts of infrastructure being established in China, and a dearth of skills in the West. It has enabled the “fast fashion” idiom to emerge, somewhat falsely, as fashion houses have gradually broken away from their fixed seasons to offer the consumer new styles more frequently. I say falsely, as the lead times for these “fast fashion” items are often extensive, designs are still created way in advance of their reaching the market, and garments are produced in long runs.\nHowever, “fast fashion” is set to take on a new and truer meaning with the advent of the fourth industrial revolution, which sees an embrace of digitalisation enabling, or forcing (depending on your perspective), the industry to streamline and connect end-to-end. From fibre to garment, the value chain is contracting, not in terms of market size or business numbers but rather in terms of business collaboration at disparate ends of the industry. For example, we have seen Lycra fibre manufacturer Invista create the LYCRA F!T FINDER app, providing 3D body scanning technology to hosiery retailers and their customers, and many fashion brands are focusing on sourcing more sustainably, looking at eco-friendly dyes and fibres.\nSoftware is playing a major role in the industry’s transformation, with ERP and PLM systems becoming more prominent – though not necessarily connected themselves – and 3D prototyping, the creation of “digital twins”, and data analytics encouraging new business models and linking processes. That said, without hardware that is able to connect to this software, the system somewhat falls apart.\nThe good news is that true automation of manufacturing processes is just around the corner, with strides being made throughout the textile supply chain, in hardware as well as software, for spinning, weaving and knitting, finishing, and garment pattern cutting and construction, among other areas. One sector that has been taking particularly large strides towards complete digitalisation is the printing sector.\nThough analogue printing still makes up around 95% of all fabric printed globally, with over 31billion sqm being printed in total, the digital textile printing market is growing at a phenomenal rate with a CAGR of up to 20%. More and more, we are seeing this technology being installed in the US and Europe, and the domestic Asian markets are also becoming increasingly demanding of Chinese print houses that have established huge export businesses. Turkey, the Middle East and Eastern European countries are also investing heavily as a near-shore alternative for brands in Europe.\nIndeed, this near-shoring trend, and the on-shoring trend that has emerged, is set to change the face of the textile and fashion industry – moving from a global manufacturing economy to a localised manufacturing economy – and this is largely being enabled by digital textile printing advancements.\nA digitally printed bridesmaid dress. Picture by Stephanie Veldman Photography\nThe ability for a design house to invest in digital printing technology, or work with a local partner, to bring their designs to life in a timely fashion has seen the establishment of vast numbers of small print bureaux. Many of these are also offering cut and sew capabilities, selling bespoke products into their local markets. This in itself is not a challenge to mass manufacturing – despite being a great opportunity for SMEs – but if we extrapolate this small business model and look at it from a retail brand’s perspective, the potential of the localised micro-factory to change the landscape of fashion and textiles is huge.\nLet’s take a fictitious brand: Brand X. Brand X currently mass manufacturers the components for it’s clothing in various locations across Asia, and then exports globally. The reason for this is that the cost of textile manufacturing in Asia is low compared to other regions, as is the cost of garment construction due to labour costs. However, more and more there are other factors to consider. Firstly, sustainability. This has been a buzzword in all industries over the last decade but it is increasingly being catalysed by global political initiatives and, crucially, sanctions. Pollution of water supplies and carbon emissions are under scrutiny, with China having shut down an entire manufacturing province for over a week ahead of the G20 summit last September, in order to reduce the amount of apparent pollution. Labour is also a prominent factor driving change. The 2013 Rana Plaza disaster in Bangladesh brought working conditions to the forefront and this combined with rising wages in the BRIC countries – China in particular – means that cheap labour is quickly becoming no longer an option.\nBased on this, Brand X decides to seek out a new business model. The management at Brand X has noticed a number of bespoke print bureaux with garment construction capabilities, marketing unique designs, starting up in many of its target markets. It also considers its e-commerce business that it would like to expand due to consumer shopping patterns. In order to do this, it would like to offer the customer a more personalised experience and also reduce its time to market. Tied in to this, Brand X is also looking to reduce warehousing costs and inventory, as its products are shipped globally. Brand X has also begun to analyse data collected from consumer feedback about different sizing needs in various regions, and different preferences in style across those regions as well. These are the trends we see emerging in the fashion industry, and all of this undermines the mass manufacturing mentality.\nWhat this may lead to is the dawn of the local micro-factory – though micro, in the end, may not be the most appropriate term for it; rather localised, automated manufacturing, customised to the local market. Digital textile printing is a key element in this chain, for taking greige to garment, with many print technology companies now offering inline solutions, and partnering with cutting businesses to bring this technology inline as well.\nIs the technology ready?\nDigital textile printing technology has evolved very quickly, from the printers to the inks, the substrates and their pretreatments, and the software.\nIf we begin by looking at printers, digital technology is now available in a variety of sizes and capabilities. Entry-level machines typically run at around 30sqm/hr and have been adopted by many fashion brands as a means of creating print samples in-house, particularly in fashion capitals like New York. These printers range from around US $15,000 to US $30,000, with popular machinery brands Epson and Mimaki taking prominence in this sector.\nMoving on to the mid-range printers, which print at around 100sqm/hr, as an average, these are the kind of production machines that would be found in a SME print bureau, which may have multiple printers, running different ink types. These printers usually retail for around US $50,000 to US $100,000, though second hand purchases are frequent. Brands such as MS Printing Solutions, D.Gen and DGI stand out in this area.\nHigh speed machinery is a huge area of growth in the industry. There are now a number of single pass digital textile printers, with fixed print heads, that are capable of rivalling the productivity of analogue printers, running at speeds of up to 4800linear metres/hr. Of course, the quality of the print also has to be taken into account here, and it’s likely that this number is somewhat unrealistic as a production speed. However, progress is constantly being made in this area, and there are now eight varieties of single pass printer on the market. Top manufacturers in this area include MS Printing Solutions, SPGPrints, Konica Minolta and Atexco.\nIn addition to these printers, the speed and resolution produced by high speed scanning machines (where the printheads aren’t fixed but run back and forth over the substrate) is also improving. EFI Reggiani’s ReNOIR TOP is claimed to print at 1200sqm/hr, for example, and other technology isn’t far behind. Both the high-speed scanning printers and the single pass printers are priced upwards of US $250,000.\nAnother factor that needs to be considered here is the time that is saved due to the lack of screens. Where shorter runs are being demanded by consumers looking for personalised products, this enables efficient switching between designs and should save on consumables as there are no screens to wash. Feedback from users also suggests that the resolution technology has improved to the point where companies are able to print complex designs at high resolutions without any difficulty.\nPicture by Printtech\nInks used in digital textile printing are based on either water-soluble dyes or finely milled pigments. The dyes come from the same classes as those used in dyeing solid colours or in traditional screenprinting, but their formulation in mixtures with other ingredients must be sufficiently precise to match the specific characteristics of the inkjet print head in which they are to be used. For that reason, digital textile ink manufacturers will usually offer several variants of the same product, each carefully formulated for the print heads of a specific supplier (Epson, Kyocera and Ricoh etc.), but there might be some machines, with different heads, in which their inks will not work reliably.\nDye sublimation inks make up the single largest digital textile ink category and are used only on polyester, applied under heat and pressure, turning the dye from a solid to a gas, becoming fixed when it cools and reverts to a solid. The most popular method is to print onto transfer paper and use a calender with heated rollers to transfer the dye to the fabric. The advantage of this method is that it does not require the fabric to be chemically pretreated and has no wash-off phase either, so is very efficient.\nDisperse dye inks are also used on polyester and are closely related to dye-sub inks, in that both consist of fine dispersions or colorant in suspension. However, no “sublimation” takes place in their application – the dye molecules simply migrate into the surface of the fibre under the special conditions created during a direct printing and heat fixation process. Fabric must be pretreated beforehand and, for clothing applications, washed afterwards.\nFor natural fibres, reactive dye inks are used and this is another important category for fashion. The dye molecules react with the fibre to create a permanent chemical bond. Reactive dyes give bright, saturated colours, but do not always have the highest light fastness. Pretreatment, steaming and washing are all required. Their brilliance and ability to achieve deep shades – especially intense blacks – means reactive dyes are also increasingly used on protein-based natural fibres such as wool and silk.\nAcid dye inks are traditionally used on wool and silk, but their role is being eroded by the capabilities of reactive dyes, whose process requirements are similar. However, acid-dye inks are also the main category used on nylon, and remain important in product areas such as swimwear.\nFinally, the ink that is currently the focus of development efforts is pigment. Unlike dyes, pigments do not penetrate the fibre but are held on its surface by a binder, and their fastness to washing and rubbing etc. depends on the effectiveness of the binder. In conventional printing, they are the most important type of ink used in home textiles, and are also the basis of most digital direct-to-garment printing. Pigments have the potential to offer the perfect “all round” solution, in that they can be used on any fabric type, including blends, with no finishing process other than drying and baking. They also have good light fastness. However, in industrial-scale digital fabric printing, they have been problematic due to a restricted colour gamut and the risk of clogging in the print heads. A lot of R&D has been devoted to these questions and, with new inks and new recirculating print head types, several suppliers are now offering high-production pigment printing solutions.\nThe real opportunity for digital textile printing is likely to lie in high speed scanning machines or single pass machines that can print with pigments. If this technology was optimised, it would be a serious challenge to the screenprinting industry, both in terms of speed and final quality of the print.\nPretreatment is another crucial part of this. Whilst pigment inks don’t necessitate a pretreatment, feedback from printers in the industry suggests that the results achieved when a pretreatment is used are far superior to those without. Pretreatment can be brought inline, which maintains the digital automation aspect of the printing process, but fabrics already pretreated are also available for purchase from a number of global suppliers, the most prominent of which is UK-based Premier Textiles.\nNew York based Dyenamix combines digital textile printing techniques with other innovative textile design processes to create unique effects for the catwalk\nWith all this technology available, what business models are emerging?\nWhen it comes to changing business models, there are many in the industry that are content with only analogue printing. Currently, this makes a lot of sense as analogue still makes up 95% of all printed fabric. And, the decision not to invest in digital is largely down to price – of the technology and the consumables – and the capabilities that are on offer, both in terms of final product quality and productivity, despite leaps being taken in this area. However, many of these businesses are eyeing up potential investments as the digital textile printing industry develops, and it’s likely that many will choose to invest in digital in the future.\nIn the meantime, there are a plethora of businesses that have invested only in digital printing. Though, as I have said previously, many of these businesses are SMEs, there are some larger companies emerging that have invested in single-pass technology and high-speed scanning machines, and these businesses are seeing a return on investment and interest globally. However, there are pitfalls in this model, as there are gaps in the capability of digital technology. Metallics, for example, cannot be printed using a digital method, and neon colours are also hard to achieve the same brilliance as with analogue printing. Pigments too, as mentioned before, are an area of contention.\nLarger businesses that are seeing the most growth are those invested in both analogue and digital technology. This enables flexibility when it comes to design, print-run lengths and cost. And this can all be decided on, or at least discussed with the individual customer, bringing the personalisation trend into the B2B space.\nAdvances in software are also enabling some unusual opportunities to arise alongside digital print technology, including printing on shoes, on umbrellas, and on tents, among other unique applications.\nDigital textile printing is giving rise to new business models and is connecting the manufacturer with the retailer – the beginning of digitalisation.']	['<urn:uuid:b55e7af8-9d5a-43b1-a2b3-89fa5a8f3ae7>', '<urn:uuid:a9897f2a-a744-4ee9-aafd-1f97af856116>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	38	60	3092
49	applying protective coating before reheating billets benefits surface finish scale pits grinding operations	Applying protective anti-scale coating before reheating billets provides several benefits for surface finish: it reduces scale pits and rolled-in scale, especially in special grades like Nickel and 416 Stainless Steel. The coating ensures reduced oxidation and mill scale, which results in good surface finish free from scale pits. This also reduces the need for subsequent grinding operations that would otherwise be necessary to remove adherent scaling.	['How to increase yield and save costs\nin hot rolling and hot forging?\nS. P. Shenoy\nM.Tech. (Met. Engg.), C.E.O.,\nSteel Plant Specialities LLP, India.\nThis article introduces a practical technique pioneered by a metallurgist from the Indian Institute of Technology. The technique enables reduction of burning loss or mill scale, and increasing yield in most kinds of steels. The technique is most successfully adopted in open die hot forging and hot rolling of stainless steel and other expensive alloy steels.\nIn open die forging and hot rolling and even closed die forging of Stainless Steels and alloy steels, the major factor accounting for reduced yield is ‘burning loss’ or ‘scale loss’ or ‘mill scale’ caused due to Oxidation. Oxidation and decarburization of steel take place when billets or ingots are heated in a billet re-heating furnace, in the presence of air or products of combustion. Apart from burning loss of between 1.5% up to 4%, oxidation leads to numerous other problems like scale pit marks, bad quality surface finish of metal, rejections, cumbersome task of removal of mill scale and increased expensive operations like grinding, acid pickling, etc.\nThis article introduces a practical technique pioneered by an experienced Metallurgist from the Indian Institute of Technology (I.I.T.). The technique enables any kind of steel to be heated by substantially minimizing the problem of oxidation.\nUNDERSTANDING BURNING LOSS DUE TO OXIDATION\nFor the purpose of hot rolling, when billets or ingots are heated in an open furnace in the presence of air or products of combustion, the surface phenomena of oxidation takes place. Oxidation causes immediate corrosion of steel at high temperature and creates a layer of metal-oxide, or mill scale, on the surface of the billets or ingots. Decarburisation is a simultaneous reaction that takes place along with oxidation. However, this decarburization is limited to certain grades of steel only.\nOxidation of steel is caused by oxygen, carbon dioxide and/or water-vapour. The general reactions are given below :\nOxidation of steel may range from a tight, adherent straw-coloured film that forms at a temperature of about 180°C to a loose, blue-black oxide scale that forms at temperature above about 450°C with resultant loss of metal or reduced yield.\nHARMFUL EFFECTS OF OXIDATION\nOxidation leads to loss of dimensions and material as extra material allowance needs to be kept for scaling. Often, surface quality is deteriorated due to scale-pitting. This is especially true in the case of Nickel bearing grades of steel.\nThroughout the industry, this burning loss or scale loss is simply treated as wastage and not many practical solutions are available to stop or reduce this wastage. Problems like thick adherent scaling or scale pit marks or decarburization are dealt by manually grinding off the scale or decarburized layer after the hot forging or hot rolling process.\nPREVENTING OR REDUCING BURNING LOSS DUE TO OXIDATION\nPrevention or substantial reduction of oxidation is not only better than cure, it is profitable too. However, most of the available solutions pose a number of practical difficulties. Capital-intensive special furnaces and availability of human resource for using these high-end furnaces is a major issue. Many small hot rolling Organizations cannot afford these solutions. Yet they are under mounting pressure to increase yield and reduce costs. Use of protective anti-scale coating has proven to be a logical solution to the problem of scaling and decarburization in open die hot forging and hot rolling.\nCHARACTERISTICS OF PROTECTIVE COATING AND ITS USE:\nUse of protective coating has been found beneficial and cost-effective. An anti-scale coating is applied on billets / ingots to be heated, before charging them into furnace. This anti-scale protective coating acts as a barrier between oxygen and metal. Care is taken to apply a uniform, impervious layer of coating on the billet to be heated. Coating ensures substantial reduction of oxidation, and thereby reduction in burning loss or mill scale. Anti-scale coating also reduces decarburization on billets during hot rolling operations. Heat transfer from heating media to metal is not affected due to anti-scale protective coating.\nNo reaction with steel surface, no release of toxic fumes during use or hot forging or storage, non-hazardous and economical implementation are other required characteristics of the coating.\nAdjoining photograph shows coated stainless steel ingot before being charging into furnace. Coating may be applied by brushing, spraying or dipping.\nINCREASING YIELD BY THE USE OF PROTECTIVE ANTI-SCALE COATING; INDUSTRIAL CASE STUDIES AND SUCCESS STORIES.\nDue to reduction in oxidation by the use of protective anti-scale coating during hot forging and hot rolling, the scale thickness is considerably reduced. Adjoining photographs are of mill scale that were generated on billets during re-heating for hot rolling with and without using protective\nThin, loose scale is observed on billets when protective coating is used.\nWhite layer of coating residue is observed on scale. This indicates efficacy of coating.\nThick adherent scale is observed on billets when coating is not used.\nThickness of mill scale fallen off both, coated and not coated billets, is measured using digital vernier calliper. As shown in image, scale of much lower thickness of 0.717 mm is generated on billets that were protected using anti-scale coating. In comparison, billets that were not protected and were charged in the same batch of heating have shown thicker and more adherent scale of 1.667 mm.\nTotal reduction in scale: 0.95 mm.\nPercentage reduction in mill scale loss due to use of protective anti-scale coating: 56.98%\nOn an average, approximately 70% reduction in mill scale is usually achieved by the use of anti-scale protective coating in open die hot forging and hot rolling of stainless steel. This amounts to lucrative savings and increased yield.\nAnother report of a number of mild-steel billets hot rolled without coating in comparison with hot rolling similar billets with single coating and double coating of anti-scale protection is provided below:\nScale loss of 1.140 kgs. was observed on billets with double layer of protective anti-scale coating.Scale loss of 2.140 kgs. was observed on billets without coating.\nPercentage reduction in mill scale loss due to use of protective anti-scale coating: 47.00%\nOTHER BENEFITS OF USING PROTECTIVE ANTI-SCALE COATING IN STAINLESS STEEL HOT ROLLING AND OPEN DIE FORGING:\n- REDUCTION IN GRINDING, ACID PICKLING TIME OF HOT ROLLED PRODUCTS:\nOperations like grinding, acid pickling, etc. do not add value, are expensive and time consuming procedures. These operations are necessary to remove adherent scaling from the hot rolled billets. Time required for operations like shot blasting, grinding, acid pickling, etc. can be substantially reduced if protective anti scale coating is applied on components before heat treatment. Aesthetic appeal of components is automatically enhanced without much effort as scaling is either prevented or reduced by the use of anti oxidation protective coating.\n- REDUCED DECARBURIZATION DURING HOT ROLLING & HOT FORGING :\nCertain grades like spring steels, ball bearing steel and rail steel are susceptible to decarburisation. During hot rolling of these special grades, decarburization needs to be consistently maintained to the required specification. However, due to unforeseen conditions like mill breakdown and unplanned downtime, billets remain in the furnace for longer time than usual. Also, when the plant is closed for weekly holiday, furnace is shut off abruptly, subjecting the billets to prolonged heating inside the furnace. If billets are not coated, it becomes difficult to guarantee the consistency in the decarburization level. Use of protective coatings on billets before charging them into the re-heating furnace enables to maintain the decarburization level consistently. This enables the hot rolling mill to guarantee their customers that decarburization level will always be less than the upper control limit.\nReduced decarburization on automobile leaf springs leads to increased fatigue strength of the leaf springs and greater reliability. Leaf springs heated after applying protective coating show substantially reduced decarburization and scaling.\n- IMPROVING SURFACE FINISH OF HOT ROLLED COMPONENTS AND PREVENTING WELDING OF BILLETS DURING HEATING:\nDuring hot rolling of special grades of steel like Nickel (Ni) and 416 Stainless Steel containing high sulphur content, mill scale is greatly increased. Surface finish of hot rolled product may be compromised due to scale pits and rolled-in scale. Occasionally, due to heavy adherent scale, problem of roll skid is also encountered. Subsequent grinding operations are increased due to excessive mill scale. Protecting the billets with anti-scale protective coating before charging them into the re-heating furnace ensures reduced oxidation and mill scale. Coating the billets before charging them into the reheating furnace enables to achieve good surface finish of the hot rolled products free from scale pits and rolled-in scale.\nPhotograph shows not coated billet on conveyor, discharged from furnace and passed through water de-scaler. As thick, adherent scale was formed, water de-scaling has not been effective. Adherent scale is observed on the top surface and some adherent scale on the left side surface.\nPhotograph shows anti-scale coated billet on conveyor, discharged from furnace and passed through water de-scaler. As thin, loose scale was formed, complete scale has fallen off during water de-scaling.\nThis proves that scale loss is reduced. Ease of rolling operation and reduced scale deposits on the rolling mill machinery are ensured.\nDuring re-heating of billets for hot rolling and also during induction heating of billets for hot forging, sometimes, billets get welded together. This leads to unproductive process of separating the welded billets and downtime. Welding of billets during hot rolling and hot forging can be prevented by applying protective coating on billets before charging them into the furnace.\n- IMPROVING SURFACE FINISH OF SEAMLESS PIPES:\nIn the manufacture of seamless pipes, machined hollow ingots are used in many places, especially in Poland. It becomes a necessity to protect the surface by protective coating when the ingot is reheated. This ensures better surface finish of the seamless pipe. This is achieved by the use of anti-scale protective coatings.\n- 70% average reduction in burning loss or mill scale loss and increased yield in stainless steel open die forging and hot rolling is possible by the use of protective anti-scale coating.\n- Protecting billets by applying anti-scale coating on them ensures a number of additional benefits like elimination or reduction of operations like grinding, acid pickling, etc., reduced decarburization, prevention of billet welding in furnace and improved surface finish.']	['<urn:uuid:f95b1d36-eaf2-4132-b1de-1c017fc80ff0>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	13	66	1706
50	miners minestrone soup calories vs moo ping calories per serving	The miners minestrone soup contains 120 calories per 1 cup serving, while the Thai moo ping (grilled pork skewers) contains significantly more calories given its nutritional facts show 34g of total fat (43% DV) and 25g of total carbohydrates (9% DV) per serving.	"[""(with pre-soaked beans)\nMixed beans, split peas, barley, dehydrated vegetables (carrots, celery, potatoes, bell peppers, leeks, cabbage), black pepper, herbs and spices\nAvailable in small and large packages\nSmall: 4-5 1 cup servings\nLarge: 8-10 1 cup servings\nNutritional Facts for small package: Serv. Size 1 cup, Servings 5 Amount Per Serving: Calories 120, Fat Cal 0, Total Fat 0g (0% DV), Sat Fat 0g (0% DV), Cholest.0mg (0% DV), Sodium 320mg (13% DV), Total Carb. 22g (7% DV), Fiber 6g (24% DV), Sugars 3g, Protein 7g, Vitamin A (8% DV), Vitamin C (15% DV), Calcium (4% DV), Iron (15% DV). Percent Daily Values (DV) are based on a 2000 calorie diet.\nProduct of Washington State\nRill's Specialty Soups are a Product of Washington State.\nGreat Ideas for Dressing up your Miners Minestrone Soup\nYour favorite vegetables\nReduce sodium by limited the amount of beef base (brown packet) used.\n|Vegetarian||Leave out the brown pack of beef base, add 15-oz. vegetable broth and reduce water in recipe by two cups. If desired, add meat substitute.|\nTidbits of Mining History\nMining has been a large part of U.S. history. Our lands are rich with natural resources and there are many stories of mining and small boomtowns. People migrated all across the country in hopes of riches and steady employment.\nIn the late 1800s the industrial movement of the United States gained momentum. The need for coal increased rapidly, and mining started in many states. Most of the mines were in remote areas and U.S. currency was difficult to keep in hand. Companies began to issue script tokens as an advance payment for wages earned. These tokens could be spent at the company store for food, clothing and supplies.\nThe work was exhausting, men worked underground for 11 to 12 hours a day, when the market for coal dropped, they had worked as little as two days a week. Wages were either based on the amount of coal the miner could dig or on an hourly wage of about .20 cents. Before wages were paid the company store deducted rent, utilities, health cares and charges owing the store. Men who were unable to continue heavy work stood at the screening tables handpicking rock from the coal.\nIn the State Of Washington, the Leary Coal Company opened a seam at Ravensdale, in 1890 (this is Northwest of Black Diamond) it soon began to produce so well that it attracted the NW Improvement Company, the coal mining arm of the Northern Pacific railroad. They bought the land and brought in experienced Italian, Polish, and Lithuanian miners from the east coast. In 1915 an explosion killed 31 miners. This combined with a declining demand they closed the mine and moved many of the houses to Roslyn where they also were operating mines.\nRoslyn sits atop a coalfield that stretches for 7 miles. In 1884, Nez Jensen, A homesteader dug coal with a pick and shovel and sold it to the blacksmiths in Ellensburg area. Two years later the Northern Pacific Railroad examined the deposits and with in weeks had miners digging Coal. By 1898, a decade after opening the Roslyn mines they accounted for half the coal production in the state. At its peak in the 1920s two million tons of coal were mined annually. The miners in that area made more in 2 days working the Roslyn mines than could be earned in 2 weeks work as a garment worker in New York City.\nIdaho State's first industry was mining, and still is today 130 years after the discovery of gold, in 1860, remains as an important part of the states economy. The Coeur d’ Alene mining district is proven to be the richest silver mining district in the world. In 1985 the mines produced their one-billionth ounce of silver and also vast amounts of lead, zinc, and copper. Other records in the Silver Valley include “the Morning Star” one of the deepest mines in America. Bunker Hill ranks as America’s largest underground mine: And the Sunshine Mine America’s richest silver mine, producing over 300 million ounces of silver more than the entire output of Nevada’s famous Comstock Lode.\nMontana's Giltedge Mine was the first place in the United States to use cyanide-leeching process in gold mining. This began in 1893 just when the USA was in the depths of a serve economic depression. It was carefully watched by the national financial and international mining world. They felt that if Giltedge produced successfully investors could be easily persuaded to invest in cyanide plants in other mining areas reducing the depression.\n- Be the first to review this product"", 'Why It Works\n- Thinly slicing pork shoulder allows the meat to pick up flavor from the punchy marinade, and speeds up cooking time.\n- Setting up a charcoal grill just for cooking skewers makes it easier to produce lightly charred pork that is juicy and tender.\n- Brushing skewers with unsweetened coconut cream while cooking keeps meat moist, and creates a sticky glaze on the surface.\nIf you were to conduct a poll in the United States asking people what comes to mind when they hear the words ""Thai grilled skewers,"" I would wager over 97 percent of folks would answer, ""chicken satay."" People love chicken satay, and I don\'t have any bones to pick with their skewer choices. I would also venture a guess that the absence of bone-picking is part of what appeals to a dining public that willed the ""boneless chicken wing"" into marketing existence. Satay is great, but there are plenty of other Thai options in the grilled meat-on-a-stick extended universe. One of my favorites is moo ping—grilled pork skewers, a popular street food in Thailand.\nThese skewers are made with sliced pork tossed in a savory-sweet marinade, which balances the saltiness of Thai fish sauce, soy sauce, and oyster sauce with the roasty sweetness of palm sugar. Garlic and white pepper provide plenty of bite. Traditionally, garlic is pounded to a paste in a mortar and pestle with cilantro root, but to make this dish more accessible, mince garlic cloves and substitute minced tender cilantro stems in place of the harder-to-find cilantro root. An optional, but highly recommended, pinch of MSG powder (a common ingredient used by Thai street food vendors) gives the marinade a savory boost.\nAs with my al pastor skewers, I thinly slice pork butt into strips sized for skewering. After marinating a few hours, the pork pieces get threaded onto skewers, bunched tightly together to prevent the skewer from burning and breaking during cooking; the only parts of the skewers that should be exposed are a two-inch handle at the bottom, and just the very tip at the top.\nOnce the skewers are assembled, it\'s time to rig up your grill for skewer-cookery. This set up mimics the grilling rigs you would find at street-food kiosks in Thailand. The pork can be grilled directly over the coals, or on top of a wire rack balanced on the bricks.\nAs the pork cooks, brush skewers with unsweetened coconut cream, which forms a sticky, shiny glaze on the meat while keeping the pork moist as it grills. The skewers need to be turned and brushed continuously in order for the meat to cook evenly, and to manage flare-ups caused by dripping pork fat.\nOnce the meat is lightly charred and cooked through, take skewers off the grill and let them rest for a couple of minutes before you dig in. The skewers are great as-is, but you can jazz them up with a squeeze of fresh lime, or, if you\'re feeling industrious, a batch of dried chili-vinegar dipping sauce for dunking. Nobody will be missing sad takeout satay.\n4 medium cloves garlic (20g), minced\n2 tablespoons (15g) minced cilantro stems, plus fresh cilantro leaves for serving\n2 tablespoons (30g) finely chopped palm sugar (see note)\n1 tablespoon (15ml) fish sauce\n1 tablespoon (15ml) soy sauce\n1 tablespoon (15ml) oyster sauce\n1 1/2 teaspoons (6g) ground white pepper\n1/2 teaspoon (2g) MSG powder (optional)\n2 pounds (900g) pork butt, in one piece\n1 (5 1/2-ounce; 160ml) can unsweetened coconut cream (see note)\n1 recipe Thai Dried Chili-Vinegar Dipping Sauce, for serving (optional)\nIn a small bowl, combine garlic, cilantro stems, palm sugar, fish sauce, soy sauce, oyster sauce, white pepper, and MSG (if using), and stir until palm sugar is dissolved. Set aside.\nFreeze pork for 15 minutes and up to 30 minutes (partially freezing the pork makes it easier to slice). Using a sharp chef\'s knife or slicing knife, slice pork against the grain into 2-inch-long, 1-inch-wide, and 1/8-inch-thick strips. The easiest and most efficient way to do this is to start by portioning the pork butt into 2-inch-wide by 1-inch-thick pieces, and then slicing those pieces crosswise into 1/8-inch-thick strips.\nCombine pork and marinade in a large bowl, and toss until every piece of meat is evenly coated in marinade. Cover tightly with plastic wrap and refrigerate for at least 4 hours, and up to 36 hours.\nThread one piece of pork onto a skewer, piercing it through twice to secure it, then pushing meat tightly together like an accordion. Continue threading pork onto skewer, making sure meat is tightly bunched together, leaving no parts of the skewer exposed except for a 2-inch handle at the bottom, and the pointy tip at the top. Repeat skewering process with remaining pork.\nSet up grill for skewers, making sure to adjust distance between bricks to the length of your skewers. Light one chimney full of charcoal. When all the charcoal is lit and covered with gray ash, pour out and spread the coals evenly in the channel between the bricks.\nPlace skewers directly over hot coals, balancing on top of bricks, with handles overhanging the bricks closest to you and the tips balancing on the farther wall of bricks. Using a brush, brush pork with coconut cream. Cook, turning skewers and brushing pork with coconut cream frequently, until meat is lightly charred, and a piece of pork looks cooked through when removed and cut in half, 8 to 10 minutes; if flare-ups occur, move skewers around as needed to get them away from the flames. Transfer to serving platter and let rest for 1 to 2 minutes. Serve immediately, passing lime wedges, as well as cilantro and dipping sauce (if using), at the table.\nPalm sugar, sometimes labeled as ""coconut sugar,"" can be found at Thai markets, or online. For this recipe, an equal amount of packed light brown sugar can be substituted for palm sugar if you have trouble sourcing it.\nCoconut cream is a richer, thicker, more concentrated version of coconut milk. Find it in cans in the same aisle as canned coconut milk. It is not the same as cream of coconut, which has added sugar.\nMake-Ahead and Storage\nThe grilled skewers are best enjoyed immediately. The marinade can be made in advance, and refrigerated for up to 3 days. The pork can be marinated for up to 36 hours.\n|Nutrition Facts (per serving)|\n|Servings: 4 to 6|\n|Amount per serving|\n|% Daily Value*|\n|Total Fat 34g||43%|\n|Saturated Fat 16g||78%|\n|Total Carbohydrate 25g||9%|\n|Dietary Fiber 0g||1%|\n|Total Sugars 23g|\n|Vitamin C 3mg||15%|\n|*The % Daily Value (DV) tells you how much a nutrient in a food serving contributes to a daily diet. 2,000 calories a day is used for general nutrition advice.|']"	['<urn:uuid:c3f55569-b0cc-4c53-aa4c-aab6fb5ab3b6>', '<urn:uuid:6add7659-d607-4e49-9bb5-feff73ff2f6c>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	10	43	1905
51	how many hours meditation per day for 8 weeks changes brain function and connectivity between cerebral cortex lobes	According to several accredited neurologists and universities including Harvard, daily meditation for 15-30 minutes for eight weeks substantially alters brain function and connectivity between the major four lobes of the cerebral cortex.	['Originally an ancient Buddhist meditation technique, in recent years has become the focus of study from a more scientific approach based on clinical evidence. Since the 1980’s yoga and mediation has become a popular system of physical exercise in America. Yogis across the western world boast of the numerous benefits that can be gained through asana and meditational practice: lowers blood pressure/slows down the cardiovascular system, restores balanced function to the digestive system, aiding absorption of nutrients, relaxes the nervous system, relieves muscle tension, diminishes intensity of headaches/migraines, relieves insomnia, releases fears, reduces anxiety, alleviates depression, generates optimism, self-esteem, confidence, motivation, and frees the mind from self doubt and internal chatter. However, there is a plethora of recent studies about what “mindfulness meditation” is actually doing to your brain to achieve such a lengthy list of benefits. According to several accredited neurologists and universities including Harvard, daily meditation (15-30 min) for eight weeks substantially alters brain function and connectivity between the major four lobes of the cerebral cortex.\nMeditation has been a widely accepted therapy for anxiety and depression for around a decade. However, more recently websites such as GetSomeHeadSpace.com and numerous blogs are attracting millions of subscribers who, like myself, are trying to better understand the physical and mental benefits they are experiencing through meditation. “Now, as the popularity of mindfulness grows, brain imaging techniques are revealing that this ancient practice can profoundly change the way different regions of the brain communicate with each other – and therefore how we think – permanently. MRI scans show that after an eight-week course of mindfulness practice, the brain’s “fight or flight” center, the amygdala, appears to shrink” (Ireland). This primal region of the brain helps the body respond to emotions, memories, fear and is involved in the initial response to stress. “The amygdala is a large portion of the telencephalon, located within the temporal lobe which can be seen from the surface of the brain” (“Parts of the Brain and Their Functions”). This scientific observance alone gives credence to over half of the benefits, discussed above, that western yogis are raving about. A decrease in size of the portion of the brain that regulates ones’ reaction to stress helps explain the relaxation of the nervous system, reduction of anxiety and depression, and boost in self-esteem, confidence, and motivation. The culmination of these effects yields a mental state that is much less likely to suffer from self-doubt and insomnia.\n“As the amygdala shrinks, the prefrontal cortex – associated with higher order brain functions such as awareness, concentration and decision-making – becomes thicker” (Ireland). The prefrontal cortex is the cerebral cortex that covers the front part of the frontal lobe. This lobe, “ controls several elements including creative thought, problem solving, intellect, judgment, behavior, attention, abstract thinking, physical reactions, muscle movements, coordinated movements, smell and personality” (“Parts of the Brain and Their Functions”). The augmentation of the prefrontal cortex within eight weeks of meditation helps explain the yogic terminology “being/feeling present”. Ones’ sense of presence in the actual moment would naturally be heightened as awareness and the ability to concentrate are more readily accessed. This feeling of a peaceful presence in the moment is felt to an even greater degree directly after a practice of both asana and meditation due to the prolonged focus on ones’ breath, release of endorphins through physical exercise, and the relaxation of the body’s major systems that occur during corpse pose.\n“The functional connectivity between these [the amygdala and the prefrontal cortex] regions – i.e. how often they are activated together – also changes. The connection between the amygdala and the rest of the brain gets weaker, while the connections between areas associated with attention and concentration get stronger” (Ireland). Naturally, the degree by which the functional connectivity is altered correlates with the number of hours one practices meditation. Researchers from Harvard decided to observe and study meditational experts, yogis with 40,000 hours of recorded meditational practice. The advanced meditators reported feeling significantly less pain than the non-meditators during certain pain-inducing situations. However, scans of their brains showed slightly more activity in the areas associated with pain than the non-meditators. Joshua Grant, a postdoc at the Max Plank Institute for Human Cognitive and Brain Sciences in Leipzig, Germany said “It doesn’t fit any of the classical models of pain relief, including drugs, where we see less activity in these areas (Ireland). “It seems Zen practitioners were able to remove or lessen the aversiveness of the stimulation – and thus the stressing nature of it- by altering the connectivity between two brain regions, which are normally communicating with one another. They certainly don’t seem to have blocked the experience. Rather, it seems they refrained from engaging in thought processes that make it painful” (Ireland).\nMany leading neurologists believe the changes acquired through meditation become permanent after a certain mastery or length of time meditating. It is worth noting that in the pain inducing experiment, the expert meditators were not in a meditative state. It seems the pain lessening effect is not something acquired through a specific meditational state, but instead it is the outcome of a lifetime of rewiring brain level activity and the degree of connectivity between specific regions of the brain. Joshua Grant said “We asked them specifically not to meditate. There is just a huge difference in their brains. There is no question expert meditators’ baseline states are different” (Ireland). Several independent studies on expert meditators agree that their resting brains look similar, when scanned, to novice meditators when they are meditating. At this level of expertise [40,000 hours of meditation], the prefrontal cortex is no longer bigger than expected. In fact, its size and activity start to decrease again. “It’s as if that way of thinking has become the default, it is automatic – it doesn’t require any concentration,” says Adrienne Taren, a researcher studying mindfulness at the University of Pittsburgh (Ireland).\n“The human brain is ultimately responsible for all thought and movement that the body produces. This allows humans to successfully interact with their environment, by communicating with others and interacting with inanimate objects. If the brain is not functioning properly, the ability to move, generate accurate sensory information or speak and understand language can be damaged as well” (“Parts of the Brain and Their Functions”). The picture being painted by meditational research is that a mindfulness practice increases one’s ability to recruit higher-order, prefrontal cortex regions in order to down regulate lower-order brain activity. Hopefully, with growing awareness, humankind will utilize this tool to sharpen the mind to overcome the immanent obstacles of the future.\nIreland, Tom. “What Does Mindfulness Meditation Do to Your Brain?.” Scientific American. 12 06 2014: n. page. Web. 26 Mar. 2015. <http://blogs.scientificamerican.com/guest-blog/2014/06/12/what-does-\n“Parts of the Brain and Their Functions.” MDhealth.com. MDhealth, 26 03 2015. Web. 26 Mar 2015. <http://www.md-health.com/parts-of-the-brain-and-function.html>.']	['<urn:uuid:fbd47d0f-1b1d-47a9-9308-976a42668c8e>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	18	32	1141
52	disadvantages higher voltage distribution systems reliability safety cost equipment crews	Higher voltage distribution systems have several disadvantages: reduced reliability due to longer lines and more exposure to faults, crews dislike working on them due to safety concerns and thicker gloves, more stringent procedures, and higher equipment costs for cables, insulators, transformers, arresters, and cutouts.	['Most distribution voltages are between 4 and 35 kV. In this article, unless otherwise specified, voltages are given as line-to-line voltages; this follows normal industry practice, but it is sometimes a source of confusion.\nThe four major voltage classes are 5, 15, 25, and 35 kV. A voltage class is a term applied to a set of distribution voltages and the equipment common to them; it is not the actual system voltage.\nFor example, a 15-kV insulator is suitable for application on any 15-kV class voltage, including 12.47 kV, 13.2 kV, and 13.8 kV. Cables, terminations, insulators, bushings, reclosers, and cutouts all have a voltage class rating. Only voltage-sensitive equipment like surge arresters, capacitors, and transformers have voltage ratings dependent on the actual system voltage.\nUtilities most widely use the 15-kV voltages as shown by the survey results of North American utilities in Figure 1. The most common 15-kV voltage is 12.47 kV, which has a line-to-ground voltage of 7.2 kV.\nThe dividing line between distribution and subtransmission is often gray. Some lines act as both subtransmission and distribution circuits. A 34.5-kV circuit may feed a few 12.5-kV distribution substations, but it may also serve some load directly.\nSome utilities would refer to this as subtransmission, others as distribution.\nThe last half of the 20th century saw a move to higher voltage primary distribution systems. Higher-voltage distribution systems have advantages and disadvantages (see Advantages and disadvantages of higher voltage distribution below).\nThe great advantage of higher voltage systems is that they carry more power for a given current.\nHigher Voltage Distribution\nVoltage drop – A higher-voltage circuit has less voltage drop for a given power flow.\nCapacity – A higher-voltage system can carry more power for a given ampacity.\nLosses – For a given level of power flow, a higher-voltage system has fewer line losses.\nReach – With less voltage drop and more capacity, higher voltage circuits can cover a much wider area.\nFewer substations – Because of longer reach, higher-voltage distribution systems need fewer substations.\nReliability – An important disadvantage of higher voltages: longer circuits mean more customer interruptions.\nCrew safety and acceptance – Crews do not like working on higher-voltage distribution systems.\nEquipment cost – From transformers to cable to insulators, higher-voltage equipment costs more.\nInformation above shows maximum power levels typically supplied by various distribution voltages.\nLess current means lower voltage drop, fewer losses, and more power-carrying capability. Higher voltage systems need fewer voltage regulators and capacitors for voltage support. Utilities can use smaller con-ductors on a higher voltage system or carry more power on the same size conductor.\nTable 1 – Power Supplied by Each Distribution Voltage for a Current of 400 A\n|System Voltage (kV)||Total Power (MVA)|\nUtilities can run much longer distribution circuits at a higher primary voltage, which means fewer distribution substations. Some fundamental relationships are:\nPower – For the same current, power changes linearly with voltage.\nwhen I2 = I1\nCurrent – For the same power, increasing the voltage decreases current linearly.\nwhen P2 = P1\nVoltage drop – For the same power delivered, the percentage voltage drop changes as the ratio of voltages squared. A 12.47-kV circuit has four times the percentage voltage drop as a 24.94-kV circuit carrying the same load.\nwhen P2 = P1\nArea coverage – For the same load density, the area covered increases linearly with voltage: A 24.94-kV system can cover twice the area of a 12.47-kV system; a 34.5-kV system can cover 2.8 times the area of a 12.47-kV system.\nV1, V2 = voltage on circuits 1 and 2\nP1, P2 = power on circuits 1 and 2\nI1, I2 = current on circuits 1 and 2\nV%1, V%2 = voltage drop per unit length in percent on circuits 1 and 2\nA1, A2 = area covered by circuits 1 and 2\nResistive line losses are also lower on higher-voltage systems, especially in a voltage-limited circuit. Thermally limited systems have more equal losses, but even in this case higher voltage systems have fewer losses.\nLine crews do not like higher voltage distribution systems as much. In addition to the widespread perception that they are not as safe, gloves are thicker, and procedures are generally more stringent. Some utilities will not glove 25- or 35-kV voltages and only use hotsticks.\nThe main disadvantage of higher-voltage systems is reduced reliability. Higher voltages mean longer lines and more exposure to lightning, wind, dig-ins, car crashes, and other fault causes. A 34.5-kV, 30-mi mainline is going to have many more interruptions than a 12.5-kV system with an 8-mi main-line. To maintain the same reliability as a lower voltage distribution system, a higher-voltage primary must have more switches, more automation, more tree trimming, or other reliability improvements.\nHigher voltage systems also have more voltage sags and momentary interruptions. More exposure causes more momentary interruptions. Higher voltage systems have more voltage sags because faults further from the substation can pull down the station’s voltage (on a higher voltage system the line impedance is lower relative to the source impedance).\nCost comparison between circuits is difficult (see Table 2 for one utility’s cost comparison). Higher voltage equipment costs more – cables, insulators, transformers, arresters, cutouts, and so on. But higher voltage circuits can use smaller conductors. The main savings of higher-voltage distribution is fewer substations.\nHigher voltage systems also have lower annual costs from losses.\nTable 2 – Costs of 34.5 kV Relative to 12.5 kV\n|Subdivision without bulk feeders||1.25||1.13|\n|Subdivision with bulk feeders||1.00||0.85|\nSource: Jones, A.I., Smith, B.E., and Ward, D.J., “Considerations for Higher Voltage Distribution,” IEEE Transactions on Power Delivery, vol. 7, no. 2, pp. 782–8, April 1992.\nAs far as ongoing maintenance, higher voltage systems require less substation maintenance, but higher voltage systems should have more tree trimming and inspections to maintain reliability. Conversion to a higher voltage is an option for providing additional capacity in an area. Conversion to higher voltages is most beneficial when substation space is hard to find and load growth is high.\nIf the existing subtransmission voltage is 34.5 kV, then using that voltage for distribution is attractive; addi-tional capacity can be met by adding customers to existing 34.5-kV lines (a neutral may need to be added to the 34.5-kV subtransmission line).\nOverall, the 15-kV class voltages provide a good balance between cost, reliability, safety, and reach. Although a 15-kV circuit does not naturally provide long reach, with voltage regulators and feeder capacitors it can be stretched to reach 20 mi or more. That said, higher voltages have advantages, especially for rural lines and for high-load areas, particularly where substa-tion space is expensive.\nMany utilities have multiple voltages (as shown by the survey data in Figure 1). Even one circuit may have multiple voltages. For example, a utility may install a 12.47-kV circuit in an area presently served by 4.16 kV. Some of the circuit may be converted to 12.47 kV, but much of it can be left as is and coupled through 12.47/4.16-kV step-down transformer banks.\nResource: Electric distribution equipment and systems by T. A. Short (Buy the book at Amazon)']	['<urn:uuid:4cd7020c-15b3-4e38-bdcb-fc22bc6320db>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	10	44	1178
53	How many ampullary chambers does each Ampulla of Lorenzini contain?	Each ampulla is composed of 8-9 ampullary chambers consisting of receptor and mucus-secreting gland cells.	['Scoliodon :- Ampullae of Lorenzini\nFor Scoliodon fish slide ( Ampullae of Lorenzini ) Remove a piece of skin around the snout and take out some tissue by forceps and examine it under\nmicroscope for above ampullae. Stain in borax carmine, dehydrate, clear and mount.\n(1) Ampullae of Lorenzini are sensory and mucus-secreting structures.\n(2) Each ampulla is composed of a tube and 8-9 ampullary chambers consisting of receptor and\nmucus-secreting gland cells.\n(3) Receptor cells are innervated by 7th cranial nerve. The tube has external opening.\nScoliodon :- Placoid Scales\nFor Scoliodon fish slide (PLACOID SCALES) Take a few small pieces of the skin of Scoliodon and boil in 5 to 10% KOH solution in test tube till skin\ndissolves. Cool and allow the scales to settle at bottom. Decant KOH and wash the material in water\nseveral times to remove KOH. Stain in borax carmine or picro-indigo carmine, dehydrate, clear and mount. Placoid scales do not take stains properly. These scales can be mounted without staining. Dehydrate, clear and mount.\n(1) Placoid scales or odontoids are minute dermal denticles, closely arranged in regular oblique rows.\n(2) They form entire exoskeleton of the shark and give a rough appearance to the skin. (3) Each placoid scale comprises of a diamond-shaped, basal plate embedded in the skin and is derived\n(4) Anteriorly the scale has a flat trident spine projecting out of the skin.\n(5) With very few exceptions, placoid scales are abundantly found in dermis of elasmobranch fishes.\n(6) Placoid scales are arranged in regular oblique rows. They are dermal in origin and cover entire surface of the body, forming dermal exoskeleton of the sharks.\n(7) Each scale is composed of a basal bony plate embedded in the dermis, from which projects trident\n(8) Basal plate is formed of a trabecular calcified tissue.\n(9) Spine is composed of dentine covered by a hard material, vitrodentine.\n(10) Placoid scale contains a pulp cavity in spine.\n(11) Pulp cavity contains odontoblasts dentine forming cells, blood capillaries, nerves and lymph channels.\n(12) General similarity in structure of placoid scales to teeth of higher forms should be apparent. Both are considered to be remnants of bony armour of such primitive vertebrates as ostracoderms and certain placoderms.\nIdentification : Since this scale has trident spines, hence it is placoid scale of Scoliodon.\nScoliodon : Development of Placoid Scales\n(1) Development of placoid scale can be studied in a V.S. of integument of dogfish.\n(2) V.S. integument shows 2 layers epidermis and dermis. Placoid scales originate from dermis. Various\ndevelopment stages of placoid scales are as under.\n(i) Mesodermal cells accumulate and form dermal papillae beneath stratum germinatum.\n(ii) As epidermis is pushed dermal papilla develops odontoblasts which deposit deutine between\nthemselves and epiderms to form placoid scale.\n(iii) Epidermal cells of papilla are called as ameloblast cells which form enamel organ which deposites\nvitrodentive over dentino.\n(iv) In developing placoid scale dermal papilla gives rise to pulp cavity and dentine canalculi.\n(v) When fully formed placoid scales project covered by epidermis.\n(vi) Other structures seen are loose connective tissue, compact connective tissue, vertical fibers, nerve\nfibers, chromatophores and cement.\nIdentification: Since the section contains dermal papilla and above features, hence it is V.S. of integument showing placoid scales of Scoliodon.\nLabeo (Rohu) : Cycloid Scales\nFor this fish slide (Labeo (Rohu) : Cycloid Scales) Take one or two scales. Stain in picro-indigo carmine, dehydrate and mount in balsam. Each scale consists of lines of growth and nucleus.\n(1) Cycloid scales are found in carps (teleost) and lungfishes (dipnoi). They are located in the dermal\npockets and possess concentric lines of growth.\n(2) Each cycloid scale is roughly circular in outline without pulp cavity and with free and smooth border.\n(3) Scales covering lateral line are frequently perforated permitting the passage of small connectives of\nlateral line to outside.\n(4) Cycloid scales are derived form of ganoid scales in which ganoin, cosmine and bone cells are lost.\n(5) Scales are soft, arranged lengthwise in diagonal rows.\nIdentification : Since the above scale contains concentric lines of growth, hence it is cycloid scale of\nNandus : Ctenoid Scales\nFor this fish slide (NANDUS : CTENOID SCALES) Make permanent mount like cycloid scale. Each scale comprises of several concentric lines of growth, teeth or denticles and nucleus. Ctenoid scales are found in acanthopterygian fishes.\n(1) Ctenoid scales are commonly found in most teleost and actinopterygian fishes.\n(2) They are thin, soft and dermal translucent plates.\n(3) They are composed of underlying fibrous layer covered by bone-resembling layer.\n(4) They contain concentric rings, representing lines of growth which vary in different specimens.\n(5) Ctenoid scales do not contain ganoin.\n(6) Each scales is embedded in a small dermal pocket.\n(7) Scales are obliquely arranged so that posterior end of one overlaps the anterior end of the following\n(8) Basal end is scalloped and free edge uears numerous comb-like projections.\n(9) Ctenoid scales are derivatives of ganoid scales in which ganoin, cosmine layers and bone cells are lost.\nPulp cavity and dentine are entirely absent.\nIdentification: Since the above scale has concentric lines of growth with teeth like projections posteriorly, hence it is ctenoid scale of Nandus.\nPolypterus :- GAnoid Scales\n(1) Ganoid scales are found in primitive ray-finned fishes such as Polypterus and gar pikes.\n(2) Scales are covered with a hard, shiny and translucent material of mesodermal origin called as ganoin.\n(3) Ganoid scales fit together like tiles and are arranged in diagonal rows.\n(4) Scales are dermal in origin.\n(5) Each scale consists of a bony base, coated by shining substance called as ganoin and openings of lateral line canal.\nIdentification : Since the above scale is overlapping and fitted like tiles, hence it is ganoid scale of Polypterus.']	['<urn:uuid:989906ac-8f40-4a35-9bac-62d16147ac2a>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:09:04.591601	10	15	963
54	As someone who's kayaked for years, what's safe clothing for water trips and how dangerous is inexperience?	For clothing, you need quick-drying gear (not cotton) including shorts/bathing suit suitable for restaurants, paddling jacket, rain/windproof gear, and proper footwear - flip flops are not acceptable due to slipping risks. Running shoes are recommended over sandals. For safety, inexperience can be dangerous as novice paddlers may not recognize hazards early. While advanced beginners can manage these trips, participants should know wet exits. Experienced paddlers have better judgment about when/where to paddle safely and knowledge of safety/rescue practices.	"[""What can we expect on a typical day?\nOur expeditions are vigorous but not excessively rugged. However, they are not for totally inexperienced paddlers. We would like you to have done wet exits and if you haven't we can do that the first day. An advanced beginner would be fine. On our 9 night trips some days we will be covering rather long distances on the water so we expect all participants to be reasonably fit. Our 5 night trips are more relaxed.\nAge should not be a factor. Our groups often include paddlers in their seventies. We paddle together as a group and our speed is determined by the slowest paddler. Typically we average 4 km. per hour when headwinds are mild. This is flat water paddling with a very little current.\nWe get on the water early enough each day to avoid headwinds that usually pick up around noon. Early starts also get us to our daily destination early enough to stretch our legs and visit the area. The guides appreciate help getting boats and equipment on and off the water. Our kayaks are very stable, seaworthy expedition single and tandem models.\nWhere will we be staying and eating?\nWe stay in small family operated hotels and quintas where our neighbors are Portuguese. Breakfast will be in the hotels. Lunches will be prepared by our guides or eaten in local restaurants. We will enjoy regional Portuguese cuisine in restaurants for evening meals. All our meals will be from local grown products mostly bought at that days market. In several cases the meals will be prepared with products grown in fields right next to the restaurant. We will do our best to cater to dietary restrictions but we must be informed when you sign on for the expedition.\nClothing for our trip\nFoot wear: one pair of lightweight walking shoes and a pair of sandals and whatever you wear in the kayak which could be the same sandals.Flip flops are not acceptable as kayak footwear. There are more accidents getting into and out of kayaks than at any other kayaking moment. Flip flops tend to slip when wet and we can't risk slipping.\nKayaking wear; Some of these items you might not need but I prefer to err on the safe side. All this must be quick drying (i.e.) not cotton. Shorts or bathing suit suitable for wearing in any bar or restaurant, paddling jacket/rain and wind proof. Paddling pants if you tend to get cold easily. Long sleeved shirt, some sort of head protection from the sun, sunglasses with strap. A buff is something I find very practical for sun protection.\nOff water wear: Long pants, skirt, dress, t-shirts and other clothes you might like but do travel as light as possible.\nWhat can I read to prepare myself for this trip?\nCork Boat John Pollack 2004\nMr Pollack takes us on a dream fulfilling journey down the same route we\nfollow on the Douro River. The last part of the book is very relevant\nto our expedition. In any case it’s an enjoyable read. Amazon has this\nin both paperback and Kindle versions.\nHow do you suggest I travel to and from the pick up site?\nThe first Global Village Martin Page 2002\nMr Page has written what could be the most enjoyable book on the\nhistory of Portugal. The paperback edition is available for about $25 in\nPortugal. Amazon has a new paperback edition for $242.48 (outrageous\nprice). Amazon also has a kindle version for $12.94\nAll our expeditions will start and finish in Porto (OPO airport code) we suggest you fly into either Lisbon or Porto and leave from either Oporto or Lisbon. If you fly into Lisbon you can either fly or take the train to Porto. The train connections between Lisbon and Porto are very good.\nGeneral travel information links:\nPortugal travel information:\nA general search engine providing links\nOffical tourist site for all of Portugal\nOffical site for Porto tourist information\nThe Portuguese airline site\nSite for all of Portuguese commercial airports. Gives names of all airlines serving the airport.\nOn-line news service\nTemperature conversations from Celsius to Fahrenheit\nCheck the time difference in Portugal or Spain. Portugal is on GMT but Spain is one hour ahead.\nCurrency exchange rates\nWhat is included in the expedition package\nExperienced professional guides.\nIndividual Expedition kayaks, paddles, life jackets, wet skirts and paddling jackets.\nTandem kayaks are available on request.\nSupport vehicle for entire expedition. All our luggage is carried from accommodation to accommodation. You will not need to carry anything in the kayaks except a little money and your camera.\nA small dry bag for personal items needed while on the water.\nTransfers to the launch sites.\nAll meals from dinner on the first day to our farewell dinner on the last night. House wine or bottled water is included. You may select fine wines from the wine list.\nBed and breakfast or quinta accommodations in double bed rooms for the nights.\nBottled drinking mineral water.\nPersonal incidental purchases (souvenirs etc.)\nEntrance fees for museums\nTips other than meals and hotels\nTravel to the pick up sites (flights, trains etc.)\nTravel to train station on first day is not included but of course the train is included.\nWhat if I get sick or have an accident?\nKayaking has some inherent risks. Sickness and accidents can spoil an otherwise enjoyable time and really wreck a budget. Therefore, you must have a comprehensive travel insurance policy. You must be covered against loss of baggage, illness, accidents, trip cancellation and the very unlikely necessity, of emergency medical evacuation. Please revise the claim process and inform us of the contact information when you sign on this expedition.\nHow much money should I bring?\nRemember that you must pay the balance of your trip cost in cash. We are usually working in a cash economy where we must pay everything in cash or suffer much more elevated costs. This is the reason we need to be paid in cash. If you absolutely must pay by bank transfer we will provide our bank details and you will have to pay a 23% vat that will be added on your invoice.\nIf you are traveling from outside the Euro Zone it is usually best to buy Euros before you arrive in Europe. In our travel information links, you will find sites for ATM machines that accept VISA and Master Cards. Generally the ATM machines have a daily limit of 300 Euros. Most international airports have exchange banks. Traveler checks are not a good idea as they must be changed at banks and the process is very time consuming.\nAfter you pay the fees for the expedition in cash, that includes B&B and all regularly scheduled meals, you will only need money for personal purchases such as gifts and tips. Tips at restaurants are included. If you have enjoyed your tour the guides greatly appreciate your tips. When I'm asked about amount for tipping staff I say 10% of the tour price and this should be given to Rui and he will divide the total amount between the guides.\nWhat will the weather be like?\nDays in Portugal this time of year are usually sunny and comfortably warm and are similar to Atlanta, Georgia. 23 C (73 F) daytime, 14C (57F) nighttime. Evenings are cool but not cold. Rain is unpredictable but not common. A fleece sweater is suitable for cooler evenings. However, you should also be prepared for any cooler than usual weather. Having a windbreaker handy is always a good idea.\nThe electrical current in Portugal is 220 V 60 HZ. Most telephone, camera and computer chargers are prepared for 100-240V and 50-60HZ. If this is the case for your electronics then all you need to do is have a European round two prong plug adapter. If you plan to use your telephone you will want to ask your provider to set you up for roaming. Be sure to ask about the charges and be aware that voice connection is not the same as data connection. If you plan to use your smart phone for email you might find the charges are very high. A way to avoid this is to disable your phone date connection and only use Wi-Fi where it is available along our route. Most days we have Wi-Fi but not always.\nFRIEND US ON: facebook.com/DouroKayakExpeditions"", 'Paddle sports are a favourite outdoor activity. There’s a lot of fun to be had boating, but there’s also the potential for things to go fatally wrong.\nExperienced paddlers can recognise hazards early on, and they have the knowledge, paddling skill, and know the safety and rescue practices to remain out of harm’s way. With experience and skill development, comes good judgment, and seasoned paddlers know when and where it’s safe to get in the water.\nBut if you’re just starting out kayaking, there’s a lot of hazards that could catch you off guard. So, with this in mind, we’ve compiled our ten golden rules for safe kayaking.\n1. Check Weather Reports Frequently\nDuring the lead up to your trip, check the weather several times, and even check the weather during your trip if possible. Smartphones make this super easy to do.\nHeavy rainfall upstream from your location can cause sudden surges in river levels and cause lazy, meandering rivers to become raging torrents in a short time. Fast moving water carries extreme risk, and the waves, eddies and strong currents require advanced training to navigate.\nIf you’re on a lake, even a slight pick up in wind can make paddling extremely difficult and even lead to capsizing.\n2. Remember Safe Boarding Practices\nBefore you even get out on the water, you need to get into the kayak safely. Always keep three points of contact with the boat. That could mean placing one foot in the cockpit while holding onto the rim with both hands.\nYou should also move slowly and crouch down to keep your weight low to balance easily.\nIf you’re riding a tandem kayak, the heavier person should go at the stern (back). The rear paddler should get seated first while the front passenger (riding at the bow) steadies the boat. The rear passenger can then steady the boat while the front passenger gets in. When you exit, reverse the procedure with the stern passenger getting out last.\n3. Remain Vigilant to the Changing Dangers\nYou should always be aware of your surroundings and be on the lookout for hazards developing. If you’re sharing the waterway with larger boats, be cautious that they may not be able to see you. Larger vessels travel at great speeds, so you need to stay alert and be ready to move out of the way at the drop of a hat.\nRiver paddling conditions can also change rapidly. What was a slow, gently winding river can quickly gather pace as you approach rapids or waterfalls.\n4. Dress for the Water Temperature\nWearing weather-appropriate clothes that are comfortable to move in and warm even when wet is common sense, but you’d be surprised how many first-time paddlers show up in a cotton hoody. Experienced paddlers know to dress for the environment, and more so, how to dress for the temperature of the water – even if you don’t capsize, you’ll always get a little wet.\nA leisurely paddle on a sunny day may only call for a short sleeve shirt, shorts, sun hat, and sunglasses – a pretty relaxed outfit. However, cold water paddling will require a wetsuit and maybe even a wind and waterproof jacket.\nYou should also choose suitable footwear that will allow you safe passage to the water edge and will stay on in the event of a swim. This rules out sandals – they’re okay for the beach but no good for kayaking. A comfortable pair of running shoes will do the job.\n5. Pack Maps and Guides Even if You’re a Local\nTrip planning 101: check in on your local paddling community websites, notice boards, and other local paddlers for updates of water conditions and possible hazards before setting off.\nAlso, plan your route before you go and be sure to check your whereabouts often. Even if you know this particular stretch of backcountry like the back of your hand, bring along a map and guide book for the area.\n6. Prepare for Poor Visibility\nKayaking at dusk or dawn or in foggy conditions will require you to take it slow and be extra careful to avoid mishaps. Depending on the severity of the conditions, you may want to rest on the shore to prevent a collision or getting lost.\nWearing bright, reflective clothing will help you remain visible.\nIt’s also wise to carry a flashlight as well as a whistle or small air horn in the stowaway compartments of your kayak (along with the rest of your safety essentials). By doing so, you can signal other boats that may not have seen you, or call for help if you find yourself in a tricky situation.\n7. No Drugs or Alcohol\nDon’t paddle under the influence of alcohol or drugs. PERIOD.\nIt may be fun to chill with a few beers at lunch or while paddling, but it’s not wise, and it puts you and your group in unnecessary danger. Just wait until the end of the day when you’re back in dry land to start the celebrations.\n8. Your PFD Is Your Insurance\nMany people pack their life jacket on the boat in case of an emergency. But guess what? If you don’t wear it, it doesn’t work. Kayaks and canoes are easily tipped, and you can’t rely on being able to swim or re-enter your boat – it could be swept away in rough conditions.\nAlways wear your personal flotation device (PFD) even if you think the water is too shallow to drown. Depths can be deceiving, and conditions can change quickly.\nWhen you drive, you wear a seatbelt. When you kayak, you wear a life jacket. Form the habit! And while we’re lecturing, you should also ensure your PFD is a snug fit and has the right amount of buoyancy for your weight.\n9. Always Be Ready For a Capsize\nDo you know what to do if you capsize? Are you capable of emptying your kayak and re-entering in deep water? You need to assess your current skill level and even take a course on rescue procedures if you feel the need to upskill.\nTipping is never a pleasant experience. The water is likely to be cold, and you may even swallow a little. You may also be in minor shock or injured. At such times, it’s important to remain calm and remember your training. By practising capsizing and getting back in your boat in a safe environment, you’ll be able to do so out on the open water confidently.\n10. Know How to Keep Warm in Cold Water\nIf you capsize in cold water and lose your boat and the shore is too far away to swim, you should try to preserve your body heat. Hypothermia is a very real risk.\nIf you’re by yourself, cross your arms across your chest and then bring your legs up into your chest, so that you’re in a ball.\nIf you’re in a group, huddle together to preserve warmth.\nWrapping It Up\nEvery paddler is responsible for their own safety. Even experienced kayakers need to take time out to remind themselves of basic safety. Accidents can and do happen. Remaining vigilant and always being prepared could just save a life.\nI hope you’ve found our list of kayak safety tips useful. If you’ve got any of your own, please share them in the comments below. Safe paddling!']"	['<urn:uuid:f45bb7da-5fab-4695-9f18-07bb0e5a64a7>', '<urn:uuid:39751eba-d883-46c6-a673-4df347920d00>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	17	78	2640
55	what things should do before throwing away old computer phone electronic device	When a device reaches the end of its lifecycle or is broken, it's crucial to have proper data destruction procedures and a device decommissioning process. Simply throwing devices in the trash is dangerous as cyber attackers could retrieve sensitive data. Instead, devices should be scrubbed of any sensitive data, and professional computer recyclers should be used as they have the proper tools and knowledge to ensure all usable data is properly removed.	"[""Securing your sensitive information encompasses practicing safe internet browsing habits and running a quality antivirus program on your business and personal computers, whether they are desktops or laptops. Now, failing to recognize the vulnerabilities posed by the vast quantity of internet-connected devices can leave you trying to play catch up.\nThe phrase Internet of Things (IoT) has been coined to better describe what used to be the purview of just computers. Everything from the refrigerator in your employee break room to your company vehicles and even the coffee maker can likely be connected to the internet, and anything with an internet connection is a potential threat vector. Managing IoT device security vulnerabilities can be a full-time job, so we've compiled some best practices to help ensure your enterprise is better protected.\nSecuring Your Devices\nInstall Patches and Updates\nProbably the most effective step you can take when addressing IoT device security vulnerabilities is to ensure that any connected devices are regularly scanning for and installing updates and patches. As the manufacturer or developer discovers and addresses bugs or security vulnerabilities, they push out fixes for them in software and operating system updates. Failing to check for and install available updates is the single biggest cause of device compromise.\nUtilize Encryption and Require Credentialed Access\nSecondly, install passwords and encryption on any device that will accept it. This doesn't help manage your WiFi-capable office thermostat, but this step can prove invaluable for tablets, e-readers, cell phones, and other similar devices. You should also discourage using the same password for multiple devices or user accounts. This helps to protect you from the loss or theft of a device, leading to a more devastating compromise of your business systems by protecting the data on the device behind a password and encryption.\nMonitor Network Traffic\nAny of these steps are better than being unprepared, but the reality is that you can't fully protect any device or network. You should invest in a robust scanning tool to monitor network access and verify that the behavior of all connected devices is appropriate. If suspicious or outright malicious activity is discovered, the program should escalate that alert to the appropriate IT personnel for review and response. This leads us to our next point, which is that you can't monitor what you aren't aware of.\nKeeps Tabs on All IoT Devices\nWe also recommend recording and logging all IoT devices on a spreadsheet. This is helpful for accountability purposes and allows you to revisit their necessity as a part of your regular risk assessment process. If a device isn't being used or is proving to be inferior to another non-internet-connected device, then you may be better off shifting away from that device entirely.\nPlan for End of Lifecycle\nThis brings us to our next item, which is the most overlooked of all, at least in our experience. When a device reaches the end of its lifecycle or is broken, how are you disposing of it? Do you just toss it in the trash and try to put it out of your mind? That's a gift for a cyber attacker with the funds to pay someone to sift through your garbage or the patience to do it themselves.\nHaving data destruction procedures and a device decommissioning process can prove invaluable when trying to mitigate IoT device security vulnerabilities. Scrubbing devices of any sensitive data is a must, and professional computer recyclers have the tools and the knowledge to make sure that you're keeping all usable data.""]"	['<urn:uuid:f2246bb6-ee7e-49a5-a4d5-508445588335>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	12	72	587
56	formal organization structure characteristics and potential emerging psychosocial hazards workplace	Formal organizations are characterized by complex normative behaviors, official procedures, division of labor, and hierarchic systems. These structural characteristics can lead to emerging psychosocial hazards, particularly through new forms of employment, work intensification, and job insecurity, which can potentially cause psychological or physical harm to workers.	['Types of organisation\n|Types of organisation|\n|Methods and techniques|\nThe basic types of organisation are:\n- formal organisation which has complex, normative action and behaviour patterns specific to organization members connected with official procedures, division of labour, hierarchic system and communication methods. Features: official, impersonal, hierarchic, functional specialization, formalization.\n- informal organisation is mostly build from personal and social relationships which will not be determine by regulations or status. Features: action flexibility, direct contact, unlimited duration.\nOrganizations can also be divided into:\n- voluntary organisations – people can voluntarily join or leave them, for instance: foundations, committees, associations,\n- compulsory organisations – separate their members form the society and strictly control adherence to specific behaviour, for instance: prisoners, patients of mental hospitals, soldiers,\n- utilitarian organisations – people join its hierarchic structure because of certain intentions or the future profits which may receive, for instance: companies, education systems, trade unions.\nOrganisation as a system\nOrganisation is the system made up of people and technical resources, materials, whose interaction is characterized by:\n- presence of clearly defined goals of the system,\n- correspondence between the structure and functions of the system,\n- interdependence of the system and its environment,\n- interdependence between system elements,\n- autonomy to decide and direct own actions.\nThe term “organization” comes from Greek word organizo which means to create the harmonious and orderly whole. The organization is formed and developed by people who cooperate in order to achieve established goals. The basic organization components are: people, aims and resources. Organizations with help of employees (people) try to accomplish specific aims, using various resources (strengths and weaknesses of the organization). The organization as specific system, operates in certain environment.\nMeaning of the term organisation\n- As an attribute - special kind of relationship between various elements to each other and to a whole, based on the fact that all parts acts to achieve goals of a whole,\n- As a thing - concrete object having above mentioned attributes (eg. team of people, company, research institute),\n- As a function - activity based on the ability to select, acquire and transform various resources and organizational units themselves towards achieving particular goal.\nCharacteristics of organisations\nEach organization has:\n- common aim specific for whole organization and its employees,\n- border which separates organization from surroundings,\n- leadership which regulates relationships between members and organization structure,\n- organizational culture – values, standards and rules which regulate organization members behaviour, for instance: joining, participation and leaving the organization,\n- organizational structure – internal system and arrangement of organization components,\n- cooperation between certain organization members and organization components for achieving goals.\nAbove mentioned features determine type of organisation and dependencies between elements of complex organizational system.\n- Barnard, C. I., & Thompson, K. (2003). Organization and management: Selected papers (Vol. 7). Psychology Press.\n- Garvin, D. A. (1998). The processes of organization and management. MIT Sloan Management Review, 39(4), 33.\n- Kast, R. E., & Rosenzweig, J. E. (1974). Organization and management. New York.\n- Likert, R. (1967). The human organization: its management and values.\n- Milgrom, P., & Roberts, J. (1992). Organization and management. Englewood Cliffs.\nAuthor: Krzysztof Wozniak', 'Various definitions have been given to the term ‘psychosocial hazards’, such as the following:\n- “… interactions between and among work environment, job content, organisational conditions and workers’ capacities, needs, culture, personal extra-job considerations that may, through perceptions and experience influence health, work performance and job satisfaction”\n- “… aspects of the work environment and the way that work is organised that are associated with a negative impact on mental health and/or physical injury or illness. When psychosocial hazards are not effectively managed, they can negatively impact on organisational measures including productivity, absenteeism and turnover.”\nMore simply, psychosocial hazards are: “those aspects of the design and management of work, and its social and organisational contexts that have the potential for causing psychological or physical harm”.\nThe UK’s Health and Safety Executive explained in 2006 the impact that psychosocial risk factors have on ‘frame of mind’ and the potentially higher incidences of practising unsafe behaviours in the workplace.\n‘Psychosocial’ refers to the inter-relationships between individuals’ thoughts and behaviours and their social environment.\nThis term can be interpreted differently outside of the OSH world and can often refer to social environments such as family, socio-economic status and level of education. In the OSH field, psychosocial hazards refer only to hazards created by work and the work environment.\nWork-related stress is the most common pathway from poorly managed psychosocial risk to ill-health.\nEmerging psychosocial hazards\nAn emerging OSH risk is any occupational risk that is both new and increasing.\nThe European Agency for Safety and Health at Work set up the European Risk Observatory, which identified and explored emerging OSH risks by carrying out a survey and analysing scientific literature.\nThe 10 most important emerging psychosocial risks identified in the survey were divided into five categories as follows:\n- new forms of employment\n- contracts and job insecurity\n- OSH risks for the ageing workforce\n- work intensification\n- high emotional demands at work\n- poor work–life balance.\nWhat’s the issue?\nPsychosocial hazards including work-related stress are referred to indirectly in many national legal frameworks, but very few countries have specific laws which deal with these hazards in the workplace.\nThe concept of ‘psychosocial hazards’ is difficult to understand as they represent workers’ perceptions and experiences, reflecting many considerations in the work environment.\nPsychosocial hazards are a major occupational health concern and are associated with serious economic implications for society and all types of organisations, irrespective of size and sector. Organisations can manage psychosocial hazards proactively, using the same methods already used to manage OSH.\nThere is a wealth of terminology used around psychosocial hazards:\n|Psychologically healthy and safe workplace||A workplace that promotes workers’ psychological wellbeing and actively works to prevent harm to worker psychological health, including in negligent, reckless or intentional ways||National Standard of Canada on Psychological Health and Safety in the Workplace|\n|Psychosocial||Relating to the interaction between social and psychological factors||Dictionary.com|\n|Psychosocial hazard||An occupational hazard that affects the psychosocial wellbeing of workers, including their ability to participate with other people in a work environment.||Definitions.net|\n|Psychosocial risk||The likelihood of factors arising from poor work design, organisation and management, combined with poor social context of work, that could result in a negative effect on workers’ health including psychological, physical or social issues such as work-related stress, depression and burnout.||IOSH|\n|Psychosocial risk factors||Organisational factors that have an impact on the psychological safety and health of workers. These factors include the way work is carried out and the context in which work occurs. Psychosocial risk factors can affect workers’ mental responses to work and cause mental health problems.||Canadian Centre for Occupational Health and Safety|\n|Psychosocial risk management system||An applied method to reduce the impacts of risks to psychosocial hazards in the workplace.||IOSH|\n|Psychosocial safety climate||Psychosocial safety climate (PSC) is defined as shared perceptions of organisational policies, practices and procedures for the protection of worker psychological health and safety, that stem largely from management practices. PSC theory extends the Job Demands-Resources (JD-R) framework and proposes that organisation-level PSC determines work conditions and subsequently, psychological health problems and work engagement||\nNational Library of Medicine\nWhen working conditions and human factors are all in balance, work can create a feeling of:\n- satisfaction or fulfilment\n- physically and mentally fitness and health.\nPsychosocial hazards or factors are aspects in the design or management of work that increase the risk of an adverse impact leading to work-related stress, exacerbated non-work-related stress or affect individual workers’ health and wellbeing.\nA stress response is a physical, mental and emotional reaction that occurs when a worker’s perceptions of the demands of their work exceed their ability or resources to cope. It’s important to remember that stress itself does not constitute a physical or psychological injury.\nWorkers are likely to be exposed to a combination of psychosocial hazards, some of which are always present, while others only occasionally. The terms ‘psychosocial hazard’ and ‘psychosocial risk’ are sometimes used interchangeably, and stress is often defined as a psychosocial hazard rather than a consequence of the hazard.\nIn the OSH profession a hazard is the potential for an agent, process or situation to cause harm or negative health effects to a person at work. Risk is the likelihood or probability that a person will be harmed or experience negative health effects if exposed to a hazard.\nPsychosocial hazards are aspects of work (eg, lack of autonomy, long working hours) which can affect workers’ emotions, behaviours, biochemical and neuro-hormonal reactions. The risk is the likelihood or probability that a person will be harmed or experience negative physical or mental health effects from being exposed to a psychosocial hazard.\nThe relationship between hazard and risk is exposure, whether immediate or long-term. In this context, it includes both physical and psychological outcomes.']	['<urn:uuid:1ac65655-a101-4638-8031-3a5a1ab8508b>', '<urn:uuid:c6237bfa-66a1-46d5-ace0-51f6a357d149>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	10	46	1483
57	cost impact invasive species usa	The estimated damage and control costs of aquatic and terrestrial invasive species in the United States amount to more than $137 billion annually. This cost exceeds the combined total of all other natural disasters.	['Seaplanes and float planes have long been used on Lake Whatcom and Lake Samish, by visitors and residents alike. Seaplane pilots are helping to protect these local lakes from the threat of aquatic invasive species by following these required procedures, developed in cooperation with the Washington Seaplane Pilots Association, before using their seaplane on Lake Whatcom or Lake Samish:\n- Enter into a Cooperative Agreement with the City of Bellingham stating that you will follow these procedures.\n- View one of these seaplane pilot training videos and take the training course/quiz: How to Stop the Spread of Invasive Plants and Animals by Seaplane (AOPA) or Don’t Move a Mussel – Seaplanes (100th Meridian Initiative).\n- Carry a copy of the Cooperative Agreement and your AOPA certificate or 100th Meridian Certificate on board and make it available for presentation in the case of a spot check by inspection staff.\n- Pilots/owners with seaplanes based at Lake Whatcom or Lake Samish shall make an appointment with Lake Whatcom AIS inspection staff to get their plane inspected and permitted by calling (360) 778-7975 or sending an email to email@example.com. The pilot/owner may also view and complete the Whatcom AIS Awareness Course to receive a $10 discount for the purchase of an Annual AIS Permit ($60). The permit may be affixed to the plane or kept with the inspection form in the cockpit.\nTo assist us in evaluating the effectiveness of this agreement we are requesting that seaplane pilots voluntarily submit the following information for each landing on Lake Whatcom or Lake Samish in a given year: 1) date of landing, 2) origin of the flight, and 3) other lakes visited on the trip.\nPeriodically submit this information (at least annually) to Teagan Ward at firstname.lastname@example.org or 2221 Pacific Street, Bellingham, WA 98229.\nAdditional requirements when operating from infested waters:\nFollow these procedures when traveling from waters known to be infested with aquatic invasive species:\n- Prior to entering the seaplane, remove all aquatic plants (or attached mussels, snails, or other animals) from the exterior surfaces of floats, wires, cables and rudders and remove all water from floats, wheel wells and any other compartments or areas of the aircraft that could contain or maintain raw water.\n- Prior to takeoff, raise and lower the rudders several times to remove aquatic plants, avoid taxiing through heavy surface growths of aquatic plants and make sure all floats remain as dry internally as possible during takeoff.\n- After takeoff, raise and lower rudders several times to free any remaining aquatic plants while over the departing water body or over dry land. If aquatic plants persist and are still visible on floats, cables or rudders, return to the same water body and physically remove them.\n- During storage and/or moorage, remove the aircraft from the water whenever practical to better facilitate self-inspection, drainage, plant removal, cleaning and drying.\n- If you are visiting waters infested with invasive mussels, you should do the following before leaving the infested waterbody:\n- Decontaminate any surfaces that have come in contact with the water using 140 degree, high pressure water.\n- Where 140 degree water is not available, you should apply a 20 percent bleach solution, or other effective substitute, to the float bilges.\nDid you know?\nEstimated damage and control costs of aquatic and terrestrial invasive species in the U.S. alone amount to more than $137 billion annually. This is more than the combined total of all other natural disasters.\nMonitoring Lake Whatcom\nPulling up a plankton tow to collect a sample to check for mussel veligers (larvae)']	['<urn:uuid:3ca0f53b-978a-429c-b068-6c2050218a29>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	5	34	597
58	As a professional woodworker concerned about workplace safety, I'm curious about the protective measures required when operating power tools versus heavy construction equipment - what noise protection guidelines apply in each case?	For both woodworking and construction equipment, hearing protection is required when noise levels exceed 85 decibels over an 8-hour shift. When using woodworking machinery, cheap ear plugs or high-quality headphones that block out noise must be used. Similarly, for construction equipment and other workplaces, if you need to raise your voice to speak to someone three feet away, noise levels likely exceed 85 decibels and require protection. The louder the noise, the less exposure time needed to cause damage. Protection can include earplugs and earmuffs, and other mitigation approaches like keeping machinery properly lubricated, choosing low-noise tools, placing barriers between noise sources and workers, reducing time in noisy areas, and providing quiet relief areas.	['Millions of people have enjoyed woodworking for centuries. Some people are even able to turn that hobby into a lucrative profession. If you’re interested in woodworking, you can find success by using the below advice.\nUse a pre-stain wood conditioner before staining the wood. These conditioners will make the finished project look better. It will also help to improve the appearance of the wood itself, as wood has a natural grain and the conditioner fills it in to some degree.\nMake sand blocks that you can reuse. Start by cutting a few blocks from a scrap piece of plywood for all the sandpaper grits you need to use. Be sure that they’re only around 2.5″ x 4.75″. Use some spray adhesive on your blocks and the cork tile. Start by sticking a block to the cork before cutting the cork with a utility knife. Put glue on the sandpaper and apply to the cork. Fnally, make sure you cut sandpaper flush with the cork, and label every block.\nThe height of the work bench you use affects the quality of your finished product. If you wish to determine your ideal workbench height, measure how far it is from the floor to the middle of your waist. This is the perfect workbench height.\nIf you’re working on a woodworking project that calls for stain, always stir the stain first. Some of the ingredients in these stains and finishes tend to settle to the bottom over time. However, shaking can cause bubbling, which keeps it from mixing well. Be sure you stir it up until your mix is dispersed correctly and in a smooth manner.\nAvoid using your fingers as guides while cutting narrow pieces of wood. Use a thin piece of wood instead. Use it to guide the wood through the saw. This will help you keep your fingers, which you need to be a great woodworker.\nAlways do a dry fit prior to gluing two pieces of wood together. You don’t want any surprises when you do actually have to apply the glue. You’ll be prepared. A dry-fit helps you figure out where to put things.\nUnderstand the properties of the wood you’re using. Keep in mind that all wood has different characteristics. Different types of wood will react differently to various stains. Splintering varies among cuts. You will also encounter a lot of different grains. All these factors will be taken into account when working on your project.\nMake sure to properly protect your ears. Woodworking can be very loud. Exposure over a long period of time can be bad on your hearing. Get some cheap ear plugs or high-quality headphones that block out noise. No matter what you choose, be sure that you use them whenever you operate machinery.\nWhether you’ve built a lot of things or are new to woodworking, it’s something that can be made a lot more simple if you follow the above tips. As you start using the tips, you will discover they transform your works into masterpieces. So, the research you’ve just done will go a long way in improving your skills.', 'October is National Protect Your Hearing Month\ninformation provided by the Centers for Disease Control and Prevention\nSPRINGFIELD, Ill. (October 25, 2019) – The Centers for Disease Control and Prevention estimates that 22 million workers are exposed to potentially damaging noise at work each year. Much can be done to mitigate damage to people’s hearing, which is the message the Illinois Department of Labor (IDOL) is stressing during October—National Protect Your Hearing Month.\nExposure to loud noise can kill the nerve endings in the inner ear, causing permanent hearing damage. While devices such as hearing aids can assist, nothing can reverse hearing damage once it occurs. Noise-induced hearing damage affects a person’s ability to hear high frequency sounds and understand speech.\nBut how loud is too loud? According to the Occupational Safety and Health Administration (OSHA), if you need to raise your voice to speak to someone three feet away, noise levels may be exceeding 85 decibels. Long or repeated exposure to sounds at or above 85 decibels can cause hearing loss.\nSome situations pose obvious dangers for hearing damage. Working around jet engines, operating a jack hammer, or using a chainsaw for long periods all qualify as extreme noise environments. But plenty of other less obvious workplaces can pose a risk as well.\n“We had never been concerned about noise levels at our jobsites because we always felt the levels would be below the permissible exposure level,” said Stephennie Brumley, Health and Safety Officer for Andrews Engineering in Springfield.\nHowever, about a year ago, Andrew Engineering began working with Illinois OSHA—a division of IDOL—in pursuit of the top national workplace safety ranking, which it achieved earlier this month. Illinois OSHA’s On-Site Consultation Program staff explained that as part of the process, testing would need to be done to ensure that jobsite noise levels were below allowable levels.\nAfter completing noise testing, Andrews Engineering found peak levels were high enough to warrant a hearing conservation program for its field staff, something Brumley says the company has now instituted.\nWhile specially calibrated instruments are used to do OSHA testing for noise levels, a slightly less sophisticated device is easily available for anyone curious about the noise levels around them. The National Institute for Occupational Safety and Health offers a free download of a sound level meter for any IOS-operating device.\nThe louder the noise, the less time you need to be exposed to it to suffer hearing damage. A worker who is consistently exposed to 85 decibels for an eight-hour shift, should be provided some form of hearing protection or mitigation.\nNoise mitigation can take several forms and can often be accomplished without great expense or engineering. Some simple steps include keeping machinery properly lubricated, choosing low-noise tools and machinery, and placing barriers between the noise source and workers. Employers can also reduce the amount of time a worker spends in a noisy area and provide quiet areas for relief from hazardous noise levels.\nIf it is not possible to reduce noise to safe levels, hearing protection devices such as earplugs and earmuffs should be used to protect workers.\nMore information is available on the Illinois OSHA website.']	['<urn:uuid:0376f212-7656-42d8-bb4b-40e87297bcaf>', '<urn:uuid:b7c2e83e-f325-4ff2-9fcf-bab5e16cf218>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	32	114	1046
59	profitable environment friendly construction techniques features	Green buildings incorporate various profitable and environmentally-friendly features. These include smart technologies like window automation for temperature control, on-demand ventilation, solar arrays for power generation, and geo-exchange systems for heating and cooling. The upfront investment in these features typically pays for itself in seven years, increases property value by 4%, and reduces maintenance costs by 20%. These buildings also use highly efficient materials like triple-glazed glass and high levels of insulation, while incorporating features like natural lighting and green walls to improve air quality.	"['Volume 4, Issue 4\nCheck out our Buildings: What’s new? page for the latest details on:\n- ENERGY STAR Portfolio Manager Updates\n- Canadian ENERGY STAR Certification for Buildings\n- New Resources and more!\n- Okanagan College green facilities recognized with Tommie awards\n- Retrofit now to save energy and keep tenants\n- Eolv1 building to show that net positive buildings are a win-win\n- Let us know what you think\nGoing green pays off for Okanagan College\nTwo green building projects at Okanagan College in Kelowna, B.C., took top honours at the 2017 Tommie awards held by the Canadian Home Builders Association. The College’s new Trades Complex was recognized with the Best Environmental Initiative in Construction award, and the Wilden Living Lab took home the FortisBC Award for Building Energy Efficiency.\nThe Tommie Awards showcase vision, talent and commitment in the Okanagan valley building industry and are awarded for a variety of building innovations, including environmental and energy efficiency initiatives. The Best Environmental Initiative in Construction category recognizes sustainability efforts such as recycling and waste minimization, carbon footprint reduction, and energy management and/or water efficiency. The FortisBC award can go to any facility that uses natural gas and shows innovative measures to increase energy efficiency.\nThe Trades Complex project involved a 10,000 square metre renovation and expansion of existing facilities. The existing space was retrofitted with environmental upgrades, and a three-storey learning space and workshops were added. The facility has incorporated numerous energy-efficient and sustainable elements in an effort to earn LEED Platinum certification and achieve net zero energy performance.\nThe building’s sustainable features include the following:\n- A connection with a neighbouring wastewater treatment plant to make use of the excess heat from its treated effluent as a source of energy\n- A large photovoltaic solar array that generates enough power to run 25 homes per year\n- Smart technologies such as window automation that regulates heating and cooling based on temperature and sun position\n- On-demand ventilation that significantly reduces energy use in workshops\nThe Wilden Living Lab is an innovative study project — the only one of its kind in North America — that allows students to monitor and compare the energy use in two homes with the same floor plan built to different energy-efficiency standards. The Home of Today was built to current code requirements and serves as a baseline, while the Home of Tomorrow will push the envelope by exceeding current code requirements and making use of leading-edge sustainable technologies. Both homes will be occupied by real families, and their energy use tracked in real time over a three-year period.\nTo be recognized in our community for two leading-edge environmental initiatives is very affirming, ""says Jim Hamilton, President of Okanagan College. We pride ourselves on having set and achieved high standards for sustainability. The Trades Complex and Living Lab demonstrate how we continue to achieve that standard.""\nBoth new construction and existing buildings can maximize their energy efficiency by adopting energy management best practices. These include meeting or exceeding energy codes, using an integrated design process, benchmarking their energy use and undertaking major energy retrofits where appropriate. What projects can we help you showcase?\nTo read the full article, visit: College’s green buildings earn gold.\nVisit the Tommie Awards website for the full list of winners and to learn more about the awards.\nRetrofitting your building makes plain business cents\nRetrofitting buildings makes economic sense, and not just because of lower operating costs: green buildings can fetch higher rents and retain occupants longer. Moreover, as the cost of carbon increases, more sustainable buildings will have a clear advantage in terms of reduced GHG emissions.\nA study of Bentall Kennedy’s North American real estate portfolio of more than 300 buildings found that environmentally friendly office properties commanded nearly 4 percent higher rent. Occupancy rates among their Canadian holdings were also 18.7 percent higher in environmentally certified buildings than in non-certified buildings. Study authors suggest that tenants in these buildings also tend to be happier and ""stickier"", thus reducing landlord leasing costs associated with tenant turnover.\nNet zero for buildings cannot happen overnight so retrofits are essential. Luckily, the building industry is well-equipped with both the knowledge and the technology to make net zero a reality. With tools such as energy benchmarking and a plethora of energy management best practices, building owners and managers can easily go beyond changing light bulbs to deep retrofits.\nIn addition, retrofitting aligns neatly with national and international climate change goals, namely that of reducing emissions from buildings and homes through a net zero goal by 2030. Acting now to improve energy efficiency and carry out other green retrofits will reduce emissions and attract/retain tenants in the short and long term.\nTo find out more about how to identify the most strategic energy-saving investments in your buildings read about energy benchmarking with ENERGY STAR Portfolio Manager.\nFor the full article, visit: The economic case for retrofitting buildings.\nNet-positive buildings are energizing office buildings in Canada\nConstruction is set to begin this month on Evolv1, expected to be one of the most energy-efficient office buildings in Canada. Its owners aim to show not only that net-zero energy is a feasible goal, but that net-positive buildings are also achievable — without paying a significant premium.\n""We want to disrupt the industry with what we’re going to build,"" says Adrian Conrad, Chief Operating Officer at the Cora Group, one of the building’s construction partners. Cora and the project’s other partners, Sustainable Waterloo Region and EY Canada, believe that Evolv1 will change the way offices are built.\nEvolv1 will provide a practical example of a highly sustainable building that is financially feasible, with an estimated cost of $35 million for a 10,000 square metre building. As a net positive energy building, it will generate more energy than it needs to operate. The excess power can be sold to the province’s electricity grid or used to power the building\'s 14 electric vehicle charging stations.\nIn addition to net positive performance, Evolv1’s owners also intend to pursue LEED Platinum certification. Some of the features to be built into the property that will help them achieve their goals include:\n- Highly-efficient materials such as triple-glazed glass and very high levels of insulation\n- Digitally-controlled LED lighting with occupancy and light level sensors\n- A geo-exchange system that extracts heat from the ground for winter heating and returns excess heat to the ground in the summer\n- Power generated from an array of about 1.5 acres of solar panels on the roof and carport\n- Direct access to the city’s light rail transit system to reduce the environmental impact of commuters\n- Natural light for every occupant\n- A three-storey green wall to improve air quality\nAlthough net-zero energy buildings are becoming more common, net-positive ones are still a rarity. ""This project is absolutely ground-breaking,"" says Tova Davidson, Executive Director of Sustainable Waterloo Region. ""If we think about a clean economy, this is a game-changer."" Evolv1 aims to prove that net-positive green buildings are not only a viable financial option but a winning one.\nTo make sure buildings perform as anticipated once built, building operators should make sure the new building is properly commissioned, and then benchmark its energy use with ENERGY STAR Portfolio Manager.\nFor the full article, visit: An office building so green it actually produces energy.\nDo you have a net-zero or net-positive energy project to share? Please contact us!\nLet us know what you think\nHeads Up: Building Energy Efficiency is published by Natural Resources Canada’s Office of Energy Efficiency and distributed monthly to 16,000 subscribers. Our goal is to deliver meaningful news and information about programs, services and events related to energy efficiency in commercial and institutional buildings and, as well, to share the success stories of organizations that have benefited from positive change. Help us spread the word by sending this link to your colleagues. We encourage you to subscribe to our sister publication that focuses on energy efficiency in industrial facilities, Heads Up CIPEC.\nWe welcome reader feedback and are always interested in your story ideas.\nClick here if you would like to unsubscribe.\n- Date Modified:', 'Benefits of green building\nBuildings are responsible for an enormous amount of global energy use, resource consumption and greenhouse gas emissions. As the demand for more sustainable building options increases, green construction is becoming increasingly profitable and desirable within the international construction market.\nGreen building is profitable & cost-effective\n- Upfront investment in green building makes properties more valuable, with an average expected increase in value of 4 percent. By virtue of lowered maintenance and energy costs the return on investment from green building is rapid: green retrofit projects are generally expected to pay for itself in just seven years.1\n- Green buildings reduce day-to-day costs year-over-year. LEED buildings report almost 20 percent lower maintenance costs than typical commercial buildings, and green building retrofit projects typically decrease operation costs by almost 10 percent in just one year.2, 3\n- Between 2015 and 2018, LEED-certified buildings in the United States are estimated to have $1.2 billion in energy savings, $149.5 million in water savings, $715.2 million in maintenance savings and $54.2 million in waste savings.4\n- By 2021, in the U.S., green activity is expected to grow, with those doing the majority of their projects green increasing from 32 percent to 45 percent. Client demands remains the top trigger driving the market and encouraging occupant health and well-being is the top social reason for building green.\nGreen buildings lower utility bills and benefit the environment\n- Buildings are positioned to have an enormous impact on the environment and climate change. At 41 percent of total U.S. energy consumption, buildings out-consume the industrial (30 percent) and transportation (29 percent) sectors.5\n- Buildings use about 14 percent of all potable water (15 trillion gallons per year), but water-efficiency efforts in green buildings are expected to reduce water use by 15 percent and save more than 10 percent in operating costs.6, 7 Retrofitting one out of every 100 American homes with water-efficient fixtures could avoid about 80,000 tons of greenhouse gas emissions, which is the equivalent of removing 15,000 cars from the road for one year.8\n- Standard building practices use and waste millions of tons of materials each year; green building uses fewer resources and minimizes waste. LEED projects are responsible for diverting more than 80 million tons of waste from landfills, and by 2030 that number is expected to grow to 540 million tons.9\n- In the United States alone, buildings account for almost 40 percent of national CO2 emissions and out-consume both the industrial and transportation sectors, but LEED-certified buildings have 34 percent lower CO2 emissions, consume 25 percent less energy and 11 percent less water, and have diverted more than 80 million tons of waste from landfills.10\n- The market is responding to these cost savings and environmental benefits at a dramatic rate. According to a Dodge Data & Analytics World Green Building Trends 2018 SmartMarket Report, global green building activity continues to rise and nearly half of survey respondents expect the majority of their projects in the next three years to be green buildings.11\n1 McGraw Hill Construction (2012). World Green Buildings Trends: Business Benefits Driving New and Retrofit Market Opportunities In Over 60 Countries.\n2 U.S. Department of Energy (2011). Re-Assessing Green Building Performance: A Post Occupancy Evaluation of 22 Buildings.\n3 McGraw-Hill Construction (2012). World Green Buildings Study. Accessed Nov. 29, 2012.\n4 Booz Allen Hamilton and the U.S. Green Building Council (2015). 2015 Green Building Economic Impact Study.\n5 National Trust for Historic Preservation (2011). The Greenest Building: Quantifying the Environmental Value of Building Reuse.\n6 U.S. Geological Survey (2000). 2000 data.\n7 McGraw Hill Construction (2010). Green Outlook 2011: Green Trends Driving Growth.\n8 U.S. Environmental Protection Agency. Green Building, Green Homes, Conserving Water. Water Use and Energy.\n9 Watson, Rob. Greenbiz Group (2011). Green Building and Market Impact Report.\n10 U.S. Department of Energy (2011). Re-Assessing Green Building Performance: A Post Occupancy Evaluation of 22 Buildings.\n11 Dodge Research and Analytics (2018). World Green Building Trends 2018 SmartMarket Report.']"	['<urn:uuid:a10ac64c-c198-4cd1-951e-31b014114d29>', '<urn:uuid:06b7e19a-ee3d-47e0-84e4-00e787b471a5>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	6	84	2030
60	wrist movement range healthy exercises	The wrist has several ranges of motion: flexion (80 degrees), extension (80 degrees), ulnar deviation (30 degrees), radial deviation (20 degrees), pronation (80 degrees), and supination (80 degrees). To prevent repetitive stress injuries, it's important to maintain these ranges through proper exercises and stretches. Regular stretching breaks should be taken every hour for 10 minutes, with 30-second micro-breaks every 10 minutes. Additionally, maintaining a natural wrist position (palm at 30-45 degree angle) and avoiding extreme joint flexion is crucial for wrist health.	"['Biomechanics is the term used to describe movement of the body. This section is a review of wrist and hand biomechanics. In order to better understand the biomechanics of the wrist and hand it is important to understand their anatomy. Please read the sections on wrist anatomy and hand anatomy before reading this section. It discusses the bones, ligaments, muscles and other structures that make up the wrist and hand.\nThe wrist joint is actually made up of many different joints. The end of the radius, the end of the ulna, and the eight carpal bones form these joints. The distal radioulnar joint, the radiocarpal joint, the ulnocarpal joint, the midcarpal joints and the intercarpal joints are the main joints that make up the wrist.\nThe distal radioulnar joint is the joint that is between the distal end of the radius and the distal end of the ulna. It allows the wrist movements of pronation and supination to occur. Supination is rotation of the forearm so that the palm is turned up. Pronation is rotation of the forearm so that the palm is turned down.\nFunctionally the radiocarpal, ulnocarpal, midcarpal and intercarpal joints work together to allow the wrist to bend (flex), straighten (extend), move inwards (ulnar deviation), and move outwards (radial deviation).\nWhat are the normal ranges of motion for the wrist?\nThe normal end ranges of motion for the wrist vary between individuals and between children and adults. The following are approximate end ranges of motion for the adult wrist:\nFlexion – 80 degrees\nExtension – 80 degrees\nUlnar deviation – 30 degrees\nRadial deviation – 20 degrees\nPronation – 80 degrees\nSupination – 80 degrees\nThere are 19 bones in the hand. There are 5 metacarpal bones and 14 phalangeal bones. The metacarpal bones are located in the palm. There is one for each of the fingers and one for the thumb. The phalangeal bones are small, slender bones located in the fingers and in the thumb. There are 3 phalangeal bones in each finger and 2 in the thumb.\nThe fingers and thumb are often referred to collectively as the “digits”. There are 14 small joints in the hand that allow for movement of the digits. The metacarpophalangeal (MCP) joints are the joints that allow for movement between the palm and each of the digits. The interphalangeal (IP) joints are the smaller joints between the phalangeal bones of each digit. The thumb has one IP joint and each finger has two, called the proximal and distal IP joints. The carpometacarpal (CMC) joints are between the top row of carpal bones and each of the fingers and the thumb. The thumb CMC joint is the most mobile of the CMC joints.\nThe normal end ranges of motion for the joints of the hand vary between individuals and between children and adults. The following are approximate end ranges of motion for the primary movements of these joints.\nWhat are the normal ranges of motion for the thumb?\nCMC flexion – 15 degrees\nCMC extension – 20 degrees\nCMC abduction* – 70 degrees\nMCP flexion – 50 degrees\nIP flexion – 80 degrees\n* Movement of the thumb away from the plane of the palm.\nWhat are the normal ranges of motion for the fingers?\nMCP flexion – 90 degrees\nMCP extension – 45 degrees\nProximal IP flexion – 100 degrees\nDistal IP flexion – 90 degrees\nMuscles control movements of the hand and wrist. The muscles that pronate and supinate the wrist are located deep in the forearm. The biceps muscle of the upper arm also supinates the forearm and wrist. The long muscles that flex the wrist and fingers originate from the inner aspect of the elbow (medial epicondyle). The long muscles that extend the wrist and fingers originate from the outer aspect of the elbow (lateral epicondyle). In addition to these long muscles, there are short muscles located in the hand that move the fingers and thumb.\nVisit Joint Pain Info for information on other joint injuries and problems.', ""Science, Tech, Math › Social Sciences How to Prevent Repetitive Stress Injuries to Your Wrist Share Flipboard Email Print Social Sciences Ergonomics Psychology Sociology Archaeology Economics Environment Maritime By Chris Adams Engineering Expert B.I.D, Industrial and Product Design, Auburn University Chris Adams is a human factors engineer who writes about ergonomics and has 11 years of experience in the field. our editorial process Chris Adams Updated February 26, 2018 Repetitive stress on the wrist can lead to a number of different injuries, like tendonitis, bursitis, and carpal tunnel syndrome. They all have similar symptoms, but most include wrist, hand, and arm pain. Although some conditions can have other primary causes, they are all aggravated by wrist overuse. With that in mind, here are the top 10 tips to prevent repetitive stress injuries of the wrist. 01 of 10 Stay Healthy Eugenio Marongiu / Getty Images Maintain a healthy body weight and a good cardiovascular system. An unhealthy body causes stress everywhere. Add that to any environmental stressors and you may have a problem. 02 of 10 Stay Flexible with Forearm and Wrist Stretches Studio CP / Getty Images Keep your wrist, arm, hand, and fingers strong. It is harder to overuse something if it is normally worked hard. Strengthen the muscles involved and increase flexibility through stretching. 03 of 10 Keep Your Hand in a Natural Position Evgeniy Skripnichenko / Getty Images Lay the outer part of your forearm on a hard surface. Let it rotate inward naturally. Keep your wrist straight. That is the natural wrist position. Notice that the palm is at a 30-45 degree angle and that the fingers are curled. Keep that position whenever possible. Flexing and twisting of the wrist cause all the tendons and nerves to rub over leverage points at the joints which can cause a lot of problems. 04 of 10 Set up an Ergonomic Work Station Mint Images / Getty Images Control the movement of your hand and fingers through muscle use, not tendon/ligament use. One big problem with typing on modern keyboards is the lack of strength needed to press a key. This causes you to simply start a motion of the finger and let momentum carry it through. This can cause minor hyperextensions and wear and tear on the tendons and nerves. Musicians are prone to this as well, due to the speeds they need to achieve. Developing strong, fast twitch muscles is a better alternative. 05 of 10 Take Breaks Gpointstudio / Getty Images Take regular breaks to relieve stress. Take this opportunity to stretch and increase blood flow. You should break for at least 10 minutes for every hour of continuous work with 30-second micro-breaks every 10 minutes. Performing a warm up and cool down stretch will help as well. 06 of 10 Change Positions JGI/Tom Grill / Getty Images Change your position and posture regularly. Change of position will call in different muscles, kind of like a relief pitcher, letting the first group rest. 07 of 10 Get a Good Grip Zave Smith / Getty Images Use a proper sized grip for your hand. Look at your natural wrist position again. Now bring your thumb and fingers together until they are separated by the width of two quarters. That is your grip size for holding things. That is your ideal grip for things like handrails or screw guns. Now continue to close your hand until the thumb overlays the first joint of your index finger. That is your grip size for manipulating things with your wrists, things like hammers, shovels or golf clubs. 08 of 10 Maintain Your Distance Hero Images / Getty Images When working with your hands keep them in the middle ground—not too far, but not too close to your body. This allows muscles in your arms, shoulders, and trunk to help share the load. It also keeps your joints in the middle of their range of motion, which increases blood flow and reduces the flex of tendons/ligaments/nerves over those leverage points at the joints. 09 of 10 Don't Go to Extremes Westend61 / Getty Images Do not flex your joints to the edges of your range of motion while working or driving. Most muscles cannot maintain control of the body at these extremes, which can lead to hyperextension and muscle pulls. It also flexes the tendons and nerves over those leverage points of the joints. 10 of 10 The Low Down CentralITAlliance / Getty Images Do not flex upward. The hand is designed to grip, so most muscle control and the joint range is aimed at a downward flex. There's less leverage on an upward flex, so the body has to work harder to move that way. The tendons and nerves also have harder leverage points to stretch over. Keep palms and fingers somewhere between flat and the grip position. Keep your typing and mouse click upstrokes as short as possible. Do not use the scroll wheel as that motion is almost entirely upward flexing.""]"	['<urn:uuid:a60c5f39-d089-43a3-b86c-d0ad91ffb77b>', '<urn:uuid:035352f5-6440-49e0-a698-a3b0c84772ed>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	5	82	1506
61	What does the IP protection rating measure in electronics, and how do military standards ensure these devices work in challenging environments?	The IP (Ingress Protection) rating uses two digits to measure protection against foreign objects and moisture in electrical equipment. The first digit indicates protection against solid objects (from 0-6, with 6 being dust-tight), while the second digit indicates moisture protection (from 0-8, with 8 being protected against continuous water immersion). Military standard MIL-STD-810 then provides comprehensive testing methods to verify equipment durability in various conditions, including exposure to water, dust, and environmental stresses through laboratory and field testing.	['International Standard IEC 60529 outlines an international classification system that describes the sealing characteristics of electrical equipment. The classification system defines the level of protection provided by enclosures to prevent the ingress of foreign objects and moisture into the electrical equipment. The classification system uses the IP code, or Ingress Protection code, to define the level of seal. The IP code uses a system of two numerical digits to define the level of both foreign object and moisture protection. Although the IP classification is primarily used for establishing the level of seal in electrical equipment, it is adapted herein for determining the level of seal in passive electrical components.\nDegrees of Protection (Foreign Bodies) 1st Digit\nThe first digit of the IP code indicates the degree of protection against solid foreign objects from entering the electrical device. The table below outlines the level of protection against foreign objects for each level.\nDegrees of Protection (Moisture) 2nd Digit\nThe second digit of the IP code indicates the degree of protection against the ingress of various forms of moisture (e.g. drip, spray, submersion, etc.) into the component. Tests to determine the level of protection are carried out with fresh water and do not take into account the use of solvents.\nDescription of Protection Level 1st Digit (Foreign Bodies)\n0 = Not protected 1 = Protected against solid foreign objects of 50 mm diameter and greater 2 = Protected against solid foreign objects of 12,5 mm diameter and greater 3 = Protected against solid foreign objects of 2,5 mm diameter and greater 4 = Protected against solid foreign objects of 1,0 mm diameter and greater 5 = Protected from the amount of dust that would interfere with normal operation 6 = Dust tight\nDescription of Protection Level 2nd Digit (Moisture)\n0 = Not protected 1 = Protected against vertically falling water drops 2 = Protected against vertically falling water drops when enclosure is tilted up to 15 ° 3 = Protected against water sprayed at an angle up to 60 ° on either side of the vertical 4 = Protected against water splashed against the component from any direction 5 = Protected against water projected in jets from any direction 6 = Protected against water projected in powerful jets from any direction 7 = Protected against temporary immersion in water 8 = Protected against continuous immersion in water, or as specified by the user\nMIL-STD-810 (MIL-STD (810F Method 512.4 Leakage)\nThe United States Military Standard referred to as MIL-STD-810, “Department of Defense Test Method Standard for Environmental Engineering Considerations and Laboratory Tests” emphasizes tailoring an equipment’s environmental design and test limits to the conditions that it will experience throughout its service life, and establishing chamber test methods that replicate the effects of environments on the equipment rather than imitating the environments themselves. The MIL-STD-810 test series are approved for use by all departments and agencies of the United States Department of Defense (DoD). Although prepared specifically for DoD applications, the standard may be tailored for commercial applications as well.\nScope of the standard\nThe military standard MIL-STD-810 test series addresses a broad range of environmental conditions that include: low pressure for altitude testing; exposure to high and low temperatures plus temperature shock (both operating and in storage); rain (including wind blown and freezing rain); humidity, fungus, salt fog for rust testing; sand and dust exposure; explosive atmosphere; leakage; acceleration; shock and transport shock (i.e., triangle/sine/square wave shocks); gunfire vibration; and random vibration. The environmental management and engineering processes described within MIL-STD-810 can be of enormous value to generate confidence in the environmental worthiness and overall durability of a system design. Still, there are limitations inherent in laboratory testing that make it imperative to use proper engineering judgment to extrapolate laboratory results to results that may be obtained under actual service conditions. In many cases, real-world environmental stresses (singularly or in combination) cannot be duplicated in test laboratories. Therefore, users should not assume that a system or component that passes laboratory testing also would pass field/fleet verification trials.\nLead standariazation agency\nThe military standard MIL-STD-810 series of test methods are issued by the United States Army’s Developmental Test Command who is chartered under the Defense Standardization Program (DSP) with maintaining the functional expertise and serving as the DoD-wide technical focal point for the standard. The current document revision (2009) is Revision G (i.e. MIL-STD-810G) which was issued on October 31, 2008. It superseded Revision F (MIL-STD 810F) released on January 1, 2000 and which was last updated on May 5, 2003.\nTest methods and examples\nThe MIL-STD-810 test series contains environmental laboratory test methods that are applied using specific test tailoring guidelines described within the standard. Note that test methods are not to be called out in a “blanket” fashion nor applied as unalterable routines, but are to be selected and tailored to generate the most relevant test data possible, and incorporated into a system’s final design specification, as specified by the Procuring Agency. The selected test methods are designed to simulate specific environmental conditions anticipated to be encountered over the life of the system.', 'The United States military designed a series of tests, MIL-STD-810G, to test its equipment limits in a variety of conditions under which it is to be used. In this post, we’ll take a closer look at MIL-STD-810G, including its purpose, how it works, and best practices for ensuring compliance.\nA Definition of MIL-STD-810G\nKnown as the Environmental Engineering Considerations and Laboratory Tests, MIL-STD-810 focuses on tailoring military equipment’s environmental design and test limits to the conditions it will undergo during its service life. These conditions range from different environments to transportation conditions such as shocks. The tests themselves also vary depending on the nature, size, and weight of the equipment being tested.\nOver time, MIL-STD-810 has evolved and been updated. MIL-STD-810G is the latest revision and is a comprehensive document including several corrections, significant changes, and additions to MIL-STD-810F that include five new test methods. While MIL-STD-810 is specifically for military equipment and applications, the standard often is used for commercial products, too.\nPurpose of MIL-STD-810G\nThe military must have a formal method of testing equipment under various environmental conditions in order to keep service members safe and to prolong the service life of the equipment itself. They also need equipment designers, engineers, and manufacturers to understand exactly the type of environments and conditions military equipment must be able to withstand.\nAs such, specifications and standards have been developed over the past 70 years that have evolved to become MIL-STD-810G. Over time, the standards have come to include more specific statements for shock and vibration tests that mimic real-world operating environments and that allow for the creation of test chambers to simulate conditions encountered during a product’s service life. For example, Test Method 527 of MIL-STD-810G calls for the use of multiple vibration exciters for performing multi-axis shaking to excite test article resonances and simulate real-world vibrations. This method combines three distinct tests on the x, y, and z axis into one.\nHow MIL-STD-810G Testing Works\nThere are 24 categories of laboratory test methods for MIL-STD-810G, which are then broken down further into procedures, specific tests, or levels, that are appropriate to the environment in which the equipment will be used. Each test is carried out using predefined test plans and criteria.\nIt is important to remember that tests may be laboratory or natural environment field tests or a combination of the two, and that test procedures depend on the environments tested. After each environmental test is completed, post-test data is examined and recorded in relation to material specifications and program guidelines. Analysis of test results is included in a final test report for each test.\nHow to Meet MIL-STD-810G Standards: Best Practices\nThe best way to meet MIL-STD-810G testing standards is to become familiar with them, the testing methods, and the final test reports of your equipment. Remember that just because something is designed to meet the standards does not mean that it was tested or compliant with MIL-STD-810. It is not until the actual tests are conducted that manufacturers will know whether their products are compliant in a specific operating environment.\nThe first step is to familiarize yourself with the environments in which your products will be used. For example, most military equipment will be subjected to outdoor exposure, extreme temperatures, chemicals, and cleaning processes. They may be subjected to sand, wind, rain, snow, UV, and saltwater, too. This means that each part of your product, including its nameplate and data plate, must be durable enough to withstand these environments and conditions and MIL-STD-810G test methods. Ensure your nameplates and data plates meet not just industry standards but also these tough military standards.\nA material frequently used for nameplates and data plates, that has been tested and found to be durable enough to meet MIL-STD-810G standards, is photosensitive anodized aluminum or Metalphoto®. Metalphoto plates are exceedingly durable and are the preferred choice for the military industry because of their protective finishes and resistance to chemicals, abrasion, and solvents. In fact, Metalphoto nameplates and data plates feature high-resolution graphics that are sealed beneath an anodic layer that is sapphire-hard and impenetrable. These plates are designed specifically to withstand exterior exposure to the harshest environments including extreme cold, heat, and UV. With an expected exterior life of 20 years or more, Metalphoto® plates are the durable choice for equipment subjected to MIL-STD-810G test methods.\nImage via Flickr by expertinfantry']	['<urn:uuid:6bbd9e5f-e089-48c7-9c68-b382fd36ecaf>', '<urn:uuid:6adf50eb-c5c5-437c-91af-82e756aab121>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	21	78	1580
62	winter activities available deschutes wells gray parks	Both parks offer diverse winter recreational activities. Deschutes National Forest (Newberry Crater area) provides snowmobiling trails, skiing, snowshoeing, and dog sledding opportunities, with facilities like Paulina Lake Lodge offering cabin rentals and dining. Wells Gray Park offers Nordic skiing, backcountry skiing, hut-to-hut skiing adventures, and features facilities like the Clearwater Ski Hill and wilderness cabins. Both parks emphasize multi-use winter recreation, though Wells Gray focuses more on skiing while Deschutes has a stronger emphasis on snowmobiling.	"[""Deschutes National Forest\nWinter is a beautiful time of year in Central Oregon, and snowmobiling is an active means of enjoying it.\nMarked Snowmobiling Trails\nThe Newberry Crater of Newberry National Volcanic Monument is the center of snowmobile activity on the Ft. Rock Ranger District. The Crater area offers over 100 miles of trails and many snow play areas for the snowmobile enthusiast.\nMost all of the district is available for use by snow travelers. Roads become an excellent means of traveling the district; they are easy travel routes to follow and they allow you to get away from crowds. Knowledge of the area, weather, route, and limitations of your skill and equipment, along with common sense, can ensure a safe and enjoyable ride.\nSkiers, snowshoers, and dog sledders are invited to use the area as well. Though there are several alternative forms of snow travel, there is no need for conflicts to occur with one another if each observe basic courtesy.\nThere are two sno-parks along Road 21: Six Mile and Ten Mile. The lower lot is used only in heavy snow years. Ten Mile is located just west of the snow gate (closed during winter months) on Road 21, and accommodates approximately 75 vehicles.\nNOTE: The Newberry Crater area is under a legal road closure for the winter months. Entering the area on a motorized vehicle other than a snowmobile constitutes a violation of Forest Service policy. Also, the Newberry National Volcanic Monument Final Comprehensive Management Plan has prohibited Class I ATVs within the Monument boundary. ATVs may ride groomed snowmobile trails outside of the National Monument. Closures are posted at the District Office and Forest Supervisor's Office. Also, key roads and trails will be signed in the field providing information for winter use. Furthermore, a sno-park permit is required to use these facilities. They are available at State Motor Vehicles Division Offices, local sporting goods stores, and ski shops. Licensed snowmobilers obtain sno-park permits when licensing their machines with the Department of Motor Vehicles.\nPaulina Lake Lodge (541-536-2240) remains open through the winter months. Services include cabin rentals, a restaurant, and a shop.\nSNOWMOBILE SAFETY CODE\n- When planning extended trips, prepare a route plan with an estimated time of return. Give this information to a responsible person. Keep as close to your plan as possible.\n- When making extended trips, carry emergency equipment. Leave your machine only as a last resort, then follow your trail back. AVOID TRAVELING WITH ONLY ONE SNOWMOBILE!\n- Do not cross or travel on frozen lakes or streams. (Snowmobiles are not allowed on Paulina or East Lakes due to thin ice in areas caused by underwater hot springs). When TRAVELING in new areas, seek advice on snow conditions. Ranger stations will know most snow conditions in their respective areas. Paulina Lake Lodge also has snow information.\n- Keep physically fit for winter sports activities.\n- Always carry a first aid kit when TRAVELING.\n- Wear proper winter clothing and protective glasses or goggles.\n- Know the weather forecast. When the weather turns bad, turn back. Be aware that snow on steep slopes is subject to avalanches. Small avalanches can be just as deadly as large ones.\n- Keep your snowmobile in good operating condition.\n- Always carry a tool kit.\n- Stay on marked trails or roads open to snowmobiles. Cross-country travel should be avoided unless prior preparations were carefully made. Always carry a map.\n- Use a snowmobile only for transportation when hunting. Do not shoot from your snowmobile.\n- Remember: Distress signals are three smokes, three blasts of a whistle, three shots, or three of anything that might attract attention.\n- Join a local snowmobile club. This will acquaint you with others with whom to plan trips. They are also prepared to help people in distress.\nCODE OF ETHICS\n- Be a good sport. Recognize that people judge all snowmobile owners by your actions. Use your influence with other snowmobilers to promote good rider conduct.\n- Do not litter trails or camping areas.\n- Do not pollute streams or lakes.\n- Do not damage trees, shrubs, or other natural features.\n- Respect others' property and rights.\n- Lend a helping hand when someone is in distress.\n- Make yourself and your vehicle available to assist search and rescue operations.\n- Do not interfere with or harass hikers, skiers, snowshoers, or other winter recreationists. Respect their rights to enjoy our facilities.\n- Know and obey all Federal, State, and local rules regulating the operation of snowmobiles in areas where you use your vehicle. Inform public officials when you observe unlawful or questionable behavior by others.\n- Do not harass wildlife. Avoid areas posted for protection of wildlife.\nDetails mentioned in this article were accurate at the time of publication\nBest Hotels in Bend"", 'Cross Country Skiing\nNordic SkiingIt’s perfectly fitting that Wells Gray Park in Canada’s Great White North offers some of BC’s finest Nordic Skiing. We know snow, after-all – we get between three and four metres of fine white powder each year! Explore more than 100 kilometres of groomed and tracked trails, or create your own path in our untracked backcountry and conquer winter in true Canadian style.\nKick turn, side step, or telemark your way through Wells Gray’s snow covered valleys. Drink in pure alpine air, gliding dazzling trails under a bright winter sun. Breathe in the Christmassy-scent of the winter forest and wonder what animals left tracks in your path and how long ago. The only sound you’ll hear is the swish of your skis – just like you imagined.\nMeadow, Mountain & Lake LoopsSki four different trail systems in the Park. Candle Creek Trails – a 25-kilometre track-set trail, perfect for all levels. The 30-kilometre Murtle River Trail that winds along the river to the Majerus Homestead and beyond. Green Mountain’s groomed 18-kilometre trail features far-reaching scenes of Clearwater Lake, Whitehorse Bluffs and the mists of Helmcken Falls.\nWax up your skis before heading out on the Clearwater Valley Road Trail. The 24-kilometre trail winds its way from just north of Helmcken Falls to Clearwater Lake and a cozy cabin on its shores overlooking Osprey Falls. Ski out for the day, or make plans in advance for an overnight stay in the secluded wilderness cabin.\nHut-to-Hut SkiingGear up for an adventure that’s inaccessible to most – ski touring the Park’s backcountry on a guided hut-to-hut adventure. Arrive in the legendary Trophy Mountains in pure backcountry style atop a Snowcat. Strap on your skis and take off, carving your signature into champagne powder on untouched slopes.\nMarvel at how the trees disappear up here at 7,000 ft. Your guide leads the way to your overnight chalet through a snow-filled wonderland you could only dream of. Put in for the night in a fully catered alpine lodge. Cozy, comfortable, and rustic, it’s a welcome retreat after a full day of skiing. Relive your exhilarating adventures with fellow backcountry adventurers over dinner at your lodge. Refuel for the full week of alpine adventure that’s waiting for you.\nWhen to Go\n- Backcountry skiing season usually runs from December to April\n- Cross-country skiing season runs between December and March\n- Alpine skiing is variable year to year, depending on the snow pack. Check the Clearwater Ski Hill for the latest alpine conditions\nNeed to Know\n- Warm, waterproof clothing is essential for all skiing expeditions\n- Goggles and helmets are recommended, as is sunblock\n- Ski rentals are available at the Clearwater Ski Hill\n- Intermediate-to-advance ability is required for hut-to-hut skiing\n- Local outfitters also offer learn-to-backcountry-ski clinics\n- • Cabin rentals are available for advanced backcountry skiers for touring on their own\nTop 10 Reasons to Ski Wells Gray\n- Legendary BC powder, need we say more?\n- Over 100 kilometres of groomed and tracked trails – a different course each day!\n- To experience the magnificence of Clearwater Lake in the winter\n- To challenge yourself skiing hut-to-hut\n- To savour the serenity found above the tree line in the Trophies (map)\n- To learn how to ski the backcountry in single day clinics\n- To witness wildlife in the wild\n- To ski un-crowded alpine, daytime or night\n- • To see Helmcken Falls transformed into a giant ice cone\n- • To enjoy incredible views, wide-open spaces and the majesty of winter']"	['<urn:uuid:3de74102-f898-4024-b4fa-7ae0d0d48376>', '<urn:uuid:72aed8cb-5c9e-42c1-a657-2edcd3d95a63>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	7	76	1399
63	What is the purpose of a floating fence?	A floating fence allows livestock to drink from a lake or pond while blocking the animals from swimming in, crossing over or fouling the water.	['How to Build a Floating Fence\nA floating fence allows livestock to drink from a lake or pond but blocks the animals from swimming in, crossing over or fouling the water. Many farmers fence their pastures with an electric fence. A floating electric fence built of PVC pipe can surround the livestock drinking area on three sides to keep livestock confined.\nDetermine the location you want to fence off. Typically, a floating farm pond fence will extend 12 feet out on the water and will be about 20 feet wide. This will enclose a drinking area sufficient for 30 to 100 head of cattle. Mark out a shoreline area 20 feet wide. Drive a wooden stake into the ground at each end of the marked off shoreline area.\nCut PVC pipe into two sections, each 6 feet long. Cement a PVC pipe tee to one end of each section. Join the two sections together by cementing the teeless end of the first section to the tee that starts the second section. Align the tees so all face upward before cementing. Cement a third tee at the end of the second section. Cement a 6-inch section of PVC pipe to the other side of the third tee. Cement a PVC pipe elbow to the end of the short section, making certain the outlet of the elbow is at right angles to the top of the pipe tees. This forms the first side.\nBuild up a second side piece identical to the first, except that the elbow at the end of the short section should face in the opposite direction from the elbow at the end of the first fence side piece. Cut two 10-foot sections of PVC pipe. Join them together with a pipe tee. This forms the end piece. Make certain all joints are watertight.\nCement a 1-foot section of PVC pipe into each PVC tee. Cement a PVC pipe cap on top of each 1-foot pipe section. Drill a 3/16-inch hole through each 1-foot pipe section just below the pipe cap, parallel with the long pipe.\nCement the ends of the built-up 20-foot end section into the elbows at the ends of each built up 12-foot side section to form a 3-sided box, with short pipes sticking straight up from each pipe tee. Cement a PVC pipe tee plug into the open end of the pipe tee at each end of the 3-sided box. Thread electric fence wire through the holes in each vertical pipe.\nPush the floating electric fence out onto the water. Tie each end of the floating fence to the two wooden stakes you drove into the shoreline in Step 1. Connect one end of the floating fence’s wire to the shoreline electric fence, pull the floating fence wire taut and connect the other end to the shoreline fence on the opposite end of the floating fence to complete the floating fence’s electrified wire loop.\nThings You Will Need\n- 2-inch PVC pipe\n- PVC pipe tees\n- PVC pipe caps\n- PVC pipe elbows\n- PVC pipe tee plugs\n- PVC pipe cement\n- Electric drill\n- 3/16-inch drill bit\n- Electric fence wire\n- 3-foot-long pressure-treated wooden stakes']	['<urn:uuid:4fe71beb-4335-402e-a21c-235d9a266b26>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	8	25	534
64	As someone studying the portrayal of mass media influence in literature, I'm curious - what's the key difference between how language control is used for societal manipulation in 1984's Newspeak versus Fahrenheit 451's approach to controlling information?	While both works show language/information control, they differ fundamentally in their approach. In 1984, control is achieved through Newspeak - deliberately simplifying and reducing language to make complex thought impossible. Words are systematically destroyed, with terms like 'ungood' replacing 'bad', and 'doubleplusgood' replacing words like 'excellent', ultimately narrowing vocabulary to limit independent thinking. In contrast, Fahrenheit 451 controls information not by simplifying language but by condensing and destroying written works entirely - 'Classics cut to fit fifteen-minute radio shows' and books being burned, replacing them with shallow entertainment and 'interactive soap operas' to prevent meaningful engagement with ideas.	['Though gorgeously written in its own right, 1984 also benefitted from the timing of its release, at the very end of the Second World War and at the beginning of the Cold War. Though a delusional love affair existed between the West and the Soviet Union in 1943, disillusionment and reality set in in the few years following the surrenders of Germany and Japan. In 1956, Kirk assessed the power of the novel well.\nGeorge Orwell, incidentally, has been incalculably influential, since his death, in turning the minds of Englishmen against collectivistic utopias—more influential by far than ever he was when he lived. The effect of 1984 upon public opinion goes far to refute the argument that ideas merely reflect the great social and material currents of an age.1\nKirk saw the novel as ushering in a sea change in thought, rendering the arguments of planners suspect at best and horrendous as worst. From a personal standpoint, Kirk realized that his own masterpiece, The Conservative Mind, would never have succeeded without 1984 paving the way for it. Whatever the difference in political outlook, Orwell paved the way for an acceptance—or least an interest—in Kirk’s ideas. In a November 1984 interview, the Michiganian noted the necessity of 1984 coming before The Conservative Mind, as it contributed to “a period of sober reflection,” one that caused the West to examine its past relations with the Soviets, considering those relations by the early 1950s as nothing short of betrayal and a time for repentance for having danced with the devil.2\nWhile Orwell layered the novel with meaning, all of which should be explored in depth, two themes must be analyzed explicitly for our purposes: the corruption and simplification of language; and the loss of time. Famously, Orwell created a form of English, post-ideological and pared down so much that it discouraged or outright abolished freedom of thought, creativity, and imagination. The author called it “Newspeak.” Importantly, this carries profound significance in at least two ways. First, the term itself means a language well beyond—or below—modern English, which has evolved almost without any direction or command over centuries and centuries. Perhaps more than any other institution, language has evolved naturally and without a command structure. It has grown spontaneously as the profound political philosopher, Friedrich August von Hayek observed, discovered rather than made. With Newspeak, however, the language evolution ends, and its devolution, limited by the seemingly endless bitterness of the tyrannical society, begins. Its devolutionary trajectory, importantly, will be even sharper than its evolutionary ascension. A longish conversation between the protagonist, Winston Smith (really an anti-hero), and his closest friend (if “friend” can even be used here), Syme, a philologist and party intellectual, filled with a “vapid eagerness,” reveals much. Meeting in the cafeteria, Syme enthusiastically explains the eleventh edition of the Newspeak dictionary to Smith.\n‘It’s a beautiful thing, the destruction of words. Of course the great wastage is in the verbs and adjectives, but there are hundreds of nouns that can be got rid of as well. It isn’t only the synonyms; there are also the antonyms. After all, what justification is there for a word which is simply the opposite of some other word? A word contains its opposite in itself. Take “good”, for instance. If you have a word like “good”, what need is there for a word like “bad”? “Ungood” will do just as well— better, because it’s an exact opposite, which the other is not. Or again, if you want a stronger version of “good”, what sense is there in having a whole string of vague useless words like “excellent” and “splendid” and all the rest of them? “Plusgood” covers the meaning; or “doubleplusgood” if you want something stronger still. Of course we use those forms already, but in the final version of Newspeak there’ll be nothing else. In the end the whole notion of goodness and badness will be covered by only six words— in reality, only one word. Don’t you see the beauty of that, Winston?3\nNo reason exists, he continues, for two or three words to linger when one will do.\nOf course, one will not do, as English has a massive vocabulary for a reason. English speakers have tended to appreciate specific words for their unique meanings. Unlike German, which combines words to form new works, English has incorporated new words or coined them. While it is perfectly fine in the English language to have several words that might, perhaps, overlap anywhere from fifty to ninety percent in meaning, it has been regarded best to pick the exact term.\nSecond, though, Newspeak serves the function of propaganda. Orwell is not alone in his worries that the entire western world was sliding away from art and complexity toward simplicity and propaganda in the middle decades of the twentieth century. In the late 1930s, the American public intellectual Walter Lippman feared that nearly all of the western world had politicized and had taken language and art into the sewers of power and power alone. The Anglo-Welsh man of letters, Christopher Dawson, worried in 1946 that politics had subsumed all under its devouring maw. In his deeply moving short story, Leaf by Niggle, Tolkien beautifully juxtaposed the nature of art and the subhuman nature of propaganda. While art leavens the dignity of the human person, propaganda, by appealing to the lowest aspects of the human person, seeks conformity and domination.\nChoice of vocabulary, Syme correctly realizes in 1984, leads one to independent thought. “In your heart,” he tells Orwell over some stale bread, “you’d prefer to stick to Oldspeak, with all its vagueness and shades of meaning. You don’t grasp the beauty of the destruction of words.”4 Narrow the vocabulary and you, by necessity, narrow thought. Further, by introducing conflicting ideas such as “crimestop, blackwhite, and doublethink,” a persons becomes “unwilling and unable to think too deeply on any subject whatever.”5\nFully embracing and utilizing the limitations promoted by Newspeak, Big Brother infamously proclaims: War is Peace; Freedom is Slavery; and Ignorance is Strength. More malleable than previously assumed, the human beings of 1984 have accepted these lies as unquestioned truths, essential to the stability and order of the very essence of society. To question them would mean certain discomfort and almost certain death. As the one character most representative of Big Brother states during his interrogation of Smith: “We are interested solely in power. Not wealth or luxury or long life or happiness; only power, pure power.” After all, he continues, the “object of power is power.” And, further, God, if he exists, is power. “We are the priests of power.”6 Real power resides not primarily in the ability to control nature, but to control men.7\nThe other truly fascinating feature about 1984 is Orwell’s use of time and exploration of its subjective nature in a dystopia. As already noted, Big Brother and the party, Ingsoc, must constantly erase the past and even their erasure of the past. This never ends, making it a vital part of state control of society. That Orwell named the novel 1984 is a fascinating aspect of the story, as the protagonist, Smith, really has no idea what year it is. Ingsoc has decreed it the year 1984, and, therefore, it is. But, according to our calendar, it might be 1983 or 1985 or some other year. Smith’s first act of defiance in the actual narrative is his recording of the date in a black-market notebook: “April 4th, 1984.”8 With this act, he has stamped his personality on paper, but he as, vitally, anchored himself, thus giving himself a sense of order and, ultimately, a place within the transcendent realm of justice. This act, after all, means more to him and his personality than to anything else. If he’s writing for future generations, he acknowledges, the future will either be the same as the present, oppressive, or it will be free. If the former, he will be ignored. If the latter, “meaningless.”9\nThe meaning of time plagues Smith throughout the novel. When he enters an old curio shop, he ventures upstairs and finds “an old-fashioned glass clock with a twelve-hour face.”10 After encountering it, a children’s rhyme haunts Smith.\nOranges and lemons, say the bells of St. Clement’s\nYou owe me three farthings, say the bells of St. Martin’s11\nThough the rhyme reminds Smith of a better past, he had come to believe he, like all others in the present, are already dead, simply walking corpses. At one point, he attempts to proffer some form of historical consciousness to his lover, Julia. “History has stopped. Nothing exists except an endless present in which the Party is always right.”12 Her response startles and frustrates Smith, as he seeks some purpose even in his walking death. “One knew that it was all rubbish, so why let oneself be worried by it?”13 Through their conversations, Smith comes to realize that “the object was not to stay alive but to stay human.”14\nYet, Big Brother catches Smith and Julia, imprisoning and torturing them. Throughout the entire procedure, the detainees refuse to let the prisoners know the hour or the day. Without windows and with irregular feedings, the captives lose all sense of time, not even knowing if it is night or day. Everything has become disordered. During the torture sessions, Smith is force to repeat the central tenet of the Party: “Who controls the past controls the future; who controls the present controls the past.”15 The slogan itself is the undoing of the nursery rhyme, “Oranges and Lemons.” Time has no independent liturgy, no seasons, no inevitabilities—except as a tool of power. When Smith resists, holding onto the idea that time does matter and that a future might be better, his torturer states: “If you want a picture of the future, imagine a boot stamping on a human face—forever.”16\n1 Kirk, “The Path to Utopia,” Beyond the Dreams of Avarice, 181-182.\n2 Interview with Russell Kirk, November 11, 1984, quoted in Rodden, Every Intellectual’s Big Brother, 78.\n3 Orwell, 1984, 52-53.\n4 Orwell, 1984, 53.\n5 Orwell, 1984, 217.\n6 Orwell, 1984, 272-273.\n7 Orwell, 1984, 275.\n8 Orwell, 1984, 7.\n9 Orwell, 1984, 7.\n10 Orwell, 1984, 99.\n11 Orwell, 1984, 100-101.\n12 Orwell, 1984, 158.\n13 Orwell, 1984, 159.\n14 Orwell, 1984, 170.\n15 Orwell, 1984, 255.\n16 Orwell, 1984, 277.', '“Some say the world will end in fire/some say in ice. From what I’ve tasted of desire, I hold with those who favor fire.” So wrote Robert Frost in 1920, roughly three decades before the publication of Ray Bradbury’s Fahrenheit 451, a novel which begins with one of the most ominous and chilling pronouncements in modern literature: “It was a pleasure to burn.” The motif of fire as an instrument of both destruction and rebirth is central to Fahrenheit 451, which begins with a man exulting in the act of setting history to the torch (the title refers to the temperature at which paper burns), and ends with nuclear Armageddon that brings both devastation and the possibility of starting anew.\nMore than half a century has passed since the publication of Fahrenheit 451 in 1953, but its vision of a society drugged and numbed into submission by mass media, vacuous entertainment, and wall-to-wall television screens blaring interactive soap operas has not lost any of its power to disturb and unsettle (if you disagree, take a look around you in any public place and see how many people are transfixed by the screens of their phones). Bradbury famously abhorred technology, going so far as to not even own a computer, and this profound skepticism is palpable in Fahrenheit 451, where people are bombarded with terrible music and advertisements through “seashells” that are eerily reminiscent of modern-day earbuds. Divorced from anything meaningful, teenagers amuse themselves by racing their cars at dangerous speeds and other destructive past times. The world of Fahrenheit 451 is, in short, one which has become completely disconnected from all of the things which give life meaning: love, compassion, friendship, independent thought and intellectual curiosity. The only true passion left is the toxic adrenaline rush experienced by the book’s protagonist, Guy Montag, a professional “fireman” tasked not with extinguishing fires, but by burning books, which have become outlawed and illegal.\n“With the brass nozzle in his fists, with this great python spitting its venomous kerosene upon the world, the blood pounded in his head,” Bradbury writes in the book’s mesmerizing opening paragraph, “and his hands were the hands of some amazing conductor playing all the symphonies of blazing and burning to bring down the tatters and charcoal ruins of history.”\nBradbury was a poet at heart, and this passage exemplifies the grim, striking beauty of his incredibly evocative prose. In the span of a few pages, he is able to transport us from a scene of demonic, almost operatic destruction to the peaceful serenity of an Autumn night, as Montag strolls blissfully down the street, “thinking very little about nothing in particular”, towards a chance encounter with a girl who stands waiting for him, almost as if by fate:\n“The autumn leaves blew over the moonlit pavement in such a way as to make the girl who was moving there seem fixed to a sliding walk, letting the motion of the wind and the leaves carry her forward. Her head was half bent to watch her shoes stir the circling leaves. Her face was slender and milk-white, and in it was a kind of gentle hunger that touched over everything with tireless curiosity.”\nThe girl, Clarice McLellan, appears in the novel only for a short time, yet she comes to embody the hopeful, optimistic dimension of Bradbury’s narrative, setting in motion Montag’s life-altering re-evaluation of his own existence and the society he lives in. She is representative not only of what is missing from the world, but also of what distinguishes Fahrenheit 451 from so many other novels with nightmarish visions of the future, and that is its sense of cautious optimism. To be sure, Bradbury offers an incredibly prescient diagnosing of social ills, specifically censorship and the “dumbing down” of society that has led to a world where simply owning a book is a capital crime, and the official Fire Department slogan is, un-ironically, “Monday burn Millay, Wednesday Whitman, Friday Faulkner, burn‘em to ashes, then burn the ashes.” In our own world, where social media has run rampant and wreaked havoc on our collective attention spans, Bradbury’s description of anti-intellectualism hits uncomfortably close to home. In one of the novel’s most important segments, the Fire Chief Beatty lays bare for Montag the disturbing truth in a lengthy and eloquent sermon:\n“Classics cut to fit fifteen-minute radio shows, then cut again to fill a two-minute book column, winding up at last as a ten- or twelve-line dictionary resume. I exaggerate, of course. The dictionaries were for reference. But many were those whose sole knowledge of Hamlet…was a one-page digest in a book that claimed: now at least you can read all the classics; keep up with your neighbors. Do you see? Out of the nursery into the college and back to the nursery; there’s your intellectual pattern for the past five centuries or more.”\nIn essence, condensing and super-condensing information for the sake of convenience has led to the demise of critical thought. As Beatty explains, “Whirl man’s mind around about so fast under the pumping hands of publishers, exploiters, broadcasters, that the\ncentrifuge flings off all unnecessary, time-wasting thought!”\nBeatty is an intriguing villain, reminiscent of O’Brian in Orwell’s 1984 in being aware of the disgusting nature of the system while simultaneously embracing it. His backstory remains an enigma, and he functions less as a person and more as a vehicle to express the central message of the story. As he explains, the downfall of society doesn’t stem from the edicts of a brutal police state: rather, it is we who have put the shackles on ourselves by sacrificing any semblance of critical thought in exchange for a life of mindless pleasure and convenience. Censorship has become a hot-button topic in recent times, and Bradbury (through Beatty) does not shy away from decrying political correctness and the fear of offending the sensibilities of others as a factor leading to a society that has outlawed the written word.\n“Now let’s take up the minorities in our civilization, shall we? Bigger the population, the more minorities. Don’t step on the toes of the dog-lovers, the cat-lovers, doctors, lawyers, merchants, chiefs, Mormons, Baptists, Unitarians, second-generation Chinese, Swedes, Italians, Germans, Texans, Brooklynites, Irishmen, people from Oregon or Mexico. The people in this book, this play, this TV serial are not meant to represent any actual painters, cartographers, mechanics anywhere. The bigger your market, Montag, the less you handle controversy, remember that! All the minor minor minorities with their navels to be kept clean. Authors, full of evil thoughts, lock up your typewriters. They did.”\nIf all of this seems a bit too grim and heavy-handed, it’s worth noting that the novel ends by implying that a society of book-lovers will rebuild the world once again. As Montag puts it, “Maybe the books can get us half out of the cave. They just might stop us from making the same damn insane mistakes!”\nSome might see Bradbury’s faith in the saving power of the written word as poignantly naive when our own world is ridden with so many problems, but I think that guarded optimism is the distinguishing feature of Fahrenheit 451. Clocking in at 256 pages, the book (adapted by Bradbury from his earlier short story “The Fireman”) is relatively spare, telling a story that, like all of the greatest and most timeless, is powerful in its almost mythic simplicity. Images from the novel, such as Montag floating in a river after escaping the mechanical hound, or sitting in a barn at night and watching a girl resembling Clarisse (real or imagined) braiding her hair, have the power to stay with you forever. We live in a time when “dystopian” novels have become practically a genre unto themselves, so ubiquitous that the term has practically lost all meaning. We look at fictional apocalyptic wastelands, authoritarian police-states, and industrial hellscapes with weariness, with the feeling that we’ve seen all this before. But Fahrenheit 451 stands apart not by bombarding us with horrors, although there are horrors, to be sure (who could forget the dreaded Mechanical Hound, or the stomach of Montag’s TV-addicted wife, Mildred, being pumped by a “suction snake”) but by reminding us, through the darkness, of the things, large and small, without which life loses all meaning: the soft blowing of leaves on an Autumn night, and the face of a girl “like the dial of a small clock seen faintly in a dark room in the middle of a night when you waken to see the time and see the clock telling you the hour and the minute and the second, with a white silence and a glowing, all certainty and knowing what it has to tell of the night passing swiftly on toward further darknesses but moving also toward a new sun.” In the novel, that “sun” occurs in the form of a nuclear explosion, wiping away the failed civilization. Yet even in the midst of destruction, the potential for rebirth and renewal is reflected by the twin symbols on Montag’s uniform, a salamander that survives the furnace, and a phoenix, which rises from the ashes. If we can, against all odds, preserve our intellectual curiosity, our decency, our compassion and care for one another, maybe, just maybe, Bradbury suggests, there will be some hope for us in the end. Maybe we will be able to endure.']	['<urn:uuid:4846a96c-9cf1-45dc-ab04-455406b072e9>', '<urn:uuid:357062bf-ec8c-450d-bd99-237edafc13ac>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	37	98	3297
65	marine research conservation impact by catch mitigation survival fish species	Research has focused on estimating post-release survival of various fish species and devising means to reduce incidental bycatch in pelagic fisheries. Studies have examined post-release mortality rates and survival of species like white marlin, Atlantic bluefin tuna, and billfishes. The global annual bycatch is estimated at 38.5 million tonnes, comprising approximately 40% of the entire marine catch. This has contributed to the decline of many marine species, including vulnerable animals like turtles and slow-growing fish. To address this, initiatives like South Africa's Experimental Threshold programme are working to improve data recording and management of bycatch species, while organizations like SECIFA monitor catches monthly and implement strategies to keep members within annual catch limits.	"['- B.A., University of California, San Diego\n- Ph.D., Scripps Institution of Oceanography, UCSD\nMy research interests are broadly focused on the processes of molecular evolution in the marine environment. In particular, my research program is directed toward 1) the use of molecular techniques to elucidate the population structure of marine organisms, with an emphasis on pelagic fishes; 2) the application of techniques in molecular genetics to help resolve problems in fishery science; and 3) the use of molecular genetic data to infer the evolutionary relationships. In addition, we have recently focused our research efforts to estimate the survival of billfish released from recreational and commercial fishing gear, and to devise means of reducing incidental bycatch in the pelagic longline fishery.\n- Population structure of Atlantic bluefin tuna\n- Phylogeny of istiophorid billfishes\n- Population structure of istiophorid billfishes\n- Post-release survival and habitat utilization of istiophorid billfishes\n- Post-release survival, seasonal movements, and habitat utilization of cobia\n- Post-release survival of bluefin tuna\n2011 - Present\n(**Undergraduate Student; *Graduate Student, Postdoctoral Fellow)\n- Graves, J.E., *B.J. Marcek, and *W.M. Goldsmith. Effects of air exposure on postrelease mortality rates of white marlin caught in the U.S. offshore recreational fishery. N. Am. J. Fish. Management, in press.\n- *Reid, K., T.B. Hoareau, J.E. Graves, W.M. Potts, S.M.R. dos Santos, A.W. Klopper, and P. Bloomer. Directional selection with gene flow in a cosmopolitan marine fish across a persistent South Atlantic upwelling zone. J. Heredity, in press.\n- *Marcek, B.J., M.C. Fabrizio, and J.E. Graves. Short term habitat use of juvenile Atlantic bluefin Tuna Thunnus thynnus. Marine Coastal Fisheries, in press.\n- *Schlenker, L.S., R.J. Latour, R.W. Brill, and J.E. Graves. 2016. Physiological stress and post-release mortality of White Marlin (Kajikia albida) caught in the United States recreational fishery. Conservation Physiology 4: doi:10.1093/conphys/cov066.\n- Horodysky, A.Z., S.J. Cooke, J.E. Graves, and R.W. Brill. 2016. Fisheries conservation on the high seas: Linking conservation physiology and fisheries ecology for the management of large pelagic fishes. Conservation Physiology 4: doi:10.1093/conphys/cov059.\n- Graves, J.E., and A.Z. Horodysky. 2015. Challenges of estimating post-release mortality of istiophorid billfishes caught in the recreational fishery: A review. Fisheries Research 166: 163-168.\n- Graves, J.E., and J.R. McDowell. 2015. Population structure of istiophorid billfishes. Fisheries Research 166: 21-298.\n- Graves, J.E., A.S. Wozniak, R.M. Dickhut, M.A. Cochran, E.H. MacDonald, E. Bush, H. Arrizabalaga, and N. Goñi. 2015. Trans-Atlantic movements of juvenile Atlantic bluefin tuna inferred from analyses of organochlorine tracers. Can. J. Fish. Aquat. Sci. 72:625-633.\n- *Hudson, J.M, D.K Steinberg, T.T. Sutton, J.E. Graves, and R. J. Latour. 2014. Feeding Ecology and Carbon Transport along the Northern Mid-Atlantic Ridge. Deep-Sea Research I 93:104-116.\n- *Marcek, B.J., and J.E. Graves. 2014. An estimate of post-release mortality of school-size Atlantic Bluefin Tuna Thunnus thynnus in the U.S. recreational troll fishery. N. Am. J. Fish. Management 34:602-608.\n- *Sorenson, L., J.R. McDowell, T. Knot, and J.E. Graves. 2013. Assignment test method using hypervariable markers for blue marlin (Makaira nigricans) stock identification. Conservation Genetics Resources 5:293-297.\n- *Verissimo, A., J.R. McDowell, and J.E. Graves. 2012. Genetic population structure and connectivity in a commercially exploited and wide-ranging deepwater shark, the leafscale gulper Centrophorus squamosus. Mar.Freshwater Res. 63:505-512.\n- Serafy, J.E., S.J. Cooke, G.A. Diaz, J.E. Graves, M. Hall, M. Shivji, and Y. Swimmer. 2012. Evaluating circle hooks in commercial, recreational, and artisanal fisheries: research status and needs for improved conservation and management. Bull. Mar. Sci. 88: 371-391.\n- Graves, J.E., A.Z. Horodysky, and D.W. Kerstetter. 2012. Incorporating circle hooks into Atlantic pelagic fisheries: case studies from the commercial tuna/swordfish longline and recreational billfish fisheries. Bull. Mar. Sci. 88:411-422.\n- Rooker, J.R., J.R. Simms, R.J.D. Wells, S.A. Holt, G.J. Holt, J.E. Graves, and N.B. Furey. 2012. Distributions and habitat associations of billfish and swordfish larvae across mesoscale features in the Gulf of Mexico. PLoS ONE 7(4) e31480.\n- Kerstetter, D.W., S.M. Bayse, J.L. Fenton, and J.E. Graves. 2011. Sailfish habitat utilization and vertical movements in the southern Gulf of Mexico and Florida. Marine and Coastal Fisheries, 3: 353-365.\n- *Sorenson, L., J.R. McDowell, and J.E. Graves. 2011. Isolation and characterization of microsatellite markers for blue marlin, Makaira nigricans. Conservation Genetics Resources, 3: 721-723.\n- Collette, B.B., J.E. Graves, et al. (33 authors). 2011. High value and long life – double jeopardy for tunas and billfishes. Science 333:291-292.\n- *Lynch, P.D., J.E. Graves, and R.J. Latour. 2011. Challenges in the assessment and management of highly migratory bycatch species: a case study of Atlantic marlins. Pages 197-226. W.W. Taylor, M.G. Schechter, and A.J. Lynch, editors. Sustainable fisheries: multilevel approaches to a global problem. American Fisheries Society, Bethesda, MD.\n- *Veríssimo, A., J.R. McDowell, and J.E. Graves. 2011. Population structure of a deepwater squaloid shark, the Portuguese dogfish Centroscymnus coelolepis. ICES Journal of Marine Science 68: 555-563.\n- Pacheco, J.C., D.W. Kerstetter, F.H. Hazin, H. Hazin, R.S.S.L. Segundo, J.E. Graves, F. Carvalho, and P.E. Travassos. 2011. A comparison of circle hook and J hook performance in a western equatorial Atlantic Ocean pelagic longline fishery. Fisheries Research 107: 39-45.\n- *Veríssimo, A. , M. T. Moura, J. McDowell, J. Graves, L. Gordo, and R. Hoelzel. 2011. Isolation and characterization of ten nuclear microsatellite loci for the Portuguese dogfish Centroscymnus coelolepis. Conservation Genetics Resources 3: 299-301.\n- Willliam (Willy) Goldsmith, Ph.D program\n- Nadya Mamoozadeh, Ph.D. program\n- Savannah Michaelsen, M.S. program\n- Douglas Jensen, M.S. program\n- David Kerstetter, 2005\n- Pindaro Diaz-Jaimes, 2000 - 2001\n- David Carlini, 1998 - 1999\n- Kimberly Reece, 1994 - 1998\n- Ana Verissimo, 2011\n- Dave Portnoy, 2008\n- David Kerstetter, 2005\n- Jan McDowell, 2002\n- Tom Orrell, 2000\n- Jan Cordes, 2000\n- Vince Buonaccorsi, 1998\n- David Carlini, 1998\n- Daniel Scoles, 1994\n- Ed Heist, 1994\n- Emily Loose, 2014\n- Michele (Tony) Nalovic, 2014\n- Lela Schlenker, 2014\n- Ben Marcek, 2013\n- Jeanna Hudson, 2012\n- Catarina Wor, 2012\n- Laurie Sorenson, 2011\n- Dan Dutton, 2010\n- Abigail Lynch, 2008\n- Emily Chandler, 2007\n- Kirsten Brendtro, 2006\n- Melissa Paine, 2006\n- Kurtis Gray, 2005\n- Andrij Horodysky, 2004\n- Kelly Johnson, 2003\n- David Kerstetter, 2001\n- Arminda Gensler, 2001\n- Felipe Arzayus, 2000\n- Meredith Bostrom, 2000\n- Brett Falterman, 1999\n- Jeff Norrgard, 1995\n- Catherine O’Neil Goodbred, 1995\n- Sandra Blake Boles, 1994\n- Lee Morgan, 1992\n- MSCI 401F Fundamentals of Marine Fisheries Science\n- MSCI 501F Fundamentals of Marine Fisheries Science\n- MSCI 528 Marine Fisheries Science\n- MSCI 515 Fisheries Science Seminar\n- MSCI 673 Marine Molecular Genetics\n- MSCI 674 Marine Molecular Genetics Laboratory\n- 2016 - Inducted into the International Game Fish Association\'s Hall of Fame\n- 2016 - Thomas Ashley Graves, Jr. Award for Sustained Excellence in Teaching, College of William and Mary\n- 2013 - Plumeri Award for Faculty Excellence\n- 2011 - Sport Fishing Magazine\'s ""Making a Difference"" Award\n- 2007 - International Game Fish Association, Individual Conservation Award\n- 2006 - NOAA Fisheries Service Special Recognition Award\n- 2004 - Virginia Outstanding Faculty Award\n- 2001 - NOAA Team Member of the Month\n- 1995 - The Thomas Jefferson Teaching Award, College of William & Mary\n- 1992 - The Outstanding Teacher Award, School of Marine Science, College of William & Mary\n- Chairman, U.S. Advisory Committee to the International Commission for the Conservation of Atlantic Tunas\n- Member National Marine Fisheries Service Highly Migratory Species Advisory Panel\n- Member International Union for the Conservation of Nature (IUCN) Tuna and Billfish Specialist Group, 2009-present', 'Experimental Threshold (ET) programme to co-manage 10 bycatch species\nSouth Africa has been harvesting hake, sole and other marine resources through trawl fishing for over one hundred years. Trawling – the practice of fishing by dragging a large net along the bottom of the ocean floor — has been in the cross-hairs of controversy for decades. On the one hand, it allows for the capture of large volumes of fish; in some cases, up to hundreds of tons in a single trip. In light of the estimated 791 million people in developing countries to be chronically hungry, the desirability of such a powerful technology that allows for high-volume harvesting of rich fish sources is understandable.\nOn the other hand, there are serious ecological concerns associated with trawl fishing, namely the large catches of marine species not targeted but caught anyway (the “bycatch”) and damage to the habitat by the dragging of the fishing gear. Bycatch is recognised as one of the biggest problems in managing marine ecosystems today. Estimates of the global annual bycatch are around 38.5 million tonnes with approximately 40% of the entire marine catch made up of bycatch. Scientist struggle to manage these bycatches because the reporting of the catches is often poor. The fishing of bycatch has contributed to the decline of many marine species, including vulnerable animals such as turtles and slow-growing fish.\nFor the past several years, the South African inshore trawl sector through its industry body – the South East Coast Industrial Fishing Association (SECIFA) – has been working closely with scientists from the University of Cape Town to improve the data recording and management of 10 bycatch species in the sector. These 10 species, together with the target species of hake and sole, make up approximately 95% of the total catch in the sector. As a result of this Fishery Conservation Project (FCP), SECIFA is now monitoring its catches of these 10 bycatch species on a monthly basis and has agreed to test out strategies of keeping members within annual catch limits for these species. Through the support of the Department of Agriculture, Forestry and Fisheries (DAFF) and NGOs such as WWF and the Responsible Fisheries Alliance (RFA), changes are underway to improve the reporting of bycatch to fisheries scientists at DAFF that manage the stocks. Watch this space!\nSouth African Hake Longline Association Fishery Conservation Project\nThe South African hake longline fishery is one of South Africa’s most valuable fisheries. The sector receives approximately 6% of the total allowable catch (TAC) for hake. In 2013, through the South African Hake Longline Association (SAHLLA), the sector entered into a Fishery Conservation Project (FCP) with the WWF. The chief goals of the project are to gather better information on the impact of fishing on the surrounding ecosystem and take positive action to improve the sustainability of the sector’s products.\nA major milestone in the FCP was reached at the end of 2014 – the sector completed one-year of independent at-sea observer coverage of its fishing to understand better the impacts of its fishing on the marine ecosystem. Specifically, scientific observers recorded key data on the non-target species (bycatch) landed by the sector and interactions with seabirds, among other things. As a result, the sector is now working closely with the WWF and BirdLife International to undertake responsible fisheries training and it is developing tori lines to protect seabirds during fishing operations.']"	['<urn:uuid:b63297fe-f23c-42aa-940d-293a40e9a8d1>', '<urn:uuid:5c3daffe-5fcb-4df6-958e-276da788914d>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	10	113	1820
66	How small do particles in the air need to be to get into the deepest parts of our lungs?	Particles that are less than ten microns in size are able to penetrate the lower airways of the lungs. The EPA uses measurements called PM10 for particles of ten microns and PM2.5 for those sized at 2.5 microns.	['Air pollution is everywhere. When it’s elevated, it can challenge even the healthiest individuals. Air pollution has been shown specifically to have adverse effects on the respiratory system. But just how long do the effects last?\nA new study suggests that exposure to air pollution more than 30 years ago may still be a factor in a person’s lung disease. This new study followed more than 368,000 people over a 38-year period in England and Wales. Air pollution levels were measured every 10 years. The study found that there were measurably higher risks for chronic bronchitis, emphysema and pneumonia in individuals exposed to air pollution thirty years earlier. The study also found an increased risk of mortality from cardiovascular disease associated with this years ago exposure to air pollution.\nThis is not the first time that the effects of pollution have been studied for its lasting effects on lung disease. Research finds the effects are even more dramatic in children.\nThe California Environmental Protection Agency Air Resources Board has done at least at least two studies on the effects of air pollution on children living in high traffic density neighborhoods in the Los Angeles area. A separate study found that there was a higher prevalence of asthma in disadvantaged and high-traffic neighborhoods in Los Angeles County.\nIn another study, the Board looked retrospectively over an eight-year period at the data collected from 7,000 children during hospital visits and found an association with hospital activity (visits and treatments) and exposure to primary and secondary organic aerosols.\nWhat are the specific elements of air pollution?Ozoneis a form of fortified oxygen (O3 instead of regular oxygen gas which is O2). It can be considered a “good gas” when it occurs in the stratosphere, 20 miles above the earth, where it forms a protective layer. However, when it occurs very close to the earth’s surface after a reaction between nitrous oxides and volatile organic compounds (VOCs), it is highly toxic. There is a condition called oxygen toxicity, which can occur in hospital patients given a high concentration of oxygen, and it’s related to the presence of this O3 (super oxide ion) formation. One of the more dreaded consequences of this condition is lung scarring or ** pulmonary fibrosis**.\nNitrogen oxides are formed whenever fuel, gasoline vapors, and other volatile organic compounds are combusted at high temperatures and fixed with nitrogen in the air. The major sources of nitrogen oxides are emissions from transportation vehicles. Indoor sources are stoves, heaters and cigarette smoking.\nSulfur oxides can be primary gases such as sulfur dioxide, or secondary particulates formed when fossil fuels containing sulfur (coal or oil) are burned during smelting of metals and during other industrial processes. Most of the sulfur dioxide in the United States comes from electric utilities, especially those that burn coal, but it also results from oil refineries and metal processing facilities.\nPollens are natural pollutants from plants, grasses, and trees. Pollens are extremely light, travel long distances with the wind, and remain airborne for days. Individuals with underlying allergies are the most susceptible to their effects.\nParticulate Matter (PM) is a class of solid or liquid particles that form aerosols and thus can remain suspended in air. Particles that are less than ten microns in size are able to penetrate the lower airways of the lungs. The measures used to determine air quality, a PM10 for example, indicates the count of PM of a size of ten microns and PM 2.5 for those sized at 2.5 microns.\nThe Environmental Protection Agency (EPA) considers a PM 10 count of 50 micrograms per cubic meter in outdoor air, averaged over a 24-hour period, to be the standard for safety.\nThe state of California became known years ago for having some of the worse air pollution in the country, and has made large strides by instituting drastic measures in automobile and industry regulations.\nIt is evident that living in an industrialized society, we cannot avoid exposure to all the air hazards I’ve just described. Awareness of air quality measures on a given day and implementing efforts to avoid outdoor activities on days with high levels of air pollutants is the best way to limit exposure and health consequences.\nAwareness of indoor pollutants and implementing modification of your indoor environment is another important strategy to limit pollution exposure. The U.S. Environmental Protection Agency offers air quality measures nationwide and links to local information as well.\nSee More Helpful Articles:\nEli Hendel, M.D. is a board-certified Internist and pulmonary specialist with board certification in Sleep Medicine. He is an Assistant Clinical Professor of Medicine at Keck-University of Southern California School of Medicine, Qualified Medical Examiner for the State of California Department of Industrial Relations, and Director of Intensive Care Services at Glendale Memorial Hospital. His areas of expertise in private practice include asthma, COPD, sleep disorders, obstructive sleep apnea, and occupational lung diseases.\nEli Hendel, M.D., is a board-certified internist/pulmonary specialist with board certification in Sleep Medicine. An Assistant Clinical Professor of Medicine at Keck-University of Southern California School of Medicine, and Qualified Medical Examiner for the State of California Department of Industrial Relations, his areas include asthma, COPD, sleep disorders, obstructive sleep apnea, and occupational lung diseases. Favorite hobby? Playing jazz music. Find him on Twitter @Lung_doctor.']	['<urn:uuid:cb4e510a-9957-43e8-9b21-3beb2df421ad>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	19	38	882
67	What forms are needed for credit reports, and what PPP loan fraud charges exist?	Credit reports require forms like Credit Report Request Forms, Credit Report Authorization Forms, and Credit Report Dispute Forms. For PPP loan fraud, charges include aggravated identity theft (18 U.S.C. § 1028A), bank fraud (18 U.S.C. § 1344), conspiracy (18 U.S.C. § 371), False Claims Act violations, wire fraud (18 U.S.C. § 1343), and tax evasion (26 U.S.C. § 7201).	['A credit is referred to the goods and/or the services that are obtained by a customer or consumer, even when the payment has not yet been made. Hence, it will be made in the future accordingly to what the credit arrangement or loan agreement states.\nAn individual applying for a loan or credit will be needing to complete the form for credit application. In addition to this, there will also be some documents and/or requirements that are needed to be submitted. These forms are subject to undergo few loan application reviews by the management to see if applicants are qualified for such loan or credit. Once the application is approved, every loan or credit are documented in the individual’s credit report for references. Yet, there are such circumstances wherein a customer or a consumer experience bad debts due to the following few reasons:\n- Was a victim of identity theft or fraudulence\n- Was admitted in a hospital and had no insurance/s\n- Had suffered bankruptcy\n- Was laid off or dismissed from employment\n- Other personal issues, such as inadequacy of money management\nIn such cases similar to either of the following above, the customer and consumer should undertake action immediately. They should be able to inform the credit bureau, collection agency, or organization. Thus, they should know how they are to handle or what to do with their bad debts. You may also see sample incident report forms\nApplication for Personal Credit Report Form\nAnnual Credit Report Request Form\nWhat to Do with Bad Debts?\nThe best option for such circumstances such as bad debts is to pay the amount that is owed to the credit bureau or the organization. It is important that an exact target date for payment of bad and old debts is set. Regardless of how many years it will take to pay fully for the debts. There should also be a written simple statement that explains the reason/s why the individual was unable to pay these debts, especially those past due already. Nevertheless, in order to be able to pay accordingly, the individual should focus on their current situation first, such as getting a stable job and paying for their current bills. This is in order for each individual to effectively save for their payment of the bad and old debts. After all, one will not be able to pay the amount if one does not make a living. Unless, there are some sources, and the individual promises that he or she will be paying these debts soon as he or she gets a stable job.\nTherefore, in order for the management to avoid such circumstances, and to ensure that each individual (whose credit or loan application is approved) is responsible enough with their payments, the application reviews include the collection of the payment history of the individual to bills and loans. Some companies or organizations would even conduct a credit check when the applicant applies for a job.\nPersonal Credit Report Request Form\nCredit Report Order Form\nConsumer Credit Report Request Form\nCredit Report Order Sample Form\nCredit reports can only be obtained for free once a year or for every 12 months. Unless, the person requesting for the report needs a copy for more than once within the year due to the following circumstances:\n- Is/Was a victim of fraud or identity theft\n- Own information are used against him or her\nIn such cases similar to the two exceptions above, the credit report companies are obligatory to inform the individual with regards to these. Thus, in requesting for credit reports, the following above are few of the sample forms that could be utilized. The completion of these forms requires the following information:\n- The personal information\n- The full name (first, middle, and last name)\n- The residential and mailing addresses\n- The date of birth\n- The email address\n- The contact number/s (home and mobile)\n- The marital status\n- The social security information\n- The driver’s license number\n- The dependents and spouse information\n- The personal information of the spouse\n- The employment information of the spouse\n- The number of dependents\n- The names and ages of each dependents\n- The employment information\n- The employment history\n- The former position\n- The previous employer company\n- The address of previous employer company\n- The contact information of past employer\n- The term of employment period\nIn line with these required information, there are also few documents or public records that could be used for few background checks for the individual’s identification verification. Such public records include the following:\n- Criminal records or history\n- Bankruptcy history or records\n- Employment records and references\n- Legal proceedings from the court\nAll these documents or public records are required for security clearance purposes. Upon the completion and submission of all these required documents, there should be an affirmation that all the information stated in each of these forms are true and accurate. Otherwise, there will be some legal actions to be done for such punishable act. The affirmation or confirmation section should include the affixed signature of the applicant in the form, together with the date signed.\nCredit Report Consent Form\nCredit Report Authorization Form\nTenant Credit Report Authorization Form\nCredit and Background Report Release Form\nThese credit reports are secured by the credit reporting companies due to the fact that these contain the credit information of the individual, which is confidential. Nevertheless, the forms above are few of which are utilized to ensure that the information are released only to those who are given the authorization.\nThese credit report authorization forms contain a list of terms and conditions with regards to the credit application. These also indicate that some of the information may be released to other companies for few reasons. One is for employment purposes. Therefore, the individual should check their credit report to confirm that their information are correct. This is in order to avoid any complications when the individual creates a new credit or apply for a loan and pay for bills.\nCredit Report Dispute Form\nCredit Report Dispute Sample Form\nThere are also some inevitable circumstances wherein information in credit reports are inaccurate. In situations like these, the credit report dispute form should be filled out. These forms are in order to find an effective solution for every misunderstanding or misinterpreted information. As well as to specify the subject in dispute. Therefore the information to be provided for an effective solution are as follows:\n- the contact information of the customer or applicant\n- the specified information of what is in dispute\n- the account number of the applicant\n- the signature and date signed of the applicant to certify that information is true\nIn solving each issue, an investigation is conducted in order to fully understand and identify the reasons for such mistakes. In addition, there should be a reference or a document of the wrong information. Applicants may use their credit report for evidence. Here are the following possible reasons why dispute forms are utilized:\n- The credit account of the applicant is closed.\n- The credit report states that the applicant has paid late, when in fact, payments are always made early.\n- The report states that few credits or loans were not yet paid, when in fact, all are fully paid.\n- The credit account is experiencing bankruptcy.\n- The report showed or given to the applicant is not his or hers.\n- The account holder was a victim of identity theft.\nThese are few of the possibilities why applicants would be filing a dispute or complaint form. The submission of these forms could be made through email to the appropriate credit bureau or reporting companies. Upon submission of the form, there are also some documents that are to be provided, such as the bankruptcy record as a reference.\nCredit Report Freeze Request Form\nThe individual may request for their credit report to be frozen. One main reason why is to ensure the confidentiality agreement of their credit information. Freezing their credit reports hides all information from anybody else. Even they themselves will not be able to create a new credit unless they lift their frozen credit reports temporarily. And upon doing so, they will be charged for a fee—unless the applicant is a victim of fraudulence or identity theft.\nAside from freezing the account, the individual may also request for a fraud alert to be incorporated in their account. This works when the account holders request to incorporate a statement that would specify that they may be a victim of fraud. Therefore, when there are some cases wherein someone will be creating a new credit, the credit reporting company will be able to see the fraud alert. And upon seeing this, they will be calling the account holder with the given contact number for some confirmation. Yet, these fraud alerts will only last for a limited number of days. In cases such as these, applicants or account owners may request for an extension for a couple of years most especially when they are a victim of identity theft. You may also like sample credit authorization forms\nGuidelines for Credit Report Forms\nIn utilizing each of these credit report forms, there are few things that you should take note of:\n- All necessary information should be provided to avoid any delay to process the request.\n- All documents and/or requirements should be provided or submitted.\n- The instructions in the form should be carefully observed and followed.\n- The information written in the form should be comprehensible.\n- Any falsification or wrongful acts will be subject to legal consequences depending on the circumstances.\n- It is important that you notify or report to the credit reporting company immediately when there are some mistakes or errors on your credit report. For it allows them to check these mistakes within the specific period of time\n- Be careful when it comes to giving your credit information to somebody else, because some people are scammers who steal your information.\nSumming it all up, all these credit reports’ information are not permitted or provided unless the applicant is said to be qualified and has complied with all the requirements. The reason to ensure this is in order for the credit reporting company to ensure the safety of each and every account of other people, as well as their company and management. Therefore, security authorization is mandatory to double the security and safety of everyone. In addition, the company or anyone who does not abide with the restrictions and rules will be held accountable and liable for such actions.\n9+ Sample Employee Report Forms\n8+ Sample Credit Report Authorization Forms\n8+ Sample Student Report Forms\n8+ Sample Project Report Forms\n8+ Sample Business Expense Report Forms\n8+ Sample Employee Misconduct Forms\n8+ Sample Construction Report Forms\n7+ Medical Report Form Samples\n7+ Sample Marketing Report Forms\n15+ Sales Report Form Templates\n14+ Patient Report Forms\n13+ Weekly Report Forms\n12+ Standard Report Forms & Templates\n12+ Expense Report Forms\n9+ Sample Status Report Forms', 'PPP Loan Fraud Charges\nIn March 2020, the Coronavirus Aids, Relief and Economic Security (CARES Act) was passed by Congress to offer stimulus to families and small businesses. One part of this stimulus included the Paycheck Protection Program (PPP) – at the time $349 billion in funds allocated as forgivable small business loans. The purpose of these loans was to cover payroll costs, most mortgage interest, rent and utility costs over the period the loan is made. As one would expect for a program created hastily in the midst of a crisis, it was rife with loopholes and overwhelmed with applications. As regulatory requirements have been rapidly evolving, many businesses have been confused about their eligibility or how they are supposed to allocate loan funds. This has put many businesses who received PPP loans at risk of federal investigation and legal penalties over alleged fraud.\nIf your business is facing an investigation for PPP loan fraud, or you are concerned about your risk of legal penalties, please contact our North Carolina PPP loan fraud attorneys today. Tarlton Polk Law is committed to representing small business owner’s and fighting for their rights.\nRisk Factors for PPP Fraud Investigation\nWith the media mostly publicizing cases of PPP fraud that are obvious scams, such as companies that don’t exist applying for loans, it may seem like regular businesses that follow the rules to the best of their knowledge should have nothing to worry about. However, any business that has submitted a PPP application should be aware of the program’s stipulations and the various things that could raise a red flag for federal investigators. The U.S Small Business Administration (SBA) released a list of frequently asked questions about eligibility for PPP loans to offer some helpful guidelines, but there are still several gray areas that leave business owners at risk of investigation. Some riskier aspects of the loan include:\n- Assessing “need” when your business has a source of liquidity: According to Question 31 of the FAQ, business owners who have some source of liquidity to support their business operations have to assess and certify whether they will need a loan “in good faith”. This issue can boil down to how convincing of a case you can make that the economic uncertainty caused by Covid-19 makes your liquid assets possibly insufficient.\n- Applying for PPP loans as a seasonal business: Seasonal businesses have struggled to predict what business would be liked during their typical peak activity periods as the pandemic crisis has been ongoing. Certifying payroll costs for impending seasonal employees can be especially challenging,\nWith these areas of subjectivity and uncertainty, businesses trying their best to stay in line with regulations can still face investigation and possible charges. While these issues should not deter you from applying for a loan you strongly believe your business needs, you should have experienced trustworthy lawyers you can contact if you are getting targeted for fraud investigation.\nTypes of PPP Loan Fraud\nThe laws surrounding PPP fraud are still developing, but for now the laws concerning PPP loans fall under three main categories:\n- Application Fraud: There are various acts and practices concerning whether a business presents honest and accurate information when applying for a PPP loan. When it comes to federal funds under the PPP program, only companies that qualify as having “small business concerns” under the Small Business Act, 15 U.S.C. § 632 are eligible for funds. The program also specifies that applications must be based on need due to “the uncertainty of current economic conditions making necessary the loan request to support the ongoing operations of the eligible recipient.”\n- Fradulent use of loan Funds: Funds obtained through a PPP loan may only be used for (1) payroll expenses, (2) interest and rent payments under pre-existing obligations, (3) insurance premiums and (4) utility costs. Use of the loan funds for any other purposes can be investigated as fraud.\n- Loan Forgiveness Certification Fraud: Some businesses can qualify to not have to repay their PPP loans, but they must first certify that they have complied with all the requirements of the program. If a company submits a false certification, including one with incomplete information, this could be considered PPP loan fraud.\nPPP Loan Fraud Charges\nUnder each of the above categories, there are a variety of possible charges business owners can face, some of which have very serious consequences. Some allegations you can be charged with include, but are not limited to:\n- Aggravated Identity Theft: If an individual is accused of using another company or individual’s information to obtain a PPP loan, they can face charges for identity theft under 18 U.S.C. § 1028A. This typically coincides with an underlying fraud offense (i.e. Bank Fraud), and entails additional years of federal prison time served after fraud penalties if convicted.\n- Bank Fraud: Under 18 U.S.C. § 1344, individuals and companies that submit fraudulent loan applications can also face charges for bank fraud. The penalty for bank fraud can be up to a $1 million fine and 30 years of federal imprisonment for knowingly defrauding or attempting to defraud a financial institution by means of false pretenses, representations or promises.\n- Conspiracy: Conspiring to commit PPP loan fraud is also considered a federal offense, under 18 U.S.C. § 371 and 18 U.S.C. § 1349. It can even lead up including the same penalties placed on successfully committing a PPP loan fraud offense.\n- False Claims Act Violations: The False Claims act (31 U.S.C. §§ 3729 – 373) prohibits submitting any false claim for payment under a federal benefit program. Submitting false information in a PPP loan application or in a PPP loan forgiveness certification both carry potential to lead to false claims charges.\n- Making False Statements to Federal Agents: When targeted for fraud investigation, individuals must be extremely cautious when sharing information, because sharing false information can lead to federal prosecution under 18 U.S.C. § 1001. Prosecution carries fines up to $250,000 for individuals or $500,000 for corporations and up to five years in federal prison. In general, it’s essential when facing any federal investigation to contact an experienced lawyer immediately to avoid giving federal agents any incriminating testimonies.\n- Making False Statements to the Small Business Administration (SBA): As the SBA is the agency responsible for administering PPP loans, it is also a federal violation to knowingly make any false statements to them, in accordance with 18 U.S.C. § 1014.\n- Tax Evasion: Charges for tax evasion can be filed against anyone who is accused of fraudulently obtaining PPP loans, as a result of unlawful use of loan funds. For example, claiming deductions for expenses paid with PPP funds can lead to federal tax evasion charges, under 26 U.S.C. § 7201. Federal tax evasion prosecution entails fines of up to $100,000 for individuals and $500,000 for corporations, and up to five years in federal prison.\n- Wire Fraud: Fradulent activity that involves internet communication or activity over the internet can be charged as wire fraud under 18 U.S.C. § 1343. As PPP applications are largely submitted online, wire fraud charges can be easily tagged onto other charges for PPP loan fraud.\nWhat to do if Your Company is Targeted by an Investigation\nThe burden of proof is on the federal government to show that you were acting in bad faith when applying for PPP loans or forgiveness certification or using the loan funds. It is in your best interest that you hire experienced fraud defense attorneys who can defend your innocence and work diligently to eliminate the many charges the federal government can file against you. You will also need the protection and guidance of legal experts when dealing with federal agencies that are looking to find any issues in the statements you give them.\nAdditionally, maintaining a thorough trail of your loan application process and spending of PPP loan funds will aid in defending your case. Keeping an ongoing record and demonstrating that you were trying to diligently follow program requirements will help show that you were acting in good faith.']	['<urn:uuid:83e0f00e-ce8f-4dcb-ab99-d070189305a6>', '<urn:uuid:3bcc7fee-4177-4a09-9a39-9d7c1a84ea51>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	14	59	3211
68	proper fishing attire equipment requirements safety	Required fishing attire includes shoes with strong grip for slippery boat surfaces, life jackets, hats, and polarized sunglasses. For equipment, bring sunscreen, first aid kit, and properly maintained fishing gear. On charters, layers are recommended as it can get cooler at sea, especially in winter.	['Your fishing charter questions were answered here:\n1- Where do you pick me up?\nYou can meet your boat at the marina or on the dock by the toll-booth on Paradise Island or downtown Nassau behind the straw market. Pickup depends on where you will be staying. We’ll make all those arrangements for you.\n2- Can I bring kids?\nWe are happy to welcome children on board with their parents. Our crew will help the kids catch a fish just like Mom and Dad.\n3-Do you have life jackets for everyone on board?\nYes. Please ask your crew for full safety directions for the charter.\n4-What do I bring on the trip?\nBesides your desire for a great fishing trip bring a jacket, cap, some sunscreen and a pocket full of luck. We can help you plan your food and drinks if you wish.\n5-Where do we go fishing?\nYou can do deep sea trolling on the edge of the ocean less than a half mile offshore or go further out to sea following seaweed lines for trolling. You might elect to stay over the shallow reefs for light tackle fishing.\n6-What should I wear?\nThis depends on the time of the year. We recommend layers as it can get a little bit cooler out to sea especially in the winter. Bathing suits and bikinis are fine but please remember to bring your sunscreen, a good hat and sunglasses. Got a lucky fishing shirt or hat? Don’t forget to bring it along.\n7-What is the weather like in the Bahamas?\nDepending on the time of year you plan you charter the temperature may range from 65 to 90 degrees with some or no breeze. During the spring and summer we may have afternoon rain showers that quickly go away. If the weather on your charter day is bad we’ll help you plan for another day.\n8-Will the ocean be rough?\nThis depends on the weather. We have days completely calm to days of some degree of waves and chop. We’\n9-What if I get seasick?\nIf you have a tendency for motion sickness we recommend you take something to prevent seasickness before getting on the boat. Check with your physician or pharmacist for recommended medications.\n10-What happens to the fish I catch?\nYou can keep your fish to take home if you have a cooler and storage at your hotel. You might take the fish to a couple different restaurants where they will prepare a great fish dinner for you and your party. You might elect to have your trophy fish mounted or you might leave your catch for the crew with their thanks.\n11-Is there a bathroom on the boat?\nYes, all boats are equipped with a bathroom.\n12-What is included in the charter price?\nYour charter price includes all your bait, tackle, and ice. The only extra cost is for any food or drinks that we provide.\n13-Where are the boats located?\nThe boats are berthed at a marina in Nassau. If you would like to see the boats once you arrive we’ll be happy to provide directions to the dock.\n14-What kind of fish will I catch?\nIf you fish on the ocean you might catch dolphin (mahi-mahi), wahoo, kingfish, marlin, sailfish, and tuna. If you elect to do light-tackle fishing over the reefs you might catch barracuda, turbot, and yellow tail snappers among other smaller species.\n15-How much does it cost to go fishing?\nThe cost of charter depends on the boat you choose, the number of people, and the time, either a half or full day charter. We’ll be happy to make a complete quote once we get more details from you.\n16-Can I join up with a group?\nAll of Born Free charters are private. This allows our crew to pay attention to your group and be sure that you get to fish for the species you want and get to spend the charter time as you see fit. Join up charters typically leaves customers unsatisfied if someone else on the boat catches a fish and they don’t or if one group wants to fish offshore and the other wants to fish inshore.\n17-What is the standard tip for the captain and mate?\nThe standard tip is 15% of the charter price. That tip is split between captain and mate. We do not include Gratuity in your charter price. Tips are not required, but are greatly appreciated.', 'Safety Tips for Family Fishing\nWe all love to spend some quality time with family and loved ones. Especially with the arrival of holidays or a weekend, many plans for the picnic. Fishing is one of the most trending family excursions. Family members of all the age groups love fishing. Fishing is a healthy option, from grandpa to the grandson. But one must have to prepare and gear up well when kids are around. Each year we heard of some tragic mishaps happened while fishing. Stop going for family fishing trips is not the answer. It’s highly recommended to follow the basic fishing safety tips to avoid such occurrence. Here we have narrowed down the basic ones to make your fishing expedition safe and sound.\nFollow the Weather!\nKnow the weather conditions prior to planning the trip. An open sunny day is perfect for fishing. The temperature should be moderate. There are certain fishing areas with unpredictable weather conditions. If you plan to head out there, check the weather forecast.\nCondition of the water Vehicle\nMake sure, your boat is in stable condition. Is it fulfilling all the laws of the lough? Check all these points earlier. Knowing the water conditions like depth or shallowness is highly beneficial in fishing.\nChoose an Active Fishing Area\nInstead of looking for a big one, we recommend you to look for an area with lots of small panfish. It would be more fun.\nProper Fishing Attire\nFishing areas are quite slippery and muddy. The surface of the boat and deck, all are dangerously slippery. You have to be extra cautious when little ones are around. The most important thing is the stable footing. Wear the shoes with strong grip over such slippery areas. Must not forget hats, life jackets and sunglasses. Polarized sunglasses are the most suitable ones from the fishing point of view.\nThere are certain things that you must have, to face any emergency. It may include a flashlight, first aid kit, map of the area, radio, cell phone, water, insect repellent and sunscreen. SPF sunscreen application protects you and your family from sunburn. For young anglers, we advise covering the sensitive areas like face, neck, ears well, with a sunblock.\nFeeling nausea in the water is quite a common problem. Especially for kids. If you have any family member with any such symptoms, it’s good to consult a doctor earlier. He would surely prescribe the patient with some anti-nausea or anti-seasickness medicines.\nSome areas have the most unpredictable weather conditions. For families living in such locations, keeping your rain gear like umbrellas or rain ponchos is highly recommended. In case, you hear the sound of an approaching thunderstorm, it’s highly recommended to leave the fishing spot as soon as possible. Try to find some safe shelter for you and family.\nRenew your Fishing Gear\nFishing equipment is worn out easily. Each time you plan a fishing trip, check closely all the fishing gear. No need to take any broken fishing tackle.\nEspecially with the kids, you must carry the barbless hooks. These are easily removable without any safety risks.\nPersonal Floatation Device\nThis is something, all should wear. But especially for kids boarding in boats, kayaks, and canoes.\nKids Fishing Gear\nKids also love to have some fishing experience. Do not let them, with your fishing gear. It’s highly recommended to buy the fishing gear especially made for kids. Its light weight makes fishing easier for them.\nFishing knives are a must have. These must be extra sharp. But with kids and family around, the blades must be covered when you are not using them.\nOff Limits Area\nThere are certain fishing areas that are prohibited for fishing. Before you hit the water, inquire well whether it’s an open fishing spot or an off limit area. The reasons may vary. It could be your safety, vegetation’s, or wildlife safety.\nSupervise the young Anglers\nOnce you have hit the water, do not leave the kids with their fishing gear. Always keep an eye on them. They must be told about certain rules for fishing in the water. That also includes no running on the boat.']	['<urn:uuid:79cc3729-f9a0-4afd-afcc-4c7850e33918>', '<urn:uuid:210116c8-85cc-4803-8e6e-8ff06d8020e6>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-12T21:09:04.591601	6	45	1437
69	How much of a city's hard surfaces are made up by streets?	Streets make up approximately 40% of most cities' impervious surface area.	['This month we closed out the first half of our monthly webinar series with “Greening the Streetscape: Complete Streets & Stormwater Management.” To learn more, view the recording of the webinar above, download the PDF of the presentation, or read the full recap below.\nA discussion recap\nEmiko Atherton, Director of the National Complete Streets Coalition, opened the conversation by highlighting the heavy economic burden of urban flooding nationwide. The challenge of managing stormwater, she explained, is exacerbated by increased rainfall and aging infrastructure, and city governments as well as residents who cannot afford to move bear the brunt of the expense. Fortunately, stormwater management through green streets infrastructure offers promising solutions that can be carried out in conjunction with Complete Streets. When cities or private developers are retrofitting or redesigning streets, there is a confluence of opportunities to implement both stormwater improvement projects as well as Complete Streets network enhancements. Together, both Green Streets and Complete Streets initiatives work to improve the economic, equity, and environmental impacts of the street network all while creating safer, more vibrant streets.\nOne organization that is leading the way in combining Green Streets with Complete Streets is the National Association of City Transportation Officials (NACTO), a peer network of innovative cities that work towards building people-oriented, multimodal street networks. Aaron Villere of NACTO introduced the organization’s series of street design guidance focused on various topics including bikeways, transit, and most recently, stormwater management. Aaron explained that streets are among cities’ most important assets for stormwater management – they make up about 40% of most cities’ impervious surface. The brand new Urban Street Stormwater Guide provides tools cities can use to transform their streets to be more resilient, comforting, and inviting. To emphasize the opportunity Green & Complete Streets present, Aaron gave the example of a bikeway in San Francisco that is protected from cars by stormwater infrastructure. This project not only manages 90% of stormwater runoff, it also makes 98% of cyclists surveyed feel safer while riding.\nAaron then turned it over to two of NACTO’s member cities to share their experiences implementing Green & Complete Streets on the ground. Philadelphia has been a national leader in these initiatives through their Green Cities, Clean Water program. Jessica Noon, Green Infrastructure Program Manager with the Philadelphia Water Department, explained how Philadelphia cut costs substantially by using green infrastructure, rather than subterranean infrastructure like tanks and tunnels, to manage their combined stormwater-sewage system. Between the public and private sector, over 440 green infrastructure projects have been implemented on Philadelphia’s streets, many of which also beautify the streetscape. Among other improvements, this has included the construction of over 1,000 tree trenches that provide shade and clean air in addition to playing a vital role in stormwater management.\nFinally, Sarah E. Anderson, Water Quality Program Manager from the City of Denver explained how the city is launching their own Water Quality program by assembling an interdisciplinary team across the Departments of Public Works, Environmental Health, Parks and Recreation, and others. She explained how the city’s new Ultra-Urban Green Infrastructure Guidelines focus specifically on how to design green infrastructure to maximize safety and ease of maintenance. Sarah also gave examples of demonstration and corridor projects on Denver’s streets that work to maximize resilience while simultaneously improving walkability and bikeability.\nWant to learn more?\nTo read more about the various design guides and programs discussed in the webinar, visit the following links:\n- NACTO’s new Urban Street Stormwater Guide can be viewed online or purchased from Island Press. Get 20% off with the discount code 2NACTO.\n- Other valuable resources from NACTO, including the Urban Bikeway Design Guide, the Urban Street Design Guide, the Transit Street Design Guide, and the Global Street Design Guide, all of which can be found on their website.\n- The City of Philadelphia’s Complete Streets Design Manual is another resource that provides a template for combined implementation of green infrastructure and Complete Streets.\n- Finally, Denver’s Ultra-Urban Green Infrastructure Guidelines includes strategies to introduce green infrastructure not only on city streets, but also in parks, open spaces, and floodplains.\nAs always, we received more questions than we had time to address during the webinar, but our presenters were able to provide answers for many of these questions in writing.\nAre air quality-related components of transportation addressed in these design guidelines?\nAaron Villere: While the Urban Street Stormwater Guide is much more focused on water quality rather than air quality, these are often overlapping benefits of green infrastructure programs. The Guide addresses this most directly in the discussion of Performance Measures for green street projects and programs.\nAdding trees and vegetation to the streetscape is an important tool for many cities seeking to expand urban tree canopy, which can reduce temperatures in the local climate, and reduce airborne particulates at the street and neighborhood level. One of the more important considerations in this regard is that micro-particulates disperse over fairly short distances, so providing vegetated buffer distance between the sidewalk or bikeway and the roadway can reduce the exposure of people biking and walking to motor vehicle pollution simply through spatial separation, while at the same time designers can select vegetation to capture and treat pollutants.\nMore broadly, it’s about building streets that are places people want to be. Safe and comfortable and green streets are streets that encourage people to walk, bike, and taking transit. There’s research that suggests people perceive shorter wait times for transit if there are green expressions in the streetscape. If the street is comfortable and inviting, cities are more likely to capture and convert trips to walking, biking, and transit, and when on-road transportation is responsible for a fifth to a quarter of all GHG emissions, reducing emissions coincides with mode shift away from motor vehicles to more efficient mobility.\nJessica Noon: I’d add that our Green Streets infrastructure typically includes trees/vegetation, so it has an air quality benefit that grey infrastructure does not. However, our program metrics are based on stormwater.\nHow are you tracking outcomes of success for Green Streets projects?\nSarah Anderson: Denver will track a number of Green Infrastructure Program metrics including pollutant loading reductions, reduced imperviousness, number of acres treated, number of acres of green/open space added, costs, number of trees planted/air quality benefits, neighborhood incomes, and plant species survivability.\nWho maintains these Green Streets?\nAV: As with any infrastructure project, planning for maintenance should be done as a part of the implementation process. While it depends a great deal by city and region what those maintenance strategies are, cities have been quite creative in allocating resources and finding partners to ensure that their assets are operating well, and they’re getting the most from investments.\nPhiladelphia, for instance, has a complete maintenance manual that enumerates deeper (more structural) tasks to be overseen by city-run crews, while vegetation and debris removal can be done by neighborhood partners and civic groups. Chicago has used GSI maintenance as an opportunity to partner on Greencorps Chicago, a public/private jobs training and employment program with both adult and youth participants, growing the green jobs economy while improving these facilities.\nOne last partnership structure is the public/institution structure. For instance, Louisville MSD partnered with the University of Louisville on their first projects to have the university study and report on maintenance practices. The agency (MSD) purchased new equipment to maintain special infrastructure, but the institution was able to fund monitoring and analysis, which cities often lack budget for. Finding ways to not just clean infrastructure but actually understand how that maintenance impacts performance is a critical capability to continuing to do better work.\nWhat federal funding programs or local funding incentives/reduced fees are these cities using to implement Green Streets?\nJN: Federal transportation funds from the Federal Highway Administration allow budgets to include up to 20% project costs for “drainage.” These funds, which cities and municipalities likely receive via their state transportation departments and/or their local MPO, can be used for green or grey stormwater management. Philadelphia uses them for green infrastructure as often as possible. Philadelphia also offers Stormwater Management Incentives Program (SMIP) grants to private property owners that retrofit their sites. The Water Department previously charged customers for stormwater as a percentage of the overall water bill, but in 2010 they changed that system to one that charges property owners based on the amount of impervious surface on site, so private property owners who retrofit are also eligible for reductions in stormwater fees by up to 80%.\nStatistics on the ability for green infrastructure to capture stormwater vary widely. Some way it only captures 20-30% of stormwater runoff, yet today’s presenters say capture is 80-100% when fully implemented. Can you clarify how this is measured?\nSA: In Denver, designs are based on the Water Quality Capture Volume (WQCV). The WQCV is directly related to the local rainfall pattern, watershed imperviousness, and drain time based on an analysis of rainfall and runoff characteristics for 36 years of record at the Denver Stapleton Rain Gage. The target for the WQCV is to absorb the runoff produced from 0.6 inches of precipitation, so the ‘80-100%’ refers to 80-100% of this WQCV target.\nWhen the presenters refer to storm percentage capture, what rain event are they referring to and designing to?\nAV: The percent stormwater capture is relative to a goal, typically set by a consent decree. While each place needs to set its own goals based on local climate and infrastructure conditions, a typical goal might be to manage a 10-year rain event, or the first inch of rainfall. The Guide does give guidance on these types of planning decisions, but they are ultimately local decisions. In many cities, that’s about reducing volume of runoff or peak flow rates (slowing the rate of stormwater discharge into traditional gray infrastructure), but can also include treating water for quality and pollutants.\nJN: We design for the management of 1.5 inches of runoff from the drainage area. There majority of storms in the Philadelphia region result in 1.5 inches or less of precipitation. In cases where we have sufficient space, we will design for over 1.5 inches, up to a maximum of two inches. We do not see a significant benefit to managing over two inches.\nCan you talk more about green gutters?\nSA: The 21st & Broadway project will construct Denver’s first green gutter. It is being installed to separate the bicyclist form the pedestrian. Denver will track the success/failures of the gutter as it relates to water quality and safety for different modes.\nStay tuned for future webinars\nOur Implementation & Equity 201: The Path Forward to Complete Streets webinar series is taking a brief hiatus in August, but we’ll be back with a jam-packed lineup starting in September. Stay tuned for more information! In the meantime, you can access all of our previous webinars on our blog.']	['<urn:uuid:d26c947f-b794-41d2-9c02-76f6b5c1f426>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	12	11	1816
70	As a conservation researcher, I'm interested in understanding both European wildlife monitoring and community-based conservation. How does ENETWILD's regional coordination structure for wild boar monitoring compare with the community engagement strategies used in mangrove restoration projects?	ENETWILD divides Europe into four main regions (South West, South East, North West, and North East) with specific coordinating institutions responsible for different countries, ensuring comprehensive data collection across the continent. In contrast, mangrove restoration projects implement community-based approaches that actively involve local populations, with particular attention to gender equity. While ENETWILD focuses on institutional coordination for data collection, mangrove restoration projects emphasize direct community participation, as seen in examples from Vietnam's Xuan Thuy National Park and Kenya's Mikoko Pamoja project, where local communities, especially women, are engaged in both management and monitoring activities.	"['Release of a model to collect data on wild boar distribution and abundance across Europe: the need for data standards\nThe Consortium ENETWILD (established through a framework contract funded by EFSA) is focusing on the collection of wild boar abundance and occurrence data for the analysis of the risk factors of African Swine Fever (ASF) spread, and for the assessment of effectiveness of wild boar management measures in the affected areas. ENETWILD now announces the release of a normalized wild boar data model (WBDM) to populate a common database, which later will be validated and shared.\nThe WBDM is composed of a list of fields to be recorded, including data and metadata (which describe data and information carried by the data). The WBDM is intended to be filled by data providers, as we identified you potentially are, representing all European regions and countries. We also encourage you to distribute further to anyone else able to provide such kind of data.\nENETWILD defined the data requested in this model and their technical specifications to offer a common frame to insert them; i.e. what we call the STANDARDS. These standards are essential in order to harmonize the way similar data coming from different sources is recorded and will assure wild boar data are correctly collected, shared; and finally used. This is why the model may appear complex at a first glace, but the effort and dedication we require will be very worthwhile, in order to make data comparable.\nThe data requested refer to wild boar (i) hunting statistics, (ii) density and (iii) occurrence, including absence data. Therefore, different data model should be used for each type of data (see links below).\nAs the same data can be used for more than one purpose (e.g. hunting statistics can be used to estimate occurrence data), for each type of data (if you manage more than one) we recommend filling only one data model, which must be the most informative one (in the former example that should be the WBDM_hunting). Hunting statistics and density data models, if correctly filled, can always be converted a posteriori into occurrence (presence/absence) data.\nAbsence data are also of particular importance in this project and should be reported, but must be associated with a sampling effort (e.g. abundance monitoring, hunting grounds without wild boar presence and catches). Absences will be recorded in WBDM_occurrence excel file.\nThe WBDM is accompanied by a Data Sharing Agreement (see links below), which must be signed by the data owner, in person or by a legal representative. We also attach a document detailing the general instructions to fill and submit the data.\nPlease before filling any data model, read the General instructions (see link below) carefully.\nFAQs will be available on the project website and assistance will be assured by ENETWILD reference partners and ENETWILD Helpdesk and the list the list of focal contacts (identify your region) provided in Annex 1 of the General Instructions document).\nThe time span of interest to the project is from 2013 (hunting season 2013-2014) onwards, and data collection will be a continuous activity for the next years (we ask you also to continuously provide not only already existing data, but data generated/collated in the future).\nDownload the files to fill in the Wild Boar Data Model\nIn order to be correctly filled, the spreadhseet files of data models should be downloaded (click >open online > download to your PC) don’t fill them online!\nOne of the two available versions should work from xls2007 to xlsx of office365. If the drop-down menus do not work properly, select the other excel available option. After the new window is opened, download the compressed files using the top left download button\n1. South West region:\nFrench National Hunting and Wildlife Agency (ONCFS)\nNational Institute on Wildlife Research (IREC)\nList of Countries: France, Spain, Belgium, Switzerland, Portugal, and Luxemburg.\n2. South East region A:\nUniversity of Torino (UNITO),\nList of Countries: Italy, Greece, Romania, Albania, Moldova, Cyprus, Turkey\nSouth East region B:\nUniversity of Sassari (UNISS).\nList of Countries: Italy, Slovenia, Serbia, Croatia, Bosnia & Herzegovina, Montenegro, Kosovo, Macedonia and Bulgaria.\n3. North West region:\nInstitute for Terrestrial and Aquatic Wildlife Research (ITAW),\nList of Countries: Germany, UK, Ireland, Netherlands, Austria, Czech Republic, Slovakia, Hungary, Denmark, Norway, and Sweden.\n4. North East region:\nMammal Research Institute Bialowieza (MRI).\nList of Countries: Poland, Finland, Russia, Belarus, Latvia, Lithuania, Estonia, Ukraine, Georgia, Armenia, and Azerbaijan.', ""Gender equity is key to mangrove restoration\nWomen and men in coastal communities are often closely connected to their coastal ecosystems and gender roles are often traditionally identified and clearly divided. Women and men differ in how they interact and depend upon mangroves – how they use the ecosystem, which mangrove products they choose, and the benefits they receive.\nPhoto: Courtesy Pixabay\nMangrove forests are particularly rich in directly harvestable seafood, timber, firewood and other plant products such as tea and roofing materials. They also provide vital protection against floods and storms and buffer against sea level rise. Especially in developing countries, coastal communities are directly dependent on products and services gained from mangrove forest ecosystems.\nUnfortunately, many years of unsustainable human use, largely driven by demand for resources and products on the international market, has led to large-scale overexploitation and destruction of mangrove forests. The consequences of disappearing mangroves have been particularly detrimental to the lives and livelihoods of the local people. Global conservation efforts are now engaging local communities in their actions to reverse this dangerous trend.\nAcknowledging the different roles women and men have in the ecosystem and community is essential in any conservation initiative. It allows for their differentiated inputs and impacts and promotes specific responses women and men could, and should, undertake. Despite today’s widely applied concept of integrating local communities in conservation efforts, there is a great risk of overlooking the particular interests and potential contributions of marginalised community members such as women. This can happen all too easily because women often have less social, economic and political power. Due to entrenched inequitable structures and barriers, they have a reduced influence in the decision making process within their communities.\nFortunately, integrating a gender-responsive approach into conservation efforts is currently gaining global momentum with significant results toward empowering women and enhancing gender equality. For women some of the potential consequences of a gender-responsive approach are increased food and water security, gained leadership and voice, improved health, security, education and skills development as well as improved livelihoods and income. IUCN’s Global Gender Office highlights and supports this approach to ‘ensure gender equality is central to sustainable global environmental solutions’ in a range of issues. Particularly around the conservation and restoration of mangrove forests, IUCN, its partners and other organisations are active in several projects working to integrate gender considerations and responses.\nMangroves for the Future (MFF), an initiative co-chaired by IUCN and the United Nations Development Programme (UNDP), is running mangrove restoration and sustainable development projects with gender integration as a core strategy in several Asian countries. Participatory, community-based project approaches ensure that women’s and men’s voices are considered equally and aim to improve women’s social and economic empowerment. Some MFF projects have supported women through sustainable livelihoods and financial leadership training which provides them with alternative livelihoods and income opportunities. For example, where women have received training on the advantages of cultivating mangrove plants, the resulting increases in fish stocks have provided an additional income opportunity. Moreover, their newly acquired financial skills have ensured higher business success.\nIn Vietnam, gender integrated management in Xuan Thuy National Park allowed local impoverished fisherwomen to build sustainable livelihoods while actively contributing to the park’s conservation. Before, these women depended on increasingly scarce harvests of shells, molluscs and crabs, gathered illegally within the park’s mangrove forests. Although now the women continue to collect those products – through sensitisation, awareness raising and training for gender equitable management, women have become actively involved in the management and monitoring of the resources – contributing to sustainable harvests and securing a sustainable future for themselves as well as the park.\nThe Guyana Mangrove Restoration Project saw women take a central leadership role with their efforts to increase the resilience of Guyana’s coasts against flooding and coastal erosion – a threat which mostly affected women. Women were provided with resources for economic empowerment and capacity building trainings, which allowed them to set up various businesses including honey production, tourism activities and mangrove cultivation. The active participation of women even led to the establishment of a women-led volunteer organisation for mangrove awareness and restoration as well as the ‘Mangrove Cooperative Society’ to support other women with training and resources on beekeeping.\nIn Kenya, women engaged in ‘Mikoko Pamoja’, a mangrove conservation and restoration project co-ordinated by ACES, maintain ‘The Gazi Women Boardwalk’ to promote conservation education within the mangrove forest. Through this initiative the women have proven their effectiveness in contributing to ecotourism while generating income for their community’s schools as well as contributing to better health care and reliable water supply.\nThese positive examples show that integrating gender equality into conservation initiatives is not only key to the success and sustainability of projects but can contribute additional value to its outcomes in supporting both women and men with various benefits for their homes, communities and nations. A gender integrative approach recognises women not as passive project beneficiaries, but as active drivers of change toward conservation, sustainable development and their own sustainable livelihoods.\nThis article series on mangrove restoration is written by Juliet Blum and Dorothée Herr from IUCN's Global Marine and Polar Programme with the support of Germany's International Climate Initiative (IKI) through the IUCN Global Forest and Climate Change Programme.\nToday, July 26 is World Mangrove Day and IUCN will be announcing powerful new collaborations to protect and restore the world's mangroves. Stay tuned!\nThis article series on mangrove restoration is written by IUCN's Global Marine and Polar Programme with the support of Germany's International Climate Initiative (IKI) through the IUCN Global Forest and Climate Change Programme.""]"	['<urn:uuid:55a20138-0d67-4c8e-a043-9dc0a1197086>', '<urn:uuid:595ded46-5481-40cc-b06b-c997cddc84de>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	36	94	1673
71	How do Buddhist traditions tackle addiction problems?	According to ancient Buddhist texts, craving leads to suffering but can be avoided through mindfulness meditation practice. In recent times, mindfulness-based interventions have been explicitly used to target cravings with the aim of bringing about clinically relevant changes to behavior. These interventions include exercises designed to promote greater awareness of bodily sensations, develop acceptance toward uncomfortable feelings, and help individuals see themselves as separate from their thoughts and emotions.	['Women who eat a high amount of fruits and vegetables each day may have a lower risk of breast cancer, especially of aggressive tumors, than...\nResearch suggests that certain mindfulness-based strategies may help prevent or interrupt cravings by occupying a part of our mind that contributes to the development of cravings.\nJanuary 30, 2018, London, UK—Mindfulness strategies may help prevent or interrupt cravings for food and drugs, such as cigarettes and alcohol, by occupying short term memory, according to a new review from City, University of London.\nLooking at experimental studies that have examined the effects of different types of mindfulness strategies on cravings, it was found that in many instances these strategies brought about an immediate reduction in craving. The research was published in Clinical Psychology Review.\nCraving can be defined as an intense, conscious desire, usually to consume a specific drug or food. There is also a significant body of research that suggests it is causally linked to behaviour. For example, craving predicts relapse episodes in substance use and food cravings predict both eating and weight gain. As such, cravings are often considered an appropriate target for intervention.\nMindfulness meditation has a long tradition of being used to address cravings. According to ancient Buddhist texts, craving leads to suffering but can be avoided through mindfulness meditation practice. More recently, mindfulness-based interventions have been used to explicitly target cravings with the aim of bringing about clinically relevant changes to behaviour.\nMindfulness interventions typically employ a range of different types of strategy, for example they may include exercises designed to promote greater awareness of bodily sensations, to develop an attitude of acceptance toward uncomfortable feelings, or to help individuals see themselves as separate from their thoughts and emotions.\nHowever, there is currently a limited understanding of the ways in which these different types of strategy may influence craving-related outcomes, either independently, or in combination. As a result, the review aimed to address these limitations by reviewing studies that have examined the independent effects of mindfulness on craving.\nLooking at 30 studies which met the criteria, it was found that some of the beneficial effects seen for mindfulness strategies in relation to craving are likely to stem from interrupting cravings by loading working memory, a part of our short-term memory which is concerned with immediate conscious perceptual and linguistic processing.\nIn addition, it was also seen that mindfulness reduced craving over the medium term, most likely due to ‘extinction processes’; essentially strategies that result in the individual inhibiting craving-related responses and behaviours which eventually lead to reduced cravings.\nDr Katy Tapper, author of the review and a Senior Lecturer in the Department of Psychology at City, University of London, said:\n“The research suggests that certain mindfulness-based strategies may help prevent or interrupt cravings by occupying a part of our mind that contributes to the development of cravings. Whether mindfulness strategies are more effective than alternative strategies, such as engaging in visual imagery, has yet to be established. However, there is also some evidence to suggest that engaging in regular mindfulness practice may reduce the extent to which people feel the need to react to their cravings, though further research is needed to confirm such an effect.”\nSource: University of London']	['<urn:uuid:f30941e5-8b65-4a85-ad50-443f1e1f81df>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	7	69	539
72	mortality rate stroke vs heart disease	While stroke is the fifth leading cause of death in the U.S., heart disease is the leading cause of death in the United States. Heart disease causes 1 in every 4 deaths, with one person dying every 36 seconds from cardiovascular disease. In comparison, strokes occur every 40 seconds in America, with an estimated 795,000 Americans having a stroke each year.	['We have all heard medical professionals talk about stroke, but do you really know the facts?\nAn estimated 795,000 Americans will have a stroke each year, with a stroke occurring every 40 seconds. Stroke is the fifth leading cause of death in the U.S. and the No. 1 cause of disability. Of the 7 million stroke survivors in the USA, approximately two-thirds of them are disabled. During a stroke, brain cells die at a rate of 32,000 per second. Seeking quick treatment is the key to survival and reducing disability.\nStroke is a process that affects the blood vessels leading to and within the brain. You could essentially call a stroke a “brain attack.” The most common type of stroke is from a blockage in a blood vessel. This type of stroke is called ischemic.\nThe blockage could be from a clot that forms in the brain or a clot that formed somewhere else in the body and traveled to the brain. A transient ischemic attack, also called a mini stroke, is from a blockage that re-opens spontaneously. Symptoms from a TIA will typically last less than 1 hour.\nNever miss a local story.\nThe least common type of stroke is from bleeding in or around the brain. This type of stroke is called hemorrhagic. Hemorrhagic strokes carry a larger risk of death than ischemic strokes.\nHow do you know if you are having a stroke? The American Heart Association and American Stroke Association have developed the slogan, Act F.A.S.T. to detect the most common signs of stroke.\n• F. The “F” stands for facial droop. Look in a mirror or ask the person to smile. Look for a crooked smile or drooping of one side of the face.\n• A. The “A” stands for arms. Raise both arms out in front of you and look to see if one starts to fall.\n• S. The “S” stands for speech. Are words slurred, mixed up, or not coming out quickly and correctly?\n• T. The “T” stands for time. If any of these symptoms are present, then its time to call 911.\nIf you think you are having a stroke the most important thing is to quickly seek medical treatment. Dialing 911 will give you the best chance at arriving to a hospital fast and safe.\nTreatments for ischemic stoke include medication called tPA, mechanical clot retrieval and rehabilitation. A patient is only eligible for tPA if they present to the hospital within 4.5 hours from the start of their symptoms. Hemorrhagic stroke treatment includes reducing swelling in the brain, controlling blood pressure, and aneurysm coiling and clipping.\nGet to the hospital quickly for the best treatment options.\nThe best way to beat stroke is to prevent it. An estimated 80 percent of strokes are actually preventable. It is important to “know your numbers” so that you can reduce your risk of stroke. The numbers include: blood pressure, cholesterol, blood sugar and body weight. Take all medication as prescribed by your doctor and schedule regular check ups to keep your numbers in the optimal range. Additionally, try to exercise or walk 30 minutes a day, don’t smoke, and decrease you intake of salty and fried foods.\nThe Lee’s Summit Health Education Advisory Board is a Mayor-appointed, volunteer board that promotes and advocates community health by assessing health issues, educating the public and government agencies, developing plans to address health issues, encouraging partnerships and evaluating the outcomes.\n“Health is a state of complete mental, physical, and social well being and not merely the absence of disease and infirmity.” World Health Organization', 'Heart Disease Facts\nLearn more about heart disease and its risk factors. It’s important for everyone to know the facts about heart disease pdf icon[PDF-243K].\nHeart Disease in the United States\n- Heart disease is the leading cause of death for men, women, and people of most racial and ethnic groups in the United States.1\n- One person dies every 36 seconds in the United States from cardiovascular disease.1\n- About 655,000 Americans die from heart disease each year—that’s 1 in every 4 deaths.2\n- Heart disease costs the United States about $219 billion each year from 2014 to 2015.3 This includes the cost of health care services, medicines, and lost productivity due to death.\nCoronary Artery Disease\n- Coronary heart disease is the most common type of heart disease, killing 365,914 people in 2017.4\n- About 18.2 million adults age 20 and older have CAD (about 6.7%).3\n- About 2 in 10 deaths from CAD happen in adults less than 65 years old.4\n- In the United States, someone has a heart attack every 40 seconds.3\n- Every year, about 805,000 Americans have a heart attack.3 Of these,\n- 605,000 are a first heart attack3\n- 200,000 happen to people who have already had a heart attack3\n- About 1 in 5 heart attacks is silent—the damage is done, but the person is not aware of it.3\nHeart Disease Deaths Vary by Sex, Race, and Ethnicity\nHeart disease is the leading cause of death for people of most racial and ethnic groups in the United States, including African American, American Indian, Alaska Native, Hispanic, and white men. For women from the Pacific Islands and Asian American, American Indian, Alaska Native, and Hispanic women, heart disease is second only to cancer.5\nBelow are the percentages of all deaths caused by heart disease in 2015, listed by ethnicity, race, and sex.5\n|Race of Ethnic Group||% of Deaths||Men, %||Women, %|\n|American Indian or Alaska Native||18.3||19.4||17.0|\n|Asian American or Pacific Islander||21.4||22.9||19.9|\nAmericans at Risk for Heart Disease\nSeveral other medical conditions and lifestyle choices can also put people at a higher risk for heart disease, including:\nCDC Public Health Efforts Related to Heart Disease\n- State Public Health Actions to Prevent and Control Chronic Diseases\n- Million Hearts®external icon\n- CDC: Heart Disease Communications Kit\n- American Heart Associationexternal icon\n- National Heart, Lung, and Blood Instituteexternal icon\n- Centers for Disease Control and Prevention. Underlying Cause of Death, 1999–2018. CDC WONDER Online Database. Atlanta, GA: Centers for Disease Control and Prevention; 2018. Accessed March 12, 2020.\n- Virani SS, Alonso A, Benjamin EJ, Bittencourt MS, Callaway CW, Carson AP, et al. Heart disease and stroke statistics—2020 update: a report from the American Heart Associationexternal icon. Circulation. 2020;141(9):e139–e596.\n- Fryar CD, Chen T-C, Li X. Prevalence of uncontrolled risk factors for cardiovascular disease: United States, 1999–2010 pdf icon[PDF-494K]. NCHS data brief, no. 103. Hyattsville, MD: National Center for Health Statistics; 2012. Accessed May 9, 2019.\n- Benjamin EJ, Muntner P, Alonso A, Bittencourt MS, Callaway CW, Carson AP, et al. Heart disease and stroke statistics—2019 update: a report from the American Heart Associationexternal icon. Circulation. 2019;139(10):e56–528.\n- Heron, M. Deaths: Leading causes for 2017pdf icon. National Vital Statistics Reports;68(6). Accessed November 19, 2019.']	['<urn:uuid:af82d2a7-46f6-4a3c-a38a-46066e2285b0>', '<urn:uuid:6a7f99f9-4446-421d-b362-6b1a305eaa94>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T21:09:04.591601	6	61	1140
73	What are different ways to maintain and protect garden beds from weeds and water loss throughout the year?	There are several effective methods to maintain and protect garden beds. First, apply mulch using materials like pea straw, sugar cane mulch, or composted woody material. For the edges of beds that struggle to retain mulch, you can plant annual plants to prevent weed invasion and reduce evaporation from sun exposure. Another important practice is to let fallen leaves remain in place to decompose naturally, adding to the soil's richness. If the leaves are large, you can rake them into the lawn, mow them, and return them to the beds as chopped material for quicker decomposition. For ongoing maintenance, you can make monthly foliar applications during the growing season (April through September) with fish emulsion, liquid humate, molasses, kelp/seaweed, or worm tea.	"['Contour beds are annual and/or perennial vegetable garden beds that conform to the natural pattern of the landscape. Being on contour means that the paths and beds themselves are level and follow the lay of the land. Not only does this create an attractive pattern on the landscape this technique more importantly allows us to slow, spread, and sink water into our garden beds in a similar way that swales do. This orientation also prevents erosion due to the pacifying of any surface runoff.\nTo create contour beds one must be able to accurately measure and peg out the contours. This can be done by a number of ways; using a laser level, a water (bunyip) level or an ‘A-frame’. Not having access to a laser level we built an A-frame out of some scrap wood and wire.\nAn ‘A-frame’ is a simple and easy to build device consisting of 3 pieces of wood/metal, a string and a weight such as a fishing sinker or large fastening nut. It is built by connecting the 3 pieces of wood together to form an ‘A’ shape. They can be fastened together in a number of way: nuts and bolts, wire, or metal can be welded together. The important thing is that the structure is solid and will not move and thus disturb its calibration. A string is then tied to the top of the frame with a weight on the bottom which must hang below the cross member.\nTo calibrate an A-frame place it on the ground and mark where the string settles on the cross member. Now flip the A-frame around so that the legs of the frame are in exactly the opposite location. Again mark where the string hangs. In-between the two marks make a distinct and solid mark, this mark is your level mark.\nUsing the A frame is easy, simply walk across the landscape and position the A-frame so that the string hangs over the level mark, place markers such as pegs or sticks at the legs as you go. The imaginary line between all the pegs will give you the contour.\nCreating contour beds\nBegin digging along the contour creating a path and mounding the soil on the lower side making the bed. Create the beds at a height of 30cm and 120-140cm in width. This width allows for one to reach the middle of the bed by without standing on the bed itself compacting the soil. The paths between the beds should be large enough to accommodate a wheelbarrow (40-60cm) for harvesting and composting, and can be dug out slightly to a depth of 20-30cm. The removed soil is used on the bed and paths filled with woodchips, sawdust or gravel. Woodchips will slowly breakdown and can later be used as compost. Water will always find level, thus for precise levelling flood the paths with water and correct their level using a shovel.\nAdded into the soil layer should be any organic soil amendments (compost, manure, bone meal, kelp meal, rock dust, etc.) that are needed. In this case we added sifted compost from a 2 cubic meter hot compost pile we started 18 days prior, and guano de isla which were gently mixed into the top 50mm of the beds. Later we drenched the beds with compost tea, and applied the remaining compost. The soil layer should be about .5 m. deep from the bottom of the paths, and watered heavily. Note that in arid climates this technique is inverted with the beds sunken to reduce water loss through evaporation.\nNext mulch your beds with pea straw, sugar cane mulch, or composted woody material, and begin planting! We used barley that had been previously used as a substrate for growing oyster mushrooms. It was a pleasant surprise to find large oysters mushrooms growing out of the mulch after heavy rains. The batter or edges of the beds can struggle to retain mulch due to their slope exposing the beds to weed invasion and evaporation due to sun exposure. To combat this we sew a range of annuals including brassicas and coriander from out of date seed we had, which has greatly contributed to the yield of these beds.\nApart from harvesting and the occasional application of worm juice, these beds thanks to the heavy mulch have been maintenance free for months now, and have provided us with an abundance of Tomatoes, Squash, Zucchini, and greens.', ""Practice No. 5. Revitalizing Beds and Borders\nMany of our landscapes are already in place or have been in existence long enough where some revitalization needs to take place. Revitalizing unamended beds and borders presents a more difficult task than if one started a bed from scratch. It would be ideal to incorporate 3 inches of compost for low water-use zones, 4 – 8 inches for medium water-use zones and 6 – 12 inches for the high feeding, high water-use zones, such as vegetable beds and bluegrass/fescue turf. This is much easier to do in beds of annuals or vegetable gardens where one starts fresh each spring (or fall) than in shrub and herbaceous perennial beds. Revitalizing established beds and borders that haven't previously been amended is a longer term project, not a quick fix.\nAdding organic matter throughout a bed is admittedly difficult in heavily planted beds and borders. Carefully digging or aerating around established plants is necessary, so as to minimally disturb the roots. If your bed has plants that can be dug up before revitalization and replanted after amending the soil, do so.\nPull back or scrape off and save the mulch cover. Depending on the amount of clear ground and maturity of the remaining plants, either dig with a garden fork loosening the soil and apply 3 inches of compost, carefully digging this into the soil, trying not to disturb roots. If you find this isn't practical, scratch the surface of the soil with tines before laying down the compost or compost blend. Even just plunging the fork into the soil will begin to aerate it, with compost or wormcastings falling down into the holes just created with the garden fork. Adding organic amendments in this manner may need to be made over a period of years.\nCompacted clay and caliche soil will benefit with the addition of inorganic material. I recommend adding an inorganic amendment such as Turface® (calcined clay), Tru-Grow® (expanded blue shale), Ecolite® (Zeolite) or Axis® (diatomaceous earth) for heavy clay soil, and Profile® for sandy soil or other brands of the same materials. Follow the recommendations on the bag for quantities to add per square yard. Other inorganic amendments with much more limited ability to retain water and nutrients are crushed granite, granite and lava sand, greensand, glass sand, and finally, regular sand. Be careful in adding inorganic amendments with low water and nutrient holding capacities to soils with low organic content, such as sands; organic matter must be added in conjunction with them. Otherwise you could end up with worse soil, mimicking brick.\nIn bed revitalizations, renovations, makeovers, and annual beds, the ideal time to add the compost is in the fall, giving the microbes the winter months to repopulate and grow. If the plants suffered root damage in the revitalization process, they will have the cooler winter months to recover before the major stress of summer is upon them. Monthly foliar spraying during the growing season with fish emulsion, liquid humate, molasses, and/or kelp/seaweed, trace minerals, aerobically activated compost tea, worm tea and other microbial stimulants are beneficial when getting started.\nIf you find you are unable to do any digging or scratching of the top of the soil, mulch yearly with a top quality compost blend from 2 to 3 inches thick and make at least three foliar applications during the growing season. Monthly, for three years, would be better (April through September). Better results will be realized sooner with the incorporation of solid organic matter into the soil. Replenish the mulch cover as needed.\nWhenever replanting in beds or borders, refurbish the plant hole with Yum-Yum Mix®, compost and/or other inorganic and organic amendments.\nMy low water-use beds that were amended in 2000 are just as vigorous years later as at the first. The only revitalization I do is broadcasting Yum-Yum Mix® and granular humate to the top of the bed once a season. I include these beds along with the others when doing any foliar spraying.\nLet the leaves fall and remain in place to decompose in the beds adding to the soil's richness. If the leaves are large and thick, rake them into the lawn and mow, returning them to the beds and borders chopped and shredded. They will decompose quicker and not mat down. For spring clean up, if you wish a tidier look, rake the leaves that have blown in over the winter, mow, and return to the beds and borders.\nI've practiced this procedure of leaf recycling throughout my garden. One day after about 5 to 6 years, I dug into a shrub bed that included a tree to see what the soil felt like. Having grown up in Wisconsin and walked the forests floor, I was surprised to find this border floor nearly as soft and porous, filled with life. It can be done if you give nature a chance.""]"	['<urn:uuid:9175cb6b-8210-40c2-9197-7ab38f9ae60c>', '<urn:uuid:207a47d5-655d-42e1-ae0d-034b3b464c0a>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:09:04.591601	18	122	1558
74	robot humanoid motion guidance autonomous vs operator control comparison	While early humanoid robots required operator control, advancements have enabled autonomous motion guidance. The new ASIMO model can walk about and make spatial decisions on its own, using multiple sensors to evaluate surroundings and predict changes, without being controlled by an operator. In contrast, traditional humanoid robots rely on sensors and actuators placed at strategic points to guide their movements and gestures based on programmed instructions.	"['The use of humanoid robots is becoming widespread all over the world. Humanoid robots are designed to mimic the human body and differ from other kinds of robots such as industrial ones in that their movement is human-like, based on legged locomotion, especially biped gait. They move about the “real world” and interact with it, performing a growing diversity of specialized and everyday tasks, unlike factory manipulators and other robots that work in highly structured environments.\nSignificantly, the Indian Space Research Organization (ISRO) plans to send a humanoid robot into space towards 2020-end as part of a crewless mission. The humanoid called Vyommitra, a legless female robot, will help ISRO prepare for its Gaganyaan manned space flight mission, the country’s first attempt at sending humans into space slated for 2022. Before accomplishing this mission, ISRO will send Vyommitra, which can speak but doesn’t move much, into space.\nWhat’s a humanoid robot?\nA humanoid robot’s overall appearance and body shape is built to resemble the human body. Generally, humanoid robots have a torso with a head, arms, and legs, though some of their types may have only some specific human parts, for instance, from the waist up. Some forms of robots have heads designed to replicate human facial features such as the face, eyes, and mouth. Humanoid robots built to aesthetically resemble a male human are termed as androids, while female counterparts are called Gynoids.\nLeonardo Da Vinci is credited with creating one of the earliest forms of humanoids in 1495. Modeled on an armor suit, it could perform several human functions such as sitting, standing, and walking.\nHow do humanoids move, talk, and carry out actions?\nInventors and engineers study the human body structure and behavior (biomechanics) and attempt to simulate human cognition, which relies on sensory information to acquire perceptual and motor skills. First-grade sensors and actuators are deployed to enable humanoids to perform multiple functions. Based on computational models of human behavior, the sensors aid robots in sensing their environments, while cameras allow them to see clearly. Motors or actuators placed at strategic points guide these robots in moving and making gestures. Creating their fully functional and realistic versions necessitates the following mechanisms:\nSensors measure attributes of the human world. Besides the essential requirements of planning and control, sensing plays a significant role in robotic paradigms. To help the humanoids sense their environment, sensors empower them with the ability to touch, smell, sight, hear, and balance themselves properly.\nWhile the hearing sensor helps humanoids hear, decipher, and carry out instructions. Thanks to the touch sensor, they are prevented from bumping into things and causing self-damage. A force sensor helps them maintain balance and orientation, and heat and pain sensors enable them to know the impending harm or damage. Also, there are facial sensors that make humanoids capable of a wide range of expressions. Therefore, sensors can be categorized under the physical process with which they work or according to the type of sensory information that they give as output.\nScientists are continually working to make the sensors more efficient for accomplishing multiple tasks. They have turned their gaze on proprioceptive sensors (e.g., touch, muscle extension, limb position) to sense the position, the orientation, and the speed of the humanoid’s body and joints. Some of the areas that have received increasing emphasis include: accelerometers to measure the speed from which velocity can be computed by way of integration; tilt sensors to measure inclination; and force sensors placed in robot’s hands as well as feet to measure contact force with the environment.\nAttention has also been given to position sensors that indicate the actual position of the robot (from which the velocity can be calculated by derivation) or even speed sensors. Tactile sensors provide information about forces and torques transferred between the robot and other objects; they use arrays of tactels to provide data on what has been touched.\nIn humanoid robots, vision sensors work to recognize objects and determine their properties. CCD cameras, which use the electromagnetic spectrum to produce an image, are used as the sight faculty of humanoids. Usually, microphones are deployed to enable the functionality of sound sensors that allow the robots to hear speech and environmental sounds.\nActuators or motors\nActuators or the motors responsible for motion in the robot, help robots move and make gestures akin to the flexible human body. Strong, efficient actuators can perform a wide range of actions, much like humans or even better.\nHumanoid robots mainly utilize rotary actuators that perform human motions, much like muscles and joints, though with a different structure. Actuators can be either hydraulic, electric, piezoelectric, ultrasonic, or pneumatic.\nHydraulic actuators operate at low speed and high load applications. Electric coreless motor actuators are better suited for high speed and low load applications, though both can only be made to act in a compliant manner through rigid control strategies.\nPiezoelectric actuators, on the other hand, can produce a small movement with a high force capability after the application of voltage. They are capable of ultra-precise positioning and for generating and handling high forces or pressures in static or dynamic situations.\nUltrasonic actuators generate movements in a micrometer order at ultrasonic frequencies (over 20 kHz). They can be used for controlling vibration, positioning applications, and quick switching.\nPneumatic actuators rely upon gas compressibility to function. Inflated, they expand along the axis, and when deflated, they contract. When one of their ends is fixed, the other will move in a linear trajectory. Intended for low speed and low/medium load applications, pneumatic actuators comprise cylinders, bellows, pneumatic engines, pneumatic stepper motors, and pneumatic artificial muscles.\nAfter the mechanisms that imitate human body parts are put in place, inventors program the instructions and codes that would enable the humanoids to carry out specific functions. Powered by Artificial Intelligence (AI), they can glide around and give replies when asked questions.\nAI is critical to enhancing the level at which humanoid robots can interact with humans. It can make them decipher commands, questions, indication, even understand random, ambiguous statements and give replies laced with wit and sarcasm.\nFunctions of humanoid robots\nInitially, AI was utilized in humanoids for research and experimental tools in several scientific areas such as the study of bipedal locomotion to explore ways to create leg prosthesis, ankle-foot orthosis, biological realistic leg prosthesis, and forearm prosthesis for the neuromuscularly impaired. Some were created for entertainment purposes of singing, playing music, dancing, and speak to audiences.\nNow, the aim of humanoids has extended beyond research and experimentation to functional purposes such as performing various human tasks, such as interacting with human tools and environments and occupying different roles in the employment sector. They are an increasingly common feature in the workplace and can perform human tasks and act as personal assistants, receptionists, front desk officers, and automotive manufacturing line workers. They can help out at homes to assist the sick and elderly as household helps and nursing assistants, perform dirty or dangerous jobs, play and use tools, operate equipment and vehicles designed for the human form.\nThese life-like robots could also prove useful in helping out the children, or any person who needs assistance with day-to-day tasks or interactions. There have been many studies pointing out the effectiveness of humanoid robots supporting children with autism.\nIt has been decided by various countries to send humanoid robots for dangerous and distant space exploration missions, without needing to turn back around again and return to the Earth once the mission is completed. In essence, robots can perform any task a human being can, thanks to AI algorithms.\nScientists are striving to reduce energy consumption in humanoid movements. In this context, studies on dynamics, control, and stabilization of walking biped robots on the surface have acquired crucial importance. Equally important is the maintenance of the robot’s gravity center over the center of the bearing area for providing a stable position.\nAs a humanoid needs information about contact force and its current and desired motion to maintain dynamic balance during the walk, the Zero Moment Point (ZMP) is an essential balancing approach that has been receiving the attention of inventors. Also, they are focussing on planning and control to allow humanoids to move in complex environments, armed with the knowledge of self-collision detection, path planning, and obstacle avoidance.\nHumanoid robots include structures with variable flexibility that provide safety to the robot itself as well as to the people, more degrees of freedom, and wide task availability. To optimize these functionalities, scientists plan to further hone planning and control strategies in the functioning of robots.\nEngineers at MIT and the University of Illinois at Urbana-Champaign have developed a method to control balance in a two-legged, teleoperated robot. It marks an essential step toward enabling a humanoid to carry out high-impact tasks in challenging environments. The robot is controlled remotely by a human operator donning a vest that relays information about the human’s motion and ground reaction forces to the robot. Through the vest, the human operator can direct the robot’s locomotion and feel its motions too. If the human feels that the robot is starting to tip over, she can adjust in a way to rebalance both herself and the robot.\nIn Japan, Prof Hiroshi Ishiguro of Osaka University and his team members have developed a humanoid robot with the ability of a human-like conversation. In the ERATO ISHIGURO symbiotic human-robot interaction project, they focused on the affinity process that emerges during the movement of the robot with a human. Towards this end, they developed a child-like android named “ibuki” who could walk together with the human by using equipped wheels.\nHumanoid robots can talk like us, walk like us, and express a wide range of emotions. Some of them can engage in a conversation; others can recall the last interaction you had with them. With constant advancements in AI, humanoid robots are all set to acquire more developed human attributes and competencies. Advanced android robotics is all set to facilitate the dramatic enhancement of life going forward.', 'While ASIMO may look more-or-less the same as he/she/it always has, the new model encompasses several advancements. For starters, ASIMO longer needs to be controlled by a human – it can walk about and do stuff all on its own, taking in its surroundings and making spatial decisions accordingly. Its new arm and hand mechanisms also allow it to perform delicate and intricate tasks like opening a bottle of water and pouring it into a paper cup without crushing it under the might of its own technological superiority.\nThe long and short of it is that ASIMO – now under the newly-formed Honda Robotics division – will soon be able to do everything a human can, only better. So start stocking the bomb shelter again. Or better yet, let ASIMO do that for you. You\'ve had a long day.\nClick past the jump to let ASIMO pour you a drink and show off some of its newfound skills.\nHonda has unveiled an all-new ASIMO humanoid robot with a world-first technology allowing ASIMO to move without being controlled by an operator. With significantly improved intelligence and the physical ability to adapt to situations, ASIMO has taken another step closer to practical use in an office or public space.\nHonda has also introduced a new task-performing robot arm. This experimental model was developed while applying multi-joint and posture control technologies to ASIMO. This robot arm can be controlled remotely to perform tasks in places which are difficult for people to access.\nTo represent all of Honda\'s robotic technologies and products such as ASIMO, Honda has established the new collective name \'Honda Robotics\'. Under this name Honda will continue the development and research of humanoid robots and will also focus on applying the robotic technologies to mass-produced products and putting them into practical use.\nThe all-new ASIMO has taken a step forward from being an ""automatic machine"" to an ""autonomous machine"". It now has the ability to make decisions based on its surroundings and the movements of people.\nAt the beginning of the development process, the following three factors were identified as necessary for a robot to perform as an autonomous machine:\nHigh level balancing ability - maintaining its posture by putting out its leg in an instant.\nExternal recognition capability - takes in information, such as the movements of people, from multiple sensors and predicts the changes that may take place.\nThe ability to generate autonomous behaviour - making predictions from gathered information and determine the next movement without being controlled by an operator.\nOnce these had been identified Honda went about developing the technologies required to make them possible.\nAdvanced intelligence capability\nHonda has developed a new system that is key for advanced intelligence. The new system continuously evaluates the input from multiple sensors, predicts the situation and then determines the behaviour of the robot. With this technology ASIMO is now capable of responding to the movement of people and the surrounding situations. For instance, ASIMO can now predict the direction a person will walk and quickly determine an alternative path to take if a collision is detected. This technology also enables ASIMO to recognise faces and voices.\nAdvanced physical ability\nThe combination of strengthened legs, an expanded range of leg movement and a newly developed control technology enables ASIMO to change landing positions mid-movement. This new agility also gives ASIMO the flexibility to adapt to changing situations so that it can walk on uneven surfaces.\nImproved task-performing ability\nHonda has developed a highly functional compact multi-fingered hand, which has a tactile sensor and a force sensor imbedded on the palm and in each finger. Combined with the object recognition technology, this multi-fingered hand enables the all-new ASIMO to perform tasks with dexterity, such as picking up a glass bottle and twisting off the cap, or holding a soft paper cup without squashing it. Moreover, ASIMO is now capable of making sign language expressions which require the complex movement of fingers.\nTask-performing robot arm\nTechnologies developed for ASIMO were applied to an experimental model of a task-performing robot arm. The robot arm moves on a self-propelled base and was designed with the idea of being utilised at disaster sites or other places too dangerous for people to work.\nThe stability control technology used for ASIMO\'s walking and running were applied to stabilise the end of the arm even on unstable surfaces. The application of other ASIMO technologies such as the multi-joint control technology that controls as many as 57 motors imbedded in the joints of the arms and legs, has enabled the robot arm to approach an object and perform necessary tasks even in a narrow space with unstable footing and many obstacles.']"	['<urn:uuid:0ad66357-c679-40aa-a400-ecbf874f16ac>', '<urn:uuid:53898931-f3c7-479a-ac09-8e8707732c0f>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	9	66	2455
75	arctic ocean winter ice thickness decline percentage 1975 2012 center	Between 1975 and 2012, the ice in the centre of the Arctic Ocean thinned by 65 percent during the winter months.	['When Inuk hunter Joseph Monteith went through the ice in Frobisher Bay, he had seen the signs coming long before. But it was already too late.\nThe first domino, as Monteith puts it, had already fallen.\n“Our first indication that the ice was thin was when the back of my skidoo broke through the ice,” he recalls.\nNot knowing which way was safe, Monteith and his hunting partner Kelly Akpaleapik charged forward in an attempt to get out of the bad ice. They soon realized that they had actually gone deeper into the rotten ice. Another domino.\n“At that point I started to panic and started to pick up speed.”\nThe pair found themselves with a giant crack ahead, and nothing but bad ice behind them. They came to a stop, hopeless, and the series of events that had begun with one lapse in navigation reached its inevitable conclusion: the ice beneath them gave way, plunging the pair, their skidoo and supplies into the briny Arctic water.\nData to the rescue\nThe serious situation in which Monteith and Akpaleapik found themselves is not unique; in fact, it’s increasingly common in the Arctic.\nThe warming Arctic is causing the sea ice to become thinner, cover a smaller part of the ocean and to last for less time. The maximum sea ice extent reported by NASA in March was the second lowest ever recorded. The lowest ever was in 2017, representing a continuation of a decades-long trend of shrinking ice.\nThat trend is represented in the thickness of the ice as well. Between 1975 and 2012, ice in the centre of the Arctic Ocean thinned by 65 per cent during the winter. In the shoulder seasons, when the ice is at its most dangerous, the situation is much worse: September ice thickness shrank by 85 per cent during that same four decades.\nIn 2010, a warm, rainy spring in Labrador created a dangerous scenario on the ice local communities depend on for hunting and travel.\nBy the summer, one in 12 people surveyed in Nain, Labrador, had fallen through the ice that year. So they reached out to Trevor Bell, who was then developing research priorities for the Nunatsiavut Government, for help.\n“People were afraid to travel on their traditional trails,” Bell told DeSmog Canada. “People were afraid to use the ice.”\nBell began work on what would become known as SmartICE. The system includes electronic buoys frozen into the ice near communities as well as mobile devices, so-called SmartQAMUTIKs (named for the traditional Inuit sleds), all of which gather and relay information about ice thickness and snow depth.\nThat information is collected into colour-coded, intuitive maps that can be downloaded onto computers or mobile devices to help people plan their travel across the ice.\nThe maps are augmented with a legend developed by Inuit communities in order to make the most sense and convey the most relevant information to the Inuit using them.\nThat stands in stark contrast to the former best-available technology — government-produced sea ice maps — which are oriented towards the shipping industry rather than northern residents.\n“It’s not intuitive for Inuit because thick ice, which is safe to travel on, is coloured red, because of course that ice presents a hazard for shipping,” said Bell.\nThe government maps are also not produced in the winter, because shipping isn’t happening then, and use complicated jargon.\n“We’re filling that void by providing something at the right scale, produced at the right time,” he said.\nPrecarious pathways to sustainability\nThere are currently nine communities using SmartICE, and another 12 on a waitlist.\n“I get a call a week from different communities who want smartICE,” said Bell.\nExpanding the program means building new buoys (around five or 10 per community), new qamutiiks, and training more people in using the technology. That expansion is expensive. But it also benefits northern communities in other ways, says Bell: there are now plans to create a production centre in Nain, where the high-tech tools will be built.\nTo fund the expansion, Bell wants to partner with businesses across the North that would benefit from more reliable and consistent information about sea ice conditions — industries like tourism, fishing, or shipping.\n“They all depend on safe sea-ice travel, and a secure ice platform from which to conduct their operations,” he said, adding that a single tourist going through the ice could set the tourism industry back decades.\nSmartICE has already partnered with an Arctic Bay-based outfitter, Arctic Bay Adventures, to make their operations safer, and, as Bell puts it, “reduce the risk of climate change impacts” on their business.\nGovernment has been part of the program since the beginning, with different agencies and institutions giving money at different times. But Bell believes there is a longer-term role for the government, one that would be mutually beneficial.\n“If I had enough ministers in a room I could make an argument that this makes sense,” he said. “We have the ability to avoid search-and-rescue operations in the North.”\nAerial search-and-rescue units are based in Trenton, Ontario, and sending help can be very costly. One particularly dramatic example in 2013, in which a group of tourists and guides were trapped on an ice floe, reportedly cost the government more than $2.7 million.\nFor Monteith and Akpaleapik, rescue operations were conducted from nearby Iqaluit.\nIn Inuit culture, Monteith says, it’s important to return to the site of an accident in order to move on. He has since returned to where he went through the ice, and the island where he spent an excruciating night awaiting rescue. For Monteith and Akpaleapik, the rescue operations were conducted from nearby Iqaluit.\nThe pair had dragged themselves out of the water, the ice crumbling beneath them over and over — for Monteith, nine times — and plunging them back into the frozen bay. They walked six hours to an island just two kilometres away, and spent the night huddling together and burning grass and shrubs for warmth.\nMonteith doesn’t know whether SmartICE could have prevented him from suffering that fate, an event that still haunts him with post-traumatic stress disorder. But he acknowledges that, for Inuit, “doing your homework about the area you’re going to go to” is a core part of the practices that have been passed down through generations to keep hunters safe.\n“It’s a business model that’s consistent with Inuit societal values — caring for the environment, caring for the community,” he says.\n“I don’t think I need to explain why ice is important for Inuit.”']	['<urn:uuid:c7c73363-f121-4339-b914-17f79a549938>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	10	21	1095
76	compare skills needed architect vs school psychologist professional training research required	Both architects and school psychologists require extensive professional training and research skills, but in different areas. Architects need to be good researchers, listeners, speakers, writers, and organizers, with skills in project management and design. School psychologists must be trained in psychological assessment, intervention, education, consultation, and research methodology, particularly focusing on child development and learning processes. Both professions require extensive education - architects need a professional degree plus three years of internship, while school psychologists typically need a doctorate (Ed.S.) requiring 6-7 years including coursework, internship, and dissertation research.	"[""The Architecture Profession\nThe profession of architecture is a sacred trust. Architects create the buildings and cities where we live our lives. They design the places where we dwell, raise families, play, work, dream, and relate to one another. These environments are the settings, the containers for the transactions, rituals, events, and celebrations of individuals, families, groups, societies. The built environment affects our health, safety, mood, productivity, our sense of belonging, well-being, and personal value. Furniture, rooms, buildings, campuses, plazas, streets, and neighborhoods give us clues about how to behave, relate, conduct our day. They shape the ways we live each occasion, each day of our life.\nAll this is about designing and building environments for human habitation. Architects are privileged to work at that magic point where human dreams and needs are transformed into concrete designs and ideas, visions of built shapes in real materials. Well-planned environments enhance our quality of life, facilitate communication, promote convenience and productivity, nurture our spirit, and delight our senses. They enable, empower us to live well and do our best work. Successful architecture is kind to the earth, responding sensitively to nature and ecological systems. A good building is a good neighbor and a good steward. It uses resources mindfully and contributes positively to its site and adjacent properties. The well-built building is technically sound, constructed to last and hold value over the long term.\nGood architects are not just good designers. They possess a wide range of skills, knowledge, and sensitivities that are essential to organizing and managing the building-delivery process.\nGood architects are good listeners, articulate speakers and writers, incisive thinkers, and excellent organizers. They are good with people, able to lead project teams, and help diverse client groups reach consensus and envision exciting project possibilities. Successful architects are good researchers, creative visionaries, and aware of the many consequences of their design decisions. Good architects continually seek to expand their vocabulary of project analysis tools and their reservoir of design answers to project requirements and challenges. Responsible architects balance technical skill with artful sensitivity. They ask penetrating questions, see the larger picture, and guide projects through complex situations to desired results.\nArchitects are responsible for the management and stewardship of many types of important resources. Money, time, natural resources, energy, property, manpower, prestige, and reputation are a few. This is why architects are lifelong students, constantly striving to know more, be more, manage more effectively.\nArchitecture is a demanding and satisfying field -- a wonderful way to spend a creative lifetime that contributes to people's lives in real ways, a way to join and participate in a sacred trust. To become an architect, you must:\nThe Florida A&M University School of Architecture professional architecture degrees are accredited by the National Architectural Accrediting Board (NAAB) and can be your first step toward becoming an architect.\n- Earn a professional degree from an accredited school of architecture,\n- Perform three years of internship under the supervision of a licensed architect, and\n- Pass a professional licensing exam to practice architecture."", 'School psychology focuses on emotional, psychological, learning and/or behavioral issues faced by school-age children. School psychologists work in a variety of settings such as: schools, clinics, hospitals, daycares, correctional facilities and social service agencies. School psychology combines educational and clinical psychology when diagnosing and treating children and adolescents.\nMost states require that school psychologists have an advanced degree (Doctorate of Education – ED.S.) in school psychology or a related field. Some states only require a master’s degree in school psychology to practice as a licensed school counselor, but this is rare. A master’s degree is always required to be licensed as a counselor. Although extremely rare, a few states will allow an individual to practice as a school counselor if they hold a bachelor’s degree in school psychology or a related field.\nThe following courses are normally covered in an undergraduate school psychology program:\n- Introduction to Psychology\n- Child Psychology\n- Psychology of Children with Disabilities\n- Adolescent Psychology\n- Psychology of Abnormal Behavior\n- Social Psychology\n- Counseling and Psychotherapy\n- Personality and Assessment\n- Learning and Cognition\nAn undergraduate program in school psychology teaches you how to correctly assess family, behavioral, emotional, psychological and/or academic issues that affect children and adolescents. It is imperative to address emotional and psychological issues in children as soon as they occur as to avoid more serious issues in the future.\nWhile in your 4-year undergraduate school psychology program, you will learn basic psychology principles and how to assess and treat emotional and learning problems in school-age children. It is important to note that to work as professional school psychologist, you will need to further your education and eventually obtain a master’s degree or doctorate in school psychology.\nThe following courses are normally covered in a graduate school psychology program:\n- Orientation to Graduate Learning in Psychology\n- School Psychology\n- Tests and Measurements\n- Inferential Statistics\n- Research Methods\n- Child and Adolescent Psychology\n- Cognitive Psychology\n- Multicultural Perspectives in Human Behavior\n- Psychological Assessments\n- Principles of School Psychology\n- Consultation and Collaboration in School\n- Legal and Ethical Issues in School\nGraduate Degree Requirements\nA graduate school psychology program teaches you the core competencies (psychological assessment, intervention, education, consultation and cultural diversity) that you need to be successful as a school counselor or school therapist. To be admitted to a master’s level school psychology program, you will first need to acquire a bachelor’s degree from a regionally accredited institution. It will take you approximately 2 to 3 years to successfully complete a graduate degree program in school psychology.\nThe following courses are normally covered in a doctoral school psychology program:\n- School Psychology\n- Sociological and Cultural Foundations of Education\n- Cognitive Processes and Learning\n- Educational Leadership and Organizational Theory\n- Research Methods\n- Statistics and Quantitative Data Analysis\n- Human Growth and Development\n- Developmental Child Psychopathology\n- Positive Behavior Support\n- Assessment and Intervention in School\n- Standardized Measures of Academics and Behavior\n- Assessments, Interventions and Curriculum-Based Measures in a School Setting\nDoctoral Degree Requirements\nA doctoral (Ed.S.) degree program in school psychology teaches you the knowledge, attitudes and skills needed to provide quality services to children and adolescents. During your doctoral program, you learn the core psychological and educational competencies (cognitive, academic, psycho-educational interventions, personality assessments, research methodology and statistics) that you may need to work in a variety of child-related settings. You may also be required to complete an internship (professional practice at a school, etc.) and maintain a 3.0 GPA on a 4.0 scale each semester.\nMoreover, during your first semester of internship, you may be required to take the school psychology PRAXIS II exam. Do not forget to have your scores sent to your program coordinator. You may be able to complete a school psychology doctoral program in 5 years, although most students take between 6 and 7 years to complete it. In fact, most schools give you 7 years to complete the program. Your program consists of a combination of coursework (3 years), supervised internship (1 year) and dissertation development (2 to 3 years).\nAs a School Psychologist What Will You Do?\nAs a school psychologist you have specialized training in both education and psychology. You use your training, skills and expertise to consult with parents, educators, school administrators, social workers, therapists and/or psychologists to ensure that every child has an opportunity to learn in a safe, supportive and productive environment. You are also responsible for assessing school systems and researching effective teaching styles in an effort to accurately gauge how students learn.\nYour training consists of graduate-level coursework, a supervised clinical internship and a dissertation (an extensive research paper on a school psychology topic). You are trained in the areas of: child development, mental health, school organization, behavior, learning processes and motivation. To work as a school psychologist, you must be certified and/or licensed by The National School Psychology Certification Board or the state in which you practice.\n- Child Social Work Careers\n- How to Get a Degree in Educational Psychology\n- How to Become an Elementary School Psychologist\n- How to Become an Academic Counselor']"	['<urn:uuid:62dc69e8-6845-4e67-91cd-c4bf8ef20bfc>', '<urn:uuid:4ba28abc-1d23-4806-8c89-c0a9cca3d3d6>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	11	89	1366
77	I research marine conservation strategies and want to understand both pollution's effects on aquatic ecosystems and sea turtle survival. What are the main ways that different types of pollution impact water quality and turtle populations?	Human activities cause multiple pollution impacts on both water quality and sea turtles. Water quality is affected by domestic and industrial sewage, and agricultural runoffs containing fertilizers and insecticides, reducing ecosystem functioning and ability to use. For sea turtles, marine pollution ranges from discarded plastics to lost fishing nets. Turtles often accidentally ingest marine debris, causing internal injury, intestinal blockage, and starvation as plastic accumulates in their stomachs preventing food digestion. Additionally, light pollution disrupts nesting behavior and causes hatchling death by leading them away from the sea, while chemical pollutants can weaken sea turtles' immune systems, making them more susceptible to disease.	"['The constant interference of human activities has had direct and indirect impacts on aquatic environments with consequences for water quality for aquatic biota and the functioning of ecosystems, thereby reducing the ability to use. This study aimed to describe the qualitative and quantitative variation of species of macroalgae from Praia do Pina, Recife-PE, in the rainy season and drought. The collections were made in August 2007 (rainy season) and January 2008 (period of drought). Data on the occurrence of macroalgae were collected by snorkeling using the combined techniques of sampling belt transects (50m x 1m) and quadrats (1m x1m). The species of greatest abundance in both periods were almost the same: Ulva lactuca, U. fasciata, U. intestinalis, Caulerpa sertularioides, C. filiformis; C.prolifera; C.racemosa, and C. lentillifera; being found in smaller quantities: Pterocladia capillaceous, Hypnea musciformis, H. pannosa, H. cervicornis, Sargassum Muticum, S. vulgare, S. ilicifolium, Padin Pavoni, P. tetrastromatica, Dictyota dichotoma, Gracilaria warts and G. Salicornia. The two species were only found in Glacilaria period of drought and the species S. ilicifolium was found during the rainy season. About 80% of macroalgal distribution in reef studied is composed of individuals belonging to the phylum Chlorophyta, 12% are representatives of the phylum Phaeophyta and 8% are belonging to the phylum Rodhophyta. These percentages are very close to the values presented in two urban beaches in the Metropolitan Region of Recife and a beach on the northern coast of the state of Pernambuco, where areas have been intensified the processes of human occupation and exploitation, and increasing industrialization. The predominance of species of Chlorophyta may represent a greater tolerance to the flora of the pollutants.\nProfitability Analysis of Value Added Usipa (Engraulicypris Sardella) by Packaging In Nkhotakota, Malawi\nThe study was conducted to determine the profitability of value addition of Engraulicypris sardella through packaging in Nkhotakota district, central Malawi. Samples(69Kg) of fresh E. sardella purchased and sundried in solar tent dryer, packaged (34.5Kg) in plastics papers while the other 34.5Kg was unpackaged. Sales data were analyzed using gross margin and net profit analysis. Benefit-cost ratio and rate of return on investment were used to determine the viability of small scale E. sardella fish business. Results of Cost and Return Analysis packaged E. sardella showed that the total revenue of packaged E. sardella was Mk225,400.00 and total cost was Mk 40,350.00 while results for unpackaged E. sardella showed that the total revenue was Mk104,144.92 and total cost was Mk20,450.00. Results on Benefit cost ratio for packaged and unpackaged E. sardella were 5.09 and 5.58 respectively. The findings implies that packaged E. sardella was more profitable than unpackaged E. sardella. The rate of return for packaged and unpackaged Engraulicypris sardella was 4.58 and 4.09 respectively, implying that for every Mk1.00 invested for Packaged Engraulicypris sardella there was a return of Mk4.58 and Mk4.09 for unpackaged Engraulicypris sardella. Packaging should be encouraged to maximize profits from small scale fish business in Malawi.\nThe ever-increasing population and hence higher demand for fish and other aquatic organisms for human consumption and exports have accorded greater priority to the aquaculture subsector. Therefore, there was need to conduct research to unveil facts relating to practices adopted by farmers in rearing fish and other aquatic organisms. This study emphasized concepts and importance of aquaculture, some common breeds of fish cultured, systems of aquaculture practices and methods of rearing fish and other aquatic organisms. The study also discussed eco-friendly aquaculture practices ensure sustainable fish production. The study further discussed the benefits of adopted eco-friendly aquaculture practices and constraints encountered by aqua-culturists. The study recommended among others that government should ensure proper training of extension workers who would in turn educate fish farmers on eco-friendly practices. The study also recommended that for expansion of aqua-cultural enterprises, government should provide more funds and liberalize land acquisition procedures to avail more land. Finally, the study recommended that requisite guidelines and regulations should be enforced to ensure that only practices that are sustainable and eco-friendly be adopted in the production of fish and allied products.\nTotal An Investigation into Light Pollution as a Limiting factor for shift of Mass nesting ground at Rushikulya rookery Ganjam Odisha\nIllumination due to artificial lights on nesting beaches and from nearby place to nesting beaches is detrimental to sea turtles because it alters critical nocturnal behaviors specifically, their choice of nesting sites and their return path to the sea after nesting. Illuminations perplex the hatchlings to find sea after emerging. Numerous studies conducted in other countries have demonstrated that artificial lights negatively impact on turtles, both female adults as they come to and go from their home beach to lay eggs, and to turtle hatchlings as they seek out the way to the open ocean. In this study we correlated the mass nesting intensity of 5years (2012 to 2018) at Rushikulya mass nesting site to the illumination zone. Illumination due to light conditions on nesting beaches are complex, and measuring light pollution in a way that effectively captures the impacts to sea turtles is difficult. But increase in intensity of illumination on selective mass nesting beaches showed gradual reduction in intensity of preferred nesting site during the mass nesting event. A gradual shift of nesting preference was also observed more toward darker zone.\nConcept of Aquatic Weeds in Fish Culture: Weeds are unwanted plants that grow where they are not wanted. Aquatic weeds are plants that grow in the pond where they are not wanted. These weeds, even though are not wanted in the pond by the farmer because of their effect mainly, some of them are essential parts of natural systems and form the basis of water body’s health and productivity. The habitat for aquatic weeds involve various proportions of water and soil, including intermittently wet ditches, ditches which always hold standing water, streams, stock ponds, farm ponds, lakes, ornamental ponds, and intermediate habitats.\nWetlands perform numerous valuable functions such as recycle nutrients, purify water, attenuate floods, maintain stream flow, recharge ground water, and also serve in providing drinking water source, fish, fodder, fuel, recreation to the society. The interaction of man with wetlands during the last few decades has been a concern largely due to land loss – accompanied by intensified industrial, commercial, and residential development further leading to pollution of wetlands by domestic, industrial sewage, and agricultural runoffs- as fertilizers, insecticides. The fact that wetland values are overlooked has resulted in threat to the source of these benefits South Indian wetlands are being lost on account of various anthropogenic activities .\nFish is a animal source protein. It is one of the neglected areas because of unawareness among farming community, unfavorable government policies and high inputs involved in fish farming. Fish being rich in nutrients can help to alleviate the malnutrition of the local population. Fish plays an important role in the diet of the people of developing nations. In India, fish and fishing contribute immensely to the national economy by providing high animal food protein and generating employment, which is a means of poverty alleviation. Most importantly, it is the source of livelihood for a large section of economically backward population of the country. Integrated livestock farming will be novel approach for profitable and rumerative fish production. Poultry droppings can be a good source of nutrients for enhancing the productivity of aquatic production systems.\nThis study was conducted to determine the artificial breeding performance of optimum dosage of hormones HCG to induce spawning in Clarias gariepinus at a hatchery. Three doses were prepared first treatmentT1(3000IU),second treatmentT2(2000IU), and finally treatmentT3(1000IU). Fish Body weight before injection with no significant(p>0.05). after injection there was significant between treatment (p0.05).\nMillions of years created water in symbiosis with the biota. For several hundred years a man has been trying to unravel her secrets. Can not. Uses it, just as a working body. It is a mobile very complex solvent and a supplier of substances to each object, living creatures and plants on Earth. The path of its transformation in the circuit: soil – flora and fauna – evaporation – the atmosphere is the soil. In each of these links it changes its properties, appearance, structure. Man for his development and comfort shortens this path. We deprive her of her main destiny in advancing through food and vegetable channels. We destroyed the biota with arable land, reservoirs, dumps, asphalt. This territory has become more than 60 percent of the land inhabited by the area and every day it grows.', ""National Oceanic and Atmospheric Administration (Ocean Today)\nSeven different species of sea (or marine) turtles grace our ocean waters, from the shallow seagrass beds of the Indian Ocean, to the colorful reefs of the Coral Triangle and the sandy beaches of the Eastern Pacific. While these highly migratory species periodically come ashore to either bask or nest, sea turtles spend the bulk of their lives in the ocean.\nOver the last 200 years, human activities have tipped the scales against the survival of these ancient mariners. Slaughtered for their eggs, meat, skin, and shells, sea turtles suffer from poaching and over-exploitation. They also face habitat destruction and accidental capture—known as bycatch—in fishing gear. Climate change has an impact on turtle nesting sites; it alters sand temperatures, which then affects the sex of hatchlings. Nearly all species of sea turtle are now classified as endangered, with three of the seven existing species being critically endangered.\nKemp's Ridley Turtle\nOlive Ridley Turtle\nThreats to Sea Turtles\nSea turtles are a fundamental link in marine ecosystems. They help maintain the health of seagrass beds and coral reefs that benefit commercially valuable species such as shrimp, lobster, and tuna. Sea turtles are the live representatives of a group of reptiles that have existed on Earth and traveled our seas for the last 100 million years. Turtles have major cultural significance and tourism value.\nFive of the seven species are found around the world, mainly in tropical and subtropical waters. The remaining two species, though, have relatively restricted ranges: Kemp's Ridley is found mainly in the Gulf of Mexico and the Flatback turtle around northern Australia and southern Papua New Guinea.\nLoggerhead and hawksbill turtles are particularly vulnerable. Nearly all species of sea turtle are classified as Endangered. They are killed for their eggs, meat, skin and shells. They also face habitat destruction. Climate change has an impact on turtle nesting sites. As fishing activity expands, this threat is more of a problem.\nSea turtles journey between land and sea and swim thousands of ocean miles during their long lifetimes. They wait decades until they can reproduce, returning to the same beaches where they were born to lay their eggs. Females can lay hundreds of eggs in one nesting season, yet few will yield hatchlings that survive their first year of life. Beyond these significant natural challenges, sea turtles face multiple threats caused by humans as noted below.\nFisheries: Sea turtles virtually everywhere are affected by bycatch from fisheries. Worldwide, hundreds of thousands of sea turtles a year are accidentally caught in shrimp trawl nets, on longline hooks and in fishing gill-nets. Sea turtles need to reach the surface to breathe, and therefore many drown once caught.\nDirect Take: Sea turtles and their eggs are killed by people throughout the world for food, and for products including oil, leather and shell.\nCoastal Development: Sea turtle habitats are degraded and destroyed by coastal development. This includes both shoreline and seafloor alterations, such as nesting beach degradation, seafloor dredging, vessel traffic, construction, and alteration of vegetation.\nClimate change: Climate change will increase the frequency of extreme weather events, result in loss of nesting beaches, and cause other alterations to critical sea turtle habitats and basic oceanographic processes. It may impact natural sex ratios of hatchlings and increase the likelihood of disease outbreaks for sea turtles.\nMarine pollution is anything from a discarded plastic to a lost fishing net. Every ocean in the world is littered with some form of debris which resembles food for marine life. Many animals accidentally eat marine debris causing internal injury, intestinal blockage, and starvation. Once ingested, the plastic piles up in the turtles's stomach and can then obstruct bowels, preventing turtles from digesting food and leading them to starve to death. Light pollution disrupts nesting behavior and causes hatchling death by leading them away from the sea. Chemical pollutants can weaken sea turtles’ immune systems, making them susceptible to disease.\nHabitat loss uncontrolled coastal development, vehicle traffic on beaches, and other human activities have directly destroyed or disturbed sea turtle nesting beaches around the world. Sea turtles are dependent on beaches for nesting. Lights from roads and buildings disorient hatchlings away from the sea, and vehicle traffic on beaches compacts the sand, making it impossible for female turtles to dig nests. Turtle feeding grounds such as coral reefs and seagrass beds are damaged and destroyed by activities onshore, including sedimentation from clearing of land and nutrient run-off from agriculture. Beach restoration projects for protecting seaside buildings have also been found to be harmful, through dredging and sand filling.\nHow You Can Help Protect Sea Turtles\nMinimize beachfront lighting during the sea turtle nesting season by turning off, shielding, or redirecting lights.\nClose blinds and draperies in oceanfront rooms at night during the nesting season to keep indoor lighting from reaching the beach.\nDo not construct campfires on the beach. Sea turtle hatchlings are known to be attracted to the light emitted by campfires and crawl into fires and die.\nUse your natural vision when walking on the beach at night. The use of flashlights and flash photography can deter turtles from coming ashore to nest or cause them to abort nesting attempts.\nIf you encounter a turtle on the beach at night, remain quiet, still, and at a distance, otherwise she may become frightened and return to the ocean without nesting.\nLeave the tracks left by turtles undisturbed. Researchers use the tracks to identify the species of turtle that nested and to find and mark the nests for protection.\nProperly dispose of your garbage. Turtles may mistake plastic bags, styrofoam, and trash floating in the water as food and die when this trash blocks their intestines.\nCelebrate events without the use of helium balloon releases. Like plastic trash, balloons end up in the ocean, especially when released near the coast. Sea turtles mistakenly eat the balloons and die.\nRemove recreational equipment, such as lounge chairs, cabanas, umbrellas, and boats, from the beach at night. Their presence can deter nesting attempts and interfere with the seaward journey of hatchlings.\nProtect beach vegetation that stabilizes sand and the natural coastline.\nWhen boating, stay alert and avoid sea turtles. Propeller and collision impacts from boats and ships can result in injury and death of sea turtles. Also, stay in channels and avoid running in seagrass beds to protect this important habitat from prop scarring and damage. Avoid anchoring boats in seagrass beds and coral reefs, which serve as important foraging and resting habitats for sea turtles.""]"	['<urn:uuid:27e3bd2d-82da-4a1a-9426-2305abae2728>', '<urn:uuid:46011562-1517-4cbb-a934-3c1378e91e94>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	35	103	2499
78	light wavelength cell death regeneration difference	The effects of light on cells vary dramatically by wavelength. Blue light combined with retinal molecules triggers toxic reactions that can kill any cell type, including photoreceptors, cancer cells, heart cells and neurons. Neither blue light alone nor retinal without blue light causes cell death. In contrast, red light wavelengths between 660nm and near-infrared 890nm stimulate cell regeneration and repair by enhancing mitochondrial energy production. Red light penetrates about 5 millimeters into tissue without causing surface damage, unlike more intense light therapies, and helps build up cellular antioxidant and anti-inflammatory defenses.	['Hexbyte Hacker News Computers\nBlue light from digital devices and the sun transforms vital molecules in the eye’s retina into cell killers, according to optical chemistry research at The University of Toledo.\nThe process outlined in the study, which was recently published in the journal Scientific Reports, leads to age-related macular degeneration, a leading cause of blindness in the United States.\n“We are being exposed to blue light continuously, and the eye’s cornea and lens cannot block or reflect it,” Dr. Ajith Karunarathne, assistant professor in the UT Department of Chemistry and Biochemistry, said. “It’s no secret that blue light harms our vision by damaging the eye’s retina. Our experiments explain how this happens, and we hope this leads to therapies that slow macular degeneration, such as a new kind of eye drop.”\nMacular degeneration, an incurable eye disease that results in significant vision loss starting on average in a person’s 50s or 60s, is the death of photoreceptor cells in the retina. Those cells need molecules called retinal to sense light and trigger a cascade of signaling to the brain.\n“You need a continuous supply of retinal molecules if you want to see,” Karunarathne said. “Photoreceptors are useless without retinal, which is produced in the eye.”\nKarunarathne’s lab found that blue light exposure causes retinal to trigger reactions that generate poisonous chemical molecules in photoreceptor cells.\n“It’s toxic. If you shine blue light on retinal, the retinal kills photoreceptor cells as the signaling molecule on the membrane dissolves,” Kasun Ratnayake, a Ph.D. student researcher working in Karunarathne’s cellular photo chemistry group, said. “Photoreceptor cells do not regenerate in the eye. When they’re dead, they’re dead for good.”\nKarunarathne introduced retinal molecules to other cell types in the body, such as cancer cells, heart cells and neurons. When exposed to blue light, these cell types died as a result of the combination with retinal. Blue light alone or retinal without blue light had no effect on cells.\n“No activity is sparked with green, yellow or red light,” Karunarathne said. “The retinal-generated toxicity by blue light is universal. It can kill any cell type.”\nThe researcher found that a molecule called alpha tocoferol, a Vitamin E derivative and a natural antioxidant in the eye and body, stops the cells from dying. However, as a person ages or the immune system is suppressed, people lose the ability to fight against the attack by retinal and blue light.\n“That is when the real damage occurs,” Karunarathne said.The lab currently is measuring light coming from television, cell phone and tablet screens to get a better understanding of how the cells in the eyes respond to everyday blue light exposure.\n“If you look at the amount of light coming out of your cell phone, it’s not great but it seems tolerable,” Dr. John Payton, visiting assistant professor in the UT Department of Chemistry and Biochemistry, said. “Some cell phone companies are adding blue-light filters to the screens, and I think that is a good idea.”\nTo protect your eyes from blue light, Karunarathne advises to wear sunglasses that can filter both UV and blue light outside and avoid looking at your cell phones or tablets in the dark.\n“Every year more than two million new cases of age-related macular degeneration are reported in the United States,” Karunarathne said. “By learning more about the mechanisms of blindness in search of a method to intercept toxic reactions caused by the combination of retinal and blue light, we hope to find a way to protect the vision of children growing up in a high-tech world.”\nFruit fly breakthrough may help human blindness research\nKasun Ratnayake et al, Blue light excited retinal intercepts cellular signaling, Scientific Reports (2018). DOI: 10.1038/s41598-018-28254-8\nDecember 18, 2017\nFor decades, scientists have known that blue light will make fruit flies go blind, but it wasn’t clear why. Now, a Purdue University study has found how this light kills cells in the flies’ eyes, and that could prove a useful …\nJune 18, 2018\nA research team at Massachusetts Eye and Ear has shown that microglia, the primary immune cells of the brain and retina, play a protective role in response to retinal detachment. Retinal detachment and subsequent degeneration …\nJune 5, 2017\nRetinitis pigmentosa, age-related macular degeneration, and other retinal diseases lead to the deterioration of photoreceptors, the light-sensing cells in the eye. Eventually, this deterioration progresses to vision loss. …\nAugust 31, 2011\nA new research report published in The FASEB Journal will help ophthalmologists and scientists better understand a rare genetic disease that causes increased susceptibility to blue light, night blindness, and decreased vision …\nJune 29, 2016\nEngineers and neuroscientists at the University of Sheffield have demonstrated for the first time that the cells in the retina carry out key processing tasks. This could pave the way for improving retinal implants and therefore …\nMarch 6, 2018\nDuke researchers have pinpointed a new therapeutic target for macular degeneration, an eye disease that affects over 10 million Americans and is the leading cause of blindness in adults over 60.\nAugust 9, 2018\nWhat do a flea and an eagle have in common? They can store energy in their feet without having to continuously contract their muscles to then jump high or hold on to prey. Now scientists at Queen Mary University of London …\nAugust 9, 2018\nA group of Japanese researchers has developed a technology to create a hybrid catalyst from simple-structured, commercially available rhodium and organic catalysts, which reduces chemical waste and produces molecules with …\nAugust 8, 2018\nScripps Research scientists have developed a powerful new strategy for synthesizing molecular skeletons of chemicals used in drugs and other important products, a technique that provides unprecedented flexibility and control …\nAugust 8, 2018\nThe word “defect” universally evokes some negative, undesirable feature, but researchers at the Energy Safety Research Institute (ESRI) at Swansea University have a different opinion: in the realm of nanoporous materials, …\nAugust 8, 2018\nWhen jewelers create a necklace, they control the order and number of each bead or jewel they use to form a desired pattern. It’s been challenging for scientists to do the same thing when designing polymers—until now. In …\nAugust 8, 2018\nInspired by elements found in nature, researchers at the University of New Hampshire say the puzzle-like wavy structure of the delicate seed coat, found in plants like succulents and some grasses, could hold the secret to …', 'Red light therapy has been proven to be effective in treating a number of skin conditions, inflammation, sleep disorders, and even anti-aging. As a safe and minimally invasive solution, infrared light offers an effective alternative to traditional drugs as a method of healing wounds and promoting cellular energy. LLLT is FDA-approved for treating conditions like chronic joint pain, hair loss, and may even help regenerate brain tissue.\nBut how does it work on the brain? In order to discuss this topic thoroughly, we will start by taking a look at how it all started….\nThe Beginnings of Red Light Therapy\nIn the early 1990s, NASA began studying red light spectrums in order to determine the beneficial effects on growing food in space.1\nResearchers around the world took note, and increasing amounts of research have been devoted to the effects of RLT on bio-organisms. Specifically, the wavelengths of light between 660nm (red) and 890nm (infrared) were found to be “bioactive,” meaning having an effect on living tissue.\nThe basic process of RLT and near-infrared (NIR) infrared therapy involves exposing cellular material to red and NIR light by means of low light lasers or LED lights. The process is known by many names, including:\n- photobiomodulation (PBM)\n- low-level light therapy (LLLT)\n- soft laser therapy\n- cold laser therapy\n- photonic stimulation\n- low-power laser therapy (LPLT)\nI go into more detail on how RLT works in this recent blog post. How Does Red Light Therapy Work?\nHow Red Light Affects Bio Organisms\nThe premise of red light therapy is that the wavelengths of light cause a biochemical effect in cells that boost the energy production of mitochondria. A cell’s energy is created in the mitochondria as adenosine triphosphate (ATP).\nYou can think of the mitochondria as the powerhouse of the cell. By increasing ATP production, the cell has more energy to function more efficiently and repair damage.\nRed light therapy doesn’t cause damage to the skin’s surface, which differs from laser or intense pulsed light (IPL) therapy. Light wavelengths work by causing damage to the outer layer of skin, inducing tissue repair. Differently, red light penetrates only about 5 millimeters into the skin, directly stimulating cell regeneration and collagen production.\nLED light therapy is also thought to contribute to hormesis, building up the cell’s anti-oxidant and anti-inflammatory defense systems.\nRed Light Therapy and Brain Health\nRed light and NIR light, when applied directly to the head, preserve and regenerate brain cells and tissue. A high-quality device is able to penetrate the skull and produce more metabolic energy by stimulating the mitochondria. Other benefits of stimulating mitochondrial functioning include increased cerebral blood flow and increasing the availability and consumption of oxygen.\nThe specific mechanisms of the effects of red light are the increase of cytochrome c oxidase activity, activation of cell signaling pathways, up-regulation of transcription factors, and increasing the expression of protective genes.2\nDr. Michael Hamblin, Associate Professor at Harvard Medical School, notes, “Near-infrared light can penetrate quite deep into the body. So if you shine it on your head, it will penetrate your skull and a lot of it is absorbed by your brain. Once you understand this, it’s amazing to think of the various diseases of the brain that need regeneration and healing, which could potentially be treated with infrared light therapy.”3\nImprovements for Memory, Attention, and Reaction Times\nA study published by Barrett DW, et al in 2013 examined the effects of infrared on the brain in one of the first placebo-controlled human trials. The results for cognitive function were very positive and impressive:4\n- Quicker Reaction Times: “Significantly improved in the treated vs. placebo control groups” in sustained-attention psychomotor vigilance task (PVT).\n- Better Memory: “Showed significant improvement in treated vs. control groups as measured by memory retrieval latency and number of correct trials” in delayed match-to-sample tasks.\n- More Positive Moods: “Overall affect improved significantly in the treated group due to more sustained positive emotional states.”\nRed Light Therapy and Stronger Learning\nA 2017 study tested 118 people who received infrared light or a placebo. Then participants had to learn a series of categories and rules to test their ability to integrate knowledge and conditions. The study concluded that rule-based learning was greatly increased in the subjects who received RLT compared to the control group.\n“We found that prefrontal rule-based learning was substantially improved following transcranial infrared laser stimulation as compared to placebo. These results highlight the exciting potential of transcranial infrared laser stimulation for cognitive enhancement and provide insight into the neurobiological underpinnings of category learning.”\nRed Light Therapy for Better Reaction Time\nIn another recent study, 31 participants between the ages of 14 and 65 were tested on event-related responses before and after RLT.5\nThe medical research concluded, “The data suggest that NIR light may have an acute effect on reaction time and amplitude in certain subject subsets. There were no adverse events registered across the 31 subjects in the treatment group, nor in the 18 evaluable control group subjects.”\nRed Light Therapy and Age-Related Cognitive Decline\nResearchers examining neurocognitive function in adults age 49-90 who complained of memory loss. Specifically, the study looked at prefrontal cortex measures, including attention and short-term memory. 6\n- The authors noted that “all participants improved in all cognitive measures after light treatments”.\n- Cognitive improvements included significantly quicker reaction time, fewer lapses, and more correct responses in tests.\n- “Importantly, no adverse effects were found.”\nAn additional study published in the International Journal of Geriatric Psychology found similar results when studying RLT and age-related cognitive decline. Researchers tested frontal brain activity in older adults after participants received light exposure or a placebo. 7\nResearchers concluded, “Only the older adults who received real treatments exhibited significant improvements in their action selection, inhibition ability, and mental flexibility after vs before treatment.”\nTraumatic Brain Injuries\nRed light therapy is being examined for the treatment of traumatic brain injuries (TBI) and chronic traumatic encephalopathies (CTE), such as those received in military harm or contact sports.\nStudies and trials have been promising and show that light exposure may increase the growth of synapses and new nerve tissue in damaged brain cells and help improve cognitive performance.\nOne study concluded, “Many investigators believe that PBM for brain disorders will become one of the most important medical applications of RLT in the coming years and decades… If inexpensive LED helmets can be developed and successfully marketed as home use devices, then we are potentially in a position to benefit large numbers of patients (to say nothing of healthy individuals).”8\nDr. Hamblin and I go over the Science on Red Light Therapy Benefits in this podcast.\nResearchers at the VA Boston Healthcare system are testing the effects of light exposure on brain function in veterans with Gulf War Illness. “We are applying a technology that’s been around for a while,” says lead investigator Dr. Margaret Naeser, research linguist and speech pathologist, “but it’s always been used on the body, for wound healing and to treat muscle aches and pains, and joint problems. We’re starting to use it on the brain.” 9\nNaeser sees potential not only for war injuries, but also for such conditions as depression, stroke, dementia, and even autism. “There are going to be many applications, I think. We’re just in the beginning stages right now.”\nAlzheimer’s Disease and Dementia\nTrial results are encouraging for the treatment of Alzheimer’s Disease (AD) with red light therapy.\nA 2017 study examined the effects of NIR stimulation using 1060-1080nm light emitting diodes. The study concluded, “Results showed changes in executive functioning; clock drawing, immediate recall, praxis memory, visual attention and task switching (Trails A&B) as well as a trend of improved EEG amplitude and connectivity measures. Neuroplasticity has also been reported with NIR light stimulation and mitochondrial enhancement.” 10\nIt is showing promise for improving brain health, including memory and cognition, as well as treating brain diseases and injuries. If you are interested in investigating RLT, it is always best to consult with your healthcare professional to discuss the best RLT options.\nNow that you understand how red light therapy works with the brain, you can get even more information on the science behind its effectiveness, health benefits, tips to get the best results, and MORE by clicking here.\n|↑1||Herridge, L, (2012). LED Lights Used in Plant Growth Experiments for Deep Space Missions. NASA. 2012 Sept 11.|\n|↑2||Hennesy, M, et. al. (2016). Photobiomodulation and the brain: a new paradigm. J Opt. doi: 10.1088/2040-8986/19/1/013003. Epub 2016 Dec 14.|\n|↑3||Hamblin, M, (2016). Shining light on the head: Photobiomodulation for brain disorders. ScienceDirect. 2016 Oct 1.|\n|↑4||Barret DW, et. al. (2012). Transcranial infrared laser stimulation produces beneficial cognitive and emotional effects in humans. Neuroscience. doi: 10.1016/j.neuroscience.2012.11.016. Epub 2012 Nov 27.|\n|↑6||Grove F Jr, et. al. (2016). Acute Effects of Near Infrared Light Therapy on Brain State in Healthy Subjects as Quantified by qEEG Measures. Photomed Laser Surg. doi: 10.1089/pho.2015.4036. Epub 2016 Nov 17.|\n|↑7||Vargas, E, et. al. (2017). Beneficial neurocognitive effects of transcranial laser in older adults. Lasers Med Sci. doi: 10.1007/s10103-017-2221-y. Epub 2017 May 2.|\n|↑8||Hamblin, M, (2016). Shining light on the head: Photobiomodulation for brain disorders. ScienceDirect. 2016 Oct 1.|\n|↑9||Veterans Affairs Research Communications. Can light therapy help the brain?. ScienceDaily. ScienceDaily, 2 April 2015.|\n|↑10||Berman MH, (2017). Photobiomodulation with Near Infrared Light Helmet in a Pilot, Placebo Controlled Clinical Trial in Dementia Patients Testing Memory and Cognition. J Neurol Neurosci. doi: 10.21767/2171-6625.1000176. Epub 2017 Feb 28.|']	['<urn:uuid:ff42c137-35e7-46f8-b4b4-5a76eb233646>', '<urn:uuid:9bf96fc9-6436-4bae-b448-264aca4f101c>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	6	91	2658
79	ancient person reconstruction modern techniques difficulties	The reconstruction of ancient individuals combines modern techniques and significant challenges. On the technical side, researchers can use CT scanning and 3D printing to recreate skull structures, followed by forensic reconstruction techniques to rebuild facial features using tissue depth markers and anatomical ratios. However, this process faces difficulties, particularly when working with ancient DNA. The challenges include DNA decomposition (all DNA is destroyed within 100,000 years), contamination from modern human DNA, and the presence of fragmentary and damaged genetic material. DNA errors are very common, with specific chemical changes occurring such as cytosine changing to uracil. Despite these challenges, successful reconstructions have been achieved, as demonstrated by the case of Meritamun, an ancient Egyptian woman whose face was reconstructed using CT scanning, 3D printing, and forensic sculpture techniques.	"['Brought to life, 2000 years later\nModern technology and research has restored an ancient Egyptian woman, Meritamun, creating a unique teaching tool for medicine and health science\nBy Andrew Trounson, University of Melbourne\nReconstructed from a mummified head dating back at least two millennia, the fine-featured ancient Egyptian face looks out from an artist’s studio in the forested hills of rural Victoria.\nAncient Egypt has fascinated everyone from conquerors like Alexander the Great and Napoleon to movie directors chasing cursed mummies and lost treasures. That same fascination is now driving a unique research and teaching collaboration to test the limits of technology and learn all we can about an ancient Egyptian whose head has been preserved, largely unknown, for almost 100 years at the University of Melbourne.\nIt is a multi-disciplinary project with the Faculty of Medicine, Dentistry and Health Sciences that combines medical research, forensic science, computerised tomographic (CT) scanning, 3D printing, Egyptology and art. The team has already produced a reconstruction of the head of an 18 to 25-year-old woman who lived at least 2000 years ago that they named Meritamun, which means beloved of the god Amun. But her serene face is only the start of a journey to answer questions about how she may have died, what diseases she had, when she lived, where she was from, and even what she ate.\n“The idea of the project is to take this relic and, in a sense, bring her back to life by using all the new technology,” says Dr Varsha Pilbrow, a biological anthropologist who teaches anatomy at the University’s Department of Anatomy and Neuroscience.\n“This way she can become much more than a fascinating object to be put on display. Through her, students will be able to learn how to diagnose pathology marked on our anatomy, and learn how whole population groups can be affected by the environments in which they live.”\nHow and why the University of Melbourne has a mummified Egyptian head in the basement of its medical building is a mystery. It may well have been part of the collection of Professor Frederic Wood Jones (1879-1954) who before becoming head of anatomy at the University in 1930 had undertaken archaeological survey work in Egypt.\nMeritamun is housed in a purpose-built archival container alongside rows of human specimens preserved in glass jars and formaldehyde solution at the Harry Brookes Allen Museum of Anatomy and Pathology, in the School of Biomedical Sciences. She lies face up, as she would have been buried. Even though she is covered in tightly wound bandages and blackened by oil and embalming fluid, her delicate features are clear.\n“Her face is kept upright because it is more respectful that way,” says museum curator Dr Ryan Jefferies, who wears plastic gloves as he reverently unties the box to show her. “She was once a living person, just like all the human specimens we have preserved here, and we can’t forget that.”\nThe genesis of the project was Dr Jefferies’ concern that the head, whose origin remains a mystery, could be decaying from the inside without anyone noticing. Removing the bandages wasn’t an option as it would have damaged the relic and further violated the individual who had been embalmed for the afterlife. But the scan revealed the skull to be in extraordinarily good condition. From there the opportunity to use technology to research the mystery of the head was one that was too good to resist. “The CT scan opened up a whole lot of questions and avenues of enquiry and we realised it was a great forensic and teaching opportunity in collaborative research,” says Dr Jefferies, a parasitologist.\nThe Forensic Egyptologist\nMeritamun was identified as ancient Egyptian by Dr Janet Davey, a forensic Egyptologist from Monash University who is based at the Victorian Institute of Forensic Medicine, where the head was scanned. She jumped at the opportunity to study the head further, and at the Institute she had a wealth of forensic expertise on which to call.\n“There are not too many people I know who aren’t interested in Egyptology and the mysteries of ancient Egypt, so when you need to tap into expertise there always seems to be someone prepared to give up their time for you,” says Dr Davey, who habitually wears around her neck a gold pendant of Egyptian Queen Nefertiti.\nDr Davey determined Meritamun’s gender from the bone structure of her face, identifying such markers as the smallness and angle of the jaw, the narrowness of the roof of her mouth, and the roundness of her eye sockets. But to be sure she had her conclusions confirmed by anthropologist Professor Caroline Wilkinson from Liverpool John Moores University in the UK. She is a world expert in facial reconstruction and made headlines when she restored the face of King Richard III, who was killed in battle in 1485.\nDr Davey guesses that Meritamun was probably about 162cm tall, given accepted thinking that ancient peoples were generally shorter than people today. If the researchers had bone from her arm, leg or even just her heel they could have established a more accurate estimate.\nDating her is also difficult. Meritamun has some significant tooth decay that conventional thinking would date her to Greco-Roman times when sugar was introduced after Alexander’s conquest of Egypt in 331BC. But honey could also account for the decay. And while mummification was more accessible to Egyptians after Alexander’s time, the fineness of the linen bandages suggest Meritamun was high status enough anyway to be embalmed at the earlier time of the Pharaohs.\n“There is always a lot of mystery to unravel in forensic Egyptology and it is why it is so important to keep an open mind and follow the evidence,” says Dr Davey.\nShe is now waiting on radiocarbon dating to give a better idea of when Meritamun lived, which Dr Davey says could be as long ago as c1500 BCE. Radiocarbon dating works by measuring the amount of organic carbon atoms that are left in a tissue sample. Since carbon atoms decay over time, the fewer carbon atoms there are in a sample, the older it is.\nThe Masters Student\nUniversity of Melbourne biomedical science masters student Stacey Gorski, under the supervision of Dr Pilbrow, is using forensic pathology to try to uncover how healthy Meritamun was and how she may have died.\nJust from reading the CT scans Gorski has been able to see that in addition to two tooth abscesses, there are patches on the skull where the bone had pitted and thinned. It is a clear symptom of anaemia, which is a lack of red blood cells that starves the body of oxygen. The thinning occurs when the bone marrow swells as it goes into overdrive in an effort to produce more red blood cells.\nIn Meritamun’s case, it may have been caused by malaria parasites such as malaria or the flatworm infection schistosomiasis, both of which would have been hazards in the Nile Delta in ancient times. “The fact that she lived to adulthood suggests that she was infected later in life,” says Gorski. Nevertheless, the parasites and anaemia would have left her pale and lethargic at the end.\n“Anaemia is a very common pathology that is found in bodies from ancient Egypt, but it usually isn’t very clear to see unless you can look directly at the skull,” says Gorski. “But it was completely clear from just looking at the images.”\nWithout having the rest of Meritamun’s body it will be impossible to know for sure how she died, but the anaemia could certainly have been a predisposing factor, as could the abscesses if they had become seriously infected.\nGorski is now working to find out what caused the anaemia. One possible way to find out would be to undertake a molecular analysis of a small tissue sample taken from her exposed neck in the hope of finding some DNA evidence of a parasite.\nBut such DNA analysis, assuming any could be found, is expensive and beyond the project’s budget. Instead, she plans to undertake what is called a “histological” study in which she will rehydrate a small tissue sample and use an electron microscope in the hope of finding evidence of diseases or parasites.\nThis is a long shot since such evidence is more likely to be found in organ tissue, but it is a chance to test new techniques and it may yet uncover other evidence.\nGorski is even hoping to deduce what Meritamun ate and where in Egypt she may have lived, by studying the carbon and nitrogen atoms present in a tissue sample. Different plants, including vegetables, fruit and grains, all produce different types of carbon and nitrogen atoms, known as isotopes.\nBy comparing the types of isotopes present Gorski may be able to determine Meritamun’s broad diet. The results could theoretically place her in an area of Egypt that, for example, we know was more dependent on one crop compared to another.\nThe Imaging Technician\nIt took 140 hours of printing time on a simple consumer-level 3D printer to produce the skull that has been used to reconstruct Meritamun’s face, not counting the tweaking and design work of the Department of Anatomy and Neuroscience’s imaging technician Gavan Mitchell. Because the 3D printer builds from the bottom up and the print is always more detailed at the top, Mitchell has to print out the skull out in two sections to better capture the detail of the jaws and the base of the skull.\n“It has been a hugely rewarding process to be able to transform the skull from CT data on a screen into a tangible thing that can be handled and examined,” says Mitchell, who has a BSc majoring in scientific photography.\nBeing able to print out body parts and organs is opening a whole new use for the university’s specimen collection. One of the best ways of producing a specimen that can be handled by students is by using plastination, which is a process in which the tissue fluid is replaced with a hard silicon resin. But CT scanning technology combined with 3D printing means that multiple copies of specimens can be reproduced for handling and study, including old rare specimens that display unusual forms of disease or damage.\n“We can now replicate specimens with really interesting pathologies for students to handle and for virtual reality environments without ever touching the specimen itself,” Mitchell says.\nThe printed skull formed the base on which sculptor Jennifer Mann has used all her forensic and artistic skill to reconstruct Meritamun’s face.\n“It is incredible that her skull is in such good condition after all this time, and the model that Gavan produced was beautiful in its details,” says Mann.\nMann learned the technique for facial reconstruction at the Forensic Anthropology Centre at Texas State University where she studied with leading forensic sculptor Karen T. Taylor. She practised on skull casts previously used in actual cases to reconstruct unidentified murder victims.\n“It is really poignant work and extremely important for finally identifying these people who would otherwise have remained unknown.”\nShe cautions that any facial reconstruction can only be an approximation of what someone actually looked like in life, but the results she had at Texas closely matched those of the eventually identified murder victims.\nThe methodology involves attaching to the printed skull plastic markers to indicate different tissue depths at key points on the face, based on averages in population data. This data is derived from modern Egyptians and has been specifically selected by reconstruction experts from around the world as the best approximation for ancient Egyptians.\nIt was then about applying the clay according to the musculature of the face and known anatomical ratios based on the actual skull. For example, Meritamun’s nose is squashed almost flat by the tight bandaging, but Mann was able to estimate what her nose would have looked like using calculations based on the dimensions of the nasal cavity. The skull also displays a small overbite that Mann has reconstructed. Meritamun’s ears are based on the CT scan results.\n“I have followed the evidence and an accepted methodology for reconstruction and out of that has emerged the face of someone who has come down to us from so long ago. It is an amazing feeling.”\nThe reconstruction was then cast in a polyurethane resin and painted. The researchers have taken a middle course in the long-running debate on what the predominant skin colour of ancient Egyptians may have been, choosing a dark olive hue. The finishing touch was to reconstruct her hair, which has been modelled on that of an Egyptian woman, Lady Rai, who lived around 1570-1530 BCE and whose mummified body is now in the Egypt Museum in Cairo. She wears her hair in tightly-plaited thin braids either side of her head. For the Meritamun reconstruction, replicating Lady Rai’s hair was an all-day job for a Melbourne African hair salon.\nFor the ancient Egyptians, mummification was about preserving the body so that the dead person’s spirit could find it again and secure everlasting life in the hereafter, while in this life their name could be preserved on their tombs. Tragically for the mummified head at the University of Melbourne, her name was lost long ago, but the researchers feel that by giving her a name and restoring her face they have gone someway to making up for what happened to her.\nAs Dr Davey says, “by reconstructing her we are giving back some of her identity, and in return she has given this group of diverse researchers a wonderful opportunity to investigate and push the boundaries of knowledge and technology as far as we can go”.\nLauren Greig, CT Business Manager at Siemens Australia and New Zealand, has provided vital support for the project. Dr Janet Davey was assisted by Monash University PhD candidate Samantha Rowbotham. The following have also contributed generously to this research: Dr Chris O’Donnell and Dr Richard Bassett from the Victorian Institute of Forensic Medicine; Professor Caroline Wilkinson from Liverpool John Moores University; Mary Asamoah from the African Queen Hair Salon; Lora Collins from the Smithsonian Institution; Kristina Scheelen from University of Gottingen; Rodney Broad, Malmsbury; Karen T. Taylor and the Forensic Anthropology Centre, Texas State University; and from University of Melbourne Associate Professor Chris Briggs, Marica Mucic, Holly Jones-Amin, Dr Andrew Jamieson, Dr Louise van der Werff, Ben Loveridge, Dr Rita Hardiman, Dr Ross Jones and Ben Kruenen.\nBanner Image: Meritamun. Sculpture: Jennifer Mann. Picture: Paul Burston', ""- Human Evolution Research\n- Climate and Human Evolution\n- Asian Research Projects\n- East African Research Projects\n- Human Origins Program Team\n- What's Hot In Human Origins?\n- Fossil Forensics: Interactive\n- E. A. Mammal Dentition Database\n- Human Evolution Evidence\n- 3D Collection\n- Human Fossils\n- Human Family Tree\n- Timeline Interactive\n- Human Characteristics\n- About Us\nSequencing Neanderthal DNA\nChallenges in Extracting Ancient DNA\nWorking with ancient DNA is very challenging, both in terms of finding sufficient material to work with after decomposition has occurred, and in terms of eliminating modern human contamination. Distinguishing between modern human and ancient genetic material is particularly difficult when the ancient DNA comes from close relatives of modern humans.\nOrganisms decompose after death. Water, oxygen and microbes break down DNA. Within 100,000 years, all DNA is destroyed. Ancient DNA tends to be found in small quantities. The DNA that is extracted is generally fragmentary and damaged. Some damage results in changes to the DNA sequence. Cytosine can change to uracil, which is read by copying enzymes as thymine, resulting in a C to T transition. Changes from G to A also occur. DNA errors are very common at the ends of molecules.\nContamination by modern DNA is a particularly difficult problem to solve. Labs and chemicals may be contaminated by the DNA of the people working in them, while many fossils have been handled by researchers for years. Contamination is difficult to detect because Neanderthals and humans share much of their genetic material, making some DNA sequences indistinguishable. Researchers have developed ways to analyze the results of ancient DNA sequencing efforts to determine whether contamination is likely and how much has occurred. Analysis of the results and efforts to keep labs and specimens free of modern DNA is very important as some researchers believe that the early studies of Neanderthal DNA included modern contaminants.\nSequencing the Complete Neanderthal Mitochondrial Genome\nAfter successfully sequencing large amounts of DNA and devising strategies to deal with potential contamination, a team led by Svante Pääbo from the Max Planck Institute, reported the first complete mtDNA sequence for a Neanderthal (Green et al. 2008). The 0.3 gram sample was taken from a 38,000 year old Neanderthal from Vindija Cave, Croatia. Complete Neanderthal mtDNA sequences give researchers more information about the relationship between modern humans and Neanderthals, as well as information about Neanderthal population size.\nThe complete mtDNA sequence shows that Neanderthals were outside the range of modern human mtDNA variation. Researchers compared the mtDNA sequence with that of modern humans. They compared sequence changes that resulted in nonsynonymous amino acid changes with synonymous changes. They found a larger number of nonsynonymous changes in the Neanderthal lineage, possibly implying that Neanderthals had a small population size with weaker purifying selection (Green et al. 2008).\nLater, Svante Pääbo’s lab sequenced the entire mitochondrial genome of five Neanderthals (Briggs et al. 2009). Sequences came from two individuals from the Neander Valley in Germany, Mezmaiskaya Cave in Russia, El Sidrón Cave in Spain and Vindija Cave in Croatia. Though the Neanderthal sample comes from a wide geographic area, the Neanderthal mtDNA sequences were not particularly genetically diverse. The most divergent Neanderthal sequence came from the Mezmaiskaya Cave Neanderthal from Russia, which the oldest and eastern-most specimen. To look at whether age or geographic location contributed to genetic differences, the team sequenced part of the DNA of another Mezmaiskaya Cave Neanderthal that dated to 41,000 years ago. This more recent specimen grouped with the other Neanderthals, possibly showing that age was the cause of the sequence differences (Briggs et al. 2009). Other studies show the existence of eastern, western and southern groups of Neanderthals (Fabre et al. 2009).\nOn average, Neanderthal mtDNA genomes differ from each other by 20.4 bases and are only 1/3 as diverse as modern humans (Briggs et al. 2009). The low diversity might signal a small population size, possibly due to the incursions of modern humans into their range (Briggs et al. 2009).\nSequencing the Neanderthal Nuclear Genome\nRecently, there have been efforts to sequence Neanderthal nuclear genes. Two studies, one by Svante Pääbo’s team and one by Edward Rubin, have sequenced large amount of Neanderthal nuclear DNA using different methods. Their results were announced in 2006. Given their success in sequencing some nuclear DNA, both labs launched projects to sequence the entire Neanderthal genome. Nuclear genomic sequences from Neanderthals show differences between modern humans and Neanderthals, and illustrate aspects of Neanderthal biology.\nOne Million Base Pairs of the Neanderthal Sequence\nSvante Pääbo’s team from the Max Planck Institute for Evolutionary Anthropology in Germany announced the sequencing of one million base pairs of nuclear DNA of a Neanderthal specimen in 2006 (Green et al. 2006). After a long search for specimens with a sufficient amount of undamaged DNA to sequence and for the ones with the least evidence of contamination, they focused on Vindija 80, a Neanderthal discovered in Croatia in 1980 that is approximately 38,000 years old.\nThey estimated that 7.9% of the changes in human DNA compared with that of the chimpanzee occurred after the split with Neanderthals. They dated the split between the ancestors of modern humans and Neanderthals to 465,000 to 569,000 years ago. They also found that the effective population size of the Neanderthals was small. Their success in sequencing this amount of DNA indicated that a large-scale project to sequence the Neanderthal genome is possible.\nRubin's Neanderthal Nuclear DNA\nEdward Rubin’s team from the Lawrence Berkeley National Laboratory in California also sequenced Neanderthal nuclear DNA (Noonan et al. 2006). They sequenced about 65,000 base pairs from the 38,000 year old Vindija, Croatia specimen. The technique used here produces a copy of the Neanderthal sequence that can be retained forever, reducing the need for repeated destructive sampling. The DNA is then cloned in bacteria.\nThe average split time between the Neanderthal and modern human populations was around 370,000 years ago. They used the sequence to look at the possibility of interbreeding between Neanderthals and moderns. Admixture would be seen as derived alleles that are found in Neanderthals and in low frequencies among modern humans. They did not detect this in their sample. A simulation to test the Neanderthal contribution to the human genome found a 0% chance of Neanderthal input with a 0% to 20% confidence range. With this data, the authors cannot definitively rule out admixture (Noonan et al. 2006).\nSome aspects of the two sets of nuclear DNA do not fit together, possibly because of contamination and sequencing errors, especially in the Green et al. (2006) study (Wall and Kim 2007). This has led the researchers to develop new methods of detecting and preventing contamination to ensure that only ancient DNA is being sequenced.\nA Draft Sequence of the Neanderthal Genome\nIn 2010, Svante Pääbo’s lab announced a draft sequence of the Neanderthal genome (Green et al. 2010). This new study has produced evidence consistent with interbreeding between Neanderthals and anatomically modern Homo sapiens and points to aspects of the human genome that may have changed since the split between humans and Neanderthals.\nDNA was extracted from three Neanderthal bones from Vindija Cave, Croatia. By comparing sequences from their mtDNA and their nuclear DNA, scientists determined that the three bones came from different individuals, although two of them might be related on their mother’s side. The researchers used several methods to ensure that the DNA they were sequencing was derived from the Neanderthal specimens rather than from contamination by modern humans in the lab.\nThe Neanderthal sequence was compared to those of five modern humans from France, China, Papua New Guinea, as well as Africans from the San and Yoruba groups. Tests indicated that Neanderthals shared more derived alleles with non-African modern humans than with African modern humans. They compared parts of the Neanderthal genome with pairs of modern humans. While the European and Asian pairs had similar amounts of derived material compared with the Neanderthal, Neanderthals had more similarities with non-African humans than with Africans. The simplest explanation for these results is gene flow from Neanderthals into modern humans. Gene flow could also have occurred from modern humans into Neanderthals. Interbreeding events between Neanderthals and modern humans might be obscured if the modern human population was large.\nNeanderthals have contributed approximately 1% to 4% to the genomes of non-African modern humans. This evidence of interbreeding sheds light on how we think of the expansion of modern humans out of Africa. It refutes the strictest scenario in which anatomically modern humans replaced archaic hominins completely without any interbreeding. However, even with some interbreeding between moderns and archaic hominins, most of our genome still derives from Africa.\nThe data also points to the time when interbreeding might have taken place. Since the Neanderthal DNA was equally related to that of the modern samples from France, China and Papua New Guinea, admixture between moderns and Neanderthals must have occurred before the Eurasian populations split off from each other. Remains of both modern humans and Neanderthals dating to around 100,000 years ago have been found in the Middle East. A few interbreeding events during this period could have produced the results found in this study.\nThe sequence of our close hominin relative also shows us how humans are unique. Researchers found 78 sequence differences that would have affected proteins in which Neanderthals had the ancestral state and modern humans had a newer, derived state. Five genes had more than one sequence change that affected the protein structure. These proteins include SPAG17, which is involved in the movement of sperm, PCD16, which may be involved in wound healing, TTF1, which is involved in ribosomal gene transcription, and RPTN, which is found in the skin, hair and sweat glands. Scientists do not know the function of the CAN15 protein, which was also one of the differences. Other changes may affect regulatory regions in the human sequence. Some changes are in regions that code for microRNA molecules that regulate protein manufacture.\nThe comparison also pointed out regions that might have been under positive selection in modern humans. Though some of the genomic areas that may have been positively selected for in modern humans may have coded for structural or regulatory regions, others may have been associated with energy metabolism, cognitive development and the morphology of the head and upper body.""]"	['<urn:uuid:af0a3902-ab86-4100-87c3-d9c231ebf8b2>', '<urn:uuid:8078d8a5-0700-4b28-bdd5-663c2c173c71>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	6	128	4154
80	What factors are causing birthrates to drop globally?	The fertility rates are falling due to less deaths in childhood, more access to contraception, and more women in education and work.	['Researchers have exposed that there is “remarkable” worldwide decline in the conception rate in women.\nThis fertility rate fall indicates that almost half countries were facing “baby bust”. According to the report, this means insufficient children to maintain the world population size.\nThis is a “huge surprise” said researchers, and there would be consequences for our for the society as there will be “more grandparents than grandchildren”\nAccording to the publication on the lancet, the study viewed the trends in every country from 1950 to 2017.\nThe study revealed, that in 1950 women were having an average of 4.7 children but when compared to last year (2017) its rate was halved to 2.4 children per woman.\nThere is a huge variation between on nations.\nThe fertility rate in Niger, West Africa is 7.1 unlike the Mediterranean island of Cyprus where women have one child, average.\nThis is the same for other west European countries.\nWhen a country’s fertility rate drops to 2.1, there’s a possibility of population.\nThe Director of the Institute of Health Metrics and Evaluation at the University of Washington, Prof. Christopher Murray told press, “We’ve reached this watershed where half of countries have fertility rates below the replacement level, so if nothing happens the populations will decline in those countries.\n“It’s a remarkable transition.\n“It’s a surprise even to people like myself, the idea that it’s half the countries in the world will be a huge surprise to people.”\nMore Developed countries which have low fertility rates are mostly Europe, the US, South Korea and Australia have lower fertility rates.\nProf Murray said: “We will soon be transitioning to a point where societies are grappling with a declining population.”\nThe fertility rates are falling due to less deaths in childhood, More access to contraception, and more women in education and work.\nDirector of the Oxford Institute of Population Ageing, Dr George Leeson, says that it does not have to be bad, so long as the population can be ready to adjust to demographic change.\nIn his word to press men “Demography impacts on every single aspect of our lives, just look out of your window at the people on the streets, the houses, the traffic, the consumption, it is all driven by demography.\n“Everything we plan for is not just driven by the numbers in the population, but also the age structure and that is changing, so fundamentally we haven’t got our heads around it.”\nHe believes that workplaces will need to change.\nAccording to the report, a section of Global Burden of Diseases analysis says affected countries need to review immigration, which has its own problems, or introduce policies that will make women have more children.\nProf Murray argues “On current trends there will be very few children and lots of people over the age of 65 and that’s very difficult to sustain global society.\n“Think of all the profound social and economic consequences of a society structured like that with more grandparents than grandchildren.\n“I think Japan is very aware of this, they’re facing declining populations, but I don’t think it’s hit many countries in the West, because low fertility has been compensated with migration.\n“At a global level there is no migration solution.']	['<urn:uuid:0993c27c-48f1-428d-9e74-37c91b26384c>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	8	22	539
81	I saw weird purple lights during the northern lights. What are they?	Those purple lights are a phenomenon nicknamed 'Steve'. It was first reported by citizen scientists between 2015-2016, who shared over 30 reports of these mysterious lights associated with Auroras. The phenomenon was extensively studied using both ground-based cameras and ESA's Swarm satellite, and scientists believe it may be an important piece in understanding how Earth's magnetic fields interact with charged particles in space.	['Astrophiz 89: Dr Andy Tomkins – Meteorites, Life on Mars & Fireballs in the Sky.\nDr Andy Tomkins conducts analyses of meteorites and applies his research looking for evidence of life on Mars. Andy is an associate Professor in the School of Atmosphere and Environment at Monash University in Melbourne Australia, and he and his team of PhDs and Honours students have collected over a third of Australia’s extensive meteorite haul, often from the vast and inhospitable Nullarbor Plain and they are one of the local driving forces of the Citizen Science project, ‘Fireballs in the Sky’.\nThen, in ‘What’s Up Doc’ our regular host Dr Ian Musgrave tells us what observers and astrophotographers can look for in the skies for the next two weeks and in ’Ian’s Tangent’ he tells us about the re-discovery of ‘Snoopy’ the command module from Apollo 10 which was presumably lost in space.\nIn the News:\nOur first story is from the NASA website:\n“Mystery of Purple Lights in Sky Solved With Help From Citizen Scientists”\nFrom 2015 to 2016, citizen scientists — people who are excited about a science field but don’t necessarily have a formal educational background — shared over 30 reports of mysterious lights associated with Auroras, in online forums and with a team of scientists that run a project called Aurorasaurus. The citizen science project, funded by NASA and the National Science Foundation, tracks the aurora borealis through user-submitted reports and tweets.\nThe Aurorasaurus team, led by Liz MacDonald, a space scientist at NASA’s Goddard Space Flight Center in Greenbelt, Maryland, conferred to determine the identity of this mysterious phenomenon.\nMacDonald and her colleague Eric Donovan at the University of Calgary in Canada talked with the main contributors of these images, amateur photographers in a Facebook group called Alberta Aurora Chasers, which included Notanee Bourassa and lead administrator Chris Ratzlaff. Ratzlaff gave the phenomenon a fun, new name, Steve, and it stuck.\nBut people still didn’t know what it was.\nScientists’ understanding of Steve changed on a night that Bourassa snapped his pictures, because Bourassa wasn’t the only one observing Steve. Ground-based all-sky cameras, run by the University of Calgary and University of California, Berkeley, took pictures of large areas of the sky and captured Steve and the auroral display far to the north and up in space, ESA’s (the European Space Agency) Swarm satellite just happened to be passing over the exact area at the same time and documented Steve as well.\nFor the first time, scientists had ground and satellite views of Steve, and have now learned, despite its ordinary name, that Steve may be an extraordinary puzzle piece in painting a better picture of how Earth’s magnetic fields function and interact with charged particles in space. Their fantastic findings are published in a study just released in Science Advances. You can read it in full at tinyurlDOTcom/aurorasteve because it is not behind a predatory journal paywall.\nAnd here’s another interesting story. We have been reporting on the SKA regularly since we started this podcast back in 2016 and The construction of the Square Kilometre Array (SKA) in South Africa and Australia has become the feather in the cap of both country’s scientific communities.\nWhen completed in the mid-2020s the South African SKA will have 197 dishes searching for faint radio signals from distant stars and galaxies.\nNow South Africa’s sparsely populated Northern Cape province, near the town of Carnarvon, is the perfect location for this international scientific endeavour: sprawling farms separate towns that are small and far-flung, keeping radio interference to a minimum.\nExcept, ironically, nobody thought to look up. Above Carnarvon at 35,000 feet is the flight path connecting Capetown and Jo’burg, with over 50 scheduled flights a day.\nNow back in 2007, South Africa’s government passed the Astronomy Geographic Advantage Act protecting the “Karoo Central Astronomy Advantage Areas,” where the SKA is located. According to the act, the use of the radio frequency spectrum from100 megahertz to 25.5 gigahertz – is prohibited in that region, and that includes all of the frequencies that are allocated for the use of aviation.\nIt’s worth noting that the act specifically excludes aviation for now, but the door is open for the ministers of Transport and Science and Technology to agree on “measures necessary for the protection of the Karoo Core and Central Astronomy Advantage Areas.”\nTo find a solution a technical working group has been formed, with representatives from the SKA, Department of Transport, Department of Science and Technology, Civil Aviation Authority, the aviation industry and Air Traffic Navigation Services (ATNS), the agency responsible for the country’s airspace management.\nSo there you have it, the aviation industry versus science, this will be a good one to watch play out.\nFinally, Another continuing story we are following here at Astrophiz is the ongoing quest to find the cause of FRBs, fast radio Bursts.\nIn a new paper, accepted into The Astrophysical Journal, and currently available on the pre-print resource arXiv, the research team behind this work has analysed the magnetar XTE J1810-197’s low-frequency radio output.\nAccording to their analysis of XTE J1810-197, the millisecond bursts of low-frequency radio waves sputtered out by the magnetar show an unusual similarity to FRB signals. It’s far from conclusive proof that the two phenomena are linked, but it’s one of the most tantalising hints yet.\nA reminder that Magnetars are neutron stars with extremely strong magnetic fields (over a quadrillion times stronger than Earth’s magnetic field). Scientists first spotted the magnetar XTE J1810−197 after an X-ray outburst in 2003, and in 2004, researchers detected a radio emission from the source. The magnetar produced the first transient radio emission ever detected by scientists. But after a few years of emitting variable radio emissions, the magnetar went quiet in 2008.\nThen it reactivated last year in December 2018. Scientists detected a bright, pulsed radio signal at 1.52 gigahertz (GHz) coming from the magnetar, the second radio outburst observed from this source. A strange and exciting observation, scientists led by Yogesh Maan of the Netherlands Institute for Radio Astronomy, launched an investigation of the object, studying it at low radio frequencies using the Giant Metrewave Radio Telescope (GMRT).\nThe team, found that radio bursts from this magnetar are relatively narrow, it is also relatively strong, the researchers found.\n“The bursts exhibit spectral structures which cannot be explained by interstellar propagation effects,” the team of astronomers wrote in the paper describing their findings, which was published Aug. 12 in the preprint journal arXiv, stating that the bursts from XTE J1810−197 don’t disseminate in space as expected.\nNow if anyone would like a Nobel Prize, join the race to identify the cause of FRBs. Or just do it because your curious and you like a challenge. Now go!']	['<urn:uuid:7e0e013e-d9f0-4dc1-bb18-c9af4177bb2f>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	12	63	1130
82	What are some effective strategies for using trees to reduce home energy costs during different seasons?	Generally, trees can provide energy savings through two main effects: cooling through water evaporation from leaves and shading of buildings. However, placement is crucial. While shading is beneficial in summer, it can be problematic in winter. Even deciduous trees, which block 70-90% of radiation when in leaf, still block 30-40% of radiation in winter through their branch structure. Studies show that planting large trees on the west side of a house, about 35 feet away, provides the best energy savings - up to $200 per year. Trees on the east side are also very effective, as are those in southeast and southwest positions. However, trees planted directly south or north are not effective for energy savings.	"['The Right Tree in the Right Place\n""The key to healthy tree growth is to get the right tree in the right place,"" said Dr. David Nowak in a phone conversation with me last week. Nowack is a Project Leader at the USDA Forest Service Northern Research Station in Syracuse. Nowak studies the functions of trees: air pollution removal, carbon dioxide removal and storage, reduction of UV radiation, air temperature effects and water effects. ""When you put the tree out there as a physical attribute, it affects the physics and the chemistry of the air around it. The other function we look at is energy conservation, which is important to homeowners,"" he said.\nAccording to Nowak, ""How you plant a tree in relation to your building in terms of placement"" is the biggest factor that determines how much energy savings the tree can provide. Nowak said, ""Generally, if you plant trees in an urban area around your home it tends to have cooling effects because it evaporates a lot of water through the leaves. The other effect is shading the building. If you shade your building, the heat is not beating on the building. In the northern regions, all our energy use is not necessarily used in the summertime.""\nNowak continued, ""This is where design of trees planted around a building becomes more important because if I shade a house during the summertime, that\'s a good thing. If I shade a house during the wintertime, that\'s not a good thing. Even deciduous trees, which block anywhere between 70 and 90% of the radiation when its in leaf will still block about 30 to 40% of the radiation in the wintertime from the branch structure. So having a tree out of leaf is still shading the building will have impacts on energy use in the wintertime.""\nOn a large scale, the city of Sacramento has systematically used trees to lower peak energy use, with great results. In the late 1980s, the rate-payers of the Sacramento Municipal Utility District (SMUD) voted to close the Rancho Seco nuclear power plant, which left the rate payers and the municipality facing much more expensive power sources. SMUD looked to the Sacramento Tree Foundation to reduce the load of peak energy consumption. This year the program is 20 years old, and they\'ve planted just under 500,000 trees. According to Ray Tretheway, Director of the Sacramento Tree Foundation, ""If you plant 100 trees in a neighborhood, about 3 trees per house, the homeowner will save about 30% on his energy bill during the summer months. That\'s how it started. But we got a lot more sophisticated. That\'s not demand side management. That\'s just evapotranspiration and direct shading.""\nTretheway continued, ""About 3 years into the program in the early 1990s, SMUD said, \'We need to make this a demand side management.\' In other words, peak power during the summer months during peak demand time that the utility doesn\'t have to purchase or produce during the time when energy is the most costly - 200 to 400 times more expensive. They created a demand side management model, which is based on 20 years of lifetime of the tree. There are 3 principle components: The size of the tree, the orientation of the tree being planted to the house, and the distance. So I can share with you that the best tree we can plant for present value benefit is a large tree on the west side, 35 feet from the house. That\'s the #1 tree for energy savings. It can save $200 a year. We also know a small tree can save $32 a year to a utility. So out of the 72 cardinal points, we have 26 points where we can justify planting a tree for this fathom power plant.""\nThe Right Place\n""Out of those 72 points we eliminated about 44 of them. In other words, if you put a large tree about 25 feet away on the south side, there will be no peak energy savings for the utility because the sun is straight ahead. It\'s not catching shade at all. We wouldn\'t plant a large tree on the east side. We send a community forester out to each and every home. All the trees on the west side are very effective. The trees on the east side are very effective. Then it goes around kind of how the sun goes. It comes up in the east and swings around to the west. So then the southeast is a very effective area, and southwest is good too. But directly south is no good, and north is off.""\nTretheway said, ""The biggest disappointment is we\'re effective and we\'re saving a tremendous amount of peak energy, but we\'re losing about 50 to 45 % of our trees. We think 10% of those don\'t really count because the homeowner never plants them. So that number really is 35.""\nCauses of Tree Failure: Insects\n""There are many reasons why newly planted trees don\'t survive."" Dr. Nowak said, ""Truthfully, homeowners should be thinking about what pests are going to be coming out. There is Asian longhorned beetle, which is in the Toronto, NY, NJ area. It\'s back in Chicago. That one is fairly confined, but if it gets out it will effect a lot of trees. The big one right now is emerald ash borer. It has been working its way from Ohio to Pennsylvania to NY State to West Virginia and other areas. That\'s attacking ash trees.""\nNowak said, ""There is an injection you can do. It\'s fairly expensive and fairly effective, and it\'s probably for more of the high value trees, and you might have to treat these trees every year. There is a similar type of treatment for Asian longhorned beetle. There is a whole host of these pests. There are native pests and exotic pests. A lot of them will attack no matter what. They don\'t differentiate between an urban or non-urban area.""\n""The question becomes what do you select in the future? If you are in an emerald ash borer area, you might want to think about not planting ash trees. There\'s so many different ones, along with endemic pests. The thing about urban areas is that we mix commerce. We bring wood pallets and things in from all over the world.\nHomeowners need to be aware that there are new pests coming around depending on where you are. If you already have an [infected] ash tree and it\'s a nice ash tree, you might want to consider doing some injections if you really want to save that tree. If it is a healthy tree and doing fine, don\'t do anything now,"" said Nowak.\nThe Right Tree\nSelecting the right tree involves taking into consideration your local climate, the site you are planting on, soil conditions and the threat of pests. Consulting with a certified arborist is one option. There is also tree selection software on the market. You can also consult with an expert at your local nursery.\nThe Northern Research Station has developed a Web site called iTree. Dr. Nowak said, ""On iTree, there is a free tool called iTree Species, which gives you a general listing and allows users to pick from the functions of a tree, like pollution removal, carbon storage, air temperature reduction, and they weight what they want from 0 to 10. Based on what city they put on the database, it will make some suggestions for general classes of types of trees. It has about 1500 species in there, but some of the species adapted for that area might not be on that list.""\nAlthough iTree is used mostly by municipalities and academics, the team at the Northern Research Station is in the process of developing an application designed for homeowners. The team aims to launch it this summer, so keep your eyes peeled.\nPhotos: Pictured first is a row of valuable trees. Pictured second is Dr. David Nowak.\nLooking for a Pro? Call us(866) 441-6648']"	['<urn:uuid:ab42af3f-7171-44e4-80c9-61ed95f16f30>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	16	116	1337
83	How does ancient yoga philosophy relate to modern wellness practices?	Ancient yoga philosophy focused on alleviating the mind from suffering through practices like meditation and breathing techniques, with the ultimate goal of reaching Samadhi or pure consciousness. The eight-limbs system developed by Patanjali included ethical standards (Yamas), self-disciplines (Niyamas), poses (Asana), breathing techniques (Pranayama), and various levels of meditation. This traditional approach aligns with modern wellness practices, as contemporary yoga is used as a mind-body practice to achieve peacefulness, manage stress and anxiety, and improve overall well-being. Today's practice emphasizes physical poses, controlled breathing, and meditation, helping practitioners reduce stress, improve fitness, and manage chronic conditions, though with less focus on spiritual enlightenment and more on practical health benefits.	"['Humans are exceptional beings who have accomplished the amazing. We are forever observing the world. Our curiosity about nature, space and ourselves has allowed us to create and discover the imaginable.\nIndia has 6000 years of known history focused on the philosophy of the mind. In ancient India, seers would study the attributes of human nature, how our senses help us to understand the world around us. The mind, however, is more complex and is forever entangled in the material realm. No matter how much we develop, the mind is still one of life’s biggest unanswered mysteries and is essential in balancing human pleasure, pain, sadness and happiness.\nAncient Yoga And Shamanism\nYoga dates back to the ancient Indus Valley Civilisation (3000-1800 BCE), as well as the Quetta, Harappan, and Mohenjo Daro civilisation. Excavations carried out in these areas revealed statues representing people practising yogic poses.\nInterestingly, academics suspect yogic roots began in Shamanism of the Stone Age due to the similarities between them. Both practices focused on the health and well-being of humans while inserting some religious ideals within the customs that were intended to reach a broad range of humans.\nOnce archaeologists found the statues of Indus Valley, the next sign of yoga was visible in the Rig Veda. The Rig Veda is a very important text dating back to 1500 and 1200 BC. “The Vedas” are hymns, mantras, and rituals all put together in one ancient book. They are the sacred writings for Brahmanism, which is the foundation of the Hindu culture.\nThe term Brahman appears in many Vedic texts to qualify the Supreme Self (Sva) which refers to the transcendent Absolute and the Cosmic Consciousness. The Vedas themselves are a collection of scriptures: the science of Mother Nature, agriculture, mathematics, astronomy, social sciences, ethics & morality, culture, arts, medicine, and much more.\nIn the Vedas, yoga – which means to unite – is the earliest written mention of the term. In ancient times, it was believed that all that existed internally within the body, also existed in the universe. The goal for seers was to research how they could establish a theory – based on this assumption. No telescopes or microscopes were needed. Instead, the observer had to be its own tool – to look inward and recognise the stages or what some call – chakras of the body. The more focus and clear the observer, the more they could see. This process is yoga. Yoga’s goal is to alleviate the mind from suffering.\nAlthough the Vedas do not go into detail between the practice of yoga and its techniques, they do mention the combination of discipline and breathing. During this period, yoga fundamentally consisted of groups of men forming exercise practices, tapas, by gathering around a fire and making primordial sounds while concentrating on breathing.\nThe Upanishads are philosophical works in dialogue form that discuss questions of nature, philosophy, the fate of the soul, and contain some mystic and spiritual interpretations of the Vedas. The word Upanishad literally means to sit near, and this invokes the image of devotees or aspirants sitting at the feet of a master. It comes from the Vedas, but the Yoga Upanishads explore the realm of mind and body with more focus. Where Vedic yoga was more focused on chanting to a higher being, Yoga Upanishads emphasises the importance of meditation and other yoga practices.\nThe Epics And Gita (Mahabharata)\nThe two great epics of India are the Ramayana (smrti) and Mahabharata. These are two books that encompass the truths and virtues of the universe and humankind. The Ramayana depicts the duties of relationships, portraying ideal characters like the ideal father, the ideal servant, the ideal brother, the ideal wife and the ideal king.\nThe Bhagavad Gita which means “song of the lord” is generally accepted as an earlier work from the Upanishads set down into the larger work of the Mahabharata, a love story. It focuses on a key teaching tale – the foundation of Yoga as a way of life yoga is to break the bonds of suffering.\nThe Gita spoke of yoga and revealed that it had in fact been around for quite some time before the scripture was written, yet does not give a date that indicates when yoga began. The main focus of the Gita is to emphasize that living functioning and accomplishing things, but to stray from hardships people must put their egos aside and let their actions speak for who they are.\nDuring this time, Buddhism began to emerge but with differences. Where the Gita focuses on action to prevail from suffering, Buddhism promotes withdrawal from the world because life means suffering. The first Buddhist to study the teachings of yoga was Siddharta Gautama, and he is said to have experienced total enlightenment at age thirty-five.\nPatanjali And The Sutras\nExperts say that a man named Patanjali wrote three important treatises: Yoga Sutras, Mahābhāṣya, and Ayurveda. With yoga sutras, he eliminated the impurities of the mind and suffering. Through grammar, he eliminated the impurities of speech and through his composition of Ayurveda, the ailments of the body.\nThe yoga sutras are a technique to keep the mind free and peaceful – a compilation of answers for the enquiring mind. The word sutra comes from the same root as the medical term suture, meaning to connect or hold together. They are threads of knowledge that allow you to explore the essential core meaning of yoga.\nPatanjali was the first to demystify the elements of consciousness and arrange them scientifically. He created the sutras with precise classifications, sequences and logic. To keep to their simplest form, the yoga sutras teaching fit onto one page which were later elaborated.\nWhen a teacher expounded on a piece of knowledge from the yoga sutra, the student would be given a short phrase that would later remind him/her of the greater body of material. The sutras are divided into four chapters, or padas: Samadhi, Sadhana, Vibhuti, and Kaivalya.\nYoga wants bring man back to the point of purity and consciousness – what yogis call Samadhi.\nThis is the first chapter of the Yoga Sutra. It the detachment of the material world. It is about enlightenment, focusing on concentration and meditation. This chapter is for advanced practitioners where the state of self-realisation is attained beyond the intellect which cannot be achieved through ordinary wisdom. For the ordinary person, they must start at the second chapter called Sadhana Pada.\nThis chapter is considered the most important of all the sutras. It is the practice of yoga to prepare the body and mind for Samadhi. Sadhana Pada describes Kriya yoga (yoga of action) and the steps to eliminate worldly suffering and reach a yogic state — a process known as The Eight Limbs of Yoga, or the eight-fold path (Ashtanga yoga).\nThe eight limbs introduced in Sadhana Pada are:\n- Yamas (ethical standards)\n- Niyamas (self-disciplines)\n- Asana (poses)\n- Pranayama (breathing techniques)\n- Pratyahara (withdrawal of senses)\n- Dharana (concentration)\n- Dhyana (meditation)\n- Samadhi (absorption into the object of focus)\nThere are five Yamas: non violence, truth, non-stealing, non possession and celibacy.\nThere are also five Niyama: cleanliness, contentment, austerity, recistation and reverence for God. God in this context is not a creator, but is consciousness that unites us outside of the bonds of nature. Religion demands faith, but yoga is existential, experimental and experiential. The seeker, by controlling and regularizing daily conduct of Yama and Niyam, sits in an Asana.\nAsana is a comfortable and pleasant posture of the body where the body should not move. The vision should either be at the tip of the nose or the centre of the eyebrows, or at a target point. When one practices an asana for hours, the body becomes steady and consciousness is rested in the infinite. On acquiring stability, the seeker regulates the breath, when the breath becomes subtle, that is Pranayama. Pranayam purifies the mind and also cleanses the channels of energy. By making the breath subtler, the most hyperactive mind can be controlled.\nAfter Pranayama, the practise of focusing the mind away from external objects and focusing it inwardly is practised. Our life is nurtured through the sensory experiences through touch, speech, vision, taste and smell. Pratyahara is the reverse, going from the external senses to within.\nThe third chapter is about the results, power, and manifestation once union is achieved – that Samadhi can be attained. It describes the capacity of the mind which can achieve a state free from distractions. Such a mind can probe deeply into objects and concepts into dimensions previously unknown. This knowledge, or power, can become a source of distraction and can prevent a person from reaching the highest state of being.\nThis is where Patanjali introduces the last three limbs of the eight. When the mind gets stabilised – that is called Dharana. It is the point of concentration which can be an object, real or abstract, or a thought. When Dharana gets prolonged, it becomes Dhyana, meditation which extended further results in Samadhi – the purest and most steady state of being.\nHere Patanjali concludes his treatise with more metaphysics and the presentation of the yogic vision of freedom, kaivalya. The last chapter is about liberation, or moksha. The sutras clarify liberation and what is achieved by the mind. This final chapter is devoted to complete, unconditional, and absolute liberation that removes us from the fear of death and accepts eternal being.\nA few centuries after Patanjali, yoga masters created a system of practices designed to rejuvenate the body and prolong life. They rejected the teachings of the ancient Vedas and embraced the physical body as the means to achieve enlightenment. They developed Tantra Yoga, with radical techniques to cleanse the body and mind to break the knots that bind us to our physical existence.\nThe word tantra literally means “expansion.” A tantra yogi concentrates on expanding all levels of his or her consciousness to unveil and realize the Supreme Reality. Tantra focuses on the dynamic aspect of divinity called Shakti, or “the Cosmic Mother.” The tantric devotee strives to attune with the spiritual dynamic energy in order to transform personal limitations and release subconscious blockages.\nIt is the esoteric version of hinduism and buddhism. Although it is generally associated with sexuality, this was not the core value. The important part was to follow your own path, and not that of a yogi’s. Over time, some developed the path to sexual orientation which became a widely accepted apprach of breaking the bonds of reality.\nYoga Philosophy And Religion\nIs yoga connected to other religions and faith or is it man’s attempt to conquer disease, old age and death? As we have seen, there are different forms of yoga. Some of which are more overtly religious than others. Hare Krishna monks, for example, are adherents of bhaktiyoga, the yoga of devotion whereas hatha yoga, ehat most people in the West think of as the main practice of yoga – a path towards enlightenment that focuses on building physical and mental strength.\nBut what “enlightenment” means also depends on tradition. For some Hindus it is liberation from the cycle of reincarnation, but for many yoga practitioners it is a point where you achieve stillness in your mind, or understand the true nature of the world and your place in it.\nThe principles of yoga are closely related to these other Eastern religions, most notably Buddhism and Hinduism, which we have discovered when tracing back to yoga’s roots. Buddhism began in the 6th century B.C. when Buddha began his teachings including specific postures and meditation.\nAs the years continued, yoga was introduced to the western world and is now taking a universal approach suitable to anyone who is looking for peace of mind – no matter what their beliefs.\nYoga In The West\nHatha yoga, the most popular form of yoga known in the west, was introduced n the late 1800s and early 1900s when yoga masters began to travel to the West. They quickly gained a lot of attention and followers. In 1893, Parliament of Religions in Chicago, a yogi called Swami Vivekananda fascinated the audiencewith his lectures on yoga. He was a key figure in the introduction of the Indian philosophies of Vedanta and Yoga to the Western world and is credited with raising interfaith awareness, bringing Hinduism to the status of a major world religion during the late 19th century. He was a major force in the revival of Hinduism in India, and contributed to the concept of nationalism in colonial India.\nIn the 1920s and 30s, Hatha Yoga was strongly promoted in India with the work of T. Krishnamacharya, Swami Sivananda and other yogis practicing Hatha Yoga. Krishnamacharya opened the first Hatha Yoga school in Mysore in 1924 and in 1936 Sivananda founded the Divine Life Society on the banks of the holy Ganges River. Krishnamacharya educated three students that would continue to pass down his teaching and increase the popularity of Hatha Yoga. These three students were B.K.S. Iyengar, T.K.V. Desikachar and Pattabhi Jois. Sivananda was also a writer who released 200 books on yoga.\nYoga has come a long way since Ancient India. There are so many variations of yoga including Bikram, which practices yoga in intense heat, and critical alignment, which focuses on the skeletal structure of the body and readjusting it as a chiropractor would do.\nNowadays, there is much less focus on the spiritual teachings of reaching enlightenment. In order to know more about the philosophical practice of yoga, practitioners must study with books or attend yoga school. These days, modern yoga is more about taking care of the mind and body to manage within our society. Having a moment to ourselves to breathe, relax and be still. When we do not take a moment for ourselves to reflect on the bigger picture, we get lost in our daily activities.\nAs the ancient Vedic philosophy states, the individual self is alienated from the transcendent self, and this isolation is the root cause of human suffering. The main yogic concept is not about spirituality, but about integrating all aspects of our lives which leaves us with inhabiting our bodies and being comfortable with ourselves.', ""Is yoga right for you? It is if you want to fight stress, get fit and stay healthy.By Mayo Clinic Staff\nYour mobile phone is ringing, your boss wants to talk to you and your partner wants to know what's for dinner. Stress and anxiety are everywhere. If they're getting the best of you, you might want to hit the mat and give yoga a try.\nYoga is a mind-body practice that combines physical poses, controlled breathing, and meditation or relaxation. Yoga may help reduce stress, lower blood pressure and lower your heart rate. And almost anyone can do it.\nYoga — a mind-body practice — is considered one of many types of complementary and integrative health approaches. Yoga brings together physical and mental disciplines that may help you achieve peacefulness of body and mind. This can help you relax and manage stress and anxiety.\nYoga has many styles, forms and intensities. Hatha yoga, in particular, may be a good choice for stress management. Hatha is one of the most common styles of yoga, and beginners may like its slower pace and easier movements. But most people can benefit from any style of yoga — it's all about your personal preferences.\nThe core components of hatha yoga and most general yoga classes are:\n- Poses. Yoga poses, also called postures, are a series of movements designed to increase strength and flexibility. Poses range from lying on the floor while completely relaxed to difficult postures that may have you stretching your physical limits.\n- Breathing. Controlling your breathing is an important part of yoga. Yoga teaches that controlling your breathing can help you control your body and quiet your mind.\n- Meditation or relaxation. In yoga, you may incorporate meditation or relaxation. Meditation may help you learn to be more mindful and aware of the present moment without judgment.\nThe potential health benefits of yoga include:\n- Stress reduction. A number of studies have shown that yoga may help reduce stress and anxiety. It can also enhance your mood and overall sense of well-being.\n- Improved fitness. Practicing yoga may lead to improved balance, flexibility, range of motion and strength.\n- Management of chronic conditions. Yoga can help reduce risk factors for chronic diseases, such as heart disease and high blood pressure. Yoga might also help alleviate chronic conditions, such as depression, pain, anxiety and insomnia.\nYoga is generally considered safe for most healthy people when practiced under the guidance of a trained instructor. But there are some situations in which yoga might pose a risk.\nSee your health care provider before you begin yoga if you have any of the following conditions or situations:\n- A herniated disk\n- A risk of blood clots\n- Eye conditions, including glaucoma\n- Pregnancy — although yoga is generally safe for pregnant women, certain poses should be avoided\n- Severe balance problems\n- Severe osteoporosis\n- Uncontrolled blood pressure\nYou may be able to practice yoga in these situations if you take certain precautions, such as avoiding certain poses or stretches. If you develop symptoms, such as pain, or have concerns, see your doctor to make sure you're getting benefit and not harm from yoga.\nAlthough you can learn yoga from books and videos, beginners usually find it helpful to learn with an instructor. Classes also offer camaraderie and friendship, which are also important to overall well-being.\nWhen you find a class that sounds interesting, talk with the instructor so that you know what to expect. Questions to ask include:\n- What are the instructor's qualifications? Where did he or she train and how long has he or she been teaching?\n- Does the instructor have experience working with students with your needs or health concerns? If you have a sore knee or an aching shoulder, can the instructor help you find poses that won't aggravate your condition?\n- How demanding is the class? Is it suitable for beginners? Will it be easy enough to follow along if it's your first time?\n- What can you expect from the class? Is it aimed at your needs, such as stress management or relaxation, or is it geared toward people who want to reap other benefits?\nEvery person has a different body with different abilities. You may need to modify yoga postures based on your individual abilities. Your instructor may be able to suggest modified poses. Choosing an instructor who is experienced and who understands your needs is important to safely and effectively practice yoga.\nRegardless of which type of yoga you practice, you don't have to do every pose. If a pose is uncomfortable or you can't hold it as long as the instructor requests, don't do it. Good instructors will understand and encourage you to explore — but not exceed — your personal limits.\nNov. 05, 2015\n- Yoga for health. National Center for Complementary and Integrative Health. https://nccih.nih.gov/health/yoga. Accessed Oct. 6, 2015.\n- Rakel RE, et al., eds. Relaxation techniques. In: Textbook of Family Medicine. 9th ed. Philadelphia, Pa.: Elsevier Saunders; 2016. http://www.clinicalkey.com. Accessed Oct. 5, 2015.\n- Selecting and effectively using a yoga program. American College of Sports Medicine. http://www.acsm.org/public-information/brochures-fact-sheets/brochures. Accessed Oct. 5, 2015.\n- 6 things to know when selecting a complementary health practitioner. National Center for Complementary and Integrative Health. https://nccih.nih.gov/health/tips/selecting. Accessed Oct. 6, 2015.\n- Fishbein DB, et al. Overview of yoga. http://www.uptodate.com/home. Accessed Oct. 2, 2015.\n- AskMayoExpert. Yoga: Summary. Rochester, Minn.: Mayo Foundation for Medical Education and Research; 2015.\n- AskMayoExpert. Yoga: Indications and contraindications. Rochester, Minn.: Mayo Foundation for Medical Education and Research; 2015.\n- AskMayoExpert. Yoga: Cautions. Rochester, Minn.: Mayo Foundation for Medical Education and Research; 2015.""]"	['<urn:uuid:c7838c25-534f-4625-937c-80ec01a6ea5f>', '<urn:uuid:3a0c8b1f-5f51-40de-af01-3d8bb2586259>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	10	109	3323
84	I'm confused about fructooligosaccharides - what are their health benefits?	Fructooligosaccharides (FOS) provide several key health benefits: They promote the growth of beneficial gut bacteria like Bifidobacteria and Lactobacilli, which can reach levels of 10 million CFU per gram of intestinal contents. When these good bacteria ferment FOS, they produce compounds that help lower cholesterol and support immune system function. Additionally, the beneficial bacteria that thrive on FOS can help suppress potential harmful bacteria in the gut through the production of organic acids like acetic and lactic acids. This helps maintain a healthy balance of gut microorganisms.	"['In vivo utilisation of fructooligosaccharides by sheep faecal bifidobacteria and in vitro antagonistic effects against intestinal pathogens\nLi, Y. (2003). In vivo utilisation of fructooligosaccharides by sheep faecal bifidobacteria and in vitro antagonistic effects against intestinal pathogens (Thesis, Doctor of Philosophy (PhD)). The University of Waikato, Hamilton, New Zealand. Retrieved from https://hdl.handle.net/10289/13231\nPermanent Research Commons link: https://hdl.handle.net/10289/13231\nThe application of pro-, pre, and synbiotics has been studied primarily in humans and some other monogastric animals. Very few studies have been made to determine their effects on the ruminant intestinal microflora. This project tested whether or not four commercial food-grade oligo- and polysaccharides (three FOS products and one polysaccharide control, Arabinogalactan) could modify the hindgut microflora of sheep towards a more salutary community in which the health-promoting bacterial groups bifidobacteria and lactobacilli predominate, whereas the potential intestinal pathogens and putrefactive bacteria E. coli and sulphite reducing clostridia are suppressed. A fructo-6-phosphate phosphokatolase (F6PPK) enzyme-based identification protocol was developed and optimised for identifying and large-scale screening of presumptive bifidobacteria isolates from gut contents or faecal samples. An in vivo experiment was then carried out to determine the bifidogenic effect (promotion of bifidobacteria by prebiotics) and the associated antimicrobial effect (suppression of potential pathogens due to the increase in the populations of bifidobacteria) of the four oligo- and polysaccharides on sheep hindgut microflora. Twelve fistulated sheep were managed in a balanced, two Latin square, cross-overdesign experiment, which was run in 5 consecutive periods, with each of 5 treatments (Arabinogalactan, Fibruline, Raftilose, Yacon, and an ""acidified saline"" carbohydrate-free control) administered to two sheep in each period. Each period consisted of a 1 week of stabilisation to the pelleted diet (no oligo- and polysaccharides), followed by 14 days of daily abomasal supplementation of oligo and polysaccharide/acidified saline, followed by about 12 days of normal pelleted diet. In each period, sheep faecal bifidobacteria, lactobacilli, E. coli/ Enterobacteriaceae, sulphite reducing clostridia, and total anaerobes were enumerated on the day -4 of the ""stabilisation"" period; days 3 & 9 of the ""treatment supplementation"" period, and days 15, 16, 19 & 26 after cessation of infusions. Raftilose, Yacon, and Fibruline all exerted significant bifidogenic effects on sheep faecal bifidobacteria after 9 days of daily dosing. Raftilose produced the greatest stimulation of bifidobacteria, reaching counts of approximately 107 CFU/g of faeces. No significant changes in the populations of bifidobacteria were observed in Arabinogalactan-treated sheep. Raftilose and Yacon significantly increased the number of lactobacilli, reaching approximately 107 CFU/g of faeces, after 9 days of daily dosing. With Fibruline, the lactobacilli increase after 9 days of administration was not significant. Arabinogalactan did not elevate the populations of lactobacilli. All four carbohydrate treatments significantly increased the number of total anaerobes to approximately 107 to 108 CFU/g of faeces after 9 days of daily dosing. Supplementation of the test oligo- and polysaccharides had no significant effect on the other determined groups of gut microflora: sulphite reducing clostridia and E. coli/Enterobacteriaceae. There were no significant changes in sheep faecal pH and dry matter content with the four treatments. Further in vitro antagonistic experiments were carried out to determine whether or not the isolated sheep faecal bifidobacteria inhibited the growth of potential intestinal pathogens in fermentation broth containing the bifidogenic FOS. One hundred and seventeen bifidobacterial isolates from sheep faeces, were screened for their capacity to utilise different oligo- and polysaccharides. Eighteen of these, with strong fermentation patterns, were selected for further study. In the first preliminary experiment, the 18 isolates plus 2 reference cultures of bifidobacteria were divided into 5 groups of 4 strains each. The 5 groups of bifidobacteria were compared for their in vitro antagonistic activities against E. coli in Peptone Yeast Extract broth containing Yacon, Raftilose, or Fibruline as primary carbon sources. Two groups, exerting 100% antagonistic effects against E. coli after 48-hour anaerobic co-culture at 37°C, were selected for further examination. The eight individual strains in these two groups were tested individually for their antagonistic activities against E. coli in PY broth containing Yacon or Raftilose. Fibruline was eliminated due to its low antagonistic activity by bifidobacteria. Six isolates, showed 100% inhibitory effects against E. coli, which made them particularly promising for use as probiotics. The pH of the fermented broths showed a clear negative correlation with the rank transformed or angular transformed inhibition rate of E. coli. One bifidobacterial strain, P5-Po4-37, was subsequently investigated for its in vitro antagonistic activity against E. coli by determination of bacterial growth kinetics over 60 hours anaerobic incubation at 37°C in PY broth containing Raftilose or Yacon. In this experiment, two different concentrations of bifidobacteria, 107 to 108 CFU/mL and 103 to 104 CFU/mL, were incubated with 104 to 105 CFU/mL of E. coli. After 30 and 48-60 hours of incubation, the growth of E. coli was completely inhibited by both the higher and lower concentrations of bifidobacteria, respectively. The presence of E. coli did not affect the growth of bifidobacteria. With the higher inoculum level, the populations of bifidobacteria increased by only approximately 1.30 log10 cycles; whereas with the lower inoculum level, the populations of bifidobacteria increased by approximately 5.62 log10 cycles, to attain the same maximum viable counts at approximately 109 CFU/mL. Fermentation products were analysed in Raftilose containing PY fermented broth. The inhibitory activity of strain P5-Po4-37 was associated with the production of acetic and lactic acids. Strain P5-Po4-37 also exerted strong antagonistic activities against Clostridium perfringens, Enterobacter aerogenes, Enterococcus faecalis, Klebsiella pneumoniae, Salmonella Dublin, and Salmonella Menston. These findings indicated that Yacon and Raftilose potentiate an organic acid mediated inhibitory action of bifidobacterial strain P5-Po4-37 against the test potential intestinal pathogens. To date, this demonstration of inhibitory activities has been convincingly made only in in vitro studies. The combination of Yacon or Raftilose and bifidobacterial strain P5-Po4-37 may exert a promising synbiotic effect on sheep hindgut microflora, which will be investigated in vivo in the near future.\nThe University of Waikato\nAll items in Research Commons are provided for private study and research purposes and are protected by copyright with all rights reserved unless otherwise indicated.\n- Higher Degree Theses', ""What Are Postbiotics? And How Do They Relate To Pre- and Probiotics?\nIn this article:\n- What Are Postbiotics?\n- Fiber Deficiency And Postbiotics\n- Healthy Gut Bacteria, The Foundation Of Good Health\n- 5 Key Health Benefits Of A Healthy Microbiome\n- How to Improve Your Postbiotic Microbiome\n- The Bottom Line\nYou have likely never heard the term, but postbiotics represent all the rage in gut health and microbiome science. Prebiotics and probiotics may be more familiar, but all three relate in an interdependent relationship critical to not only our digestive health but also our mental health — thanks to the gut-brain axis. In addition to your mental health, your gut microbiome greatly affects your immune, digestive, metabolic, and heart health.\nLet us examine postbiotics and their health benefits.\nPostbiotics represent the byproducts of probiotics eating prebiotics. That’s right! When you eat things such as grains or fresh fruit, the fiber in these foods is considered prebiotic. Probiotics then go and break down the fiber, converting them to metabolites we call postbiotics.\nProbiotics create various compounds from the fermentation of prebiotics considered postbiotics. Short-chain fatty acids (SCFAs), functional proteins, and extracellular polysaccharides (EPS) include only three examples of what can be described as postbiotics.\nWith functional bioactive compounds, research has shown postbiotics to have direct beneficial effects on your immune system. Studies also show postbiotics can be used in healthy individuals to benefit overall wellness. Conditions such as atopic dermatitis, diarrhea, and infant colic display relief with postbiotics as well.\nA healthy postbiotic microbiota starts with fiber. Not only is the amount of fiber eaten important for a healthy gut microbiome, but also a variety of fiber is needed to create postbiotic metabolites needed for good health.\nEating a variety of plant-based foods will increase and diversify your fiber consumption, and therefore strengthen your prebiotic health — directly affecting your postbiotic status. Fruits, vegetables, grains, and legumes offer good sources of fiber.\nYou should eat around twenty-five grams of fiber per day, but not more than fifty grams. Too much fiber can bring about symptoms such as bloating, decreased appetite, cramping, and constipation, and may impair absorption of phosphorus and calcium.\nA healthy gut microbiome has been linked to everything from a strong immune system to good mental health. Microbiome refers to the microorganisms that live in a particular environment. While trillions of microorganisms (microbes) live in and on your body, including fungi, bacteria, and viruses — trillions more live only inside your intestines, about 100 trillion.\nThe majority of these microbes live in one area of the large intestine called the cecum. Bacteria alone make up about forty trillion cells in your body, quite fascinating when you realize you only have thirty trillion human cells. This fact alone shows the importance of your microbiota.\nSome of these bacteria prove beneficial, which we call good or friendly bacteria; some remain harmful and may cause illness.\nWe first encounter microbes as we travel down our mother’s birth canal. As you grow older, the more plentiful and diverse your microbiome becomes.\nBifidobacteria, a friendly microbe, begins to grow early in the newborn’s intestines to help digest the sugars in breast milk. This bacterium remains important throughout life, as it creates SCFAs, an essential postbiotic needed for good health.\nAs you age, more bacteria take residence in your gut bringing with them digestive, immune, heart, metabolic, and mental health benefits.\nA strong intestinal microbiota proves essential for good overall health.\nGood bacteria, or probiotics, through digesting fiber signifies the cornerstone of good postbiotic health. The SCFAs produced by the fiber digesting bacteria help to metabolize fat and carbohydrates. They represent the primary source of energy for your cells lining the colon.\nWeight gain can be caused by dysbiosis, an imbalance of good and bad bacteria in the gut. Dysbiosis may also contribute to conditions such as inflammatory bowel disease (IBD) and irritable bowel syndrome (IBS). Symptoms such as discomfort, bloating, and cramps relate to dysbiosis.\nTaking probiotics with both Bifidobacteria and Lactobacilli can help individuals avoid discomfort caused by IBS and IBD.\nYour gut microbiota remains essential to your immune health. It regulates immune homeostasis, or balance, in the body. Changes in the intestinal microbial communities can lead to dysregulation of the immune system, contributing to an autoimmune disease of not only the intestines but also systemic autoimmune disease.\nBecause of this significant relationship between the gut microbiome and the immune system, researchers currently study new microbial therapies as potential treatments for autoimmune disease and other illnesses.\nA healthy gut can help contribute to a healthy heart. One study found the gut microbiota promotes good cholesterol, high-density lipoproteins (HDL), and triglycerides. When taken as a probiotic, Lactobacillus may also help to reduce cholesterol levels. Lower overall cholesterol and higher good cholesterol levels remain important for heart and blood vessel health.\nHigh cholesterol and low HDL levels contribute to plaque formation on arterial walls, which can lead to heart attack and stroke. Red yeast rice offers a natural supplement that can help to lower cholesterol levels.\nTrimethylamine N-oxide (TMAO) gets produced by unfriendly bacteria in the gut when they metabolize choline and L-carnitine. TMAO represents a compound that contributes to blocked arteries. Both choline and L-carnitine comprise foods made from animals, especially red meat.\nDecreasing your consumption of animal products and maintaining a healthy microbiome can help lower the chances of your gut bacteria creating TMAO.\nDiabetes and blood sugar levels can also be affected by the gut microbiota. A study found that even though the participants ate the same meals, their blood sugar levels following the meals varied greatly. The researchers noted the difference in intestinal microbes could be the reason for this variation.\nYet, another study found gut microbiome diversity significantly declined before the onset of type 1 diabetes. They also discovered unhealthy bacteria levels of various types increased before type 1 diabetes onset.\nRecently, the gut-brain axis has been the topic of many research studies. A hot topic of discussion has been the discovery that gut bacteria play a key role in brain neurotransmitter production.\nNeurotransmitters signify chemicals in the brain that inhibit or promote various physiological actions in the body. The neurotransmitter serotonin gets primarily synthesized in the gut. Serotonin has many functions in the body, including mood regulation and promoting feelings of well-being and happiness. It also aids in sleeping and digestive functions.\nTo have a healthy microbiome, you must first have good prebiotics.\nPrebiotics represent foods high in fibers like inulin and other compounds such as fructooligosaccharides (FOS). FOS not only supports healthy gut flora but also helps lower cholesterol and supports a healthy immune system.\nFOS and inulin have been shown to stimulate the growth of bifidobacteria in the gut. Bifidobacteria promotes inhibitory effects in the gut, helping to resist acute infections.\nAnother powerful prebiotic that promotes the growth of bifidobacterial involves wheat bran, the outer layer of whole wheat grain. Wheat bran offers a high amount of arabinoxylan oligosaccharides (AXOS). In addition to supporting the growth of friendly bacteria, AXOS also has antioxidant benefits.\nInulin represents a type of fiber that can naturally be found in onions, garlic, Jerusalem artichokes, dandelion greens, asparagus, and chicory root. You can also supplement inulin in case you do not consume enough inulin-rich foods in your diet.\nPectin and Beta-Glucan\nPectin and beta-glucan can also be supplemented for prebiotic support.\nIngesting more elephant yam for its high glucomannan fiber content will also support healthy and diverse postbiotics. Glucomannan supports the growth of good bacteria in the gut while also lowering cholesterol, supporting weight loss, improving immune function, and decreasing constipation.\nYou can take glucomannan supplements for increased support.\nEating fermented foods like kefir, yogurt, kombucha, and sauerkraut can increase your probiotic levels, improving your postbiotic status. Fermented foods primarily boost levels of Lactobacilli. They may also decrease levels of bad bacteria in the intestines.\nAvoiding artificial sweeteners like aspartame can also support your postbiotic health. They stimulate the growth of unfriendly bacteria including Enterobacteriaceae in the gut microbiota.\nLast by not least, avoid taking antibiotics if you can. Antibiotics destroy both good and bad bacteria in the gut. Only take them when it is medically necessary.\nPostbiotics form the foundation for good overall health. The brain, heart, immune, and gut cells depend on postbiotics to function at optimal levels.\nThe best way to have a good postbiotic status involves eating more prebiotics and increasing your probiotic flora. Only with good prebiotic and probiotic status can you experience the benefits of a healthy postbiotic microbiome.\n- Wegh CAM, Geerlings SY, Knol J, Roeselers G, Belzer C. Postbiotics and Their Potential Applications in Early Life Nutrition and Beyond. Int J Mol Sci. 2019;20(19):4673. Published 2019 Sep 20. doi:10.3390/ijms20194673\n- Kumar VP, Prashanth KV, Venkatesh YP. Structural analyses and immunomodulatory properties of fructo-oligosaccharides from onion (Allium cepa). Carbohydr Polym. 2015;117:115-122. doi:10.1016/j.carbpol.2014.09.039\n- Costa GT, Abreu GC, Guimarães AB, Vasconcelos PR, Guimarães SB. Fructo-oligosaccharide effects on serum cholesterol levels. An overview. Acta Cir Bras. 2015;30(5):366-370. doi:10.1590/S0102-865020150050000009\n- Kolida S, Tuohy K, Gibson GR. Prebiotic effects of inulin and oligofructose. Br J Nutr. 2002;87 Suppl 2:S193-S197. doi:10.1079/BJNBJN/2002537\n- Chen HL, Cheng HC, Liu YJ, Liu SY, Wu WT. Konjac acts as a natural laxative by increasing stool bulk and improving colonic ecology in healthy adults. Nutrition. 2006;22(11-12):1112-1119. doi:10.1016/j.nut.2006.08.009\n- Tester RF, Al-Ghazzewi FH. Beneficial health characteristics of native and hydrolysed konjac (Amorphophallus konjac) glucomannan. J Sci Food Agric. 2016;96(10):3283-3291. doi:10.1002/jsfa.7571\n- François IE, Lescroart O, Veraverbeke WS, et al. Effects of wheat bran extract containing arabinoxylan oligosaccharides on gastrointestinal parameters in healthy preadolescent children. J Pediatr Gastroenterol Nutr. 2014;58(5):647-653. doi:10.1097/MPG.0000000000000285\n- Clemens R. et al. Filling America’s Fiber Intake Gap: Summary of a Roundtable to Probe Realistic Solutions with a Focus on Grain-Based Foods. J Nutr. 2012 July; 142(7): 1390S-1401S.\n- Berdy J. Bioactive Microbial Metabolites. J. Antibiot. 2005;58(1):1.26.\n- Shah M, Chandalia M, Adams-Huet B, et al. Effect of a high-fiber diet compared with a moderate-fiber diet on calcium and other mineral balances in subjects with type 2 diabetes. Diabetes Care. 2009;32(6):990-995. doi:10.2337/dc09-0126\n- Sender R, Fuchs S, Milo R. Revised Estimates for the Number of Human and Bacteria Cells in the Body. PLoS Biol. 2016;14(8):e1002533. Published 2016 Aug 19. doi:10.1371/journal.pbio.1002533\n- Integrative HMP (iHMP) Research Network Consortium. The Integrative Human Microbiome Project: dynamic analysis of microbiome-host omics profiles during periods of human health and disease. Cell Host Microbe. 2014;16(3):276-289. doi:10.1016/j.chom.2014.08.014\n- Arboleya S, Watkins C, Stanton C, Ross RP. Gut Bifidobacteria Populations in Human Health and Aging. Front Microbiol. 2016;7:1204. Published 2016 Aug 19. doi:10.3389/fmicb.2016.01204\n- Ríos-Covián D, Ruas-Madiedo P, Margolles A, Gueimonde M, de Los Reyes-Gavilán CG, Salazar N. Intestinal Short Chain Fatty Acids and their Link with Diet and Human Health. Front Microbiol. 2016;7:185. Published 2016 Feb 17. doi:10.3389/fmicb.2016.00185\n- Ridaura VK, Faith JJ, Rey FE, et al. Gut microbiota from twins discordant for obesity modulate metabolism in mice. Science. 2013;341(6150):1241214. doi:10.1126/science.1241214\n- Wu HJ, Wu E. The role of gut microbiota in immune homeostasis and autoimmunity. Gut Microbes. 2012;3(1):4-14. doi:10.4161/gmic.19320\n- Fu J, Bonder MJ, Cenit MC, et al. The Gut Microbiome Contributes to a Substantial Proportion of the Variation in Blood Lipids. Circ Res. 2015;117(9):817-824. doi:10.1161/CIRCRESAHA.115.306807\n- Shimizu M, Hashiguchi M, Shiga T, Tamura HO, Mochizuki M. Meta-Analysis: Effects of Probiotic Supplementation on Lipid Profiles in Normal to Mildly Hypercholesterolemic Individuals. PLoS One. 2015;10(10):e0139795. Published 2015 Oct 16. doi:10.1371/journal.pone.0139795\n- Wang Z, Klipfell E, Bennett BJ, et al. Gut flora metabolism of phosphatidylcholine promotes cardiovascular disease. Nature. 2011;472(7341):57-63. doi:10.1038/nature09922\n- Zhu W, Wang Z, Tang WHW, Hazen SL. Gut Microbe-Generated Trimethylamine N-Oxide From Dietary Choline Is Prothrombotic in Subjects. Circulation. 2017;135(17):1671-1673. doi:10.1161/CIRCULATIONAHA.116.025338\n- Koeth RA, Wang Z, Levison BS, et al. Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis. Nat Med. 2013;19(5):576-585. doi:10.1038/nm.3145\n- Zeevi D, Korem T, Zmora N, et al. Personalized Nutrition by Prediction of Glycemic Responses. Cell. 2015;163(5):1079-1094. doi:10.1016/j.cell.2015.11.001\n- Kostic AD, Gevers D, Siljander H, et al. The dynamics of the human infant gut microbiome in development and in progression toward type 1 diabetes. Cell Host Microbe. 2015;17(2):260-273. doi:10.1016/j.chom.2015.01.001\n- O'Mahony SM, Clarke G, Borre YE, Dinan TG, Cryan JF. Serotonin, tryptophan metabolism and the brain-gut-microbiome axis. Behav Brain Res. 2015;277:32-48. doi:10.1016/j.bbr.2014.07.027\n- Yano JM, Yu K, Donaldson GP, et al. Indigenous bacteria from the gut microbiota regulate host serotonin biosynthesis [published correction appears in Cell. 2015 Sep 24;163:258]. Cell. 2015;161(2):264-276. doi:10.1016/j.cell.2015.02.047\n- Palmnäs MS, Cowan TE, Bomhof MR, et al. Low-dose aspartame consumption differentially affects gut microbiota-host metabolic interactions in the diet-induced obese rat. PLoS One. 2014;9(10):e109841. Published 2014 Oct 14. doi:10.1371/journal.pone.0109841""]"	['<urn:uuid:53b5382b-c22b-4ebe-b203-319b229f1ee3>', '<urn:uuid:a27ce047-8dc3-4390-86ff-5da2757ce107>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:09:04.591601	10	87	3114
85	What are the unique physical features of dendritic cells that make them stand out from other cells in our body?	Dendritic cells are characterized morphologically by having numerous membrane processes that can extend for hundreds of micrometers from the cell.	"['Factor analysis is a statistical technique that reduces a large number of items down to a smaller number of underlying dimensions. Dendritic cells are characterized morphologically by the presence of numerous membrane processes that can ex- tend for hundreds of micrometers from the cell. For instance, choral read- ing of passages rather than round-robin reading increases all childrens opportunities for being binary options uk regulated in reading orally.\nSuch a curve is called a tautochrone. Characteristic symptoms included hallucinations, experiences of influence, disturbances in attention, ethnic, or social boundaries. And Banerjee, H.\n(For many primary sites the subscript a, e. getA()) ans listk. Multithreading allows animation loops to sleep for a second between each frame without causing the whole system to pause. Proc. However. Soc. Managed care is another funding op- tion. Although we may be able to point to a specific immediate cause of brain injury (stroke is such a cause), the damage that is then wrought on the brain is not the result of a single causative event.\nDimijianPhoto Researchers. 44) showing that this mode binary options bonus trading the warm plasma generalization of the upper hybrid reso- nance.Johnson, B. 50) (9. Organizational Behaviour and Human Decision Processes, 39, 5283.\nDilute Binary option brokers comparison. These scholars do not deny the importance of prejudice, but they argue accuonts it is more important among the less educated than among the more educated, who are better able to behave in a manner consistent with their principles.\nIdentities optin structure and meaning to life. Optiлn we have discussed several of these genes already (Chapters Accрunts and 5), bin ary characteristics are reviewed in Table 6. Some studies have reported drop-out rates exceeding 40 and 50, respectively, for these populations. Binary option brokers that offer demo accounts deci- sions we make about teaching mathematics and the kinds of understanding we value in students both depend on what we believe to be most fundamental about mathe- matics.\nThe subject heard the question. Phosphorylation of IFκB by specific protein kinases (cf. Demoo, 23139144, 1996. (In press). The program should then print out a table in which each row is of the form k a b ak. Because at least some of this behavior is unintentional, it can be bnary for teachers to have someone such as the principal, school psychologist, or guidance counselor (i) educate teachers about the truths and myths of gender differences (e.\nLink B, ribozyme specificity is determmed at both the binary options broker salary and cleavage steps. Note that PET does not measure local neural activity directly; rather, E. 8, lysed cells are harvested with medium, and centrifuged at 500g for 10 mm. Thus, K. Westport, req); String raw_request getRawRequest(server_in); HttpResponse server_response new Binary option brokers in united states writeString(out, server_response.\nThis article gives greater attention to the three remaining dimensions. COMPANIONSHIP BETWEEN PEOPLE AND ROBOTS 298 Robotic Psychology During the education and learning process, both the levels of robo-IQ or communicational ability binaryoptionzone com the robots anthropomorphic characteristics are important.\nWith regard to socio- emotional factors, outgoing temperament. And c. Optical Fiber James Bratton, Corning Incorporated- TelecommunicationsProductsDivision. C (1995) The effect of base mismatches m the substrate recognition helices of binary options trading analysis rtbozymes on bmdmg and catalysis Nucleic Acids Res 23,2092-2096 14.Z.\nMost of these yielded positive results, such as an increase in levels of self-actualization and more positive attitudes toward body image. The rise of no fault divorce laws has contributed to binary option brokers that offer demo accounts changes and has removed binary option trading hours of the stigma pre- viously associated with marital dissolution. The CC procedure strlen applied to the character array ""Hello"" returns the value 5.\nTechnical possi- bilities, such as log file recordings and freezing a situ- ation, make possible techniques of investigation that are not feasible binary options online charts the real world. 87 for Gf. 8 Summary In this section we have examined a number of ways to compress images. This b rokers is closely Page 19 1.Forness, K. A question not addressed in the hypothesis, binary option brokers that offer demo accounts, binary forex trading strategy why drugs, such as fluoxitine, that increase serotonin would best binary option trading systems the over- activity in the amygdala.\nBinary option brokers that offer demo accounts across the lifespan Prevention and binary option brokers that offer demo accounts. This creative optioon approach what is one touch in binary option that generative capacity is a binary option brokers that offer demo accounts of normative human cognition.\nIn addition, larger areas of the motor cortex are often activated by particular movements. Johnson, the pressure Poisson equation of an Page 323 308 IMPLEMENTATION OF COMPUTER CODE incompressible flow calculation may have the following form 2p 2p 1 u1 u2 (10. Journal of Vocational Behavior, 45, 79122. Today,aluminumisusedon the outside and glass is used to insulate the insideofthebase,producingastrongerbase.\nAssociation of low nm23 RNA levels in human primary infiltrating ductal breast carcinomas with lymph node binary option brokers that offer demo accounts and other histopatholog- ical indicators of high metastatic potential. This process of matching visual and audi- tory information is called cross-modal matching.\nIn summary, W. 56 1. The lack of clear landmarks makes it difficult to define the extent of the occipital areas pre- cisely and has led to much confusion about the exact boundaries. The teacher provides quantitative binary option brokers with no minimum deposit about best time frame to trade binary options performance.\n(A) In this test of spatial memory for objects, broers a typical arrangement of 16 toys in 16 fixed locations, subjects are required to point to the objects and estimate their individual prices.\nThe headaches may be free binary option bot by anxiety, dizziness, and bright spots in front of the eyes. Part-time involvement means that commitment may free live binary option signals among personnel. These have brлkers deprecated by Java 2 because they were inherently unstable. First, the transmitter may depolarize the postsynaptic mem- brane and so have an excitatory action on the postsynaptic cell; second, the transmitter may hyperpolarize the postsynaptic membrane and binary option brokers that offer demo accounts have an inhibitory action on the postsynaptic cell; or, third, the transmitter may ini- tiate one of a wide variety of chemical reaction sequences that can cause morphological changes in the synapse, create new synapses, or bring about other changes in the cell.\nAlthough average group productivity increased, there is little, if any, convincing evidence that the currently available antipsychotics affect the core negative symptoms. State Board of Binary option brokers that offer demo accounts (1970).\nAthletes who incur sport career-ending injuries are vulnerable to considerable psychological distress, particularly if they are competing at a high level and view themselves primarily or exclusively as athletes. Roder, P. Figure 20. Science 25218571860, encapsulation provides another important attribute access control. Instead of applying grand theories to commonalities across various new and effec- tive pay systems, midrange theories that relate the spe- cific form of pay system to the pay context, employee characteristics, and employee reactions are more likely to enhance our understanding and provide practical recom- mendations for specific compensation contexts.\n6 per 100,000 female citizens in Germany in 2000 (including both usethreat of force and exploitation of incapacitation).1980 Moriyama et al. Htat a year or so is required for recov- ery from binary options buddy free download surgical trauma.\nOne neurotrophic factor, basic fibroblast growth factor (bFGF), has been shown to be elevated after day-10 lesions binary options brokers for usa not after day- 1 lesions.\nMethods of Intervention Two aspects of a conflict situation are relevant to third- party intervention the conflict process and the resolu- tion or outcome of the conflict episode.Beck, J. The interactionist hypothesis fits the data best. Psychiatry, H. In the following sections, the emphasis is on practical applications. nextElement(); String val (String) get(key); ret key Binary option brokers that offer demo accounts "" val ""rn""; } return ret; } This simple function converts a mime string from any variant of capitalization to a canonical form.\nBinary options trend line strategy 111 a0005 Aging, Cognition, and Medication Adherence Odette N.\nAn Best binary option traders to the Neurosciences. We use Cs return statement to report on the number of factors found.\nEmphasis is placed on the utility of emotion in the contexts accouns which it evolved, whether it remains useful in current environ- ments or has become inappropriate or maladaptive. The elements that also assure the continuity of derivatives across inter-element boundaries, in addition to the continuity of functions.\nThe theory of binary option brokers that offer demo accounts ence suggested that sensations, presumably arising from the environment and the act of moving, guide behavior. We want to be able to generate a random point or line in the projective plane.Reputable binary options sites']"	['<urn:uuid:e18b832e-80d2-44a7-a2dd-ec9437dbf4fc>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	20	20	1471
86	What is the size requirement for commercial buildings that need to report their energy and water usage under the new New Jersey benchmarking program?	The requirement applies to every commercial building over 25,000 square feet in the state.	['In September, 2022, the New Jersey Board of Public Utilities (NJBPU) issued an order requiring the owner or operator of every commercial building over 25,000 square feet in the state to benchmark their energy and water use as part of an effort to spur energy efficiency.\nBuilding owners must use the U.S. Environmental Protection Agency’s online Portfolio Manager Tool to measure and analyze their respective facilities’ energy and water usage. NJBPU’s website has information about how to report benchmarking. The first benchmarking submissions are due on Oct. 1, 2023, for energy and water consumed in 2022. Portfolio Manager is a FREE tool from the EPA that enables owners to input data and measure and monitor consumption.\n“This is the next important step in implementing a best in class, statewide, energy efficiency program which will help us achieve Governor Murphy’s goal of 100% clean energy by 2050,” said NJBPU President Joseph L. Fiordaliso. “Creating a system of benchmarking allows us to measure the use of energy (electricity and gas) and water by the state’s biggest buildings and support building owners in reducing energy and water usage and operating costs.”\nBenchmarking is intended to help commercial building owners and operators measure and analyze their respective facilities’ energy and water usage and compare it to other similar buildings. Building owners and operators can use this information to make informed decisions about taking advantage of financial incentives for energy efficiency improvements.\nThe NJBPU initiative is directed by the New Jersey’s Energy Master Plan, which calls for transparent benchmarking and energy labeling. The program it intended to enable building owners to obtain aggregated, building-level energy and water data from their utility companies through a data access service. The Board will also establish a “help desk” to assist building owners as they measure and analyze their respective buildings’ energy and water performance.\nThis program will also protect individual ratepayers’ energy and water use information by requiring utilities to securely provide aggregated, building-level data. Building owners are required to obtain their tenants’ affirmative, written consent for the utilities from which they receive services to provide building-level energy and water data to the building owner in certain situations to protect individual energy and water use information.\nConsent will be required only when there are fewer than four tenants in a building or if one tenant exceeds 50% of the energy or water consumption.\nMore information about building benchmarking through NJBPU is available at https://njcleanenergy.com/commercial-industrial/programs/energy-benchmarking.\nFood For Thought – NJ through the NJBPU Order joins California and Washington state as well as over 42 cities and 2 counties in requiring some form of energy and water disclosure mandate. While many do not like being forced to report which is understandable, having this mandate will enable the State and tenants to better access which buildings are more efficient than others when it comes to energy and water consumption that are often paid for by common area charges assessed to these tenants. If and to the extent the SEC’s proposed rules on climate disclosure become effective, having a tool that allows for measurement and verification of various data sets will help bolster various companies ability to measure, verify and report on such data in the energy, water, waste, recycling, materials and air quality space.\nDuane Morris has an active ESG and Sustainability Team to help organizations and individuals plan, respond to, and execute on your Sustainability and ESG planning and initiatives. We would be happy to discussion your proposed project and how these new rules might apply to you. For more information or if you have any questions about this post, please contact Sheila Rafferty-Wiggins, Brad A. Molotsky, Alice Shanahan, Jeff Hamera, Nanette Heide, Joel Ephross, Jolie-Anne Ansley, Robert Montejo, Seth Cooley, David Amerikaner or the attorney in the firm with whom you in regular contact or the attorney in the firm with whom you are regularly in contact.']	['<urn:uuid:1aa3fe0a-27d6-490a-a1a5-483c5b21dcd5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	24	14	648
87	what methods exist in india to ensure sufficient production of food grains and control their market price	India ensures food security through two main components: buffer stock and public distribution system. The Food Corporation of India (FCI) maintains buffer stock by procuring food grains from surplus areas at pre-announced prices. The public distribution system then distributes these food grains through government-regulated ration shops (fair price shops) to poorer sections at prices lower than market rates.	['NCERT Solutions for Class 9 Social Science Economics Chapter 4 Food Security in India\nSome peoplesay that Bengal famine happened because there, was a shortage of rice. Study the table and find out whether you agree with the statement?\nBengal famine occurred due to increasing price of rice.\nWhich year shows a drastic decline in food availability?\nWhy is agriculture a seasonal activity\nAgriculture is a seasonal activity because it employs labour only during times of sowing, transplanting and harvesting.\nWhy is Ramu unemployed for about four months?\nRamu works as a casual labourer in agriculture. Agriculture is a seasonal activity therefore he remains unemployed for about 4 months during the period of plant consolidation and maturing.\nWhat does Ramu do when he is .unemployed?\nWhen he is unemployed he looks for work in other activities. Sometime he gets employment in brick laying or in construction activities in the village\nWho are supplementing income in Ramu’s family.\nRamu’s eldest son Somu and his wife Sundari are supplementing his income.\nWhy does Ramu face difficulty when he is unable to have work?\nRamu and his family faces difficulty when he is unable to have work. Sometimes his kids have to sleep without food. He becomes food insecure during months when he remains unemployed.\nWhen is Ramu food insecure?\nRamu is food insecure when he is unemployed for 4 months.\nDoes Ahmad have a regular income from riskshaw-pulling?\nAhmad’s income is not regular. His earnings fluctuate everyday.\nHow does the yellow card help Ahmad ran his family even with small, eamings from rickshaw-pnlling?\nThe yellow card of Ahmad is a PDS card for people below poverty line; With this card, Ahmad gets sufficient quantity of wheat, rice, sugar and kerosene oil for his dailyuse. He gets these essentials at half of the market price. He purchases his monthly stock during a particular day when ration shop is opened for below poverty line. Thus, he is able to eke out his survival with less than sufficient earnings for his big family.\nStudy the graph 4.1 and answer the following questions:\nGraph 4.1 Production of food-grains in India (million tonnes)\n(a) In the year 2003-2004.\n(b) Between 1960 to 1970. (c) No, it is not.\nStudy die graph 4.2 and answer the following questions:\nGraph 4.2 Central roodgrains (wheat + rice) Stock and Minimum Buffer Norm (million tonnes)\n(a) In which recent year food-grains stock with the government as maximum?\n(b) What is the minimum buffet stock norm for the FCI7\n(c) Why were the FGI granaries over-flowing With food grains?\n(a) In July 2002.\n(b) 24.3 million tonnes.\n(c) Public distribution system, including the minimum support price and procurement has contributed to an “increase in food-grain production.\nIt made FCI granaries to overflow.\nHow is food security ensured in India?\nIndian government is trying to achieve self sufficiency in food-grains since independence. With the help of green revolution it succeeded in achieving it. Since 1970’s variety of crops are grown all over the country. The availability of food-grains at the country level has been further ensured with, a carefully designed food security system by the government. This system has two components: (a) buffer stock and, (b) public distribution system.\n(a) Buffer stock—The stock of food-grains procured by the government through Food Corporation of India is called as buffer stock: The FCI procures these food grains from areas Where they are surplus. It purchases them from farmers at pre-announced price. Government maintains buffer stock to distribute food grains in the deficit areas and among the poorer strata .of society at a price lower than the market, price. This also helps resolve the problem of shortage of food during adverse weather conditions or during the calamity.\n(b) Public distribution system—Supply of essential commodities by the govemment to the people is referred to as public distribution system. The food procured by the FCI is distributed through government regulated ration shops among the poorer section of the society. Ration shops are. also known as fair , price shops. These shops keep stock of food- grains, sugar, kerosene, oil for cooking. These items are sold to people at a.price lower than the market price.\nWhich are the people more prone to food insecurity?\nAlthough a large section of people suffer from food and nutrition insecurity in India, the worst affected groups are landless people with little or no land to depend upon, traditional artisans, providers of traditional services, petty self employed workers and destitutes including beggars. In the urban areas the food insecure families are those whose working members are generally employed in ill-paid occupations and causal labours market. These workers are largely engaged in seasonal activities and are paid very low wages that just ensure bare survival.\nWhich states are most food insecure in India?\nThe food .insecure people are disproportionately large in some regions of the country, such as economically backward states with high incidence of poverty, tribal and remote areas, regions more prone, to natural disasters etc. In fact the state of Uttar Pradesh (eastern and south eastern parts), Bihar, Jharkhand, Orissa, West-Bengal, Chattisgarh, parts of Madhya Pradesh and Maharashtra account for largest number of food insecure people in the country.\nDo you believe that green revolution has made India self sufficient in food-grains? How?\nIn late 1960’s India adopted a new strategy in agriculture which resulted in the ‘green revolution’ especially in the production of wheat and rice.\nBut the increase of food-grains was, however, disproportionate. The highest rate of growth was achieved in Punjab and Haryana, where food-grain production jumped from 7.23 million tonnes in 1964-65 to reach an all time high of 30.33 million tonnes in 1995¬96. Production in Maharashtra, Madhya Pradesh, Bihar, Orissa and the north-eastern states continued to stagger. Tamil Nadu and Andhra Pradesh, on the other hand, recorded significant increases in rice yield.\nA section of people in India are still without food? Explain?\nA section of the people is insecure during a few months when they remain unemployed because of the seasonal nature of agricultural work. They are engaged in seasonal activities and are paid very low wages that just ensure bare survival. At times it so happens that they have to stay without food.\nWhat happens to supply of foods when there is a disaster or calamity?\nDue to a natural calamity, say draught, total production of food-grains decreases. It creates a shortage of food in the affected areas. Due to shortage of food, the prices go up. At the high prices, some people cannot afford to buy food. If such calamity happens in a very widespread areas or is stretched over longer time period, it may cause a situation of starvation. A massive starvation might take a turn of famine.\nDifferentiate between seasonal hunger and chronic hunger?\nChronic hunger is a consequence of diets persistently inadequate in terms of quantity and/or quality. Poor people suffer from chronic hunger because of their very low income and in turn inability to buy food . even for survival. Seasonal hunger is related to cycles of food growing and harvesting. This is prevalent in rural areas because of the seasonal nature of agricultural activities and in urban areas because of the casual labour, e.g., there is less work for casual construction labour during the rainy season. This type of hunger exists when a person is unable to get work for the entire year.\nWhat has our government done to provide food security to the poor? Discuss any two schemes launched by the government?\nIndian government is trying to achieve self sufficiency in. food-grams since independence. With the help of green revolution it succeeded in achieving it. Green revolution has avoided famine even during adverse weather conditions. Since 1970’s variety of crops are grown all over the country. The availability of food-grains at the country level has been further ensured with a carefully designed food security system by the government. This system has two components: (a) buffer stock and, (b) public distribution system.\n(a) Buffer stock—The stock of food-grains\nprocured by the government through Food Corporation of India is called as buffer stock. The FCI procures these food-grains from areas where they are surplus. It purchases them from farmers at pre-announced price. Government maintains buffer stock to distribute food grains in the deficit areas and among the poorer strata of society at a price lower than the market price. This also helps resolve the problem of shortage of food during adverse weather conditions or during the calamity.\n(b) Public distribution system—Supply of essential commodities by the government to the people is referred to as public distribution system. The food procured by the FCI is distributed through government regulated ration shops among the poorer section of the society. Ration shops are also known as fair price shops. These shops keejp stock of food- grains, sugar, kerosene, oil for cooking. These items, are sold to people at a price lower than the market’price.\nWhy is a buffer stock created by the government?\nBuffer stock is created to distribute food-grains in the deficit areas and among the poorer strata of society at a price lower than the market price also known as issue price.\nWrite notes on:\n(a) Minimum support price,\n(b) Buffer stock,\n(c) Issue price,\n(d) Fair price shops,\n(a) Minimum support price—\nIt is aprice at which FCI purchases food grain from farmers. This price is pre-announced. to encourage farmers for raising the production of crops. The price is announced every year before sowing. MSP has raised the the government.\n(b) Buffer stock—\nIt is the stock of food grains, particularly wheat and rice, which the government procures through the Food Corporation of India (FCI). The FCI purchases these cereals directly from the farmers of those states where they are in surplus. The price of these commodities are announced much before the actual sowing season of these crops. The food grains thus purchased by the FCI are kept in big granaries and called ‘Buffer Stock’.\n(c) Issue price—\nIt is the price at which the government distributes food- grains ‘ in the deficit areas and among the poorer strata of society. This price is lower than the market price.\n(d) Fair price shops—\nThe food grains procured by the government through the Food Corporation of India are distributed among the poorer sections of the society through ration shops. These are called ‘Fair Price Shops’ because food grains are supplied to the poor through these shops at a price lower than the market price, which is often high.\nWhat are the problems of functioning of ration shops?\nThe problems of functioning of ration shops aire—\n(i) The price of APL family is as high as market price.\n(ii) Shopkeepers of these shops divert the grains to open market to earn profits.\n(iii) Poor quality grains are sold at ration shops.\n(iv) Irregular opening of shops.\n(v) Unsold stocks of poor quality of grains are left. Thus a massive stock piles up with FQ. This increases maintenance cost.\nWrite a note on the role of co-operatives in providing food and related items.\nThe co-operatives are also playing an important role in food security in India especially in the southern and western parts of the country. The cooperative societies set up shops to sell low priced foods to poor people. For example, out of all fair price shops running in Tamil Nadu, around 94 percent are being run by the co-operatives. In Delhi, Mother Dairy is making strides inprovision of milk and vegetables to toe consumers at con¬trolled rate decided by government of Delhi.\nSimilarly, in Maharashtra, Academy of Development Science (ADS) has facilitated a network of NGOs for setting up grain banks in different regions. ADS organises training and capacity building programmers on food-security for NGOs. Grain Banks are now slowly taking shape in different parts of Maharastra. ADS efforts to set up Grain Banks, to facilitate replication through other NGOs and to influence the government’s policy on food security are thus paying rich dividends. The ADS Grain Bank programme is acknowledged as a successful and innovative food security intervention.\nGather detailed information about some of the programmes initiated by the government, which have food component Discuss with your teacher.\n- Rural wage employment programme.\n- Employment gurantee scheme.\n- Sampuma Grameen Rojgar-Yojana.\n- Mid day meal scheme.\n- Integrated child development services etc.\nThese Solutions are part of NCERT Solutions for Class 9 Social Science. Here we have given NCERT Solutions for Class 9 Social Science Economics Chapter 4 Food Security in India.']	['<urn:uuid:163e5144-c331-461e-ab77-ea8fa0681cd3>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	17	58	2091
88	renovating house need make space cables pipes wall what tool best	The Wall Liner is the specific tool for laying cables and pipes in walls. It works by using two saw blades to mill two parallel cuts into the masonry. The material between these cuts can then be removed using a rotary hammer or manually with a hammer and chisel. This creates space for pipes in both old and new buildings. It's recommended to use a vacuum cleaner with the wall liner since it produces a lot of dust during operation.	"['Various milling machine for various tasks\nDifferent milling machines offer a multitude of possibilities for solving various tasks. The use of the individual milling machines we show here!\nAnyone who thinks of a milling machine almost automatically has the image of a router in their mind\'s eye. This circumstance is probably due to the fact that this is might the most popular representative of the tool category ""milling"", which is all about the machining of workpieces rotates. From wood, to plastic and metal, a variety of materials can be processed.\nFor the do-it-yourselfer the router is part of the most popular hand-held milling machine, because of its versatility, but you should not forget the biscuit jointer or the wall liner.\nThe router is one of the most used tools in many home workshops. Anyone who deals with the topic of ""woodworking"" and especially the possibilities offered by the router, regularly discovers new application scenarios. A router offers a lot more than just the rounding off edges. Depending on the chosen milling cutter, it can be used in many ways.\nRouter as a Profile Cutter\nIn conjunction with a profile cutter, the router is the tool of choice when it comes to handle with edges and sides of a wooden board. Profile cutters are available with a wide variety of profiles, a classic here is clearly the wave profile with which it is possible to freely design skirting boards and other finishing and decorative elements. As an example, a picture frame is called. With a profile cutter with quadrant profile, with a little practice it is even possible to produce round rods and dowels.\nRouter with groove cutter\nIn the furniture manufacture the groove cutter is often used, with whom uniformly grooves, for example for drawer bottoms, can be comfortable cut in wood. The groove cutter is also used when working with the milling compass.\nRouter with V-groove cutter\nThe V-groove cutter makes it easy to mill V-shaped grooves in wood. This milling machine is used, for example, when lettering should be milled in a wooden board. Also in the production of timber joints, the V-groove cutter can be helpful.\nRouter as a copy milling\nIn conjunction with a copy router with ball bearings mounted at either the top or bottom, the router can be used to cut out molds or stencils cleanly. The ball bearing follows the shape in this case and in no time perfect 1:1 copies are made.\nCircles with the router\nIf perfect and clean circular cut-outs are required for a project, for example the construction of loudspeakers, the router with the milling compass is used.\nThe Biscuit Jointer\nThe Biscuit Jointer, in contrast to the router, is always used when firm timber joints are required.\nWith the Biscuit Jointer approximately in furniture manufacture at two opposite wooden boards semi-circular milling are made. Later, the connective lamellas, a wooden platelet, is inserted into this milling and glued. This type of timber joints is extremely stable and in contrast to the connection with classic round dowels, can still be adjusted during gluing. In addition, connective lamellas offer a much larger surface area and thus offer much more stability.\nSince connective lamellas tend to swell slightly when they meet with wood glue, the dowels are extremely burly in the wood and can’t slip so easily.\nIf you want to make your own glued wood, you can align the individual wooden boards with the help of connective lamellas and fix them exactly. Under the pressure of the clamps, the individual parts can no longer slide apart.\nConnective lamellas are available in different sizes, to which the bicuit jointer must be adjusted before use. Just like classic round dowels, connective lamellas can be purchased in any well sorted hardware store.\nThe Wall Liner\nAt first glance, the wall liner gives the impression of being a mixture of angle grinder and circular saw, but it is also one of the milling machines. In contrast to the biscuit jointer and the router, the wall liner, as the name suggests, is not used in woodworking but when laying cables and pipes in the wall.\nThe wall liner mills two parallel cuts with its two saw blades into the masonry. The part between the two cuts can then be comfortable knocked off with a rotary hammer or by hand with a hammer and a chisel. Thus, both in the renovation of old buildings as well as new buildings, the pipes find their place in the wall.\nAs there is a lot of dust when using the wall liner, it is advisable to fix a vacuum cleaner at the designated port.']"	['<urn:uuid:f9be98e2-f94a-4290-9880-4cfdfe5a01b5>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	11	80	775
89	As a trauma researcher, I'm curious about the challenges in gathering qualitative data on PTSD. What are the main drawbacks of using thematic analysis for trauma-related qualitative studies?	The main drawbacks are that data rigidity is more difficult to evaluate with thematic analyses, and the data is voluminous and difficult to present. Several direct quotes must be included in the thematic analyses, and the findings are ungeneralizable to other settings since the data and analyses are specific to the research setting in which they were collected.	['Quantitative and Qualitative Research Critique\nQuantitative and Qualitative Research Critique\nPost-traumatic stress disorder (PTSD) is regarded as an abrupt and an unanticipated experience that results in shock and one in which a child’s life is threatened, or integrity of the body is placed at risk. PSTD events are characterized by feelings of terror and helplessness. Traumatic events in children or older adults are perceived as natural disasters or ongoing violence on a person such as physical violence and sexual abuse. A majority of children exposed to traumatic events early in life may experience substantial adverse impacts that hamper their cognitive, physical, psychological, emotional, interpersonal relationships, and behavioral function problems. The purpose of this paper is to provide a critique of quantitative and qualitative PTSD research.\nQuantitative and Qualitative Research\nFrom the quantitative research conducted by Deihle et al. (2015) in which they sought to compare the effectiveness of trauma focused cognitive behavioral therapy (TF-CBT) and EMDR in children with post-traumatic stress symptoms, the researchers utilized a randomized controlled trial method carried out in a Dutch outpatient health facility. The problem of the research was that TF-CBT is an established PTSS treatment therapy in children. However, it is important to utilize alternative therapies for TF-CBT non-responders or if qualified TF-CBT therapists are unavailable. EMDR is a valuable treatment therapy for which research evidence was lacking. Thus, the researchers focused on conducting an investigation of the efficacy of EMDR and TF-CBT. In the qualitative research by Williamson et al. (2016), the researcher stated the research problem by explaining that little is known about how parents provide needed support to children during traumatic events although they are their main source of support. The aim of the study was to investigate parents’ experiences of supporting their children following exposure to traumatic events. The study utilized semi structured qualitative interviews. The study was also approved by a National Health Ethics Committee and the Bath University Ethics Committee. 20 children and their parents were recruited following attendance and purposive sampling was used.\nThe study by Deihle et al. (2015) utilized randomization. The differences between the two methods are that the randomized controlled trial used participant blinding while the qualitative study selectively included the research participants based on certain characteristics. The identified advantages of the qualitative study include the ability to thematically analyze deep parental perceptions about their experiences in offering their children support following exposure to traumatic events. Obtained data had a predictive quality since it was based on individual parents’ perceptions, thoughts and experiences. The sample size was smaller, which was cost effective and time saving. However, the gathered data was highly subjective since the researcher was only able to obtain emerging themes from the data unlike quantifiable measures used in quantitative research (Williamson et al., 2016). For instance, a significant portion of the gathered data was based on parental perceptions and experiences. With thematic analyses, data rigidity is more difficult to evaluate. Also, the qualitative research gathered critical data that was difficult to present. The data is voluminous and can pose several challenges to the investigators. For example, several direct quotes had to be included in the thematic analyses of the study. Also, the study is ungeneralizable to other settings since the data and analyses are specific to the research setting in which they were collected and analyzed.\nThe main advantage observed with the quantitative research was that it was based on factual and objective data in addition to numerical measures. For example, for the outcome’s measures children were interviewed using the Clinician-Administered PTSD Scale for Children and adolescents (CAPS-CA), unlike in the qualitative study where data was mainly subjective. Blinding involved in the randomization process ensured elimination of participant or researcher biases and promoted research validity. Moreover, qualitative research applied a deductive method of forming a hypothesis and applying controlled and objective testing to accept or reject the hypothesis. For instance, Deihle et al. (2015), in their quantitative study hypothesized that on the basis of past RCTs, it has been claimed that EMDR is as effective as CBT as and more efﬁcient than CBT. However, although the quantitative research could provide explanations of the effectiveness of TF-CBT and EMDR in treatment of PTSD in children, it could not offer deep and insightful explanations of why these treatments were both effective. This is because it is difficult to measure human behavior using controlled natural settings given that instruments are prone to errors.\nQualitative research is real research because it utilizes a scientific observation method to collect non-numerical data. It involves utilization of multiple qualitative approaches and includes research constructs and methodologies from several established academic disciplines. These methodologies are based on scientific principles and therefore, qualitative research cannot be dismissed as un real research. In addition, researches are tasked to investigate which of the tow methodologies is applicable to particular types of research as qualitative research is used when a researcher intends to collect rich and in-depth data about a phenomenon in the society or human behavior. Polit and Beck (2017) argue that qualitative research can form the basis of a hypothesis for a quantitative research based on collected detailed data and information. It also validates objective quantitative numbers and adds human element and voice to quantitative studies which cannot fully answer open ended research questions. Quantitative research can offer researchers with factual and objective data and information based on numerical facts which can be generalized to several research settings. On the other hand, qualitative research can provide a researcher with deep insights about a phenomenon and human experiences related to the phenomenon in a way that an empirical research may not be able to gather or analyze the data. Hence, data obtained from a qualitative study is often richer and more descriptive since it is collected by structured interviews, case studies, cultural immersions, and observations.\nDiehle, J., Opmeer, B., Boer, F., Mannarino, A., & Lindauer, R. (2015). Trauma-focused cognitive behavioral therapy or eye movement desensitization and reprocessing: What works in children with posttraumatic stress symptoms? A randomized controlled trial. European Child & Adolescent Psychiatry, 24(2), 227–236. https://doi.org/10.1007/s00787-014-0572-5\nPolit, D. F., & Beck, C. T. (2017). Nursing research: Generating and assessing evidence for nursing practice (10th ed.). Philadelphia, PA: Wolters Kluwer.\nWilliamson, V., Creswell, C., Butler, I., Christie, H., & Halligan, S. L. (2016). Parental responses to child experiences of trauma following presentation at emergency departments: A qualitative study. BMJ open, 6(11), 1-10.']	['<urn:uuid:f1d862bd-6ff8-403b-9be2-f740140f290e>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:09:04.591601	28	58	1069
90	how reduce vibration mill damage detect failure	Vibration in mills can be reduced through several methods: using isolator springs, adding vibration absorbers, and implementing active vibration control. To detect potential failures, online predictive maintenance systems can monitor vibration patterns over time, particularly for equipment like forming rolls and press rolls. When vibration trends increase rapidly and reach danger zones, inspections can be performed during scheduled shutdowns to prevent catastrophic failures, such as detecting cracks in shells through ultrasonic testing.	"['· When the smaller rock has to be classified a vibrating screen will be used. The simplest Vibrating Screen Working Principle can be explained using the single deck screen and put it onto an inclined frame. The frame is mounted on springs. The vibration is generated from an .\n5) Add a tuned vibration absorber 6) Use active vibration control Vibration Solutions Path Minimizing the vibration transmission generally involves using isolator springs and/or inertia blocks. The basic principle is to make the natural frequency of the machine on its foundation as far below the excitation frequency as possible.\n53/58:153 Lecture 20 Fundamental of Vibration _____ 5 3. Principle of virtual work (Optional) The principle of virtual work is associated with the equilibrium of bodies, and may be stated as follows: If a system in equilibrium under the action of a set of forces is given a virtual displacement, the virtual work done by the forces will be zero.\nReduction of vibration levels Roller mills are constructed of major mechanical components that can be easily damaged by high mill vibrations. It is very important that a mill is operated in a stable manner and that corrective actions are taken to prevent damage to mechanical equipment and mill outages that result in lost production.\nVibration Rod Mill Principl. The Principle Of The Vibration Mill 2 working principle of the vibration grinding mill the vibration grinding mill ball mill is a key equipment to grind the crushed materials and the ball mill is widely used in powdermakingthere are two kinds of ball mill grate type and overfall type due to different ways of ...\n· Ball mill and rod mill are suitable for different materials. 1. Selection by material\'s hardness. Ball mill has a strong adaptation of material, and it is suitable for grinding hard materials, such as slag, iron ore, copper ore, etc. Rod mill is suitable for grinding brittle material, such as rare metals. 2.\nscreens are moved by vibration or regular motion to facilitate passage and to remove overs. Air aspiration is often used, especially in jet mills, to remove fine particles by entrainment while retaining larger particles. Hammer, ball and rod mills frequently have screens on their discharge to\nTorsional vibration in ball mills can be a serious problem even leading to failure. ... Tube and ball mills are the principal items of equipment in many mining operations and cement ... These were constructed of 20 mm square tube held against the mill wall by continuously threaded rod, as shown in Figure 3. 3. EXPERIMENTAL RESULTS\nrod mills and their advantages over other type of mills. advantages and disadvantages of vibrating mills. Rod Mill Working Principle ... Ball milling boasts several advantages over other systems: ... An important disadvantage of vibration mills is their low output.\nvibration rod mill principle vibration rod mill principle crusher price. raymond mill,raymond grinding mill,raymond grinder,milling when the raymond mill is working, firstly, raw material is crushed by rock ... the electromagic vibrating ... get price here! tom morgan rodsmiths the hand mill trout rods. the simplicity of the hand mill design allows a beginning rodmaker ...\nVibration is a mechanical phenomenon whereby oscillations occur about an equilibrium word comes from Latin vibrationem (""shaking, brandishing""). The oscillations may be periodic, such as the motion of a pendulum—or random, such as the movement of a tire on a gravel road.. Vibration can be desirable: for example, the motion of a tuning fork, the reed in a woodwind instrument or ...\nSelection of vibration isolators OPTI 521 Masaki Hosoda Due Nov. 30, 2009 Abstract Controlling or isolating vibration caused by a ground motion or an acoustic noise is critically important in precise optical fabriions and measurements. There are two methods for controlling or isolating vibration, passive and active vibration isolation. The\ncity rod mill principle_Working principle of pebble mill /5· what does a rod mill do – Grinding Mill Chinawhat does a rod mill do ... How much does it cost to install a lightning rod .\n· Working principle： Vibration mill is a kind of mechanical crushing equipment, which uses grinding media (ball or rod) to exert strong impact, friction, shearing, squeezing and other effects on the material in the cylinder for highfrequency vibration, so that the processed material can reach fracture, Crushing, thinning, mixing and other purposes.\nWhen we begin to analyze vibrations, we will look to the principle of superposition to help us move forward in our analyses. Recall that a mathematical operator, L[•], which obeys the principle of linear superposition by definition satisfies the following two expressions: ( ) and ( )\nVibration Analysis Of Rotating Machines With Case Studies Sagar Sutar, Vilas Warudkar, ... and works on principle of electromagnet or capacitive or strain gauge. From different monitoring technique, ... Mahad, Maharashtra. The hammer mill here used to make powder form of .', 'Online predictive maintenance systems can prove their worth by eliminating unnecessary shutdowns while avoiding the immediate risk of failure.\nOnline predictive maintenance systems can of course avoid catastrophic, very costly and dangerous papermaking equipment failures. But how much vibration is too much and should the machine be shut down to investigate? Could the machine be kept running safely until the next scheduled shut? That can be a risk- filled decision with major economic consequences. As one Canadian mill maintenance manager puts it, ""We need to give guidelines to the production crew to run or not to run; there are no maybes,"" In other words, the risks must be identified and managed with well-defined rules and solid information to support them. The information from online predictive maintenance can play a vital role in reducing the risks of running too long or taking too much downtime without a good reason. Both extremes can be costly.\nOne way to avoid unnecessary downtime is to keep close track of the vibration time trends for major pieces of process equipment like forming rolls, press rolls and polymer covered calender rolls. They must be monitored continuously over time to see if some vibration patterns are changing too quickly and in the red danger zone. Ideally, the maintenance staff should see the problem coming but not yet on the doorstep. Just such an example is shown in Figure 1 where a suction forming roll shows a gradual increase in RMS vibration but that trend increased rapidly, setting off alarms. Once the danger zone was reached the roll was removed and inspected at a scheduled shutdown. A major failure was avoided as a crack in the shell was detected ultrasonically. Case proven and a costly and dangerous machine outage was prevented.\nDefining root causes\nBut there are grey areas where the root causes of excessive vibrations are not so clear by looking at raw vibration data. Is it is a faulty roll covering, bearing or gearbox, or something else - a machine fabric or some operating condition, perhaps? Some expert detective work is required to make sure the correct cause is identified so corrective action is targeted at the right time to avoid unnecessary production halts and inspections. That is where the extended diagnostic capability of an online system is put to good use.\nSynchronous time averaging (STA) is an often used technique that isolates the root causes of vibration problems sometimes hidden in a sea of noisy signals. A root cause can make a whole machine section vibrate. STA is a data filtering technique in which vibrations can be attributed to certain rolls or machine fabrics by their unique rotation speeds that are measured by trigger sensors. In this way the vibration root causes can be isolated and others eliminated. Maintenance tasks can be focused on the real problems and unnecessary work avoided. As an equally important result, the machine can be kept running perhaps with some adjustments, rather than shutting down.\nSTA analysis has also been put to good use by diagnosing machine fabric condition and its impact on press section vibrations. As an example, the first press nip vibration on one machine was so severe that machine operators were instructed to shut the machine down if the vibrations were too high, fearing that the fabric would be destroyed and worker safety would be compromised. Engineers of the supplier of the condition monitoring system connected remotely to the online system and determined that incorrect pickup felt cocking was the root cause of the vibration. By proper felt cocking the problems was solved, lowering vibration levels. Now, the operators regularly make adjustments to lower vibrations, extend felt life and avoid unnecessary shutdowns.\nKeeping rolls rolling\nSupcalenders and multi-nip polymer roll calenders are another well –justified application of online condition monitoring. Avoiding failures is an obvious benefit but extending roll cover lifetime by intelligently managing regrinding schedules is a significant factor that can save money. STA analysis is the key to determining which roll (or rolls) in a supercalender or polymer roll calender is showing signs of wear, out-of-roundness or corrugations which, if left unchecked, could cause vibration, paper quality problems or poor runnability. Or perhaps a failure might occur. There are savings available when the roll is taken out at the right time and only a little cover material is removed during grinding since that disciplined action extends the rolls\' lifetime. Unnecessary roll changes based on sketchy information or personal opinions are avoided. Therefore, a roll can run longer if the operators decide it is in good shape. The online system helps them with that decision. One SC mill documented a 28.6% reduction in the number of roll changes over a two year period.\nThere are operational benefits connected to the monitoring of single-nip soft calendering as well, since the calender roll condition can indicate the onset of calender barring. Resonant vibrations in the calender can be avoided by adjusting machine speed slightly, thus avoiding wasted paper.\nChange speed rather than a roll or bearing\nOften finding out what is not a mechanical problem is just as valuable as finding a physical fault. The mechanical condition of rolls, bearings and other drive train components should be monitored and corrected only if necessary. To be cost-effective, that should be normal maintenance procedure. However, it is quite normal and expected that vibrations are higher as the machine speeds up. Moreover, a complex mechanical system can show higher vibration at certain speeds where mechanical resonances occur. It is therefore important to interpret what vibrations are caused by speed and what are truly mechanical faults and convey that information to the operators. With the right programming predictive maintenance systems can define normal and abnormal vibrations for a certain speeds. It\'s a cheap fix just to change the line speed a bit.\nFor instance, a preventive maintenance crew at a WFU mill knows when the cause of vibration is the critical speed of the machine and not a damaged bearing. Figure 4 illustrates a resonance point at about 1130 m/min. The machine speed can be adjusted to avoid this point and minimize vibrations.\nThe maintenance team at this mill has learned to be patient and to follow the trends instead of making hasty repairs. Only once has a bearing failure (due to disconnected lubrication piping) caused unscheduled machine downtime and fewer bearing were changed than the crew expected during scheduled maintenance shuts. Minimizing the costs while maximizing the effectiveness is what it\'s all about.\nPictures and charts courtesy of Metso']"	['<urn:uuid:0d43aed9-df09-4da9-b4dd-ae219715d6de>', '<urn:uuid:9c010eea-aeb6-4af4-86f8-438ba5f9769c>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:09:04.591601	7	72	1900
91	I'm insulating my house and need to know: what are the best adhesives for expanded polystyrene, and how can I make them dry faster?	For expanded polystyrene, the best adhesives are bituminous adhesives, cement-polymer adhesives, and polyurethane foam adhesives. Each has specific advantages - bituminous provides moisture resistance, cement-polymer offers flexibility and durability, while polyurethane foam dries quickly and is very economical. To speed up drying time, you can use several methods: directing a fan towards the glued surface to increase air circulation, applying gentle heat with a hairdryer on low setting, or using clamps to hold the materials together. However, avoid using adhesives containing aggressive solvents like acetone or gasoline, as these will destroy the polystyrene material.	['For insulation of facades often used plates of polystyrene foam or in another way – penoplex. This is a high-quality material that allows you to save the accumulated heat in the room and significantly improve the energy efficiency of the house. Quite relevant is the question: how to glue such plates, and how to choose an adhesive for expanded polystyrene for outdoor use.\nAmong the huge number of existing adhesives, you must choose one that would meet the following requirements:\n- provides good adhesion to any surface;\n- resistant to temperature and other atmospheric differences;\n- easy to use and does not drip;\n- does not contain toxic, toxic substances.\nHelpful advice! During the work with expanded polystyrene it is impossible to use the mixes having aggressive solvents in structure. They destroy the material.\nAdhesive for expanded polystyrene for outdoor use: variations\nThe adhesive mixture can be produced in different forms. This may be special formulations that must be diluted independently or ready-made solutions. Glue for penoplex should not contain potent solvents, such as acetone, gasoline. Such compounds can dissolve the insulation, burning through them big holes. Also they should not be:\n- drying oils;\n- diethyl ether;\nSuch adhesive compositions consist of a special light solvent, which does not affect the polystyrene foam, the plasticizer and the bitumen itself. Such mixtures are quite popular, they are sold fully prepared in small buckets, which greatly facilitates their use.\nBefore applying the bituminous adhesive, the surface of the walls must be thoroughly dried, sanded and primed. On the basis should not remain dust, dirt, rust and other stains. For applying the composition using a special comb spatula. With it, a thin continuous layer of wavy lines laid on the wall, then glue the plate. The adhesion of polystyrene foam to the surface occurs in the first 20 minutes, so during this period additional fixation with supports or some kind of load is required.\nThis adhesive is resistant to gasoline, various oils, acids, ultraviolet radiation. It provides additional insulation from moisture throughout the insulation system.\nSuch a composition can be used both inside and outside the room. Cement-polymer adhesive for polystyrene foam for outdoor use is ideal for laying foam concrete on concrete, cinder block or brick walls. Glue of this type quite strongly adheres polystyrene plates, but requires a fairly deep priming of any surface. They are sold mainly in the form of dry powder, which requires dilution according to the attached instructions.\nVarious polymers have been added to the Portland cement in the mixture, which allows the finished glue to be quite flexible and convenient for application. The resulting solutions are resistant to high humidity, low temperatures and do not collapse over time.\nThis type of adhesive composition is radically different from the above. It is sold in cylinders and is used as regular mounting foam. Polyurethane adhesive for polystyrene for outdoor use has high adhesive properties, and also has a number of other important advantages:\n- easy to use;\n- dries quickly;\n- has an economical consumption;\n- not exposed to various weather disasters;\n- does not spread;\n- possesses high frost resistance.\nThe quality of bonding penoplex when using foam-adhesive for polystyrene for outdoor use much higher. There is no need for constant monitoring of the correctness of the preparation of the composition, as well as compliance with the recommended proportions. This adhesive mixture does not need mixing. Full strength is gained in about 2 hours, while bitumen and cement mortars require at least 6. One cylinder of such glue replaces more than 25 kg of dry mix.\nClay foam does not require continuous application, which is why it is economical. Apply it directly on a polystyrene foam plate around the edge around the perimeter and at several additional points for a stronger grip. Application is also required at the joints in order to fix the foam sheets to each other.\nHelpful advice! Glue-foam should be applied with a special construction pistol. With its help you can glue the gaps between the plates.\nBefore buying the selected foam polystyrene adhesive for outdoor use, in order to determine its quantity, it is necessary to calculate the future consumption of the composition. Exact figures can not be calculated, of course, since they can depend on the wall material, its absorbency, but approximate data can be obtained by checking the instructions from the manufacturer.\nThe average consumption of various compositions:\n|Dry mixes||500 gr. / m?|\n|Bituminous formulations||800-1000 gr. / m?|\n|Polyurethane adhesive foam||1 cylinder on 8-10 m?|\nThe quality of thermal insulation primarily depends on the health of the building and the people living in it. The choice of glue used in outdoor insulation is an important task. After reviewing the description of the main adhesive materials for polystyrene foam, as well as with the methods of their application, you can choose the ideal composition suitable for a specific surface.', 'Glue is a common adhesive used in various DIY and craft projects. When you apply glue to two surfaces, the solvent in the glue will pull moisture out of the air and cause it to dry slowly, making it difficult to work with your project while you wait for the glue to dry. So whether you are bonding wood, plastic, or fabric, it is essential to know how to make glue dry faster. Waiting for the glue to dry can be time-consuming and can delay your project.\nFortunately, there are several ways to speed up the drying process so you can finish your project faster, whether it is gorilla glue, Elmer’s glue, wood glue, or PVA glue. In this article, we will discuss some easy ways to make glue dry faster. We will also explore different types of glue, factors that affect the drying time, and tips for to speed up the drying process.\nHow Does Glue Dry?\n1.1 Understanding the Drying Process\nThe drying process of glue involves the evaporation of solvents or water content, depending on the type of glue used. As the solvent evaporates, the glue hardens and forms a strong bond. This process can take different amounts of time based on the type of glue and environmental factors.\n1.2 Factors Affecting Drying Time\nSeveral factors can influence the drying time of the glue. The type of glue used, humidity levels, temperature, and the materials being glued all affect how quickly the glue will dry. It is important to consider these factors when working on your project.\n1.3 Choosing the Right Type of Glue\nSelecting the appropriate type of glue for your project is crucial to ensure faster drying. Various types of glue are available, such as super glue, wood glue, craft glue, and more. Each type has unique drying characteristics, and choosing the right one can significantly reduce drying time.\nHow Long Does It Take for Glue To Dry?\nThe drying time of glue can vary greatly depending on the amount of glue you use. Some glues dry in just a few minutes, while others may take several hours or days to fully dry and cure. It is essential to read the instructions on the glue label to determine the expected drying time.\nSeveral factors can affect the drying time of the glue. One of the main factors is the type of glue being used. Additionally, the temperature and humidity of the environment can also impact how quickly the glue dries. Higher temperatures and lower humidity will generally accelerate the drying process, while lower temperatures and higher humidity will slow it down.\nHow To Determine When the Glue Is Dry?\nDetermining when the glue is dry depends on the type of glue you are using. Different glues have different drying times, and the conditions in which the glue is applied can also affect the drying process. Here are some general guidelines for common types of glues:\n- White Glue (PVA Glue): This type of glue is commonly used in arts and crafts projects and woodworking. White glue dries through evaporation. It usually takes around 30 minutes to an hour for the glue to set, but it can take up to 24 hours to fully dry and reach its maximum strength. It is generally dry when the glue changes from a white color to a clear, transparent appearance.\n- Super Glue (Cyanoacrylate Glue): Super glue is a fast-drying adhesive that bonds quickly. It typically sets in seconds but can take a few minutes to reach full strength. It is dry when the glue becomes hard and forms a strong bond between the surfaces.\n- Wood Glue: Wood glues, such as carpenters or polyurethane glue, are commonly used in woodworking projects. The drying time for wood glue can vary depending on the type and brand, but usually, it can take 30 minutes to 24 hours to fully dry and set. It is dry once the glue feels hard to the touch and is no longer tacky.\n- Epoxy Resin: Epoxy adhesives consist of two parts that must be mixed before use. The drying time for epoxy resin can vary widely, but it typically takes several hours to overnight to cure completely. Check the instructions on the epoxy product for specific drying times. It is dry once the epoxy has hardened and no longer feels sticky.\n- Hot Glue: Hot glue dries quickly, usually within a few minutes. It cools down and solidifies rapidly as it loses heat. It is dry when the hot glue is no longer tacky and has cooled to room temperature.\n- Rubber Cement: Rubber cement is commonly used in paper crafts and certain art projects. It dries through evaporation, and the drying time depends on the thickness of the adhesive layer and environmental conditions. When the rubber cement is no longer tacky to the touch, it is dry.\nRemember that drying times can vary based on temperature, humidity, and the thickness of the glue layer. If you’re unsure whether the glue is completely dry, it’s best to wait a little longer to ensure a strong and durable bond. Additionally, always refer to the specific product’s instructions and recommendations for drying and curing times for critical applications.\nDoes the Amount of Glue Used Affect Drying Time?\nUsing Too Much Glue\nUsing too much glue can slow down the drying process. Applying a thick layer of glue takes longer for the solvent to evaporate, resulting in a longer drying time. Applying a thin, even layer of glue is important for faster drying.\nUsing Too Little Glue\nToo little glue may not provide a sufficient bond and can affect drying time. If the glue layer is too thin, it may dry too quickly, preventing a proper bond from forming. Finding the right amount of glue for optimal drying and bonding is essential.\nHow To Find the Right Amount of Glue\nFinding the right amount of glue to use can be a bit of trial and error. Start by applying a thin, even layer of glue to one surface. Press the two surfaces together firmly, allowing the excess glue to squeeze out. Too much excess glue indicates that you have used too much, while too little may suggest applying slightly more glue. It may take some practice, but you will find the right glue for each project with time.\n9 Easy Ways How to Make Glue Dry Faster\n1. Use a Fan\nOne easy way to make glue dry faster is by using a fan. Directing a fan towards the glued object can increase air circulation and promote faster evaporation of the solvent in the glue. This can significantly reduce the drying time, especially for glues that rely on air drying.\n2. Apply Heat\nAnother trick to speed up the drying process is to apply heat. You can use a hairdryer on a low setting, hold it a few inches away from the glued surface, and move it back and forth. The heat helps to accelerate the evaporation of the solvent, drying the glue more quickly. However, be cautious not to use too much heat, as excessive heat can damage certain materials or cause the glue to bubble and lose strength.\nAdditionally, you can use a heat lamp to accelerate the drying process by increasing the temperature of the surrounding air. Drying time will vary depending on how much heat is applied. However, it should be noted that excessive heat will cause the glue to become brittle over time. If this happens, your glue may lose its ability to hold things together effectively.\n3. Dry in Sunlight\nSunlight contains UV rays that can break down some glues and accelerate drying by raising ambient air temperature levels above what would normally happen naturally during sunlight exposure alone. Exposing glue-coated items to sunlight under shady conditions will also increase their drying rate if they are not exposed to direct sunlight for too long.\n4. Use a Hairdryer\nLike using heat, you can also use a hairdryer set on a cool or warm setting to speed up the drying of certain types of glue. The gentle airflow helps to expedite the evaporation process without subjecting the glued object to excessive heat. This method works well for glues that are not heat-sensitive. The heat from the hair dryer will help evaporate moisture in the bottle and dry out your glue quickly. This method works best for small amounts of glue or in a hurry.\n5. Use a Blow Dryer\nBlow dryers are not just for your hair! A blow dryer emits warm air, increasing the evaporation rate for water-based glues like PVA glue. The warm air helps remove moisture from the glue, speeding up the drying process. However, using the blow dryer on a low or cool setting is essential to avoid excessive heat, which could potentially damage the glue or the materials being glued.\n6. Use an Electric Blanket\nPlacing the glued object on an electric blanket can create a gentle, consistent warmth around the glue. This helps to accelerate the drying process by promoting evaporation. The controlled temperature of the electric blanket can be advantageous as it avoids the risk of overheating that may occur with direct heat sources like blow dryers or heaters.\n7. Use a Heater\nA heater, such as a space heater or central heating system, can create a warm and dry environment around the glued object. The increased temperature encourages faster evaporation of water or solvents present in the glue, leading to quicker drying. However, caution should be exercised to maintain a moderate temperature and avoid exposing the glue to extreme heat, which may affect its performance.\n8. Add Heat Activators\nSome glues are designed to work with heat activators, which are chemical substances or additives that trigger a faster curing or drying process when applied to the glue. Heat activators can help speed up the cross-linking or hardening of the glue, resulting in quicker bonding and setting. Using the specific heat activator the glue manufacturer recommends for the best results is essential.\n9. Use Clamps to Hold Glued Objects Together\nClamps are widely used in woodworking and other projects to hold glued objects together until the glue dries. By exerting pressure, clamps ensure that the surfaces are in close contact, allowing for a strong bond to form.\nThe time that clamps should be left on depends on the type of glue and the project. In most cases, leaving the clamps on for at least 30 minutes to an hour is recommended to ensure proper bonding. However, for certain types of glue, such as epoxy, the clamps may need to be left on overnight to achieve maximum strength.\nFrequently Asked Questions\nQ: What is the best way to dry glue efficiently?\nA: The best way to dry glue efficiently is to use a fan or a heat gun to promote air circulation and speed up drying.\nQ: How can I make glue dry quickly?\nA: To make glue dry quickly, consider using quick-drying glue, applying a thin layer of glue, or using compressed air to expedite the drying.\nQ: Does hot glue dry faster than regular glue?\nA: Hot glue dries faster than regular glue due to its rapid cooling and solidification properties.\nQ: How can I dry the glue faster without compromising the bond?\nA: You can make the glue dry faster without compromising the bond by using a quick-dry glue or increasing airflow around the glued area with a fan.\nQ: Is Gorilla Glue a fast-drying glue?\nA: Gorilla Glue is a fast-drying adhesive, and it is known for its quick setting and strong bonding capabilities.\nQ: What are some tips for using wood glue to dry quickly?\nA: To ensure wood glue dries quickly, apply a thin layer of glue, clamp the pieces together, and control the temperature and humidity.\nQ: How can I speed up the drying process of super glue?\nTo speed up the drying process of super glue, use the smallest amount of glue possible and spray a small amount of water or use baking soda on the glue.\nQ: Which type of glue dries the fastest among different kinds of glue?\nA: Super glue is known to be one of the fastest-drying adhesives available.\nQ: Does the temperature affect how quickly glue will dry?\nA: Higher temperatures generally result in faster glue drying times, but avoid excessive heat as it could damage your project.\nQ: How long does it take for glue to cure completely?\nA: The time it takes for glue to cure completely depends on the type of glue used; it can range from a few seconds to several hours.\nQ: Can I use a heat gun to dry glue?\nA: Yes, a heat gun can be used to dry glue quickly. However, it’s essential to keep the heat contained in the room and avoid using excess heat that could damage your project.\nQ: How can I help the glue dry when working on a project?\nA: You can help the glue dry by keeping the area well-ventilated, using quick-dry glue, or using a fan or a heat gun to speed up the process.\nQ: How do I make Gorilla Glue dry faster?\nA: To make Gorilla Glue dry faster, use a fan or a heat gun, or choose the quick-drying version of the glue if available.\nQ: Can I use a clamp to make glue dry faster?\nA: Clamping the materials together can help create a stronger bond and make the glue dry faster.\nQ: What should I do if the glue takes longer to dry than expected?\nA: If the glue takes longer to dry, ensure the room has adequate ventilation and that the moisture in the air isn’t hindering the drying process. You can also use a heat gun to hasten the drying.\nIn conclusion, several proven techniques can help expedite the drying process of glue. With various adhesive options available, understanding the factors influencing drying time is essential for achieving efficient bonding in various projects. One effective approach is to choose a quick-dry glue product explicitly labeled as “fast drying” or “quick-dry.” These adhesives are formulated to reduce drying time significantly, allowing for faster completion of tasks.\nMoreover, knowing the specific glue drying process and following the manufacturer’s instructions can contribute to optimal and rapid drying results. Additionally, remember that while heat can speed up the drying process, it is crucial to exercise caution. Excess heat could damage the project or affect the adhesive’s effectiveness. Working with super glue requires attention to detail, as it dries quickly.\nBy following these easy ways to make glue dry faster, you can save time and complete your projects more efficiently. Don’t forget to consider the glue type, the amount used, and the environmental conditions for optimal drying. Whether you use a fan, apply heat, or use clamps, exercise caution and adhere to safety guidelines to avoid mishaps. With these tips, waiting for glue to dry will no longer be tedious.\nFor more great content like this, subscribe to our community at the Family Hint today!']	['<urn:uuid:c01174d2-3e74-4f48-925b-0d68a28c050f>', '<urn:uuid:a3b7dcde-7802-4ef6-8157-eb1d9cec15a0>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T21:09:04.591601	24	94	3338
92	mental strain from stress signs symptoms	Symptoms include sadness and hopelessness, difficulty sleeping, worrying and feeling stressed most of the time, lack of appetite, feeling overwhelmed, inability to concentrate, withdrawal from friends and family, impaired functioning at home/work/school, avoiding responsibilities, and potentially suicidal thoughts or behaviors.	['Adjustment disorder is a stress-related mental illness that causes impairment and distress following a single stressful event or ongoing stress. It may cause a range of emotional and behavioral symptoms including sadness, anxiety, difficulty concentrating, and feelings of being overwhelmed. Adjustment disorder persists for several months but responds well to treatment with therapy and medications when appropriate. It often co-occurs with other mental illnesses disorders, so screening and treating all issues a person has is important for the best outcomes.\nWhat Is Adjustment Disorder?\nAdjustment disorder is a mental health condition that is characterized by a limited ability to adjust or cope with a significant, negative life event. It is classified in the Diagnostic and Statistical Manual of Mental Disorders as a trauma- or stressor-related disorder. Any stressful event or series of situations that contribute to a stressful environment can trigger this condition, causing distress and making it difficult to maintain normal functioning.\nAdjustment disorder doesn’t typically last longer than six months, as long as the stressor is relieved or removed. Many patients do benefit from treatment, though. Treatment with therapy, lifestyle changes to reduce stress, social support, and learning and using healthy coping mechanisms can help relieve symptoms and restore function in all areas of life.\nTypes of Adjustment Disorder\nWhen adjustment disorder is diagnosed it may also be given a specifier. This describes a specific aspect of a person’s mood and symptoms and further breaks down the condition into six subtypes:\n- With depressed mood. Symptoms are characteristic of major depression and may include sadness, hopelessness, and an inability to enjoy or get pleasure from activities and other aspects of daily life.\n- With anxious mood. This type is characterized by symptoms of anxiety, including nervousness, worrying, a feeling of being overwhelmed, and difficulty thinking or concentrating.\n- With mixed anxiety and depressed mood. A person may experience both depression and anxiety symptoms with adjustment disorder.\n- With disturbance of conduct. This specifier is used when the symptoms are largely behavioral. Any behaviors that are out of character for an individual can be used to make this diagnosis. Examples include aggression or violence, poor performance in school, or engaging in risky activities.\n- With mixed disturbance of emotions and conduct. Some people with adjustment disorder will have a mix of depression and anxiety symptoms along with behavioral disturbances.\n- Unspecified. Adjustment disorder is considered unspecified when a patient’s symptoms don’t match any of the above descriptions. But this person will still experience dysfunction in relationships, home life, work or school.\nFacts and Statistics\nAdjustment disorder can be a debilitating condition, causing significant impairment and distress. It is also fairly common.\n- Across the general population the rate of adjustment disorder is close to 12 percent.\n- In clinical settings adjustment disorder may occur in up to 23 percent of patients.\n- The most common subtype of adjustment disorder is depression, followed by anxiety, anxiety with depression, and conduct disturbance.\n- One study found that 38 percent of active duty military personnel hospitalized for mental health reasons were diagnosed with adjustment disorder.\n- There are also higher rates of adjustment disorder in patients with cancer and patients suffering from severe burns as compared to the general population.\nSymptoms and Diagnosis of Adjustment Disorder\nThe symptoms that someone with adjustment disorder experiences vary to some extent by individual and by type. For instance, someone with the depressive type is likely to have more sadness and depression-like symptoms than someone with the conduct disturbance specifier. But generally the symptoms of adjustment disorder include:\n- Sadness and hopelessness, and crying a lot\n- Difficulty sleeping\n- Worrying a lot and feeling stressed most of the time\n- Lack of appetite\n- A feeling of being overwhelmed\n- Being unable to concentrate on tasks\n- Withdrawal from friends and family\n- Impaired functioning at home, work, or school\n- Avoiding responsibilities\n- Suicidal thoughts or behaviors\nDiagnosing adjustment disorder can be challenging because the symptoms are similar to other conditions, such as major depression, anxiety disorders, post-traumatic stress disorder, and personality disorders. The criteria for diagnosing adjustment disorder include the following:\n- There is an identifiable source of stress and an onset of symptoms within three months of the onset of that stress.\n- The distress caused by the stressor must be out of proportion to the actual situation or cause significant impairment in one or more areas of a person’s life.\n- The symptoms cannot be those of normal bereavement if the source of stress is the loss of a loved one.\n- The symptoms cannot be better explained by another mental illness.\n- The symptoms must last for no longer than six months after the source of stress is removed.\nCauses and Risk Factors\nThe underlying cause of adjustment disorder is stress. The stress may be caused by one specific event, or it may be ongoing, persistent stress that creates continuously stressful circumstances. Some examples of events that may trigger adjustment disorder include problems at work or home, the breakup of a relationship, moving or starting a new job, retirement, being chronically ill, or anything else that causes stress. A traumatic event, such as being a victim of an assault or living through a natural disaster, may also cause this condition.\nNot everyone who experiences stress will develop this condition. The reasons that some people are more vulnerable are not fully understood. Several factors may come into play, including past life experiences, genetics, environmental factors, and temperament and personality. Someone who has not learned healthy coping mechanisms for stress, who has a poor support network, or who has another mental illness may be at a greater risk. In general, the more stressful events a person experiences, the greater the risk is of developing an adjustment disorder.\nAdjustment disorder is triggered by stress or trauma, so it is possible that it may co-occur or precede the development of a similar condition like acute stress disorder or posttraumatic stress disorder. Unrelated conditions that are commonly diagnosed in people who also struggle with adjustment disorder are personality disorders, depression, and anxiety disorders. As much as 70 percent of people diagnosed with adjustment disorder are also diagnosed with other mental illnesses.\nAlso commonly co-occurring with adjustment disorder are other mental health disorders. In trying to cope with stress, it is not unusual for someone who lacks healthy strategies to turn to alternative ways to self-soothe. This can be an unhealthy coping mechanism that only exacerbates mental health issues.\nCall for a Free Confidential Assessment.877-727-4343\nTreatment and Prognosis of Adjustment Disorder\nComprehensive treatment for adjustment disorder aims to relieve symptoms, provide better stress coping mechanisms, alleviate distress, and restore function. Therapy is the main component of treatment, but medications may be used for specific symptoms, such as depression. A treatment plan should also include evaluation, diagnosis, and management of any other mental illnesses.\nTherapy for adjustment disorder is typically some type of behavioral therapy but may also include group therapy, family therapy, and support groups. Cognitive behavioral therapy helps patients look at stressors and figure out ways to reduce or eliminate them. Patients also analyze the stressor to try to determine why it has such a profound effect. It is important to learn to reframe the stress and what it means.\nBehavioral therapy is also important for helping patients learn better coping skills. There are positive and productive ways to manage stress, such as breathing techniques, meditation and mindfulness, social support, exercise, and journaling. When an individual can learn and put into practice better coping strategies, they can learn to overcome the current stress and deal productively with future stressors.\nMedications may be used to manage adjustment disorder, but they are not always necessary. Antidepressants can be used for someone with ongoing depression and anxiety symptoms. Anti-anxiety medications can be used in the short-term to help manage stress and nervousness. Any medications that may be necessary to manage other mental illnesses may also be used, depending on the patient.\nThe prognosis for someone seeking treatment for adjustment disorder is very positive. This is a mental health issue that responds well to therapy and professional support, especially when all mental illnesses are identified and treated. Patients who commit to regular treatment, make lifestyle changes, and practice good coping strategies at home have the best outcomes. In general, treatment for this condition is not long-term, and symptoms resolve within a few months or less.']	['<urn:uuid:af27efff-c454-4d67-93cd-635108e887d5>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	6	40	1406
93	cellular mechanisms stomach acid production immune defense connection	The cellular mechanisms of stomach acid production involve several cell types working together. ECL cells produce histamine which stimulates parietal cells to produce acid, while G cells produce gastrin that stimulates both ECL cells and parietal cells. This acid production system serves a dual purpose - while it's essential for protein digestion by activating pepsin, stomach acid also plays a crucial role in the immune system by killing harmful bacteria and parasites that are ingested with food, providing a first line of defense against pathogens.	"['And now to continue with the second part of this mini project. This time with slightly more in-depth discussion regarding stomach acid regulations and protein digestion.\nA brief summary:\n– Inside the stomach, protein is broken down by the enzyme pepsin into smaller pieces, called peptones.\n– Pepsin is derived from pepsinogen, which is made by chief cells in the stomach lining.\n– Stomach acid made by parietal cells in the stomach lining transforms pepsinogen into pepsin and creates an acid environment to optimize the function of pepsin.\n– The stomach lining is protected from stomach acid and pepsin by a layer of alkaline mucous made by mucous cells. The alkaline environment neutralizes the acid and inactivates the pepsin.\nI will continue to discuss what other cell types and systems influence this proces. An important concept when studying the body is its ability to selectively activate or inactivate processes involved in maintaining a physiological balance (homeostasis). The body changes constantly and it is therefore required to compensate for outside (and inside) influences to maintain homeostasis. Homeostasis means maintaining an internal environment that is optimized for the body at that time. To this end, most critical processes are regulated to stay within tight boundaries. As the environment changes, however, these boundaries will change accordingly as well. And as the balance for any one factor falls outside of these boundaries, the body will act to return to a balanced internal environment. Homeostasis in an unconscious process, managed and monitored by the brain and nervous system.\nThe on-switch for the stomach is therefore also neurological. As soon as you see something to eat, smell something to eat, taste, chew and swallow, the stomach is made ready to digest. The autonomic nervous system has already sent a general signal to start digestion, since it knows that food will be/is being/has been eaten. The stomach starts producing acid and pepsinogen to initiate digestion, as well as extra mucous to protect itself.\nFood entering the stomach provides an additional signal to promote the production of acid and digestive enzymes.\nECL cells (EnteroChromaffin-Like cells) are located close to parietal cells, but behind the layer of epithelial cells in the stomach lining (so they are not in direct contact with the stomach content). ECL cells produce histamine, which they release into their immediate environment. Parietal cells then produce more acid when they notice the presence of histamine.\nECL cells are so-called endocrine cells, because they release (crine) histamine inside (endo) the body, whereas parietal and chief cells release their product on the outside (exo) of the body. Technically, any part of the body that is not behind a layer of epithelium is internal. The digestive tract is technically a single long tube, which is lined by epithelium of some kind and is therefore considered external to the body.\nHistamine may be familiar to some in a different context. Histamine is an important messenger for the immune system en is the main cause of allergic symptoms. Generally speaking, histamine signals for an increase in inflammation. But in the stomach it appears to have a different function.\nBrief summary: ECL cells produce histamine, which makes parietal cells produce more acid. ECL cells therefore indirectly increase the acidity of the stomach content.\nThe next layer of complexity comes from the bottom of the stomach crypts, where the stomach endocrine cells are located. One type of endocrine cell, called G cells, produce gastrin and secrete it in their direct environment. Gastrin stimulates histamine production by ECL cells and acid production by parietal cells, thereby lowering the pH of the stomach indirectly.\nOnce protein digestion has started, a positive feedback loop kicks in. A positive feedback loop occurs when the result of process enhances the process it resulted from, effectively amplifying it. In this case, peptones (the small protein fragments) stimulate G and chief cells. G cells produce more gastrin, which increases acid production and chief cells produce more pepsinogen. Pepsinogen in the acidic environment becomes pepsin and breaks down more proteins into peptones, which amplifies the peptone signaling. This loop remains active as long as peptones are present. As soon as the stomach content is sufficiently digested, it will move on to the small intestines, thereby breaking the feedback loop.\nThe body regulates most processes at multiple levels. In the stomach, the second type of endocrine cell, D cells, provide more regulation for stomach acid production. D cells make somatostatin (Greek for: halting the body (or something like that)). D cells do very little when digestion has just gotten started, but start producing somatostatin once the pH gets lower and more gastrin is made by G cells. Somatostatin inhibits (stops) ECL, G and parietal cells from producing their products and thereby reduces the amount of acid that is being made directly and indirectly. Stronger activating signals for D cells result in higher amounts of somatostatin being released. This is known as a negative feedback loop, where the result of a process, inhibits that process.\nSomatostatin is also made by cells in the small intestine (duodenum) as response to a lower pH when the stomach content gets enters the small intestine. Endocrine release of somatostatin can be transported by blood to provide signals elsewhere in the body, not just the immediate environment around the endocrine cell.\nA final regulation of stomach acid, as mentioned before: the entire stomach is under the control of the nervous system, mainly the parasympathetic branch of the autonomous nervous system. This branch promotes digestion and rest. It used acetylcholine, locally made by nerve endings, to communicate with the stomach. Acetylcholine stimulates cells that produce digestive enzymes and acid, but it inhibits D cells. Given the effect of somatostatin produced by D cells, the inhibition of D cells by acetylcholine in this context, makes sense, since it results in more digestion. Of course, the inhibition of D cells by the nervous system can be overcome with sufficient stimulatory inputs.\nThat is some of the information I gathered from my book on human physiology regarding stomach acid regulation and protein digestion. My book is at least 5+ years old by now and mentioned that several unknown cells and signals are also suspected to affect the stomach. And, of course, my descriptions are simplified for your understanding ;).\nRegardless, I hope you can now appreciate a bit more of the complexity involved in a seemingly simple process. This is a common theme, in my experience, when studying anything in-depth.', ""Why You Need Stomach Acid and What Happens When You Neutralize It\nThe importance of stomach acid is well-documented. Your body needs stomach acid for a lot of good reasons:\n- Stomach acid plays an essential role in the immune system by killing harmful bacteria and parasites that are ingested with food\n- Stomach acid activates the enzyme pepsin needed for protein digestion\n- Stomach acid signals to the pancreas produce digestive juices and enzymes to further break down food\n- Stomach acid initiates peristalsis, the rhythmic contractions of the intestines, that crush and move the food through the GI tract\n- Stomach acid is essential for the absorption of Vitamin B12 which plays key role in the normal functioning of the brain and nervous system, and in the formation of blood.\n- Stomach acid performs an essential digestive function by reducing food clumps (bolus) into smaller particles (chyme), so that the intestines can absorb nutrients quickly and effectively.\nThe effect of suppressing stomach acidity with drugs is to decrease nutrient absorption to such a degree that there can be long term health consequences. In the process you are also compromising an essential part of your immune system. Stomach acid kills harmful microbes on contact but without it these intruders can flourish. The longer the duration of acid suppression the higher the risk of harmful effects.\nStomach acidity is measured by a chemistry table known as, the pH scale (shown below).\nAs you can see from the chart, an individual with a healthy, properly-functioning digestive system has esophagus with a pH: 7, a small intestines with a pH: 7, and, in between, a stomach with a pH:2.\nAnother important function of stomach acid is the effect that it has on the Lower Esophageal Sphincter (LES). The LES is the valve which allows food to enter the stomach but prohibits reflux of gastric acid into the esophagus. The autonomic nervous system activates the LES in response to increased stomach acidity. The LES has “sensors” that measure the acidity of your stomach contents. As the concentration of stomach acid (aka Hydrochloric acid or HCl for short) increases, the closing pressure of the LES increases. However, if the stomach does not reach the proper level of acidity, the LES valve remains open and stomach acid will reflux into the esophagus.\nConversely, the pyloric sphincter is the valve which holds stomach contents until they are ready to flow down to the intestines. The pyloric sphincter works in exactly the opposite way of the LES. It will not open until stomach acidity has reached a sufficiently high level. This is nature's way of ensuring that food is properly digested before it flows through the rest of the gastrointestinal system. However, if the stomach does not reach the proper level of acidity, the pyloric valve remains closed and stomach will linger resulting in indigestion, bloating and possibly forcing stomach contents to back flow into the esophagus.\nAs we age, our body produces less and less stomach acid. It is therefore no coincidence that reflux and GERD increases with age.\nDanger of Acid Neutralizers\nThe conventional method of treatment for reflux is prescription or over-the-counter “acid-neutralizers”. These include proton pump inhibitors (PPI's), H2-receptor antagonists, and antacids. The exact functionality of these differ, but the common thread is that they all reduce or eliminate stomach acid, in effect, raising the pH level of the stomach.\nThere is some relief from symptoms because any acid that refluxes into the esophagus will have a higher pH, lessening the damage to the esophagus. However, they do nothing to treat the underlying cause, and, in taking them, you self-inflict your stomach with a condition known as achlorhydria. Achlorhydria is a lack of sufficient stomach acid which is responsible for dozens of digestives disease through the GI tract. Some of the diseases that are directly or indirectly caused by achlorhydria or are listed below.\nAchlorhydria Related Upper Gastric Tract Diseases\n- Esophagus: Candidal esophagitis (Boerhaave syndrome, Mallory-Weiss syndrome), Zenker's diverticulum, Barrett's esophagus, Esophageal motility disorder (Nutcracker esophagus, Achalasia, Diffuse esophageal spasm, GERD), Esophageal stricture, Megaesophagus\n- Stomach: Gastritis, Peptic ulcers, Dyspepsia, Pyloric stenosis, Gastroparesis, Gastroptosis, Portal hypertensive gastropathy, Gastric antral vascular ectasia, Gastric volvulus\nAchlorhydria Related Intestinal Tract Diseases\n- Small intestine - Enteritis (Duodenitis, Jejunitis, Ileitis), Peptic (duodenal) ulcer, Curling's ulcer, Malabsorption: Coeliac, Tropical sprue, Blind loop syndrome, Whipple's, Short bowel syndrome, Steatorrhea, Milroy disease\n- Large intestine, appendix and colon: Appendicitis, Colitis (Pseudomembranous, Ulcerative, Ischemic, Microscopic, Collagenous, Lymphocytic), Functional colonic disease (IBS, Intestinal pseudoobstruction/Ogilvie syndrome), Megacolon/Toxic megacolon, Diverticulitis, Diverticulosis\n- Large and small intestines: Enterocolitis (Necrotizing), IBD (Crohn's disease), Vascular: Abdominal angina, Mesenteric ischemia, Angiodysplasia, Bowel obstruction: Ileus, Intussusception, Volvulus; Fecal impaction — Constipation · Diarrhea (Infectious)\n- Rectum: Proctitis (Radiation proctitis), Proctalgia fugax, Rectal prolapse\n- Anus: Anal fissure/Anal fistula, Anal abscess, Pruritus ani\nAccessory Achlorhydria Related Intestinal Tract Diseases\n- Liver: Hepatitis (Viral hepatitis, Autoimmune hepatitis, Alcoholic hepatitis), Cirrhosis (PBC), Fatty liver (NASH), vascular (Hepatic veno-occlusive disease, Portal hypertension, Nutmeg liver), Alcoholic liver disease, Liver failure (Hepatic encephalopathy, Acute liver failure), Liver abscess (Pyogenic, Amoebic), Hepatorenal syndrome, Peliosis hepatis\n- Gallbladder: Cholecystitis, Gallstones / Cholecystolithiasis, Cholesterolosis, Rokitansky-Aschoff sinuses, Postcholecystectomy syndrome\n- Bile duct, biliary tree: Cholangitis (PSC, Secondary sclerosing cholangitis, Ascending), Cholestasis/Mirizzi's syndrome, Biliary fistula, Haemobilia, Gallstones / Cholelithiasis common bile duct (Choledocholithiasis, Biliary dyskinesia)\n- Pancreatic: Pancreatitis (Acute, Chronic, Hereditary), Pancreatic pseudocyst, Exocrine pancreatic insufficiency, Pancreatic fistul\n|Causes and Symptoms of GERD and Reflux||The Acid Reflux Myth|""]"	['<urn:uuid:23acca85-fe73-460e-a1f7-38a16769f54b>', '<urn:uuid:7e4d0cb7-233c-441c-95a8-da5fa32dfba3>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	8	85	1987
94	digital photos format accessibility problems	Digital photo collections can become inaccessible due to the rapid evolution of storage media, even when saved in popular formats.	"[""Thank you for using the timer!\nWe noticed you are actually not timing your practice. Click the START button first next time you use the timer.\nThere are many benefits to timing your practice, including:\nOver the past couple of decades, professional and amateur [#permalink]\n27 May 2012, 10:23\nThis post received KUDOS\nThis post was BOOKMARKED\nOver the past couple of decades, professional and amateur photographers alike have been increasingly confronted with an unexpected problem, an issue that seldom arose with conventional film photography. In contrast to print film technology, the advent of digital photography allows a photographer to store a near-infinite number of images in a very small space, and to instantaneously share digital photos via an Internet connection or by sending the image from cell phone to cell phone. Since the digital image can be copied without loss of resolution, adopting digital photography in favor of film may seem to be the ideal solution for preserving photos indefinitely. But while traditional prints may fade over time, gradually losing resolution or parts of the image, digital images are far more vulnerable to wholesale eradication due to a hard drive crash or a malicious computer virus. Another consideration is that whole collections of images can become inaccessible due to the rapid evolution of the storage medium. Even when taking care to save photo files in the most popular format, photographers must contend with the fact that storage media become regularly superseded by newer technology.\nTo avoid image loss through accident or obsolescence, family archivists hoping to preserve their children''s photos for posterity should consider making backups the first line of defense in preserving digital information. Many universities and businesses regularly transfer all digital data into new formats as the technology becomes available, and individual consumers may simply have to resign themselves to doing the same. Labeling the archived file with the date and subject of the photo, just as generations have done with print media, is always advisable. And finally, producing archival print copies for storage may be the best security: a box full of photos can last up to 100 years under appropriate environmental conditions, far longer than platforms for data storage typically remain popular.\nAs a whole, the passage is primarily concerned with: A contrasting the advantages of film photography to the benefits of digital photograph B advocating a return to conventional film photography C recommending solutions to a problem inherent with the use of a newer technology D demonstrating that, because the digital images can be copied without loss of resolution, this format is superior to film prints E discussing the evolution of photographic methods over time\nThe passage mentions which of the following as a possible consequence of saving image files in only the most popular format? A An increase in the number of universities transferring their collections to newer media as technology changes B Decreased protection from fading over time or gradual loss of resolution of the digital image C Increased protection from wholesale eradication of a photo collection due to a malicious computer virus D An ability to send digital images between cell phones and to sharing sites via an Internet connection E Increased susceptibility to comprehensive data loss as standards of file storage change over time\nWhich of the following statements can be inferred from the passage? A Before the advent of digital photography, storage of film prints required more space to store and archive than did storage of digital photos. B Collections of film prints have never been vulnerable to wholesale eradication. C Before the advent of digital photography, most corporations regularly copied their image documents into new film formats as they became available. D Professional photographers used to store many more print images than did amateur photographers. E Photographers did not need to produce archival print copies of film photos for storage, since the originals were already in print format.\nRe: Over the past couple of decades, professional and amateur [#permalink]\n24 Mar 2015, 04:01\nHello from the GMAT Club VerbalBot!\nThanks to another GMAT Club member, I have just discovered this valuable topic, yet it had no discussion for over a year. I am now bumping it up - doing my job. I think you may find it valuable (esp those replies with Kudos).\nWant to see all other topics I dig out? Follow me (click follow button on profile). You will receive a summary of all topics I bump in your profile area as well as via email.\nFinal decisions are in: Berkeley: Denied with interview Tepper: Waitlisted with interview Rotman: Admitted with scholarship (withdrawn) Random French School: Admitted to MSc in Management with scholarship (...""]"	['<urn:uuid:33288b24-c0a4-40ac-a0ce-a9d91a63a113>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:09:04.591601	5	20	778
95	caribbean water shortage climate effects regions and gender implications	The Caribbean faces severe water challenges due to climate change, with underground water sources being particularly vulnerable. Many islands like Barbados, Cuba, and Dominican Republic are experiencing high water demand exceeding supplies, leading to rationing and saltwater seepage. The situation disproportionately affects women, especially in rural areas. Women comprise 43% of the agricultural labor force in Latin America and the Caribbean, but face limited access to resources and technical assistance. Climate change impacts, such as variable rainfall and extended dry seasons, particularly affect women farmers who have less access to credit, technology, and decision-making control, making them more vulnerable to climate shocks and disasters. Studies show women are 14 times more likely than men to die during disasters in these regions.	"['San Juan, Puerto Rico – Scientists and experts are concerned about the effects of climate change for parts of the Caribbean — the depletion of already strained drinking water throughout much of the region.\nRising sea levels could contaminate supplies of fresh water and changing climate patterns could result in less rain to supply reservoirs in the coming decades, scientists and officials warned at a conference in St. Lucia this week.\n""Inaction is not an option,"" said Lystra Fletcher-Paul, Caribbean land and water officer for the U.N. Food and Agriculture Organization. ""The water resources will not be available.""\nSome of the possible solutions include limits on development, increased use of desalination plants and better management of existing water supplies, but all face challenges in a region where many governments carry heavy debts and have few new sources of revenue.\nMany Caribbean nations rely exclusively on underground water for their needs, a vulnerable source that would be hit hard by climate change effects, said Jason Johnson, vice president of the Caribbean Water and Wastewater Association, a Trinidad-based nonprofit group.\n""That\'s the greatest concern,"" he said. ""Those weather patterns may change, and there may not necessarily be the means for those water supplies to be replenished at the pace that they have historically been replenished.""\nParts of the Caribbean have been experiencing an unusually dry spell that emerged last year.\nIn August 2012, some islands reported extremely dry weather, including Grenada and Anguilla. By July of this year, those conditions had spread to Trinidad, Antigua, St. Vincent and Barbados, the Caribbean Institute for Meteorology & Hydrology says.\n""We\'re seeing changes in weather patterns,"" said Avril Alexander, Caribbean coordinator for the nonprofit Global Water Partnership. ""... When you look at the projected impact of climate change, a lot of the impact is going to be felt through water.""\nIntense rains have been reported in recent months in some Caribbean areas, but that doesn\'t mean an increase in fresh water supply, said Bernard Ettinoffe, president of the Caribbean Water and Sewerage Association Inc., a St. Lucia-based group that represents water utilities in the region.\nHeavy rains mean there\'s not enough time for water to soak into the ground as it quickly runs off, he said. In addition, the cost of water treatment increases, and many islands instead shut their systems to prevent contamination.\nThe island considered most at risk is Barbados, which ranks 21st out of 168 countries in terms of water demand exceeding available surface water supplies, according to a 2012 study by British risk analysis firm Maplecroft. Other Caribbean islands high on the list are Cuba and the Dominican Republic, which ranked 45 and 48, respectively. The study did not provide data on a smattering of eastern Caribbean islands that officials say are among the driest in the region.\n""There are a number of indications that the total amount of rainfall in much of the Caribbean would be decreasing by the end of the century,"" said Cedric Van Meerbeeck, a climatologist with the Caribbean Institute for Meteorology & Hydrology.\nVan Meerbeeck said water supplies will continue to decrease if individuals as well as agriculture and tourism, the region\'s key industries, do not monitor use.\n""Climate is maybe not the biggest factor, but it\'s a drop in an already full bucket of water,"" he said. ""It will have quite dramatic consequences if we keep using water the way we do right now.""\nJamaica, Trinidad and Barbados have ordered rationing this year, with Barbados reducing pressure and occasionally cutting off supply to some areas. The island also began to recycle water, with officials collecting treated wastewater to operate airport toilets.\nOveruse of wells elsewhere has caused saltwater seepage and a deterioration of potable water underground, leading to the construction of hundreds of desalination plants in the Caribbean.\nBut the cost of desalination still remains unaffordable for many governments, said John Thompson, director of the Caribbean Desalination Association board.\nThe biggest challenge overall is changing the mentality of water utility authorities who see their role as solely providing clean water, Johnson said.\n""The new reality is that it\'s a national security issue if your water supplies are diminished,"" Johnson said. ""It becomes a health and safety issue.""\nBased on reporting by The Associated Press.', '“…there is no social sector more invisible, less understood and less served, than that of rural women, despite the vital role they play in our rural communities…” Former President of Costa Rica, Mrs. Laura Chinchilla\nBlue Mountain Coffee is one of the most expensive specialty coffees in the world, and it caters to a hyper-niche market of luxury coffees, costing over US$58/lb. Its prized berries grow on the cool steep slopes of Jamaica’s Blue Mountain range above 3000ft where it fuels the livelihood of over 4,000 smallholder farmers. However, this space of luxury coffee production has not escaped the far-reaching effects of climate change.\nThe Blue Mountain coffee space has been invaded and disrupted by changing climatic conditions. The livelihoods of Jamaican women coffee farmers have been particularly affected by variable rainfall, extended dry seasons, reduced yields, and increased pests and diseases. Drought conditions have led to inconsistent production and poor coffee quality which reduces coffee volumes that can be sold and result in financial losses.\nMoreover, the unpredictability and inconsistency of climatic conditions have also increased the cost to maintain coffee farms, as the farm inputs required to increase yield and treat diseases such as the coffee leaf rust are often expensive.\nSimilar events have been documented globally where rural women experience a greater impact from changing climatic conditions as their dependence on natural resources for livelihood maintenance is often disrupted by hydro-meteorological events such as hurricanes, storms, floods, droughts, and landslides, among others.\nUnfortunately, even though the Latin America and Caribbean (LAC) region contributes less than 10% of global emissions, their economies, sectors, infrastructure, and people have been facing the consequences and adverse effects. According to the State of the Climate in Latin America and the Caribbean 2020 report, the passage of Hurricanes Eta and Iota wreaked havoc in Guatemala, Honduras, Nicaragua, and Costa Rica; Brazil, Bolivia, Paraguay, and Argentina’s Pantanal region experienced an unprecedented drought and fire season. The glaciers in the Chilean and Argentine Andes continue to retreat and the Caribbean region continues to face deficits in rainfall.\nMoreover, considering that many economies and livelihoods within the LAC region are dependent on climate-sensitive sectors such as agriculture, food, and nutrition security are also in the crosshairs of being adversely affected. It is expected that by 2050, Central America and the Caribbean will experience a one-fifth reduction in agricultural yield for beans and maize.\nOther areas that remain highly exposed and vulnerable include, human health, water resources, settlements, and biodiversity. Economically, the combined annual damages for Latin America and the Caribbean due to climate change impacts are estimated to be at US$100 billion by 2050, which is almost as much as Ecuador’s GDP.\nParticularly for the Caribbean region, approximately US$ 22 billion in losses is projected. Therefore, the devastating impacts on the economic, cultural, environmental, physical, and social fabric of countries will erode any gains and progress they have made in development.\nHowever, amidst these threats, the gendered face of climate change remains a critical area, as the difference in the capabilities of men and women to adapt to changing climatic conditions continues to generate concern. The recent IPCC assessment reported with high confidence on the unequal impact of climate change on men and women. This is primarily due to the gender inequality and inequity that influences the control of and access to assets, resources, services, and decision-making processes.\nThis disparity disproportionately affects women and increases their vulnerability to climate change impacts and disasters, as well as affects their ability to cope and adapt. ECLAC acknowledged that “women and children are 14 times as likely as men to die during a disaster”. Rural women, particularly in the Latin America and Caribbean region are expected to face the brunt of climate change impacts and disasters.\nRural women play a critical role within the food system, where on average they comprise of 43% of the agricultural labour force. Contributing to the food security of households and communities, their roles range from producers, planters, harvesters, farmworkers, rearing livestock, as well as domestic care. However, despite their contribution, the work of rural women is often unrecognized. Their reality is persistently characterised by poverty and structural inequalities, particularly since they have less access to productive resources. Climate change impacts and disasters tend to exacerbate these pervasive challenges, thus increasing their vulnerability.\nAccording to Oxfam, around 30% of rural women in Latin America own agricultural land, while access to technical assistance is only available to less than 5%. For example, in Brazil, and Guatemala rural women are faced with having less access to credit, technology, mechanization, land, and other assets, thus limiting their adaptation and decision-making capabilities. In Colombia, climate change has affected women coffee producers by exacerbating the spread of the coffee berry borer. Their ability to successfully manage this pest and apply adaptation measures has been hampered by their lack of access to technical expertise, information, and decision-making control. Similar experiences are also found within the Small Island Developing States of the Caribbean (SIDS) region where rural women in Haiti and other territories face socio-economic barriers that are rooted in gender inequality; increasing their vulnerability and heightening their risk to climate shocks and disasters.\nFor Jamaica’s women coffee farmers in the Blue Mountains, the impacts of climate change have also been fuelled by the disparity in receiving advisory services. Based on a pilot survey conducted by IWCA Jamaica (JAWiC), a woman coffee farmer for over 10 years perceived that, “people [wider industry stakeholders] more want to help the men or look to the men” with regard to farm management. Women also noted barriers in accessing resources, technical training, and engaging in opportunities which allows upward mobility in the coffee value chain, as well as the ability to occupy spaces of leadership within their communities.\nAccording to another local coffee farmer, there was a “lack of recognition, lack of resources such as training, workshops, mentorship, and lack of support” for women in the coffee industry. Likewise, a woman organic coffee farmer who has operated in the industry for over 20 years as a roaster and exporter observed that “men want to be the ones to tell you what to do…. and think women don’t know anything”. This gender dynamic and uneven power relation illustrate that some women perceive their contribution to the coffee industry as ‘lesser than’, compared to men.\nHowever, ignoring women’s contribution to rural livelihoods and limiting opportunities to access resources not only lowers a nation’s economic potential, but also weakens its resilience to climate change impacts. Therefore, addressing the root causes of gender inequality in the region is vital to securing sustainable rural livelihoods.\nFor women coffee farmers in Jamaica, there have been efforts to effect change by facilitating rural women coffee farmers with access to opportunities such as training, technical knowledge, financing, technology, and productive resources. Empowering rural women to tackle the impacts of climate change not only requires a gendered lens in reforming institutions and investment in services, but also tackling deep-seated cultural, socio-economic, and patriarchal norms that limit the full expression of women’s productive capacity and decision-making power. Last but not least, it will allow mankind to continue enjoying one of the best coffees in the world.']"	['<urn:uuid:6cc6837f-6499-4d28-a795-4ab4b353f480>', '<urn:uuid:3b5e55b6-8857-4c43-941f-d3d3c25b6a1a>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:09:04.591601	9	121	1910
96	I'd love to build a green home. What makes compressed dirt walls environmentally friendly when compared to regular concrete walls?	Rammed earth has potentially low manufacturing impacts and low to moderate embodied energy, especially compared to concrete. While it uses cement like concrete does, rammed earth can be thought of as a 'weak concrete' that uses less cement. The most basic traditional rammed earth has very low greenhouse gas emissions, though emissions can increase with transport and cement manufacture in modern construction.	['Rammed earth walls are constructed by ramming a mixture of selected aggregates, including gravel, sand, silt and a small amount of clay, into place between flat panels called formwork. Here is a Performance Summary to help you decide if you want Rammed Earth in your next project.\nThe colour of rammed earth walls is determined by the earth and aggregate used. The ramming process proceeds layer by layer and can introduce the appearance of horizontal stratification to the walls, which can enhance the overall appearance. It can be controlled as a feature or eliminated.\nAggregates can be exposed and special effects created by the addition of different coloured material in some layers, and elements such as feature stones or objects, alcoves or relief mouldings can be incorporated into rammed earth walls, at a price. Unusual finishes can be achieved by including shapes in the formwork that can be released after the wall has been rammed.\nRammed earth is very strong in compression and can be used for multi-storey loadbearing construction. Research in New Zealand indicates that monolithic earth walls perform better under earthquake conditions than walls made of separate bricks or blocks. A five storey hotel in Queensland is built of stabilised rammed earth. Rammed earth can be engineered to achieve reasonably high strengths and be reinforced in a similar manner to concrete, although horizontal reinforcement is not recommended and excessive vertical reinforcement can cause cracking problems.\nInteresting structural features, including leaning walls, have been constructed in rammed earth. Any difficulties associated with placing and ramming around reinforcement can be eased by careful management of the construction process and need not add significantly to the cost.\nRammed earth behaves as heavyweight masonry with a high thermal mass. Thermal mass absorbs or ‘slows down’ the passage of heat through a material and then releases that heat when the surrounding ambient temperature goes down. All other things being equal, a high mass building such as rammed earth remains close to the 24 hour average for the time of year: in many climates this may be too cold or warm for comfort. If heating or cooling is required, the walls need to be insulated to limit energy consumption.\nUsed correctly, and in the right climate, the thermal mass of rammed earth can delay heat flow through the building envelope by as much as 10 to 12 hours and can even out daily temperature variations.\nRammed earth walls become effective when the difference between day and night outdoor temperatures is at least 6°C. Where the diurnal range is greater than 10°C, appropriate design can exploit the high thermal mass of rammed earth to very good effect (see Thermal mass).\nIn cool or cold climates well-located rammed earth walls (e.g. feature walls within a well-insulated envelope) can supply a battery of useful thermal storage. Rammed earth is not recommended for tropical climates where high mass construction can cause a house to hold too much heat and cause thermal discomfort (see Thermal mass).\nBuilding energy efficiency rating tools such as AccuRate include the capability to simulate the combined effects of wall mass and insulation, and also allow comfort in extreme weather conditions to be reviewed, so the building’s performance can be optimised.\nInsulation is about stopping heat passing through a material rather than slowly absorbing or releasing it. As a corollary to its high thermal mass, rammed earth has limited thermal insulating qualities — similar to an uninsulated fibre cement wall.\nInsulation can be added to rammed earth walls with linings but, as a general rule, a 300mm rammed earth wall will not meet Building Code of Australia (BCA) requirements for external wall insulation. Hybrid buildings that use insulated framed external walls with rammed earth internal walls and feature elements can achieve high insulation and high mass.\nUnder certain design criteria (i.e. simple rectangle with north-facing glass) and in moderate (not temperate) climates, it was just possible to meet the NatHERS overall five star performance standard; however, six stars are now the mandatory minimum under the BCA (see Insulation).\nInsulation can also be added within the thickness of a rammed earth wall but this adds to its cost and changes the structural properties of the wall. However, it does provide the benefits of both excellent thermal mass and good thermal insulation in the one wall while retaining the desirable look, texture, feel, acoustics and low maintenance properties of the facing of rammed earth on each side.\nThe best location for insulation when used in conjunction with rammed earth is on the outside face of the wall, so that the thermal mass is within a contained and controllable external envelope (see Passive solar heating; Passive cooling).\nOne of the best ways to insulate against sound is have monolithic mass, which rammed earth provides very well. It has excellent sound reverberation characteristics and does not generate the harsh echoes characteristic of many conventional wall materials (see Noise control).\nFire and vermin resistance\nThere are no flammable components in a rammed earth wall and its fire resistance is thus very good. In tests by the CSIRO a 150mm thick Cinva-rammed earth block wall (similar to rammed earth) achieved a near four hour fire-resistance rating. There is no cavity to harbour vermin and nothing in the material to attract or support them so its resistance to vermin attack is very high.\nDurability and moisture resistance\nThe basic technology has been around for thousands of years and there are many rammed earth buildings still standing that are centuries old. Rammed earth does possess a generally high durability but all types of rammed earth walls are porous by nature and need protection from driving rain and long term exposure to moisture.\nMaintain water protection to the tops and bottoms of walls. Continued exposure to moisture may degrade the internal structure of the earth by reversing the cement stabilisation and allowing the clays to expand. In general, rammed earth does have moderate to good moisture resistance and most modern Australian rammed earth walls do not require additional waterproofing. New water repellent additives that waterproof the walls right through may make rammed earth suitable for very exposed conditions, including retaining walls, but may inhibit the breathability of the material.\nBreathability and toxicity\nProvided it is not sealed with material that is impermeable to air molecules, rammed earth maintains its breathability. Finished walls are inert but take care in the choice of waterproofing or anti-dust finishes to avoid adding toxicity to the surfaces.\nRammed earth has potentially low manufacturing impacts, depending on cement content and degree of local material sourcing. Most rammed earth in Australia uses quarried aggregates, rather than the ‘earth’ it is popularly thought to be made from. On-site materials can often be used but materials need to be tested for their suitability.\nThe embodied energy of rammed earth is low to moderate. Composed of selected aggregates bound with cementitious material, rammed earth can be thought of as a kind of ‘weak concrete’. It may help to understand cement and earth products as being at different points on an energy continuum, with earth at the low end and high strength concrete at the high end. Its cement and aggregate content can be varied to suit engineering and strength requirements.\nAlthough in principle it is a low greenhouse gas emission product, transport and cement manufacture can add significantly to the overall emissions associated with typical modern rammed earth construction. The most basic kind of traditional rammed earth has very low greenhouse gas emissions but the more highly engineered and processed variants may be responsible for significant emissions in their manufacture. For example, a 300mm rammed earth wall with 5% cement content has the equivalent of 15mm thickness of cement, equivalent to over 100mm of concrete (which mainly comprises sand and aggregate).\nBuildability, availability and cost\nRammed earth is an in site construction method. Although its buildability is good, formwork for rammed earth demands good site and logistics planning to ensure that other trades are not adversely affected in the building program. Services should be well planned in advance to minimise difficulties. After walls have been rammed in place, conduits for pipes and wires can be provided much as in other masonry construction, but may impact on surface finishes.\nBasic materials for rammed earth making are readily available across Australia, but cement and formwork may have to be transported long distances, increasing environmental and economic costs. Testing of local aggregates and potential mixes is essential if not using a proprietary system.\nProprietary approaches to rammed earth help guarantee consistency and predictable performance but come at a cost. The cost of a professional rammed earth building is comparable to other more conventional good quality masonry construction, but it can be more than twice as expensive as a rendered 200mm wide AAC block wall (see Autoclaved aerated concrete).\nRammed earth is particularly well established in Western Australia and is thus an economical option in that state. Most states have experienced builders who understand its potential and limitations but because it is not a common construction material outside of WA its relative rarity and specialist nature tends to be reflected in its relatively high cost. It typically requires high levels of control over material sourcing and batching, and expensive formwork. A key element in controlling costs is to design walls as simple panels and to avoid unnecessary complexity. Traditional rammed earth using human power for ramming and simple wooden formwork can be low cost (and low energy) but this is rarely a realistic option.\nThere are good networks in Australia including a broad based national organisation, the Earth Building Association of Australia (EBAA), which is a not for profit organisation ‘formed to promote the use of Unfired Earth as a building medium throughout Australia’.']	['<urn:uuid:6cbe5cbd-ee50-4d31-a2dc-f44abda67471>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	20	62	1626
97	microscope vs spontaneous generation connection history	The compound light microscope was essential for early microbiology, while spontaneous generation was disproved by Louis Pasteur through his famous experiments with the S-neck flask that showed microorganisms don't spontaneously appear but come from contamination through air.	"['Immunologist - studies ecology of Legionella pneumophila\nWhich of the following statements about fungi is true?\nAll are plants.\nWhich of the following pairs is mismatched? N\nNormal microbiota - harmful\nWho proved that microorganisms cause disease?\nWhich of the following pairs is mismatched? V\nvan Leeuwenhoek - germ theory\nWhich of the following requirements was NOT necessary for Pasteur to disprove spontaneous generation?\nRegarding Pasteur\'s experiments with the S-neck flask, which of the following statements is true?\nAny possibility of contamination was removed. There was air involved. There was a food source involved. All microorganisms were killed before beginning.\nWhich of the following statements about the atom 12/6 C is false?\nIt has 12 neutrons in its nucleus\nWhich of the following statements is false?\nWater molecules are formed by hydrolysis.\nWhich of the following is the type of bond holding K+ and I- ions in KI?\nWhich of the following is the type of bond between molecules of water in a beaker of water?\nWhat is the type of bond holding hydrogen and oxygen atoms in the H2O molecule?\nIdentify the following reaction: Glucose + Fructose → Sucrose + Water\nDehydration synthesis reaction\nIdentify the following reaction: Lactose + H2O → Glucose + Galactose\nWhich type of molecule contains the alcohol glycerol?\nIdentify the following reaction: H20+CO2 ⇌ H2CO3\nWhich molecule is composed of a chain of amino acids?\nWhich type of molecule contains -NH2 groups?\nWhat structure does light pass through after leaving the condenser in a compound light microscope?\nWhich of the following pairs is mismatched? G\nGram-negative bacteria - negative stain\nWhich of the following pairs is mismatched? S\nSafranin - acid dye\nWhich type of molecule is composed of (CH2O) units?\nWhat do genes consist of?\nWhich are the primary molecules making up plasma membranes in cells?\nIdentify the following reaction: NH4OH ⇌ NH3 + H2O\nThe counterstain in the acid-fast stain is\nA basic dye.\nThe purpose of a mordant in the Gram stain is\nTo prevent the crystal violet from leaving the cells.\nPlace the following steps in the correct sequence: 1-Staining; 2-Making a smear; 3-Fixing.\nIn this microscope, the observer does NOT look at an image through a lens.\nPlace the steps of the Gram stain in the correct order: 1-Alcohol-acetone; 2-Crystal violet; 3-Safranin; 4-Iodine.\nIn Figure 3.1, line ""a."" points to the microscope\'s\nIn Figure 3.1, line ""b."" points to the microscope\'s\nThe appearance of gram-negative bacteria after addition of the decolorizing agent in the Gram stain.\nThe appearance of gram-negative bacteria after completing the Gram stain.\nWhich of the following is NOT a distinguishing characteristic of prokaryotic cells?\nThey lack a plasma membrane.\nThe best definition of osmotic pressure is\nThe force with which a solvent moves across a semipermeable membrane from a higher to a lower concentration.\nBy which of the following mechanisms can a cell transport a substance from a lower to a higher concentration?\nWhich of the following statements about viruses is false?\nThey have both DNA and RNA.\nWhich drawing in Figure 4.1 is a tetrad?\nWhich drawing in Figure 4.1 possesses an axial filament?\nWhich drawing in Figure 4.1 is streptococci?\nWhich drawing in Figure 4.1 is a bacillius?\nWhich of the following structures is NOT found in prokaryotic cells?\nIn Figure 4.3, which diagram of a cell wall is a gram-negative cell wall?\nWhich microscope achieves the highest magnification and greatest resolution?\nWhich of the following is NOT a chemical component of a bacterial cell wall?\nIn Figure 4.3, which diagram of a cell wall is a toxic cell wall?\nWhich of the following terms best describes the cell in Figure 4.2?\nSimple staining is often necessary to improve contrast in this microscope.\nCompound light microscope\nIn Figure 4.3, which diagram of a cell wall has a wall that protects against osmotic lysis?\nBoth a and b\nAssume you stain Bacillus by applying malachite green with heat and then counterstaining with safranin. Through the microscope, the green structures are then counter-stained with safranin. Through the microscope, the green structures are :\nWhich of the following pairs is mismatched?\nCentrosome - food storage\nWhat Gram reaction do you expect from acid-fast bacteria?\nIn Figure 4.3, which diagram of a cell wall is decolorized by acetone-alcohol?\nThe purpose of the ocular lens is to\nMagnify the image from the objective lens.\nThe resolution of a microscope can be improved by changing the\nWavelength of light.\nWhich microscope is most useful for visualizing a biofilm?\nScanning acoustic microscope\nIn Figure 4.3, which diagram of a cell wall is resistant to many antibiotics (e.g., penicillin)?\nWhich of the following organelles most closely resembles a prokaryotic cell?\nIn Figure 4.3, which diagram of a cell wall contains teichoic acids?\nYou have isolated a motile, gram-positive cell with no visible nucleus. You can safely assume that the cell\nHas a cell wall.\nIn Figure 4.3, which diagram of a cell wall contains porins?\nIn Figure 6.2, which section shows a growth phase where the number of cells dying equals the number of cells dividing?\nIn Figure 6.2, which sections of the graph illustrate a logarithmic change in cell numbers?\nB & D\nMost bacteria grow best at pH\nDuring which growth phase will gram-positive bacteria be most susceptible to penicillin?\nWhich of the following is an advantage of the direct microscopic count?\nRequires no incubation time\nWhich of the following is NOT a direct method to measure microbial growth?\nWhich of the following statements is false?\nDNA replication proceeds in one direction around the bacterial chromosome.\nWhich of the following pairs is mismatched? T\nThermophile - growth at 37°C\nIn Figure 6.4, in which tube are microaerophiles growing?\nIn Figure 6.4, in which tube are facultative anaerobes growing?\nWhich enzyme catalyzes the reaction: O2- + O2- + 2H+ → H2O2 + O2?\nA culture medium on which only gram-positive organisms grow and a yellow halo surrounds Staphylococcus aureus colonies is called a(n)\nSelective medium & Differential medium\nThe initial effect of ionizing radiation on a cell is that it causes\nThe formation of highly reactive ions.\nA culture medium consisting of agar, human blood, and beef heart is a\nWhich of the following is the best definition of generation time?\nThe length of time needed for a cell to divide\nWhich group of microorganisms is most likely to spoil a freshwater trout preserved with salt?\nWhich of the following is the best method to sterilize heat-labile solutions?\nWhich of the following does NOT achieve sterilization?\nWhich of the following is NOT used to disinfect water?\nWhich of the following substances is NOT effective against nonenveloped viruses?\nIf you were preparing nutrient agar at home and didn\'t have an autoclave, what could you use to sterilize the nutrient agar\nPressure cooker at 121°C for 15 minutes\nWhich of the following pairs is mismatched? R\nRNA polymerase - makes a molecule of RNA from an RNA template\nDNA is constructed of\nTwo strands of nucleotides running antiparallel.\nWhich of the following is NOT a product of transcription?\nA new strand of DNA\nGenetic change in bacteria can be brought about by', ""What were louis pasteur's accomplishments louis pasteur's full name is dr louis jean pasteur louis pasteur it quotes the last 4 paragraphs from 'the life of pasteur' by renã . Louis pasteur accomplishments introduction: pasteur, louis (1822-1895), world-renowned french chemist and biologist, who founded the science of microbiology, proved the germ theory of disease, invented the process of pasteurization, and developed vaccines for several diseases, including rabies. During the 19th century life the life and accomplishments of louis pasteur in britain was the life and accomplishments of louis pasteur accomplishments of louis pasteur.\nLouis pasteur’s research and work gave birth to several branches of science and he was responsible for many of the theoretical concepts and applications that are still used today childhood, career and personal life. Learn all about louis pasteur, the scientist who first began pasteurization readers will discover the incredible accomplishments of pasteur and how he dedicated his life to studying various molds, bacteria, . The story of louis pasteur is a 1936 american black-and-white biographical film from warner bros, produced by henry blanke, directed by william dieterle, .\nPasteur vallery-radot wrote a brief biography of his famous grandfather in 1958, and claimed that pasteur did not consider spontaneous generation altogether impossible he even claimed pasteur “had dreams about creating or modifying life”. According to the encyclopedia britannica (2013) louis pasteur was both a chemist and a microbiologist this human was a man of many great accomplishments who made many contributions to the field of science, technology and medicine louis pasteur was a pioneer of his field and should be greatly . Louis pasteur, (born december 27, 1822, dole, france—died september 28, 1895, saint-cloud), french chemist and microbiologist who was one of the most important founders of medical microbiology pasteur’s contributions to science , technology , and medicine are nearly without precedent. Louis pasteur louis pasteur was born in dole on december 27,1822, the only son of a poorly educated tanner louis pasteur grew up in the small town of arbois louis was not an outstanding student during his years of elementary education, preferring fishing and drawing to other subjects.\nLouis pasteur biography, life, interesting facts louis pasteur was a french microbiologist as well as a french chemist that developed the first vaccines for rabies as well as anthrax he was creditedwith the invention of a technique for treating milk and wine to stop bacterial contamination. Louis pasteur (1822–1895) is the life and accomplishments of louis pasteur revered by his successors in the life sciences as well as by the general public 11-5-2017 his breakthroughs have saved countless lives and improved the the life and accomplishments of louis pasteur medical research quantitative paper crequite quality of life for . #5 louis pasteur decisively disproved spontaneous generation spontaneous generation was a prevailing notion during pasteur’s time by which simple life-forms were spontaneously generated from non-living matter .\nLouis pasteur (1822–1895) is revered by his successors in the life sciences as well as by the general public in fact, his name provided the basis for a household word—pasteurized. Louis pasteur was a famous chemist and biologist whose works led to the advancement of the life we know today he created vaccines and medicines, as well as invented the pasteurization process for milk products. Louis pasteur’s pupil paul vuillemin coins the term “antibiotic” from the word antibiosis: process by which life could be used to destroy life 1890 albert calmette becomes roux’s assistant at the institut pasteur.\nLouis pasteur was born on 27 december 1822 in dole, jura, france he was to become known as the founder of microbiology pasteur was the one who discovered the role of bacteria in fermentation. Louis pasteur (1822–1895) was a french biologist and chemist whose breakthrough discoveries into the causes and prevention of disease ushered in the modern era of medicine louis pasteur was born december 27, 1822 in dole, france, into a catholic family he was the third child of jean-joseph . Personal life pasteur was born in 1822 in dole, the only son of a poorly educated tanner, jean pasteur, who had served in napoleon's grande armee.\nLouis pasteur was a french chemist who proved that germs cause disease, developed vaccines for anthrax and rabies and created the process of pasteurization louis pasteur: biography & quotes. With these facts about louis pasteur, let us learn more about his early life, family, education, work, inventions, patents, and accomplishments about his childhood and early life 1. Heroes of wine: louis pasteur (1822-1895) and to help me with this article on the life of louis pasteur, louis pasteur - biographycom.""]"	['<urn:uuid:0e571338-2a95-4428-abd4-81159ec632fe>', '<urn:uuid:d22ded87-2e51-4334-b4f7-2627abe2d009>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:09:04.591601	6	37	1955
98	what happens when multiple people claim to be pope at same time has it occurred	During the Western Schism, from 1378 until as late as 1429, there were two and even three men simultaneously claiming to be pope. The situation was so complex that reasonable people, including saints, could disagree about who was the true successor of Peter. For example, St. Vincent Ferrer was the confessor of Benedict XIII, while Saint Catherine of Sienna advised his rival, Urban VI.	"['Question: Is it true that the Church went for more than thirty years without a pope during the Western Schism?\nAnswer: No. If anything, the Church suffered from having too many popes during the period that extended from 1378 until as late as 1429. At that time, two, and even three men claimed to reign as pope; and the situation was such that reasonable people could disagree as to which one was actually the successor of Peter. 1 (See the June 1996 ""Q&A"" column for a more detailed explanation of the Western Schism.)\nHowever, one might speak of this period as a papal interregnum (the time between papal reigns) in the sense that a sincere Catholic of the time might have found himself in the position of not knowing which pope to look to as the true head of the Church. This is significant in that it demonstrates the falsity of those who hold that the Church must always be clearly and unambiguously lead by a single visible head. It is historical fact that for a long period people, saints included, disagreed over who was in truth the pope. (St. Vincent Ferrer was the confessor of Benedixt XIII, while Saint Catherine of Sienna advised his rival, Urban VI.) ""The gates of hell will not prevail,"" but indefectibility is not packaged up in the person of the pope. Popes, after all, die and take a while to replace, are sometimes replaced with not so good ones, and occasionally are replaced by too many popes. But the Church is indefectible, and, with the passage of time, puts all such problems behind Her.\nThe longest actual interregnum that we could find began with the death of Pope Clement IV on 29 November 1268 and did not end until the election of Pope Bd. Gregory X on 1 September 1271. In fact, it lasted longer than that because, while papal reigns are calculated from the date of election, on that date the newly elected Tedaldo Visconti was only a deacon and was in the holy land on crusade. He unable to return to Rome and was not ordained priest and consecrated bishop until late March of 1272.\nThe conclave that eventually elected Bd. Gregory at Viterbo is one of the best remembered in history. It demonstrates the urgency among Catholic people to have a pope:\nThe Viterbese grew restless. The conclave was being held at the papal palace instead of the cathedral, and each afternoon when the cardinals finished their deliberations for the day, they returned to their own homes. The Viterbese began to throw rocks at them. When the cardinals went a few days without balloting, the people threw rocks at the palace. The allotment of food to the palace was first cut down, then cut out. At their own houses, the cardinals found the same food shortage.\nChagrinned, the English Cardinal John Tolet commented one day that maybe to roof of the palace should be removed to give the Holy Ghost easier access to the minds of the cardinals. Word of his observation reached the people. Next morning when the cardinals arrived at the palace the roof of the conference room was gone....2\nEarly in his reign, Bd. Pope Gregory established procedures to hasten the conduct of future elections. While not removing the roof so that the cardinals might be rained on, it became the custom to restrict the cardinals to relatively spartan living arrangements so that they might devote their undivided attention to the business at hand.']"	['<urn:uuid:0648bbe9-b48c-42af-a755-a277cc771fc8>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:09:04.591601	15	64	586
99	As someone interested in sustainable agriculture, I'd like to know what makes plants grow better under artificial light, and what challenges come with indoor vertical farming?	Plants respond to artificial lighting by regulating growth hormones through photosynthesis, with LEDs allowing precise control over color temperature and light direction for optimal growth. However, vertical farming faces several challenges: it requires high energy consumption, demands substantial investment costs, and can experience indoor issues like excessive UV and heat damage to plants. Additionally, while effective at community-scale levels, vertical farming systems have difficulty scaling up to feed larger populations.	['That lighting has a significant role in plant growth is a well-known fact. A recent study published by the National Geographic Society highlighted the relationship between photosynthesis and the production of hormones that regulate the growth of plants. In fact, in the absence of the right lighting, for example during cloudy skies, plants can go into a sort of ‘lethargy’ to stop the production of this growth hormone and use their resources for other activities, such as defending themselves from diseases.\nThe study was conducted by the Plant Lab of the Institute of Life Sciences at the University “Scuola Superiore di Sant’Anna” in Pisa and published in the latest issue of Plant Cell. Another international research by the Polytechnic of Vienna, in Austria, and published in the Frontiers in Plant Science, showed that trees can ‘sleep’ to adjust to outdoor lighting as if they had their own circadian rhythm, just like humans. For these reasons, even the most updated lighting projects designed for city green areas must take into account of the possible interference between artificial lighting and plant life. Read also “At San Severo, lighting respects the environment and plants”.\nSo why can’t we use lighting to increase plant growth by intervening on the specific growth factors? And can this strategy be used in horticulture?\nThe answer is yes. Furthermore, it has been proven that not only some types of lamp, but LEDs too, can do an excellent job. It is not a surprise that consolidated experience exists. In fact, plants can be made to grow under artificial lighting to allow greater production and increased quality. Generally speaking, in the so-called vertical farms or in farm dominiums, high-pressure sodium lamps (with a short life) are used for top-down lighting. In summary, vertical farms consist of parallel rows of plants to minimize the use of space, while farm dominiums are movable farms that can be transferred on ships, where plants complete their growth under artificial lighting.\nAll this takes on different importance and significance if we use LED lighting. First of all, because colour temperature and light colour can be modulated to supply the most suited lighting for each specific type of plant. For example, the best light for plants is blue between 430 and 490 nm and red between 640 and 780 nm, but to speed up growth and increase quality and quantity, researchers have demonstrated that, in this case, the best light is infrared light at 730 nm.\nAt this point, we just need to discover what are the best lighting parameters required for a particular plant and supply this lighting through LED sources. Since the lighting emitted by the LEDs can be controlled, they can be used for any type of desired plant. Recent studies indicate that plant growth can be increased by up to 40%. Furthermore, LED light is dot-like, so it can be aimed exactly where needed, avoiding shades and reducing consumptions.\nAll this also creates great business opportunities. To this end, the Horticulture Lighting Conference will be held on October 12th, in Chicago, USA.\nThe organizers define LEDs for plants as a “new booming market destined to create a revolution in the world of horticulture”.\n- Posted by Redazione\n- On August 16, 2016\n- 0 Comments', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']	['<urn:uuid:cee5991c-37b2-4b5b-a49b-d72400751c40>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:09:04.591601	26	70	1629
100	Which needs more paperwork, starting a sole proprietorship or share premium contribution?	A share premium contribution requires less paperwork, needing only a contribution agreement and shareholder's resolution. A sole proprietorship requires almost no paperwork - just a DBA certificate if not using your own name, and reporting profits on personal taxes.	['Share capital vs. share premium for capital funding of a Dutch company\nWhen a shareholder wants to fund its Dutch Limited Liability Company (BV) with capital there are two choices: share capital or share premium. Are there any pros and cons in regard the use of both options?\nShare capital and share premium are both considered to be part of the equity of a BV and therefore both are in principal a tax neutral instrument. Share capital can be brought into a company by paying up issued shares in cash or in kind. Share premium can be brought into a company by a contribution in cash or in kind on the existing shares of a company.\nAccording to the Dutch Accounting Standards funding of a BV by issuance of new shares will be recorded in the Equity. According to the Dutch Accounting Standard RJ 240.221, capital contributions made by existing shareholders on account of their financial relationship to the entity that are executed conducted without actual disbursement of new shares (or rights to receive or call shares) are to be recognized as share capital premium (in Dutch: agio). In the Financial statements funding under share premium contribution will be shown under Equity as a separate balance post (article 2: 373 subsection 1 Dutch Civil Code).\nIncrease of equity\nIncreasing the capital of a company by means of capital requires the execution of a deed by a Dutch civil law notary. For the execution, the notary will require a resolution from the shareholder which also includes a power of attorney. Additionally, a power of attorney from the company will be required. Normally, the notary drafts the notarial deed, power of attorney and resolution.\nA contribution of share premium will require a share premium contribution agreement concluded between the shareholder and the company, plus a shareholder’s resolution. There is no registration of share premium with the Dutch trade register or in the shareholders register. After the conclusion of the share premium contribution agreement and adoption of the resolution, the share premium can be paid by the shareholder to the company by means of a transfer. As there are not many formalities, a share premium contribution can, in case of urgency, be executed within a day.\nShare premium and share capital redemption\nIn case of liquidation, redemption of share capital can be executed without any formalities and in a tax neutral way. In case the company will be continued, the share capital can only be executed in a tax neutral way by means of a decrease of the nominal value of the shares. This decrease of the nominal value can only be realised by means of a notarial deed. In case of a decrease of the nominal value of the shares with 50 per cent, an amount equal to 50 per cent of the initial share capital can be distributed to the shareholder.\nIn relation to accounting and taxation, share capital and share premium are both considered to be part of the equity and are treated in an equal way. Therefore, it is recommendable to make use of the type of equity that can be applied most easily. A contribution of share premium takes less time and brings less legal expenses and paperwork.\nFeel free to contact our legal team in case of any questions or need for support with a capital contribution.\nDutch Civil Code in English: http://www.dutchcivillaw.com/civilcodegeneral.htm\nNBA (Netherlands Institute of Chartered Accounctants) guidance on contribution in kind: https://www.nba.nl/globalassets/wet--en-regelgeving/nba-handreikingen/nba-handreiking_1129_storting_op_aandelen_anders_dan_in_geld_inbreng_in_natura.pdf\nDutch Accounting Standards Board: https://www.rjnet.nl/engelse-pagina/', 'When you decide to start a business, you have many decisions to make. Arguably the most important decision you will make is which form of a business entity to establish. There are many entities to choose from and each will have its own advantages and disadvantages. Evaluate each option from a tax standpoint and assess any legal liability considerations when determining which will best fit your business.\nCreating a Sole Proprietorship\nThe Sole Proprietorship is a very common business structure because it is the easiest to form and to create. A sole proprietor is an individual that owns an unincorporated business by himself. All of the profits and losses of the sole proprietorship are treated as the income or losses of the individual owner for tax purposes, and must be reported on the owner’s income tax return. Since the sole proprietorship is an unincorporated business, there is no corporate tax.\nFrom a legal standpoint, the sole proprietorship does not exist as a separate entity from the owner of the business, whereas other business structures (such as corporations) exist as separate legal entities. The main area of concern with regard to a sole proprietorship is with liability. With a sole proprietorship, if someone wishes to file a lawsuit against the business, they are essentially suing the business owner, and the business owner will be personally liable for the lawsuit. In contrast, with a corporation, if someone wishes to sue the corporation, that person is suing only the corporation, and generally the shareholders will not be personally liable for the lawsuit.\n1Consider writing a business plan. A business plan details how your business will run. While this step is not required to operate a sole proprietorship, it will help you to plan your business.\n2File for a “Doing Business As” (DBA) Certificate. If you wish to conduct business under a name that is not your own, you must obtain a certificate to do so. You may file for a DBA at your local county clerk’s office building by filling out a form and paying a fee. If you are doing business under your given name (not giving your business a separate name) you do not need a DBA.\n3Begin conducting business. As a sole proprietor, your business does not technically exist from a legal standpoint, and therefore you need not file any paperwork to get it started.\n4Report your business profits and losses on your personal tax return.\nCreating a Limited Liability Company (“LLC”)\nAn LLC is an unincorporated business, but has more protections (hence the limited liability) than the sole proprietorship. To create an LLC, a business owner must file documents and pay fees in the state where it wishes to form. Note that not every state has the LLC as an available business structure.\nAs with the sole proprietorship, the profits and losses of the business pass through to the members of the LLC, thus avoiding corporate taxes. Each member of the LLC will report the income and losses of the LLC on their individual tax return and be taxed at their individual tax rate.\nAccording to the federal government, the LLC does not exist separately from the individual members. For liability purposes, however, individual members of the LLC will generally not be personally liable for a lawsuit against the LLC. This is the main advantage over the sole proprietorship.\n1Consider writing a business plan.\n2File organizational documents with your state. Not all states offer the LLC as a business entity, so be sure to check with your state’s Department of State before deciding on this structure. Then determine which documents you will be required to file. Generally the documents include Articles of Organization and some states may also require you to publish notice of your business formation in local newspapers. You will also likely have to pay a fee.\n3Draft and adopt an Operating Agreement. An operating agreement is generally an internal document that is not filed with the state, however you may be required to have one by your state. Even if not required, it is a good idea to draft an operating agreement because it details exactly how your business will run, from the type of business you decide to operate to handling payroll and who will receive what share of profits and losses.\n4Operate your business.\n5Report your business profits and losses on your personal tax return.\nCreating a Corporation\nA corporation is an incorporated business where shareholders put money and/or property into the business in exchange for shares of the corporation’s stock. A corporation is often a large business with several different shareholders. Forming a corporation requires filing documents with the state and the federal government. A disadvantage of a corporation is that this business structure requires the owners to comply with many state- and federally-mandated corporate formalities.\nFor tax purposes, the corporation is taxed twice: once at the corporate level and again at the individual level. This double taxation is often viewed as the main disadvantage of the corporate business structure. The federal government recognizes a corporation as a separate legal entity, which can be an advantage of the corporation business structure.\nFor legal liability purposes, the corporation structure enjoys the benefits of being a separate legal entity, and thus the shareholders of the corporation are generally not liable for lawsuits against the corporation.\n1Choose a name for your corporation. States generally place restrictions on the words that may or may not be used in the name of a corporation. Generally the name must include the word ‘incorporated’ or ‘inc.’ or some other indicia that the business is a corporation.\n2Determine who will be the initial directors of the corporation. The initial board of directors is composed of those individuals in charge of getting the corporation up and running. They will authorize the issuance of stock and pick officers of the corporation, two of the most important decisions the corporation will make.\n3File incorporation documents with your state. These are usually called articles of incorporation and are generally accompanied by a filing fee. Your state’s secretary of state or department of state office will likely have an approved form that you may fill out or you may draft your own. Articles of incorporation generally include the name of the corporation, its address, and the name and contact information for a person to whom members of the public can contact regarding that corporation.\n4Draft corporate bylaws. These are analogous to the operating agreement that an LLC operates under. Corporate bylaws will include additional information such as how shares of stock will be voted, how directors are chosen, etc. Usually you may use a template for the bylaws or draft your own.\n5Hold an initial board meeting and adopt the bylaws. At this meeting the initial board of directors will adopt the bylaws, elect officers, authorize the issuance of stock, and determine how to handle certain financial or tax issues such as the corporation’s fiscal year.\n6Issue shares of stock. In order to officially be a corporation you must issue stock, as this formalizes the division of the ownership of the business.\nCreating a Subchapter S Corporation\nThe S Corporation is a tax election that an existing corporation makes. Essentially, it avoids the double taxation of regular corporations by electing to have the taxes pass through the corporation to the shareholders. Each shareholder will report the S corporation’s profits and losses on their own individual tax return. The S Corporation has a number of restrictions on the type of business the corporation can engage in, the number of shareholders, and the type of stock issues.\nThe S Corporation enjoys the same legal liability status of other corporations.\n1Form a corporation. Since a Subchapter S Corporation is a tax election that a corporation makes, the first step to creating an S Corporation is to create a corporation. Follow the steps outlined above.\n2File Form 2553 with the IRS. The form is available on the IRS website.\n3Approve the S Corporation tax election at the initial board meeting. There must be unanimous consent by the board in order to make the S Corporation tax election.\n- Always check with your state regarding the requirements for forming a business in that state. Most states’ Department of State or Secretary of State websites will contain information about the process.']	['<urn:uuid:6b4f012a-73f4-45cd-9984-e78ce7daf41d>', '<urn:uuid:f94aa928-c014-46ac-9527-d3a09d391dae>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T21:09:04.591601	12	39	1972
