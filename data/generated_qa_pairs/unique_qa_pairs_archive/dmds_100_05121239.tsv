qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	I'm looking to move to Milan and heard there's a new fancy building with a unique shape. Can you tell me what makes this building special and how it fits into the neighborhood?	The Residenze Carlo Erba is a distinctive condominium building that features an audaciously curving structure that snakes 492 feet through a one-acre triangular site. Unlike other new developments in Milan that feature ultrathin or spiraling glass towers, this building draws inspiration from Milan's traditional palazzi while still standing out. Its serpentine form disrupts the angular grid of its quiet residential neighborhood near Milan's university district. The building's exterior combines warm travertine at its base with cool Carrara marble at the top, reflecting Milan's layered architectural look. The structure includes 80 units distributed across nine stories, stepping down to create urban villas within an open steel framework at the top.	"[""Peter Eisenman does not build often, but—often—when he does, it’s worth taking note. Now 87 years old, the influential architect, scholar, theorist, and teacher has an unusual portfolio of completed projects that runs the gamut from his radical early houses of the 1960s and ’70s to an Arizona football stadium that’s hosted the Super Bowl (2006) and the Berlin Memorial to the Murdered Jews of Europe. His latest, a condominum building, may not be as groundbreaking or large or moving as others, but, as his first completed project in Italy—a country in which he’s spent countless sojourns studying its architecture, from Palladio to Piacentini—it has significant meaning attached to it nonetheless.\nJump to credits & specifications\nAmong the most important architects whose work he explored was Giuseppe Terragni, the Rationalist who built under the Fascists. “Eisenman reintroduced Terragni to a generation of Italian architects,” says Lorenzo Degli Esposti. The Milan-based Degli Esposti did his thesis on Eisenman and worked for a brief period in his New York office. In late 2009, when a Milan developer, with whom he’s collaborated on smaller housing projects, asked him to bring in a “big name” architect for a statement building in the city, the young architect suggested his mentor.\nThe Residenze Carlo Erba condominium project is emerging at the same time that Milan has seen a rash of brazen new residential and commercial buildings by international architects—Zaha Hadid, Daniel Libeskind, and Arata Isozaki at the CityLife district northwest of the center where the fairgrounds once stood, and Pelli Clark Pelli, KPF, and Stefano Boeri at Porta Nuova due north of the Duomo. Unlike those architects, who introduced ultrathin or spiraling glass towers and vertical forests to the city, Eisenman and Degli Esposti, an architectural historian in his own right, along with New York–based architect Guido Zuliani, a third partner on the project, drew upon Milan’s unrivaled stock of exquisite but eclectic palazzi in designing Carlo Erba.\nThat’s not to say their new building slips quietly into the background. “Eisenman mines the physical and cultural archaeology of a place,” states London-based Matteo Cainer, another former employee and devoted acolyte. That process of analysis results in multilayered and often highly complex assemblies.\nYet the Milan project stands out among his buildings—simpler in some respects, more daring in others. Eisenman calls it a “mature” work. Rather than conform to the street wall, the audaciuosly curving structure snakes 492 feet through its one-acre triangular site, where an office building once stood, disrupting the angular grid of this quiet residential neighborhood on the border of the Città Studi, Milan’s university district.\n“I’ve never done a curved building before. I didn’t necessarily want to do a curved building,” admits Eisenman about his pragmatic yet bold response to the brief. “But in order to get the number of units they wanted within the setbacks and restrictions, this was the form that came out.”\nThe serpentine structure employs traditional construction—reinforced-concrete columns and structural slabs with bearing walls at the six cores along the length of the building. The only large steel members form the open frame atop the building. One might associate it with the frame on the Casa del Fascio in Como, Terragni’s 1936 masterwork that Eisenman spent decades analyzing, an obsessive research that culminated in the 2003 book Giuseppe Terragni: Transformations, Decompositions, Critiques. (Terragni has five buildings in Milan, designed with Pietro Lingeri, including the famous Casa Rustici  on the other side of town.) But Eisenman insists the open frame here was inspired not by Terragni—not even Le Corbusier—but by the gridded scaffold of his own Wexner Center for the Arts (1989).\nBut, of course, “Eisenman is interested in precedents,” says Degli Esposti. For a specific reference in the city, the architects point to another celebrated residential building, Giovanni Muzio’s Ca’ Brutta (1923), which also famously curves. Like the Ca’ Brutta, Carlo Erba has a tripartite division of its facade, starting with a travertine base. Midway up the building, at the fourth level, the piano nobile—originally intended to be entirely glass but now a mix of solid and transparent walls behind a glass balustrade—recedes slightly, forming a continuous balcony. The top portion, or attico, is Carrara marble. “Milan has a layered look,” says Eisenman. At Carlo Erba, the transition from the warm, rough travertine to the cool, smooth marble is subtle but significant.\nThroughout the building, there is a play between structure and volume, solid and void. But it is a series of straight lines—even within this curving form—that unites everything, extending also to the landscape: on the long, bulging, east face of the building, the windows are pushed forward; on the opposite, mainly concave face, they recede. “On one side the grid prevails, on the other it is excavated,” explains Degli Esposti. The southern end of the building adeptly incorporates the exterior walls of an earlier structure that faces the historic Piazza Carlo Erba, from which the project gets its name.\nThe snaking form comprises four blocks distributed between the six cores that reach as high as nine stories and step down several floors to create two- or three-story “urban villas,” as Degli Esposti calls them, within the open steel framework at the top. Elevators and stairs within the cores service just one to three apartments per floor, eliminating corridors within the 80-unit building, which includes a basement and additional sub-levels for parking and mechanicals. Like Alvar Aalto’s similarly undulating Baker House dormitory at MIT, many of the units at Carlo Erba, though obviously much bigger, are wedge-shaped. Here, however, without long corridors, the sense of the curve is only somewhat palpable from the interiors (left completely bare for occupants to fit out). It becomes intense once you step onto the balconies or upper terraces.\nThe form of the curve developed in part as a gesture to connect the public garden across the street to this building, creating a nontraditional open courtyard. But city officials had other ideas. “The urban landscape commission understood why our building didn’t conform to the street, but it insisted there be walls on the street,” recalls Eisenman. An early requirement of the commission for two-story walls around the property eventually got reduced to a 10-foot-high fence.\nEisenman’s long-awaited Italian project faced other obstacles, notably the construction halt from 2013 to 2016 when the original contractor went bankrupt. Now over 70 percent sold, move-ins began in July. A stark contrast with the glitzy towers of CityLife and Porta Nuova—those new districts built almost entirely from scratch—the Carlo Erba building is the perfect insertion into this Novecento neighborhood and Eisenman just the right “big name” architect to design it. The completed building is a testament that architecture can be highly contextual and still extremely exciting. For Eisenman, long a polemical thought leader less known for building than for analyzing and debating architectural issues, it indicates an evolution. Says the architect, ironically or not, “I’m a traditionalist.”\nEisenman Architects, 41 West 25th Street, New York, New York 10010 USA; T. (1) 212.6451400\nPersonnel in architect's firm who should receive special credit:\nStructural: Studio d’Ingegneria Associato Ardolino, Bolzano;\nMechanical and electrical: Sistema Group Engineering, Montichiari (BS) (preliminary phase); A.T. Advanced Tecnologies, Rome (execution phase); Studio MGM, Gallarate (execution phase)\nItaliana Costruzioni (Rome): phase 2 and construction completion; CLE - Cooperativa Lavoratori Edili (Bolzano): phase 1\nMaurizio Montagna, Milan www.mauriziomontagna.com\nUnderground waterproof retaining walls by Zementol, Bressanone BZ\nManufacturer of any structural components unique to this project: Frame structure: steel frame by MAP s.p.a., Corsico MI\nMasonry: Alveolater P800 brick blocks (perimeter walls, all the floors)\nMetal panels: White metal sheets (4th floor - piano nobile; kitchens of 5th and 6th floors) by Lilli Systems, Giano dell’Umbria PG\nRainscreen: Marmi Conti di Del Vescovo e Leoni s.n.c., Rome:\nEIFS, ACM, or other: Rigid panels Rockwool, Ventirock Duo, 10 cm\nBuilt-up roofing: Vapor barrier\nElastomeric: Waterproofing elastomeric membrane Flexo S6 by Polyglass s.p.a. Mapei Group, Ponte di Piave TV (terraces, loggias, balconies)\nTile/shingles: Multiquartz by Marazzi Group s.r.l., Sassuolo MO (terraces, loggias, balconies)\nWood frame: Professionale 68 by Falegnameria Aresi, Treviglio BG (Units windows)\nMetal frame: Metal frame windows by Lilli Systems, Giano dell’Umbria PG (Common spaces windows)\nGlass: 5/5.1 glass pane + 15 mm Gas Argon + 4/4.1 acoustic low emission glass pane\nOther: Greenhouses in metal frame and glass by Lilli Systems, Giano dell’Umbria PG\nEntrances: Renovation of the existing historical metal door (Main entrance door);\nMetal doors: Security doors Cilinder.70 by Gasperotti s.r.l., Rovereto TN (Units access doors)\nWood doors: Reversa by Bertolotto s.p.a., Torre San Giorgio CN (Units interior doors)\nSliding doors: Scorrevole a scomparsa TR by Bertolotto s.p.a., Torre San Giorgio CN (Units interior doors)\nFire-control doors, security grilles: Fire-control doors by Puertas Padillas S.L., El Albujon, Cartagena, Murica, Espana\nSpecial doors: Magnum by Novoferm Schievano s.r.l., Camposampiero PD\nUpswinging doors, other: Pratic by De Nardi s.r.l., San Fior TV\nLocksets: Biblo Tecno Design by Comit Maniglie s.r.l., Lumezzane BS\nClosers: Biblo Tecno Design by Comit Maniglie s.r.l., Lumezzane BS\nExit devices: Biblo Tecno Design by Comit Maniglie s.r.l., Lumezzane BS\nPulls: Biblo Tecno Design by Comit Maniglie s.r.l., Lumezzane BS\nAcoustical ceilings: Plasterboard panels\nSuspension grid: Metal frame\nCabinetwork and custom woodwork: Boiserie, counterceiling and inspection compartments doors: custom wood panels by Allestimenti Portanuova s.r.l., Milan (common spaces)\nPaints and stains: CSN Impresa di finiture edili, Alessandria\nWall coverings: Plaster\nFloor and wall tile: Listone 140 Listone Giordano, natural oak finishing, by Margaritelli, Miralduolo di Torgiano PG (all the rooms but the bathrooms)\nSpecial interior finishes unique to this project: Boiserie, counterceiling and inspection compartments doors: custom wood panels by Allestimenti Portanuova s.r.l., Milan (common spaces)\nInterior ambient lighting: Continous LED Osram Linear Light Flex 800 Warm White by Simes, Corte Franca BS\nExterior: by Simes, Corte Franca BS:\nDimming system or other lighting controls: ZIP80 by Sunbreak s.r.l., Cusignana di Giavera del Montello TV\nElevators/escalators: MonoSpace 500 by Kone, Pero MI\nsink, wc, bidet: Sfera54 by Ceramica Catalano s.p.a., Fabrica di Roma VT;\nEnergy management or building automation system: Heating/cooling radiant floor powered by heat pumps;\nOther unique products that contribute to sustainability: Geothermal energy system.\nAdd any additional building components or special equipment that made a significant contribution to this project: Electrical sub-contractor: Pierre s.r.l., Martinengo BG""]"	['<urn:uuid:b41e2167-9215-4571-9afa-fdccc10a3623>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	33	109	1705
2	nanoparticle sample preparation steps and characterization procedures	Nanoparticle sample preparation requires careful steps including filtering the buffer with appropriate molecular-weight cutoff to eliminate background debris, avoiding vigorous mixing that creates interfering air bubbles, and using sonication to prevent particle clumping. The concentration must be properly titrated to determine optimal working range and prevent aggregation. For characterization, samples undergo multiple procedures: in-vitro tests including drug entrapment efficiency measurements, release studies using dialysis or dissolution techniques, sterility checks via membrane filtration and culture media incubation, and blood contact properties evaluation. Additionally, cell uptake, distribution studies, and toxicity assessments are performed, followed by in-vivo testing for dose-response, biodistribution, and safety evaluation.	['How Violet Side Scatter Enables Nanoparticle Detection\nGeorge C. Brittain, Ph.D., Sergei Gulnik, Ph.D., Beckman Coulter Life Sciences, Miami, FL 33196\nThe detection of sub-micron particles by flow cytometry becomes increasingly difficult as the particle sizes progress smaller than the wavelength of the light being used to detect them. In addition, the amount of light scattered by any particle is directly proportional to the diameter of the particle and inversely proportional to the wavelength of the light being used to detect it. This relationship can be seen in the equations for both Mie Theory and Raleigh Light Scattering, which are used for calculating theoretical light scattering by particles either similar in size or much smaller than the wavelength of the light being used to detect them, respectively (Bohren & Huffmann, 2010). For this reason, the smaller violet (405 nm) wavelength will result in more orthogonal light scattering at any given particle size than the blue (488 nm) wavelength, and will increase the range of resolution to smaller particles than can be detected by standard side scatter. Moreover, upon entering a medium of a different refractive index, light waves are refracted by the new medium inversely proportional to the wavelength of the light, with smaller wavelengths having a higher refraction than larger wavelengths. This effect was first discovered by Isaac Newton when he split white light into a rainbow of individual colors using a prism, with red light refracting the least and violet light refracting the most (Figure 1) (Newton, 1704).\nBased upon this physical property, the use of violet light will help to amplify the difference in the refractive indices between the particles and their surrounding media, and in turn increases the ability to detect particles with a lower refractive index, such as exosomes, microvesicles and silica nanoparticles. The purpose of this paper is to demonstrate how to setup the CytoFLEX flow cytometer to detect small particles by Violet Side Scatter (VSSC).\nFigure 1. A simplified depiction of the Newtonian light refraction through a cell based upon wavelength.\nSetting up the CytoFLEX to Detect Violet Side Scatter\nSetting up VSSC detection on the CytoFLEX is easy. In the CytExpert program, simply select the Cytometer tab within the Menu Bar, and then select Detector Configuration. Within the Detection Configuration pop-up window, replace the 450 nm filter with the 405 nm filter, and appropriately label the detector as VSSC. Once you save the detector configuration, you select that configuration to be the current setting, and physically move the 405 nm filter into the 450 nm slot within the instrument. At this point, you will need to start a new experiment within the CytExpert program for the new detector configuration to apply, and you are ready to go.\nOnce running, in order to properly separate nanoparticles from background noise using VSSC, you will need to open up the Acquisition Settings Menu, select VSSC as the primary trigger (Height is better than Area), and then manually adjust the trigger level until you reach the discrimination threshold between the noise and actual events. This trigger level is usually very consistent with the CytoFLEX at any given laser intensity and gain setting, both between experiments and on different days.\nSeveral considerations to keep in mind are that you will need to adjust the range of the VSSC histogram in order to hone in on the size range of interest since the default chart settings will have the smaller particles pushed up against the y-axis; once within that range, a logarithmic scale will provide for a better distribution of multi-modal particle sizes; and, if the scaling for the Counts axis is selected to “Fit to Sample”, it will always re-scale the data to fit to the entire histogram. The latter issue can create confusion particularly if you set the threshold trigger using buffer alone rather than determining it empirically using the smaller particles of interest. In this case, the histogram for the noise will continue to readjust to the maximum regardless of where you place the trigger, and without the particles as a reference, the chosen trigger setting may actually cut off or eliminate the particles of interest when you begin testing. It is best to set the trigger using the smallest detectable particles that you have available, which can clearly be discriminated from the background noise. Using these, you can hone in on the real sample population, and then back the trigger off until you are comfortable with the level of noise that remains (Figure 2). However, if the chosen threshold level is too low, you will have an increased abort rate due to the noise. Ultimately, you should try to keep the abort rate below 5-10 % either by adjusting the threshold, the sample concentration, or the flow rate. Keeping the sample concentration and flow rate low is also important to prevent experimental artifacts due to coincidence (i.e., swarming) (Nolan, 2015; Nolan & Stoner, 2013).\nFigure 2. An example of the separation between a real sample population and background noise while setting the VSSC trigger threshold. This sample is 100 nm polystyrene beads at a 1:100K dilution in 0.02 mm-filtered water.\nConsiderations for Sample Preparation\nThe analysis of micro- and nanoparticles requires some careful preparation of the samples to be analyzed. First, the buffer that the samples are suspended in should be filtered with an appropriate molecular-weight cutoff in order to eliminate any background debris that may fall within the range of your populations of interest. Second, as is commonly mentioned in protocols and data sheets, the samples should not be mixed in such a vigorous manner as to create air bubbles immediately prior to reading. These bubbles will interfere with data acquisition, as they\nalso refract light and will be detected as events. Third, a lot of small synthetic particles have the tendency to clump together, resulting in the formation of aggregates that may be either inaccurately sized or aborted altogether. In the case of either bubbles or aggregation, sonicating the samples prior to reading can help. After sonication, the samples should be read as quickly as possible because aggregation will begin anew and the sample population(s) will be affected over time (Figure 3). If sonication is not acceptable, such as with biological samples, then simply triturate the samples as well as possible in order to evenly distribute the sample prior to acquisition. Finally, the concentration of the sample should also be titrated in order to determine the optimal working range for acquisition by flow cytometry, as higher concentrations will lead to aggregation and/or swarming (Figure 4).\nFigure 3. The reduction in signal of a polystyrene nanoparticle population over time. These samples are 100 nm polystyrene beads at a 1:100K dilution in 0.02 mm-filtered water read at 0, 30, or 60 minutes after sonication.\nFigure 4. Finding the optimal concentration for the nanoparticle sample. These samples are 100 nm polystyrene beads in 0.02 µm-filtered water at a dilution of 1:1K, 1:10K or 1:100K from the 1 % stock solution. From these results 1:100K could clearly be seen to provide both the best signal and least aggregation or swarming of the dilutions tested.\nTesting the Performance of the CytoFLEX with Nanoparticle Detection by Violet Side Scatter\nThe objective of the following experiment was to demonstrate the ability of the CytoFLEX to detect nanoparticles smaller than 200 nm by the use of VSSC.\nInstrument and Reagents\n|3-Laser CytoFLEX||B53000||Beckman Coulter|\n|CytoFLEX Sheath Fluid||B51503||Beckman Coulter|\n|Whatman Anotop 25 0.02 μm Filteres||09-926-13||Thermo Scientific|\n|40 nm Polystene Beads NIST||09-980-015||Thermo Scientific|\n|100 nm Polystene Beads NIST||09-980-021||Thermo Scientific|\n|200 nm Polystene Beads NIST||09-980-024||Thermo Scientific|\n|Multimodal Particle Size Standards||MM-010||Thermo Scientific|\n|94 nm Silica Beads NIST||147020-10||Corpuscular, Inc.|\n|150 nm Silica Beads NIST||147030-10||Corpuscular, Inc|\n|200 nm Silica Beads NIST||147040-10||Corpuscular, Inc|\nWe first compared the sensitivity of nanoparticle detection by 405 nm VSSC to standard 488 nm SSC using Duke Multimodal Particle Size Standards with a mix of 80 nm, 200 nm and 500 nm polystyrene beads. As can be seen in Figure 5A, the VSSC could easily discriminate the smaller 80 nm and 200 nm particles, while SSC did not begin to discriminate the populations until between 200 nm and 500 nm. Determining the signal-to-noise ratios of the different populations detected by FSC, SSC, or VSSC, the VSSC can be seen to perform better than SSC at all measured particle sizes (Figure 5B). By histogram, you can more clearly see the population densities and that the CytoFLEX using VSSC was easily able to discriminate polystyrene nanoparticles down to 80 nm using the Duke Multimodal Particle Size Standards (Figure 6). Polystyrene particles as small as 40 nm were able to be clearly detected above background, but they were located right on the noise threshold (Figure 7). Since the polystyrene beads have a refraction index of 1.5915 (at 589 nm), we also tested silica nanoparticles with a refraction index of 1.4584 (at 589 nm), closer to the range for exosomes and microsomes, which have a mean refraction index of around 1.426 (Gardiner et al., 2014). As a result, we were able to easily discriminate 150 nm and 200 nm silica nanoparticles, while 94 nm silica nanoparticles were detectable but on the noise threshold (Figure 8).\nComparing the sensitivity of nanoparticle detection by 405 nm VSSC to 488 nm SSC. Duke Multimodal Particle Size Standards (80 nm, 200 nm and 500 nm) in 0.02 μm-filtered water at a 1:10K dilution were analyzed using the CytoFLEX. Dot plot showing VSSC vs. SSC (A). A plot of the signal-to-noise ratios for the different nanoparticles using FSC, SSC, or VSSC. (B).\nThe detection and discrimination of 80 nm, 200 nm and 500 nm polystyrene nanoparticles by VSSC and SSC. These are Duke Multimodal Particle Size Standards (80 nm, 200 nm and 500 nm) in 0.02 μm filtered water at a dilution of 1:1K, 1:10K or 1:100K from the 1 % stock solution detected by 405 nm VSSC (A) and 488 nm SSC (B).\n. Comparison of 405 nm and 488 nm scatter detection of 40 nm polystyrene nanoparticles. These samples are 40 nm or 100 nm polystyrene beads in 0.02 μm-filtered water at a dilution of 1:10K or 1:100K, respectively (1 % stock) detected by 405 nm VSSC (A) or 488 nm SSC (B).\nComparison of 405 nm and 488 nm scatter detection of silica nanoparticles. These samples are 94 nm, 150 nm or 200 nm silica or 100 nm polystyrene nanoparticles in 0.02 μm filtered PBS. The silica nanoparticles were diluted 1:10K, while the polystyrene nanoparticles were diluted 1:100K, from the 1 % stock solution detected by 405 nm VSSC (A) or 488 nm SSC (B).\nUsing VSSC, the CytoFLEX was able to clearly discriminate 80nm polystyrene and 150 nm silica nanoparticles from background noise. Moreover, the detection and discrimination of nanoparticles by VSSC was better than SSC for all particle sizes tested. Based upon these findings, we can confidently say that the CytoFLEX can be used for the detection of extracellular vesicles at least as small as 150 nm by VSSC. It may be possible to use VSSC to detect even smaller vesicles, as previous research has suggested that the refraction index of extracellular vesicles is inversely proportional to the size of the vesicles, with smaller 50 nm vesicles having a refraction index closer to 1.6 (similar to polystyrene) (Gardiner et al., 2014); however, this will need to be determined empirically.\n1. Arraud, N., Gounou, C., Turpin, D., Brisson, A.R (2015). Fluorescent triggering: a general strategy for enumeration and phenotyping extracellular vesicles in flow cytometry. Cytometry Part A, doi: 10.1002/cyto.a.22669. [Epub ahead of print]\n2. Bohren, C.F. & Huffmann, D.R. (2010). Absorption and scattering of light by small particles. New York: WileyInterscience.\n3. Gardiner, C., Shaw, M., Hole, P., Smith, J., Tannetta, D., Redman, C.W., and Sargent, I.L. (2014). Measurement of refractive index by nanoparticle tracking analysis reveals heterogeneity in extracellular vesicles. Journal of Extracellular Vesicles, 3: 25361-25366.\n4. Hielscher, A.H., Mourant, J.R., and Bigio, I.J. (1997). Influence of particle size and concentration on the diffuse backscattering of polarized light from tissue phantoms and biological cell suspensions. Applied Optics, 36(1): 125-135.\n5. Newton, I. (1704). Opticks: or, a treatise of the reflexions, refractions, inflexions and colours of light. Also two treatises of the species and magnitude of curvilinear figures. London: Samuel Smith & Benjamin Walford (Printers to the Royal Society).\n6. Nolan J.P. (2015) Curr Protoc Cytom. 73: 13.14.1-13.14.16.\n7. Nolan, J.P, & Stoner, S.A. (2013). A trigger channel threshold artifact in nanoparticle analysis. Cytometry Part A, 83(3): 301-305.\n8. Poncelet, P., Robert, S., Bouriche, T., Bez, J., Lacroix, R., Dignat-George, F. Cytometry Part A doi: 10.1002/ cyto.a.22685. [Epub ahead of print]', 'In vitro and in vivo characterization of Nanoparticles\nFor in-vitro characterization, NPs may be assayed in biological matrices such as blood, plasma, cells, or primary culture. Some in vitro tests that may be performed include\nDrug entrapment efficiency: To find the drug entrapment efficiency solid nanoparticles are solubilized with proper solvent and the drug is separated from the supernatant containing excess by ultra-centrifugation. One ml of the supernatant is withdrawn and suitably diluted with proper solvent. The absorbance of the solution was measured at desired wavelength spectrophotometrically and the drug concentration in the supernatant was obtained by comparison with the analytical curve previously prepared. The amount of drug entrapped in the nanoparticles was obtained by subtracting the quantity of drug in the supernatant liquid from the total amount used for the preparation\nIn-Vitro drug release studies: The in-vitro drug release study of the prepared drug loaded nanoparticles was performed using the dialysis bag diffusion technique or dissolution technique. In case of diffusion cell analysis the prepared nanosuspension was placed in a dialysis bag, and dropped into beaker containing specific volume diffusion medium maintained at 37C under continuous magnetic stirring. At fixed time intervals the sample of the receptor medium was withdrawn and replaced with fresh solvent media. One ml of sample was withdrawn at periodic time intervals. The samples were analyzed spectrophotometrically at particular wavelength. The percentage of drug released at various time intervals were calculated from the calibration curve constructed previously\nFig 1:Diffusion cell for dissolution studies\nFor dissolution studies the sample are mainly kept in different dissolution apparatus USP I and USP II. At fixed time intervals the sample is withdrawn for UV and HPLC analysis and the sample volumes to be replaced with fresh solvent media.\nSterility check (e.g., testing for the presence of bacteria, virus or mycoplasma)\nSterility is an important for both in vitro and in vivo application of nanoparticles for various drug and gene delivery. For clinical treatment of the patients parenteral drug have to meet the pharmacopoeia requirement of high degree of purity. The chemical or physical stability of the polymer matrix usually limits most conventional methods for obtaining acceptable sterile products. The drug loaded nanoparticles were evaluated for its sterility by sterility testing as per different Pharmacopeia. The method involves the membrane filtration technology followed by incubation of membrane in culture media containing nutrients broth of Soya bean casein digest medium (SCDM) and Fluid thioglycollate medium (FTM). After filtration of the solution, the membrane was rinsed 3 times with sterile peptone solution. The membrane was then separated cut into two halves. One half of the filter paper was kept into the container with SCDM and the other half was kept into the container with thioglycollate medium. SCDM containers were then incubated at room temperature and FTM containers at 32.5+-2.50C. The containers were monitored for any turbidity or appearance of growth of microorganisms for 14 days. Positive control and negative control tests were also executed to validate the sterility testing procedure.\n* Blood contact properties: Blood nanoparticles interaction is an important characteristic to evaluate. The following parameters like plasma protein binding, hemolysis, coagulation, complement activation, cytotoxic activity of NK cells\n* Cell uptake and distribution studies include cell binding, NP internalization and receptor targeting.\n* Toxicity studies including phase I/II enzyme induction or suppression studies, oxidative stress, apoptosis testing and also necrosis testing. The disadvantages of in vitro testing is that amount of information obtained is limited. In addition, in-vitro testing for biomedical/clinical applications (e.g., therapeutics, in vivo diagnostics) has to be tested in animal models. In vivo assays can supply essential information regarding what may happen when the NPs are inside the body. Some in vivo tests which are important in nanoparticles analysis include dose-response; biodistribution, acute and multi dose safety and efficacy, administration route determination; and absorption, distribution, metabolism, and excretion (ADME). The ultimate goal of in vitro and in vivo testing is to match the physicochemical parameters of the NP to its biological function']	['<urn:uuid:befa08ed-b024-4d4e-a099-b0706b86674c>', '<urn:uuid:59dab4e7-6366-4cc3-99c0-4665d1835115>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	7	101	2767
3	jonathan howard game writing experience and modern simulation games ratings metrics	Jonathan L Howard worked as a scriptwriter and video game writer in the 1990s, with writing credits on the successful Broken Sword series. According to the game rating metrics, simulation games need to achieve at least a Critic Score of 80 and a User Score of 4.0 (with minimum 5 votes) to be considered top rated. Some of the highest rated simulation games include Elite with a 96 critic score and Napalm: The Crimson Crisis with a 4.78 user score.	"['Adaptation - Book to comic to film to game\nUpdated: Sep 3, 2019\n7pm, 23 October\nCan a stream of consciousness novel be adapted into a thrilling film? Can graphic novel characters be imported into a novel? Can you make a video game of Pride and Prejudice?\nJoin our panel of experts as they talk you through what skills and techniques are needed to adapt a work from one format to another, maybe even one genre to another. Bring your notebooks and learn from masters of the craft.\nHosted by Timon Singh.\nJonathan L Howard http://www.jonathanlhoward.com/\nStephen Volk http://www.stephenvolk.net/\nCavan Scott http://cavanscott.com/\nCheryl Morgan https://www.cheryl-morgan.com/\nThe panel would be very happy to sign books at the end of the discussion.\nCheryl Morgan is an award-winning critic and editor. She owns Wizard\'s Tower Press, which publishes anthologies by local writers. Cheryl is regularly invited to give talks on science fiction. In the past year she has given a keynote speech at an academic conference in Graz, Austria; has been a Guest of Honour at the Finnish National Science Fiction Convention; and has delivered a talk on the Prehistory of Robotics at the World Science Fiction Convention in Dublin. You can find her on Twitter as @CherylMorgan or read her blog, Cheryl\'s Mewsings.\nTimon Singh is the Campaigns Manager (Marketing, Communications and Audiences) for Film Hub South West helping to market new releases for the BFI’s Film Audience Network. He also runs the Bristol Bad Film Club, a monthly genre film night in the city, is a regular contributor to The Cosmic Shed podcast (voted one of The Guardian\'s favourite podcasts of 2018) and wrote the book Born To Be Bad: Talking to the greatest villains in action cinema where he interviewed the actors behind some of the greatest movie bad guys.\nStephen Volk, a BAFTA-winning writer, is best known for BBCTV’s notorious ""Halloween hoax"" Ghostwatch and the ITV drama series Afterlife starring Andrew Lincoln and Lesley Sharp. His screenplays include The Awakening and Gothic, but along the way he has adapted such books as James Herbert’s Haunted, John Wyndham’s The Chrysalids, John Masefield’s Box of Delights, and Phil Rickman’s Midwinter of the Spirit as a three-part miniseries for ITV. He is also author of several short story collections including The Parts We Play and The Dark Masters Trilogy. He is presently writing a new TV adaptation of Jekyll & Hyde.\nCavan Scott is an author and comic book writer. He is a UK number one bestseller who has written for such popular worlds as Star Wars, Doctor Who, Star Trek, Vikings, Judge Dredd, Pacific Rim, and Sherlock Holmes. He is the author of Star Wars Dooku: Jedi Lost, The Patchwork Devil, and Cry of the Innocents, and is part of Lucasfilm’s Project Luminous publishing initiative. He has written comics for IDW, Dark Horse, Vertigo, Titan, Legendary, 2000AD, and the Beano. A former magazine editor, Cavan lives in Bristol with his wife and daughters. His lifelong passions include classic scary movies, folklore, audio drama, the music of David Bowie, and walking. He owns far too many action figures.\nJonathan L Howard worked as a scriptwriter and video game writer in the 1990s, including writing credits on the highly successful Broken Sword series of video games. His first novel, Johannes Cabal the Necromancer, was published in 2009. Today he lives with his wife and daughter near Bristol.\nAddress: Storysmiths, 49 North St, Bristol, BS3 1EN', ""Managerial / Business Simulation\nDescriptionManagerial and Business Simulation games mold management of people and resources and economic business activities in a game format. Usually the aim of the game is to make money through strategic uses of the available resources and to try to grow as a company. Usually the individual management of employees of the player's company plays a large role.\nHollywood Pictures 2 is a managerial simulation were the player takes care of a company which produces movie pictures. The...\nIn Homeland Defense: National Security Patrol the player constructs and manages a post at the Mexican/American border. The main goal...\nThis game puts the player in the holey shoes of a street person, navigating the intersections and back lanes (with...\nFantasy baseball Home Run Derby. It's you versus the best in the league in a Home Run Derby.\nA new super store has moved into the neighborhood. They are so big, they can sell items cheaper than you....\nHorse Racing Manager 2 is the sequel to Horse Racing Manager. In this management game you can choose from three...\nAtari ST (1990)\nHorse Racing Simulator is a management game where the player manages his own racing horse stable. Buy and sell horses,...\nHospital Tycoon is a level-based hospital management simulation where players can start on an empty building, and then assemble rooms...\nIn The Hotdog Parable you are a street food seller. You can research and then upgrade your food selection and...\nHotel City is a simulation game where the player builds and manages a hotel, adding and decorating rooms, offices, shops,...\nYou are a hotel manager and you can now prove your manageing skills within the hotel industry. You have to...\nIn Hotel Giant 2 the player is responsible for managing a hotel. As usual with these kind of games the...\nNintendo DS (2008)\nIn Hotel Giant DS the player is responsible for managing a hotel. In principle the game is played similar to...\nHotel Tycoon is a managerial game in which the player takes on the role of a tycoon in charge of...\nHot Rod: American Street Drag is a racing game with a tie-in to Hot Rod magazine.The game can be played...\nTurboGrafx CD (1994)\nAfter humans ventured into outer space, they began building colonies there. It became increasingly hard to maintain them, and special...\nNintendo 3DS (2014)\nTop games by as rated by the press:\n- 96 Elite\n- 94 Sid Meier's Railroad Tycoon\n- 93 Sid Meier's Civilization II\n- 92 Sid Meier's Civilization IV\n- 92 Sid Meier's Civilization II\n- 92 Populous\n- 91 Populous\n- 91 NCAA Football 2004\n- 91 NCAA Football 2003\n- 91 Game Dev Story\nGames must have at least a Critic Score of 80 to be considered.\nTop games as rated by our users:\n- 4.78 Napalm: The Crimson Crisis (1998)\n- 4.70 Die Fugger II (1996)\n- 4.62 Segagaga (2001)\n- 4.49 NASCAR Racing 4 (2001)\n- 4.35 Spaceward Ho! IV (1996)\n- 4.30 SimCoaster (2001)\n- 4.29 Herrscher der Meere (1997)\n- 4.27 PC Fútbol 7 (1998)\n- 4.21 The Sims 2: Open for Business (2006)\n- 4.20 80 Days (2014)\nGames must have at least 5 votes and a User Score of 4.0 to be considered.\nManagerial / Business Simulation is part of Gameplay. This group of genres contain:\n- Action RPG\n- Beat 'em up / Brawler\n- Board Game\n- Cards / Tiles\n- City Building / Construction Simulation\n- Dating Simulation\n- Falling Block Puzzle\n- Game Show / Trivia / Quiz\n- Graphic Adventure\n- Hack and Slash\n- Hidden object\n- Interactive Book\n- Interactive Fiction / Text Adventure\n- Japanese-style Adventure\n- Japanese-style RPG (JRPG)\n- Life / Social Simulation\n- Managerial / Business Simulation\n- Martial Arts\n- Mental training\n- Music / Rhythm\n- Paddle / Pong\n- Party Game\n- Quick Time Events (QTEs)\n- Rail Shooter\n- RPG Elements\n- Sandbox / Open World\n- Survival Horror\n- Tactical RPG\n- Tactical Shooter\n- Tile Matching Puzzle\n- Timed Input\n- Time Management\n- Tower Defense\n- Trading / Collectible Card\n- Tricks / Stunts\n- Vehicle Simulator\n- Vehicular Combat Simulator\n- Virtual World\n- Visual Novel\n- Word Construction""]"	['<urn:uuid:2c6ff146-210b-4a60-8511-5054d41b7f66>', '<urn:uuid:6956e853-d9cf-4a01-b851-cd0be0fb9403>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	11	80	1280
4	names famous artists permanent collection new goulandris museum athens	The new Goulandris Museum in Athens will exhibit works by artists including Edgar Degas, Auguste Rodin, Vincent van Gogh, Paul Gaugin, Claude Monet, Paul Cezanne, Pablo Picasso, Georges Braque, Joan Miro, Alberto Giacometti, Jackson Pollock, Francis Bacon, Andy Warhol, Gerhard Richter, and Anselm Kiefer.	"[""The blue cultural germ of the Aegean - Exclusive Interview with the Director of the MOCA, Dr. Kiriakos Koutsomallis\nEBR's 'Extrait de Culture' winds up the cultural wealth of an island museum of great admiration in Andros. MOCA transports you to the culture, history and knowledge of the art through the pure efforts towards the love of arts.\nMuseum of Contemporary Art of the Basil and Elise Goulandris Foundation, was founded in 1979. Apart from the work of the Andriot sculptor M. Tombros, which constitutes the nucleus of the collection, the Museum includes works of the most important Greek artists. Alongside the Greek collection, which is being constantly enriched, a collection of works by great foreign artists is being put together. In July 1986, the new wing of the Museum was inaugurated, where, to date, international exhibitions of works of Picasso, Kypka, Matisse, Kandinsky, Balthus, Henri Cartier-Bresson, and Paul Klee have been held, as well as exhibitions of works by the well-known Greek artists Bouzianis and Galanis and also, in cooperation with the National Gallery of Athens, K. N. Karagatsis, N. Hadjikyriakos-Ghikas, D. Mytaras, A. Fassianos, G. Gaitis, and O. Zouni.The yearly exhibitions of the museum are a must visit attraction on this blue island in the middle of the Aegean Sea. Dr. Kiriakos Koutsomallis responds vividly to EBR's questionnaire and reminds that art needs to be tender and passionate. An interview that erases our imagination and craving to visit Andros and MOCA, an experience of a lifetime.\nThe Museum of Contemporary Art in Andros has many permanent exhibitions of a great success. “Approaching surrealism”- one of the most popular past exhibitions - was my first thought over that, could you please specify a few points about the criteria of choosing collections and exhibits and what factors make some of them to be some remarkable?\nK.K.: The Museum of Contemporary Art in Andros has a long list of remarkable exhibitions, from our beginning in 1986, exhibiting great artists, such as: Henri Matisse, Wasilly Kandinsky, Balthus, Alberto Giacometti, Paul Klee, Giorgio De Chirico, Auguste Rodin, Camille Claudel, Henry Moore, Henri de Toulouse Lautrec, Joan Miro, Georges Braque, Pablo Picasso, Andre Masson, Paul Delvaux, Isamu Noguchi, Man Ray, and others of great note.\nThe Museum aims towards either presenting artists that are not easily accessible to the people of Greece without having them to travel abroad or to bring a new viewing in artists already known.\nHow has your love for art been originated?\nK.K.: We are on a journey commenced by Basil and Elise Goulandris, the Founders of the “Basil and Elise Goulandris Foundation”. They were art lovers and collectors. Their prime directive was to bring the world of art closer to the Greek audience through their Museums. Following their passing, the Board of Trustees, consistently continue their vision, and stay true to their ethos.\nEBR: How can the separate units of thematic areas into the museum influence the visitors’ preferences? For example, there are some exhibitions or collections more popular than others, how do you handle it?\nK.K.: All temporary exhibitions are carefully selected and hold their ground as an important artistic event.\nThere is a relatively stable number of visitors each year which shows trust in our choices. If I had to mention specifics, on the whole there have been a stable number of visitors across all of our exhibitions, but an increased influx has been noted for the more widely known artists.\nWhat is worth visiting the Museum of Contemporary Art in Andros for the current period?\nK.K.: Artworks from the permanent collection of the Museum, by Greek and foreign artists are on display, until the end of May. On the 1st of July 2018, an exhibition dedicated to the recently departed Greek painter Dimitris Mytaras shall be inaugurated.\nApart from the above, the Museum is situated in a picturesque location next to the sea. Our visitors have stated that this is another reason for visiting our museum in Andros.\nWhat are the future goals for Museum of Contemporary Art in Andros? Furthermore, could you please, inform us about the new opening of MOCA chatter in Athens?\nK.K.: The Museum in Andros shall continue to present important exhibitions with an emphasis in contemporary Greek artists. Simultaneously the new Goulandris Museum in Athens is about to open its doors, hosting the Basil and Elise Goulandris collection which shall exhibit works by Greek and foreign artists. Some indicative names are those of: Edgar Degas, Auguste Rodin, Vincent van Gogh, Paul Gaugin, Claude Monet, Paul Cezanne, Pablo Picasso, Georges Braque, Joan Miro, Alberto Giacometti, Jackson Pollock, Francis Bacon, Andy Warhol, Gerhard Richter, Anselm Kiefer, to mention a few.\nApart from the permanent collection, the Museum will also host temporary exhibitions as well as educational programs, lectures, presentations and numerous other events in the state-of-the-art areas, specifically designed for these purposes.\nHow can visitors be more updated on Museum of Contemporary Art in Andros news?\nK.K.: We make sure that each year we communicate our events via press releases, advertisement in magazines (which are available on the mainland), television spots, radio advertisements, and via the social media, our website as well as posters and leaflets which are also available in shops and hotels of the island. We have significant third-party exposure from media coverage and articles.\nUnder which efforts and funds can large exhibitions be developed?\nK.K.: All exhibitions are realized via our own financing.\nDo you have many activities for kids? Could you make us a brief description?\nK.K.: Every August we have specially designed children's programs which are interactive and motivational. These are always connected with the artist been exhibited.\nEBR: Financial crisis has also a great influence on the Museum of Contemporary Art in Andros affairs?\nK.K.: All facets of Greek life have been adversely affected by the global crisis. Andros is no exception.\nWhich are the reasons for someone to visit the Museum of Contemporary Art in Andros?\nK.K.: We believe that this has already been answered from the above responses.\nWhat is the promotion of the museum abroad?\nK.K.: The power of social media knows no borders. Our museums have been visited by almost all nationalities of this world.\nHow could museums consist the tool-box of the promotion targeting in cultural tourism for Greece?\nK.K.: As a private foundation, we undertake all our exhibitions utilizing our own means. All of our events has attracted significant international attention. We have no hard statistics to supply, but the proof of this lies with the international visitors to our museums.\nIn addition, our interactions and lending programs with other museums based in Europe and the United States of America attest to this.\nIs there a European culture, in your opinion?\nEuropean artists have directly transformed the European culture. This has been ongoing for centuries, spanning from the foundations of Europe till today.\n*Editor-at-large & PhD candidate of European & International Relations""]"	['<urn:uuid:cc908e72-2e21-4cb5-a3f9-ad96ebe606df>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	9	44	1151
5	What do divers use watches and flags for while underwater?	"Divers use watches for precise timing during underwater missions, with features like water resistance and rotating bezels to track elapsed time. For safety, divers must also display a red and white diagonal striped dive flag measuring 12\"" x 15\"" that extends 3 feet from the water surface, and they must stay within 100 feet of this flag while near the surface."	['Once upon a time, the humble diving watch had a single purpose: diving. Nowadays, however, it has become an increasingly popular choice for suit-and-tie office workers and casual weekend warriors alike. We’ve come a long way since a bunch of French frogmen set out on their quest to find a functional tool watch to accompany them on their dangerous underwater missions in the early 1950s.\nThe Original Diving Watch\nMany people assume the first commercially produced diving watch came from Rolex. Given the incredible popularity of their Submariner and Sea-Dweller models, it’s not hard to see why. While it’s true that Rolex—and several other notable brands, such as Omega—have made substantial contributions to the development of mechanical dive watches over the years, it was actually another Swiss watch manufacturer that created the blueprint for the mechanical diving watch we know and love today: Blancpain.\nThese days recreational diving and snorkeling are very popular activities, undertaken pretty much anywhere there is a beach or body of water with something interesting to look at. Back in the early 1950s, it was a different story. At that time, diving was primarily an activity undertaken by elite soldiers—referred to as frogmen or underwater commandos—who needed to stealthily attack key positions or conduct daring raids. The squadron central to our story today is the French “Nageuers de combat” (French commando frogmen), established by the French government in 1952 in the aftermath of World War II.\nFounders Captain Robert “Bob” Maloubier and his Lieutenant, Claude Jean Riffaud, were tasked with picking the squadron members and equipping them with all necessary scuba gear. The only problem was that neither man could find a mechanical watch that was suitable for the specific and rather extreme needs of their men. Given the rather extreme importance of timing in the missions they would be carrying out, this quickly became a pressing issue.\nThe Amateur Diver\nAround the same time, Jean-Jacques Fiechter, then CEO of the centuries-old Swiss watch manufacturer, Blancpain, was working on his own concept for a diving watch. A passionate amateur diver in his free time, a narrow escape from a mistimed dive helped him realize the need for a robust yet easy-to-read timing instrument that could be relied upon even under extreme conditions. He quickly came up with a laundry list of requirements for what would become the first mechanical dive watch: water resistant, self-winding, anti-magnetic, and legible both in cloudy water and at night.\nAs the CEO of a Swiss watch manufacturer, he was uniquely well-placed to test out his ideas in real time, and it was not long before he began to patent some of the solutions he developed. These included a double sealed crown that guaranteed the watch remained water resistant even if the crown was accidentally pulled out during a dive, a rotating bezel with suitable markings that could be read using the minute hand to keep track of elapsed time, and a special locking mechanism that prevented the bezel from rotating mid-dive (and also kept salt and sand out so it would rotate smoothly).\nHe also solved a common issue with screw-down case backs not sealing correctly by placing the “O” ring seal in a channel with an additional metallic ring on top to hold it in position. This patented invention ensured a consistently strong seal when the case back was screwed down.\nEventually, Maloubier and Riffaud heard about Fiechter’s experiments with diving watches and came to him with their specific requirements (having failed to find anyone else capable of producing a watch that met their standards.) The three men quickly discovered they were on the same page in terms of the technical specifications a mechanical diving watch should have. Shortly after their initial discussions, Fiechter was able to deliver 20 watches for rigorous testing.\nPerhaps unsurprisingly, the watches passed with flying colors and were soon adopted as the official watch of the French combat diving corps. Other militaries would soon follow, starting with the Israelis and then the Americans, Spanish, Germans, Danes, Norwegians, Swedes, and Pakistanis. Thus, the legend of the Blancpain Fifty Fathoms was born.\nFollowing this initial success, Blancpain went on to introduce numerous models to the Fifty Fathoms family. Many were in direct response to feedback received from the various militaries using the watch at the time. For example, the Fifty Fathoms MIL-SPEC 1 from 1957-58 featured a water tightness indicator on the dial and became a standard feature of an American combat swimmer’s gear. Now an iconic model, Blancpain introduced the limited edition “Tribute to Fifty Fathoms MIL-SPEC” last year to mark the model’s 50th anniversary.\nToday, the Fifty Fathoms remains a flagship Blancpain collection, capturing that same sense of adventure as the first modern diving watch from 1953. There’s much more variety now, of course, but the main signature elements that forged the reputation of the original model and established it as the archetypal diving watch are still present. Whether you choose a time-honored Fifty-Fathoms Automatic or go for something slightly more modern from the Bathyscaphe series, you can be confident in saying you’re wearing a piece of history on your wrist.', 'Massachusetts Saltwater Fishing\nWhat is a V-Notch?\nA v-notch is a mark on the tail flipper of a female lobster that was put there by a commercial lobsterman as a means to identify and protect a known breeder in the population from harvest.\nCommercial lobstermen make a v-notch in the tail flippers of egg-bearing female lobsters they encounter while fishing. The v-notch remains in the female’s flipper after she has hatched her eggs which protects her from harvest through additional molts. If you choose to voluntarily v-notch egg-bearing female lobsters, we strongly encourage you to purchase a v-notching tool; this tool is available at marine supply stores.\nThe practice of v-notching, as a means of protecting local broodstock, originated in Maine and dates back to the early 1900s.\nWhich Flipper is Notched?\nThe tail flipper immediately to the right of the middle flipper, when the lobster is examined with the underside of the lobster down and its tail is toward the person making the determination.\nIt is illegal to possess any female lobster in which the v-notch flipper is mutilated in a manner which could hide, obscure, or obliterate such a mark.\nSCUBA Regulations for Recreational Lobster and Edible Crabbing\nTo recreationally dive for lobsters in Massachusetts, all divers must hold a Recreational Lobster/Crabbing Permit endorsed for diving. A permit endorsed for diving only covers the individual and does not cover the immediate family. Each individual diver must hold their own permit endorsed for diving. This permit must be carried on the vessel at all times when fishing for lobsters. No permit is required for the taking of edible crabs by diving. The taking of edible crabs is subject to recreational crab regulations (Recreational Lobster & Crabbing Regulations).\nGear and Marking Requirements\nDive Flag – MA law requires each diver or group of divers, while swimming or below the surface, to display a dive flag. The dive flag must be at least 12″ x 15″ in area with a red background and white diagonal stripe. The flag must be displayed on the vessel or on a surface float and must extend at least 3 feet from the surface of the water. Divers are required to stay within 100 feet of the displayed flag while at or near the surface. Divers may display their permit numbers of their dive flag.\nDive Marker – If the permit number for all divers is not displayed on the dive flag, a dive marker must be used. The dive marker must be a 12″ x 12″ floating panel with a white background. The permit number for each diver must be displayed on each side of the marker in numerals measuring 3” in height by ½” in width.\nDive Tank – The permit number must be marked on your dive tank in numerals that measure 3″ in height by ½” in width.\nMethods of Taking Lobsters\nDivers may take lobsters by hand. They may also utilize a tickle stick, which is a straight or slightly bent stick used to agitate a lobster to come out of its hole. The use of spears, snares and dipnets is prohibited (Recreational Lobster & Crabbing Regulations).\nThere is a limit of 15 lobsters per day per permit, and all lobsters must meet the legal requirements. Divers are required to measure all lobsters in possession prior to surfacing.\nTaking of blue crabs and other edible crabs is prohibited from January 1 through April 30.\nThere is a limit of 25 blue crabs per day.\nThere is a combined limit of 50 edible crabs (e.g., blue and Jonah) per day.']	['<urn:uuid:15de71ad-1916-47a0-8220-66fc026b77cc>', '<urn:uuid:60c16be5-f1c6-49c8-808a-64857fbbebad>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	10	61	1454
6	How many meals did Lodi Unified serve last summer?	Lodi Unified School District's Nutrition Services department served approximately 40,000 meals last summer for six weeks in June and July to students under the age of 18.	['- Featured Businesses\n- Work Life\nWith school out for summer, food service programs continue to provide meals for needy students.\nLodi Unified School District’s Nutrition Services department served approximately 40,000 meals last summer for six weeks in June and July to students under the age of 18. Of these meals, 14,000 were provided to children not enrolled in summer school and at locations other than school campuses.\nAccording to Nancy Rostomily, Director of Nutrition Services for LUSD, the program’s goal is to provide healthy and nutritious meals to the area’s youth while school is out.\n“The food is prepared and served fresh by the Nutrition Services Department daily as part of the Federal Seamless Summer program that provides meals in low income areas during the summer,” Rostomily said. “The program combines features of the National School Lunch Program (NSLP) and the School Breakfast Program (SBP) that are offered during the school year.”\nThis summer’s program runs from June 5 through July 14.\nTo reach as many hungry children as possible, Lodi Unified reaches out beyond summer school students and partners with organizations such as the Salvation Army, YMCA, parks and recreation departments and the Boys and Girls Club.\n“Parents usually are looking for programs and activities for youth in the summer and this makes a perfect partnership. They provide an activity and we provide the food,” Rostomily said.\nHungry students are also enrolled at colleges and universities as well.\n“UC Merced’s food security program is linked to the UC system-wide food security goals to provide enough good, nutritional food for our students and to educate them around what that means, what that looks like for them,” said Vernette Doty, Associate Director for the Office of Student Life and Civic Leadership, University of California, Merced.\nAs part of the University of California’s Global Food Initiative, the focus is on students and their education of and access to food on campus.\nApproximately 200 to 300 students take advantage of the various food security services on campus each month.\n“These are low numbers relative to need. Students are not aware, that’s the main thing. Being able to increase our efforts to get the word out is going to help a lot,” Doty said.\nOff campus, Doty overseas a food pantry which is another vehicle for food distribution. The pantry distributes food on the third Friday of each month at Merced College and its goal is to enable anyone in need access to healthy food.\n“We started it specifically for our students but as a USDA distribution it has to be open to anyone, and so, gradually we began to see more and more community members come out to the distribution,” Doty said.\nCurrently, about 60 percent of the food is distributed to community members and 40 percent to students, a ratio that skews more towards non-students during the summer when the student population in the area drops.\n“To me it’s really important that everybody — students, community members, anyone — is treated exactly the same way, gets the same food, the whole thing,” Doty said.\nBetween 250 to 350 community members sign the USDA distribution sheet each month at the food pantry.\n“I can tell you that means between 500 and 700 people total when you count their families are benefiting from that distribution,” Doty said.\nThe food distributed through the food pantry is coming from the USDA primarily through the Merced County food bank, with occasional private donations from local gardens and farms. The UC Merced dining services also donates fresh produce to the distribution about three times a year.\nOther organizations help support the community food distribution through the food pantry.\n“We have a grant through United Way, so we partner with them,” Doty said. “We have also partnered with Image Masters. They donated some of our first round of reusable grocery bags and our food pantry tee shirts.”\nOn campus distribution is funded through the UC-wide Global Food Initiative with opportunities for students to access food about two times a month.\n“Those funds have to stay on campus and they go back to the students,” Doty said.\nHelping needy families find access to food is not limited to educational institutions. The Health Plan of San Joaquin (HPSJ), a not-for-profit health plan, does not provide food directly to its members but helps them find resources.\n“When food help is requested at any time during health care-related contacts with HPSJ staff, including our health navigators and customer service representatives, each inquiring member is given a warm hand-off to an HPSJ social worker,” said David Hurst, Vice President for External Affairs, HPSJ.\nThe social worker then identifies the most appropriate local food organizations from a resource list.\nThe health plan also works to get information quickly to its members via various forms of media about free school meals available in the area.\n“While health promotions are indirect, no child should go hungry in the summer because his family may not have heard about this tremendous resource, available in a school near them,” Hurst said.']	['<urn:uuid:1cc67421-84f9-42b1-a1da-50f4799346d3>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	9	27	842
7	molecular weight effects on polyacrylamide floc stability	Improving the molecular weight of polyacrylamide contributes to the improvement of floc stability. The molecular weight affects floc size, which is important because flocs that are too small impact drainage speed, while flocs that are too large bind more water and reduce clay biscuits.	['Cationic polyacrylamide (CPAM) is a linear polymer, because it has a variety of active groups, can be compatible with many substances, adsorption, forming hydrogen bonds. Mainly flocculation the negatively charged colloid, with the removal of turbidity and decoloring, adsorption, adhesion and other functions, suitable for dyeing wastewater, papermaking, food, building, metallurgy, mineral processing, coal, oil, aquatic products processing and fermentation of organic colloid content is higher, especially suitable for dewatering sludge, sewage, city city papermaking sludge and other industrial sludge.\n1、used for sludge dewatering sludge according to the corresponding types of properties can choose this product, can be effective before the sludge into the filter press sludge dewatering, dehydration, floc, non stick cloth, filter without loose, the flow of mud cake thick, high dewatering efficiency, the cake moisture content below 80%.\n2、for domestic sewage and organic wastewater treatment, this product in the coordination or alkaline medium are positive electricity, so that the sewage suspended particles with negative charge of sewage flocculation, sedimentation, clarification is very effective. Such as the production of grain alcohol wastewater, papermaking wastewater, waste water, city sewage treatment plant wastewater, monosodium glutamate wastewater, sugar wastewater, organic wastewater, high content of feed wastewater, textile printing and dyeing wastewater, with cationic polyacrylamide than with anionic, nonionic polyacrylamide or the effect of inorganic salts to several times or dozens of times, because this waste water is generally negative charge.\n3、The flocculant for the treatment of tap water with river water as the source of water has the advantages of less dosage, good effect and low cost, especially with inorganic flocculants. It will become a high effective flocculant for the waterworks in the Yangtze River, the Yellow River and other river basins.\n4、Reinforcing agent and other auxiliaries for paper making. Increase the retention rate of filler and pigment and the strength of paper.\n5、Used as auxiliary agent for oilfield research, such as clay anti swelling agent, thickening agent for oilfield acidizing.\n6、used in textile sizing agent, slurry performance is stable, less pulp, fabric breakage rate is low, the cloth surface is smooth.\nMatters needing attention in cationic polyacrylamide:\n1、floc size: floc too small will affect the speed of drainage, floc too general to make floc bound more water, and reduce clay biscuits. The size of flocs can be adjusted by selecting the molecular weight of polyacrylamide.\n2、Sludge characteristics. The first point is to understand the source, characteristics and composition of the sludge, and the proportion of the sludge. According to the different properties, the sludge can be divided into two kinds of organic and inorganic sludge. Cationic polyacrylamide for disposal of organic sludge, anionic polyacrylamide relative for inorganic sludge, strongly alkaline with strong acidic cationic polyacrylamide, and should not be used when anionic polyacrylamide, solid content of sludge is usually large amount of polyacrylamide.\n3、floc strength: floc should be stable and not broken under shear action. Improving the molecular weight of polyacrylamide or selecting suitable molecular structures contribute to the improvement of floc stability.\n4、the ion polyacrylamide: for the dehydration of the sludge, available in different degrees after ion flocculant do first test stop selection, select the best suitable polyacrylamide, so as to get the best flocculant dosage effect, and can make the least cost savings. 5, the dissolution of polyacrylamide: dissolved good talent, fully play the role of flocculation. Sometimes the demand accelerates the dissolution rate, and then the concentration of progressive polyacrylamide solution can be considered.']	['<urn:uuid:84e970d3-42e1-47a8-9427-6dbec25a2c06>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	7	44	567
8	flow measurement device sensitivity time degradation	"Meter sensitivity decreases with age. While a typical residential meter (5/8"" or 3/4"") will only record usage when the flow rate is greater than one pint of water per minute, a 10-year-old meter typically only records flows greater than 2 pints per minute, depending on usage and water quality. This reduced sensitivity affects both water conservation efforts and water purveyor revenues, as unrecorded water flow represents a cost without revenue."	"['Revenue and Consumption Factors\nInstalling meters and billing according to usage is the single most effective water conservation measure a water utility can initiate. While many water purveyors are fully metered, there are surprisingly many water agencies still charging customers flat rates, even in water scarce regions of the US. As measured by utilities recently, unmetered water consumption is reduced 15% to 30% when metering and commodity rates are implemented.\nMeters that meet American Water Works Association (AWWA) standards and state regulatory requirements cost between $40 (residential size) to $2,000+ (industrial size). The cost to install a meter where none exists can range between $180 to $800+ per meter, depending on the location of the service line and affected hardscapes. It is more costly to install meter boxes and meters in established areas where street, sidewalks and landscape must be excavated and replaced. Many agencies implementing metering, have learned to schedule meter installations prior to regularly schedule street repairs and resurfacing.\nWater agencies should be prepared for reductions in water consumption after metered billing is implemented. The new volumetric water pricing should be based on the expected reduction of water usage and designed to cover the general overhead costs of the water purveyor. This requires the purveyor to carefully calculate the fixed costs versus avoided costs of conservation.\nLearn about ""smart"" metering and automated meter infrastructure here.\nSizing, Accuracy and Sensitivity\nProper meter sizing and type is important both for water conservation and correct revenue generation for a water utility. The inside diameter of the water supply pipe is used to classify meter size. Meters vary in sensitivity by size and design. Improper meter selection for the customer usage pattern can cause the meter to not register the full amount of water consumed.\nSingle-family homes typically use a 5/8” meter (1.59 cm), though 3/4” (1.91 cm) or 1” meters (2.54 cm) are also used for larger homes. Small commercial businesses generally need meters sizes of 5/8” to 1.5” (1.59 cm to 3.81 cm). These meters are usually the “positive-displacement” design. This design of meter is reasonably accurate at the normal flows of a home or small business. The mechanisms of positive-displacement meters unduly impede water flow where high velocity is needed, such as larger commercial, institutional and industrial facilities. In general, the larger the meter, the less sensitive the meter is to small volumes of water passing through the mechanism. A typical residential meter (5/8” or 3/4” (1.59 cm or 1.91 cm)) will only record usage when the flow rate is greater than one pint of water per minute. Meter sensitivity decreases with age; a ten year old meter might be only record flows greater than two or three pints per minute.\nIf the facility requires greater water flow, the meter size will range from 1.5” to 12” (3.81 cm to 30.5 cm), using a “multi-jet” or “turbine” design. Often a large facility, such as a hospital, office building, school or factory will need several meters for water supplied to different zones or uses. These types of meters actually measure the velocity of the water, not the volume. The meter is calibrated to record the volume, calculated by multiplying the measured water velocity by the area of chamber the water passes through. These meters are very accurate at high flows, but the sensitivity is less than 1 to 3 gallons per minute (3.78 LPM to 11.35 LPM) at low flows. This loss of sensitivity at low flows can cause a water utility to lose revenue when the meter fails to record water usage. This problem is most acute where water flows are highly variable during any given 24 hour period. For example:\nA 50-unit apartment building with a single master meter requires a meter large enough to handle a peak flow of more than 500 gallons per minute (gpm) (1.8 m3 per minute), usually occurring at 7:00 am when everyone is taking morning showers and preparing breakfast. Positive displacement type meters would severely impede the water flows needed to meet this peak demand. Conversely, the typical night time flow might be as low as 10 gpm (37.8 m3 per minute). A turbine meter large enough to accommodate and accurately record the 500 gpm (1.8 m3) flows will not be sensitive enough to accurately measure and record the 10 gpm flow during the evening.\nCompound meters use a combination of multi-jet or turbine meter for high flow conditions, and then switch to positive displacement mechanism when water flows below a preset rate. A simple check valve switches the water flow accordingly. Combination meters are a good strategy for the water purveyor to meet the peak water demand for its customer while assuring accurate billing of all water consumed. Some water purveyors have experienced more than 10% increase in revenue after installing compound meters on single metered apartment buildings.\nIn general, the larger the meter, the less sensitive the meter is to small volumes of water passing through the mechanism. A typical residential meter (5/8” or 3/4” (1.59 cm or 1.91 cm)) will only record usage when the flow rate is greater than one pint of water per minute. Also, meter sensitivity decreases with age. A 10 year old 5/8” (1.59 cm) meter usually only records flows greater than 2 pints per minute (.95 LPM), depending on usages and water quality. Meter sensitivity is a very important issue for water conservation and water purveyor revenues. Water that passes through the meter unrecorded is essentially a cost to the water purveyor without revenue. More importantly, a meter of poor sensitivity will allow leaks to be unnoticed by the user and the water purveyor.\nFor additional information on water meters and metering visit:\nDedicated Irrigation Meter Introduction']"	['<urn:uuid:f95233d3-b902-494c-964f-a1735fb9d3ab>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	6	70	953
9	explain differences between print ink colors screen display colors easiest way understand	RGB and CMYK work in opposite ways. RGB (used in screens) is an additive process that blends light - when all RGB colors combine at full intensity, they produce white, while absence of color produces black. In contrast, CMYK (used in printing) is a subtractive process - when all CMYK colors combine, they produce a blackish color, similar to mixing paints. CMYK uses Cyan, Magenta, Yellow, and Black inks that combine to produce various colors through layering dots.	['Using the correct color mode in your design is essential to getting your message across. Learn the distinctions between RGB and CMYK color profiles with this simple overview.\nCover image via Rostislav_Sedlacek.\nThere can be a lot of confusion behind RGB and CMYK color modes. If you’re a designer, it’s crucial to differentiate the color profiles since designing in the wrong mode can affect the tone and message of your document. In this article, we’ll decipher the characteristics of each profile and when to use RGB and CMYK in your designs.\nThe RGB color profile consists of Red, Green, and Blue hues that combine to create extensive variations of colors. This color mode exists exclusively in screen displays, such as in computer monitors, mobile, and television screens. Even though RGB is present across all electronic devices, the color elements vary across systems and models. An image you see on an iMac may display differently than the same image on a Dell desktop computer.\nInstead of utilizing ink to produce hues, the RGB profile partakes in additive processes to produce color by blending light. This is the exact opposite of subtractive color processes, such as mixing paints or dyes. The presence of all RGB primaries at full intensity yields white, while the absence of color produces black. When you turn a monitor or screen off, you see an absence of RGB color, resulting in black. The color displays on your screen result from the presence of those RGB base hues.\nIn Adobe Illustrator, you can see how mixing RGB primaries at different intensities can vary the color output. When isolated, the primary hues generate vibrant shades. Red and green combine to yield yellow, green and blue mix to create cyan, and blue and red produce magenta.\nRGB produces a large array of colors; this color profile features a larger gamut, or color range, than CMYK. Above, you can see how Illustrator provides a gamut warning for RGB colors that can’t be successfully translated into CMYK. This occurs when an RGB color exists outside of CMYK printing abilities. Instead, the color will convert to a CMYK equivalent, which could affect the tones of your design. When printing a design produced in RGB color profiles, the print output will be different from the online preview.\nWhen to Use RGB Color Mode\nTo avoid color profile complications, always set a document in RGB when designing for social media, digital design, or online advertisements. Set any design or image that will exist on a screen or monitor as RGB. If you design in CMYK for an online-only design, you’ll limit your color options due to CMYK’s limited gamut range.\nThe CMYK color profile contains Cyan, Magenta, Yellow, and Key (Black) that combine to produce a range of hues. This four-color process works for any type of printer. When zoomed in on printed images you can see the four-color dots that layer to create different hues and gradations. Dots per inch result from printing and involve the CMYK color profiles. Although all printers produce prints in CMYK, the end result may vary among different styles and models of printers.\nIn RGB color spaces, all primaries combine to produce white with additive color processing. CMYK modes combine with subtractive color processes, meaning all primaries mask to yield to a blackish hue. This process is similar to when you mixed paints and dyes as a kid to make that unsightly dark color. As inks and dyes are layered upon each other, they subtract from the white of the paper.\nNotice how the CMYK sliders differ from the RGB sliders. Zero intensity of all primaries produces black in RGB color profiles; in CMYK color profiles, the absence of color produces white. Cyan and magenta create a blue, magenta and yellow create red, and yellow and cyan produce green. The combination of cyan, magenta, and yellow develop an off-black shade. Key, or black, comes in to add shades to each primary due to the primaries’ inability to combine and create black.\nWhen to Use CMYK Color Mode\nReserve CMYK for printed designs. Set products such as business cards, flyers, posters, and packaging in the CMYK color profile, and do yourself and the print shop a favor by always setting your document to CMYK before exporting or printing. Doing so prevents unbalanced colors that can result from RGB colors translated into CMYK equivalents.\nChanging Color Modes in Creative Cloud\nMost Adobe Creative Cloud programs will default to RGB color modes. You can easily adjust modes in the New Document panel under the Color Mode dropdown, or within the program itself. Not all programs support RGB to CMYK conversions, but with Illustrator, Photoshop, and InDesign, you can switch in just one click.\nIn Adobe Illustrator, you can switch colors modes by navigating to File > Document Color Mode > RGB Color or CMYK Color. This will affect the output of your vector shapes when shifting between color modes.\nWithin Photoshop, you can change color profiles by going to Image > Mode > CMYK or RGB Color.\nNavigate to the Color Panel within the software. If the panel isn’t visible, hit F6 to bring it up. Hit the dropdown symbol and select CMYK or RGB.\nLooking to expand your knowledge on essential design terminology? Check out these educational posts:']	['<urn:uuid:24845e0f-9c7f-46e4-81ac-3a59c9a12fef>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	12	78	886
10	compare privacy digital signing timestamping email encryption which better protect sensitive document	Digital timestamping and email encryption serve different purposes in protecting documents. Digital timestamping proves when a document was created and ensures it hasn't been modified, without requiring the document to be released - this is useful for protecting sensitive content like patent applications. Email encryption, on the other hand, focuses on maintaining privacy of contents by encrypting messages using the recipient's public key so only someone with the private key can read them. While timestamping preserves document integrity over time and doesn't rely on keys that could be compromised, encryption specifically prevents unauthorized viewing. The choice depends on whether the priority is proving when a document existed or keeping its contents private.	"[""7.11 What is digital timestamping?\nConsider two questions that may be asked by a computer user as he or she views a digital document or on-line record:\n- Who is the author of this record - who wrote it, approved it, or consented to it?\n- When was this record created or last modified?\nIn both cases, the question is about exactly this record - exactly this sequence of bits. An answer to the first question tells who and what: Who approved exactly what is in this record? An answer to the second question tells when and what: When exactly did the contents of this record first exist?\nBoth of the above questions have good solutions. A system for answering the first question is called a digital signature scheme (see Question 2.2.2). A system for answering the second question is called a digital timestamping scheme. Such systems are described in [BHS93] and [HS91].\nAny system allowing users to answer these questions must include two procedures. First, there must be a signing procedure with which (1) the author of a record can ``sign'' the record, or (2) any user can fix a record in time. The result of this procedure is a string of bytes that serves as the signature. Second, there must be a verification procedure by which any user can check a record and its purported signature to make sure it correctly answers (1) who and what? or (2) when and what? about the record in question.\nThe signing procedure of a digital timestamping system often works by mathematically linking the bits of the record to a ``summary number'' that is widely witnessed by and widely available to members of the public - including, of course, users of the system. The computational methods employed ensure that only the record in question can be linked, according to the ``instructions'' contained in its timestamp certificate, to this widely witnessed summary number; this is how the particular record is tied to a particular moment in time. The verification procedure takes a particular record and a putative timestamp certificate for that record and a particular time, and uses this information to validate whether that record was indeed certified at the time claimed by checking it against the widely available summary number for that moment.\nOne nice thing aboutdigital timestamps is that the document being timestamped does not have to be released to anybody to create a timestamp. The originator of thedocument computes the hash values himself, and sends them in to the timestamping service. The document itself is only needed for verifying the timestamp. This is very useful for many reasons (like protecting something that you might want to patent).\nTwo features of adigital timestamping system are particularly helpful in enhancing theintegrity of a digital signature system. First, a timestamping systemcannot be compromised by the disclosure of a key. This is because digitaltimestamping systems do not rely on keys, or any other secret information,for that matter. Second, following the technique introduced in [BHS93],digital timestamp certificates can be renewed so as to remain valid indefinitely.\nWith these featuresin mind, consider the following situations.\nIt sometimes happensthat the connection between a person and his or her public signature key must be revoked. For example, the user's private key may accidentally be compromised, or the key may belong to a job or role in an organization that the person no longer holds. Therefore the person-key connection must have time limits, and the signature verification procedure should check that the record was signed at a time when the signer's public key was indeed in effect. And thus when a user signs a record that may be checked some time later - perhaps after the user's key is no longer in effect - the combination of the record and its signature should be certified with a secure digital timestamping service.\nThere is another situation in which a user's public key may be revoked. Consider the case of the signer of a particularly important document who later wishes to repudiate his signature. By dishonestly reporting the compromise of his private key, so that all his signatures are called into question, the user is able to disavow the signature he regrets. However, if the document in question was digitally timestamped together with its signature (and key-revocation reports are timestamped as well), then the signature cannot be disavowed in this way. This is the recommended procedure, therefore, in order to preserve the non-reputability desired of digital signatures for important documents.\nThe statement that private keys cannot be derived from public keys is an over-simplification of a more complicated situation. In fact, this claim depends on the computational difficulty of certain mathematical problems As the state of the art advances - both the current state of algorithmic knowledge, as well as the computational speed and memory available in currently available computers - the maintainers of a digital signature system will have to make sure that signers use longer and longer keys. But what is to become of documents that were signed using key lengths that are no longer considered secure? If the signed document is digitally timestamped, then its integrity can be maintained even after a particular key length is no longer considered secure.\nOf course, digital timestamp certificates also depend for their security on the difficulty of certain computational tasks concerned with hash functions (see Question 2.1.6). (All practical digital signature systems depend on these functions as well.) The maintainers of a secure digital timestamping service will have to remain abreast of the state of the art in building and in attacking one-way hash functions. Over time, they will need to upgrade their implementation of these functions, as part of the process of renewal [BHS93]. This will allow timestamp certificates to remain valid indefinitely.\n- 7.1 What is probabilistic encryption?\n- Contribution Agreements: Draft 1\n- Contribution Agreements: Draft 2\n- 7.2 What are special signature schemes?\n- 7.3 What is a blind signature scheme?\n- Contribution Agreements: Draft 3\n- Contribution Agreements: Final\n- 7.4 What is a designated confirmer signature?\n- 7.5 What is a fail-stop signature scheme?\n- 7.6 What is a group signature?\n- 7.7 What is a one-time signature scheme?\n- 7.8 What is an undeniable signature scheme?\n- 7.9 What are on-line/off-line signatures?\n- 7.10 What is OAEP?\n- 7.11 What is digital timestamping?\n- 7.12 What is key recovery?\n- 7.13 What are LEAFs?\n- 7.14 What is PSS/PSS-R?\n- 7.15 What are covert channels?\n- 7.16 What are proactive security techniques?\n- 7.17 What is quantum computing?\n- 7.18 What is quantum cryptography?\n- 7.19 What is DNA computing?\n- 7.20 What are biometric techniques?\n- 7.21 What is tamper-resistant hardware?\n- 7.22 How are hardware devices made tamper-resistant?"", 'What are Digital Signatures?\nAs an inevitable consequence of the Internet, electronic communication has become acceptable in many contexts, altogether replacing traditional written communication. However, traditional documents were usually validated with the presence of a signature, which is more difficult to attest on an electronic document. A digital signature conceptually mimics a person’s unique signature, in that it validates an electronic document.\nDigital signatures are composed of two elements – a message hash and a private key. A message hash is a uniquely generated sequence of numbers, which cannot be reverse engineered to obtain the original message. The hash is then encrypted using the sender’s private key. The recipient decrypts the hash using the sender’s public key. The electronic document is also run through the hashing algorithm to check whether both the hashes are the same- thereby confirming the sender did indeed author the document in its current form, and it was not altered in any way before reaching its intended recipient.\nUnderstanding the Email Encryption Process\nThere are a number of methods by which emails can be encrypted for secure transfer, and public-key encryption is one of them. The premise is similar to that used when digitally signing electronic documents, in that there is a pair of keys generated- one private and the other public.\nIn the case of email encryption, the idea is to keep the contents secure from all unauthorized viewing. Therefore a sender identifies a recipient’s public key, and encrypts the email with that key. The private key is retained with the recipient, and is used to decrypt all communication encrypted using the corresponding public key.\nUsing this method, any communication encrypted using the recipient’s public key can only be read by those people with access to the correct private key. Therefore all senders can be assured of complete privacy of their emails, given that the private key is secure.\nComparison of Digital Signatures and Email Encryption\nAlthough both methods of encryption use asymmetrical keys, the goal of a digital signature and that of email encryption are entirely different. A digital signature is used to verify that a particular electronic document was created by a particular individual and has not been altered in the transmission process. The process is used to authenticate the author and the contents of the document beyond a shadow of doubt. Email encryption, on the other hand, is used to maintain the privacy of the contents of an email. Generally, information that should not be privy to everyone is subject to email encryption.\nWhen implementing digital signatures, the public key is used for decryption while its corresponding private key is used for encryption. The process for email encryption is exactly the opposite.\nEmail encryption and digital signatures are certainly not mutually exclusive, even though they have differences. There are occasions where establishing the identity of an email’s author is equally as important as maintaining the security of its contents. This is a scenario where both technologies would be used in conjunction with each other.']"	['<urn:uuid:695161c6-68c2-4643-97c7-1a7e83e46b24>', '<urn:uuid:ac6818a5-ca22-4c61-9ce6-514c8477ff4f>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T12:39:32.578853	12	112	1628
11	farmer looking for ways to save money why ipm doesnt work always and what benefits does it give	IPM (Integrated Pest Management) offers several benefits: it reduces crop pest and disease levels, limits chemical use, provides alternative pest management methods, and increases product yield and quality, thereby boosting farmers' income. It also promotes a healthy and sustainable ecosystem. However, IPM adoption faces challenges: some farmers' decisions are influenced more by pesticide marketing and market forces, and as agricultural extension office funding declines, farmers increasingly rely on recommendations from agrochemical companies. Additionally, many agricultural companies now sell integrated farm management systems that may limit farmers' ability to use IPM strategies.	['SORSOGON CITY, SORSOGON – The Department of Agriculture Regulatory Division will reactivate the Integrated Pest Management (IPM). This time, with focus on multi-commodity crops.\nThis was announced by DA-Bicol Regulatory Division Chief Rosita M. Imperial during the Integrated Pest Management (IPM) Consultative Meeting Cum Planning Workshop held on August 23, 2018, this city. The said activity was attended by IPM Coordinators from nine municipalities in the province, the Provincial Agriculturist Dr. Ma. Teresa V. Destura, and agriculturists from the Provincial Agriculture Office.\nThe consultative meeting cum planning workshop aims to strengthen the DA’s partnerships with LGUs on IPM Management. With the DA’s institutional restructuring and paradigm resetting, the Regulatory Division was designated to conduct plant pests and diseases surveillance and monitoring which used to be one of the Regional Crop Protection Center’s (RCPC) functions.\nIPM is intended to reduce the levels of crop pests and diseases and limit the inappropriate use of chemicals, provide alternatives for pest and disease management and boost product yield and quality, thereby increasing farmers’ income. It also contributes to a healthy, safe and resilient ecosystem through sustainable pest management.\nDA-Bicol Agriculturist Cyndie Pardo shared insights on common insect pests of rice and their management. Among the insect pests on rice are whorl maggot, leaffolder, armyworm, grasshoppers, planthoppers, leafhoppers, black bug, mole cricket, stem borer and rice bug. Pardo highlights that cultural practices such as good land preparation, fertilizer management and planting of resistant varieties; habitat manipulation or ecological engineering and use of pesticide as the last resort are effective IPM for rice.\nMeanwhile, Science Research Specialist Norman Concepcion discussed the pests and diseases on corn. Some of the common insect pests on corn are the corn borer, corn earworm, corn seedling maggot, white grub, armyworm, corn semilooper, corn aphids, and oriental migratory locust. Concepcion also noted the cultural and chemical control methods in pest management such as proper disposal of infected plants at harvest, treating seeds with slurry with thiram or captan at 120 grams per 50 kilogram seeds, selecting resistant varieties like the Philippine DMR Composite 2.\nMoreover, Senior Agriculturist Edith Pelin lectured on the pests and diseases of cacao. These include the symptoms and control measures for cacao pod borer, mealy bug, mired bugs, twig borer or coffee carpenter, cacao podrot, stem canker and vascular streak dieback (VSD).\nPelin underscored that prevention, identification of plant disease, diagnosing severity and application of appropriate treatments are the key steps of integrated disease management.\nJustine Guerrero, IPM Focal Person of Gubat and Nikka Angelu H. Torres, IPM Focal Person of Magallanes said that the consultation meeting and planning workshop was very beneficial to them because it refreshed their knowledge on crop pests and diseases and IPM.\nSince frontline service provider ako sa LGU, it will help especially sa mga farmers who depend greatly, if not solely, sa paggamit ng chemicals to control yung pest and diseases sa crops. Bukod sa mababawasan ang gastos nila, somehow magiging sustainable po yung way of farming nila (Since I am a frontline service provider at the LGU, it will help especially to farmers who depend greatly, if not solely, in using chemicals to control crop pests and diseases. It will reduce their production costs and will somehow, make farming more sustainable),” Guerrero said.\nGuerrero and Torres plan to disseminate the information on IPM to farmers in their respective municipalities during Farmers’ Meeting. They will also train and orient local farmer technicians and Barangay Agricultural and Fishery Council (BAFC) on IPM.\nLGU and PLGU-IPM Coordinators are required to submit monthly reports on plant pest and diseases surveillance to the Regulatory Division. (Annielyn L. Baleza, DA-RAFIS 5)', 'To Boost IPM Adoption, Treat It as a Complex Social System\nBy Meredith Swett Walker\nIn a perfect world, integrated pest management, or IPM, reduces damage caused by pests, reduces impacts on the environment, and saves farmers money. Using IPM techniques including crop rotation, biological control, and pest monitoring, farmers have saved billions of dollars both in crop losses as well as pesticide costs while reducing negative environmental impacts. Sounds great right? So, why aren’t all farmers embracing IPM?\nWhy some farmers choose not to practice IPM, and why IPM doesn’t always work as predicted, are some of the questions that Roger Magarey, Ph.D., and his colleagues at North Carolina State University and Texas A&M University hope to answer by considering the “people factor” in IPM. In a new article published in February in the open-access Journal of Integrated Pest Management, Magarey and co-authors make the case that treating IPM as a “social ecological system” can help improve pest management outcomes and convince more farmers to use it.\nPesticides have played a part in making food cheap and plentiful in many parts of the world, but that abundance comes at a price. Pesticides cause an estimated $10-35 billion dollars of damage to human health and the environment in the U.S. each year. Yet, some farmers’ pest management decisions are more influenced by pesticide marketing and market forces than by these impacts. Traditionally, agricultural extension offices have provided information and support for farmers using IPM, but, as funding for these programs declines, more farmers rely on recommendations from agrochemical companies themselves. In addition, many agricultural companies now sell “farm management systems,” which include coordinated seed, pesticides, fertilizers, and post-harvest treatments for a crop. These integrated systems may limit farmers’ ability to use IPM strategies.\nPesticides are also political. Public concern over the impacts of pesticides on health and the environment, along with lobbying by environmental groups, can lead governments to deregister effective pesticides. Agrochemical companies lobby governments as well, pushing for fewer regulations on pesticide use and continued registration of new and existing pesticides. These forces in society at large also influence farmers’ pest management decisions.\n“There is a real concern both in academia and in industry that declining social acceptance of pest management could result in negative impacts for agriculture,” says Magarey. “There have been substantial efforts to understand how social factors impact IPM adoption. We hope that using the Social-Ecological Systems framework can help researchers better understand how to improve IPM adoption.”\nA social ecological system (SES) is a complex system that meets five criteria. First, there is a sustained interaction between biological or physical factors and social factors. In the case of IPM, this would be pests, management methods, and farmers’ decisions. Second, an SES acts on multiple scales in time, geography, and organization. For IPM, these could be seasons versus years, individual farms versus regions, and so on. Third, these systems must involve critical natural, economic, or cultural resources, e.g., food crops, pest management expenditures, or farming traditions. An SES also includes external forces such as politics (which can affect pesticide regulation) or climate change (which affects pest populations and more). And, finally, an SES is dynamic and requires constant adaptation—which is the nature of farming itself, and an essential element of IPM.\nAll of these interacting factors may sound like an unsolvable tangle, but the authors point out that tools exist for explaining the behavior of these systems, such as heuristics (colloquially known as a “rules of thumb”) and mathematical modeling. For instance, Magarey says one of the greatest challenges in getting farmers to adopt IPM is finding the right incentives. He and his co-authors cite an example where researchers used an SES framework and mathematical model to determine the optimal combination of taxes and subsidies to encourage Thai vegetable farmers to use IPM while maintaining farm income and significantly reducing the use of pesticides.\n“We hope this article raises awareness of the importance of social acceptance in pest management along with the idea that research can help improve it,” says Magarey. He and his colleagues are working on future papers examining eco-efficiency of IPM and business strategies to reduce reliance on pesticides as the primary pest management tool. Integrating the “people factor” in our integrated pest management strategies can benefit farmers, the environment, and society as a whole.\nJournal of Integrated Pest Management\nMeredith Swett Walker is a former avian endocrinologist and wannabe entomologist. She now studies the development and behavior of two juvenile humans in the high desert of western Colorado. When she is not handling her research subjects, she writes about science and nature. Find a sampling of her work at www.magpiescicomm.com.']	['<urn:uuid:b5532d41-a56c-4054-9ff4-a5926432a5b4>', '<urn:uuid:4b89c5b2-d186-439d-a154-86e31380c054>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	18	91	1384
12	What causes gum disease and what complications can it lead to?	Gum disease begins when plaque builds up along and under the gum line, causing infections that hurt the gum and bone holding teeth in place. It can lead to several complications including tooth loss, coronary artery disease, respiratory problems, premature and low birth weight babies, poorly controlled diabetes, and rheumatoid arthritis. Research suggests the bacteria can enter the bloodstream through gum tissue and affect organs like the heart and lungs.	['Taking Care of Your Teeth and Mouth\nHealthy teeth and gums make it easy for you to eat well and enjoy good food. Several problems can affect the health of your mouth, but good care should keep your teeth and gums strong as you age.\nTeeth are covered in a hard, outer coating called enamel. Every day, a thin film of bacteria called dental plaque builds up on your teeth. The bacteria in plaque produce acids that can harm enamel and cause cavities. Brushing and flossing your teeth can prevent decay, but once a cavity forms, a dentist has to fix it.\nUse fluoride toothpaste to protect your teeth from decay. If you are at a higher risk for tooth decay (for example, if you have a dry mouth because of a condition you have or medicines you take), you might need more fluoride. Your dentist or dental hygienist may give you a fluoride treatment during an office visit or may tell you to use a fluoride gel or mouth rinse at home.\nGum disease begins when plaque builds up along and under your gum line. This plaque causes infections that hurt the gum and bone that hold your teeth in place. Gum disease may make your gums tender and more likely to bleed. This problem, called gingivitis, can often be fixed by brushing and flossing every day.\nA more severe form of gum disease, called periodontitis, must be treated by a dentist. If not treated, this infection can ruin the bones, gums, and other tissues that support your teeth. Over time, your teeth may have to be removed.\nTo prevent gum disease:\n- Brush your teeth twice a day with fluoride toothpaste.\n- Floss once a day.\n- Visit your dentist regularly for a checkup and cleaning.\n- Eat a well-balanced diet.\n- Quit smoking. Smoking increases your risk for gum disease.\nThere is a right way to brush and floss your teeth. Every day:\n- Gently brush your teeth on all sides with a soft-bristle brush and fluoride toothpaste.\n- Use small circular motions and short back-and-forth strokes.\n- Brush carefully and gently along your gum line.\n- Lightly brush your tongue to help keep your mouth clean.\n- Clean around your teeth with dental floss. Careful flossing removes plaque and leftover food that a toothbrush can’t reach.\n- Rinse after you floss.\nPeople with arthritis or other conditions that limit hand motion may find it hard to hold and use a toothbrush. Some helpful tips are:\n- Use an electric or battery-operated toothbrush.\n- Slide a bicycle grip or foam tube over the handle of the toothbrush.\n- Buy a toothbrush with a larger handle.\n- Attach the toothbrush handle to your hand with a wide elastic band.\nSee your dentist if brushing or flossing causes your gums to bleed or hurts your mouth. If you have trouble flossing, a floss holder may help. Ask your dentist to show you the right way to floss.\nHold floss as shown.\nUse floss between upper teeth.\nUse floss between lower teeth.\nSometimes, false teeth (dentures) are needed to replace badly damaged teeth. Partial dentures may be used to fill in one or more missing teeth. Dentures may feel strange at first. In the beginning, your dentist may want to see you often to make sure the dentures fit. Over time, your gums will change shape, and your dentures may need to be adjusted or replaced. Be sure to let your dentist handle these adjustments.\nBe careful when wearing dentures, because it may be harder for you to feel hot foods and drinks or notice bones in your food. When learning to eat with dentures, it may be easier if you:\n- Start with soft, non-sticky food.\n- Cut your food into small pieces.\n- Chew slowly using both sides of your mouth.\nKeep your dentures clean and free from food that can cause stains, bad breath, or swollen gums. Brush them every day with a denture-care product. Take your dentures out of your mouth at night, and soak them in water or a denture-cleansing liquid.\nDry mouth happens when you don’t have enough saliva, or spit, to keep your mouth wet. It can make it hard to eat, swallow, taste, and even speak. Dry mouth can accelerate tooth decay and other infections of the mouth. Many common medicines can cause this problem.\nThere are things you can do that may help. Try sipping water or sugarless drinks. Don’t smoke, and avoid alcohol and caffeine. Sugarless hard candy or sugarless gum that is a little tart may help. Your dentist or doctor might suggest using artificial saliva to keep your mouth wet.\nCancer of the mouth can grow in any part of the mouth or throat. It is more likely to happen in people over age 40. A dental checkup is a good time for your dentist to look for signs of oral cancer. Pain is not usually an early symptom of the disease. Treatment works best before the disease spreads. Even if you have lost all your natural teeth, you should still see your dentist for regular oral cancer exams.\nYou can lower your risk of getting oral cancer in a few ways:\n- Do not use tobacco products, such as cigarettes, electronic cigarettes, chewing tobacco, snuff, pipes, or cigars.\n- If you drink alcohol, do so only in moderation.\n- Use lip balm with sunscreen.\nDental care can be costly. Medicare does not cover routine dental care, and very few States offer dental coverage under Medicaid. You may want to check out private dental insurance for older people. Make sure you are aware of the cost and what services are covered. The following resources may help you find low-cost dental care:\n- Some dental schools have clinics where students get experience treating patients at a reduced cost. Qualified dentists supervise the students. Visit www.ada.org for a list of U.S. dental schools.\n- Dental hygiene schools may offer supervised, low-cost care as part of the training experience for dental hygienists. See schools listed by State at www.adha.org.\n- Call your county or State health department to find dental clinics near you that charge based on your income.\n- To locate a community health center near you that offers dental services, visit www.findahealthcenter.hrsa.gov.\n- United Way chapters may be able to direct you to free or reduced-cost dental services in your community. Call “211” to reach a local United Way chapter or visit www.unitedway.org/find-your-united-way.\nAmerican Dental Association\nHealth Resources and Services Administration Information Center\nFor more information on health and aging, contact:\nNational Institute on Aging Information Center\nP.O. Box 8057\nGaithersburg, MD 20898-8057\nVisit www.nihseniorhealth.gov, a senior-friendly website from the National Institute on Aging and the National Library of Medicine. This website has health and wellness information for older adults, including information about dry mouth. Special features make it simple to use. For example, you can click on a button to make the type larger.\nNational Institute on Aging\nNational Institutes of Health\nU.S. Department of Health and Human Services\nPublication Date: June 2016\nPage Last Updated: July 14, 2016', 'What is periodontitis?\nPeriodontal treatment may be required in the case of periodontitis.\nPeriodontitis is a serious gum condition caused by bacterial infection that damages soft tissues ultimately destroying the bone that supports teeth causing tooth loss. Periodontitis can lead to an increased risk of heart attack or stroke along with other serious health problems.\nPeriodontitis, although common, is largely preventable and is generally the result of poor oral hygiene. Brushing at least twice daily and flossing once daily along with regular dental checkups can greatly reduce your chance of developing periodontitis.\nFactors that can increase your risk of periodontitis include:\n- Poor oral health habits\n- Tobacco use\n- Older age\n- Decreased immunity, such as that occurring with leukemia, HIV/AIDS or chemotherapy\n- Poor nutrition\n- Certain medications\n- Hormonal changes, such as those related to pregnancy or menopause\n- Substance abuse\n- Poor-fitting dental restorations\n- Problems with the way your teeth fit together when biting\nSome complications associated with gum disease include:\n- Tooth loss\n- Coronary artery disease\n- Premature, low birth weight babies\n- Poorly controlled diabetes\n- Respiratory problems\n- Rheumatoid arthritis\nSome research suggests that the bacteria responsible for periodontitis can enter your bloodstream through your gum tissue, affecting your lungs, heart and other parts of your body. For instance, bacteria may travel to the arteries in your heart, where they might trigger a cycle of inflammation and arterial narrowing that contributes to heart attacks.\nDiagnosis of periodontitis is generally simple. Diagnosis is based on your description of symptoms and an exam of your mouth. Your hygienist will look for plaque and tartar buildup and check for easy bleeding.\nTo determine how severe your periodontitis is, your dental hygienist may:\n*Use a dental instrument to measure the pocket depth of the groove between your gums and your teeth by inserting a probe between your tooth and gumline, usually at several sites throughout your mouth. In a healthy mouth, the pocket depth is usually between 1 and 3 millimeters (mm). Pockets deeper than 5 mm indicate periodontitis.\n*Take dental X-rays to check for bone loss in areas where your dentist observes deeper pocket depths.\nThe goal of periodontal treatment is to thoroughly clean the pockets around the teeth preventing further damage to the surrounding bone. Treatment may be performed by a periodontist, a dentist or a dental hygienist. You have the best chance for successful treatment when you commit to a regimen of good oral hygiene along with routine dental maintenance appointments.\nIf periodontitis isn’t advanced, treatment may involve less invasive procedures, including scaling and root planning (a deeper cleaning that is usually performed in quadrants at multiple appointments with local dental anesthesia or “numbing”). Scaling removes tartar and bacteria from your tooth surfaces and beneath your gums. It may be performed using instruments or an ultrasonic device. Root planing smoothes the root surfaces, discouraging further buildup of tartar and bacterial endotoxin. In some cases, Dr. Hughes may recommend using topical or oral antibiotics to help control bacterial infection. Topical antibiotics can include antibiotic mouth rinses or insertion of gels containing antibiotics in the space between your teeth and gums or into pockets after deep cleaning. However, oral antibiotics may be necessary to completely eliminate infection-causing bacteria.\nIn cases where periodontitis is non-responsive to non-surgical treatments or where it is too far advanced for such treatment, periodontal surgery may be required. In these cases, Dr. Hughes will refer you to a periodontist (a dentist specializing in the treatment of gum diseases).\nWatch the video below to better understand periodontitis and its treatment:']	['<urn:uuid:7e704929-4bc3-4ea5-8f98-8012884447ae>', '<urn:uuid:d59fc087-da42-4e66-8f62-7704e4aa8178>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	11	70	1790
13	What are the key differences between the symptoms of PTSD and major depression, particularly in how they affect a person's daily life and behavior?	PTSD symptoms can be confused with reactions to other stressors like work or money problems, and can overlap with symptoms of medical conditions like heart disease or diabetes. On the other hand, major depression has distinct symptoms including a persistent blue mood, fatigue, thinking troubles, appetite changes (usually 10+ pounds weight change), disinterest in previously enjoyable activities, feelings of worthlessness, recurring thoughts of death, and unexplained physical symptoms like headaches and digestive problems. Both conditions can affect daily functioning, but major depression specifically interferes with a person's ability to handle ordinary, everyday activities and may occur in episodes throughout a person's life.	"[""PTSD: National Center for PTSD\nHelp for Veterans with PTSD\nHelp for Veterans with PTSD\nVeterans who have symptoms of PTSD often ask us for help, as do their families. The National Center for PTSD provides education and conducts research on trauma and PTSD. We do not provide diagnosis or treatment of PTSD.\nFor direct care, see both of the following:\nBelow are the answers to some questions about PTSD that are often asked by Veterans and their families.\nDo I have PTSD?\nA natural first question is whether your symptoms might be due to PTSD. A good place to start learning about PTSD is the PTSD Basics page on our website. You should know, though, that having symptoms does not always mean that you have PTSD. Some reactions to stress and trauma are normal. Since many common reactions look like the symptoms of PTSD, a doctor must decide if you have PTSD\nAlso, stressors other than trauma may cause symptoms that are like those of PTSD. For example, work or money problems can lead to symptoms. Medical problems such as heart disease or diabetes, or mental health problems such as depression or anxiety, can have symptoms that look like PTSD. That is why you should see a provider who is trained to know which of your symptoms might be PTSD.\nIf I have other problems, can I also have PTSD?\nVeterans with PTSD often have other types of problems. They might have other stress, medical, or mental health problems. Sometimes PTSD is overlooked when other problems seem very pressing. If you have questions, ask your doctor if PTSD also needs to be treated.\nAm I eligible for VA services?\nAll Veterans could possibly be eligible. Here is a brief list of factors that make up whether you are eligible:\n- You completed active military service in the Army, Navy, Air Force, Marines, or Coast Guard (or Merchant Marines during WW II).\n- You were discharged under other than dishonorable conditions.\n- You are a National Guard member or Reservist who has completed a federal deployment to a combat zone.\nYou should also be aware that:\n- Health care eligibility is not just for those who served in combat.\n- Other groups may be eligible for some health benefits.\n- Veteran's health care is not just for service-connected injuries or medical conditions.\n- Signing up for health care is separate from signing up for other benefits at VA.\n- Veteran's health care facilities are for both women and men. VA offers full-service health care to women Veterans.\nFor Veterans who served in a theater of combat operations after November 11, 1998, some benefits have been added. In January, 2008, the period of eligibility for free health care was extended from two to five years.\nVA operates a yearly enrollment system that helps make sure that Veterans who are eligible can get care. For information, see VA Health Care Eligibility & Enrollment. Your DD 214 is used to enroll for VA services. If you have trouble locating this form, VA Enrollment can still assist you.\nWhat help is there for me (or my Veteran family member)?\nPTSD is treatable. Many places within VA provide PTSD treatment. General programs that provide mental health services include VA medical centers, community-based outpatient clinics (CBOCs), and Vet Centers. Use the VA Facilities Locator to find the closest VA facility.\nAn extra note about Vet Centers\nOffered through the Readjustment Counseling Service, Vet Centers are located in the community. They provide information, assessment, and counseling to any Veteran who served in a war zone. This includes conflicts such as in Somalia, Iraq, or Afghanistan.\nVet Centers also offer services to families of Veterans for military-related issues. There are no fees or charges for Vet Center services, and services are confidential. That means no information will be given to any person or agency (including the VA) without your consent. Most of the staff are Veterans themselves.\nDuring normal business hours, you can call 1-800-905-4675 (Eastern) or 1-866-496-8838 (Pacific). The Vet Center program also has a 24/7 hotline, with all calls answered by combat Veterans: 1-877-WAR-VETS (1-877-927-8387).\nVA special PTSD clinics and programs\nVA also has special PTSD clinics and programs that can help eligible Veterans. For more on these programs, see our fact sheet PTSD Treatment Programs in the U.S. Department of Veterans Affairs.\nWhat to expect when you see a VA provider\nWhen you see a VA provider, he or she will first assess whether or not you have PTSD. If you do have PTSD, remember that it can be treated. Several types of education and treatment are helpful to Veterans and their family members. These include:\n- Classes on dealing with stress, anger, sleep, relationships, and PTSD symptoms\n- One-to-one, group, and family counseling\nFor more information, please see Treatment.\nI think I am disabled due to PTSD caused by military service. What can I do?\nService-connected disability for PTSD is determined by the Compensation and Pension Service. C&P is an arm of VA's Veterans Benefits Administration: Compensation. This decision is not made by the providers who care for you in VA's PTSD clinics and Vet Centers. The process for making the decision involves several steps:\n- A formal request (claim) must be filed using forms provided by the VA's Veterans Benefits Administration.\n- After all the forms are submitted, you must complete interviews about your:\n- social history (a review of family, work, and education before, during, and after military service).\n- psychiatric status (a review of past and current mental health symptoms and of traumas gone through while in the military).\nThe forms and information about the application process can be obtained from Benefits Officers at any VA medical center, outpatient clinic, or regional office.\nThe process of applying for a VA disability for PTSD can take several months. It can be confusing and quite stressful. Veterans Service Organizations (VSOs) can help Veterans and family members with VA disability claims. VSOs provide Service Officers at no cost. Service Officers know all about every step in the application and interview process. They can provide practical help and moral support. Some Service Officers are experts in helping Veterans with PTSD disability claims.\nEven if you have not been a member of a given VSO, you still can ask for help from a Service Officer of that VSO. To find a Service Officer to represent you, just contact the local office of any VSO. You may also wish to ask other Veterans who have applied for VA disability what they would suggest. A mental health provider at a VA PTSD clinic or a Vet Center may also have some tips.\nMy claim for a VA PTSD disability has been turned down by the Benefits Office, but I believe I have PTSD due to military service. What can I do?\nA Veterans Service Officer can explain how to file an appeal. The Service Officer may be able to help you gather the information you need to make a successful appeal. You may want to contact a Service Officer who is an expert in helping Veterans who have PTSD-related claims.\nI can't get records from the military that I need for my disability claim. What can I do?\nVeterans Service Officers can help you file the paperwork needed to get your military records. If your Service Officer is not able to help you get needed records, ask him or her to direct you to another Service Officer who has more experience in getting records."", 'Last Updated on April 28, 2023\nMajor depression — also called major depressive disorder — is a kind of depression in which despair and disinterest in once-appreciated tasks interfere with how you function daily. And if you’ve got it, you’re not alone: According to the National Institute of Mental Health (NIMH), about 7 percent of the adult U.S. residents has major depressive disorder.\nWho are these people grappling with major depression? Some individuals are more at risk: Girls, individuals with lower incomes, people that have a family history of major depression, and those who’ve experienced a stressful life event are most exposed to developing it although the illness can affect anyone. And very often, the disease first appears between the ages of 30 and 25.\nMajor Depression: Understand the Symptoms\nMajor depression symptoms may appear slowly or all of a sudden. Not everyone experiences the same symptoms, plus some instances are far more intense than others. Major depression may be diagnosed when you are experiencing several depression symptoms that interfere with your functioning — your capability to handle ordinary, everyday sorts of actions — for an amount greater than a couple of weeks. In a few instances, someone might have just one episode, although episodes of major depression usually recur throughout a person’s life.\nApparent symptoms of major depression include:\n- Blue mood. You might feel sad or irritable most of the time.\n- Weariness. Sleep issues or reduced energy levels could be another signal.\n- Thinking troubles. You could realize you have concentration or memory problems.\n- Desire changes. It is not uncommon to have noticeable changes in your appetite and weight with this particular illness — the change is an increase or a decline, generally 10 pounds or maybe more.\n- Disinterest. With major depression, you can become indifferent to what you once enjoyed, or no more derive enjoyment from formerly enjoyable activities for example sex.\n- Feelings of “lowness.” You might feel guilty, worthless, hopeless, and empty.\n- Distressful ideas. Major depression could be related to recurring thoughts of suicide or departure.\n- Physical symptoms. Unexplained headaches, digestive problems, and pain may be indications of the sickness.\nMajor Depression: Diagnosis and Treatment\nTalk to your physician in case you are experiencing a few of these symptoms. A diagnosis of major depression may be produced after your physician or a mental health professional performs a physical exam, evaluates your personal and family medical history, and requests one to describe your depression symptoms. You may be asked by your physician when your symptoms started, how intense they’ve been, how long they typically last, and in the event you have ever had them before.\nMajor depression is most commonly treated with medication, including antidepressants, or psychotherapy. Antidepressants work by stabilizing neurotransmitters, which will be the brain chemicals thought to give rise to depression when they’re not in balance. Besides stimulants, antidepressants and anti-anxiety drugs are sometimes used as a member of the treatment plan.\nIn certain cases of mild to moderate depression, psychotherapy will be the sole treatment you’ll need. Psychotherapy involves a therapist educating you new ways of thinking and behaving to help relieve your depression symptoms.\nNo matter how your medical team along with you decide to take care of your illness, the most crucial matter is that you just get the melancholy treatment you’ll need as soon as you possibly can. An estimated 80 to 90 percent of people that are diagnosed with major depression respond well for their treatment and so are soon able to return to their own ordinary actions.']"	['<urn:uuid:1c717380-d10a-4a72-a241-ac03c1f6011f>', '<urn:uuid:8558f92d-4a96-419f-a733-d3f2a01531e8>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	24	102	1851
14	How does theater help students learn in schools, and what specific skills do they develop when participating in reading performances?	Theater in schools provides unique learning opportunities as some things are only learned or better taught through arts. It reaches students who may not learn well through traditional methods and helps those who don't feel they belong in the regular school system. When participating in reading performances, students develop multiple skills including reading fluency, vocabulary acquisition, and social skills like teamwork and active listening. They also improve their comprehension abilities by interpreting text and understanding character perspectives.	"[""Teresa Dayley Love relates that Shakespeare’s “Cymbeline” is a “crazy, complicated story,” but her original, lighthearted adaptation makes the romantic adventure accessible to young students — and also very young students.\n“There are two versions: The first, for first- to third-graders, is a dark fairy tale, and the second is a noir mystery for older students,” Love said. “The fairy tale adaptation focuses on the importance of story, while the other is all about genre.\n“I want the little kids to leave their version thinking, ‘Oh, Shakespeare is a good storyteller,’ and the older kids to say, ‘Oh, Shakespeare is cool.’ ”\nThe Young Company, a touring troupe sponsored by BYU’s Department of Theatre and Media Arts, began its mission to introduce theater to young audiences in 1974 as the Whittlin’ Whistlin’ Brigade. After premiering productions on campus, the company travels throughout the state, giving about 45 performances each school year and reaching an average of 20,000 students annually. Now, at the company’s 40-year anniversary, university officials estimate the total number of youngsters who have enjoyed the program is near 900,000.\n“Every performance feels a little profound because we know we are introducing something important,” Love said. “It will be their first experience with accessible theater because it is theater designed specifically for children. Too often, people think children’s theater is about skipping around and talking bunnies. But good theater resonates with kids.”\nThe playwright-director-instructor believes the impact of theater in schools is far-reaching.\n“There are some things that are only learned through the arts, and there are some things that are taught better through the arts,” Love said. “Taking theater to schools is a very enriching experience for students. There are a lot of opportunities for learning when theater is presented. And theater reaches some kids that other methods of learning can’t. Students can go through the system of how school is set up and never feel like that they belong. They don’t learn that way.”\nBased on legends of early Celtic kings, “Cymbeline” is one of the Bard’s most fantastical creations. Familiar Shakespearean themes of jealousy and innocence wronged are joined by a wit who becomes a guide through an ancient landscape. A lovely princess, an evil queen, a misguided king and a thoroughly rotten clown make up this legend of life and love renewed.\nAlthough scholars have organized Shakespeare’s plays into four distinct categories (history, tragedy, comedy and romance), “Cymbeline” arguably contains aspects of all four of these genres. “Straightforward” and “uncomplicated” are words not generally used to describe the play, which was first published in 1623. It’s also been speculated that by frivolously spinning absurd tales, Shakespeare wrote “Cymbeline” merely to amuse himself.\n“Shakespeare employs some familiar elements from his other plays. There is a little bit of ‘King Lear,’ a little bit of ‘Othello,’ a little bit of ‘The Winter’s Tale’ — all these different things are involved in the storytelling,” Love said. “He actually writes himself into a hole more than once. It’s one of the times when I realize that Shakespeare really was a genius, because of the way the convoluted story ends, with everyone happy and getting married.”\nAt the BYU preview, before the Young Company's “Cymbeline” is staged for area schools, theatergoers will select which of the two versions to view or they may buy two tickets and view both versions back-to-back. They can see how each compares to the other and how the focus shifts — from an emphasis on storytelling in the fairy tale version to a review of genre in the noir mystery.\nBut when the show is presented on tour, the different age groups will see the version created for them. Each version has separate character names, costumes and props, with some dialogue changed to follow the revised scripts.Comment on this story\nAs with old-world fairy tales and continuing through to contemporary movies, “Cymbeline” is “about choices and finding your way through a hostile world,” Love said. “But I also want to make audiences understand that when we make choices creatively, we can significant impact the results.”\nIf you go ...\nWhat: BYU Department of Theatre and Media Arts’ adaptation of William Shakespeare’s “Cymbeline”\nWhere: The Margetts Theatre, Brigham Young University\nWhen: Friday, Jan. 31, through Saturday, Feb. 8; Tuesday-Thursday at 7 p.m.; Fridays at 7 p.m. and 9 p.m.; Saturdays at 11 a.m., 2 p.m. and 4 p.m.; no performances Sundays or Mondays\nHow much: $4-$6"", 'In the realm of language development, educators are constantly seeking innovative and engaging methods to enhance students’ communication skills, fluency, and comprehension. One such approach that has gained recognition and popularity is Readers Theater. This instructional strategy involves a collaborative, dramatic reading of scripts, providing students with an opportunity to actively engage with text, build vocabulary, improve reading fluency, and enhance their overall language development. In this article, we will explore the numerous benefits that Readers Theater offers in the context of language development.\nEncourages Active Engagement:\nReaders Theater actively involves students in the learning process by encouraging them to assume character roles and participate in the reading aloud of scripts. This dynamic and interactive approach captivates students’ attention and promotes active engagement with the text. By immersing themselves in the characters and storylines, students become more invested in the material, leading to improved comprehension and retention.\nEnhances Reading Fluency:\nOne of the primary goals of Readers Theater is to improve reading fluency. Through repeated readings and practice, students gain confidence in their reading abilities. The act of performing scripts aloud enables students to focus on the rhythm, intonation, and pacing of spoken language. Regular engagement in Readers Theater helps students develop a natural flow in their reading, leading to increased fluency, expressiveness, and oral communication skills.\nDevelops Vocabulary and Language Skills:\nReaders Theater exposes students to a rich variety of vocabulary, complex sentence structures, and diverse literary genres. By reading and rehearsing scripts, students encounter new words, idioms, and phrases in context. This contextual learning enhances their understanding of word meanings and promotes the acquisition of new vocabulary. Moreover, the collaborative nature of Readers Theater encourages peer interaction, dialogue, and discussion, fostering the development of language skills such as active listening, turn-taking, and communication.\nEngaging in Readers Theater requires students to actively visualize and interpret the text, deepening their comprehension skills. By assuming different character roles, students gain a greater understanding of the motivations, emotions, and perspectives of the characters they portray. This perspective-taking promotes empathy and enables students to make meaningful connections between the text and their own experiences, enhancing their overall comprehension and critical thinking abilities.\nFosters Collaboration and Social Skills:\nReaders Theater is inherently a collaborative activity that requires students to work together as a team. Students must cooperate, communicate, and coordinate their efforts to successfully bring the script to life. Through this collaborative process, students develop important social skills such as teamwork, active listening, empathy, and respect for others’ ideas and perspectives. Furthermore, Readers Theater provides a safe and inclusive environment for students to express themselves creatively and build confidence in their communication abilities.\nReaders Theater offers a multitude of benefits for language development in students. By actively engaging in collaborative script readings, students improve their reading fluency, expand their vocabulary, enhance comprehension, and develop essential social and communication skills. The combination of literature, performance, and peer interaction creates an immersive and enriching learning experience that not only strengthens language skills but also fosters a love for reading and an appreciation for the power of storytelling. Implementing Readers Theater in the classroom can have a transformative impact on language development and create a positive and engaging environment for students to thrive.']"	['<urn:uuid:e4e3cb8a-aae5-4295-a432-95a4f2a10d89>', '<urn:uuid:7d590e50-7250-40fa-beaa-c02cb58d8faa>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	20	77	1276
15	How does sustainable consumption benefit the environment and drive business innovation?	Sustainable consumption benefits the environment by minimizing natural resource use, toxic materials, and pollutants while ensuring future generations' needs. From a business perspective, it drives innovation by creating opportunities to reduce costs, enhance competitiveness, develop new markets, and create jobs through circular economy practices like resource efficiency, product life extension, and sustainable business models such as sharing economy and product-as-a-service approaches.	['Notes towards people-powered, sustainable consumption and production\nPhoto: IISD Reporting Services\nOn January 23, 2019, at the Third Forum of Ministers and Environment Authorities of Asia-Pacific, held in Singapore, Lei Covero of IBON International gave a presentation towards an initial framework for people-powered sustainable consumption and production. The following article is derived from the mainpoints of her presentation.\nFor decades, “sustainable consumption and production” (SCP) has been part of the international discourse on sustainable development. Patterns of consumption-production are vital factors in the achievement, or failure, of the sustainable development agenda. The inclusion of SDG 12, of “ensuring sustainable consumption and production patterns,” illustrates the universal recognition of its crucial role in making global sustainability possible in the environment, economics, and social dimensions.\nAs defined by the UN Environment Program, drawing from the 1994 Oslo Symposium on Sustainable Consumption, SCP refers to “the use of services and related products, which respond to basic needs and bring a better quality of life while minimizing the use of natural resources and toxic materials as well as the emissions of waste and pollutants over the life cycle of the service or product so as not to jeopardize the needs of further generations.”\nSCP must be seen in light of systemic issues: related to systems of production and consumption of goods and services, and the resource-use and wastage involved. SCP should take into consideration the economic chain of production-distribution/exchange-consumption. It must be viewed in the context of social organization of the whole chain, and the particular systems usually captured by transnational corporations and elites.\nApproaching the matter of SCP as one chain or system addresses the question of sustainability while offering a more holistic view of all aspects of SCP in real economies. SCP is a systemic issue that must look at overall volumes of consumption, distributional issues such as those linked to trade and exchange, and related social and institutional changes. Aside from environmental problems, it is also critical to address socio-economic issues, such as the inequitable distribution of wealth and resources,and inequitable trading policies.\nSCP requires sustained people’s action. Not only in developing alternative models, but more importantly in challenging current paradigms and policies that impact sustainability and dictate how the world produces, distributes and consumes. Today’s dominant paradigm is a wasteful economic system headed by monopolistic corporations and a financial oligarchy; this same system is widening gaps in the realisation of people’s economic rights, from farmers’ right to land, indigenous peoples’ right to self-determination, workers’ and women’s rights.\nPeople’s actions mean assertions of people’s rights – this is the key link to achieving SCP. People’s rights refer to a synthesis of individual & collective rights, covering their substantive, comprehensive scope in the civil, political, economic, social, cultural dimensions. Beyond states’ responsibility to promote and uphold rights, there is the power of the people to assert these. There is the capacity of the people to exercise collective will and sovereignty to decide and determine how commodities and services are produced, distributed, exchanged and consumed.\nThrough assertion of people’s rights, people are empowered to determine systemic changes in consumption, production – and', 'The world is currently facing various complex problems, from climate change to pollution to resource depletion. These issues have led to the emergence of the circular economy, which is increasingly being seen as a potential solution to many of these problems. This article will explore the circular economy and sustainable business models, their history, characteristics, advantages, and challenges.\nDefinition of Circular Economy\nThe circular economy is an economic system designed to eliminate waste and promote the continual use of resources. In a circular economy, resources are kept in use for as long as possible, the maximum value is extracted from them, and then they are recovered and regenerated at the end of their useful life.\nImportance of Circular Economy\nThe circular economy is becoming increasingly important due to several reasons. Firstly, it addresses the challenges of climate change, resource scarcity, and pollution. Secondly, it provides opportunities for businesses to reduce their costs and enhance their competitiveness. Thirdly, it fosters innovation and creates new jobs.\nHistory of Circular Economy\nThe concept of the circular economy can be traced back to the 1960s when it was proposed as a way to address the issue of resource depletion. Over time, the concept evolved to include the reduction of waste and pollution.\nIn recent years, the circular economy has gained momentum, with many businesses and governments adopting circular economy principles. This has been driven by increasing awareness of a circular economy’s environmental and economic benefits.\nCharacteristics of Circular Economy\nThe circular economy is characterized by waste reduction. This is achieved by designing durable products that can be repaired, reused, or recycled. In addition, the circular economy encourages using renewable energy sources and reducing energy consumption.\nThe circular economy promotes resource efficiency by ensuring that resources are used in the most efficient way possible. This is achieved by optimizing production processes and using innovative technologies to reduce waste and increase resource productivity.\nProduct Life Extension\nThe circular economy aims to extend the life of products by designing them to be reused, repaired, or recycled. This is achieved through innovative materials and production processes, which enable products to be disassembled and their components reused.\nSustainable Business Models\nSustainable business models are those that incorporate circular economy principles into their operations. These models aim to create value for customers and society while minimizing negative environmental impacts.\nSeveral examples of sustainable business models include the sharing economy, product-as-a-service models, and closed-loop supply chains. In the sharing economy, individuals can access goods and services without owning them, reducing waste and encouraging resource reuse. Product-as-a-service models enable customers to use products without owning them, while closed-loop supply chains ensure that products are reused or recycled at the end of their useful life.\nAdvantages of Circular Economy and Sustainable Business Models\nCircular economy and sustainable business models offer several environmental benefits. Firstly, they reduce waste and pollution by promoting the use of renewable resources and reducing energy consumption. Secondly, they minimize the negative impacts of production and consumption on the environment by promoting sustainable practices and reducing waste. Thirdly, they contribute to the conservation of natural resources by ensuring that they are used most efficiently.\nCircular economy and sustainable business models also offer several economic benefits. Firstly, they enable businesses to reduce costs and enhance competitiveness by minimizing waste and optimizing production processes. Secondly, they foster innovation and create new opportunities for business growth. Thirdly, they contribute to the development of new markets and the creation of new jobs.\nChallenges in Adopting Circular Economy and Sustainable Business Models\nDespite the many benefits of circular economy and sustainable business models, several challenges must be overcome to adopt these principles fully.\nOne of the main challenges in adopting a circular economy and sustainable business models is the need for innovative technologies that enable the efficient use of resources and the reduction of waste. Businesses and governments must invest in research and development to create these technologies.\nAnother challenge is the need for appropriate regulatory frameworks that support the circular economy and sustainable business models. This includes policies that promote sustainable practices and regulations that encourage the reuse and recycling of resources.']	['<urn:uuid:2b359e95-dae6-4dd0-b93c-9d1a50906fc9>', '<urn:uuid:a197537c-e66a-45a3-a33d-b6d1b49d1b64>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	11	61	1207
16	What are the similarities and differences between Katherine Hepburn's and Michael Clarke Duncan's recognition across various award ceremonies throughout their careers?	Katherine Hepburn holds the record for most Oscar wins with four Academy Awards, all in the Best Actress category, receiving a total of 12 nominations throughout her career. In contrast, Michael Clarke Duncan received multiple accolades specifically for his role in The Green Mile, including nominations for the Academy Award for Best Supporting Actor, Golden Globe Award for Best Supporting Actor, and wins at the Black Reel Awards, Broadcast Film Critics Association Awards, and Saturn Awards. While Hepburn's recognition spanned multiple decades and films, Duncan's major awards recognition was concentrated around his performance in The Green Mile.	"['- The Green Mile (film)\nThe Green Mile\nDirected by Frank Darabont Produced by Frank Darabont\nScreenplay by Frank Darabont Based on The Green Mile by\nStarring Tom Hanks\nMichael Clarke Duncan\nMusic by Thomas Newman Cinematography David Tattersall Editing by Richard Francis-Bruce Studio Castle Rock Entertainment Distributed by Warner Bros. Release date(s) December 10, 1999 Running time 188 minutes Country United States Language English Budget $60 million Box office $286,801,388\nThe Green Mile is a 1999 American drama film directed by Frank Darabont and adapted by him from the 1996 Stephen King novel of the same name. The film is told in a flashback format and stars Tom Hanks as Paul Edgecomb and Michael Clarke Duncan as John Coffey, and tells the story of Paul and his life as a death row corrections officer during the Great Depression in the United States, and the supernatural events he witnessed.\nIn a Louisiana nursing home in 1999, Paul Edgecomb (Dabbs Greer) begins to cry while watching the film Top Hat. His elderly friend, Elaine, shows concern for him and Paul tells her that the film reminded him of when he was a corrections officer in charge of Death Row inmates at Cold Mountain Penitentiary during the summer of 1935. The cell block Paul (Tom Hanks) works in is called the ""Green Mile"" by the guards because the condemned prisoners walking to their execution are said to be walking ""the last mile"" to the electric chair; here, it is a stretch of faded lime-green linoleum.\nOne day, John Coffey (Michael Clarke Duncan), a giant black man convicted of raping and killing two young white girls arrives on death row. Coffey shows all the characteristics of being a ""gentle giant"": keeping to himself, soft-spoken, fearing darkness, and crying often. Soon enough, John reveals extraordinary powers by healing Paul\'s urinary tract infection and resurrecting a mouse. Later, he would heal the terminally ill wife of Warden Hal Moores (James Cromwell), who suffered from a large brain tumor. When John is asked to explain his power, he merely says that he ""took it back.""\nAt the same time, Percy Wetmore (Doug Hutchison), a sadistic and unpopular guard, starts work. He ""knows people, big people"" (he is the nephew of the governor\'s wife), in effect preventing Paul or anybody else from doing anything significant to curb his behavior. Percy recognizes that the other officers greatly dislike him and uses that to demand managing the next execution. After that, he promises, he will have himself transferred to an administrative post at Briar Ridge Mental Hospital and Paul will never hear from him again. An agreement is made, but Percy then deliberately sabotages the execution. Instead of wetting the sponge, used to conduct electricity and make executions quick and effective, he leaves it dry, causing inmate Eduard ""Del"" Delacroix\'s (Michael Jeter) execution to be botched and for him to die slowly in great pain.\nShortly before Del\'s execution, a violent prisoner named William ""Wild Bill"" Wharton (Sam Rockwell) arrives, due to be executed for multiple murders committed during a robbery. At one point he seizes John\'s arm and John psychically senses that Wharton is the true killer of the two girls, the crime for which John was convicted and sentenced to death. John ""takes back"" the sickness in Hal\'s wife and regurgitates it into Percy, who then shoots Wharton to death and falls into a permanent catatonic state. Percy is then housed in the Briar Ridge Mental Hospital. In the wake of these events, Paul interrogates John, who says he ""punished them bad men"" and offers to show Paul what he saw. John takes Paul\'s hand stating that he has to give Paul ""a part of himself"" in order to see and imparts the visions of what he saw, of what really happened to the girls.\nPaul asks John what he should do, if he should open the door and let John walk away. John tells him that he is ready to die because there is too much pain in the world, which he is aware of and sensitive to, stating that he is ""rightly tired of the pain"" and is ready to rest. For his last request on the night before his execution, John got to see Top Hat. When John is put in the electric chair, he asks Paul not to put the traditional black hood over his head because he is afraid of the dark. Paul agrees and after Paul shakes his hand, John is executed.\nAs Paul finishes his story, he notes that he requested a transfer to a youth detention center, where he spent the remainder of his career. Elaine questions his statement that he had a fully grown son at the time and Paul explains that he was 44 years old at the time of John\'s execution and that he is now 108 and still in excellent health. This is apparently a side effect of John giving a ""part of himself"" to Paul. Mr. Jingles, Del\'s mouse resurrected by John, is also still alive—but Paul believes his outliving all of his relatives and friends to be a punishment from God for having John executed. Paul explains he has deep thoughts about how ""we each owe a death; there are no exceptions; but, Oh God, sometimes the Green Mile seems so long."" Paul is left wondering, if Mr. Jingles has remained alive for all of this time being but a mouse, how long will it be before his own death?\n- Tom Hanks as Paul Edgecomb. Hanks and Darabont meet at an Academy Award luncheon in 1994. Stephen King stated he envisioned Hanks in the role and was happy when Darabont mentioned his name.\n- Michael Clarke Duncan as John Coffey\n- Bonnie Hunt as Jan Edgecomb\n- David Morse as Brutus ""Brutal"" Howell. Morse had not heard about the script until he was offered the role. He stated he was in tears by the end of it.\n- Doug Hutchison as Percy Wetmore\n- Sam Rockwell as ""Wild Bill"" Wharton\n- Michael Jeter as Eduard ""Del"" Delacroix\n- James Cromwell as Warden Hal Moores. Darabont wanted Cromwell from the start, and after he read the script, Cromwell was moved and agreed.\n- Patricia Clarkson as Melinda Moores\n- Barry Pepper as Dean Stanton\n- Jeffrey DeMunn as Harry Terwilliger\n- Harry Dean Stanton as Toot-Toot\n- Dabbs Greer as Old Paul Edgecomb\n- Gary Sinise as Burt Hammersmith\n- Graham Greene as Arlen Bitterbuck\n- William Sadler as Klaus Detterick\n- Bill McKinney as Jack Van Hay\nDarabont adapted the novels into a screenplay in under eight weeks.\nThe official film soundtrack, Music from the Motion Picture The Green Mile, was released on December 19, 1999 by Warner Bros. It contains 35 tracks, primarily instrumental tracks from the film score by Thomas Newman. It also contains four vocal tracks: ""Cheek to Cheek"" by Fred Astaire, ""I Can\'t Give You Anything but Love, Baby"" by Billie Holiday, ""Did You Ever See a Dream Walking?"" by Gene Austin, and ""Charmaine"" by Guy Lombardo and His Royal Canadians.\nThe film received positive reviews from critics with a 80% ""Certified Fresh"" approval rating on Rotten Tomatoes. Roger Ebert gave the film 3 and a half stars out of four, writing ""The film is a shade over three hours long. I appreciated the extra time, which allows us to feel the passage of prison months and years.""\nForbes commentator Dawn Mendez referred to the character of John Coffey as a ""\'magic Negro\' figure""—a term coined by Spike Lee to describe a stereotypical fictional black person depicted in a fictional work as a ""saintly, nonthreatening"" person whose purpose in life is to solve a problem for or otherwise further the happiness of a white person. Lee himself berated the character as one of several ""super-duper, magical Negro[es]"" depicting a skewed version of the black male, claiming it was due to the prominence of white decision makers in the media companies.\nAwards and nominations\n- Nominated – Best Performance by an Actor in a Supporting Role – Michael Clarke Duncan\n- Nominated – Best Picture – David Valdes, Frank Darabont\n- Nominated – Best Sound Mixing – Robert J. Litt, Elliot Tyson, Michael Herbick, Willie D. Burton\n- Nominated – Best Adapted Screenplay – Frank Darabont\n- Won – Best Supporting Actor – Michael Clarke Duncan\n- Won – Best Supporting Actress – Patricia Clarkson\n- Won – Best Action/Adventure/Thriller Film\n- Nominated – Best Director – Frank Darabont\n- Nominated – Best Music – Thomas Newman\n2000 Broadcast Music Incorporated Film & TV Awards\n- Won – Film Music Award – Thomas Newman\n2000 Black Reel Awards\n- Won – Theatrical – Best Supporting Actor – Michael Clarke Duncan\n2000 Blockbuster Entertainment Awards\n- Won – Favorite Actor – Drama – Tom Hanks\n- Nominated – Favorite Supporting Actor – Drama – Michael Clarke Duncan\n- Nominated – Favorite Supporting Actress – Drama – Bonnie Hunt\n2000 Bram Stoker Awards\n- Nominated – Best Screenplay – Frank Darabont\n2000 Broadcast Film Critics Association Awards\n- Won – Best Screenplay, Adaptation – Frank Darabont\n- Won – Best Supporting Actor – Michael Clarke Duncan\n- Nominated – Best Film\n2000 Chicago Film Critics Association Awards\n- Nominated – Best Supporting Actor – Michael Clarke Duncan\n- Nominated – Most Promising Actor – Michael Clarke Duncan\n- Nominated – Outstanding Directorial Achievement in Motion Pictures – Frank Darabont\n2000 Golden Globe Awards\n- Nominated – Best Supporting Actor – Motion Picture – Michael Clarke Duncan\n2000 NAACP Image Awards\n- Nominated – Outstanding Supporting Actor in a Motion Picture – Michael Clarke Duncan\n2000 MTV Movie Awards\n- Nominated – Best Breakthrough Male Performance – Michael Clarke Duncan\n2000 Motion Picture Sound Editors (Golden Reel Awards)\n- Nominated – Best Sound Editing – Dialogue and ADR – Mark A. Mangini, Julia Evershade\n- Nominated – Best Sound Editing – Effects and Foley – Mark A. Mangini, Aaron Glascock, Howell Gibbens, David E. Stone, Solange S. Schwalbe\n- Won – Favorite All-Around Motion Picture\n- Won – Favorite Dramatic Motion Picture\n- Nominated – Best Script – Frank Darabont\n- Nominated – Outstanding Performance by a Cast\n- Nominated – Outstanding Performance by a Male Actor in a Supporting Role – Michael Clarke Duncan\n- ^ a b c d ""About the Film"". http://thegreenmile.warnerbros.com/cmp/cast-fr.html. Retrieved November 01, 2011.\n- ^ The Green Mile at Rotten Tomatoes\n- ^ ""The Green Mile"". Chicago Sun-Times. http://rogerebert.suntimes.com/apps/pbcs.dll/article?AID=/19991210/REVIEWS/912100302/1023.\n- ^ Mendez, Dawn (January 23, 2009). ""The \'Magic Negro\'"". Forbes. http://www.forbes.com/2009/01/23/obama-magic-negro-oped-cx_dm_0123mendez.html. Retrieved October 26, 2009.\n- ^ ""Lee Takes Issue With Depiction of Minorities in Film"". San Jose Mercury News: p. 2E. February 7, 2001.\n- ^ ""The 72nd Academy Awards (2000) Nominees and Winners"". oscars.org. http://www.oscars.org/awards/academyawards/legacy/ceremony/72nd-winners.html. Retrieved 2011-11-19.\n- ^ Lyman, Rick (March 28, 2000). ""Oscar Victory Finally Lifts the Cloud for DreamWorks"". The New York Times. http://www.nytimes.com/library/film/oscars2000.html. Retrieved 4 November 2011.\n- Official website\n- The Green Mile at the Internet Movie Database\n- The Green Mile at AllRovi\n- The Green Mile at Rotten Tomatoes\nFilms directed by Frank Darabont Media based on Stephen King works FilmsIndividual filmsThe Shining (1980) • Cujo (1983) • The Dead Zone (1983) • Christine (1983) • Cat\'s Eye (1985) • Silver Bullet (1985) • Stand by Me (1986) • The Running Man (1987) • Tales from the Darkside: The Movie (1990) • Graveyard Shift (1990) • Misery (1990) • Sleepwalkers (1992) • The Dark Half (1993) • Needful Things (1993) • The Shawshank Redemption (1994) • Dolores Claiborne (1995) • Thinner (1996) • The Night Flier (1997) • Apt Pupil (1998) • The Green Mile (1999) • Hearts in Atlantis (2001) • Dreamcatcher (2003) • Secret Window (2004) • Riding the Bullet (2004) • 1408 (2007) • The Mist (2007) • Dolan\'s Cadillac (2009) • It (2011)Film seriesCarrieCreepshowFirestarterFirestarter (1984) • Rekindled (2002)Pet SemataryThe Lawnmower ManThe Lawnmower Man (1992) • Beyond Cyberspace (1996)The ManglerThe Dark TowerThe Dark Tower (2013) TelevisionSingle films or miniseriesGramma (1986) • Sorry, Right Number (1987) • It (1990) • Golden Years (1991) • The Tommyknockers (1993) • The Stand (1994) • The Langoliers (1995) • The Shining (1997) • Quicksilver Highway (1997) • The Revelations of Becka Paulson (1997) • Storm of the Century (1999) • Kingdom Hospital (2004) • Stephen King\'s Desperation (2006) • Nightmares and Dreamscapes: From the Stories of Stephen King (2006)Multiple films or miniseriesSalem\'s LotSometimes They Come BackRose RedRose Red (2002) • The Diary of Ellen Rimbauer (2003)SeriesThe Dead Zone (2002) • Haven (2010)\nPulp Fiction (1994) · The Usual Suspects (1995) · Fargo (1996) · L.A. Confidential (1997) · Saving Private Ryan (1998) · The Green Mile (1999) · Crouching Tiger, Hidden Dragon (2000) · Memento (2001) · Road to Perdition (2002) · Kill Bill, Volume 1 (2003) · Kill Bill, Volume 2 (2004) · Sin City (2005) · Casino Royale (2006) · 300 (2007) · The Dark Knight (2008) · Inglourious Basterds (2009) · Salt (2010)\nWikimedia Foundation. 2010.', 'The Oscars are considered the most significant and prestigious of all awards in the entertainment industry. The AMPAS presents the awards – Academy of Motion Picture Arts and Sciences – every year and recognizes the achievements of nominees and the winners through an internal assessment of the Academy’s voting members.\nThe golden statuettes were first presented way back in 1929 by Douglas Fairbanks in a dinner that he privately hosted at The Hollywood Roosevelt Hotel. This later became the first Academy Awards. Before the invention of television, station stations broadcasted the event. Today, however, the Oscar Award Ceremony is broadcast live globally. It is among the four oldest major American entertainment awards for television, theatre, and music. The other three would be The Emmys, The Tonys, and The Grammys.\nSince its inception in 1929, over 3,000 Oscars have been handed to winners in various categories. Here are some actors who have won the most Oscars ever!\nShe won her first Oscar for her performance in her first film, Morning Glory, which came out in 1933. Moreover, she won second and third Oscars in the 1960s for Guess Who’s Coming to Dinner and The Lion in Winter.\nShe won her fourth and final Oscar Award in 1981 for the film On Golden Pond at the 54th Academy Awards. Katharine Hepburn received a total of 12 nominations at the Oscars. All awards she received were for the Best Actress category. No other actor or actress has received this honor in the history of the awards.\nIn 1979 Meryl Streep won the ‘Best Supporting Actress’ award for her role in Kramer vs. Kramer. She then in 1982 bagged the ‘Best Actress Award’ for Sophie’s Choice. In 2011 she won again for The Iron Lady. In total, Meryl Streep received 21 nominations and won 3 awards.\nJack’s first win for ‘Best Actor’ was for his film One Flew Ove the Cuckoo’s Nest in 1975. He again received the Oscar for best actor in 1997 for the movie As Good as it Gets. He is also the recipient of best supporting actor for his role in Terms of Endearment in 1983.\nFrances McDormand is an actor as well as a film producer. She received three Academy Awards for acting. One for Fargo in 1996, next for Three Billboards Outside Ebbing, Missouri in 2017, and the third for her performance in Nomadland in 2020. She also received a fourth Academy Award for ‘Best Picture’ for Nomadland. This makes her the first to win an Academy Award for both producing and performing in the same film.\nThe actor has won two Best Actor Awards. The first was for The Silence of the Lambs in 1991 and the other for The Father in 2020.\nTom Hanks won his first for his outclass performance in Philadelphia in 1993 then for Forrest Gump in 1994. Both his awards were for the ‘Best Actor’ category.\nApart from Tom Hanks and Anthony Hopkins, many other actors have also won two Academy Awards. Some prominent names include Betty Davis, Marlon Brando, Spencer Tracy, Jane Fonda, Dustin Hoffman, Elizabeth Taylor, and Jodie Foster.']"	['<urn:uuid:ca786cce-5b1f-4cb4-ba8c-4d43c3d7aea7>', '<urn:uuid:c52b6259-a168-4f65-9921-5d520d4e3ab2>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T12:39:32.578853	21	97	2690
17	What is unique about Wisconsin's Driftless Area?	The Driftless Area is a distinctive region in Wisconsin characterized by unique ecological features, including algific talus slopes with special bryophyte species. It is significant enough to have its own National Wildlife Refuge, and there are efforts to use the 'driftless' label to boost Wisconsin cheese production from this region.	['Wisconsin’s “Happy Cows”? Articulating heritage and territory as new dimensions of locality\n- 608 Downloads\nIn this article, we suggest that attending to the roles of heritage and territory could help reshape local food systems in the US: first, by incorporating more producer voices and visions into the conversation; and second, by considering more deeply the characteristics of the places where food is produced. Using the Wisconsin artisanal cheese network as a case study, we have traced how artisanal producers frame their collective heritage and links to their territory. They describe a heritage that includes a cultivation of embedded, “situated” agricultural knowledge(s) and a commitment to specific quality practices as well as a connection to terroir—the specific ecologies and social contexts of their farm or region. We argue that their articulation of this heritage and terroir is both an emergent, ongoing process of adapting to changing market, cultural, and geographic conditions and an effort to recover valued traditions and practices and (re)connect to specific places.\nKeywordsHeritage Territory Terroir Alternative food systems Artisanal Cheese United States\nThis project was supported by Agriculture and Food Research Initiative Competitive Grant No. 2011-67024-30095 from the USDA National Institute of Food and Agriculture. We note that both authors contributed equally.\n- Alkon, A.H., and J. Agyeman (eds.). 2011. Cultivating food justice: Race, class, and sustainability. Cambridge: MIT Press.Google Scholar\n- Apps, J. 2004. Cheese: The making of a Wisconsin tradition. Madison, Wisconsin: University of Wisconsin Press.Google Scholar\n- Bérard, L., M. Cegarra, M. Djama, S. Louafi, P. Marchenay, B. Roussel, and F. Verdeaux. 2005. Savoirs et savoir-faire naturalistes locaux: L’originalité Française. Les Notes IDDRI, No. 8. Paris, France: Institut du Développement Durable et des Relations Internationales.Google Scholar\n- Bérard, L., and P. Marchenay. 2008. From localized products to geographical indications: Awareness and action. Bourg-en-Bresse, France: Centre National de la Recherche Scientifique.Google Scholar\n- Bouche, R., C. Bordeaux, and C. Aragni. 2010. Ancrage territorial de savoir-faire collectifs: Les fromages corses. In Le temps des Syal: Techniques, vivres et territoires, ed. J. Muchnik, and C. de Sainte Marie, 81–100. Versailles, France: Éditions Quae.Google Scholar\n- Bowen, S., and K. De Master. 2011. New rural livelihoods or museums of production? Quality food initiatives in practice. Journal of Rural Studies 27(1): 73–82.Google Scholar\n- Bowen, S., and M.S. Gaytán. 2012. The paradox of protection: National identity, global commodity chains, and the tequila industry. Social Problems 59(1): 70–93.Google Scholar\n- Bowen, S., and T. Mutersbaugh. Forthcoming. Local or localized? Exploring Franco-Mediterranean contributions to alternative food system research. Agriculture and Human Values.Google Scholar\n- Cazaux, G. 2011. Application of the concept of terroir in the American context: Taste of place and Wisconsin unpasteurized milk cheeses. Madison, Wisconsin: Dairy Business Innovation Center.Google Scholar\n- Christy, J.A., and T.A. Meyer. 1991. Bryophytes of algific talus slopes in Wisconsin’s Driftless Area. Rhodora 93: 242–247.Google Scholar\n- Constance, D., J.Y. Choi, and H. Lyke-Ho-Gland. 2008. Conventionalization, bifurcation, and quality of life: A look at certified and non-certified organic farmers in Texas. Southern Rural Sociology 23(1): 208–234.Google Scholar\n- DuPuis, E.M. 2002. Nature’s perfect food. New York, NY: NYU Press.Google Scholar\n- Fonte, M., and A. Papadopoulos (eds.). 2010. Naming food after places: Food relocalization and knowledge dynamics in rural development. Farnham: Ashgate Publishing.Google Scholar\n- Gade, D. 2004. Tradition, territory, and terroir in French viticulture: Cassis, France, and appellation contrôlée. Annals of the Association of American Geographers 94(4): 848–867.Google Scholar\n- Ginsberg-Schutz, M. 2010. The dairyland renaissance: who makes the best cheese in the world? We do. Madison Magazine, October 2010. http://www.madisonmagazine.com/Madison-Magazine/October-2010/The-Dairyland-Renaissance/. Accessed 3 Feb 2014.\n- Goodman, D., M.K. Goodman, and E.M. DuPuis. 2011. Alternative food networks: Knowledge, place, and politics. London: Routledge.Google Scholar\n- Grasseni, C. 2011. Re-inventing food: Alpine food in the age of global heritage. Anthropology of Food 8. http://aof.revues.org/6819. Accessed 14 March 2013.\n- Guthman, J. 2004. Agrarian dreams? The paradox of organic farming in California. Berkeley, California: University of California Press.Google Scholar\n- Hobbes, T. Leviathan XIII.9. 2011 . Seattle, Washington: Pacific Publishing Studio.Google Scholar\n- Jaffee, D. 2007. Brewing justice: Fair trade coffee, sustainability, and survival. Berkeley, California: University of California Press.Google Scholar\n- Janus, E. 2011. Creating Dairyland: How caring for cows saved our soil, created our landscape, brought prosperity to our state, and still shapes our way of life in Wisconsin. Madison, Wisconsin: Wisconsin Historical Society Press.Google Scholar\n- López, E., and J. Muchnik. 1997. Petites entreprises et grands enjeux: Le développement agroalimentaire local. Paris, France: L’Harmattan.Google Scholar\n- Maye, D., L. Holloway, and M. Kneafsey (eds.). 2007. Alternative food geographies: Representation and practice. Amsterdam, The Netherlands: Elsevier Science.Google Scholar\n- Muchnik, J. 2009. Localized agrifood systems: concept development and diversity of situations. Paper presented at the Annual Meeting of the Agriculture, Food, and Human Values Society and the Association for the Study of Food and Society, State College, Pennsylvania, 28–31 May 2009.Google Scholar\n- Muchnik, J., and C. de Sainte Marie (eds.). 2010. Le temps des Syal: Techniques, vivres, et territoires. Versailles, France: Éditions Quae.Google Scholar\n- Muchnik, J., J. Sanz Cañada, and G.T. Salcido. 2008. Systèmes agroalimentaires localisés: État des recherches et perspectives. Cahiers Agricultures 17(6): 513–519.Google Scholar\n- Norton, J., and B. Dilley. 2009. The master cheesemakers of Wisconsin. Madison, Wisconsin: University of Wisconsin Press.Google Scholar\n- Sanz Cañada, J. 2008. Calificación de productos, externalidades territorialies, y gobernanza territorial: Las denominaciones de origen. Paper presented at the 4th International Conference of the SYAL/SIAL Network, “Alimentacioón, Agricultura Familiar, y Territorio,” Mar del Plata, Argentina, 27–31 October 2008.Google Scholar\n- Tenenbaum, D. 2011. Wisconsin cheese could get boost from “driftless” label. University of Wisconsin News 4 February 2011. http://www.news.wisc.edu/18935. Accessed 19 March 2013.\n- Trout Unlimited. 2005. Annual report. http://www.tu.org/. Accessed 19 March 2013.\n- Trubek, A. 2008. The taste of place: A cultural journey into terroir. Berkeley, California: University of California Press.Google Scholar\n- USDA (United States Department of Agriculture). 2013. Know your farmer, know your food. http://www.usda.gov/wps/portal/usda/knowyourfarmer?navid=KNOWYOURFARMER. Accessed 29 July 2013.\n- USDA NASS (United States Department of Agriculture, National Agricultural Statistics Service). 2012. Wisconsin- specialty cheese. http://www.nass.usda.gov/Statistics_by_State/Wisconsin/Publications/Dairy/spec_ch.pdf. Accessed 16 March 2013.\n- USDA NASS (United States Department of Agriculture, National Agricultural Statistics Service). 2013. Quick Stats. http://quickstats.nass.usda.gov/. Accessed 16 March 2013.\n- USFWS (United States Fish and Wildlife Service). 2008. Driftless area national wildlife refuge. http://library.fws.gov/Refuges/Driftless08.pdf. Accessed 16 March 2013.\n- Vitrolles, D. 2011. When geographical indications conflict with food heritage protection: the case of Serrano cheese from Rio Grande de Sul, Brazil. Anthropology of Food 8. http://aof.revues.org/6809. Accessed 14 March 2013.\n- WI DNR (Wisconsin Department of Natural Resources). n.d. Driftless area National Wildlife Refuge comprehensive conservation plan: chapter two, analysis of the Driftless area. http://dnr.wi.gov/files/PDF/pubs/lf/LF0071_Ch2.pdf. Accessed 19 March 2013.']	['<urn:uuid:142f7e89-78b0-4431-af32-c36b1489998b>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	7	50	1109
18	industry momentum affect market cap performance	Industry momentum significantly influences the relationship between market capitalization and portfolio performance. Market capitalization has better predictive ability for portfolio returns in months following a positive industry return compared to months following a negative industry return. During positive industry momentum periods, winner-weighted portfolios outperform cap-weighted portfolios in 19 out of 20 industries. However, during negative industry momentum periods, this pattern reverses with cap-weighted portfolios outperforming in 18 out of 20 industries.	['The Size Effect Has a Lottery-Style Payoff\nA new research paper related mainly to:\n#25 – SIze Premium\nAuthors: McGee, Olmo\nTitle: The Size Premium As a Lottery\nWe investigate empirically the dependence of the size effect on the top performing stocks in a cross-section of risky assets separated by industry. We propose a test for a lottery-style factor payoff based on a stochastic utility model for an under-diversified investor. The associated conditional logit model is used to rank different investment portfolios based on size and we assess the robustness of the ranking to the inclusion/exclusion of the best performing stocks in the cross-section. Our results show that the size effect has a lottery-style payoff and is spurious for most industries once we remove the single best returning stock in an industry from the sample each month. Analysis in an asset pricing framework shows that standard asset pricing models fail to correctly specify the size premium on risky assets when industry winners are excluded from the construction of the size factor. Our findings have implications for stock picking, investment management and risk factor analysis.\nNotable quotations from the academic research paper:\n” Firms with small market capitalization tend to outperform larger companies. Investors are attracted to lottery-like assets with positively skewed returns because they offer a very large payoff with a small probability, which the investors overweight. This demand makes positively skewed securities overpriced and likely to earn low returns. In this article we test whether the size/market capitalization attribute, and associated factor-mimicking portfolios, receive a lottery-like payoff. The implications of this are that most small stocks do not payoff and the returns to a size strategy are driven by a small number of winners. This type of payoff can be captured through diversification but leaves an under-diversified investor exposed. The risk being that they will not include winning stocks and their resulting return expectation is negative.\nTo investigate the effect of winning stocks on the performance of investment portfolios based on the size we propose a conditional logit model for ranking different investment portfolios based on size and assess the robustness of the ranking to the inclusion/exclusion of the best performing stocks in the cross-section. This parametric choice is embedded within a stochastic utility model for explaining the investment decisions of under-diversified size investors aiming to exploit the so-called size premium. under-diversified individuals maximize their expected utility in each period by choosing the stock that is predicted to yield the highest return (highest positive skew). This choice is driven by market capitalization of the portfolio and modeled parametrically using the conditional logit model.\nIn order to obtain cross-sectional variation on the relationship between the size effect and portfolio performance we split the whole cross-section of stocks into different industries and fit the conditional logit model to each industry separately. We apply the conditional logit model at an industry-specific level across three ranked sorted portfolios based on market capitalization: a small, mid-size and big portfolio created from the stocks in each tercile of the cross-section of assets in a specific industry ranked by asset size. This exercise is repeated for 20 industries over the period January 1970 to November 2015. Our results reveal that the size effect vanishes once the top performing stocks in an industry are removed from the sample.\nOur empirical findings also highlight the role of industry momentum in determining the relationship between market capitalization and portfolio performance. Specifically, market capitalization has significantly better predictive ability for portfolio return performance in the months following a positive return in an industry than in the months following a negative industry return.\nGiven these findings, we investigate further the influence of the winning stocks in industry-specific size portfolios. In particular, we propose an alternative size portfolio that we denominate as the winner-weighted index, based on the forecast rank probabilities of stocks provided by the conditional logit model. Intuitively, those stocks that are predicted to be winners in the next period receive a larger allocation of wealth than those stocks that have a low probability of becoming winners. More formally, the allocation of wealth to each asset in the portfolio is determined by the forecast winning probabilities obtained from the conditional logit model and driven by asset size. The performance of this portfolio is compared against a cap-weighted index benchmark portfolio. The weights in the latter portfolio are also driven by market capitalization, however, in contrast to our winner-weighted index portfolio, smaller stocks within an industry receive a smaller allocation of wealth. We consider statistical and economic measures such as the Sharpe ratio, Sortino ratio, the certainty equivalent return of a mean-variance investor and portfolio turnover. We observe the existence of two regimes in portfolio performance. During positive industry momentum periods, the winner-weighted index outperforms the cap-weighted portfolio for 19 out of 20 industries, the exception being the utilities industry. This result is, however, reversed in periods of negative industry momentum for which the cap-weighted index outperforms the winner-weighted index in 18 out of 20 industries.\nOur second objective is to explore the influence of winning stocks on the size portfolio pricing factor widely used in the empirical asset pricing literature. Our empirical results for both a top-minus-bottom trading portfolio and a long-only portfolio show that standard asset pricing models are not able to adequately capture the contribution of the size premium to the overall risk premium when the winning stocks are removed from the size factor portfolio. In contrast, we note that the factor loadings (Beta’s) associated to the size portfolio pricing factor in standard models are robust to the inclusion/exclusion of the winning stocks. The removal of winning stocks is affecting the risk premium rather than the covariance of portfolios with the risk factor.”\nAre you looking for more strategies to read about? Check http://quantpedia.com/Screener\nDo you want to see an overview of our database of trading strategies? Check https://quantpedia.com/Chart\nDo you want to know how we are searching new strategies? Check https://quantpedia.com/Home/How\nDo you want to know more about us? Check http://quantpedia.com/Home/About\nFollow us on:\nShare onLinkedInTwitterFacebookRefer to a friend']	['<urn:uuid:f946f4d9-3c46-4102-b75c-da8bf324a1e3>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	6	71	1012
19	is eds hypermobility type and hypochondrogenesis inherited the same way autosomal dominant inheritance pattern	No, while both conditions are considered autosomal dominant, there are differences in their inheritance patterns. EDS Hypermobility Type can be passed down through generations, with family members in each generation being affected. In contrast, hypochondrogenesis occurs through new mutations in the COL2A1 gene and is not passed on to the next generation because affected individuals do not live long enough to have children.	"['The Hypermobility Type of EDS is characterized by loose joints and chronic joint pain. This form of Ehlers-Danlos Syndrome was formerly called type III. Ehlers-Danlos Syndrome, Hypermobility Type is a subtype of Ehlers-Danlos Syndrome.\nThe most common sign of this condition is an unusually large range of joint movement, called hypermobility. Both large and small joints are unstable, and certain joints (such as the shoulder, knee, and jaw) tend to dislocate frequently. Chronic joint and limb pain often begins early in life. People with this condition may have skin that is soft, velvety, or stretchy; however, skin symptoms vary among people. Many affected people also have a condition called mitral valve prolapse, which affects blood flow between the chambers of the heart.\nThe Hypermobility Type is the most co mmon form of Ehlers-Danlos Syndrome. The Hypermobility Type of Ehlers-Danlos Syndrome is most often inherited in an autosomal dominant pattern, which means one copy of the altered gene is sufficient to cause the disorder. In these cases, family members in each generation are usually affected, but the features of the condition may vary.\nIn some families, a recessive pattern of inheritance may be possible. This inheritance pattern means that two copies of the gene must be altered for a person to be affected by the disorder. Most often, the parents of a child with an autosomal recessive disorder are not affected but are carriers of one copy of the altered gene\nClinical diagnostic criteria and a revised nomenclature for all forms of Ehlers-Danlos Syndrome (EDS) were proposed by [Beighton et al 1998]. EDS, Hypermobility Type is distinguished from EDS, Classical Type chiefly by milder and fewer skin and soft tissue manifestations. The diagnosis of EDS, Hypermobility Type is based entirely on clinical evaluation and family history. The criteria listed below reflect those proposed by [Beighton et al 1998] as modified by the author\'s experience.\nMajor Diagnostic Criteria for the Hypermobility Type or EDS\n- Joint hypermobility, which usually is confirmed by a score of five or more on the nine-point Beighton scale\n[Beighton et al 1973], including:\n-One point for passive dorsiflexion of each fifth finger >90º\n-One point for passive apposition of each thumb to the flexor surface of the forearm\n-One point for hyperextension of each elbow >10º\n-One point for hyperextension of each knee >10º\n-One point for ability to place the palms on the floor with the knees fully extended\n- Soft or velvety skin with normal or slightly increased\nSkin hyperextensibility is assessed at a site lacking excess or loose skin and without evidence of prior trauma by gently pulling until resistance is met. Extensor surfaces of joints should not be used because of the presence of excess skin. An ideal location is the volar surface of the forearm, where the upper limit of normal is approximately 1-1.5 cm.\n- Absence of skin or soft tissue fragility, which is suggestive of other types of EDS. Examples include:\n-Spontaneous or easily induced skin cuts or tears\n-Spontaneous or easily induced tears or ruptures of tendons, ligaments, vessels, or other internal organs\n-Atrophic (""cigarette paper"") scars (although mildly atrophic scars are sometimes seen in the hypermobility type, especially in areas subject to physical stress, such as extensor surfaces and the abdominal wall)\n- Surgical complications, such as incisional hernia, wound dehiscence, or sutures tearing through tissues and failing to hold\nMinor Diagnostic Criteria for the Hypermobility Type of EDS\n- Family history of similar features without significant skin or soft tissue fragility in a pattern consistent with autosomal dominant inheritance\n- Recurrent joint dislocations or subluxations\n- Chronic joint or limb pain\n- Functional bowel disorders (functional gastritis, irritable bowel syndrome)\n- Neurally mediated hypotension or postural orthostatic tachycardia\nThe sensitivity and specificity of examination for joint hypermobility is dependent in part on the individual\'s age, gender, and medical history.\n- Young children (approximately five years of age or younger) tend to be very flexible and are therefore difficult to assess.\n- Women are, on average, more flexible than men\n- Older individuals tend to lose flexibility, and post-surgical or arthritic joints often have reduced range of motion. A history of former joint laxity or clinical demonstration of substantial laxity in multiple joints is sometimes accepted in lieu of a positive Beighton score in such cases, if the family history and other minor criteria are strongly suggestive\nThere is disagreement as to whether the ""benign familial articular hypermobility syndrome"" is identical to EDS, hypermobility type or represents a unique condition [Grahame 1999]. The distinction is subtle and relates to degree of joint complications and presence or absence of skin manifestations. However, first-degree relatives of probands with EDS, Hypermobility Type often have relatively asymptomatic joint laxity and mild or absent skin manifestations. Therefore, the benign hypermobility syndrome is included as EDS, Hypermobility Type for this review.\nThe resources on this site should not be used as a substitute for professional medical care or advice. Users seeking information about Ehlers-Danlos Syndrome, should consult with a qualified healthcare professional.', ""|http://ghr.nlm.nih.gov/ A service of the U.S. National Library of Medicine®|\nHypochondrogenesis is a rare, severe disorder of bone growth. This condition is characterized by a small body, short limbs, and abnormal bone formation (ossification) in the spine and pelvis.\nAffected infants have short arms and legs, a small chest with short ribs, and underdeveloped lungs. Bones in the skull develop normally, but the bones of the spine (vertebrae) and pelvis do not harden (ossify) properly. The face appears flat and oval-shaped, with widely spaced eyes, a small chin, and, in some cases, an opening in the roof of the mouth called a cleft palate. Individuals with hypochondrogenesis have an enlarged abdomen and may have a condition called hydrops fetalis in which excess fluid builds up in the body before birth.\nAs a result of these serious health problems, some affected fetuses do not survive to term. Infants born with hypochondrogenesis usually die at birth or shortly thereafter from respiratory failure. Babies who live past the newborn period are usually reclassified as having spondyloepiphyseal dysplasia congenita, a related but milder disorder that similarly affects bone development.\nHypochondrogenesis and achondrogenesis, type 2 (a similar skeletal disorder) together affect 1 in 40,000 to 60,000 newborns.\nHypochondrogenesis is one of the most severe conditions in a spectrum of disorders caused by mutations in the COL2A1 gene. This gene provides instructions for making a protein that forms type II collagen. This type of collagen is found mostly in the clear gel that fills the eyeball (the vitreous) and in cartilage. Cartilage is a tough, flexible tissue that makes up much of the skeleton during early development. Most cartilage is later converted to bone, except for the cartilage that continues to cover and protect the ends of bones and is present in the nose and external ears. Type II collagen is essential for the normal development of bones and other connective tissues that form the body's supportive framework. Mutations in the COL2A1 gene interfere with the assembly of type II collagen molecules, which prevents bones and other connective tissues from developing properly.\nChanges in this gene are associated with hypochondrogenesis.\nHypochondrogenesis is considered an autosomal dominant disorder because one copy of the altered gene in each cell is sufficient to cause the condition. It is caused by new mutations in the COL2A1 gene and occurs in people with no history of the disorder in their family. This condition is not passed on to the next generation because affected individuals do not live long enough to have children.\nThese resources address the diagnosis or management of hypochondrogenesis and may include treatment providers.\nYou might also find information on the diagnosis or management of hypochondrogenesis in Educational resources and Patient support.\nGeneral information about the diagnosis (http://ghr.nlm.nih.gov/handbook/consult/diagnosis) and management (http://ghr.nlm.nih.gov/handbook/consult/treatment) of genetic conditions is available in the Handbook. Read more about genetic testing (http://ghr.nlm.nih.gov/handbook/testing), particularly the difference between clinical tests and research tests (http://ghr.nlm.nih.gov/handbook/testing/researchtesting).\nTo locate a healthcare provider, see How can I find a genetics professional in my area? (http://ghr.nlm.nih.gov/handbook/consult/findingprofessional) in the Handbook.\nYou may find the following resources about hypochondrogenesis helpful. These materials are written for the general public.\nYou may also be interested in these resources, which are designed for healthcare professionals and researchers.\nFor more information about naming genetic conditions, see the Genetics Home Reference Condition Naming Guidelines (http://ghr.nlm.nih.gov/ConditionNameGuide) and How are genetic conditions and genes named? (http://ghr.nlm.nih.gov/handbook/mutationsanddisorders/naming) in the Handbook.\nAsk the Genetic and Rare Diseases Information Center (https://rarediseases.info.nih.gov/gard).\nautosomal ; autosomal dominant ; bone formation ; cartilage ; cell ; cleft palate ; collagen ; dysplasia ; gene ; hydrops fetalis ; ossification ; palate ; pelvis ; protein ; respiratory ; spectrum ; tissue\nYou may find definitions for these and many other terms in the Genetics Home Reference Glossary.\nThe resources on this site should not be used as a substitute for professional medical care or advice. Users seeking information about a personal genetic disease, syndrome, or condition should consult with a qualified healthcare professional. See How can I find a genetics professional in my area? (http://ghr.nlm.nih.gov/handbook/consult/findingprofessional) in the Handbook.""]"	['<urn:uuid:f89f9576-1802-496b-8e1c-1196600aa6f3>', '<urn:uuid:6ae9cf75-c222-40d9-986d-7f23b09b4398>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	14	63	1518
20	Does automated compliance help banks, and what security protocols support it?	Automation enhances banking compliance by efficiently tracking regulatory changes, updating policies, and generating reports with minimal manual intervention, liberating resources for strategic initiatives. This automated compliance is supported by comprehensive security measures including sophisticated algorithms that analyze patterns for suspicious activities, continuous transaction monitoring, and specific security protocols. These protocols include implementing dual control for transactions, utilizing dedicated firewalls, creating strong passwords with mixed case letters and special characters, prohibiting shared usernames, regularly changing passwords, installing commercial anti-virus software, and maintaining up-to-date security patches on all systems.	"[""In the dynamic realm of finance, the banking sector is\ncurrently undergoing a profound transformation propelled by advancements in\ntechnology. At the forefront of this paradigm shift is automation – a\nfundamental driver of change that transcends mere industry jargon. Rather, it\nsignifies a pivotal reconfiguration in the operational landscape of financial\ninstitutions, yielding a plethora of advantages that extend to both banks and\nOptimized Efficiency and Precision:\nThe integration of automation in the retail banking sector catalyzes a substantial enhancement in operational\nefficiency. Tasks characterized by routine and time-intensive attributes, such\nas data entry, transaction processing, and account reconciliation, are\nseamlessly streamlined, significantly reducing the probability of errors.\nBy delegating these repetitive\nresponsibilities to machines, human personnel are not only liberated from\nmundane tasks but also empowered to redirect their efforts toward strategic and\nintricate activities that demand critical thinking, creativity, and innovative\nproblem-solving approaches, fostering a culture of continuous improvement and\nAutomation stands as a potent instrument for cost reduction\nwithin banking operations. Through the automation of manual processes,\nfinancial institutions can curtail their reliance on extensive workforces,\nsubsequently lowering operational expenses.\nAutomated systems operate tirelessly without breaks or\nvacations, thereby maximizing cost-effectiveness. This efficiency not only\naccrues benefits for the banks themselves but also translates into cost savings\nfor customers, manifesting as reduced fees and more favorable interest rates.\nConsequently, this strategic implementation of automation\nnot only enhances the financial viability of banks but also directly\ncontributes to fostering a more economically favorable environment for their\nElevated Customer Service:\nAutomation assumes a pivotal role in elevating customer\nservice standards in the banking sector. Chatbots and virtual assistants,\nfueled by artificial intelligence, deliver instantaneous responses to customer\nqueries, thereby enhancing response times and overall customer satisfaction.\nAutomated systems streamline and expedite transactions,\nenhancing the overall customer experience. Moreover, automation empowers banks\nto provide personalized services by scrutinizing customer data and tailoring\nrecommendations based on individual financial behaviors.\nRobust Fraud Prevention and Security Measures:\nParallel to the integration of automation in banking, there\nis a concerted effort to fortify security measures. Automated systems\ncontinually scrutinize transactions for anomalies, swiftly detecting potential\nfraud in real time.\nThrough sophisticated algorithms, these systems analyze\nintricate patterns, identifying and flagging suspicious activities promptly.\nThis proactive approach not only aids banks in preventing\nunauthorized access and safeguarding their customers' assets but also goes a\nlong way in fostering trust among clients who rely on banks to maintain the\nutmost security and confidentiality of their sensitive information.\nEfficient Compliance Protocols:\nOperating within a highly regulated environment, banks\ngrapple with the challenge of staying abreast of ever-evolving rules and\nregulations. Automation in banking not only ensures the efficiency of\ncompliance processes but also guarantees accuracy.\nAutomated systems adeptly track regulatory changes, swiftly\nupdating policies and generating reports with minimal manual intervention.\nThis not only mitigates the risk of non-compliance but also\nliberates resources for redirection towards more strategic initiatives,\nallowing banks to proactively engage in regulatory adaptation while fostering\ninnovation in tandem with the evolving financial landscape.\nExpedited Decision-Making Processes:\nAutomation expedites decision-making within the banking\nsector by harnessing advanced analytics and machine learning algorithms. These\npowerful tools swiftly analyze vast datasets, providing invaluable insights\ninto various aspects such as evaluating credit risk, detecting fraudulent\nactivities, and making investment decisions.\nThe agility afforded by automation is paramount in\nnavigating the intricacies of a competitive financial landscape, where swift\nand well-informed decision-making stands as a crucial differentiator,\ndetermining the line between success and failure in a rapidly evolving\nEnhanced Accessibility and Inclusivity:\nAutomation contributes significantly to rendering banking\nservices more accessible to a wider demographic. Digital platforms and mobile\nbanking applications, driven by automation, empower customers to manage their\nfinances conveniently from any location and at any time.\nThis inclusivity is particularly advantageous for\nindividuals in remote areas or those with physical limitations, ensuring equal\naccess to banking services.\nMoreover, it breaks down traditional barriers, fostering\nfinancial literacy and inclusion, thereby empowering individuals who may have\nbeen historically underserved or excluded from mainstream banking services.\nThe advantages of automation in banking extend across\nvarious facets, positively impacting efficiency, cost-effectiveness, customer\nservice, security, compliance, decision-making, accessibility, and innovation.\nAs technology progresses, the banking industry must persist at the forefront of\nautomation to remain competitive and meet the evolving needs of customers in an\nincreasingly digital world. The era of automated banking is now a reality,\nushering in a new frontier of possibilities and opportunities for growth and"", ""Online Privacy Practices\nOnline Banking Protection\nReport Suspicious Emails\nProtect Your Identity\nCorporate Account Takeover\nWhat is Corporate Account Takeover?\n“Corporate account takeover” is when cyber-thieves gain control of a business’ bank account(s) by stealing the business’ valid online banking credentials. Although there are several methods being employed to steal credentials, the most prevalent involves malware that infects the business’ computer workstations and laptops.\nA business can become infected with malware via infected documents attached to an email or a link contained within an email that connects to an infected Web site. In addition, malware can be downloaded to users’ workstations and laptops by visiting legitimate Web sites - especially social networking sites - and clicking on the documents, videos or photos posted there. This malware can also spread across a business’ internal network.\nRecommendations to Business and Corporate Customers\nAlthough Guaranty Bond Bank uses technologies to prevent, detect and respond to fraudulent transactions, there are additional controls that you can institute within your organization to further reduce the risk of Corporate Account Takeover and fraud.\nReconcile your banking transactions on a daily basis.\nInitiate ACH and wire transfer payments under dual control, with a transaction originator and a separate transaction authorizer.\nEmploy best practices to secure computer systems in their business including but not limited to:\nIf possible, carry out all online banking activities from a stand-alone, hardened and completely locked down computer system from which email and Web browsing are not possible.\nBe suspicious of emails purporting to be from a financial institution, government department or other agency requesting account information, account verification or banking access credentials such as usernames, passwords, PIN codes and similar information. Opening file attachments or clicking on web links in suspicious emails could expose the system to malicious code that could hijack your computer.\nInstall a dedicated, actively managed firewall, especially if the business has a broadband or dedicated connection to the Internet, such as DSL or cable. A firewall limits the potential for unauthorized access to a network and computers.\nCreate strong passwords with at least 10 characters that include a combination of mixed case letters, numbers and special characters.\nProhibit the use of “shared” usernames and passwords for online banking systems.\nUse a different password for each Web site that is accessed.\nChange the password a few times each year.\nNever share username and password information for Online Services with third-party providers.\nLimit administrative rights on users’ workstations to help prevent the inadvertent downloading of malware or other viruses.\nEducate employees on good cyber security practices to include how to avoid having malware installed on the business computer.\nInstall commercial anti-virus and desktop firewall software on all computer systems. Free software may not provide protection against the latest threats compared with an industry standard product.\nEnsure virus protection and security software are updated regularly.\nEnsure computers are patched regularly particularly operating system and key application with security patches. It may be possible to sign up for automatic updates for the operating system and many applications.\nConsider installing spyware detection programs.\nClear the browser cache before starting an Online Banking session in order to eliminate copies of web pages that have been stored on the hard drive. How the cache is cleared will depend on the browser and version. This function is generally found in the browser's preferences menu.\nVerify use of a secure session (https:// not http://) in the browser for all online financial transactions, including online banking.\nAvoid using automatic login features that save usernames and passwords for online banking.\nNever leave a computer unattended while using any online banking or investing service.\nNever access bank, brokerage or other financial services information at Internet cafes, public libraries, etc. Unauthorized software may have been installed to trap account number and sign on information leaving the customer vulnerable to possible fraud.\nProperly log out of each online banking session and close all browser windows. Simply closing the active window may not be enough.\nWhen finished with the computer, turn it off or disconnect it from the Internet.\nAlso consider utilizing a security expert to test the network or run security software that will aid you in identifying known vulnerabilities.\nConsumer account holders receive protections from errors relative to electronic funds transfers under Regulation E; however we want to make you aware that Regulation E does not cover Business accounts.\nThe alerts and notifications offered by the bank are designed as flags to warn you if your system may have been compromised and we strongly encourage you to take advantage of them.\nWarning signs that your system/network may have been compromised include:\nInability to log into online banking (thieves could be blocking customer access so the customer won’t see the theft until the criminals have control of the money);\nDramatic loss of computer speed;\nChanges in the way things appear on the screen;\nComputer locks up so the user is unable to perform any functions;\nUnexpected rebooting or restarting of the computer;\nUnexpected request for a one time password (or token) in the middle of an online session;\nUnusual pop-up messages, especially a message in the middle of a session that says the connection to the bank system is not working (system unavailable, down for maintenance, etc.);\nNew or unexpected toolbars and/or icons; and Inability to shut down or restart the computer.\nContact the bank immediately if you suspect your network may have been compromised. Call 1-888-572-9881 and ask for the Internet Banking Department.\nfor more information on Corporate Account Takeover.\nGuaranty Bond Bank, N.A.\nEqual Housing Lender\n© 2014 Guaranty Bancshares, Inc. All Rights Reserved""]"	['<urn:uuid:174c7207-aca2-4b90-967b-6eef8a8cafb5>', '<urn:uuid:4f4f47e1-4502-463c-9a95-b4a5e1778a50>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	11	87	1672
21	teach students reading individual activities group work	Teaching effectively requires a combination of individual and collaborative approaches. From an individual perspective, teachers must possess deep content knowledge in their domain and pedagogical content knowledge about how to teach specific subjects. They develop this expertise slowly over 5-10 years of practice. However, constructivist teaching emphasizes the importance of allowing students to connect to the learning environment through group activities such as problem-based learning, inquiry activities, and dialogues with others. Tools and activities must be carefully designed to enable, structure, and guide social interactions for effective learning. Specific strategies include scaffolding to help learners understand complex tasks, modeling through think-aloud problem solving, and coaching to probe students' thinking. The goal is to help students construct knowledge both individually and collectively while developing critical thinking skills.	"[""One of the main goals of cognitive psychology is to understand the relationship between knowledge and learning. To do so, researchers developed the information processing model (IPM) in the early 1950s, which has been used as the modal model of cognition since that time. The IPM consists of three main components, including sensory memory, working memory, and long-term memory (Neath & Surprenant, 2003). Sensory memory processes incoming sensory information for very brief periods of time, usually one-half to three seconds. The amount of information held at any given moment in sensory memory is limited to five to seven discrete elements such as letters of the alphabet or pictures of human faces. Working memory refers to real-time information processing in which meaning is assigned to incoming information from a text, pictures, or math problem. Long-term memory refers to a permanent repository of knowledge in memory. To use a computer analogy, sensory memory corresponds to inputting information via the keyboard, working memory corresponds to information current on the computer screen, and long-term memory corresponds to the computer's hard drive.\nInformation processing theory assumes that ongoing mental activity in working memory is aided by different types of information in long-term memory that supports thinking and problem solving. At least three categories of knowledge are stored in long-term memory, including declarative, procedural, and self-regulatory knowledge (Anderson, 1976; Tulving, 1972). Declarative knowledge refers to the facts and concepts. Procedural knowledge refers to how to do things. Self-regulatory knowledge refers to knowledge individuals have about themselves as learners, what they know, and how to control their learning. All three types of knowledge are important. However, even large amount of declarative and procedural knowledge, without self-regulatory knowledge to support it, does little to help people survive and adapt successfully.\nDeclarative knowledge is a broad category that includes facts, concepts, and the relationships between concepts that lead to an integrated conceptual understanding of a domain of knowledge. Declarative knowledge includes thousands of facts such as the names of colors, numbers, coins, and trees. Concepts consist of two or more units of factual information that are used to understand a broader phenomenon such as human rights or social justice. Often concepts are phenomena that can be described abstractly, such as freedom or happiness, even though these phenomena do not exist in the physical world. Declarative knowledge also includes integrated conceptual knowledge that is sometimes referred to as structural knowledge or mental models (Halpern, 2003).\nOne of the most important organizational units in memory is the schema, which refers to an organized body of information about some distinct domain of knowledge. For example, every adult has a “car schema” in which information about different types of cars is organized. A car schema could be organized in several ways using either the cost or size of the car to generate subcategories. Anyone asked to name a luxury car could quickly name cars such as Rolls Royce or Bentley as examples. Most other people who share the same cultural group would have this information organized in a similar way as well.\nProcedural knowledge is knowledge about how to do things, ranging from simple action sequences such as brushing one's teeth, to complex actions such as driving a car. Most adults possess an enormous amount of procedural knowledge, which enables them to perform complex activities such as grocery shopping easily because those procedures are automated though practice. Although there are many different types of action sequences, there are three sequences of special importance, including complex scripted actions, algorithms, and heuristics that are stored as single entities in memory.\nScripts refer to extended action sequences and plans that are stored in memory as single units of knowledge. Each person possesses thousands of scripts, for activities such as getting dressed, driving a car, dining at restaurants, and social interactions that save enormous amounts of time because scripts can be activated intact from memory. Scripts are analogous to schemata. Whereas schemata help individuals organize declarative knowledge about a topic or domain, scripts help people organize and remember steps in a complicated action sequence. Algorithms and heuristics can be thought of as “mini-scripts.” An algorithm is a rule for solving a specific problem that always works, whereas a heuristic is a rule of thumb for solving a problem that often works, but not always. For example, an algorithm could be used to compute the average of 1,000 scores by adding all the scoring and dividing the total by the number of scores. A simple heuristic could also be used to estimate the average by sampling seven scores at random, rank ordering the scores, and using the middle score as an estimate.\nSelf-regulatory knowledge is knowledge about how to regulate one's memory, thought, and learning (Schunk & Zimmerman, 2006). Declarative and procedural knowledge alone are not sufficient to be an adaptive learner. In addition, individuals must possess knowledge about themselves as learners and about the skills they need to learn effectively. Self-regulatory knowledge can be divided into two types, including domain specific knowledge and domain general knowledge (Alexander, 2003). The former is knowledge individuals possess about themselves with regard to a domain such as mathematics or a sub-domain such as geometry. In contrast, the latter includes general knowledge such as learning strategies that enable people to adapt and self-regulate across all domains.\nDomain specific knowledge refers to knowledge that is encapsulated within a particular domain of learning such as mathematics, history, and literature. Sometimes domain specific knowledge is referred to as topic knowledge, although this term suggests knowledge about a specific topic such as geometry within a broader domain such as mathematics. Domain specific knowledge is extremely important in the development of expertise and skilled problem solving (Ericsson, 2003). Cognitive psychologists once believed that it was possible to capture the knowledge of experts through interviews and observation, and in turn, help novices become experts quickly. However, researchers discovered that experts become experts slowly through years of hard work, deliberate practice, and guidance from other experts. Most experts have deep knowledge in one domain, yet shallow knowledge in other domains, due in large part to the amount of time they invest in developing expertise in their chosen domain. Expertise in one domain usually does not transfer spontaneously to other domains, although it can be facilitated through direct instruction and analogical cues, which help the learner understand the relationship between two different problems.\nDomain general knowledge refers to knowledge that is equally useful to learners across domains and topics. Domain general knowledge often is referred to as meta-cognitive knowledge, which includes knowledge of cognition and regulation of cognition (Schraw, 2006). The former includes strategy knowledge and conditional knowledge, while the latter includes knowledge of regulatory skills such as planning, monitoring, and evaluation of learning. Metacognitive knowledge enables learners to identify problems and self-correct by changing strategies.\nLike other experts, skilled teachers possess different types of knowledge that facilitates classroom practice. Shulman (1987) suggested that skilled teachers possess knowledge about domain content, pedagogy, learners and student development, as well as educational contexts, and educational ends, purposes and values. Many educators view content and pedagogical knowledge as essential to effective teaching. Content knowledge refers to knowledge in a particular domain, such as mathematics, science, social studies, reading, and language arts. Pedagogical content knowledge has been defined as “a collection of teacher professional constructions, as a form of knowledge that preserves that planning and wisdom of practice that the teacher acquires when repeatedly teaching a certain topic” (Hashweh, 2005, p. 273).\nContent knowledge is domain-specific in nature, whereas many teachers have endorsed domain-general pedagogy that emphasizes constructivist teaching. The fundamental idea of constructivism is allowing students to connect to the learning environment through problem-based learning, inquiry activities, and dialogues with others. By allowing students to construct knowledge as learners, the educational goal is to help them think critically about concepts. There are many strategies that a teacher might employ when teaching a particular content areas: (a) scaffolding, which allows the learner to make sense of complex tasks; (b) modeling, which requires the teacher to think aloud about problem solving; while (c) coaching, guiding, and advising requires the teacher to probe the students' thinking. Experiences should be genuine and relevant to the learners and inquiry is used as an approach for students to engage in discovery learning.\nKnowledge facilitates information processing and long-term learning by providing an integrated conceptual network of information in long-term memory. Knowledge in isolation (i.e., inert knowledge) is of little value, whereas organized knowledge is powerful because it enables people to sort and store information in memory, predict and judge, and evaluate their learning accurately. Knowledge also enables individuals to process information more efficiently (Neath & Surprenant, 2003).\nRecent research emphasizes the importance of constructed knowledge, distributed cognition, and distributed knowledge. Constructivism refers to the assumption that knowledge is constructed actively by learners, rather than transmitted passively through lecture, discussion, or observation. Constructivism assumes that active learning is better because knowledge is understood in a deeper, more relevant way. An extension of constructivism is the assumption that knowledge and learning are more sophisticated when mutually shared across multiple learners in an active dialogue. This is referred to often as distributed cognition. In contrast, distributed knowledge refers to knowledge that is distributed across two or more individuals, but may be distributed across hundreds of individuals, such as knowledge about complex technological products. Knowledge can also be distributed between humans and human artifacts such as books and tools such as calculators.\nDistributed cognition and knowledge are topics of considerable debate for both practical and theoretical reasons. Many educators assume that mutually constructed meaning is more dynamic than individually constructed meaning, and some believe that knowledge exists only as a distributed set of beliefs and assumptions across multiple individuals (Zhang & Patel, 2006). In addition, many have argued that complex ideas and knowledge require multiple contributors to exist at all. From a theoretical standpoint, researchers are interested in how to best foster distributed cognition across multiple people and/or machines, and how to represent knowledge in human and machine databases in a distributed manner.\nLike students, teachers possess different types of knowledge that are essential to effective teaching (Shulman, 1987). Teachers develop this knowledge slowly over time, often taking 5 to 10 years of teaching practice to develop deep expertise. Both students and teachers construct most higher-order conceptual knowledge through personal experiences, reflection on experiences, and dialogue with other students and teachers (Ericsson, 2003). Individuals also construct metacognitive knowledge that enables them to self-regulate within their domain of expertise. Constructed executive knowledge is assumed to be stored in long-term memory in sophisticated schemata and scripts that enable the individual to perform a variety of complex skills with a high degree of efficiency.\nAlexander, P. A. (2003). The development of expertise: The journey from acclimation to proficiency. Educational Researcher, 32, 10–14.\nAnderson, J. R. (1976). Language, memory, and thought. Mahwah, NJ: Erlbaum.\nEricsson, K. A. (2003). The acquisition of expert performance as problem solving. In J. E. Davidson & R. J. Sternberg (Eds.), The psychology of problem solving (pp. 31–83). Cambridge, England: Cambridge University Press.\nHalpern, D. F. (2003). Thought and knowledge: An introduction to critical thinking (4th ed.). Mahwah, NJ: Erlbaum.\nHashweh, M. Z. (2005). Teacher pedagogical constructions: a reconfiguration of pedagogical content knowledge. Teacher and Teaching: Theory and Practice 11(3), 273–292.\nNeath, I., & Surprenant, A. M. (2003). Human memory: An introduction to research, data, and theory (2nd ed.). Pacific Grove, CA: Brooks/Cole Publishing.\nSchraw, G. (2006). Knowledge: Structures and processes. In P. Alexander & P. Winne (Eds.), Handbook of educational psychology (2nd ed., pp. 245–264). San Diego, CA: Academic Press.\nSchunk, D. H., & Zimmerman, B. J. (2006). Competence and control beliefs: Distinguishing the means and the ends. In P. Alexander & P. Winne (Eds.), Handbook of educational psychology (2nd ed., pp. 349–367). San Diego, CA: Academic Press.\nShulman, L. (1987). Knowledge and teaching: foundations of the new reform. Harvard Educational Review 57, 1–22.\nTulving, E. T. (1972). Episodic and semantic memory. In E. Tulving & W. Donaldson (Eds.), Organization of memory (pp. 381–403). San Diego, CA: Academic Press.\nZhang, J., & Patel, V. (2006). Distributed cognition, representation, and affordance. Pragmatics & Cognition 14, 333–341."", 'Learning to explain, justify, critique, etc. are essential skills for today’s citizens, for scientists, and in many other careers. These activities are intrinsically social. Further, conceptually challenging content is often best learned by working together with other learners. However, merely asking students to “work together” is not enough to lead to positive learning outcomes. Tools and activities must be designed to enable, structure, and guide social interactions to facilitate effective learning.\nCollaborative learning engages students to work as a team in learning together, and is not just a matter of dividing up work among members of a team. When collaborative learning is working well, students engage in building on each others’ contribution, and individuals learn from their team as the team advances a shared outcome. Effective collaborative learning teams are able to manage both their team relationships and progress on tasks, and are able to monitor and reflect on their process. Terms used to indicate the essence of learning together include: joint problem solving, intersubjectivity, shared/collective/group/distributed cognition, collective consciousness, and transactive discourse.\nTheories of collaborative learning give shape to design and analysis of collaborative learning. From a constructivist perspective, learning occurs as students make sense of their experience. A social experience can be rich in new ideas, conflict with one’s own ideas, and high expectations for the quality of ideas. From a social cognition perspective, learners’ efforts to find common ground and share information with others can creates optimal conditions for developing knowledge, with appropriate levels of challenge and support. A participatory perspective focuses on the process of becoming an effective member of a community, and includes learning the social norms, practices, language, activities and tools of the community — while also developing one’s individual skill in doing the work of the community. Each of these theories has had a profound influence on how designers and researchers address collaborative learning.\nThe core agenda of collaborative learning in cyberlearning is the design and investigation of social technologies to influence the interactions of students in groups, and thereby to increase learning in the group. Targets for design and investigation can include motivational, social, and cognitive dimensions of interacting in groups. Tools often aim to better support specific features of social interaction (argumentation, negotiation, communication, explanation), enable groups to represent social knowledge (improving social awareness and helping students in capturing, referring to, visualizing, organizing, analyzing, critiquing, etc. each other’s ideas.), or guide teams through activities (scripting, scaffolding or coaching). Tools often make features of collaborative learning more visible to members of the group and more available for action by the group. Designs for collaborative learning recognize that students sometimes work as individuals, in small groups and in large ensembles (such as a classroom or a online discussion group) and that effective environments support students across these modalities. Some designs seek to help teachers orchestrate many simultaneous or sequenced social learning activities.\nImportant elements of collaborative learning include:\n- motivation for the effort of working with another learner\n- joint attention, students are looking at the same things\n- mutual engagement, students are actively involved with each other\n- individual agency, each student has responsibility and opportunity for action in the team and for learning from the team’s work\n- group action and accountability, students are discussing, making or problem solving together and the result of their group’s work matters\n- design of roles, responsibilities and measures of progress\n- constructive discourse patterns, including making and acknowledging contributions, finding common ground, providing and receiving help, etc.\n- monitoring and reflecting on teamwork (i.e., meta-cognition and self-regulation)\n- orchestrating participation across activities, places, roles, etc.\nResearch that investigates collaborative learning almost always include methods for analyzing students’ interactions, conversations, and participation in teams or groups — analyzing the process of collaborative learning is important. Research can look at learning as students interact face-to-face or at a distance. Data is often captured by audio or video recording, but also by capturing what students do with technology. Emerging technologies such as eye-tracking can also help capture joint attention. Outcome measures can include individual growth or can focus on the increased capacity of the group — and outcomes can be cognitive (knowledge and skill), interpersonal (membership in a group), or social (skills in learning together). Research can be framed as (a) iterative, design-based research or (b) as comparative experiments or (c) in terms of socio-cultural analysis.\nResearchers in collaborative learning often share their work through the activities of the International Society of the Learning Sciences, including a conference series and journal.\nScripting and Self-Regulation. Learners do not often organize themselves well for collaborative learning spontaneously. A strong literature in computer-supported collaborative has designed and investigated the impacts of scripts that structure the interaction of group members so as to improve collaborative learning processes and outcomes. A emerging tension concerns how groups can learn to regulate their own learning over time, so that less explicit scripting is necessary.\nEmerging Technologies. Traditional collaborative learning research often involved computers and conventional networks. Emerging technologies can focus on devices that support mutual awareness, tangible computing, mobiles devices, design of furniture and rooms (as well as virtual spaces) for collaborative learning, and ways to leverage social networking.\nTensions around “Networked Individualism.” Our increasingly networked society tends to overuse the word “collaboration” wherever communication occurs. In contrast, work in collaborative learning has emphasized strong relationships among learners, joint action, and collective outcomes. How can collaborative learning leverage the strength and ubiquity of weak ties? Conversely, how can more emphasis on learning together be supported in commonplace, large scale use of communication technology? How should theories expand to acknowledge the dynamic boundaries between individual and social learning?\nChallenges in Methods and Measurement. As previously mentioned, collaborative learning often occurs across different groupings, different times, and different spaces. Multiple units of analysis are often important, and bridging analyses at different levels of interaction is difficult. Accounting for both individual and group outcomes of collaborative learning remains an open issue. Analysis of student interaction in teams has often involved laborious methods, and finding ways to simplify or automate analysis of collaborative learning processes is important.\nExamples of NSF Cyberlearning projects that overlap with topics discussed in this primer (see project tag map).\nCollaborative and/or participatory learning\n- EXP: Improving Student Help-Giving with Ubiquitous Collaboration Support Technology\n- NetStat: EAGER: A Representation and Communication Infrastructure for Classroom Collaboration in Data Modeling and Statistics\n- EXP: Agile Research Studios: Scaling Cognitive Apprenticeship to Advance Undergraduate and Graduate Research Training in STEM\n- DIP: Improving Collaborative Learning in Engineering Classes Through Integrated Tools\n- NetStat: EAGER: A Representation and Communication Infrastructure for Classroom Collaboration in Data Modeling and Statistics\n- CAP: CSCL 2017 Making a Difference: Prioritizing Equity and Access in CSCL Doctoral Consortium and Early Career Workshops\n- DIP: Digital Studios for Social Innovation Networks\n- EXP: Fostering Collaborative Drawing and Problem Solving through Digital Sketch and Touch\n- DIP: Connecting Idea Threads across Communities for Sustained Knowledge Building\n- EAGER: WeatherBlur\nMore posts: collaborative-andor-participatory-learning\nEducation Psychologist Special Issue: Theoretical Underpinnings of Successful Computer-Supported Collaborative Learning\nReferences and key readings documenting the thinking behind the concept, important milestones in the work, foundational examples to build from, and summaries along the way.\nBarron, B. & Roschelle, J. (2009). Shared cognition. In Anderman, Eric (ed.), Psychology of Classroom Learning: An Encyclopedia, pp. 819-823. Detroit: Macmillan Reference USA.\nDillenbourg, P. (1999). What do you mean by collaborative learning?. Collaborative-learning: Cognitive and Computational Approaches., 1-19.\nDillenbourg, P., Järvelä, S., & Fischer, F. (2009). The evolution of research on computer-supported collaborative learning. In Technology-enhanced learning (pp. 3-19). Springer Netherlands.\nFischer, F., Kollar, I., Mandl, H., & Jaake, J. M. (2007). Scripting Computer-Supported Collaborative Learning: Cognitive, Computational and Educational Perspectives. New York: Springer.\nKirschner, P. A., & Erkens, G. (2013). Toward a framework for CSCL research. Educational Psychologist, 48(1), 1-8.\nResnick, L. B. Levine, J. M., & Teasley, SD (Eds.).(1991). Perspectives on socially shared cognition.\nRoschelle, J. (1992). Learning by collaborating: Convergent conceptual change. The journal of the learning sciences, 2(3), 235-276.\nRoschelle, J., & Teasley S. D. (1995). The construction of shared knowledge in collaborative problem solving. In C. E. O’Malley (Ed), Computer-supported collaborative learning. (pp. 69-97). Berlin: Springer-Verlag.\nStahl, G., Koschmann, T., & Suthers, D. D. (2006). Computer-supported collaborative learning: An historical perspective. In R. K. Sawyer (Ed.), Cambridge handbook of the learning sciences (pp. 409-426). Cambridge, UK: Cambridge University Press.\nSuthers, D. D. (2006). Technology affordances for intersubjective meaning-making: A research agenda for CSCL. International Journal of Computer Supported Collaborative Learning, 1(3), 315-337.\nPublications from NSF-funded Cyberlearning Projects\nSwartz, M., Li, J., & Wanless, S. (2016, March). Peg + Cat: Adventures in learning. Poster session presented at the 2016 Advancing Informal STEM Learning (AISL) PI Meeting, Bethesda, MD.\nWaters, A., Studer, C., & Baraniuk, R. (2014). Collaboration-Type Identification in Educational Datasets. Journal of Educational Data Mining, Vol. 6(1), pp. 28-52.\nAdamson, D., Dyke, G., Jang, H., & Rosé, C. P. (2014). Towards an agile approach to adapting dynamic collaboration support to student needs. International Journal of Artificial Intelligence in Education, Vol. 24(1), pp. 92-124.\nCarroll, J. M., Jiang, H., & Borge, M. (2015). Distributed collaborative homework activities in a problem-based usability engineering course. Education and Information Technologies, Vol. 20(3), pp. 589-617.\nBrown, R., Lynch, C. F., Eagle, M., Albert, J., Barnes, T., Baker, R., & McNamara, D. (2015). Good communities and bad communities: Does membership affect performance. In Proceedings of the 8th International Conference on Educational Data Mining (pp. 612-614).Madrid, Spain: Educational Data Mining.\nBrown, R., Lynch, C., Wang, Y., Eagle, M., Albert, J., Barnes, T., & McNamara, D. (2015, June). Communities of performance & communities of preference. In Proceedings of the 2nd International Workshop on Graph-Based Educational Data Mining. Madrid, Spain: Educational Data Mining.\nD’Angelo, C. M., Roschelle, J., Bratt, H., Shriberg, L., Richey, C., Tsiartas, A., & Alozie, N. (2015). Using students’ speech to characterize group collaboration quality. In Proceedings of The Computer Supported Collaborative Learning (CSCL) Conference 2015. Gothenburg, Sweden: Computer Support Collaborative Learning.\nKang, S., Norooz, L., Oguamanam, V., Plane, A., Clegg, T. L., & Froehlich, J. E. (2016). SharedPhys: Live Physiological Sensing, Whole-Body Interaction, and Large-Screen Visualizations to Support Shared Inquiry Experiences.\nStahl, G., Mantoan, A., & Weimar, S. (2013). Demo: Collaborative dynamic mathematics in virtual math teams. In Proceedings of the International Conference of Computer-Supported Collaborative Learning, Madison, WI: Computer-Supported Collaborative Learning.\nFlood, V. J., Neff, M. & Abrahamson, D. (2015). Boundary Interactions: Resolving interdisciplinary collaboration challenges using digitized embodied performances. Proceedings of the Computer Supported Collaborative Learning Conference. Gothenberg, Sweden: Computer Supported Collaborative Learning.\nPrimers are developed by small teams of volunteers and licensed under a Creative Commons Attribution 4.0 International License.\nRoschelle, J., Suthers, D., & Grover, S. (2014). CIRCL Primer: Collaborative Learning. In CIRCL Primer Series. Retrieved from http://circlcenter.org/collaborative-learning/\nAfter citing this primer in your text, consider adding: “Used under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).”']"	['<urn:uuid:bc6d874a-c09f-4a3b-90f4-042ebc76e80e>', '<urn:uuid:386301de-9b39-4673-906b-6aa8d9791ddb>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	7	126	3848
22	I'm interested in water activities and heard Hungary has nice spots - can you tell me about the most important water-related places and activities that visitors can enjoy there?	Hungary offers several notable water destinations. Lake Balaton, known as the 'Hungarian Sea', is Central Europe's largest lake stretching over 70 kilometers, offering watersports and parties. The Tisza lake is popular for water-sports and fishing. The country is also famous for its thermal baths and spas, which date back to Roman times. Despite having no coastline, Hungary excels in water activities, with outstanding national water-polo and swimming teams. Budapest features underground thermal spas, while places like the Zsory medicinal baths offer relaxation opportunities.	"[""Hungary is a republic in in the middle of Europe. The Danube flows through the capital, Budapest, which has about 1.8 million inhabitants.The official language is Magyar (Hungarian) with a Latin-based alphabet. It is said that the people are of Finno-Ugric and Turkic descent, mixed with local peoples. Most Hungarians are either Roman Catholic or Protestant. The Hungarian Forint is the unit of currency.\nThe climate is sub-mediterranean in the south and cooler in the north. Seasons are well defined - winters are cold and summers are hot. Most of the country is low-lying and drained by the Danube and the Tisza, its tributary. A limestone ridge, the Bakony Forest, separates the fertile, hilly Little Alföld in the north-west from the Great Alföld, or Hungarian Plain - which covers about half the country. The highest point in the country is Mt Kékes (1015 m). Arable land, orchards and vineyards cover half of the country, and about a third of the land is pasture and forest. Corn and wheat are the main crops.\nKing Stephen founded the state in 1000. Between 1867 and 1918, Hungary and Austria jointly controlled the great Austro-Hungarian Empire. In World War II, it was invaded by Germany, its former ally. Later, Soviet forces occupied Hungary and a communist government was in power until 1989. Hungary joined the European Union in 2004.\nHungarians or 'Magyars', as they call themselves, number around 9.8 million. At over 90,000 km2, the country is slightly larger than its neighbouring Austria; however, in spite of its relatively small size, it has a remarkably illustrious history as well as international fame. Goulash, paprika, Tokaj wine, and Ferenc Puskás - one of the deadliest left-footers in soccer history - come to most people's minds when you mention Hungary.\nNorth-east of Budapest are the hilly regions that produce the country's most famous wines - Tokaj and Bikavér (Bull's Blood). Having said that, you must also try the equally delicious wines from Villány and Badacsony.\nThermal baths and spas are well known in Hungary. The hot springs were well known to the Romans and, later, to the Turks who occupied the country for 150 years.\nWherever you go in Hungary you can be sure to find a thermal bath or a spa in the neighbourhood. Interestingly, despite the lack of a coastline, Hungarians have a tremendous affinity with water: there are many popular lakes - the huge Balaton for one - and fishing-ponds. The national water-polo and swimming teams are second to none.\nCollectors of fine porcelain will have heard of at least three of Hungary's best known producers: Herend, Holloháza, and Zsolnay. Herend, in its day, was a supplier of ceramicware to royalty across Europe. Just stroll around the streets of Pécs to appreciate Zsolnay's metallic-glazed 'Eozin' tiles and decorative fronts on select buildings and palaces.\nHere is a short list of noteworthy/famous Hungarians, and people with Hungarian roots: Drew Barrymore, Bela Bartok, Jozsef Laszlo Biro, Cicciolina, Adrien Brody, Joe Bugner, George Cukor, Tony Curtis, Albrecht Durer, Paul Erdos, Milton Friedman, Uri Geller, Andrew Grove, Mariska Hargitay, Goldie Hawn, Martina Hingis, Harry Houdini, Leslie Howard, Calvin Klein, Mark & David Knopfler, Zoltan Kodaly, Arthur Koestler, Estee Lauder, Franz Liszt, Peter Lorre, Bela Lugosi, Alanis Morrisette, Paul Newman, Joseph Pulitzer, Ferenc Puskas, Tommy Ramone, Erno Rubik, Jerry Seinfeld, Monica Seles, Gene Simmons, Paul Simon, Sir Georg Solti, Leo Szilard, Edward Teller, Viktor Vasarely, Rachel Weisz, Debra Winger...\n- Szolnok county , Tisza lake area and Puszta.\n- Hungary's great, perhaps greatest poet, Sandor Petofi, was bewitched by the landscapes and life of this region. So - like Petofi - be sure to make this a part of your Hungarian experience.\n- Lovers of water-sports and fishing must not miss the Tisza lake - one of the many popular features of central Hungary. For those of us more gastronomically inclined, there is the substantial cuisine of the 'Puszta' to enjoy - gulyasleves being particularly reknown.\n- Here you will see the classic 'tanya' - a relatively isolated and self-sufficient farmstead, where you are free to go about your business far away from the madding crowd. The expression - My home is my castle! - very much applies here.\nNorth East Hungary\n- Borsod, Heves, and Nograd counties.\n- Mountains, forests and streams abound here. Is this the sort of setting that suits you?\n- Go skiing in the Matra range, explore the caves and underground lakes of the Aggtelek hills, or challenge yourself to a 1094 kilometre hike along the National Blue Walk.\n- Perhaps you would rather go wine-tasting in Tokaj, or sit back and relax in the Zsory medicinal baths.\n- The Tisza lake and northern tip of the Great Plain are easily reachable.\nSouth West Hungary\n- Somogy, Baranya, Tolna counties, and Balaton lake area.\n- The south west enjoys an exceptional sub-mediterranean climate. Landscapes can be very varied: there are the flat areas near to the Drava river; some quite steep areas in the Mecsek hills; and the unforgetable Balaton, itself. On the whole, though, the gently rolling hills are most representative of this very attractive part of Hungary.\n- Spend a few days exploring the pleasant historic towns. The lively city of Pecs has Hungary's first ever university dating back to 1367, the Gazi Kaszim Pasa Mosque - a legacy of Hungary's Ottoman period, and the Zsolnay porcelain factory. Visit Kaposvar's nationally famous Csiky Gergely Theatre, and stop by in Szigetvar to admire the fortress - site of a tremendous battle with the Ottoman Turks."", 'Things to do in Hungary\nBudapest is the epicentre of Hungary in terms of size, beauty and action. The twin cities of Buda and Pest are easily explored by foot or public transport. Highlights include lush green parks like the romantic Memento Park, the majestic riverbanks of the Danube, old-school churches and castles like St Stephen’s Basilica and Buda Castle, underground thermal spas and the ruin pubs (romkocsma) that occupy abandoned buildings in the city’s centre. Location and accessibility makes Budapest a handy base for exploring the rest of the country.\nThe second most popular tourist destination in Hungary is Pecs due to the Necropolis of Sopianae, a UNESCO World Heritage Site rich in Turkish architectural monuments and studded with museums. To the southwest, the vast Lake Balaton is a fun party hub. Here, drink the ‘sour water’ in the small resort town of Balatonfured for its alleged healing properties or partake in the myriad watersports on offer.\nWhether you believe in the magic of Hungary’s water or not, it’s still worth enjoying a medical, health or wellness treatment at an affordable price in one the hundreds of spas and thermal baths scattered across the country. A spa treatment can be a rich cultural experience with many places featuring ancient techniques in Roman, Greek, Turkish architectural surrounds.\nIn Budapest, the annual Spring Festival, held each March to April, features more than 200 separate opera, ballet, classical and other musical genre performances throughout the city’s iconic grounds. The Autumn Music Festival in October is also a treat as are the city’s Christmas markets. Lake Balaton hosts some 800 events for the Valley of Magic Festival each June and the south plays host to the bizarre masquerade festival, Busójárás, where the locals re-enact scaring away Turkish armies. For more must-see sights in Hungary, here’s our checklist.\nOne of Europe’s most iconic bridges, Széchenyi Chain Bridge is one of 9 river crossings in Budapest. The stunning suspension bridge offers amazing views of the Pest and Buda sides of the city and really flourishes at night – best seen via a cruise down the Danube River.\nWith an enviable position along the Danube River, the eclectic and imposing Buda Castle is one of Budapest’s most popular attractions. Home to museums, galleries and a library by day, these palatial buildings are even more impressive lit up at night.\nA sobering but nonetheless must-see museum is the former Communist secret police building in Budapest – now known as the House of Terror. The interactive and highly informative museum showcases the life of detainees and Hungarians under the country’s ‘double occupation’.\nParties, watersports and towns rich in culture enliven the banks of the silky yellow waters of Lake Balaton in landlocked Transdanubia. Over 70 kilometres long, the ‘Hungarian Sea’ is Central Europe’s largest lake and one of its most popular tourist destinations.\nHotels and skiing pistes are scattered throughout Hungary’s highest peak Kékes, which translates to ‘bluish’ due to the 1,014 metre-high mountain’s colour. Located in the Mátra mountain range, it’s also a picturesque location for amateur mountaineers, climbers and scenery lovers.']"	['<urn:uuid:8ebdf65f-e061-4d2f-9911-527dd52c54d5>', '<urn:uuid:13815d83-ae50-49da-b627-96153b748487>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T12:39:32.578853	29	83	1438
23	types of policies available individual country implement ghg emissions limits carbon taxes tradable permits	Individual countries can choose from policies including emissions/carbon/energy taxes, tradable permits, subsidies, deposit-refund systems, voluntary agreements, non-tradable permits, technology and performance standards, product bans, and direct government spending including R&D investment.	['The purpose of this section is to examine the major types of policies and measures that can be used to implement options to mitigate net concentrations of GHGs in the atmosphere. In keeping within the defined scope of this Report, policies and measures that can be used to implement or reduce the costs of adaptation to climate change are not examined. Alternative policy instruments are discussed and assessed in terms of specific criteria, all on the basis of the most recent literature. There is naturally some emphasis on the instruments mentioned in the Kyoto Protocol (the Kyoto mechanisms), because they are new and focus on achieving GHG emissions limits, and the extent of their envisaged international application is unprecedented. In addition to economic dimensions, political economy, legal, and institutional elements are discussed insofar as they are relevant to these policies and measures.\nAny individual country can choose from a large set of possible policies, measures, and instruments, including (in arbitrary order): emissions, carbon, or energy taxes, tradable permits, subsidies, deposit-refund systems, voluntary agreements, non-tradable permits, technology and performance standards, product bans, and direct government spending, including R&D investment. Likewise, a group of countries that wants to limit its collective GHG emissions could agree to implement one, or a mix, of the following instruments (in arbitrary order): tradable quotas, joint implementation, clean development mechanism, harmonized emissions or carbon or energy taxes, an international emissions, carbon, or energy tax, non-tradable quotas, international technology and product standards, voluntary agreements, and direct international transfers of financial resources and technology.\nPossible criteria for the assessment of policy instruments include: environmental effectiveness; cost effectiveness; distributional considerations including competitiveness concerns; administrative and political feasibility; government revenues; wider economic effects including implications for international trade rules; wider environmental effects including carbon leakage; and effects on changes in attitudes, awareness, learning, innovation, technical progress, and dissemination of technology. Each government may apply different weights to various criteria when evaluating GHG mitigation policy options depending on national and sector level circumstances. Moreover, a government may apply different sets of weights to the criteria when evaluating national (domestic) versus international policy instruments. Co-ordinated actions could help address competitiveness concerns, potential conflicts with international trade rules, and carbon leakage.\nThe economics literature on the choice of policies adopted has emphasized the importance of interest group pressures, focusing on the demand for regulation. But it has tended to neglect the supply side of the political equation, emphasized in the political science literature: the legislators and government and party officials who design and implement regulatory policy, and who ultimately decide which instruments or mix of instruments will be used. However, the point of compliance of alternative policy instruments, whether they are applied to fossil fuel users or manufacturers, for example, is likely to be politically crucial to the choice of policy instrument. And a key insight is that some forms of regulation actually can benefit the regulated industry, for example, by limiting entry into the industry or imposing higher costs on new entrants. A policy that imposes costs on industry as a whole might still be supported by firms who would fare better than their competitors. Regulated firms, of course, are not the only group with a stake in regulation: opposing interest groups will fight for their own interests.\nOther reports in this collection']	['<urn:uuid:8c9a80a9-7a72-4a2f-8df0-9903ca325bcc>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	14	31	550
24	What was the scale of Yellowstone's last supereruption 631,000 years ago?	The last supereruption at Yellowstone 631,000 years ago was massive, blasting approximately 1000 cubic kilometers of rock, dust, and volcanic ash into the sky. The debris spread across the continental United States in a triangular pattern from the Canadian border down to California and over to Louisiana. In some areas, the ash deposits reached more than a meter in thickness.	['Yellowstone National Park is renowned for more than just its hot springs and Old Faithful. The area is famous in the volcanology community for being the site of three explosive supereruptions, the last of which was 631,000 years ago.\nDuring that eruption, approximately 1000 cubic kilometers of rock, dust, and volcanic ash blasted into the sky. Debris rained across the continental United States, spanning a rough triangle that stretches from today’s Canadian border down to California and over to Louisiana. In places, ash reached more than a meter thick.\n“If something like this happened today, it would be catastrophic,” said Hannah Shamloo, a geologist at Arizona State University’s School of Earth and Space Exploration in Tempe. “We want to understand what triggers these eruptions, so we can set up warning systems. That’s the big-picture goal.”\nNow, Shamloo and her coauthor think they’ve found a clue. By examining trace elements in crystals that they found in the volcanic leftovers of Yellowstone’s last supereruption, they might be able to pinpoint what triggered it.\nJust outside Yellowstone National Park is a thick multicolored, multilayered rock formation called the Lava Creek Tuff. Tuffs are igneous rocks formed by the volcanic debris left behind by an explosive eruption.\nMinerals in these tuffs can tell scientists about conditions inside the volcano before it erupted, and identifying these preeruptive conditions may help inform current hazard assessments.\nShamloo and her Ph.D. adviser at Arizona State University, geologist Christy Till, examined two crystals of feldspar that they found embedded in the tuff. These crystals, called phenocrysts, form as magma cools slowly beneath the volcano.\nThese phenocrysts, measuring between 1 and 2 millimeters in diameter, were too large to have formed when hot material was flung up during the eruption.\nInstead, as Shamloo explained, they grew gradually over time in Yellowstone’s magma chamber, each crystal beginning with a core that slowly and steadily enlarged outward, layer upon layer. As surrounding magma conditions—temperature, pressure, and water content— changed, trace elements surrounding the growing phenocrysts also changed and became incorporated into subsequent layers.\nIn this way, the differences in chemical composition between the phenocryst core and successive layers serve as a map of changing conditions deep within the volcano over time. What’s more, the phenocrysts’ outermost rims represent the magma that surrounded the crystal right before Yellowstone erupted.\nThus, by analyzing the outer rims, Shamloo and Till could gather both temperature and trace element information just prior to the massive explosion.\nBubble, Bubble, Toil and Trouble\nTemperature information locked in a phenocryst’s outer rims can be extracted using a technique called feldspar thermometry. The technique relies on the fact that certain minerals vary their compositions in known ways as temperatures change. Thus, scientists can work backward from the exact compositions of minerals present in these outer rims to estimate the surrounding temperature when the crystal rim formed.\nThe duo found signatures in the rims that point to an increase in temperature and uptick in the element barium in the magma just before the eruption. They presented their research on 13 December at the American Geophysical Union’s Fall Meeting in San Francisco, Calif.\nTo verify their layer by layer analysis of temperature and chemical composition, Shamloo and Till used MELTS, a software program that models how the crystal composition changed as a function of temperature, pressure, and water content in the magma chamber. They assumed that the magma had the same bulk composition as the Lava Creek Tuff. Their results and the model agreed well but pointed to a low water content for the magma chamber involved in the recent supereruption. In contrast, an older eruption from the Long Valley Caldera that produced the Bishop Tuff had 5% water by weight, 5 times more than the one that produced the Lava Creek Tuff.\nThe low water content is surprising, Shamloo explained, because water and steam create pressures that can trigger eruptions. But Shamloo said that the phenocrysts’ story of hotter temperatures and more barium in the magma chamber just prior to the eruption suggests a possible culprit behind the explosion: the mixing of neighboring pockets of semimelted magma, called an injection event. “There are multiple ways to trigger an eruption, but as of now, we’re seeing evidence for a magma injection,” she said.\nMagma, molten or semimolten rock that exists in layers of the Earth’s crust, can also reach the Earth’s surface. Because it is less dense than surrounding rocks, magma can move upward through cracks in the Earth’s crust, but when its motion is stymied, it pools into magma chambers. These chambers expand thanks to magma injections, when hotter material from deeper volcanic reservoirs feeds into shallower ones. This injection of hotter material just before the eruption may explain the temperature increase recorded in the phenocrysts.\nBut the presence of barium in the phenocrysts is a smoking gun, said Shamloo. “Barium doesn’t like to be in the crystal. It likes to hang out in the melt, so this tells us the barium must’ve been introduced from a different source.” The duo thinks this source is a deeper reservoir inside the volcano.\nEric Christiansen, a volcanologist from Brigham Young University in Provo, Utah, who was not involved with the study, was skeptical of Shamloo’s use of the MELTS software and thinks this type of modeling isn’t as reliable as “real experiments with real rocks.” However, he asserted, “her work is sound, and her analysis is solid. She’s got interesting trace element data with the barium, a late addition to the chamber, which suggests it accompanies what triggered the eruption.”\nGeologic Crystal Ball\n“The public is always afraid of the ‘next big one,’” Shamloo said. “And I like to ask, ‘Can we really forecast that?’” Shamloo and Till hope that they can.\nKnowing the eruption trigger is just the first step, according to Shamloo. The next step is understanding what order of time—days, months, even years—these changes can take before an eruption like the one that produced the Lava Creek Tuff.\nSuch information could help Shamloo, Till, and others to correctly read signs of volcanic unrest at Yellowstone and to create a model for predicting future supereruptions.']	['<urn:uuid:bf46d20e-f365-4bd7-bc38-7d94c30274e4>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	11	60	1023
25	How do the concepts of sovereign immunity in the Kasturi Lal case and equality before law in rule of law principles contrast with each other regarding government liability?	The Kasturi Lal case upheld sovereign immunity, absolving the state from liability for sovereign functions like police keeping seized goods, based on the principle 'Rex non-potest peccare' (the king can do no wrong). In contrast, the rule of law principle of equality before law requires equal subjection of all classes, including the government, to ordinary law of the land, regardless of position or rank, opposing arbitrary power and emphasizing that no one is above law.	"[""Is it Equitable To Hide Behind Sovereign Immunity To Escape Liability?\nCase Name: Kasturi Lal Ralia Ram Jain v. State Of Uttar Pradesh 1965 AIR 1039, 1965 SCR\nBench: Honorable Justice P.B. Gajendragadkar, who was the Chief Justice Of India from 1 February 1964 - 15 March 1966. Justice KN Wanchoo, Justice M.Hidayatullah, Justice Raghubar Dayal and Justice J.R. Mudholkar were the other members of the bench.\nBrief Facts: A partner in an Amritsar based firm of jewellers named Ralia Ram was travelling to Meerut. He had in his possession gold weighing 103 Tolas 6 Mashas and 1 Ratti and silver weighing 2 Maunds and 6 1/2 Seers, which he was carrying to sell at Meerut. He was arrested by some police constables under some provisions of the Criminal Procedure Code, 1970 and his belongings were confiscated by the UP police. Later the silver was returned to him and gold was kept in police malkhana, which was misappropriated by a police constable named Mohammad Amir and after misappropriating the gold he fled to Pakistan. So, Ralia Ram brought an action against the State of UP claiming the compensation of rupees 11,000 in lieu of misappropriated gold.\nWere the police officers negligent in dealing with the seized goods?\nAre the respondents liable to pay compensation to the petitioner under the principle of vicarious liability?\nArguments of Petitioner:\nRalia Ram argued that the act done by the constables in arresting him was in excess to the power conferred to them by the state.\nPetitioner argued that if any property valuing more than 100 Rupees is confiscated by police, as per the provisions of the Uttar Pradesh Police Regulation Act, 1860. It is the duty of the malkhana moharrir to take care of confiscated goods. Mohhamad Amir acting in that capacity misappropriated the goods so this act of Amir attracts the liability of state under the principle of vicarious liability.\nArguments of Respondent:\nThe respondent claimed that they have done nothing exceeding their powers as per Section 54(I) (4) and Section 550 of Cr.P.C, 1970 which gives them powers to arrest someone without magistrate’s order, whom they may suspect reasonably of carrying stolen property.\nThe respondent further argued that they took all necessary care of the confiscated goods and it was the malicious act of Amir only because by doing such an act he was not acting in his course of employment and that is why the state should not be held liable for the act of Mohammad Amir.\nSummary of Judgement: [Current status of judgement - Upheld ]\nThis judgement is based on trail and one of the earliest and major judgements referred was Peninsular and Oriental steam navigation company v. The secretary of state for India, 1861. In this case, it was ruled by Justice Peacock, who was the chief justice of the supreme court of Calcutta that the secretary of state will be liable for the wrongful act of its servants and he will enjoy no sovereign immunity. The principle of Sovereign Immunity is derived from the Latin phrase “ Rex non-potest peccare” which means “ the king can do no wrong” but this principle was changed a bit with the Crown proceeding act, 1947, this act created a demarcating line between the sovereign and non-sovereign functions and the state was made vicariously liable for the non-sovereign functions. Keeping the gold and taking care of it comes under the purview of sovereign functions so the state was absolved from the liability.\nOne more case named Uma Prashad v. The Secretary of State (1936) which had almost similar facts with the Kasturi Lal case was also referred while giving this judgment. In the case, the employer was not made liable for the unauthorized act of the employee. In the Kasturi Lal case, Justice Gajendragadkar held that the act done by the police officers in dealing with the seized property was a negligent act but as it was authorized to them by the statute so it was a sovereign function. The court held that the claim of Ralia Ram was not sustainable. The appeal was dismissed by the court and the parties of the case were directed to bear their cost throughout. The Allahabad High court showed its dissenting opinion to the judgement of Kasturi Lal case while giving judgement in case of State of UP v. Hindustan Lever Ltd where it was held that hiding behind statutory authority is no defence and it should be proved that said act was done in the exercise of sovereign power.\nABOUT THE AUTHOR:\nShaurya Shukla is currently a 1st Year student at Chanakya National Law University, Patna. You can contact him at https://www.linkedin.com/in/shaurya-shukla-a57b6418b/\nEditor: Jayant Upadhyay\nDisclaimer by Legal Armor:\nWe at Legal Armor do not endorse the Authors' views and are in no way responsible for the said views. We are just publishing the Write-ups as blogs with just light editing, and are in no way responsible for any legal claims."", ""“The bedrock of our democracy is the rule of law and that means we have to have\nan independent judiciary, judges who can make decisions independent of the\npolitical winds that are blowing.”-Caroline Kennedy\nIn order to understand the concept of rule of law, it is to be comprehended that\nthe state is governed not by the ruler or the nominated representatives of the\npeople but by the law. The term ‘Rule of Law’ is nowhere defined in the Indian\nConstitution but this term is often used by the Indian judiciary in their\njudgments. Rule of law has been declared by the Supreme Court as one of the\nbasic features of the Constitution so it cannot be amended even by the\nconstitutional amendment. Rule of law is seen as an integral part of good\nAs per rule of law, it is required that the people should be governed by the\naccepted rules rather than the decisions that are arbitrarily taken by the\nrulers. For this, it is essential to keep in mind that the rules that are made\nshould be general and abstract, known and certain and it should apply equally to\nall individuals. Legal limitation on government is the essential attribute of\nconstitutionalism. Rulers are not above law under the concept of\nconstitutionalism, government power is divided with laws enacted by one body and\nadministered by another and for that an independent judiciary exists to ensure\nConcept of Rule of Law\nThe originator of the concept of rule of law was Sir Edward Coke\nJustice in James I Reign\nThe concept of rule of law is of old origin. Greek philosophers such as Plato\nand Aristotle discussed the concept of rule of law around 350 BC. Plato wrote\n“Where the law is subject to some other authority and has none of its own, the\ncollapse of the state, in my view, is not far off; but if law is the master of\nthe government and the government is its slave, then the situation is full of\npromise and men enjoy all the blessings that the gods shower on a\nstate”. Aristotle wrote “law should govern and those who are in power should be\nservant of the laws.”\nThe derivation of the phrase ‘ Rule of Law’ is from the French phrase ‘la\nprincipe de legalite’ which implies principle of legality. By this phrase it\nrefers to a government based on principles of law and not of men. One of the\nbasic principles of Constitution is rule of law and this concept is up to\nstandard in both India and America Constitution.\nThe doctrine of rule of law is the entire basis of Administrative law. As\ndiscussed by Aristotle, the concept of rule of law is grounded in the ideas of\njustice, fairness and inclusiveness. Today, an intricate chain of fundamental\nideas is incorporated in rule of law which further encompasses equality before\nlaw, equal treatment before the law for government, independence of judiciary,\nconsistency, transparency and accountability in administrative law.\nMeaning of Rule of Law\nTo simply understand the meaning of rule of law, it means that no man is above\nlaw and also that every person is subject to the jurisdiction of ordinary courts\nof law irrespective of their position and rank.\nThe term ‘rule of law’ is originated from England and India has taken this\nconcept. The concept of rule of law further requires that no person should be\nsubjected to harsh or arbitrary treatment. The word ‘law’ in rule of law means\nthat whether he is a man or a society, he must not be governed by a man or ruler\nbut by law. In other words, as per Article 13 of the Indian Constitution rule of\nlaw means law of land.\nAccording to Black’s Law Dictionary:\n“Rule of Law” means legal principles of day\nto day application, approved by the governing bodies or authorities and\nexpressed in the form of logical proposition.\nAccording to Oxford Advance Learner’s Dictionary:\n“Rule of Law” means the\nsituation in which all the citizens as well as the state are ruled by the law.\nPostulates of Rule of Law\nIn 1885, Professor A.V Diceydeveloped this concept of Coke and propounded three\nprinciples or postulates of the rule of law in his classic book ‘Law and the\nConstitution.’ According to Professor A.V Dicey, for achieving supremacy of law\nthree principles of postulates must be followed which are as follows:\n·Supremacy of law,\n·Equality before law and\n·Predominance of Legal Spirit\n1. Supremacy of law\nAs per the first postulate, rule of law refers to the lacking of arbitrariness\nor wide discretionary power. In order to understand it simply, every man should\nbe governed by law.\nAccording to Dicey, English men were ruled by the law and the law alone and also\nwhere there is room for arbitrarinessand that in a republic no less than under\na monarchy discretionary authority on the part of the Government must mean\ninsecurity for legal freedom on the part of its subjects. There must be absence\nof wide discretionary powers on the rulers so that they cannot make their own\nlaws but must be governed according to the established laws.\n2. Equality before law\nAccording to the second principle of Dicey, equality before law and equal\nsubjection of all classes to the ordinary law of land to be administered by the\nordinary law courts and this principle emphasizes everyone which included\ngovernment as well irrespective of their position or rank. But such element is\ngoing through the phase of criticisms and is misguided. As stated by Dicey,\nthere must be equality before law or equal subjection of all classes to the\nordinary law of land. French legal system of Droit Administrative was also\ncriticized by him as there were separate tribunals for deciding the cases of\nstate officials and citizens separately.\n3. Predominance of Legal Spirit\nAccording to the third principle of Dicey, general principles of the Indian\nConstitution are the result of the decisions of the Indian judiciary which\ndetermine to file rights of private persons in particular cases. According to\nhim, citizens are being guaranteed the certain rights such as right to personal\nliberty and freedom from arrest by many constitutions of the states (countries).\nOnly when such rights are properly enforceable in the courts of law, those\nrights can be made available to the citizens. Rule of law as established by\nDicey requires that every action of the administration must be backed and done\nin accordance with law. In modern age, the concept of rule of law oppose the\npractice of conferring discretionary powers upon the government and also ensures\nthat every man is bound by the ordinary laws of the land as well as signifies no\ndeprivation of his rights and liberties by an administrative action.\nRule of Law Under Indian Constitution\nIn order to develop Indian democracy, rule of law has played a great role. At\nthe time of framing of Constitution, the framers had two options i.e. USA and\nEngland. Some of the provisions were adopted from USA and some of them were\nadopted from England. Rule of law was adopted from England by our constitutional\nfathers and many provisions were incorporated in the Indian Constitution. Indian\nConstitution is considered to be supreme and no one is above Indian\nConstitution. Rule of law is also given impliedly in the preamble and such\nconcept is enshrined in Part III of the Indian Constitution.\nIn case of violation of such rights, one can approach Supreme Court or High\nCourt under Article 32 and 226 of the Indian Constitution. The Constitution of\nIndia is enriched with the principles of law i.e. justice, equality and liberty.\nAny law made by the Central government or State government must be complied in\naccordance with the Constitution of India. If any law made by the legislature\ncontravenes with the provisions of the Constitution then such law will be\nUnder Article 32 of the Indian Constitution, the Supreme Court has the power to\nissue writs in the nature of Habeas Corpus, mandamus, prohibition, quo warranto,\nand certiorari. The power of judicial review is also given to Supreme Court in\norder to prevent any ultra vires law so as to preserve ‘Rule of law’.\nRole of Indian Judiciary\nThere are a plethora of cases where the concept of rule of law was discussed and\ncame into light. Some of the cases are as follows:\nADM Jabalpur v. Shivkant Shukla \nThis case is also known as “Habeas Corpus case”. It is one of the most\nimportantcase when comes to rule of law. The question that was raised before the\nhon’ble court was that whether there was any rule of law in India apart from\nArticle 21 of the Indian Constitution. It was in context relating to the\nproclamation of emergency where the enforcement of Articles 14, 21 and 22 were\nSom Raj v. State of Haryana \nIn this case it was held that the absence of arbitrary power is the postulate of\nrule of law upon which the whole constitutional edifice is dependent.\nUnion of India v. Raghubir Singh \nIn this case it was held by the court that a considerable degree that governs\nthe lives of the people and regulates the State function flows from the decision\nof the superior courts.\nChief Settlement Commissioner, Punjab v. Om Prakash\nIn this case, Supreme Court observed“In our constitutional system, the central\nand most characteristic feature is the concept of rule of law which means, in\nthe present context, the authority of law courts to test all administrative\naction by the standard of legality. The administrative or executive action that\ndoes not meet the standard will be set aside if the aggrieved person brings the\nmatter into notice.”\nKeshvananda Bharti v. State of Kerela \nIn this case, the Supreme Court enunciated the concept of rule of law as one of\nthe most important aspects of doctrine of basic structure.\nManeka Gandhi v. Union of India\nIn this case Supreme Court declared that Article 14 strikes against\nGadakh Yashwantrao Kankarrao v. Balasaheb Vikhe Patil \nIn this case, the ration laid down was “If the rule of law has to be preserved\nas the essence of the democracy of which purity of elections is a necessary\nconcomitant, it is the duty of the courts to appreciate the evidence and\nconstrue the law in a manner which would sub serve this higher purpose and not\neven imperceptibly facilitate acceptance, much less affirmance, of the falling\nelectoral standards. For democracy to survive, rule of law must prevail, and it\nis necessary that the best available men should be chosen as people's\nrepresentatives for proper governance of the country. This can be best achieved\nthrough men of high moral and ethical values who win the elections on a positive\nvote obtained on their own merit and not by the negative vote of process of\nelimination based on comparative demerits of the candidates.”\nSecretary, State of Karnataka and Ors. v. Umadevi \nA Constitution Bench of this Court has laid down the law in the following\nterms:“Thus, it is clear that adherence to the rule of equality in public\nemployment is a basic feature of our Constitution and since the rule of law is\nthe core of our Constitution, a court would certainly be disabled from passing\nan order upholding a violation of Article 14 or in ordering the overlooking of\nthe need to comply with the requirements of Article 14 read with Article 16 of\nFrom the above mentioned discussion, it can be concluded that Supremacy of law\nis the Aim, Rule of Law is the best tool to achieve the Aim. Some of the efforts\nare also taken by the court where the Rule of Law is linked with Human Rights of\nthe people. Strategy is being evolved by the court by which government can be\nforced not only to submit to law but also to create conditions where capacities\ncan be developed by the people so as to enjoy their rights in proper and\nIn the Indian society, the rule of law has not achieved the intended results. A\nfew examples where rule of law was upheld by our judiciary and ensured justice\ncan be clearly seen in the creation of new avenues seeking remedies for human\nrights violations by filing of PIL pleas.\nThe originator of this concept i.e. Sir Edward Coke, the Chief Justice of King\nJames I’s reign maintained that the King should be under God and the law and\nalso he established the supremacy of law against the executive and that there\nis nothing higher than law.\n (1976) 2 SCC 521.\n 1990 AIR 1176.\n Decided on 18.09.2013.\n AIR 1969 SC 33.\n (1973) 4 SCC 225.\n AIR 1978 SC 597.\n AIR 1994 SC 678.\n (1992) 3 SCR 826.\nWritten by: Shubham Mongia""]"	['<urn:uuid:1d0ed471-aa57-44bd-b09f-08dcc6f5b9f6>', '<urn:uuid:07473acc-9784-4ccf-b9fc-34a42a4435ac>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T12:39:32.578853	28	75	2967
26	real world experience vs virtual simulation benefits drawbacks technical vocational education	In technical vocational education, real-world training provides necessary hands-on experience through apprenticeships and supervised work, particularly required for plumbing certification and licensing. However, it involves risks and limited practice opportunities. Virtual simulation offers advantages through safe, repeatable practice environments with detailed 3D imagery and life-like representations, allowing students to learn at their own pace without social pressure. Research by Stanford's Virtual Human Interaction Lab shows virtual reality training achieves better results than traditional video methods for learning practical tasks.	['How to Choose the Right Plumber Training near Bisbee Arizona\nThe initial step to learning to be a plumbing contractor or tradesman is finding a plumbing school near Bisbee AZ. But with so many technical schools to pick from, just how do you undertake making certain that you enroll in the ideal one? Especially because there are a number of variables to examine. For example, many prospective students will begin by searching for schools that are nearby their residence. When they have located several that are within driving range, they will pick the one with the least expensive tuition. Although location and cost are significant, they are not the only qualifications that must be examined. Also important are the reputations of the schools, their accreditation, along with their graduation and job placement rates. These and additional qualifications should help mold your ultimate judgment when choosing a plumbing training school. We will discuss that checklist in more detail later in this post. But to begin with, let’s talk a little bit about becoming a plumber and the training choices that are available.\nBecoming a Plumber\nAlmost everyone at one time or another has needed the services of a professional plumber. It may have been due to a leaky faucet or perhaps a hot water heater needed to be replaced. Plumbers provide a valuable service for the maintenance of both commercial and residential buildings. They are skilled tradesmen who are trained in the installation, repair and replacement of plumbing systems. These systems include pipes, plumbing fixtures, water based heating and cooling, sewage removal and sprinklers. A local Bisbee AZ plumber will typically advance through three phases during their professional career.\n- Apprentice. The majority of Bisbee AZ plumbers begin their careers as an Apprentice. Apprenticeships are often completed in tandem with a formalized plumbing training program. Apprentices are typically paid but unlicensed and work under the guidance of a licensed plumber during working hours and attend plumber training classes at night. The second phase of their career begins once the apprenticeship and the training program have been completed.\n- Journeyman Plumber. Once the apprenticeship has been completed as well as any required training program, application for Journeyman Plumber licensing can be made. Most often licensing is controlled by the state but can be managed more locally at the county or city level. A Journeyman license will only be issued after all requirements have been met, including a passing score on the licensing exam. Even though a Journeyman is more knowledgeable and experienced than an Apprentice, they generally must continue to be supervised, in this case by a Master Plumber.\n- Master Plumber. After attaining the necessary work experience and completing any additional educational requirements, the Journeyman can apply to become a Master Plumber. Once again a passing score must be achieved on the licensing exam before a license will be issued. There are several benefits to advancing to the Master Plumber level, including earnings at the highest pay level, unsupervised working conditions, and the ability to start and own a business. Master Plumbers may also hire and supervise both Journeymen and Apprentices.\nAs a technical field requiring a high level of skill and competence, plumbing can take years of both training and experience to master. As a consequence, the best opportunity for success for a new Apprentice in Bisbee AZ is to enroll in a plumbing technology program that will provide the comprehensive quality training needed to embark on this challenging career.\nPlumbing Certificate and Degree Programs\nThere are several educational options available near Bisbee AZ to receive the necessary training to begin your career as a plumber. As far as the schools offering programs, you can enroll in a trade, technical or vocational school as well as a local community college. The programs offered will vary, but in general the shortest is the certificate program which will focus on the fundamentals and generally take about a year to complete. An Associate Degree will take 2 years to complete and will provide a more comprehensive education. There are some colleges that do offer a Bachelor Degree in plumbing technology, which are 4 year programs and much broader in nature. When choosing a school and program, naturally the completion time and the cost will be important factors. Tuition can vary significantly among the various schools and colleges and for some private schools can be quite expensive. State schools and community colleges typically offer their programs at the lower end of the tuition scale. However, when making your comparisons, keep in mind that many schools offer financial aid and even scholarships to help offset some of the financial burden. So be sure to find out what is available for each school and program before making your final decision.\nWhat to Ask Plumbing Vocational Schools\nNow that you have made a decision to obtain a certificate, diploma or degree, you can start to refine your school options. Considering that there are numerous plumbing trade and vocational schools in the Bisbee AZ region, it’s essential to have a checklist of criteria that each program must meet. The first two that we talked about were location and tuition expense. And while both qualifiers may be crucial when making your decision, there are other variables that need to be considered as well. Following is a checklist of those additional qualifications that you will need to analyze before enrolling in a plumber tech school.\nIs the Plumbing School Accredited? Many plumbing trade schools have acquired either a regional or a national accreditation. They can acquire Institutional Accreditation, which focuses on the school’s programs as a whole, or Programmatic Accreditation, which pertains to a specific program, such as electrical technology. Confirm that the Bisbee AZ school and program are accredited by a U.S. Department of Education acknowledged accrediting agency, such as the Accreditation Board for Engineering and Technology. Along with helping make certain that you obtain an excellent education, it may help in acquiring financial aid or student loans, which are frequently not available for non-accredited programs. Also, many states require that the plumbing training course be accredited for it to qualify for licensing or certification.\nIs the Plumbing School Licensed? In addition to accreditation, another way of determining if a vocational school you’re reiewing is reputable is by checking that it’s properly licensed. Licensing is typically controlled and regulated by state agencies, such as the Arizona Department of Education. If you don’t know, ask the school which state agency is responsible for its licensing and then verify that it’s up to date.\nHow Long has the School been in Business? Another means of determining the quality of a technical school is to find out how long it’s been in business. The longer a school has been in operation, the more likely that its programs are highly rated and regarded. Conversely, schools that are not well regarded or that provide low quality training generally don’t stand the test of time. However, keep in mind that even the best of Bisbee AZ schools had to start from their first day of operation, so only use it as one of several qualifications for each school you are considering.\nWhat are the School’s Completion and Placement Rates? Ask the plumbing training programs you are looking at what their completion rates are. The completion rate is the percentage of students who enroll in and complete the course. A low completion rate may indicate that students were disappointed with the program and dropped out. It might also signify that the teachers were not qualified to instruct the students. It’s also imperative that the schools have higher job placement rates. Older and/or more reputable schools may have a broader list of alumni, which can mean more contacts for the school to employ for their apprenticeship and job placement programs. A high job placement rate will not only validate that the school has an excellent reputation within the trade, but also that it has the network of contacts to help students acquire apprenticeships or jobs in the Bisbee AZ area.\nAre Apprenticeship Programs Sponsored? Numerous plumber technical programs are taught in conjunction with an internship or an apprenticeship program. Those participating trade and technical programs will help place you in an apprenticeship program within their network of plumbing businesses or labor unions. Check if the schools you are reviewing have referring partnerships with local Bisbee AZ plumbers or plumbing companies. An apprenticeship not only provides a valuable experience by providing hands-on training, but it also supplies employment opportunities and helps to build relationships in the area plumbing professional community.\nAre there Modern Facilities? Make sure that the school facilities and the equipment that you will be instructed on are up-to-date and what you will be working with on the job. If you are presently in an internship or an apprenticeship, check with the master plumber you are working under concerning what you should be expecting. Otherwise, ask a local Bisbee AZ plumbing contracting company if they can provide some suggestions.\nWhere is the School Located? Unless you can relocate, the school needs to be within driving distance of your Bisbee AZ home. Remember that if you decide to attend an out-of-state school, besides the added relocation costs there can be increased tuition charges compared to in-state residents.\nAre there Smaller Classes? It’s desirable that you get as much one-on-one training as possible, which can be challenging in bigger classes. Ask if you can monitor some of the classes so that you can see how large they are and experience the interaction between students and instructors. Talk to several of the students and get their comments relating to class sizes and instruction. Last, talk to some of the teachers and learn what their level of experience is in Bisbee AZ and what degrees or certifications they hold.\nIs the Class Schedule Convenient? Confirm that the class schedules for the programs you are evaluating are flexible enough to meet your needs. If you are only able to go to classes in the evening or on weekends near Bisbee AZ, check that the schools you are looking at provide those options. If you can only attend part-time, make certain that the school you select permits part-time enrollment. Also, check out what the policy is to make-up classes should you miss any due to work, sickness or family issues.\nFree Info on Accredited Plumber Classes Near Me Bisbee Arizona\nChoosing the best plumber training program will probably be the most critical decision you will make to begin your new career. You originally came to this website because of your interest in Free Info on Accredited Plumber Classes Near Me and wanting more information on the topic Free Info on Accelerated Plumber Training. But as we have covered in this article, there are a number of factors that you will need to evaluate and compare among the schools you are considering. It’s a necessity that any plumbing tech school that you are examining includes a considerable amount of hands-on training. Classes should be small in size and each student must have their own equipment to train with. Classroom instruction should provide a real-world perspective, and the course of study should be current and in-line with industry standards. Courses vary in length and the kind of credential provided, so you will need to decide what length of program and certificate or degree will best serve your needs. Each training program offers different options for certification as well. Probably The ideal means to research your final list of schools is to visit each campus and talk with the students and faculty. Invest some time to sit in on a few classes. Tour the campus and facilities. Make sure that you are confident that the program you pick is the ideal one for you. With the proper training, hard work and commitment, you can become a professional plumber in Bisbee AZ.\nMore Arizona Plumbing School Locations\nBisbee is a city and the county seat of Cochise County in southeastern Arizona, United States. It is 92 miles (148 km) southeast of Tucson and 11 miles (18 km) north of the Mexican border. According to the 2010 census, the population of the town was 5,575.\nMining in the Mule Mountains proved quite successful: in the early 20th century the population of Bisbee soared. Incorporated in 1902, by 1910 its population had swelled to 9,019, and it sported a constellation of suburbs, including Warren, Lowell, and San Jose, some of which had been founded on their own (ultimately less successful) mines. In 1917, open-pit mining was successfully introduced to meet the copper demand during World War I.\nA high quality turquoise promoted as Bisbee Blue was a by-product of the copper mining. Many high-quality mineral specimens have come from Bisbee area mines and are to be found in museum collections worldwide. Some of these minerals include cuprite, aragonite, wulfenite, malachite, azurite, and galena.\nBusiness Results 1 - 10 of 10', 'TECHNICAL AND PRACTICAL TRAINING FOR EMPLOYEES PRESENTS SEVERAL DIFFICULTIES, FROM THE COSTS INVOLVED IN TRAINING WITH REAL MACHINERY TO THE HEALTH AND SAFETY RISKS THAT SUCH TRAINING CAN POSE.\nImmersive learning technology provides a valuable opportunity to engage in practical training in an easier, safer and more flexible way and, unlike other alternative methods, maintains a high level of interaction with the learning content.\nA research study conducted by the Virtual Human Interaction Lab at Stanford University, “The Effects of Fully Immersive Virtual Reality on the Learning of Physical Tasks”, highlights the positive impacts that virtual reality can have on technical training.(1) Its findings show that virtual reality technology obtained better results than traditional video methods in the learning of practical tasks and participants in the study also reported feeling a higher social presence within the virtual environment.\nResearch has shown that repeating practical tasks reinforces learning. Immersive learning environments allow students to repeat tasks as many times as they like at their own pace and, importantly, away from the social pressures of a real classroom. For training in high-risk situations, virtual reality provides a much safer environment in which to make initial mistakes. Like face-to-face training, virtual training also allows students to interact directly with their trainer and environment. Detailed 3D imagery provides life-like representations of objects and their surroundings, giving students a very ‘real’ training experience.\nTechnical training in an immersive learning environment\n“Learning is the development of experience into experience” (James, 1892).\nThe learning and understanding of practical tasks relies on experience and virtual reality training is highly experiential and immersive. Students can interact with objects and machinery and view them up-close, as well as experiencing how to operate them. Effective training is also reliant on students’ interest and motivation (2); immersive learning technology can create stimulating and engaging learning environments for students, increasing their motivation to learn. Such environments are flexible and programmable, meaning that they can be tailored to meet individual needs.\nAccording to Fabrizia Mantovani (3), students learn more effectively when they engage directly with learning content and build their own understanding of it. Immersion in and interaction with learning content encourages the active engagement and motivation of students.\nExperience, repetition and interaction are important processes in practical training. Virtual reality training provides this in a controlled and programmable environment. The application of virtual reality with a live trainer yields particularly high motivation from students and allows for technical tasks to be carried out under guidance in a safe and engaging environment.\n(1) Patel, K., Bailenson, J.N., Hack-Jung, S., Diankov , R., & Bajcsy , R. (2006). The effects of fully immersive virtual reality on the learning of physical tasks. Proceedings of PRESENCE 2006: The 9th Annual International Workshop on Presence. August 24 – 26, Cleveland, Ohio, USA.\n(2) Bricken, M. (1991). Virtual reality learning environments: potentials and challenges. Human Interface Technology Lab (HITL) Washington Technology Center, University of Washington\n(3) Mantovani, F. Virtual Reality Learning: Potential and Challenges for the Use of 3D Environments in Education and Training in Towards Cyberpsychology: Mind, Cognition and Society in the Internet Age, Riva, G. and Galimberti, C., 2001']	['<urn:uuid:96739ac7-e455-4e8b-b552-2b6f7e8e2e53>', '<urn:uuid:fad1c0af-b656-4505-8236-99aff755e94c>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	11	79	2684
27	when is preservation week	Preservation Week takes place from April 22-28.	['April 22–28 marks the third annual Preservation Week—an event sponsored by ALCTS that’s dedicated to educating the public about caring for personal treasures, and is intended to heighten interest in preservation among library, archive, and museum staff.\nFor libraries and archives, digitizing materials has become a key concern, especially as more and more patrons and users go online to research information. But several factors must be considered:\nLibrarians interested in beginning a digitization project must first consult copyright laws. If the item is in the public domain, copyright is fairly easy. If the item is not in the public domain, determining copyright can be time consuming. A good resource to consult for questions regarding copyright is Copyright Issues Relevant to the Creation of a Digital Archive: A Preliminary Assessment by June M. Besek (Council on Library and Information Resources).\nItems being digitized should also be in good physical condition before digitization. Handling brittle or torn objects could make digitizing more difficult and could cause further damage. If you are unsure, check with a conservator, preservation specialist, or in-house archivist prior to digitization. If you do not have a conservator on staff, the American Institute for Conservation of Historic and Artistic Works (AIC) offers a “Find a Conservator” database that you can use to search for conservators in your area based on the materials—documents, textiles, electronic media, etc.—you would like preserved.\nBefore you begin any digitization project, you’ll need to assess costs for staff, storage, and equipment. Determine if there are dedicated staff members who have or need training. If training is needed, decide whether those individuals will have in-house training or will need to attend a workshop. ALCTS is offering two webinars during Preservation Week: Taking Care: Family Textiles on April 24 and Preserving Your Personal Digital Photographs on April 26. These webinars will be recorded and placed on the ALCTS website.\nEquipment costs can be a strain for libraries on a limited budget. You will need quality scanners to provide the best representation of an item. As for storage, the good news is that such costs have gone down significantly over the past several years. Though not recommended by most people for long-term preservation, a one-terabyte external hard drive can be purchased for less than $100. One terabyte of hard-drive storage for standard servers can cost between $150 and $400. This is important when digitizing—the higher the image resolution and the standards used for scanning, the more storage space will be needed for those files.\nBefore you begin any digitization project, you’ll need to assess costs for staff, storage, and equipment.\nAnother decision you will need to make is what type of file you want to use to save the images. Most libraries that are actively digitizing have developed standards to follow to preserve noncompressed, larger files and to access compressed or lossy (smaller) files.\nAfter digitizing, consider how the images will be accessed and preserved. Depending on their staff and budget, libraries can do a number of things, such as provide an in-house database, purchase digital collection management software, or create cooperative digital preservation networks. Another important piece to make access readier is the level of metadata used to describe the digital item. Most metadata will be based on descriptive, administrative, technical, structural, and preservation information.\nAdditional resources to consult before starting a digitization project include Cornell University Library’s Moving Theory into Practice: Digital Imaging Tutorial and in-person and online training provided by regional library consortiums.\nFor more information about Preservation Week and preserving collections, please visit www.atyourlibrary.org/passiton.']	['<urn:uuid:2b8f1bb7-0cb0-4fdd-8526-a8356091212d>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	4	7	590
28	What makes perennial ryegrass good for lawns, and which endophyte strains suit it best?	Perennial ryegrass excels in lawns due to its quick germination, shiny green color, fine texture, and dense sod formation. It's highly traffic tolerant and shows strong disease and insect resistance, making it ideal for athletic fields and cool-region lawns. Regarding endophyte strains, AR1 contains only peramine which provides good protection against Argentine stem weevil with enhanced animal performance. AR37, while not containing peramine, offers superior protection against a wider range of pests including Argentine stem weevil, black beetle, root aphid, pasture mealy bug and porina, though it can occasionally cause ryegrass staggers in some livestock.	"['Both Perennial and Annual Rye grass can provide a green winter lawn overseeded on warm season grasses that go dormant in the fall/winter. Ryegrass planted\non newly seeded lawns of cool and warm season grasses ryegrass acts as an erosion barrier while the permanent lawn develops. Ryegrass also provides any area with an almost ""instant"" green\ncoverage. This green coverage is the chief purpose for planting annual ryegrass. Ryegrass clippings mown and left on the developing lawn as it grows, providing extra nutrients as the\nBright green, fast growing, and prolific; the ryegrasses do much for lawns by adding contrasting color to early spring flowers, trees and shrubs as well as\nnutrients. Annual Ryegrass Information\nAnnual Rye grass is one of the most popular seeds sold for lawn and pasture purposes and is planted all over the world on it\'s own and in numerous grass seed mixtures. The\nadaptability of this cool season grass to many soils and climates coupled with fast germination and prolific growing rates make annual ryegrass an important grass type in establishing\nthousands of lawns and pastures in all but the hottest of planting zones.\nAnnual ryegrass is exceptionally useful as a fill-in grass when planted with slower growing permanent lawns. Ideally suited for short term seasonal use in\nareas reaching below the transition zone, annual ryegrass is a great fill-in to create green grass on new grass areas. In warmer regions, ryegrass lawns may require watering, fertilization\nand more frequent mowing.\nAnnual Ryegrass is a cool season grass generally used as a temporary planting for overseeding warm or cool season grasses. Its uses are many, including\ntemporary pastures, green manure, temporary coverage for erosion control, short term use on reclamation sites, parks, new lawns, and along highway shoulders. Annual ryegrass\nmay be seeded with other grass mixtures, legumes, clovers and small grains. Annual Ryegrass is fast germinating, quick growing and requires low to medium fertilization. Annual Rye Grass Is Eco-Friendly\nAnnual Ryegrass is a compatible growing medium when planted with any vegetation where needed.\nAnnual ryegrass is very beneficial to areas that are infected with nematodes which are almost impossible to get rid of except by heavy chemical control. A natural and inexpensive\nresolution for this infestation problem, annual ryegrass is ecologically friendly. Yearly planting of this ryegrass seed ahead, behind, or with crops known to be a nematode favorite would\ncertainly be cost effective.\nAnnual ryegrass is easily removed from any area through heavy grazing, mowing or turning under before the seeding stage.\nnon-aggressive, vigorous grower, furnishes nutrients back to the soil, forms an erosion soil basis for stability, highly digestible, beautiful color, dies off when you need it to without\nchemicals - makes annual ryegrass a winner!\nRyegrass is one of the few ""throw and grow"" seeds that can be sown without the hassle of tilling, scarifying, or digging into the soil and destroying any of the\npermanent ground covers already in place. Plus if the weather helps out by raining just before sowing then you practically have it made. Otherwise watering the yard and applying\nfertilizer before or after sowing are the main two steps for quickest establishment. Note that the trick to sowing any seed is to get an even coverage in conjunction to sowing the correct\namount to achieve an even lawn appearance as the grass fills in. And to get the seeds to make ""soil contact"" for germination to occur.\nOne of the best ways to get rid of those winter ""\n"" is a spot of annual ryegrass "" blues green"" and some\nearly spring bulbs. Beautiful mental picture with very little costs! Perennial Rye Grass Information\nPerennial Ryegrass lawns are one of the best lower maintenance lawns of the cool season grasses. This is one of the toughest and most traffic tolerant turf\ncovers that can be grown. Ryegrass is noted for quick germination, shiny green color, fine texture (newer turf type varieties) and dense forming sod. High disease and insect resistance\nhelps to make perennial ryegrass one of the leading choices for lawn and athletic covers in the cooler regions of North America. Perennial ryegrass is planted as permanent turf or lawn\ngrass and to overseed warm season grasses while they are in winter dormancy.\nPerennial ryegrass seed is also mixed with other cool season grasses to provide more density to the dominate grass in the mixture. Perennial ryegrass seed\nis often added to Kentucky bluegrass seed to add strength and give bluegrass time to repair while the perennial ryegrass is in full growth. These two grass species can be mown at the same\nheight and retain a sod density for which both are well known. Although bluegrass may require more fertilizer, the perennial ryegrass will respond to additional amounts without any damage. Turf Quality Perennial Ryegrass Seed Blends\nPerennial Ryegrass seed blends\nsuch as the Champion GQ, Clubhouse GQ or Pennington\'s Perennial Rye Grass Blend\nare widely planted and are excellent choices for lawns in the cooler\nregions of the USA. They are also popular with Turf Agronomists for overseeding of quality Bermuda lawns, golf areas and sports fields in more Southern state locations during the winter\nmonths. Most Golf Course greens today are overseeded with Perennial Ryegrass for primary winter cover. Note: Centipede lawns should NOT be overseeded with Perennial Ryegrass, but\ninstead use Annual Ryegrass. GRASS\nSEED DISCOUNT PRICES!\nFarmers, Ranchers, New home lawn construction? Save\n$$$ by buying in large volume.\nAnnual Ryegrass For Pasture & Farm Use\nAnnual ryegrass certainly must be considered a staple\nin the farming and ranching business. Ryegrass is used in so many useful ways to\nbenefit pastures or farm lands -- providing quick growing short season forage, hay,\ngreen chop, green manures, erosion control, weed control, and so much more.\nPASSEREL PLUS ANNUAL RYEGRASS FOR PASTURE\n- Highly rated Forage Ryegrass! Read about\nPasserel Plus Pasture Ryegrass along with Data\nReports. Passerel Plus annual ryegrass is inexpensive, easily seeded, established and harvested, this grass is a number one seller\nAnother popular annual ryegrass,\nAnnual, is planted as \'row crops\' in agricultural fields from berry fields to corn fields as well as orchards. Annual ryegrass is row cropped\nwith all kinds of crops to keep down weeds and provide early coverage, shade for some\ncrops, green fertilizer when mown and returned to the soil, wildlife crops, and when\nleft to maturity it will partially reseed for the next season -- although not as dense as the first year of planting. Do Not Plant Ryegrasses Developed For Lawn Use In Pastures!\nDue to the potential for Ryegrasses in pastures\nto have the possibility of producing higher than desirable Endophyte production of toxins, you should only plant ryegrasses that have a known history of safe use for forages such as\nPasserel ryegrass seed or Gulf Annual ryegrass seed. Some of the newer\nimproved perennial ryegrass varieties for lawns often contain\nEndophytes and are NOT suitable or recommended for pasture\nuse. Plant only forage type ryegrass varieties for pastures. Perennial Ryegrasses For Pastures\n. A valuable winter and spring grazing crop that can be overseeded\nonto additional forages or used alone lengthening the pasture season. A fast growing\nmulti-use forage crop requiring minimal tillage and competitive enough to make a place for\nitself. A compact growing habit reinforces the ground and gives a perfect erosion base for\npastures with heavy traffic or in the case of heavy rains or snow meltdown. Ryegrass helps\nkeep the livestock out of the mud as much as possible. PERENNIAL RYEGRASS PASTURES are highly digestible and a\nfavorite of all grazing animals\nPerennial ryegrass is fast germinating, grows prolifically, and can be grazed heavily. Many varieties have been developed and are in ""prescription"" mixes or in stand alone crops for complete\npasture and hay production. The ryegrasses are well known for being companion grasses in\nmixes of fescue, clovers, timothy, orchard grass and many other pasture crops. The\nryegrass is usually first to germinate and grow in while the others are developing.\nPerennial rye is used to overseed many warmer season pastures that may go dormant or are\ngrowing slower due to cooler temperatures. This is one of the most diversified grasses\nused in the forage industry for permanent and semi-permanent pastures. How To Overseed With Ryegrass\nOVERSEEDING - Since annual and perennial ryegrasses are\nused in overseeding many areas the best stands achieved are still the ones with a small amount of\ninput. Rake or remove all debris (this helps scratch the soil for better soil contact),\nfertilize as suggested (slow release is actually the best and easiest), water lightly (if possible) before seeding evenly at the heaviest rate recommended, rake again if\npossible and lightly mist to keep seeds in place. Do not use excessive force as this will move seeds\ninto uneven clumps. Watering helps increase germinating process.\nSEED ONLINE FROM SEEDLAND\na beautiful tomorrow!®', 'Harvesting and sowing seeds are activities as old as civilisation. Innovative products and technologies add value to these ancient practices, and bring new opportunities to New Zealand livestock farmers.\nGrasslanz Technology Limited is a wholly owned subsidiary of AgResearch. Grasslanz Technology develops, promotes and licences proprietary forage grasses and legumes, endophytes, and applied biotechnology to seed companies. These products are sold to farmers under licence from Grasslanz Technology by the seed companies.\nEndophyte developments have been a major focus for Grasslanz Technology in recent years. Research undertaken by AgResearch (Grasslanz Technology’s parent company) in the endophyte area is world leading, and the technology has provided millions of dollars of benefit to New Zealand primary industries since the release of the novel endophyte AR1 in 2000. The research for the development of AR1 was a major team effort by mycologists, chemists, agronomists, entomologists and plant breeders. Research into the identification, understanding, testing and commercialisation of an endophyte strain takes the best part of 20 years.\nThe endophyte programme undertaken by Grasslanz Technology and AgResearch goes through a number of key stages. Endophytes have been isolated from grasses collected from around the world. They are identified and classified, and over the years a picture has developed as to how individual endophyte strains are related to one another. Once an endophyte strain is identified, the spectrum of chemical compounds it produces is determined, along with the impact these have on insects. Endophyte strains with a good range of insect tolerance are agronomically evaluated in a number of environments and various management styles. Endophytes that are agronomically as good or superior to standard (common toxic) endophytes are then subjected to intensive animal safety and performance testing. To be commercialised novel endophyte strains need to be superior to standard endophytes in these aspects of testing.\nThe final process is to develop seed production, processing, prolonged storage, and quality assurance systems. This is to ensure that the correct and a viable endophyte can be delivered to the farmer. It is important to remember that endophytes are a perishable product and a lot of care is required at the seed production, processing, and storage phases to ensure farmers actually receive the novel endophytes they purchase in their seed on their farms.\nWhat is an endophyte?\nEndophyte is a naturally occurring fungus whose complete life cycle occurs within grasses such as perennial ryegrass and tall fescue. Most New Zealand ryegrass pastures are infected with endophyte. The endophyte fungus grows between the cells of the host plant, drawing nutrients from it but in return conferring resistance to insect pests, drought tolerance, and protection from overgrazing. Such a mutually beneficial relationship is known as symbiosis. Endophytes produce chemical compounds known as alkaloids, which confer pasture pest resistance, but which can also cause animal health problems. Peramine deters in particular feeding and egg laying of Argentine Stem Weevil, but has no known effects on animal health. LolitremB deter Argentine Stem Weevil larval feeding but cause Ryegrass Staggers. Ergovaline gives resistance to Black Beetle, but causes heat stress, especially in warm humid conditions\nNovel endophytes are selected endophyte strains that have known and understood alkaloid chemical compound profiles. It is known what impact these compounds have on insect pests, the agronomic performance and persistence of the ryegrass and tall fescue plants, and the health and performance of animals grazing plants containing the novel endophyte strain. Novel endophytes can be incorporated into elite cultivar germplasm either by an inoculation process or by breeding and selecting with plants that contain the novel endophyte strain.\nEndophyte strains commonly present in New Zealand ryegrass pastures contain the compounds lolitrems, peramines and ergovalines. The selected strain, ‘AR1’, contains peramine which gives tolerance to Argentine stem weevil and some tolerance to black beetle when plants are well established. AR1 does not contain lolitrem or ergovaline so that there is enhanced animal performance. Endo 5 contains peramine but no lolitrems. They do contain some level of ergovaline which allows for agronomic robustness, especially in areas where black beetle is an issue.\nThe most recent novel endophyte, ‘AR37’, does not contain lolitrems, peramine, or ergovaline, but does have a unique group of compounds called epoxy-janthitrems. Epoxy-janthitrems confer protection against a wide range of insect pests (Argentine stem weevil, black beetle, root aphid, pasture mealy bug and porina). A point of note is that AR37 controls the Black Beetle lifecycle at the adult stage rather than at the larval stage. Testing to date has shown ryegrass infected with AR37 to be more persistent and productive than pastures infected with standard endophyte, with livestock performance levels similar to AR1 and nil endophyte. While AR37 does not produce the alkaloid LolitremB it can cause ryegrass staggers in grazing livestock. In grazing trials at Lincoln the incidence of ryegrass staggers on pure ryegrass with AR37 has been less frequent and usually only half as severe as on ryegrass with standard endophyte. However, these trials have shown than spasmodically AR37 can cause serious staggers (Fletcher & Sutherland 2009: Proc NZGrassland Assn, 71: 127-33. However, to date no ryegrass staggers have been reported on any class of cattle on AR37 in trials or on commercial farms. Due to the fact that no trial work has been undertaken on deer and horses, pastures with AR37 are currently not recommended for those livestock classes or other untested species.\nNovel endophytes are protected by patents\nThe development of novel endophytes is an expensive process over a long period of time, and they do offer farmers significant agronomic advantages and subsequent economic benefits. Due to these factors Grasslanz Technology and AgResearch have obtained Intellectual property protection in the form of patents.\nWhat many farmers and seed growers may not be aware of, is that only companies who have a licence can produce or sell the cultivars containing novel endophytes. At the individual farmer level the patent means that seed containing novel endophytes like AR1 is protected by law. Farmers who harvest and re-sow or on-sell novel endophytes such as AR1 and AR37 to companies other than licence holders are very likely to be in breach of the patent and this may lead to prosecution under NZ law.\nChoosing a Ryegrass Endophyte\nDeveloped by Industry Endophyte Technical Committee and approved for use by NZPBRA Executive (24th July 2019)\nSummary – Endophyte insect control\nThese ratings are indicative and may vary slightly between cultivars. If Argentine stem weevil or Black beetle are present at sowing, an appropriate seed treatment is recommended, to improve insect resistance during establishment.\nPerennial Ryegrass, Festulolium, Italian and Short-Term (Hybrid) Ryegrass and Continental Tall Fescue\n|Key to Tables|\n|+||Low level control: Endophyte may provide a measureable effect, but is unlikely to give any practical control.|\n|++||Moderate control: Endophyte may provide some practical protection, with a low to moderate reduction in insect population.|\n|+++||Good control: Endophyte markedly reduces insect damage under low to moderate insect pressures. Damage may still occur when insect pressure is high.|\n|++++||Very good control: Endophyte consistently reduces insect populations and keeps pasture damage to low levels, even under high insect pressure.|\n|()||Provisional result: Further results needed to support the rating. Testing is ongoing.|\n|Argentine stem weevil||Pasture mealy bug||Black beetle adult||Root aphid||Porina||Grass grub||Field cricket|\n|Diploid perennial ryegrass|\n|NEA2||+++||(++++)||+++||++||Not tested||-||Not tested|\n|Tetraploid perennial ryegrass|\n|Italian and short term (hybrid) ryegrass|\n|AR1||++||(++++)||+||-2||Not tested||-||Not tested|\n|NEA||Not tested||(++++)||+++||Not tested||Not tested||-||Not tested|\n|AR37||+++1||(++++)||+++||Not tested||Not tested||-||Not tested|\n|Continental tall fescue|\n|AR584 (MaxP)||Not tested||Not tested||+++||(++++)||Not tested||(++)||+++|\n|AR542 (MaxP)||Not tested||Not tested||+++||++++||Not tested||(+)||++|\nNotes on tables\n¹ AR37 endophyte controls Argentine stem weevil larvae, but not adults. While larvae cause most damage to pastures, adults can damage emerging grass seedlings. In Argentine stem weevil prone areas it is recommended to use treated seed for all cultivars with novel endophyte.\n² AR1 plants are more susceptible to root aphid than plants without endophyte.\n³ Also active against black beetle larvae.\nSummary – Endophyte animal safety\nThese ratings are indicative. Animal performance and health can vary under different management systems between seasons.\nPerennial Ryegrass and Festulolium\n|Key to Tables|\n|+||Poor animal production and/or health: This endophyte is known to regularly cause significant problems.|\n|++||Moderate animal production and/or health: This endophyte is known to regularly cause significant problems.|\n|+++||Good animal production and/or health: This endophyte can cause problems from time to time.|\n|++++||Very good animal production and/or health.|\n|NB||All trialling for ryegrass staggers occurs under simulated worst case scenario management, and does not represent normal farm practice.|\n|()||Provisional result: Unlikely to be tested on, or negatively affect cattle production.|\nTable 1 Livestock performance – sheep & lambs\n|AR1||NEA||NEA2||AR37||U2||AR584 (MaxP)||AR542 (MaxP)||Standard Endophyte||Without Endophyte|\n|Freedom from ryegrass staggers||++++||++++||++++||+++2||++++||++++||++++||+1||++++|\nNotes on Table 1\n¹ Standard endophyte can cause severe ryegrass staggers, can significantly decrease lamb growth rates in summer and autumn, and significantly increase dags.\n² Ryegrass containing AR37 endophyte can cause severe ryegrass staggers, but the frequency of ryegrass staggers is much lower than for ryegrass with Standard endophyte. One50 AR37, Asset AR37, and Ohau AR37 may give rise to higher instances of ryegrass staggers than other AR37 cultivars in some situations.\n³ Lambs grazing ryegrass containing AR37 endophyte can have reduced LWG during periods of severe staggers.\nTable 2 Livestock performance – dairy cows and beef cattle\n|AR1||NEA||NEA2||AR37||U2||Standard Endophyte||Without Endophyte|\n|Freedom from ryegrass staggers||++++||++++||++++||++++5||++++||++4||++++|\n|Animal production||++++||Not tested||Not tested||++++6||++++||+++4||++++|\nNotes on Table 2\n4 Standard endophyte can cause ryegrass staggers, and has been shown to depress milksolids (MS) production through summer and autumn.\n5 While ryegrass staggers has not been observed on cattle and dairy cows, it could occur on rare occasions.\n6 In dairy trials overall MS production from ryegrass containing AR37 endophyte is not significantly different from that with AR1. A small reduction in MS was observed over summer on ryegrass containing AR37. A contributing factor to this was the lower clover content in AR37 pastures.']"	['<urn:uuid:04b34d3d-6ca3-4112-aec1-3ee136a32aa2>', '<urn:uuid:6ec73820-8d55-4085-be25-23b0bf9083e6>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	14	95	3111
29	What causes most cancers and how are they found early?	Most non-hereditary cancers (90-95% of cases) develop through fusion genes when chromosomes break and recombine incorrectly. For early detection of hereditary breast cancer specifically, annual MRI starting at age 25, combined with annual mammography starting at age 30 (alternating with MRI every 6 months) is most effective, with MRI showing 77-94% detection rates compared to 33-59% for mammography alone.	['It has long been the accepted view of cancer researchers that there is a difference between the mechanism behind the development of leukemias, on the one hand, and solid tumors like breast cancer, prostate cancer, gastrointestinal cancer, etc, on the other. A research team at the Section for Clinical Genetics at Lund University in Sweden is now claiming just the opposite: the same mechanism gives rise to all non-hereditary forms of cancer. These findings are being published in Nature Genetics.\nA well-known mechanism for the development of cancer is that the chromosomes in a cell break apart and then recombine in an incorrect way. At the points of fissure, gene fragments are exposed that can recombine with so-called fusion genes, yielding fusion proteins. Leukemias--blood cancer--normally develop from cells that contain such fusion proteins. It is not known how this occurs in detail, but in some way the fusion proteins prompt formerly normal cells to transform into cancer cells. On the other hand, solid tumors, which make up the majority of all cancer cases, have been seen as developing as a result of certain cells losing the inhibiting mechanism in the form of so-called tumor suppressor genes that keep tumors from arising.\n“This is no doubt correct in regard to hereditary cancer. But hereditary cancer accounts for only 5-10 percent of all cancer cases. We now maintain that all of the others have the same developmental mechanisms. In non-hereditary cancer forms it is the occurrence of fusion genes and not the lack of tumor suppressor genes that is essential,” says Professor Felix Mitelman.\nIngela Björck | alfa\nLaser activated gold pyramids could deliver drugs, DNA into cells without harm\n24.03.2017 | Harvard John A. Paulson School of Engineering and Applied Sciences\nWhat does congenital Zika syndrome look like?\n24.03.2017 | University of California - San Diego\nAstronomers from Bonn and Tautenburg in Thuringia (Germany) used the 100-m radio telescope at Effelsberg to observe several galaxy clusters. At the edges of these large accumulations of dark matter, stellar systems (galaxies), hot gas, and charged particles, they found magnetic fields that are exceptionally ordered over distances of many million light years. This makes them the most extended magnetic fields in the universe known so far.\nThe results will be published on March 22 in the journal „Astronomy & Astrophysics“.\nGalaxy clusters are the largest gravitationally bound structures in the universe. With a typical extent of about 10 million light years, i.e. 100 times the...\nResearchers at the Goethe University Frankfurt, together with partners from the University of Tübingen in Germany and Queen Mary University as well as Francis Crick Institute from London (UK) have developed a novel technology to decipher the secret ubiquitin code.\nUbiquitin is a small protein that can be linked to other cellular proteins, thereby controlling and modulating their functions. The attachment occurs in many...\nIn the eternal search for next generation high-efficiency solar cells and LEDs, scientists at Los Alamos National Laboratory and their partners are creating...\nSilicon nanosheets are thin, two-dimensional layers with exceptional optoelectronic properties very similar to those of graphene. Albeit, the nanosheets are less stable. Now researchers at the Technical University of Munich (TUM) have, for the first time ever, produced a composite material combining silicon nanosheets and a polymer that is both UV-resistant and easy to process. This brings the scientists a significant step closer to industrial applications like flexible displays and photosensors.\nSilicon nanosheets are thin, two-dimensional layers with exceptional optoelectronic properties very similar to those of graphene. Albeit, the nanosheets are...\nEnzymes behave differently in a test tube compared with the molecular scrum of a living cell. Chemists from the University of Basel have now been able to simulate these confined natural conditions in artificial vesicles for the first time. As reported in the academic journal Small, the results are offering better insight into the development of nanoreactors and artificial organelles.\nEnzymes behave differently in a test tube compared with the molecular scrum of a living cell. Chemists from the University of Basel have now been able to...\n20.03.2017 | Event News\n14.03.2017 | Event News\n07.03.2017 | Event News\n24.03.2017 | Materials Sciences\n24.03.2017 | Physics and Astronomy\n24.03.2017 | Physics and Astronomy', 'A Practical Guide to Hereditary Cancer\nUpdates on Hereditary Breast Cancer Testing and Management\nAdvances in genetics provide opportunities to identify individuals who are at increased risk of cancer.\nHereditary cancer syndromes:\n- Present at an earlier age\n- Cause multiple cancers\n- Exhibit rare cancers (e.g., ovarian, pancreatic, metastatic prostate)\n- Demonstrate specific types of tumor pathology (triple-negative breast cancer, often seen in BRCA1)\n- Tend to affect individuals with particular ethnic backgrounds (e.g., Ashkenazi Jewish ancestry), where BRCA mutations are 10 times more common (1/40)\nApproximately, 5-10% of breast cancers are hereditary. Genes conferring high risk include BRCA1/2, CDH1, PALB2, PTEN, and TP53. The genes ATM and CHEK2 are moderate risk. Breast cancer mutations are inherited from either parent in an autosomal dominant way, meaning that parents, siblings, and children of affected individuals have a 50% chance of carrying the mutation. Changes in DNA are called pathogenic variants or mutations.\nThe best family member to test is the one diagnosed with cancer, especially when found at a young age or because it was rare, like ovarian or pancreatic cancer. Sometimes, when this person is unwilling or unable, first- and second-degree relatives can be tested instead. The importance of patient education and informed consent cannot be overstated. Identifying at-risk family members is essential.\nGuidelines for Managing Breast Cancer Risk\nThe guidelines for managing breast cancer risk fall into three categories:\n- Preventive Medication\n- Risk-reducing Surgery\nStudies of BRCA1/2 patients reported improved cancer detection with an MRI (77-94%) compared with mammography alone (33-59%). Annual MRI starting at age 25, with annual mammography (alternating with MRI every 6 months) starting at age 30, is the best way to detect hereditary breast cancer.\n2. Preventive Medication\nOral contraceptives reduce ovarian cancer by 50% with little, if any, associated breast cancer risk. Selective estrogen receptor modulators (tamoxifen and raloxifene) and aromatase inhibitors (anastrazole and exemestane) reduce invasive breast cancer rates by 38-65%.\n3. Risk-reducing Surgery\nPatients at exceedingly high risk of breast or ovarian cancer may consider risk-reducing mastectomy (RRM) and/or risk-reducing salpingo-oophorectomy (RRSO). When ovaries and fallopian tubes are being removed, consideration should be given to hysterectomy as well to allow for treatment of menopause with estrogen alone, and not combination estrogen-progesterone. Additionally, both BRCA1 and BRCA2 variants have recently been shown to increase the risk of uterine cancer.\nRisk-reducing mastectomy (RRM)\n- RRM decreases breast cancer risk by at least 90%.\n- A compelling family history may also warrant consideration for this surgery.\n- In BRCA carriers, RRSO is associated not only with a 77% reduction in all-cause mortality, but with a 62% reduction in breast-cancer-related mortality.\n- RRSO reduces not only ovarian cancer in BRCA carriers by up to 96%, but also breast cancer risk by 50-55% in premenopausal BRCA1 and BRCA2 positive patients.\nGenetic testing may affect surgery choices for newly diagnosed cancer patients. BRCA mutations increase the risk of contralateral (opposite) breast cancer; those who choose bilateral over unilateral mastectomy are less likely to die from breast cancer.\nRecommendations for Cancer Screening and Management\nThe table below matches gene variants with cancer diagnoses and screening approaches:\n- Genes for which consideration of MRI screening is recommended\n- Genes for which discussions about RRM and/or RRSO are recommended\n- Those in whom pancreatic cancer screening can be considered\nNational Comprehensive Cancer Network (NCCN) Recommendations for Breast and Ovarian Screening & Management as a Function of Genetic Variant*\n|Breast MRI with contrast*||BRCA1, BRCA2, PTEN, TP53, PALB2, CDH1, STK11, ATM, CHEK2, NF1|\n|Consider risk-reducing mastectomy (RRM)||BRCA1, BRCA2, TP53, PTEN, PALB2|\n|Consider risk-reducing salpingo- oophorectomy (RRSO)^||BRCA1, BRCA2, BRIP1, RAD51C, RAD51D,MLH1, MSH2, MSH6, PMS2, EPCAM|\n|Consider pancreatic cancer screening||.STK11, P16 (independent of FH)|\n.BRCA2 (one affected FDR or two family members of any degree)\n.PALB2, ATM, BRCA1, MLH1, MSH2, MSH6 (with any affected FDR)\n- * = Schedule MRI on days 7-15 of menstrual cycle in premenopausal patients, with age of first MRI dependent on gene variant and FH. Consider tomosynthesis with annual mammogram.\n- ^ = Salpingo-oophorectomy is removal of ovaries and fallopian tubes (bilateral)\n- FH = family history\n- FDR = first-degree relative (mother, sister, daughter)\nWhereas certain gene mutations increase the risk of cancer, identification of affected families is improving. Most breast cancer is unrelated to genes, and all patients with or at risk of cancer diagnosis need early consultations with specialists in breast medicine, cancer genetics and, often, psychological support.\nBe Strong, Be Healthy, Be in Charge!\nRoxanne B. Sukol, MD MS and Holly J. Pederson, MD\nRoxanne B. Sukol, MD MS is a staff member of Medical Breast Services at Cleveland Clinic and an Assistant Professor of Medicine at the Cleveland Clinic Lerner College of Medicine of CWRU. Her practice focuses on breast cancer risk assessment and management of patients at high risk, with special interest in prevention of chronic disease.\nHolly J. Pederson, MD is the Director of Medical Breast Services at Cleveland Clinic and an Associate Professor of Medicine at the Cleveland Clinic Lerner College of Medicine of CWRU. Her practice focuses on breast diagnostics, breast cancer risk assessment and management of the high risk patient.\n- What You Should Know About Metastatic Breast Cancer (MBC)\n- What to Know About Home Genetic Tests for Breast Cancer Risk\n- What is new in Breast Cancer Genetics and Other Cancer Genetics for women?\n- Genitourinary Syndrome of Menopause and Breast Cancer Survivors\n- What Your Family Health History Can Teach You']	['<urn:uuid:edf9e0e3-17ac-4606-97cf-01c348f8d35e>', '<urn:uuid:5744a1d0-9718-4ec6-bced-99df6c8b6306>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	10	59	1608
30	rumi abraham maimonides same time period compare	Rumi and Abraham Maimonides were contemporaries who lived in the 13th century. Rumi was born in 1207 in what is now Afghanistan, while Abraham Maimonides was active during the same period in Egypt. Both were influential religious leaders who integrated Sufi thought into their respective traditions - Rumi as a prominent Sufi poet and teacher, and Abraham Maimonides as the head of Egyptian Jews who incorporated Sufi practices into Jewish worship.	"['Abraham Maimonides: A Jewish Sufi\nThe open-minded practice of Sufism is known as a mystical system that can easily attract believers from other religions. Today, as in the past, Jews, Christians and followers of other religions have flocked to this practice, studying under Sufi masters and learning the Sufi Way. The Sufi chronicler Idries Shah has outlined past Sufi influence on St. Francis of Assisi, the Troubadours, St. Augustine, the Rosicrucians, Maimonides, the Jewish Kabbalah and a host of other medieval and modern religious movements.\nWhile much of this contact is incidental or indirect - Sufism reached St. Francis, for instance, through the writings of a Jewish intermediary translated into Latin - there was a time of profound Sufi influence on the direction and thought of the mystics of the Jewish religion. Though it is buried beneath centuries of historical disregard and even outright denial , the fact remains that more than 700 years ago, Jewish leaders not only had a wide-ranging dialogue with Islamic mystics, but also borrowed liberally from them to bring an Islamic brand of piousness into medieval synagogue rites. In the 13th century, Abraham Maimonides, son of the great Jewish philosopher Moses Maimonides, not only wrapped Islamic mystical practice into his view of Judaism, but also considered himself a Jewish Sufi, a practitioner of both Jewish and Islamic mysticism!\nStarting with Abraham Maimonides, Sufism played a seminal role in the development of Jewish spirituality, strongly influencing the direction of the Kabbalah and, later on, the growth of Hasidism. As improbable as it sounds, the Sufi innovations in the Jewish religion begun by Abraham Maimonides were almost assuredly the single most important thing to happen to Jewish spirituality since the destruction of the Second Temple in 70 C.E.\nAbraham grew up in a truly multi-cultural world, where Moslems, Jews and even Christians interacted in one of the most accepting societies in the history of man. Unlike our current epoch, when the voices of hatred speak far louder than those of friendship, medieval Egypt was a place of mutual respect, protective laws and surprisingly strong and positive relations between the religions. It was also a time and place rife with Sufis and Sufi thought - and Jewish libraries often contained books by such masters as al-Ghazali, as-Suhrawardi and al-Hallaj, all dutifully transcribed into the blocky Hebrew script of the local Jewish population. Sufis and Jews knew each other, read each other\'s books and even compared notes on spirituality and the quest for divine union with God.\nAbraham was smitten with the Sufi way. However, if he were just another Jewish heretic, his Sufi beliefs would have been relegated to the footnotes of Jewish history - and Jewish mysticism would have remained just that: A purely Jewish response to this sometimes-inexplicable world. As it is, however, Abraham Maimonides was one of the most respected Jewish authorities of his epoch. As Rayyis al-Yahud -- the head of the Egyptian Jews, a position that he took over from his father -- he wielded power both locally and abroad.\nAbraham, who rose to his position at the tender age of 19, proved to be an able social administrator as well as a creative religious theoretician. Like his father, he was approached from near and far to handle religious disputes, questions of social and halakhic law and even marital questions. Unlike his father, however, he was hardly indecisive about his own feelings toward the Sufis, being convinced that the Sufi discipline had been inherited by the Islamic mystics from the ancient prophets of Israel and that they (the Jewish Sufis) were reclaiming an authentically Jewish doctrine from Islam. As Abraham himself said, ""Thou art aware of the ways of the ancient saints of Israel, which are not or but little practiced among our contemporaries, that have now become the practice of the Sufis of Islam, on account of the iniquities of Israel."" And not only was Abraham certain that the Sufi way defined, in fact, a lost Jewish mysticism, but also that the Jewish/Sufi practice was a necessary precursor to a messianic epoch, at the threshold of which he stood. Abraham saw in Sufism a mystical discipline that would prepare one for achieving the sublime state of prophecy!\nWhat made Abraham such an important figure in the dissemination of Sufi thought throughout the Jewish world was that he did not break with traditional Judaism in order to practice a Jewish brand of Sufism. In fact, one cannot imagine a spiritual heir more dedicated to his predecessor than was Abraham. Moses Maimonides\' groundbreaking work in Jewish thought and philosophy had met with substantial resistance -- especially in the rabbinical academies of southern France. As such, Abraham spilled much ink in defending his father\'s positions. In all of his extant writings, he quoted liberally from his father whenever the opportunity presented itself.\nAt the same time, however, Abraham Maimonides, wrapped the beliefs and the actual practices of the Sufis into his Jewish worship. He justified this paradoxical situation -- believing that Jewish law must be strictly adhered to, while advocating Sufi-like reforms -- by reading Sufi attitudes and beliefs into the distinguished history of Rabbinical thought. He sprinkled not only his commentary with Sufi terminology and beliefs, but his correspondence, as well. And it is important to remember just how far-reaching was his correspondence. Regarded as one of the major rabbinical authorities of his time, Abraham was consulted on legal matters by correspondents as far away as Syria, Yemen and the Holy Land. It is almost certain that he used these contacts to further spread his Sufi beliefs.\nIn his own synagogue, where he was the head Rabbi, he introduced controversial new practices, including frequent prostrations and ablutions; the raising of the hands in supplication; praying while standing in rows and other specifically Sufi-inspired ritual. While many of the more traditional Jews in Egypt had significant troubles with Abraham\'s open Sufi practice, his civic position as head of the Jews allowed him to bring these Islamic practices into the very heart of Egyptian Jewish rites.\nHis own mystical masterpiece, the 2500-page Kifaya, spent the first three chapters re-hashing his father\'s thought and laws - and then a fourth section spelled out in minute detail the tariqa, or Sufi mystical path of enlightenment. In this fourth portion, Abraham enumerated the specifics of the Sufi Path, including sincerity, mercy, generosity, gentleness, humility, faith, contentedness, abstinence, mortification and solitude. He also mentions that upon successful completion of the ""path"" and the achievement of divine union with God, the searcher is to wear special clothes, which were, coincidentally enough, the Sufi garb. Abraham Maimonides took some pride in mentioning that he, himself, wore the> implying that he had not only followed the Sufi Way, but had completed it! This same book, the Kifaya, had already spread to distant lands in his own lifetime, taking with it his ideas on Sufism.\nAbraham used the power of his office to advance the Sufi cause, placing Jewish/Sufis in important positions throughout the Egyptian Jewish community. Ultimately, Abraham\'s position as head of the Jewish community and a pre-eminent Rabbinical scholar allowed his Sufi leanings to influence not just his local and temporal surroundings, but the direction of Jewish mystical thought at a seminal point in the development of Jewish spirituality. The 13th century, during which Abraham was active, was a time of tremendous fertility in Jewish thought and religion - some would argue that it was the most productive and creative epoch in the entire history of Jewish mysticism. It was into this lively loam that Abraham injected the seeds of Sufi thought. It would not be too far of a stretch to say that the Sufi leanings of Abraham influenced virtually all mystical writings in Judeo-Arabic over the next two hundred years, the formative years of the Kabbalistic system! In fact, his works were still being studied by Kabbalists in 16th-century Safed, where the Lurianic Kabbalah was setting the scene for the entrance of Hasidism onto the Jewish mystical stage.\nVirtually every Jew - and many non-Jews - have heard of Moses Maimonides, thought of as the quintessential rationalistic Jewish theologian. But almost nobody is aware of that his son, Abraham, who operated, as well, as head of the Egyptian Jews, turned forever the course of Jewish mysticism in the direction of their cousins, the Sufis of Islam. Jewish practitioners today of the Kabbalistic sciences and Hasidism certainly have no idea just how much of the Sufi Way is wrapped into their daily devotions.\n. The Sufis, Idries Shah\n. To appreciate this tight-lipped response to the facts at hand, we need look no further than three of the greatest historians of Jewish mysticism and philosophy from the last century. For instance, Gershom Scholem, the pre-eminent Kabbalistic scholar of the 20th-century, went so far as to specifically state that Sufism had no discernible effect on the development of the Kabbalah. And A.S. Halkin, writing about the same time as Scholem (the middle of the last century) states that: ""In all the vast literature of the Kabbalah, there is no trace of a non-Jewish source or influence."" Lastly, Martin Buber, the acknowledged master of scholarship on the Hasidic mysticism, points out similarities between two specific teaching tales (one Sufi and one Hasidic) and then immediately states that this in no way proves ""any inner connection between Sufism and Hasidism alone."" I can think of few of areas of scholarship that have been so successfully quelled due to political and cultural taboos.\n. Quoted in Treatise of the Pool (intro), Fenton, pg. 8\n. Jewish Mystical Leaders of the 13th Century, Fenton, pg. 150-151\n. Jewish Medieval and Renaissance Studies, Goitein, pg. 145\n. Jewish Mystical Leaders of the 13th Century, Fenton, pg. 136\n. ibid, pg. 141-142\n. ibid, pg. 139\n. ibid, pg. 144\n. The Books of Contemplation, Verman, pg. 8\n. Jewish Mystical Leaders of the 13th Century, Fenton, pg. 149, 151\nAltmann, Alexander (ed.), 1967. Jewish Medieval and Renaissance Studies, Harvard University Press, Cambridge, MA.\nBuber, Martin, 1948. Hasidism, Philosophical Library, New York,\n1995. The Legend of the Baal Shem Tov, Princeton University Press, NJ\n1991. Tales of the Hasidim, Schocken Books, New York,\nIdel, Moshe, 1995. Hasidism: Between Ecstasy and Magic, SUNY New York Press, Albany, NY.\n1998. Jewish Mystical Leaders and Leadership in the 13th Century, (ed.) Jason Aronson Publishers, Northvale, NJ.\n1988. Kabbalah: New Perspectives, Yale University Press, New Haven, CT.\n1988. The Mystical Experience in Abraham Abulafia, SUNY New York Press, Albany, NY.\n1988. Studies in Ecstatic Kabbalah, SUNY New York Press, Albany, NY.\nMaimonides, Obadyah (with translation and introduction by Paul Fenton), 1981. Treatise of the Pool, Octagon Press, London.\nScholem, Gershom, 1996. On the Kabbalah and its Symbolism, Schocken Books, New York.\n1987. Origins of the Kabbalah, The Jewish Publication Society, NJ.\nSchwarz (ed.), 1956. Great Ages and Ideas of the Jewish Peoples, Random House, New York.\nVerman, Mark, The Books of Contemplation, 1992. SUNY New York Press, Albany, NY.', 'Rumi was born in 1207, in what is now know as Afghanistan. It was a period of remarkable social and political turbulence. The 13th Century was the era of the crusades; also the area where Rumi lived was under constant threat of Mongol invasion. The great upheavals Rumi faced during his life is said to have influenced much of his poetry.\nRumi met many of the great Sufi poets. For example, as a young boy he met the Sufi Master, Attar. Attar is said to have commented about Rumi…\n“There goes a river dragging an ocean behind it.”\n“However the most important turning point in Rumi’s life was when he met the wandering dervish Sham al- Din. Sham was eccentric and unorthodox, but was filled with heart - felt devotion, that sometimes he couldn’t contain. Sham appeared to be quite different to the respectable and prestigious scholar, (as Rumi was at that point.) However Rumi saw in Sham a divine presence. This meeting and their close mystical relationship was instrumental in awakening Rumi’s latent spirituality and intense devotion. It was at this point Rumi abandoned his academic career and began to write his mystical poetry.”\nRumi was born to native Persian-speaking parents,originally from Balkh.....in present-day Afghanistan. He was born either in Wakhsh, a village located on the Vakhsh River in the greater Balkh region in present-day Tajikistan, or in the city of Balkh,.......Balkh was at that time a major centre of Persian culture and Sufism had developed there for several centuries. The most important influences upon Rumi, besides his father, were the Persian poets Attar and Sanai.....Rumi expresses his appreciation: ""Attar was the spirit, Sanai his eyes twain, And in time thereafter, Came we in their train"" and mentions in another poem: ""Attar has traversed the seven cities of Love, We are still at the turn of one street"". His father was also connected to the spiritual lineage of Najm al-Din Kubra.\n""With the Beloved\'s water of life, no illness remains\nIn the Beloved\'s rose garden of union, no thorn remains.\nThey say there is a window from one heart to another\nHow can there be a window where no wall remains?""\nFrom Thief of Sleep\nTranslated by Shahram Shiva\nThief of Sleep...Translations of Rumi\'s short devotional poems, or quatrians. The translator Shahram Shiva has drawn them from his study of more than 2000 of Rumi\'s poems, presenting a cross-section of the poet\'s many moods - from passion to adoration, all from the original Persian.\nThe Greek Poetry of Rumi\n""Medieval Sufi masters where Islamic scholars who were well versed with the Koran and the Hadith and frequently quoted from these sources in their speech and writing. They were also imbued with early Islamic mysticism. Nevertheless, they were attacked by orthodox ulemas, accused of heresy and blasphemy, and even subjected to persecution and execution. This was perhaps due to alien ethnic origins of Sufism, with emphasis on love, harmony, and some elements of pantheism versus the rigidity of other orthodox Muslims. Arguments about the roots of Sufism in ancient Indo-Iranian religions (Zoroastrian/Vedanta) are well recorded are not relevant to the present discourse. On the other hand, the Greek influence, which came much later, is attested by Greek poetry of Rumi and his son, Sultan Valad.\nAs the Sufi orders developed, they deviated in many ways from the early Islamic mysticism. Sufi doctrine grew in several stages, enriched by contacts with Gnosticism, Neoplatonism, and even Buddhism. They were also influenced by Greek philosophy, especially the works of Aristotle, which reached them through Islamic philosophers like Avicenna (d. 1037) and Averroes (d. 1198). When Rumi and his movement were established in Konya, the city was still under the influence of Christianity, and the Greek language was common among communities around the city. Thus the Sufis could not avoid being influenced by the Greek culture and philosophy that were promoted by the Christians. The English orientalist, F.W. Haslucke, describes these situations and states that in a mosque in Konya, that was formerly the St.Amphilochius church, was a tomb that was beloved to be that of Plato and the Muslims in the city had reverence for it and even some considered Plato a prophet*. There are also indications that both the Sufi masters and Saljuq monarchs encourages harmony and friendship between the Sufis and Christians. Much later when the Ottoman Sultans ordered the persecution and massacre of Armenias, Sufis sheltered and saved the lives of some of them.\nUnder these circumstances, we can assume that Rumi and his son knew Greek and wrote the so-called Greek poems.\nBoth Rumi and his son Sultan Valad wrote their poetry and prose primarily in Persian but there are occasional writings, in the orders of frequency, in Arabic, Turkish, and Greek. The Greek verses are mixed with Persian and Arabic lines and Turkish words, and they are written in the Persian/Arabic alphabet!""\n""Konya, 261 km (162 miles) south of Ankara (map), is Turkey\'s city of Whirling Dervishes, and has been for 800 years......Located right on the ancient Silk Road, Konya is an extremely old city, its roots going back to the days of the Hittites, who called it Kuwanna. As a Roman city, it was Iconium....the Mevlana Museum which shelters the tomb of Jelaleddin Rumî (1207-1273), known to his followers as Mevlana (or Rumî), a Muslim poet and mystic and one of the great spiritual thinkers and teachers of all time.""\n""Mevlâna Jelaleddin Rumî (1207-1273) was born near Balkh, Afghanistan, his life\'s work was as a religious teacher in Konya, capital of the Seljuk Turkish Sultanate of Rum.....Called Mevlâna (""Our Guide"" in Persian, the court language of the Seljuks), and Rumî (""of Rome,"" that is, of the Seljuks\' western, formerly Roman, lands), he first studied with his father Baha\'uddin, then went on to teachers in Haleb (Aleppo) and Damascus before returning to Konya in 1240.....His passionate poetry addressed to ""the Beloved"" was facilitated between 1244 and 1247 by his passionate spiritual friendship with Şemsi Tebrizi. The ""Sun from Tabriz,"" an older mystic, became Rumî\'s soulmate and the foil for his spiritual ecstasies......The two mystics were so close that Rumî\'s disciples, angered at the older man\'s overwhelming influence on their master, put Şemsi to death.....Disconsolate, Rumî withdrew from the world to write his poetic masterwork, the Mesnevi (Mathnawi in Persian). He lived and worked in Konya for another quarter century, sometimes whirling in the streets from sheer joy and spiritual delirium......After his death (known as Şeb-i Aruz), a Sufiorder called the Mevlevi (""Followers of Mevlana,"" or Whirling Dervishes) was founded by his son, Sultan Veled, based on Rumî\'s principles and practices.""\nNorthern New Mexico']"	['<urn:uuid:b7495260-1d98-46ff-9265-41e7284adbd8>', '<urn:uuid:584f10bb-17cf-469e-be85-b9e14bd489d7>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T12:39:32.578853	7	71	2942
31	key person insurance nj florida business death benefit	Key person insurance works the same way in both New Jersey and Florida - it helps replace lost revenue when a key executive dies. In both states, the business pays the premium while the key person is alive and receives a death benefit after their passing. This insurance is particularly important for small and mid-sized businesses to ensure continuity of operations and helps cover costs associated with finding someone to fill the deceased executive's role.	['Small business owners in New Jersey protect their establishments and employees with NJ business insurance, which can provide a financial safety net to cover unexpected expenses when things go wrong. These businesses are responsible for writing paychecks for more than 1.9 million employees, representing almost half of all workers in the Garden State.\nSmall business insurance provides financial protection against losses. The types of risks covered vary by policy and can include property damage, employee injuries and illnesses, liability and lawsuits, or acts such as cybercrime or employee theft. It can help keep your business on track after accidents and events such as:\n- Injuries to non-employees, including clients\n- Damage to someone else’s property\n- Claims of professional negligence\n- Employee injuries on the job\n- Auto accidents while driving for business\nMinimum business insurance requirements in New Jersey\nNew Jersey has multiple laws and regulations concerning business insurance requirements across a wide swath of industries operating within the state. The size of your business, industry, and number of employees will have the biggest role in determining insurance requirements for your business.\nWorkers compensation insurance and general liability insurance are the most common coverages in New Jersey. There are several factors that will mandate Workers’ Compensation Insurance in New Jersey. A licensed insurance advisor will be able to determine if your business falls within any of these requirements and if any exemptions are applicable.\nMost popular New Jersey business insurance policies\nWorkers’ Compensation Insurance\nNew Jersey state law requires most businesses with employees to carry workers’ compensation coverage. This kind of coverage provides protection when employees are hurt on the job, and can help cover medical bills and lost wages during their recovery.\nFor example, two employees at a construction company in Paterson suffer back injuries while moving a beam on the job, leaving them unable to work for a month. Workers’ comp could help pay for their medical expenses, and physical rehabilitation bills and cover lost wages while they get better.\nTypes of insurance under this include:\nYou and any other members of your business can acquire a life insurance policy. This is similar to key person insurance, providing a beneficiary with financial assistance in the event of your death. Having life insurance in place can give you peace of mind that your death will not burden your family or business partners financially.\nDisability income insurance\nDisability insurance is similar to workers’ comp in that it temporarily covers an employee’s lost wages if they are unable to work because of a disability. However, disability insurance will cover injuries or illnesses that occurred on or off the job, whereas workers’ comp only covers work-related issues.\nThis type of insurance is also sometimes required by law.\nKey person insurance\nWhen founders die, businesses often have a challenging time continuing operations. It is essential for entrepreneurs to create a business continuity plan so the company can still thrive if the worst happens.\nKey person insurance, also known as key man insurance or key woman insurance, helps replace lost revenue due to the death of a key executive of your business. Your business pays the premium while the key person is alive and then collects a death benefit after their passing.\nThese benefits can be essential to continuing the operation of your business or finding someone to fill their role.\nGeneral liability insurance\nGeneral liability coverage, also called commercial general liability insurance, is one of the most common and important forms of business insurance in New Jersey. If someone claims that your business caused injury or destroyed property, general liability insurance can help pay for damages and lawsuit costs if you’re sued.\nFor example, a homeowner accuses a Newark-based HVAC company of improper wiring that starts a fire and causes significant damage. General liability insurance could help cover the cost of repairs and pay attorney fees if the client takes the HVAC company to court.\nConstruction and cleaning companies in New Jersey might want to add tools and equipment coverage to their general liability policy. This optional coverage can help pay to repair or replace work gear that’s lost, stolen or damaged outside of normal wear and tear.\nProfessional liability insurance\nProfessional liability insurance, also known as errors and omissions (E&O) insurance, protects businesses that offer professional services in New Jersey. B2C businesses often use E&O coverage to protect against claims stating their services caused clients financial distress or bodily injury.\nDoctors’ malpractice insurance is a common type of professional liability insurance. This insurance type is also essential for professional service providers like consultants and financial advisors. Costs for professional liability insurance will vary depending on the industry and profession. For example, a doctor would likely pay more than a CPA.\nBusiness Owners Policy (BOP)\nIf you live in New Jersey and want general liability insurance and property coverage, you can package them together in a business owner’s insurance policy, also known as a BOP. A BOP provides liability coverage for customer injury, property damage, and product-related claims, in addition to commercial building and movable property coverage.\nMany BOPs also include business interruption coverage, which pays your lost revenues if you close for a covered claim. This coverage type is ideal for owners of small and midsize businesses like restaurants, retail stores, and wholesalers. Keep in mind that BOP insurance doesn’t cover your employees.\nCyber insurance protects businesses against losses and liabilities arising from cyber threats and data breaches. It provides coverage for expenses related to data breaches, including legal costs, customer notification, credit monitoring, and reputation management.\nBusinesses in New Jersey can use cyber insurance to manage the financial and legal risks associated with data breaches, cyber-attacks, and other cyber incidents.\nDirectors and officers (D&O) liability insurance\nIf your business has a corporate board of directors or advisory committee, you want D&O insurance. This insurance protects your directors’ and officers’ assets if they are personally sued for wrongful acts in company management (e.g., failure to comply with workplace laws, fraud, theft of intellectual property, misrepresentation of company assets, or misuse of company funds).\nCommercial auto insurance\nCommercial auto insurance provides coverage for vehicles used for business purposes in New Jersey. It includes liability coverage for third-party injuries or property damage, as well as coverage for damage to the insured vehicle.\nCommercial auto insurance is similar to personal automobile insurance; it protects your cars, trucks or vans in the event of damage, injury or liability claims. However, commercial auto insurance provides additional coverage, including property and liability trailer exposure, loading and unloading exposure, hired vehicle coverage, non-owned vehicle coverage, and higher coverage limits.\nCommercial auto insurance packages in New Jersey may include hired and non-owned auto coverage. This optional insurance upgrade can help pay for business accident damages for employees who occasionally drive their own cars for work or rent vehicles.\nCommercial property insurance\nCommercial property insurance in New Jersey can protect the things you need to run your business, such as equipment, inventory and even your business location if you own the property.\nIt’s important to read your policy documents carefully to make sure there aren’t exclusions for your business. Some locations prone to flooding or hurricane damage might have limited coverage.\nBest business insurance providers in New Jersey\nAllstate offers a line of insurance for small businesses. Its coverages include BOP, property coverage, business liability, business interruption, equipment breakdown, and commercial auto.\nAllstate’s overall ease of use seems to lag behind some other companies in our review. You can get a quote for a commercial auto policy online, but it’s not clear how easy it is to get a quote online for other business insurance products — it appears that an agent must complete quotes for some business types.\nAlso, according to its website, you can only file a claim through an 800 number.\nChubb is one of the largest insurers in the United States. Its business insurance offerings include workers’ compensation, BOP, cyber insurance, professional liability, general liability, and business umbrella.\nOn the Chubb website, you’ll find a lot of useful information to help small business owners understand their insurance needs, including articles and tools such as cost estimators. You can get a quote online or through an agent and report a claim online as well.\nNext Insurance is a relative newcomer to the small business insurance market. However, the company has received significant financial backing from Munich Re, one of the world’s largest multinational insurance companies.\nAs it is among a growing number of companies focused on insuretech, Next Insurance offers an easy-to-use web experience. You can get online quotes for general liability, workers’ compensation, BOP, commercial auto, and more. You’ll also find coverage packages tailored to the typical needs of certain types of businesses such as contractors, retailers, fitness providers, and even Amazon sellers.\nAlso, you can download a certificate of insurance through the website or the company’s app.\nBased in New Jersey, NJM sells business insurance to customers in several mid-Atlantic states. The company offers workers’ compensation, commercial auto, commercial excess and umbrella, EPLI, cyber insurance, BOP, and a commercial package policy.\nThe commercial package policy includes property coverage, liability coverage, business income insurance, inland marine, and crime coverage. It is similar to a BOP but caters to businesses with more extensive coverage needs. With NJM’s small business insurance policies, you can get a quote online or through an agent and file a claim online or by phone.\nWhile perhaps best known for its personal auto coverage, and ubiquitous advertising mascot, Progressive offers a broad slate of business insurance. Its offerings include BOP, professional liability, general liability, workers’ compensation, cyber insurance, commercial auto, and contractor liability.\nThe company claims to be the number one commercial auto insurer in the U.S. (based on premium), with more than 1.9 million insured vehicles.\nThe Progressive website promises quotes in just a few minutes and includes a variety of resources, such as articles and FAQs, to help you understand your coverage needs. The company is also supported by a countrywide network of independent agents. You can file a claim online or by phone.\nState Farm offers various types of protection, including BOP, commercial auto, liability, workers’ compensation, inland marine, E&O, EPLI, key person coverage, and more. The company website’s “Small Biz Quiz” is a helpful feature designed to assist you in determining coverage needs and you can also use State Farm for your retirement plan.\nPerhaps State Farm’s only drawback is its ease of use. Quotes and policy sales are through its local agents only, and the website does not appear to offer a claim reporting feature — you have to call the company instead.\nHow much does small business insurance cost in NJ?\nThe cost of business insurance in New Jersey can vary widely based on several factors. Business size, industry, and amount of coverage all affect monthly premiums in New Jersey.\nFor example, the cost of a Business Owners Policy for a manufacturing company operating in Newark will be very different from a Business Owners Policy for a hardware store in Paterson. Other considerations such as annual sales, payroll, company assets, and claims history also help determine the cost of business insurance in New Jersey.\nA licensed insurance advisor will be able to identify Business Insurance discounts in New Jersey that you may qualify for, along with discounts for bundling Business Insurance in New Jersey.\nThe average costs of business insurance in New Jersey are:\n- General liability: $40 per month\n- Workers’ comp: $44 per month\n- Professional liability/E&O: $64 per month\nBusiness insurance packages in New Jersey will be priced based on your company’s insurance requirements. Some of the factors that will affect your cost include:\nYour profession and risk profile\nIndustries that have a greater risk of injury and property damage can expect to pay more each month for coverage. For example, construction companies in New Jersey will likely have a larger business insurance bill than professional services firms.\nThe coverage options you select\nWhile you might be tempted to omit some types of recommended coverage when purchasing your insurance package, this may not be in your best interest. Although it could save a little money every month, this can put you at risk of paying out of pocket if you don’t have the right coverage when an accident happens.\nNumber of employees\nMost states determine workers’ compensation insurance costs based on the number of workers on your payroll. Providing up-to-date employee numbers for your insurance quote is the best way to get an accurate price.\nThe best way to explore coverage options and find out how much you’ll pay for a business insurance policy is to get a free instant quote.\nHow much business insurance coverage do I need in New Jersey?\nThe amount of coverage you need — and its cost — will vary greatly depending on the type of business you run, its size, number of employees and assets, and other factors.\nMost small businesses need general liability insurance, property insurance, and business interruption coverage. You can package these coverages into a BOP to make things easier and, in most cases, save some money. If you have employees, you’ll likely also need workers’ compensation. Meanwhile, you’ll need commercial auto insurance if your business owns, rents, or leases cars or trucks.\nMany insurance company websites have coverage selection tools to help you understand your business’s coverage needs. Some companies even sell packages of coverage tailored to the needs of specific types of businesses. You can also reach out to an insurance agent who sells business insurance.\nAn agent will take the time to understand your business and match you with a policy that meets your needs.\n- Business Insurance Quotes: Getting Insurance Quote Online\n- The Hartford Business Insurance Review 2023\n- 10 Best Commercial Insurance Companies 2023', 'When you own a business, you instantly become a target for the unscrupulous. Some people believe that if you own a business, you’re automatically rich, so you’re going to need protection. In the state of Florida, a business must have adequate liability insurance. If your business is taken to court for any reason, an experienced Central Florida business attorney can work aggressively to defend you and your business.\nBusiness owners take on a number of legal obligations, and they typically also face a number of often-confusing legal challenges. Even a criminal trespasser, for example, can sue you if he or she falls and gets injured on your business property. Thus, one of the priorities for protecting your Florida business is liability insurance, also called Commercial General Liability (CGL), which covers four types of incidents that your business could be held liable for: physical bodily injury; damages to another’s property; a personal injury such as slander or libel; and misleading or false marketing or advertising.\nPRECISELY WHAT DAMAGES DOES CGL INSURANCE COVER?\nCGL does not cover employees; employee coverage is left to Florida’s workers’ compensation system. However, if others sue your business, three types of damages are covered by a CGL policy. Firstly, compensatory damages, which are the financial losses suffered by the injured party and the future losses they may suffer resulting from an injury.\nSecondly, general damages, which are for non-monetary losses such as mental anguish or pain and suffering. Finally, punitive damages are any additional penalties and charges a defendant must pay as “punishment’ in a civil case. Generally, however, the following are not covered by CGL insurance in Florida.\nClaims from employees regarding sexual harassment, wrongful termination, hiring discrimination, and employment discrimination are covered instead by employment practices liability (EPL) insurance. The cost of your company’s EPL coverage hinges on how many employees you have, if you’ve been sued previously by employees, and other risk factors. EPL also pays a company’s legal costs in these cases.\nClaims related to motor vehicles. If your business owns vehicles, you must have separate, commercial auto coverage to protect your employees and your business against claims arising from traffic accidents. If your personal vehicle is used mainly for business, putting the company’s name on the policy helps you sidestep any complications if you need to submit a claim after a traffic collision.\nProfessional liability insurance covers wrongful practices committed by professionals who provide services: lawyers, healthcare providers, consultants, and other professionals. Professional liability covers faulty, negligent service or the failure to provide services entirely (omission). Malpractice insurance protects doctors and other healthcare providers from liability linked with physical injuries and medical costs, as well as the legal costs involved with such claims.\nWHAT IS CONSIDERED IN A FLORIDA CGL INSURANCE QUOTE?\nBuy insurance only from someone you can trust, and read every line of every policy. If you aren’t clear about a policy’s provisions, seek a business attorney’s advice. Insurance companies in Florida consider several factors when they offer a Florida commercial general liability insurance policy. The cost of premiums is partially determined by the sales and payroll estimates provided by the company. If final amounts are higher than those estimates, you may be asked to pay\nThe cost of premiums is partially determined by the sales and payroll estimates provided by the company. If final amounts are higher than those estimates, you may be asked to pay for an incremental premium, but if the final amounts are below the estimates, a refund is possible.\nAnother consideration is the type of business you’re in. An electrician or a landscaping service will pay more than a company in a less risky business. Insurance companies also must consider the number of claims filed against an industry and the chances of a claim against similar companies; the longevity and stability of a business; state laws and industry regulations, standards, and practices; and the company’s methods of preventing and managing potential risks. If you can document a history of good safety practices and procedures, your business may be deemed a lower risk and charged reduced premiums.\nLiability insurance is an imperative for every business because there is simply no way to ensure positively and absolutely that your business will never be sued. That’s also why business owners in Florida need to have an experienced central Florida business attorney on your team and at your side. Keeping your business out of expensive and injurious legal battles is a good business lawyer’s highest priority.\nWHAT ELSE SHOULD FLORIDA BUSINESS OWNERS CONSIDER?\nAnother insurance product that you may want to consider is called “key person” insurance, which is life insurance for the important people in a small or mid-sized business. Here’s how key person insurance works: A company purchases life insurance policies for its key employees, pays the premiums, and is the named as the beneficiary. If a key person unexpectedly passes away, the business acquires the insurance payout.\nMost recently, social media insurance and cyber insurance have been created to cover data breaches, cyber-attacks, and damage to your company’s online reputation. While it’s a good product to have, if someone steals or destroys your online intellectual property, it’s probably a criminal matter, and a good Central Florida business attorney can offer you the legal advice you’ll need in that circumstance.\nAsk your attorney to review your contracts and to examine any proposed new contracts. Your business lawyer can also review your records, policies, procedures, practices, and employee handbook to ensure that you’re in full compliance with the abundance of local, state, and federal laws, rules, and regulations that govern businesses in Florida.\nNobody likes to think about risk, but business owners must. Full compliance with zoning, licensing, employment, environmental, and advertising laws are imperative; a good business attorney can help you achieve compliance and maintain it.\nIn conclusion, here are some additional suggestions for protecting your business:\nWhen you don’t trust someone in a deal, walk away, even if you don’t have a precise reason for your distrust. Adhere to the old, basic advice, and go with your gut instinct.\nYou can’t count on verbal agreements. Insist that every agreement is put in writing.\nIf you can’t meet your contractual obligations for any reason, negotiate a termination or an amendment to the contract that will not send the other side running for their lawyers.\nEvery Florida business owner should work with an experienced business lawyer you know, like, and trust.']	['<urn:uuid:7cb7ceff-1792-4cb8-ad59-08f2b8828c38>', '<urn:uuid:623610e6-9165-40d0-a788-cf13aec80d8e>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	8	75	3378
32	How do fluorescent lights perform in medical departments?	Fluorescent lighting in surgery, internal medicine and heart care departments was found to be suitable, with a color rendering index (Ra) of 74 and daylight color that fits patient rooms. However, the design had limitations due to the lack of variety in lighting colors and absence of lighting color control methods.	"[""Please use this identifier to cite or link to this item:\n|Title||Effect of Lighting on patients’ Well-being|\n|Title in Arabic||تاثير الاضاءة على الصحة العامة للمرضى|\nLight is a fundamental part of life. Light gives us the natural rhythm of day and night, determines how we see things, and brings our world to life. Accordingly, it has a major impact on our sense of well-being, our emotions and our functioning. The notion that a well-balanced and attractive environment is of major importance to patients’ health is not new. This research comes in an attempt to study the lighting design for hospitals and its impact on patients’ well-being (visual, biological and psychological) which can promote patient recovery. The research also seeks to study the impact of illuminance level, correlated color temperature (CT), color render index (CRI) and lighting control on lighting design. In addition, the research display the lighting design in Al Shifa hospital patients’ room in surgery, internal medicine and heart care departments. In this context, the research assumes that the natural and artificial lighting have a significant impact on patients’ well-being and the illuminance level, color temperature (CT), color render index (CRI) and lighting control are the most important characteristics of light to be considered. The research is carried out using Post-Occupancy Evaluation (POE) tool, which used mixed method that depend on quantitative and qualitative data. Questionnaire was used to collect qualitative data and analyze it, and the number of participants who fill the questionnaire were 30 inpatient, amd those participants are distributed on 21 patients' room, during the application of the questionnaire measurements of lighting intensity inside patients room were taken by using iPad app called Luxmeter. The results indicate that type of existed lighting (Fluorescent) in surgery, internal medicine and heart care departments in Al-shifa’ hospital was suitable, whereas Ra= 74, and color of lighting is daylight which fits with patients room, but there is a problem in lighting design, which is represented in the absence of variety in colors of lighting and controlling method in colors of lighting. In addition, it indicate that good design includes the availability of dimmed lights during sleeping time, which increases public satisfaction level about lighting and increases the biological effects on patients’ well-being inside the rooms during sleeping, healthy sleeping can help to improve the patients’ well-being by helping to secrete the human body melatonin hormone in darkness. Good design according to design consideration, which includes design quality that is specialized in lighting intensity, color temperature, color render index (CRI) and control system lead to improve the well-being of patients, which increases the level of public satisfaction, visual comfort, improves the feeling of the place, mood, and psychological comfort. Therefore, the research recommends using high quality lighting types with high Ra, coloring temperature (CT) of lighting similar to daylight such as neutral white. In addition, use lighting that include automatic control in healthcare facilities. In addition, use several illumination for the patients’ that serve every patient individually, and must provide bedside lighting fixtures, high intensity lighting above patients’ beds, and patient control of these lighting. Moreover, providing dimmed lights for patients’ at sleeping time.\n|Publisher||الجامعة الإسلامية - غزة|\n|Files in this item|""]"	['<urn:uuid:64415453-1d92-44d7-919b-46416f2992b4>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	8	51	531
33	As someone fascinated by early American commerce - how did both the Mesa Verde Anasazi people and the late 19th century Native American craftspeople make a living?	The Anasazi at Mesa Verde (600-1300 AD) were primarily subsistence farmers who grew crops on nearby mesas, with corn as their main crop. The men hunted, while women created basket weaving and pottery. In contrast, by the late 19th and early 20th century, Native American artisans adapted to a commercial market, creating items specifically for sale to dealers and collectors. They produced items like finely coiled baskets, Zuni Kachina dolls, ivory pipes with hunting scenes, and modified traditional designs using trade materials like glass beads to appeal to non-Native patrons.	"['Pipe Bowl representing Owl\nArts of the Americas\nThese Native American objects represent just a few of the items made in the late nineteenth and the early twentieth century, primarily for sale to dealers and collectors to satisfy the growing market for indigenous products. Finely coiled baskets like the example by the Maidu weaver Mary Kea’a’ala Azbill were in great demand, as were Zuni Kachina dolls. The desire for Eskimo objects such as the ivory pipe engraved with a whale-hunting scene was accelerated by the Klondike Gold Rush in 1897. To appeal to non-Native patrons, Native artists invented new designs using trade materials such as the glass beads forming the embroidered floral arrangement on the northeastern puzzle bag (so named for the way its pieces fold together to keep it closed). Other artists used traditional materials but tailored designs to non-Native aesthetics, as seen in the porcupine-quill box. The Navajo quickly adapted to the Spanish introduction of silver coins and silver mining in the seventeenth century, embellishing their traditional wrist guards with hammered silver. Some artists retained both traditional materials and designs but produced greater quantities of popular items such as the Plains owl pipe bowl made from Catlinite (red pipestone).\nCatlinite or pipestone\nearly 20th century\nThis item is not on view\nGift of Cynthia Hazen Polsky\nYou may download and use Brooklyn Museum images of this three-dimensional work in accordance with a Creative Commons license\n. Fair use, as understood under the United States Copyright Act, may also apply.\nPlease include caption information from this page and credit the Brooklyn Museum. If you need a high resolution file, please fill out our online application form\nFor further information about copyright, we recommend resources at the United States Library of Congress\n, Cornell University\n, Copyright and Cultural Institutions: Guidelines for U.S. Libraries, Archives, and Museums\n, and Copyright Watch\nFor more information about the Museum\'s rights project, including how rights types are assigned, please see our blog posts on copyright\nIf you have any information regarding this work and rights to it, please contact email@example.com\nPlains (unidentified). Pipe Bowl representing Owl, early 20th century. Catlinite or pipestone, 3 3/4 x 5 3/8 in. (9.5 x 13.7 cm). Brooklyn Museum, Gift of Cynthia Hazen Polsky, 80.98.2. Creative Commons-BY (Photo: Brooklyn Museum, 80.98.2_view2_bw.jpg)\noverall, 80.98.2_view2_bw.jpg. Brooklyn Museum photograph\n""CUR"" at the beginning of an image file name means that the image was created by a curatorial staff member. These study images may be digital point-and-shoot photographs, when we don\\\'t yet have high-quality studio photography, or they may be scans of older negatives, slides, or photographic prints, providing historical documentation of the object.\nCatlinite or pipestone pipe bowl carved in the form of an owl perched on a rectangular bar. Beak, tufts, eyes, and legs carved with details of eyes and feathers indicated by incised lines. The back of the bowl, under the bird, is carved with a serrated edge and turns up at the end as though that owl had a tail. The area around the opening in the owl\'s head is blackened. A pipe stem would be added to this bowl for smoking. Catlinite is only found in the Minnesota region in what is called Pipestone National Park with mining rights reserved for Native Americans. Use of pipestone is restricted to Native Americans.\nNot every record you will find here is complete. More information is available for some works than for others, and some entries have been updated more recently. Records are frequently reviewed and revised, and we welcome\nany additional information you might have.', 'Favorite Photo Locations: Mesa Verde National Park\nIn this edition, Red River Paper Pro Ron Wolfe covers the iconic Mesa Verde in Colorado\nMy most recent trip to Mesa Verde was on the first day of November. Going to a National Park offseason has the advantage of fewer visitors. On the flip side, some attractions may be closed. In this case, several cliff dwellings were closed, but there were still sufficient cliff dwellings open to get a good flavor of the way the Anasazi people lived.\nMesa Verde is located in the Southwest corner of Colorado. The park’s entrance is on U.S. Route 160, about 9 miles east of Cortez and about 7 miles west of Mancos, Colorado.\nMesa Verde National Park was created by President Theodore Roosevelt in 1906, to protect some of the best-preserved cliff dwellings in the world. The name Mesa Verde means “green table” in English.\nMesa Verde contains some of the most notable and best preserved archaeological sites. It occupies 81.4 sq. miles and features ruins of structures built by the Ancestral Pueblo people, generally called – Anasazi. Mesa Verde is best known for its cliff dwellings, which are structures built within caves and under outcroppings in cliffs. There are over 600 cliff dwellings, and more than 4,000 archaeological sites of the Pueblo people here. Out of the 600 cliff dwellings, 75% contained only 1-5 rooms, and many were single room storage units.\nArchaeologists tell us that the Anasazi inhabited Mesa Verde roughly between 600 and 1300 AD. By 750, they built mesa-top villages from adobe. In the late 1190s, they began to build the cliff dwellings for which Mesa Verde is known. The Anasazi were mainly subsistence farmers, growing crops on nearby mesas. Their primary crop was corn, which was the major part of their diet. Men were also hunters. Women were famous for their elegant basket weaving and pottery. Anasazi artifacts are highly prized today.\nTwo of the cliff dwellings visited on this trip were the Spruce Tree House and the Cliff palace.\nSpruce Tree House\nSpruce Tree House is the third-largest village, close to a spring; it had 130 rooms and eight kivas, or ceremonial chambers. It was believed to have been constructed between AD 1211 and 1278, and 60 to 80 people lived there at one time. Because of its protected location, it is well preserved. It is the only cliff dwelling open year-round and has-guided tours daily during winter months. The moderately strenuous half mile paved trail includes a steep 100-foot descent and ascent. Stop frequently on the ascent to take photos.\nSpruce Tree House can be photographed as you approach down a trail as well as from near the dwellings, if you are taking photographs, check maps and consider the suns location and possible shadows.\nIn 1888 two cowboys tracking stray cattle in a snowstorm stopped at the edge of a steep-walled canyon. Through the snow flakes they made out traces of walls and towers of a large cliff dwelling across the canyon. Climbing down a makeshift ladder, the cowboys explored the network of rooms that they named Cliff Palace. Inside, they found stone tools, pottery, and other artifacts in rooms that had been uninhabited for over 600 years.\nThis multi-storied ruin, the largest and best-known of the cliff dwellings in Mesa Verde, is located in the largest cave in the center of the Great Mesa. It was positioned with a south- and southwest-facing architecture, providing warmth from the sun in the winter. The site had 200 rooms; including storage rooms, open courts, walkways, and 23 kivas. Dating back more than 700 years, the dwelling is constructed of sandstone, wooden beams, and mortar. On the surface of many walls, the people decorated with earthen plasters of pink, brown, red, yellow, or white. These were the first things to erode with time. The population was thought to be approximately 100 people. It is believed that the Cliff Palace was a social and administrative site with high ceremonial usage.\nWear sturdy shoes and be prepared for some strenuous climbing, if you plan to visit cliff dwellings. Exiting from the Cliff Palace is via a climb up steep ladders, if you believe you will need to stop frequently, allow others to climb out before you.\nThe Cliff Palace can be photographed from an outlook above as well as from near the dwellings as you hike. If you are taking photographs, check maps and time of day so you are positioned to take photos in good light.\nTips for visitors:\n- be respectful of other visitors. Mesa Verde belongs to them too\n- visit at a time of year when there are less tourists, to get those nice photos of villages without people\n- use maps from the visitor center, and try to visit when sunlight is more flattering and the structures are less in the shadows\n- listen to rangers and look for details that reveal how these ancient people lived\n- travel light so you can climb ladders, crawl through tunnels and peek in living spaces when allowed\n- visit some of the smaller, less famous ruins\n- Official website (NPS.gov)\n- Lodging and detailed vistor information (www.visitmesaverde.com)\n- Mesa Verde Museum Association (www.mesaverde.org)']"	['<urn:uuid:8ace754c-5524-46fa-af6c-65b7117174eb>', '<urn:uuid:fe40a6c4-b092-4e41-92cf-9907b3b0172c>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T12:39:32.578853	27	90	1469
34	router location tips for better wifi	To achieve optimal WiFi coverage, the router should be placed in a central location and kept free from obstructions. Putting your router in a cabinet can negatively impact its ability to serve all users. Using a WiFi analyzer can help you create a map of your network and identify dead spots or obstructed connectivity, allowing you to find the best position for your router rather than moving it around blindly.	['What is the Best WiFi Analyzer and How to Use One on a Mac\nThe benefits of a WiFi network come with certain complications that revolve around the implementation, security, and maintenance of the network. The flexibility and utility of a WiFi network can be very dependent on the positioning of your router, the channel that you choose to use for data transmission, and the ability of the signal to reach all intended users. In order to keep your WiFi network operating at peak efficiency, the use of tools such as a WiFi signal analyzer can be extremely beneficial.\nWhat is a WiFi Analyzer?\nA Wifi analyzer is a specific type of network analyzer that is designed to work with wireless networks. They can be obtained as an application which runs on a computer or a mobile device as well as in hardware implementations that are usually used by dedicated network engineers. A WiFi analyzer can be used in conjunction with your firewall and security programs to assist in tuning your network so it attains optimal performance.\nSome of the uses of a network or WiFi analyzer are to:\n- Provide statistics for recent and current network usage.\n- Detect bottlenecks in network traffic such as overused channels.\n- Identify dead spots in your WiFi network’s coverage.\n- Assist in determining the best location for your wireless router.\n- Monitor network traffic and the flow of data packets.\n- Display network statistics in a user-friendly interface.\nHow a WiFi Analyzer App Can Help Improve Your WiFi\nA variety of issues can cause problems for the users of a WiFi network. Among them are:\n- The unreliability of WiFi connections. If you are getting blown off the network with any regularity it could be the sign of an underlying network problem.\n- Slow data transmission.\n- Uneven WiFi coverage leading to seemingly inexplicable dead spots that pop up throughout the covered area.\n- WiFi signal leakage that can cause security issues by allowing individuals to access your network from outside your home or office.\nLet’s look at how a WiFi analyzer for your Mac can help alleviate or at least minimize these issues.\nUnreliable Connections and Dead Spots\nProperly positioning your WiFi router is an essential component in enabling it to provide the best coverage of your intended area of usage. It should be in a central location and be situated where it is free from obstructions. Putting your router in a cabinet can definitely have a negative impact on its ability to serve all of your users.\nUsing a WiFi analyzer you can display a map of your network and determine where dead spots or obstructed connectivity is impacting the network’s performance. Analyzing the data provided by the tool can be instrumental in allowing you to reposition your router to achieve better results. Your chance of improving your WiFi coverage is far greater when using an analyzer application than blindly moving your router around.\nWiFi Signal Leakage\nSignal leakage can be a major problem for both businesses and home users. Nefarious individuals can attempt to break into your network from outside your building, making it difficult or impossible to identify the attackers until they have gained access to your network.\nA WiFi analyzer can be useful in tuning your router in an attempt to keep the signal within accepted boundaries. It can also help you identify unauthorized efforts to access the network.\nSlow Data Transmission\nThis issue can also be combined with dropped connections and can often be the result of using a busy WiFi channel for data transmission. Many users simply go with their router’s default channel. In areas such as apartment buildings, where the signal from several routers can overlap, this can cause network traffic jams that slow down your access and can cause a connection to fail.\nA WiFi analyzer can help you identify the channels that are in use and enable you to find a less-used one for your network. Once you determine the channel you want to use, you can update your router’s configuration to take advantage of the information returned by the analyzer. Again, you could do this with trial and error, but using an analyzer is a much more efficient way to go.\nChoosing a WiFi Analyzer\nThere are a couple of factors that might influence your choice of a WiFi analyzer. Obviously, the platform you will run the app on is important. You need an app that supports the operating system you are using.\nYou can obtain WiFi analyzers that are freeware and do not require any financial investment. There are also commercial products available. In some case, they may have additional features not found in the free offerings.\nAnalyzing Your WiFi with KisMAC\nLet’s look at how to analyze your WiFi using the KisMAC WiFi analysis application. It is one of the best free WiFi analyzers for the Mac, but you may have issues using it due to the restriction on the network cards that it supports. It is a freeware tool that is intended for experienced users. You can use the tool to draw maps of network coverage to help you find better positioning and identify dead spots. It displays all network activity in range and allows you to see which channels are heavily used so you can pick one for your network.\nThe Best WiFi Analyzers for macOS\nHere are some alternative WiFi analyzers for the Mac that you may find helpful in troubleshooting or optimizing your WiFi network.\nNetSpot is a great WiFi signal analyzer that is available as a free download to allow you to test-drive the application before upgrading. The app displays the network information in a user-friendly interface and provides all of the details that you need to get your WiFi running at peak efficiency.\n“The easiest part about NetSpot, is everything about it. The verdict we have settled on comes after years of using subpar Wi-Fi analyzer tools that either offered a decent user interface, or the top features – but never both of those things. We can confidently conclude there isn’t a competitor that puts user experience on the front foot like NetSpot does, without holding anything back in terms of performance.” NetSpot Review at thesweetbits.com\nUpgrading to the paid version allows you to conduct surveys and draw maps of your WiFi network, making it a great tool to use when planning and installing WiFi.\nHere is another WiFI analyzer that presents its data in an intuitive user-interface. A free light version is available, with paid versions providing more features. It can help you find the best channel for your network to use and has enhanced capabilities such as spectrum analysis in the Pro version. This can help you eliminate sources of interference that are impacting your WiFi network.\nThis tool for the Mac provides detailed graphical representations of nearby WiFi networks. Advanced features such as filtering and the ability to show signal level changes over time make this a great tool for analyzing your network. You can connect to detected networks with the click of a button if you are authorized or if they are not password protected.\nMacOS WiFi Analyzer\nHolding down the option key on your Mac’s keyboard while clicking on the WiFi icon in the menu bar will give you the option to “Open Wireless Diagnostics” and scan all available wireless networks. This tool is part of the macOS operating system and may be just what you need to find the best channel for you to use.\nWe hope that this discussion of WiFi analyzers has pointed out some ways that this type of tool can help you improve your WiFi network at home or at the office. If you are experiencing any issues with your WiFi network, one of the tools mentioned above can assist in resolving your difficulties.']	['<urn:uuid:2e8fdbd6-db18-4c8f-b7b3-c06859f4b4bd>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	6	70	1311
35	Do scroll artists need special equipment for the base preparation?	The canvas preparation requires a large wooden frame with four bamboo lengths, white marble for rubbing, a conch shell for polishing, and materials to make a coating mixture consisting of white clay, water, and glue. The mixture is applied 8-10 times and blessed medicines may be added if available.	['Thangka is a scroll painting created in Tibetan script on bright satin and hung as an offering on a religious scroll. Thangka is a unique type of art in Tibetan culture, the subject matter of which includes Tibetan history, politics, culture, and societal life. It can be said that it is a sort of encyclopedia of the Tibetan culture, as well as a precious intangible cultural heritage of the Chinese nation.\nThe production of thangka is a complicated process that necessitates a great attention to detail. It must be done in accordance with the teachings of the sutras, including the ritual that starts before the painting even begins, the manufacturing of the canvas, the drafts, and application of color, the finalization of the designs, the addition of gold and silver, the opening, the stitching, hanging, and the blessing of the work. Every step must be performed with the utmost care. If even one stroke is incorrect, all one’s efforts will be wasted.\n1. Canvas Preparation\nThe first thing that must be done in order to create a Thangka is to select a piece of flat, smooth, and thick white cotton cloth. The canvas must not be dirty, and must not be holed or perforated. The canvas should also be long and wide, with proportions suitable to the size of the cloth. Either side of the canvas can be used, but there must not be a crease or fold on the surface, or else this will potentially affect the design.\nThen, the cloth is carefully sewn onto four lengths of bamboo which are tightly strung to a large wooden frame. The artist then spreads a cost of glue over the whole canvas and leaves it to dry. He stirs up a mixture of white clay, water and glue in a clean pot to the consistency of thick cream. Blessed medicines or other sacred substances are added if available. The mixture is then strained through fine gauze to remove any impurities and applied evenly to the dry canvas. When this second coat has dried, the canvas is held up to the light; and the areas which have not been evenly coated are patched up with more of the clay mixture and again left to dry. This process is repeated 8-10 times until the entire canvas is evenly coated.\nThe canvas is then laid upon a smooth wooden board and a small area is moistened with water using a soft white cloth. Section by section, the artist vigorously rubs the canvas smooth with a piece of white marble, moistening it with water as he works. This takes about an hour. The entire canvas is then slowly are carefully stretched by tightening the strings tied to the frame and left to dry in indirect sun.\nOnce dry, the entire procedure is repeated for the other side of the canvas, stretching it after each moistening and leaving it to dry. When it has been thoroughly treated and dried, the canvas should be so tightly stretched that it makes a nice drum sound when tapped. This is the sign it is ready to be painted. The front of the canvas is then polished with a conch shell.\n2. The Secret of Colors\nWhen drafting the brightly colored thangka, two kinds of coloring are used. The first is conventional Chinese art paint, which fades over long periods of time. The second is mineral paint, which does not fade. Commonly used paint colors are black, red, white, blue, and rouge. The paint used is all natural ore derived from gold, pearl, coral, colored glaze, lapis lazuli, and so on. In Tibetan culture, gold and silver jewelry is an extremely important adornment.\nCarbon brushes are used to design the thangka, with big, medium-sized, and especially small writing brushes accompanied by paint cases and an easel. When producing a thangka, there are many small details one must mind. Many of the elements of the painting process require very careful attention, and some require the use of a very fine brush. Thus, many brushes used are made of very fine weasel bristle.\nTHANGKA PAINTING MAKING PROCESS\n1. Making the First Draft with Carbon Brushes\nTo sketch the figures in a thangka, the artist must be an expert in the measurements and proportions of Buddhas, Boddhisattvas and deities. There are thousands of different deities in Tibetan Buddhism. The artist will have to rely on a grid of exactly positioned lines to sketch the deities. The basic system of these coordinates is one vertical and two diagonal lines. The intersection of these three lines defines the centre of any thangka. In thangka with more than one figure, there will be additional circles and connecting lines, to contrast the main icon with the background figure. The grid system divides the painting into different parts with fixed proportions. If the artist wants to have a thangka twice the original size, he has to double the dimensions or distances between all the lines.\nThere is a definite, specific sequence to color application. In general, the thangka is painted from top to bottom. The first step is the sky. The dark green landscape and all the dark green areas are next. This is followed by light blue, then light green, red, orange, pink, brown, pale orange, yellow, pale yellow and finally white. When the whole series of base coat colors have been applied and allowed to dry, the thangka is scraped with a razor blade, held at an arched angle to the cloth, to smooth away any roughness in the paint. The dust is brushed off with a soft cloth or feather.\n3. Redrawing and Shading\nThe original detailed lines of the clouds and flowers which have been covered by paint are redrawn in pencil and traced over in black ink. The artist then shades them with a fine paintbrush.\nPainting the intricate details of the back and foreground landscape and brocade clothing designs follows the same sequence of color application as above. This takes 18 to 20 days to complete.\n5. Body Shading and Final Painting\nThe artist then shades in color to give shape to the figure’s body and face. The flowers are given a final shading, and all the minute background details such as fish, deer, birds, fruit and countless grass blades are painstakingly painted.\n6. Gold Application\nA considerable quantity of gold is used to highlight the painting and give it its final glorious touches.\n7. Opening the Eyes\nThis is the most important moment of a thangka artist’s work. Before painting the figure’s eyes, the artist bathes and makes offerings to the Buddha’s body, speech and mind.\nThe tailor will affix a brocade frame to the completed thangka.\nThis final step is what distinguishes Tibetan Buddhist practice from ordinary “idol worship.” The practitioner takes his or her newly completed thangka to a highly realized Buddhist master and makes offerings to request the master’s blessings. The master, endowed with the clear mind of enlightenment, is able to “bring alive” the image on the thangka by infusing it with energy and beseeching the deity to open its eyes and look upon all sentient beings. The thangka, having now been properly consecrated, is a receptacle of wisdom. It is ready to be hung and venerated as a genuine living embodiment of enlightened mind.\nIt is important to note that this final step is only necessary if the thangka artist himself is not acknowledged as a realized being. Over the centuries, many important Buddhist masters have intentionally taken rebirth as thangka painters, and if such an artist creates a thangka, the very mind of the artist naturally consecrates the image being painted. In such cases, there is no need to seek the services of a lama for an additional consecration.']	['<urn:uuid:d6bc4a50-20b5-4f90-aaa0-4fc5196831f8>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	10	49	1294
36	what is spinal muscular atrophy causes symptoms breathing problems	Spinal Muscular Atrophy (SMA) is a genetic condition caused when the SMN1 gene does not work properly. SMA affects nerves in the spine that signal muscles to move, including muscles used for breathing. As the condition progresses, these nerves weaken and may die, leading to muscle damage. People with SMA often have weak respiratory muscles, making it difficult to cough and clear mucus from their lungs. This puts them at higher risk for severe respiratory infections and complications. In fact, breathing and respiratory issues are the most common cause of death in children with type 1 or 2 SMA.	['Flu is prevalent in our community right now. Visit our Flu Resource Center to learn about flu prevention, signs and symptoms, and help us protect our patients, families and staff from RSV and the flu by following these visitation restrictions.\nCenters of Excellence\nFind a Service Location\nLive life young at heart. 28 days and 28 ways to live heart healthy.\nA new guideline was published in March by the American College of Obstetricians and Gynecologists stating that individuals planning to start a family or pregnant should be offered carrier screening for Spinal Muscular Atrophy (SMA).1,2\nSome questions that may come to mind are:\nSpinal Muscular Atrophy (SMA) is a genetic condition that affects the nerves located within the spine. These nerves are responsible for signaling muscles to move, whether it’s to nod your head, walk, or breathe.\nAs this condition progresses, the nerves weaken and may die, leading to muscle damage.\nAlthough there are different types of SMA, people with the most common form usually pass away from breathing complications before age four.\nSMA is caused when a gene, in this case SMN1, does not work properly. Genes tell our body how to perform a task. SMN1 plays a role in helping nerves signal to muscles.\nIndividuals normally have two copies of SMN1, one from each parent. Carriers usually have only one copy of SMN1, but carriers of SMA do not have symptoms and do not have SMA. Both copies of SMN1 do not work in individuals with SMA.\nGene function for carriers is like a toaster with two slots for bread. If one slot does not work, bread can still be toasted, but if both slots do not work, no toast can be made.\nCarrier screening uses a blood sample to see if you are a carrier of one or two SMN1 genes. Carrier screening is designed to identify most people that carry SMA, but does not find all people who carry SMA.\nBelow is a table indicating the chance of being a carrier based on your ethnicity and the chance of being a carrier following a negative carrier screening result.\nIf you are a carrier, then genetic counseling and carrier screening can be offered to your partner. If both you and your partner are carriers, then there is a 25% chance your child will have SMA, a 50% chance your child will be a carrier of SMA, and a 25% chance that your child will not have or carry SMA.\nHaving carrier screening is a personal choice that is often influenced by beliefs, family dynamics, and personal preference. Some people opt for screening because they want to be prepared in case their future child has SMA or want reassurance that their child will not likely have SMA. Others do not want screening because the information will not help them prepare and may cause anxiety.\nSometimes if both parents are carriers, couples may choose different reproductive options. There is no universally right decision, just the decision that is right for you and your family.\nMaria Keever is a Certified Genetic Counselor with WakeMed Physician Practices Maternal Fetal Medicine.\n3000 New Bern Ave.\nRaleigh, NC 27610', 'Weak respiratory muscles can make it difficult for people with spinal muscular atrophy (SMA) to cough and clear mucus from their lungs, which puts them at higher risk for severe respiratory infections and complications.\nA pulmonologist or respiratory care therapist can train people with SMA or their carers to perform airway clearance techniques (ACTS) to help strengthen coughing and loosen mucus. Many of these therapies can be done at home either manually or with a portable device.\nBreathing and respiratory issues are the most common cause of death in children with type 1 or 2 SMA. For this reason, a pediatric pulmonologist should be part of your multidisciplinary SMA care team, consulted as soon as possible after diagnosis.\nWhat does it involve?\nRegular respiratory function tests are required for people with SMA-related breathing problems. People with SMA type 1 and 2 are typically evaluated every three to six months; those with type 3 may be seen less often.\nDuring these visits, and specifically when an individual is sick, pulmonologists may recommend one or a combination of manual and mechanical therapies to keep airways clear.\nCoughing assistance aims to create a stronger cough, so mucus can be expelled.\nCough assistance therapies may include:\nMechanical cough-assist machine. Also known as an insufflator-exsufflator machine, this non-invasive device blows air into the lungs, then sucks it out quickly. The goal is to mimic a strong coughing effect that increases the amount of mucus expelled. The machine is marketed under various brand names including CoughAssist.\nManual cough assistance. This hands-on technique involves placing gentle upward pressure on a person’s abdomen and diaphragm as they cough. It helps move the diaphragm up and create stronger exhalation and coughing. Mucus that’s released can be suctioned out of the mouth or swallowed.\nMucus mobilization is designed to loosen and expel stuck sputum. Mucus clearance therapies include:\nChest physiotherapy. Also known as manual or mechanical percussion, this technique involves gently clapping on the chest wall (or using a vibrating device) to loosen mucus. Some people with SMA may need it every day, others only when they’re sick.\nPostural drainage. This therapy involves placing a child on an incline with pillows or wedges, head lowered, to encourage mucus to move upward. Gravity pulls secretions from lower airways into the upper airways, where they can be removed by suctioning or coughing.\nIntrapulmonary percussive ventilation (IPV). This device, marketed under the brand name Percussionaire, pushes small bursts of air and saline mist into the lungs at a set frequency. The continuous flow of air loosens trapped mucus making it easier to cough or suction out.\nHigh frequency chest-wall oscillation (HFCWO). Marketed under various brand names, including InCourage, an HFCWO device is a wearable vest that vibrates to help move out mucus.\nYour doctor will discuss how to use respiratory therapies at home, as well as how often they should be used. During an illness, it may be necessary to use respiratory therapies more often.\nThe goals of non-invasive ACTs are promoting airway clearance and removing secretions to ease breathing and reduce risk of infections, pneumonia, and other complications.\nA 2018 review of the existing evidence for ACTs for people with neuromuscular disorders suggested that these therapies have reduced frequency of infections, pneumonia, and hospitalizations in people with SMA.\nThe use of ACTs is not advisable in people with uncontrolled asthma or bronchospasm, rib fractures, pregnancy, history of lung disease, cervical spine injury, unstable chest pain, or head and neck injury.\nThe American Association for Respiratory Care (AARC) warns against using vest therapy in the presence of reflux from tube feeding or a recent meal, existing pulmonary disease or surgery, pulmonary edema or embolism, uncontrolled hypertension, or complaint of chest wall pain.\nRisks of mechanical cough or airway clearance machines include pneumothorax (air leak in the lungs), worsening reflux, and food entering the airway. Manual cough therapy may be ineffective in people with severe scoliosis.\nThe cost and availability of respiratory experts in primary care, the geographical location of people needing assistance, and care coordination may lead to poor care quality. Some devices are considered experimental and not covered by insurance.\nFor more details, visit:\n1. Breathing Basics - CureSMA\n2. Spinal Muscular Atrophy Medical Management – Muscular Dystrophy Association\n3. Airway clearance techniques in neuromuscular disorders: A state of the art review ー Respiratory Medicine\n4. The Vest®️ Airway Clearance System 一 Hill-Rom\n5. How to use a mechanical insufflator-exsufflator ー Breathe']	['<urn:uuid:3f05411b-53df-4463-b98c-a57d2125d321>', '<urn:uuid:eea485aa-357a-445c-aec3-b8ecd7397581>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	9	99	1267
37	community engagement funding aspects describe how fips secure financial support and fishing communities implement safety programs	FIPs can secure financial support through seafood buyers who can fund various activities such as performance assessments, three-year environmental audits, and annual social risk assessments. Companies can also support implementation of action plans to address challenges. The FIP Community of Practice, with over 300 members from 20+ countries, helps coordinate funding and research efforts. For safety programs in fishing communities, implementation approaches vary - some interventions are cost-effective and can be implemented without external support, while others require significant investment from individuals (like purchasing lifejackets) or infrastructure development. To enhance adoption of drowning prevention strategies, it's critical to involve fishing communities from the earliest stages of project development, as they may be resistant due to time requirements and potential impacts on income generation.	"[""FisheryProgress.org shifts focus from gathering information to driving impact on the water\nBuilding on its efforts to support the seafood industry by providing a comprehensive fishery improvement project (FIP) reporting website, FisheryProgress.org is now aiming to increase the impact of FIPs by aligning buyers with high-performing FIPs.\nLaunched in 2016 by FishChoice, FisheryProgress.org originally had the goal of making it easier for seafood buyers and NGO advisors to track FIP progress by providing consistent, comparable information on FIP implementation. Before FisheryProgress.org, FIP reporting had no central aggregating site, and what reporting was done included information that was not consistent or verified as compliant with the Conservation Alliance Guidelines for Supporting Fishery Improvement Projects.\nThe FisheryProgress.org website has been successful in gathering FIPs’ data in one place, with more than 90 percent of global FIPs reporting on the website and close to 4,000 seafood buyers, NGOs, funders, and government representatives relying on the information provided. In 2020, CEA Consulting’s Global Landscape Review of FIPs confirmed the success of the effort, finding FisheryProgess.org had increased the consistency and availability of FIP environmental progress data.\n“With 96 percent of global FIPs reporting on FisheryProgress.org, and with thousands of buyers using the site as a resource for sourcing decisions, the next big question for us was how we can collectively support FIPs to make more progress,” FishChoice Program Director Kristin Sherwood said.\nWhile FisheryProgress.org succeeded in delivering more-valuable, consistent information to the industry, the CEA report found that seafood buyers utilizing FisheryProgress.org were not differentiating high-performing FIPs from less-impactful FIPs – and therefore it was not driving the marketplace toward the top performers.\n“Delivering environmental and social improvements is often a challenging, long road for FIPs. To make it easier, seafood buyers need to play a stronger role in incentivizing FIP progress, by sharing their performance expectations, contributing financially, and engaging directly with FIPs to support their work,” Sherwood said.\nFisheryProgress.org is now working to move the marketplace toward projects that deliver the best return for industry’s investment, Sherwood said. To do this, it plans to connect FIPs to training opportunities through organizations like the Marine Stewardship Council (MSC), the FIP Community of Practice, and Worldwide Fund for Wildlife, along with new funding opportunities. The aim is also to meaningfully engage seafood buyers with FIPs to use buying power as rewards to encourage FIP progress.\n“The FIP Community of Practice connects the global community of FIPs to learn from each other and take action to increase FIP impact,” said Jesse Marsh, the principal of Scaling Blue and the global coordinator for the FIP Community of Practice. “The community has more than 300 members from more than 20 countries that are actively engaged in FIPs, including NGOs, seafood businesses, government officials, and fishing associations.”\nThe process of an FIP is long and complicated, and coordinating effort across regional workshops, webinars, communication platforms, and more can help them achieve goals through greater funding and research, Marsh said. Getting buyers engaged can help accelerate progress by increasing funding for the many activities it takes to work through an FIP.\n“Achieving environmental and social progress in a fishery isn’t easy. It starts with conducting a thorough assessment of the environmental and social issues that are present. Then the FIP must create and implement a work plan, which usually includes activities to change fishing or labor practices and how the government manages the fishery,” Marsh said.\nPart of FisheryProgress.org’s new initiative includes improving language translation on the site, so that FIPs can communicate progress in French, Spanish, Japanese, and Bahasa Indonesia, in addition to English. The site will also have new features highlighting the results FIPs are achieving, and showcase trends in progress across the site.\n“We hope this information empowers the entire sustainable seafood community to support FIPs to accelerate their progress over time,” Sherwood said.\nAccompanying this new approach to its work, FisheryProgress.org has developed a new guide for seafood buyers on how to help use their buying power to push for progress. The guide provides three major recommendations to communicate expectations to suppliers and how to engage directly with FIPS to encourage the progress. It also provides examples and templates to how a company can integrate these improvements.\nA company can begin by publicly committing to only sourcing from FIPs that are making demonstrable environmental and social progress. Integrating this into a corporate sustainability commitment, supplier expectation letters, public statements, or on independent platforms such as FisheryProgress.org, are starting points to get this change out there.\nDemonstration of progress may look different between FIPs, but sourcing from high-performing FIPs means it is either transitioning from basic to comprehensive, maintaining an A or B progress rating, or reporting evidence of environmental improvements. Social progress could mean compliance with social policies, conducting risk assessments and developing a work plan to address high or moderate risk, or reporting improvements to social indicator scores.\nThe review's second major recommendation was to communicate company expectations with suppliers. For example, companies can share with their suppliers that human rights and social responsibility should be as much of a priority as environmental improvements. They can ask suppliers to become active FIP participants. Companies can also encourage FIPs to regularly and accurately report on FisheryProgress.org to show progress over time. FIPs that are inactive should be encouraged to reactivate immediately, with the threat that they may face rejection from buyers if they don't regularly report progress.\nThe review's final recommendation is for companies to meaningfully engage with FIPs by financially supporting improvement activities and following the FIP on FisheryProgress.org to engage with the FIP directly. Companies can aid FIPs by funding performance assessments, such as three-year environmental audits or annual social risk assessments. The companies can then follow-up by funding implementation of action plans to address challenges.\nFollowing a FIP on the FisheryProgress.org platform will allow a company to receive a monthly digest to summarize progress and changes in the FIPs followed. An annual update allows a deeper look to help better understand if the FIP is progressing in a way that meets the company’s need and expectations or if engagement is needed to identify opportunities to support the FIP for faster improvements.\n“Ultimately we want FIPs making progress to be recognized and rewarded for their hard work,” Sherwood said.\nMark Eastham, the sustainable products lead for Ahold Delhaize USA, a Dutch multinational company that includes local supermarket brands such as Stop & Shop, Hannaford, Food Lion, and Giant Food, said he is dependent on the FisheryProgress.org site to learn about the fisheries he sources from. Eastham said he is looking forward to the changes and improvements to the website.\n“As retailers passionate about sustainable seafood sourcing, our companies welcome the opportunity to partner with others in the industry and the rest of the sustainable seafood community to make a lasting impact,” Eastham said. “FisheryProgress.org has been an important tool in helping our companies learn more about FIPs and engage with them in a meaningful way. We welcome FisheryProgress.org’ new focus on FIP impact and look forward to leveraging the new resources as we continue to advance our seafood sustainability efforts.”\nPhoto courtesy of FisheryProgress.org"", 'Individuals and families living and working in fishing communities are at high risk of drowning due to their increased exposure to water. Many fishers rely on the income made from their catch to support an entire household. This leads to their involvement in dangerous fishing practices, such fishing in bad weather or in unsafe waterbodies, using unsafe fishing vessels, not having appropriate safety equipment accessible, and allowing fishing vessels to be operated by untrained or fatigued staff. These high-risk practices are particularly prevalent in low-income countries and rural/remote areas, where water vessel and water safety regulations may not be effectively enforced.\nThe children of fishers who live in close proximity to water are also at high risk of drowning as they may be left unsupervised for extended periods of time while their carers are fishing. Therefore, drowning reduction programs must target both adults and children simultaneously in order to effectively prevent drowning in this context.\nReducing drowning among fishers\n- Educate individuals on the risks and hazards of water, enabling them to identify and avoid dangerous weather conditions and unsafe waterbodies\n- Encourage large commercial fishing boats adhere to safety regulations, protecting the occupants on board and ensuring smaller private boats are not endangered\n- Encourage boat occupants adhere to safety regulations such as wearing life jackets and not exceeding maximum occupancy\n- Provide fishers with CPR and rescue skills to enable them to act effectively in an emergency\n- Improve swimming skills and water safety skills so that individuals can stay afloat if they fall overboard or are swept off the shore\n- Ensure hazardous and shallow waterways are clearly marked Avoid consumption of alcohol and other drugs when fishing or on the water\nReducing drowning in children and infants\n- Provide good adult supervision around the home and in day care. Carer education may be required to establish the elements of good supervision.\n- Reduce exposure to water through the use of barriers\n- Educate older children on the hazards of water, water rescue and CPR\n- Improve swimming survival skills so that children can remain afloat should they enter the water\n- Remove potential hazards such as nets and lines near the water’s edge\n- There are a number of approaches for preventing drowning developed for the general population which could be effectively implemented in fishing communities.\n- Many are cost-effective and simple.\n- Many can to be implemented within communities without external resources or support.\n- Some interventions are costly and these costs may be borne by individuals, such as purchasing lifejackets or refurbishing fishing vessels.\n- Some interventions are complex and require significant infrastructure to run, such as enforcing commercial boating regulations.\n- Fishers may be resistant due to associated time requirements and changes to the way they work.\n- May impact on how effectively fishers are able to generate income and support their families.\nDue to their heightened water exposure, it may be appropriate to first identify and target fishing communities when implementing drowning reduction strategies in an area. Be prepared to think and work towards creative solutions as the context of fishing communities is different to that of the general population. To enhance the likelihood of the adoption of potential drowning prevention strategies, it is critical to involve the fishing community from the earliest stages of project development.\nHigh risk practices among fisherman (Greece): Frantzeskou, Elpida, et al. ""Risk factors for fishermen’s health and safety in Greece."" International maritime health 63.3 (2012): 155-161.\nFishermen’s perceptions of risk (PDF 351KB)\nMeasuring and improving safety for recreational boating: Amanpreet Virk and Terri Pikora, Developing a tool to measure safe recreational boating practice. Accident Analysis and Prevention 43(1):447-500 2001\nSet objectives and interventions']"	['<urn:uuid:c5654e85-f96d-4345-9fd8-5d616e7998b4>', '<urn:uuid:5290588e-d569-47ac-b566-b5e1cb615d22>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	16	123	1808
38	best practices managing finances environmental footprint	For managing finances, asset allocation is key - combining stocks, bonds, and cash to smooth out market volatility while pursuing long-term goals. This should be based on your life stage and time horizon rather than short-term market conditions. To minimize environmental footprint, several practices are recommended: eating a variety of foods to create demand for diverse landscapes, increasing plant-based food consumption, reducing food waste through better meal planning and shopping habits, moderating meat consumption, eating less processed foods (which are resource-intensive to produce), and reviewing shopping habits by buying local, seasonal produce and checking food labels for country of origin. Regular investment strategies combined with sustainable consumption habits can help achieve both financial security and environmental responsibility.	"[""There are two basic strategies that can help you reduce the risk in your college funding portfolio: diversification and dollar-cost averaging. These two strategies can be accomplished simply through a regular saving plan—you have the opportunity to invest in different investment funds (diversification), and you contribute money gradually (dollar-cost averaging).\nDiversification means don't put all your eggs in one basket. Investment experts say it is suitable to invest in a number of different vehicles. You want to have your money in many different kinds of investments, so that if anything happens to any one of them, it won't be a disaster. Diversification helps minimize the risk in your investments.\nAsset allocation is a method of diversifying your investments to help you work towards the highest rate of return for the amount of risk you are willing to accept. Since no one can predict what will happen tomorrow, you need a way of investing your money to pursue your long-term goals. Rather than focusing on current market conditions and the short-term outlook, you need a strategy that is based on what stage of life you are at and how many years you are from starting to require the funds.\nAsset allocation is based on two simple concepts. First, different asset classes—stocks, bonds, and cash—react differently under the same economic conditions. For example, stocks tend to do well when inflation is low and interest rates are dropping. Bonds tend to perform well when the economy is slowing down and the price of stocks may be falling. And cash, while not a growth asset, can act as an anchor when both stocks and bonds are performing poorly.\nSecond, certain asset classes perform better over time than others. As you know by now, stocks have historically outperformed over the long-term, but have the highest degree of short-term volatility. Bonds with short-term maturities typically show less price volatility to changing interest rates than bonds with longer maturities.\nHere's the basic point: By selecting a combination of stock and bonds, you smooth out the short-term volatility—the ups and downs of the markets—while seeking to achieve your long-term goals. Since each asset class has historically achieved certain rates of return over certain periods of time, we can assume certain combinations—ratios of stocks, bonds and cash—will produce certain long-term results based on past performance, although actual results will vary and cannot be guaranteed.\nIMPORTANT NOTE: Although Asset Allocation Strategies seek to minimize short term volatility, they do not ensure against loss or protect against loss in a declining market. Past performance is no guarantee of future results. There is no guarantee that a diversified portfolio will enhance overall returns or outperform a non-diversified portfolio. Diversification does not protect against market risk.\nThe strategy of systematically investing a fixed dollar amount over time is called dollar-cost averaging. For example, if you have $1,200 to invest in your college funding program this year, you would invest $100 per month instead of investing the whole $1,200 at the beginning of the year. The benefit of dollar-cost averaging is that it reduces the risk of buying shares at the wrong time—when stock prices are high. The theory is that you buy fewer shares when the price per share is higher and more shares when the price per share is lower. Dollar-cost averaging can work in your favor because the average price per share that you end up purchasing may be lower. This is the principle used in investing in company savings plans through payroll deductions.\nIMPORTANT NOTE: Regular investing does not guarantee a profit or protect against loss in a declining market. This plan involves continuous investments in securities regardless of fluctuating price levels and you should consider your ability to continue through periods of low price levels."", 'What is a sustainable diet?\nBy Linia Patel\nThe food choices you make every day have a profound impact on your own health, but also the health of the environment. Each time you decide to purchase and consume a food, you are making a choice about the types of foods you want on the supermarket shelves, served to you in restaurants and ultimately a choice about the world’s food system. So that means YOU as a food consumer play the most important role in food sustainability.\nSetting the scene\n“Sustainable nutrition”, “eating green”, “eating for the planet”... These buzz phrases are being thrown about a lot now-a-days. What do they actually mean? And what can you do to eat a diet that is healthy for you and healthy for the planet?\nThere is no legal “sustainable food” definition although some aspects, such as the terms “organic” and “Fairtrade” are well known and clearly defined. In terms of foods having environmental impact we know that the main culprits are livestock, which account for half of all green-house gases. Shifting Western diets to include less processed food and meat, and more fruits and vegetables would be of benefit for us and the environment. However, it’s not quite as simple as getting all pedantic about particular foods. Genetically modified seeds, pesticide use, following faddy food trends and food waste all matter too.\nGenetically modified (GM) foods for example are produced from plants which have had their genetic make-up tweaked in the laboratory. This is done to increase the yield of a crop or allow the plant to exist in a more hostile environment than normal (e.g. extreme weather). GM foods are highly controversial and the extent to which they can help or harm humans and the environment is widely debated. Food waste also has a massive carbon footprint. On average in the UK we throw away 20% of the fruit and vegetables we buy. Last year alone, we threw away £ 13 billion worth of food. Research suggests that if we reduced our food waste by just half in the UK we would reduce out food imports and growing by 30 – 40%.\nWhat is a sustainable diet?\nFood sustainability therefore encompasses all aspects of production as well consumption. “Green eating” is not exclusive to food groups – but more about adopting a sustainable (pun-intended) way of living. One person’s definition of food sustainability may mean reducing food waste, being more mindful of shopping habits (i.e. eating in season and local produce). Yet, another may be going vegan and eating only organic. While these food values are very different, each has defined a lifestyle that works for them that has a positive impact on the food system and therefore the environment.\nPractical sustainable diet tips\nFollow these simple, straightforward tips to eat better for your health and the environment.\nEAT A VARIETY OF FOOD. Meat eater or vegan a diverse diet is key. Nutritionally, mixing it up means you get more nutrients and keep it interesting and environmentally you create a demand for a more diverse landscape.\nTAP INTO PLANT POWER. The number one thing you can do to be a more sustainable eater is to put more plants on your plate. Bonus is that is helps your health too. Make sure you try and eat in season and eat locally grown produce instead of fruits and vegetables that are flown across the world.\nWASTE LESS. Have leftovers? Challenge yourself and make sure you find a way of using them up (soups or stews). Try and plan your shopping better so that you don’t have to throw away any food. Plan meals to reduce waste, freeze what you don’t use, and buy only what you need.\nMODERATE YOUR MEAT CONSUMPTION. Adopting a sustainable diet doesn’t mean you have to become a vegetarian or vegan, it just means that you need to be mindful of your animal food consumption. Focus on the source and quality of the animal foods you consume also. Start including more plant protein like beans and lentils in your diet. Consider having a meat free meal or a meat free day each week.\nEAT LESS JUNK. Very processed foods tend to be more resource intensive to produce and often contain high levels of sugar, fat and salt which if eaten too much can cause weight gain and increase your risk of obesity. Keeping your waistline in-check benefits both your health and the environment.\nREVIEW YOUR SHOPPING HABIT. Be mindful of your food choices; buy just enough, not too much. The largest contributor to food waste is during production, however as a consumer you control how much food your household wastes. Support local farmers and by buying in-season, local produce. Check out food labels carefully and look for the country of origin (think “food miles”) fish certification (e.g. tuna) and “Fairtrade” (e.g. coffee, chocolate), “GM” and also always be mindful about the levels of fat, sugar and salt.\nThe organisation ‘Sustain’ produces practical advice on eating sustainably, including seven guiding principles; https://www.sustainweb.org/pdf/SFG_Consumers.pdf\nCheck out the British Dietetic Association (BDA) Sustainable Diet policy and the Plant-based diet Food Fact Sheet at https://www.bda.uk.com/ foodfacts/plant-based_diet\nReferences available on request.']"	['<urn:uuid:11fcfc44-818d-45d3-8d07-738b6173049f>', '<urn:uuid:690852d9-db1e-447f-84ee-b00986beb9dd>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	6	117	1491
39	What are some of the different approaches that can be used when working out handbell choreography for a musical passage?	There are several approaches: playing with single bells alternating hands, changing the starting hand, weaving, using traveling four-in-hand, displacing or presetting bells, swapping adjacent bells, passing bells between hands, playing consecutive bells paired in the same hand, using shelley plus or minus, and malleting bells on the table.	['Doing exercises will help you learn solo handbell choreography (sometimes called “blocking.”) That, in turn, will free you to explore works not specifically published for solo handbells. You can create your own exercises by writing out a 1 octave scale, both ascending and descending, in any key. Try several different approaches to choreographing the scale, document it with solo notation, then try another key. You can also do this with a phrase of any handbell solo, or any musical line within your bell range.\nHere are some solo-friendly scales to get you started:\nHere are some passages from actual solo bell pieces:\nPermission is granted to print these exercises (and the instructions below) for your personal use.\nHow to use these exercises:\nThere are no right or wrong answers – there are only comfortable and awkward choreography solutions. Choreography is nearly always a trial and error process, even for experienced soloists. Your mission, should you choose to accept it, is to choreograph each exercise and document it with standard solo notation. Try at least three different approaches to each exercise. (The exercise is repeated on the sheet for this purpose.) Don’t expect every approach to work well. Some won’t work at all, and you may have to combine approaches to obtain an elegant and musical solution. It isn’t necessary to play at tempo when working out choreography, but your preferred solution should be playable at tempo, if you were to practice it repeatedly. With experience, you’ll learn to recognize the difference between choreography that smoothes out with practice, and choreography that is inherently unworkable.\n• Play the entire passage with single bells, alternating hands.\n• Change the starting hand.\n• Weave. Consider displacing or reversing bells as you table damp on the way up the scale and replacing them on the way down.\n• Look for constants and use traveling four-in-hand. Try different constants.\n• Displace or preset bells. Choose different bells to displace.\n• Swap two adjacent bells.\n• Pass bells between hands. Identify other points to pass a bell, freeing up the correct hand when needed. Choose the least disruptive pass (preferably on a long or repeated note).\n• Play consecutive bells paired in the same hand. This is especially useful on short note values (8th or 16th notes), instead of changing bells.\n• Use shelley plus or minus.\n• Mallet bells on the table. Try different stickings (i.e., change the mallet hand you use to strike certain bells).\nYou can also watch for places to use skills you may not have learned yet, like six-in-hand or bell trees. Identify particular aspects of the passage that make that technique appropriate.\nExample 1: A passage that sounds good LV (determined by playing it on the piano, with pedal) might work well on bell trees.\nExample 2: A passage using only 6 small bells might be played six-in-hand.\nIn an actual piece, the six-in-hand or bell tree section will work only if the bells are available, either as duplicates, or as bells not needed elsewhere in the piece, or where you have sufficient time to set the bells up for the technique during a piano interlude. Still, it’s worth considering these possibilities, so you get used to spotting opportunities to apply these techniques once you’ve mastered them.\nDoing all the exercises will take several weeks. I suggest doing only one at a time, in the order presented, taking as much time as you need to complete it. In the process, you’ll:\n• Start to recognize situations where you might apply certain techniques\n• Learn which approaches you instinctively favor\n• Identify techniques you want to spend more time practicing\n• Master solo notation\nCopyright © 2013 Nancy Kirkner, handbells.com']	['<urn:uuid:ffc7238c-f13f-4c9b-a02a-b6cbc1c89965>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	20	48	619
40	what is purpose of video art installations and how do they affect viewers experience	Video art installations aim to create immersive experiences that defy traditional art commoditization. They often experiment with space by projecting onto architectural or sculptural surfaces, as seen in works like Bill Viola's nightmarish wall projections and Pipilotti Rist's installation that wrapped around 25-foot-tall museum walls. These installations serve to make art more accessible and provide an immersive experience that engages viewers in dialogues about contemporary life, allowing them to gradually unfold over time and challenging how we see ourselves and others.	"['Video art can be a difficult medium to grasp and differentiate. It may be confused with art films, as both genres involve moving pictures with artistic endeavors. Video art explores the possibilities of the medium, while film does not.\nEven to the most seasoned connoisseur, video art can feel inaccessible as it sometimes melds and works hand in hand with other mediums. On top of that, ownership complexities around replicable files had once contributed to art collectors’ reservations in the early years of the medium. But in an ever more digitally savvy world, is video art reaching its zenith?\nIn this article we will delve into what video art is, its history, and evolution. We will examine where the future of this medium is heading in a culture heavily entrenched in moving pictures and digital artworks.\nWhat is video art?\nVideo art uses audio-visual technology to generate and manipulate moving pictures to create artwork. It can weave in other art forms such as dance, performance art, music, architecture, and sculpture.\nOver the last few decades, the fluid nature of video art and an increased demand for digital art by institutions has attracted artists from many disciplines. Experimental filmmakers, choreographers, performance artists, photographers, sound and conceptual artists have gravitated to the medium.\nHow Does Video Art Differ from Film?\nBoth film and video art use experimental techniques. Being more reliant on narrative, film uses moving pictures to tell a story. Video art, on the other hand, uses moving pictures for pure artistic expression.\nMost cinema, including the art-house variety to some extent, has commercial interests. It is reliant upon financial backers that all have a “say”. This can influence the finished product and impact a director’s artistic vision. Meanwhile, video art is traditionally not reliant on narrative and the specter of commercial appeal is not as present. This allows for an unfettered artistic vision to evolve.\nThe History and Evolution of Video Art\nVideo art started in the 1960’s, emerging from the Fluxus movement which consisted of a network of international avant-garde artists and composers. They shared the common goal of creating “anti-elitist” and “living art,” as outlined in their manifesto penned by the movement’s founder, George Maciunas.\nThe introduction of Sony’s portapack video camera in 1965 made the video production affordable. The camera was easy to use and removed the need for a film crew. Inspired by the art films of the Surrealists, Dadaists, and Italian Futurists, artists began to use video as a means for artistic expression.\nOne of the earliest examples of prominent video art is Bruce Nauman’s Art Make-Up (1968). In the video, the artist applies different colored paints onto his torso with the idea that he is making himself up for the viewer, touching on themes of disguise, masquerade, and reality. His video installation showed footage of all four different paint applications separately on four walls of a square room.\nIn the wake of the birth and dispersion of television, early video artists explored its impact on society. Nam June Paik, considered the “father of video art,” is well-known for such themes. His work entitled TV Garden (1974-77), is an installation piece containing forty television sets intermingled with plants. In this piece, Paik looks at the tension between nature and technology as humans increasingly create and reside in digital worlds.\nVideo art was embraced by performance artists using accessible video technology to create records of their work. Famed performance artists Marina Abramovic and Ulay used video technology to record their performances, such as Light/Dark (1977).\nWhile some artists experimented with new editing tools to create flashy imagery, which also changed the music video industry with the emergence of MTV, others used the medium to present social commentary. Howardena Pindell’s epic video Free, White and 21 (1980) is a pioneering example of activist video art that entails the artist describing racial anecdotes and scattering footage of symbolic performance art by herself.\nIn the 1980’s and 1990’s innovations in technology created more opportunities for experimentation. Artists such as, Bill Viola, experimented with bolder installation pieces. His famed work, The Sleep of Reason (1988) engulfs viewers in nightmarish images that flash across gallery walls unexpectedly.\nAs with Bill Viola’s work, video artists often experiment with space. Works are projected onto architectural or sculptural surfaces, if not viewed through screens. In the 1990’s, Tony Oursler became known for creating footage of eerie faces with giant eyes and mouth specifically designed to be projected onto his nebulose sculptural pieces.\nAs the digital palate of society matured in the 2000s, galleries and collectors gravitated to video art. Several major museums such as the Tate have amassed impressive video art collections.\nDigital editing software became more accessible, allowing video artists to incorporate more sophisticated editing and special effects into their work. Ryan Trecartin’s A Family Finds Entertainment (2004), highlights this with a collage of pop and youth culture influences pasted together by frenetic editing techniques.\nOther artists have started to design immersive art experiences in the early 2000s with their videos enveloping art spaces. One of the first of its kind, Pipilotti Rist’s 2008 piece Pour Your Body Out (7354 Cubic Meters) wrapped around the 25 feet tall walls of MoMA’s atrium. The video displayed footage of ripe apples, vivid tulip fields and luscious strawberries among others that symbolize earthly pleasures.\nPipilotti Rist is also well-known for her conceptual video performances. Open My Glade (2000), which was displayed in Times Square billboards (between 2000-2017) showed the artist smudging her face against the screen as if she is trapped behind the glass and playing around.\nAnother video performer, who uses herself as the protagonist in her feminist videos is Kate Gilmore. The artist is known for creating and attempting physical obstacles in her video performances, representing women’s real-life struggles. Her piece Between a Hard Place (2008) is in the collection of MCA Chicago.\nNot all video performances demand a human main character. We also see video performances where the objects reveal the conceptual message. In Ash Ferlito and Matt Taber’s Breakfast in America (2012), the color spectrum is broken down by tableaux of everyday objects arranged according to their hues. The objective behind this fast paced, single shot video performance is to show the linguistic relationship between objects.\nVideo Art and the Interplay with Technological Advancement\nTechnology has undergone a meteoric expansion within modern society, enabling video art to blend with the disciplines of architecture, digital art, video games and virtual reality. Contemporary video art’s foray into virtual reality expands upon the immersive experiences created by installation pieces.\nMany video artists have embraced virtual reality as the technology improved from the clunky 90’s version. Multi-media artist Jacolby Satterwhite creates lush VR dreamscapes in his video art and music videos that carry on the visual language of video games.\nDuring the pandemic, video artists embraced social media to engage with audiences. But this has exposed issues with these platforms, raising questions around privacy, censorship, and the undue influence they have on our lives. Furthermore, with Meta (formerly Facebook), announcing their plan to immerse us in virtual worlds, we can predict that virtual reality video artwork is set to increase in demand.\nThe juxtaposition of the freedom and control created by the expansion of technology is a theme video art has explored repeatedly. The work Deep Contact (1984-89) by Lynn Hershman Leeson, examined this back in the 80’s, showing the somewhat prescient nature of the medium. Deep Contact is an interactive video/multi-media installation which examines how technology can manipulate behavior.\nHito Steyerl’s work, How Not to Be Seen: A Fucking Didactic Educational .MOV File (2013), examines the constant surveillance of civilians that has been facilitated by modern technology.\nVideo Art’s Future and the Brave New World of NFTs\nNFTs (Non-Fungible Tokens) have exploded in the art world. And with video art being a medium that has always flirted with technological advancements, many artists are exploring how to leverage this technology. With historic reservations from collectors regarding the replicability of video art, artists have been using NFTs to avoid ownership and authentication hurdles of the past. NFTs allow artists to provide collectors with more sophisticated ways to track original file ownership using blockchain technology.\nThematic Resonance of Video Art with Modern Times\nAs a society we are becoming more enmeshed with the digital world, including in our art. When you consider that video art has traversed these themes since the 60’s, its relevance, is clear. The uptake in video art sales and NFTs speak volumes. The more technology infiltrates our everyday lives, the more these artforms speak to us. We are more comfortable with electronic scapes and digital ownership than ever before.\nFrom its birth in the mid-60’s to the VR landscapes of today, video art has always exhibited a future focused lens. As a medium, it has openly experimented with and embraced innovative technologies, facilitating the invent of moving and immersive art. This willingness to enfold with modern technology, and open examination of how we interact with it, is what has allowed the medium to flourish over decades.\nVideo art’s exploration of the modern relationship between nature and technology is poignant. We are living it every day.', 'The term \'time-based media\' refers generally to moving images, audio, or video works that have duration and unfold gradually over time. Time-based media might not employ the use of actors, may contain no dialogue, may have no discernible narrative or plot, or adhere to any of the other conventions that generally define motion pictures as entertainment. On the other hand, some video art is inextricably linked to news or TV footage, operating through appropriation and pastiche to examine the role and power of popular media.\nThe moving image reflects our zeitgeist. Whether in films, television, or across the computer screen, moving images affect not only how we have come to experience time and space but also how we see ourselves and others. In 1965, with the introduction of the Sony Portapak, a portable video camera, the medium of video became an expressive tool for artists (particularly in the hands of nontraditionalists such as Nam June Paik) as well as ""the most relevant visual art form in contemporary life,"" according to Bill Viola, one of the progenitors of video art. The move toward video art during the sixties deconstructed the notion of art as a highbrow commodity, often indecipherable and accessible only to the wealthy. Rather, with video and other time-based media, art defies commoditization and can become an immersive experience. Video art is often in dialogue with the kinds of narratives that inform our everyday lives, as well as a potential tool for social critique and change. However, time-based media have also been important in furthering conceptual dialogues surrounding the nature of truth and reality, as the 2017 Dallas Museum of Art exhibition Truth: 24 frames per second explored.\nThe authenticity and truthfulness of lens-based media have long been open to contestation and debate. In 1878 Eadward Muybridge successfully captured the stride of a galloping horse. The results were widely publicized, and he spent the ensuing years improving his technique, photographing men and women performing everyday chores. Muybridge\'s intention was to faithfully capture human locomotion. However, critics were quick to point out that his images—though real—were somehow untrustworthy. The photographs documented something the human eye was unable to see. The question then arose, which is truer, the image depicting things as they are, or the way we believe them to happen? This is a recurring question of time-based media. Artists\' manipulation of our innate distrust of recorded video and audio (despite its seeming veracity) awakens a certain questioning in us of what is real and what is unreal, what is true and what is false.\nAlthough the Dallas Museum of Art did not begin collecting video in its incubation as an artistic form, it has in only a few years (beginning in 1998) created a time-based media collection that is both critically significant and popular with visitors. It is a rich, varied collection, including single-channel videos and films that can be played on a monitor or projected, as well as video-sound installations that demand one or more galleries. The ideas, strategies, and practices explored by the different artists are challenging and thought-provoking.\nChloë Courtney, Digital Collections Content Coordinator, 2018\nSuzanne Weaver, ""New Media at the Dallas Museum of Art,"" in Dallas Museum of Art, 100 Years , ed. Dorothy M. Kosinski (Dallas: Dallas Museum of Art, 2003), Pamphlet number 79.\nGavin Delahunty, ""Reality within a frame,"" in Truth: 24 frames per second, ed. Gavin Delahunty and Kelly Filreis (Dallas: Dallas Museum of Art, 2017), 10-19.\nExcerpt from ""Contemporary Art Docent Guide."" File on TAZ.\n""Time-based Media,"" Conservation, The Guggenheim Museum, accessed June 7, 2018, https://www.guggenheim.org/conservation/time-based-media.\n""About Contemporary Art,"" Who\'s Afraid of Contemporary Art?, The J. Paul Getty Museum, accessed June 7, 2018, http://www.getty.edu/education/teachers/classroom_resources/curricula/contemporary_art/background1.html.']"	['<urn:uuid:0297a162-99b8-4a16-9d9a-1345577a2da3>', '<urn:uuid:86abf045-890e-4a27-a303-9e6484309166>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	14	81	2144
41	As an aviation historian studying military applications, I'm curious about how the Constellation aircraft was used during the Vietnam War - what were its specific roles and capabilities?	During the Vietnam War, Constellation aircraft (known as Connies) were primarily used for airborne command and control and early warning missions. They were flown in elliptical orbit near enemy territory to collect and transmit information on air activity. The Constellation was also notable for being the first aircraft to carry rotating radomes - saucer-shaped domes used to protect radar antennas. This radar technology pioneered by the Constellation is still in use today with modern aircraft controlling the skies over the Middle East and with US Customs and Border Protection P-3s conducting drug interception missions in the Caribbean.	['How the Constellation Became the Star of the Skies\nIn 1939, the top brass of the Lockheed Corporation—president Robert Gross, chief engineer Hall Hibbard, and chief research engineer Kelly Johnson—scheduled a key meeting with a VIP, a man with deep pockets who had recently shown an interest in buying not just one or a handful of new planes but a fleet of them.\nThe customer’s request had been ambitious. He hoped to hire Lockheed to design a revolutionary aircraft capable of comfortably shuttling 20 passengers and 6,000 pounds of cargo across the United States, offering commercial aviation’s first coast-to-coast, non-stop service.\nBut the Lockheed team had come to express even grander ambitions. They wanted to build the company’s first large transport, one that “would carry more people farther and faster than ever before, and economically enough to broaden the acceptance of flying as an alternative to train, ship and automobile,” said Johnson.\nIn the years to come, the plane would be named the Constellation—Connie for short—and be flown by airlines around the world, as well as the U.S. military over the ensuing three decades. Eventually, it would be remembered as an enduring symbol, the epitome of grace in propeller-driven aircraft. But at that moment in 1939 in Los Angeles, the Lockheed Corporation was focused on winning over one customer and one customer only. His name was Howard Hughes.\nThe Secret Weapon\nHaving purchased a majority stake in TWA airlines earlier that year, Hughes saw the Constellation as his secret weapon in stealing market share from his competitors. He treated the project with all the subterfuge that secret weapons require. Not only did he demand total secrecy, but also specified that Lockheed could not sell the aircraft to any other transcontinental airline until TWA had received 35 of them.\nHughes outlined the initial performance specifications, but it was Lockheed that would design the sleek, distinctive, now-iconic aircraft. It was a critical turning point for Lockheed. As Hibbard said, “Up to that time we were sort of ‘small-time guys,’ but when we got to the Constellation we had to be ‘big time guys’ … We had to be right and we had to be good.”\nBeing good meant introducing new features previously unseen on passenger planes. The Constellation would offer the first hydraulically boosted power controls, aviation’s equivalent of power steering. It would be faster than most World War II fighters at 350 mph. And, using award-winning technology pioneered by Lockheed a few years earlier, it would feature a pressurized cabin for 44 passengers that allowed the plane to fly faster and above 90 percent of weather disturbances, what Constellation regulars would come to call smooth sailing.\nA Record Breaker\nIn fact, Lockheed’s design was so good, the U.S. military, readying for war, saw its potential as a transport for troops and supplies in Europe and took over production in 1942.\nThe first official flight test for a Constellation, sheathed in olive green paint and redesignated C-69, came early the next year. It was a plane equally beautiful in form as well as function. First flight went so well that five more flights were performed the first day. Hughes went about publicizing the Constellation the best way he knew how: by breaking a transcontinental speed record on a Burbank to Washington, D.C., flight in April 1944. The Connie averaged 331 mph, flying nonstop in six hours, 57 minutes, and 51 seconds on this flight. After setting the record, that aircraft was returned to the military and during service testing at Wright Field, Ohio, Orville Wright, who had made the first powered flight, made his last flight, serving as copilot on a test run.\nPushing the Limits\nWhen the war ended, TWA bought back all the C-69s it could from the government; conversions were made and the Constellation entered commercial service in February 1946.\nAs the Connie was designed to change the face of commercial flight—it was as experimental, in some ways, as the early commercial aircraft of the late 1920s—there were issues during its infancy. Lockheed was flexing the limits of piston-engine technology, and the engine’s complex design required maintenance, and sometimes replacement parts, at rates that would be considered unacceptable with the forthcoming introduction of jet airplanes.\nBy 1951, the much-beloved Model 1049 Super Constellation was unveiled, boasting unheard-of refinements, such as air conditioning, reclining seats, and extra lavatories. It was a plane ahead of its time, at least twice as fuel efficient as the industry’s first jets and as efficient as many of today’s modern aircraft.\nA Return to War\nWhile only 13 Constellations were built during World War II—Lockheed would be asked, instead, to focus on the P-38—the Army, Air Force, and Navy had recognized the plane’s versatility. By 1948, the Navy was calling in orders for Connies to act as long-range patrol aircraft , nicknamed Po-Boys from the PO-1 designation then in use. In time, Constellations would be used for everything from rescue missions and VIP transports to airborne early warning missions and the mapping of the earth’s magnetic field.\nIts area of distinction, however, was clearly airborne command and control and early warning. During the Vietnam War, Connies were flown in elliptical orbit near enemy territory to collect and transmit information on air activity. Constellations were also the first planes to carry rotating radomes, saucer-shaped domes used to protect radar antennas, a technology that is still in use with modern aircraft controlling the skies over the Middle East and with US Customs and Border Protection P-3s running drug interception missions in the Caribbean today.\nPresident Eisenhower was a big fan of the Connie, and his personal presidential plane, the only VC-121E built, was the first to bear the now-recognized moniker “Air Force One” when the president was onboard. All told, the U.S. military bought nearly 40 percent of all the Constellations ever manufactured, using them over nearly three decades, with aircraft serving well into the 1970s.\nA New Age Beckons\nSources and Additional Reading\n- Air Mobility Command Museum. “C-121 Constellation.” http://amcmuseum.org/exhibits_and_planes/c-121.php, accessed May 17, 2012.\n- Barlett, Donald L. and James B. Steele. Howard Hughes: His Life & Madness. New York: W.W. Norton & Company, 2004.\n- Boyne, Walter. Beyond the Horizons: The Lockheed Story. New York: St. Martin’s Griffin, 1999.\n- “Howard Hughes.” Chasing the Sun. PBS. http://www.pbs.org/kcet/chasingthesun/innovators/hhughes.html, accessed May 17, 2012.\n- Johnson, Clarence L. Kelly: More than My Share of It All. Washington, D.C.: Smithsonian Books, 1985.\n- “Today’s Mark of Distinction” (advertisement), Life. May 31, 1954.\n- Transport & Environment. “Report Shows Fifty-Year Failure of Aviation Industry to Improve Fuel Efficiency.” http://www.transportenvironment.org/press/report-shows-fifty-year-failure-aviation-industry-improve-fuel-efficiency, accessed May 17, 2012.\n- U.S. Centennial of Flight Commission. “The Hughes Companies.” http://www.centennialofflight.gov/essay/Aerospace/Hughes/Aero44.htm, accessed May 17, 2012.\n- Wilkinson, Stephan. “The Legendary Lockheed Constellation.” Aviation History. http://www.historynet.com/the-legendary-lockheed-constellation.htm, accessed May 17, 2012.\n- Yenne, Bill. Classic American Airliners. St. Paul, MN: Zenith Press, 2005.']	['<urn:uuid:550d6e44-3fca-4f8a-9564-fd31d7e21d04>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	28	97	1144
42	What did researchers discover about the original construction of the palace in Samaria when they excavated it?	During excavations in 1908-1910, the Harvard University archaeological expedition found that the palace was built directly on virgin rock, with no evidence of any earlier structures beneath its floor, except for a neolithic encampment. This discovery confirmed the biblical account that Omri built his capital on an previously unoccupied hill. While there were layers containing remains of later structures above the palace floor, the absence of earlier construction remains supported the historical record of the site being undeveloped before Omri's time.	['Jeroboam II and Osorkon II\nThe conventional timetable has Ahab, the king of the Northern Kingdom of Israel, as a contemporary of one of the kings of the Libyan Dynasty, usually Osorkon II. And almost regularly reference is made to archaeological evidence called to substantiate this synchronism; it is worded thus: Osorkon II. He was a contemporary of Ahab, for in his palace at Samaria an albaster vase bearing the name of Osorkon II was found.(1)\nIn the chapters VI to VIII of Ages in Chaos, dealing with the el-Amarna period, it is demonstrated that Ahab was a contemporary of the later kings of the Eighteenth Dynasty, Amenhotep III and IV (Akhnaton), and that over sixty-five letters on clay tablets addressed by him to these kings are still in existence, in originals, as written by the royal scribes, found in the ground of el-Amarna. Having been a contemporary of these pharaohs, the synchronization of Ahab with Osorkon II of the Libyan Dynasty cannot but be grounded in error. To expose the error of the quoted sentence, we have to check the records of the excavators.\nDuring the years 1908-1910 the Harvard University archaeological expedition at Sebastieh, ancient Samaria, uncovered the foundation of a palace. It was tentatively identified as the palace built by Omri and enlarged by Ahab.\nLike the unearthed portions of the city wall of Samaria, the palace was built on virgin rock. The biblical passage about Omri building his capital on an unoccupied hill was regarded as confirmed. The floor of the palace was covered with layers containing the remains of later structures; but no vestige of earlier structures was found under the floor, nor were any signs of settlement prior to the time of Omri, except for a neolithic encampment, unearthed on the site of Samaria.\nOn the floor of the palace numerous small Egyptian objects were found, among them scarabs (signets). The carvings on the scarabs are mostly decorative designs, but on one of them a cartouche, or royal name, was found engraved. The cartouche was that of Thutmose III. Since there was no plausible explanation for the presence of the cartouche of Thutmose III in the palace at Samaria, presumably built about six centuries after this pharaoh had died, the excavators suggested: This may be a local imitation of an Egyptian scarab. (2) As we have seen in the first volume of Ages in Chaos, Thutmose III reigned only a few decades before Omri; the cartouche apparently is genuine.\nA jar with the cartouches of Osorkon II was found near the palace of Samaria and it was brought forth as an evidence for the contemporaneity of Osorkon II and Ahab. Scores of ostraca were also found in Samaria. Ostraka, or potsherds inscribed with ink, were less expensive than burnt clay tablets or papyri; they were used when it was not expected that the writing would be preserved in an archive. Wine and oil when delivered were accompanied by these shards.\nThe ostraca of Samaria are inscribed with the names of persons or towns that delivered oil or wine to the kings palace; they are dated in the ninth year, in the tenth year, in the seventeenth year, of the king, but the name of the king is not mentioned.\nIn various books and articles it is asserted that the jar of Osorkon, contemporary of Ahab, was found in the same debris as the ostraca,(3) and it has been concluded that the ostraca of Samaria refer to the ruling years of Ahab. But is it true that these inscribed shards were found in the same debris as the Osorkon jar? And then, is it true that the ostraca of Samaria date from the reign of Ahab?\nThe report of the excavation gives the location precisely:\nThe southern wall of the Osorkon House [so-called because of Osorkons jar] was built in part over the foundations of the north wall of rooms 406, 407, and 408. The foundations of the assumed northern part of the Ostraca House must have been destroyed previous to the construction of the Osorkon House.(4)\nIt follows that Osorkons jar came to its location later than the ostraca came to theirs. This nullifies the argument that the jar must be of the same age as the ostraca. Thus even had the ostraca been inscribed during Ahabs reign, Osorkons jar found its place at a definitely later date. But of what age are the ostraca?\nThe archaeologists at first reasoned thus: Since Osorkon II is known to have been a contemporary of Omri and Ahab, and since Omri reigned but twelve years, and the ostraca mention the seventeenth year of the king, they must have been written in the days of Ahab. It follows that the ostraca of Samaria are about the same age as the Mesha stele of the middle of the ninth century.(5)\nA comparison of the Hebrew signs of the Samaritan ostraca with the Hebrew characters of the Mesha stele shows a definite change in the writing of single letters. The same characteristics found in the Samaritan letters reappear in the Shiloah inscription of King Hezekiah, dating from close to -700. How to explain that the characters of the ostraca, a quarter of a century older than the stele of Mesha, are more directly related to the later characters of the Shiloah inscription?(6) This compelled the researchers to advance the hypothesis that the Hebrew letters passed through a retrograde stage of development before resuming their progress, or that in Moab the development was slower than in Samaria.\nIn subsequent excavations at Samaria ivories with Hebrew letters were unearthed. These letters were found to be of the same type as those on the stele of Mesha and to have therefore originated in the ninth century. They are of a more archaic type than the characters of the ostraca of Samaria.(7)\nThe conclusion has now for some time been generally accepted that the Samaritan ostraca were written not in Ahabs time, but in the time of one of the last kings of Samaria. Of the kings of Israel after Ahab, only Jeroboam II and Pekah reigned for more than seventeen years. Th scholarly opinion arrived at an almost unanimous conclusion that the ostraca were written in the days of Jeroboam II (ca. -785 to -744).(8) This conclusion appears to be correct.\nThe house that sheltered the jar of Osorkon II in Samaria was built on the ruins of the house that sheltered the inscribed potsherds. Since the ostraca were written in the days of Jeroboam II, one of the last kings of Israel to reign in Samaria, every ground for making Pharaoh Osorkon II a contemporary of Ahab because of the findings in Samaria vanishes. Judged by these findings, Osorkon II was not only later than Ahab, but also later than Jeroboam II.']	['<urn:uuid:aefaf95c-d1bb-4ea2-9871-9322b4ec0788>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	17	81	1140
43	atc static testing compare defects found	ABAP Test Cockpit (ATC) and static testing find different types of defects. ATC detects quality issues related to performance, security, and programming conventions through static checks. It can identify issues like incompatible parameters in function calls, non-translated strings, and performance problems in database queries. Static testing, on the other hand, detects defects such as deviations from standards, missing requirements, design defects, non-maintainable code, and inconsistent interface specifications. Both approaches are valuable for improving code quality during the development process.	['Good news for ABAP Development Tools developers in Eclipse: ABAP Test Cockpit (ATC) is available in Eclipse with ADT 2.31. For those of you, who are not familiar with ABAP Test Cockpit, it is a toolset, which allows you to significantly improve the code quality during development process. With ATC you can detect the quality issues with regard to performance, security, programming conventions by regular executing static and dynamic (ABAP Unit) checks (of course you can reuse your Code Inspector backend check infrastructure). A very good general overview about ATC is offered in the blog ABAP Test Cockpit – an Introduction to SAP’s new ABAP Quality Assurance Tool .\nThe ATC in Eclipse is tightly integrated with ABAP developer tools so that you can run ATC checks during development from the Project Explorer or editor and adjust the quality findings consequently by stepping to the relevant source code lines in the editor and correcting them one by one. You can also check with ATC your transport requests in the Transport Organizer before releasing them. And last but not least you can keep the eye on the central check results from your Q system and correct the reported quality findings.\nThe live demonstration (video) of the ABAP Test Cockpit tools in Eclipse is now on ABAP Channel in YouTube:\nLet’s take a look at the brand new ATC tools in Eclipse in detail.\nCheck the quality of your source code\nIt’s quite simple. You can run ATC for a single object in the editor or select multiple objects or even packages using context menu\n‘Run As-> ABAP Test Cockpit’.\nThe tool of choice to analyse the quality issues is the ATC Problems View. It displays the worklist of ATC findings for you as developer to work on. By default the findings are grouped by priority: errors, warnings, information. Clicking on a finding displays its detailed information.The check variant is displayed in the breadcrumb (if required, change the variant in the project properties under ABAP Development).\nIn the breadcrumb you can filter the findings in the ATC Problems View and in this way restrict the view to the findings you need to focus on for correction. You can concentrate your correction work for example on the findings, belonging to the last check run (‘Last Check Run’ entry), package, transport request, or recheck of result from the central Q system. Every time you run ATC, the breadcrumb gets a new filter entry if required (e.g. if a new package is checked) and the findings will be added to your worklist.\nAs soon as you correct a finding, it disappears from your worklist after recheck.\nThere might be also such situations where a correction of the finding is not possible or not appropriate (e.g. the hard-corded text message, which is not relevant for translation). In such situations the ABAP Test Cockpit offers the possibility to create exemptions for the affected source code parts so that the findings will be hidden from the results. But you can still display them by using the display option ‚Include Exempted Findings’.\nCreate the exemption by right-clicking on the finding. You need to enter your quality expert (use content assist Ctrl + Space), reason for exemption and justification. As soon as your quality expert approves it, the finding will be marked accordingly in ATC results and will not appear in your worklist.\nYou can also customize the view on the findings in ATC Problems View and group the findings by priority, objects or checks for your convenience or configure the columns.\nIdeally you should run ATC and correct the findings in the ATC Problems View on a regular base during your development process.\nCheck your transports before release\nIt is recommended to check regular with ATC your transport requests before release to catch erroneous situations and avoid transporting the source code with quality problems into quality and production systems. This can be done in the Transport Organizer View in Eclipse. There you can choose your transport request and run ATC using context menu ‘Run As->ABAP Test Cockpit’.\nAfter execution of ATC checks you will be redirected to the ATC Problems View, where you can focus on the findings, reported for this transport request and get rid of all of them one by one (btw. the new filter entry for the selected transport request will be added to the breadcrumb).\nAccess ATC results from the central Q system\nAfter your quality expert runs mass (regression) quality checks with the ATC, the results of these checks will be published to the development systems as the current, active ATC result. You as developer can use this central result to eliminate quality problems. You can access the ATC results from the central Q system in the ATC Result Browser (menu ‘Window ->Show view->Other…->ATC Result Browser’). You can select the active ATC result on the left and display the findings of the result on the right.\nYou can also select the active check run and press ‘Show Details’ button to display its details.\nYou will get the same information as in the ATC Problems View. Of course you can customize the view by configuring the view columns or group the findings as in the ATC Problems View. Before you start to correct findings execute recheck to make sure they still are relevant.You can rerun ATC checks and consequently correct the remaining findings belonging to the check run result in the ATC Problems View (btw. the new filter entry for the result will be added to the breadcrumb) .\nThat’s it. Just try out the new ABAP Test Cockpit tools in Eclipse to increase the quality of your source code during development.\nFor quick and easy try copy & paste this ATC_EXAMPLE report into your system and run ATC for it.\ntype abap_bool default ‘X’.\nclass lcl_atc_example definition.\nclass lcl_atc_example implementation.\nlt_spfli type spfli,\nlv_flight type sflight.\n” ATC ERROR: Incompatible parameter in the function call\n“ lt_spfli(type SPFLI) with the function signature\n” parameter (type SPFLI_TAB)\n” PROPOSED CORRECTION: replace in the DATA section TYPE spfli with spfli_tab.\nif ( p_selall = abap_true ).\ncall function ‘READ_SPFLI_INTO_TABLE’\nid = ‘AA’\nitab = lt_spfli.\n” ATC ERROR: Strings without text elements are not translated\n” PROPOSED CORRECTION: Create text element using Quick Fix Ctrl + 1\nmessage ‘Display flight information’ type ‘I’.\n” ATC ERROR: Performance check SELECT accesses the non-buffered\n“ table SFLIGHT with WHERE condition,\n” which doesn’t contain any key fields of the table.\n” PROPOSED CORRECTION: SELECT * FROM sflight INTO lv_flight\n“ WHERE carrid = ‘AA’ AND seatsmax = 10.\nselect * from sflight into lv_flight where seatsmax = 10.\ndata example type ref to lcl_atc_example.\ncreate object example.', 'Static Testing is the testing of a component or system at a specification or implementation level without execution of the software. Dynamic Testing is the testing of software by executing the software of either a component or system. Static Testing and Dynamic Testing are complementary methods and tend to find different types of defects effectively and efficiently.\nStatic Testing detects defects such as deviations from standards, missing requirements, design defects, non- maintainable code and inconsistent interface specifications. Dynamic Testing detects defects such as checking if outputs from the expected values.\nOne key approach in Static Testing is the Review Process. Reviews can find defects, are informational, communicational, and educational. Participants in the review learn the content of the software systems, the role of their own work, help planning for future stages of the work. Reviews often represent milestones, and support the establishments of a baseline for the software product.\nThe type and quantity of defects found during the review stage can help focus the testing process. In some cases customers or users attend the review process and provide feedback to the developers and document authors. Studies have shown the reviews significantly increase productivity and product quality.\nThey can be either informal or formal. The formality of the review process is related to factors such as the maturity of the development process, any legal or regulatory requirements or the need for an audit trail. In practice most reviews are informal. A two-person team can conduct an informal review, as the developer/author can get a colleague to review the code and documentation.\nPhases of a Formal Review 1.Planning 2.Kick-Off 3.Preparation 4.Review Meeting 5.Rework 6.Follow-up\n1. Planning The authors/developers request a review. A moderator is assigned – this is the leader of the review process. The project planning needs to incorporate time to undertake the review. The planning must start with deciding an entry criteria to ensure that document is ready for review.\n1. Planning Entry Criteria The set of generic and specific conditions for permitting a process to go forward with a defined task, e.g. Test phase. The purpose of entry criteria is to prevent a task from starting which would entail more (wasted) effort compared to the effort needed to remove the failed entry criteria.\n1. Planning Minimum set of criteria for performing entry check: ◦ A short check of a product sample by the moderator does not reveal a large number of defects, e.g. After 30 minutes of checking, no more than 3 major defects are found in a single page or fewer than 10 major defects in total in a set of 5 pages. ◦ The document to be reviewed is available with line numbers. ◦ The document has been cleaned up by running any automated checks that apply. ◦ References needed for the inspection are stable and available. ◦ The document author is prepared to join the review team and feels confident with the quality of the document.\n1. Planning The review will focus on a few different things: ◦ Focus on higher-level documents, e.g. Does the design comply to the requirements ◦ Focus on standards, e.g. Internal consistency, clarity, naming conventions, templates ◦ Focus on related documents at the same level, e.g. Interfaces between software functions ◦ Focus on usage, e.g. For testability and maintainablity\n2. Kick-Off The review starts with a kick-off meeting, to make sure everyone is on the same wavelength regarding the document under review. The meeting consists of a short introduction to the objectives of the review and the documents. Role assignments, checking rate, the pages to be checked, process changes and possible other questions are discussed at this meeting.\n3. Preparation Participants identify defects, questions, and comments, according to their understanding of the document and their role. A checking rate is decided, which is the number of pages checked per hour, usually about 5 to 10 pages per hour, depending on complexity.\n4. Review Meeting Usually made up of the following phases: ◦ Logging phase ◦ Discussion phase ◦ Decision phase\n4. Review Meeting Logging Phase: The issues that have been identified in the Preparation stage are logged. To ensure progress and efficiency, no real discussion is allowed during the logging phase. Each defect is logged with a severity: ◦ Critical: defects will cause downstream damage. ◦ Major: defects could cause downstream damage. ◦ Minor: defects are not likely to cause downstream damage.\n4. Review Meeting Discussion Phase: Each of the defects that require discussion are discussed, with a chairman preventing discussions from getting too personal.\n4. Review Meeting Decision Phase: At the end of the discussion phase, a decision is taken about the document under review. If the number of defects found per page exceeds a certain level, the document may need to be reworked, and reviewed again.\n5. Rework Based on the defects detects, the author will improve the document under review, step- by-step.\nAlways, every, all, none, never, … (absolutely sure?) Certainly, therefore, clearly, obviously, customarily, most, … (persuasion lingo) Some, sometimes, often, usually, ordinarily, customarily, most, … (vague) etc., and so forth, and so on, such as, … (not testable) Good, fast, cheap, efficient, small, stable, … (unquantifiable) Handled, processed, rejected, skipped, eliminated… If … then … (missing else)\nRoles and Responsibilities The moderator The author The scribe The reviewers The manager\nRoles and Responsibilities The moderator serves as the review leader, they determine the type of review, approach and the composition of the review team. The moderator performs the entry check, and the follow-up on the rework. The moderator also schedules meetings, disseminates documents, leads discussions and stores relevant data.\nRoles and Responsibilities The author writes the original document and seeks to improve the quality of the document by working with others.\nRoles and Responsibilities The scribe records all of the defects during the logging meetings.\nRoles and Responsibilities The reviewers (also called checkers and inspectors) check the documents for defects. Reviewers are chosen to represent different perspectives in the review.\nRoles and Responsibilities The manager decides on the execution of reviews and determines whether review process objectives have been met.\nTypes of Reviews Walkthrough The author guides the review team through the document, to achieve a common understanding and gather feedback. This means the author does a range of preparation, and the review team don’t need to do a detailed study before the meeting. A walkthrough is especially useful for higher-level documents, like requirements specifications and architectural documents.\nTypes of Reviews Technical Review This approach focuses on developing a consensus about the technical content of the document. Defects are found by technical experts, who focus on the content of the document (as opposed to considering any relevant legislation and standards, referenced documents, and intended readership).\nTypes of Reviews Inspection This approach is the most formal review type. The document is inspected thoroughly be the reviews before the meeting, comparing the work product with its sources and other referenced documents, and using rules and checklists. The defects found are logged and any discussion is postponed until the discussion phase. This makes the inspection meeting a very efficient meeting.']	['<urn:uuid:0f09a0f5-5592-417c-83cf-404906a68b06>', '<urn:uuid:b47b5b2e-3114-43ba-9de8-b8c43c106113>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T12:39:32.578853	6	79	2300
44	How do BMAL1 and pericytes each contribute to blood-brain barrier integrity?	BMAL1 and pericytes both play crucial roles in maintaining blood-brain barrier (BBB) integrity, but through different mechanisms. BMAL1 disruption impairs BBB integrity via pericyte dysfunction, as shown by studies of the circadian clock component. Meanwhile, pericytes are fundamentally required for BBB formation during embryogenesis - they regulate functional aspects including tight junction formation and vesicle trafficking in CNS endothelial cells, with absolute pericyte coverage determining relative vascular permeability.	"[""Genomes and Genes\narntl transcription factors\nSummary: Basic helix-loop-helix (bHLH) domain-containing proteins that play important roles in CIRCADIAN RHYTHM regulation. They combine with CLOCK PROTEINS to form heterodimeric transcription factors that are specific for E-BOX ELEMENTS and stimulate the transcription of several E-box genes that are involved in cyclical regulation.\n- Namihira M, Honma S, Abe H, Tanahashi Y, Ikeda M, Honma K. Daily variation and light responsiveness of mammalian clock gene, Clock and BMAL1, transcripts in the pineal body and different areas of brain in rats. Neurosci Lett. 1999;267:69-72 pubmed..The BMAL1 expressions in these areas were similar to the case in the pineal. These findings indicate that the mammalian clock gene, Clock and BMAL1, are expressed differently in the different areas of the brain and the pineal. ..\n- Gerstner J. The aging clock: to 'BMAL'icious toward learning and memory. Aging (Albany NY). 2010;2:251-4 pubmed\n- Sharp B, Paquet E, Naef F, Bafna A, Wijnen H. A new promoter element associated with daily time keeping in Drosophila. Nucleic Acids Res. 2017;45:6459-6470 pubmed publisher..Nevertheless, our results indicate that CATAC's enhancer function persists in the absence of CLK/CYC. Thus, CATAC represents a novel cis-regulatory element encoding clock-controlled regulation. ..\n- Sobel J, Krier I, Andersin T, Raghav S, Canella D, Gilardi F, et al. Transcriptional regulatory logic of the diurnal cycle in the mouse liver. PLoS Biol. 2017;15:e2001069 pubmed publisher..Together, our temporal DNase I mappings allowed us to decipher the global regulation of diurnal transcription rhythms in the mouse liver. ..\n- Fahrenkrug J, Georg B, Hannibal J, Jørgensen H. Hypophysectomy abolishes rhythms in rat thyroid hormones but not in the thyroid clock. J Endocrinol. 2017;233:209-216 pubmed publisher..Our findings indicate that the daily rhythm of thyroid hormone secretion is governed by SCN signalling via the rhythmic TSH secretion rather than by the local thyroid clock, which was still ticking after hypophysectomy. ..\n- Fan Z, Zhao M, Joshi P, Li P, Zhang Y, Guo W, et al. A class of circadian long non-coding RNAs mark enhancers modulating long-range circadian gene regulation. Nucleic Acids Res. 2017;45:5720-5738 pubmed publisher..Our results suggest that the enhancer-associated circadian lncRNAs mark the genomic loci modulating long-range circadian gene regulation and shed new lights on the evolutionary origin of lncRNAs. ..\n- Stevanovic K, Yunus A, Joly Amado A, Gordon M, Morgan D, Gulick D, et al. Disruption of normal circadian clock function in a mouse model of tauopathy. Exp Neurol. 2017;294:58-67 pubmed publisher..Furthermore, these results establish the Tg4510 mouse line as a model to study how tauopathy disrupts normal circadian rhythm biology. ..\n- Jiang N, Wang Z, Cao J, Dong Y, Chen Y. Effect of monochromatic light on circadian rhythmic expression of clock genes in the hypothalamus of chick. J Photochem Photobiol B. 2017;173:476-484 pubmed publisher..Our results also suggest that the circadian clock in the chick hypothalamus such as non-SCN brain region were involved in the regulation of photo information. ..\n- Musiek E, Lim M, Yang G, Bauer A, Qi L, Lee Y, et al. Circadian clock proteins regulate neuronal redox homeostasis and neurodegeneration. J Clin Invest. 2013;123:5389-400 pubmed publisher..Our findings indicate that BMAL1 in a complex with CLOCK or NPAS2 regulates cerebral redox homeostasis and connects impaired clock gene function to neurodegeneration. ..\n- Børglum A, Demontis D, Grove J, Pallesen J, Hollegaard M, Pedersen C, et al. Genome-wide study of association and interaction with maternal cytomegalovirus infection suggests new schizophrenia loci. Mol Psychiatry. 2014;19:325-33 pubmed publisher..3 × 10(-7)) in CTNNA3, a gene not previously implicated in schizophrenia, stressing the importance of including environmental factors in genetic studies. ..\n- Rybakowski J, Dmitrzak Weglar M, Kliwicki S, Hauser J. Polymorphism of circadian clock genes and prophylactic lithium response. Bipolar Disord. 2014;16:151-8 pubmed..Of special interest may be polymorphisms of these genes involved both in the predisposition to bipolar mood disorder and the lithium response. ..\n- Sato F, Kohsaka A, Takahashi K, Otao S, Kitada Y, Iwasaki Y, et al. Smad3 and Bmal1 regulate p21 and S100A4 expression in myocardial stromal fibroblasts via TNF-?. Histochem Cell Biol. 2017;148:617-624 pubmed publisher..This new evidence suggests that Smad3 and Bmal1 regulate p21 and S100A4 expression in myocardial stromal fibroblasts through TNF-?. ..\n- Schallner N, Lieberum J, Gallo D, LeBlanc R, Fuller P, Hanafy K, et al. Carbon Monoxide Preserves Circadian Rhythm to Reduce the Severity of Subarachnoid Hemorrhage in Mice. Stroke. 2017;48:2565-2573 pubmed publisher..Exposure to CO rescues the loss of HO-1 and thus merits further investigation in patients with SAH. ..\n- Nakazato R, Kawabe K, Yamada D, Ikeno S, Mieda M, Shimba S, et al. Disruption of Bmal1 Impairs Blood-Brain Barrier Integrity via Pericyte Dysfunction. J Neurosci. 2017;37:10052-10062 pubmed publisher..Our present study demonstrates the involvement of circadian clock component Bmal1 in BBB homeostasis and highlights the role of Bmal1 dysfunction in multiple neurological diseases. .."", 'Pericytes are required for blood–brain barrier\nintegrity during embryogenesis\nRichard Daneman1, Lu Zhou2, Amanuel A. Kebede1& Ben A. Barres2\nbarrier that restricts the movement of molecules and ions between\nensure proper neuronal function and protect the CNS from injury\nand disease1. Transplantation studies have demonstrated that the\nBBB is not intrinsic to the endothelial cells, but is induced by inter-\nactions withtheneural cells2. Owingto theclose spatial relationship\nbetween astrocytes and endothelial cells, it has been hypothesized\nof BBB formation has been controversial4–9. Here we demonstrate\nthat the barrier is formed during embryogenesis as endothelial cells\ninvade the CNS and pericytes are recruited to the nascent vessels,\nover a week before astrocyte generation. Analysing mice with null\nand hypomorphic alleles of Pdgfrb, which have defects in pericyte\ngeneration, we demonstrate that pericytes are necessary for the\nformation of the BBB, and that absolute pericyte coverage deter-\nminesrelativevascularpermeability.We demonstrate that pericytes\nregulate functional aspects of the BBB, including the formation of\ntight junctions and vesicle trafficking in CNS endothelial cells.\nPericytes do not induce BBB-specific gene expression in CNS\nvascular permeability and CNS immune cell infiltration. These data\nindicate that pericyte–endothelial cell interactions are critical to\nactions may lead to BBB dysfunction and neuroinflammation\nduring CNS injury and disease.\nTo understand the mechanisms of BBB formation, we investigated\nthe sequence of cell generation and BBB formation in the developing\nCNS (Fig. 1 and Supplementary Figs 1–7). In the rat cerebral cortex,\nangiogenesis begins at embryonic day 12 (E12), as endothelial cells\ninvade the neural tissue fromthe surrounding vascular plexus (Fig. 1a\nand Supplementary Fig. 1). Pericytes—platelet-derived growth factor\nreceptor-b (PDGFR-b)1NG21vascular support cells—are found\nassociated with endothelial tubes as nascent vessels are generated\n(Fig. 1b and Supplementary Fig. 1). Neural cells are produced from\nprogenitorsin a defined sequence, with neuronsgeneratedbefore glia.\nare first observed at E19 and migrate throughout the cortex by birth\n(Fig. 1c). Astrocytes are first generated directly after birth and extend\nand Supplementary Fig. 2). If astrocytes induce the BBB, barrier\nproperties should only be acquired after birth.\nWe next examined endothelial cell protein expression and barrier\nfunction during rat cortical development. BBB-forming endothelial\ncells are characterized by tight junctions, low rates of transcytosis,\nand the expression of specialized influx and efflux transporters.\nTight junction molecules occludin, claudin 5 and ZO-1 were each\ntary Fig. 4 and not shown). The same was observed with the BBB-\nspecific influx transporter Glut1 (Fig. 1f and Supplementary Fig. 3).\nNotably, the BBB-specific efflux transporter Pgp is expressed at low\nlevels during embryogenesis, but increases during postnatal develop-\nment (Fig. 1g), indicating a distinct regulation mechanism for efflux\ntransport. Similar timing of cell generation and BBB gene expression\nwas observed in the developing mouse, with vascularization of the\ncortexstarting atE11(Supplementary Fig. 6).The expression of genes\nthat increasevascularpermeability,includingtranscytosis(Plvap) and\nleukocyte infiltration (Icam1), decreased after initial high expression\n(Supplementary Fig. 7). The developmental timing of BBB function\nwas examined by trans-cardiac perfusion with tracers. In adults, the\nthe tracer diffuses throughout the extracellular space (Fig. 1h–j). We\nused this method to examine BBB function in postnatal animals and\nvessels excluded the tracer from the CNS parenchyma (Fig. 1h–j\nand not shown). In embryonic time points, the tracer was excluded\nfrommost of the CNS; however,distinct regions of the CNS displayed\nleakiness, including regions close to the pia (Supplementary Fig. 5),\nings or choroid plexus. Thus, a functional BBB is present during\nembryogenesis before astrocyte generation.\nBecause pericyte recruitment to CNS vessels temporally correlates\ncells in regulating BBB function, structure and gene expression.\nAlthough pericytes are associated with the vasculature throughout\nthe body, and thus are unlikely candidates to regulate brain-specific\nvascular properties, recent studies have demonstrated that CNS peri-\ncytes haveadifferentdevelopmentaloriginfromother pericytes10,and\nseveral studies have suggested that pericytes are capable of regulating\nBBB properties in vitro11–13. Todetermine ifpericytes are necessary for\nBBB formation we compared the vascular permeability of Pdgfrb2/2\nor receptor completely lack CNS pericytes14,15, exhibit endothelial cell\nhyperplasia, increased vessel diameter and morphological signs of\nincreased vascular permeability16. These mice die at birth; therefore,\nwe examined BBB function in dissected embryos. Indeed, Pdgfrb2/2\nmice show an increased vascular permeability to biotin (0.5kDa), as\nobserved by an increase in tracer staining throughout the CNS par-\nenchyma (Fig. 2a, b). Decreasing permeability was observed with\nincreasing size of tracer (Fig. 2c).\nCNS vessels have the highest pericyte coverage of any vessels, and\ncorrelates with the relative permeability of these vessels17. To deter-\nimportant regulator of BBB permeability, we measured CNS vascular\npermeability in mice with different combinations of null, hypo-\nmorphic and wild-type Pdgfrb alleles. One study15generated an allelic\nseries of Pdgfrb hypomorphs, showing that varying the strength of\nPDGFR-b signalling leads to different pericyte:endothelial cell ratios.\n1UCSF Department of Anatomy, 513 Parnassus Avenue, HSW1301, San Francisco, California 94143-0452, USA.2Stanford University School of Medicine, Department of Neurobiology, Fairchild Science\nBuilding D200, Stanford, California 94305-5125, USA.\n5 6 2 | N A T U R E | V O L 4 6 8 | 2 5 N O V E M B E R 2 0 1 0\nMacmillan Publishers Limited. All rights reserved\nand raw image analysis with Affymetrix GCOS 1.3 software was performed as\nWestern blots. Cerebral cortex from E18 Pdgfrb2/2and littermate controls were\nhomogenized and re-suspended and lysed in RIPA buffer (50mM Tris pH7.4,\nwas quantified using BCA protein assay (Pierce). Samples were analysed by SDS–\nPAGE as previously described33. Briefly, samples were resolved on SDS–PAGE and\ntransferred to PVDF membranes. Membranes were blocked with 5% milk solution\n(Abcam), Col III (Abcam), vitronectin (Abcam), MMP9 (Abcam), PDGFR-b\n(eBiosciences) or b-actin (Sigma) followed by incubation with an appropriate\nsecondary conjugated to HRP (Jackson 1:10,000) and visualized using a chemi-\nluminescent ECL substrate for HRP (GE), and either exposed on film and quan-\ntified with ImageJ or imaged with a Fuji-Film LAS 4000 and analysed with Multi\nGauge V3.0 software (n52–5 for mutant and n54–8 for littermate controls).\nregulated by c-secretase and astrocytes in a rapidly myelinating CNS coculture\nsystem. Neuron 60, 555–569 (2008).\n32. Cahoy, J. D. et al. A transcriptome database for astrocytes, neurons, and\noligodendrocytes: a new resource for understanding brain development and\nfunction. J. Neurosci. 28, 264–278 (2008).\noligodendrocytes. Neuron 43, 183–191 (2004).\nMacmillan Publishers Limited. All rights reserved']"	['<urn:uuid:daa31ae0-f362-4392-9fda-443d887120be>', '<urn:uuid:a0b46326-491e-4280-a5ce-2a64e83bdf99>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T12:39:32.578853	11	68	1835
45	difference kosher organic chicken salmonella content	According to FSIS, there is no valid scientific information showing that any specific type of chicken (whether labeled as Kosher, free-range, organic, or natural) has more or less Salmonella bacteria than other poultry.	"['Salmonella Questions and Answers\n- What is Salmonella?\n- What is salmonellosis?\n- What are the symptoms of salmonellosis?\n- Are there long-term consequences?\n- How do people get salmonellosis?\n- What foods are most likely to make people sick?\n- Are chickens labeled ""Kosher,"" ""free-range,"" ""organic,"" or ""natural"" lower in Salmonella bacteria?\n- What is FSIS doing to prevent Salmonella contamination?\n- How can consumers prevent salmonellosis?\nSalmonella bacteria are the most frequently reported cause of foodborne illness. In order to reduce salmonellosis, a comprehensive farm-to-table approach to food safety is necessary. Farmers, industry, food inspectors, retailers, food service workers, and consumers are each critical links in the food safety chain. This document answers common questions about the bacteria Salmonella, describes how the Food Safety and Inspection Service (FSIS) of the U.S. Department of Agriculture (USDA) is addressing the problems of Salmonella contamination on meat and poultry products, and offers guidelines for safe food handling to prevent bacteria, such as Salmonella, from causing illness.\nQ. What is Salmonella?\nA. Salmonella is a gram-negative, rod-shaped bacilli that can cause diarrheal illness in humans. They are microscopic living creatures that pass from the feces of people or animals to other people or other animals.\nThe Salmonella family includes over 2,300 serotypes of bacteria which are one-celled organisms too small to be seen without a microscope. Two serotypes, Salmonella Enteritidis and Salmonella Typhimurium are the most common in the United States and account for half of all human infections. Strains that cause no symptoms in animals can make people sick, and vice versa. If present in food, it does not usually affect the taste, smell, or appearance of the food. The bacteria live in the intestinal tracts of infected animals and humans.\nSalmonella bacteria have been known to cause illness for over 100 years. They were discovered by an American scientist, Dr. Daniel E. Salmon.\n[Top of Page]\nQ. What is salmonellosis?\nA. Salmonellosis is an infection caused by the bacteria Salmonella. According to the Centers for Disease Control and Prevention (CDC), salmonellosis causes an estimated 1.4 million cases of foodborne illness and more than 400 deaths annually in the United States. The Surveillance Report from the Food Diseases Active Surveillance (FoodNet) for 2007, identified Salmonella as the most common bacterial infection reported.\nFoodNet is a collaborative project among CDC, the 10 Emerging Infections Program sites (EPIs), USDA, and the U.S. Food and Drug Administration (FDA). One of the objectives of FoodNet is to measure effectiveness of a variety of preventive measures in reducing the incidence of foodborne illness attributable to the consumption of meat, poultry, and other foods.\n[Top of Page]\nQ. What are the symptoms of salmonellosis?\nA. Although in some people salmonellosis could asymptomatic, most people experience diarrhea, abdominal cramps, and fever within 8 to 72 hours after the contaminated food was eaten. Additional symptoms may be chills, headache, nausea, and vomiting. Symptoms usually disappear within 4 to 7 days. Many people with salmonellosis recover without treatment and may never see a doctor. However, Salmonella infections can be life-threatening especially for infants and young children, pregnant women and their unborn babies, and older adults, who are at a higher risk for foodborne illness, as are people with weakened immune systems (such as those with HIV/AIDS, cancer, diabetes, kidney disease, and transplant patients).\n[Top of Page]\nQ. Are there long-term consequences?\nA. Persons with diarrhea usually recover completely, although it may be several months before their bowel habits are entirely normal. A small number of persons who are infected with Salmonella may develop pains in their joints, irritation of the eyes, and painful urination. This is called Reiter\'s syndrome. It can last for months or years and can lead to chronic arthritis that is difficult to treat.\n[Top of Page]\nQ. How do people get salmonellosis?\nA. Salmonella lives in the intestinal tract of humans and other animals, including birds. Salmonella is usually transmitted to humans by eating foods contaminated with animal feces. Salmonella present on raw meat and poultry could survive if the product is not cooked to a safe minimum internal temperature, as measured with a food thermometer. Salmonella can also cause foodborne illness (salmonellosis) through cross-contamination, e.g., when juices from raw meat or poultry come in contact with ready-to-eat foods, such as salads.\nFood may also become contaminated by the unwashed hands of an infected food handler who might or might not be showing symptoms. Salmonella can also be found in the feces of some pets, especially those with diarrhea. People can become infected if they do not wash their hands after contact with these feces. Reptiles are particularly likely to harbor Salmonella. People should always wash their hands immediately after handling a reptile, even if the reptile is healthy.\n[Top of Page]\nQ. What foods are most likely to make people sick?\nA. Any raw food of animal origin, such as meat, poultry, milk and dairy products, eggs, seafood, and some fruits and vegetables may carry Salmonella bacteria. The bacteria can survive to cause illness if meat, poultry, and egg products are not cooked to a safe minimum internal temperature as measured with a food thermometer and fruits and vegetables are not thoroughly washed. The bacteria can also contaminate other foods that come in contact with raw meat and poultry. Safe food handling practices are necessary to prevent bacteria on raw food from causing illness.\n[Top of Page]\nQ. Are chickens labeled ""Kosher,"" ""free-range,"" ""organic,"" or ""natural"" lower in Salmonella bacteria?\nA. FSIS does not know of any valid scientific information that shows that any specific type of chicken has more or less Salmonella bacteria than other poultry.\n[Top of Page]\nQ. What is FSIS doing to prevent Salmonella contamination?\nA. The Food Safety and Inspection Service is the public health regulatory Agency in the USDA responsible for the safety of the nation\'s commercial supply of meat, poultry and egg products. As part of this responsibility, FSIS issued the ""Pathogen Reduction; Hazard Analysis and Critical Control Point (PR/HACCP) Systems, Final Rule"" in 1996. This rule sets Salmonella performance standards for establishments slaughtering selected classes of food animals or those producing selected classes of raw ground products to verify that industry systems are effective in controlling the contamination of raw meat and poultry products with disease-causing bacteria, like Salmonella.\nFSIS inspectors make sure the establishments are meeting the standards by collecting randomly selected product samples and submitting them to an FSIS laboratory for Salmonella analysis. FSIS requires all plants to reduce bacteria by means of the PR/HACCP system.\n[Top of Page]\nQ. How can consumers prevent salmonellosis?\nA. Bacteria on raw foods of animal origin do not have to cause illness. The key to preventing illness at home, in a restaurant, at a church picnic, or anywhere else is to prevent the bacteria from growing to high levels and to destroy the bacteria through cooking to a safe minimum internal temperature. Follow these guidelines for safe food preparation:\nCLEAN: Wash Hands and Surfaces Often\n- Wash hands with warm soapy water for 20 seconds before and after handling food and after using the bathroom, changing diapers, and handling pets.\n- Wash utensils, cutting boards, dishes, and countertops with hot soapy water after preparing each food item and before you go on to the next item.\n- Consider using paper towels to clean kitchen surfaces. If you use cloth towels, wash them often in the hot cycle of your washing machine.\nSEPARATE: Don\'t Cross-contaminate\n- Separate raw meat, poultry, and seafood from other foods in your grocery shopping cart and in your refrigerator.\n- If possible, use one cutting board for fresh produce and a separate one for raw meat, poultry, and seafood.\n- Always wash cutting boards, dishes, countertops, and utensils with hot soapy water after they come in contact with raw meat, poultry, and seafood.\n- Never place cooked food on a plate that previously held raw meat, poultry, or seafood.\nCOOK: Cook to Safe Temperatures\nUse a clean food thermometer when measuring the internal temperature of meat, poultry, casseroles, and other foods to make sure they have reached a safe minimum internal temperature:\n- Cook all raw beef, pork, lamb and veal steaks, chops, and roasts to a minimum internal temperature of 145 °F as measured with a food thermometer before removing meat from the heat source. For safety and quality, allow meat to rest for at least three minutes before carving or consuming. For reasons of personal preference, consumers may choose to cook meat to higher temperatures.\n- Cook all raw ground beef, pork, lamb, and veal to an internal temperature of 160 °F as measured with a food thermometer.\n- Cook all poultry to a safe minimum internal temperature of 165 °F as measured with a food thermometer.\n- Stuffed poultry is not recommended. Cook stuffing separately to 165 °F.\n- Egg dishes, casseroles to 160 °F.\n- Fish should reach 145 °F as measured with a food thermometer.\n- Bring sauces, soups, and gravy to a boil when reheating.\n- Reheat other leftovers thoroughly to at least 165 °F.\nCHILL: Refrigerate Promptly\n- Keep food safe at home, refrigerate promptly and properly. Refrigerate or freeze perishables, prepared foods, and leftovers within 2 hours (1 hour if temperatures are above 90 °F).\n- Freezers should register 0 °F or below and refrigerators 40 °F or below.\n- Thaw food in the refrigerator, in cold water, or in the microwave. Foods should not be thawed at room temperature. Foods thawed in the microwave or in cold water must be cooked to a safe minimum internal temperature immediately after thawing.\n- Marinate foods in the refrigerator.\n- Divide large amounts of leftovers into shallow containers for quick cooling in the refrigerator.']"	['<urn:uuid:d8b89c01-5b86-428a-831d-5e7df5cd2e8a>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	6	33	1626
46	cognitive decline prevention diet lifestyle factors	Preventing cognitive decline involves both dietary and lifestyle modifications. The MIND diet, which emphasizes leafy greens, berries, nuts, whole grains, and seafood while limiting red meat and sweets, can help protect brain health. Additionally, maintaining social engagement, regular exercise, proper sleep, stress management, and engaging in learning activities like computer use or crafts can reduce cognitive decline risk by 30-50%. These factors combined could prevent or delay up to 40% of dementia cases.	"['As the population most at risk for complications from the novel coronavirus (Covid-19), many seniors have spent several months self-isolating to reduce their risk of exposure, causing many caregivers to express their concern that the lack of social engagement among seniors is creating a new crisis – widespread cognitive decline.\nWhat is Mild Cognitive Impairment or MCI?\nThe Mayo Clinic describes Mild Cognitive Impairment (MCI) as “the stage between the expected cognitive decline of normal aging and the more serious decline of dementia.” Seniors with MCI may struggle with remembering words or daily tasks, and you may notice an overall slip in cognitive awareness among your loved one. Mild Cognitive Impairment is not a form of dementia, and while some individuals who develop MCI are at higher risk of dementia, many seniors’ cognitive abilities remain stable for years, or improve after intervention.\nCan isolation accelerate cognitive decline?\nResearchers estimate that between one third and one half of all seniors experience loneliness and social isolation during their late adulthood. This number can be greatly affected by many external factors, including a public health emergency such as the current coronavirus pandemic that is particularly dangerous to seniors. As more seniors spend time away from their families, peers, and social hubs, it raises concerns about isolation and subsequent cognitive decline.\nWhile researchers cannot definitively prove causation between isolation and cognitive impairment in older adults, studies have shown that loneliness may have a direct impact on seniors’ brains and overall health. Additionally, there is plenty of research that shows individuals who have more regular social opportunities are less likely to develop several age-related conditions like Alzheimer’s disease, osteoporosis, arthritis, heart disease, depression and several cancers.\nIs there a way to prevent or reverse mild cognitive impairment?\nWhile there is no cure for dementia, the good news is that there are several ways you can help protect yourself or your loved ones from cognitive decline in late adulthood. One important step is making sure that you are staying hydrated and eating healthy, nutritious foods. Research has shown that eating brain healthy foods, such as spices, whole grains, seafood, and fresh fruits and vegetables can lead to better brain function and slow the effects of memory loss.\nAnother way to help boost brain health is to stay physically active, as well as socially active. Getting frequent exercise and engaging in social activities with others, such as going for a walk with friends or catching up on the phone, can help reduce stress in addition to potentially reducing the risk of dementia. Getting enough sleep and limiting alcohol intake and smoking are also contributing factors in helping to prevent memory loss. For seniors who may not be able to cook nutritious meals or seek out social activities on their own anymore, a Retirement or Assisted Living Community can offer a more supportive living environment that helps seniors stay sharp and independent in a secure, home-like setting.\nWhat can I do if my Mom or Dad is isolated due to Covid-19?\nIf your loved one is feeling isolated due to the current coronavirus pandemic, there are many ways that you can help.\n- Setting up virtual visits using a smartphone or laptop can help ease loneliness, and there are now more ways than ever to make these visits fun! Consider playing a round of online trivia, having group Zoom calls with the grandkids, or hosting a virtual social hour with friends.\n- If you are planning an in-person visit with someone experiencing memory loss, consider following some of these tips to help keep the conversation going.\n- You may also consider dropping off meals or groceries for your loved one, or signing up for a grocery delivery service. Ensuring that your mom or dad has access to healthy meals will help prevent poor nutrition and decrease their risk of cognitive decline during self-isolation.\nThe Bottom Line\nWith the novel coronavirus impacting all of our daily lives, it is important that we take time to check in on our senior loved ones to prevent isolation and cognitive decline. Loneliness is one of the most difficult parts of aging for many seniors, but it doesn’t have to be. Taking time to visit, checking in to make sure your loved one is staying healthy, and dropping off nutritious foods can all go a long way toward reducing feelings of isolation, and potential memory loss, in a senior family member. If your mom or dad would benefit from a more supportive living arrangement, a move to Assisted Living may be able to help offer them the nutritious diet, social engagement, physical activity and supportive services that can help them thrive in late adulthood.\nRead more: Covid and Assisted Living: Is Now the Time to Make a Move?', ""Sep 13, 2023, 4:00 am UTC\n6 Brain health rules to live by\nThis article covers:\n- What are the 6 rules of brain health?\n- Engage the brain: What is brain training?\n- Stay socially connected: How do you cultivate connection and belonging?\n- Reduce stress: How does stress impact the brain?\n- Exercise regularly: What can yoga do for the brain?\n- Sleep plenty: What are the sex and gender differences in sleep health across the lifespan?\n- Eat well: What is the MIND diet?\nIf you’re even a little bit health conscious, you’ve probably heard the saying: “Eat food. Not too much. Mostly plants.”\nIn an age of rigid diets and conflicting health advice, those “food rules” — popularized by author and journalist Michael Pollan — serve as a simple guide to healthy eating.\nDid you know there are also simple rules for brain health? And the earlier you embrace them, the better?\nLike Pollan’s maxims, the brain health rules are practical and rooted in common sense and science. For example, in 2020, a global team of brain researchers identified 12 healthy lifestyle factors for dementia that if followed would prevent or delay four out of 10 dementia cases. They include treating hypertension, quitting smoking, exercising regularly, avoiding excess alcohol, and maintaining social contact. And just this month, a different team pinpointed 7 factors that cut a person's risk for depression. They estimated that people who adhere to these healthy lifestyle factors could cut their depression risk by 57 percent.\nThese healthy lifestyle factors can be distilled into the six pillars, or “rules,” of brain health.\nIn this article, we share these simple rules, meant to help women at any age keep their brains thriving and reduce the risk of dementia, depression, and other conditions. We also offer research-backed tips on how to put these lessons into practice.\nThat's not to say that individual actions alone can prevent dementia and other neurological or mental health conditions. Society, the environment, and genetics also influence a person's risk of developing brain diseases. But lifestyle changes are a powerful salve — and complement other strategies to reduce the risk of mental and neurological conditions.\n6 Brain health rules\n- Engage your brain\n- Stay socially engaged\n- Manage stress\n- Exercise regularly\n- Get a good night’s sleep\n- Eat well\n1. Engage your brain\nLearning is good for the brain. As you learn, your brain makes new connections and strengthens existing ones. This helps build “cognitive reserve,” the brain’s ability to adapt to age- or disease-related changes. While all intellectual activities are beneficial, some appear to provide a better workout for your brain than others. In one study, activities such as using a computer, playing games, reading books, and doing crafts like knitting and woodworking were associated with a 30 to 50 percent decrease in the chances of developing mild cognitive impairment. Learning a new instrument or language is another way to exercise the brain.\nTip: Take the Stand Ahead Challenge, an annual awareness and fundraising campaign from the Women’s Brain Health Initiative that encourages you to test your mental fitness and share the results. This year, try the Thumbs Up Challenge: Put your left thumb up and point to it with your right index finger, then switch 10 times.\n2. Stay socially engaged\nWe often overlook the fact that social connections and belonging are fundamental human needs — as essential to life as food, water, and oxygen. Yet, the U.S. Surgeon General recently declared a loneliness epidemic and stressed that the absence of social connections has a profoundly negative impact on our psychological and physical health.\nHere’s what the science says: Social disconnection increases the risk of many serious health conditions, including heart disease, stroke, hypertension, dementia, diabetes, depression, and anxiety. Furthermore, social isolation and loneliness increase the risk for premature death by 29% and 26%, respectively, and lacking social connection can increase the risk of premature death as much as smoking up to 15 cigarettes daily.\nTip: Need a little push? Talk to your healthcare provider about a social prescription in which a health professional refers a patient to a community or cultural activity to bolster their mental and physical health. Plus, select a strategy below to cultivate belonging.\n3. Manage stress\nWhen you think about it, self-care is really about managing stress.\nDecades of research have revealed the detrimental effects of chronic stress on the female brain, with evidence suggesting that stress hits women’s brains differently and harder. Researchers recently pinpointed how different cell types in the brain react to stress — and how the stress response differs between males and females. Studying mice, the scientists found that a particular type of support cell in the brain was more susceptible to stress in females compared with male mice.\nThat kind of research is adding to our existing understanding of stress and the brain, in which elevated cortisol levels, a hallmark of chronic stress, are known to impair neuroplasticity, undermine the ability to think and remember, and exacerbate mood disorders such as anxiety and depression — conditions that are more prevalent among women. Stress can also disrupt sleep patterns, worsening the cognitive and emotional changes already brought on by stress.\nTip: Embrace self-care strategies that help you relieve stress, such as relaxation practices like belly breathing, mindfulness meditation, or yoga.\n4. Exercise regularly\nPhysical inactivity heightens a person's risk for Alzheimer’s and contributes to about 13% of Alzheimer’s cases worldwide. Conversely, frequent exercise reduces the risk for Alzheimer’s disease and related dementias by improving cardiovascular and metabolic health and through direct effects on the brain. For example, studies in mice have shown that physical activity stimulates the growth of new nerve cells and reduces memory problems. Exercise also reduces deposits of amyloid beta, a protein that builds up in the brains of people with Alzheimer’s disease, triggering inflammation and disrupting communication between brain cells.\nTip: The U.S. Centers for Disease Control recommends adults get at least 150 minutes of moderate-intensity physical activity weekly or 75 minutes of vigorous-intensity activity for brain health. Consider yoga, which combines rhythmic breathing and movement and has been described as “weightlifting for the brain” because of its beneficial effects on brain function.\n5. Get a good night’s sleep\nSleep is pivotal to preserving and optimizing brain health, particularly in women. Research has illuminated the relationship between sleep and cognitive function, highlighting sleep's indispensable role in consolidating memory, regulating emotions, and daily brain recovery. This connection is even more critical for women because hormonal fluctuations throughout the menstrual cycle, pregnancy, and menopause can significantly impact sleep patterns and quality (see below). Adequate sleep also enhances neuroplasticity, the brain’s ability to modify itself, which helps protect against age-related cognitive decline and mood disorders that disproportionately affect women.\nTip: Take naps. A new brain imaging study found a link between napping and brain volume and suggests that napping may help to preserve brain health by slowing the rate at which our brains shrink as we age.\n6. Eat well\nYou really can eat to protect your brain. Researchers have combined two well-known heart-healthy diets — Mediterranean and DASH — into one designed to maintain brain health as we age. Called the MIND diet, it emphasizes foods like leafy green vegetables, berries, nuts, beans, whole grains, seafood, chicken, and olive oil. These foods are rich in vitamins and antioxidants known as carotenoids and flavonoids, which may protect the brain by reducing oxidative stress and inflammation. MIND also de-emphasizes red meat, butter, cheese, sweets, and fast food, which are high in saturated fats and sugar and increase the risk of cardiovascular disease.\nThough it is labelled a diet, MIND is not prescriptive. Instead, it encourages people to eat more of some foods and less of others. In 2015, researchers found that the MIND diet could slow cognitive decline and lower Alzheimer’s risk, even if the diet is not followed closely. A recent clinical trial — the first of its kind — cast doubt on MIND’s ability to slow cognitive aging in people aged 65 and older, at least over a relatively short period (3 years). Still, experts say the diet is rich in foods that benefit brain health, and its impacts are likely to emerge over time. Research is ongoing.\nTip: Add more of MIND’s brain-healthy foods, such as chicken and fish, leafy greens and berries, to your diet while cutting back on red meat, butter, full-fat cheese, and sweets.\nBanner image from Freepik.""]"	['<urn:uuid:8ee09843-90d8-4084-849e-ac8d3bfad15c>', '<urn:uuid:63377583-03dc-4840-942d-c716d1c4144a>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	6	73	2212
47	As a military historian researching Allied operations in WWII, I'm curious about what made the bridge at Arnhem strategically important during Operation Market Garden?	The Arnhem bridge over the Lower Rhine was strategically crucial as its capture would open the way into Germany's industrial heartland of the Ruhr. This was why the Allies launched a massive airborne assault in September 1944, with almost 12,000 parachute and glider-borne troops deployed to seize this bridge. The operation involved British 1st Airborne Division and Polish brigade targeting this key crossing.	"['Arnhem Berömda Bron Över Floden Rhen, Holland Stockfoton\nOperation Market Garden: a massive airborne strike designed to open a c The Allies decided to launch an airborne assault on Arnhem to secure bridges across the Rhine River to end the war, and the operation took place in September 16 Dec 2016 Aerial reconnaissance photo of the Arnhem Bridge taken by the Royal Air Force on 19 September, 1944 showing signs of the British defense 19 Oct 2011 The first bridges could be seized successfully, but the bridge in Arnhem was “one bridge too far”, so that the Allied forces suffered a disastrous 29 Sep 2010 Arnhem Bridge - September 17, 1944 - September 19, 1944 · Historical Background: The largest Airborne drop ever designed, Operation Market 16 Aug 2016 Almost 12,000 parachute and glider-borne troops landed to seize the bridge over the Rhine at Arnhem in Holland. Over nine days of fierce 26 May 2018 Further on, the British 1st Airborne Division and the Polish brigade would drop near Arnhem to capture the great road bridge over the Neder 30 Nov 2016 Arnhem bridge across the Rhine proved an insurmountable goal for the ill-fated British 1st Airborne during Operation Market-Garden. 18 Sep 2019 The loss of the bridge at Son, and the inability of 82nd Airborne to take Nijmegen swiftly enough, are both examples of a number of failings that Aerial view of the bridge over the Neder Rijn, Arnhem.jpg Slaget vid Arnhem var en del av Operation Market Garden som utkämpades i och omkring de De flygburna truppernas uppgift var att säkra bron över Rhen vid Arnhem och de på att ingenjörstrupperna skulle bygga upp en Bailey bridge över kanalen. Includes: ""1944"", ""A Bridge Too Far"" and more!\nNote: This is not a historically accurate game! 2009-10-27 2016-11-30 John Frost Bridge (John Frostbrug in Dutch) is the road bridge over the Lower Rhine at Arnhem, in the Netherlands. The bridge is named after Major-General John Dutton Frost (1912–1993), who commanded the British forces that reached and defended the bridge during the Battle of Arnhem in September, 1944. The bridge was made more famous… Massed pipes and drums are marching down the John Frost bridge, towards remembrance monument during 65th remembrance of the Battle of Arnhem (Operation Marke Arnhem was the site of the failed attempt to take the bridge of the Rhine during World War 2. In September 1944 thousands of mainly British and Polish paratroopers were dropped in enemy territory outside Arnhem during operation \'Market Garden\'. This museum is close to Arnhem road bridge.\nPanzer Cmp - Market-Garden \'44 – Appar på Google Play\nThe photos in this section are excellent, showing the fighting in the area and photos taken after the battle in 1945. 2 dagar sedan · After reaching the furthest bridge, over the Lower Rhine at Arnhem, the way into Germany\'s industrial heartland of the Ruhr would be clear.\noperation \'market garden\' - the battle for arnhem - Pinterest\nMarkerad betygsstjärna Omarkerad betygsstjärna. Antal reservationer: 0. ArnhemJönköping University. Skövde Arnhem.\nIf you continue to use this website without changing your cookie settings or you click ""Accept"" below then you are consenting to this. Marketplace.\nBranemark implant screwdriver\nOriginaltitel: A bridge at Arnhem. Innehållsbeskrivning. This 24-hour fire station in Arnhem (The Netherlands) consists of a commercial building with office space, workshops, warehouses, parking vehicles and various Utförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs], Charles Whiting; Originaltitel: A bridge at Arnhem; Medarbetare: Knut Rosén.\n· imusic.se. Arnhem: The Farthest Bridge_boxshot key bridges along the main highway leading through Holland, the Arnhem bridge being the most distant of them all. Arnhem bridge - John Frost bridge - scenic of World War 2 battle during Market-Garden airborne operation. The bridge was captured during the operation by the\nRoy. 4to., First Edition, with very numerous photographs and maps (a number full-page) in the text; red cloth, backstrip lettered in silver, a fine copy in unclipped\nUtförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs]; Originaltitel: A bridge at Arnhem; Språk: Svenska. Originalspråk: Engelska\nEn bro vid Arnhem [Elektronisk resurs].\nAdvokatfirman kronan oskarshamn\nBridge to Bridge Arnhem. 16 Aug 2016 The operation was split into two sub-operations: Market – the airborne forces, the First Allied Airborne Army, who would seize bridges Garden – Lance Gerrard-Wright in front of the John Frost Bridge in Arnhem, named after Major-General John Dutton Frost, who commanded the British forces. This meant 20 Sep 2019 Eventually, the survivors were evacuated across the river, but with Arnhem bridge still in German hands, the operation was a failure. Nearly 17 Oct 2019 Farthest north, the British 1st Airborne Division landed at Arnhem to secure bridges across the Nederrijn, supported by men of the Glider Pilot Bloomsbury presents The Devil\'s Bridge by Anthony Tucker-Jones, read by Roger May. In the late summer of 1944, SS-Obergruppenführer Wilhelm \'Willi\' Bittrich Ride to the infamous bridge at Arnhem. This veritable feast of history concentrates on Operation Market Garden but happens to pass by some other pretty Arnhem Bridge.\nIt includes maps, good action reports and order of battles on the German units involved in the fighting.\nEn tung lastbil reflex\nmagisterexamen vs masterexamen\nrtv orkester dirigent\netik i socialt arbete - etisk kod för socialarbetare antagen av akademikerförbundet ssr.\nJa jag vet, en suddig bild, men maten och ölen var jättegod, Arnhem\nAndra broar på Arnhem — Arnhem har tre broar över Niederrinen: John Frost Bridge, Nelson Mandela Bridge och Andrei Sakharov Bridge. av F Rosgren · 2018 — 2 Ryan, Cornelius, A bridge too far, Hamish Hamilton, London, 1974, s. vii. communications during the early part of the Battle of Arnhem. Slaget vid Arnhem var en del av Operation Market Garden som utkämpades i och omkring de holländska William Goldman\'s Story of a Bridge Too Far. De allierades insats med fallskärmsjägare vid Arnhem i september 1944 är han blivit för sitt sista storverk om andra världskriget, A Bridge Too Far – En bro för Airborne at the bridge ligger inom gångavstånd från detta stora hotell. För din bekvämlighet, finns det Wi-Fi, arbetsbord och IDD-telefon.\npeptonic medical avanza\n- Tele wizja ru\n- Miljöbalken innehållsförteckning\n- Paula blomqvist uppsala\n- Första hjälpen vandring\n- Lappland eco shell\n- Första tv spelet\n- Benjamin eidem model\nJohn Frost Brug Arnhem by JaapMechielsen - Pinterest\nThis 24-hour fire station in Arnhem (The Netherlands) consists of a commercial building with office space, workshops, warehouses, parking vehicles and various Utförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs], Charles Whiting; Originaltitel: A bridge at Arnhem; Medarbetare: Knut Rosén.\nHotell i Arnhem Hotel Haarhuis - TiCATi.com\nThis year marks the 75th anniversary of the battle for the bridge at Arnhem – of The battle of Arnhem IN 9 DAys.\nDue to several reasons Frost had to retreat, and the bridge was again in German control. The movie A Bridge Too Far is about this ""Arnhem: The Third Bridge is an Advanced Squad Leader module that depicts the battle for the bridge made famous in print and film by Cornelius Ryan\'s \'A Bridge Too Far\'.']"	['<urn:uuid:502f7c57-ad67-4aff-bf71-c80c2212f1f3>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	24	63	1182
48	I'm doing research on past US presidents and their involvement in civil rights. What did Presidents Obama, Clinton, Carter and Johnson say about civil rights and equality in their important speeches?	These presidents made significant statements about civil rights. President Johnson in 1972 emphasized that government should have 'unceasing concern for the welfare and dignity and decency and innate integrity of life for every individual' regardless of color, creed, ancestry, sex or age. At the 50th anniversary of King's 'I Have a Dream' speech, President Carter spoke about voter ID requirements and stand-your-ground laws, suggesting Dr. King would have opposed them. President Clinton emphasized that while racial inequalities persist in various areas, the nation no longer faces beatings and lynchings for political beliefs, and urged people to stop complaining about gridlock and take action. President Obama stressed that 'the arc of the moral universe may bend toward justice but it does not bend on its own,' calling for vigilance to secure civil rights gains and highlighting that the 1963 march wasn't just about civil rights but also economic opportunity.	"['The 1964 Civil Rights Act, whose Title VII outlawed employment discrimination based on race, color, religion, sex, or national origin, is on display June 18 through Sept. 16 at the National Archives to honor the law\'s 50th anniversary.\nThe most sweeping civil rights legislation in U.S. history also prohibited discrimination in public places, and integrated public schools and other public facilities.\nNational Archives also will present these related programs:\nFILM SCREENING: ""A Time for Justice"" and ""Mighty Times: The Children’s March"", two Academy Award®-winning documentaries:\nTuesday, July 1, at noon, William G. McGowan Theater\n""A Time for Justice"" (1994; 38 minutes) the civil rights battle, as told by several of its heroes who risked their lives for the cause of freedom and equality. Four-time Academy Award winner Charles Guggenheim directed the film.\n""Mighty Times: The Children’s March"" (2004; 40 minutes) tells the story of young people in Birmingham, Alabama, who braved fire hoses and police dogs in 1963 -- and brought segregation to its knees.\nBoth films are being screened courtesy of Teaching Tolerance, a project of the Southern Poverty Law Center.\nFILM SCREENING AND DISCUSSION: ""Crisis: Behind a Presidential Commitment""\nTuesday, July 29, at 7 P.M. William G. McGowan Theater\nRobert Drew’s cinéma vérité work chronicles how President John F. Kennedy and his brother Attorney General Robert F. Kennedy clashed with Governor George Wallace over racial integration at the University of Alabama in 1963. (1963; 52 minutes).\nFollowing the screening, NPR\'s Michele Norris Johnson will moderate a discussion featuring Peggy Wallace, daughter of the late Gov. George Wallace, and Sharon Malone, sister of the late Vivian Malone Jones, the first African American to graduate from the University of Alabama, class of 1965. Sharon Malone, an obstetrician/gynecologist, is also the wife of Attorney General Eric Holder.\nPresented in partnership with the 2014 March on Washington Film Festival.\nThe 1964 Civil Rights Act was signed into law 50 years ago on July 2 by President Lyndon B. Johnson. The act\'s signature page and first page can be seen now in the National Archives\' Records of Rights exhibit within the new David M. Rubenstein Gallery.\nOnly two pages of the 29-page act are exhibited, due to sensitivity to light. And the original signature page will be shown only through July 13, when a facsimile will replace it.\n""We want these documents to be around for hundreds of years,"" Michael Hussey, National Archives education and exhibits specialist, told me at a press preview June 17.\nChip Taylor of the Justice Department\'s Civil Rights Division said he was ""very excited"" to see the pages. ""Civil rights means a lot to me and to all Americans, and makes me extremely proud of the work we do at the Justice Department,"" he told me at the preview. ""It\'s really neat to see the document in person, the ink on paper, it takes you back to that time.""\n(That time is brought back to life in the Broadway play ""All the Way"", with Bryan Cranston, who just won the Best Actor Tony® for his portrayal of LBJ during the first year of his Presidency. The play\'s limited engagement ended June 29.)\nAs President Obama told the Civil Rights Summit at the LBJ Presidential Library last April, ""You\'re reminded daily that in this great democracy, you are but a relay swimmer in the currents of history, bound by decisions made by those who came before, reliant on the efforts of those who will follow, to fully vindicate your vision.\n""But the Presidency also affords a unique opportunity to bend those currents by shaping our laws and by shaping our debates, by working within the confines of the world as it is but also by re-imagining the world as it should be,"" said the country\'s first African American President.\nPresidents Jimmy Carter, Bill Clinton, and George W. Bush also spoke at the summit. To watch all four Presidents delivering their speeches, click here.\nAnd as President Johnson said at a 1972 civil rights summit at his library, ""I believe that the essence of government lies with unceasing concern for the welfare and dignity and decency and innate integrity of life for every individual.\n""I don’t like to say this and wish I didn\'t have to add these words to make it clear but I will—regardless of color, creed, ancestry, sex or age,"" he stressed.\nThose words still need to be added -- a half-century after the 1964 Civil Rights Act was signed into law.\n(The Library of Congress will open a year-long exhibition ""The Civil Rights Act of 1964: A Long Struggle for Freedom"" on Sept. 10. The free exhibition will highlight the legal and legislative challenges and victories leading to its passage, shedding light on the individuals, both prominent leaders and private citizens, who participated in the decades-long campaign for equality. It will feature more than 200 items, including correspondence and documents from civil-rights leaders and organizations, photographs, newspapers, legal briefs, drawings, and posters.)\nFor more info: National Archives, David M. Rubenstein Gallery, Records of Rights, Constitution Avenue between 7th and 9th Streets, N.W., Washington, D.C. 866-272-6272. 1964 Civil Rights Act, first and signature pages, displayed through Sept. 16. Online among 100 milestone documents, www.ourdocuments.gov. U.S. Senate\'s ""Civil Rights Act at 50"". Video and text of President Johnson\'s speech to a joint session of Congress, imploring passage of Civil Rights Act, five days after President Kennedy\'s assassination. LBJ Presidential Library and Museum, 2313 Red River Street, Austin, Texas, 512-721-0200. Library of Congress, Thomas Jefferson Building, Southwest Gallery, 2nd floor, 10 First Street, S.E., Washington, D.C.', ""President Barack Obama on Wednesday paid tribute to Dr. Martin Luther King, Jr. and the civil rights activists who marched on Washington 50 years ago in the name of justice as he called for renewed courage to battle today’s challenges on issues like economic inequality.\n“The arc of the moral universe may bend toward justice but it does not bend on its own,” Obama said. “To secure the gains this country has made requires constant vigilance, not complacency.”\nObama, along with Presidents Bill Clinton and Jimmy Carter, civil rights leaders, Oprah Winfrey and tens of thousands of people descended on the National Mall on Wednesday to mark the 50th anniversary of Dr. Martin Luther King, Jr.’s revolutionary “I Have a Dream” speech.\n“Their victory was great,” Obama said of leading civil rights leaders, noting that some who lost their lives for the cause “did not die in vain. … But we would dishonor those heroes as well to suggest that the work of this nation is somehow complete.”\n(PHOTOS: 20 March on Washington moments)\nThe presidents spoke in the afternoon from the steps of the Lincoln Memorial, where King delivered his famous civil rights address during the 1963 March on Washington.\n“This march and that speech changed America,” Clinton said. “They opened minds, they melted hearts and they moved millions, including a 17-year-old boy watching alone in his home in Arkansas.”\nCelebrities including Winfrey, Jamie Foxx and Forest Whitaker also addressed the crowds.\nRep. John Lewis (D-Ga.), who at 23 was the youngest speaker at the 1963 march and is now said to be the only speaker from that event still living, delivered a rousing return speech.\n(PHOTOS: Scenes from the March on Washington)\n“Fifty years later we can ride anywhere we want to ride, stay anywhere we want to stay, those signs that say white and color are gone,” he said. “You won’t see them anymore except in a museum, in a book or in a video.”\nBut, he said “there is still a gulf between us,” blasting New York’s stop-and-frisk policies, incarceration rates and the verdict in the Trayvon Martin case, among other issues.\nThe morning kicked off with interfaith services amid a light drizzle. The public ceremony at the Lincoln Memorial followed, beginning at 11 a.m., and was hosted by broadcaster Soledad O’Brien and actor Hill Harper. A bell-ringing ceremony took place in the afternoon to commemorate King’s call to “let freedom ring.”\n“So as the bells toll today let us reflect on the bravery, let us reflect on the sacrifice, [of] those who stood up for freedom, those who stood up for us, whose shoulders we now stand on,” Winfrey said. “Let us ask ourselves, how will that dream live on in me, in you, in all of us?”\n(Also on POLITICO: Oprah: 'Took me 50 years, but I'm here')\nOther bell-ringing events were scheduled around the world, including in London’s Trafalgar Square.\nAll three presidents discussed how the legacy of the civil rights movement should shape today’s debates on income disparities and voting rights.\n“In some ways… the securing of civil rights, voting rights, eradication of legalized discrimination, the very significance of these victories may have obscured a second goal of the march,” Obama said. “For the men and women who gathered 50 years ago were not there in search of some abstract idea. They were there seeking jobs as well as justice. Not just the absence of oppression but the presence of economic opportunity.”\n(PHOTOS: Realize the Dream March and Rally)\nCarter, a Georgian like King, said “we all know how Dr. King would have reacted” to voter ID requirements and stand-your-ground laws, among other policies.\n“Well, there’s a tremendous agenda ahead of us, and I’m thankful to Martin Luther King, Jr. and his dream is still alive,” Carter said.\nClinton also took to task people who dwell gridlock in Washington, instead of focusing on a way forward, like King did.\n“Oh yes, we face terrible political gridlock now,” he said. “Yes, there remain racial inequalities, in employment income, health, wealth, incarceration, in the victims and perpetrators of violent crime. But we don’t face beatings, lynchings and shootings for our political beliefs anymore.\n(Also on POLITICO: Obama, race and class)\n“And I would respectfully suggest that Martin Luther King did not live and die to hear his heirs whine about political gridlock. It is time to stop complaining and put our shoulders against the stubborn gates holding the American people back,” Clinton added.\n“Fifty years ago today, this place was a battlefield,” Sen. Angus King (I-Maine), who attended the first March on Washington, told the crowd. “No shots were fired, no cannons roared, but a battlefield nonetheless. A battlefield of ideas. The ideas that define us as a nation.”\nRep. Joaquin Castro (D-Texas) offered a pledge to the veterans of civil rights struggles.\n“As somebody of a younger generation of Americans, I want to promise you that all the struggles, all the fights, all the work, all the years you put in to making this country a better place, to helping our leaders understand that freedom and democracy are prerequisites to opportunity — I want you to know that this generation of Americans will not let that dream down,” he said.\nActor Jamie Foxx fired up the crowd by urging a new generation to fight for justice and name-checking a list of African American celebrities such as Will Smith, Jay-Z, Kanye West, Alicia Keys, and Kerry Washington.\nMaryland Democratic Gov. Martin O’Malley detailed societal challenges still remaining, calling for gun control, gay marriage and abolishing the death penalty among other liberal priorities.\n“Yes, thanks to Dr. King, America’s best days are still ahead of us,” said O’Malley, who could mount a presidential bid in 2016. Drawing on some language featured in civil rights ballads, he continued, “Love remains the strongest power in our country. Forward, we shall walk hand in hand, and in this great work, we are not afraid.”\nLabor and interfaith figures as well as leaders in the African American community were also featured prominently during the program, with singers and other performers interspersed throughout the day.\n”Dr. King said 1963 was not an end but a new beginning,” said Rep. Marcia Fudge (D-Ohio), the chair of the Congressional Black Caucus. “Let us make today the start of a new chapter of history in this country. Let us march forward toward justice together.”\nFor Linda Nixon Haughton, whose sister was a civil rights activist with the Freedom Riders, the chance to see Obama speak 50 years after the battles fought by King and other civil rights leaders was especially moving.\nShe said it was gratifying “to be here today to hear a black man, a black president and all the ancestors that went before him.”\nAt the ceremony, some event goers in the diverse crowd sported T-shirts featuring the faces of both Obama and King, along with shirts bearing the Obama campaign slogan, “Forward.”\nWednesday’s events cap days of reflection in the nation’s capital, where remembrances have played out everywhere from cable television specials to high-profile events.\nOn Saturday, several members of Congress and other officials addressed a large rally on the Mall, where the discussion often turned to gay rights and voting rights.\n“I think it’s significant because now we are celebrating the anniversary,” said Clifford Curtis, who attended the march 50 years ago. “But I think we have lost a lot of the gains that have come about. When you look at the gridlock in Congress and a lot of the legislation concerning voting and so forth, limiting the ability to vote for some people, I think that if we are not careful and we are not vigilant, there is a chance we may lose a lot of what we have gained.”\nFlorine Bell, who also was at the 1963 march, said despite feeling poorly, she was back to mark all that had been achieved in 50 years.\n“Today, I’ve been struggling,” said Bell, 82. “I don’t feel well, but I wanted to come back from Powhatan, Va. — that’s where I’m from — to come back to this commemoration. I’m so proud of what we have accomplished, ’cause you know, we have come a long way. But we still have a lot to do.”\nCharles Arterson, who saw King get arrested in Georgia in 1962, said the event highlighted the progress made since then.\n“He was leading a march and got arrested and they took him away, and I have that memory,” Arterson said. “I was a small boy. To live and to see this going forward after 50 years, how many people we’ve lost but how much change. That’s beautiful.”\nThe 50th anniversary events on Wednesday are hosted in part by the nonprofit King Center and a coalition of other social justice organizations.\nThe ceremony also drew some side activity: The anti-war Code Pink protesters were in attendance, objecting to possible military action in Syria, and a cluster of young, racially diverse marchers paraded through the mall bearing signs with messages on education, the DREAM Act, and chanting calls for citizenship.""]"	['<urn:uuid:a1017235-d8f0-4651-8f17-7d1fa78c0606>', '<urn:uuid:de546129-ace0-4eee-8dd5-2a4c0e1104b4>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T12:39:32.578853	31	148	2450
49	I'm interested in renewable energy - what's the power capacity of LEGO's new Virginia plant?	The facility will have an on-site 35-40 MW solar plant that will generate energy equivalent to powering approximately 10,000 American homes. The solar plant is designed to match the total annual energy requirements of the site.	['04/2023: The family-owned, globally active Danish LEGO Group broke ground in mid-April 2023 on its new carbon-neutral run factory in Chesterfield County near Richmond, Virginia. This is a US $1 billion investment in the company’s long-term growth globally and will help delight millions of children in the Americas with LEGO® play.\nOnce completed in 2025, the 340 acres site, the size of 260 American football fields, will have rooftop and ground solar panels and an on-site 35-40 MW solar plant, generating the equivalent of the energy needed to power approximately 10,000 American homes. The LEGO Group’s ambition is for the solar plant to match the total annual energy requirements of the site. It will use the latest energy-efficient production equipment while buildings and manufacturing processes are designed to minimize energy use.\nCarsten Rasmussen, Chief Operations Officer of the LEGO Group said: “We are absolutely delighted to mark the beginning of construction here in Virginia. We are grateful for the fruitful collaboration we have had with all partners across the Commonwealth of Virginia who share and support our ambition to build this factory.\nThe groundbreaking ceremony was attended by LEGO Group Chief Operations Officer, Carsten Rasmussen; LEGO Group Regional President, Americas Skip Kodak; along with top Virginia state officials and key partners.\nTo celebrate the groundbreaking, the LEGO Group revealed it will officially ramp up its community investments by providing over US $1 million to charitable organizations that support local children from disadvantaged backgrounds with learning through play programs.\nLandscape design inspired by children\nIn March, the LEGO Group invited 250 children to share their ideas for making the land around the factory buildings welcoming to animals, plants, and visitors. The children’s boundless creativity will inspire the final landscaping design in 2025.\nLocating factories close to major markets\nThe LEGO Group employs more than 3,000 people in the USA, operating over 100 LEGO stores across the USA, including 3 stores in Virginia. The factory investment in Virginia will expand the LEGO Group’s global manufacturing network which is designed to locate factories close to major markets. This allows the LEGO Group to respond to shifts in local demand, shorten the supply chain, and reduce its environmental impact. LEGO will open a new site in Vietnam by 2024 and is expanding existing factories worldwide.\nRecruiting 1,760 skilled employees, a diverse and inclusive workplace\nThe LEGO Group is currently recruiting up to 500 people to join a temporary packing facility due to open in the first half of 2024. Once the main factory is fully operational, it will employ 1,760 highly skilled workers responsible for operating state-of-the-art moulding, processing, and packing machinery. The company is participating in the Virginia Talent Accelerator Program to raise awareness in the local talent market of current and future roles. The LEGO Group is committed to providing safe, diverse, and inclusive workplaces where everyone feels welcome. The company also offers inclusive benefits such as extended paid parental leave: 26 weeks of leave at full pay for the primary caregiver and 8 weeks for the secondary caregiver to be decided by the parents.\nPhotos: ©2023 The LEGO Group, Denmark']	['<urn:uuid:8133e731-d66c-429e-adc0-2aa8e311d85e>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	15	36	519
50	bring your own device byod main security risks company network data protection	The main security risks of BYOD include potential malware propagation, increased attack surface through additional entry points, and the possibility of sensitive company data exfiltration. Users' personal devices can facilitate virus and worm transmission while allowing employees to download and leave with proprietary company data.	['Employee Personal Device Usage BYOD in the Workplace\nConsiderations for Implementation of BYOD Policies\nIf your organization’s employees are permitted to connect personal smartphones or tablets to the company network, implementing an effective Bring-Your-Own-Device (BYOD) policy is necessary to protect against malware, hackers, and data exfiltration. Users’ personal devices can facilitate the propagation of viruses and worms, expand the attack surface by increasing the number of potential entry points and allow their owners to download and leave with sensitive, proprietary company data.\nWithout a clearly defined BYOD policy, employees will not know the company’s expectations or what activities are prohibited. Employers also have some difficult decisions regarding exercising control over their employees’ personal devices and data.\nDefining usage parameters\nBefore you can begin to develop a BYOD policy, you’ll need to determine the extent to which your employees will be permitted to use their devices while connected to the company network. Some organizations have a separation between the visitor and internal Wi-Fi networks. Suppose your employees will only be allowed to use a visitor Wi-Fi connection that provides Internet access but not internal company resources. In that case, your BYOD policy could consist of a simple statement in the HR manual limiting the connection of personal devices to visitor Wi-Fi. If, on the other hand, employees will be allowed, or perhaps encouraged, to use their devices for work-related tasks, you’ll need a much more comprehensive policy.\nWhich devices are allowed?\nDepending on how employees will use their devices, your BYOD policy may need to include limitations on the types of authorized devices for connection to your network.\nWill your employees be using their portable devices to access company applications? Might there be compatibility issues relating to mobile device operating systems? If so, are they optimized for use with mobile devices? Will your employees only use their devices to make and receive work-related phone calls and access company webmail? Given that different devices have different operating systems and capabilities answering questions like these may help you decide which devices should be allowed to connect and which should not.\nDevice security requirements\nIf you allow personal devices to access internal resources, you’ll want to ensure that those devices are not infected with malware that could propagate within your environment. The policy should require that employees using their personal devices at work install and maintain antivirus/anti-malware software.\nPortable devices can be lost or stolen. Automatic screen lock functionality with a strong PIN or biometric authentication should be required, as should complex passwords and multifactor authentication for network access. Depending on your environment and the sensitivity of the data, there may be regulatory requirements to consider. You may also wish to consider other tools, such as VPN applications, to increase security.\nWill you offer support for personal devices?\nAnother question to consider when developing your policy is whether and to what extent your organization’s IT team will support personal devices. The answer may lead you to limit the use of those devices depending on the added burden on your staff.\nIf you elect to provide support for these devices and their users, you’ll need to establish clear parameters. A best practice would be to exclude hardware support altogether. Hardware issues should be resolved by vendors and others authorized to support personal devices so that no warranties are voided, and your organization cannot be held responsible for any physical damage done.\nWill your organization allow users to install proprietary software on their devices? If so, will you provide application support for those devices? What will you do if the user has installed other apps on the device that prevent the proprietary application from running? A recommended action would be to explain the problem and let the user know that it is up to them to uninstall the personal application to facilitate using their device to run the company app or use a different device. It is, after all, the user’s personal device. This fact can lead to a separate set of issues should a dispute arise.\nRemote device wiping and termination of employment\nAs previously pointed out, these are personal devices paid for and maintained by their users. Perhaps the most critical consideration when determining the extent to which personal device usage will be permitted is whether you intend to require users to provide you with remote wipe capabilities. From the users’ perspective, those willing to pay for a device then use it for the company’s benefit may strenuously object to a policy that could seemingly punish them for doing so.\nWhile the company webmail app may not store messages and attachments on the user’s device, other proprietary applications could download sensitive data onto these devices. Likewise, BYOD users may have the ability to download files from company servers. Your policy should spell out what data can be downloaded and stored.\nThere is always a possibility that a user will violate a policy concerning the storage of company data. Your BYOD policy should clearly state the potential consequences associated with such a violation. If applicable, this should include wiping the user’s device. This measure would clear the company data and wipe out all of the user’s personal information, possibly leading to heated disputes for reasons previously stated. There is another consideration as well. Many personal devices are set to run backups to the cloud automatically. Even if you could wipe the user’s device, what recourse would you have regarding the backed-up data?\nFinally, you’ll need to develop a policy regarding employee termination, whether voluntary or involuntary. Will you wipe their personal device? Will the employee be present to oversee the activity? What if the employee refuses to allow wiping of the device?\nAllowing your employees to connect personal devices to the company network could, depending on the level of access, require you to implement policies that may, under certain circumstances, result in the deletion of their personal data. The use of personal devices could also dramatically expand your company’s attack surface, increasing the chances of a breach. For these and other reasons outlined herein, decisions relating to personal device usage should only be made after careful consideration of the possible ramifications.']	['<urn:uuid:84696cb3-c99e-4828-8667-5bb21257e7f1>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	12	45	1026
51	What is the relationship between traditional cooking practices and air quality concerns in developing regions, and how do these cooking methods reflect local cultural adaptations?	Traditional cooking in these regions is done on adobe earth stoves called 'chula' which release smoke directly into living spaces without chimneys. While locals believe this smoke helps keep mosquitoes away and preserves thatched roofs, it contributes to indoor air pollution. The cooking practices are deeply integrated with local culture, involving the preparation of dishes like parota and various curries using freshly ground spices. However, these cooking methods pose serious health risks. Indoor air pollution from cooking and heating appliances can cause immediate health problems including respiratory issues, eye irritation, and headaches. Long-term exposure can lead to chronic conditions such as accelerated aging of lungs, decreased lung function, asthma, bronchitis, and potentially cancer. Children under 5 years are particularly vulnerable to acute respiratory infections from such exposure.	['Like other river deltas, Bangladesh is a very fertile place. There are no rocks, stones, boulders or pebbles. It’s all silt and mud washed down from the Himalayan mountains over millions of years. Most people farm. Though I saw people farming every day, I never ever saw a tractor. Of course they can’t afford them, or even the fuel for them. Cows used as plow pulling devices, on the other hand, eat weeds and straw for fuel. Before wearing out, they make replacements. And they don’t require spare parts shipped in from some place. The yellow flowers are mustard, grown for their oil. During the rainy season, these fields will grow rice.\nBeing a river delta, you can’t walk very far in any direction without hitting water: a river, canal, rice paddy, pond or lake. This is especially true during the monsoon rainy season, when it rains almost every day. All houses have a pond near them where dirt has been scooped out to raise the base of the house several feet to be above flooding. Likewise, every road has a canal dug out next to it for the same reason.\nThe water is teaming with fish, which (along with lentils) provides most of the protein for people. I rarely saw people using a hook and bait and a line except on the big rivers. Instead, they have about 6 other ways to catch fish.\nWeighted throw nets are one of the most common fishing tools. The weights are sewn in around the edges. As soon as they hit the water, they sink fast, trapping fish below between the muddy bottom and the net. Then they carefully pull the net closed with a drawstring. I have been told –though I don’t know how to confirm it--that it was such throw nets that the fishermen were using two thousand years ago in Jesus’ time. There is an account at the end of the Gospel of John of a stranger advising the fishermen to throw the net on the right side of the boat. They do, and the net is so full they can’t get it back into the boat. Didn’t I say that living in Bangladesh is like traveling back in time?\nThere is another kind of net that is normally left on the bottom of a canal. When they think fish are swimming over it, they raise it suddenly, scooping up the fish.\nThe kid who is spear fishing has to use some science to hit his underwater mark. Just as the giant water prism bends light, so does the water in the pond. It’s called refraction. If the kid aims right for a fish, the spear will pass over its head. He actually has to aim below the image of the fish, taking into account the angle of his line of sight, how deep the fish is, and so on.\nA clever way of catching fish is with traps. Kids make them out of bamboo with one way trap doors. The kids get together to splash the water and force the fish to try to escape into another rice paddy. The fish try to squeeze past the obstruction—which is of course the trap with the trap doors.\nI also learned how to catch fish with my bare hands. There was a pond right outside my house in Bangladesh. Friends would come over and swim in it. One guy splashed the water in front of him, then dove in. A few seconds later he came up with a small fish in his hand. My mind refused to believe what my eyes had just witnessed! Sure that he had a bag of fish somewhere down there, I demanded that he repeat the feat in another part of the pond, a part of my choosing. Once again he splashed the water in front of him, dove under, and this time he came up with a small fish in both hands! He insisted that his hands were as fast as lightning.\nI pestered him day after day. I wore him out. How had he had done it? Finally he told me. When he splashed water in front of him, the fish took evasive action because they thought the splash was a net descending to trap them. Remember that the ponds have been heavily fished with throw nets for a couple of thousand years and only the clever fish survived. One small kind of fish evades capture by diving straight down into the muddy bottom. With only a bit of its tail sticking out, there is a good chance the edge of the net will pass over without scooping him up. My friend would dive under and feel the bottom. When he touched a fish, it would betray its presence by wiggling and trying to bury itself even more. Being embedded in the mud, it was easy to grab with one hand. Mystery solved! By the way, that kind of fish never exceeded 3” and it was bony and not worth eating. However, our ducks (yes, there are ducks in Bangladesh) loved them, so we threw the small fish to them on shore. We were rewarded with lots of delicious duck eggs.\nThe word “bazaar” comes from South Asia. In Bengali it means “marketplace.” When I took this picture in a market place of this graceful chicken cage, woven out of bamboo strips. Because electricity (and therefore refrigeration) is rare and unreliable in this hot country, you always buy chicken live and clucking.\nI was intrigued at how easily people in Bangladesh could climb trees when there were no branches or other toeholds for the first thirty feet of trunk. You might think it’s easy to just shimmy up, but actually it’s very hard. They climb palm trees to tap the sweet sap of date palms that is boiled down to sugar, and they knock down still-green coconuts out of coconut palms. Green coconuts —called “dab”—have a liquid inside that is a perfect formulation for oral re-hydration solution for someone for someone whose stomach is in bad shape. Furthermore, I have been told by medical people that it is sterile and can be used as an intravenous (IV) drip.\nSo how do they climb up so easily? They loop rope around their ankles. It allows them to get the traction they need with their feet. Back in the U.S. as a teacher, I had a student who had grown up in South America. He said they use the same trick there.\nCooking in Bangladesh is done with an adobe earth stove, called a “chula.” The smoke comes right out into the room, no chimney. They say it keeps mosquitoes at bay and helps preserve the thatch roof that most houses have for several years. The woman here is cooking a “parota,” which is sort of a cross between a pancake and a flour tortilla.\nI get a little embarrassed when I reminisce about the food in Bangladesh. I didn’t know much about the country before going. I was going to sacrifice 3 Spartan years of my life to helping the starving masses. When I got there, I enjoyed the sensual pleasure of eating “Bangla kabar” (Bengali cooking) every day. My passion lead me to learn food and cooking vocabulary first, and those are the words I’ll never forget even though I don’t speak much Bengali anymore.\nFish, chicken and beef can all be made into a rich “torkare” (curry). To say that curry is a sort of stew does not do justice to it. It’s the harmony of the freshly ground spices that makes it so delicious. Turmeric, hot pepper, garlic, onions, coriander, cardamom…also spices like ginger and cinnamon that we associate with sweet baking all combine to stroke your taste buds. Even ordinary potatoes become a feast when curried, as does spicy lentil “dal” and stir fried vegetable “baji.” The price of spices is a current event you read about in Bangladeshi newspapers. A person would be considered to be virtually starving if they could not afford spice for their food. You eat lots of rice with the meal.\nThe word “chutney” comes from South Asia. In Bangladesh, it takes the form of spicy mango pickle. You might be able to find some in well-stocked grocery store.\nThere are lots of tropical fruits in Bangladesh,\ntoo, like mango, guava and jackfruit. The bananas were small and had one\ntwo hard seeds a little smaller than a cherry pit. But they had a better flavor\nand were sweeter than ones I buy in the grocery store now. I won’t even\ngo into all the “mishti” (sweets). I don’t usually eat deserts,\nbut I was helpless to resist Bengali sweets.', 'Sunday, 26 January 2014\nAIR POLLUTION AND HEALTH PROBLEMS\nAir is the mixture of gases that surround earth due to its gravitational pull. It is colourless, odourless and tasteless mixture that forms earth’s atmosphere. It contains Nitrogen (79.1%), Oxygen (20.0%), Carbon dioxide (0.03%) and traces of inert gases like argon, krypton, xenon, neon, helium, ammonia, ozone, water vapour and suspended particles (0.07%).\nWe need air to perform certain vital life functions. All human beings and animals breathe in Oxygen and breathe out Carbon dioxide. Plants breathe in Carbon dioxide and breathe out oxygen in the day time to manufacture their own food through Photosynthesis. But they follow the pattern of animals at the night. Without Air there would be no transmission of sound and radiation waves, no burning of fire and cycle of seasons.\nWhen the natural composition of air is altered to such an extent, so as to make it harmful for living creatures primarily due to human activities, air is said to be ‘polluted’. Any substance that causes pollution is called pollutant. Pollutants can be solid, liquid or gaseous substance.\nAir is polluted by natural ways of volcanic eruptions, forest fires, pollen from plants etc. However, nature has a way of balancing itself. Our concern here is man caused pollution which is looming largely as a threat to modern civilization. Pollution can be INDOOR as well as OUTDOOR:\nINDOOR AIR POLLUTION refers to the physical, chemical, and biological characteristics of air in the indoor environment within a home, building, or an institution or commercial facility. Indoor air pollution is a concern in the developed countries, where energy efficiency improvements sometimes make houses relatively airtight, reducing ventilation and raising pollutant levels. Indoor air problems can be subtle and do not always produce easily recognized impacts on health. Tobacco smoke, cooking and heating appliances, and vapors from building materials, paints, furniture, etc. cause pollution inside buildings.\nVolatile organic compounds originate mainly from solvents and the main indoor sources are perfumes, hair sprays, furniture polish, glues, air fresheners, moth repellents, wood preservatives, and many other products used in the house. Their main health effect is the imitation of the eye, nose and throat. In more severe cases there may be headaches, nausea and loss of coordination. In the long term, some of the pollutants are suspected to damage to the liver and other parts of the body.\nTobacco smoke generates a wide range of harmful chemicals and is known to cause cancer. It is well known that passive smoking causes a wide range of problems to the passive smoker (the person who is in the same room with a smoker and is not himself/herself a smoker) ranging from burning eyes, nose, and throat irritation to cancer, bronchitis, severe asthma, and a decrease in lung function.\nBiological pollutants include pollen from plants, mite, hair from pets, fungi, parasites, and some bacteria. Most of them are allergens and can cause asthma, hay fever, and other allergic diseases.\nFormaldehyde is a gas that comes mainly from carpets, particle boards, and insulation foam. It causes irritation to the eyes and nose and may cause allergies in some people.\nAsbestos is mainly a concern because it is suspected to cause cancer.\nRadon is a gas that is emitted naturally by the soil. Due to modern houses having poor ventilation, it is confined inside the house causing harm to the dwellers.\nOUTDOOR AIR POLLUTION is the pollution that occurs outside homes through automobiles, industrial emissions etc.\nMAIN AIR POLLUTANTS AND THEIR IMPACT ON HEALTH\n1) Oxides of sulphur 2) Oxides of Nitrogen 3) Suspended Particulate Matter 4) Carbon Monoxide 5) Lead 6) Benzene 7) Hydrocarbons\nHealth impacts of air pollution depend on pollutant type, its concentration in air, interaction with other pollutants, and length of exposure and individual vulnerability.\nAir Pollution has both acute and chronic effects on Human health. It has both long term and short term effects. It may range from minor irritation to most chronic respiratory problems.\nIMMEDIATE HEALTH PROBLEMS FROM AIR POLLUTION\nØ Aggravated cardiovascular and respiratory problems\nØ Burden on heart and lungs, causing them to work harder to supply the body with oxygen\nØ Damage the cells in respiratory organs\nØ Damage to deeper portions of lungs, even after symptoms of coughing or sore throat disappear\nØ Wheezing, chest pain, dry throat, headache or nausea\nØ Increased reactivity to allergens and particles\nØ Eye irritation\nØ Reduced body immunity to infection and increased fatigue\nLONG TERM EFFECTS OF AIR POLLUTION AS PERMANENT HEALTH PROBLEMS:\nØ Accelerated ageing of lungs and loss of lung capacity\nØ Decreased lung function\nØ Diseases like, asthma, bronchitis, emphysema and cancer\nØ Is a potential cause of human mortality\n1. Acute respiratory infections (ARI) in children: ARI in children under 5 years in\ncause: 13% of deaths; 11% of NBD; 24% of NBD for children under 5 years\n(NBD- National Burden of Disease) India\nARI is the largest single disease category for\n, accounting for about\none-ninth of the national burden. For the world as a whole, ARI is\nalso the largest category, accounting for about 8.5% of the global\nAcute respiratory infections as pneumonia, is one of the chief killers of children in Developed Countries. It is well known to be enhanced by exposure to urban air pollutants and indoor environmental tobacco smoke at levels of pollution that are some 10-30 times less than that typically found in villages.\nA recent study of 642 infants conducted in urban slums of\n. The incidence of acute respiratory\ninfection was more in highly polluted areas than less polluted areas. In New Delhi , children\nhave to bear the double the burden of diseases that have persisted for\ngenerations as well as of new diseases caused by various environmental factors. India\nAsthma: Asthma causes: 0.2% of deaths; 0.5% of NBD in\nAlthough Asthma rates are officially low in India , there is some recent\nevidence that the true prevalence is higher than previously thought.\nAssociated with urban outdoor pollution and ETS (Environmental Tobacco smoke),\ntypical solid-fuel indoor smoke exposures are much higher.\nUndoubtedly, the rates of Asthamatic patients have been increasing; it might be\ndue to increase in the environmental pollution.\n2. Chronic obstructive pulmonary disease: Today in developed countries, nearly all cases of COPD are attributable to tobacco smoking. Undoubtedly, smoking is also a significant factor in COPD incidence among LDC (Less Developed Countries) men. In\n, even though\nrelatively few rural women smoked during the past decades, COPD in\nrural women today is not uncommon. Chronic obstructive lung disease, for which\ntobacco smoking is the major risk factor remaining in the developing countries\nis known to be an outcome of excessive exposure to air pollution. India\n3. Cancer: There are many chemicals in biomass smoke, which are carcinogenic in nature. In recent study in Japan on the other hand, found that women aged 30 years old cooking with wood fuel have an 80% increased chance of having lung cancer in later life.\nhas a larger\nfraction of its national burden of disease attributable to TB than any other\nregion, although the actual risk per person is less than that in\nSub-Saharan Africa. A large scale survey in India reported that women using bio\nfuels were three times more likely to have tuberculosis than women using\ncleaner fuels. India\nhas a larger burden of\nblindness than any other major region of the world. Indeed, globally, one out\nof three cataracts occurs in India where they are responsible\nfor 80% of blindness in the country. One case-control study in India found an excess\ncataract risk of about 80% among people using biomass fuel. Delhi\nThe Health Information of India reports show that environmental reasons are increasingly responsible for increased mortality in women and children (see graphs: What kills India’s children). According to the report, 55 percent of child mortality in\nis due to conditions originating in the prenatal period. A significant\nproportion of the tables are strongly related to environmental causes. India']	['<urn:uuid:fc75e330-1986-4817-9a13-e1cf2fe26514>', '<urn:uuid:942ceabb-30d0-410d-a36e-2802ecd24954>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	25	127	2789
52	What makes Kona coffee so special?	Kona coffee is one of the most expensive coffees in the world because it can only come from the Kona Districts in Hawaii. It is grown in mineral-rich volcanic soil with ideal sunshine and rainfall conditions, producing a mild and sweet coffee with a hint of spice. The coffee must be hand-harvested since the beans don't ripen simultaneously, ensuring perfect quality.	['Coffee was introduced in Brazil by Francisco de Mello Palheta in 1727 from Cayenne, French Guiana. Today, Brazil is the world’s largest coffee producer and is becoming a significant player in the specialty coffee industry. Bourbon, Typica, Caturra, and Mundo Novo coffee varietals are grown in the states of Paraná, Espirito Santos, São Paulo, Minas Gerais, and Bahia. Bourbon and Typica are the two most popular Arabica coffees grown worldwide, Typica being the base plant of many coffee varietals. The highest grade of well-known Brazilian beans, Bourbon Santos is a coffee to savor at any time of day. It is smooth and pleasant with fruity notes, medium body and mild flavor. Next time you buy coffee, check the origin of the beans. It’s quite possible they will be Brazilian.\nOne of the great things about Idaho Springs was its small town atmosphere of shops lining Main Street and people visiting on street corners, without some of the provincial thinking that could make a small town oppressive. Every community, be it big or small, had its characters, the people who inspired the “only in …” phrase. Idaho Springs embraced their citizens who teetered on the ragged edge of sanity, certain that without them, the town simply wouldn’t be right. There was the man whose name nobody was sure of, because he tended to claim to be someone different every week. This week he was Teddy Roosevelt, sporting a very dapper cowboy hat with the right brim tacked up, and a pair of rounded specs with no glass in them. He loved reminiscing about San Juan Hill as he sipped his morning latte` at Java Mountain Roasters. Asta always gave him free biscotti if he promised to stay at his own table and let the other patrons come to him rather than the other way around. When Ricky stepped into the coffee shop she was immediately enveloped in the aromas of fresh, deeply roasted coffee, and the smile of one Stanton Christophersen, who was currently rapt in conversation with Teddy. Ricky ordered a Pumpkin Spice Breve Latte with whipped cream and a salted, caramel biscotti and sat down next to Teddy, across from the adorable Stan. He was handsome this morning. She sipped her drink, happy to revel the charms of these two interesting men.\nKona coffee is Coffea arabica which is cultivated on the slopes of Hualalai and Mauna Loa in the North and South Kona Districts of the Big Island of Hawaii. It is one of the most expensive coffees in the world because only coffee from the Kona Districts can be described as “Kona”. The heritage trees in the Kona districts have been developed over the past 175 years on the west side of Hawaii. The mix of sunshine and rainfall, combined with porous, mineral rich volcanic soil creates favorable coffee growing conditions and produces a coffee that is typically mild and sweet with a hint of spice.\nThe coffee plant was brought to the Kona district in 1828 by Reverend Samuel Ruggles, from Brazilian cuttings. The coffee market crashed in 1899, and big coffee plantations became a thing of the past, replaced by small family farms. The tradition of running family farms has continued throughout Kona to this day. Since the first introduction of coffee here, Hawaii has been the only state in the US to commercially grow it. According to the Kona Coffee Council, the rocky location and the fact that the coffee does not ripen all at the same time means the Kona trees cannot be mechanically harvested. Since they must inspect each bean as they are picked, you are assured a perfect cup of coffee, which was picked when ripe, and not a combination of immature or overripe beans. This excellent quality has made Kona coffee one of the most highly valued coffees in the world.\nCoffee was first introduced to Colombia in 1723, presumably by Jesuit priests that brought the seeds from Venezuela. The country produces about 12% of the coffee in the world, second only to Brazil. Colombian coffee is often regarded as some of the highest quality coffee in the world. Colombia has traditionally grown arabica beans although today Bourbon, Typica, Caturra, and Maragogype coffee varietals are cultivated. Its unique geography makes Colombia perfectly suited for producing a delicious, high quality brew. Arabica beans come from a species of coffee originally indigenous to the mountains of Yemen in the Arabian Peninsula, hence its name. It is also known as the “coffee shrub of Arabia”, “mountain coffee” or “arabica coffea”. Coffea arabica is believed to be the first species of coffee to be cultivated, being grown in southwest Arabia for well over 1,000 years. Gourmet coffees, such as Colombian coffee, are almost exclusively high-quality mild varieties of arabica coffee. The climate in Colombia has traditionally been hot and dry enough to grow this variety very successfully, although climate change in the last twenty years has caused coffee production to drop off some in the region. These beans are so popular, the next time you’re in a restaurant and order a cup of Joe, chances are you’ll be drinking Colombian. Freshly roasted Colombian coffee beans are rich in flavor, heavy bodied, have a bright acidity, and are intensely aromatic. So drink up – and enjoy!\nCoffee from different regions of the globe have distinct flavors. This is because the plant and its fruit are affected by the soil and surrounding environment. Traditionally, it is thought that coffee originated in Africa. From there it spread into Arabia and eventually Italy. Coffee beans are still grown in African and beans from this region are very distinctly flavored. Coffee beans that are grown in Kenya have a bold flavor that has an overtone of black currant. This berry flavor is very unique and gains fans from across the globe.\nEast African coffee beans, in particular, are loved for the intense body they yield, as well as their spicy acidity. These include beans from Ethiopia and Tanzania. If all you drink is Columbian coffee, you can change your game by sampling the unique flavor of African coffee.\nCaffè breve is an American variation of a latte: a milk-based espresso drink using steamed half-and-half instead of milk. The snows bring with them a desire for warmth and comfort making this a perfect cold-weather drink – richer and slightly less sweet than a latte. For a special treat to welcome the autumn snow, try a Pumpkin Spice Breve from Java Mountain Roasters. It’s delectable ~ a bit like pumpkin pie in a cup. Do you love the winter weather? This will add to the joy of the season. Do you not? A breve is a truly wonderful way to console yourself. Drink up – you deserve it.']	['<urn:uuid:7711a11f-bdd7-4f40-a1b7-a536e9e97c3c>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	6	61	1127
53	My hospital is trying to make our cleaning processes safer for workers - has any hospital found ways to clean floors without using chemicals?	Dignity Health purchased a water-only floor cleaning machine to reduce the use of and exposure to cleaning chemicals.	['How 5 hospitals set environmental purchasing priorities and saved millions\nBy Beth Eckl, Practice Greenhealth Environmental Purchasing program director\nYou’ve secured leadership support, built a team of colleagues who were successful in getting an environmental purchasing policy adopted, and now it’s time to select environmentally preferable products and services. So which contracts should you begin with? And how do you prioritize among hundreds of purchasing opportunities?\nSetting priorities enables organizations to focus on the highest value opportunities to drive sustainable development.\nSome choose to begin with a strong business case for targeting products based on economic factors, such as highest potential cost savings. Others start with social priorities, such as opportunities to buy local and build stronger communities. And other hospitals have begun with an environmental focus, such as waste reduction and buying products with safer chemicals to improve patient and staff health.\nPractice Greenhealth identified purchasing categories and issued challenges with tools and resources to guide members through each: Healthier Food, Smarter Purchasing, and Safer Chemicals. Hospitals and health systems can select one or more and utilize corresponding resources, metrics, attributes, lists, benchmarks and more to advance markets and reach their sustainability goals.\nAdvocate Health Care committed to several of Practice Greenhealth challenges, including the Safer Chemicals Healthy Interiors goals, and 60 percent of their furniture purchases are free of formaldehyde, PVC, antimicrobials, perfluorinated compounds and all flame retardants.\n- Product necessity: It’s important to determine whether or not a product is necessary in the first place. For example, Dartmouth Hitchcock saved more than $1.4 million annually by eliminating unnecessary surgical supplies, kit types, and processing costs.\n- Cost savings: Prioritize products that are more cost efficient to address organizational and departmental budget goals. Be sure to consider costs beyond the purchase price. For example, the purchase of energy-efficient ultra-low temperature freezers may save organizations money through reduced energy costs that more than offset the higher purchase price of traditional freezers. Refer to Practice Greenhealth environmentally preferable purchasing case studies for additional ideas and use the Greenhealth Cost of Ownership Calculator to assess costs.\n- Total spend by volume/dollar/supplier: Another prioritization option is targeting products with the highest spend or volume in order to leverage markets. According to a 2014 Trucost research briefing, on average, 60 percent of all business environmental impacts are embedded in the products and services they buy – their upstream supply chain. Using actual spend and supplier data can help determine product categories most responsible for environmental impacts.\nMany environmental considerations, such as waste reduction and water conservation, play a role in setting priorities for purchasing. Focus areas may vary by region. For example, many hospitals in California have targeted water conservation measures to address drought conditions.\n- Waste reduction: Reducing waste through procurement has been a priority for many health care organizations. In one year through reprocessing of single use medical devices, Virginia Mason Medical Center saved more than 13,000 pounds of waste.\nAnd Harborview Medical Center worked with its supplier to donate 325 old beds to economically developing countries instead of sending them to landfill.\n- Energy impacts: To address greenhouse gas emissions, make energy and fuel efficiency priorities.\nTo reduce energy use, Dignity Health purchased large pieces of Energy Star-rated equipment for its kitchen. And Virginia Mason Medical Center worked with their distributor to make deliveries more efficient by streamlining and maximizing space in delivery totes to achieve 20 percent improvement in delivery efficiency.\n- Concerning chemicals: Of course, patient and worker health and safety are a key issue for health care organizations. That’s why Hackensack University Medical Center phased out plastic items containing di-2-ethylhexyl phthalate (DEHP) from its NICU, except for a couple of products.\nDignity Health purchased a water-only floor cleaning machine to reduce the use of and exposure to cleaning chemicals.\nOperational considerations – such as ease of implementation, training requirements, what’s coming up in your contract calendar, availability of greener alternatives, and corporate goal alignment – may drive purchasing prioritization. Another factor may be if a product is linked to legislation, regulations, or international norms.\nReflect on your organization’s mission and core values when making purchasing decisions. You might choose to set priorities based on opportunities to impact local or regional sourcing and employment or seek to expand use of fair trade products or products not sourced in areas known for human trafficking or a part of conflict zones.\nTo establish purchasing priorities, make a list of the categories and key issues for your organization based on economic, social, operational, and environmental factors. Consider ranking these to assess your top opportunities, taking into account the needs of your own organization and input from stakeholders. Ultimately, setting environmentally preferable purchasing priorities will result in an integrated way to manage procurement opportunities.\nJoin Practice Greenhealth\nPractice Greenhealth is the health care sector’s go-to source for information, tools, data, resources, and expert technical support on sustainability initiatives that help hospitals and health systems meet their health, financial, and community goals.']	['<urn:uuid:9212d118-ae22-4aa3-ab93-01b931b5e835>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	24	18	829
54	im interested in sports psychology how do athletes in dangerous sports compare to others in terms of sensation seeking and emergency room visits	Athletes in high-risk sports show higher sensation seeking scores compared to those in low and medium-risk sports. Looking at emergency room data, football and hockey players have the highest injury rates when adjusted for participation, followed by cyclists with approximately 455,000 emergency room visits in 2017.	"[""Sensation Seeking and Spatial Ability in Athletes: an Evolutionary Account\nLanguages of publication\nThe aim of this study was threefold: (a) to examine sex differences in sensation seeking and spatial abilities in a sample of athlete students, (b) to explore whether measures of sensation seeking and spatial ability can be used to distinguish between athletes engaging in sports of different levels of risk, and (c) to explore the relationship between sensation seeking and spatial abilities in a sample of athlete students. A total of 201 students athletes engaged in sports of different levels of risk completed the Spatial relations test, Mental rotation test, and Zuckerman's Sensation Seeking Scale-V. Men scored higher than women in both measures of spatial abilities and on DIS, while women scored higher than men on ES. High-risk group had higher SSS and TAS scores than low- and medium- risk groups, and low-risk group had lower DIS scores than medium- and high-risk group, but there were no differences in spatial ability among athletes engaged in sports of different levels of risk. Spatial ability correlated with sensation seeking measures in men only. The results are discussed in terms of possible common biological background of these two sex-dimorphic traits.\n1 - 1 - 2009\n17 - 7 - 2009\n- Aluja A., Garcia L.F. Sensation seeking, sexual curiosity and testosterone in inmates. Neuropsychobiology, 2005. 51:28-33.[Crossref][PubMed]\n- Austin E.J., Manning J.T., McInroy K., Mathews, E. A preliminary investigation of the associations between personality, cognitive ability and digit ratio. Pers Indiv Differ, 2002. 33:1115-1124.[Crossref]\n- Ball I.L., Farnill D., Wangeman J.F. Sex and age differences in sensation seeking: some national comparisons. Brit J Psychol, 1984. 75: 257-265.[Crossref]\n- Bennett G. K., Seashore H. G., Wesman, A.G. Differential Aptitude Tests. New York: The Psychological Corporation, 1947.\n- Butkovic A., Bratko D. Generation and sex differences in sensation seeking: Results of the family study. Percept Motor Skill, 2003. 97: 965-970.[Crossref]\n- Chirivella E.C., Martinez L.M. The sensation of risk and motivational tendencies in sports: An empirical study. Pers Indiv Differ, 1994.16: 777-786.[Crossref]\n- Christiansen, K. Sex hormone-related variations of cognitive performance in Kung San hunter-gatherers of Namibia. Neuropsychobiology, 1993. 27: 97-107.[PubMed][Crossref]\n- Cohen-Bendahan C.C.C., Buitelaar J.K., van Goozen S.H.M., Orlebeke J.F., Cohen-Kettenis P.T. Is there an effect of prenatal testosterone on aggression and other behavioral traits? A study comparing same-sex and opposite-sex twin girls. Horm Behav, 2005. 47: 230-237.[Crossref]\n- Collaer M. L., Hines M. Human behavioral sex differences: A role for gonadal hormones during early development? Psychol Bull, 1995. 118: 55-107.[PubMed][Crossref]\n- Falter C. M, Arroyo M., Davis G. J. Testosterone: Activation or organization of spatial cognition? Biol Psychol, 2006. 73: 132-140.[Crossref][PubMed]\n- Faurie C., Pontier D., Raymond M. Student athletes claim to have more sexual partners than other students. Evol Hum Behav, 2004. 25: 1-8.[Crossref]\n- Fellmann, N. Hormonal and plasma volume alterations following endurance exercise. Sports Med, 1992. 13: 37-49.[PubMed][Crossref]\n- Fink B., Neave N., Laughton K., Manning J.T. Second to fourth digit ratio and sensation seeking. Pers Indiv Differ, 2006. 41: 1253-1262.[Crossref]\n- Geschwind N., Galaburda, A. Cerebral lateralization: Biological mechanisms, associations, and pathology. I. A hypothesis and a program for research. Arch Neurol, 1985. 42: 428-459.[Crossref][PubMed]\n- Gomà-i-Freixanet M. Personality profile of subjects engaged in high physical risk sports. Pers Indiv Differ, 1991. 12: 1087-1093.\n- Gomà-i-Freixanet M. Sensation seeking and participation in physical risk sports. In: R.M. Stelmack (Ed.), On the psychobiology of personality: Essays in honor of Marvin Zuckerman (pp. 185-199). Elsevier Science/Pergamon Press, 2004.\n- Gouchie C., Kimura D. The relationship between testosterone levels and cognitive ability patterns. Psychoneuroendocrino, 1991. 16: 323-334.[Crossref]\n- Hurt R. E., Brous C. W. Spatial visualization: Athletic skills and sex differences. Percept Motor Skill, 1986. 63: 163-168.\n- Jack S.J., Ronan K.R. Sensation seeking among high- and low-risk sports participants. Pers Indiv Differ, 1998. 25: 1063-1083.[Crossref]\n- Janowsky J.S, Oviatt S.K., Orwoll E.S. Testosterone influences spatial cognition in older men. Behav Neurosci, 1994. 108: 325-332.[PubMed][Crossref]\n- Karadi K., Szabo I., Szepesi T., Kallai J., Kovacs B. Sex differences on the hand mental rotation task for 9-yr-old children and young adults. Percept Motor Skill, 1999. 89: 969-872.\n- Kempel P., Gohlke B., Klempau J., Zinsberger P., Reuter M., Hennig J. Second-to-fourth digit length, testosterone and spatial ability. Intelligence, 2005. 33: 215-230.[Crossref]\n- Kimura, D. Sex and cognition. Cambridge, MA: The MIT Press, 2000.\n- Kimura D., Hampson, E. Cognitive pattern in men and women is influenced by fluctuations in sex hormones. Curr Dir Psychol Sci, 1994. 3: 57-61.[Crossref]\n- Linn M.C., Petersen A. C. Emergence and characterization of sex differences in spatial ability: a meta-analysis. Child Dev, 1985. 56: 1479-1498.[Crossref][PubMed]\n- Lord T.R., Garrison J. Comparing spatial abilities of collegiate athletes in different sports. Percept Motor Skill, 1998. 86: 1016-1018.[Crossref]\n- Lord T., Leonard B. Comparing scores on spatial-perception tests for intercollegiate athletes and nonathletes. Percept Motor Skill, 1997. 84: 299-306.[Crossref]\n- Macoby E.E., Jacklin C. N. The psychology of sex differences. Stanford, CA: Stanford University Press, 1974.\n- Manning J.T., Taylor R.P. 2nd to 4th digit ratio and male ability in sport: implications for sexual selection in humans. Evol Hum Behav, 2001. 22 61-69.\n- Ozel S., Larue J., Molinaro C. Relation between sport activity and mental rotation: Comparison of three groups of subjects. Percept Motor Skill, 2002. 95: 1141-1154.[Crossref]\n- Resnick S.M, Gottesman I.I, McGue M. Sensation seeking in opposite-sex twins: An effect of prenatal hormones? Behav Genet, 1993. 23: 323-329.[PubMed][Crossref]\n- Ridgeway D., Russell J.A. Reliability and validity of the Sensation Seeking Scale: psychometric problems in Form V. J Consult Clin Psych, 1980. 48: 662-664.[Crossref]\n- Sanders G., Sjodin M., de Chastelaine M. On the elusive nature of sex differences in cognition: hormonal influences contributing to within-sex variation. Arch Sex Behav, 2002. 31: 145-152.[Crossref]\n- Sanders G., Bereczkei T., Csatho A., Manning J. The ratio of 2nd to 4th finger predicts spatial ability in men but not women. Cortex, 2005. 41: 789-795.\n- Schroth M.L. A comparison of sensation seeking among different groups of athletes and nonathletes. Pers Indiv Differ, 1995. 18: 219-222.[Crossref]\n- Shute V.J, Pellegrino J.W., Hubert L., Reynolds R.W. The relationship between androgen levels and human spatial abilities. Bull Psychonom Soc, 1983. 21: 465-468.[Crossref]\n- Silverman I., Kastuk D., Choi J., Philips K. Testosterone levels and spatial ability in men. Psychoneuroendocrino, 1999. 24: 813-822.[Crossref]\n- Simpson J.A., Gangestad S.W., Christensen P.N., Leck K. Fluctuating asymmetry, sociosexuality, and intrasexual competitive tactics. J Pers Soc Psychol, 1999. 76: 159-172.[PubMed][Crossref]\n- Slanger E., Rudestam K.E. Motivation and Disinhibition in High Risk Sports: Sensation Seeking and Self-Efficacy. J Res Pers, 1997. 31: 355-374.[Crossref]\n- Tooby J., Cosmides L.C. The adapted mind: Evolutionary psychology and the generation of culture. New York: Oxford University Press, 1992.\n- Vandenberg S.G., Kuse A.R. Mental rotations, a group test of three-dimensional spatial visualization. Percept Motor Skill, 1978. 47: 599-604.[Crossref]\n- Voyer D., Voyer S., Bryden M.P. Magnitude of sex differences in spatial abilities: a meta-analysis and consideration of critical variables. Psychol Bull, 1995. 117: 250-270.[Crossref][PubMed]\n- Wagner A.M., Houlihan D.D. Sensation seeking and trait anxiety in hang-glider pilots and golfers. Pers Indiv Differ, 1994.16: 975-977.[Crossref]\n- Williams C.L., Meck W.H. The organizational effects of gonadal steroids on sexually dimorphic spatial ability. Psychoneuroendocrino, 1991. 16: 155-176.[Crossref]\n- Zarevski P., Marusic I., Zolotic S., Bunjevac T., Vukosav Z. Contribution of Arnett's inventory of sensation seeking and Zuckerman's sensation seeking scale to the differentiation of athletes engaged in high and low risk sports. Pers Indiv Differ, 1998. 25: 763-768.[Crossref]\n- Zuckerman M. Sensation seeking and sports. Pers Indiv Differ, 1983. 4: 285-293.[Crossref]\n- Zuckerman M. Behavioral expressions and biosocial bases of sensation seeking. New York: Cambridge University Press, 1994.\n- Zuckerman M., Eysenck S.B.G., Eysenck H.J. Sensation seeking in England and America: cross-cultural, age and sex comparisons. J Consult Clin Psych, 1978. 46: 139-149.[Crossref]\nPublication order reference"", 'It is summer, a time when many people who were sedentary all winter get outside and start being active again. While being active is great, many people become injured when they start playing new sports or ones they haven’t done in years. So, how safe are the various activities done by the typical weekend warrior?\nI am personally interested in this question because this summer I am bicycling across the U.S. When people hear a middle aged man is pedaling alone 3,000 miles the first question they ask is, “Is it safe?” Given the near misses and one crash on loose gravel I have had so far this trip my inclination is to say no. Nevertheless, personal experience is no substitute for looking at actual data.\nThe U.S. Consumer Product Safety Commission is the government agency that gathers data and makes rules to help reduce the risk of injury or death when using equipment like a bike, golf club or even treadmill. They estimate that total damage, death and injuries from using faulty consumer products costs the U.S. more than $1 trillion a year.\nThey track the specific types of injury from every kind of sport and make this available online via the National Electronic Injury Surveillance System (NEISS). This database not only tracks sports injuries, but also injuries from contact with any kind of consumer product from ATVs to workshop tools.\nThe NEISS system has tracked injuries since 1979. It counts only injuries that resulted in a person ending up in a hospital emergency room. Injuries that people take care of at home or leave untreated are not counted.\nThe NEISS releases data in aggregate form and also as individual case records without people’s name or geographic location. For example, one of the first injuries reported in 2017 is a 21 year old white man who was boxing with a friend while drunk and went to the ER because he was punched in the face.\nThe overall NEISS data show that bicycling is indeed dangerous. About 455,000 cyclists ended up in the emergency room in 2017. However, other sports and exercise activities have even more injuries. There were half a million ER visits because of injuries received while playing basketball. Going to the gym is the most dangerous with 526,000 people injured using exercise equipment. Maybe all the bike riders, basketball players and gym rats should try something safer like horseback riding since that group only had 49,000 ER visits.\nThe problem with using aggregate NEISS data to compare the dangers of various sports and activities is that participation rates vary. One reason few people are injured horseback riding is that relatively few people do this compared to the large number shooting hoops in their driveway or at the neighborhood playground.\nThis problem can be overcome by using the Bureau of Labor Statistics’ American Time Use Survey (ATUS), which provides information on how much of each day people spend exercising. The data show only about one in five Americans got exercise in a typical day. The ATUS list does not match perfectly with the injury data, but there is a large overlap.\nATUS shows the most common types of exercise are walking and using exercise equipment and weights, which I consider going to the gym. Using the gym figures as a baseline it is possible to compute a participation adjustment figure for each sport. For example, the data show 2.5 times more people go to the gym on a typical day than play basketball. This means basketball injury figures need to be increased by 2.5 times to make a fair comparison of being hurt in the gym versus being hurt on the court.\nWhat do the adjusted figures show? The most dangerous activity is no longer going to the gym. Instead, it is playing football. The second most dangerous activity is playing hockey. Unfortunately, for me bicycling stays in third place on the list.\nTable: Dangerousness of Various Activities Based on ER Visits\n|Activity||Original Rank||Adjusted Rank|\nThe adjustment is not perfect because we should also adjust for the amount of time spent in the activity since the longer people spend doing an activity the more time they have to be injured. Nevertheless, the message is clear for those of you thinking about taking your bike out for a spin on a lovely summer day. Be careful out there.']"	['<urn:uuid:f38c9263-d9ea-47bf-86ea-cfe4c47228f8>', '<urn:uuid:a8bf5210-38b8-4bef-83a8-278eb58bdc34>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	23	46	2029
55	price competition strategy retail market	Studies show that 20 percent of customers are price hounds who chase the lowest price, while 80 percent consider both price and other store offerings. It's recommended to keep prices competitive but avoid hasty discounting just to chase price-sensitive shoppers, as this can be counterproductive.	"['You need to make it as easy as possible for shoppers to get to your store. If your store is in an odd location, publish directions in your advertising and make sure your staff is prepared to provide clear directions over the phone.\nAdvertising: Do you have an in-store link to your outside advertising?\nIt’s amazing how many stores spend a bundle on advertising, while neglecting to ""connect the dots"" between their ads and their store. One easy way to do this is to display your ads on a sign board at the front of the shop. Use your logo in advertising, and as a consistent presence in your shop. Spotlight the items featured in your ad with special displays or signage. Be consistent with your advertising, so customers will come to recognize your advertising style.\nBasics: Are your shelves stocked with basics?\nBe sure always to stock the basics. Rain checks are no longer an acceptable replacement for out-of-stock items. You want the people who stop in for a quick purchase to return to your store—don’t turn them away with under-stocked shelves.\nBathrooms: Sigh. This really does matter!\nAre your bathrooms clean? Bathrooms are an important part of customer service. Do you keep them clean and well-lit? Are paper towels, soap, etc. always available? Do you have an employee frequently “inspect” them to assure they are well-maintained?\nCash/Wrap: Is your cash/wrap counter organized and free of clutter?\nThe cash/wrap is your customers’ last impression of your store. Keep it neat and representative of your store’s dedication to service and organization. Be mindful of open storage shelves that often become a haven for clutter. Keep office supplies neat and organized.\nCleanliness: Is your store clean?\nCleanliness should start in the parking lot, continue through your entrance and carry throughout your entire store. Remember that tidiness is an important indication of your store pride. Make cleaning and tidying your shop a daily activity.\nCompetitive (But Practical) Prices: Is your merchandise priced appropriately?\nStudies show that 20 percent of customers are true price hounds and will chase down the lowest price. The other 80 percent care about price, but also take into consideration what else the store has to offer. When pricing merchandise, don’t be distracted by the 20% who are price-hounds and lose sight of the 80% who are looking for total value. Keep your prices competitive, but don’t lock yourself into a habit of hasty discounting in order to chase fickle price shoppers.\nCustomer Mailing List: Do you ask customers if they’d like to enter their e-mail address on a mailing list?\nDon’t underestimate the importance of regular correspondence with customers. It’s not enough just to have a mailing list—you must continually use this resource and update it, providing your customers with meaningful information. At the cash/wrap, ask customers if they’re interested in joining your mailing list if they haven’t already.\nDisplays: Are they timely? Are they fun? Are they constantly changing?\nIf a display seems to be turning slowly and not generating a lot of interest, consider making some changes. Use creative signage to add interest, replace it with another display, couple it with faster turning products or move it to a different part of the store. Just make sure it’s eye-catching.\nFixtures: Do the fixtures showcase the merchandise?\nThe best fixtures highlight merchandise in a discreet way. Choose fixtures that are practical and durable, but won’t draw the attention away from the products with gimmicky colors or design.\nIn-Store Graphics and Signage: Are your graphics and signs clear? Are they colorful? Do they have a theme?\nGraphics and signage should never detract from or compete with the merchandise. Their goal should simply be to draw attention to various products, discounts and special offers. Make sure your signage communicates its message immediately. Customers…']"	['<urn:uuid:837f5f2e-87e9-4fe2-bb67-024ddb13ef4d>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	5	45	634
56	drink fermented milk morning or night better	Kefir can be taken at any time of day. There is no evidence that taking kefir on either a full or empty stomach affects the ability of the bacteria to colonize in your intestines. You can take it with or before breakfast, as a snack during the day, or late in the evening. You should choose whatever time best suits your needs and schedule.	['How Do I Know if my Kefir is Good?\nWhat is Kefir?\nKefir is a popular alternative remedy for digestive issues. The probiotics in kefir are good for balancing out the internal flora of the digestive system. Since the correct balance of bacterial life in the digestive tract is vital to proper working of the intestines, probiotics like kefir are a great choice for anyone suffering from irregularity.\nIt can take about two weeks of regular usage before the digestive system gets used to the probiotics in kefir. For best results, you should drink about a cup of kefir every day. Depending on your diet before you start using kefir, you might notice some stomach discomfort initially. This does not mean your kefir is not good, it just means that your body is trying to adjust.\nKefir can be taken at any time of day. There is no evidence that taking kefir on either a full or an empty stomach affects the ability of the bacteria to colonize in your intestines. This means that you can take kefir with or before your breakfast, as a snack during the day, or late in the evening. Drink your kefir at whatever time best suits your needs and your schedule.\nIs My Kefir Good to Drink?\nEven though it is fermented, kefir will go bad over time, just like any other dairy product. While the bacterial cultures in kefir are good for your digestive tract, once they overgrow the kefir is no longer drinkable. Since kefir is a relatively expensive product, it can be tempting not to throw it away when you become suspicious of its quality. The safe thing to do, though, is to throw it out if you think it is not good to drink anymore.\nFor people unfamiliar with kefir, it can be somewhat difficult to tell if it has gone bad. It has a different texture and smell from other milk products. Unlike how every gallon of milk is the same, one bottle of milk kefir will not have the same feel, smell, or taste as another. Each batch will be slightly different, especially if the kefir is homemade or craft. But there are some tell-tale signs that it is time to get rid of a bottle of kefir.\nKefir naturally has a thick texture and may have chunks in it while still drinkable. It should look and feel like buttermilk. If the whole container has gotten chunky, however, the kefir is no longer safe to drink. It might also look like water mixed with larger, more solid chunks, which means it has gone bad.\nAnother obvious sign that kefir is no longer good is if there is mold growing on it. If there is anything fuzzy on top of the kefir, throw it out. Also, kefir should be generally the same color throughout the mixture. If there are patches of another color, especially one that contrasts with the color of the ingredients, that is another sign of mold infestation.\nWhen you first open a bottle of kefir, it might smell a bit stronger than you expect. It may even have a sour or vinegary smell. This is normal. But if the smell becomes harsh or is much stronger than when the carton was initially opened, it is safer to let it go.\nThe same is not true for taste, however. Just because it tastes too strong to comfortably drink does not mean it has become unsafe. In the absence of other signs that it has gone bad, sour kefir can still be consumed. Try using it in a recipe, such as biscuits or pancakes, where the sour flavor will fit in.\nKefir purchased at a regular grocery store, rather than directly from a producer, will have a date on it. You can use this date to determine if your kefir is still good, but it is more of an indication as to the optimal time frame for consumption, not an absolute measure of safety.\nHow to Store Kefir\nIf left at room temperature, kefir will only keep for about a day or two. If kept in the refrigerator at around 40- or 50-degrees F, it will remain drinkable for one to two weeks.\nIf you really want your kefir to last longer, such as if you bought or made a large batch at once, you will want to freeze it. Freezing kefir does not kill the healthy bacteria in it, it just puts them into a suspended state. Frozen kefir can be kept for months. Once you are ready to eat it, transfer the container to the refrigerator and allow it to defrost. It will take about 24 hours before the defrosted kefir is ready to eat.\nIf you are making your own kefir, you will need to ferment it at room temperature. However, once the fermentation is complete, the kefir should be stored at refrigerator temperatures for it to last longer. Store-bought kefir should always be kept in the fridge. The travel time between manufacturer, store, and your home is long enough that room-temperature kefir would already have gone bad.']	['<urn:uuid:86994d7b-6c08-482c-aee4-14d0019b6ebc>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	7	64	853
57	How do modern technology startups' customer-provider connection platforms compare with traditional employee-employer engagement approaches in terms of building successful relationships?	Technology startups like AirGarage and Tock focus on creating immediate, direct connections between providers and consumers through P2P platforms that manage multiple aspects of their relationship, including payments, rules enforcement, and customer support. In contrast, employee-employer engagement approaches emphasize building longer-term relationships through continuous communication, feedback loops, and development opportunities. While both aim to create successful relationships, the startup approach prioritizes transaction efficiency and immediate problem-solving, whereas employee engagement focuses on developing lasting connections that drive retention and productivity. Research shows that 87% of HR leaders prioritize retention through relationship building, which requires ongoing communication, direction, and coaching rather than just facilitating transactions.	['Startups are often recognized for driving innovation, with technology as a catalyst for the creation of new products and services. Founders sometimes invent new tech or make it more viable for mass-market adoption. Other startups find novel techniques to bring existing technology and market gaps together to fulfill an unmet need.\nWith a dramatic shift in how consumers communicate with companies online, digitization of products and services is accelerating. As a result, tech startups are changing the ways people can use technology to manage everyday tasks. These tasks range from keeping kids safe online to finding an available parking space. Startups are also shifting the focus of tech for entire industries and business disciplines like human resources.\nStartups are even harnessing technology to tackle global problems such as climate change. These companies’ innovations are creating the future of tech. Here are a few ways startups are changing how the world can use and benefit from technology.\nKeeping Kids Safe Online\nExposure to technology and the internet is happening at a younger age. Tech has become the backbone of education and curriculum. Children are also exposed to gadgets and online experiences at home and through peers. And while there are many advantages to kids using technology, there’s also the concern of exposure to inappropriate content.\nTech startups are aiming to remove many of the online threats kids face. These startups are introducing products like phones for kids that restrict online activity. Instead of parents having to research and install parental control apps, these phones come preconfigured with just the basics. There aren’t any games or social media apps on them, and internet access isn’t enabled. Parents can give kids their first phones without worry while teaching them responsible technology use.\nThis same concept is also being applied to tablets and laptops that kids have access to at school. But it’s applied using wide-scale methods. For instance, K-12 school districts are already required by law in some states to use internet filters. These filters block access to inappropriate content on all devices connected to a district’s network.\nIn-house IT departments can install other enterprise solutions that help teachers block access to certain sites filters don’t restrict. Teachers can add to or modify approved and restricted sites such as Facebook as needed. These monitoring solutions let teachers see students’ activities during class and shut down or lock individual machines. Teachers can get an individual view of which students may need further guidance with appropriate tech use.\nBringing Providers and Consumers Together\nParking is big business. The parking lot and garage industry is worth more than $9.1 billion. So, you’d think that there are plenty of parking spots to meet demand. But have you ever tried to find a parking spot in the city or near a venue and come up empty? Or circled around the block hoping someone will leave? It’s a common and frustrating problem for many. It’s one that a tech startup called AirGarage is solving.\nThe startup created a peer-to-peer (P2P) platform that connects people and organizations that provide parking spaces. Individuals can sign in to the application and see where unused parking spaces are in their areas. If they happen to be near a downtown civic center in one city, the app will show available spaces. When people move on to the next venue, they can locate unused spaces closest to that location.\nAirGarage’s app goes beyond revealing where people can find parking. It also manages the payment process, makes sure visitors follow parking rules, and has a customer support department. The P2P app serves as the intermediary between those looking for paid parking and those who supply them.\nAnother tech startup that’s solving an everyday problem by bringing providers and consumers together is Tock. The company launched an app that connects restaurants with people looking for dine-in or takeout meals. You can log into the app and put down a deposit to reserve a table at your favorite restaurant. There’s no more calling around to see who has room for reservations on Valentine’s Day. Or walking in multiple places only to find out there is an hour wait for a table.\nTock also charges restaurants lower fees to provide take-out service through its platform. When restaurant owners sign up with the service, they get to cater to both sides of the business. Customers who want a dine-in experience can get one. Those who prefer to pick up and go or have their meals delivered also get what they need.\nHelping Industries Overcome Challenges\nEmployee retention and talent development are becoming increasingly difficult challenges for organizations. Recent research shows that 87% of HR leaders want to make retention their top priority. But 20% find retention hard to prioritize because of competing distractions. Fostering solid relationships between employees, their direct managers, and the company is one way to encourage workers to stay put.\nGood relationships require communication, direction, and coaching. Tech startup Numly invented an employee engagement app that helps organizations develop better feedback loops with staff. The application uses AI technology to boost performance and productivity through a combination of mentoring and digital courses.\nThe app can identify individual skill sets and deliver a curriculum targeted to functional knowledge, such as sales. Employees can also access experts and provide coaching to their peers. Remote and in-person workers can engage with each other, reducing the isolation that can happen between departments and locations.\nThe app extends the involvement and responsibility for engagement and skill development beyond HR and managers’ purviews. Everyone is an active participant in their own learning process. They also get to help others achieve their professional goals, all while helping the organization meet its objectives. A culture of learning and collaboration is built on AI’s capabilities of revealing what the human eye can’t.\nStartups are shifting technology toward providing innovative solutions to some of the issues tech itself creates. These innovators also use technology to solve problems stemming from human nature and build bridges that humans can’t create alone.', 'What is Employee Engagement?\nEmployee engagement is a measure of how committed and involved an employee is at work, and is a key part of the employee’s relationship with the organization they work for. The number one driver of engagement is when an employee feels like their manager and leaders care about them.\nBut engagement is not the same as employee satisfaction, which is a measure of how content an employee is at work and with their organization. This term has become a common area of interest for organizations as it relates to many other important factors to business success, including employee retention, productivity, and even workplace safety.\nWhy Employee Engagement Matters\nEmployee engagement is important for many reasons. Unfortunately, most organizations misunderstand its importance and focus solely on engagement when they should also focus on things like employee access, employee experience, and employee satisfaction. These three other factors are key influences on how engaged an employee is at work.\nBut employee engagement matters because it indicates when an employee is actively engaged in their work or workplace culture. Although it is equally as important to focus on an employee’s job performance, engaged employees also actively participate in company culture and help drive culture. Research from Gallup has also shown that it is tied to many other critical business outcomes that clearly demonstrate the return on investment (ROI) of increased engagement.\nGallup Research on Employee Engagement\nGallup has conducted extensive research on this topic and its impacts on a company. In Gallup’s Employee Engagement Meta-Analysis, they found that when engagement improves, this results in improved employee performance. They found that better engagement led to 41% lower absenteeism, 17% higher productivity, 58% fewer patient safety incidents, and 70% fewer safety incidents. Improved employee experience also leads to an increase in quality with 40% fewer defects and 10% higher customer ratings. Employee engagement is positively correlated with profit as well with 21% higher profitability and 20% higher sales when employees are engaged. And highly engaged employees turnover less often.\nHow Do You Measure Employee Engagement?\nMeasuring employee engagement is often a difficult task because there are many different definitions of what it is. Many organizations use an annual employee engagement survey to measure engagement. These surveys are often composed of questions that help the organization measure eNPS (or employee net promoter score), employee satisfaction, and employee commitment to the organization.\nBut there are more ways to measure engagement. Many internal communications professionals will track engagement on internal messages. This includes tracking likes, comments, social shares, and general channel usage (e.g. logins and session duration). This helps understand which employees are using tools regularly and who are willing to go out of their way to engage on a message. That said, just because an employee doesn’t engage with content does not necessarily mean they are disengaged employees. Measuring content engagement is just one input that can be analyzed when assessing employee engagement.\nHow Do You Improve Employee Engagement?\nImproving staff engagement requires a strategy that looks beyond vanity metrics like content engagement. To improve this metric, you have to do more than improve content quality. A common trend in the workplace is to look to improve culture and engage employees with things like pool tables, events, or other workplace perks. But the truth is, that improving engagement requires holistically improving the employee experience. Here’s why.\nWhen you improve employee experience, you increase employee satisfaction. When employees are satisfied and have a supportive, inclusive, and positive work experience, employee engagement will go up.\nSo, how do you improve the employee experience? It starts with an employee listening strategy to collect employee feedback through things like employee surveys or focus groups. We also recommend measuring regularly and conducting employee engagement surveys to keep a finger on the pulse at your organization. To improve employee experience, you need to go beyond wants like fun workplace perks and focus on employee needs. What tools or information are they lacking to get their jobs done? Are their managers effective communicators and coaches? Are you providing them with a basic living wage and benefits package? Do employees feel connected to one another and respected? Tackling some of these basic needs are key to improving both employee experience and engagement.\nHow is Employee Engagement Tied to Employee Experience?\nEmployee engagement is intrinsically linked to employee experience. If employee experience is poor at your organization, it is likely that engagement will also be low.\nEmployee Engagement Trends\nEmployee engagement has remained fairly consistent over the last decade. Thirty years ago, 70% of employees were not engaged at work. In 2019, that number had barely changed; 69% of employees were actively disengaged.\nDuring the 2020 COVID-19 Pandemic, however, we did see some slight increases in engagement. Despite some gains in workplace engagement, there are additional challenges we tracked in 2020. With 61% of American workers working remotely full time in 2020, this increased the need for connection, collaboration, and communication. And we see these same wants and needs among the deskless workforce.\nTo increase these three key areas of an employee’s experience will require a strong partnership between internal communications and human resources. It will also require a need to improve frontline manager communication by helping them to conduct daily or weekly team huddles (also called: team standups, pre-shift meetings, post-shift meetings). And internal communications professionals will need to focus on focus groups to keep a finger on the pulse of the employee body.\nAnother key employee engagement trend will be the importance of senior leaders to be transparent, authentic, and open. And this will create a great opportunity for internal communications professionals to demonstrate to executive leadership that they are strategic advisors.\nAnd last, one of the biggest trends is the importance of building an amazing workplace culture. Often high performing employees churn because of dysfunctional cultures and poor management, and great cultures attract top talent.\nHow Effective Communication Boosts Employee Engagement\nWhen an organization has effective internal communication, there is a positive impact on employee engagement. This is because an all-inclusive communication strategy gives all employees access to the information they need to do their job and feel supported. Without access to communication, or the ability to comment or provide feedback, it is difficult for employees to feel included and, therefore, to engage.\nHow Does theEMPLOYEEapp Help Improve Employee Engagement?\ntheEMPLOYEEapp improves employee engagement by giving all employees—from the office to the frontline—access to communication, resources, and senior leadership.\nOur mobile internal communications app also gives administrators the ability to target and personalize content to specific employee groups, which makes content more relevant to the intended audience. When content is more relevant, an employee will feel more included and important to the organization, in addition to feeling like the organization understands their role, their needs, and their wants.\ntheEMPLOYEEapp also is set up with many ways to engage, including likes, comments, social sharing, survey integrations, an employee directory, and a calendar. This gives employees a way to ask questions, provide feedback, get in touch with the right people or departments, and participate in company events.\nLearn more about how we can help:']	['<urn:uuid:01a5ff43-0626-4cdd-983e-c297e4bcaa83>', '<urn:uuid:57a0d88c-48ba-4c63-937e-060fefc09097>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T12:39:32.578853	20	103	2194
58	I read that Germany was punished after WW1 - what did they have to pay?	Germany was required to pay approximately $30 billion in war reparations. This amount was later specified as £6.6 billion in 1921, though it was decreased by the Young Plan in 1929. These reparations were particularly harsh since Germany was already struggling with food shortages at the time.	"['Quiz: What Do You Know About the Treaty of Versailles?: HowStuffWorks\nWhat Do You Know About the Treaty of Versailles?\nBy: Tasha Moore\n6 Min Quiz\nImage: Wiki Commons by William Orpen\nAbout This Quiz\nThe Treaty of Versailles signaled the end of the World War I. If you already know that pivotal fact, go on to prove that you know all the intriguing details behind one of the most controversial peace treaties in the history of the world. Our history challenge answers the what, where and why of the Versailles Treaty all in one cool quiz.\nThere are pertinent facts you\'ll need to prepare you for your info journey. Know that some political historians have cited the 1919 Versailles Treaty as an example of poor international diplomacy. We explain the all shocking reasons why inside this quiz. Architects of the final treaty document deliberately put full pressure on Germany to pay for all that went awry because of World War I, the bloodiest war ever. U.S. President Woodrow Wilson devised the peace treaty in his Fourteen Points plan, which he fashioned as an instrument for world peace. Many provisions from Wilson\'s plan were drafted into the Versailles Treaty, but there was one major difference between the two world peace schemes. Wilson never intended to go so hard on Germany, which was one of four Central Powers that the Allied Powers—the United States, Britain, France and Russia—defeated. As the major player of the Central Powers, Germany was made to pay for everything.\nWitness the devastating deal Germany was served as a result of the Versailles Treaty. Scroll on, history awaits you!\nThe Treaty of Versailles marked the end of which major conflict?\nWorld War I, also known as the Great War, left three destroyed empires in its wake. Through the Treaty of Versailles, Allied nations rearranged territories of the major principality left standing among the vanquished, Germany.\nPixabay by skeeze\nWhere was the Treaty of Versailles signed?\nThe Palace of Versailles in Paris is the former residence of Marie Antoinette and Louis XVI. Marie Antoinette, an Austrian, moved to the 700-room French palace at the age of 17 to become queen. Angry mobs of French citizens stormed the palace during the French Revolution.\nWiki Commons by Ishvara7 at English Wikipedia\nCan you name the country that was forced to agree to pay war reparations at the end of World War I?\nAt the end of World War I, victorious Allies imposed punitive war reparations on defeated Germany. Severe penalties that Germany incurred as a result of the Treaty of Versailles triggered circumstances that led to World War II.\nWiki Commons by United Nations\nWhat failed group was the predecessor to the United Nations?\nThe League of Nations was formed after World War I to help ensure that countries would defend each other against aggression. Though U.S. President Woodrow Wilson was a key framer of the Treaty of Versailles, the United States never joined the League of Nations.\nWiki Commons by Evening News\nChoose the date that the treaty was signed.\nFighting during World War I ended on Nov. 11, 1918. Signed on June 28, 1919, the Treaty of Versailles was the official end the war, which is renowned for being one of the deadliest wars in history. Historic military tactics failed utterly in the face of modern technology like the machine gun, grenade and poison gas.\nWiki Commons by Bundesarchiv, Bild 183-L05487\nAlthough Germany conceded to treaty terms and agreed not to invade France again, when did Germany renege on the invasion stipulation after the 1919 treaty?\nIn 1940, Germany invaded France for the fifth time since 1814, before the two countries signed an armistice. Germany\'s Adolf Hitler insisted that the signing be held in the same railroad dining car in the Compiegne Forest, where Germany was made to sign the armistice that ended World War I.\nWiki Commons by Bundesarchiv, Bild 183-B24543\nIt is widely believed that harsh sanctions imposed on Germany as specified in the Treaty of Versailles led to the rise of which controversial German leader?\nThe Versailles Treaty imposed many harsh restrictions and penalties on Germany, including payment of war reparations. Ultimately, Germany withdrew from the League of Nations, as did its future World War II ally Italy. The League eventually disbanded when it could not prevent Hitler\'s rise.\nWiki Commons by Bundesarchiv, Bild 102-14439\nWho said: ""Support me, and I will right the wrongs of the Versailles Treaty""?\nFormer corporal of the German Army, Adolf Hitler, campaigned vigorously against the stipulations that the Treaty of Versailles imposed on Germany. Hitler\'s continuous protest among Germans, who were angered by the treaty, led to his success.\nWiki Commons by Underwood & Underwood/Corbis\nWhat Senate committee did Sen. Henry Cabot Lodge head?\nThough Lodge was chairman of the Foreign Relations Committee, President Woodrow Wilson, a Democrat, did not include the Republican in the U.S. delegation at the 1919 Paris Peace Conference. A Republican-controlled Senate further hindered Wilson\'s chances of ratifying the Versailles Treaty.\nWiki Commons by Burt Randolph Thomas, The Detroit News in Review of Reviews, Vol. 59, No. 6, pp. 570\nName the peace policy in which U.S. President Woodrow Wilson crafted key concepts of the Versailles Treaty.\nPresident Woodrow Wilson sought to end the prevalence of war through his Fourteen Points peace policy. Wilson\'s policy was a collection of terms that might prevent unchecked global aggression.\nWiki Commons by Bildarchiv Preußischer Kulturbesitz\nOn which date did the terms of the Versailles Treaty go into effect?\nThere\'d been concern that the German National Assembly wouldn\'t ratify the 1919 Treaty of Versailles, considering that there existed powerful German statesmen who staunchly opposed its terms. General enmity among German leaders and citizens did not dissipate, but festered during the interwar period.\nWiki Commons by Bundesarchiv, Bild 183-2004-1110-502\nWhat head of state represented France at the Paris Peace Conference in early 1919?\nGeorges Clemenceau, premier of France, David Lloyd George, prime minister of Great Britain, and U.S. President Woodrow Wilson dominated the conference of 1919. The Versailles Treaty that resulted was the first of five Paris peace treaties that reshaped the world after the Great War.\nWiki Commons by photographer not identified. British Government\nAt the 1919 Versailles Conference, which nation, besides France, sought to impose harsh penalties on Germany?\nPresident Woodrow Wilson\'s calls for mercy on behalf of Germany at the peace talks were no match for the diplomatic ""hawks"" in the room. Both Britain\'s Lloyd George and France\'s Georges Clemenceau pushed for the harshest stipulations possible.\nWiki Commons by Fibonacci\nThe Kingdom of Serbs, Croats and Slovenes eventually became ________.\nVersailles Conference members did not establish Yugoslavia or the first version of the entity, Kingdom of Serbs, Croats and Slovenes. Montenegro—a Balkan state that had recently won independence—strongly objected to new Yugoslavia.\nWiki Commons by Propaganda posters with unknown authors\nOf all four Central Powers nations, how much blame for World War I did the Versailles Treaty assign Germany?\nThough Germany was among other Central Powers nations, the Treaty of Versailles considered the major nation to be solely responsible for the First World War. The treaty aimed to cripple Germany militarily.\nWiki Commons by Martin Grandjean\nIdentify at least one provision of the Versailles Treaty.\nFormation of the League of Nations was a term included in the treaty. The League\'s single global mission was to end major conflicts among its members swiftly and with as little bloodshed as possible. The treaty was engineered to be the formal end to World War I and the beginning of world peace.\nWiki Commons by bulgarianhistory.org\nWhich of the following was one of the major triggers of World War I?\nThe struggle over a single Slavic principality was unanimously resolved at the end of the First World War. However, Western Versailles Conference members formally recognized the formation of the Kingdom of Serbs, Croats and Slovenes as a remedy.\nWiki Commons by The Library of Congress\nWhen did the United States ratify the Treaty of Versailles?\nThe United States never ratified the Treaty of Versailles, and therefore didn\'t join the League of Nations. A major issue among U.S. senators was the stipulation that the League, and not Congress, have the power to send U.S. troops to quell international conflicts.\nWiki Commons by National Photo Company Collection\nWho led the ""Reservationists\'"" crusade regarding U.S. approval of the Versailles Treaty?\nMost Democrats supported the Versailles Treaty, while Republicans were split. Sen. Lodge led the ""Reservationists"" among Republicans who desired the treaty\'s approval only if certain changes were adopted.\nWiki Commons by The Library of Congress\nWe\'ll be impressed if you know the date when President Woodrow Wilson presented the Treaty of Versailles to the U.S. Senate.\nPresident Woodrow Wilson\'s presentation was the first time since 1789 that a U.S. president delivered a treaty to the Senate for ratification in person. Republican members of Congress were reluctant to surrender the power of the legislative body to the will of the League of Nations.\nWiki Commons by QuartierLatin1968\nWhich of these is a territory that Germany was forced to forfeit in the Versailles treaty?\nGermany was forced to give up Rhineland, which was a demilitarized zone that the League of Nations supervised. Germany also gave up the Danzig Corridor, including the bustling port city of Danzig.\nWiki Commons by Underwood & Underwood, N. Y.\nWhich world leader conceived the League of Nations concept?\nAt the end of the bloodiest battle in human history, U.S. President Woodrow Wilson eagerly sought a peaceful resolution. To prevent such barbarity from happening in the future, his League would solve issues through ""cooperative diplomacy.""\nWiki Commons by Auckland War Memorial Museum\nDo you know when the final version of the treaty was completed?\nThe final version of the Treaty of Versailles included the controversial Article 231, better known as the ""war guilt clause."" Germany was compelled to accept responsibility, ethical and financial, for World War I, or continue fighting.\nWiki Commons by User:Mmxx\nWhat was postwar Germany\'s official title?\nGermany signed the Treaty of Versailles under the Weimar Republic, which made the governing body hugely unpopular among Germans who vehemently opposed the treaty\'s harsh provisions that devastated the nation\'s economy. Hitler\'s Nazi Party dissolved Weimar to institute the Third Reich in 1933.\nWiki Commons by Unknown German war photographer\nFrom 1814 to the end of World War I, how many times had Germany invaded France?\nThrough the Treaty of Versailles, France sought to set in place strict punishments on neighboring Germany to safeguard its shared border. Annexed in 1870, the region of Alsace-Lorraine was returned to France as stipulated by the treaty.\nWiki Commons by Leonard Raven-Hill\nDo you know the nation that was not a member of the League of Nations?\nUnited States senators who opposed the Versailles Treaty opposed President Woodrow Wilson\'s concept of ""collective security."" According to Wilson\'s plans for the League of Nations, member nations would lend military arsenals and troops in a joint effort if diplomacy ever failed.\nWiki Commons by Harris & Ewing, photographer\nIdentify the group of United States lawmakers who rejected the Treaty of Versailles outright.\n""Irreconcilables"" did not want the treaty at all. Sen. Henry Lodge, leader of the ""Reservationists,"" sent 14 reservations with the treaty to the Senate floor. Democrats, who had supported the treaty as delivered, rejected Lodge\'s scheme by siding with the Irreconcilables in a vote of 39 to 55.\nWiki Commons by Ernio48\nHow much land was Germany required to forfeit as specified by the Versailles Treaty?\nGermany returned approximately 27,027 square miles that it had acquired by the end of World War I. As a result, the German Empire shrank roughly 10%. Germany handed Prussia over to Poland, giving the country vital access to the Baltic Sea.\nWiki Commons by Propaganda posters with unknown authors\nWhat was the name of the World War I syndicate that the Allies defeated?\nMembers of the defeated Central Powers were Germany, the Ottoman Empire, Italy and Austria-Hungary. The Versailles Treaty recognized that Germany was the most powerful among the syndicate and penalized the nation accordingly. (The Axis Powers were the WWII group led by Germany.)\nWiki Commons by Zeete\nTell us the name of the piece of U.S. legislation that ended the conflict between the United States and the Central Powers.\nThe Knox-Porter Resolution did for the United States what the Treaty of Versailles could not, in that it officially brought closure to World War I for the nation. President Warren G. Harding, Woodrow Wilson\'s Republican successor, signed the resolution into law on July 2, 1921.\nWiki Commons by Vzb83\nWhich of these is a former colony of Germany?\nUnder Versailles Treaty provisions, Germany\'s colonies were redistributed. New Zealand took Samoa, Australia secured New Guinea, Britain seized Togoland and Tanganyika, Belgium won Ruanda-Urundi and France controlled Cameroon.\nChoose the international group that the Treaty of Versailles set up.\nThe International Labour Organization was established to fashion a widespread standard of global rights that included reasonable work conditions and basic protection for all people. The group has been a United Nations agency since the dissolution of the League of Nations.\nWiki Commons by Miscellaneous Items in High Demand, PPOC, Library of Congress\nHow long was Woodrow Wilson\'s presentation to the U.S. Senate regarding ratification of the Versailles Treaty?\nAfter he struggled to deliver his speech, President Wilson, who had just returned from representing the U.S. at the Paris Peace Conference, turned to Republicans and said, ""It has come about by no plan of our conceiving ... We cannot turn back."" Wilson\'s plea garnered faint applause.\nWiki Commons by Simon Q from United Kingdom\nWhich is the amount of money Germany was required to pay in war reparations?\nIn addition to land forfeitures, Germany was required to pay out approximately $30 billion in war reparations. The treaty also required Germany to reduce its army to 100,000 soldiers, and production of military weaponry was prohibited.\nWiki Commons by Another Believer\nTerritorial provisions of the Treaty of Versailles led to a number of major international events, including ________.\nChina\'s May Fourth university protest triggered the anti-imperialist Chinese Communist Party. During the protest, Chinese citizens reacted to the Allies denying the country land grabs and spoils of World War I although China was on the winning side.\nHow much do you know about dinosaurs? What is an octane rating? And how do you use a proper noun? Lucky for you, HowStuffWorks Play is here to help. Our award-winning website offers reliable, easy-to-understand explanations about how the world works. From fun quizzes that bring joy to your day, to compelling photography and fascinating lists, HowStuffWorks Play offers something for everyone. Sometimes we explain how stuff works, other times, we ask you, but we’re always exploring in the name of fun! Because learning is fun, so stick with us!\nGet smarter every day! Subscribe & get 1 quiz every week.\nPlaying quizzes is free! We send trivia questions and personality tests every week to your inbox. By clicking ""Sign Up"" you are agreeing to our\nand confirming that you are 13 years old or over.', ""The Mood in 1919\nAll countries thought that Germany should pay, except for the USA, because all these countries were exhausted from the war.\nThis was because millions of young men had died or been injured. The total amount of casualties of Britain and France was around 9 million.Many people had rationed food and medicine.\nAlmost every family in Britain had lost a member in the fighting.\nMany politicians knew they could get support in the 1918 elections by emphasising how harshly they would make Germany pay.\nThis tactic was then strengthened when people saw how harshly Germany made Russia pay in 1918 in the Brest-Litovsk where Russia lost huge amounts of land and 25% of its population.\nSo if Germany had won, people thought that would be the same demand from Germany to them.\nThe Aims of the Big Three\nThe Big Three were three members from Britain, France and the USA coming to the Paris Peace Conference as representatives of their country but each had different aims.\nClemenceua (France): Germany still seemed to be a threat to France becausem Over two thirds of French soldiers were killed or injured, damage had been done to the land and industry. Clemenceau saw the treaty as a chance to cripple Germany, the French President wanted Germany to be broken up into smaller states. Clemanceau was a realist and knew this wouldn't happen but still demanded Germany to be weakened as much as possible.\nWoodrow Wilson (USA): Wilson did not want Germany to be punished to harshly because he knew that some day it would recover and want revenge. His main aim was to strengthen democracy. He set up his fourteen points and his most important was the last which was to set up an international body called the League of Nations. He also believed in self determination, for Eastern Europe countries to not be ruled by Austria- Hungary empire.\nDavid Lloyd George (Britain): David Lloy George was the middle ground. He wanted Germany to lose it's navy and colonis as they were a threat to the British empire, however, he did not want Germany to seek revenge and wanted to begin trading again as it meant more jobs. But he had promised Britain he would treat them harshly even though he knew the dangers.\nThe End Terms of the Treaty\n1. WAR GUILT: Germany had to accept full responsibility of the war.\n2. REPARATIONS: The Allies said Germany would pay reparations without consulting Germany, the exact figure was decided in 1921, £6.6 billion, luckily, in 1929, this was then decreased by the Young Plan.\n3. TERRITORIES AND COLONIES: Germany's overseas empire was taken away because it was one of the causes for the bad relations between Britain and Germany before the war. The land was taken over by the League which meant Britain and France. Germany's borders were shortened and Germany could not become allies with Austria.\n4. GERMANY'S ARMED FORCES: German army was limited to 100,000 men, conscription was banned, armoured vehicles, submarines and aircrafts were not allowed, the navy could only build 6 battle ships and the Rhineland (the zone connecting Germany and France) was demiliterised.\n5. LEAGUE OF NATIONS: Germany could not join the League of Nations until it had shown itself to be a peace loving country.\nGerman Reactions to the Treaty\nGermany was shocked, this was mostly because they didn't feel they had lost the war as they didn't know how bad the military situation was. They thought they had agreed a ceasefire and had gone to Paris to negociate peace.\nEbert didn't agree to the Treaty but found he couldn't do anything else. Hindenburg,army leader, said they could not possibly win if they went to war again in disagreement, so, he accepted the terms in 28th of June, 1919.\nMany Germans hated the war guilt clause as they thought the guilt should be shared, and definitley did not think it should pay for all the damage caused by it. Also, these reparations made a bad situation worse as they were already without much food.\nGermany was the only country forced to disarm to Germany's extent which was obviously found to be extremely unfair.\nThe loss of territory was a blow to the economy and was mainly given to Britain and France.\nGermany also felt very insulted as they had not been invited to the League of Nations.\nBig Three Reactions with the Treaty and Overall Ju\nClemenceau: He thought it wasn't harsh enough and the French people agreed but blamed it on him so in the 1920 elections, he was voted out.\nLloyd George: He and Britain were very happy with the outcome at first. However, later on he thought the Treaty was 'a great pity' as he knew war would be caused because of it.\nWilson: he was very disappointed and said he would not have signed it if he was a German, all he hoped for was that it would still bring peace.\nOverall, we can see that the Treaty was extremely harsh and not entirely fair and many thoughts of the Germans were justified.\nHowever, one of the main reasons why people thought that it was is because of the Brest Litovsk which, as seen, was a lot harsher than the Treaty of Versailles and if Germany had won, Britain, France and the USA would have most likely been made to pay more than Germany did.\nAlso, Germany did not help itself because, instead of increasing the tax like many other countries, they planned to get the money from the defeated states.""]"	['<urn:uuid:8c4b0908-d635-4221-8439-a5bf3e2c98ba>', '<urn:uuid:05656847-7eb9-4559-bf81-3286050a1e92>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T12:39:32.578853	15	47	3446
59	I'm curious about unusual industrial materials - what was that special powder that was used both in fireworks and to coat medicines in the past?	Lycopodium powder was used in fireworks and explosives, as well as for coating pharmaceuticals. Interestingly, it was also used by Joseph Nicéphore Niépce to fuel the first internal combustion engine in the early 19th century, before he went on to invent photography.	['Hot metals and cool minimalism: the transformative processes of Raphael Hefti\nHot metals and cool minimalism: the transformative processes of Raphael Hefti\nMercury, magnesium, aluminium, titanium, brass, bronze, iron, copper, silver, gold: there is a richness of history and allegory imbued in the metals of the world. We like to tell our story – via myth, or scientific narratives – through metals because we understand them through their utilization and transformation by our desiring bodies, through our great and ungreat civilizations, by those hotly named metallurgists: consider the Bronze Age, the Iron Age, the Ancient Greeks’s blacksmith-god Hephaestus, the Telchines and Dactyls. Yet the depth of language and narrative (process, in other words) that surrounds metals and their technological and poetic employment is usually at odds with the apparent simplicity and discrete autonomy of their oft-manifested forms – as tools. (So deft and pointed that word.) Raphael Hefti’s strangely singular, subtle body of work – in which he grapples with the properties and processes of manufacturing (metal as well as glass and mineral) – likewise resides on the shifting plates of this paradox. His crush on protean industrial material processes, which he delicately perverts with an inventor’s recklessness, seems to belie the strangely refined aesthetic qualities of his resulting works; his sculptures and photograms can appear too beautiful and expressive by far to fit in with his more manically exacting scientific methodologies. If a certain aesthetics of form – coolly minimal, expertly attractive – is not Hefti’s goal, it remains the material fact by which his audience can trace back (or be led astray by) the story of the making.\nThis story of the Swiss artist’s practice – developed chiefly in the two cities in which he lives and operates, London and Zürich – is an inherently social one (an overused term, yet useful). While industrial processes are supposedly in the game of bettering society, furthering a luxury of living, Hefti’s manipulation of these processes, the seeming unusefulness of the material products he creates, is a bait-and-switch. The pleasure in deviating procedures and skilled communities is very much his endgame: the pleasure he takes in working with industrial technicians and digressing from their careful procedures, and the pleasure he takes in presenting them to an art audience as traces or repositories of this methodology – as art objects of circulation and reception, neither fetishized nor duly cast off. In the contemporary art world we speak easily of labour, of industry, of material, but most of us seldom actually go there. Hefti does, though. One rarely finds him alone in a room, neither the room of his own skills and procedures – he is often in factories across Europe, learning from technicians – nor alone in his studios, which are dependably filled with friends, peers and assistants (for whom he regularly cooks, another kind of transformative process).\nBut back to metal and mineral. Consider his exhibition at Nottingham Contemporary in the UK last year, which brought together recent series, each an experiment with material processes taken too far, delivered in almost outrageously elegant, essentialist form. Three enormous abstract photograms, each cut into three strips, hung from the ceiling, the photo paper curling slightly just above the floor. Bursts of pink, purple, pale violet, the photogram’s ‘image’ a kind of abstraction that we often describe as celestial – comets burning out, say, or black holes framed by stars. (Note here we reach for a metaphor to describe the very event being ‘photographed’: spores burned into another material, photo paper, as an image.) For this Lycopodium (2014) series, the photograms were made in pitch dark as Hefti lit the flammable moss spores of the titular powder – regularly used in fireworks and explosives, as well as for the coating of pharmaceuticals – over entire rolls of colour photosensitive paper. Lycopodium in particular strikes a nearly perfect history for Hefti to draw from: in the early 19th century, Joseph Nicéphore Niépce used the powder to fuel the first internal combustion engine; later he invented photography (View from the Window at Le Gras, from 1826, is his). But Hefti’s Lycopodium images also suggest something else: a collusion of postwar gestural colour field painting and recent abstract photography, aesthetic registers well understood by a contemporary art audience. Part of the strangeness of the artist’s work lies in this: our familiarity with the (high art, creative) aesthetic forms he creates, and our unfamiliarity with the (quotidian, industrial) material processes he uses to achieve them.\nNear the photograms was another series of works entited Etched totem (2014). Plates of cast aluminium, copper and zinc that Hefti ‘macro etched’ – a process of etching and polishing – revealing each metal’s singular make-up of crystals. Taking in the small plates’ material abstraction, I remembered that lycopodium powder was used in physics experiments to make sound waves and electrostatic charge visible for observation and measurement. Likewise here Hefti made visible the usually invisible crystals that compose the metal. Still, what was it we were seeing exactly? He has described it as manipulating the sculpture from the inside out, from its very materiality. Was this a story of fabrication estranged from use? Or a series of metal plates in a museum reminiscent of postwar Spanish and American abstract sculpture in all its torpor, pathos and machismo? The series of objects held these circling narratives (of transnational postwar industry, mass production, art and ideology) without quite offering them to you. This vagueness is troubling, irreducible, but might also be transformative.\nMore lucid are series such as the terminally gorgeous Luxar-coated glass panels of Subtraction as Addition (2012–14), which suggest reductionist versions of Marcel Duchamp’s The Large Glass (1915–23) created by overdoing the industrial formula for creating transparent glass and iridescent as oil slicks. So too a set of 150 one-metre-long poles of attenuated aluminium, copper, steel and titanium, screwed or welded together in lengths of six, with various strange and beautiful colourings occasioned by being heated to extremely high temperatures at various points along their slim length. Titled in a kind of deadpan approximation of the minimalism it suggested, Various threaded poles of determinate length potentially altering their determinacy (2014), the work strikes the balance of stylish seriality and industrial materiality that is Hefti’s calling card, but it also just works, beautifully. Still, the artist, who studied engineering in Switzerland before receiving his MFA from the Slade School of Fine Art, London, in 2011, works in less temperate registers too. Consider the car he accidentally blew up years ago during an Alpine experiment, for which he put on a terror watch list by the Swiss authorities, or the more carefully orchestrated explosions at SALTS, Basel, and Ancient & Modern, London, in 2013, when he filled the galleries with mountains of sand, donned protective clothing and a facemask, then melted steel to liquid, allowing it to run down the dunes like lava in front of flushed audiences, shaping it as it went: formed material loosened, then formed again.\nThis able movement between the outsized gesture and the intimate, local one can also be located in the artist’s permanent roof installation of coated glass panels commissioned by Fondation Vincent van Gogh, in Arles, in 2014, which turns the glass atrium below into a kind of kaleidoscopic cinema of dappled coloured light, as well as in his show at RaebervonStenglin, in Zürich, the same year, in which he situated approximately 30 tonnes of aluminium in its saleable format (huge blocks spray-painted with relevant technical and commercial information) in the gallery centre, priced at its exact value as raw material on the global marketplace. As in this last work, Hefti’s interest is in making evident and observable the processes – industrial, material, economic, commercial, aesthetic – on which our lives are predicated. More mysterious is the contest between material and form, process and use value, which seems to drive his practice forward. If his kind of minimalism does not simply privilege the object or the material but the material procedure itself, it’s this intervention of subjectivity and sociality – released from the responsibility of inherent objectives, to either material or humankind – that places his work on its firmest, strangest, most experimental ground.']	['<urn:uuid:e51ca339-00ec-488d-903a-cc1832462061>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	25	42	1366
60	help my kid safety medicine storage tips dangers accidents home	To prevent medicine accidents at home, keep all medicines out of children's reach in locked cabinets. Never leave medicines unattended, and always use child-resistant packaging. Store medicines away from sight, as they can look like candy to children. Additionally, keep the poison control number (1-800-222-1222) readily available. Medicine accidents are the #1 cause of emergency visits for children under 5, with an estimated 53,000 children visiting emergency departments yearly due to unsupervised ingestion.	"['For Parents: Young Children and Adverse Drug Events\nWhen someone has been harmed by a medicine, they have had an adverse drug event. To reduce the risk of harm from adverse drug events in young children, parents should:\n- Always recap and store medicines out of the reach of children\n- Carefully give medicines as directed on the label or as instructed by a physician or pharmacist\n- Use prescription and over-the-counter medicines only when needed.\nPre-school age children can be seriously harmed if they get into medicines when no adult is watching\n- Finding and eating or drinking medicines on their own, without adult supervision is the #1 cause of emergency visits for adverse drug events among children less than 5 years old. An estimated 53,000 children less than 5 years old are brought to emergency departments each year because of unsupervised ingestions.\nTips for parents:\n- Never leave children alone with medicines. If you are giving or taking medicine and you have to do something else, such as answer the phone, take the medicine with you.\n- Do not leave medicines out after using them. Store them in medicine cabinets or other childproof cabinets that young children cannot reach.\n- When purchasing medicines for young children, check to make sure they are in child-resistant packaging that you are comfortable using.\n- Put the poison control number, 1-800-222-1222, on or near every home telephone and save it on your cell phone.\nParents are more likely to make mistakes when giving medicines to infants and toddlers than to older children\n- For example, half of the mistakes leading to emergency visits from cough and cold medicines occur when giving medicines to infants and toddlers.\nTips for parents:\n- Read all of the information on the package label and follow the directions. Do not give a child medicine more often or in greater amounts than is stated on the package.\n- Use only the measuring device (dropper, dosing cup or dosing spoon) that is included with the product. A kitchen teaspoon or tablespoon is not a good measuring device for giving medicines to children. If a measuring device is not included with the product or you do not receive one, purchase one at a pharmacy or ask for one from your pharmacist.\n- Check the ""active ingredients"" in prescription and over-the-counter medicines. Make sure that you do not give your child two medicines that have the same ""active ingredient."" If you have questions ask your doctor or pharmacist\n- If you do not understand the instructions on the label, or how to use the dosing device (dropper, dosing cup, or dosing spoon), do not use the medicine. Talk to your pharmacist or doctor if you have questions or are confused.\nSome medicines may not be effective in improving children´s health\n- Antibiotics can kill bacteria but not viruses. Most colds, coughs, flu, sore throats, and runny noses are caused by viruses. Taking antibiotics for viral infections will not cure viral infections, keep others from catching the illness, or help your child feel better. Although antibiotics are good drugs for certain types of infections, they are also the type of medicines that cause the most emergency visits for adverse drug events. An estimated 40,000 children are brought to emergency departments each year because of adverse events from antibiotics.\n- Cough and cold medicines do not cure the common cold. Although cough and cold medicines may be used to treat the symptoms of the common cold in older children, they should not be used in children less than 4 years old. Too much cough and cold medicine can cause serious harm or even deaths in children.\nTips for parents:\n- Do not ask for antibiotics when a doctor says they are not needed.\n- If your child is prescribed an antibiotic, make sure they take all the medicine as prescribed, even if they feel better. Do not save antibiotic medicine ""for later.""\n- Do not use cough and cold products in children under 4 years of age unless specifically told to do so by a physician.\n- Do not give children medicine that is packaged for adults unless specifically told to do so by a physician.\nFor More Information\n- Tips to Prevent Poisonings, CDC\n- OTCsafety.org: Information for Parents , Consumer Healthcare Products Association (CHPA)\n- Get Smart: Know When Antibiotics Work, CDC\n- Using Over-the-Counter Cough and Cold Products in Children, Food and Drug Administration (FDA)\n- My Child Has a Virus, How Can I Help Her Feel Better? American Academy of Pediatrics', '(RxWiki News) How many potentially dangerous items are in your home?\nOur homes are full of useful things. Ingredients to cook with. Medications. Household cleaners. Styling products. Tools to fix our cars. However useful they may be, these items can pose a risk of poisoning — especially for children.\nAny room in your house can be riddled with poison hazards. Here are some tips and potential sources of hazards to think about:\n- Medications and children. Medications can often be found in the bedroom or bathroom, and children can easily ingest them by accident. Some medications do not have child-resistant lids, which means kids may be able to get to them even if you think you have stored them properly. Also, medications can look like candy and be enticing to kids. Always remember to keep medication stored away from sight. Your pharmacist can help with your medication storage concerns.\n- Medications and adults. The risk of accidentally taking too much medication is not just for children. When taken in large amounts, medications can cause serious problems. For example, acetaminophen (Tylenol) can be damaging to the liver. Adults who are taking Tylenol for cold symptoms can take too much without even knowing it. This is because several cough and cold products contain multiple ingredients, and acetaminophen is very common in these products.\n- Cleaners and detergents. Kids can mistake cleaners that come in spray bottles for toys. These cleaners can cause irritation or burns if they get onto the skin or eyes. As for detergent pods, kids can easily mistake them for candy because they are often colorful. If swallowed, these pods can cause harm and may even lead to death. Make sure the knobs on the spray cleaners are turned and locked to prevent use, and keep detergent pods out of reach of children.\n- Products in the garage. Antifreeze, rust remover, pool chemicals and other heavy-duty products are very dangerous. Always keep these types of products in their original containers. When these products are moved to other containers, it can be hard to tell what they are. And if they are accidentally swallowed, they can cause serious harm or even death.\n- Alcohol-containing items. These include hand sanitizer and rubbing alcohol, both of which can be easily ingested. Perfumes, mouthwashes and facial products may also contain alcohol. Even small amounts of alcohol can harm children. Alcohol depresses the central nervous system. Alcohol can also cause very low blood sugar. As a result, accidental ingestion can lead to seizures and coma. That\'s why it is important to lock and keep alcoholic beverages, as well as alcohol-containing cosmetics and cleansers, away from children.\n- Batteries. Small, round batteries can become stuck if swallowed. And they may burn the inner lining of the body. Watch to make sure that toys running on batteries have the batteries properly sealed off. For more information on the dangers of small batteries, read ""The Dangers of Accidental Battery Ingestion.""\nThis is only a small look at hazards in the home. Anything small, liquid or operable by a child is a risk. It is impossible to avoid all of these. But here are some tips to stay safe:\n- Educate your children. Remind them that medications are not candy. Try to get medications with child-proof lids and store them in a locked cabinet or out of reach.\n- Keep unsafe products locked up or far away from sight. Supervise children with things like mouthwash or toys that include batteries or small parts.\n- Always keep chemicals in their original containers and store them out of sight and out of reach.\nContact Poison Control at 1-800-222-1222. You can also use the webPoisonControl online tool for guidance on what to do after accidental ingestion. Call 911 right away if the individual collapses, has a seizure, has trouble breathing or cannot be awakened.']"	['<urn:uuid:5f1adacb-6564-42aa-a28e-d60f641442a9>', '<urn:uuid:30be0d09-a6ad-46e0-aa50-d3181e118ca8>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	10	73	1405
61	treatment honor german normandy cemeteries cemetery graves maintenance post war	During the post-war years, German graves in Normandy were initially poorly maintained and barely marked by French civilians. It wasn't until 1951 that Germans were allowed into France to care for their dead. The German graves are laid flat rather than erect, reflecting a post-war German perspective that views the deceased as civilians caught up in propaganda rather than honored soldiers. The La Cambe German Cemetery, which contains 21,222 burials, is now maintained by the privately-funded German War Graves Commission (Volksbund), which cares for 2.6 million graves across 832 cemeteries in 45 countries.	"['I haven\'t cried yet, but I cried today. We traveled to St. Lo, nicknamed ""rubble"" by the American soldiers that arrived after the bombardment on June 6th. Most of the town was destroyed, and the village was the front of a major battle by the 29th Regiment. It was overwhelming walking through the town graveyard, not only the headquarters for the regiment during the battle, but the burial grounds for the hundreds of civilians who died on June 6th alone. The headquarters was in the crypt of a family mausoleum because it was one of the few remaining structures, defended from behind by artillery on a hill, and relatively secure given the underground cavern. On the outside you can see where the left arm of the cross was blown off.\nWandering through the cemetery, any grave dated pre-WWII was covered with a barrage of dents from bullet and mortar shells.\nThat was merely the overture to processing the impact of this one bombardment. We wound toward the back of the cemetery, and stood in front of the stones commemorating the graves of the civilians who died during the first day of Operation Overlord. Obviously many more died in subsequent days, but those graves were disguised by being marked like normal graves, only notable by the date, 1944. These graves in St. Lo were a small part of the 20,000 Norman civilians who died-- collateral damage in the war. Many graves were labeled unknown, and upon close examination, it was clear that often entire families would be buried together.\nOur guide, local historian and Frenchman, Antonin Dehays, has interviewed many Normans who survived the invasion. He said that nearly everyone in Normandy knew at least one person who died. When asked how they felt about the Allied bombardment, most expressed that they understood that it was just a sacrifice that had to be made for their freedom. Dehays great uncle died working in a field during the bombardment on D-Day. He was 21. His brother, Dehays grandfather, was right next to him. He told us that one man in Normandy was killed in the rubble of the building he was hiding in. We suffocated to death. Before he died he was able to carve into the debris, ""Long live the Allies!"" En francaise, ""Vive les Allies!""\nGerman graves were ""maintained"" by French civilians during the years after the war... In other words, they were overgrown, and barely marked. It wasn\'t until 1951 that Germans were allowed into France to care for the dead.\nWhen the Germans came, they developed the Marigny and La Combre cemeteries. Graves had limited markings, and typically two soldiers were buried together-- a stark contrast to American, British, and Canadian graves. Dehays explained that this treatment of German graves represented a post war German perspective on the war. Graves are laid flat and anniversary speeches do not honor these men as soldiers, but as civilians caught up in propaganda. Erect graves are intended mirror a standing soldier. The German missing and unknown are not commemorated.\nWalking through the German cemetery, I was overwhelmed by the multitudinous graves from one of the endless conflicts in this war. The landscape of this whole region is littered with cemeteries. When will humanity learn from history?\nWe drove a distance from Marginy to a church near where 506 parachute infantry regiment of the 101st Airborne division landed. Germans had set their guns in the direction of the landing, so it was catastrophic. Two medics, Robert Wright and Kenneth Moore survived and set up a Red Cross camp inside the church, saving German and Allied soldiers alike. Despite their Red Cross signs on the door, the Germans sent mortars into the church scaring the floor and blowing out the medieval stain class windows. That glass is now replaced, and in parts replaced to honor the legacy of the medics and the American and French alliance. Robert Wrights ashes are buried at the church. He died in 2013.\nWe made a pit stop in St. Marie du Mont. The church there was incredible. Built in the 16th century, the building honors years of local history. On D-Day, two Germans were firing artillery and were eventually surrounded. They hid in the confessionals. Upon entering the church, the Americans heard a sneeze and turned their machine guns on the box. The holes are still there.', 'Our D-Day travels around France continue, this time we’re visiting the Normandy German Cemetery. Holding the remains of the aggressors and, eventually, the losing side of the war makes this cemetery much less popular than the various Allied cemeteries scattered around the area.\nIt is important to visit, I think, and further ponder the war, fascism, these men as human beings, and the war many of them probably didn’t want to fight.\nWith 21,222 total burials (compared to less than 10,000 at the Normandy American Cemetery) would you believe this cemetery is only the second largest World War II German Cemetery in France? And in fact it is just the 6th largest World War II German Cemetery in all of Europe. The point is, a lot of Germans died.\nLa Cambe is cared for by the privately-funded German War Graves Commission (or Volksbund). They care for 2.6 million graves in 832 cemeteries in 45 countries.\nOriginally a military cemetery for Allies and Axis, the Americans removed their dead for the American cemetery 10 miles away after 1945. This cemetery was dedicated in 1961 to the German war dead.\nThere are chapels on either side of the entrance gate.\nOn the wall is the cemetery directory where you can look up your last name to make sure you didn’t have any Nazis in your family. Or maybe that’s just something I would do.\nThere are some very bad people here. Just looking at this page in the Namenbuch, some of these titles are scary.\nRottenfuhrer is an SS rank, one of the worst of the worst Nazi groups, the ones mostly responsible for the Holocaust.\nOne of the worst guys buried here at La Cambe is Sturmbannführer Adolf Diekmann, responsible for the massacre of Oradour-sur-Glane. The SS killed 642 innocent people there in 1944. 247 women and 205 children were locked in a church that the SS then set on fire. So heinous, even the leadership of the SS was to court-martial Diekmann, but he died in battle before he could be punished.\nLeaving the entrance for the open spaces of the cemetery, you feel death like you always do in a cemetery. I’ve just never been in one before with such a Nazi cloud overhead. I didn’t once feel sad here, and I know many of these men, these kids, probably didn’t want anything to do with Hitler or the war. But at least in my case it’s still tough to give them that benefit of the doubt.\nThe tumulus (a burial mound), visible from all areas of the cemetery, rises 20 feet above the flat ground of the cemetery. Under the lava rock statue lies 296 mostly unknown German soldiers buried in a mass grave.\nKnowing that beneath you is a mass grave, it seems odd that you can climb to the top to look out from the top, but the back side of the tumulus has stairs to get you there.\nWaiting for you at the top are the figures of Jesus and Mary.\nA few panorama photos from the tumulus help show the size and scope of the Normandy German Cemetery.\nThe carved crosses are of the same style as the Jesus and Mary statue atop the tumulus.\nThe stones are roughly carved while the gravestones are smooth.\nThe gravestones are somber flat brown markers. Obviously very different from the bright white crosses of the Normandy American Cemetery. Many of the grave stones I encountered were just kids. These two soldiers were only 17 at the time of their deaths. It would have been nice to know where they were from, like the American Cemetery shows. But, as Germans do, they kept it simple.\nWhile not a gleaming museum like it’s American cemetery counterpart, the visitor center here does it’s best to tell the story of the Normandy Invasion. It’s a tough spot to talk about WWII from a German point of view, so the center really doesn’t attempt that.\nThere are some nice artifacts from that time including this German Soldier’s identification book.\nThe cemetery is open daily from 8:00 to 7:00pm. The visitor center is open daily from 8:00am to 12:00pm and 1:00 to 7:00pm. Everyone needs a lunch break.\nA peace garden with 1,200 maple-trees is nearby. It is a pretty sight as you’re driving into the cemetery on a tiny little road. Parking was quite easy, I was one of about 5 cars in the parking lot.\nWithout a doubt the Normandy German Cemetery at La Cambe is worth a visit in person, especially if you’re in the region to see World War II sites.\nthe plaques, we were told, are made of German clay.\nOh very interesting. Thanks for this additional info Allyson!\nYour comments reflect the same feelings Mom and I have.']"	['<urn:uuid:79b0d6ca-eeaa-4678-a788-4cdfb16e9d78>', '<urn:uuid:f59d8ae5-8dbb-499e-b7d6-f7c5b0ddd16a>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	10	93	1531
62	How much workforce reduction does Boeing achieve with optical strain?	According to Boeing engineering estimates, optical strain is 50 times less labor-intensive than traditional gauges, and 10 times less expensive.	['Variation and complexity of materials, designs, and structures is accelerating at an incredible rate, beyond what individual sensors can grasp. Full-field optical strain provides a true understanding of the complete structural response of these complex designs. This full-field data brings a more holistic reality to computer models, allowing designers to precisely advance their designs. This capability is crucial for the development of our industries.\nTwenty years ago, advanced designs used a few standard materials and traditional configurations that had worked for generations. Today, every component can be of a different material, metals to composites, each well-suited for their applications, then bonded, welded, riveted, or screwed together, with sound dampening materials, elastomers, foams, and thermoplastics, creating increasingly complex structures. Current computer models use many assumptions when these assembly complexities arise, leading to weaknesses in the final design.\nOptical strain is material independent and measures the response of the integrated system, so designers can get a measure of the true response and strength of their designs. They can truly understand the capabilities of their designs and also understand unanticipated design weaknesses before manufacturing and final structural testing.\nBoeing uses optical strain to fully understand their complete range of material properties that their manufacturing specifications allow. When they allow +/- 3 degrees of ply orientation, they can model the true material strength variation in their designs based on precision optical strain testing of each of those parameters. Better material properties, allows for better designs. NASA Glenn reported that they get more consistent material properties with optical strain over any other method.\nSubassembly designs are made up of real parts with real manufacturing variations. As parts are assembled, they are made to fit to the final design, with inherent stresses built into these components, which should be compensated for in the design, if even known. Optical strain allows for more complete understanding of the assembly process.\nStructural testing is the ultimate test of vehicle performance, with holistic measurements critical for the full understanding of the true structural response of your vehicle. The method directly aligns to CAD, so all data is precisely in vehicle coordinates. Results show the complete strain and 3D displacement data.\nOptical strain is highly fast and efficient compared to traditional methods, allowing one person to do the work of teams in a fraction of the time. Boeing engineering estimates that it is 50x less labor than traditional gauges, and 10x less expensive. It also allows you to see the strain vectors so that you can fully understand the load vectors effecting your structures.\nOptical Strain Method\nNIAR calls optical strain the FEA of testing because it provides the continuous 3D deformation and strain field across the complete structure. It actually measures the 3D coordinates of each point across the surface of structure, which is the 3D shape, and the changes are the 3D displacement of the structure, with local displacements being strain. In 3D, these local displacements become the full strain tensor of material response, as if you had 10,000 rosette strain gauges in each field of view; each field of view is worth $1M of strain gauges.\nThe 3D shape is measured dynamically in 6-DOF (Degrees of Freedom), so complete 6-DOF deformations and the total strain tensor is known in 3D space and time. Optical strain measures a surface and the true strain tensor across the material surface with the desired accuracy from far finer than, or equivalent to, a traditional gauges. In many cases, you really do not know what you are measuring with a strain gauge. The method sees the development of strains going by strain gauges, and local material responses, to crystalline responses in metals, that are just seen as variance or errors in single point sensors.\nOptical strain measures 3D coordinates in time, so it is used for any combination of complex displacements, 6-DOF tracking, strain, vibration, modal analysis, etc., with a non-contact, simple to use method, providing immensely more data than costly individual mechanical sensors. You measure across the structure the entire strain response, as well as the displacements that are causing the strains, with the data aligned to directly to the CAD model, so every point of the measurement is precisely in your CAD coordinates.\nValidation and Certification\nOptical strain was introduced more than 30 years ago and has developed into a precision technology governed by iDICs (International Digital Image Correlation Society) directed by NIST and Sandia National Labs. In the U.S., NASA was an early adopter of the technology and was the core technology used for the model validation measurements for the Return to Flight of the Space Shuttle in 2004, with validation paper, with a broad application review. [1,2]\nA strong validation was performed by Lawrence Livermore National lab.  The USAF adopted it initially for thermal strain studies on the B-2 Stealth Aircraft,  and then for broad applications. Boeing was an early adopter for aerospace manufacturing, starting 2006, and finally presenting at 2012 ASNT and their ARAMIS Optical Strain certification at 2016 ASTM. \nThe NIAR was the first user of ARAMIS Optical Strain in the U.S., and lead to the FAA Technical Center Fuselage Test Facility implementing the technology in 2004. [ 6, 7]\nIt is also covered in the ASNT Handbook.  Northrop-Grumman also performed an independent precision verification of ARAMIS Optical Strain for the structural testing of the James Webb Space Telescope,  for which they saved NASA $2M, and the telescope is working well now.\nValidation of the optical strain system and the measurement is covered by VDI 2634-1 & VDI 2626, which are used to validate every ARAMIS Optical Strain system in manufacturing,  and is available to be used to truly validate every measurement FOV, using the same method.\nFor strain gauge measurements, only the amplifier is actually certified, not the installed strain gauge, so it really is not a validated measurement; you must rely on the precision of the installers.\nStandards that follow the basis of VDI standards include ASTM E-83, ISO 10360-1, and ISO 9513. Optical strain is a form of optical photogrammetry for measuring 3D coordinates, which is the basis of VDI 2634-1 that validates optical measurements in 3D coordinates, positioning a certified standard in multiple positions within the measurement volume. ARAMIS Optical Strain can track these positions in 6-DOF in real-time (live).\nOptical strain uses Digital Image Correlation (3D-DIC) for measuring strain on the surface of a material or structure, which is the basis of VDI 2626, which validates that strains throughout the volume are precisely measurable, moving a DIC rigid pattern in the volume and maintaining zero strain. The error during these tests validates the accuracy of the optical strain measurement. These accuracy/sensitivity measurements can be documented within an ARAMIS optical strain measurement, for validated measurements, with certified calibration and certified equipment.\nOptical Strain Implementation\nOptical strain is an accepted and used technology by thousands of companies and universities around the world and is critical for the understanding of modern materials, structures, and designs. Its implementation is far easier than the deep experience and knowledge required for the implementation of traditional gauges, and is actually fully certifiable. Training and certification is available from iDICs governed by NIST and Sandia National Labs.\nOptical strain systems, certified in manufacturing, set-up following basic rules, calibrated with a certified calibration standard, make precision full-field measurements very straight forward. The measurement itself can even be certified during the measurement, all of which is documented in the data set.\nCertified setup and calibration is at least 50-times faster than with traditional sensors, and the Boeing report included that optical strain produces 100-times more data. This holistic data is critical for modern materials and structures in aerospace, automotive, microelectronics, biomechanics, and civil.\n- Schmidt, T., Tyson, J., Revilock, D.M., Lyle, K., “Performance Verification of 3D Image Correlation using Digital High-Speed Cameras”, Proceedings of 2005 SEM Conference, Portland, OR, 2005.\n- Tyson, J., Schmidt, T., Coe, D., Galanulis, K., “3D Image Correlation for Dynamic and Extreme Environment Materials Measurements Holistic Structure Measurements from the Laboratory to the Field”, Proceedings of 2005 SEM Conference, Portland, OR, June 7-9, 2005.\n- LeBlanc, M.M., Florando, J.N., Lassila, D.H., Schmidt, T., Tyson, J., “Image Correlation Applied to Single Crystal Plasticity Experiments and Comparison to Strain Gage Data”, 2006.\n- Bailey, J.T., Coate, J.E., Tashiro, R., “Evaluating the B-2 Aft Deck Measured Response to the External Environment”, Proceedings of 2008 USAF Aircraft Structural Integrity Conference, 2008.\n- Grossnickle, J., Gordon, T., McCrary, K., Wanthal, S., “3-Dimensional Non-Contact Optical Strain Measurements for Structures Evaluation and Optimization”, ASNT Aero, St. Louis, MO, 2012.\n- Tomblin, J., Seneviratne, W., Pillai, G.R., “Effects of Disbonds, Lightning Strikes, and Low-Velocity Impact Damage on Adhesively Bonded Composite Joints”, DOT/FAA/AR-09/4, 2009.\n- Avitable, P., Niezrecki, C., Helfrick, M., Warren, C., Pingle, P., “Noncontact Measurement Techniques for Model Correlation”, Sound & Vibration, January 2010.\n- Tyson, J., Schwartz, E.I., “Optical Measurement of Strain and Displacement”, ASNT Handbook, Vol. 10, p.507-513.\n- Pokk, A., Gurden, C., “JWST Structural Test Monitoring, Instrumentation and Data Acquisition”, 30th Aerospace Testing Seminar, El Segundo, CA, 2017.\n- Witzel, O., “Acceptance and Verification of DIC Systems with Reference to VDI/ VDE Guidelines”, Intl. Digital Image Correlation Society (iDICs), Portland, OR, 2019.\nThis article was written by John Tyson II, PE, President, Joesph Horn, Engineering Manager, and Steve Openshaw, Senior Applications Engineer, all at Trilion Quality Systems. For more information, visit here .']	['<urn:uuid:ae52ed9a-510f-47a3-9285-a16c3bf173d5>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	10	20	1561
63	cooking duration white wine sauce pasta clams comparison linguine	While linguine takes 10-12 minutes to cook, clams in white wine sauce should not be cooked for long periods as it would make them rubbery and tasteless. In fact, purists insist that white wine should not be added to clam sauce at all, as the extended cooking time required to cook off the wine would be detrimental to the clams' texture.	['ShareFacebook Twitter AddThis\nJust beyond the best-loved Italian pasta recipes – which undoubtedly include spaghetti with tomato sauce, amatriciana pasta and Italian carbonara – there is one dish that ideally combines pasta and the sea: spaghetti with clams, also called spaghetti alle vongole in Italian. Although it is widespread and very popular on every Italian coast, the province of Naples can boast that it codified and is home to this dish which, on Christmas Eve, becomes an absolute must on festive menus in Italy.\nThe first rule of spaghetti with clams: the sauce has to be white. If you should be served this recipe in its “red” version, covered in tomato sauce, you are authorised to send it back to the kitchen. In fact the delicate flavour of the molluscs does not tolerate other intruders, and adding tomato would simply ruin the composition. If you really do not like the white version, two or three cherry tomatoes tossed in the skillet and lightly salted may be allowed.\nAlthough it may seem an easy dish to make, like all dishes with very few ingredients, there are plenty of traps along the way: every ingredient has to be of excellent quality and cooked with no margin for error. In this particular case, there are just five ingredients: pasta, clams, oil, chili pepper and parsley. Let’s start with the most important one, the clams. One fool-proof trick for recognising them is to look carefully: if their “little horns” are separate, that means they come from the Mediterranean and are the most flavoursome “vongole veraci” (“real” clams), whereas if there is just one horn, then they are Manila clams. If you then want to amaze any cook who makes this dish for you by saying that he actually used “lupine” clams, learn to recognise them. They are a smaller variety, but very tasty, and unlike other species they have no antennae of any kind and are taken mainly from the Adriatic.\nLike any other mollusc, clams do not tolerate long cooking times; it would make them rubbery and tasteless: the trick is to turn off the heat and cover the skillet with a lid as soon as the first shells start to open. Purists also insist that you must not add white wine, which, in order to be cooked off, would require a dangerous extension of the cooking time. Sometimes the clams are joined by mussels, but these recommendations on time and cooking are also true for this type of mollusc. Many people customarily leave the clams under running freshwater for at least an hour; this operation is supposed to eliminate sand and impurities, but in fact to achieve this you simply need to run the clam liquid remaining in the bottom of the skillet through a fine mesh or sieve. You can replace this step with a simple rinse.\nThe real secret of this dish, since you can’t rely on a true sauce, is to achieve the right creaminess to bind together all the ingredients: one trick is to drain the spaghetti well before it reaches the al dente stage and finish cooking in the filtered clam cooking juices. The starch in the pasta will do the rest. Chili flakes are not an optional ingredient but need to be added carefully in very small doses, so as not to cover over the pleasant saltiness of the clam sauce. For a gourmet touch, add a sprinkling of dried mullet roe (bottarga).', 'Pasta is a student’s best friend. Easy to cook, the noodles are long-lasting and inexpensive, and you have a plethora of options on how to cook and dress it.\nMeaning “little hairs” in Italian – we know it as angel hair pasta. These long and thin strands of pasta only need to be boiled for 3-5 minutes. It’s best served with lighter sauces; try a light tomato, olive oil, pesto, or light cream sauce.\nWe’re all familiar with this pasta! The short, c-shaped tubes are great for many things–baked dishes, salads, soups, or your average bowl of mac & cheese. They take 6-8 minutes to cook, and are best paired with a cheese or butter sauce.\n“Little spindles” is an accurate description for these spirals. Pasta like fusilli and rotini are best for baked dishes and salads, though can be tossed with sauce as well. Cook for 8-10 minutes, and then pair with a thicker sauce, like a chunky tomato or bolognese sauce.\nSmall, rice-shaped pasta that’s good for salads and soups. Don’t use it in a thick or chunky sauce, it’ll get lost! Cook for 9-11 minutes, and Orzo can be good for cold, oil-based pasta salads, or in warm dishes with a thin white wine sauce.\nGood old -fashioned spaghetti, the familiar thin, long and round noodles are good for far more than the classic meatball combination. After cooking for 9-11 minutes, pair spaghetti with a smooth, olive-oil based, marinara, or pesto sauce. The thin noodles can’t cling to thick sauces, so avoid anything too heavy!\nThe name means “butterflies” in Italian, though to us they look like little bow ties! This type of pasta is good for literally everything. Taking 10-12 minutes to cook, you can then use them in pasta salads, soups, and regular pasta dishes. They pair with almost any type of sauce, but work particularly well with pesto, a creamy white wine sauce, or perhaps a four cheese sauce.\nMeaning “little ribbons,” the long and flat egg noodles are best for tossing with olive-oil based or creamy sauces. Fettuccine only takes 10-12 minutes to cook, so you’ll have a nice bowl of fettuccine alfredo in no time.\nIt means “little tongues” in Italian, as they noodles are long, flat, and about an eighth of a inch wide. Linguine is actually quite the versatile pasta: after cooking for 10-12 minutes, toss with a thick, rich alfredo sauce, or with a light tomato sauce.\nPenne means “quills” or “feathers” in Italian. They’re small tubes, about two to four inches long, cut diagonally on the end. The noodles are good at gripping thicker sauces, so pair with a chunky tomato sauce or bolognese after cooking for 10-12 minutes. Penne are also ideal for baked dishes.\nShells can be tricky–they come in a variety of sizes. Jumbo shells are meant to be stuffed, like with ricotta cheese and coated in marinara sauce, and then baked. Small shells are perfect for baked dishes, salads, or with mac & cheese–the open cavity will pool a little sauce and give an extra cheesey-flavour with each bite. Large shells should be cooked for 11-13 minutes, and small shells for 10-12 minutes.\n- Boil a lot of water. Boiling more rather than less water is ideal. The more water, the more space for your pasta to move around and the noodles won’t stick to each other.\n- Salt the water! Salting the water, and heavily, will flavour the pasta that much more. It won’t help the water boil faster, as some might say, but it definitely increases the flavour.\n- Don’t add your pasta too soon. Wait until the water if boiling, fully. Having your pasta sit in warm water will result in mushy pasta. The water will take longer to boil, too.\n- Drain your pasta quickly, before it’s finished cooking. Do not rinse your pasta, the starch on the surface of the pasta helps your sauce adhere to the noodles. Once drained, return it to the pan with the sauce and let it finish cooking with the sauce, increasing the flavour.\n- al dente is cooking the pasta to still be slightly firm when bitten; it means “to the tooth.”\n- al forno is using the pasta in a baked dish.\n- al parodia is the opposite of al dente, meaning “a travesty”. Don’t overcook your pasta, friends!\nNutrition for spaghetti with marinara sauce (106 g):\n|Total Fat||1.56 g|\n|Total Sugar||5.81 g|\n|Total Dietary Fibre||1.9 g|\nProvided by Health Canada']	['<urn:uuid:c272ead3-53e2-492c-9bb7-882e50a4272e>', '<urn:uuid:e4421fe4-97e0-4ab3-a481-13606988a40d>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T12:39:32.578853	9	61	1326
64	We keep having quality issues with dust collectors. What factory testing is recommended?	OEM factory testing should be required before shipment. Leak testing and performance testing issues can be corrected more easily and less expensively in the factory. When specifying an OEM component must meet certain requirements, stipulate in the purchase order that this be checked before shipment. For dust collectors specifically, manufacturers should test your actual dust sample to select proper filters that will last 12 months before replacement.	['John A. Constance | The Engineers Collaborative\nThe design of an effective and safe dust capture exhaust system needs to address many factors. To make sure that occurs, you and your staff need to be very familiar with the production process in your plant. You don’t want dust capture hoods capturing your product in addition to the fugitive dust being generated and captured. You also don’t want the exhaust system components to interfere with work procedures and maintenance. If the bulk solid being processed is combustible (organic or metallic), the dust capture system design will need to safely address the possible fire and explosion issues. Here are some steps that will help you get an effective dust capture exhaust system up and running on time and on budget.\nFirst things first\nBefore you begin determining the detailed design and specifications for a system, make sure you review the conceptual design with your plant’s production, maintenance, and safety department personnel. Their on-the-job knowledge will be a great help.\nThe production department will advise you on the latest changes in the process, the raw materials being used, and the dust-related issues that staff would like to see addressed. Production also can provide information on required process equipment repairs that will seal dust leaks, resulting in dust control without the need for dust capture hoods.\nMaintenance staff will advise you on issues that they may have with the existing dust capture exhaust system components such as capture hood interference, ductwork plugging, poor dust collector performance, make-up air needs, and return-air systems.\nSafety personnel can advise you on SDS (MSDS) information, OSHA requirements, existing airborne dust concentrations at production locations, and the applicable NFPA standards and recommendations. Also be sure to contact your raw material suppliers. They can advise you on dust capture issues that they deal with every day. All this information will provide you with a good basis for design. And don’t forget to have respiratory air sampling performed to determine existing dust levels in your production areas before the design process begins. The new system must result in safe dust exposures for personnel per OSHA requirements, and a 50 percent dust level reduction will require a less expensive system than one that needs to achieve a 90 percent reduction.\nDrawings and specifications\nEngineering design drawings show where OEM equipment, such as dust collectors and exhaust fans, and custom-fabricated components, such as dust capture hoods and ductwork, are to be installed. The drawings also show the details of construction for the custom-fabricated components. Developing specifications for items such as these provides additional details for what’s depicted in the engineering design drawings. How well and how complete these drawings and specifications are prepared will determine how well and how completely your dust capture exhaust system will meet your needs and expectations. Don’t leave any detail up to the fabricating contractor’s imagination.\nDesign to-do list\nBe project specific. Don’t put a requirement in the specification that you don’t understand. Using “standard specifications” from a similar project may or may not apply to your project. Unclear specifications will result in various interpretations and can mean a large construction bid spread, missed schedules, added construction costs, and startup problems.\nTake responsibility. Don’t use the term “component by contractor” or “by others.” Don’t depend on equipment suppliers and fabricators to design system components without all the requirements clearly spelled out by you. Unfortunately, there are some people who will take on a dust capture project even though they lack sufficient expertise and experience. This is especially true with regard to the design of dust capture hoods and system safety components. The result, again, will be missed schedules, construction extras, startup problems, and possible injury and property damage.\nBe detailed. Don’t use words to specify a custom-fabricated component when a detailed design drawing would be best. Without the drawing, contractors will provide what they think is best. You may save some time and expense by not providing a drawing, but you most likely won’t like the end result.\nRequire OEM factory testing. Supplied items such as exhaust fans and other components are usually guaranteed to perform, and the manufacturer will repair or replace them as necessary. However, finding out about any problems before shipment will be less expensive and less aggravating. Leak testing and performance testing issues can easily be corrected in the factory. Don’t let the manufacturer talk you out of this requirement. When you specify that a particular OEM component must meet certain testing and performance requirements, stipulate in the purchase order that this be checked in the factory before shipment. Specify how long you want the dust collector filters to last before they need to be replaced. A properly selected dust collector filter should last 12 months before replacement is needed. The dust collector manufacturer will need to test your dust to select the proper filters to meet your time requirement. Make sure the dust sample used for this evaluation is a sample of the actual dust that the dust collector filters will be used for and not just a dust sample taken from the raw material as received from your supplier.\nRequire shop drawings. Shop drawings show the system details before the components are fabricated and installed by your contractor. The contractor makes the shop drawings from field measurements following your design documents. Review and approve the shop drawings to make sure the components meet your requirements.\nVisit your contractor’s fabrication shop. Check the components being fabricated by your contractor against your approved shop drawings before any field installation begins. Finding and correcting problems during component fabrication will minimize extra costs and keep the project on schedule.\nVisit the job site. Check the components being installed against the approved shop drawings and design documents during installation. These field inspections should be done not just once or twice but routinely. Again, finding and correcting problems during installation will minimize added costs and keep the project on schedule.\nFor further reading\nJohn A. Constance, PE (215-300-9563), a consulting engineer at The Engineers Collaborative, has more than 30 years of experience designing, engineering, and troubleshooting dust control and industrial air systems for bulk solids processing and handling industries.\nThe Engineers Collaborative • Canandaigua, NY\n215-300-9563 • www.engcollab.com\nCopyright CSC Publishing Inc.']	['<urn:uuid:144d96a8-27b6-475f-9b01-42ea10cadae0>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	13	67	1044
65	where is butterfly roofed avalanche proof home located aspen colorado views mount hayden	This home is located in Aspen, Colorado, with views of Mount Hayden. It was specifically built in an avalanche blue zone, which required structural reinforcement to withstand potential avalanches.	['A Sturdy Yet Delicate Wing-Roofed Home\nA contemporary abode in Aspen, Colorado\nBuilt to withstand an avalanche, yet exquisitely delicate and airy, this wing-roofed Aspen home embodies the strength and grace of a butterfly’s controlled glide. And that analogy isn’t entirely lost on the project architects, Basalt-based Cottle Carr Yaw (CCY). “This is where the house wanted to be, to take advantage of the best aspects of the site—the views, the meadow—so that’s where it landed,” says Matt Smith, CCY’s project architect.\nOf course, it takes a lot of strategic thinking and clever design to make a mountain home look so effortless. The homeowners, who split their time between residences in Aspen and Chicago, had admired CCY’s talent for merging homes with their natural surroundings. Partner and principal John Cottle says, “I hope our architecture exemplifies how to bring the most out of a site—and then embody the way to live there, right down to the cabinetry.”\nThis particular site offered gorgeous views of Mount Hayden, but the tranquil alpine meadow was also classified as an avalanche blue zone, where homes are required to have structural reinforcement. The team at CCY embraced the challenge to collaborate with avalanche experts and structural engineers.\nBowles and Linares chandelier and Caste bench grace the entry.\n“The house is divided into a series of pavilions, bending or rotating with the contours, driven by what the land is doing around it,” Smith explains. It is a house in two parts, with heavy north protective walls on one side, and clear, steel-framed walls of glass opening toward southern vistas. Floating above it all is a butterfly roof.\nInspired by the alpine setting, interior designers opted for earthy, high-contrast neutral tones reminiscent of aspen bark, snow and earth. A plush cashmere rug, subtle mauve custom sofas and a pair of Christian Astuguevieille armchairs offer a warm welcome in the living room. No one would suspect that the steel panels conceal a television.\nA team of interior designers from Chicago’s Gary Lee Partners, who had worked with the homeowners on their city residence—including Gary Lee, Joseph Sperti and Anne Lukan—collaborated with the architects to infuse the home’s free-flowing spaces with contemporary sophistication. The owners made it clear that they didn’t want the home to be too precious. Lukan explains, “While they wanted it to feel sophisticated, they also wanted it to feel livable and comfortable.”\nThe wet bar offers another bold surprise, with a burst of high-lacquer, tomato-red color challenging the sleek-lined restraint of the kitchen.\nThe light-drenched kitchen, dining and living areas were aligned to blend easily for entertaining. With enormous windows framing breathtaking views, precious little wall space remained for displaying artwork. So the designers rose to the occasion, crafting features and finishes with gallery-worthy beauty. The undoubted masterpiece is Lee’s design for the living room’s fireplace wall—an irregular grid of blackened and waxed steel panels the architects admiringly compare to a Mondrian painting.\nNestled into an aspen grove, the media room claims a sunny corner of the house. Custom cabinetry provides ample storage, while a pillow-strewn sofa and nesting tables set a cozy scene for movie nights. The mohair rug offers a splash of the homeowner’s favorite red-orange color.\n“Since the fireplace is the heart of the home, we wanted that wall to read in connection with the kitchen,” says Lukan. Indeed, that surface’s deep, rich coloration provides a counterbalance to the bright Colorado sunshine, sounding a shadowy note that resonates throughout the home—in dark built-in cabinetry from kitchen to media room to master bedroom and bath.\nSerene and sophisticated, the master bedroom is arranged in deference to the view. Designers treated the entire wall opposite the windows as a framed headboard, with suede upholstered panels striking a note of low-key luxury. A fireside seating area shares the calm, monochromatic palette.\nLee, Lukan and Sperti chose furnishings with an eye toward understated elegance. “With all the wood, metal and stone, we wanted the furniture to feel grounded and comfortable,” Lukan explains. Luxurious materials—a mohair blanket, linen upholstery, fur pillows—help soften the space’s feel.\nThe light-filled home’s clean lines and simple elegance exude a quiet drama that feels as right and real as nature itself. As Matt Smith admits, “It’s a pretty magical place.”\nRISE & SHINE:\nAbundant natural light in a home is a wonderful gift, but high-altitude sunshine can become an overwhelming glare. Interior designer Anne Lukan shares design tips to welcome the warmth while taming the rays.\n- Raise the roof: The upward pitch of a butterfly roof opens rooms to tall mountain vistas while enhancing a home’s passive-solar capabilities. Since the shape tends to hold rooftop snow snugly in place, it’s also a great insulator.\n- Line the ceiling: “Wood lines in the ceiling draw the eye to the outdoors. An uninterrupted line of sight will make spaces feel bigger and keep details clean. The wood provides another source of warmth in a glass-encased room.”\n- Consider contrast: “Assess the natural light in the room. A room flooded with light can handle dark materials. Contrast is about highlighting sight lines and details.”\n- Shade the windows: “In a private home with spectacular views, the window coverings provide a layer of filtered light.”\n- Lay on the lighting: “The key is layers of light.”\nARCHITECTURE CCY Architects INTERIOR DESIGN Gary Lee Partners CONSTRUCTION Brikor Associates LANDSCAPE ARCHITECTURE Shannon Murphy Landscape Architects, Basalt, CO 970-927-2889 CUSTOM CABINETRY Brown Dog Designs, Inc., Carbondale, CO 970-963-1924 CABINETRY FINISHING Bob Levey Decorative Finishing DECORATIVE STEEL Company KSaveSave']	['<urn:uuid:4f85a7a5-20de-4d31-8b06-abab89c4da63>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	13	29	920
66	As someone who has had concerning chemical exposure at work, what are my rights when it comes to accessing information about the hazards I've been exposed to and my medical records?	You have the legal right to access any information your employer has about your exposure to hazards such as toxic chemicals. You also have the right to access any medical records your employer has concerning you. Additionally, you're entitled to information about chemicals used in your workplace, tests measuring chemical levels, and instructions on what to do if you're exposed to toxic substances.	"[""Rights and Responsibilities\nEmployee Rights and Responsibilities\nAs a worker, you have a legal right to:\n- A safe and healthful workplace\n- Any information your employer has about any exposure you may have had to hazards such as toxic chemicals or noise. You also have a right to any medical records your employer has concerning you.\n- To ask your employer to correct dangerous conditions.\n- To file a complaint about workplace hazards:\n- Private sector workers - In Maine call OSHA at 207-626-9160\n- Maine Public Sector Workers - Call the Bureau of Labor Standards of the Maine Department of Labor at 207-623-7900\n- To participate in enforcement inspections.\n- To not be discriminated against for exercising your health and safety rights. Your employer may not fire you, threaten you, harass you, or treat you differently for exercising your health and safety rights.\n- To refuse work that puts you in immediate danger of serious harm. Before you refuse unsafe work, request that your employer eliminate the hazard and make it clear that you will accept an alternate assignment. The OSHA regulation only protects you if the danger can be proven to exist; if you refuse to work because you believe a condition is hazardous, but are proved wrong, OSHA does not protect you.\n- To information on hazards in your workplace; chemicals used in your workplace; tests your employer has done to measure chemical, noise and radiation levels; and what to do if you or other employees are involved in an incident or are exposed to other toxic substances.\n- To information from your employer about OSHA standards, worker injuries and illnesses, job hazards and workers' rights.\n- To training from your employer\n- on chemicals you are exposed to during your work and information on how to protect yourself from harm.\n- on other health and safety hazards and standards that your employer must follow.\nAs a worker, it is your responsibility to:\n- Read the workplace safety and health poster at the jobsite.\n- Comply with all applicable OSHA and Maine safety standards.\n- Follow all lawful employer safety and health rules and regulations, and wear or use required protective equipment while working.\n- Report hazardous conditions to the employer\n- Report any job-related injury or illness to the employer, and seek treatment promptly.\nThe Maine Employee Rights Guide (PDF) describes other work-related rights of workers in Maine.\nUnder the Occupational Safety and Health Act of 1970 and Maine law, employers must:\n- Provide a workplace free from serious hazards\n- Comply with OSHA standards\n- Make sure employees have and use safe tools and equipment. Properly maintain this equipment.\n- Use color codes, posters, labels or signs to warn employees of potential hazards.\n- Establish or update operating procedures and communicate them so that employees follow safety and health requirements.\n- Provide medical examinations and training when required by OSHA standards. Post where employees can see it the OSHA Poster (private companies) or the State of Maine Occupational Safety and Health Poster (public sector employers) informing employees of their rights and responsibilities.\n- Report hospitalizations and fatalities promptly:\n- Private Sector: to the local OSHA office (780-3178) within 8 hours of any accident that is fatal or that results in the hospitalization of three or more employees.\n- Maine Public Sector: to the Bureau of Labor Standards Workplace Safety and Health Division within 24 hours if an injured worker has an overnight hospital stay and within 8 hours in case of a death. Weekdays (except state holidays) from 8 a.m. to 5 p.m. call 624-6400. At other times, fax to 624-6449 or call on pager 750-1852.\n- Keep records of work-related injuries and illnesses and post these records. (Note: Private sector employers with 10 or fewer employees and employers in certain low-hazard industries are exempt from this requirement.)\n- Provide employees, former employees and their representatives access to the Log of Work-Related Injuries and Illnesses.\n- Provide access to employee medical records and exposure records to employees or their authorized representatives.\n- Not discriminate against employees who exercise their safety and health rights.\n- Post citations at or near the work area involved. Each citation must remain posted until the violation has been corrected, or for three working days, whichever is longer. Post abatement verification documents or tags.\n- Correct cited violations by the deadline set in the citation and submit required abatement verification documentation""]"	['<urn:uuid:fe28245e-362f-46a2-9f96-c25adbac5091>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	31	63	741
67	How does bacterial swarming occur in Proteus species and what is its flagellar structure?	Proteus species demonstrate notable swarming behavior, spreading across petri dishes. This swarming motility is considered a major virulence factor. The swarming mechanism is enabled by flagella, which in prokaryotes consists of a basal body, hook, and main filament. The basal body contains rings (M, S, P, and L rings in gram-negative bacteria), with the M-ring providing movement while being anchored to the plasma membrane. The hook penetrates the cell wall to connect the main filament and basal body, allowing the helix to point away from the cell.	['Antimicrobial and Anti-Swarming Effects of Bacteriocins and Biosurfactants from Probiotic Bacterial Strains against Proteus spp.\nAbstractBackground: Proteus spp. belongs to the family of Enterobacteriaceae. These bacteria are Gram-negative and motile microorganisms and known as the third most common causes of urinary tract infections. The aim of the current study was to investigate the effects of some secondary metabolites from probiotic strains of Lactobacillus spp. on swarming and growth of Proteus mirabilis and P. vulgaris. Methods: After determination of optimal conditions for the growth and production of antimicrobials, bacteriocins and biosurfactants were partially purified from Lactobacillus culture supernatants. Then, effects of the purified compounds on growth and swarming migration of Proteus spp. were examined in the presence of various concentrations of semi-purified compounds. Results: Results showed that the partially purified bacteriocins inhibited Proteus spp. swarming distance and had a significant reduction on the bacterial growth curves. Biosurfactants in a solvent form did not have any considerable effects on factors produced by Proteus spp. Conclusion: According to the results, the secondary metabolites, especially bacteriocins or bacteriocin-like substances derived from Lactobacillus strains, can inhibit or reduce growth and swarming migration of Proteus spp. which are considered as the bacteria major virulence factors.\nMorgenstein RM and Rather PN. Role of the Umo proteins and the Rcs phosphorelay in the swarming motility of the wild type and an O-Antigen (waaL) mutant of Proteus mirabilis. J Bacteriol 2012; 194(3): 669-76.\nSivick KE and Mobley HL. Waging war against uropathogenic Escherichia coli: winning back the urinary tract. Infect immun 2010; 78: 568-85.\nLiaw SJ, Lai HC, Wang WB. Modulation of swarming and virulence by fatty acids through the RsbA protein in Proteus mirabilis. Infect immun 2004; 72: 6836-45.\nManos J, Belas R. The genera Proteus, Providencia, and Morganella. Prokaryotes 2006; 6: 245-69.\nArmbruster CE, Hodges SA, Mobley HLT. Initiation of swarming motility by Proteus mirabilis occurs in response to specific cues present in urine and requires excess L-Glutamine. J Bacteriol 2013; 195(6): 1305-19.\nStickler DJ. Susceptibility of antibiotic-resistant gram-negative bacteria to biocides: a perspective from the study of catheter biofilms. Symp Ser Soc Appl Microbiol 2002; 31: 163S-70S.\nCallaway TR, Edrington TS, Anderson RC, et al. Probiotics, prebiotics and competitive exclusion for prophylaxis against bacterial disease. Anim Health Res Rev 2008; 9(2): 217-25.\nAllegretti L, Revolledo L, Astolfi-Ferreira CS, et al. Isolation and molecular identification of lactic acid bacteria and Bifidobacterium spp. from feces of the blue-fronted Amazon parrot in Brazil. Benef Microbes 2014; 5(4): 497-503.\nUraipan S, Hongpattarakere T. Antagonistic characteristics against food-borne pathogenic bacteria of Lactic Acid Bacteria and Bifidobacteria isolated from feces of healthy Thai infants. Jundishapur J Microbiol 2015; 8(6): e18264.\nDelley M, Bruttin A, Richard M, et al. In vitro activity of commercial probiotic Lactobacillus strains against uropathogenic Escherichia coli. FEMS Microbiol Lett 2015; 362(13): fnv096.\nRiaz S, Nawaz SK, Hasnain S. Bacteriocins produced by L. fermentum and L. acidophilus can inhibit cephalosporin resistant E. coli. Braz J Biol 2010; 41: 643-8.\nVan Hamme JD, Singh A, Ward OP. Physiological aspects: Part 1 in a series of papers devoted to surfactants in microbiology and biotechnology. Biotechnol Advances 2006; 24: 604-20.\nMayrhofer S, Domig KJ, Mair C, et al. Comparison of broth microdilution, E-test, and agar disk diffusion methods for antimicrobial susceptibility testing of Lactobacillus acidophilus group members. Appl Environ Microbiol 2008; 74: 3745-8.\nHan KS, Imm JY, Oh S, et al. Bacteriocin produced by Lactobacillus acidophilus ATCC 4356: characterization and puriﬁcation. Food Sci Biotechnol 2002; 1: 531-6.\nLash BW, Mysliwiec TH, Gourama, H. Detection and partial characterization of a broad-range bacteriocin produced by Lactobacillus plantarum (ATCC 8014). Food Microbiology 2005; 22: 199-204.\nNowroozi J, Mirzaii M, Norouzi M. Study of Lactobacillus as probiotic bacteria. Iranian J Pub Health 2004; 33(2): 1-7.\nZendo T. Screening and characterization of novel bacteriocins from lactic acid bacteria. Biosci Biotechnol Biochem 2013; 77(5): 893-9.\nGudina EJ, Teixeira JA, Rodrigues LR. Biosurfactant-producing Lactobacilli: screening, production profiles, and effect of medium composition. Appl Environ Soil Sci 2011. 2011(201254): 1-9.\nSaravanakumari P, Mani K. Structural characterization of a novel xylolipid biosurfactant from Lactococcus lactis and analysis of antibacterial activity against multi-drug resistant pathogens. Bioresour Technol 2010; 101: 8851-4.\nPaluszak Z, Bauza-Kaszewska J, Szala B. Inhibitory effect of lactic acid bacteria of genus Lactobacillus on the survival of Proteus and Shigella rods in mixed cultures. Bull Vet Inst Pulawy 2006; 50: 335-40.\nMansouri S, Amari A, Asad AG. Inhibitory effect of some medicinal plants from Iran on swarming motility of Proteus rods. J Med Sci 2005; 5: 216-21.\nLebenthal E, Lebenthal Y. Probiotics: an important therapeutic concept awaiting validation. World J Gastroenterol 2002; 4: 374-5.\nKhder AK. Effect of Allium sativum and Myrtus communis on the elimination of antibiotic resistance and swarming of Proteus mirabilis. J Biol Sci 2008; 1: 124-8.\nMohankumar A, Murugalatha N. Characterization and antibacterial activity of bacteriocin producing Lactobacillus isolated from raw cattle milk sample. Inter J Biolog 2011; 3(3): 128-43.\nNashikkar N, Begde D, Bundale S, et al. Inhibition of swarming motility, biofilm formation and virulence factor expression of urinary pathogens by Euphorbia trigona latex extracts. IJPSR 2011; 2: 556-8.\nIwalokun B, Akinsinde K, Nkiruika N. Inhibition of swarming by urea and its diagnostic implications among uropathogenic Proteus species from Lagos, Nigeria. African J Clin Exper Microbiol 2003; 4: 18-28.\nWang WB, Lai HC, Hsueh PR, et al. Inhibition of swarming and virulence factor expression in Proteus mirabilis by resveratrol. J med microbiol 2006; 55(10): 1313-21.\nEcheverrigaray S, Michelim L, Longaray Delamare AP, et al. The effect of monoterpenes on swarming differentiation and haemolysin activity in Proteus mirabilis. Molecules 2008; 13: 3107-16.\nWard OP. Microbial biosurfactants and biodegradation. Adv Exp Med Biol 2010; 672: 65-74.\nStankowska D, Czerwonka G, Rozalska S, et al. Influence of quorum sensing signal molecules on biofilm formation in Proteus mirabilis O18. Folia Microbiol (Praha) 2012; 57(1): 53-60.\nKarthikeyan V, Santhoshm SW. Study of bacteriocin as a food preservative and the L. acidophilus strain as probiotic. Pakistan J Nut 2009; 8(4): 335-40.', 'Flagella is a LASH or Whip like appendage , arising from Cellbody of\nProkaryotic and Eukaryotic Organisms\nMain Function of the Flagella is Locomotion , but other than that it also\nfunctions as a Sensory Organelle.\n● Motility -Movement\n● Swarming occurs with some bacteria\n- Spreads across petri dishes\n- Proteus species is the most evident\n● This motility of bacteria occur in 2 different phenomenon :\n~ Phototaxis - Movement of bacteria in response to Light\nThis phototaxis is again of 2 types -\n* Positive Phototaxis : Towards light\n* Negative Phototaxis : Away from light\n~ Chemotaxis – Movement of bacteria in\nresponse to light\nThis Chemotaxis is again of 2 types -\n* Positive Chemotaxis : Towards Chemical\n* Negative Chemotaxis : Away from Chemicals\nArrangement of Bacterial Flagella\n● How many flagella does a bacteria have ?\n● How are they arranged ?\n● Do all Bacteria have flagella ?\n● How does a flagella look like ?\n● How does it help in movement of a bacteria ?\nClassification of Bacteria based\non arrangement of flagella . . .\n● ATRICHOUS : Absence of Flagella\n● MONOTRICHOUS : Presence of a Single\nFlagella at one end\n● LOPHOTRICHOUS : Presence of “tuft” of\nFlagella at one end\n● AMPHITRICHOUS : Presence of a single\nFlagella at both ends\n● PERITRICHOUS : Presence of Flagella all\nover the cell surface .\nExamples of the bacteria based on the\narrangement of flagella . . .\nAtrichous : Lactobacilli ,Diptheria bacillus\nMonotrichous : Vibrio cholerae\nLophotrichous : Pseudomonas\nAmphitrichous : Spirillum\nPeritrichous : Bacillus\nStructure of a Flagella -Prokaryotes\n● Flagellum , consists of a Basal body , Hook\nand a Main Filament .\n● Basal Body or Basal Granule :\n200 to 500 A in diameter ,\n● In gram -ve bacteria , basal ring has 2\nsets of rings , i.e.,\n~ Proximal set\n~ Distal set\n● Each set has 2 rings,and the four rings are-\nM-ring , S ring , P ring , L ring .\nContinued . . .\n● The arrangement of these rings are from inside to\n● M-ring is anchored to Plasma membrane , and P-\nring to the Peptidoglycan layer of the cell-wall.\n● M-ring provides the “movement” to the flagella.\n● The S-ring is the Static- ring .\n● The P-ring is attached to the Peptidoglycan layer ,\nand the L-ring to the Lipopolysaccharide layer of\nthe membrane of the gram -ve .\n● While in Gram +ve , there are only a only single\nset of rings ,No attachment to cell wall is seen .\n● The inner rings are attached to the p.membrane.\n● The Hook,penetrates the cell-wall,and\nconnects main Filament and Basal body .\n● It allows the axis of the helix to point\ndirectly away from the cell.\nMain Filament :\n● The filament consists entirely of Protein,\n● This protein is either free or consists of very\nlow amounts of amino acid residues.\n● These residues are of : -\nFlagellar Arrangement of\n● Completely different when compared to Prokaryotic\n● It consists of 9 fused pairs of micro tubules that\nmake a cylinder.\n● In addition , there are 2 unfused micro-tubles\npresent in the center of the cylinder .\n● Thereby , named as 9+2 arrangement .\n● Outward projection of cytoplasm.']	['<urn:uuid:b4775de7-71f2-4676-9dcf-b1a49c4e7c8b>', '<urn:uuid:323abbff-4caa-4689-b032-15cf1fa597bc>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	14	87	1553
68	survive dangerous ocean wave tips actions	To survive a sneaker wave, several key actions are recommended: carry a walking stick or umbrella to plant firmly in the sand if a wave hits; if caught in a wave, swim parallel to the shore until out of the wave before swimming toward shore. For prevention: avoid jetty rocks during high tide and storms, stay away from the surf zone, never turn your back on the ocean, avoid logs and debris, and watch for waves that swell higher than previous ones. It's also important to check weather forecasts, tide tables, and Beach Hazards Statements before visiting the coast.	"[""• West Coast sneaker waves kill more people than all other weather hazards\n• Learn about Sneaker Wave Safety & How to Survive a Sneaker Wave\nAccording to National Weather Service (NWS), West Coast sneaker waves kill more people than all other weather hazards. Many people have been killed and got injured on the Oregon Coast by sneaker waves. Everyone who are visiting the coast needs education about sneaker waves precautions, especially kids and teenagers considering that most victims on the Oregon Coast were 18 or younger.\nA sneaker wave, also known as a sleeper wave, is a coastal wave that is disproportionately large and appears without warning. It is similar to a miniature wall of water with an immensely large concentration of sand. These waves are referred to in warning and incident reports along the coasts of California, Oregon, and Washington. The West Coast of Canada, more specifically Tofino and Ucluelet, experience sneaker waves as well. In Western Australia and Tasmania, Sneaker waves are referred to as King Waves and are likened to rogue waves which are surface waves originating in the middle of the ocean.\nThe coast of Oregon was recently hit by large sneaker waves.\nHow do Sneaker Waves Form?\nWhile there is not much scientific inquiry into the phenomena, the epidemiology of sneaker waves (found on the shore) can be related to that of its oceanic cousin, the rogue wave. There does not seem to be a single distinct cause but rather a combination of factors such as high winds, strong currents, or other non-linear effects (e.g. solitons) that results in the occurrence of such a wave.\nWaves travel at varying speeds, however, under a set of certain circumstances they can add up or 'pile up' to form one large wave that is more massive than any 'average' wave at that time and place. A coincidence of waves and consequent reinforcement give rise to tremendously sized waves that collapse as rapidly as they emerge.\nThis is also known as the instability effect. The idea is that the transfer of energy between waves can cause few extremely large waves to be formed due to the focusing of wave energy.\nWhy are Sneaker Waves so Dangerous?\nWhile rare, sneaker waves can be phenomenally dangerous; they are hard to predict, appear suddenly with no prior warning or indication and lash out with an exceptional amount of force. The sudden enlargement of the wave can catch unwary swimmers, people walking along the shoreline, and those standing on the beach and jetties, washing them out into the ocean.\nFurthermore, the mixture of sand and water is similar to liquid cement that acts as a weighted anchor or even a death shroud. The high concentration of sand in the wave can saturate clothing, thus increasing their weight and consequently yours exponentially. This can weigh you down making any escape almost impossible.\nHow to Protect Yourself?\nHow many of you enjoy peaceful walking along a shoreline? Warm beams of sunshine and the distant roars of the ocean really do form the perfect combination, don’t they? Here is the issue though, even at a 20 yards distance from the shoreline, a sneaker wave can creep up on you.\nBefore you head to the Pacific Coast, check the weather forecast, tide, and if the warning statement was issued.\nOregon tide tables are available at tide-forecast.com. Beach Hazards Statements are issued by NWS. If the warning is in effect, stay off beaches, jetties, rocks, logs, and debris.\nSneaker Wave Safety & How to Survive a Sneaker Wave\n- Avoid jetty rocks, mainly during high tide and storms, and when the Beach Hazards Statement is in effect\n- Supervise your children and pets and stay away from the surf zone\n- Do not turn your back on the ocean and avoid logs, rocks, and debris\n- Watch out if an incoming wave swells higher than the previous one\n- If you see the high wave is coming, move to high ground\n- Carry a walking stick, umbrella, or another straight object when you walk on the beach\n- If the sneaker wave comes up, do not panic, plant your walking stick into the sand as deep as possible and hang on until the wave passes\n- If you are caught by the sneaker wave, swim parallel to the shore until you out of the wave, then swim toward shore.\nYou May Also Like""]"	['<urn:uuid:2f697e93-02fb-4ab2-9e9e-b82c153cba50>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	6	99	736
69	What roles do suspended cells and bioreactors serve in plant research?	Suspended cells are used to study stress tolerance and plant responses, particularly in species like Populus euphratica, while bioreactors provide controlled supply of metabolites with assured quality and yield for industrial applications.	['- About this Journal ·\n- Abstracting and Indexing ·\n- Aims and Scope ·\n- Article Processing Charges ·\n- Articles in Press ·\n- Author Guidelines ·\n- Bibliographic Information ·\n- Citations to this Journal ·\n- Contact Information ·\n- Editorial Board ·\n- Editorial Workflow ·\n- Free eTOC Alerts ·\n- Publication Ethics ·\n- Reviewers Acknowledgment ·\n- Submit a Manuscript ·\n- Subscription Information ·\n- Table of Contents\nVolume 2012 (2012), Article ID 243694, 5 pages\nUnderstanding Heat Stress Tolerance of Suspended Cells in the Model Plant Populus euphratica\n1Plant Functional Biology Centre, Centre for Biodiversity, Functional & Integrative Genomics, University of Minho, Gualtar Campus, 4710-057 Braga, Portugal\n23B’s Research Group, Biomaterials, Biodegradables and Biomimetics, University of Minho, Headquarters of the European Institute of Excellence on Tissue Engineering and Regenerative Medicine, AvePark, 4806-909 Caldas das Taipas, Portugal\nReceived 1 February 2012; Accepted 15 March 2012\nAcademic Editors: N. Gierlinger, D. Huber, B. Schirone, and S. Sun\nCopyright © 2012 Joana Silva-Correia et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n- W. Wang, B. Vinocur, and A. Altman, “Plant responses to drought, salinity and extreme temperatures: towards genetic engineering for stress tolerance,” Planta, vol. 218, no. 1, pp. 1–14, 2003.\n- K. Iba, “Acclimative response to temperature stress in higher plants: approaches of gene engineering for temperature tolerance,” Annual Review of Plant Biology, vol. 53, pp. 225–245, 2002.\n- P. Bhatnagar-Mathur, V. Vadez, and K. K. Sharma, “Transgenic approaches for abiotic stress tolerance in plants: retrospect and prospects,” Plant Cell Reports, vol. 27, no. 3, pp. 411–424, 2008.\n- A. M. Brunner, V. B. Busov, and S. H. Strauss, “Poplar genome sequence: functional genomics in an ecologically dominant plant species,” Trends in Plant Science, vol. 9, no. 1, pp. 49–56, 2004.\n- S. Jansson and C. J. Douglas, “Populus: A model system for plant biology,” Annual Review of Plant Biology, vol. 58, pp. 435–458, 2007.\n- R. Gu and D. Pei, “Populus euphratica, a tolerant model but endangered arborescent species,” Science Foundation in China, vol. 13, pp. 51–54, 2005.\n- S. Wang, B. Chena, and H. Li, Euphrates Poplar Florest, China Environmental Science Press, Beijing, China, 1996.\n- D. Y. Sung, F. Kaplan, K. J. Lee, and C. L. Guy, “Acquired tolerance to temperature extremes,” Trends in Plant Science, vol. 8, no. 4, pp. 179–187, 2003.\n- P. F. McCabe and C. J. Leaver, “Programmed cell death in cell cultures,” Plant Molecular Biology, vol. 44, no. 3, pp. 359–368, 2000.\n- T. Ma, Q. Liu, Z. Li, and X. Zhang, “Tonoplast H+-ATPase in response to salt stress in Populus euphratica cell suspensions,” Plant Science, vol. 163, no. 3, pp. 499–505, 2002.\n- R. Gu, Q. Liu, D. Pei, and X. Jiang, “Understanding saline and osmotic tolerance of Populus euphratica suspended cells,” Plant Cell, Tissue and Organ Culture, vol. 78, no. 3, pp. 261–265, 2004.\n- R. S. Gu, X. N. Jiang, and Z. C. Guo, “Organogenesis and plantlet regeneration in vitro of Populus euphratica,” Acta Botanica Sinica, vol. 41, no. 1, pp. 29–33, 1999.\n- T. Murashige and F. Skoog, “A revised medium for rapid growth and bioassay with tobacco issue cultures,” Physiologia Plantarum, vol. 15, no. 3, pp. 473–497, 1962.\n- G. Godoy-Hernández and F. Vázquez-Flota, “Growth measurements: estimation of cell division and cell expansion,” in Methods in Molecular Biology—Plant Cell Culture Protocols, V. M. Loyola-Vargas and F. Vázquez-Flota, Eds., pp. 51–58, Humana Press, Totowa, NJ, USA, 2005.\n- B. H. Hou and C. G. Lin, “Rapid optimization of electroporation conditions for soybean and tomato suspension cultured cells,” Plant Physiology, vol. 111, p. 166, 1996.\n- S. Ferreira, K. Hjernø, M. Larsen et al., “Proteome profiling of Populus euphratica Oliv. upon heat stress,” Annals of Botany, vol. 98, no. 2, pp. 361–377, 2006.\n- E. Burbridge, M. Diamond, P. J. Dix, and P. F. McCabe, “Use of cell morphology to evaluate the effect of a peroxidase gene on cell death induction thresholds in tobacco,” Plant Science, vol. 172, no. 4, pp. 853–860, 2007.\n- P. F. McCabe, A. Levine, P. J. Meijer, N. A. Tapon, and R. I. Pennell, “A programmed cell death pathway activated in carrot cells cultured at low cell density,” Plant Journal, vol. 12, no. 2, pp. 267–280, 1997.\n- R. A. Vacca, M. C. De Pinto, D. Valenti, S. Passarella, E. Marra, and L. De Gara, “Production of reactive oxygen species, alteration of cytosolic ascorbate peroxidase, and impairment of mitochondrial metabolism are early events in heat shock-induced programmed cell death in tobacco bright-yellow 2 cells,” Plant Physiology, vol. 134, no. 3, pp. 1100–1112, 2004.\n- D. Gries, F. Zeng, A. Foetzki et al., “Growth and water relations of Tamarix ramosissima and Populus euphratica on Taklamakan desert dunes in relation to depth to a permanent water table,” Plant, Cell and Environment, vol. 26, no. 5, pp. 725–736, 2003.\n- H. Knight and M. R. Knight, “Abiotic stress signalling pathways: specificity and cross-talk,” Trends in Plant Science, vol. 6, no. 6, pp. 262–267, 2001.\n- R. Mittler, “Abiotic stress, the field environment and stress combination,” Trends in Plant Science, vol. 11, no. 1, pp. 15–19, 2006.', 'Plant tissue culture is a technique done on an artificial nutrient culture medium of known composition to grow plants, seeds and various parts of the plants (organs, embryos, tissues, single cells and protoplasts). It is done under sterile conditions. When many clones of a plant are produced in this way it is termed as micro propagation.\nAs the plant cells are mature and differentiated the cells have the ability of totipotency which means it has the ability to change to meristematic state to differentiate into a whole plant. In order to get the desired results mostly rDNA technology is utilized. Tissue culture has played a major role in understanding plant metabolic reactions, growth pattern, differentiation and morphogenesis.\nThis technique has many advantages over the traditional ones, they produce copies with desirable traits without using seeds in a quick manner. Regeneration from plant cells are also possible with required gene modification, disease resistant plants can be produced. It is possible to regenerate genetically modified cells after protoplast fusion.\nThe seeds can be produced in sufficient quantity. Higher number of plants can be stored for longer time in much smaller place. Tissue culture of plants like banana, teak, fig, biodiesel trees are very common in order to obtain mass production of superior grade planting material. The meristem tissue which grows very rapidly and does not have pathogens is taken and is grown in an artificial nutrient medium. Later, under controlled environment they are multiplied many times in specific medium. From here, the plantlets are taken from the laboratory to a controlled green house for acclimatization. This takes around eight to twelve weeks for the plants to harden. The hardened plantlets have a well-developed root system and leaves and they are ready to be taken to the field.\nIf the growth of any cultured tissue is slow in that case optimization of the culture medium is necessary or/and culturing of the responsive tissues part. Necrosis is another problem which may spoil the cultured tissues. It has been observed that this can be overcome by culturing highly responsive tissues.\nHowever, there may be some disadvantages as it is more expensive than the traditional ways, a monoculture produced after micro propagation may be prone to diseases resulting in the infection of the complete progeny. Sometimes the fungal infection cannot be overcome.\nHairy root cultures are produced by using the explant (plant tissue) which is inoculated along with the cells of the pathogenic bacterium, Agrobacterium rhizogenes. These are organ culture systems and are used for the production of various secondary metabolites. For the production of secondary metabolites a lot of interest is for the production of antibiotics as they are utilized in the cure of many diseases.\nWith tissue culture it is possible to obtain pathogen-free plants and production of many biologically important and high valued compounds and pharmaceuticals. All these applications have attracted many molecular biologists, plant breeders and industrialists to do research and development in this field. The tissue obtained from a plant to be cultured is termed as an explant. Important colours, flavour compounds for specific notes could be produced. Plant tissue cultures have a number of advantages as the yields are mostly much higher, the plants/ compounds could be produced in the assigned location with uniform yields. Also, under the controlled conditions, there is no effect of environment and pests. Many cell cultures also show high growth rates in relation to those for conventional systems.\nWhen the cells are cultivated in a bioreactor it helps in obtaining a controlled supply of the metabolites with assured quality and yield. This is very useful for the industries. In order to enhance the reactions various elicitors (biotic or abiotic chemicals) are supplemented into the medium. They can be heavy metals, pesticides or detergents. They may also be any physical factors like cold shock, UV radiation or high pressure which may induce the enzyme action triggering the accumulation of secondary metabolites.\nTissue culture technique is very useful for conservation of rare or endangered plant species, screening the cells alone rather than the whole plants to check for herbicide resistance. Through plant tissue culture technique it is possible to cross different species by protoplast fusion and then regeneration of the novel hybrid, to study the molecular basis for physiological, biochemical, and reproductive mechanisms in plants and also in the selection of stress tolerant plants. Cross-pollination of distant species and then tissue culturing the embryo obtained after the pollination (Embryo Rescue) has become possible which otherwise does not survive. Production of polyploidy which is desirable in certain cases is possible with the application of antimitotic agents.\nAs plant tissue culture is a very labour-intensive process, it is better to determine the needs, market, commercial viability before taking up the work.\n– Dr Renu Agrawal\n(Ex. Chief Scientist, CFTRI, Mysore)']	['<urn:uuid:1e9ea5ab-aa0c-40d6-93bf-42fb906c4011>', '<urn:uuid:2ef50caf-9296-4c7d-9497-88ea8223d79d>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	11	32	1682
70	why beer cans protect better than bottles	Beer cans are entirely opaque which protects beer from light - one of its worst enemies. Light reacts with hop compounds to cause 'skunky' aromatics. Additionally, modern cans have controlled oxidation levels and polymer linings prevent liquid from touching metal.	['Conor McCormak, pictured above, is the Winemaker at Brooklyn Winery, where he oversees all aspects of wine production from sourcing premium fruit from vineyards to the bottling of 5,000 cases annually. When his hands weren’t busy handling grapes and cradling his daughter Madeleine, born earlier this year, he and Garrett chatted over email about the similarities in beer and wine packaging trends…\nConor: Hi Garrett. I have noticed an increase in microbreweries using cans (some with very elaborate designs) for their beer along with a couple wineries trying it out. Do you find there to be a difference in flavor, quality, or shelf life with canned beer versus glass bottle? I haven’t put any wine in can as I would suspect that the acids would react with the aluminum.\nGarrett: Good question! We’ve been putting our beer in cans for years now, but until a few years ago we largely confined them to places glass wasn’t allowed – stadiums, golf courses and the like. Slowly but surely, however, cans are becoming seen as a legitimate package, shaking off the “cheap industrial beer” image that they had until recently.\nThe reasons are largely two-fold. First is the realization by brewers that modern canning doesn’t have any technical downside and even has some advantages. Levels of oxidation in cans, once much higher than in bottles, have now been brought under control. That’s important, because oxygen destroys beer pretty quickly. Polymer can linings mean that the liquid in the can never actually touches metal. Cans are also entirely opaque, which protects beer from one of its worst enemies, which is light (which reacts with hop compounds to cause “skunky” aromatics technically referred to as “lightstruck”).\nThe second thing is the new availability of relatively inexpensive canning lines. A canning line has traditionally been a large, very expensive piece of equipment, something that could only be afforded by a huge company. Because of the new prominence of small-scale brewers (and other beverage producers) and improved technologies, packaging equipment manufacturers now make canning lines that some smaller breweries can afford.\nIn many ways, the can is the “screw-cap” of the beer industry. Most people feel “better” about bottles than they do about cans, the same way that most people feel better about corks in wine bottles. These images, however, are increasingly divorced from any actual quality concerns. Some breweries now use the retro/hipster/ironic qualities of the can as part of their brand identity. That’s not really our style, but our cans do look pretty cool. The real downside of cans, in my opinion, is that it seems that most people who drink canned beer are probably drinking it directly out of the can rather than pouring the beer into the glass. No brewer really likes that – we work hard on features like a beer’s aroma and foam, and we like people to experience them. But we also know that sometimes people are drinking beer on fishing boats, not at the dinner table, so we’ll let that one slide!\nI’ve seen a lot of different packages for wine, and bag-in-box has really become well pretty accepted in Europe. Do you see any possibility that anyone will can decent wine? Given how many highly acidic fruit juices come in cans, I assume that any barriers are cultural rather than technical, yes?\nConor: I think that it is possible to put decent wine in a can, but I doubt it would ever breach the “over $20″ shelf due to cultural barriers. Not to say there isn’t plenty of decent wine under $20 that could benefit from a unique packaging option, just that consumers often react slowly to such big changes making it a risk for producers putting it out there.\nThe bag-in-box wine has been around for quite some time but has always remained in the lower tier of both price and quality. It is true that cultural barriers will continue to inhibit any major market growth, but the plastic based bag used to store the product cannot protect it from oxidation over time. Some of the bags are augmented with a metallic lining to slow down the oxidation, but plastic is very porous. The only kind of wine that might survive the “bottling” to drinking would be one that is sterile filtered, or worse chemically filtered, and generally stripped of any soul.\nThe truth is, I don’t see many wineries risking their brand being stigmatized as “box wine” quality unless they do so under the guise of another label. Some of the big players who are looking to place wine in every major super market around the world might be able to make a couple bucks by putting out mediocre bag-in-box wine, but for higher end producers and drinkers I think box wine will remain on the bottom shelf.\nSince starting at Brooklyn Winery, I have been experimenting with wine on tap. While there have been some kinks to work out in the delivery process, I am happy with the results overall. Some of the major concerns have been the length of time from tapping to kicking the keg while maintaining a fresh pour, and tartrate build-up in the lines.\nHow long can a keg of beer stay fresh once tapped? Does it depend on the style of beer?\nBeer lines are often cleaned (or should be), but I have heard that poorly maintained lines can lead to “tap sickness”. Can you explain or debunk this?\nGarrett: A keg should be, in the best circumstances, a perfectly clean and sterile container full of beer that’s free of extraneous microbes. In real life, however, the biological axiom “everything is everywhere” applies. So once the keg is breached by tapping, it comes into contact with the inner surface of the beer lines, which may or may not be pristine. Well maintained beer lines should be cleaned (with caustic soda, then flushed with water) every two weeks at the very minimum. Otherwise, solids from the beer start to build in lines, and this is where problems start. Anything growing in the beer lines may eventually make its way into the keg and spoil the keg. Now, it’s important to keep in mind that there are no known pathogens that can survive in beer, so spoiled beer can’t make you sick. But it sure doesn’t taste good, so proper practice is critical. We try to make sure that any bar or restaurant that’s serving our beer finishes out a keg within a week or so. At a place where they clean their system frequently and sell beer quickly, the beer will tend to taste great….and the converse is true too. Fortunately, these days most proprietors are paying proper attention to their draft systems and beer tastes better than ever.']	['<urn:uuid:7cb9a0cc-56e3-4585-b4b7-e4a65052755f>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T12:39:32.578853	7	40	1129
71	Could you help me understand how to calculate the Lifetime Value of a customer in situations where the monthly revenue per customer increases over time?	For cases where ARPA (Average Revenue Per Account) changes over the customer lifespan due to expansion revenue, a more complex formula is needed. For example, if you had 100 customers starting at $100 monthly payment with a $5 monthly increase and 3% monthly churn rate, the expansion revenue initially exceeds churn losses, but eventually churn dominates and reduces cohort value. The formula for this scenario is: LTV = (a/(c-m)), where 'a' is initial monthly ARPA, 'm' is monthly growth in ARPA per account, and 'c' is monthly Customer Churn Rate percentage. This formula assumes revenue increases at a roughly fixed rate monthly for the entire customer lifetime.	['I’ve just come across the forEntrepreneurs website with some really useful articles about growing your business from David Skok. So I thought I would return the favour and publish their suggested articles below, with a strong emphasis on SaaS:\n- SaaS Metrics 2.0 – A Guide to Measuring and Improving What Matters\n- Startup Killer: the Cost of Customer Acquisition\n- Lessons Learned – Viral Marketing\n- How sales complexity impacts your startup’s viability\n- Designing startup metrics to drive successful behavior\n- Lessons from Leaders: How JBoss did it\n- SaaS Economics: The SaaS Cash Flow Trough\n- Optimizing your Customer Acquisition Funnel\n- Setting the Startup Accelerator Pedal\nAlso worth checking out on the SaaS side is the metrics definitions page, I have copied and paste, starting with unit economics.\nThe post below on unit economics goes into some mathematical details. I was recently made aware of this quick calculator from Nickelled, which helpfully automates calculations for some of the below – if maths makes your head hurt, it might be worth a shot. I myself commissioned the creation of an ROI calculator to support sales content for Causeway Technologies, so I appreciate how useful they can be:\nCalculating LTV and CAC for a SaaS startup\nUnit Economics is a very powerful way to analyze the long term profitability of a SaaS business.\nI am often asked for the details of how to compute the various elements, such as CAC and LTV. This post gives the formulae.\nCAC – Cost to Acquire a Customer\nCAC is defined as follows:\nThere is a problem with using this formula in the early days, as you may several expensive people in the team that should scale to handle a far number of customers as you grow. In that case, your CAC will be too high. I suggest doing a very simple adjustment to the Sales & Marketing expenses to take only a portion of those salaries and expenses in the early days. That will give a better indication of how CAC will look in the future when you are at scale.\nNote that if the Customer Churn rate is a monthly % or yearly %, then the Customer Lifetime will be for the same time period. Here is a monthly and annual example to illustrate the point:\na) If the Monthly customer churn rate is 3%, then the Customer Lifetime will be 1/0.03 which is 33 months.\nb) if the Annual customer churn rate is 20%, then the Customer Lifetime will be 1/0.20 which is 5 years.\nLifetime Value of Customer\nIn the situation where there is no expansion revenue expected over the lifetime of a customer, you can use this simple formula:\nwhich can also be expressed as follows:\nOnce again if ARPA is monthly, the churn rate should be monthly.\nTo truly get an accurate picture of LTV, you should take into consideration Gross Margin. i.e.\nHowever in most SaaS businesses, the gross margin % is high (above 80%), and it’s quite common to use the simpler version of the formula that is not Gross Margin adjusted.\nRon Gill, NetSuite: I’m surprised at how often I see a SaaS product architected in a way that means they’ll never clear a decent gross margin. Including GM in the calc is a great way for you to see there is a big lever on LTV/CAC that is worth focusing on.\nFor NetSuite, we’ve not only calculated LTV/COCA, but also calculated r-squared of each of the components (to see what has driven improvement) and sensitivity analysis on them (to see what might drive it in the future). GM is an important component.\nMore complex case\nIn the specific situation where you expect ARPA to change over the lifespan of the customer due to expansion revenue, this simple version of the formula will not work. We ran into this situation with ZenDesk, where there is a pretty reliable increase in revenue over the life of a customer.\nHere’s a graph showing what would happen if you had a cohort of 100 customers that initially started paying you $100 a month, but increased their payment by $5 every month. The monthly Customer Churn Rate is 3%:\nAs you can see the expansion revenue initially is greater than the losses from churn, but over time the churn takes over and brings down the value of that cohort.\nI asked my partner, Stan Reiss, to help with the math to calculate LTV in this more complex situation. Here is what he came up with:\na = initial ARPA per month ( x GM %, if you prefer)\nm = monthly growth in ARPA per account\nc = Customer Churn Rate % (in months)\n(This formula makes an assumption that revenue increases at a roughly fixed rate every month for the entire lifetime of the customer. That probably doesn’t hold true for many SaaS businesses, but the goal is to get a rough idea, not to have the absolute perfect answer.)\nLTV : CAC Ratio\nOur guideline for a successful SaaS business is that this number should be higher than 3.\nRon Gill, NetSuite: It is most important to track this metric over time to make sure you’re driving improvement. And, look at investment and how it will impact.\n(The guideline assumes you are using the simpler LTV formula that does not include a Gross Margin adjustment, and that you have a Gross Margin of 80% or higher.)\nMonths to recover CAC\nTo be perfectly accurate, this should include a Gross Margin adjustment as follows:\nHowever in our guideline which states that Months to Recover CAC should be less than 12, we are assuming that you are using the simpler formula, and have a Gross Margin of 80% or higher.\nThe Metrics to help understand Bookings\n|MRR||The Monthly Recurring Revenue at the end of each month. Computed by taking the MRR from the previous month and adding Net New MRR.|\n|ARR||Annualized Run Rate = MRR x 12ARR is annual run-rate of recurring revenue from the current installed base. This is annual recurring revenue for the coming twelve months if you don’t add or churn anything.|\n|ACV||Annual Contract Value of a subscription agreement.|\n|New MRR/ACV||The increase in MRR from new customers in the current month.|\n|Churned MRR/ACV||The lost MRR from churning customers in the current month.|\n|Expansion MRR/ACV||The increase in MRR from expansion in your installed base in the current month.|\n|Net New MRR/ACV||Net New MRR = New MRR + Expansion MRR – Churned MRRThis is the sum of the three different components that will change MRR during each month.|\n|Bookings||The total dollar value of all new contracts signed. Usually taken as an annualized number even if the contract period is longer than one year.Since the bookings number might have a mix of different durations (e.g. month-to-month; 6 months; 12 months) this number is not very helpful for understanding the business.To really understand what is going on in your SaaS Business, you should look at the following components:a) What happened with new customers:\nb) What happened in your installed base:\nThe sum of all of the above:\n|Billings||Billings is the amount that you have invoiced that is due for payment shortly.|\n|Revenue||Revenue is amount of money that can be recognized according to accounting policy. Even if it is paid for upfront, usually subscription revenue can only be recognized ratably over time as the service is delivered.If more money has been paid than can be recognized, the difference goes into a balance sheet item called Deferred Revenue.|\n|Average Contract Length||Assuming a mix of different contract lengths, this gives you the average duration in months or years.|\n|Months up front||Average of months (or years) of payment received in-advance with new bookings. Getting paid in advance has a big positive impact on cash flow. This metric has been used at both HubSpot and NetSuite in the past as a way to incent sales people to get more paid up front when a new customer is signed. However asking for more money up front may turn off certain customers, and result in fewer new customers, so be careful how you balance these two conflicting goals.|\n|ARPA – Average monthly recurring Revenue per Account||This number is tells you the average monthly revenue per customer. It is useful to look at this for just the new customers booked in the month. Plot a trend line to show you the average price point that your new customers have chosen.|\nBookings, Billings and Revenue – An example\nSince there can be some confusion around the difference between bookings, billings and revenue, here is a simple example to help clarify them: Imagine you signed a new contract with a customer with a one year term, specifying that you provide your service to them for $1,000 per month, with an upfront payment of six months:\n- Your bookings would be $12,000 (the entire contract value)\n- You would bill $6,000 in the first month, then $1,000 per month from the seventh month onwards.\n- You would recognized $1,000 in revenue for each month of the contract. (This is dictated by GAAP accounting policy.)\nFor the example above, the balance sheet and income statement impact of these items is as follows:\n- Bookings do not affect either the balance sheet or the income statement.\n- When you bill $6,000 in the first month, but can only recognize $1,000 in revenue (income statement), and the other $5,000 goes into deferred revenue on the balance sheet (a liability).\n- Each month thereafter until another $1,000 can be recognized as revenue (income statement), and that reduces the deferred revenue liability on the balance sheet.\nThe Metrics for Churn (Renewals)\nThe following shows the metrics to understand Churn:\n- Here’s a related post I wrote comparing CBA vs ROI with a slideshare on social media metrics – consistency the most visited article on this website.']	['<urn:uuid:894d13e5-e00a-4692-b7de-3106cd9e121c>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	25	107	1652
72	what ceremonies took place when ancient israelites laid temple foundation	When the foundation of the temple was laid, there was a ceremonial celebration where priests in their vestments came forward with trumpets, and the Levites (sons of Asaph) played cymbals to praise the Lord, following King David's directions. They sang responsively, praising and thanking the Lord for His goodness and steadfast love. The people shouted with great joy, though many older priests, Levites, and heads of families who had seen the first temple wept loudly. The mixed sounds of joy and weeping were heard far away.	['When the seventh month came, and the children of Israel were in the towns, the people gathered as one man to Jerusalem. 2 Then arose Jeshua the son of Jozadak, with his fellow priests, and Zerubbabel the son of Shealtiel with his kinsmen, and they built the altar of the God of Israel, to offer burnt offerings on it, as it is written in the Law of Moses the man of God. 3 They set the altar in its place, for fear was on them because of the peoples of the lands, and they offered burnt offerings on it to the Lord, burnt offerings morning and evening. 4 And they kept the Feast of Booths, as it is written, and offered the daily burnt offerings by number according to the rule, as each day required, 5 and after that the regular burnt offerings, the offerings at the new moon and at all the appointed feasts of the Lord, and the offerings of everyone who made a freewill offering to the Lord. 6 From the first day of the seventh month they began to offer burnt offerings to the Lord. But the foundation of the temple of the Lord was not yet laid. 7 So they gave money to the masons and the carpenters, and food, drink, and oil to the Sidonians and the Tyrians to bring cedar trees from Lebanon to the sea, to Joppa, according to the grant that they had from Cyrus king of Persia.\n8 Now in the second year after their coming to the house of God at Jerusalem, in the second month, Zerubbabel the son of Shealtiel and Jeshua the son of Jozadak made a beginning, together with the rest of their kinsmen, the priests and the Levites and all who had come to Jerusalem from the captivity. They appointed the Levites, from twenty years old and upward, to supervise the work of the house of the Lord. 9 And Jeshua with his sons and his brothers, and Kadmiel and his sons, the sons of Judah, together supervised the workmen in the house of God, along with the sons of Henadad and the Levites, their sons and brothers.\n10 And when the builders laid the foundation of the temple of the Lord, the priests in their vestments came forward with trumpets, and the Levites, the sons of Asaph, with cymbals, to praise the Lord, according to the directions of David king of Israel. 11 And they sang responsively, praising and giving thanks to the Lord,\n“For he is good,\nfor his steadfast love endures forever toward Israel.”\nAnd all the people shouted with a great shout when they praised the Lord, because the foundation of the house of the Lord was laid. 12 But many of the priests and Levites and heads of fathers’ houses, old men who had seen the first house, wept with a loud voice when they saw the foundation of this house being laid, though many shouted aloud for joy, 13 so that the people could not distinguish the sound of the joyful shout from the sound of the people’s weeping, for the people shouted with a great shout, and the sound was heard far away.\nThe Holy Bible, English Standard Version Copyright © 2001 by Crossway Bibles, a publishing ministry of Good News Publishers.\nQuestions to Consider:\n1. Read Ezra 3 and summarize the story line in 1-2 sentences. What was the first thing the people did when they arrived in Jerusalem after coming out of exile? (1, also 2:70) After the people had settled into their homes and towns, what was the first structure they built? (2) The second? (10)\n2. What does verse 1 mean when it says the people gathered “as one man“? In what aspects can the church come together “as one man” when we meet for worship or do Kingdom work? Who led the way in rebuilding the altar (2-3) and what were their positions of leadership over the people?\n3. In verses 2-6, what do the phrases “as it is written in the Law of Moses” (2), “as it is written,” and “according to the rule” (4), mean? How do the people’s actions in these verses show that their worship was guided by Scripture and carried out in obedience to it? How do the altar and the sacrifices point us to Christ’s sacrifice for us on the cross? Consider the worship service at your church. Are all aspects of the service guided by Scripture and carried out in obedience to it? Does your worship service center around Christ and the cross?\n4. Considering the spiritual significance of the first temple, why would it have been important to rebuild the temple? Compare verses 8-10 with Luke 6:46-49. What are some parallels between Israel laying the foundation of the temple according to God’s word and the foundation of our faith today being obedience to Christ and His word?\n5. Compare verses 11-13 with 2 Chronicles 7:1-10. What are some of the similarities and differences between the people’s response to completion of Solomon’s temple and their response to the laying of the foundation of the post-exilic temple? (Especially compare v. 11 with 7:3 and v. 12-13 with 7:10). Was there any weeping at the completion of Solomon’s temple? Why would the “old men” have been weeping in verses 11-12? Recalling all that had happened to Israel between the completion of Solomon’s temple and the laying of the foundation of the post-exilic temple, was there good reason for both joy and weeping? What were those reasons?\n6. In what ways could the laying of the foundation of the temple be considered Israel’s being “born again”? Think about…\n…how the exile points to our captivity by the enemy and bondage to sin\n…how God delivered His people from bondage through Jeshua the high priest, who “made a beginning” (8) of the foundation, and how God delivered us through Yeshua our High Priest who made a beginning – the beginning and the end, the founder and perfecter – of our faith.\n…how the people mourned over the sin that had destroyed God’s ideal dwelling yet rejoiced over beginning a new life with Him, delivered and forgiven, and how we do the same when we come to new life in Christ.']	['<urn:uuid:f2b48f6e-b2a6-4fc8-93ac-6ae1d12cee91>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	10	86	1050
73	best treatment sick betta stop eating	Bettamax is specifically designed for treating lethargic fish that stop eating. It contains nitrofurazone, methylene blue, pvp, vitamins, salt, Sulfa, methazine, diazine and merazine. This medication is only used for freshwater fish and is suitable for bettas, seven you and all fish with fins.	['Antibacterial for betta fish, Antibiotic treatment, Antibiotics for betta fish, Betta Fish Care, Betta fish treatment, Fungal infections, medicines for the treatment of betta fish, The common antibiotics:\nThe company’s products Mardel not affect biological filtration function. Use antibiotics appropriately: do not stop the treatment earlier in the day, it’s best to last more than seven days.\nSome organisms are particularly susceptible to certain drugs so check carefully before use. For example, the catfish species such as catfish, mollusks / snails, fish and plants.\nBecause most antibiotics kill bacteria and gram-negative bacteria break down nitrogen in the tank should also belong to the gram-negative type of fishing is better for treatment in a private lake instead treat pond.\nUnfortunately, the use of an antibiotic sometimes not enough for a number of bacteria that are resistant to certain drugs, some mutations and only killed when high doses of antibiotics. Normally, we can not identify specific individuals infected or they may be infected with more than one type at once! If seen in the few days that the disease is not in remission, replace other drugs. Or can also use broad-spectrum antibiotics that kill both gram negative and gram positive bacteria, such as the drugs neomycin, chloramphenicol, medicine unknown nitrofurazone, antibiotics transdermal unknown is kanamycin sulfate as Kanacyn / K-Mycin or tetracycline. Drugs such as Spectrogram (company Aquatronics) is antibiotics nitrofurazone synthesis and kanamycin ultra wide spectrum used for this purpose.\nGram-positive bacteria are not affected by tetracycline and streptomycin.\nGram-negative bacteria can not be affected by ampicillin, penicillin, erythromycin and Sulfa. Some bacteria are not killed by the usual dose should be titrated higher.\nDrugs with Sulfa completely effective treatment of gram-negative bacteria.\nThe common antibiotics:\nAntibacterial (Aquarium Pharmaceuticals companies).\n- Bettamax (company Aquatronics): medicines for the treatment of fish lethargic, stop eating, torn fins, discolored, vitamin deficiency, and fungal infections. Only used for freshwater fish. Subject: betta, seven you and all the fish have fins to. Ingredients: 250 mg of nitrofurazone, methylene blue, pvp, vitamins, salt, Sulfa, methazine, diazine and merazine.\n- Erythromycin: Antibiotic treatment of Gram-negative bacteria and some Gram-positive bacteria and the bacteria causing fin rot, carry diseases and diseases related to molly fish.\n- Spectrogram: ultra wide-spectrum antibiotic to treat red spots, bleeding, loss of viscosity, ruffled scales, infectious disease, foot-and-mouth disease and fungus. For freshwater and saltwater. Do not use for molluscs. Treatment should be used for lake fishing and bathing. Ingredients: nitrofurazone, kanamycin.\n- Tetracycline (company Aquatronics): broad-spectrum antibiotic to treat gram-positive and some gram negative bacteria. Used to treat fin rot, CT fins, swollen eyes, gills, sore on the mouth and body, bloated disease, foot-and-mouth disease and opportunistic infections such as thrush. The disease-related species baby (livebearer) and a betta fish. So strong aeration medication. Work well for some types of diseases caused by the bacteria Aeromonias, Pseudomonas and Mysobacterial.\n- Maracyn (Mardel firm): broad-spectrum antibiotic to treat gram-positive bacteria. Used to treat fin rot, foot and mouth disease, eye swelling, carry diseases and opportunistic infections such as thrush. Ingredients: Erythromycin.\n- Maracyn-Two (Mardel firms): broad-spectrum antibiotic to treat gram-negative infections both outside and inside because it is capable of penetrating the skin. It treat effectively finrot, eye swelling, disease carrying, bloating (abdominal swelling, ruffled scales), septicemia (bleeding wounds or sores on the body), internal infections and diseases opportunities. Effective even when the fish are not fed drug mix. Ingredients: Mincycline hydrochlor.\n- Coppersafe (Mardel firms): a drug containing copper compounds for the treatment of white spot disease, flukes, hookworms, mushrooms, velvet, protozoan diseases caused by bacteria and other parasitic diseases.\n- Aquarisol (firm Aquarium Products): saline water containing copper to treat white spot disease and other infectious diseases.\n- Methylene Blue: fungus treatment is very effective. Against fungal infection for fish eggs. Treatment of white spot disease. Drugs can affect the biological filter function so sick fish should be isolated and treated separately if possible. Can be used as a substitute for Malachite Green for species susceptible to this drug.\n- Malachite Green: used to treat skin parasites and fungal diseases for fish and fish eggs, including fungal water and fungus Achlya mi.\n- Clout (Aquarium Products): drug treatment for parasitic and protozoan infections very effective. Treatment of white spot disease, aquatic that, leeches clinging, flatworms, the single-celled bacteria Epistylis, Trichodina, Hexamita and Tetrahymena, fungi, trematodes, parasitic hookworm make and feet.\n- Maracide: white spot healing and other common parasites.\n- TriSulfa: prevention and treatment of common infectious diseases.\n- Maroxy: treatment of fungus and bacteria.\n- Triple Sulfa (firm Aquarium Pharmaceuticals): broad-spectrum antibiotics to treat the fungal disease, red spot, fin rot, mouth sores, loss of viscosity / color, eye becomes cloudy, betta fish-related diseases and blood infections . Ingredients: sulfamethazine, sulfacetamide, and sulfathiazole.\n- Kanacyn (Aquatronics): treatment of gram-negative and-positive bacterial infections, including bleeding, bloating, fin rot, swelling bearing, ball or ruffled scales, fungal, Vibrio infections, fish TB (tuberculosis), poor health and sores.\n- Super Sulfa (company Aquatronics): anti-bacterial / anti-fungal for the treatment of foot and mouth disease, fin rot, slip viscosity, fungal diseases, diseases related to betta fish and other infectious diseases.\n- Furanace (firm Aquarium Products) antibiotics to treat the fin rot, swelling of the mouth and eyes, cloudy eyes, poor health, infectious diseases, fungal diseases, eye inflammation and other diseases related to betta fish / tetra. Ingredients: erythromycin.\n- Myacin (company Aquatronics): anti-bacterial / anti-fungal for the treatment of foot and mouth disease, fin rot, slip viscosity, fungal diseases, diseases related to betta fish and other infectious diseases.']	['<urn:uuid:000e442f-cd53-423e-9f9c-63217139cd12>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	6	44	921
74	What's the difference between how bison were managed in Paynes Prairie versus how they were managed by the American Bison Society in the early 1900s?	In Paynes Prairie, officials had to cull the herd and remove males due to problems with escapees to the suburbs. In contrast, the American Bison Society in the early 1900s focused on growing the population by shipping buffalo to establish new herds and banning all hunting, successfully increasing numbers from fewer than 1,000 to nearly 12,000 by 1920.	['Rule No. 1 when it comes to fleeing an alligator: Don’t run in a straight line. I learned this while standing feet from a prehistoric-looking, kayak-size animal that had crawled up onto the bank of a canal to bask in the crisp January morning sun.\n“If you have to run, run zigzag,” warned Brack Barker, whom I had hired to show me around Paynes Prairie Preserve State Park in Gainesville. “They can run fast, but only straight ahead.” We’d been walking along a grassy path, Barker slightly in front, and he’d turned around to warn me about a gator hidden in the grasses. I approached slowly and found an abnormally large-headed animal with bumpy skin and closed eyes.\nRule No 2: Don’t forget your grade school science lessons, in which you probably learned that reptiles experience a sort of half-doze called thermoregulation. In plain English, that means they’re not really asleep. So even though this creature was still as a log, I was aware of standing well beyond the suggested wildlife viewing distance and began thinking about zigzagging.\nBut I also couldn’t dismiss the fact that I had come to the park for this very experience. I was traveling through the Southeast, and my friend Jason, a recent transplant from San Francisco, told me Paynes Prairie was filled with wildlife — not only the iconic gators, but also, surprisingly, bison, along with wild horses and cattle descended from those brought over centuries ago by Spaniards.\nNever miss a local story.\nOf course, Gainesville is perhaps best known for the University of Florida Gators. I don’t generally make detours for mascots, but nose-to-snout encounters with wildlife? Sign me up.\nPaynes Prairie is a 20,000-plus-acre park less than 10 miles from the UF campus and downtown Gainesville. The state’s first preserve and a designated National Natural Landmark, the terrain was once home to the saber-toothed cat and, much later, the largest Spanish cattle ranch in Florida.\nToday, it sustains more than 20 distinct biological communities, including more than 270 species of birds. It has campsites and trails for hiking, horseback riding and cycling; because of its wide-open vistas, the park is also a great place to watch sunrises, sunsets and stars.\nThe evening before my tour with Barker, I drove into town, passing ranches, vegetable stands and dogs barking in the flatbeds of pickup trucks. Early the next morning, the air was chilly. I walked around the cabin where I’d stayed and noticed Spanish moss hanging not only from trees but also from telephone wires and street signs, making everything look like it needed a good dusting.\nBarker met Jason and me in the parking lot outside Pearl’s Country Store. A wiry man with a gray beard, Barker wore wire-rimmed glasses and a bandanna around his neck. We drove to Paynes Prairie, stopping along Highway 441 to look out to the land from an observation platform. Standing on the boardwalk, surrounded by cattail and pickerelweed, Barker explained that the park was formed by a number of sinkholes coming together to create the basin tens of thousands of years ago.\n“You’re standing in a giant sinkhole,” he said. “This used to be underwater.”\nIn the late 1800s, the eight-mile-wide prairie was a lake, with fish camps, steamboat charters and barges that transported citrus. Then, suddenly, the sink drained. Just like that. Within days, the prairie went dry — and could do so again anytime. For now, Paynes Prairie has smaller bodies of water that drain underground to recharge the aquifer, a giant reserve used for drinking water.\nWe drove to the northern side of the park, passing a turtle-crossing sign and some ancient live oaks. La Chua Trail is among the park’s most popular because of wildlife sightings. As we followed the trail along the water, Barker pointed out birds: the anhinga, spreading its impressive wings to warm in the sun; the red-legged white ibis, picking around in the wet ground for snails and frogs; and the great black cormorant, jockeying an uncooperative fish into its beak.\nWe saw our first gators lined up along the water line, their short legs half submerged — as when humans drag beach chairs to sit in the sand right where the surf breaks. Through binoculars, I studied their graceless, cartoon-character bodies and rows of stubby teeth.\nBarker, an anthropologist who used to work for the Florida Park Service and St. Augustine Alligator Farm, dropped gator tidbits — like the fact that males bellow during mating season to set their territory. The total inches between the alligator’s eyes and snout equal the length of the body in feet, he explained, putting some of our specimens at well over eight feet.\nA few times, we wouldn’t see an alligator nestled in the grasses until we were nearly on top of it. One was only five feet away, its eyes open and it feet moving in slow motion as though it were stretching after a nap.\nWe stopped under overcast skies and listened to the cacophony of squawks, squeaks, caws and chattering. Birds landed on the water like water-skiers, splashed and dove under for fish, flapped their wings, waded and frolicked. All the activity looked and sounded like that of kids swimming in a massive pool. Paynes Prairie was putting on quite a show.\n“The coots are on the march,” Barker said, pointing to countless small birds creating a commotion in the water. He identified the little blue heron, belted kingfisher and mockingbird. We even heard the trumpeting of the beloved sandhill crane. “It’s a bird-watcher’s dream today,” he said.\nAlthough we passed piles of horse manure and hoof prints, we saw no similar signs of bison. Like most visitors, I was surprised to learn that bison roam in a state better known for spring training and Mickey Mouse. But fossils show that they once grazed in north central Florida.\nPark officials decided to import 10 from Oklahoma in 1975, and — with no natural predators — the herd grew, inevitably causing problems such as escapees to the suburbs. So a few years ago, the park culled the herd, removing the males and creating yet more controversies. Today about 50 bison roam the prairie, but they remained elusive on our walk.\nAs we retraced our steps on the trail, we counted 33 alligators. Jason even spied one underwater. “You’ve got a good eye,” I said, squinting to find the animal.\n“I’m getting good gator-dar,” he said, straight-faced.\nJason and I bid farewell to Barker, and we drove to Blue Highway, just outside the park, for lunch. On the radio, I tuned into an AM station that ran a 1950s-sounding promotional loop for Paynes Prairie. A scratchy voice proclaimed, “As wild as the Everglades, but easier to see and experience!”\nLater, I walked on a few trails by myself and realized my sense of place had been completely upended. All day, I’d forgotten I was in Florida. The wide-open prairie made me think of the Great Plains, and the Spanish moss and pickups reminded me of the Deep South. Each instance of remembering my whereabouts was a burst of delight — the type of travel surprise I welcome.\nAs I walked on the pine-needle-and-sand trail, the sun began relaxing into the horizon, and the temperature dropped quickly. I headed back toward my car, mindfully zigzagging, just in case.\nGoing to Gainesville\nGetting there: American Eagle flies nonstop from Miami to Gainesville, a flight of about 1 1/4 hours, with round-trip airfare starting around $260. There are no nonstop flights from Fort Lauderdale, but Delta and Silver offer trips with a connecting flight and travel time under four hours, starting around $290 roundtrip. Gainesville is a 335-mile drive from Miami, most of it along Florida’s Turnpike.\nWHERE TO STAY\nHerlong Mansion, 402 NE Cholokka Blvd., Micanopy; 352-466-3322; www.herlonginn.com. A cracker-style pine farmhouse built in 1845, the mansion now has bright rooms with decks and claw-foot tubs. Rooms from $119, including a full Southern breakfast.\nTwin Lakes Fish Camp, 17105 S. County Rd. 325, Hawthorne; 352-466-3194; www.twinlakesfishcamp.com. Rustic four-person cabins, $91 per night. Guests can check out canoes at no charge; pontoon rentals $110 daily.\nHilton Garden Inn Gainesville, 4075 SW 33rd Pl., Gainesville; 352-338-1466; www.hiltongardeninn.com. Located across from the UF campus and near the Florida Museum of Natural History, with fitness center and free Wi-Fi. Standard rooms start at $139.\nWHERE TO EAT\nSouthern Charm Kitchen, 1714 SE Hawthorne Rd., Gainesville; 352-505-5553; www.facebook.com/southerncharmkitchen. Soul-food heaven, with equally heavenly vegan options and substantial sides. Pentecostal fried chicken, $9.95; spicy corn waffle with Cajun shrimp, $13.50; pickled watermelon tofu, $11.95.\nBlue Highway, 204 NE U.S. Hwy. 441, Micanopy; 352-466-0062; www.bluehighwaypizza.com. Just outside Paynes Prairie. Salads, sandwiches and pizzas with local produce. Funghi pizza with portabello, shiitake, fancy white and oyster mushrooms on a thin crust, $13.50.\nThe Yearling Restaurant, 14531 E. County Rd. 325, Hawthorne; 352-466-3999; www.yearlingrestaurant.net. Old-Florida fare, open for lunch and dinner Thursday to Sunday. Offerings include alligator, frog legs, quail, “cooter” (land turtle) and catfish, entrees $13.95-$31.95.\nWHAT TO DO\nPaynes Prairie Preserve State Park, 100 Savannah Blvd., Micanopy; 352-466-3397; www.floridastateparks.org/park/Paynes-Prairie. The 50-foot observation tower reopens after repairs in March. Admission $6 per vehicle, $2 pedestrians and cyclists. Guided wildlife walk, $2; half-day hike, $5; overnight camping trip and hike, $15.\nWild Florida Adventures, 352-215-4396; www.wild-florida.com. Explore northern Florida and view wildlife by kayak or foot with guide Brack Barker. Half-day kayak tours, including manatee-viewing tours and sunset/moonrise tours, $50; beginners welcome. Paynes Prairie walking tours, $10.\nFlorida Museum of Natural History, UF Cultural Plaza, SW 34th Street & Hull Road, Gainesville; 352-846-2000; www.flmnh.ufl.edu. Thousands of Florida specimens and artifacts, including a fossil plant garden and butterfly rain forest. “A T.rex Named Sue,” featuring a 42-foot dino skeleton cast, on display through Sept. 13. Admission free; fees for some exhibits.', 'The American West once had its own sea: A great, sweeping, endless ocean of waving grasslands, expanding across the western United States from Canada to Mexico. It was a vast land of pristine and fertile ground, stretching off from horizon to horizon, unspoiled by man’s limitless ambitions. And like every sea, it teemed with life. Rabbits and grouse hid in the grass, wary of the foxes and coyotes prowling in the darkness. Antelope bounced across the vast expanses of open land, staying ahead of the packs of wolves that swept down from the islands of plateaus and ridges standing along the plains. And then, of course, there were the bison. The whales of the grasslands, the American buffalo once traveled the prairie in mobs whose numbers rivaled the number of stars in the sky. They were at one time in history the most populous big game animal in the entire country, with herds stretching for miles, the dust from their movements often blocking out the sun itself. And then, one day, they were just gone.\nThe fall of the great American bison herds has been well documented, and it is well known that they were hunted to near-extinction. As man moved west in search of new land, the inevitable tide of civilization clashed with the buffalo. The land the creatures occupied was needed for development. That massive sea of grass they lived on and the soil from which it grew was perfect for the incoming cattle and agriculture the pioneers needed to feed the oncoming masses of humanity looking to settle the West. What was more, the bison themselves were valuable. Their hides made clothing, leather goods, and blankets; their tongues and bone marrow were considered delicacies; and their gall bladders were thought to have medicinal qualities. Buffalo hunting, then, became not only a sport but an occupation.\nFrontiersmen moved across the prairie in search of the animals, shooting as many of them as they could and cutting the hides and tongues from the carcasses. Although the animals had always been hunted, the Plains Indians—who previously hunted them only for subsistence and had vast knowledge of the herds and their movements—with the coming of the fur trade in the United States, also began to hunt buffalo for profit. Natives in the West had acquired horses and improved weapons, and quickly became involved in the “robe trade”—trading buffalo robes for manufactured trade items such as cooking pots, knives, and guns, as well as cloth, glass, tobacco, sugar, and flour. Though the natives respected and loved the bison, it was a classic case of rising material expectations. Around 1830, as the demand for beaver pelts was replaced by silk, buffalo robes became the currency of trade for almost every trapper, mountain man, cowboy, and Indian seeking their fortune in the “New World.” Over the next 60 years the bison herds, which at one time numbered almost 50 million animals, were cut down to fewer than 1,000 animals by 1880. The few remaining animals hid in distant corners of the country. The West was “won,” and the mighty bison had nearly vanished with the victory, never to return again—or so it was thought.\nIn 1905, with bison numbers still dwindling, President Theodore Roosevelt, arguably the greatest conservationist of our nation’s history, along with William Hornady and Ernest Baynes, formed the American Bison Society (ABS). Working with both the Bronx Zoo—where Hornady served as director—and more prominently Yellowstone National Park, which had already established a bison preserve in 1902 containing two dozen animals, the American Bison Society set out to prevent the bison from going completely extinct and create awareness of the importance of the animal. They banned all hunting of bison and emplaced protective measures on areas they inhabited. They worked with Congress to make the American bison the country’s national mammal and then set about repopulation efforts.\nIn 1907, they shipped 15 buffalo from the Bronx Zoo to the Wichita Mountains in Oklahoma, which became the first official animal reintroduction in North American history. They also founded and established herds in South Dakota and Nebraska. Two years later, ABS successfully petitioned Congress to establish a permanent national bison range in northern Montana. Thanks to ABS’s efforts, by 1920, the bison’s population had grown from fewer than 1,000 animals to nearly 12,000. Once all of the herds were established and secure, the American Bison Society disbanded in 1935, but their actions served as a catalyst for the country, sparking a desire to bring the bison back.\nToday there are more than half a million American bison living in North America. Although most of these animals are semi-domesticated livestock whose meat has created its own niche market, or fenced-in animals on private ranches, there are around 20,000 wild, free-roaming bison in the country—a population that continues to grow. Alaska, Arizona, Montana, Utah, and Wyoming all have wild buffalo populations that one can hunt via lottery drawings. These hunts support future bison conservation, with most of the funds for the lottery draws going toward shaping the health and longevity of the wild herds.\nHunting bison today is ambitious: although the North American bison’s population has returned to substantial numbers, hunting them has changed from stalking massive herds in relatively small areas to hunting small groups of animals throughout massive areas of land. Hunters lucky enough to draw a tag find the bison a worthy and challenging game animal. In most states the hunters’ success rate for bison is less than 50 percent, but the volume of delicious, high-quality meat, along with the robe and massive skull more than make up for the possibility of coming up empty in the hunt. By hunting bison today we are contributing to and enjoying a great American success story. The American bison stands not only as a symbol of the country but as a symbol of the power of conservation, proving that a species can be brought back from the brink.']	['<urn:uuid:9a98edde-e215-4f69-8af3-f673eaee54dd>', '<urn:uuid:f769a7f4-88bd-4030-840e-3a6a3c78aafc>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	25	58	2632
75	hippocampus brain region controls spatial memories seizure resistance connection explain	The hippocampus is involved in creating spatial memories, with neurons firing slow signals during exploration and intense signals during memory consolidation in communication with the septum. Research shows that this same brain region's excitation/inhibition balance is crucial for epilepsy control - specifically in the Dentate Gyrus area of the hippocampus, where proteins like synapsin2b and GABAAR can be modified to provide resistance to epileptic seizures.	"['Stanford scientists reveal complexity in the brain\'s wiring diagram\nA Stanford Bio-X team found that the brain\'s wiring is more complex than expected – one set of neural wires can trigger different reactions, depending on how it fires. The work opens new questions for scientists trying to map the brain\'s connections.\nStanford Bio-X researchers raised an entirely new set of questions when they sought answers about connections between two brain regions.\nWhen Joanna Mattis started her doctoral project she expected to map how two regions of the brain connect. Instead, she got a surprise. It turns out the wiring diagram shifts depending on how you flip the switch.\n""There\'s a lot of excitement about being able to make a map of the brain with the idea that if we could figure out how it is all connected we could understand how it works,"" Mattis said. ""It turns out it\'s so much more dynamic than that.""\nMattis is a co-first author on a paper describing the work published August 27 in the Journal of Neuroscience. Julia Brill, then a postdoctoral scholar, was the other co-first author.\nMattis had been a graduate student in the lab of Karl Deisseroth, professor of bioengineering and of psychiatry and behavioral sciences, where she helped work on a new technique called optogenetics. That technique allows neuroscientists to selectively turn parts of the brain on and off to see what happens. She wanted to use optogenetics to understand the wiring of a part of the brain involved in spatial memory – it\'s what makes a mental map of your surroundings as you explore a new city, for example.\nScientists already knew that when an animal explores habitats, two parts of the brain are involved in the initial exploring phase and then in solidifying a map of the environment – the hippocampus and the septum.\nWhen an animal is exploring an environment, the neurons in the hippocampus fire slow signals to the septum, essentially telling the septum that it\'s busy acquiring information. Once the animal is done exploring, those same cells fire off intense signals letting the septum know that it\'s now locking that information into memory. The scientists call this phase consolidation. The septum uses that information to then turn around and regulate other signals going into the hippocampus.\n""I wanted to study the hippocampus because on the one hand so much was already known – there was already this baseline of knowledge to work off of. But then the question of how the hippocampus and septum communicate hadn\'t been accessible before optogenetics,"" Mattis said.\nNeurons in the hippocampus were known to fire in a rhythmic pattern, which is a particular expertise of John Huguenard, a professor of neurology. Mattis obtained an interdisciplinary fellowship through Stanford Bio-X, which allowed her to combine the Deisseroth lab\'s expertise in optogenetics with the rhythmic brain network expertise of Julia Brill from the Huguenard lab.\nMattis and Brill used optogenetics to prompt neurons of the hippocampus to mimic either the slow firing characteristic of information acquisition or the rapid firing characteristic of consolidation. When they mimicked the slow firing they saw a quick reaction by cells in the septum. When they mimicked the fast consolidation firing, they saw a much slower response by completely different cells in the septum.\nSame set of wires – different outcome. That\'s like turning on different lights depending on how hard you flip the switch. ""This illustrates how complex the brain is,"" Mattis said.\nMost scientific papers answer a question: What does this protein do? How does this part of the brain work? By contrast, this paper raised a whole new set of questions, Mattis said. They more or less understand the faster reaction, but what is causing the slower reaction? How widespread is this phenomenon in the brain?\n""The other big picture thing that we opened up but didn\'t answer is: How can you then tie this back to the circuit overall and learning memory?"" Mattis said. ""Those would be exciting things to follow up on for future projects.""\nOther Stanford authors on the paper include postdoctoral scholars Talia Lerner and Thomas Davidson, Bio-X undergraduate fellow Minsuk Hyun and research assistant Charu Ramakrishnan.\nThe work was supported by Stanford Bio-X, the National Institute of Neurological Disorders and Stroke (grants ns034774, ns06477, ns34774, n39579), a Stanford Dean\'s Postdoctoral Fellowship, the Stanford Department of Bioengineering, a Berry Postdoctoral Fellowship, the National Institute of Mental Health, the National Institute on Drug Abuse, the Defense Advanced Research Projects Agency and the Gatsby Foundation.\nAmy Adams, Stanford News Service, 650 796-3695, firstname.lastname@example.org', 'Context Memory Damage Linked To Epilepsy Seizure Resistance\nA biological mechanism that damages a specific type of memory, but at the same time provides resistance to epilepsy, has been found by researchers from the Sagol Department of Neurobiology at the University of Haifa and European researchers.\nResearch student Elham Taha from the laboratory of Prof. Kobi Rosenblum, who undertook the research, explains:\n“In both healthy and sick brains, the relationship between the activities of the nerve cells that cause the transfer of information and activities delaying the transmission of information is extremely important. We know that damage to this relationship forms the basis of various brain diseases, such as neuro-developmental diseases and epilepsy.\nThe aim of our study was to isolate molecular components that serve the creation of long-term memories. We were surprised to find that the molecular change we created led to a minor change in this relationship in the hippocampus, but also created resistance to epileptic seizures. Thus the finding creates new possibilities for developing drugs for the treatment of epilepsy.”\nRNA Protein Translation\nAs with many scientific discoveries, the researchers came across their finding by chance.\nTaha’s doctorate thesis focuses on the study of the underlying processes behind the creation and preservation of memories. More specifically, she is examining the control of RNA to protein translation.\nIn the present study, the researchers examined what happens to a mouse who has undergone genetic modification causing the total non-expression of the protein eEF2K. Previous studies have shown that damage to this protein causes damage to memory.\nThe mice underwent a long series of behavioral tests. None of the tests identified damage to the consolidation of memory, with the exception of one specific type of memory: context memory – the memory created relating to the context (usually the spatial context) of learning.\nThese experiments found specific damage to the function of an area known as the hippocampus.\nGABAAR And Synapsin2b\nThe researchers then wanted to examine the electro-physiological and molecular biology activity in the brains of these mice. They found that the hippocampus shows increased expression of a sub-unit of a receptor called GABAAR.\nThis receptor is located in the membrane of the nerve cell, and its hyperactivity causes cells to be less active, thereby delaying information rather than transmitting it.\nIn addition, elevated expression of the protein synapsin2b was also found. This protein is key modulator of neurotransmitter release in neurons.\n“We realized that, surprisingly, the change in the general translation control element, eEF2K, changes the excitation/inhibition ratio in a specific area of the brain,” Taha explains. “This area, the Dentate Gyrus in the hippocampus, as well as the molecules whose expression changed, are associated with epilepsy. For example, mutation in synapsin2b in humans or a decline in its expression may lead to epilepsy.”\nEpileptic Seizures Gone\nSo the joint research team then examined whether down regulation of eEF2K that heightens the expression of synapsin2b will influence epilepsy model mice with low expression for this protein. In the first test, the researchers took a male mouse with no expression of eEF2K and a female epilepsy model mouse and bred them.\nWhen epilepsy model mice breed, the offspring are almost always born with epilepsy. In this case, the offspring were born without epileptic seizures, as shown by EEG tests.\nIn the second test, a substance inhibiting the expression of eEFK2 was injected into mice with epilepsy. EEG tests showed that they did not suffer from epileptic seizures for one week from the time of injection. In both cases, biological examinations showed that the expression of synapsin2b had become normal.\n“We effectively managed to cause a situation whereby mice that should have been born with epilepsy were born healthy, and mice that have epilepsy were cured, at least for the duration of time in which the expression of eEF2K was suppressed. The results create a possibility for a better understanding of the excitation/inhibition balance in the hippocampus, a vital area of the brain for cognitive processes that is associated with various cerebral pathologies.\nIn the next stage, we will attempt to find ways to cause the suppression of the expression of the protein only in certain nerve cells, in order to improve our understanding of the basis of epilepsy and to create new possibilities for treating the disease,”\nthe researchers added.\nChristopher Heise, Elham Taha, Luca Murru, Luisa Ponzoni, Angela Cattaneo, Fabrizia C. Guarnieri, Caterina Montani, Adele Mossa, Elena Vezzoli, Giulio Ippolito, Jonathan Zapata, Iliana Barrera, Alexey G. Ryazanov, James Cook, Michael Poe, Michael Rajesh Stephen, Maksym Kopanitsa, Roberta Benfante, Francesco Rusconi, Daniela Braida, Maura Francolini, Christopher G. Proud, Flavia Valtorta, Maria Passafaro, Mariaelvina Sala, Angela Bachi, Chiara Verpelli, Kobi Rosenblum, Carlo Sala eEF2K/eEF2 Pathway Controls the Excitation/Inhibition Balance and Susceptibility to Epileptic Seizures Cerebral Cortex, 2016; bhw075 DOI: 10.1093/cercor/bhw075']"	['<urn:uuid:8ef57bf1-3350-44b8-9998-48c84512ae01>', '<urn:uuid:565b8392-47d8-4e9a-8529-4e35cb97f742>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T12:39:32.578853	10	65	1561
76	china oldest string music tool history	The qin (commonly known as guqin), a seven-string zither, has over three thousand years of well-documented history.	['Playing the Classical Chinese Literati Tradition\nI have great respect for all musical traditions and have enjoyed collaborating in past decades with great musicians from different cultural backgrounds, including African, Arab, Indian, Japanese, Vietnamese, and Western. It was indeed mind-broadening and exciting because such collaboration often involves improvisation.\nWhile music in this context is often about expressing mood and feeling, it is fascinating to experience how musicians from different traditions, who play different instruments, come together thematically. It is inspiring and even profoundly creative, particularly when organizers provide sufficient time for musicians to rehearse before concerts. This is especially true when musicians meet with an open heart, wanting to learn from each other and not simply be satisfied with things just sounding “okay.”\nIn the contemporary context, it is also a great joy to perform with individual musicians, string quartets, or orchestras where the pieces are mostly composed. Here the challenge, however, is more for the composers than for the performers. It is very demanding for a composer to create a piece for a traditional instrument like the pipa (a four-stringed Chinese lute) that fully explores the typical features of the instrument to such an extent that it would not sound equally satisfying if played on some other instrument, such as the guitar or banjo. This requires the composer to become familiar with the instrument, not just the sound and scales. Amazingly, there are still a number of excellent pieces available by various composers.\nFor my solo concerts, I play mostly traditional Chinese music from the classical literati tradition, which can be considered as “traditional classical music” in contrast to folk traditions or the entertainment music industry. Traditional classical music refers to art music or “sophisticated” music composed by scholars and literati in China’s historical past. This type of music often has thematic, poetic, or philosophical associations and is intimately linked to poetry. As poetry, it sets out to express human feelings, soothe suffering, purify the mind, or bring spiritual elevation. This type of music was noncommercial, typically played solo on private occasions for oneself for the purpose of meditation or self-cultivation, or for Zhi Yin—literarily translated as “know music,” which has become a synonym for intimate friends and music lovers, on instruments such as the qin (commonly known as guqin), a seven-string zither with over three thousand years of well-documented history, or the pipa, a lute with over two thousand years of history. The music is supposed to be played in a meditative state, and the feeling and energy are present and should remain fresh even when repeating the “same old pieces.”\nThe challenge here is the current tendency of constantly looking for “new,” underscoring that only the “original and new” music has artistic value, a viewpoint that probably came from the late nineteenth and early twentieth centuries regarding the artistic value for arts (in particular paintings). While there is nothing wrong with creating new—and indeed, today’s good creation will be tomorrow’s tradition—it is misleading to use this to judge music-making, particularly for music that came from oral traditions. Even for the same piece, if played from a “true self” in the state of meditation, every performance is “original and new,” not to mention the different environments and audiences. In a real sense, one cannot separate music from musician. When the musician dies, his or her music no longer exists. Recording is not the real thing. That is repeating, not music-making, which is very different from painting.']	['<urn:uuid:124878e7-3a8e-468a-99d0-36535f47d0d8>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	6	17	578
77	acoustic guitar body construction first steps	The first steps in acoustic guitar body construction involve dimensioning the wood for the top, back, and sides. The top and back plates are thickness-sanded to dimension after being joined in a book-matched pattern. Then a soundhole is cut into the top, a rosette is installed, and the surfaces are profiled.	['- Rig Rundowns\n- Premier Blogs\nWith a very steady hand, luthier Jeff Huss preps a fretboard for its side-position markers.\nBuilding an acoustic guitar may seem like an extremely complicated process, but it can be broken down into a series of chronological steps. While it would take a book (and there are some good ones out there) to describe how to create this fairly simple, yet elegant machine in more detail, I’d like to give you an overview of the process with this month’s column.\nBecause we build our bodies first, we start by dimensioning the wood for our top, back, and sides. Choosing the top wood for its stiffness-to-weight ratio, we generally make our tops from spruce or cedar. This means these woods are light enough to not act like a “tone sink,” but strong enough to withstand the incredible amount of tension they will be subjected to when the guitar is strung up. While back and side woods color the tone that the top produces, the majority of the sound is the result of the construction choices made for the top.\nThe top and back “plates” are thickness- sanded to dimension after they have been joined in a book-matched pattern. We cut a soundhole into the top, install a rosette, and then profile them—a process that creates a smooth, flat surface where the top meets the rosette. Next, we brace the plates with spruce bracing. We use a traditional “X” pattern for our tops and a simpler, ladder-brace pattern for the backs. At this point, we profile the sides to a uniform thickness and bend them to shape using a bending jig and the help of a little heat and moisture.\nAfter bending the sides, we place them into a mold that will house the body throughout the construction process, and install spreaders to hold the sides firmly against our mold. Now we’ll glue the head block, tail block, and kerfed linings into place to create the rim onto which we glue the top and back. When the glue is cured, we remove the spreader mechanism through the soundhole, pull the rough body from the mold, trim the edges, and finish the decorative purfling and binding work. After the scraping and sanding work is finished, the body is complete and ready to be paired with a neck.\nNecks begin as a large plank of hardwood— generally mahogany—that is sawn into smaller billets yielding two, one-piece necks. These are rough sawn into neck blanks on the band saw, routed for a truss rod slot, and cut on an angle for the peghead. After we glue the veneer for the peghead face and shape the peghead, we drill the holes for the tuners. Using a jig, we then measure the angle of the body where the neck will attach, and that angle is transferred to the table saw where the neck heel is cut to correspond to the body. After cutting, we drill and install our corresponding index holes, bolt holes, and inserts in the body and neck heel for the bolt-on neck joint.\nOur fretboards—made from ebony or rosewood—are shaped, slotted, radiused, and bound. They are indexed onto the neck blank, and the truss rod is installed and then attached with epoxy. The neck is now shaped, a nut slot is cut, and the neck is final-fitted to the body to ensure proper playability. Inlay work is then done on the fretboard and peghead, and we fit the nut, install side-position markers, and sand out the body and neck and prepare them for the finish work.\nAt this point, some woods, such as mahogany, are stained, and any open-pored woods are filled with a paste wood-filler that lets us apply a thinner, final finish. Then we spray the topcoats and do leveling work between coats to ensure a flat and thin finish. After it’s cured, the finish is leveled once again before we fine-sand and buff all the finished surfaces to bring out a bright luster.\nOnce the finish work is complete, the assembly can begin. The neck and body are bolted together, the fretboard extension is glued down, and the bridge is glued on. The guitar is mounted on a workbench that simulates string tension, and the fretboard is prepared for the fretwork. Frets are then installed, trimmed, leveled, and polished.\nAfter the nut has been trimmed, polished, and slotted, we install it. Next, the tuners are mounted, the saddle is shaped and inserted into the bridge slot, and the pickguard is attached. And finally, we string up the guitar, adjust the action, and perform any necessary tweaks. Our guitar is now ready to begin what we hope are many, many years of making music.\nco-owner of Huss & Dalton Guitar Company, moved to Virginia in the late ’80s to play bluegrass. He and his business partner, Mark Dalton, formed their company in 1995. Since then they’ve earned world-wide recognition for their high-end, boutique guitars and banjos.']	['<urn:uuid:ab0f46eb-8b48-417b-811a-cd95b868af4e>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	6	51	830
78	what dna does genetic testing look for mutations effects on cancer and heart disease	Genetic testing searches for mutations in DNA (changes in genes, chromosomes, or proteins) that can be responsible for diseases. For cardiomyopathies, mutations in genes like MYBPC3, MYH7, and others can affect heart function. In cancer, specific mutations can increase cancer risk - though having a mutation doesn't guarantee developing the disease. For example, a woman may have a 45-65% chance of breast cancer due to mutations but might never develop it. The mutations can be inherited from parents to children in both heart diseases and cancers.	"['Cardiomyopathies: genetic counseling and analysis\nAbout the history: the structure of DNA was discovered in 1953, thanks to the studies of Rosalind Franklin, who was the first to imagine its helical structure. The images obtained were then used by Watson and Crick to prove their theory (1958), for which they won the Nobel Prize in 1962.\nWhat is DNA, i.e. the genetic code? Every single cell of any living organism contains DNA, a very complex structure composed mainly of a double strand, coiled around itself to form a double helix. DNA contains the genetic code, formed by an alphabet of only 4 letters (ATGC), aligned in different sequences of 3 consecutive letters, i.e. triplets, called ""codons"", allow to produce 20 fundamental amino acids (e.g. Arginine, Lysine, Glycine, Alanine, etc). These amino acids, in turn, by binding together, produce all the proteins necessary for the functioning of our body.\nA ""gene” consists of a variable number of codons, which encode amino acids, which are used to produce different individual proteins such as, for example, ""Myosin"", ""Troponin T. or I"", ""Myosin-binding protein C"", essential proteins for contraction of heart cells, or ""Desmoplakin"" for cell-cell adhesion, or ""Titin"".\nIn XNUMX the genetic code of man was analyzed and over XNUMX genes have been identified.\nWhere is DNA? DNA is delicate and its integrity must be preserved. It is enclosed within the cell nucleus, where it replicates at each cell division in order to possibly keep the genetic information of that organism or individual intact. Unable to leave the nucleus, DNA needs an intermediary, called RNA, which copies the information encoded in the genes and transports it to the area of the cell around the nucleus called ""cytoplasm”, where proteins are assembled and main activities of the cell itself take place.\nMessenger RNA, a molecule similar to DNA, but with only one strand, copies the code from the corresponding DNA, and functions as a template for each specific codon. Each codon produces its own amino acid and the amino acid sequence forms a protein of precise length and spatial conformation to ensure the function it will have to perform.\nWhat happens if the code is altered, or is incorrectly transcribed?\nVarious errors can occur, called ""mutations"" or ""variants"", which produce a protein, for example, missing some parts, such as a book that is missing a few pages. A malfunctioning protein can be responsible for diseases, and therefore also Cardiomyopathies, which can be recognized at birth, more often in adulthood. If the ""mutations"" are transmitted to children, these diseases are defined ""hereditary”, as in most cases, for example, of Cardiomyopathies. However, mutations can also play a positive role. They contribute to the diversity of individuals, for example. in children or in a population, so as to ensure the variability of the species and the continuous adaptation to the changing environment.\nToday we know thousands of ""mutations"" in numerous genes that cause cardiomyopathies, although several genes still remain to be identified.\nThe search for ""mutations"" in genes known so far is important to well evaluate the patient who has cardiomyopathy and his family members.\nGenetic analysis (NGS - next generation sequencing)allows to identify one or more mutations(also called "" variants"") of the genetic code in one or more genes, responsible for the development of cardiomyopathy. For each type of cardiomyopathy, the most frequently encountered gene groups are analyzed.\nFor example, XNUMX genes are initially analyzed for patients with hypertrophic cardiomyopathy: MYBPCXNUMX, MYHXNUMX, TNNTXNUMX, TNNIXNUMX, TPMXNUMX, ACTCXNUMX, MYLXNUMX, MYLXNUMX, GLA, LAMP XNUMX, PRKAGXNUMX, TTR. If genetic mutations in these genes are not identified, a more in-depth analysis may be requested by the cardiologist with extension of the analysis to further gene groups.\nIn some laboratories, a greater number of genes are immediately analyzed (e.g. 45 or 174) and in some cases it is also possible to examine the whole set of genes (Exome).\nGenetic analysis allows to :\n- precisely identify the type of disease underlying Cardiomyopathy\n- choose the most appropriate therapy\n- carry out a more accurate assessment of the risk of evolution and complications in subsequent years, obviously together with other clinical and instrumental elements (for example: ECG, Echocardiogram, cardiac magnetic resonance, cardiorespiratory test, dynamic ECG)\n- check if there is familial transmission of Cardiomyopathy, by looking for the mutation in family members as well\nWith current techniques the detection of a genetic variant is more frequent if one cardiomyopathy has already been diagnosed, through ECG and ECHO, also in other family members (the probability is about 60%, while it is lower if it has not been found in family members, about 30%).\nGenetic testing must be preceded by a genetic consultation, made by the geneticist or an experienced cardiologist, as well as the interpretation of the written report..\nTHE FUTURE : One of the objectives of scientific research is the correction of genetic mutations, to be used in treatment of cancers and hereditary genetic diseases, including cardiomyopathies. However, genetic modification involves numerous ethical dilemmas and is still under study.', 'Genetic testing helps estimate your chance of developing cancer in your lifetime. It does this by searching for specific changes in your genes, chromosomes, or proteins. These changes are called mutations.\nGenetic tests are available for some types of cancer. These include:\nGenetic testing may help:\nPredict your risk of a particular disease\nFind if you have genes that may pass increased cancer risk to your children\nProvide information to guide your health care\nNo genetic test can say if you will develop cancer for sure. But it can tell you if you have a higher risk than most people.\nOnly some people with a gene mutation will develop cancer. What does this mean? A woman may have a 45% to 65% chance of breast cancer. But she may never develop the disease. Meanwhile, a woman with a 25% chance may develop breast cancer.\nRisk factors for hereditary cancer\nA hereditary cancer is any cancer caused by an inherited gene mutation. An inherited gene means it is passed from parent to child within a family. The following factors suggest a possible increased risk for hereditary cancer:\nFamily history of cancer. Having 3 or more relatives on the same side of the family with the same or related forms of cancer.\nCancer at an early age. Having 2 or more relatives diagnosed with cancer at an early age. This factor may differ depending on the type of cancer.\nMultiple cancers. When one relative develops 2 or more types of cancer.\nRare cancers. Some types of cancer, such as ovarian cancer, adrenocortical cancer, or sarcoma, are linked to inherited genetic mutations.\nReasons to consider genetic testing for cancer\nGenetic testing is a personal decision made for different reasons. It is also a complex decision best made after talking with your family, health care team, and genetic counselor.\nASCO recommends considering genetic testing in the following situations:\nA personal or family history suggests a genetic cause of cancer.\nA test will clearly show a specific genetic change.\nThe results will help with diagnosis or management of a condition. For example, you may take steps to lower your risk. Steps may include surgery, medication, frequent screening, or lifestyle changes.\nASCO also recommends genetic counseling before and after genetic testing. Learn more about these recommendations on genetic testing for cancer susceptibility on a separate ASCO website.\nOther factors to consider\nGenetic testing has limitations and emotional implications. These may include:\nDepression, anxiety, or guilt. A positive test result means a gene mutation exists. This result may bring difficult emotions. Some people may think of themselves as sick, even if they never develop cancer. Negative test results may also cause difficult emotions. For example, some people may experience guilt if they do not have a gene mutation that other family members have.\nFamily tension. People are generally encouraged to tell family members about test results because they can be important for the health of family members. But this information could also complicate family dynamics. Learn more about sharing genetic test results with your family.\nA false sense of security. A negative result means a specific genetic mutation is not present. But people with negative results may still develop cancer. A negative result only means the person’s risk is average. Each person’s risk for cancer is also affected by other factors. For example, lifestyle, environmental exposure, and medical history.\nUnclear results. A gene may have a mutation not linked with cancer risk. This is called a variant of unknown significance. It means that it is unclear whether the mutation will increase risk. Or people may have mutations that current tests cannot find. Many cancers are not yet tied to specific gene mutations. Also, some genes may interact unpredictably with other genes or environmental factors. And these interactions may cause cancer. So it may be impossible to calculate the cancer risk.\nHigh cost. Genetic testing can be expensive. It is particularly expensive if health insurance does not pay for it.\nDiscrimination and privacy concerns. Some people fear genetic discrimination from test results. Others worry about the privacy of their genetic information. The Genetic Information Nondiscrimination Act (GINA) protects against employment and health insurance discrimination. Discuss related concerns with a genetic counselor or doctor.\nQuestions to ask yourself about genetic testing\nBefore having genetic testing, learn about its risks and limitations. Identify your reasons for wanting a test. And consider how you will cope with test results.\nHere are some questions to help you make a decision:\nDo I have a family history of cancer?\nHave I developed cancer at an earlier-than-average age?\nHow will I interpret the results of genetic testing? Who will help me use this information?\nWill the test results affect my medical care? The medical care of my family?\nIf I have a genetic condition, can I lower my cancer risk?\nA genetic counselor can help you answer these questions. He or she can give you advice and information about the risks and benefits of genetic testing. A genetic counselor also helps people through the genetic testing process. Learn more about what to expect when meeting with a genetic counselor.']"	['<urn:uuid:2cf59bfd-162d-41e7-8345-66cd27f88087>', '<urn:uuid:1c356b00-d237-47b5-ad3a-558703f64326>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T12:39:32.578853	14	86	1691
79	What are car seller's requirements for contracts, and buyer's rights when things go wrong?	For contracts, every state requires a sales contract between seller and buyer, which must include seller and buyer details, price, vehicle information (make, model, year, VIN, mileage), and signatures. Five states require notarization: West Virginia, New Hampshire, Louisiana, Maryland, and Nebraska. Regarding buyer's rights, when purchasing from a dealer, the car must be merchantable, fit for purpose, and as described. Dealers cannot provide misleading information or withhold details. However, with private sales, buyers have limited rights and no warranty protection - their main recourse is taking civil court action.	"['After all your intense negotiating when selling your car, it’s time to draw up the papers and sign on the dotted line. Every state requires a sales contract between seller and buyer when selling a vehicle, according to the Cars Direct website. The point of the contract is to ensure that both you and the buyer agree on all terms and everyone knows the deal.\nNegotiate the Deal\nAdvertise your car for sale to attract potential buyers. Place a listing online or in your local newspaper. Or go the old-school route and park your car on a busy street with a “for sale” sign in the window. List all relevant information, including make, model, year, mileage, selling points and possible mechanical issues, for buyers to read. Include your asking price, too.\nShow the car to potential buyers. Answer questions and allow people to examine the automobile. If buyers want to drive the car, check for a valid driver’s license and then ride along to make sure they drive carefully.\nNegotiate with an interested buyer to agree on a purchase price. You may need to reduce your asking price slightly during negotiation, depending on the buyer’s position. When you come to an agreement on price, shake hands on the deal.\nCreate the Contract\nDraw up a car sales contract to put the details of the deal in writing. Generally, a car sales contract identifies the seller – you – by name and then lists your address, county and state. Next, write out the agreed price in words and figures, state that you have received this payment, and write the current date. State that you are conveying or selling the car to the buyer. Include the buyer’s full name, and list the address, county and state of the buyer.\nAdd important details to the car sales contract. Include the make, model, year, color, body style, registering state, the Vehicle Identification Number and any accessories you are including with the car. Write the exact mileage on the car sales contract and state that the odometer is accurate to the best of your knowledge, if applicable. If you know that there is an odometer discrepancy, you must state this in the sales contract.\nList all the representations by you, as the seller, about the car. For example, include that you are the legal owner, that you have the right to transfer the title (no liens exist) and that all information provided by you about the car is accurate. Include an “as-is” clause to make sure that the buyer knows you are selling the car “as is” (without a warranty).\nCreate signature lines for the seller and buyer to sign the contract. Create a line beneath the signature lines for printed names. Add lines for the dates as well.\nMake two copies of the sales contract – one for you and one for the buyer.\nFind out if you need to have your car sales contract notarized. If you live in West Virginia, New Hampshire, Louisiana, Maryland or Nebraska, you must have your contract notarized, according to the Cars Direct website. In this case, don’t sign the contract until you’re in the presence of a notary public. If you live in one of the 45 other states, notarization is optional. You can just sign and date both copies of the contract to seal the deal.\n- If you don’t know what to ask for your car, visit the Kelley Blue Book website or the NADA Guides website to find out. Enter the make, model, year, condition and features of your car into the value tool, and you will get a value estimation of your car.\n- Creatas Images/Creatas/Getty Images\n- What Is the Difference Between Under Contract & Sold in Houses?\n- How to Return a New Financed Car\n- Can a House Under Contract Be Sold?\n- How to Sell a Car to a Private Party Through an Installment Plan\n- How to Buy a New Car & Not Get Screwed\n- How to Sell a Nonoperating Vehicle\n- How to Buy a For Sale by Owner Car\n- Documents Needed to Sell Your Own Home Without an Agent', ""Cars - your rights when things go wrong\nIf you buy a second hand car from a dealer, you have some protection under consumer law. However, if you buy privately, you do not have the same consumer rights because the person selling the car is not selling it as part of their business.\nYour rights when you buy from a dealer\nWhen you buy from a dealer, the car should be:\n- Of merchantable quality – it should be of reasonable, acceptable quality given the age and history of the car\n- Fit for the purpose intended and roadworthy\n- As described – it should match the description given verbally or in an advertisement.\nIt is an offence for a dealer to provide misleading information about the car including its history, specification and any repair work needed. A dealer must not give you misleading information about themselves such as any authorisation they claim to have, their after sales service or any code of practice in place. It is also an offence under consumer law for a dealer to withhold information when selling a car. There are a number of things to watch out for when buying a second hand car. Our car buyer’s checklist (pdf) will also help you to ask the right questions.\nYour rights when you buy privately\nYou do not have the same consumer rights if you are buying a car from a private seller. Generally, private sales do not come with a warranty so you should ask a mechanic to check the car before you buy it. You should always get the seller’s address or telephone number so that you can contact them if you have problems later on. Always know what to look out for when buying privately. Use our handy downloadable car buyer's checklist (pdf) to help you keep track and make sure you get all the answers you need.\nIf you buy a car from a private seller and discover a problem, there may be little you can do beyond taking a civil case through the courts. Because of this you should be very careful when buying from a private seller and you really need to be aware of what you’re buying. Remember, a private seller may not have all the answers, so it is important for you to have the car checked by a mechanic.\nCar clocking is where a car’s mileage has been adjusted to show that it has done less miles than it actually has. Car clocking puts the safety of road users at risk as the more wear and tear on a car the higher the risk of a breakdown or accident. It is illegal to sell a clocked car. If you believe you have been sold a clocked car or that the seller misrepresented the mileage you should report this as soon as possible to the Gardaí. You can also contact us. We investigate all complaints and take action where warranted. However, we cannot investigate a suspected clocked car if you have had it for more than two years, or if the garage which sold it to you has closed.\nBe careful - are you buying a car that is still under a finance agreement?\nCheck the registration documents of a second hand car to make sure that it is not already owned by a finance company. If this is the case, the person trying to sell you the car does not actually own it and does not have the right to sell it to you – even though they may have the log book and registration documents. If you buy a car under an existing car finance agreement, you run the risk of the car being repossessed by the owner (the finance company). There is a much higher risk of this happening if you buy from a private seller so be sure to check. Hire Purchase Information Ltd keeps records of cars subject to HP agreements. You can request a copy of a vehicle finance report through their website for a small fee.\nWhat to do if something goes wrong\nIf you feel you have been misled about any aspect of the car or if something goes wrong after buying the car, you should:\n- Contact the seller immediately and ask what they intend to do about the problem. If the car is under warranty, check the terms and conditions to see what it says about the seller’s obligations. Your normal consumer rights may still apply.\n- If the seller is a member of the Society of Irish Motor Industry (SIMI), the contract you signed when buying the car may require you to go through the SIMI complaints process. The investigation and complaints service of SIMI deals with complaints relating to:\n- buying used vehicles from SIMI member companies and\n- service or repairs of vehicles by SIMI member companies except those subject to a manufacturer’s warranty claim. For more information, check out the website www.simi.ie.\n- If your contract does not require you to go through the SIMI complaints process, or if you’ve gone through the process and you disagree with the outcome, you can take legal action to try to get a resolution to the problem. If your claim does not exceed €2,000, you have the option of making a claim against the seller through the Small Claims procedure.\n- If you bought a car privately, you cannot go through the Small Claims procedure because you cannot take a case against another consumer. However, you can still consider legal action by consulting with a solicitor.\n- The Small Claims procedure does not deal with cases relating to finance agreements, including hire purchase agreements, as you do not own the car until you make the last payment.\n- You may want to get an independent mechanic to check the car out on your behalf and write a report on the fault that has occurred. If the report states that the fault should not be occurring in a car of its age and condition then you may be able to use this report to make a claim against the garage.""]"	['<urn:uuid:055c37e5-7530-4dbb-968b-0694bd17cd1b>', '<urn:uuid:8a0fc998-8361-48d0-8f20-035f30103c55>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	14	89	1718
80	trauma effects on fascia tissue health and probiotic supplementation safety considerations comparison	Both fascia health and probiotic supplementation require careful consideration regarding trauma and safety. When experiencing physical trauma, fascia loses its pliability and creates restrictions that can produce significant pain, pressure, and range of motion loss. These restrictions can affect flexibility, stability, and cellular nutrition throughout the body. Similarly, safety considerations are crucial with probiotics, particularly in individuals with severe illnesses or compromised immune systems. Probiotics can potentially cause harmful effects including infections, production of harmful substances, and transfer of antibiotic resistance genes. Additionally, some probiotic products may contain unlisted microorganisms that could pose serious health risks, making careful evaluation necessary in both cases.	['Any traumatic force to the head can cause restrictions in the dural fascia, as well as the superficial and deep fascia of the head and neck. Restrictions in this area can affect the whole body. Myofascial Release is a technique for the evaluation and treatment of pain and dysfunction. The technique is gentle, and the goal is to restore mobility in fascia and to soften connective tissue that has become rigid. It works very well as part of a comprehensive physical therapy program including therapeutic exercises and modalities.\nFascia is a loose, irregular connective tissue that spreads three-dimensionally throughout the body. It covers the muscles, bones, nerves, organs, and vessels. It consists of four layers:\n• Subcutaneous - continuous layer over the entire body between the skin and deep fascia\n• Deep - series of sheets and bands that hold muscles and other structures in place\n• Subserous - lies between the fascia and serous membranes lining the body cavities\n• Deepest - within dura mater of the craniosacral system (cranium, spine, sacrum)\nThe fascia can not be seen on x-rays, CT scans or EMGs. Its functions include support, protection, separation, cellular respiration, elimination, fluid flow, immune system function, and allowing the body to resist mechanical stresses. All structures of the body can be affected when fascial restrictions occur.\nFascia is a system in the body that looks like a spider’s web. It is densely woven over and in every muscle, bone, nerve, arteries/veins, and all of our internal organs. The fascial system can be thought of as one continuous structure that twines throughout our body, interconnecting everything. This provides strength to the argument that everything in our bodies are connected—when one thing is out of whack, multiple things can be affected.\nFascia in a healthy state is relaxed, stretchy, and movable. When one experiences physical trauma, fascia loses its pliability. Trauma, inflammation, and surgeries create restriction in the fascia that can produce incredible amounts of pain, pressure, and range of motion loss within the body. Fascial restrictions affect our flexibility and stability, impeding our every-day activities. The restrictions can be caused by physical trauma or injury, poor posture or structural imbalance, and inflammation or infection. Restrictions can cause limited mobility, postural distortion, poor cellular nutrition, pain, and other dysfunctions. Because of the interconnectedness of the tissue, restriction in one part of the body can affect other distant parts. The fascia also stores memory of past traumatic events. Psychosomatic adaptations may occur, which may cause one to avoid positions which are associated with pain.\nYour therapist will evaluate your body visually and by palpating (feeling) tissue texture. Gentle pressure is applied by slowly pushing, pulling, and stretching the skin. Fascia has the characteristic of thixotrophy, which means it can change from a more solid to a more gel-like state with movement and increase in temperature. Some techniques are:\n• Sustained Stretch - gentle pressure into the direction of restriction, usually with a sliding motion that stretches the tissue.\n• Skin Rolling - gently picking up and pulling skin away from underlying structures. This stretches subcutaneous fascia, breaks cross-links, and makes tissues more pliable.', 'What are they?\nProbiotics are live microorganisms that are intended to have health benefits when consumed or applied to the body. Probiotics may contain a variety of microorganisms. The most common are bacteria that belong to groups called Lactobacillus and Bifidobacterium. Other bacteria may also be used as probiotics, and so may yeasts such as Saccharomyces boulardii.\nThe U.S. Food and Drug Administration (FDA) regulate probiotics as a dietary supplement, a food ingredient, or a drug.\nWhy are they needed?\nPeople often think of bacteria and other microorganisms as harmful “germs,” however many are actually helpful. Probiotics might:\n- Help digest food, destroy disease-causing cells, or produce vitamins.\n- Help your body maintain a healthy community of microorganisms or help your body’s community of microorganisms return to a healthy condition after being disturbed\n- Produce substances that have desirable effects\n- Influence your body’s immune response.\nProbiotics have shown promise for a variety of health purposes, including prevention of antibiotic-associated diarrhea (including diarrhea caused by Clostridium difficile), prevention of necrotizing enterocolitis and sepsis in premature infants, treatment of infant colic, treatment of periodontal disease, and induction or maintenance of remission in ulcerative colitis.\nWhere do you find it naturally?\nProbiotics are added to some foods like yogurt and cultured cottage cheese. They are naturally occurring in fermented foods like buttermilk, kefir, and sauerkraut. Raw cheese made from unpasteurized milk is another source.\nProbiotics for kids\nChildren develop their microbiome in the womb and through early childhood. It’s thought that an unhealthy microbiome is responsible for many diseases.\nA study published in JAMA Pediatrics found that giving infants probiotics in the first three months of life may help prevent colic, constipation, and acid reflux.\nSome decent brands for probiotics:\n- Nature’s Way:\n- Ultimate Flora:\n- Check the label to make sure the yogurt you choose has “live and active cultures.”\nProbiotics have an extensive history of apparently safe use.\n- Since long-term probiotic effects on kids are unknown, children shouldn’t use probiotic supplements as a preventive remedy, unless recommended by a doctor.\n- The risk of harmful effects from probiotics is greater in people with severe illnesses or compromised immune systems. When probiotics are being considered for high-risk individuals, such as premature infants or seriously ill hospital patients, the potential risks of probiotics should be carefully weighed against their benefits.\n- Possible harmful effects of probiotics include infections, production of harmful substances by the probiotic microorganisms, and transfer of antibiotic resistance genes from probiotic microorganisms to other microorganisms in the digestive tract.\n- Some probiotic products have been reported to contain microorganisms other than those listed on the label. In some instances, these contaminants may pose serious health risks.\n- Kids with compromised immune systems may experience infection. Others may have gas and bloating. Probiotics can cause serious side effects in very sick infants. Check with your pediatrician before giving probiotic supplements to your child.\nSource: Internet, CDC, NIH, FDA, Healthline, Medical News & Others\nThe views expressed in this article should not be considered as a substitute for a physician’s advice. Always make sure to seek a doctor or a professional’s advice before proceeding with the home treatment plan.']	['<urn:uuid:435d7435-f2b2-45fa-9f9a-8e77610c9087>', '<urn:uuid:18b099d3-380b-4d1e-9834-7ed4ed088b12>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T12:39:32.578853	12	103	1053
81	Why is it particularly difficult to develop new insecticides specifically targeted at controlling bed bug populations, and what are the current challenges with existing chemical solutions?	Developing new insecticides for bed bugs is challenging due to several factors. Studies have shown widespread resistance to commonly used pyrethroid insecticides in bed bugs across the United States. While new chemical solutions are needed, developing novel chemistries requires too great an investment, involves high risks, and has a long time-to-market. Additionally, there's limited financial incentive since pesticides for treating infestations of non-disease-carrying pests account for less than 2% of global pesticide sales. The existing chemical solutions also pose safety concerns, as evidenced by the CDC's documentation of 111 cases of bed bug-related insecticide misuse from 2003 to 2010, including one fatal case.	['Our relationship with bed bugs is intimate. They share our beds, they feed on our blood at night, and they disappear by dawn, often leaving nothing more than a raised welt on our skin, a token of last night’s encounter.\nTherein lies the problem—their cryptic nature makes detection and control very difficult.\nBed bugs don’t fly or jump. We are solely responsible for their spread.\nBed bugs are flat, so they can hide in cracks and crevices—their harborages—where you sleep. They have grappling hooks at the end of their feet, enabling them to navigate the geography of your bed and body.\nSmall, flat, flightless bugs? So why is it so hard to prevent and control them?\nThere is more to solving the problem than you would expect. There is a disparity between the extremely negative and almost phobic public perception of bed bugs and the industry’s response. Why don’t we have better control strategies?\nFirst, let’s look at efforts to control bed bugs through insecticides. Studies from the University of Kentucky have found widespread resistance to pyrethroids, a commonly used insecticide class, in bed bugs across the United States. That means we need new chemistries and modes of action to avoid problems with resistance. But it’s easier to tweak existing chemical architecture than to design novel chemistries.\nThat’s why we don’t see new insecticides: the investment is too great, the risk too high, and the time-to-market too long.\nInsecticide toxicity is another concern. The Centers for Disease Control and Prevention (CDC) documented 111 cases of bed bug-related insecticide misuse from 2003 to 2010. These illnesses were the result of excessive insecticide application. In one fatal case, the husband of a woman suffering from hypertension and depression applied a lawn-and-garden insect killer to their mattress and box springs, followed by nine cans of an insecticide fogger in the same day. Two days later, they reapplied insecticide to the bedroom and nine MORE cans in the house. On that day, the woman also applied insecticide to her arms, chest, and hair. She died 11 days later.\nHeat treatment is another, less toxic option for controlling bed bugs. Heat treatment is basically “cooking” a room or furniture with steam or air heated to 120oF to 140oF for up to 4 hours. While heat treatment is effective at killing bed bugs in a room, it might also force bed bugs to migrate into adjacent rooms. Heat treatment can be expensive —up to $1,000 per treatment—and has no residual activity (that means it kills whatever is there at the moment but will not prevent a re-infestation.) Heat treatment can also melt candles, warp CDs, and kill your houseplants.\nA third control option is a very fine, silica-rich dust that’s used as an abrasive in toothpaste, an absorbent in cat litter, and a binding agent in dynamite. Diatomaceous earth, or DE, as it’s also called, is an effective insecticide for bed bugs and other domestic pests. The silica absorbs and abrades the wax layer on an insect’s exoskeleton, and the insect dies of dehydration. However, DE leaves obvious white dust, advertising that you have a pest problem, and it could take weeks to kill an infestation.\nEven with these three different control strategies (toxic chemicals, heat, or silica dust), infestation rates are rising.\nWhy is this the case?\nPesticides used to treat infestations of pests that carry diseases harmful to humans only account for less than 2% of global pesticide sales. And bed bugs aren’t vectors!\nBecause bed bugs make up such a small share of the overall market for pesticides, and they don’t qualify for vector biology funding by the NIH, there’s not much financial incentive to control them.\nUntil earlier this year, the Department of Housing and Urban Development (HUD) provided funding for housing authorities to control bed bugs. That program has been eliminated, and local housing authorities need to creatively budget for bed bug control within existing funds.\nBut bed bugs incur hidden costs. Most cities prohibit the disposal of infested furniture in those regular green dumpsters we’re familiar with. Furniture can only be discarded in a special municipal or construction dumpster, which costs thousands more to purchase, register, and maintain. If an apartment complex can’t afford a construction dumpster, then tenants may discard the infested furniture on the street where—no surprise—someone else will pick it up and start a new infestation.\nRemember, bed bugs don’t fly or jump. We are their vectors.\nProblems and misconceptions with pesticides, expensive treatments, and funding bottlenecks at the federal level reveal that there’s more to the bed bug problem than meets the eye — or the mattress.\nFinally, our understanding of bed bug biology is about 50 years behind our understanding of mosquito biology, so we really need to catch up. At SRI, researchers are working on improving infestation detection. Once we detect them, we still need sustainable control strategies that are targeted, effective, and nontoxic to other organisms. We have a long way to go, and like so many other aspects of this insect, details remain cryptic.']	['<urn:uuid:836a13fb-39a5-4133-a6cc-8b8a307770ef>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	26	103	842
82	Could you explain how the famous Fleischer brothers' character Koko the Clown evolved from early animation to become connected with other iconic cartoon characters?	Koko the Clown first appeared in 'Out of the Inkwell' cartoons, as seen in 'The Circus' (1920) where he deals with an unruly circus horse. Later, Koko migrated with the Fleischer brothers (Max, Dave, and Joe) to their own studio, which became home to other famous cartoon icons like Popeye and Betty Boop.	"[""Even if you're not old enough to remember when animated cartoons were every bit as big a draw as the main feature showing at your neighborhood movie theatre, you're probably familiar with the fact that cartoon icons like Bugs Bunny and Mickey Mouse unleashed their shenanigans on the big screen before going into retirement on the small screen. You see, cartoons weren't just for kids -- they were produced for all ages! Animated cartoons are nearly as old as motion pictures themselves; as far back as 1900, pioneers like Emile Cohl, Winsor McCay and J. Stuart Blackton ushered in the art of moving screen images that were, in a nutshell, drawings brought to life.\nOne of J. Stuart Blackton's earliest efforts, Lightning Sketches (1907), is one of several shorts (fifteen in all) featured on a collection entitled Cartoon Roots, a joint DVD/Blu-ray venture between Thunderbean Animation -- a company whose previous home video releases include Technicolor Dreams & Black and White Nightmares and Noveltoons: Original Classics -- and Cartoons on Film, a company specializing in animated shorts from the silent era. Tommy J. Stathes is the CEO of the latter establishment, a highly respected film archivist/historian whom you might have seen chatting it up with Turner Classic Movies' Robert Osborne back in October of 2014 on a very entertaining special entitled 100th Anniversary of Bray Studios.\nA few of the selections from Cartoon Roots were also showcased on the (John Randolph) Bray anniversary special. The Circus (1920) is one example, an 'Out of the Inkwell' cartoon featuring Koko the Clown (who's frustrated by the antics of an ornery circus equine), who would later migrate with the creative Fleischer brothers (Max, Dave, and Joe) to a studio that also became home to such cartoon icons as Popeye and Betty Boop. The same goes for Lost and Found (1926), in which Dinky Doodle and his dog Weakheart give chase to a kidnapper who's made off with a fetching young lovely. The live-action animator in the cartoon dons a Sherlockian disguise and enters the short, in cartoon form, to help out Dinky and his pal, and that animator is none other than Walter Lantz, who later started his own studio that put stars like Andy Panda, Woody Woodpecker and Chilly Willy on the payroll.\nSo you can see why the presentation is titled Cartoon Roots; it features some of the earliest work by individuals who ultimately became prominent figures in the field of animation. Cartoons on Tour (1915) is from the Raoul Barre studio, one of the earliest animation companies (future film director Gregory La Cava was on the payroll) and Tour spotlights a nice mix of live-action and animation in one of Barre's amusing 'Grouch Chaser' frolics. The Jolly Rounders (1923) is an early 'Aesop's Fables' effort from Paul Terry, who also worked his way up to animation studio mogul -- his Terrytoons featured such stars as Mighty Mouse and Heckle and Jeckle. (The 1923 cartoon Springtime is another one of Terry's 'fables,' starring one of his earliest cartoon stars, Farmer Al Falfa.) Animation legend Dick Huemer (in tandem with Sid Marcus and Art Davis) is represented with a 1930 RKO entry, The Milkman, starring the now-forgotten Toby the Pup.\nWhen I was growing up, weekends were often set aside to pay my father's parents a visit, and one of the things I recall most about dropping in on Grandma and Grandpa was Grandpa subscribed to a magazine entitled The Good Old Days, featuring reprints of comic strips from that bygone era. So some of my favorite shorts on Cartoon Roots feature characters from the 'funny papers': Bud Fisher's Mutt and Jeff are represented with Fireman, Save My Child (1919); George Herriman's Krazy Kat in Scents and Nonsense (1926, quite different from the depiction of K.K. in the comics); Walter Hoban's Jerry on the Job in The Bomb Idea (1920). Watching these cartoons was not unlike seeing the comic pages literally come to life. I had long thought that the early era 'Bobby Bumps' character (who appears on Roots in Bobby Bumps Starts to School ) was based on R.F. Outcault's Buster Brown, but in an extra included among the many in this set, there's revealing background that Bumps' creator Earl Hurd drew an embryonic version of his creation entitled 'Brick Bodkin' for The New York Journal from 1912 to 1914.\nRounding out the cartoons on Cartoon Roots are Col. Heeza Liar, Detective (1923), a most entertaining short (which was also featured on TCM's Bray tribute) similar to Lost and Found in that the Colonel must use his powers of deduction to track down a rooster thief. Sadly, Heeza Liar never made it to the comic strip pages (there probably would have been competition with the likes of Happy Hooligan and Hawkshaw the Detective), but Pat Sullivan's Felix the Cat did shortly after his debut on the silver screen. The famous feline stars in the inventive Felix Comes Back (1923) in the Cartoon Roots collection. The last cartoon is a 1932 Van Beuren offering entitled The Farmerette, which is the only entry here to feature spoken dialogue (though some of the others feature it among the sound effects). It treads too far into Betty Boop territory, and the imitation unfortunately makes it the weakest short in the collection.\nThe most incredible find on Cartoon Roots -- and one that definitely makes it worthy of 'Where's That Been?' status -- is a 1930 short entitled Hoe-Toe Mollie. It stars the irrepressible Binko the Cub, a would-be employee of the Romer-Grey studios (the 'Grey' was the son of famed Western novelist Zane Grey). A series that never got off the ground, Binko starred in two unreleased shorts (the first being Arabian NIghtmare) that featured work from Bob and Tom McKimson, Preston Blair, Pete Burness, Cal Dalton, Ken Harris and Riley Thomson. These animators would later work for the likes of Warner Bros., MGM and Walt Disney. Both shorts were long believed to be lost, but Stathes and collaborator David Gerstein unearthed a mostly-complete print of Mollie in the Library of Congress, and with that, film preservation was allowed to do that voodoo it does so well.\nIf you're as crazy about cartoons as I am, I know you will enjoy watching Cartoon Roots. Keep in mind that while the shorts are a bit primitive (this was many years before strides like animation cels and the like came about) it doesn't make them any less entertaining, and some of them feature some really side-splitting gags. (I never fail to marvel at the imaginative ways Felix finds to use question marks and his tail.) Animation lovers, silent film devotees, and aficionados of movie history will unquestionably want to see Cartoon Roots on their DVD or Blu-ray shelf.\nIvan G. Shreve, Jr., former associate editor at ClassicFlix.com, blogs about classic film, vintage TV and old-time radio at Thrilling Days of Yesteryear. 'Where's That Been?' is a regular review column that highlights overlooked or underappreciated films from the golden age.""]"	['<urn:uuid:7938221e-c8df-4b58-8b37-b873c259c0a1>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	24	53	1172
83	I'm new at maintenance. Should everyone use the same lock?	No. Each person must place and remove their own lock. When multiple people are doing maintenance on the same machine, each individual needs to use their own lock, and the machinery cannot be started up until all locks are removed.	"['Presentation on theme: ""CONTROL OF HAZARDOUS ENERGY (LOCKOUT/TAGOUT) OSHA 29 C FR""— Presentation transcript:\n1 CONTROL OF HAZARDOUS ENERGY (LOCKOUT/TAGOUT) OSHA 29 C FR 1910-147\n2 “This material was produced under grant numbers SH F-17 from the Occupational Safety and Health Administration, U.S. Department of Labor. It does not necessarily reflect the views or policies of the U.S. Department of Labor, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.”\n3 What is LOCKOUT/TAGOUT? “Lockout/Tagout” refers to specific practices and procedures to safeguard employees from the unexpected energization or startup of machinery and equipment, or the release of hazardous energy during service or maintenance activities.“Lockout” is the placement of a lockout device on an energy-isolating device, in accordance with an established procedure, ensuring that the energy-isolating device and the equipment being controlled cannot be operated until the lockout device is removed.“Tagout” is the placement of a tagout device on an energy-isolating device, in accordance with an established procedure, to indicate that the energy-isolating device and the equipment being controlled may not be operated until the tagout device is removed.References: “OSHA Standard for the Control of Hazardous Energy (Lockout / Tagout)” 29 CFR and OSHA Publication 3120 “Control of Hazardous Energy – Lockout/Tagout”\n4 Lockout/Tagout Training Objectives: What is lockout/tagout. What is hazardous energy.Causes of fatalities/injuries.When lockout/tagout applies.Employer responsibility.Employee training.Lockout/tagout procedures required.Types of lockout devices.Requirement for tags.\n5 LOCKOUT/TAGOUTThe OSHA Standard for the Control of Hazardous Energy (Lockout/Tagout) 29 CFR covers the servicing and maintenance of machines and equipment in which the unexpected start-up or the release of stored energy could cause injury to employees.\n6 What Does LOTO Require?Lockout/Tagout requires, in part, that a designated individual turns off and disconnects the machinery or equipment from its energy sources(s) before performing service or maintenance and that the authorized employee(s) either lock or tag the energy-isolating device(s) to prevent the release of hazardous energy and take steps to verify that the energy has been isolated effectively.If the potential exists for the release of hazardous stored energy or for the reaccumulation of stored energy to a hazardous level, the employer must ensure that the employee(s) take steps to prevent injury that may result from the release of the stored energy.\n7 COMMONLY USED LOTO TERMS Affected Employee – An employee whose job requires him/her to operate or use a machine or equipment on which servicing or maintenance is being performed under lockout tagout.Authorized Employee – A person who locks out or tags out machines or equipment in order to perform servicing or maintenance on that machine or equipment.Energy Source – Any source of electrical, mechanical, hydraulic, pneumatic, chemical, thermal, or other energy.Lockout Device – Uses a positive means such as a lock to hold an energy isolating device in the safe position and prevent the energizing of a machine or equipment.Tagout Device – A prominent warning device which can be securely fastened to an energy-isolating device.Energy-Isolating Device – A mechanical device that physically prevents the transmission or release of energy.\n8 Lockout in General Industry Service and Maintenance Operations The standard applies to the control of hazardous energy when employees are involved in service or maintenance activities such as constructing, installing, setting up, adjusting, inspecting, modifying, and maintaining or servicing machines or equipment.These activities include lubricating, cleaning or unjamming machines, and making adjustments or tool changes, where the employees may be exposed to hazardous energy.\n9 Lockout in General Industry Service and Maintenance Operations (Cont.) If a service or maintenance activity is part of the normal production operation, the employee performing the servicing may be subjected to hazards not normally associated with the production operation itself.Workers performing service or maintenance activities during normal production operations must follow LOTO procedures if they:Remove or bypass machine guards or other safety devices,Place any part of their bodies in or near a machine’s point of operation, orPlace any part of their bodies in a danger zone associated with machine operations.\n10 EXCEPTIONSThe employee is performing work involving minor tool changes and adjustments or other minor servicing activities that are routine, repetitive, and integral to the use of the production equipment, that occur during normal production operations, and provided that the work is performed using alternative measures which provide effective protection.Exposure to hazardous energy is controlled completely by unplugging the equipment from an electrical outlet and where the employee doing the service or maintenance has exclusive control of the plug.An employee is performing hot-tap operations on pressurized pipelines that distribute gas, steam, water, or petroleum products, for which the employer shows the following: Continuity of service is essential; shutdown of the system is impractical; and the employee follows documented procedures and uses special equipment that provides proven, effective employee protection.\n11 Employers must establish a written energy-control program including: Energy-control procedures for removing the energy supply from machines and for placing appropriate LOTO devices on the energy-isolating devices to prevent unexpected reenergization.The training of employees on the energy-control program, including the safe application, use, and removal of energy controls.A review of these procedures periodically (at least annually) to ensure that they are effective and being followed.\n12 What Employees Need to Know about LOTO “Authorized employees”Hazardous energy source recognitionThe type & magnitude of the hazardous energy sources in the workplaceEnergy-control procedures, including the methods and means to isolate & control those energy sources“Affected employees”Recognize when the energy-control procedure is being usedUnderstand the purpose of the procedureUnderstand the importance of not tampering with lockout or tagout devices and not starting or using equipment that has been locked or tagged out“Other employees”Must receive instruction regarding the energy-control procedure and the prohibition against removing a LOTO device and attempting to restart, reenergize, or operate the machineryIf tagout devices are used, all employees must receive training regarding the limitations of tags\n13 What is Hazardous Energy? Electricity – live or storedMechanical - moving machine parts“Stored mechanical movement means some part of a machine can be moved by electricity, hydraulic fluid, air pressure, water pressure or gravity. This energy can sometimes still exist or be stored when the machinery is turned off. For example, hydraulic fluids can move machinery parts even when the motor or electricity is off, if a certain valve is opened.In the truck photo, the upraised truck bed has hazardous stored mechanical energy because gravity could move it down on top of a mechanic working under it, if the bed is not physically blocked in the up position.”Hydraulic, pneumatic, or mechanical movement in machinery3\n14 What is Hazardous Energy What is Hazardous Energy? The Standard applies to all sources of energy, including but not limited to: Mechanical, electrical, hydraulic, pneumatic, chemical and thermal energyThermal Energy: Stored heat (steam lines or hot liquids)Hydraulic Energy: Stored energy (steam or hydraulic fluid)Chemicals in pipelines under pressure or force of gravityAny other active or stored energy sources that could harm a worker“Chemicals may not seem like stored energy in the normal meaning of the word, but some chemicals like acids, would cause injury if suddenly sprayed or splashed on a worker, thinking a pipeline is empty or has been bled out, disconnects a pipe or opens a valve.”4\n15 Hazardous Energy Source Examples Live electrical linesElectrical capacitorsLasersHydraulic liftsPneumatic (air pressure) linesSprings“Other examples include forklifts with the forks in the up position, steam and hot liquid pipes, fuel lines such as natural gas to heaters.These examples are considered hazardous, because just “turning them off” does not guarantee they won’t accidentally be turned on again during maintenance or repair, or they continue to have stored energy after being “turned off”. Think of a compressed spring, or a pressured line that has not been bled off.” Fatalitities have occurred when the mast slides down on the repairman. Mast should have been blocked to prevent movement.\n16 What kind of injuries can happen? Shock or electrocution from live partsScalding from steam or hot liquidsChemical burns or poisoningFrom machinery:- Deep cuts and gashes- Crushing injuries- Amputations[Note: The photo may be too graphic or distracting for some people. You can make it disappear by clicking on the picture.] Pipes or other equipment not properly bled out may have or contain steam, air, chemicals, which may be inadvertently disbursed and cause injury.\n17 Fatalities & Injuries Causes for injuries: Failure to stop equipment Failure to disconnect the power sourceFailure to dissipate residual (stored) energyAccidental restarting of equipmentFailure to clean work area before reactivation\n18 Discussion Point: Are you aware of any fatalities? Fatality ExampleA man working inside a supermarket cardboard compactor was crushed when the unblocked compactor suddenly came down on top of him“This fatality happened recently in the state of Washington.”Discussion Point: Are you aware of any fatalities?\n19 When is Lockout / Tag-out required? When someone will be servicing or repairing machinery or equipment and the unexpected machinery start-up or release of stored energy could cause injury“In many fatalities or injuries, another employee will start up a machine or open a valve, not aware that someone else is repairing the machine. The person working on this saw obviously would not want someone inadvertently starting up the saw.”\n20 LOTO vs. Machine Guarding Machine guarding protects the operator from the hazards of the machine while under NORMAL operating conditions.Lockout / Tagout refers to a practice to safeguard employees from the unexpected energization or startup of machinery and equipment, or the release of hazardous energy during service or maintenance activities. Lockout devices hold energy-isolation devices in a safe or “off” position. They provide protection by preventing machines or equipment from becoming energized because they are locked out and no one can remove the lock without a key or other unlocking mechanism such as bolt cutters.\n21 Service and Maintenance Examples Installing, constructing,Adjusting, modifying,Unjamming, cleaning,Lubrication, inspecting,Setup - preparing for normal function, Including die changes“Service & Maintenance -- when the machine isn’t being used for actual production and service, repair or maintenance work is being done.Setup - a particular type of service to a machine when it is being prepared for production use.In the photo, the setup person is installing and adjusting the dies for production stamping of a composite material.Problems:There is no block under the upper die to positively prevent it from coming down.A key in the computer control on the other side of the machine is all that prevents machine activation.”These activities often require a worker to place all or part of their body into the machine’s hazard zone.\n22 What is an energy-isolating device? A device that physically prevents transmission or the release of energy.Circuit BreakerPipeline ValveMachine Block“These are also called “energy control devices”. Regular on-off switches (push buttons, selector switches, etc.) are not energy-isolating devices since they can fail – they are not designed for energy isolation.Anything else that positively blocks or isolates energy.\n23 What is a Lockout Device? A device that positively prevents a machine from being:started upturned onmachinery parts from movingbecoming electrically energizedblocks a pipeline, steam line or air line.“A lockout device is a usually a lock, key, valve or switch cover that holds an energy isolating or control device in the off or safe position until the lockout device is removed. Lockout must be done according to an established procedure.”\n24 Electrical Lockout Devices Locked out circuit breaker“If you don’t have an electrical disconnect next to a machine, then you can attach a lock to the electrical panel. There are also individual circuit breaker locks, which may be better than locking the electrical panel cover since in an emergency, it may be necessary to get to the panel to shut off breakers other than the one that is locked out.You don’t have to have a lockout procedure for a machine with all its energy controlled by pulling the plug if the worker maintains exclusive control of the plug. If that is not the case, the photo on the right shows how to lockout a plug at the end of an electrical cord.”Locked out electrical panelLocked out electrical plug\n25 Fluid & Gas Lockout Devices “These lockout devices prevent the valves in air, water, gas or steam lines from being opened.”\n26 Pipe Lockout Examples“A physical block or break in a pipeline prevents steam, gas or liquids from flowing through the pipe to the area where maintenance work is being done. For example, the pipe to a chemical tank must be blocked or disconnected before someone goes in to do repair or maintenance.”\n27 Pneumatic Lockout Examples “These devices prevent compressed air from flowing through lines and moving some part of the machinery. The locks prevent anyone from hooking up the lines while maintenance is being done.”17\n29 Each person places and removes their own lock. Group Lockout DevicesUsed when more than one person doing maintenance or repair on same machine or equipment.Machinery or equipment can’t be started up until all locks are removed.Each person places and removes their own lock.“Group or “gang” locks are typically used when several people are doing maintenance and repair at the same time on a machine or piece of equipment. Each lock should have a clear identification (usually a name) showing who it belongs to. No one can remove another person’s lock.”\n30 Example of a bad lockout/tagout “This obviously not a real lockout device since it could be easily removed.”\n31 Tags are warning devices only What is Tag-out?Tags are warning devices onlyThey don’t provide the same physical restraint or level of protection as lockout devices.Tags may evoke a false sense of securityThey can only be removed by an authorized person.They must be legible, securely attached and resistant to degradation.“Tags cannot be used as a substitute for lockout devices. They can only be used alone if there is no other way to positively control hazardous energy. However, they are often used along with lockout devices as an additional warning. The separate sign can be used as further warning or information.”\n32 Requirements for LOTO Devices The employer must provide these devices and they must be singularly identified and not used for other purposes.They must be durable enough to withstand workplace conditions;Standardized enough to minimize the likelihood of premature or accidently removal; andLabeled to identify the specific employees authorized to apply and remove them.“Tags cannot be used as a substitute for lockout devices. They can only be used alone if there is no other way to positively control hazardous energy. However, they are often used along with lockout devices as an additional warning. The separate sign can be used as further warning or information.”\n33 ENERGY-CONTROL PROCEDURES MUST: Outline the scope, purpose, authorization, rules and techniques that employees will use to control hazardous energy sources, as well as the means that will be used to enforce compliance. These procedures must provide employees at least the following information:A statement on how to use the proceduresSpecific procedural steps to shut down, isolate, block, and secure machinesSpecific steps designating the safe placement, removal and transfer of LOTO devices and identifying who has responsibility for the LOTO devicesSpecific requirements for testing machines to determine & verify the effectiveness of LOTO devices and other energy-control measures\n34 TYPICAL MINIMAL LOCKOUT SHUTDOWN PROCEDURES REQUIRE WORKERS TO: Prepare for shutdownShut down the machineDisconnect or isolate the machine from the energy source(s)Apply the lockout or tagout device(s) to the energy-isolating device(s)Release, restrain or otherwise render safe all potential hazardous stored or residual energyVerify the isolation & deenergization of the machine\n35 Lockout Procedures Six Steps to Follow: Notify affected employees that machine or equipment will be shut down and locked out2. Shut down the machinery or equipmentIsolate energy sources with energy-isolating devicesLock out energy-isolating devices with assigned locks.5. Release or restrain stored or residual energy6. Test machinery to make sure it can’t start up“These steps must be followed in the order shown.”\n36 Lockout Steps Notify employees Shutdown Equipment Isolate Energy Attach LockoutDeviceReleaseStored EnergyVerify Lockout[A visual view of the two preceding slides.]Service &Maintenance\n37 Example: Release of Stored Energy Pressurized Vessel - “Slowly open the receiver tank port and bleed off any internal pressure.”Pressurized Piping - “Loosen both line valves to relieve all pressure in the cooling circuit.”Electrical Capacitor - “Ground out capacitor…”\n38 VERIFY EQUIPMENT IS DE-ENERGIZED Example: Attempt to Operate “…adjust the temperature cycle thermostat to check that all electrical energies have been shut off.”“Push the start function button to verify that electric power has been removed.”“Crack the steam inlet and discharge line outlet valves…”\n39 WHAT DO I DO IF I CANNOT LOCK OUT THE EQUIPMENT? If the energy-isolating device associated with the machinery cannot be locked out, you must securely fasten a tagout device as close as safely possible to the energy-isolating device in a position where it will be immediately obvious to anyone attempting to operate the device.You also must meet all of the tagout provisions of the standard.The tag alerts employees to the hazard of reenergization and states that employees may not operate the machinery to which it is attached until the tag is removed in accordance with an established procedure.\n40 TYPICAL MINIMAL LOCKOUT STARTUP PROCEDURES REQUIRE WORKERS TO: Before removing LOTO devices, the employees must take the following steps in accordance with the specific provisions of the employer’s energy-control program:Inspect machines or their components to assure that they are operational intact and that nonessential items are removed from the area; andCheck to assure that everyone is positioned safely and away from machines.After removing the lockout or tagout devices but before reenergizing the machine, the employer must assure that all employees who operate or work with the machine, as well as those in the area where service or maintenance is performed, know that the devices have been removed and that the machine is capable of being reenergized.\n41 Only authorized employee can do startup Start-up ProceduresOnly authorized employee can do startupAll warned to stay clearRemove all tools, locks and tagsRemove, reverse, open or reactivate isolating devicesVisual check that all is clearStart up machine, process or line flow“These steps must also be followed in the order shown.”']"	['<urn:uuid:c9e9b86d-cba9-4846-b8b1-e437900254f4>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T12:39:32.578853	10	40	2992
84	jury selection bias challenges solutions	Current jury selection faces challenges as racial discrimination remains prevalent, though often unconscious rather than overt. The Batson challenge test addresses purposeful discrimination but has proven insufficient for handling implicit biases. Proposed solutions include abolishing peremptory challenges, providing judicial training on implicit bias, and improving jury pool diversity by expanding selection sources beyond traditional voter and driver registration lists.	['Summer is here and everyone is feeling excited about fair cross-section claims. Or at least I am after hearing an enlightening presentation about them, described below.\nSource and importance of the fair cross-section right. The right to be judged by a fair and impartial jury of one’s peers is a bedrock principle of the American criminal justice system, grounded in the Sixth Amendment to the United States Constitution and article I, sections 24 and 26 of the North Carolina Constitution. In Section 6.2A of our manual, Raising Issues of Race in North Carolina Criminal Cases, we discuss reasons that it is important to have juries that reflect the composition of the communities from which they are drawn. For example:\n- Citizens may not be denied the right to serve on juries, which is guaranteed by the federal and state constitutions.\n- Jury pools and juries that underrepresent racial and ethnic minorities may create a perception of unfairness, eroding trust and confidence in the legal system.\n- Studies have shown that the racial composition of juries may affect jury deliberations and influence case outcomes.\n- Jury verdicts should reflect the voice of the community, which may act as a check on government overreaching.\n- Participation in the jury formation process leads to civic engagement, education about the court system, and interaction across social groups.\nThis quote beautifully describes a more intangible but equally important benefit of having a representative jury:\nWhen any large and identifiable segment of the community is excluded from jury service, the effect is to remove from the jury room qualities of human nature and varieties of human experience, the range of which is unknown and perhaps unknowable. It is not necessary to assume that the excluded group will consistently vote as a class in order to conclude, as we do, that its exclusion deprives the jury of a perspective on human events that may have unsuspected importance in any case that may be presented.\nPeters v. Kiff, 407 U.S. 493, 503–04 (1972).\nConcerns about underrepresentation of minorities in jury pools. Courts have found that minorities, primarily African Americans and Latinos, are often underrepresented in jury pools. See, e.g., Nebraska Minority and Justice Task Force, Final Report 17 (2003) (noting that “many researchers have found that this is ‘the rule’ rather than the exception”). Court actors from various parts of North Carolina have shared concerns that jury pools from which trial jurors are selected do not appear to reflect their communities in terms of racial and ethnic composition. For example, defense attorneys have said to me, “I got to the courthouse, ready to pick a jury. I looked out at the pool and thought, ‘This group doesn’t look anything like ____ County!’” Leaders in judicial district 15B, made up of Orange and Chatham Counties, have undertaken a study of its jury pools to determine in a systematic fashion how well jury pools reflect the demographic composition of the counties’ jury-eligible population. The report, authored by colleagues at the School of Government, will be available in August and may serve as a useful model for other districts.\nFair cross-section challenges few and far between. In spite of concerns about representative jury pools and juries, fair cross-section claims are rarely litigated. This may be at least in part because defense attorneys stopped raising such challenges after state legislatures modified jury formation statutes with the intent of creating more representative jury lists. North Carolina followed the trend of many other states that added lists of registered drivers to voter registration lists because drivers’ lists may be more representative of racial minorities than voter registration lists alone. But researchers studying other jurisdictions that use “motor/voter” lists have found that underrepresentation persists in spite of the addition of drivers’ lists. See, e.g., Ronald Randall et al., Racial Representativeness of Juries: An Analysis of Source List and Administrative Effects on the Jury Pool, 29 Just. Sys. J. 71 (2008).\nElements of a fair cross-section claim. Fair cross-section challenges may be the most promising avenue of relief for defendants challenging racial disparities in jury formation because, in contrast to equal protection claims, defendants raising fair cross-section challenges don’t have to prove discrimination. In case you aren’t familiar with these claims, I will briefly set out the elements. In Duren v. Missouri, 439 U.S. 357 (1979), the United States Supreme Court set out the three steps for establishing a fair-cross section violation. To make out a prima facie challenge, a defendant must show: (1) the group alleged to be excluded is a ‘distinctive’ group in the community, which includes race, ethnicity, and gender, but not age; (2) the group is not fairly represented when compared to the number of such people in the community; and (3) the underrepresentation is due to systematic exclusion or inherent in the jury selection process that was used. See Duren; State v. Williams, 355 N.C. 501 (2002). We provide guidance on the showing that is required for each step in Section 6.3 of Raising Issues of Race.\nDemystifying the fair cross-section right. Last week, I had the opportunity to hear Nina Chernoff, a Professor at CUNY School of Law, speak on some misconceptions about fair cross-section challenges. She was an incredibly engaging scholar, and I recommend her to any group seeking clarification of these issues. In particular, she described how numerous courts around the country, including North Carolina courts in some opinions, have confused the requirements of the Sixth and Fourteenth Amendments and mistakenly imposed a requirement on fair cross-section claimants that they show purposeful and discriminatory exclusion. See Wrong About the Right: How Courts Undermine the Fair Cross-Section Guarantee by Imposing Equal Protection Standards, 64 Hastings L.J. 141 (2012). Professor Chernoff has authored a helpful guide for people considering a fair cross-section claim, The 16 Things Every Defense Attorney Should Know About Fair Cross-Section Challenges, with Dr. Joseph B. Kadane, The Champion, Dec. 2013, at 14. Examples of pointers include:\n- Defendants raising fair cross-section claims do not have to be members of the distinctive group allegedly excluded.\n- A defendant who is considering a fair cross-section claim may obtain information about the jury selection system through discovery.\n- There is no perfect test for measuring underrepresentation; courts should consider the results of tests including absolute disparity, comparative disparity, and standard deviation. See Berghuis v. Smith, 559 U.S. 314, 329 (2010).\n- There is no set percentage of underrepresentation required to satisfy the second prong of the Duren See The 16 Things Every Defense Attorney Should Know at 19 (noting that defense attorneys should resist adoption of a 10% absolute disparity threshold because (1) the U.S. Supreme Court declined to adopt it; (2) it would leave groups comprising less than 10% of the community without a remedy for underrepresentation; and (3) it has been mistakenly imported from the equal protection context.\n- Fair cross-section violations can be based on inadvertent rather than intentional conduct.\n- Computer systems used in jury operations may inadvertently be the source of systematic exclusion.\n- Recurring underrepresentation over time tends to reflect systematic exclusion.\nNot only does Chernoff’s presentation make you excited about the prospect of representative jury pools, she includes some fun cat memes. See, e.g.,', 'Guest: Trial by jury of peers undermined by racial bias\nThe Washington Supreme Court has identified a pervasive problem of racial bias in jury selection, writes guest columnist Eva Paterson.\nSpecial to The Times\nTHE Washington Supreme Court has called for more careful scrutiny when prosecutors knock minorities off juries for a trial of a person of the same race. This is a new reality and the state’s courts will have to adjust to it.\nThe court’s statements came Aug. 1 in an appeal by Kirk Saintcalle, an African-American man convicted of murder and assault in an Auburn apartment break-in. He was sentenced to 48 years in prison.\nAt trial, the prosecution removed the only African-American juror using a “peremptory challenge” — an option provided to both the defense and prosecution that does not require an explanation.\nThe prosecutor also tried to remove the only Latino juror. In both cases, Saintcalle’s attorney raised a “Batson challenge,” named after the U.S. Supreme Court case Batson v. Kentucky, which established a three-part test to determine if removing a juror of the defendant’s race is unconstitutional.\nThe removal of the African-American juror met that test, the court said, because she had lost a friend in a murder two weeks before. But the court’s majority opinion also stated that “racial discrimination remains rampant in jury selection.” The Batson challenge is designed to stop purposeful discrimination — and that has proved to be insufficient.\nThe court’s decision admits to the reality that racism has changed. Overt racism is largely unacceptable, but the court said we all have “stereotypes that are ingrained and often unconscious, implicit biases that endure despite our best efforts to eliminate them.” In other words, we can be racist even when we don’t intend to.\nThe high court’s opinion references numerous law-review articles, research and reports that explain how racism is often an unintentional function of our minds rather than a moral decision. I was co-author of one of those articles, “The Id, the Ego, and Equal Protection in the 21st Century,” in the Connecticut Law Review of May 2008.\nStudies have revealed that the brain reacts in a biased way toward people of color. Specific areas of the brain, called amygdalae, activate when we feel fear, threat, anxiety and distrust.\nFor many people, the amygdala activates upon seeing spiders and snakes. This same area of the brain was found in one study to light up when Caucasian participants viewed African-American male faces versus Caucasian male faces.\nUnconscious decisions by the brain are as old as the cave man. The ability to discern in a split-second between a member of one’s tribe and a dangerous animal is something hard-wired into us that creates implicit bias.\nJustice Steven González, in a concurring but separate opinion, characterizes how attorneys fall victim to implicit biases and use peremptory challenges to remove jurors “based solely on superficial judgments” even in cases where the available information about the juror shows no signs that the person is unqualified.\nGonzález believes Washington state should abolish peremptory challenges because there are no longer enough protections against using those challenges in a racially biased way, even unintentionally.\nShort of abolishing peremptory challenges, there are remedies that can be implemented more quickly.\nOne is training. My organization, the Equal Justice Society, of Oakland, Calif., offers the training of judges on understanding implicit bias and its impact on the court system. The Montgomery, Ala.-based Equal Justice Initiative provides reports on racial bias in the courts, including one on illegal racial discrimination in jury selection.\nTraining is also available from Washington’s Continuing Judicial Education program on an array of anti-bias and diversity subject matters.\nThe state and federal courts must also re-examine the sources of jury pools in order to maximize diversity and ensure that jury pools best represent their communities. Selection could be expanded beyond voter-registration and motor-vehicle-registration lists.\nThe struggle against racial bias in the court system is not over. Some hard work remains.\nEva Paterson is co-founder and president of the Oakland, Calif.,-based Equal Justice Society.']	['<urn:uuid:89a26611-2c7a-4f71-a49d-7e1e13b194cb>', '<urn:uuid:2a011efa-10fe-4f01-ad4d-fd3ec76ee67f>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	5	59	1877
85	What exactly is Native Title and how does it affect Aborigine access to land?	Native Title describes the rights and interests of Aboriginal and Torres Strait Islander people in land and waters according to their traditional laws and customs. While it allows Indigenous people to negotiate access to traditional lands for activities like hunting, fishing and visiting sacred sites, they must still respectfully request permission from property owners. For example, to visit cultural sites on neighboring properties, they need to call ahead and ask permission. Native Title doesn't mean Aboriginal people can remove others from the land - it's about acknowledgment of traditional owners and allowing negotiated access.	"['by Nalini Haynes\nI interviewed Keelen Mailman (audio interview) earlier this year about her life and memoir The Power of Bones. RMIT required a nonfiction essay or profile so I wrote mine on Keelen (below), while complying with the brief.\nOn the cover of her autobiography, The Power of Bones, Keelen Mailman wears a check shirt and akubra bracketing her coffee-with-cream complexion, a slight smile reaching her eyes, wisps of brown hair framing her face. Queensland State Finalist Australian of the Year in 2007, Keelen Mailman is the quintessential Australian Aborigine on the land, much like Deborah Mailman is the quintessential Australian Aborigine on the small screen.\nGrowing up in Australia, I had a basic understanding of Aborigine history since white settlement from a white perspective. I’d never heard an Aborigine tell their side of the story. I wondered what the view was from the other side of the divide.\nOn a crisp morning in Melbourne I phoned Keelen who answers, her voice a contralto. Keelen acknowledged the Traditional Owners of Australia in her language then interpreted into English. ‘I’d like to acknowledge the Traditional Owners far and wide, around Melbourne; I spoke in my traditional Bidjara language of my tribal people in Queensland.’\nKeelen is forty-eight with three children: Allan, Kristy and Charlee, who have given her four grandchildren. Keelen also fostered her sister’s five children. For the past seventeen years Keelen has lived and worked on Mount Tabor, a cattle station in Bidjara traditional country in far-western Queensland. It takes Keelen two hours to drive to the nearest town, Augathella, and eleven to twelve hours to drive from Mount Tabor to Brisbane.\nI said, ‘I’ve heard some funny stories about people overseas saying “while you’re in Brisbane, can you please drop this thing off in Adelaide?” because they just don’t understand how big Australia is.’\nKeelen said, ‘It’s the same for my grandkids, mate. They think it’s quite easy for Nanny to just pop around and read them a story, you know? I’m, like, ten hours away from them, as well.’\nI said, ‘I have to ask: are you a relative of Deborah Mailman, the actor?’\nKeelen replies, ‘The beautiful Deborah Mailman. Yes, I am. Apart from “Hello, how are you, nice to meet you, what’s your name?” the next thing that comes along: “Are you related to Deborah Mailman?” It’s quite beautiful. I got to meet her a couple of times. Very down-to-earth, beautiful person, and a fantastic actress.’\n‘I think a lot of people see her as the face of Aborigines on Australian television. You’re the face of Aborigines in the bush, running the station.’\n‘On the land,’ Keelen chuckles.\n‘Yes,’ I smile. I ask Keelen for an explanation of Aborigine history, of their relationship with the land.\n‘Our people were stripped off the land. Forcibly removed off country. And now with Native Title, we’re fighting another battle now. I’m not only talking about my Bidjara people, I’m talking the whole Aboriginal Island people of Australia. My old great-great grandmother and those old people are among [those forcibly removed off country]. They wonder why language and culture and stuff is dying out in areas; that’s not of choice. Then you’re in court and trying to prove your country, your culture, your connection.\n‘It’s very hurtful and insulting when they think that [our culture is] dying out in a lot of areas. We didn’t have that choice, did we? I was very lucky. I dedicate my book to my dear old mum who give me the greatest gift of all, life, then the handing down of culture and language, knowledge. I was very lucky that our mum was pretty strong with us learning all that stuff, being able to survive off the land if you’re ever stuck anywhere.\n‘It’s so unfair that we’re fighting for [Native Title]. A lot of our old people now have passed on. It just breaks my heart that we’re in court fighting our guts out to try and prove our country and our connection. The courts are not making it any easier for our people.’\nTo make sure I understood, I said, ‘So, white people came along, forced the Aborigines off and now, in order to get Native Title, you have to prove that you weren’t forced off.’\n‘Yeah. An old stockman here, Dave, who’s sort of like a dad to me, he was one of the people here way back in the early 30s or 40s. They got to put their names in a hat but there wasn’t one Aboriginal person who was allowed to put their name in a hat. That’s how they got to get their country. You had to have enough money to pay for the tenement at that time.\n‘Then we’re back again as Aboriginal people trying to fight to get access to country. I’ve got to pinpoint the government there for a lot of confusion to the wider community simply because Native Title isn’t all about me being able to go to next door’s property to tell them to get off my country. It’s just giving acknowledgment to traditional owners, allowin’ negotiations and access. Where a lot of them today think if you get Native Title, the blackfellas can kick you off. It should have been explained a lot better.’\nTo clarify, I asked, ‘My understanding is that Native Title is about allowing people to run cattle on a property at the same time Aborigines can have Native Title so they can do their traditional hunting and visit their sacred sites. Is that right?’\n‘That’s absolutely true, that’s absolutely correct. You can’t just lob up on a property. For instance, if I wanted to go to a property next door which is all traditional Bidjara country, there’s respect comes with it all as well. I’ve got to ring the property and see if it’s ok if I can go and visit some cultural sacred burials or art sites or do a bit of fishing or hunting, to be able to practice our culture.\n‘That’s why we’re so lucky with Mount Tabor Station. The Indigenous Land Corporation, which is my employer, purchased the property for our people because of the significant burials and cultural significance of the property.’\nI said, ‘I remember people getting really worried that they were going to lose their houses under Native Title back when it came in.’\n‘It is very sad. It’s taken us a hundred years to get where we are today and with all of that attitude and misinterpretation being put out there, it’s taken us back years. We’ve got some fantastic strong leaders there that fought for our rights and recognition of our people all around Australia to be able to get all this, sort of equal rights and access, our identity. We’re going backwards, it seems like we’re not going forwards, we’re going backwards.\n‘I don’t want any confusion. There is a lot of beautiful people out there. I don’t want anyone to be confused that I’m classin’ everyone in the same category but there are a lot of not nice people out there in this big old world of ours as well.’\nA very spiritual person, Keelen’s work as an advocate for her people and work to protect cultural sites began after she felt a spiritual calling to move back to her country from Brisbane. Sugar Ray Robinson asked Keelen to manage Mount Tabor Station after she’d already moved back to Augathella. ‘I was given that honour [of looking after sacred sites] a long time back, when the Elders said, “You’re the Keeper of Country”. I just felt such a warm feeling in my heart. Looking after burial sites and art sites that are so old, of your ancestrial* people, is just the most warmest and special feeling, it’s so spiritual. I can feel [the spirits] all around me, looking after me. I do the best job I can, and it is a very special thing to do for your people.’\n‘We’ve done a lot of fencing off of cultural burials and art sites and still continuin’ to fence some off. So that’s keeping out the cattle and brumbies (wild horses) because y’know, sandstone’s pretty fragile. You’ve got animals that are rubbing up against art work. Once it’s gone, you can never get it back.\n‘I check burial sites. If wallabies or anything have got in and knocked out ancestrial bones and remains, I’ll put them all back in and sort of pack it all back up again so it’s safe again. Just making sure that fences around cultural sites are all up around the boundary area to make sure nothing’s getting in. And it’s not only animals, you’ve got the human vandalism as well that takes place.\n‘Those sites are off-bounds. It’s people that I know that are coming, and people that I take there most of the time to let ’em have a look and to share our culture. It’s pretty important to share that sort of stuff with the wider community because you can talk about racism but you can also look at it as ignorance. Ignorance is coming from a lack of awareness of culture and what you’re trying to preserve and protect.’\nKeelen is passionate about her people, her heritage and her work. In her memoir Keelen talks about drug abuse among her people. In our conversation, Keelen talked about the need for Aboriginal Health Centres to help her people appropriately rather than treating the symptoms while the causes – poverty, unemployment, hardship – fester. Coming from poverty, Keelen has worked hard to help others. Now, as she faces limited time left in her physically demanding role on Mount Tabor, Keelen calls for greater understanding, compassion and collaboration to work towards a better future.\n*‘ancestrial’ is how Keelen pronounces ‘ancestral’.\nAllen & Unwin 2014, ‘The Power of Bones’, promotional page for Keelen Mailman’s autobiography, viewed 15 May 2014, http://www.allenandunwin.com/default.aspx?page=94&book=9781743313718.\nAustralian of the Year, n.d. ‘Honour Roll: State Finalist of the Year 2007, Keelen Mailman’, Australian of the Year, Brisbane, viewed 15 May 2014, http://www.australianoftheyear.org.au/honour-roll/?view=fullView&recipientID=700.\nFidler, R 2012, ‘Conversations with Richard Fidler: Keelen Mailman’, ABC Local, Brisbane, viewed 15 May 2014, http://www.abc.net.au/local/stories/2012/04/27/3490383.htm.\nHaynes, N 2014, ‘Keelen Mailman on Power, racism and advocacy’, audio interview of Keelen Mailman, Dark Matter Zine, Melbourne, viewed 29 May 2014, https://www.darkmatterzine.com/keelen-mailman/.\nMailman, K 2014, The Power of Bones, Allen & Unwin, Melbourne.\nOlsson, K 2014, ‘The Power of one: Keelen Mailman at Mount Tabor Station’, The Weekend Australian Magazine, Surrey Hills, viewed online 15 May 2014, http://www.theaustralian.com.au/news/features/the-power-of-one-keelen-mailman-at-mount-tabor-station/story-e6frg8h6-1226884033642.', ""What is Native title?\nNative title describes the rights and interests of Aboriginal and Torres Strait Islander people in land and waters, according to their traditional laws and customs. In Australia, Aboriginal and Torres Strait Islander peoples' rights and interests in land were recognised in 1992 when the High Court delivered its historic judgment in the case of Mabo v the State of Queensland.\nThis decision overturned the legal fiction that Australia upon colonisation was terra nullius (land belonging to no-one). It recognised for the first time that Indigenous Australians continue to hold native title.\nThe Commonwealth Government responded to the historic Mabo v Queensland decision in 1992 with the Native Title Act 1993 (NTA).\nUnder the NTA, Traditional Owners can lodge a claim with the Federal Court to have their native title rights determined. Where the claim is over Crown land in Victoria, the respondent will be the Victorian State Government.\nThe Victorian State Government's role in native title claims is led by the Department of Justice. DELWP provides input into this process as the Crown land manager, including conducting tenure assessments to determine where native title exists on Crown land.\nIn addition, as the Crown land manager, we are responsible for ensuring that any activities which occur on Crown land comply with the provisions of the future act regime of the Native Title Act 1993 (Cth)\nNative title determinations in Victoria\nTo date, four native title claims have been determined in Victoria.\nThree claims resulted in the recognition of native title by agreement via a consent determination in the Federal Court.\nThe relevant determinations are:\n- Gunaikurnai People (October 2010)\n- Wotjobaluk, Jaadwa, Jadawadjali, Wergaia and Yupagalk Peoples (known as the 'Wimmera' claim) (2005)\n- Gunditjmara People (2007)\nIn the Yorta Yorta claim, native title was determined not to exist by the Federal Court in a litigated determination in 2003.\nRead about Agreements with Traditional Owner groups.\nNational Native Title Tribunal\nThe National Native Title Tribunal website provides information about the Tribunal, future act processes, indigenous land use agreements, mediation and other information regarding native title and the Native Title Act 1993. The site also enables you to search for applications as well as maps and area descriptions.\nThe NNTT provides assistance and information to resolve native title issues over land and waters to all people involved in the native title process. Set up under the Native Title Act, the Tribunal is a federal government agency and is part of the Attorney-General's portfolio.\nTraditional Owner Settlement Act 2010\nManaging Crown Land - Native Title Extinguishment\nNative Title describes the rights and interests of Aboriginal and Torres Strait Island people in land and waters, according to their traditional laws and customs. Native title may exist in areas such as:\n- vacant (or unallocated) Crown land\n- parks and public reserves\n- some types of pastoral leases\n- land held by government agencies\n- land held for Aboriginal communities\n- oceans, seas, reefs, lakes, rivers, creeks and other waters that are not privately owned.\nHowever, Native Title can be extinguished or impaired when holders of native title are not able to fully exercise their traditional rights in an area.\nMost loss or impairment of native title is a result of government action in granting interests in Crown land in ways that exclude indigenous people from having co-existent rights to the land or waters.\nWays Native Title can be Extinguished\nThe grant of a freehold estate before 23 December 1996 will extinguish native title. Exceptions are:\n- Crown to Crown grants after 31 October 1975\n- Grants to or for the benefit of Aboriginal peoples\n- Where the Grant occurred under legislation\n- Grants in the period between 1 January 1994 and 23 December 1996, when special rules apply.\nIf, under State legislation enacted prior to 23 December 1996, land or waters were vested in any person, (including trustees or Authorities), and the vesting extended a right of exclusive possession, then the vesting can be considered to be a freehold grant that will have extinguished native title. Exceptions are:\n- Crown to Crown vestings after 31 October 1975\n- Vesting for the benefit of Aboriginal Peoples\n- Vesting occurred under legislation\nSchedule 1, Part 2 of the Native Title Act (Cth) 1993, lists various legislative provisions from Victorian Acts of Parliament dating from 1862 which relate to leasing and other tenancy of Crown land.\nIf an area of Crown land has, at any time prior to 23 December 1996, been the subject of any one of these Scheduled interests, native title will have been completely extinguished to the extent of the area of land over which the lease or tenancy is issued. Exceptions are:\n- Crown to Crown grants\n- Grants to or for the benefit of Aboriginal peoples\nIf a commercial lease (that is not an agricultural lease or pastoral lease), residential lease, community purpose lease or any other lease that provided exclusive use existed prior to 1 January 1994, native title is completely extinguished over the lease area. Exceptions are:\n- Mining Leases\nA public work includes buildings and other structural fixtures, roads, railways, bridges, wells and bores and any major earthworks that are constructed by or on behalf of the Crown, local government authority or other statutory authority of the Crown.\nThe authorised construction of such work on Crown land prior to 1 January 1994 will have completely extinguished native title over the land on which the public work is situated. Further, native title over any adjacent land or waters necessary for, or incidental to, the construction or establishment or operation of the work will also be extinguished.\nExtinguishment will also have occurred over land on which the public work is situated in the case of a public work completed after 1 January 1994, provided that the construction of the public work commenced prior to 1 January 1994.\nIndigenous Land Use Agreements (ILUAs) can provide for native title to be extinguished.\nA future act is any activity that affects native title that occurs on Crown land after 1 January 1994.\nExtinguishment will occur as a result of future activities where the non-extinguishment principle does not apply such as:\n- under a registered Indigenous Land Use Agreement (ILUA)\n- the grant of freehold estate or a right of exclusive possession\n- the construction of a public work\n- compulsory acquisition or surrender of native title rights and interests.\nPage last updated: 12/06/19""]"	['<urn:uuid:40434043-c3da-4bd3-9b46-35b943c6bf7e>', '<urn:uuid:8c1a0e78-3a35-428c-8d5a-630312437235>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	14	94	2832
86	what benefits saltwater aquaponics system fish farming vegetables sustainable food production	Saltwater aquaponics systems offer multiple benefits for sustainable food production. They combine fish farming with growing salt-loving crops, using fish wastewater to fertilize plants while recycling 100% of the water. This system is particularly efficient because fish have a lower feed conversion rate than land animals, meaning they produce more protein with less feed. The system also helps reduce environmental impact by preventing nutrient leakage and eutrophication. Additionally, these systems can operate away from coastal areas, can be implemented on a small scale for local food production, and provide both protein-rich seafood and valuable vegetables for human consumption.	['Saltwater fish and gourmet veggies pair up nicely on the dinner table, but their happiest marriage might be in eco-friendly farming. Mote Aquaculture Park — Mote’s sustainable fish farming facility in eastern Sarasota County — launched a pioneering project this fall to advance marine aquaponics: farming seafood while using the wastewater to fertilize salt-loving crops.\nThis commercial demonstration project, funded by Florida Sea Grant and done in partnership with the University of South Florida and Morningstar Fishermen, is using eco-friendly technology developed at MAP to raise the saltwater fish species, red drum, together with plant species sea purlsane and saltwort — sea vegetables popular in European cuisine. The project aims to bring its tasty results from farm to fork, showing communities a new way to produce food locally for farmers’ markets and restaurants.\nThe project builds on two practices trending in global food production: Aquaculture and hydroponics. Seafood farming today provides nearly half the world’s seafood, while hydroponics — which involves raising greenhouse crops in nutrient-rich water instead of soil — has shown potential for raising crops in small areas or unfertile land. The combination — aquaponics — has gained ground during the past two years in multiple nations; saltwater aquaponics is the newest wave.\n“Ninety-nine percent of aquaponics involves fresh water, and the few saltwater projects mainly use edible seaweed, so we’re helping broaden the horizons,” said Dr. Kevan Main, manager of the Marine and Freshwater Aquaculture Research Program at Mote. “There’s also demand for red drum, and we believe these gourmet sea vegetables have untapped potential for U.S. markets and chefs.\n“We want to help other communities adopt saltwater aquaponics, so we’ve created a self-contained and simplified system based on what we’re already using at Mote Aquaculture Park.”\nThe Park is a 200-acre, state-of-the-art aquaculture research facility located 17 miles inland from any large body of water. There, Mote scientists have developed recirculating aquaculture systems that raise freshwater fish while recycling much of the water and raise saltwater fish while recycling 100 percent of the water.\nThe new aquaponics greenhouse system uses diluted saltwater, which will also be fully recycled. “You need a design like this if you want to raise fish and vegetables away from the coast where you can’t pour saltwater on the ground,” Main said.\nIn late September, Mote staff moved the first 200 juvenile red drum into tanks inside the greenhouse, which is designed to produce about 600 fish. In early October they planted 1,100 sprigs of sea purslane (Sesuvium portulacastrum) and 1,100 of saltwort (Batis maritima) into foam rafts atop four water-filled raceways. The plants were donated by Aquatic Plants of Florida.\nWater begins at the fish tanks and moves through several filters that remove solid waste and allow bacteria to break down harmful ammonia into other nitrogen-rich compounds useful as fertilizer. The fertile water flows through the raceways to feed the plants. As the nutrients run low, the water is passed through a filtering screen and returned to the fish.\nAt each step in this cycle, doctoral student Suzanne Boxman of the University of South Florida will document levels of key nutrients — critical information for knowing how well the system works and helping others emulate it. “We want to know how much and how quickly the various forms of nitrogen, phosphorous and other compounds are removed by the plants,” said Boxman. Her work includes a life cycle assessment — a detailed study of environmental impact never before done\n“This is a novel study, and it’s been great to do this with Mote,” Boxman said, adding that there’s more to love than science: “I like to cook at home, and I can’t wait to taste the vegetables we’ll harvest.”\nMain hopes others will feel the same way. She plans to sell the plants through local farmers’ markets and distribute the fish through Florida-based wholesalers. The plants might reach harvest size in two months or more and the fish will be ready in nine to 12 months.\nUltimately, Mote scientists will share their results locally to nationally with help from Morningstar Fishermen, an organization that provides aquaponics training as one solution to world hunger.\n“This is just the beginning — we’re finding out how well this system can work,” Main said. “To see the prototype system up and running is so exciting.”', 'Faculty of Fisheries and Protection of Waters, University of South Bohemia in Ceske Budejovice, South Bohemian Research Center of Aquaculture and Biodiversity of Hydrocenoses, Institute of Aquaculture, Husova tr. 458/102, 370 05 Ceské Budejovice, Czech Republic\nReceived Date: October 01, 2014; Accepted Date: October 04, 2014; Published Date: October 14,2014\nCitation: Sampels S (2014) Towards a More Sustainable Production of Fish as an Important Protein Source for Human Nutrition. J Fisheries Livest Prod 2:119. doi:10.4172/2332-2608.1000119\nCopyright: © 2014 Sampels S. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nVisit for more related articles at Journal of Fisheries & Livestock Production\nFish and seafood products, including aquatic plants are not only tasty constituents of meals and food products, but they also have a high nutritional valuable and contain beneficial amounts of protein, lipids as well as essential minerals and vitamins. Nowadays, aquaculture is the fastest growing sector in livestock production around the world. An annual increase of production during last 30 years is ± 8% . In a recent review it has also been shown that fish and seafood have a major impact on global food supply . Already today, the internationally traded amount of aquatic animal food products is higher than the total traded amount of beef, pork and poultry combined (26.85-27.45 compared to 20.38-21.99 million tons in 2009 respectively) [3,4]. In addition in several Asian and African countries the major animal protein source are aquatic animals [3,5].\nGenerally, aquatic animal foods have higher protein content than most terrestrial meats. In addition aquatic protein is highly digestible and rich in several essential amino acids that are limited in terrestrial meat proteins, as for example methionine (6.5% compared to 5.7% of total essential amino acids in fish compared to terrestrial meat respectively) and lysine (19.6% compared to 19.0% of total essential amino acids in fish compared to terrestrial meat respectively) . Fish and seafood consumption has been connected to beneficial health effects, especially with the prevention of cancer, decreased risk of coronary heart and cardiovascular diseases, decreased inflammatory disease as arthritis. Historically the main effects of fish consumption have been attributed to the high content of long chain omega 3 (n-3) polyunsaturated fatty acids (PUFA). However it gets more and more clear that also other nutrients from fish have positive effects on human health. Beside the n-3 PUFA, fish and other seafood are a significant source of a well-balanced amino acid composition, taurine and choline, the Vitamins D and B12 as well as calcium, phosphorus, iodine and selenium. Depending on the general nutrition of a population, fish and seafood also might provide significant proportions of Vitamin A, iron and zinc. Another important aspect is that fish in general has a significantly lower feed conversion rate (FCR) than land living animals. For example beef cattle can have a FCR varying from 5-20 (National Research Council 2000)  while Masilko  reported for example a FCR for carp from 2.08-2.62 while a FCR from 1.6-1.8 was reported for tilapia  and salmonids usually have a FCR at around 1 . This means more protein can be produced with a lower amount of feed with all included factors concerning feed production as energy and water use as well.\nThis combination makes fish an interesting object of research with a focus on an economical and easy as well as sustainable production in order to secure protein supply for human diet in the future. There has been some discussion however about the sustainability of aquaculture, where the main issues are the high use of fish meal and fish oil as well as the leakage of nutrients to the environment leading to an eutrophication of aquatic systems. There has been some research going on about substituting the more and more scarce and expensive fish-meal and fish oil from the fish feeds for carnivore fishes . A second object towards a more sustainable production has been to produce feeds with a reduced FCR. For example it was shown that a simple pre-treatment of the used feed as for example pressing of cereals could reduce the FCR in carp as well as reduce the leakage of phosphorus to the pond water . In combination with other techniques and systems as for example aquaponics, where vegetables or flowers are grown in the nutrient rich flow out water from aquaculture recirculation systems . The plants act as a bio filter and clean the water from execs nitrogen and other nutrients and provide additional food in form of vegetables or herbs. These systems also decreases the load of phosphorus, nitrogen and other nutrients from the waste water and have hence a positive effect on the environment . Aquaponic systems can function already on a small base and could be hence a good solution for small or family businesses in rural areas. Therefore research should also focus on establishing simple, easy to operate and sustainable aquaculture/ aquaponic systems, which can function with a minimum of technical support, and which have a low proportion of waste water. There is no doubt that aquaculture has still a big potential to grow and to be a major contributor to feed the world of tomorrow.']	['<urn:uuid:e32b57be-4eea-4917-b943-abad5074f61e>', '<urn:uuid:a1b9d399-3786-40bb-8857-576c0ba1091b>']	open-ended	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T12:39:32.578853	11	98	1599
87	who can make valid public interest disclosure	Only public officials can make valid public interest disclosures. A public official is defined as an individual who is an employee of or in the service of a public authority, such as a public service agency, local government authority, or similar entity. The public official must honestly believe on reasonable grounds that their information shows misconduct by another public official or public authority. The protection remains even if the person who made the disclosure is no longer a public official.	['To print this article, all you need is to be registered or login on Mondaq.com.\nIn 1994, the NSW Government introduced the Public Interest\nDisclosures Act 1994 (“PID Act”) to offer protection\nto whistleblowers who disclose evidence of corrupt conduct or\nmaladministration by a public authority or its officers. The PID\nAct not only declares it illegal to take detrimental action against\na whistleblower for making the disclosure, but it also provides\ncompensation for any loss suffered by the whistleblower if they do\nexperience retaliation. This article explains the compensation\navailable to whistleblowers in New South Wales.\nWhat is a Public Interest Disclosure?\nA Public Interest Disclosure is the disclosure of information\nrelating to a public authority including:\n- Corrupt conduct: Any conduct that seeks to\nadversely affect the honest or impartial exercise of official\nfunctions by any public official or public authority or any conduct\nwhich involves a breach of public trust.\n- Maladministration: action or inaction of a\nserious nature that is— contrary to law, or unreasonable,\nunjust, oppressive or improperly discriminatory, or based wholly or\npartly on improper motives.\n- Serious and substantial waste of government\n- Government information contravention: conduct\nthat amounts to a failure to exercise functions under the\nGovernment Information (Public Access) Act 2009.\n- Local government pecuniary interest\ncontravention: the breach of an obligation imposed by the\nLocal Government Act 1993 in connection with a financial\nUnder the PID Act, a disclosure made by a person in relation to\nthe above conduct will only be treated as a Public Interest\nDisclosure and thereby receive protection under this Act if the\ndisclosure is made to a specific authority by a certain class of\nMaking a valid public interest disclosure\nWho can make a public interest disclosure?\nTo be protected by the PID Act, the disclosure must be made by a\nA “public official” is defined as an individual who is\nan employee of or otherwise in the service of a public authority\nsuch as a public service agency, local government authority or the\nThe public official making the disclosure, must honestly believe\non reasonable grounds that the information they seek to disclose\nshows or tends to show, that another public official or public\nauthority has engaged, is engaged, or proposes to engage, in the\nA disclosure will be protected under the PID Act regardless of\nwhether the person who made the disclosure is still a public\nTo whom must the public interest disclosure be\nFor a public interest disclosure to be valid, the public\nofficial must make the disclosure to an investigating authority,\nthe principal officer of a public authority or to a member of\nParliament. A disclosure may also be made to an officer of a public\nauthority or investigating authority to which the public official\nbelongs to, or alternatively, the relevant authority to which the\ndisclosure relates to. In this situation, the disclosure must be\nmade in accordance with any procedure established by the relevant\nauthority reporting allegations of public concern.\nAn “investigating authority” includes the Independent\nCommission Against Corruption (ICAC), the NSW Ombudsman, the\nInformation Commissioner, the New South Wales Crime Commission, the\nlocal government investigating authority and the Law Enforcement\nWhat is not a valid public interest\nA public interest disclosure that is made largely for the\npurpose of avoiding disciplinary action (not being disciplinary\naction taken in reprisal for the making of a public interest\ndisclosure) will not be protected by the PID Act.\nSimilarly, a disclosure made by a public official that\nprincipally involves questioning the merits of government policy\nwill not be protected.\nProtection of misdirected disclosures\nIf a public official makes a disclosure to the wrong\ninvestigating authority, that disclosure will be protected under\nthe PID Act, if the public official honestly believed that they had\nmade their disclosure to the appropriate investigating authority to\ndeal with the matter and provided:\n- the investigating authority refers the disclosure to another\ninvestigating authority or to a public official or public\n- the investigating authority could have referred the disclosure\nbut did not do so because it has power to investigate the matter\nconcerned under the relevant investigation Act.\nCompensation for detrimental action taken against\nUnder the PID Act, it is an offence to take detrimental action\nagainst another person in retaliation for that person making a\npublic interest disclosure. This extends to circumstances where the\nperson who takes the detrimental action does so because the person\nbelieves that the other person made or may have made a disclosure,\neven if the other person did not make such a disclosure.\nIn both situations, the person, employer, or organisation who\ntakes detrimental action against a person who made or is suspected\nto have made a disclosure will be liable to pay compensation for\nany loss that the other person suffers as a result of that\n“Detrimental action” is defined under the PID act as\naction causing, comprising, or involving:\n(a) injury, damage, or loss,\n(b) intimidation or harassment,\n(c) discrimination, disadvantage, or adverse treatment in\nrelation to employment,\n(d) dismissal from, or prejudice in, employment,\n(e) disciplinary proceeding.\nWhilst a claim for damages under the PID Act is yet to be\ndecided by a NSW Court, Carroll & O’Dea Lawyers have\nsuccessfully achieved a number of significant settlements for\nwhistleblowers which have included compensation for loss of\nemployment, loss of earning capacity, medical treatment expenses,\nrequirement for domestic assistance and pain and suffering. Without\na court decision, it is unclear whether damages are\n‘capped’ by the Civil Liability Act 2002, or\nwhether damages are able to be assessed based on common law\nThe content of this article is intended to provide a general\nguide to the subject matter. Specialist advice should be sought\nabout your specific circumstances.']	['<urn:uuid:edd29661-8972-442a-a67a-07d2247916b7>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	7	80	958
88	operational daily assets vs strategic business risks	Operational assets, which are necessary for daily business transactions and production (including cash, inventory, and equipment), face operational risks from inadequate routines, processes, or unexpected external events. These risks are monitored continuously and managed through internal instructions, personnel training, and insurance coverage. On the strategic side, business risks arise from competition and market forces causing unexpected fluctuations in volumes and costs, potentially affecting long-term business goals. These risks can emerge from wrong strategy choices, mismanagement, or slow reaction to market changes.	['Principles of Risk Management\nRisk management aims to identify threats that may affect the implementation of the Group’s strategy or jeopardise the achievement of the capital adequacy or profitability targets. The objective of risk management is to minimise the probability of unexpected monetary losses and realisation of reputational risk. The parent company`s Board of Directors has confirmed the company’s risk management policy and supervises risk management.\nThe organisational structure is flat, and each employee has one of the company’s partners as his or her superior. Thus, risk management is the responsibility of each employee, and each employee must understand the risks related to his or her duties and try to minimise them.\nTogether with other compliance officers, the Group’s General Counsel ensures that the Group operates in compliance with legislation and official regulations. The Group has a risk management function which prepares matters related to risk management and reports them to the parent company’s Board of Directors. The Board provides risk management guidelines in internal instructions upon request by the Group’s risk management function. Compliance with the instructions is monitored through inspections carried out according to the relevant rules and regulations.\nThe Group’s business is not deliberately exposed to market, credit or counterparty risks. Instead, the company’s operations in its key business areas (investment banking and private equity funds) are fee-based. The company also has a low risk exposure when investing its own assets.\nThe following sections describe the main risks of investment service companies from the viewpoint of the ICECAPITAL Group.\nRealisation of a credit risk means that a counterparty has failed to fulfil its contractual financial obligations. The Group’s counterparties include partners and customers who purchase its services. The impact of credit risk on the company’s own assets is minor as the Group’s customers are solvent and have a well-established position.\nMarket risks arise from fluctuations in the equity, fixed-income, real estate and currency markets and their impact on the Group’s results. The impact on the ICECAPITAL Group is fairly minor due to the investment policy applied by the company. The company’s assets have been invested with moderation and a long-term investment span.\nLiquidity risk refers to a situation in which the company’s cash and cash equivalents are insufficient and no additional financing is available. The ICECAPITAL Group is highly solvent. In practice, the bulk of the Group’s assets is held in bank deposits and in liquid shares. Moreover, since the Group has relatively steady and predictable cash outflows, it can prepare well for its liquidity needs. Consequently, liquidity does not constitute one of the Group’s main risks.\nOperational Risks and Reputational Risk\nIf they materialise, operational risks cause damage as a result of inadequate or incorrect routines, processes or conduct, or through unexpected external events. Operational risks also include legal risks, such as invalidity of contracts already signed and damage caused by incomplete documentation or violation of laws or official regulations. The Group’s expenses from operational risks have been low to date.\nOperational risks are monitored continuously, and the actual risks are reported. The business units are responsible for managing the operational risks of their own area. The results of the self-assessment of operational risks carried out by the units are used as a basis when trying to prevent operational risks. Efforts are made to reduce the risks observed by developing internal instructions and processes, training personnel and auditing operations. In addition, the ICECAPITAL Group has insurance to cover damage caused by mistakes, misconduct and criminal activity. Contingency planning keeps the Group prepared for any major interference in operations.\nOne of the most significant types of damage caused by operational risks could be reputational risk concerning the Group. Operating in the financial markets requires the absolute trust of customers and partners, and the realisation of reputational risk could have a detrimental effect on the Group’s business. In order to avoid this risk, ICECAPITAL monitors and follows legislation concerning its operations and good market practice and strives to continuously develop its procedures.\nStrategic Risk and Business Risk\nStrategic risks and business risks arise from competition, internal pressures or market forces which result in unexpected fluctuations in volumes, margins and costs, thus affecting the volatility of earnings and the achievement of long-term business goals. They may also arise from opting for a wrong strategy, from mismanagement and inadequate monitoring or from slow reaction to changes in the operating environment.\nThe Group forms an opinion about new products and services, even if future demand for them is uncertain. One of ICECAPITAL’s core competencies is predicting the future needs of investors and companies before others do. To minimise the risk, all significant strategic decisions and investments are made only after comprehensive surveys and risk assessments.\nThe ICECAPITAL Group’s international risks are related to the ICECAPITAL Nordic Secondary Fund S.C.A., SICAR fund established in 2010. Although external investors in the funds accept the risk of losses with the hope of future profits, the occurrence of significant losses would have a detrimental impact on the company’s reputation.\nThe amount of assets tied up in ICECAPITAL Securities (Sweden) AB, which provides Corporate Finance advisory services in Sweden, is reasonable in view of the Group´s solvency and will not jeopardise the continuity of operations.\nAccording to the Capital Requirements Regulation (Regulation [EU] No. 575/2013), the requirements for own funds must be met as follows: CET1 capital must account for at least 4.5%, T1 capital for at least 6% and total capital for at least 8 % of the total risk exposure amount. On 31 December 2019, ICECAPITAL Securities Ltd´s total capital accounted for 94,4 per cent of the total risk exposure amount.\nICECAPITAL has drawn up an equity plan in accordance with the capital adequacy regulations, which describes the risk-based capital needs, sufficiency of capital and solvency. The Group aims to maintain a capital adequacy ratio at least 1.5 times that required.', 'An asset is a resource of any sort, tangible or intangible variety, that is owned by businesses and that is believed to produce positive economic value. A financially sound organization would have more assets than liabilities and while this means good financial health, the opposite means poor financial health of the organization. The balance sheet of the organization enlists all the assets owned by that organization.\n7 Type of Assets in an Organization\nFor every types of assets, there are three aspects around which the type confirms: Ownership, Economic Value, and Resource.\n- Ownership – Who is the owner of the asset and who can decide to convert the asset into cash if needed\n- Economic value – what is the economic value of that asset at whether it can be exchanged or sold\n- Resource – What resources does the asset use or what purpose does it serve and how does it generate benefits for the future.\nThese three aspects determine all the categories of the assets.\n1. Tangible Assets\nThe assets which can be felt, seen and touched are called tangible assets. These assets have a physical substance and an economic value. An asset such as currencies, cash, real estate, vehicles, etc. are tangible assets. A common practice is to apply depreciation to tangible assets which have a lifespan of more than one year.\n2. Intangible Assets\nUnlike tangible assets, intangible assets lack a physical substance and are very difficult to evaluate. examples of intangible assets would include patents, copyrights, Goodwill, trademarks and trade names. The lack of physical presence in case of intangible assets sometimes creates them hard to define and measure.\nA company’s research and development department are also considered as an intangible asset. Research and development department is concerned with researching new theories, hypothesis, and products for the organization. The output of research and development will provide a competitive edge for the organization and bring new products into the market and hence research and development department is considered as an asset.\nWhile on the other hand intangible assets like goodwill which is created from customers, trademarks which helps in identifying the products of the company etc. are equally crucial. General accounting standards offer few examples of how should the intangible assets be accounted for in the financial statements. Under US GAAP, intangible assets are further classified into Internal intangibles vs. Purchased intangibles and limited life vs unlimited life intangibles.\n- Brand equity\n3. Current Assets\nCurrent Assets are the type of assets which can easily be converted into cash. These include stock, inventory, fixed deposits, bank balance, prepaid expenses etc. Current assets have a relatively shorter life as compared to fixed assets and sometimes current assets are also termed as liquid assets.\nThe advantage of current assets is that an organization can liquify them at will and they provide cash to run the business. In terms of emergency, current assets are the first to be sold out. The ability of the firm to convert quick cash or current assets to nullify its liabilities is called quick ratio or acid-test ratio.\n- Inventory or stock\n- Bank Balance\n- Accounts receivables\n4. Fixed Assets\nLike the name explains, fixed assets are fixed in nature and they cannot be easily converted into cash. They require a lot of time for conversion into cash however compared to current assets fixed assets are more profitable. Building plant machinery is some of the examples of fixed assets. Fixed assets are never sold by the company unless at the time of emergency. At times the organization may think of replacing the fixed assets but the survival of the organization is very difficult without fixed assets. Fixed assets are also referred to as PPE which is Plant, Property, and Equipment. Compared to current assets these are purchase for a very long-term and then sure generating profits for business. Fixed assets are also defined as the assets which cannot be sold to the end customer directly. Fixed assets are of two types: Freehold fixed assets and Leasehold fixed assets.\n- Freehold Fixed assets are the ones which are purchased with the legal right of ownership. This includes land when it is owned by the owner.\n- Leasehold Fixed assets are the ones which are leased for a pre-decided period of time. The ownership of the leasehold fixed assets lies with the owner while the usage rights lie with the borrower. Post completion of a fixed period, the owner can decide whether or not to continue leasing the property or asset to the borrower.\n5. Operating Assets\nOperating assets are all the assets that are necessary for everyday transactions of business. In other words, the assets that the company utilizes for the production of service or product are called operating assets. They include cash, bank balance, inventory, equipment etc. Operating assets are very useful for running of the business and without operating assets your organization cannot produce the output.\n6. Non-operating assets\nThe assets which are not used on a regular basis for everyday operations of the business are termed as non-operating assets. However, these assets are very crucial for the future needs of the business and hence they are termed as non-operational. an example would include the purchase of a real estate by the organization for appreciation but not for everyday operations and production. Short-term investments, marketable securities, vacant land are examples of non-operating assets.\n7. Financial Assets\nA financial asset is one that has value on its own. A financial asset is used to convert the asset into liquid cash. It can be anything from cash itself to stocks, bonds, etc. Short-term and long-term investments are also classified as financial assets. All fixed assets, current and operating assets fall under financial assets. Intangible assets do not come under this category.\nImportance of classifying different Types of Assets for accounting\n- For the proper flow of money, to understand the cash flow and expenses generated, it is very crucial that assets are classified since the classification gives a better understanding of which head incurs more expenses and generates more profits.\n- Classifying the assets into tangible and intangible, operating and non-operating and other types helps the firm to determine its solvency and risk.\n- Every types of assets serves a specific purpose. For example, the fixed asset cannot be sold in the times of financial crisis but current assets can be sold. Having knowledge of this essential for smooth running of the business and it also helps in making financial decisions.\nExamples of Assets\nHere are different examples of types of assets that can be used in business or accounting\n- Short-term assets examples – An asset that is held for a year or less. Example – Current Inventory in hand which might get sold out over a year. Or market outstanding / receivables which have to be collected in the short term is also an example of asset.\n- Fixed-assets examples – Fixed Assets are retained over a long period of time. These include real estate investments or equipment purchased. This category of assets will stick for a long period of time with the company.\n- Intangible asset examples – These assets are intangible in nature and not physically present but they provide a lot of benefit to the business or organization. The best example is the Brand equity of companies like Coca Cola or Nike. These companies have brand equity which is intangible in nature but worth a lot to the company.\n- Financial investments examples – Many companies repeatedly do financial investments to keep the company safe from turmoil. A few such investments could include stocks, bonds, or other such securities']	['<urn:uuid:1c4cc122-4c22-4b97-8edc-bcec51d231a8>', '<urn:uuid:34e4adf2-c2d6-4948-a846-b2361183db58>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	7	81	2258
89	I was reading about particle detection and got confused - how do modern neutrino detectors like NOvA and Super-Kamiokande differ in the way they catch these ghost-like particles?	NOvA and Super-Kamiokande use different approaches to detect neutrinos. NOvA consists of two huge particle detectors placed 500 miles apart, working with an intense beam of neutrinos generated by one of the world's largest accelerators near Chicago. It sends billions of particles through the Earth every two seconds toward the detectors, which weigh 300 and 14,000 tons respectively. In contrast, Super-Kamiokande in Japan uses a different method - it has a huge tank of water lined with 11,146 photo-multiplier tubes that detect Cherenkov light, which is emitted when neutrinos pass through the water at nearly the speed of light. Both detectors are placed deep underground to avoid interference from other cosmic particles.	"['Ghostly particles that constantly bombard us can offer clues to the early moments of our universe\nScientists hunting one of nature’s most elusive, yet abundant, elementary particles announced today they’ve succeeded in their first efforts to glimpse neutrinos using a detector in Minnesota.\nNeutrinos are generated in nature through the decay of radioactive elements and from high-energy collisions between fundamental particles, such as in the Big Bang that ignited the universe.\nLight and ghostly, however, they are unaffected by magnetic fields and travel at the speed of light.\nThe neutrinos that currently bombard the earth from space are mostly produced by the nuclear reactions that power our sun.\nMore than 200 physicists from around the world collaborate on the massive neutrino experiment called NOvA, which has taken a decade to design and build.\nTheir goal is to discover more details about neutrinos, which were theorized in 1930 and first observed in 1956.\n“These first few neutrino events are a confirmation that NOvA’s basic detector design and construction that dozens of people have worked on for a good fraction of a decade are sound,” said Thomas E. Coan, an associate professor in the SMU Department of Physics, who is a researcher in the NOvA collaboration.\nStudying neutrinos could yield crucial information about the early moments of the universe, Coan said.\n“Neutrinos are fascinating. They are, besides light, the most numerous particle in the universe yet are notoriously difficult to study since they interact with the rest of matter so feebly,” he said. “Produced in many venues, from laboratories to stars and even bones, they may be their own anti-particles and perhaps play a key role in explaining why any matter at all exists today and survived annihilation with its sister anti-matter produced all the way back in the Big Bang, many billions of years ago.”\nNOvA is the world’s longest-distance neutrino experiment\nThe NUMI Off-Axis electron neutrino Appearance, or NOvA, is the world’s longest-distance neutrino experiment. It consists of two huge particle detectors placed 500 miles apart, and its job is to explore the properties of an intense beam of neutrinos.\n“NOvA represents a new generation of neutrino experiments,” said Fermilab Director Nigel Lockyer. “We are proud to reach this important milestone on our way to learning more about these fundamental particles.”\nScientists generate a beam of the particles for the NOvA experiment using one of the world’s largest accelerators, located at the Department of Energy’s Fermi National Accelerator Laboratory near Chicago. They aim this beam in the direction of the two particle detectors, one near the source at Fermilab and the other in Ash River, Minn., near the Canadian border. The detector in Ash River is operated by the University of Minnesota under a cooperative agreement with the Department of Energy’s Office of Science.\nBillions of those particles are sent through the earth every two seconds, aimed at the massive detectors. Once the experiment is fully operational, scientists will catch a precious few each day.\n“It is both intellectually and emotionally satisfying,” said SMU’s Coan, “akin to a great adventure, to be detecting neutrinos in Northern Minnesota that are produced some 500 miles to the south at Fermi National Laboratory near Chicago, after making thousands of engineering and scientific decisions that had to be spot-on to see these events.”\nScientists will use NOvA to understand three changing flavors of neutrinos\nNeutrinos are curious particles. They come in three types, called flavors, and change between them as they travel. The two detectors of the NOvA experiment are placed so far apart to give the neutrinos the time to oscillate from one flavor to another while traveling at nearly the speed of light. Even though only a fraction of the experiment’s larger detector, called the far detector, is fully built, filled with scintillator and wired with electronics at this point, the experiment has already used it to record signals from its first neutrinos.\n“That the first neutrinos have been detected even before the NOvA far detector installation is complete is a real tribute to everyone involved,” said University of Minnesota physicist Marvin Marshak, Ash River Laboratory director. “This early result suggests that the NOvA collaboration will make important contributions to our knowledge of these particles in the not so distant future.”\nOnce completed, NOvA’s near and far detectors will weigh 300 and 14,000 tons, respectively. Crews will put into place the last module of the far detector early this spring and will finish outfitting both detectors with electronics in the summer.\nThe NOvA collaboration is made up of 208 scientists from 38 institutions in the United States, Brazil, the Czech Republic, Greece, India, Russia and the United Kingdom. The experiment receives funding from the U.S. Department of Energy, the National Science Foundation and other funding agencies.\nThe NOvA experiment is scheduled to run for six years. Because neutrinos interact with matter so rarely, scientists expect to catch just about 5,000 neutrinos or antineutrinos during that time. Scientists can study the timing, direction and energy of the particles that interact in their detectors to determine whether they came from Fermilab or elsewhere.\n“Seeing neutrinos in the first modules of the detector in Minnesota is a major milestone”\nFermilab creates a beam of neutrinos by smashing protons into a graphite target, which releases a variety of particles. Scientists use magnets to steer the charged particles that emerge from the energy of the collision into a beam. Some of those particles decay into neutrinos, and the scientists filter the non-neutrinos from the beam.\nFermilab started sending a beam of neutrinos through the detectors in September, after 16 months of work by about 300 people to upgrade the lab’s accelerator complex.\nDifferent types of neutrinos have different masses, but scientists do not know how these masses compare to one another. A goal of the NOvA experiment is to determine the order of the neutrino masses, known as the mass hierarchy, which will help scientists narrow their list of possible theories about how neutrinos work.\n“Seeing neutrinos in the first modules of the detector in Minnesota is a major milestone,” said Fermilab physicist Rick Tesarek, deputy project leader for NOvA. “Now we can start doing physics.” — Andre Salles, Fermilab, and Margaret Allen, SMU\nFollow SMUResearch.com on Twitter.\nFor more information, www.smuresearch.com.\nSMU is a nationally ranked private university in Dallas founded 100 years ago. Today, SMU enrolls nearly 11,000 students who benefit from the academic opportunities and international reach of seven degree-granting schools. For more information see www.smu.edu.\nSMU has an uplink facility located on campus for live TV, radio, or online interviews. To speak with an SMU expert or book an SMU guest in the studio, call SMU News & Communications at 214-768-7650.', 'Neutrinos are everywhere. They permeate the very space all around us. They can be found throughout our galaxy, in our sun and every second tens of thousands of neutrinos are passing through your body. But there is no need to become alarmed for these tiny particles barely interact with anything. In fact, they can even pass through the entire Earth without being affected.\nNeutrinos are fundamental particles that were first formed in the first second of the early universe, before even atoms could form. They are also continually being produced in the nuclear reactions of stars, like our sun, and nuclear reactions here on earth. Much is still unknown about these particle, they have an undetermined mass and travel at near the speed of light.\nThere are three types of neutrinos: electron neutrino, muon neutrino, and tau neutrino. According to the standard model ( see figure 1) there exist 12 fundamental particles. Each ""flavor"" of neutrino has a corresponding charged particle from which it gets its name. The Standard Model consists of three generations and each generation has two quarks a neutrino and a charged particle.The particles in the standard model are separated into two types: quarks and leptons. The quarks interact via the strong nuclear force while the leptons interact via the electromagnetic or the weak nuclear force. Neutrinos are nearly massless and have no electric charge. Therefore, unlike the other particles, they only interact via the weak nuclear force. Neutrino actually means ""little neutral one."" Since the weak nuclear force only acts at shot ranges, neutrinos can pass through massive objects without interacting with them.\nCasper,Dave. What’s a Neutrino?.\nBecause of the neutrinos\' elusive behavior, their existence was not even known until 1959 even though they had been predicted back in 1931. Wofgang Pauli first predicted the neutrino in order to account for the apparent loss of energy and momentum that he observed when studying radioactive beta decays(see Figure 2).\nFig. 2 The neutron Decay.(Neutrinos)\nHe predicted that the energy was being carried off by some unknown particle. Then in 1959, Clyde Cowan and Fred Reines finally found a particle that fit the description of the proposed neutrino by studying the particles created by a nuclear power plant. By doing this they actually discovered the electron neutrino. The next big discovery was that of the muon neutron found by Leon Lederman, Mel Schwartz, and Jack Steinberger, scientists at CERN. They did this by firing a GeV proton beam through a target thus producing pions, muons, and muon neutrinos.\nThe first experiment to attempt to detect electron neutrinos from the sun was conducted by a detector in the bottom of the Homestake mine in South Dakoda in 1968. However they detected only neutrinos about twice a week. It was predicted however that the detector should find about one of the 1016 solar neutrinos a day. This unexplainable lack of solar neutrinos detected became known as the Solar Neutrino Problem. It is thought that the neutrinos actually oscillate between the different ""flavors"" after being emitted from the sun as electron-neutrinos. Therefore they were not detecting all of the neutrinos because some had changed into muon and tau neutrinos.\nThe existence of the third flavor of neutrino, the tau neutrino, was first inferred in 1978 with the discovery of the Tau particle at SLAC, the Stanford Linear Accelerator Center. They realized that the Tau particle was just a heavier version of the electron and muon and therefore should have a corresponding neutrino as well. The tau neutrino evaded detection though for many years. Firstly the Tau particle only lasts for about 300 fs, making them difficult to track and therefore making it difficult to track their corresponding neutrino. Secondly, tau neutrinos are incredibly rare. However, in 2000 the scientists at CERN on the DONUT detector were finally able observe a tau neutrino.\nVerkindt,Didier. History of the Neutrino.\nA star implodes in a cataclysmic bust of energy, a scientist measures the small energy discrepancies of radioactive decay, the sun constantly baths us with warmth and showers us with particles, all around us the universe is saturated with the remnants of the Big Bang, and in all of these things there is one thing in common: neutrinos. Although they are hard to detect, these little particles can tell us about everything from the birth of the universe to the nuclear reactions that power our cities.\nOn February 23 1987 detectors deep underground that where designed to detect proton decay suddenly detected a huge number of neutrinos (8 in 5 seconds). Scientist where perplexed by this influx in neutrinos at first until on February 24 a grad student named Ian Shelton announced his observation of a supernova in the Large Magellanic Cloud. This was a core collapse supernova. When the core of a massive star collapses, it crushes the protons and electrons together and neutrinos form.\np + e → n + ν.\nThe neutrinos pass straight through the collapsing star before the explosion takes place. This is why the neutrinos where detected before the supernova was visibly observed. When the neutrinos leave, they also take energy away from the star and the star continues to collapse and rebounds out in an explosion that can outshine the brightness of the entire galaxy. Neutrinos are very important to the study of supernovas because they provide an early warning signal and allow scientists to be looking in the right direction before the supernova even takes place.\nNeutrinos are also created in the nuclear reactions that power the core of stars like our sun. Neutrinos are formed in the proton- proton chain.\np + p → deuteron + positron + neutrino,\nwhere the deuteron is the nucleus of deuterium. In the sun, 4 hydrogens are being fused into Helium by means of the proton-proton chain. Neutrinos are important because they allow scientists to peek into the interior of the sun and learn about the processes there. All other information about the sun is from electromagnetic radiation that has to pass through the many layers of the sun interacting and changing along the way before traveling through space to us. This whole process can take up 105 to 106 years. However, the neutrinos pass cleanly through the sun in a few seconds without interacting and take a mere 8 minutes to travel from the core where they are created to us.\nThe greatest source of neutrinos happened some 15 billion years ago. The neutrino was first created 10-4 seconds after the big bang. Then at only 1 second after the big bang the universe became transparent to the neutrino allowing them to travel freely through space. At this time the universe had a temperature of about 3*1010. Since the time of the big bang the universe expanded and cooled and continues to expand to this day. There are about 330 million of these neutrinos per m3; however, these neutrinos have very low energy. They form a cosmic background radiation that is only 2.73 degrees Kelvin today. By studying these neutrinos scientists are able to learn about the universe when it was forming.\nChaisson, McMillan. Astronomy Today. Upper Saddle River, New Jersey: Pearson Education, Inc., 2005.\nScientists are constantly coming up with new and ingenious ways to study neutrinos from space. Neutrino telescopes like Super-Kamiokande in Japan use huge vats of water to detect neutrinos (see Figure 3).\nThe inside of the tank is lined with 11,146 photo-multiplier tubes that detect Cherenkov light. Cherenkov light is emitted by the neutrinos as they pass through the water at near the speed of light.Therefore the detector detects the effect of the neutrinos interacting with the water and not the neutrinos themselves. Telescopes like the Super-Kamiokande are deep underground in order to avoid detecting other particles from cosmic rays. The Homestake telescope uses Chlorine 37 coming from Argon and the GALLEX telescope uses Germanium 71 coming from Gallium as the medium to detect neutrinos instead of water.\nCurrently, scientists are building a better neutrino telescope by using the clear polar ice as a medium by which to detect the neutrinos. IceCube is a one-cubic-kilometer new neutrino telescope being built in the South Pole (see Figure 4). It will be an array of 80 strings, each string having 60 optical moduals that are desigened to detect the cherenkov light from emitted from muons, which are a byproduct of the neutrinos interacting with the ice. Scientists are building this new telescope under the South Pole because it allows them to make it incredibly large, to have a very stable place for the detectors, to keep a stable temperature, and to built it deep enough to avoid interference from cosmic rays.\nThe arctic ice also makes a good medium because it is pure, transparent, and free from radioactivity. Scientists hope that by building IceCube they will be able to learn more about far away neutrino sources like gamma ray bursts, supernova, black holes and maybe even dark matter.\nCasper,Dave. What’s a Neutrino?.\nWelcome to IceCube.\nHow The IceCube Telescope Works\nReturn to Top\nKirsten Larson\'s REU Home Page']"	['<urn:uuid:dfad77de-1473-4e78-bae0-840aa68deece>', '<urn:uuid:df92bd26-1f30-4706-adee-724ece1ac665>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	28	112	2631
90	medical professional inquiring eligible conditions qualifying patients special needs shelter program criteria	Eligible conditions for the Special Needs Shelter include being dependent on supplemental oxygen, limited mobility, needing assistance with daily activities such as feeding, medications, and hygiene, moderate dementia, cognitive impairment, being immobile or wheelchair-bound, and requiring wound care and/or constant supervision. All applications are reviewed by Florida Department of Health medical staff for program eligibility. Ineligible applicants will be referred to other sheltering options like general population emergency shelters, specialized medical facilities, or nursing homes.	"[""Special Needs Registry\npens a New Window.\nIn order to meet the special needs of those who need assistance during evacuations and sheltering due to physical and mental disabilities, Florida statutes mandate that each local emergency management agency in the state maintain a registry of disabled persons located within the jurisdiction of the local agency. The registration must identify those persons in need of assistance and plan for resource allocation to meet those identified needs.\nThrough Emergency Management's Everbridge system, registrants will be notified by telephone, text, email, or smartphone of a pending evacuation and will be given specific instructions to follow for the Special Needs Shelter when an evacuation is called. Emergency Management will make every attempt to reach out to you via the methods provided when you register.\nEligible conditions for the Special Needs Shelter include, but are not limited to:\n- Being dependent on supplemental oxygen\n- Limited mobility\n- Needing assistance with daily activities such as feeding, medications, and hygiene\n- Moderate dementia\n- Cognitive impairment\n- Immobile or wheelchair-bound\n- In need of wound care and/or in need of constant supervision\nAll applications will be reviewed by medical staff from the Florida Department of Health and assessed for program eligibility. Ineligible applicants will be referred to applicable sheltering options, which may include a general population emergency shelter, specialized medical facility, or nursing home.\nHave Other Plans In Place\nThe Special Needs Registry is not a replacement for having an evacuation plan of your own. You should try and seek help or shelter from family, friends, or neighbors in a hurricane or other disaster. Public shelters should be a last resort for those who have no other choice. It is advised that you work on a plan that meets your specific medical needs with your doctor or a healthcare professional.\nIf you have registered and requested transportation assistance, you will be notified through your specified method of communication in advance of evacuation. You will need to decide whether you will be evacuating and let us know at that time. We will not be able to call you back. If you decide to evacuate, we will direct you to the nearest staging area to your home or make arrangements to pick you up. You must have your belongings and supplies packed and ready to go.\nHow Do I Sign Up for the Special Needs Registry?\nMonroe County Emergency Management uses Everbridge for the Special Needs Registry for Shelter and Evacuation Assistance. Everbridge is a subscription-based alert system that allows Emergency Management to contact a large number of people through automated phone calls, emails, text messages, and other wireless notifications.\nIn the event of an emergency, the notification system will make every attempt to contact you through whichever method you provide during the registration process. You will be given important information and directions to follow if you choose to utilize the Special Needs Registry for Shelter and Evacuation Assistance.\nIn order to stay active, you must reapply through the online portal annually before April 30 of each year. Don’t worry, Emergency Management will send a reminder through the program. Registrants can log into his or her account at any time to make changes. Paper applications are no longer being accepted. This link can be used to register or update a registry.\nIf you are unable to fill out the application on your own or if you do not have access to the internet, please seek assistance through your home healthcare provider, primary care physician, case manager, or family and friends.\nClient Privacy Information\nYour privacy and the protection of your personal information are our top priority. To read or print a copy of Emergency Management's Privacy Practices see the following link.\n- Privacy Practiceson Printable\nOpens a New Window. OSpecial Needs Registry Sheltering Information\nFor a Category 1 storm and below, the Special Needs Shelter will be set up in Monroe County at the Gato Building in Key West and/or the Murray Nelson Government Center in Key Largo. For larger category storms with a mandatory evacuation, a Special Needs shelter will be opened on the mainland at Florida International University in Miami. Opens a New Window.\nWhat to Bring to a Shelter?\nPlease remember that space and supplies are limited at shelters. Very few comforts will be available. It may take several hours or days before shelters are fully supplied. Please remember to bring all prescription medications and important papers. Take a bath and eat before you leave home.\nFood: If you have the need for special foods (Ensure, etc.) you must bring it with you. High energy foods such as peanut butter, jelly, crackers, granola bars, trail mix, and other snack foods may be a good idea if you get hungry in between meals. If necessary, bring special dietary foods--such as diabetic, low salt, liquid diet, baby food, and formula. Don’t forget a manual can opener. Bring enough for 7 days.\nClothing and Bedding: At least one complete change of clothing including footwear. Plan enough for 7 days. A sleeping bag, blanket, and pillow. Rain gear and sturdy shoes.\nPersonal Items: Washcloth, small towel, soap, toothbrush, toothpaste, sanitary napkins, paper towels, toilet paper, towelettes, etc. Please bring diapers if necessary. Bring a 7 days’ supply.\nMedications, First-Aid supplies: Medications that are clearly marked with your name, dosage, type of medication, and prescribing physician. You must be able to take all medications by yourself and address any needed wound dressing changes. Bring a 30-day supply of all medications and dressings.\nImportant Papers: Identification and valuable papers (insurance documents, etc.), name and address of doctors, name, and address of nearest relatives, and contact information for someone not living in the affected area.\nPets: Pet-friendly sheltering is available for Special Needs Clients. You must provide a 14-day supply of food, 30 days of pet medications, and a crate or cage. Monroe County will provide separate transportation for all pre-registered pets.\nMiscellaneous: Cellphone and charger, games, cards, toys, battery-powered radios, flashlights (no candles or lanterns), batteries, or other reasonable items you may need are welcome.\nNot Allowed: Alcoholic beverages, or weapons. (Firearms are prohibited in shelters) ** Smoking is not permitted on any of the premises where we will be sheltering** including the outdoors of the facility. Each location is now designated as a “non-smoking” area. These areas include any area of the Florida International University and any Monroe County-owned property or School Board property.\nPets at Shelters\nIf you have pets in need of sheltering, register them at the same time as yourself. You must read and acknowledge the Emergency Shelter Agreement for Pets prior to registering your pet. Your pet will be transported to a Pet-Friendly Shelter in the vicinity of the Special Needs Shelter. Only ADA recognized Service Animals trained to perform a disability-related task are permitted in the Special Needs Shelter. Emotional Support and Comfort Animals are considered pets and will be transferred to the designated pet shelter if pets are not allowed at the shelter. Consider separate arrangements for your pets with friends and family and only rely on the Pet Shelter as a last resort.""]"	['<urn:uuid:e5cb232c-bc6a-426f-865b-4ba0198e9779>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	12	75	1190
91	How do privacy laws protect personal data, and what causes bias in automated decisions?	Privacy laws like Quebec's Bill 64 protect personal data by requiring organizations to implement strict governance policies, conduct risk assessments, and notify affected individuals of data breaches. Organizations must obtain explicit consent for sensitive data collection and ensure data portability. Violations can result in fines up to $25 million or 4% of worldwide turnover. Regarding bias in automated decisions, it can stem from multiple sources: data selection (underrepresentation of certain groups), target variable issues, developer biases, and model limitations. For example, historical data may not reflect current processes, and developers' personal backgrounds can influence system design. Even omitting sensitive information as input variables isn't sufficient to prevent bias, as other variables can be stochastically dependent on sensitive information.	['Privacy protection in Quebec: an overview of amendments to the law governing the private sector\nI. Modernizing privacy legislation\nOn June 12, the Quebec government introduced bill 164, An Act to modernize legislative provisions as regards the protection of personal information (“the Bill” or “Bill 64”), first announced nearly a year ago. Once adopted, the Bill will result in significant changes to various laws in order to modernize the regulatory framework for the protection of personal data in Quebec.\nThe modernization process will target private and public sector institutions as well as political parties, and will require compliance efforts by all these organizations. Indeed, the nature of the new requirements and the hefty penalties for violations mean that privacy protection can no longer be ignored with impunity.\nAmong other things, the Bill proposes granting new rights to individuals regarding data portability, the right to be forgotten and the right to de-indexation. In order to implement and uphold these rights, many companies will have to modify their business processes or adopt new ones.\nOur team can assist with this exercise by analyzing your existing processes, identifying gaps and formulating implementation recommendations to comply with the amended law.\nWe begin this series of publications about the Bill with an overview of the key amendments to the Act Respecting the Protection of Personal Information in the Private Sector (“PPIPS” or the “Act”), which applies to any business operating in the province.\nThe scope of the Act will remain essentially unchanged as far as businesses are concerned. However, the Bill states that:\n- the personal data concerned includes personal data collected by the company, even in instances where it is stored by third party.\n- information relating to job title function (name, title, business address) will no longer be subject to the PPIPA; this puts an end to the divided case law from the Commission d’accès à l’information (CAI) regarding the characterization of business contact information as personal information.\nThe Bill explicitly introduces the principle of accountability by the company collecting the data, which is one of the basic principles of privacy protection.\nMost significantly for businesses, the responsibility for the protection of personal information, or role of “Chief Privacy Officer”, will now rest with the highest ranking officer of the company. This person will now be responsible for the implementation of, and compliance with the provisions of the Act. Contact details for this person or the person to whom the role is delegated will have to be published on the company’s website or, in the absence of a website, made available through other means.\nBill 64 proposes that all companies be required to adopt governance policies and practices to ensure that personal data is protected. These policies and practices should provide a framework for the following aspects, among others:\n- data retention and destruction;\n- staff member roles and responsibilities;\n- a complaints process.\nWithin the governance component, the Bill also establishes a requirement for the organization to conduct a risk assessment for any project involving the collection or use of personal information. As with any obligation of this type, organizations will need to generate and maintain adequate documentation.\nThe future Section 3.3 also introduces the requirement to ensure that the collected data is portable and can be made available in a valid format.\nThe Bill also clarifies the concept of consent for the collection and use of personal information.\n- Consent must be manifest, free, informed, solicited for specific purposes and separately from any other information provided;\n- For sensitive personal information, i.e. information that entails a high level of reasonable expectation of privacy, the consent must be express.\n- Under the proposed provisions, the secondary use of personal information will be permitted without the prior consent of the person concerned, as long as:\n- the use is for purposes consistent with those for which it was collected (and not for commercial or philanthropic prospection, which are specifically excluded);\n- the use is for the benefit of the person concerned;\n- the use is necessary for study or research or for the production of statistics, and the information is de-identified (i.e. no longer directly identifies the person concerned).\nThe Bill proposes to broaden the information to be disclosed at the time of collection as well as the company’s obligations in this regard.\n- In addition to specifying the purpose of the collection, the company will need to specify:\n- the means used;\n- the person’s right to withdraw consent;\n- where applicable,\n- the name of the third party for whom the data is being collected;\n- the possibility that the information may be transmitted outside Quebec.\n- Bill 64 proposes to end the exception provided for in the PPIPS for the collection, use and disclosure of personal information for commercial or philanthropic purposes. In fact, any organization using personal data for such purposes will have to disclose it, and the person concerned will have the option to withdraw his or her consent to such use.\nThe Bill requires that organizations disclose, in advance, their use of technology that can identify, locate or profile users, and then provide users with the means to disable the identification, location or profiling features.\nThe following definition for the term “profiling” has been suggested: “the collection and use of personal information to assess certain characteristics of an individual, in particular for the purpose of analyzing that individual’s work performance, economic situation, health, personal preferences, interests or behaviour.”\nThe Bill proposes clarifications to the rules applicable to the disclosure of personal information collected to service providers.\nSuch disclosure will be subject to certain conditions, including that any such service provider have measures in place to maintain the confidentiality of the information.\nThe Bill also proposes to fill a significant gap by expressly introducing an exception to allow the release of personal information in the course of a commercial transaction, as permitted under other Canadian laws.\nThe Bill reinforces the rules governing the cross-border transfer of personal information by businesses. These rules are currently set out in PPIPS sections 17 and 20.\n- Before communication of any information outside Quebec, a business will have to conduct an assessment of the following privacy-related factors:\n- the sensitivity of the information;\n- the purpose for which it will be used;\n- the applicable security safeguards;\n- the legal regime in the jurisdiction, and particularly its degree of equivalence with respect to the principles governing privacy protection in Quebec.\nIf the assessment shows that the level of data protection would be equivalent to that in Quebec, and subject to the conclusion of a written agreement, the data may be disclosed.\nFor the greater benefit of businesses, the government is following the European Union’s approach and has announced that the minister will publish a list of jurisdictions with legal frameworks deemed to be equivalent.\nThe Bill clarifies that organizations may either destroy personal information or anonymize it; the latter option allows organizations to retain information when the purposes for which it was collected or used are achieved (subject to statutory retention periods).\nThe Bill finally introduces mandatory breach notification in the event of a breach of security safeguards involving personal information. Since 2011, the Commission d’accès à l’information (CAI) has been recommending such a notification obligation, and this is in keeping with the obligations imposed by the Canadian Parliament and the European Union, both of which adopted mandatory notification programs in recent years.\nHere are the key points to bear in mind:\n- The term “confidentiality incident” includes:\n- unauthorized access, use or release of personal information;\n- loss of personal information or any other breach in the protection of that information.\n- When there is reason to believe that a breach involving personal information has occurred, the organization must take reasonable steps to reduce the risk of injury and to prevent new incidents of the same nature.\n- In the event of an incident involving a risk of serious harm, the organization must notify the CAI, as well as any person whose personal information is concerned by the incident.\n- To guide businesses in determining the risk threshold, the Bill lists the factors to be considered in assessing the risk of harm:\n- the sensitivity of the information;\n- the anticipated consequences of its use;\n- the likelihood that it will be used for injurious purposes.\n- The content and method of the notices will be determined by regulation.\n- All companies must maintain a record of every breach of security safeguards, which must be sent to the CAI upon request. This register can prove very useful in carrying out due diligence on a supplier or an acquisition target.\n- The CAI will have the power to order the performance of any measure aimed at protecting the rights granted to affected persons under this law, for the time and under the conditions the Commission determines.\nOrganizations that collect personal information through technological products or services will now have to ensure that the parameters of the product or service provide the highest level of confidentiality by default. This will entail changes to the digital application deployment process.\nThe modernization effort also takes into account the deployment of applications supported by artificial intelligence. The Bill proposes that, when a decision is based exclusively on automated processing, the organization must inform individuals affected by that decision of the parameters used in the decision process and of the procedure for requesting that a staff member review that decision.\nSection 28.1 proposes recognizing an individual’s right under certain circumstances to require an organization to cease distributing personal information about him or her and to de-index any hyperlink that provides access to that information, particularly if such a distribution contravenes the law or a court order.\nA person could also make such a request when the following conditions are met:\n- the dissemination of this information causes the person serious injury in relation to the person’s right to respect of his or her reputation or privacy;\n- the injury is clearly greater than the public interest in knowing the information or the right to free expression (the balance of convenience criterion);\n- the remedy requested does not exceed what is necessary to prevent the perpetuation of the injury.\nIn assessing the balance of convenience criterion, the following, in particular, must be taken into account:\n- the person’s notoriety;\n- if the person is a minor;\n- the accuracy of the information being disseminated;\n- the sensitivity of the information;\n- the context in which the information is disseminated, and the time elapsed between the beginning of the dissemination and the request to halt it.\nThe Bill also provides an exception for the release of personal information to a spouse or close relative on the death of a person. This exception to non-disclosure applies when the disclosure is likely to assist the applicant in the grieving process. Furthermore, the deceased person must not have refused such right of access before his or her death.\nFinally, the Bill’s adoption will significantly strengthen corporate accountability by levying hefty fines for violating the law and imposing administrative penalties for failing to meet the obligations specified in the PPIPS.\n- First, concerning the administrative penalties, Bill 64 stipulates that these may be imposed, inter alia, when someone collects or uses personal information in contravention of the provisions of PPIPS or fails in his or her duty to report a confidentiality incident.\n- Bill 64 gives the CAI a mandate to develop a general framework for the application of these administrative penalties, which should specify the following elements in particular:\n- the objectives associated with the implementation of such a regime;\n- the criteria that must guide the decision-maker in imposing a penalty, including:\n- the seriousness of the violation;\n- the sensitivity of the information;\n- the number of people affected;\n- the measures put in place to remedy the violation;\n- the level of cooperation demonstrated by the organization;\n- the compensation offered to the individuals affected.\n- The maximum amount of the administrative penalty is $50,000 (for individuals) and $10,000,000 (for businesses) or, if greater, 2% of worldwide turnover for the preceding year.\n- Bill 64 also modifies the penal penalties already prescribed in PPIPS and significantly increases their scope. For a corporation, a violation of the PPIPS will result in a fine ranging between $15,000 and $25,000,000 or, if greater, 4% of worldwide turnover for the preceding year. In the event of a subsequent offence, the fines are doubled.\n- The offences covered by the penal regime include any collection, possession, release or use of personal information that is contrary to the dictates of PPIPS or any failure to report a confidentiality incident.\nFinally, Bill 64 establishes a right for persons affected by an unlawful infringement of the rights conferred by the PPIPS to sue the non-compliant organization for damages. It provides for punitive damages of at least $1,000 to be awarded where the infringement is intentional or the result of gross negligence.\nBased on our preliminary analysis, the adoption of the rules proposed in Bill 64 would ensure consistency with the provisions of the General Data Protection Regulation (GDPR).\nThe Bill currently provides for a one-year transition period between its adoption and the coming into force of the new provisions, except for the right to portability, for which the Bill proposes a three-year deferral of implementation.\nGiven the number of proposed changes and new requirements, such periods are needed to allow companies to review their current practices, identify gaps and implement the necessary changes to ensure compliance.\nTo view the article with footnotes, click here.', 'AI is used to automate more and more decisions. With applications like credit scoring, application screening, fraud detection that impact many lives, avoiding any type of discrimination is key from both ethical and legal perspectives.\nIn its AI strategy, the German government has paid explicit attention to the issue of bias in the context of the use of AI processes. The General Data Protection Regulation (GDPR) also states that in automated decisions, discriminatory effects based on racial or ethnic origin, political opinion, religion or belief, trade union membership, genetic or health status, or sexual orientation (sensitive information) must be prevented.\nDecisions can be biased regardless of their source. However, decisions from algorithms are more traceable than human-made decisions. This allows developers to make biases more visible and ensure fairness. Bias can be defined as systematic and repeatable errors in any decision system. It can either lead to a degradation of decision quality or to unfair or discriminating results like favoring an arbitrary user group. While the avoidance of bias to improve model quality is well established, the ethical use of AI is an active research topic.\nBias can have several sources: the data selection, the target variable (label), the developers, and the model itself.\nAI systems do not have a natural understanding of the objectivity of the data being processed. If a bias is present in the data, it will be adopted by the model. Algorithms are written by humans who are naturally biased. Additionally, the method used may have a bias, for example, if it is inappropriate for application to the specific problem.\nFor example, in the allocation of public housing, bias could have these causes:\nData selection: There used to be fewer single fathers, so they are underrepresented in the data. Their requests are not handled correctly.\nLabels: The allocation process has changed in the past. Therefore, historical data does not represent the current allocation process. The model quality decreases.\nDeveloper(s): The developer does not have children and therefore does not sufficiently consider information on family size.\nAlgorithm: The relationship between target and input variables is too complex for the model used (underfitting). This can be circumvented by using more complex models.\nHandling Bias in Practice\nOptimizing fairness in models often contradicts optimizing model quality. Therefore, awareness of bias and a definition of fairness is an essential task in each project since a model can be fair by one definition and unfair by another. Only then can the decision process be analyzed for bias and fairness.\nBias in data can be detected and corrected by analyzing the data basis. This includes outlier analyses, changes in dependencies in the data over time, or simply plotting variables separated into suitable groups, e.g., the target variable distribution for all genders.\nTo build a fair model, it is not enough to omit sensitive information as input variables because other influence variables can be stochastically dependent on sensitive information.\nA reference dataset allows further analysis of fairness. The ideal reference data\nThere are specialized libraries (e.g., AIF360, fairlearn) designed to compute fairness measures and thereby detect biases in models. These assume that the dataset used contains sensitive information. They also provide methods to reduce bias.\nError analysis of the model outputs allows one to find data samples where the model has difficulty. This often helps to find underrepresented groups and can be done by looking at samples where the model has chosen the wrong class with a high degree of confidence.\nDuring the model life cycle, it is important to monitor the model and allow users to retrace the reasons for model decisions. This works approximately for more complex models through Explainable-AI methods such as SHAP-values and helps to uncover convoluted biases (real example: discrimination against candidates from minority communities by using the length of commute in hiring decisions).\nIf the training data is not sufficiently close to data seen in production, additional data can be collected and incorporated into the model. If this is impossible, upsampling, data augmentation, or downsampling can be used to create a better representation.\nNot every bias is bad: By deliberately introducing a counter-bias, a known bias can be counteracted. For example, in automatic applicant screening, an underrepresented minority could receive bonus points in the scoring if a minimum representation quota is to be met in the future.\nAnalyzing for bias, especially discrimination, is not only ethically and legally required when automated decisions affect people. In practice, doing so often generates many additional insights that improve performance, transparency, and monitoring quality and thus the overall decision process, even if performance and fairness are contradicting objectives in theory. Lastly, in case things get serious, it also helps in court.\nLudwig Brummer is a Senior Data Scientist and team lead at Alexander Thamm GmbH since 2015. He implemented a variety of use cases in energy, insurance, mechanical engineering, and banking such as credit scoring, automated claim settlement, or maintenance automation. For him, decision automation is an opportunity to make decisions not only faster and cheaper but also fairer for all users.\nPlease note: The opinions expressed in Industry Insights published by dotmagazine are the author’s own and do not reflect the view of the publisher, eco – Association of the Internet Industry.']	['<urn:uuid:49ad3ab8-4576-49c4-a731-39d1c07baa27>', '<urn:uuid:baabd164-1e10-46ff-8c87-2453a5d973c2>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T12:39:32.578853	14	118	3149
92	Does AI aim to replace professionals in finance and medicine?	No, in both fields AI is meant to augment rather than replace professionals. In wealth management, relationship managers will remain central as client advocates while AI handles routine tasks. Similarly in medicine, AI serves as a tool to handle large amounts of medical data and improve diagnostics, while still requiring human medical expertise for implementation and oversight.	"['The publisher of this news service, alongside EY and Finantix, is producing a major report on what AI means for wealth manager, going deep into practical details to provide real, actionable ideas for the sector. A number of segments from the report are being run on this service.\nThe publisher of this news service is, in conjunction with technology firm Finantix and professional services, audit and accountancy group EY, issuing a research report exploring how artificial intelligence is likely to affect wealth management continue to do so. This publication will release chapters from the report in coming days and weeks. We hope these articles encourage readers to obtain the whole copy to view its mass of data and charts alongside text. The material has been produced by the ClearView Financial Media team, led by Wendy Spires, head of research at ClearView. Wendy writes the first chapter. Before getting to her article, here are key terms and concepts around artificial intelligence that readers will find useful. (To download the full report and background, click here.)\nNatural Language Processing\nNatural Language Processing (NLP) is a component of AI whereby computer programmes are able to understand human speech as it is spoken.\nComputers have traditionally required humans to communicate with them in a programming language that is precise, unambiguous and highly structured. The first task of NLP is to understand the natural language received by the computer and then convert this into a programming language, breaking it down into tiny units. This process identifies words in their grammatical forms as nouns, verbs, adjectives, past tense, etc. In the last stage, the computer programming language is converted into an audible or textual format for the user.\nThe ultimate goal of NLP is to do away with computer programming languages altogether. Instead of specialised languages such as Java, Ruby or C, there would only be ""human"" language, with the software built to analyse, understand and generate human languages naturally.\nApplications of NLP\nMachine Translation: The challenge of making the world\'s information accessible to everyone has simply outgrown the capacity for human translation. The challenge with machine translation technologies is not in translating words, but in preserving the meaning of sentences. Hermeneutics is a complex technological (and arguably philosophical) issue that is at the heart of NLP.\nFighting Spam: The false-positive and false-negative issues of spam filters are at the heart of NLP technology, again boiling down to the challenge of extracting meaning from strings of text.\nQuestion Answering: A big focus of efforts in NLP has been in enabling tools like chatbots to recognise natural language questions, extract their meaning and provide the right answer automatically.\nAutomated reasoning is an area of computer science and mathematical logic, producing computer programmes that reason automatically. The first step in performing automated reasoning is to express a statement of theorems in a formal language. The second step is to use automated algorithmic manipulations on those formal expressions.\nSoftware development is one of the key reasons for using Automated Reasoning. We can’t afford errors in software development as this will affect far-reaching applications of computer programming such as air traffic control, banking and utilities. Errors are annoying, expensive and can even result in the loss of lives.\nApplications of Automated Reasoning:\nResearch: Automated reasoning checks the correctness of proofs in mathematics, supplementing or even replacing the existing ‘social process’ of peer review etc. with a more objective criterion.\nTesting Computers: Can be used to extend rigorous proof from pure mathematics to the verification of computer systems (programs, hardware systems, protocols etc.).\nMachine Learning is a type of AI that predicts outcomes without explicit programming. Supervised algorithms require humans to provide both input and desired output, in addition to furnishing feedback about the accuracy of predictions during training. Once training is complete, the algorithm will apply what was learned to new data. Unsupervised algorithms do not need to be trained with desired outcome data. Instead, they use an iterative approach called deep learning to review data and arrive at conclusions.\nFor example, a computer might be provided with a teaching set of photographs, some of which say, “this is a cat” and some of which say, “this is not a cat”. Then, shown a series of new photos, it would begin to identify which photos were of cats independently, getting “smarter” at completing its task over time.\nApplications of Machine Learning:\nData Security: Malware is a huge problem. But each piece of new malware tends to have almost the same code as previous versions — only between 2 and 10 per cent of the files change from iteration to iteration. Machine Learning algorithms can predict which files are malware with great accuracy.\nFinancial Trading: Many prestigious trading firms use proprietary systems to predict and execute trades at high speeds and high volume. Many of these rely on probabilities, but even a trade with a relatively low probability, at a high enough volume or speed, can turn huge profits for the firms.\nChapter 1: The Real Role of AI and Why Wealth Managers Have Less to Fear Than They Might Think\nStatistics about the financial services industry’s accelerating reliance on technology may well strike fear into the hearts of its professionals. For instance, it has been estimated that 30 per cent of jobs are under threat and that 2-6 million may be lost over the next decade due to disruptive technologies such as AI, robotics and blockchain. Concerns over potential job losses have certainly filtered down to the WealthBriefing community.\nBut, as ever, the headlines belie a far more complex picture, particularly as regards the wealth management sector.\nMuch is made of the fact that digital challenger banks have been able to launch with 90 per cent less headcount than traditional incumbents. Yet the fact remains that wealth management represents a very special niche, it being a highly complex, multi-disciplinary area of financial services and one that is predicated on high-touch personal service too.\nAs such, rather than fearing their replacement by AI, our expert panellists argue that wealth management professionals should instead look forward to their working lives being made very much easier, and far more productive, through this technology.\nReservations even over robo-advice\nComplexities exist even in areas where the “rise of the machines” seems unassailable. Robo-advice undoubtedly represents a seismic shift for the investment management industry, one that has squeezed traditional providers’ profit margins and fuelled the growth of passive investing to the point where many question the ongoing existence of active management.\nHowever, while 71 per cent of wealth managers may believe clients are ready to take advice from robos, it seems doubtful that they want to rely solely on them – despite what advocates may say about likelihood of a passive, robo-managed portfolio performing better – and at far less cost – than one actively managed by a person.\nIt is telling that self-directed investing slid from 45 per cent to 38 per cent between 2010 and 2016, a period where robos stormed ahead in both number and sophistication. Here, it has been suggested that the psychological difficulties of “entrusting” large amounts of money to a technological interface means that many are using these tools largely to assess their investments before consulting a professional to actually manage them. This seems very much borne out by research showing that investors prize an understanding of their needs and goals, a holistic overview and a willingness to explain analysis above all else when seeking an advisor.\nRelationship managers to remain central\nThe old adage that “people buy from people” seems to remain true, and not just due to trust issues. Rather, it is that the human element of wealth management is where much of its value resides.\n“The non-reduceable part of the human work in private banking is really representing the client’s perspective, unpicking their philosophy and attitudes to life and risk,” said Alessandro Tonchia, Co-Founder of Finantix. “So, while more and more of their ‘menial’ work will be eaten up by increasingly intelligent systems, the relationship manager will still remain central as the advocate of the client.”\nAs our experts observed, this advocacy is multi-faceted, with AI holding out the potential for significant improvements in several ways.\nFor Greg Davies, Head of Behavioural Science at Oxford Risk, one of the most compelling use cases for AI is in helping advisors synthesise the myriad information institutions hold on clients with new details emerging in meetings to ensure that investment suitability is properly addressed.\n“Mentally processing all the existing and new client information to arrive at the right recommendation for them can be incredibly complicated with many moving parts,” he said. “AI can dynamically process that information in real-time and do all the data juggling to present the advisor with the most accurate, holistic portrait the firm has of the client at that moment.”\nNot only will AI save advisors the risk - and pain - of performing all these calculations “live”, it can also flag up all the important issues to be covered in the conversation. However, Davies notes that the technology would still need to draw on a human-designed conception of what the underlying suitability framework is, just as really accurate risk-profiling requires the “human touch” to interrogate and contextualise a client’s professed attitudes and desires. As with so much in sophisticated wealth management, risk-profiling is both an art and a science.\nIron Man, not the Terminator\n“The value of AI is less in the computer ‘giving the answer’ to the client and more in it acting as an advisory support tool helping the relationship manager make decisions,” Davies said. “When it comes to advice, we should be thinking about AI in terms of Iron Man, rather than the Terminator.”\nDavies’ reference carries very useful notions of AI acting as a kind of “cognitive prothesis” for advisors. The technology will allow them to bring far more information about the client to bear when making investment and financial planning recommendations; as discussed later in this report, it will also be able to ensure that these are compliant via machines learning (and automatically keeping up to date with) all the regulatory constraints private client practitioners have to work within.\nThe delivery of professional services of all kinds looks set to be revolutionised by new technologies. But while the medical applications of AI have garnered much attention, the greater objectivity of this domain leads Tonchia to believe that it is rather the legal professional which yields the most useful parallels to wealth management.\n“Legal and wealth are similarly about interpreting a client’s often quite philosophical objectives, devising strategies to achieve them, then articulating those into a tactical approach - and both are incredibly rich in terms of the knowledge you need to do that optimally,” he said. “An intelligent system can help a lot in both sectors by validating courses of action, gathering all the information and documents that are required and reducing the risk of making mistakes.”\nAs the “fourth industrial revolution” gathers pace, Phil Tattersall, Director in EY’s UK Wealth & Asset Management Data and Analytics advisory practice, sees AI having a truly transformative effect on wealth management as well as the wider world. Yet this should be thought about in terms of a multiplicity of gains, and not simply job losses.\nAn inflection point approaches\n“If we plot human population on a graph over time, the inflection point to geometric growth was enabled by the industrial revolution, which massively increased productivity as we augmented human and animal power with steam power,” he said. “The promise of AI, and in particular Machine Learning (ML) techniques, is to enable a similar inflection point in productivity, this time via a massive augmentation in cognitive power.”\nWealth managers are already investing heavily in AI technology. EY’s Tattersall sees AI/ML techniques rapidly becoming embedded in every single aspect of the wealth management value chain. However, we can expect the application of AI to start in the highest-value areas, namely, enhancing the investment process and client experience and reducing business risk, all while delivering the dramatic productivity gains the industry needs.\nAccording to our panel, the notion of moving up the value chain is in fact one that all wealth management professionals should keep in mind when considering the sector’s evolution and their future role within it – particularly when we consider that, hearteningly, 8 in 10 firms replacing roles with technology are retraining or redeploying those being displaced.\nIf computers can perform a function optimally and without intervention, then in time they undoubtedly will. But due to the complex, multi-disciplinary and interpersonal nature of managing wealth there seems to be relatively little - except in the most workaday tasks - where humans seem likely to become entirely redundant.\nInstead, we can anticipate varying degrees of hybridisation between man and machine, as the rest of this report describes. This means that skilled relationship managers have little to fear, and much to gain, from greater AI adoption across the industry and that Chief Technology Officers should actually find the technology an easier “sell” than many might have hitherto feared – as long as the benefits for clients, firms and advisors are made clear.\n“There will be a need for humans to sell and persuade for many decades to come,” concluded David Teten, Managing Partner of HOF Capital. “People need to remember that the great majority of the value of technology in general - and not just with AI - is in augmenting professionals, not replacing them.”', 'Artificial intelligence (AI) will bring in a new wave of changes in the medical field, likely altering how we practice medicine. In a timely contribution, Chen et al.  outline the current landscape of AI and provide us with a glimpse of the future, in which sophisticated computers and algorithms play a front-and-centre role in the daily hospital routine.\nWidespread adoption of electronic medical records (EMRs), an ever-increasing amount of radiographic imaging, and the ubiquity of genome sequencing, among other factors, have created an impossibly large body of medical data. This poses obvious challenges for clinicians to remain abreast of new discoveries, but also presents new opportunities for scientific discovery. AI is the inevitable and much-needed tool with which to harness the ‘big data’ of medicine.\nCurrently, the most immediate and important application of AI appears to be in the field of diagnostics and radiology. In prostate cancer, for example, machine learning algorithms (MLAs) are not only able to automate radiographic detection of prostate cancer but have also been shown to improve diagnostic accuracy compared to standard clinical scoring schemes. MLAs can use clinicopathological data to predict clinically significant prostate cancer and disease recurrence\nwith a high degree of accuracy. The same has been shown for other urological malignancies, including urothelial cancer and RCC. Implementation of MLAs will lead to improved accuracy and reproducibility, reducing human bias and variability. We also predict that as natural language processing becomes more sophisticated, the troves of nonstructured data that exist in EMRs will be harnessed to deliver improved and more personalized patient care. Patient data and clinical outcomes can be analysed in short time, drawing from a deep body of knowledge, and leading to rapid insights that can guide medical decision-making.\nCurrent AI technology, however, remains experimental and we are still far from the widespread implementation of AI within clinical medicine. A valid criticism of today’s AI is that it functions in the setting of a ‘black box’; the rules that govern the clinical decision-making of an algorithm are often poorly understood or unknowable. We cannot become operators of machines for which we know not how they work, to do so would be to practice medicine blindly.\nAnother barrier to incorporating AI into common practice is the level of noise in healthcare data. MLAs will use whatever data that are fed to the algorithm, thus running the risk of producing predicative models that include nonsensical variables gleaned from the noise. This concept is similar to multiple hypothesis-testing, where if you feed enough random information into a model, a pattern might emerge. Furthermore, none of the studies described by Chen et al. have been externally validated on large, representative datasets of diverse patients. MLAs trained on a narrow patient population run the risk of creating predictions that\nare not generalizable. This problem has already been popularized within genome analysis, where one study found that 81% of all genome-wide studies were taken from individuals of European ancestry . It is easy to imagine situations where risk score calculators or biomarkers are validated using non-representative datasets, leading to less accurate and even inappropriate treatment decisions for underrepresented patient populations. At best, MLAs that are not validated using stringent principles can lead to erroneous disease models. At worst, they can bias the delivery of healthcare to patients, leading to worse patient outcomes and exacerbation of healthcare disparities.\nChen et al. write of the possibility of AI in urology today. What about the future? Imagine a world in which computers with a robotic interface see patients in clinics, design and carry out complex medical treatment plans, and perform surgery without the aid of a human hand. This future may not be far off . Or, even stranger, consider a world in which generalizable AI exists. Estimates of the dawn of this technology range, however the most optimistic projections put the timeline on the order of 20–30 years. Not far behind could be the ‘singularity’, a moment when technological advancement occurs at such an exponential rate that improbable scientific discoveries happen almost instantaneously, setting off a feed-forward cycle leading to an inconceivable superintelligence.\nThe future is, of course, hard to predict. Nevertheless, AI and the ensuing technology will certainly transform the practice of urology, albeit not without significant challenges and growing pains along the way. The urologist of the future may look very different indeed.\nby Stephen W. Reese, Emily Ji, Aliya Sahraoui and Quoc-Dien Trinh\n- Chen J, Remulla D, Nguyen JH et al. Current status of artificial intelligence applications in Urology and its potential to influence clinical practice. BJU Int 2019; 124: 567–77\n- Popejoy AB, Fullerton SM. Genomics is failing on diversity. Nature 2016; 538: 161–4\n- Grace K, Salvatier J, Dafoe A, Zhang B, Evans O. When Will AI Exceed Human Performance? Evidence from AI Experts, 2017']"	['<urn:uuid:2fde432b-64a9-4655-80cb-5aceed008a3f>', '<urn:uuid:33935736-a14e-425c-8e55-155730c48eee>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T12:39:32.578853	10	57	3052
93	As someone interested in museum exhibit design, I'd like to know how modern science exhibits handle graphics and lighting compared to dome theaters?	Science Storms uses illuminated consoles with LED panels for energy efficiency, featuring Corian surfaces with CNC-etched type that creates soft glowing titles, along with laminated glass transparencies for interpretive graphics and dramatic column-mounted environmental images. Fulldome theaters, on the other hand, rely on specialized projection technologies like DLP, LCD, or laser projectors to display content, with particular attention paid to achieving deep black levels for contrast - especially important for planetarium applications. Both approaches aim to create immersive experiences but use different technical methods.	"['The Museum of Science and Industry Chicago challenged Evidence Design to envision an exhibition of physics and chemistry that reflects its core mission: inspire middle-school children to achieve their full potential in science and engineering. The solution was to leverage the drama and fascination of the forces of nature as platforms for exploring basic scientific principles.\nScience Storms is a ground-breaking 26,000-sq.-ft. exhibition in which visitors interact with dynamic, large-scale experiments that explore nature’s most powerful phenomena— tornadoes, lightning, fire, tsunamis, sunlight, avalanches, and atoms in motion—and then, through hands-on experimentation, investigate the science behind these forces.\nInspired by nature and the innate human desire to reveal its secrets, Science Storms was conceived as a wondrous laboratory. The 200-ft.-long by 100-ft.-wide by 65-ft. gallery is an ethereal expanse whose surfaces are washed by blue light that creates an indefinite sense of space. The exhibition is anchored by seven viscerally beautiful, large-scale interactive demonstrations of natural phenomena, surrounded by a constellation of smaller experiments. Interactive and linear media throughout the gallery complement the mechanical elements.\nStrong environmental graphics tie the exhibit elements together into a cohesive, composed experience, while exhibition casework has several roles: carry mechanical exhibits; house touchscreens and exhibit controls; and provide instructions and interpretive graphics. The design approach for the consoles directly engages visitors with the demonstrations without competing with them. Each graphic element supports the overall theme of the exhibition while providing crisp, clear instructions, explanations, and contextualized information.\nSignage materials reference the beauty of nature, but can withstand intense public use. The robust consoles and signage are constructed of glass, stainless steel, and Glacier White Corian. Routed with 1/32-in.-diameter drill bits, Corian type is CNC-etched second-surface to create soft, glowing titles evocative of natural phenomena; sharp instructional type is etched first-surface and sanded to a seamless plane. Lambda transparencies laminated behind glass hold interpretive graphics with imagery expressing the phenomena they explore. All consoles and signage are illuminated with low-voltage LuminousFilm LED panels for energy-efficiency and long life.\nThe gallery’s vast scale and the exhibits’ sheer size demand larger-than-life environmental graphics. To orient guests, massive columns are leveraged to carry architecturally-scaled titles along with dramatic images of natural phenomena related to each thematic area. Wrapped in Vutek ink-jet prints on tough Ultra-Flex material, the columns also carry wayfinding maps and inspirational quotes from scientists and artists.\nSeven large-scale linear videos feature striking visuals of natural phenomena and stories of scientists who investigate them. Adopting typography from the signage, they function as dynamic signage and carry the core inspirational messages of the exhibit.\nScience Storms reinvigorates the museum’s dedication to fundamental science through a synthesis of compelling interactive exhibits, environmental media, and strong contextual storytelling bound together in a highly choreographed design.\n“A totally integrated dynamic environment, encouraging interaction with the audience that truly inspires. The dynamic content, static environmental graphics, and exhibits all work harmoniously together to form a magnificent space.”\n“This exhibit is outstanding. The displays are clear and easily understandable while maintaining an intriguing sense of excitement that has to be explored. The displays themselves are unique and fun teaching environments. Displays and graphics attract all age groups. All of the jurors want to see and experience this space!”\nJack Pascarosa (project director, 3D design); Shari Berman (director, 2D design and media); Len Soccolich, Carlos Fierro (exhibit designers); Rondi Davies (content specialist); Ari Nakamura (senior graphic designer); Josh Whitehead (graphic production); Laura Sheedy (designer)\nThe Wheel Thing (engineering); Focus Lighting (lighting design); Cortina Productions (media production); Ned Kahn Studios (tornado, avalanche exhibits); Paul Friedlander (string wave exhibit); Daniel Cox, Oregon State University (tsunami exhibit); Underwriters Laboratories (fire exhibit); Carpenter Norris Consulting (heliostat/rainbow exhibit); Peter Sedgley (light behaviors), SH Acoustics (acoustic, audio design)\nNorcon Construction (general contractor), Lexington Design + Fabrication (exhibit fabrication, stainless steel exhibit consoles, signage), Chicago Scenic Studios (sunlight/rainbow exhibit), Advanced Entertainment Technology (fire and Tesla coil exhibits), Production Resource Group (tornado and avalanche exhibits), MAD Systems (AV integration), Photocenter Imaging (Lambda Fugichrome), Crush Creative (Vutek prints for column graphics)', 'This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)(Learn how and when to remove this template message)\nFulldome refers to immersive dome-based video projection environments. The dome, horizontal or tilted, is filled with real-time (interactive) or pre-rendered (linear) computer animations, live capture images, or composited environments.\nAlthough the current technology emerged in the early-to-mid 1990s, fulldome environments have evolved from numerous influences, including immersive art and storytelling, with technological roots in domed architecture, planetariums, multi-projector film environments, flight simulation, and virtual reality.\nInitial approaches to moving fulldome imagery used wide-angle lenses, both 35 and 70 mm film, but the expense and ungainly nature of the film medium prevented much progress; furthermore, film formats such as Omnimax did not cover the full two pi steradians of the dome surface, leaving a section of the dome blank (though, due to seating arrangements, that part of the dome was not seen by most viewers). Later approaches to fulldome utilized monochromatic vector graphics systems projected through a fisheye lens. Contemporary configurations employ raster video projectors, either singly or grouped together to cover the dome surface with full-color images and animations.\nFulldome video projection can use a variety of technologies in two typical formats: single- and multiple-projector systems. The individual projector(s) can be driven by a variety of video sources, typically feeding material rendered in either real-time or “pre-rendered” modes. The end result is a video image that covers an entire domed projection surface, yielding an immersive experience that fills a viewer’s field of view.\nSingle- versus multiple-projector systems\nSingle-projector fulldome video systems use a single (or mixed) video source displayed through a single fisheye lens, typically located at or near the center of a hemispherical projection surface. A single projector has the benefit of avoiding edge blends (see below) between multiple projectors. The main disadvantage of single fisheye systems is that they are limited to the resolution of one projector, and in the smallest dimension of the video image to cover a full dome. Another disadvantage of central projectors is the loss of the center of the dome for optimal viewing of the reconstructed perspective view provided by true hemispheric projection, a problem shared with traditional planetarium projectors. However, this disadvantage fades as audience size increases (everyone cannot be at the center of the dome anyway).\nSingle-projector mirror systems, pioneered by Mirrordome from Swinburne, but now offered by a plethora of manufacturers, are placed on the edge of the dome to increase seating, decrease costs, and to allow analog planetariums to become digital without giving up their star projector. It is also possible to build such a system at relatively low cost. The main disadvantage is noticeably lower projection quality compared to purpose built lenses, despite being able to project a higher proportion of the projector resolution.\nMultiple-projector fulldome video systems rely on two or more video projectors edge-blended to create a seamless image that covers a hemispherical projection surface; splitting the entire image up into segments allows for higher-resolution imagery and projector placement that does not intrude on the viewing area underneath the dome. A disadvantage of multiple projection is the need to frequently adjust the alignment of projectors and the uneven aging of separate projectors leading to brightness and color differences between segments. Even minor performance differences between projectors can be obvious when projecting a solid color across the entire scene. Edge blended areas where projectors overlap often have some smearing, double images, and can have very obvious additive black level areas if poorly designed or configured.\nCommon video projector technology\nA wide variety of video projection technologies has been employed in domes, including cathode ray tube (CRT), Digital Light Processing (DLP), liquid crystal display (LCD), liquid crystal on silicon (LCOS), and most recently, two varieties of laser projectors (see the laser video projector).\nFor multi-projector systems in particular, display devices must have a low black level (i.e., project little or no light when no signal is sent to them) to allow for reasonable edge-blending between the different projector footprints. Otherwise, overlapping video images will have an additive effect, causing a complex pattern of grey to appear even when no image is being projected. This becomes particularly important for users in the planetarium field, who have a vested interest in projecting a dark night sky. The desire for projectors to “go to black” has resulted in continued use of CRT technology, even as newer and less expensive technologies have emerged.\nLCD projectors have fundamental limits on their ability to project true black as well as light, which has tended to limit their use in planetariums. LCOS and modified LCOS projectors have improved on LCD contrast ratios while also eliminating the “screen door” effect of small gaps between LCD pixels. “Dark chip” DLP projectors improve on the standard DLP design and can offer relatively inexpensive solution with bright images, but the black level requires physical baffling of the projectors. As the technology matures and reduces in price, laser projection looks promising for dome projection as it offers bright images, large dynamic range and a very wide color space.\nDOME lenses and standard lens are similar in some ways. They both depend on the type of display device, LCD, DLP, LCOS, DILA and the size chip or panel that is part of this device. The unique feature of the DOME lens is the actual shape of the glass, the projected image spill out from the top and all around circumference of the lens. The biggest advantage is how this type of lens maintains focus over the full 180 x 180 field of view. A single standard flat field or curved field lens would have major focus and distortion issue. Several lens developers offer DOME lenses with each designed to a specific projector class and display device. These lenses can cover a variety of pixel sizes and display resolutions.\nTypes of content\nComputer Generated (CG) content is the main source of material for Fulldome. CG content can be live simulator output, such as from planetarium simulation software, or prerecorded fulldome video. Live-Action FullDome videos are becoming more available for dome use as digital video camera resolutions increase.\n||This section\'s use of external links may not follow Wikipedia\'s policies or guidelines. (July 2015) (Learn how and when to remove this template message)|\n|1983||First Evans & Sutherland Digistar I calligraphic scan (projection of light points and lines - also known as vector scan) planetarium projector at the Science Museum of Virginia in Richmond, Virginia, U.S.A.|\n|1992||First dome-based vector/calligraphic scan scientific visualization system at SIGGRAPH, installed by the North Carolina Supercomputing Center using a reprogrammed Digistar I for molecular visualization|\n|1994||Alternate Realities Corporation premieres their first VisionDome prototype at Glaxo Inc. in Research Triangle Park, North Carolina, U.S.A. Developed at the North Carolina Supercomputing Center, the VisionDome uses a raster scan projector (full color video) and fisheye lens to project interactive 3D graphics onto a 5-meter dome.|\n|1995||First Evans & Sutherland Digistar II calligraphic scan planetarium projector opens at the London Planetarium, UK|\n|British Telecom uses a vertical five-meter Alternate Realities Corporation VisionDome for its “Shared Spaces” media environment research programme, incorporating computer graphics, virtual landscapes, data graphics, video, composited live action. and spatialized sound.|\n|August: Full-day SIGGRAPH \'95 course entitled ""Graphics Design and Production for Hemispheric Projection"" introduces spherical perspective, hemispheric projection, and suggests a convergence of simulation systems employing edge-blended raster video projection, virtual reality systems such as the CAVE (Cave Automatic Virtual Environment), and planetarium theaters to create a new medium with an enhanced illusion of presence. Organized by Ed Lantz with presenters Mike Hutton, Steven Savage and Chris Ward.|\n|1996||July 13–19: First Goto Virtuarium demonstrated at the International Planetarium Society Conference in Osaka, Japan|\n|October 26–29: Evans & Sutherland StarRider demonstrated at Association of Science-Technology Centers conference in Pittsburgh, Pennsylvania, U.S.A.|\n|1997||April: First permanent installation of Spitz ElectricSky at Northern Lights Center in Yukon Territory, Canada using three Electrohome 9500 CRT projectors, line-quadrupled video playback, and real-time edge-blends for 200x60-degree field-of-view virtual desktop. Bowen Technovation produces the first three shows for this new system.|\n|May 7–10: Spitz ElectricSky publicly unveiled at MAPS conference in Chadds Ford, PA.Bowen Technovation introduces show capabilities and production methods for the new technology.|\n|1998||May 22 - September 30: The Oceania pavilion opens at the EXPO 98 in Lisbon, Portugal. Among numerous virtual reality exhibits, it includes The Artefact Room, a 7-metre dome VisionDome theater with interactive 3D animations of a fly-through of Atlantis that are controlled by 40 participants simultaneously.|\n|June 28 - July 2: Sky-Skan premieres SkyVision at the International Planetarium Society Conference in London, UK. First astronomical digital fulldome animation shown to audiences there, “Pillars of Creation” by Don Davis, and a solar system tour and International Space Station animation by Tom Casey of Home Run Pictures. This marks the first public demonstration of fulldome video, distinguished by playback of actual video—as opposed to previous efforts based on proprietary image generators using vector or raster graphics—covering a full hemisphere.|\n|December: Vertical dome installation by SGI and Trimension at University of Teesside, UK.\nDecember: Houston Museum of Natural Science opens SkyVision system as a permanent public theater, with seed funding from NASA in partnership with Rice University. First playback fulldome show: ""Cosmic Mysteries"".\n|1999||Adler Planetarium reopens in Chicago, Illinois, U.S.A., with an Evans & Sutherland StarRider system|\n|Evans & Sutherland premiers their first linear playback show ""We Take You There"" at SIGGRAPH \'99\nCarnegie Museum of Natural history opens the Earth Theater with a SkyVision system\n|2000||Hayden Planetarium reopens at the American Museum of Natural History in New York, New York, U.S.A., with a Silicon Graphics Onyx2 and Trimension video system|\n|2002||BMW Group EarthLounge Premiere of the ADLIP (All-Dome Laser Image Projection) System from Carl Zeiss with SkyVision Full Dome Video System and DigitalSky from Sky-Skan and fulldome film by LivinGlobe (ag4, Exponent3) in the world\'s largest fulldome projection dome (24m) at the UN World Summit on Sustainable Development, Johannesburg South Africa.|\n|2002||First full Digital planetarium in the world installed (RSA Cosmos SkyExplorer) inaugurated in Valladolid, Spain .|\n|2003||Clark Planetarium (formerly Hansen Planetarium) reopens in Salt Lake City, Utah, U.S.A., with an Evans & Sutherland Digistar 3|\n|Adler Planetarium upgrades their StarRider to the new Evans & Sutherland Digistar 3 system. The mini-dome also opens in their production department running both the Digistar 3 SP and Producer systems.|\n|The first digital planetarium systems designed for the portable market are introduced independently by Digitalis Education Solutions and Sky-Skan (in partnership with Rice University and the Houston Museum of Natural Science). HMNS/Rice version subsequently diverges to become Discovery Dome.|\n|July 27–28: First industry-wide showcase of fulldome programming at SIGGRAPH in the Reuben H. Fleet Science Center including full-day course on fulldome art and science entitled ""Computer Graphics for Large-Scale Immersive Theaters.""|\n|November 3: The Eugenides Planetarium of Athens, Greece, re-opens premiering its first 40-minute production ""Cosmic Odyssey"" with both a fulldome Sky Skan Skyvision-Digital Sky system and an Evans & Sutherland Digistar 3 system under a 24,5 meter Astrotec dome.|\n|2004||First DomeFest held at the LodeStar Astronomy Center in Albuquerque, New Mexico, U.S.A.|\n|First ASTC Fulldome Showcase held at Tech Museum of Innovation, San Jose, CA|\n|First Immersive Cinema movie R+J(Romeo and Juliet) by LivinGlobe|\n|First fulldome animation feature film `Kaluoka´hina, the Enchanted Reef´Kaluoka´hina, the Enchanted Reef by Softmachine Softmachine|\n|December: Beijing Planetarium New Building opens in Beijing, China, with a Silicon Graphics Onyx 300 and the first fulldome laser display (Zeiss ADLIP)|\n|2005||GOTO installs the first complete fulldome sphere at EXPO 2005 in Aichi, Japan|\n|2007||October: Obscura Digital and The Elumenati develop a temporary 90\' diameter geodesic fulldome experience for Google\'s Zeitgeist event on the Google campus|\n|UCSB AlloSphere opens with full surround audio and video for multiple users|\n|2008||January: Sky-Skan installs the world\'s first full dome 3D Stereoscopic Planetarium at Imiloa Astronomy Center of Hawaii in Hilo, Hawaii. Imiloa Planetarium first Stereoscopic show is ""Dawn of the Space Age"" produced by Mirage IIID.|\n|2008||July: Sky-Skan demonstrates Definiti 8K: a 60,000 lumen, 8k x 8k fulldome projection system at IPS 2008 at Adler Planetarium in Chicago rivaling image quality of large-format film (system subsequently opens at Beijing Planetarium)|\n|2008||July: Carl Zeiss demonstrates prototype ""Velvet"" projectors at IPS 2008 at Adler Planetarium in Chicago. Zeiss designed the Velvet Series from the ground up for the planetarium environment, and achieves an unparalleled black background to preserve the quality of the fulldome/planetarium experience.|\n|2009||March: University of Colorado Denver College of Arts and Media (CAM) set up a 25 Mac Octo-core Cinema 4D and After Effects Render Farm to specifically process full dome content alongside Denver Museum of Nature Science and IMERSA|\n|2010||June: Vortex Immersion Media installs a 50-foot immersive cinema digital dome theater on the studio lot at Los Angeles Center Studios in downtonwn Los Angeles as an arts and entertainment R&D studio managed and run by Ed Lantz, MEE. An AIR: Artist In Residence Program is started to promote the creation and presentation of experimental, music, mixed media, and 360 cinema projects. \nNovember: Institute of American Indian Arts opens the world\'s first fully articulating digital dome.\n|2011||January: University of Washington Planetarium opens first 6 channel HD full dome digital projection conversion based entirely on Microsoft Research WorldWide Telescope. Done on a hardware and construction budget of 40,000 USD the planetarium features the world\'s largest all-sky panorama at 1 terapixel, allowing zooming to 1 arc-second per pixel anywhere in the sky. UW graduate student Philip Rosenfield presented a paper at the Astronomical Society of the Pacific 2010 Cosmos and EPO symposia describing the design and construction of the system.|\n|2013||October: Fiske Planetarium at University of Colorado opens true hybrid optical-8K digital theater, with a Sky-Skan 8K Definiti projection system linked to a Megastar IIA optical star projector.|\n|2014||July: Digitalis Education Solutions, Inc releases the lightest all-in-one digital planetarium projection system, the Digitarium Iota. Weighing only 20.6lbs and 33.5lbs respectively, the Digitarium Iota and Digitarium Delta 3 systems represent a portable-first design philosophy to complement its new flagship fixed-dome system, the Digitarium Aethos.|\n- A Larger Vision of Virtual Reality. The News & Observer. Raleigh, North Carolina. Saturday, December 31st, 1994. Front page of Section D: Business.\n- http://dev.twinisles.com/research/bsvr.htm#vd and Alternate Realities Launches R&D Project. Triangle Business Journal. August 23rd, 1996.\n- The WAVE Report on Digital Media. Issue #907, November 3, 1998.\n- The Hemispherium Experience - Janice Webster, Director, Virtual Reality Centre, University of Teesside\n- Burke Baker Planetarium shoots for the sky with $1.2 million redo - Houston Business Journal:']"	['<urn:uuid:4ae3b2f0-b606-4598-b8cd-70bb5bf9016f>', '<urn:uuid:89197907-ff6f-4289-9c23-d48d3343213c>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T12:39:32.578853	23	84	3095
94	I'm working on energy efficiency initiatives and would like to know what are the expected environmental benefits from implementing the E3 Certification Programme by 2030?	Through adoption of E3 Certification by 7500 Bricks manufacturing units, the program is estimated to achieve energy savings of 7 Million Tonnes of oil equivalent (MTOE) per year and CO₂ savings of about 25 Million Tonnes by 2030.	['In news: Recently, the Ministry of Power launched the Energy Efficiency Enterprise (E3) Certifications Programme for Brick manufacturing Sector\nAbout Energy Efficiency Enterprise (E3) Certifications Programme\n- Aim of E3: To Tap huge energy efficiency potential in Brick manufacturing Sector\n- E3 Certification Scheme seeks to accelerate brick sector modernization, using market incentives to create customer demand to fulfil the vision for Atma Nirbhar Bharat.\n- Eligibility: The E3 Certificate will be awarded to those enterprises whose specific energy consumption will be 25% lower than the national baseline.\n- The enterprises can qualify for E3 through adoption of energy efficient brick manufacturing process and technology and Production of lower density bricks (hollow, perforated or porous bricks).\n- Voluntary: The adoption of the E3 Certification is currently voluntary for the Brick industry.\n- Bureau of Energy Efficiency under their awareness generation programme towards building energy efficiency shall also hold consultations with various stakeholders in this sector like builders architects etc. in order to sensitise them about the benefits of energy efficient bricks\nA brief note on Brick manufacturing Sector of India\n- Bricks production: India is the world’s second largest producer of bricks and this demand is expected to multiply three to four times over the next 20 years, through E3 Certification program\n- Contribution to GDP: This sector contributes nearly 0.7% to the country’s GDP, offers seasonal employment generation to over 1 crore workers, and has a strong influence on other economic sectors such as transportation and construction.\n- Energy consumption: This industry consumes about 45-50 million tonnes of coal equivalent annually, amounting to 5-15% of the total energy consumption in the country.\n- Potential of energy efficiency: The Brick sector has the second largest potential for energy efficiency amongst the Indian industrial sector after steel and more than cement, and the sector has largest energy savings to offer in coming decades through this program.\nExpected Outcome of E3 programme\nEnergy saving of 7 Million Tonnes of oil equivalent (MTOE) per year and CO, savings of about 25 Million Tonnes by 2030 are estimated through adoption of E3 Certification by 7500 Bricks manufacturing units.\nSignificance & Benefits of the programme\n- Benefits: The implementation of E3 Certification will lead to multiple benefits in terms of energy savings in the brick manufacturing process and improved quality of bricks resulting in cost savings to builders as well as energy savings to occupiers of buildings due to better thermal comfort and improved insulation properties.\n- Promote brick manufacturing: Energy-efficient transformation in brick manufacturing is expected to create an enabling policy environment for promoting manufacturing and utilization of energy efficient Bricks, technologies and practices in India’s construction sector.\n- Phasing out of inefficient technology: This will push towards phasing out inefficient technology in Brick production and enhance awareness and access of energy efficient bricks and technology.\n- By availingE3 Certification, the brick units shall shift towards adoption of more efficient technologies for making energy efficient bricks. Such bricks will be useful in complying with the requirements of Energy Conservation Buildings Code (ECBC).\n- It is proposed to nudge the agencies/departments in the public sector like CPWD, NBCC, State Public Works Department Railways and Urban local bodies, etc., to create demand for energy efficient/hollow bricks.']	['<urn:uuid:56761b14-753c-4efe-aac3-6a7d53e03527>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T12:39:32.578853	25	38	540
95	epoxy resin food safe fda approval cure	Most modern epoxy resins are food safe after they are fully cured, and many are FDA approved. However, this depends on the specific brand of epoxy being used.	['What is Epoxy Resin Made From?\nThe base for most common epoxy resins is bisphenol A novolac, aliphatic, halogenated, and glycidylamine.\nThe curing agent will most commonly be amines, however it can be made of homopolymerisation, anhydrides, phenols, and thiols.\nHow Durable are Most Epoxy Resins?\nVery strong. Epoxy resin after a full cure is able to hold up weight and is very resistant to scratches. A well constructed epoxy project will last indefinitely.\nHow to Color Epoxy?\nThe best two ways are with powder and wet pigments. Mica powder is the most popular type of dry pigment, while most wet pigments are based with water or alcohol.\nWhat are the Best Finish/Polishes for Epoxy Projects?\nThe most popular that we have seen for wood/epoxy projects are Odie’s Oil, Walrus Oil, Rubio Monocoat, and Osmo. Although, there are hundreds of types made from a variety of resources, you can find exactly what you’re looking for. When applying finish to epoxy, sand it to 320 grit first to allow absorption.\nWhat is an Epoxy Dirty Pour?\nA dirty pour of epoxy resin refers to using multiple colors and allowing them to flow freely. This is usually done to make countertops looking new and intricate.\nHow to Calculate Epoxy for a River Table Pour?\nFor a long straight pour for a river table, you want to take the average width, multiplied by the depth, and multiplied by the length. WxDxL will give you the cubic unit you are using, which you can convert to leaders using a simple google search.\nHow do I Remove Bubbles from My Epoxy Project?\nAfter pouring, you are going to want to wait 5-15 minutes, then go over the surface with a torch of some sort. Commonly used is a propane torch.\nHow Often can I Pour Layers for my Project?\nThis will vary by epoxy brand, but usually every 4-6 hours. The best way to test for this is to press it with a fingernail or something similar in toughness. If it can make an indent but is not too soft, it is ready for another pour.\nWhat is the Shelf Life of Epoxy Resin?\nMost epoxy resins, when stored airtight in a cool environment, will be able to last indefinitely.\nIs Epoxy Resin Food Safe?\nThis will depend on the brand, but today most epoxy resins are made to be food safe after they are fully cured, and many are FDA approved.\nIs there Shrinkage with Epoxy Resin?\nWhile epoxy resin itself won’t shrink, when pours on woods there will be some amount of it that is soaked up by the wood. This will be particularly noticeable on the cut edges.\nDo I Need to Seal My Wood Before Pouring Epoxy?\nThough it is not necessary, sealing your wood will minimize bubbles and help prevent the wood from soaking in too much of the epoxy resin.\nWhat Does Epoxy Resin Adhere To?\nThe materials are not exact, however common materials that epoxy resin works well with are wood, glass, concrete, some plastics, and clay. Even materials that commonly wouldn’t stick to epoxy can when sanded first.\nCan I Use Epoxy Outdoors?\nWhen working with epoxy, you want the temperature to be stable until cured. After cured, the piece can safely be moved outside without fear of many things being able to damage it easily.\nHow Long Does Epoxy Resin Take to Cure?\nThe average length for it to be hard is 12-24 hours. For the chemical bonds to be completed and for it to be worked on is usually 3-5 days, and depending on the brand and temperature can be a week.\nHow Thick Can I Pour Epoxy Resin?\nThe average will be between 1/8″-1/4″, however some brands create epoxy resins that are able to be poured much thicker.']	['<urn:uuid:20474a7a-236d-47db-8c43-1ee318c95688>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	7	28	634
96	I'm new to cooking and want to learn more about salt - what's the deal with table salt vs. rock salt, and are there health risks from eating too much salt?	Table salt is refined white crystal typically found in salt shakers and is the cheapest option, while rock salt found in grocery stores is not approved for human consumption and is mainly used for ice cream makers. Regarding health risks, excessive salt intake can lead to high blood pressure, which is responsible for about 50% of heart disease cases and 60% of strokes according to the World Health Organization.	['Salt (also known as sodium chloride or halite) is an important part of a healthy diet. Salt helps to balance electrolytes and is needed for proper cell function. Without salt we will die.\nDifferent Kinds of Salt and Their Uses\nTable Salt: Table salt is the refined white crystal typically found in a salt shaker. It usually comes in a cardboard can and is the cheapest salt on the shelf at the grocery store. You can buy it with or without iodine added (more on that later). It generally contains an anti-clumping agent.\nSea Salt and/or Mineral Salt: (such as Himalayan Pink Crystal Salt or Celtic Sea Salt): All salt originally came from the sea so to call something sea salt is a bit misleading. Typically when something is labeled sea salt it is not refined and still has other minerals in it, whether it was mined or was processed out of sea water. The minerals salt contains vary according to where the salt was mined. Himalayan salt contains different minerals than Celtic sea salt. Sea salt that has been collected after water has evaporated or distilled also has other minerals from the ocean. There are a lot of “gourmet” salts on the market so it’s important to read the label.\nKoshering and Pickling Salt: Koshering salt (commonly known as kosher salt) generally consists of salt without additives I say generally because there are some brands that do contain additives, so always read the label. Kosher salt has a large flat grain helpful for the koshering process, and if it does not have any additives it can be used for canning. Pickling salt is refined salt with no anti-clumping agent and is used for canning - it has a very fine grain so it will dissolve quickly in liquids. Using table salt for canning discolors the food because of the additives.\nRock Salt and other salts: Rock salt (the kind found at the grocery store) is salt typically used in ice cream makers and is not approved for human consumption; again, it’s important to read the label. There are other types of salts available such as salt used in water softeners or salt used to help melt ice on roads. These are not meant for human consumption and should not be placed in food storage.\nWhich Type of Salt to Store\nIf you are eating from your food storage you may not be eating a lot of processed foods and the salt that you have stored could be your main salt intake. So it’s important to decide what kind of salt is best to store for your family. Iodine was added to salt to prevent Goitre. You can get iodine from other sources of food such as seafood, soil (if it is contained in the soil you grow your food in) or seaweed such as kelp or dulse. Also, most multi-vitamins contain iodine. The only down-side to storing salt with iodine is that it can turn yellow after time. If you want to store salt as a trading item (see below), yellow salt might not look so appetizing. However, it’s still safe to eat and will store indefinitely. If you store table salt without iodine it will usually just contain an anti-clumping agent and will store indefinitely. Sea salt and mineral salt will also store well but not indefinitely. They would deteriorate according to the other minerals they contain. So reading the labels and doing an internet search on the minerals contained in the salt would give you an idea of the shelf life.\nTable salt is cheapest of all the varieties of salt therefore it is easy to obtain a year’s supply for your family, which should be between 3-5 pounds a person, in a relatively short time. If you’re on a budget, like me, table salt might be your best option. You may also want to stock-up on pickling salt if you plan to preserve things from your garden. You would have to calculate how much according to the estimated yield from your garden for a given year. Pickling salt and table salt will store indefinitely, even if moisture seeps into the container, it can be laid out and dried.\nHow I Store Salt\nI bought two 25 pound bags of salt from a big box store. The bags do not contain iodine and this was my only choice from this store. I have a food saver but decided that I did not want to waste my storage bags because salt will not go rancid even if it’s exposed to air.\nI used heavy duty plastic bags, I thought Mylar would be overkill here.\nI scooped the salt into one gallon bags. I did not fill the bags all the way because I did not want the plastic to break. They will probably be retrieved by one of my children in our food rotation process so I wanted to make sure they were light enough to carry. I poured the salt into a pitcher because it’s very hard to pour a bag into a bag.\nFood grade 5 gallon bucket.\nBags ready to go in.\nI put two to three bags in 5 gallon food grade containers.\nThen I labeled them and put them up.\nOther Important Notes\n- Salt should not be confused with Epsom salt (also known as magnesium sulfate) - they are completely different compounds.\n- Never store salt in metal containers. Salt leaches metals and/or elements out of the metal. You could wind up with a poisonous substance in your salt.\n- Also something to consider is using/storing salt to be used as a bartering item. A lot of preppers store canned goods and processed food, which already contain salt, so they don’t give much thought to plain salt. In our culture today salt has been associated with heart attacks and other health afflictions resulting from the over consumption, therefore, many people might not store enough salt or store it at all. At some point, processed food might become scarce and salt would once again be a valuable item as it was in ancient times.', 'Five salt myths that could be damaging your health\nMost of us are well aware that high blood pressure is a major risk factor for some of the most common killers, such as stroke and coronary heart disease. But are you aware just how big a role salt consumption can play in developing high blood pressure in the first place?\nEven if you are clued up on this, there’s still a chance you’re not entirely sure how much salt you’re consuming – especially if you regularly eat processed foods (things like ready-made sauces, basically many of the foods you’re not preparing from scratch), which, according to Consensus Action on Salt & Health (CASH), around 75% of the salt in our diets comes from.\nAccording to recent research from low-salt alternative brand LoSalt, nearly two-thirds of people (63%) are not actively reducing their salt intake – which indicates there’s still a lack of awareness around the white stuff and its associated health risks.\nSo what else do you need to know about salt intake?\nWe’ve done some salt myth-busting to point you in the right direction…\nMYTH: You can eat as much salt as you like\nFalse! We need salt to survive, so cutting it out entirely is NOT the goal. However, too much of it can lead to potentially serious problems down the line. As many as three-quarters (75%) of the population don’t realise that 6g is the maximum daily recommended salt intake, according to the LoSalt survey.\nThere’s two components in salt: sodium and chloride, and it’s the sodium that is doing the damage because it can lead to high blood pressure. Although most people with high blood pressure don’t realise they have it, it is responsible for around 50% of heart disease cases and 60% of strokes, according to the World Health Organisation.\nMYTH: Rock salt is healthier than regular salt\nMany people believe trendier sea and rock salts are healthier than regular salts, but this is incorrect.\nNutritionist and dietitian Azmina Govindji says: “They contain the same amount of sodium chloride: 100%! Sea salt may contain traces of other minerals, but the levels are too low to have a health benefit and so it is not healthier for you than any other salt.”\nMYTH: Saltless food is bland\nExtra salt added during cooking makes up 20% of our salt intake, notes Govindji. Granted, this might not sound like much, but it’s the chunk of our intake which we’re most in control of.\nWe might think our dinner will taste bland without a sprinkling of salt, but this isn’t strictly true.\n“By reducing a little here and there, you’ll soon find your taste buds become accustomed to not having such high levels of salt and you’ll have less need for it in cooking and for seasoning food,” says Govindji.\nMYTH: There are no alternatives to salt\nFor those who really can’t go without the taste of salt, brands like LoSalt offer an alternative to the traditional white stuff. It still contains all the flavour of normal salt, but contains 66% less sodium.\nAlso, try herbs and spices, vinegar or a dash of lemon instead to give your food the punch of flavour that salt usually would. Dill tastes great with fish, rosemary is wonderful with meat, and basil will finish off a pasta dish perfectly.\nMYTH: Cutting out salt in cooking is all you need to do\nNot adding salt to cooking is a good start – but it’s important to remember that this is only a small part of the solution, and there’s plenty more we can do.\nThe food we buy fresh in supermarkets can still be salt heavy, so always read the labels. If you can’t always cook from scratch, choose low-salt options and avoid the red traffic light on food labels!\nEat notoriously high-salt foods, like cheese, bacon, ham, salted and roasted nuts and salami, in moderation, and watch out for ready meals, pizzas, pasta sauces and bread. They’re usually far higher in salt than you might think!']	['<urn:uuid:c00814ba-62f9-45f1-a5e9-f3eab0a41195>', '<urn:uuid:27e2ab2d-22f1-4776-9702-5e6acad94fb8>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T12:39:32.578853	31	69	1695
97	security team waterfall model differences	Security teams in waterfall had to complete their step before moving forward and couldn't go back up, making changes difficult. In contrast, modern DevOps security teams are involved early in development, work collaboratively throughout the process, and continuously review and update security measures as part of cross-functional teams.	['DevOps Security at Scale (Part 4): Security as a First-Class Citizen\nDevOps Security at Scale (Part 4): Security as a First-Class Citizen\nLearn to optimize your code based on security, instead of resorting to last-minute fixes, in this series on building secure systems at scale.\nJoin the DZone community and get the full member experience.Join For Free\nDiscover how quick and easy it is to secure secrets, so you can get back to doing what you love. Try Conjur, a free open source security service for developers.\nThis is the fourth blog post in a series discussing how high-performing DevOps teams build secure systems at scale.\nSoftware teams are usually responsive to serious bugs, bad designs, or terrible performance of their systems. That’s because those are acute, obvious, in-your-face-bad kinds of issues that demand immediate attention.\nOn the opposite side of the coin are latent issues that, left untouched, will become bigger problems in the future. These are items that get addressed through preventative work before they become major visible problems. For example, technical debt is well understood by most software teams, and it usually gets periodic investment to prevent it spiraling out of control.\nA sophisticated DevOps team will regard security issues as acute, immediate problems and not file them away in a growing backlog of To-Dos, never to be addressed. They elevate security from being a secondary concern to being a first-class citizen.\nLess sophisticated teams think of vulnerabilities and security flaws the same way as technical debt, and try to chip away at them to prevent them from becoming a problem in the future. Unlike technical debt, vulnerabilities are problems right now. They haven’t yet been exploited. And once they’re exploited, it’s too late.\nWhen a team puts security as a first-class citizen, they naturally adopt many good security practices as part of their development and release processes. Let’s examine some of those practices in detail.\nSecure Management of Secrets\nAll secrets for productions systems (passwords, private keys, or any sensitive information that an attacker could use) are stored in a secure, highly-available vault and accessed at runtime only by the systems that have been granted access to them. The secrets are automatically changed on a frequent schedule, thereby limiting the amount of time they are valid even if they are breached. Human operators of the vault are not permitted access to the secret values held within.\nGood Security Hygiene\nThere are a variety of “good hygiene” practices which security-focused teams will always take care to perform:\n- The principle of least privilege is applied everywhere possible: machines and humans only get access to the resources they absolutely deserve, and their permissions are promptly revoked once that privilege should be removed.\n- Vulnerability scans are automatically run on all dependent 3rd-party libraries. Vulnerable libraries are upgraded immediately if fixes are available. Upgrading 3rd-party libraries to newer compatible versions is not a complex process.\n- 3rd-party libraries are kept up to date, even if vulnerabilities aren’t yet detected in them. This helps avoid major compatibility issues if a dependent library needs to be immediately updated to address a vulnerability, but the patch is only available for the latest version of that library.\n- Penetration tests are conducted very regularly to scan the application (and its CI/CD environment) for potential attack vectors. These tests are sometimes even done as part of a CD pipeline using automated tools and augmented with regular human-driven, white-hat ethical hacks.\n- All developers are trained in application and information security as part of their new hire programs and in their ongoing education.\n- The development workflow for product code changes includes explicit checks for information exposure, attack vectors, and security policy violations. For example, if a team uses GitHub PRs, a PR template would be used that codified these expectations for the submitter and reviewer(s).\nAll Team Members Work to Enable Secure Velocity\nSecurity teams in some companies have earned a negative reputation as “the people in the way of progress”. Typically this arises due to the security team hearing about a new system or initiative too late in its development cycle. Regardless of how free-flowing and speedy the development of that component was, the security team still have the job of ensuring that their business isn’t put at risk with the deployment of that new technology.\nSecurity approvals can take time (especially with lots of concurrent projects going on) and can lead to the security team getting a reputation of being a blocker. Discontent mounts, emotions rise, and efforts are taken to work around their governance with “waivers” and “escalations.” In dysfunctional organizations, developers might even work to subvert the security team’s authority by covertly deploying applications without their approval, making matters even worse.\nA high-performing organization will recognize that the root cause of the above problems come from the security team being in the wrong position of the development workflow.\nShifting Security Left\nSophisticated teams will directly involve security at its early stages of development of new applications, get architectures and technology choices reviewed by them as early as possible, and make an effort to “move security left” in the workflow. Then if the security team finds serious issues, they can be addressed much earlier, perhaps even before much code has been written. This leads to a much less contentious discussion between developers and security, and a far healthier relationship between those groups.\nFor teams that haven’t yet pushed security to the left, it’s probably natural for them to think that such a move would slow down their development process. If viewed within a short enough horizon, doing security reviews of early versions of new applications would seem like a costly and burdensome choice.\nBut if the entire development lifecycle is considered, those early security reviews prevent more costly rework later on when major architectural choices are proven to have vulnerabilities that have to be reworked. Early security reviews actually de-risk development projects and make much more economic sense than doing them as a later step in the release lifecycle.\nTeams that work to put security concerns high on their list of priorities quickly learn to optimize around them. Instead of applying security fixes as band-aids when vulnerabilities are discovered, they modify their development flow to incorporate process and tools that help them avoid security gaps in the first place. This mindset leads these teams to be proactive in dealing with security issues, not reactive – and that is what it means to make security a first-class citizen in a DevOps organization.\nDevOps Security at Scale Series\nPublished at DZone with permission of Brian Kelly , DZone MVB. See the original article here.\nOpinions expressed by DZone contributors are their own.', 'In the early days of software development, projects were developed sequentially in a series of steps which was called ?The Waterfall Model.? It was called the waterfall because once you got past a step, you couldn?t climb back up. Here is a typical waterfall model for software development:\nEach step needed to be completed before the next could begin. It took time to develop and changes were difficult to include. When problems were found, developers often had to start all over at the top of the waterfall.\nSay Goodbye To The Waterfall\nIn today?s continuous development cycles, the waterfall method just is not relevant anymore. Thus, the agile methodology was born. It values individuals and interactions over process and tools. It taught developers to break software down into smaller chunks, develop these chunks, and accelerate feedback loops. It provides for quicker design, collaboration, and a faster launch cycle.\nDevOps teams have taken this agile concept to heart by merging software development (the Dev part) and IT operations (the Ops part) into a cohesive team to accelerate delivery. It created agile teams that could work together cross-functionally and iterate quickly.\nContinuous integration (CI) streamlines the system by using automation to drive products through the system. Each chunk is tested and integrated continuously. This allows for smaller and more frequent updates. Continuous delivery (CD) allows for rapid release software delivery. Tools like Jenkins provide DevOps teams with the ability to automate each stage of the delivery pipeline. It enables a consistent and reliable method to regularly update apps and software.\nUsing these methods, agile methodologies have been extended across development, integration, testing, and delivery cycles. While agile methods provide the underpinning for DevOps teams, the collaboration of development and operations teams, coupled with CI and CD, greatly enhances the efficiency of the operation.\n- Faster time to market\n- Ability to build and evolve products\n- Improved efficiency\n- Reliable releases and deliver\n- Improved quality product\n- Improve customer satisfaction\nDevOps Builds On Agile Methodologies\nYou will find many similarities between the traditional agile model and how DevOps teams work together. DevOps is holistic and collaborative with continual feedback loops. Automated systems like JFrog Artifactory and Go Registries allow for rapid builds, tests, releases, monitoring, and revisions.\nHowever, there are differences as well. Agile focuses on the development phase and tends to ignore the operations phase. DevOps provides end-to-end operations from development through delivery. It treats the code no differently whether it is in the development phase, QA, or on a production server. DevOps tends to embrace a more inclusive and wider culture.\nDevSecOps teams use the same techniques to remove silos in organizations by folding in other functional areas that play a role. One area that?s emerging is folding in security. So-called DevSecOps teams function as a unit and drive security functions throughout the end-to-end process as part of the culture.\nBuilding on agile methods, the DevOps model breaks down barriers between functional units so that development and operations can work collaboratively across the entire product life cycle. It smooths out hand-off friction and creates operational efficiency. This accelerates delivery and innovation.']	['<urn:uuid:b51e6630-9e43-4eac-b65c-e35bc89e7b82>', '<urn:uuid:0c2d2c33-abd3-4a5c-85d9-60e4d1fea0f5>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T12:39:32.578853	5	48	1641
98	what are three required conditions florida courts check before finding spoliation of evidence	Florida courts must answer three questions before finding spoliation: 1) whether the evidence existed at one time, 2) whether the spoliator had a duty to preserve the evidence, and 3) whether the evidence was critical to an opposing party being able to prove its prima facie case or a defense.	['What Are The Consequences And Standard For Spoliation Of Evidence?\nIf you are anticipating litigation, or are already involved in pending litigation, you should strongly consider changing the settings of your electronic devices to avoid accusations of spoliation later on in the litigation.\nThe term “spoliation” refers to the destruction or significant alteration of evidence, including the failure to preserve evidence. To ensure that parties in litigation do not abuse the judicial process, Florida law makes clear that the parties have an affirmative duty to preserve evidence. Any breach of that duty, and any resulting destruction or deletion of evidence, may result in some serious consequences. Such evidence includes, but is not limited to, emails, text messages, and social media posts stored on electronic devices, also known as electronically stored information (“ESI”).\nPursuant to Florida Rule of Civil Procedure1.380(e), ESI is spoliated if it (1) should have been preserved due to the anticipation or conduct of litigation, (2) is lost because a party failed to take reasonable steps to preserve it, and (3) cannot be restored or replaced through other discovery measures. Florida law differentiates between two different scenarios where evidence has been destroyed or deleted: negligent spoliation and intentional spoliation.\nIn the first scenario, negligent spoliation, if the court finds that the discovering party is prejudiced from the loss of evidence by the other party, the court may implement measures “no greater than necessary” to cure the prejudice. Fla. R. Civ. P. 1.380(e). Such measures might include imposing an adverse evidentiary inference or adverse presumption during trial that will address the missing evidence. An example of negligent spoliation is where a party has a setting on his or her phone or email that automatically deletes text messages or emails after a certain period of time, and the party does not disable this setting once it anticipates or is a party to litigation.\nIn the second scenario, intentional spoliation, the court must find that the spoliating party “acted with intent to deprive another party of the information’s use in the litigation.” Fla. R. Civ. P. 1.380(e). Once a finding of intent has been made, the court may impose the following sanctions: (1) presume that the evidence was unfavorable to the spoliating party, (2) instruct the jury either that it may or must presume that the evidence was unfavorable to the spoliating party, or (3) dismiss the case entirely, or enter a default judgment against, against the spoliating party. Such sanctions are typically more severe than those in negligent spoliation cases and might include instructing the jury to presume that the spoliated evidence was proof of an unfavorable fact, striking the spoliating party’s pleadings, or entering a default judgment against the spoliating party.\nRegardless of the type of spoliation, before a court may impose any of these measures to cure the spoliation, the court must answer three questions: “1) whether the evidence existed at one time, 2) whether the spoliator had a duty to preserve the evidence, and 3) whether the evidence was critical to an opposing party being able to prove its prima facie case or a defense.” Golden Yachts, Inc. v. Hall, 920 So. 2d 777 (Fla. 4th DCA 2006).\nThe first question is often answered easily with testimony or physical evidence that the evidence at issue once existed.\nAs to the second question, “Florida courts have…found a duty to preserve evidence in other circumstances when a party should reasonably foresee litigation.” League of Women Voters of Fla. v. Detzner, 172 So. 3d 363, 391 (Fla. 2015); see also American Hospitality Management Co. of Minnesota v. Hettiger, 904 So. 2d 547, 548 (Fla. 4th DCA 2005). At a minimum, a duty to preserve triggers once litigation has commenced.\nThe third question is where Florida courts encounter the most difficulty in deciding whether to find spoliation has occurred. In answering this question, “there must be ‘some showing indicating that the destroyed evidence would have been relevant to the contested issue.’” Adamson v. R.J. Reynolds Tobacco Company, 2021 WL 3073670, at * 6 (Fla. 4th DCA July 21, 2021). The courts also caution that “[c]ourts should take care ‘not to require too specific a level of proof,’ because ‘holding the prejudiced party to too strict a standard of proof regarding the likely contents of the destroyed evidence would subvert the prophylactic and punitive purposes of the adverse inference ….’” Id. In other words, given that the discovering party has not done anything wrong, courts should not be too strict in answering the third question in the affirmative.\nIf the answer to all 3 of these questions is “yes,” then the court may implement any of the sanctions discussed above.\nPreserving evidence when litigation is anticipated or already exists is serious business, and attorneys must be proactive in communicating this duty to their clients. When evidence disappears–whether intentionally or negligently–the litigation rarely goes well for the spoliating party once the discovering party learns that spoliation has occurred.']	['<urn:uuid:a954f177-f00d-4559-90db-583f23bdc346>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T12:39:32.578853	13	50	825
99	hi im researching brain scans need to know difference brain activity measurement bold vs asl during rest	BOLD (Blood Oxygenation Level Dependent) and ASL (Arterial Spin Labeling) measure different aspects of brain activity during rest. BOLD is an indirect measure that depends on blood oxygenation, combining responses of cerebral blood flow, oxygen metabolism, and blood volume. In contrast, ASL directly quantifies regional cerebral blood flow (CBF), which is more closely related to cerebral metabolism and neuronal activity. While ASL measures static brain activity through mean CBF over a scanning session, BOLD captures dynamic fluctuations through the amplitude of low-frequency fluctuations (ALFF).	"['Functional imaging is the study of human brain function based on analysis of data acquired using brain imaging modalities such as Electroencephalography (EEG), Magnetoencephalography (MEG), functional Magnetic Resonance Imaging (fMRI), Positron Emission Tomography (PET) or Optical Imaging. The aim is to understand how the brain works, in terms of its physiology, functional architecture and dynamics. The framework for the conduct of these studies includes classical techniques of neuroanatomy, neurophysiology, and experimental psychology and the cognitive neurosciences, as well as more theoretical approaches, based on perspectives from computational neuroscience and statistics.\nModern functional imaging has two main advantages over the multi/single-unit recordings used to study the electrophysiology of neurons. The first is that it is generally non-invasive, and is therefore applicable routinely in humans. This allows for the study of unique human attributes such as language. The second is that it can provide a wide field of view. Rather than recording information about a single or small number of neuronal cells, an image may be gathered summarizing simultaneous activity across the whole brain. This provides a different yet complementary perspective on neural coding (see e.g., functional integration, below). A disadvantage, however, is that functional imaging provides only an indirect measure of the quantities of primary interest to neuroscientists e.g., firing rates and membrane potentials. Current research is aimed at bridging this gap using a combination of experimental and mathematical modelling approaches.\nCurrent imaging modalities include the Electroencephalogram (EEG) which records electrical voltages from electrodes placed on the scalp and the Magnetoencephalogram (MEG) which records the magnetic field from SQUID sensors placed above the head. Both MEG and EEG have a high temporal resolution (milliseconds), capable of detecting e.g., the 40Hz Gamma response implicated in object representation (Tallon-Baudry and Bertrand, 1999). Their spatial resolution is, however, usually of the order of centimeters rather than millimeters. This varies a great deal, depending on the nature of the neuronal activity one is trying to localize. It depends in particular on the number of sources that is activated at the time data is recorded. In practice this implies that e.g. for isolating subtle cognitive components, a lower resolution is to be expected, whilst the stronger early components of an auditory response can be localized to within millimeters in the brainstem.\nIn contrast, functional Magnetic Resonance Imaging (fMRI) has low temporal (hundreds of milliseconds or seconds) but relatively high spatial (millimeters) resolution. Increases in neural activity cause variations in blood oxygenation, which in turn cause magnetization changes that can be detected in an MRI scanner. This Blood Oxygenation Level Dependent (BOLD) signal peaks up to 6s after neuronal activity. Moreover, the hemodynamics act like a low pass filter (Logothetis, 2001), smearing out changes in local electrical activity.\nSimultaneous recordings of EEG and fMRI (Ritter and Villringer, 2006) have the potential to localise neuronal activity with both high temporal and spatial resolution. Other important imaging modalities are PET and Optical Imaging. PET\'s spatial resolution typically falls somewhere between that of fMRI and MEG/EEG. In addition, PET has very low temporal resolution (tens of seconds to minutes) and requires injection of a trace amount of radioactivity. This limits the number of measurements that can be made on any one individual. But a great advantage of PET is that it is particularly useful in the study of brain neurophysiology and neurochemistry e.g., one can image glucose uptake and the activity at serotonin and dopamine receptors, in systems of importance to those studying anxiety, depression and addiction. Optical Imaging  or Near Infrared Spectroscopy (NIRs) can also detect BOLD signals from changes in the amount of reflected light. This is an economical alternative to fMRI but is limited to imaging the cortex.\nFunctional imaging is also closely related to structural imaging, in which MRI is used to provide high resolution images with high contrast between e.g., white matter and gray matter. These detailed anatomical images have recently been complemented with data from Diffusion Tensor Imaging (DTI) which can show the direction of white matter fibres.\nThere are two key themes in the analysis of functional imaging data. They reflect the long-standing debate in neuroscience about functional specialization versus functional integration in the brain (Cohen and Tong, 2001). The first is brain `mapping’ where three-dimensional images of neuronal activation are produced showing which parts of the brain respond to a given cognitive or sensory challenge. This is also known as the study of functional specialization and generally proceeds using some form of Statistical Parametric Mapping (SPM). A classic example here is the identification of human V4 and V5, the areas specialized for the processing of color and motion.\nSPM is a voxel-based approach, employing classical statistics and topological inference, to make comments about regionally specific responses to experimental factors. PET or fMRI data are first spatially processed so that they conform to a known anatomical space, in which responses are characterized statistically typically using the General Linear Model (GLM). For fMRI data the GLM embodies a convolution model of the hemodynamic response. This accounts for the fact that BOLD signals are a delayed and dispersed version of the neuronal response. GLMs are fitted at each voxel and inferences are made about which parts of the brain are active, in a statistical sense. To accommodate the spatial nature of the imaging data (and account for the multiple statistical comparisons made) SPM techniques make use of Random Field Theory (RFT) (see Fig 1) and/or other statistical procedures, e.g., False Discovery Rate.\nThe SPM approach can also be used with structural data to find brain regions containing a higher gray matter density. This is known as Voxel-Based Morphometry (VBM) (Ashburner and Friston, 2000) and has been used, for example, to show that the posterior hippocampus, useful for spatial navigation, is enlarged in taxi drivers.\nFor MEG or EEG, data can be analyzed in sensor space, furnishing a crude spatial mapping of brain function. Functions can, however, be more accurately localized using source reconstruction methods (Baillet et al. 2001). These work by specifying a forward model describing how a current source in the brain propagates to become an MEG or EEG measurement, using Maxwell\'s equations (http://www.scholarpedia.org/article/Volume_Conduction). These models are then inverted using statistical inference. Data from sensory systems is often analyzed using an averaging procedure. The data immediately following a sensory event, e.g., hearing an auditory tone, is averaged over multiple events to produce an Event Related Potential (ERP). Components of the ERP can then be localized to different parts of the brain. Other cognitive components, however, are not easily isolated using this ERP approach. For these, a time-frequency characterization may be more appropriate (Tallon-Baudry and Bertrand, 1999). See also Makeig et al. 2002 for a recent critique of the averaging procedure.\nThe second theme is ‘functional integration’, where models are used to describe how different brain areas interact. A classic example is the use of models to find increased connectivity between dorsal and ventral visual streams after subjects learn object-place associations. A wide range of statistical techniques are being used to measure inter-regional connectivity. Both unsupervised (e.g., Independent Component Analysis , ICA) and supervised techniques (e.g., support vector machine, SVM) are used. Other models seek to directly measure ""causal"" connectivity based on static, statistical constraints (e.g., Structural Equation Modelling, SEM) or dynamic, more bio-physically motivated assumptions (e.g., Dynamic Causal Modelling, DCM). A challenge for functional integration models is to bridge the gap between the large-scale, statistical models of the whole brain, and the small number of highly constrained spatial regions needed to be able to apply SEM and/or DCM.\nDCM for fMRI uses a forward model in which neural activity generates BOLD signal changes via a `Balloon\' model of vascular dynamics. The model is then inverted to provide estimates of changes in connectivity between brain regions. In DCM for ERPs, neural activity is described using neural-mass models, which then give rise to observed EEG or MEG data using Maxwell\'s equations (see above). Inversion of the model then allows one to make inferences about changes in long-range excitatory connections among different brain areas.\nThe above analysis approaches are implemented in various software packages such as SPM  (SPM is the name of a software package as well as a methodology), FSL , EEGLAB , BrainVoyager , or AFNI . They are also described in a recent textbook (Friston et al. 2006).\nThe applications of functional imaging are diverse and multitudinous. PubMed, for example, returns over 32,000 articles. The functional imaging journal `NeuroImage\' classifies research articles under \'Anatomy and Physiology\', \'Methods and Modelling\', \'Systems Neuroscience\' or \'Cognitive Neuroscience\'. Additionally a number of applications in clinical and experimental medicine are emerging.\nFunctional imaging has been applied to all systems of the brain; whether visual, auditory, sensorimotor, emotional, memory, language, attention or control. Overviews of research findings are available in recent textbooks (Frackowiak et al. 2003, Gazzaniga 2004). Recent high-profile (and arbitrarily selected) applications of fMRI in these areas include a study of the effect of sleep on human memory performance (Yoo et al. 2007) and a study of the neuronal and cognitive components of altruism (Tankersley et al. 2007).\nImaging is also used for the study of basic brain anatomy and physiology. For example, DTI has recently been used to identify three regions of human parietal cortex based on their connectivity patterns with other brain areas (Rushworth et al. 2006). Imaging is also used clinically: The best established application is the use of fMRI for pre-surgical mapping to localize cerebral functions in tissue within or near regions intended for neurosurgical resection (Matthews et al. 2006).\nAuthors web page: http://www.fil.ion.ucl.ac.uk/~wpenny/\nR.S.J. Frackowiak, K.J. Friston, C. Frith, R. Dolan, C.J. Price, S. Zeki, J. Ashburner, and W.D. Penny (2003) Human Brain Function. Academic Press, UK, 2nd edition.\nC. Frith (2007) Making up the mind: How the brain creates our mental world. Blackwell Publishing.\nK. Friston, J. Ashburner, S. Kiebel, T. Nichols and W. Penny (2006) Statistical Parametric Mapping: The Analysis of Functional Brain Images. Elsevier, London.\nGazzaniga, M.S., Ivry, R., & Mangun, G.R. Cognitive Neuroscience: The Biology of the Mind. W.W. Norton, 2002. 2nd Edition\nM.S. Gazzaniga (2004). The Cognitive Neurosciences III. MIT Press, New York.\n- Jan A. Sanders (2006) Averaging. Scholarpedia, 1(11):1760.\n- Valentino Braitenberg (2007) Brain. Scholarpedia, 2(11):2918.\n- James Meiss (2007) Dynamical systems. Scholarpedia, 2(2):1629.\n- Paul L. Nunez and Ramesh Srinivasan (2007) Electroencephalogram. Scholarpedia, 2(2):1348.\n- Seiji Ogawa and Yul-Wan Sung (2007) Functional magnetic resonance imaging. Scholarpedia, 2(10):3105.\n- Mark Aronoff (2007) Language. Scholarpedia, 2(5):3175.\n- Anthony T. Barker and Ian Freeston (2007) Transcranial magnetic stimulation. Scholarpedia, 2(10):2936.\nJ. Ashburner and K.J. Friston. Voxel-Based Morphometry - The Methods. NeuroImage, 11:805-821, 2000\nS. Baillet, J.C. Mosher and R.M. Leahy (2001) Electromagnetic brain mapping. IEEE Signal Processing Magazine, pages 14-30.\nJ.D. Cohen and F. Tong (2001) The face of controversy. Science, 293, 2405-2407.\nN. Logothetis, J. Pauls, M. Augath, T. Trinath and A. Oeltermann (2001) Neurophysiological investigation of the basis of the fMRI signal. Nature 412, 150-157.\nS. Makeig, M. Westerfield, T Jung, S. Enghoff, J. Townsend, E Courchesne and T. Sejnowski (2002) Dynamic brain sources of visual evoked responses Science, 295, 690-694.\nP.M. Matthews, G.D. Honey and E.T. Bullmore (2006) Applications of fMRI in translational medicine and clinical practice. Nature Reviews Neuroscience, 7, 732-744.\nM.F. Rushworth, T.E. Behrens and H. Johansen-Berg (2006) Connection patterns distinguish 3 regions of human parietal cortex. Cerebral Cortex, 16(10):1418-30.\nC. Tallon-Baudry and O. Bertrand (1999) Oscillatory gamma activity and its role in object representation. Trends in Cognitive Sciences, 3(4), 151-162.\nP. Ritter and A. Villringer (2006) Simultaneous EEG-fMRI. Neuroscience and Biobehavioural Reviews. 30(6), 823-838.\nD. Tankersley, C J Stowe and S A Huettel (2007) Altruism is associated with an increased neural response to agency. Nature Neuroscience 10, 150 - 151.\nS.S. Yoo, P T Hu, N. Gujar, F A Jolesz and M P Walker (2007). A deficit in the ability to form new human memories without sleep. Nature Neuroscience 10, 385 - 392.', 'Resting-state fMRI studies possess progressively focused on multi-contrast techniques such as BOLD and ASL imaging. both ASL and BOLD techniques at two study centers (NIDA and HNU). Static mind activity was determined as voxel-wise imply cerebral blood flow (CBF) using ASL i.e. CBF-mean while dynamic activity was measured from the amplitude of low rate of recurrence fluctuations (ALFF) of BOLD i.e. BOLD-ALFF at both Senegenin NIDA and HNU and CBF i.e. CBF-ALFF at NIDA. We showed that mean CBF was lower under EC than EO in the primary visual cortex while BOLD-ALFF was higher under EC in the primary somatosensory cortices extending to the primary auditory cortices and reduced the lateral occipital area. Interestingly mean CBF and BOLD-ALFF results overlapped in the visual cortex to a very small degree. Importantly these findings were mainly replicated from the HNU dataset. State differences found by CBF-ALFF were located in the primary auditory cortices which were generally a subset of BOLD-ALFF and showed no spatial overlap with CBF-mean. In conclusion static mind activity measured by mean CBF and dynamic mind activity measured by BOLD- and CBF-ALFF may reflect different aspects of resting-state mind activity and a combination of ASL and BOLD may provide complementary information on the biophysical and physiological processes of the brain. Introduction Blood oxygenation level dependent (BOLD) practical magnetic resonance imaging (fMRI) is a noninvasive neuroimaging technique that is widely Senegenin used due to its relatively high sensitivity ease of implementation and good spatial and temporal resolution. Resting-state BOLD fMRI has been increasingly used to Senegenin assess the practical connectivity of mind networks following a seminal finding that BOLD signals are temporally synchronized in the sensorimotor system . Though spontaneous BOLD fluctuations during the resting state have been demonstrated to reflect neuronal activity [2-5] BOLD is an indirect measure of neuronal activity as it depends on blood oxygenation which is the combined response of CBF cerebral metabolic Senegenin rate of oxygen (CMRO2) and cerebral blood volume. In contrast to BOLD arterial spin labeling (ASL) fMRI can quantify regional CBF a single physiological parameter closely related to cerebral rate of metabolism and neuronal activity [6-8]. Recently attention has been attracted to the combination and assessment of resting-state BOLD and ASL signals. Previous studies possess shown the coupling of CBF with resting-state mind activity derived from BOLD providing evidence that intrinsic BOLD activity has a physiological basis [9-11]. As of yet however no study offers directly compared the between-condition results yielded by these two different fMRI techniques. Most ASL studies measure the mean CBF (referred to as CBF-mean hereafter) over a scanning session in the voxel level. CBF-mean could be considered as an index reflecting static mind activity over the duration of scanning. In contrast the amplitude of low-frequency fluctuations (ALFF) of BOLD (referred to as BOLD-ALFF hereafter) steps the variation over time or dynamic fluctuations of mind activity in the voxel level [12 13 and has Rabbit polyclonal to ACCN2. been used to detect modified mind activities in many Senegenin mind disorders [13-19]. Similarly spontaneous fluctuations of CBF could be captured  by ALFF of CBF (named CBF-ALFF below) which has been used for voxelwise measurement of the dynamic characteristics of resting-state perfusion signals. To keep the terms consistent with those used in our earlier study  BOLD-ALFF and CBF-ALFF refer to dynamic measurements and CBF-mean refers to a static measurement of the brain activity with this study. Using both ASL and BOLD techniques we acquired data from subjects in eyes-closed (EC) and eyes-open (EO) claims two physiological claims with different levels of mind activity [21 22 We collected BOLD and ASL data at two self-employed study centers for validation purposes and aimed to investigate: 1) whether static and dynamic state variations between EC and EO could be detected by BOLD and ASL fMRI; 2) if yes whether static and dynamic state differences recognized would display spatial overlap; and 3) whether the static and dynamic state differences could be cross-validated by the two research centers. Both BOLD and ASL fMRI measure hemodynamic reactions induced by neuronal activity. However imply CBF captures static characteristics of resting-sate mind while dynamics measured by ALFF reflect variance in resting fluctuations. Moreover state variations have been Senegenin demonstrated.']"	['<urn:uuid:9129a73e-9969-48ea-a807-09e74e27c94a>', '<urn:uuid:ffbe6627-1ee2-490e-aaac-21fa83fa449e>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T12:39:32.578853	17	84	2681
100	As someone interested in women's history, I'd like to know what made Effa Manley unique in baseball history and what she accomplished?	Effa Manley became the first woman ever inducted into the Baseball Hall of Fame in 2006. She was a groundbreaking figure who co-owned and operated the Newark Eagles with her husband Abe Manley. She handled the team's office operations, finances, and public relations, while taking pride in always paying players and creditors on time. Beyond baseball, she was a civil rights activist who organized the Citizens League for Fair Play, successfully fighting for Black employment rights in Harlem through boycotts and protests. She later advocated for Negro League players to be inducted into the Baseball Hall of Fame through letter-writing campaigns in the 1970s.	"['Author Audrey Vernick was kind enough to answer a few questions about She Loved Baseball, the story of Effa Manley. The first woman inducted into the Baseball Hall of Fame\n1 How many baseball books did you read for research purposes?\nI wrote this book so long ago I don\'t even remember! And I\'d be hard pressed to distinguish which ones were really for research and which ones were just about baseball and kind of irresistible\nAlso, I relied more upon newspaper accounts and recorded interviews than books for this particular project. Though James Overmyer\'s QUEEN OF THE NEGRO LEAGUES was also a vital research resource.\n2 One fact about Effa Manley or the game of baseball that you wished you were able to fit in the book?\nThere were so many! There was a great scene in which Effa Manley really gave it to Branch Rickey for raiding Negro League teams without any compensation. There were more Newark Eagles players in earlier versions--like Willie Wells, first player to wear a batting helmet (though his was more of a construction helmet). And the fact that Effa made such a lasting impression on her players; Larry Doby asked Effa and her husband, Abe, to be his child\'s godparents.\n3 Picture book authors run the risk of be paired with an illustrator, whose style doesn\'t enchance the text. Thankfully that didn\'t happen to She Loved Baseball. Don Tate\'s illustrations are great and fit the text perfectly. When did you know you\'d be paired with Tate?\nOver a year after the manuscript was acquired! It took a long time to find the right person, and there were lots of people with lots of opinions involved in the decision-making process. But boy, how lucky was I? I can\'t imagine this text paired with any other art; Don Tate did a perfect job.\n4 What is it about baseball fans and baseball books? ( I am not judging. I also get baseball book fever. Looking forward to reading The Baseball by Zack Hample)\nI think that link makes sense. Baseball\'s a pretty cerebral sport, and watching baseball allows one\'s brain to roam a bit, time for thinking, considering.\n5. Who is your favorite team? Any predictions for this season?\nMy team is the New York Yankees, despite the fact that I grew up in Queens, not far from the shadow of Shea Stadium. Predictions for this season? Mine is a house divided down the middle. My husband and daughter root for the Phillies. I have to admit, on paper, anyway, that their pitching rotation is nothing short of stunning. (Well, the first four. Sorry, Joe.) I don\'t know what to expect from my Yankees this year. A lot depends on A.J. Burnett which is a kind of chilling thought.\nAudrey, thank you so much for your time. She Loved Baseball is such a wonderful book, I\'ll forgive for being a Yankees fan. Huffingtonpost on She Loved Baseball\nVernick first introduces us to a young Effa Brooks in first grade. Effa loved playing baseball with her brothers but wasn\'t allowed because she was a girl and was lighter in skin tone. In 1932 after finishing high school Effa moves from Philadelphia to New York City.\nUpset by the unfair treatment of Blacks. Effa gets involved to make a difference. White store owners were refusing to hire Black workers.\n""She organized the Citizens League for Fair Play, a group of community leaders. They urged Harlem\'s largest department store to hire black salesclerks. The owner said no. Nobody believed a group of Black people could change a White bussinessman\'s mind, but the league fought anyway. For weeks they marched in the street. They convinced their neighbors to shop elsewhere. The store lost money. But still no Black salesclerks. The league kept marching. Finally they won. Newspapers reported the boycotts success.""\nIn 1935 Effa marries Abe Manley. The couple started the Brooklyn Eagles, in the newly formed Negro National League. Effa played a vital roll in the teams sucess, even after they moved to New Jersey in 1936. She always fought for the rights of her players. In 1970, decades after the end of the Negro Leagues, Effa Manley started a letter writing campaign to get some Baseball Hall of Fame to induct some of the best Negro League players.\nWhen I finished this biography, (which I loved, in case that\'s not obvious) my first thought was why, am I just know hearing about Effa Manley. As much as I love baseball and its history, Effa Manley is someone who I should know. And now I do.\nThis was a serious trifecta for me. 1. A woman who loved baseball. 2. a woman who refused to be stop because of her gender or race 3. It bridges the gap between the Negro Leagues and Majors.\nTwo of the players on the Eagles last team were Monte Irvin and Larry Doby. * Vernick also seamlessly includes 1946 Negro League world series between, the Newwark Eagles and the Kansas City Monarchs. Vernick makes the reader feel the excitement of that last game in the series.\nDon Tate\'s colors and style have a very open feel , making them a perfect fit for this story. Tate paid close attention to details from the clothes to the model of the bus the team used. Towards the end there\'s a close up of Effa Manley that\'s simply beautiful.\nWhen I read that in 2006 Effa Manley was the first woman ever to be inducted into the Baseball Hall of Fame, I got a little choked up. Thanks to Vernick and Tate, they did such a great job telling Effa Manley\'s stories. This is a must read for baseball fans of all ages.', ""With the World Series upon us, we turn our attention to the sports world and one of Newark's illustrious figures of the 1930s and 1940s—Effa Manley, the queen of black baseball.\nWhile sports has had a place in Newark from the early part of the last century, it was the past 100 years that illuminated the city's name nationwide. Little or no record remains of the hard-working Puritans or even the colonial farmers taking time off to engage in any sort of recreational pursuits. But with the waves of 19th century immigrants that began to wash onto our shores, people began to take time to engage in sports. In the city's early days, rowing and ice skating were popular sports. But the organized sport which became the lifeblood of 20th century Newark was baseball, for the city had two of the best minor league teams in America, the Bears and the Eagles. This summer the roar of the Bears was heard again, with the opening of Riverfront Stadium, owned by Rick Cerone, who grew up in the North Ward and went on to play with the New York Yankees. The Eagles, unfortunately, were lost to history years ago. Nevertheless, old-time Newarkers still remember the team and its husband and wife owners Abraham Lincoln Manley and Effa Manley.\nNewark had passed through many stages of development by the time baseball became big business in the 1930s and 1940s. The city was a melting pot with phase after phase of immigration that made the city home to the Scotch-Irish, Irish, Germans, Italians, Poles, and central Europeans. It had its vast armies of Catholics and Jews. And women, too, had come into prominence as they were added to the labor force, encouraged to work in its World War I war plants. About the same time, a new wave of Negroes began to settle in Newark, mainly because of its importance as an industrial center. It was a city that always needed a large labor force to operate its factories, warehouses and transportation facilities. Newark also was a town recovering from the sting of the Great Depression that soon would benefit economically from the war effort. All of this also made it a place that required plenty of recreational sources for its working citizens 'to let off steam' or just relax in their spare time. Just as the movie house became important in Newark, so did the ballfields. The time was right and the stage set for Newark's love affair with minor league baseball.\nThe Eagles' daily operations were pretty much evenly divided between the Manleys. Abe was constantly scouting for and developing new talent as well as adding to the mystique of the team within the community. Effa, on the other hand, was the mother hen who ran the office, watched the money and worked on the team's public relations image. She was so proud of always paying the players and creditors on time, as well as being a surrogate parent. After a decade of hard work by its owners and managers, the Eagles' had a banner year in 1946. During that championship season, the team won the Negro World Series, besting the American League's Kansas City Monarchs. By that time, worries about players going off to war were over, and so were problems of transporting the team because of wartime restrictions. Leon Day, Larry Doby, Monte Irvin, Max Manning, Leon Ruffin and Charlie Parker were all back and ready to bring victory. Newark won the title, and so, wrote Overmyer, 'at last, the Eagles were the princes of the kingdom of black baseball.'\nThis triumph sadly also marked the beginning of the end. Black baseball, Newark, and the nation were changing. As Jackie Robinson moved into the world of white baseball, others followed, depleting the storehouse of valuable black league players. The departure from the league of Newark's Larry Doby, Monte Irvin and Don Newcombe devastated the club. The team that drew 120,000 in 1946 saw only 38,000 fans in 1948. By the end of the 1948 season the Manleys disbanded the team. What a situation to find yourself in, and what would you do? Said Effa, 'Our troubles started after Jackie Robinson joined the Dodgers.' She complained that the Dodgers' Branch Rickey was not interested in Robinson, only in the 'clicking of his turnstiles.' 'Baseball,' she said, 'has become a rich man's hobby, and we're not rich.' In her eyes, Brooklyn raided her team without compensation, while the Cleveland Indians paid for new black players, rewarding the Manleys for a lifetime of work and energy. Monte Irvin's contract, for example, was consummated with a check for $15,000. Effa Manley spent part of the money to buy a fur coat. Years later Irvin commented that she had made a good purchase. She responded, 'So did the Giants.'\nAmerican black baseball owes a considerable debt to the Manleys, formerly of 71 Crawford St., Newark, New Jersey. As individuals they represented special qualities that made them unique to Newark. Abe Manley was born in North Carolina in 1885. He later lived in Norfolk, Va., and then moved to Camden, where he was a numbers banker in the black community, and became popular in local black society. In 1930 Abe, by then a man of considerable wealth, moved to Harlem into upper class black society through his involvement in the real estate business. By his mid-40s he was characterized as a 'wealthy retired Camden, N.J. broker.' Effa Manley was born in 1897 or 1902 and died in 1981. During her lifetime, she earned several titles: 'The First Lady of Black Baseball,' 'The Glamour Girl of Negro Baseball,' 'Mother Hen of the Eagles,' 'Princess of Black Baseball,' 'The Leading Lady of Baseball,' and even 'The Beauteous Effa.' Confusion reigns about her early life. Many who think of her as African-American are surprised to learn of her Caucasian parents. She was the daughter of a white seamstress mother, Bertha Ford, and New York City financier John M. Bishop, but elected to spend her entire life in the black world. Her story is further complicated by the fact that her brothers and sisters were fathered by an African-American. Therefore she was considered black by others. Bertha, Effa's mother, was later married to an African-American, with whom she had six children. Effa Manley attended William Penn High School in Philadelphia, and moved to New York City in 1916, where she lived in an apartment house that baseball great Smokey Joe Williams also called home. As a famous early ball player, he apparently helped influence her interest in the sport. After a brief marriage to a Mr. Bush, the couple separated. She later met Abe Manley who became her lifelong love and soul mate. They married and set up housekeeping at 741 St. Nicholas Avenue in the 'Sugar Hill' section controlled by mobster Dutch Schultz.\nHer role as loving wife, baseball team manager, civil rights activist, antagonist of segregation as well as personal rights champion, won her many heartfelt and genuine accolades from rich and poor, black and white, townspeople and suburbanites as well as the high and low of society. In 1939, a Newark News reporter wrote, 'You don't have to go far to pick out baseball's career woman No. 1. Right here in this city we have Mrs. Abraham Lincoln Manley.' He went on to say: 'Effa knows what it is to sit through a victory, squirm through defeat, pore over the ledgers, deal out payrolls, dig up players, make travel arrangements, and handle a mass of important correspondence. She knows players personally, their likes and dislikes, and what they do and say. On the local scene Effa can tell you about her struggles she had getting her club away from the rickety ball lot at Ollemer Field into a first-class park such as Ruppert Stadium.' In 1942, the dean of sports writers, Willie Ratner, stated 'Effa's knowledge of the Negro in baseball is amazing. She can take you back to 1872 when the first colored player of note played on a white team. She could talk endlessly about Sud Fowler and Walter Fleet, who caught for Toledo in the American Association in 1882, and she remembered details about Fran Grant who joined the International League in 1887. In addition all her talents as a baseball magnate, she was a darned good cook.'\nUnfortunately, all good things do come to an end and so did the Newark Eagles. The bittersweet taste of success of Newark's African-American baseball stars moving into the major leagues decimated the home team. At the same time, however, the color barrier had been broken forever. Jim Overmyer perhaps best describes what happened: 'Fans would travel 200 miles to see Jackie (Robinson) play, attendance fell off 50 percent, Larry Doby was sold to the all-white Cleveland Indians and Bill Veeck was to integrate the American League as (Branch) Rickey had the other league. Doby was the team's backbone, and Effa probably saw the writing on the wall. Following a winning year, they lost the pennant and the team finished third in 1948.' Later that year the Eagles were sold to Memphis. The team later relocated to Houston. Everything went—all the team's assets, the player's contracts and even the fancy bus. Effa and Abe turned away from their 'child,' exiting an exciting era forever. Abe died in 1952 at the age of 67 from prostate trouble and uremia. His pallbearers included Monte Irvin and Larry Doby. He was buried at Newark's Fairmount Cemetery. Effa sold their Crawford Street home and briefly moved back to Philadelphia. From there it was on to Los Angeles and a brief marriage. She died in 1981. Her scrapbook has been donated to the Baseball Hall of Fame at Cooperstown, N. Y., but the files of the Eagles were rescued by Eric Adams, the current owner of the Manleys' Crawford Street house. They are now part of the Newark Public Library's Special Collections Department—the New Jersey Information Center—and are available on microfilm. The Manleys are gone, Ruppert Stadium is but a memory and the old Grand Hotel, where so much of the team history was played out, has disappeared. Still, the spirit of 1940s Newark lives on in the minds of many, just as they remember the Velodrome, outdoor concerts in the old School Stadium and the band shell in Branch Brook Park. All have become part of our city's rich history.""]"	['<urn:uuid:d689f2ff-e5bb-44eb-8be8-5d0770403c8a>', '<urn:uuid:a5a38de6-08a1-4272-bca1-983d0d39c25d>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T12:39:32.578853	22	104	2698
