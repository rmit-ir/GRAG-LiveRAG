qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	professional hiker seeking panoramic lookout points palisades cliff top vista locations	Along the Long Path, there are several outstanding viewpoints: Clinton Point offers magnificent views over the river with Yonkers visible across; another viewpoint shows the Henry Hudson Parkway bridge; the Rockefeller Lookout provides fine views of the river with the George Washington Bridge and Manhattan skyline visible to the south; and the High Tom overlook offers outstanding views across the Hudson River and upriver to the north.	"[""From the northern end of the boat basin, proceed north along the white-blazed Shore Trail. The blazing is rather sparse in places, but the route is unmistakable, as it follows a rocky footpath along the narrow strip between the Palisades cliffs on the left and the Hudson River on the right. The trail goes through an area infested with poison ivy, so be sure to wear long pants and avoid...\nFrom the northern end of the boat basin, proceed north along the white-blazed Shore Trail. The blazing is rather sparse in places, but the route is unmistakable, as it follows a rocky footpath along the narrow strip between the Palisades cliffs on the left and the Hudson River on the right. The trail goes through an area infested with poison ivy, so be sure to wear long pants and avoid touching this plant, characterized by its three leaves.\nAlmost immediately, you’ll pass the stone ruins of a bathhouse above and to the left. In about half a mile, the trail reaches a beach and briefly runs adjacent to the Henry Hudson Drive, which descends to the river level here. Just beyond, you’ll pass the ruins of the Undercliff Bathhouse in an open area.\nFor the next three miles or so, the trail runs through a secluded area. New York City is across the river, but all is quiet, except for the waves lapping against the shore and the occasional noise from boats in the river, aircraft above, or trains on the other side of the river. You’ll pass some old stone picnic tables, and at one point you can look up at the cliffs to the left.\nAbout two miles from the start, you’ll cross a wooden bridge and pass a small waterfall, known as Lost Brook Falls. About a third of a mile ahead, you’ll notice a beach and a stone jetty to the right. This is the site of the old Lambier’s Dock. Then, in another third of a mile, you’ll reach Greenbrook Falls, a large waterfall over a rock face to the left of the trail. The trail crosses just below the falls on large boulders.\nHalf a mile beyond Greenbrook Falls, you’ll pass the ruins of a stone jetty to the right, with a beautiful view upriver to the north. Just beyond, a woods road departs to the left. This is the Huyler’s Landing Trail, marked with dark red blazes. Bear left here and follow this trail as it switchbacks up a wide woods road, soon reaching the paved Henry Hudson Drive. Turn left and follow the drive for about 250 feet, then turn right, ascend wood-and-concrete steps, and continue to ascend on a narrower path. The ascent is steady but moderate. As the trail nears the top of the cliffs, it turns right and climbs wide stone steps. Finally, just before reaching the Palisades Interstate Parkway, the Huyler’s Landing Trail ends at a junction with the aqua-blazed Long Path.\nTurn left at this junction and follow the Long Path south, parallel to the Parkway. Although the noise of the traffic on the Parkway can constantly be heard, the Parkway itself is often out of sight. At first, the Long Path follows a grassy woods road, with the blazing a little sparse. It soon dips down, climbs back to the level of the Parkway, and begins to parallel the fence of the Greenbrook Sanctuary on the left. A short distance beyond, it dips again to cross a stream. In another half mile, the trail crosses the paved entrance road to the Greenbrook Sanctuary, which is open to members only.\nOn the other side of the road, the Long Path climbs wooden steps, continues across several wet areas on puncheons (wooden planks), which may be slippery when wet, and crosses the Green Brook on a stone-faced bridge. In three-quarters of a mile, a short side trail on the left leads to Clinton Point – a magnificent viewpoint over the river, with Yonkers visible to the left across the river. There is no fencing here, so use caution and do not approach the edge of the cliffs. In two-thirds of a mile, you’ll reach another viewpoint, with the Henry Hudson Parkway bridge across the river to the right. After passing the stone ruins of the former Cadgene estate, you’ll soon arrive at the Rockefeller Lookout, with fine views over the river. The George Washington Bridge is visible to the south, with the Manhattan skyline beyond.\nHalf a mile south of the Rockefeller Lookout, the Long Path swings to the left, away from the Parkway. Just beyond, as the trail curves to the right, you’ll notice an unmarked path leaving to the left. Turn left and follow this short side trail, which leads to the High Tom overlook, with outstanding views across the Hudson River and up the river to the north. After enjoying this beautiful spot, return to the main trail, turn left, and proceed south.\nYou’ll soon arrive at Palisade Avenue in Englewood. Turn left and briefly parallel the ramp to the northbound Parkway, then turn left again and follow the sidewalk which descends along the park entrance road. You will notice the yellow blazes of the Dyckman Hill Trail, which begins here. Follow the yellow blazes as they turn left and descend stone-and-concrete steps, passing by a waterfall and then going through an underpass beneath the entrance road. The trail then turns right and continues to descend on stone-paved switchbacks and stone steps. When you again reach the park entrance road, cross the road, turn right, then descend steps to the left. Upon reaching the river level, head north until you reach the northern end of the boat basin area, where you parked your car.Publication: Submitted by Daniel Chazin on 07/11/2002 updated/verified on 07/17/2011\nThis hike begins by following the shoreline of the Hudson River and continues along the top of the Palisades cliffs, with many outstanding views.\nWhether you are going for a day hike or backpacking overnight, it is good practice to carry what we call The Hiking Essentials. These essentials will help you enjoy your outing more and will provide basic safety gear if needed. There may also be more essentials, depending on the season and your needs.\nHiking Shoes or Boots\nWater - Two quarts per person is recommended in every season. Keep in mind that fluid loss is heightened in winter as well as summer. Don't put yourself in the position of having to end your hike early because you have run out of water.\nMap - Know where you are and where you are going. Many of our hiking areas feature interconnecting network of trails. Use a waterproof/tear-resistant Tyvek Trail Conference map if available or enclose your map in a Ziplock plastic bag. If you have a mobile device, download Avenza’s free PDF Maps app and grab some GPS-enhanced Trail Conference maps (a backup Tyvek or paper version of the map is good to have just in case your batteries die or you don't have service). Check out some map-reading basics here.\nFood - Snacks/lunch will keep you going as you burn energy walking or climbing. Nuts, seeds, and chocolate are favorites on the trail.\nSunscreen and insect repellent\nRain Gear and Extra Clothing - Rain happens. So does cold. Be prepared for changing weather. Avoid cotton--it traps water against your skin and is slow to dry. If you are wearing wet cotton and must return to your starting point, you risk getting chills that may lead to a dangerous hypothermia. Choose synthetic shirts, sweaters and/or vests and dress in layers for easy on and off.\nCompass - A simple compass is all you need to orient you and your map to magnetic north.\nLight - A flashlight or small, lightweight headlamp will be welcome gear if you find yourself still on the trail when darkness falls. Check the batteries before you start out and have extras in your pack.\nFirst Aid Kit - Keep it simple, compact, and weatherproof. Know how to use the basic components.\nFirestarter and Matches - In an emergency, you may need to keep yourself or someone else warm until help arrives. A firestarter (this could be as simple as leftover birthday candles that are kept inside a waterproof container) and matches (again, make sure to keep them in a waterproof container) could save a life.\nKnife or Multi-tool - You may need to cut a piece of moleskin to put over a blister, repair a piece of broken equipment, or solve some other unexpected problem.\nEmergency Numbers - Know the emergency numbers for the area you're going to and realize that in many locations--especially mountainous ones, your phone will not get reception.\nCommon Sense - Pay attention to your environment, your energy, and the condition of your companions. Has the weather turned rainy? Is daylight fading? Did you drink all your water? Did your companion fail to bring rain gear? Are you getting tired? Keep in mind that until you turn around you are (typically) only half-way to completing your hike--you must still get back to where you started from! (Exceptions are loop hikes.)\nCheck the weather forecast before you head out. Know the rules and regulations of the area.\nThe Leave No Trace Seven Principles\n- Know the regulations and special concerns for the area you'll visit.\n- Prepare for extreme weather, hazards, and emergencies.\n- Schedule your trip to avoid times of high use.\n- Visit in small groups when possible. Consider splitting larger groups into smaller groups.\n- Repackage food to minimize waste.\n- Use a map and compass to eliminate the use of marking paint, rock cairns or flagging.\n- Durable surfaces include established trails and campsites, rock, gravel, dry grasses or snow.\n- Protect riparian areas by camping at least 200 feet from lakes and streams.\n- Good campsites are found, not made. Altering a site is not necessary.\n- In popular areas:\n- Concentrate use on existing trails and campsites.\n- Walk single file in the middle of the trail, even when wet or muddy.\n- Keep campsites small. Focus activity in areas where vegetation is absent.\n- In pristine areas:\n- Disperse use to prevent the creation of campsites and trails.\n- Avoid places where impacts are just beginning.\n- Pack it in, pack it out. Inspect your campsite and rest areas for trash or spilled foods. Pack out all trash, leftover food and litter.\n- Deposit solid human waste in catholes dug 6 to 8 inches deep, at least 200 feet from water, camp and trails. Cover and disguise the cathole when finished.\n- Pack out toilet paper and hygiene products.\n- To wash yourself or your dishes, carry water 200 feet away from streams or lakes and use small amounts of biodegradable soap. Scatter strained dishwater.\n- Preserve the past: examine, but do not touch cultural or historic structures and artifacts.\n- Leave rocks, plants and other natural objects as you find them.\n- Avoid introducing or transporting non-native species.\n- Do not build structures, furniture, or dig trenches.\n- Campfires can cause lasting impacts to the backcountry. Use a lightweight stove for cooking and enjoy a candle lantern for light.\n- Where fires are permitted, use established fire rings, fire pans, or mound fires.\n- Keep fires small. Only use sticks from the ground that can be broken by hand.\n- Burn all wood and coals to ash, put out campfires completely, then scatter cool ashes.\n- Observe wildlife from a distance. Do not follow or approach them.\n- Never feed animals. Feeding wildlife damages their health, alters natural behaviors, and exposes them to predators and other dangers.\n- Protect wildlife and your food by storing rations and trash securely.\n- Control pets at all times, or leave them at home.\n- Avoid wildlife during sensitive times: mating, nesting, raising young, or winter.\n- Respect other visitors and protect the quality of their experience.\n- Be courteous. Yield to other users on the trail.\n- Step to the downhill side of the trail when encountering pack stock.\n- Take breaks and camp away from trails and other visitors.\n- Let nature's sounds prevail. Avoid loud voices and noises.\nThe Trail Conference is a 2015 Leave No Trace partner.\n(c) Leave No Trace Center for Outdoor Ethics: www.LNT.org.""]"	['<urn:uuid:31e42710-0b44-4faf-8621-46ecf5f5c8d2>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	11	67	2063
2	statutory instruments ireland secondary legislation function	Statutory instruments are a form of secondary legislation in Ireland that are not enacted by the Oireachtas. Instead, the power to enact them is delegated to certain people or bodies including Government Ministers, local authorities and regulatory bodies. They must be consistent with primary legislation passed by the Oireachtas and can take the form of ministerial orders, regulations, rules, bye-laws and schemes. Hundreds of statutory instruments are issued each year.	"['Making new laws is one of the main tasks of the Oireachtas. A draft of a proposed new law is called a Bill. Once a Bill is signed into law by the President, it becomes an Act and is added to the Statute Book. The Oireachtas passes approximately 40 Acts each year.\nBefore a Bill can be enacted, it must be passed by both the Dáil and the Seanad. In order to be passed, a Bill must go through several distinct Stages in each House.\nA Bill that is initiated by a TD is debated first in the Dáil. If the Dáil passes the Bill, it is then debated in the Seanad. A Bill initiated by a Senator is debated first in the Seanad and then in the Dáil. Only when both Houses have passed a Bill can the President sign it into law.\nIf the Dáil passes a Bill, the Seanad can delay its becoming law, not stop it. If the Seanad votes not to pass a Bill, the Bill will lapse after 180 days. However, the Dáil has the power, within those 180 days, to pass a resolution declaring that the Bill is deemed to have been passed by both Houses.\nThe Government implements its annual budget by introducing and enacting a Finance Bill and a Social Welfare Bill each year.\nIf the Government wants to repeal or change an existing law, it does so by creating a new Act. For example, the Children Act 2001 repealed several laws including the Children Act 1908, and the Children (Amendment) Act 2015 amended the Children Act 2001 to amalgamate the child detention schools.\nThe Government may also need to introduce legislation to enact European Union laws or decisions by the European courts. For example, the Criminal Law (Human Trafficking) (Amendment) Act 2013 was passed to implement the EU Trafficking Directive.\nThe Government lists the Bills it is bringing forward and the Acts it has passed in its legislative programme.\nPrivate Members\' Bills\nWhile Bills are usually initiated by Government Ministers, Opposition TDs and Senators may also introduce Bills, and these are known as Private Members\' Bills. Senators may not initiate Money Bills or Bills to amend the Constitution.\nThe legislative process\nBefore a Bill\nBefore the Government publishes a Bill, there is usually a consultation process.\nThe relevant Department may publish a Green Paper setting out the Government’s ideas and inviting opinions from individuals and organisations. For example, the Green Paper on Energy Policy in Ireland was launched in May 2014 and 1,200 submissions were made during the consultation process.\nBefore a Bill is finalised, a general scheme of the Bill may be published, and this is often called the heads of the Bill.\nThe relevant Oireachtas committee may invite stakeholders to participate in the pre-legislative scrutiny of a new law by attending committee meetings to discuss the heads of the Bill. For example, during the pre-legislative scrutiny of the Adoption (Information and Tracing) Bill 2015, the Oireachtas Joint Committee on Health and Children heard from adoptive parents, birth mothers, birth parents, foster parents and adoption agencies.\nPassage through the Houses of the Oireachtas\nThe Bill is initiated\nIn general, Bills may be initiated in either the Dáil or the Seanad. However, money Bills and Bills to amend the Constitution must be initiated in the Dáil while private Bills can be initiated only in the Seanad.\nThere are two ways for Members to initiate a Bill. In general, a Government Member may present a Bill, whereas an Opposition Member must seek leave to introduce a Bill. A Bill introduced by an Opposition Member is known as a Private Members\' Bill (PMB) and debates on it must take place during Private Members\' time.\nWhen a Bill is presented, provided it complies with Standing Orders, it is automatically added to the Order Paper and it proceeds to Second Stage.\nWhen a Member seeks leave to introduce a Private Members\' Bill, the House may or may not grant leave. If leave is granted, the Bill is added to the Order Paper and proceeds to Second Stage.\nIn the Dáil, Government Members may present Bills. Other Deputies may also present Bills, but must do so as a group of seven or more Deputies, and each group may present only one Bill at a time. In the Seanad, the Leader of the Seanad may present a Bill on behalf of the Government. Groups of five or more Senators may also present up to three Bills at a time.\nIn the Dáil, any Member may seek leave to introduce a Bill, while in the Seanad, it takes three Senators to introduce each Bill. There is no limit to the number of Bills a Member may seek leave to introduce.\nThe general principles of the Bill are debated\nMembers are allocated a limited amount of time to make a statement on the law the Bill would create. They may also suggest other provisions they would like to be included in the Bill.\nAt the end of Second Stage, the House may, or may not, agree to allow the Bill to proceed to Committee Stage. A Member may call for a division on the question by saying “Vótáil”.\nRead a Seanad Second Stage debate.\nThird Stage – Committee Stage\nThe Bill is examined section by section and amendments may be made\nCommittee Stage is a detailed examination of each section of the Bill and an opportunity for Government and Opposition Members to make changes to the text. Committee Stage in the Dáil is usually taken in an Oireachtas select committee.\nOnce each section of the Bill has been agreed to, the Bill is set down for Report Stage.\nBefore Committee Stage, Members who want to change a Bill may put down amendments, and a list of the amendments is published. During the Committee Stage debate, Members are called on to move each amendment and discuss it with the Minister.\nThere is no limit to the number of times a Member may speak on an amendment, so Committee Stage can be lengthy. The Minister then says whether he or she accepts the amendment. A Member who disagrees with the Minister’s decision on an amendment may call for a division by saying “Vótáil”.\nRead a Dáil Committee Stage debate.\nFourth Stage – Report Stage\nAmendments arising out of Committee Stage are considered.\nReport Stage is the last opportunity for Members to make amendments to the text of a Bill. Members may not bring forward any new amendments, only amendments which arise from Committee Stage. Unlike Committee Stage, Members may speak only twice on each amendment, and their second contribution is limited to two minutes.\nWhen all the amendments have been dealt with, the Bill is received for final consideration. Fifth Stage may be scheduled for another day, but is usually taken immediately after Fourth Stage.\nIf the Government wants to introduce new amendments on Report Stage, it may do so by recommitting the Bill. Effectively this returns the Bill to Committee Stage in respect of an individual amendment.\nRead a Dáil Report Stage debate.\nFifth Stage – Final Stage\nThe Bill is passed by the House\nFifth Stage often follows directly after Report Stage. No amendments may be tabled and there is generally no in-depth discussion of the Bill. Instead, Members usually make short statements on whether the Bill would constitute good law. Then the House agrees to pass the Bill, and a Member may call a division by saying “Vótáil”.\nWhen a Bill passes Final Stage in the House in which it was initiated, it is sent to the other House. There, it must go through the Stages of debate, beginning on Second Stage. Any amendments made by the second House may be rejected by the House in which the Bill was initiated. These amendments are known as the ""cream list"". Therefore, if the second House makes amendments to a Bill, it is returned to the first House, where it goes through some of the Stages of debate again.\nOnce a Bill has been passed on Fifth Stage by both Houses, and all cream list amendments have been dealt with, the Bill is ready to be enacted.\nRead a Dáil Final Stage debate.\nThe Bill is signed into law by the President\nOnce a Bill has been passed by the Dáil and Seanad, the President signs it into law. It becomes an Act and is added to the Statute Book.\nSome Acts come into force immediately, while others are commenced on a later date by the relevant Minister.\nIn certain rare circumstances, the President has the power to decline to sign a Bill. The President consults with the Council of State before taking such a decision.\nAll laws must be compatible with the Constitution. The President may refer a Bill to the Supreme Court to seek a judgment on whether it is constitutional. If the Supreme Court finds that the Bill, or any part of it, is unconstitutional, the President declines to sign the Bill and it is not enacted.\nMembers of the Dáil and Seanad may petition the President not to sign a Bill on the grounds that it contains a proposal of such national importance that the will of the people should be sought. If a majority of the Seanad and at least one third of the Dáil petition the President not to sign a Bill, the President may agree to the request. The President then declines to sign the Bill until the proposal has been approved by the people in a referendum or by a new Dáil after a general election.\nThe Constitution sets out Ireland’s basic laws and describes how the country should be governed, the main institutions of the State and the rights and fundamental freedoms of citizens. All legislation must be compatible with Bunreacht na hÉireann, the Constitution of Ireland.\nIf the Government wishes to change the Constitution, it must hold a referendum which allows the people to vote for or against the proposed change.\nBefore holding a referendum, the Government must introduce a proposal to amend the Constitution in the Dáil as a Bill. The Bill must be passed by both the Dáil and the Seanad. Only then can the referendum take place.\nIf the majority of the votes cast at the referendum are in favour of the change, the Bill is enacted and the Constitution is amended. For example, the 2015 referendum on marriage equality resulted in the enactment of the Thirty-fourth Amendment to the Constitution (Marriage Equality) Act 2015.\nThe Acts passed by the Oireachtas are the primary legislation of Ireland. There is another category of laws known as secondary legislation or statutory instruments.\nThe Oireachtas does not enact statutory instruments. Instead, the power to enact them is delegated to certain people or bodies including Government Ministers, local authorities and regulatory bodies.\nSecondary legislation must be consistent with, and based on, the legislation adopted by the Oireachtas. If it is not, it can be overturned by the courts.\nStatutory instruments can take the form of ministerial orders, regulations, rules, bye-laws and schemes. Hundreds of them are issued each year.\nSome Acts provide for a Minister to decide when to bring them into force. For example, the Paternity Leave and Benefit Act 2016 empowers the Minister for Social Protection to bring it into force. The Minister did so by enacting Statutory Instrument No. 435 of 2016, known as the Paternity Leave and Benefit Act 2016 (Commencement) Order 2016.\nThe European Communities Act 1972 empowers Ministers to make regulations to implement European treaties and directives. For example, the Minister for Health gave effect to Directive 2014/40/EU, known as the European Tobacco Products Directive 2014, by enacting Statutory Instrument No. 271 of 2016, known as the European Union (Manufacture, Presentation and Sale of Tobacco and Related Products) Regulations 2016.\nAn Act can delegate powers to a Government Minister to legislate in day-to-day matters arising from the Act. For example, the Animal Health and Welfare Act 2013 empowers the Minister for Agriculture and Food to make certain animal health and welfare regulations. The Minister used this power to enact regulations on the microchipping of dogs in Statutory Instrument No. 63 of 2015.\nLocal authorities are empowered to make bye-laws relating to anything within their remit by virtue of the Local Government Act 1994. Dún Laoghaire-Rathdown County Council used this power to enact the Parks Bye-Laws 2003.']"	['<urn:uuid:c3ae2b35-2971-44ae-898c-b3388937715d>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	6	70	2072
3	Are Michelangelo's skeletons and Vesalius's skeletons handled differently in art?	Yes, they are quite different. In Michelangelo's work, as seen in Cunego's etching, figures are arranged with tangential connections, showing unclear spatial relationships. In contrast, Vesalius's anatomical skeletons were both animate and anatomically accurate, often depicted mourning their own death and mimicking poses from contemporary paintings.	"[""Thursday, 11 May 2017\nDomenico Cunego’s etching (1781) of Michelangelo’s fresco, “Creation of the Sun, Moon and Plants” (1511)\nDomenico Cunego (1727–1803) et al.\n“Creation of the Sun, Moon and Plants” or as inscribed on the plate: “Germinet terra herbam virentem” (Gen. Cap. I. Vers 11: [God said] “Let the earth bring forth green grass” [and it came to pass]) and “Fecitque Deus duo luminaria magna” (Gen. Cap. I. Vers.16: “God made the two great lights” [the greatest to rule in the day, the least to rule in the night, and the stars]), 1781, from the series of 40 plates first published by Gavin Hamilton (1723–98) in “Schola Italica Picturae” after Michelangelo Buonarroti’s (1475–1564) fresco by on the ceiling of the Sistine Chapel. The British Museum advises that the plates for Hamilton’s publication were later sold to Piranesi in 1780 and passed for publication to Giovanni Volpato (1740–1803). As this plate is dated 1781, I presume that it is from the Volpato edition. (http://www.britishmuseum.org/research/search_the_collection_database/term_details.aspx?bioId=122151)\nEtching and engraving on laid paper trimmed with margins around the image borderline.\nSize: (sheet) 29.1 x 46 cm; (image borderline) 25.3 x 43.3 cm\nLettered below the image borderline: (left) 'Michael Angelus Bonarota in Sacetto Sistine /\nGerminet terra herbam virentem Gen. Cap. I Vers 11”; (centre) “Romae apied Dom Curego; (right) “Dom. Cunego sculp. Romae 1781 / Fecitque Deus duo luminaria magna Gen. Cap. I, Vers.16”\nThe Harvard Art Museums offer a description of this print from a later (?) state (note my attribution to a later state is based entirely upon the font used for the publication details): http://www.harvardartmuseums.org/collections/object/276144?position=4\nCondition: Excellent impression trimmed with margins around the image borderline. The sheet has light age-toning, but is otherwise in good condition (i.e. there are no tears, holes, folds, losses or significant stains).\nI am selling this strong etching by one of the great masters of interpretative printmaking for the total cost of AU$194 (currently US$142.92/EUR131.45/GBP110.60 at the time of posting this listing) including postage and handling to anywhere in the world.\nIf you are interested in purchasing this beautifully executed print based on Michelangelo’s famous fresco, please contact me (firstname.lastname@example.org) and I will send you a PayPal invoice to make the payment easy.\nThis image fascinates me for all the wrong reasons and I had to check the source image—Michelangelo’s fresco—to see whether Cunego had made a mistake … but he hadn’t.\nFor those who believe that the legendary Michelangelo could never make a mistake, please forgive me, but from my way of looking he has made an error of judgement in this composition.\nLet me explain …\nOne of the fundamental rules of achieving spatial depth in a composition is to ensure that key subjects overlap each other to ensure that a viewer “knows” where each subject lies in relation to the others. For instance, Michelangelo presents the figure of God the Father with his hands gesturing in two different directions as being clearly in front of his accompanying angels by God’s form overlapping the small helpers. Beyond this example of overlapping, however, I see many curious points on silhouette edges where forms are tangentially, or almost tangentially, abutted (i.e. lightly “touching” each other) rather than the subject in front overlapping the one further behind. Note for instance how the head of the closer angel accompanying God “touches” the circle of the sun and how God’s hand almost touches the sun. Note also how the closer angel’s finger tangentially abuts the right foot of the flying figure shown on the left—a second view of God the Father. Actually the more that I look at this composition the more uneasy I am with it regarding tangential junctions, as I now see that the tail end of the left figure’s drapery almost touches the sun and that God’s outstretched left arm tangential connects with the head of the angel in shadow on the right.\nOf course, Michelangelo would have very good reasons to dispose the figures in this arrangement anyway that he liked. Moreover, “rules” are meant to be broken (or so I’m told) but, for me, this composition has me wondering what Michelangelo was thinking."", 'Claire Litt //\nMedical sciences study the corporeal aspect of human existence. Since our species’s emergence as anatomically modern humans in the Middle Paleolithic era, we have shared an experiential knowledge of what it feels like to have the body of a Homo sapien as the sensual medium between the physical world and our inner lives. While our biology has, largely, remained unchanged over thousands of years, our intellectual and visual concepts of what a body is have shifted over time and across cultures. A fitting analogy can be found in the writings of Renaissance painters who describe cardinals as wearing purple robes, compared to their paintings of cardinals wearing a colour that, today, is referred to as red. Surely early modern Italians saw the same colour that we see today in the cardinals’ portraits. In medical theory, the human body is the same, but in practice, the body, like the colour purple, is not what it used to be.\nThe modern body, for one thing, is medicalized. Representations of dead bodies are sterilized of emotion and regarded primarily as teaching tools. Gray’s Anatomy reveals inanimate human skeletons, laid out in unnaturally stiff positions, divorced from the people to whom they belong to allow for an empirical, scientific analysis. Western medicalization of the body harkens back to a significant event in the history of medicine: the divergence of the arts and sciences, which had hitherto been one entangled body of knowledge. Analyzing representations of skeletons and cadavers from before the first modern medical textbook, Vesalius’s De Humani Coporus Fabrica (1543), along with anatomical woodcuts from that book, reveals that art traces a diachronic corporeal experience, revealing humans’ self-conceptualization at incremental stages in history. More significantly, these images reveal that art was the means through which radical medical science broke free from standard medical doctrine to develop an anatomically correct understanding of the body. The divergence of art and science was preceded and, paradoxically, precipitated by a Vesalian moment in which they were closer than ever before.\nIn medieval and early modern Italy the most common representation of skeletons were cartoonish, dancing personifications of death. The stark contrast between the levity of these scenes and their grim meaning provided a conceptual framework for death in a culture where it was omnipresent. Published less than a decade before De Fabrica, Han’s Holbein’s Dance of Death showed skeletons that danced, sang, and gleefully made off with their victims. While the gaiety of these scenes’ treatments of death is almost offensive to modern sensibilities, they would have been regarded by many a disgruntled peasant with a sort of schadenfreude. Death was the ultimate equalizer, and it was coming for wealthy nobles just as it was for the common poor. Dancing skeletons proclaimed “As you are now so once was I. As I am now you soon shall be.” Skeletons were not bodies to be studied but reminders of mortality. While in early medical textbooks skeletons were, as they are today, inanimate, they retained a stylistic similarity to the dancing skeletons in popular culture and drew upon their associated meanings, making the text both a teaching tool and a momento mori for its reader.\nVesalius’s anatomical dissections led to radical art that inverted its predecessors’ anatomies: instead of inanimate and inaccurate bodies, his skeletons were both animate and accurate. Vesalius’s bodies are not personifications of death, but rather real dead humans – a visual and scientific reconceptualization of the human body that required both artistic realism and hyper-consciousness to force readers away from orthodox, Galenic theory. The skeleton who mourns his own death projected life into the afterlife, creating a psychological double-removal. There is a desperate hilarity in someone who is already dead being able to grieve, yet its rendering is profound, not funny. These skeletons’ emotions validated the human corporeal experience as meaningful while their accurate anatomy was a testament to experiential knowledge—knowledge gained first-hand through experiments. Experiments, Vesalius affirmed, not Galenic doctrines, were to be the basis of modern science—but, oddly enough, it was this revolutionarily accurate medical text that also spurred the first self-conscious depiction of the death of an individual, as opposed to personified death, in early modern art.\nThe skeletons signaled the value of their human experience through copying expressive iconographic gestures that originated from famous paintings of their time. The skeleton bent over with its head in its hands imitates figures in contemporary paintings on the lamentation of Christ. The “intellectual” lamenting skeleton recalls moment mori or vanities portraits in which the person contemplates, with the aid of a skull or freshly plucked flower, the brevity of life. The Latin inscription on the tomb beneath him reads “Genius Lives Forever, Everything Else Will Die” which was perhaps Vesalius’s intention in writing De Fabrica. While the skeletons were in mourning, the muscular anatomies imitated famous contemporary art. One cadaver points to the sky in an obvious reference to Raphael’s recently completed School of Athens, in which Plato points upwards to the realm of forms. Another cadaver peels off his own skin, mimicking the portrayal of St. Bartholomew in Michelangelo’s Last Judgement in the Sistine Chapel, which is just down the hall from Raphael’s School of Athens. It isn’t hard to draw the connection when only years after these paintings’ completions, Vesalius’s anatomies appear in these exact poses. We are able to read their emotions because they are derive from these artworks and greater cultural understandings of bodily expression.\nHowever, radical art only remains radical for so long. As Vesalius’s science became accepted into mainstream medicine, he no longer required the reader to engage with the text in a hyper-conscious manner. The art itself was replicated, accepted, and de-radicalized. Vesalius continued to make discoveries and correct the text of De Fabrica leading up to its publication in 1543, even while the anatomical woodcuts, which were realized to have errors too, had already been cut and printed. When De Fabrica was republished in 1555 it saw further changes to the text, but only minimal changes to the woodcuts. It was easy to reorder type, and much more expensive to have an artist redo all of the woodcuts. As a result, the text changed, and knowledge (Lat. scienza) advanced. The images became iconographic and, as they became obsolete, they came to represent cutting-edge science instead being that itself. The University of Toronto has a copy of the second addition De Fabrica (1555) that contains over one thousand annotations in Vesalius’s own hand. Unfortunately, Vesalius died, likely in a shipwreck near Greece, before the third edition was complete.\nAnatomical dissection made science more accurate and art more realistic, and allowed for art and science to develop in tandem to their culmination in the De Fabrica. Vesalius required art to help his audience re-conceptualize science. His skeleton and cadavers’ validation of experiential knowledge helped his readership accept his own methodology of experimental anatomical dissections. The paradox is that the use of art for the advancement of science also necessitated their divergence. While experiments are Vesalius’s legacy, art is not. Today’s medical text books revert to pre-Vesalian skeleton; objective and unconscious. Animation and emotion—in short, humanity—are banished. So too is death itself. In modern medical sciences, death is regarded as a topic as objects of rational inquiry and even, according to some scientists, a disease. Just as the body, death been medicalized. In North American Western culture, we have no dance of death and no lamenting skeletons, and so we must ask whether our secular and scientific ethos has made death stranger, more mysterious, and more sacred, than it was for Vesalius’s students.\nGemini, Thomas (After Jan Stephan Van Calcar). “Plate 3 Compendiosa totius anatomie delineatio, aere exarata” London, 1st ed. 1545. British Museum. No. 1982,0515.1. https://www.britishmuseum.org/research/collection_online/collection_object_details.aspxobjectId=1503576&partId=1&searchText=vesalius&from=ad&fromDate=1000&to=ad&toDate=1700&page=1.\nUnknown. 1624-1684. “Dance of Death: the Nobleman, after Holbein” Copy of Hans Holbein’s Woodcut series 1538. 1624-1684. British Museum. 1868,0822.7590 https://www.britishmuseum.org/research/collection_online/collection_object_details.aspxobjectId=1418723&partId=1&searchText=vesalius&from=ad&fromDate=1000&to=ad&toDate=1700&page=1.\nUnknown. 1530. Ein Contrafacter Todt. Augsburg, Germany. British Museum. No. 1874,0711.1982\nVan Calcar, Stephen, and the Workshop of Titian. 1543. “Cadavar Drawing” from Andreas Vesalius’s De Humani Corporis Fabrica. U.S. National Library of Medicine. https://www.nlm.nih.gov/dreamanatomy/da_g_I-B-1-07.html.\nVan der Weyden, Rogier. 1435. “The Decent from the Cross” Oil on oak panel, 220cm × 262 cm. Muse del Prado. http://www.kfki.hu/~arthp/html/w/weyden/rogier/01deposi/index.html\nVesalius, Andreas. 1688-1700. “Opus Typochromaticum” from De Humani Corpus Fabrica. Johannes Teyler. British Museum1871,1209.5124. thtps://www.britishmuseum.org/research/collection_onlinecollection_object_details.aspxobjectId=3577452&partId=1&searchText=skeleton&from=ad&fromDate=1000&to=ad&toDate=1700&page=1.\nKornell, Monique K. B. Roberts, and National Gallery of Canada. 1996. The Ingenious Machine of Nature: Four centuries of art and anatomy. Ottawa, Ontario: National Gallery of Canada.\nNeill, M. Issues of Death: Mortality and Identity in English Renaissance Tragedy. Oxford: Oxford University Press, Cazort, Mimi,\nOlds Clifford C. and Ralph G. Williams, 1976. Images of Love and Death in Late Medieval and Renaissance Art. (The University of Michigan Museum of Art.) plate LVII 14N.\nParks, Katherine. “Introduction to De Humani Corporis Fabrica” Anatomiaitaliana. Acessed April 12 2018. https://anatomiaitaliana.com/wp-content/uploads/2013/05/\nVesalius, Andreas: Andreae Vesalii Bruxellensis, scholae medicorum Patavinae professoris, de Humani corporis fabrica Libri septem. Basileae : [ex officina Ioannis Oporini], [Anno salutis eparatae 1543]. Universitätsbibliothek Basel, AN I 15, http://doi.org/10.3931/e-rara-20094 / Public Domain Mark']"	['<urn:uuid:cf82b494-65ca-48be-994f-0b7880882c91>', '<urn:uuid:81649738-bb6b-4b9e-8b64-65b7c52e1683>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T17:51:21.175311	10	46	2199
4	radon exhalation measurement methods soil suction mitigation techniques comparison effectiveness	Radon exhalation is measured using conventional charcoal canisters and radon emanometers to determine flux densities from different surfaces, while soil suction is the most common mitigation technique that uses a vent fan to pull radon downward and pipe it out of homes. The soil suction method is particularly effective in areas where radon comes from radium and uranium in rocks under the soil, and can successfully lower radon levels below the EPA recommended action level of 4 picoCuries per liter.	['3 Environmental research and monitoring (rehabilitation)\nSupervising Scientist, Darwin, 2005\nISBN 0 642 24395 6\nISSN 0 158-4030\nAirborne dispersion of radon and the subsequent inhalation of radon progeny is the major pathway of exposure of the public to ionising radiation. UNSCEAR (United Nations Scientific Committee on the Effects of Atomic Radiation) reported that radon isotopes contribute to more than 50% of the natural source of radiation exposure and measurements have shown that a dose of approximately 0.7 milliSievert (mSv) per annum is received in Jabiru due to the inhalation of radon progeny. However, it is necessary to distinguish natural and Ranger derived radon. Estimates for the mine-origin radon exposure range from less than 0.01 to more than 0.2 mSv per year at Jabiru.\nIn order to reliably model radon-222 ( 222Rn) concentrations in air following rehabilitation it is necessary to know the 222Rn exhalation source term in the Ranger area. This work was carried out in collaboration with Queensland University of Technology to determine the current source term and its temporal variation, and to develop algorithms that describe 222Rn exhalation in terms of the key characteristics of soils. This information will be used to predict 222Rn exhalation from the site after rehabilitation. The work program consisted of three main parts:\n- Measurements of 222Rn exhalation from surfaces at Ranger during the dry season;\n- Measurements of 222Rn exhalation at various sites over a seasonal cycle;\n- Measurements of 222 Rn exhalation at various sites over several diurnal cycles.\nConventional charcoal canisters and a radon emanometer (Figure 3.4) were deployed in 2002–03 to measure 222Rn flux densities from a total of 317 sites covering the following areas at Ranger uranium mine: mine pits 1 and 3, wasterock dumps, ore stockpiles and both irrigated and non-irrigated land application areas. 222Rn flux densities were highest for the ore stockpiles with fluxes from the laterite stockpile being three orders of magnitude larger than from environmental areas, which are typically 70 milliBecquerels per m2 per second (mBq�m-2�s-1). 222Rn flux densities from the waste rock dumps and the pits are 1–2 orders of magnitude higher, whereas those from the irrigated land application area were only slightly higher than the environmental areas at 112 mBq�m-2�s-1.\nRadon exhalation from soils is influenced by a variety of factors, such as soil moisture, soil porosity and soil radium-226 ( 226Ra) content. As measurements were conducted over the dry season, we were able to investigate the dependency of the 222Rn flux density from soil 226Ra content and soil porosity. Figure 3.5 shows the 222Rn flux density plotted versus the soil 226Ra content for various geomorphic groups: barren (disturbed) areas where compaction has taken place as a result of human influence, vegetated woodland and rehabilitated sites with relatively porous vegetated soils, non-compacted fine grains such as the laterite push zone or overburden zones, and non-compacted boulders. Compaction of the ground and reduction of soil porosity decrease radon exhalation whereas vegetation with established root structures leads to higher exhalation fluxes.\nFigure 3.5 222Rn flux density plotted versus the soil 226 Ra content for various sites\nSeasonal measurements of radon exhalation\n222Rn flux density measurements have been performed over the course of one year at eight sites in the region. Previous studies have indicated that there are large variations in flux between wet and dry season in northern Australia. The influence of soil moisture is the most likely reason for this. Soil moisture profiles (0–1 m) were determined in conjunction with every set of flux measurements at some seasonal sites. The soil moisture data showed large temporal variations throughout the wet season which explains the large variations of 222Rn flux densities during the wet.\nFigure 3.6 shows the annual variation of 222 Rn flux densities at four of the eight investigated sites, and the cumulative rainfall during the time. Generally, radon exhalation in the wet season was largely reduced as soil moisture retarded radon exhalation. However, localised variations were also observed. For instance, some of the sampling sites such as the Mudginberri radon station, Jabiru East or Mirray (Figure 3.6) exhibited a peak in radon exhalation during charcoal cup exposure in January, which is likely to be due to evaporation of soil water and the release of trapped radon after a short but intense rain event.\nFigure 3.6 Annual variation of 222 Rn flux density and cumulative rainfall plotted versus the date\nDiurnal measurements of radon exhalation\nThe aim of this part of the study was to establish whether there was a correlation between radon exhalation, time of day and meteorological parameters such as atmospheric pressure, site and soil temperature. Diurnal measurements were performed at five of the seasonal sites. To investigate whether diurnal radon flux density variations were dependent upon the soil moisture content, measurements were performed during the wet season and during the dry. The results indicate that there is little or no diurnal variation of 222Rn flux density and that a correlation with soil temperature and atmospheric pressure, if any, is masked by random variations of radon exhalation and measurement uncertainty', 'How Radon is Reduced – Bethlehem, Pennsylvania\nHow Radon is Reduced – Bethlehem\nHigh levels of radon gas are a common problem for homeowners in Bethlehem, Pennsylvania. If you’re among those dealing with radon, the U.S. Environmental Protection Agency (EPA) recommends you install a mitigation system to prevent radon from ever entering your home.\nRadon Mitigation: Soil Suction Technique\nThe soil suction technique is the most common method for radon mitigation in Bethlehem. This method is particularly effective because much of the radon in the Bethlehem area is due to the presence of radium and uranium in rocks under the soil. The soil suction technique uses a vent fan under the house to pull the radon downward. Pipes then carry the gas out of the home, where it becomes diluted by the outdoor air and no longer poses health risks.\nSeveral systems are available for soil suction radon mitigation. The type that is right for your Bethlehem home depends mainly upon its foundation type. In addition, the presence of a sump pump can also influence the placement of the components for soil suction mitigation devices in houses located in Bethlehem.\nBenefits of Soil Suction Method\nThe benefit of soil suction radon mitigation systems is that they require only minimal modifications to your existing home. An efficient system can have a profound effect upon radon levels in Bethlehem homes. In fact, the Pennsylvania Department of Environment Protections reports that over 100,000 state residents have already successfully lowered elevated radon levels in their homes to below the EPA recommended action level of 4 picoCuries per liter with such systems.\nCracks in concrete, spaces in brick veneers and loose pipefittings all allow radon gas to leak into residences in Bethlehem; however, the EPA does not recommend fixing these types of problems as the sole method of radon mitigation. Rather, Bethlehem homeowners should make these home improvements to complement soil suction systems or other radon mitigation techniques.\nOpen Windows Not Adequate Radon Reduction Solution\nOpening the windows and doors for ventilation is also not adequate to reduce radon gas levels in Bethlehem homes. Although fresh air dilutes the amount of radon present in the air you breathe, studies show that this only temporarily improves air quality. Radon levels typically return to their former levels within 12 hours after doors and windows are closed once again.\nDrinking Water, Groundwater, Radon Gas\nDrinking water supplies are also sources of radon gas contamination in the Bethlehem area. This is especially true if your home relies on a groundwater well rather than treated public water systems. For Bethlehem homes with high levels of radon in their water, aeration systems are used to reduce levels of the gas. In these systems, air is passed through the water, collecting the radon gas and then venting it to the outdoors. The best aeration systems can remove 99.9 percent of radon from drinking water.\nBecause many factors contribute to the best design for a radon mitigation system in your Bethlehem home, you should enlist the help of a professional. A qualified radon contractor can help you determine what method of radon mitigation is ideal for you.\n- Bethlehem, PA, 18018 USA\nemail@example.com •\xa0610-865-2341']	['<urn:uuid:e42747e2-a0e7-4793-8a39-02e804792ce1>', '<urn:uuid:e62cafe2-81ea-487b-bca8-a4817f319ddb>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	10	80	1373
5	How do social movement stages compare with popular struggles in democratic nations?	Social movements follow four distinct stages: preliminary (awareness and leader emergence), coalescence (organization and publicity), institutionalization (established structure), and decline (when people fall away or goals are achieved). In contrast, popular struggles in democratic nations, as shown by examples like Nepal's democracy movement and Bolivia's water war, don't necessarily follow these sequential stages. These democratic struggles often involve immediate mass mobilization and can achieve their goals through sustained pressure, like strikes and protests, without necessarily going through formal institutionalization. Both types of movements rely on public support and organization, but democratic struggles tend to focus on specific political or social issues and can resolve more quickly when demands are met.	"['Types of Social Movements\nWe know that social movements can occur on the local, national, or even global stage. Are there other patterns or classifications that can help us understand them? Sociologist David Aberle (1966) addresses this question by developing categories that distinguish among social movements based on what they want to change and how much change they want. Reform movements seek to change something specific about the social structure. Examples include antinuclear groups, Mothers Against Drunk Driving (MADD), the Dreamers movement for immigration reform, and the Human Rights Campaign’s advocacy for Marriage Equality. Revolutionary movements seek to completely change every aspect of society. These include the 1960s counterculture movement, including the revolutionary group The Weather Underground, as well as anarchist collectives. Texas Secede! is a revolutionary movement. Religious/Redemptive movements are “meaning seeking,” and their goal is to provoke inner change or spiritual growth in individuals. Organizations pushing these movements include Heaven’s Gate or the Branch Davidians. The latter is still in existence despite government involvement that led to the deaths of numerous Branch Davidian members in 1993. Alternative movements are focused on self-improvement and limited, specific changes to individual beliefs and behavior. These include trends like transcendental meditation or a macrobiotic diet. Resistance movements seek to prevent or undo change to the social structure. The Ku Klux Klan, the Minutemen, and pro-life movements fall into this category.\nStages of Social Movements\nLater sociologists studied the lifecycle of social movements—how they emerge, grow, and in some cases, die out. Blumer (1969) and Tilly (1978) outline a four-stage process. In the preliminary stage, people become aware of an issue, and leaders emerge. This is followed by the coalescence stage when people join together and organize in order to publicize the issue and raise awareness. In the institutionalization stage, the movement no longer requires grassroots volunteerism: it is an established organization, typically with a paid staff. When people fall away and adopt a new movement, the movement successfully brings about the change it sought, or when people no longer take the issue seriously, the movement falls into the decline stage. Each social movement discussed earlier belongs in one of these four stages. Where would you put them on the list?\nSocial Media and Social Change: A Match Made in Heaven\nChances are you have been asked to tweet, friend, like, or donate online for a cause. Maybe you were one of the many people who, in 2010, helped raise over $3 million in relief efforts for Haiti through cell phone text donations. Or maybe you follow presidential candidates on Twitter and retweet their messages to your followers. Perhaps you have “liked” a local nonprofit on Facebook, prompted by one of your neighbors or friends liking it too. Nowadays, social movements are woven throughout our social media activities. After all, social movements start by activating people.\nReferring to the ideal type stages discussed above, you can see that social media has the potential to dramatically transform how people get involved. Look at stage one, the preliminary stage: people become aware of an issue, and leaders emerge. Imagine how social media speeds up this step. Suddenly, a shrewd user of Twitter can alert his thousands of followers about an emerging cause or an issue on his mind. Issue awareness can spread at the speed of a click, with thousands of people across the globe becoming informed at the same time. In a similar vein, those who are savvy and engaged with social media emerge as leaders. Suddenly, you don’t need to be a powerful public speaker. You don’t even need to leave your house. You can build an audience through social media without ever meeting the people you are inspiring.\nAt the next stage, the coalescence stage, social media also is transformative. Coalescence is the point when people join together to publicize the issue and get organized. President Obama’s 2008 campaign was a case study in organizing through social media. Using Twitter and other online tools, the campaign engaged volunteers who had typically not bothered with politics and empowered those who were more active to generate still more activity. It is no coincidence that Obama’s earlier work experience included grassroots community organizing. What is the difference between his campaign and the work he did in Chicago neighborhoods decades earlier? The ability to organize without regard to geographical boundaries by using social media. In 2009, when student protests erupted in Tehran, social media was considered so important to the organizing effort that the U.S. State Department actually asked Twitter to suspend scheduled maintenance so that a vital tool would not be disabled during the demonstrations.\nSo what is the real impact of this technology on the world? Did Twitter bring down Mubarak in Egypt? Author Malcolm Gladwell (2010) doesn’t think so. In an article in New Yorker magazine, Gladwell tackles what he considers the myth that social media gets people more engaged. He points out that most of the tweets relating to the Iran protests were in English and sent from Western accounts (instead of people on the ground). Rather than increasing engagement, he contends that social media only increases participation; after all, the cost of participation is so much lower than the cost of engagement. Instead of risking being arrested, shot with rubber bullets, or sprayed with fire hoses, social media activists can click “like” or retweet a message from the comfort and safety of their desk (Gladwell 2010).\nThere are, though, good cases to be made for the power of social media in propelling social movements. In the article, “Parrhesia and Democracy: Truth-telling, WikiLeaks and the Arab Spring,” Theresa Sauter and Gavin Kendall (2011) describe the importance of social media in the Arab Spring uprisings. Parrhesia means “the practice of truth-telling,” which describes the protestors’ use of social media to make up for the lack of coverage and even misrepresentation of events by state-controlled media. The Tunisian blogger Lina Ben Mhenni posted photographs and videos on Facebook and Twitter of events exposing the violence committed by the government. In Egypt the journalist Asmaa Mahfouz used Facebook to gather large numbers of people in Tahrir Square in the capital city of Cairo. Sauter and Kendall maintain that it was the use of Web 2.0 technologies that allowed activists not only to share events with the world but also to organize the actions.\nWhen the Egyptian government shut down the Internet to stop the use of social media, the group Anonymous, a hacking organization noted for online acts of civil disobedience initiated “Operation Egypt” and sent thousands of faxes to keep the public informed of their government’s activities (CBS Interactive Inc. 2014) as well as attacking the government’s web site (Wagensiel 2011). In its Facebook press release the group stated the following: “Anonymous wants you to offer free access to uncensored media in your entire country. When you ignore this message, not only will we attack your government websites, Anonymous will also make sure that the international media sees the horrid reality you impose upon your people.”\nSociologists have identified high-risk activism, such as the civil rights movement, as a “strong-tie” phenomenon, meaning that people are far more likely to stay engaged and not run home to safety if they have close friends who are also engaged. The people who dropped out of the movement––who went home after the danger got too great––did not display any less ideological commitment. But they lacked the strong-tie connection to other people who were staying. Social media, by its very makeup, is “weak-tie” (McAdam and Paulsen 1993). People follow or friend people they have never met. But while these online acquaintances are a source of information and inspiration, the lack of engaged personal contact limits the level of risk we’ll take on their behalf.\nThink It Over\n- Do you think social media is an important tool in creating social change? Why, or why not? Defend your opinion.\n- Describe a social movement in the decline stage. What is its issue? Why has it reached this stage?\n- alternative movements:\n- social movements that limit themselves to self-improvement changes in individuals\n- reform movements:\n- movements that seek to change something specific about the social structure\n- religious/redemptive movements:\n- movements that work to promote inner change or spiritual growth in individuals\n- resistance movements:\n- those who seek to prevent or undo change to the social structure\n- revolutionary movements:\n- movements that seek to completely change every aspect of society\n- social movement organization:\n- a single social movement group\n- social movement sector:\n- the multiple social movement industries in a society, even if they have widely varying constituents and goals', 'NCERT Solutions for Class 10 Social Science Democratic Politics Chapter 5 - Popular Struggles and Movements PDF Download\nThere are many instances that have shown the world how people have struggled to get what they deserve. The struggles and movements earmark the power of the common. To commemorate such movements in history, Chapter 5 Civics Class 10 syllabus has been depicted. Students should know why people have chosen to revolt and get what they deserve. To understand the depth of all the movements in this chapter, you need to follow the answers in NCERT Solutions for Class 10 Civics Chapter 5. It will become easier for you to prepare the chapter and understand the intention behind adding this content to the syllabus. You can download Class 10 Political Science Chapter 5 solutions PDF file and frame the right answers for the questions in the exercise. With Vedantu’s NCERT Solution free PDF, you can have more flexibility in your study schedule. Subjects like Science, Maths, English,Hindi and Social Science will become easy to study if you have access to NCERT Solution for Class 10 Science, Maths solutions and solutions of other subjects. You can also download NCERT Solutions for Class 10 Maths to help you to revise complete syllabus and score more marks in your examinations.\nLast updated date: 16th Sep 2023\nTotal views: 667.8k\nViews today: 8.67k\nAccess NCERT Solutions for Class 10 Social Science Democratic Politics Chapter 5 - Popular Struggles and Movements\n1. In what ways do pressure groups and movements exert influence on politics?\nAns: Politics is influenced by pressure groups and movements in a variety of ways.\n• They attempt to win public support and sympathy for their cause by conducting information campaigns, organizing meetings, circulating petitions, and so on.\n• By organizing strikes and disturbances, they want to bring the government\'s attention to their demands.\n• They also use lobbying to influence decision-making.\n• The concerns they raise frequently have an impact on the policies of political parties.\n2. Describe the forms of relationship between pressure groups and political parties?\nAns: Political parties and pressure organizations can have a variety of relationships.\n• Politicians and political parties frequently organize and lead pressure groups. Most labour unions and student organizations in India are founded by or linked with one of the major political parties.\n• Political parties can emerge from movements. This is how parties like the DMK and AIADMK were created.\n• Issues highlighted by pressure or movement groups are frequently taken up by political parties, resulting in a shift in the parties\' policies.\n3. Explain how the activities of pressure groups are useful in the functioning of a democratic government.\nAns: Pressure organizations aid in the consolidation of democracy. Putting pressure on the rulers is not a bad thing in a democracy as long as everyone has the opportunity. Governments are frequently subjected to unfair pressure from a small group of wealthy and influential individuals. Pressure organizations play an important role in opposing this undue influence by reminding the government of regular individuals\' needs and concerns.\n4. What is a pressure group? Give a few examples.\nAns: A pressure group is an organization that uses rallies and demonstrations to try to influence government decisions. Pressure groups arise when people with similar viewpoints band together to achieve similar goals. FEDECOR and BAMCEF are two examples of pressure groups.\n5. What is the difference between a pressure group and a political party?\nAns: A pressure group is an organized or unorganized body that attempts to promote its own objectives. They battle to attain a common goal. Political parties run for office in order to gain political power. They have multiple interests, as well as their own ideology. They represent varied interests and each has their unique method of achieving their goals.\n6. Organisations that undertake activities to promote the interests of specific social sections such as workers, employees, teachers, and lawyers are called _____________ groups.\nAns: Sectional interest groups\n7. Which among the following is the special feature that distinguishes a pressure group from a political party?\n(a) Parties take political stances, while pressure groups do not bother about political issues.\n(b) Pressure groups are confined to a few people, while parties involve a larger number of people.\n(c) Pressure groups do not seek to get into power, while political parties do.\n(d) Pressure groups do not seek to mobilise people, while parties do.\nAns: (c) Pressure groups do not seek to get into power, while political parties do.\n8. Match List I (organisations and struggles) with List II and select the correct answer using the codes given below the lists:\n1. Organisations that seek to promote the interests of a particular section or group\n2. Organisations that seek to promote common interests\n3. Struggles launched for the resolution of a social problem with or without an organisational structure\nC.Sectional interest group\n4. Organisations that mobilise people with a view to win political\n9. Match List I with list II and select the correct answer using the codes given below the lists:\nNarmada Bachao Andolan\nAsom Gana Parishad\nSingle issue movement\nFertilizer dealers’ association\n10. Consider the following statements about pressure groups and parties.\nA. Pressure groups are the organised expression of the interests and views of specific social sections.\nB. Pressure groups take positions on political issues.\nC. All pressure groups are political parties. Which of the statements given above are correct?\n(a) A, B and C\n(b) A and B\n(c) B and C\n(d) A and C\nAns: (b) A and B\nPopular Struggles and Movements: Class 10th Civics Chapter 5 Summary\nThis is an excellent chapter to study in the SST Class 10 syllabus. It includes the different kinds of struggles humans did in the form of reform movements to fight for their right and get what they deserve. The first instance is the Nepalese movement for making the country a democracy. They struggled to free the country from the monarchy. As per NCERT Solutions of Chapter 5 Civics Class 10, It did not come to an end very quickly. They had to oppose the decisions made by the contemporary kings and finally get what they demanded. When you study Popular Struggles and Movements Class 10 Solutions, you will find how the entire country came on the streets to demand democracy and a parliamentary constitution.\nThe second part of Ch 5 Civics Class 10 depicts the Bolivian struggle where water was privatized due to the forceful decision implemented by the World Bank. This small Latin American country showed the World Bank the real power of the common people. The labour and human rights’ team maintained a four-day strike. The authority then agreed to negotiate but nothing fruitful happened. If you follow the Class 10 Politics Popular Struggles and Movements NCERT Solutions, you will learn that the Bolivian people again went on strike, forced the government to accept all their demands, and make water, the basic right of all animals, free again. This chapter holds two excellent examples of mass movement due to any conflict with the regulatory authority or the ruling government. It shows how powerful the unity of common people can be that shook the big names holding commendable positions in the world.\nWhy Should You Prefer NCERT Solutions for Class 10 Social Science Civics Popular Struggles and Movement?\nPreviously, we have studied the movements and struggles to earn freedom from colonial rule in India. This chapter is somewhat different from the kind. It shows how a country shook its ruling power when an abrupt decision is taken. The two examples in this chapter show how people should unite when the decisions made by the ruling bodies are not at all up to the mark. In fact, the Nepalese movement shows how people understood they need a democratic constitution, not nepotism of the royal family. Chapter 5 Civics Class 10 NCERT Solutions will help you grab the idea properly so that you can answer the questions accordingly.\nFollowing the NCERT Solutions for Class 10 Social Science Civics Chapter 5 will also help you to prepare the chapter within a short period. You will be able to understand how the answers should be framed so that you can score better in the exams. Class 10 Civics Chapter 5 NCERT Solutions have been framed by the top teachers of Vedantu using simple language so that every student can understand the answers well.\nFAQs on NCERT Solutions for Class 10 Social Chapter 5 - Popular Struggles And Movements\n1. How can you Prepare Chapter 5 Using the Popular Struggles and Movements Class 10 NCERT Solutions?\nOnce you are done reading the chapter, you can proceed to the exercise and follow the Ch 5 Civics Class 10 NCERT Solutions to frame the answers. You can download this solution and practice on your own. In this way, you can prepare this chapter perfectly.\n2. What Does People’s Movement Represent?\nThe two examples used in this chapter show that people know what is best for them. When a decision is made by the supreme body, it should be on the side of the common people. The movements took place when the common people understood what is necessary and good for them. Follow Chapter 5 Political Science Class 12 NCERT Solutions to understand this concept better.\n3. Why Should You Prefer Vedantu for the NCERT Solution of Political Science Class 10?\nVedantu offers the best online study platform for all subjects. Quality answers in the NCERT Solutions for Class 10 Popular Struggles and Movements will make your preparation better.\n4. How are the NCERT Solutions for Chapter 5 of Class 10 Democratic Politics Social Science prepared by Vedantu ?\nVedantu\'s NCERT Solutions for Chapter 5 of Class 10 Democratic Politics Social Science are prepared by highly talented teachers. They prepare these solutions after thorough and meticulous research. These teachers have years of experience teaching Democratic Politics in CBSE schools. They understand the requirements of Class 10 students especially. These maestros prepare these solutions after going through a tireless process of analyzing the CBSE syllabus, Board question papers, and latest guidelines by the CBSE. Only after strict scrutiny, these solutions are published to be used by the students. The solutions or any study material provided by Vedantu is absolutely free of cost.\n5. What will we learn in Chapter 5 of Class 10 Democratic Politics Social Science?\nChapter 5 of Class 10 Social Science, ""Popular Struggles and Movements"" will teach you about how people\'s struggles and movements constraint the leaders of a country. Pressure groups and movements are an indirect way of controlling the leadership of a country. you will go through various examples of movements around the world and how they affected the leadership and its decisions in that country. You will come across examples of Nepal and Bolivia’s struggles.\n6. Can I use the Solutions provided by Vedantu for Chapter 5 of Class 10 Democratic Politics Social Science for board exams?\nYes, you can indeed use Vedantu\'s NCERT Solutions for Chapter 5 of Class 10 Democratic Politics Social Science while preparing for the CBSE Board examinations. In fact, you can replicate these exact solutions in your board examination. Vedantu\'s Solutions for Class 10 students are formulated to prepare the students to write well-suited answers in examinations. Writing well-formed answers that can impress the examiner is key to scoring high marks in the CBSE Board exams. With Vedantu, you can definitely learn the skill of writing perfect answers in no time.\n7. What does Bolivia’s water war teach us according to Chapter 5 of Class 10 Democratic Politics Social Science?\nBolivia\'s water war is a significant example of how struggles do not end with the establishment of democracy. In fact, they are crucial to the working of a democracy. The people of Bolivia successfully protested against the privatization of water in their country. The struggle pressured the government to withdraw the contract for the privatization of water with an MNC. Had the people not protested they would have had to pay an exorbitant amount of money for water. Being a low-income country, this would be disastrous for them.\n8. Give an example of a situation where a political party grew out of a movement from the information in Chapter 5 of Class 10 Democratic Politics Social Science?\nIt happens at times that political parties grow out of popular movements. Asom Gona Parishad was formed when the Assam movement led by students against the ‘foreigners’ came to an end. Political parties like the DMK and the AIADMK in Tamil Nadu have also originated following a long-drawn social reform movement during the 1930s and 1940s.\nAnother recent example is the formation of the Aam Aadmi Party (AAP) which has its origins in the “India Against Corruption” movement of 2011.']"	['<urn:uuid:303f9073-3ba9-4740-8aa1-6e13c0d2a5d7>', '<urn:uuid:595cf126-ef57-47b0-8660-9c4e4b3123e0>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	12	110	3584
6	construction delays cost overruns compare burj al arab dubai ford aircraft carrier	The Burj Al Arab and Ford aircraft carrier had notably different outcomes regarding construction timelines and costs. The Burj Al Arab was completed efficiently, taking just two years for building construction and three years for the foundation work, adhering to its planned schedule despite complex engineering challenges. In contrast, the Ford aircraft carrier experienced significant delays and cost overruns - it was delivered nearly two years late, $2.5 billion over budget, and had ongoing issues with 9 of 11 weapons elevators not functioning. The Burj Al Arab maintained its planned timeline despite challenging foundation work, while the Ford carrier continues to face technical problems years after delivery.	['❝ The $13 billion Gerald R. Ford aircraft carrier, the U.S. Navy’s costliest warship, was delivered last year without elevators needed to lift bombs from below deck magazines for loading on fighter jets.\n❝ Previously undisclosed problems with the 11 elevators for the ship built by Huntington Ingalls Industries Inc. add to long-standing reliability and technical problems with two other core systems — the electromagnetic system to launch planes and the arresting gear to catch them when they land…\nProblems with the elevators add to questions about the Navy’s plan to bundle the third and fourth carriers in the $58 billion Ford class into one contract. It’s part of the service’s push to expand its 284-ship fleet to 355 as soon as the mid-2030s.\n❝ Congress gave the Navy permission for the two-at-once contract in this year’s defense spending and policy bills despite the unresolved technical issues and the lack of a Navy estimate so far of how much money it would save the service…\nSame as it ever was. At least since the end of World war 2.\n10 thoughts on “US Navy took delivery of an aircraft carrier missing elevators to bring bombs to aircraft”\nAnd, to make the temporary elevator operators more efficient…https://www.youtube.com/watch?v=h0HcDmjiIYE\nOkay, that’s one. Only 39 more to go.\nAt this moment, I’m watching an even happier version of the same kind of song a little further along the same coast: 1st half, Bournemouth 1 – 0 Manchester United. Hope it holds up.\nCrap! Martial equalized for Man U.\nU.S. Navy awards $14.9B contract for two Ford-class aircraft carriers (Feb 1, 2019) https://defence-blog.com/news/u-s-navy-awards-14-9b-contract-for-two-ford-class-aircraft-carriers.html According to the U.S. Department of Defense, Huntington Ingalls Industries – Newport News Shipbuilding is awarded the detail design and construction (DD&C) efforts for nuclear-powered aircraft carriers Enterprise (CVN 80) and unnamed CVN 81 under the following contract actions: (1) A $14,917,738,145 fixed-price-incentive-firm target modification to previously awarded contract N00024-16-C-2116 for DD&C efforts for the future USS Enterprise (CVN 80) and unnamed CVN 81. Work is expected to be completed by February 2032.\nSee also: “Would Trump’s border wall cost the same as one and a half U.S. aircraft carriers?” (April, 2017) https://www.politifact.com/california/statements/2017/apr/28/scott-peters/would-trumps-border-wall-cost-same-one-and-half-us/\nMarch 4, 2019: “In the shadow of summit and scandal last week, there was stunning news from the Defense Department: “The Pentagon has decided to cut the aircraft carrier fleet from 11 today to 10,” Breaking Defense reported. “By retiring the Nimitz-class supercarrier USS Harry S. Truman at least two decades early, rather than refueling its nuclear reactor core in 2024 as planned, the military would save tens of billions on overhaul and operations costs that it could invest in other priorities.” https://www.washingtonpost.com/opinions/2019/03/05/who-is-mothballing-us-carrier-why/?utm_term=.c804c86b385b See also https://breakingdefense.com/2019/02/pentagon-to-retire-uss-truman-early-shrinking-carrier-fleet-to-10/ (note also “operating the carrier costs about $1 billion a year\n“On Costliest U.S. Warship Ever, Navy Can’t Get Munitions on Deck” (Bloomberg July 30, 2019)\nhttps://www.bloomberg.com/news/articles/2019-07-30/flawed-elevators-on-13-billion-carrier-miss-another-deadline Navy Secretary Richard Spencer said in January that he told President Donald Trump to fire him if the service couldn’t fix the weapons elevators by July. Instead, Trump praised the Ford as “phenomenal” on July 22.\nThe elevators aren’t the only issue plaguing the $13 billion ship, which has had problems with two other core systems — the electromagnetic system to launch planes and the arresting gear to catch them when they land.\nSenate Armed Services Chairman James Inhofe on Wednesday questioned Vice Admiral Michael Gilday, Trump’s nominee for chief of naval operations, about the vessel’s progress during his confirmation hearing.\n“The ship was accepted by the Navy incomplete, nearly two years late, two-and-a-half billion dollars over budget, and 9 of 11 weapons elevators still don’t work with costs continuing to grow,” Inhofe said.\nNavy Secretary Richard Spencer on Wednesday called Rep. Elaine Luria’s criticisms about the delays affecting the USS Gerald Ford aircraft carrier “disparaging” and not helpful.\nhttps://warisboring.com/navy-secretary-spencer-fires-back-at-congresswomans-criticism-of-uss-gerald-ford-delays/ “I look at her and other leadership on [Capitol] Hill who continually disparaged the Ford as a program, and I get a little upset,” he said during a discussion at the Brookings Institution, a Washington, D.C., think tank. “You could not ask for a better disinformation program for our competitors. And I truly mean that.”\nTuesday during a House Armed Services Committee subpanel on readiness, Luria, D-Va., questioned the Ford’s costs and delays, including the ship’s troubled elevators. She called the Ford — the newest aircraft carrier being constructed by the Navy — “a nuclear-powered floating berthing barge that’s not deployable because of the aircraft elevators and the yet untested dual-band radar, catapults and arresting gear.”\nOn Wednesday, Luria issued a statement responding to Spencer’s comments. She pointed out that Spencer had made a promise to President Donald Trump in January that the weapons elevators on the Ford would be functioning by the end of summer, but they still are not.\n“I find it disappointing that the secretary finds congressional oversight disparaging,” Luria said in the prepared statement. “I have yet to see a detailed plan to fix the multitude of problems with these new technologies. The Navy accepted the design of these systems and accepted the ship in an incomplete state…so it is absolutely my role to question Navy leadership on their current failure to deliver an operational ship to the fleet.”\nAbout $13 billion has been invested in the ship, according the Luria. The ship’s 11 weapons elevators, which are needed to load ordnance on planes, have been a problem for years. The Navy commissioned the ship in July 2017 without any working elevators. The ship’s electromagnetic catapult system for launching aircraft has also faced issues.\nThe U.S. Navy next-generation aircraft carrier, USS Gerald R. Ford (CVN 78), returned to sea for this week for a series of sea trials prior to returning to Naval Station Norfolk, Virginia.\nThe sea trials took place off the U.S. east coast after the aircraft carrier completed a 15-month pierside maintenance period, known as a Post Shakedown Availability (PSA), at Huntington Ingalls Industries’ Newport News Shipbuilding (HII-NNS). The more than $12.9 billion aircraft carrier also had to undergo repairs to its elevator systems and issues with its nuclear propulsion system arising in early 2018.\nFour years after the U.S. Navy’s costliest warship was hobbled by a flaw in its propulsion system, prime contractor Huntington Ingalls Industries and subcontractor General Electric are still haggling over who will pay for fixing the defect.\nThe $13 billion USS Gerald R. Ford was forced to return to port during post-delivery sea trials in early 2018 after the failure of a main thrust bearing, a key propulsion system component that’s made by GE.\nThe Ford returned to sea for additional trials after the damage was contained. Now, the Navy wants to deploy it by midyear on its first operational patrol after years of problems and delays, formally bringing its carrier force to 11.\nThe Navy has declined to say how much it paid Huntington, although in 2018 it asked Congress to shift $30 million from other accounts to start work. https://gcaptain.com/ge-shipyard-unpaid-super-carrier-bill/', 'Burj Al Arab is the world’s tallest iconic and most luxurious hotel. This building has received numerous recognitions around the world, and is mainly known by its design which resembles the shape of a sail boats mast.\nThis 321 meter high building was built on a man-made island only 280 meters from the coast, giving all visitors a 360o view of the bay. Construction began in 1993. Engineers created a surface with a layer of rocks, which is circled with a honey-comb pattern which protects the surface and foundation from erosion. The building contains more than 70 000 m2 of concrete and 9 000 tons of steel. It took only two years to construct the building and three years to construct the foundation on beach sand, making this a geotechnical wonder.8\nGeotechnical Engineering entails to obtain information of the physical properties of the soil and rock on a proposed site known as site exploration. This information combined with the mechanics of soil can assess the risks presented by the site conditions that must be concluded in the design process of foundations, earthworks and retaining walls.\nBurj Al Arab has one of the rarest and most interesting foundations, earthworks and retaining wall phenomena’s. This include building an island 280 meter off coast as a foundation for this 321 meter high sea shore wonder.1 Throughout this report we will look at which soils are found in Dubai, as well as the properties thereof including the construction of this hotel foundation.\nIf you need assistance with writing your essay, our professional essay writing service is here to help!Essay Writing Service\nConstructing the Burj Al Arab hotel\nThe design of the Burj Al Arab has been constructed with a specific geometry which supports the sail boat like design by protecting the building from changing wind loads. The outermost wall of the building has been constructed by the use of PTFE coated fibreglass which contains air gaps at regular intervals. This double curve membrane design is able to withstand wind pressures easily.3\nAdditional cables have been provided onto the structure to prevent any deflection in materials. On the full height of the building expansion joints were provided on the right side of the building to ensure the building can withstand the wind load pressures as well as the horizontal stresses that may occur during construction and operation. The material that was used for this sail boat like structure is not only robust but it also protects the buildings’ interior from the sun by using light defusion.3\nAfter seventy thousand cubic meters of concrete and ninety thousand tons of steel, this great engineering wonder is noted as the heart of Dubai. This spectacular feature of the Burj Al Arabhotel, with its two hundred and two rooms, is located 280 meter off shore from the coast of Dubai and recognised as the best in the world. 2\nConcept Architect: Tom Willis-WrightimagesCAHJ41BO.jpg\nConstruction Engineer: WS Atkins and Partners Overseas – Multidisciplinary Consultancy.\nInterior designers: Khuan Chew, Design Principal of KCA International (London).\nLocation: Jumeirah Beach Road, Jumeirah, Dubai, United Arab Emirates.\nType/Structure: Luxury 7 stars* rating hotel/resort\nSize: 321m x 280m (1,053 ft)\nMedium: steel, glass, cement, steel cables, piles\nFrom: Antonino Terranova. The Great Skyscrapers of the World. Special gatefold edition, page 269-279.\nFigure 1: Burj Al Arab hotel\nDubai’s Soil Profile\nThe topography of Dubai (which lies within the Arabian Desert) is different from the southern portion of the UAE. Its landscape consists of sandy desert patterns consisting mostly out of crushed shell and coral and is clean and white, whereas gravel dominates in the southern regions of the country. 2 When looking at the soil properties of Dubai, it’s weak and will most probably move outwards in the case of any construction on it. See figure 1 Dubai soil map. 3\nStudies also show that Dubai’s possibility of a tsunami is minimal, due to the Persian Gulf water that is not deep enough to trigger a tsunami. Thus Dubai is classified as a stable zone, whereas the nearest seismic fault line is 120 km from the UAE, making it unlikely for Dubai to be hit by a seismic impact.2\nFigure 2: Dubai soil map.\nThe moment when Dubai laid focus on the development of this world wonder, they knew it would be an engineering challenge. Many elements must be taken in consideration in geotechnical engineering to build the world’s 15th tallest building on seabed, where its properties are known as a collapsible soil due to a lack of silt and clay.\nThe collapse phenomenon can be defined as a soil which can withstand somewhat large stresses, with little settlement at low in situ moisture content which will show signs of a decrease in volume and associated settlement with no increase of load if the moisture content rises. Therefore the change in volume goes hand in hand with the change in the soil structure.\nIt is thus evident that a number of conditions need to be met before collapsing begins: 6\nThe soil must have a collapsible fabric in its structure. This is where the specified soil has a high void ratio and yet has relatively high shear strength at low moisture content due to a coating (Colloidal) around each grain.\nPartial saturation is essential. This is where collapse settlement will not occur in soils which are located under the water table.\nIncrease of moisture content. This could be seen as the cause for the collapse to take place. With the increase of moisture the colloidal coating loses its strength and thus forces the grains to a denser state with reduction in void ratio.\nSubjected to an imposed load greater than their overburden pressure before collapse can take place. This is only applied to certain collapsible soils.\nThe typical problem associated with a collapsible soil to a building is that although it is dependent on the increase of the moisture content, collapse can take place years after construction has taken place. 6 Large magnitude settlements can occur beneath lightly loaded structures as well as collapse settlement is regularly localised due to defects in foundation, drainpipe leakage and where ponding occur during rainfall.\nThe engineering properties which most affect the cost of a construction are strengthening compressibility. Both can be enhanced by reducing the voids in the soil. Water must be displaced from the saturated soils in order to reduce the void volume. This can take months if the permeability of the type of soil present is low.\nThe following engineering solutions to the mitigation of the collapse problem are listed below: 6\nAvoidance by stopping the triggering mechanism (increase in moisture). This can be made possible by ensuring that water does not penetrate the collapsing soil horizons.\nDesign for collapse. This could be possible in certain scenarios to design a structure which could withstand the predicted collapse settlement.\nChemical stabilisation. This is to make use of a stabilising agent which could reduce the settlement.\nPiled or pier foundation. This is used only when the soil comes from a transported origin which means that the bedrock is covered with a shallow layer, making it possible to rather build on piers or piles.\nRemoval and compaction. This could be done by removing the collapsible soil to a certain depth and replace it through compacting the removed soil in layers.\nIn situ densification by surface rolling. Surface rolling can be done by making use of an impact or vibrating roller for compaction.\nBeach sand is one of many soils that have a collapsible grain structure, where its surface contains large quantities of calcium carbonate which in more defined terms are remains of microscopic plants and animals that thrive on nutrients in the water surface, where it ultimately settles to the floor.\nThe strength and the behaviour of this soil are thus dependent on the calcareous particles which it contains. These grain particles’ is well rounded due to it being rotated and shaped by the waves and is poorly graded (i.e. having a narrow particle distribution). This contributes to the high void ratio, meaning that the soil is very loose and can be seen as not a good bonding material. 6\nTable 1: Transported soil and possible engineering problem.\nTransported Soil Type\nAgency of transportation\nProblem to anticipate\nCollapsible grain structure\nTests to be performed on beach sand\nA large amount of data can be generated from soil, but it can all be wasted if the most important step of sampling is not carried out properly. Thus, in order for an analysis to be of significance to a proposed project, it should represent the bulk material of the site. Additionally, soil samples must be taken in abundance and at random, to ensure that the overall characteristics of the soil are effectively represented. See table 2 for properties beach sand.\nThe following tests were used in classifying beach sand (collapsing soil structure):3\nParticle size distribution. This test is performed to measure the particle size distribution of the soil sample by passing it through a set of sieves. This is in order to produce a grading curve for the soil, which is used to find out its classification. The solid particles in a soil can have different shapes and sizes, and these characteristics thus have a significant effect on its engineering behavior. By making use of this test one can clearly note whether the soil is well or poorly graded. As for beach sand it is known to be a collapsible soil due to it having a poorly graded grain structure and affected by an increase in moisture.\nOur academic experts are ready and waiting to assist with any writing project you may have. From simple essay plans, through to full dissertations, you can guarantee we have a service perfectly matched to your needs.View our services\nAtterburg limits. This test makes use of three separate tests namely Liquid limit test, Plastic limit test and Shrinkage limit test. This test is used to determine a relationship between the soils consistency and its moisture content. If the soil has low moisture content, it would aim to break before deformation takes place, whereas if the moisture content is too high, the soil will deform more easily. This test is of great importance due to it having an impact on settlement underneath a proposed structure. The test can be used to distinguish between the presence of silts and clays. This is important as silt has much less cohesion than clay.\nDry density. This can easily be determined in a laboratory by measuring its physical dimensions and by weighing them. The dry density of a collapsible soil lies between 900-1600 kg/m3.\nOedometer test. When a structure is build on a soil it produces settlement due to compression within the soil profile, which depends on the soil’s properties such as self-weight and also the type of load the soil is experiencing. This test makes use of a series of loads in order to measure the corresponding settlement of the soil. By knowing the soil’s stress and strain properties will allow the prediction in settlement and swelling of the soil.\nCollapse Potential test. This test is used to determine the collapse parameters in order to design accordingly. The CP (Collapse Parameters) is given in percentage, to determine the level of severity.\nTriaxial test. This test is similar to the unconfined compression test, except that the sample is surrounded by a waterproof membrane and installed in a pressure chamber (cell). This test is thus performed to estimate the stress and strain parameters of the specific soil.\nPermeability. This test is used to determine the ease of which water can flow through a soil profile, which is important for geotechnical engineers in projects.\nTable 2: Soil properties of silt sand.9\nBulk Density (Mg/ M3)\nDry Density (Mg/ m3)\nLiquid limit (%)\nPlastic limit (%)\nEffective cohesion (kPa)\nAngle of friction (deg)\nConstruction of Burj Al Arab Foundation\nConstructing a building on sea, an artificial island is needed to design and build the foundations. As many elements need to be taken into account to build an off shore structure, it is therefore important to ensure the protection of the foundation. This can only be done by evaluating all apposed loads to a structure. Seafloor stability regards to the bearing capacity and the sliding resistance thereof must be evaluated for static and combined static, operational and environmental (Like horizontal, vertical loading and overturning moments of the environment which have a return period of up to 15 seconds) loads. Structures with more or less a 150 meter depth could experience horizontal loads of 15-35% of the vertical loading, whereas the overturning moment can be ranging from 100 to 500 million kN/m.8 The change in vertical load during a storm can range from 10 – 40 % of the static vertical load. This means that the foundation needs to be strong to be able to obtain these loadings. Luckily these loadings were much less when Burj Al Arab was constructed, due to it only being 7 meters in depth.8IslandConstruction1.jpg\nFigure 3: Piling of the Burj Al Arab hotel\nThe first step in constructing the island was to place 230 concrete piles (see Figure 3), each one 40 meter in length, which was drilled into the sea bed. The foundation is therefore held in place by the friction of the sand and silts along the length of the piles, rather than the conventional bedrock. The surface was then made by using large rocks that were put together in a specific concrete pattern (honey-comb pattern) which serves as a shield to minimise erosion to the foundation.\nMaking the platform on which the building would be constructed, tube files and sheet files were drilled deep into the sea to support various boundary rocks. Once this was completed the sea water was displaced to fill the inside with concrete slabs as seen on table 3. IslandConstruction2.jpg\nThe structure was then surrounded with a temporary concrete structure to protect the island and the base of the structure, which was filled with a concrete plug slab. Lastly the concrete walls was made where the main basement floor of the building was build (See Figure 4).3\nFigure 4: Burj Al Arab Excavated Basement\nTable 3: Foundation of the Burj Al Arab\nThe Burj Al Arab being a geotechnical wonder is due to its size as well as its ability to withstand the environment and the impacts thereof. This building is only carried by a sandy soil which contains broken shells. Its ability under shear strength and pressure is very low, especially since it is located 280 meters of shore. Thus the building stands only on piles which are held into place by just the friction between the soil and the piles, making this project one of the most remarkable foundation types in history. The design of this foundation on this type of soil is breaking barriers in the building industry, making Dubai a leading country in development.\nDue to South Africa not being a first world country it is impossible for us to be compared to a city which encourages ultimate engineering in structural, hydraulic and geotechnical engineering. Therefore we cannot compare the infrastructures of their country with ours. South Africa could always strive to be a first world country by focusing on infrastructure as well as the inequality of societies. This would encourage tourism and affect the economy positively.\nCite This Work\nTo export a reference to this article please select a referencing stye below:\nRelated ServicesView all\nDMCA / Removal Request\nIf you are the original writer of this essay and no longer wish to have your work published on UKEssays.com then please:']	['<urn:uuid:3bfd4f0e-4e91-4025-96e7-13f9d4dc878d>', '<urn:uuid:c03a22c6-b8cb-4901-a631-3e39e83ec2a6>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	12	107	3771
7	I'm curious about how computer facilities are changing to keep up with all the new technology. What are the biggest shifts happening in managing these places?	Data centers are undergoing major transformations driven by several factors. The rise of cloud computing, mobile technology, and social media platforms has led to massive increases in computing power needs and data analysis requirements. This is pushing data centers toward higher density configurations, with power density per rack increasing from 6kW in 2008 to 12kW in 2016, and projected to reach 16.5kW by 2020. To adapt, facilities are moving away from traditional planning methods using basic spreadsheets toward more sophisticated approaches including 3D renderings, virtualization, and cloud computing. They're also implementing modular designs for infrastructure, including power systems, cooling units, and server racks that can be flexibly expanded as needed. Additionally, there's increased focus on data center capacity planning to assess and optimize computing resources, cooling capacity, and power loads while ensuring optimal performance and avoiding wasteful resource usage.	"['Data Center Capacity Planning: Definition, Benefits, and Solutions\nPublished on June 4, 2020,\nWhat is Data Center Capacity Planning?\nPlans are a part of daily life. Not limited to any specific industry, decisions have to be made as a natural part of existence. Leaders, however, are uniquely positioned to not just make decisions but to enable their teams to act according to strategic direction.\nData centers are no exception to this reality. Plans and strategic direction can make tangible differences in not only the data center’s functionality and overall performance but also to the end consumer.\nThis is why data center capacity planning is crucial.\nWhat is data center capacity planning?\nData center capacity planning is a type of planning which assesses the data center to ensure that the center’s workload can be adequately met.\nPractically, data center capacity planning involves a process of assessing the data center—from computing resources, cooling capacity, power load, and more—and then forming and enacting a plan to make sure that all workload demands can be met and even improved upon. As an end result, this planning can reveal areas of improvement and potential dangers which allow leadership to enact plans for data centers to function most optimally.\nWhy is data center capacity planning important?\nPlanning can generally serve to enhance operations of any data center, but its importance is being underscored in light of the shifting demands on both physical and virtual assets. With environmental concerns as well as financial needs, more attention is turning to planning to prepare for the future while deferring the consequences of a lack of planning and ill-used or under-utilized assets and financial resources.\nNot only does this shifting landscape reveal the need for data center capacity planning, but also such critical analysis of server hardware resources and more has multifaceted benefits to businesses and consumers.\nIt can give insight into power and cooling to enable optimal function for the given workload as well as attending to network and storage capacity. Careful planning can make sure that mistakes within performance are acknowledged and avoided while ensuring that mission-critical functions are not only safeguarded but also enhanced.\nFurther, data center capacity planning can eliminate waste and idle assets while decreasing the likelihood of debilitating outages that interrupt business operations.\nHow can data centers improve their capacity planning?\nBasic tools such as spreadsheets traditionally were the most essential assets for planning, but now options such as 3D renderings, virtualization, cloud computing, and even outsourcing has been proffered.\nOne tool stands out as exponentially beneficial. Many have explored the myriad of benefits from DCIM software to give unique insight and solutions into their capacity planning.\nSuch tools aren’t the only resource available to help with capacity planning; often the communication between facilities, IT, and additional business decision-makers and leaders must be strengthened in order to continually trouble-shoot, forecast, and adapt.\nAll in all, selecting tools, enhancing communication, and engaging with data center capacity planning can greatly benefit the data center’s footprint, power consumption, cooling capacity, performance times, load calculations and more.', ""Data center power and cooling strategies for increasing rack power density\nIn recent years, data centers have faced many challenges due to rapid changes in IT technologies and trends. High density data centers have become an issue that enterprises must face.\nAs people have stricter demands for internet services, enterprises must think of ways to satisfy user expectations for 24x7 availability. The number of servers keeps increasing while more computing capabilities and resources are incorporated in smaller and smaller cases. Small Form Factor (SFF), Blade Servers to Hyperscale servers to Composable Infrastructure used to achieve more flexible frames, are all being developed in response to this demand.\nHigher density servers mean higher power consumption is needed for each server. As the world focuses more and more on energy consumption, enterprises have started to treat energy efficiency issues and environmental responsibilities more seriously. Virtualization technology offers solutions for enterprises that allow each server to handle a greater workload so they are utilized to their fullest. At the current stage of the global cloud trend, many enterprises already see “Cloud First” as one of their top strategies. More IT budgets are investing in “cloud” related IT infrastructure and software and some enterprise organizations or IT suppliers even use “Cloud Only” as their main strategy. When servers are densely placed in cloud data centers, it will definitely bring greater maintenance challenges.\nSince the development of mobilization and social media platforms, the data required for massive computing power to analyze and extract has increased rapidly. With the continued development of the Internet of Things (IoT), data collected through sensors will be guided to backend data centers to perform big data analysis. These changes are leading data centers to develop towards high density. With the rapid growth in the density of data center equipment, data centers built along traditional concepts are no longer enough.\nPotential issues from high power density\nAccording to a research report by Colocation America (2014), the power density of a single rack cabinet in data centers was approximately 6kW in 2008, which reached 12kW in 2016. It is estimated that by 2020, the power density of a single rack cabinet in data centers will achieve 16.5kW. For example, when Intel retrofitted two foundries into a green data center with high power density, the power density per rack reached as high as 43kW.\nSource: Colocation America, 2014\nWith the rapid growth of power density per rack in data centers as a leading trend, enterprises must find more effective ways to face the challenges that come with it. For example, high density achieves better space utilization and the response time of system failure is reduced significantly. However, once there is a power failure, the large amounts of heat generated by the equipment cannot be extracted and will result in a server shutdown.\nThe ever-increasing power density has also far exceeded the processing capabilities of most old facilities. In previous years, each rack in a data center was designed for 6kW power density. However, when faced with high density racks of 15kW or above, facilities clearly do not meet requirements. When enterprises use technology that requires massive resources such as cloud computing or big data analysis, they also face expansion problems for the difference between available and needed capacities. In the past data center cooling design assumed that the IT work load was even and well distributed, but the actual operating environment was not so, especially in certain high density rack cabinets. Enterprises are realizing that their cooling capacities are seriously insufficient.\nThe backup power mechanism originally designed for data centers may also disappear due to this deficiency. The original UPS and cooling system designed using N+1 configuration will be forced to become fully operational due to insufficient capacities, and lose their backup functions. In addition, after deploying virtualization solutions, IT staffs can also move virtual machines dynamically. Data center loads will also change due to this and hot spots will become elusive. Power requirements will also change and result in unnecessary shutdowns.\nThe emerging modularization provides higher flexibility\nHigh density data centers still have many potential problems as administrators of data centers face greater pressure. In addition to maintaining an increasingly dense computing environment and improving its availability, they must also reduce cost and increase efficiency. Fortunately, industry professionals are integrating the modularization concept into equipment and products designed for data centers to bring greater flexibility and prepare for future workloads.\nAt the current stage, the main modularization concepts are applied to space and facility designs. Space modularization refers to the use of modules from IT infrastructure, rack cabinets and facilities provided for IT equipment to operate. Each depends on and relates to the other. In practice, the data center spaces at enterprises are used to assess the capacity needs of existing services and future expansion considerations, and are further divided into smaller spaces and viewed as modules. The modularization of facilities refers to the use of modular designs for infrastructure, including power systems such as UPSs, power distribution cabinets, in-row cooling, server racks and cold/hot-aisle containment.\n“The development of IT technology is changing every day, and infrastructure is developing towards ‘microservice’ architectures. Simply put, ‘microservice’ architecture refers to the use of modularization to form complicated large-scale applications. Modular solutions were developed for data center infrastructure years ago to provide flexibility for enterprises responding to expansion needs and to overcome power and cooling insufficiency and space challenges,” said Dr. Charles Tsai, the general manager of Delta’s mission critical infrastructure solutions business unit.\nThe modularization of the UPS system\nAccording to a research conducted by a UPS vendor, approximately 50% of respondents believe that the main cause of power outage at data centers is UPS equipment failure. This shows how important UPS systems are for maintaining data centers. As data centers acquire higher densities, they must replace old UPS systems with efficiency as a major consideration. UPS systems designed ten years ago usually have an efficiency of 85% when operating at 40% load to serve dual power input servers. The energy efficiency of current UPS systems is even greater. Take Delta’s UPS solution for example, under a light load of 20%, the AC-AC efficiency for the DPH 500kVA series UPS can be around 95% and the peak efficiency can be up to 96.5% for obvious energy cost savings.\nAnother consideration is effective space utilization in the data centers. Generally speaking, power rooms plan to install power facilities even though the space of the power room may be very limited. When data centers develop towards higher density in each rack cabinet, backup power must also increase accordingly. Enterprises can replace their legacy UPSs by new generation units with a higher power capacity. For example, Delta recently released the Modulon DPH 500 kVA modular UPS for large data centers that only takes up a space of a 19” rack cabinet and provides the world’s highest power density. The parallel expansion can also be configured up to 8 units, providing a maximum power capacity of 4MVA.\nSince it has self-diagnosis and aging detection mechanisms, it can detect the health of batteries, fan, IGBT module, DC capacitors and AC capacitors for preventive maintenance to reduce the risk of malfunctions and power loss, and protect the customer’s equipment investment.\nFor enterprises, another advantage of modular UPSs is the “plug and play” design of power modules. Either vertical (within a single system cabinet) or horizontal (in parallel) expansion can be achieved per enterprise needs. Enterprises can flexibly purchase UPSs according to their initial capacity needs and count on future operational expansion to further lower CAPEX.\nRowCool systems near hot spots reduce power losses\nAs server and IT equipment densities become higher, the requirements for facilities are also becoming more rigorous. In addition to higher power supply density, cooling has become an issue in data centers. The cooling design of data centers assumes that the IT work load is even and well distributed, but in real enterprise environments, uneven heat distribution may be generated due to dynamic moving of virtual machines or improper deployment of IT equipment.\nInsufficient cooling will become a common challenge that high-density data centers will face. The advantage of RowCool is that it is close to hot spots, and is different from RoomCool systems where losses are generated in the air delivery path under the raised floor. RowCool systems can provide sufficient cooling capacity nearby to eliminate hot spots. They are equipped with high power-saving DC or EC fans with variable fan speed control for more energy savings. A 10% fan speed reduction can save a maximum 27% of energy consumption. In addition, the N+1 backup design is used for the overall architecture and group control functions that are provided to perform linked control for the RowCool units in the area to solve the hot spot problems caused by sudden load increases.\nRowCool systems also have modular designs. For example, RowCool 29/43kW not only has built-in dual power supplies and can significantly increase the reliability and protection for system power; but it also supports hot-swappable power supplies and fans, and can reduce the maintenance time needed. Its variable fan speed control design can adjust fan speed according to the actual work load. The modularization concept is also applied to RowCool units, which can be added to required spots as needed.\nCase Study: Medium and High Density Zones co-exist in a data center\nAccording to the data center density categories of AFCOM, the data center managers' association, a low density environment is when each cabinet is under 4kW, mid density is 5kW-8kW, high density is 9kW-15kW and ultra-high density is 16kW and above. Different data centers may face different rack power density challenges. In some data centers, there might even be a mix of rack cabinets with different densities, but use the same design concepts for power and cooling systems. A leading IC design company in Taiwan plans to build a new data center at their headquarters because they need to integrate their existing IT equipment and related network and operating environments. According to onsite inspection, there are up to 80 mid-density and ultra-high density rack cabinets in this enterprise’s data center, with the ultra-high density cabinets up to 25kW.\nDelta recommends that when enterprises need both low-density and high-density/ ultra-high density rack designs for different application needs, it should divide the data center space into a high-density cabinets area and an ultra-high density cabinets area to optimize its design. Facility planning should not only include overall power and cooling needs, but also space usage for the deployment of IT systems. There are different options for the use of cooling solutions. For example, rack cabinets under 4kW may only need RoomCool for effective cooling, while it is better to equip RowCool units for higher density racks from 9kW to 15kW for optimal heat removal.\nIn this case, Delta used the modular design concept to plan the ultra-high density area for its client, and placed all of the ultra-high density rack cabinets in one zone. For its high density applications, the client chose Delta’s RowCool 95kW with the industry largest cooling capability within a 600mm wide cabinet. Delta also recommended the use of hot-aisle containment technology to prevent the mixing of cold and hot air and ensure optimal cooling efficiency.\nTechnology is developing every day. Emerging IoT, artificial intelligence, AR/VR has integrated cloud, mobile, social media and big data technologies. For enterprises, the challenges for data centers have become more and more difficult. Development towards high-density will definitely continue and infrastructure will become the most important backup when enterprises promote their innovative application services.\nFor data center administrators the development of high-density data centers will definitely bring more maintenance and management issues. The availability of data centers will also be a challenge, while costs must decrease and efficiency must increase. The use of modular designs and related facilities, including space modularization, cold or hot aisle containment technology, RowCool systems and more efficient UPSs, can ensure higher reliability architectures and more deployment flexibility for data centers.\nHow Do I Deploy the Best Back-up Power and Environment Monitoring & Management Systems in a Datacenter?""]"	['<urn:uuid:a1c2830a-ec9d-4688-a207-87d7b113fd2b>', '<urn:uuid:d0fd43e9-4a48-4be7-aa16-1fa00b2dfc77>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T17:51:21.175311	26	139	2532
8	Which location has milder temperatures, Southern Nevada or Clark County?	Clark County has milder temperatures, with an average annual temperature of 51.95°F (ranging from 41.8°F to 62.1°F), while Southern Nevada experiences extreme temperatures including 100+ degree heat that 'does nothing, animal or vegetable, much good.'	"['You could say the average Southern Nevada home gardener has a tough row to hoe.\nExcept, of course, that most home gardeners here would be happy to have anything to hoe after the valley\'s extreme temperatures, bad soil and lack of water have done their worst.\nAnd then there\'s Don Fabbi, a third-generation Nevadan who has mastered the intricacies of desert gardening as well as anyone. In raised plant beds in his yard, Fabbi grows not only the usual veggies -- tomatoes and peppers and the like -- but such exotic-for-here crops as blueberries, peanuts, sugar cane and, even, cotton.\nNo doubt about it: Gardening in the desert is tricky. But it becomes less tricky if gardeners understand a few basic principles.\nMaybe it\'s the economy, and the chance to stretch the family\'s food budget with a few home-grown favorites. Maybe, after recent food scares over spinach and tomatoes, it\'s a desire to make sure the produce we\'re eating is pure. But, whatever the reason, Southern Nevadans are taking to gardening like a tomato hornworm to an Early Girl.\n""Our class attendance has almost tripled, and people are starting to want to go back to, if you will, the green, wanting to produce their own,"" says Linn Mills, Review-Journal gardening columnist and a horticulturist for the Las Vegas Valley Water District at the Springs Preserve.\nAngela O\'Callaghan, area extension specialist for social horticulture for the University of Nevada Cooperative Extension, says more than 50 people have signed up for a class she\'s teaching later this month on basic patio vegetable gardening.\n""I was expecting 15 or 20,"" she says. ""It\'s something people have really grabbed on to.""\nBut gardening here isn\'t like gardening elsewhere. First, there is what, to many newcomers, would seem to be off-kilter seasons. For instance, gardening season here actually began weeks ago, and harvest time generally will come earlier than newcomers would expect.\n""In their mind, they don\'t have to go out and garden until it\'s Mother\'s Day or the first of June,"" Mills says. ""And right now is the ideal time to be planting things like tomatoes, peppers, squash and cucumbers.""\nOn the upside, there\'s still time for newbies to venture into their yards.\n""Anytime in March is good,"" says Frank Rauscher, staff horticulturist for Star Nurseries. ""Some Marches we freeze up to mid-March. This has been a nice warm one.\n""So, hopefully, if you plant in March, you can pretty much plant anything. Then, as you get into April, you\'re going to want plants that are more heat-hardy or make sure your microclimates (in the yard) are more favorable.""\nSpeaking of which: That extreme, 100 degree-plus heat we get in the summer does nothing, animal or vegetable, much good. So, gardeners here have to pay attention to where and when their yards are sunny and shady and plan their gardens accordingly.\nIt\'s vital, O\'Callaghan says, to ""pay attention to the direction your light comes from, and when you get direct light, and if you ever get direct light. If you do, you can grow certain things. If you don\'t, you can only grow other things.""\nRauscher adds that microclimates -- zones of temperature, sun and shade -- can vary throughout a single yard.\nIn addition to that relentless sun and heat, Southern Nevada gardeners must deal with not-so-abundant water.\n""We have to be really frugal (with water), so we try not to grow things that would be horrifically water-wasting,"" O\'Callaghan says. ""But tomatoes will not forgive you if you don\'t water them regularly. Peppers will not forgive you.""\nOne trick is to use a soil that contains a good mix of organic matter. A good fertilizer mix with good compost will hold water but won\'t drown a plant.\nAnd here\'s a desert gardening weirdness: Rauscher suspects most plants here fail not because of lack of water, but because of overwatering.\nO\'Callaghan likes to use ""this analogy of a well-wrung sponge. That\'s what people want their soil, or their compost to feel like. They want it to have that consistency.""\nAnd, Rauscher recommends that home gardeners buy ""a little six-dollar moisture meter"" which, with a bit of knowledge about how to use it, can save water, save plants and ""probably quadruple success.""\nAbout our soil: It is, Mills says, ""virtually dead.""\nSo, in-ground gardens must be tended and prepared carefully. Or, an easier, and controllable option: Use prepared soil in either raised garden beds or containers.\nMills notes that container gardening is perfect for condos, small yards or anywhere else where space is at a premium.\n""There are a lot of vegetables that look good in containers,"" Fabbi says, among them tomatoes and strawberries, and even citrus trees with small root systems such as lemons.\nAn added benefit of container gardening is that the containers can be moved to maximize or minimize the sunlight plants in them receive.\nContainer or raised beds also offer beginning gardeners a fairly inexpensive, usually high-success way to dip a toe into horticultural waters. So, O\'Callaghan says, can a few potted herbs grown on a patio or on a kitchen windowsill.\nFrom there, a beginner might experiment with kale and lettuce -- ""Make it a leaf lettuce. You don\'t want to try to grow iceberg lettuce here. There\'s little flavor and no nutrition. Why would you bother?"" -- and, ""if you\'re patient,"" some tomatoes from seedlings.\nMost of all, take it slowly and don\'t panic. Southern Nevada gardeners are blessed with multiple educational resources, including Nevada Cooperative Extension (the booklet, ""Becoming a Desert Gardener,"" includes a planting guide for an entire crop of veggies) to the Springs Preserve (as well as Mills\' R-J columns), to the Las Vegas Valley Water District and local nurseries.\nThe most important thing is to just give it a shot.\n""What\'s the worst that can happen to you?"" O\'Callaghan says. ""The worst is that, you have a plant. The best you can hope for is that you have a plant that is going to feed you.""\nBesides, she adds, ""what\'s more terrific than a hobby where you can actually eat the end product?""\nContact reporter John Przybys at firstname.lastname@example.org or 702-383-0280.', 'I’ve been remembering my move up here to Clark County from Oakland CA in 1989. Most people didn’t have home computers then and the Internet was just being developed. I was new to the area, hardly knew a soul, didn’t know my way around, and I was desperate to garden in my new home. I spent a lot of time with library books and maps.\nSo, this is for the newly relocated, or perhaps you are just new to gardening. Here is some information to help you get started gardening in Clark County.\nDid you know that our continent is classified in ‘eco-regions?” Much of Clark County WA is part of the Willamette Valley Eco-Region 3. We are at the northern tip of the Willamette Valley which is characterized by “mild, wet winters; warm, dry summers; forested foothills, alluvial valleys; and fertile soil.” We are further divided and classified as Eco-region 3a Portland/Vancouver Basin. On the eastern outskirts of the county is Eco-region 4a – Western Cascades Lowlands and Valleys.\nThe descriptions on the website give an idea of what was native to the area before urbanization. This is especially useful if you plan to include native plants in your landscaping.\nThe USDA zone for Clark County ranges from 7a -7b (foothills) to 8a – 8b (valley/flatlands). If you are familiar with the Sunset Climate zones we are Sunset zone 6 which includes the Willamette and Columbia River Valleys.\nAlthough we sometimes get a late frost in May, mid to late April is generally the last frost for the area. Early to mid October is listed as the first frost date although the last couple of years it was mid November on the west side of Vancouver.\nAccording to US Climate Data the average rainfall for Vancouver is 41 inches. This has been consistent over the years. Average snow: 2 inches. This has been typical in the past but the last couple of winters has seen more snow on the valley floor although it didn’t last more than a few days. Higher elevations of course will see more snow.\nTemperatures listed for Vancouver: annual high: 62.1, annual low: 41.8, with the average temp. as 51.95. Our summers have definitely been warming up with 100-degree days for a few days at a time. Enough to consider drought resistant plants for a summer garden.\nThe degree of cold or heat depends on where you live in Clark County. I find the west side, and downtown Vancouver, to be warmer and less windy than the east side. The east side is impacted more by the influence of the Columbia River Gorge with icy conditions in winter and the warm dry wind in late summer.\nSoils vary so it’s a good idea to have a soil test done by a local lab. Contact the WSU Master Gardener Extension for information on this. In the meantime you can also get an idea from the USDA Web Soil Survey. The information is based on data gathered over the years and will help you understand the existing soil conditions for your area. Follow the directions in the four steps outlined on the website. The result for my street address came out as 68.2% sand, 24.3% silt, 7.1% clay. It was spot on, I do have sandy loam soil which drains fairly quickly. This doesn’t replace an actual soil test done for specific soil quality but I found it very helpful and just plain interesting to know the geology of our area.\nOf course there is nothing like talking to other local gardeners. The folks I go to for that are the WSU Clark County Master Gardeners and the Clark County chapter of the Hardy Plant Society. These folks love to help and encourage other gardeners. See my Resources page for more information.\nWith that, happy gardening in Clark County WA!']"	['<urn:uuid:809561cb-4af2-42ef-ace9-5f56f631ac9b>', '<urn:uuid:24a37c0c-b5c4-4556-aeb0-8ad12da0e371>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T17:51:21.175311	10	35	1669
9	what subjective probability expert weather predictions accurate	According to studies by Murphy & Winkler (1984), weather predictions given by experts as percentage probabilities are highly accurate. When their predictions were plotted against actual weather outcomes, there was a very close match on the calibration curve.	"['The flashcards below were created by user\non FreezingBlue Flashcards.\nWhat is subjective expected utility theory?\n- Savage (1954)\n- EU assumes probability information is available to people (a coin has landed on heads 600 out of 1000 times)\n- Savage argues that, because we do not always know the probability of occurrences, we should allow subjective probabilities to substitute for objective ones\nWhat problems do subjective expected utility face?\n- Are the probabilities confined by the same rules that govern objective ones?\n- Even if they do, they are still not accurate enough to be used prescriptively\nHow is subjective EU used for decision analysis?\n- Raiffa (1968)\n- First your problems are structured, and the possible events that could occur are determined, this prevents\n- Probabilities and utilities are estimated, this can occur directly or through the choice between wagers (failure to include some of the possible events biases subjective probabilities given to those remaining - Fischhoff, Slovic & Lichtenstein, 1978)\n- Finally, each outcome’s product of utility & probability is calculated and the option with the largest sum of these products is chosen\nGive an example of decision analysis using EU\n- Subjective probability for the weather the next day:\n- Sunny: sp=.2,\n- Rain: sp=.4\n- Overcast: sp=.1\n- Showers: sp=.3\n- Utilities for option A (go for a picnic) for the weather conditions: U=100 U=-90 U=0 U=50\n- Utilities for option B (revise) for the weather conditions: (U=-50 U=80 U=50 U=30)\n- Subjective expected utilities for:\n- Picnic: .2(100) +.4 (-90) +.1(0) + .3(50) = -1\n- Revision: .2(-50) + .4(80) = .1 (50) = .3 (30) = 36\n- The option with the largest subjective expected utility is to revise\nWhat is the subadditivity effect?\n- Tversky & Koehler (1994)\n- When people estimate the probability of something and the probability of the single alternative to it, experiments show these estimates do sum to one.\n- But if the single alternative is broken down into several sub-alternatives, they sum to more than one\n- P\'s judged the probability of death from cancer in the United States was 18%, the probability from heart attack was 22%, and the probability of death from ""other natural causes"" was 33%.\n- Other participants judged the probability of death from a natural cause was 58%.\n- Natural causes are made up of precisely cancer, heart attack, and ""other natural causes,"" however, the sum of the latter three probabilities was 73%, and not 58%.\nWhat is the conjunction fallacy?\n- Linda is 31 years old, single, outspoken and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations.\n- Which is more likely:\n- Linda is a bank teller\n- Linda is a bank teller and an active member of the feminist movement\n- People choose both despite the fact that the probability of them happening together is much smaller than the first choice\n- It is argued that they are heuristically biased by the representativeness heuristic (option 2 seems more representative of Linda despite its mathematical unlikelihood\nAre probability estimates accurate?\n- For experts, yes- Murphy & Winkler (1984)\n- People give weather predictions as percentage probabilities\n- Plotted the predictions against the actual weather and it was found that there was a very close match on the calibration curve\n- No, for non experts- Christensen–Szalanski & Busheyhead (1981)\n- Asked doctors to estimate the probability of patients having pneumonia and compared these with medical tests for the disease\n- Data on the calibration curve fell way below the indicated optimal judgments, with doctors overestimating the probability of the patients having the disease\nAre the general public accurate at predicting probability?\n- No- Russo & Shoemaker (1989)\n- P\'s given 10 items of questions they were very unlikely to possess accurate knowledge of, and asked to make a high and low estimate about their values in which they are 90% sure the answer falls in\n- E.g. number of books in the old testament or MLK\'s age at death\n- Found that p\'s usually showed overconfidence despite inaccurate predictions, regardless of training, intelligence or the way confidence is elicited (Lichtenstein, Fischhoff & Phillips, 1982)\n- This has been interpreted as a tendency to be overoptimistic (optimism bias- Weinstein, 1980)\n- Argued by some psychologists such as Juslin that this is an artefact and there were too many questions with which common sensical answers were garnered, leading to a high optimism as this overconfidence is not present when people make perceptual judgments']"	['<urn:uuid:d329522f-e36b-4d5b-af3c-88be70a3b7e3>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	7	38	768
10	want cook using vegetable oils which ones healthiest tips choosing	You should choose unrefined or partially refined vegetable oils. Look for the words 'pressed', 'cold-pressed', or 'expelled' on the label. Unlike refined supermarket oils, these types retain their nutrients, including lecithin, vitamin E, and carotenes.	['Follow these pointers from Madison and your cooking will taste better, with less effort.\nCHOPPING, PEELING, AND JUICING\n1 Bending your head forward while chopping or slicing vegetables can cause strain to build up in the shoulders and neck. Instead of using your eyes as a guide, curve the fingers on your free hand under your knuckles and use your curved fingers as a guide for your hand with the knife. You should glance down occasionally as you work, but for the most part you can hold your head upright.\n2 To peel a tomato, slash a little “X” at the base with a knife, then drop it into a pan of boiling water. When you see the edges of the “X” begin to loosen and roll back (after 10 or 20 seconds), scoop out the tomato, drop it into a bowl of cold water, and then slip off the skin. To seed the tomato, cut it in half around the equator. Hold one-half in hand and pull out seeds with your fingers while squeezing gently.\n3 To extract the most juice from lemons and limes, roll them on the counter while pressing down on them. Then slice in half and juice.\n4 A mortar and pestle is a wonderful way to grind garlic into a thick paste. If you don’t own a mortar and pestle, try mincing garlic with a little salt. The salt breaks down the garlic pieces and keeps them from clinging to the sides of the knife. Work minced garlic and salt into paste with the side of a knife.\n5 A vegetable peeler isn’t always the quickest tool for peeling potatoes, especially knobby ones. Instead, use a sharp knife. Start by cutting a thin slice off either end. Then cradle the potato in your hand and remove skin with five or six swift, long strokes, going from one end to the other. Peel turnips and rutabagas in the same fashion.\n6 Keep two cutting boards in the kitchen and use one for fruits and nuts; use the other for garlic, onions, chiles, and other ingredients with strong flavors and/or odors.\n7 Slice a head of lettuce at its base with a sharp knife and let the leaves fall open. Discard any tough, outer leaves. To prevent bruising, do not twist lettuce. Instead, gently tear large leaves.\n8 Toss greens with a pinch of salt, then add the dressing (or just some oil and then a squeeze of lemon – using your hands to catch any of the lemon seeds). Use your hands; they won’t bruise the greens and you’ll know by touch when there’s enough dressing on them.\nCOOKING WITH TOFU AND SOYMILK\n9 To rid tofu of excess moisture, there is no need for elaborate weighting and pressing. Unless you are deep-frying tofu, cutting the tofu horizontally into slabs and blotting excess moisture with a paper towel is fine.\n10 Most cooks fry tofu to firm up its texture. However, if you prefer not to fry foods, you instead can drop cubes into a pot of simmering water for 5 minutes, drain the tofu, and then use the meaty cubes in stir-fries or curries.\n11 Use soymilk as you would regular milk in cooking and baking; to approximate buttermilk, add 1 tablespoon white vinegar per cup and let stand for several minutes to curdle.\nCHOOSING AND PREPARING HERBS\n12 When using dried herbs, crumble them between your fingers to release their aromatic oils, then inhale to see how much aroma there is. Add more or less dried herb, based on the aroma they release.\n13 Add dried herbs at the start of the cooking process. Fresh herbs are more volatile and are usually best added when cooking is almost completed.\n14 Not all herbs dry well. Chervil, parsley, and cilantro turn flat and grassy, but basil, sage, tarragon, and marjoram retain a fair amount of flavor. Whole or cut-leaf dried herbs are more potent than powdered versions.\n15 Fresh parsley is often quite sandy. To clean, vigorously swish bunch in bowl of water. Spin dry. Then, to remove leaves, grasp stems and hold leaves downward. Shave with large knife and chop.\nBUYING OILS AND NUTS\n16 Choose unrefined or partially refined vegetable oils. Look for the words pressed, cold-pressed, or expelled. Unlike refined supermarket oils, these retain their nutrients, including lecithin, vitamin E, and carotenes.\n17 Buy nuts in shells or skins and make sure they are unroasted. Roasted nuts are often fried and often contain salt, MSG, and sugar. Nuts and seeds are best stored in the refrigerator.\nCOOKING FRUITS AND VEGETABLES\n18 When sauteing, choose a large pan so that vegetables will have room to move around and brown. If crowded into a pan, vegetables will steam and not develop as much flavor.\n19 When salting foods, add some at the beginning of the cooking process – except with beans and large grains, which can toughen – because salt brings out flavors in foods. Add salt to each component, then add just a pinch at the end to fine-tune. Salt added only at the end of cooking doesn’t have time to bring out natural flavors in food.\n20 When sauteing, add garlic toward the end of the cooking time to keep it from burning.\n21 When making stock soups and stews, sauteing onions, carrots, and celery (along with other vegetables) before adding water help bring out their flavor. The more the vegetables are allowed to brown, the more color and depth they contribute.\n22 When making quick stocks, use an abundance of vegetables in proportion to the liquid, for intense flavor in a minimum of time. Pour the contents of the stock pot through a sieve and press firmly to extract as much liquid as possible.\n23 There are several kinds of potatoes – high-starch potatoes for baking and frying, low-starch potatoes for boiling, and all-purpose potatoes that fall in between. To identify your potatoes, slice one with a knife. If the knife is covered with a foamy substance or the potato grabs onto the knife, it has a lot of starch and is best for baking. If not, it’s a boiler. So-so, it’s all-purpose.\nPREPARING GRAINS AND BEANS\n24 To make fluffy grains, place a towel under pot lid once they have finished cooking and let stand for five minutes. The towel, not the grains, will absorb moisture in the covered pot.\n25 Use a double boiler to cook cereals without stirring constantly. Place cereal in top of double boiler, add water, and then place over pot of simmering water and cover. Twenty minutes later, it’s done.\n26 Put cooked pasta directly into pan with sauce, letting some of the cooking water drip into the pan and thin the sauce to the proper consistency for coating the noodles.\n27 When buying dried beans, be sure that the skins are intact, and that the beans are neither chipped nor cracked – signs that they are old and will take longer to cook because they are so excessively dry.\nTOFU IN CURRY-COCONUT SAUCE\nServes 2 to 3\nTraditionally this Vietnamese dish calls for frying the tofu until it is firm and golden. Instead, the tofu can be simmered in water without the use of additional fat. Serve this brothy stir-fry over rice or noodles. (See Tips #1, 9, 10, 12, 16, 17)\n- 1 pound firm tofu, cut into 1-inch cubes\n- 1 tablespoon roasted peanut oil\n- 1 small onion, thinly sliced\n- 1 small bell pepper, cored, seeded, and thinly sliced\n- 2 serrano chiles, minced\n- 1-2 teaspoons Thai curry paste or curry powder\n- 1/2 cup canned unsweetened coconut milk\n- 1/2 cup stock or water\n- 1/3 cup roughly chopped cilantro leaves\n- 3 tablespoons chopped toasted peanuts\n- Bring several quarts of water to boil. Add salt to taste; add tofu and simmer gently for 5 minutes. Remove tofu with slotted spoon, drain on paper towels, and reserve.\n- Heat wok over high heat. Add oil, heat briefly, and then add onion, pepper, and chiles. Stir-fry until vegetables soften slightly, about I minute. Stir in curry, coconut milk, stock, 1/2 teaspoon salt, and tofu. Simmer 2 minutes. Garnish with cilantro and peanuts.\nPER SERVING: 416 calories, 27g protein, 30g fat, 16g carbohydrates, 6.5g fiber, 32mg sodium, 6% vitamin A, 45% vitamin C, 33% calcium\nGREEN SALAD WITH HERBS\nServes 3 to 4\nSalads with fresh herbs have a robust flavor. The idea here is to use whatever herbs are on hand. Parsley and celery leaves, which are always available in markets, are an excellent start. If you can find basil and marjoram, add them as well. (See Tips #7, 8, 13, 15, 16)\n- 2 cups lettuce leaves\n- 2 cups spinach leaves\n- 1 cup purslane or arugula leaves\n- 4 marjoram sprigs\n- 2 tablespoons fresh basil leaves.\n- 1/2 cup celery leaves\n- 1/2 cup fresh flat parsley leaves\n- Extra-virgin olive oil\n- Fresh lemon juice or cider vinegar\n- Carefully sort through leaves removing those that are bruised, then wash and dry well. Tear lettuce, spinach, and purslane into bite-sized pieces. Strip marjoram leaves from stems but keep leaves whole. Tear basil leaves unless already quite small. Keep celery and parsley leaves in fairly large pieces. Place greens in large bowl.\n- Toss everything with a pinch or two of salt, then with just enough oil to coat. Season with lemon juice to taste and serve immediately.\nPER SERVING: 47 calories, 2g protein, 4g fat, 3g carbohydrates, 2g fiber, 172mg sodium, 32% vitamin A, 45% vitamin C, 7% calcium\nPOTATO AND PARSLEY SOUP\nServes 4 to 6\nChoose a mixture of boiling and baking potatoes for this recipe. The russets or other baking potatoes will fall apart and thicken the soup. The firmer red new potatoes will maintain their shape. (See Tips #1, 5, 15, 16, 23)\n- 1 1/2 pounds potatoes, peeled\n- 2 parsley roots if available, scrubbed\n- 1 1/2 tablespoons extra-virgin olive oil\n- 6 shallots or 1 medium onion, finely chopped\n- 2 bay leaves\n- 1/2 cup dry white wine\n- 2 cups chopped fresh flat parsley leaves\n- Salt and ground black pepper\n- 6 cups water or stock, plus more as needed\n- Quarter potatoes and slice thin. Grate parsley roots if using. Heat oil in soup pot and add potatoes, parsley roots, shallots, and bay leaves. Cook over medium heat, stirring occasionally, for 5 to 7 minutes.\n- Raise heat, add wine, and simmer until it is syrupy, about 3 minutes. Add 1 1/2 cups parsley, 1 1/2 teaspoons salt, and water. Bring to boil, lower heat, and simmer, partially covered, until potatoes are tender, about 30 minutes. Stir in more liquid if soup seems too thick, along with remaining parsley, and pepper to taste. Heat through for a minute or two, adjust seasonings, remove bay leaves, and serve.\nPER SERVING: 166 calories, 3g protein, 4g fat, 28g carbohydrates, 3g fiber, 20mg sodium, 27% vitamin A, 71% vitamin C, 4% calcium\nWHITE BEANS WITH EXTRA-VIRGIN OLIVE OIL\nMakes 2 1/2 cups, enough for several servings\nAlmost any bean can be prepared in this fashion, although white beans and chickpeas respond particularly well. Boiling the soaked beans vigorously for ten minutes will help leech out compounds that can cause digestion problems. A piece of dried kelp (kombu) mill also help. (See Tips #1, 4, 15, 16, 27)\n- 1 cup dried cannellini or other white beans, cleaned and rinsed\n- 2 bay leaves\n- 1 small onion, quartered\n- Several sprigs parsley, plus 2 tablespoons chopped fresh parsley leaves\n- 1 large garlic clove, sliced\n- 3 tablespoons plus 1 teaspoon extra-virgin olive oil\n- 1 piece kombu or pinch asafetida, optional\n- 1 1/2 teaspoons salt\n- 1 large shallot, diced, or 3 scallions, including a little green, thinly sliced\n- 1 small garlic clove, minced\n- Ground black pepper\n- Lemon wedges\n- For overnight soak: Place beans in large bowl, cover with at least 4 cups water, and soak for at least 4 hours or overnight. For quick soak: bring beans and 4 cups water to boil; boil hard for 1 minute, turn off heat, cover, and let stand for 1 hour.\n- Drain beans and cover with 6 cups fresh water in large saucepan. Bring to boil, and boil hard for 10 minutes. Skim off any foam that collects on surface. Lower heat and add bay leaves, onion, parsley sprigs, sliced garlic, I teaspoon olive oil, and kombu or asafetida if using. Cover and simmer until beans are partially tender, 30 minutes to 1 hour, depending on length of soak and freshness and size of beans. Add 1 1/2 teaspoons salt and continue cooking until tender but not mushy Remove and discard aromatics with slotted spoon. Pour off broth and reserve for stock\n- Put warm beans in bowl and add shallot or scallions, minced garlic, chopped parsley, and remaining 3 tablespoons olive oil. Toss gently with large rubber spatula so as. not to break beans. Add salt if needed and pepper to taste. Serve with lemon wedges.\nPER SERVING: 130 calories, 4g protein, 8g fat, 12g carbohydrates, 3g fiber, 4mg sodium, 5% vitamin A, 6% vitamin C, 4% calcium\nWINTER PORTOBELLO MUSHROOM STEW\nAlthough mushroom stock delivers the best results, this stew can be made with water. Serve over soft polenta, mashed potatoes, pasta, or rice pilaf. (See Tips #1, 4, 13, 15)\n- 1/4 cup extra-virgin olive oil\n- 1 large onion, cut into 1/2-inch dice\n- 2 teaspoons chopped fresh rosemary\n- Salt and ground black pepper\n- 2 pinches red pepper flakes\n- 1/2 pound portobello mushrooms, stems reserved for stock and caps sliced 3/8-inch thick\n- 1 pound large white mushrooms, stems reserved for stock and caps thickly sliced\n- 2 garlic cloves, minced\n- 3 tablespoons tomato paste\n- 1 1/2 cups Quick Mushroom Stock (recipe follows) or water\n- 1 teaspoon sherry vinegar, or to taste\n- 2 tablespoons chopped fresh parsley or tarragon leaves\n- Heat 1 tablespoon oil in large skillet over medium heat. Add onion and rosemary and cook, stirring occasionally, until onion is lightly browned, about 12 minutes. Season with salt, pepper, and red pepper flakes and remove to bowl.\n- Return pan to medium heat and add half remaining oil. When hot, add portobello mushrooms and saute until nicely browned, about 5 minutes. Add them to bowl with onion and repeat with remaining oil and domestic mushrooms. Return every, thing to pan and add garlic, tomato paste, stock, and vinegar. Simmer until sauce thickens, about 10 minutes. Add parsley and adjust seasonings. Serve.\nPER SERVING: 319 calories, 6g protein, 22g fat, 26g carbohydrates, 5.5g fiber, 30mg sodium, 57% vitamin A, 38% vitamin C, 6% calcium\nQUICK MUSHROOM STOCK\nMakes about 1 1/2 cups\nThe secret of this stock is to brown the vegetables thoroughly for an intense, caramelized flavor. The potent dried mushrooms are a must. Use this stock for any rice dish, stew, or soup that features mushrooms. (See Tips #1, 20, 21, 22)\n- 1/4 cup or more dried porcini mushrooms\n- 2 teaspoons olive oil\n- 1 onion, coarsely chopped\n- 1 carrot, chopped\n- 1 large garlic clove, sliced\n- 2 mushrooms, sliced, plus any trimmings\n- 2 teaspoons tomato paste.\n- 1 tablespoon fresh marjoram leaves or 1 teaspoon dried\n- 1/2 cup dry white or red wine\n- 1 tablespoon flour\n- Salt and ground black pepper\n- 1 teaspoon red wine vinegar or balsamic vinegar\n- Cover dried mushrooms with 2 cups hot water and set aside.\n- Heat oil in saucepan over high heat. Add onion, carrot, garlic, and fresh mushrooms and trimmings. Saute, stirring only occasionally, until well browned, about 10 minutes. Reduce heat to medium, stir in tomato paste, marjoram, and wine, and sprinkle with flour. Cover pan and cook until wine reduces to syrupy glaze, about 3 minutes.\n- Add porcini and their soaking liquid, 1/2 teaspoon salt, a little pepper, and vinegar, and simmer for 20 minutes. Strain, pressing on solids to extract as much liquid as possible. Remove dried mushrooms from strainer and add to stew.\nPER SERVING: 128 calories, 2g protein, 7g fat, 11g carbohydrates, 2g fiber, 12mg sodium, 52% vitamin A, 10% vitamin C, 3% calcium']	['<urn:uuid:617db85d-b9d9-4278-b13c-95a33e8a96b9>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	10	35	2733
11	how many countries freeland foundation work with asian police customs environmental agencies	The Freeland Foundation works with police, customs, and environmental agencies across 14 Asian countries to develop multi-agency task forces and public awareness initiatives addressing transnational organized crime.	['Protect Last Strongholds of Asian Elephants, Tigers & Other Endangered Animals from Wildlife Poaching & Trafficking\nMake a Donation\nProtect the Frontline\nFreeland Foundation trained anti-poaching rangers are on the frontline every day, protecting the animals and local communities of Southeast Asia. In its first Freeland support initiative, Terra Conservation is providing anti-poaching rangers in direct sustainable technology support. Designed in collaboration with Freeland Foundation individuals and teams, Terra Expeditionary Ranger Systems provide necessary equipment and support to these frontline wildlife protectors. Your contributions help to secure critical systems components including solar arrays, energy storage, water filtration, energy efficient lighting, communications, and first aid for Freeland’s anti-poaching rangers.\nThe Freeland Foundation is a frontline counter-trafficking organization creating a more secure world. Its critical mission is to protect vulnerable wildlife—and people—from organized crime and corruption, while revitalizing ecosystems and communities. The Freeland Foundation has organized a global team of law enforcement and development experts who work alongside government officials, local communities, students, and other NGOs across Asia, Africa and the Americas. Known globally as creative risk-takers, Freeland Foundation founder Steve Galster and his team have implemented proven anti-poaching strategies that protect people and the environment from exploitative, illicit trade. These include securing and analyzing wildlife trafficking supply chain data to inform comprehensive response mechanisms, empowering governments and civil societies to dismantle organized crime syndicates, developing and implementing policies to sustain Counter-Wildlife Trafficking, and transforming human behavior.\nMultiple Freeland Foundation programs and activities are achieving historical, quantifiable success in wildlife protection. In 2015, Freeland launched a custom mobile database application, WILDSCAN, that uniquely assists enforcement officers in identifying and tracking poached species. The SURVIVING TOGETHER program collaborates deeply with protected area managers, rangers, and surrounding communities to confront the root causes of environmental degradation head on; Surviving Together is meeting local needs by investing in existing and emerging community leaders to ensure effective installation of sustainable protections systems that can be refined, tailored, and replicated. The PROTECT program provides tailored, comprehensive training packages for rangers and managers in protected areas, dramatically improving security through law enforcement and patrolling in remaining forest reserves. For wildlife crime investigators, the DETECT program offers detailed, theoretical and hands-on forensic training towards securing crimes scenes for the collection and analysis of evidence. You can learn more about myriad other Freeland Foundation programs focused on counter-trafficking, governance and legislation, and disrupting illicit supply chains here.\nThe Wildlife Warrior\nInternational Wildlife Warrior\nSteve Galster has helped develop five organizations, including Global Survival Network (USA), Phoenix (Russia), WildAid, ASEAN-WEN, and the Freeland Foundation. He has served as a Chief of several USAID-sponsored programs between 2005-2012 and now works closely with the ASEAN Secretariat, NGOs, Police, Customs, and environmental agencies of 14 Asian countries to develop multi-agency task forces and public awareness initiatives that address transnational organized crime. His background covers investigative research, media campaigning, and program development relating to counter-human and wildlife trafficking programs in the former Soviet Union, USA, China, Afghanistan, Africa and Southeast Asia.\nAm I contributing to the Freeland Foundation or to the Terra Conservation Initiative?\nYour donation will go to Terra Conservation Initiative to support Freeland Foundation and other TCI conservation organizations.\nIs my contribution tax-deductible?\nYes, Terra Conservation Initiative (TCI) is a program sponsored by Givvor Charitable Foundation, a 501(c)(3) organization.\nHow many rangers is the Terra Conservation helping to support in Thailand?\n500 to start.\nAre the rangers being supported in this initiative focused more on wildlife or human trafficking?\nWhat are the impacts of wildlife trafficking?\nThe impacts of wildlife trafficking are broad and complex, representing a threat to wildlife species, natural habitats, and human beings. The collapse of crucial ecosystems has dire effects on our food and water supplies, while worsening the global effects of climate change. Unregulated trade of wild animals spreads diseases including SARS and Avian Influenza, endangering human health.\nAre there additional ways that I can contribute to the Freeland Foundation?\nYes, you can donate directly to the Freeland Profile on Givvor (https://givvor.com/freeland).']	['<urn:uuid:5fc28076-eb13-4fba-8145-d06c0ed3efc8>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	12	27	664
12	Could you explain the step-by-step process of attaching movable limbs to a cardboard robot puppet and securing them properly for durability?	To attach movable limbs, you first need to carefully line up the limbs and body and make holes at the ends of each limb, ensuring they're not too close to the edge. Then create corresponding holes in the right places on the body. The limbs are attached by sliding split pins through both holes and splitting the pin at the back to hold in place. For added security, it's recommended to put duct tape on top to hold the split pin in place. The puppet handle (dowel stick) should be attached using a hot glue gun and reinforced with duct tape for extra strength. For additional movement capability, a wooden skewer can be inserted into the edge of a cardboard hand to allow arm movement.	['Learn how to create fun robot puppets that actually MOVE using cardboard and other recycled materials in this make and play project for kids by Danielle Falk of Little Ginger Studio.\nAny art teacher will sing the praises of humble ol’ cardboard as a brilliant material for art-making.\nA cardboard box has endless possibilities for re-invention as a fort, rocket, shop or whatever.\nCardboard can easily be cut, glued, painted, decorated and shaped into almost anything little imaginations can come up with. And it lends itself especially well to puppet-making as it is strong but still easy to manipulate.\nAnd kids love puppets.\nYou’ve heard me rave on about how much children enjoy creating and playing with puppets before, and these robot puppets I’m about to show you are no exception.\nIt’s especially fun to create extra-large robots so that children can hide behind their robot puppets and get really lost in role-playing their characters.\nWaste not, want not!\nThis puppet-making activity is mostly made with recycled materials so you can feel good about not purchasing too many materials & being thrifty too! It is suitable for 4 year olds (with some assistance) to 12 year olds.\nHow to Make Cardboard Robot Puppets\n- lots of medium weight cardboard (ie: that is still “cuttable” with scissors). Depending on the age of the children you might wish to pre-cut some into “robot shapes”: squares, rectangles, triangles, long limbs etc.\n- acrylic paint, tempera paint or tempera paint sticks\n- a good quality high temp hot glue gun & glue gun sticks (teacher use only). Low temp mini guns won’t be strong enough.\n- school glue or wood glue & glue sticks\n- RECYCLED MATERIALS! The more the better: bottle tops, champagne bottle cages, jar lids, popsicle sticks, coffee cup holders, cardboard packing materials, cardboard rolls, egg cartons, silver bubble wrap – old nuts & bolts and screws would look amazing too!\n- washi tape, pom poms, pipe cleaners, colored construction paper (for added color) or aluminum foil to add bling!\n- 20 inch long dowel per puppet\n- thick kitchen wooden skewers\n- strong and thick duct tape – I love this brand especially (mostly because of the pretty colors!)\n- lots of split pins and something sharp to make holes in the cardboard (I used a Phillips head screwdriver). I like these long split pins as they are easier for kids to use.\n1. Create the robot body + limbs\nBegin by selecting a large square or rectangle for the body and head shape. A responsible adult will need to attach this with a hot glue gun.\nNext, choose some limbs and attach the hands and feet in the same way. We took our time arranging & re-arranging our pieces until we thought our robots had enough “personality” (Kookiness? Whimsy?).\n2. Decorate the robot\nKids can decorate their robots using paint, paint sticks, paper, washi tape, silver bubblewrap & foil in any way they like. It’s best to direct them towards using smaller pieces & ensure everything is attached properly (with glue).\nHowever in terms of how to decorate their puppets, I take a lot of pleasure in presenting children with a wealth of exciting materials. Then I step right back to watch their creativity in action! I’m always amazed at how innovative kids are.\nThe real magic happens in the second “layer” of decorating when children add details such as:\n- Eyes: coffee cup holders & bottle tops\n- Buttons & Dials: bottle tops & jar lids on the robot body\n- Antennae: pipe cleaners, metals bits etc.\nThese materials will all need to be hot glued on by a responsible adult.\n3. Attach the limbs to the body\nCarefully line up the limbs and body and make holes at the ends of each limb (not too close to the edge). Create holes in the right places on the body as well.\nLine up the holes and attach the limbs by sliding the split pins through both holes and splitting the pin at the back to hold in place. Kids LOVE doing this bit, but will probably need help to press down the pin firmly.\nPro tip: Pop some duct tape on top to hold the split pin in place.\n4. Attach the dowel stick\nOnce again, that handy responsible adult will have to use the hot glue gun to attach the dowel puppet handle. It’s also a good idea to add duct tape here for extra strength.\nPro tip: Be sure the glue dries sufficiently before attempting to use the handle. I also added a wooden skewer (rammed into the edge of a cardboard hand so kids can move one of the arms).\n5. Play with the robot puppets\nIt’s playtime! Create some silly robot voices, have fun moving the robot puppets arms and maybe make a funny Robot Puppet Show.\nPin It For Later']	['<urn:uuid:385f0e34-985c-4b40-a93b-c75ed107263e>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	21	125	817
13	workplace education expenses social impact	Workplace education has both private and social impacts. On the private side, businesses incur costs for training programs but gain benefits like increased productivity and better-skilled workers. The social benefits extend beyond the company, as trained workers can earn better wages at other firms, creating external benefits for society. Additionally, proper training leads to safer workplaces, preventing accidents and protecting employee health, which benefits both the company through avoided costs and society through reduced workplace incidents. Training is especially crucial in high-risk industries like construction, where workers are more prone to injuries.	['Social Costs and benefits\nEvery business activity which takes place has some benefits and costs attached to it. The benefits go both to the owners of the firm as well as to external stakeholders. In the same way the owners and the external stakeholders have to pay a cost for the activities of the business.\nSocial cost is the sum of private cost and external cost. For example, the manufacturing cost of a car (i.e., the costs of buying inputs, land tax rates for the car plant, overhead costs of running the plant and labor costs) reflects the private cost for the manufacturer. Water or air is also polluted as part of the process of producing the car, This is an external cost borne by those who are affected by the pollution or who value unpolluted air or water. Because the manufacturer does not pay for this external cost, and does not include this cost in the price of the car. The air pollution from driving the car is also an externality produced by the car user in the process of using his good. The driver does not compensate for the environmental damage caused by using the car.\nSocial-Cost is the cost to an entire society resulting from an event, an activity or a change in policy. Social cost equals the sum of private cost and external cost.\nWhen assessing the overall impact of its commercial actions in terms of social costs, a socially responsible business operator should take into account its own production expenses, as well as any indirect expenses or damages borne by others.\n- Private cost:-\nIt is the cost of setting up the business. The owner(s) pay for the hire of machinery, buying of materials, payments of wages. This is termed as Private Cost.\n- External Cost:-\nThe problems that the external stakeholders have to bear due to the firm’s activity are known as external cost. Example: cleaning a river which has been polluted by a firm’s waste products. Private firms usually ignore external cost.\nSocial benefits are the sum of private benefits and external benefits. For example, a college decides to slash its tuition rates by half. This encourages more people to get educated. A better-educated workforce, in turn, helps businesses produce more. Thus, even though the businesses did not pay for the reduced college tuition, they still reap a positive external benefit from the college’s move. The increase in the welfare of a society that is derived from a particular course of action. Some social benefits, such as greater social justice, cannot easily be quantified.\nSocial benefits is the sum of private benefits and external benefits\n- Private benefit:-\nThe benefit enjoyed by those involved in the production or consumption . For example, the revenue earned by the firm is a benefit for the owner and is termed as Private benefit.\n- External benefits:-\nSome firms can cause external benefits. These are the benefits to the external stakeholders due to the activity of firm. For example, a firm may train workers, which might get them better wages in other firms. These external benefits are free.\nUse of cost benefit analysis in decision making\nMost of us are familiar with the term ‘cost-benefit analysis’ and have a basic grasp of it. It refers to how a project or decision might be evaluated, comparing its costs with its benefits. In many cases, it’s a like a quantified pros-and-cons list… It’s an analysis of the expected balance of benefits and costs… Cost-benefit analysis sometimes called ‘benefit–cost analysis’ is a systematic process for calculating and comparing benefits and costs of a project, decision, government policy… CBA has two purposes:\n- Determine if it’s a sound investment/decision (justification/feasibility).\n- Provide a basis for comparing investments, decisions, projects… It involves comparing the total expected cost of each option against the total expected benefits, to see whether the benefits outweigh the costs and by how much…\nAccording to Nicole Gordon; cost-benefit analysis is used to decide if the cost of a solution and the economic benefits that would result from it are worth the risk. The main idea behind this strategy is that the benefits must exceed costs to justify the policy…\nWhen performing a cost-benefit analysis, you make a comparative assessment of all the benefits you anticipate from your project and all the costs to introduce the project, perform it, and support the changes resulting from it. Cost-benefit analyses help you to:\n- Decide whether to undertake a project or decide which of several projects to undertake.\n- Frame appropriate project objectives.\n- Develop appropriate before and after measures of project success.\n- Prepare estimates of the resources required to perform the project work.\n- Everything gets a dollar value in a cost-benefit analysis.\nThere are some advantages and disadvantages of cost and benefit analysis.\n- The main advantage of cost benefit analysis is its simplicity. You are simply looking at whether benefits outweigh costs. When you do this quantitatively, measuring the dollar amount of the benefits and the costs involved in a project, the cost benefit is very easy to see.\n- The simplicity of cost benefit analysis can paradoxically lead to complications; to gain this simplicity, you have to use a common measurement– one of the disadvantages of CBA. Determining the quantitative benefits of a project is relatively straightforward; you basically add up the costs and benefits and compare the two. However, when you factor in qualitative benefits, the picture can become more complicated.\nYou are considering implementing an employee bonus program, you will obviously incur costs. In exchange, you may receive benefits like increased employee satisfaction, decreased turnover and greater productivity. The benefits are significant but difficult to compare– apples to apples– to the costs involved… A frequently made mistake is the use of non-discounted amounts for calculating the costs and benefits; typically the cost is tangible– hard and financial– while the benefits are hard and tangible, but also soft and intangible.\nWritten by Tehmina Khan', 'How to Make Safety Training a Top Priority\nTraining is a major responsibility. Whether you’re a training professional, a veteran of company training, or have just been asked to take over or start up a training program, you have a lot riding on your shoulders. Just think about all the different kinds of training employees need throughout their careers:\n- New employee orientation\n- Sexual harassment\n- Confined spaces\n- Emergency procedures\n- Software upgrades\n- Refresher training\n- Hazard communication\nYou can probably add many more topics to this list. There’s no doubt that employee training is an ongoing need in every company. Unfortunately, however, it’s not always a top priority. As the person now responsible for training in your organization, you can bring employee learning to the top of the priority list—and become a hero in the process.\nHow can you accomplish such a feat? By turning yourself into a skilled trainer who inspires lifelong learning in company employees at every level and who keeps up-to-date on the ever-changing and always innovative training industry.\nBuilding the Best Training Program will show you how to create—and run—an effective learning program that motivates and trains your workforce to be the best in your industry. This guide is a one-stop resource for all your training needs. Use it to help you become a trainer who makes learning an engaging and enjoyable experience for your trainees.\nEmployees will look forward to learning\nnew skills and reinforcing old ones, and managers will look forward to the increased profits that your\nwell-trained workforce will produce.\nOne of the first challenges trainers face is a resistance to training from both employees and\nmanagement. Employees are frustrated at taking time away from their jobs, and managers aren’t\nconvinced that time spent training is time well-spent.\nYou can convince both groups that training is crucial by proving to them that training doesn’t\ntake employees away from their work; training is, in fact, a crucial part of their work. In order to\nremain competitive in today’s global marketplace, it is vital that your company’s workforce maintains\nthe best skills and know-how to produce the best products and services. That means initial\ntraining to get up to speed and continual training in all new technologies, systems, or methods that\nbring more efficiency, more features, and better services.\nNO TIME TO PREPARE OR DELIVER TRAINING?\nWith TrainingToday, your employees can start taking essential training\ncourses the same day you sign up! Start your free preview now!\nConversely, a poorly trained workforce is more likely to turn out poor-quality products. Even if\nemployees receive top-notch initial training, your company can fall behind in the competitive\nmarketplace if employees don’t continue to learn how to do their jobs better.\nTraining affects the bottom line in every department. Here are just a few examples:\n- Human Resources\n- Legal compliance avoids fines for the company.\n- Career training aids employee retention.\n- Discrimination training avoids costly lawsuits.\n- Harassment training avoids lawsuits.\n- More efficient employees save costs.\n- Training improves productivity—which increases profits.\n- Up-to-date training gives you an edge over competitors.\n- Regulation compliance avoids costly fines.\n- Proper procedures avoid accidents, lost workdays, and workers’ compensation costs.\n- Emergency preparation aids quick evacuation.\n- First Aid training saves lives.\nIn short, an effective overall learning program for your company is a good return on investment\n(ROI). In fact, ROI analysis is an integral part of effective training evaluation because it gives specific\nmeasurements to the financial impact of training on the company’s bottom line. ROI answers the\nquestion “For every dollar the company invests in training, how many dollars does the company get\nMore Reasons to Train\nTo justify the need for training in your company, consider that in order to thrive in today’s business\nworld, your company needs to depend on employee education to promote six critical interests:\n- Effective use of new technology. As technology continues to revolutionize the workplace, employees at all levels and with all degrees of experience rely on training to keep up with changes in their work processes. Due to its importance, this training will require a comprehensive and continuing effort.\n- Competitive edge in your market. U.S. companies now receive fierce competition from overseas operations. In many cases foreign companies are beating out U.S. firms in quality, cost, and service. In order to remain competitive in the current marketplace, employees need to know how to make better products and services for your market.\n- Safety and health of employees. In order to have a productive, creative, and committed workforce, employers need to make sure that employees are protected from workplace hazards and given the knowledge and skills they need to work safely. Safety training is a key component of any organization’s productivity and prosperity.\n- Retention of skilled workers. Skilled and creative employees seek opportunities for career development and personal growth in their jobs. They want the chance to do challenging work and be well-compensated; they also want to be with a company where they can continue to learn and enhance their skills. If they don’t receive adequate training opportunities in their organization, they will find someplace where they can.\n- Compliance with laws and regulations. Training is frequently required under a variety of government regulations. The Occupational Safety and Health Administration (OSHA), for example, requires employers to conduct annual employee training in a\nnumber of safety procedures. In other cases, although laws may not require it, training (at least of key employees) is highly advisable to avoid problems (for example, sexual harassment, discrimination, violence prevention, diversity). The cost of not adequately training employees in all of these areas can translate into large fines (for failure to meet the requirements of regulations) or expensive lawsuits (for failing to uphold the rights of protected employees). [See Appendix A: Master Training Guide for 29CFR and Appendix B: Human Resources Training.]\n- Productivity and profitability. Finally, training makes workers more skilled and knowledgeable, which makes them more productive, better able to meet quality standards, and more able to provide excellent service to customers. Training, therefore, makes organizations more competitive, more profitable, and more successful.\nThe State of the Training Industry\nThe American Society for Training & Development (ASTD) conducts an annual “State of the Industry” survey, which reveals what companies are doing for training, how much they’re spending, and what they’re reaping in return. Spending on training may fluctuate from year to year, but the overall trend is companies that are more committed to keeping their workforces well-trained and up-to-date see the best returns on their investments.\nASTD’s “State of the Industry” identifies the following training trends:\n- Company profitability is positively correlated with training expenditures.\n- Classroom training is steadily decreasing.\n- Training with learning technologies is steadily increasing.\n- The outsourcing of training is remaining steady.\nThe survey also reveals that most companies use all three methods of delivering training in a blended learning approach.\nConstruction is one of the nation’s largest industries. There are more than 702,000 construction companies in the United States. Employees in construction are more prone to injuries than other types of workers.\nOSHA believes that computer-based training programs can be used as part of an effective safety and health training program to satisfy OSHA training requirements, provided that the program is supplemented by the opportunity for trainees to ask questions of a qualified trainer, and provides trainee\nSafety meetings and safety talks are usually thought of as departmental or crew meetings. These frequent meetings, ranging from weekly to monthly depending upon the situation, allow the supervisor or trainer to review key safety topics and review any accidents incurred by members of the group.\nTesting your employee’s understanding of safety benefits everybody. Tests help you assess the effectiveness of your employee safety training. They help you measure employee understanding and retention of safe practices, gauge the competence of each trainee and document compliance with regulations.']	['<urn:uuid:041459ed-b821-49e2-a944-8923c28974eb>', '<urn:uuid:630224ae-c369-4c12-9b40-9b1cecde4971>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	5	92	2324
14	What kinds of information do websites collect through cookies and how do they use this data to improve user experience?	Websites collect various types of information through cookies, including browsing preferences, number of pages visited, language settings, social network interactions, city location based on IP address, number of new users, visit frequency and duration, browser type, and device information. This information is used for multiple purposes: to maintain website functionality, manage session responses, validate security features, enable social network sharing, and customize features like language and regional settings. In the case of third-party services like Google Analytics, this data can also be used to measure and analyze website audience and improve their services.	['Cookies are small files that some platforms, such as websites, can install on your computer. They can have a variety of functions: store your browsing preferences, collect statistical information, allow certain technical features, and so on.\nCookies are sometimes used to store basic information about the user’s browsing habits or computer, to the point of being able to recognise it in some cases.\nThe following table shows information about the cookies existing on the website of the ASSOCIACIÓ CATALANA D’UNIVERSITATS PÚBLIQUES (CATALAN ASSOCIATION OF PUBLIC UNIVERSITIES):\nThere are different kinds of cookies:\nFirst party cookies\nThese are sent to the user’s device from a computer or domain managed by the publisher itself and from which the service requested by the user is provided. Technical cookies are essential and strictly necessary for the website to function properly and to use the different options and services it offers.\nThird party cookies\nThese are sent to the user’s device from a computer or domain that is not managed by the publisher, but by another party that processes the data obtained through cookies.\nThe third party cookies on this website come from Google, so we can use the Google Analytics tool: www.google.com/analytics and https://marketingplatform.google.com/intl/en_uk/about/.\nThis application was developed by Google, which provides us with a service to measure and analyse our website audience. Google can also use this data to improve its own services and to offer services to other companies. Please see the provided links to find out about these and other uses.\nThis tool does not obtain data about your name or surname or the postal address from which you connect. The information that is obtained is related to the number of pages visited, the language, social network on which our news is published, the city to which the IP address from which you access is assigned, the number of new users, the frequency and recurrence of visits, the time of visits, the browser and the operator or type of device from which the visit is made.\nTechnical cookies are also used. Technical cookies are essential and strictly necessary for the website to function properly and to use the different options and services it offers. For example, those used for session maintenance, response time management, performance or validation of options, using security features, sharing content on social networks, and so on.\nPersonalisation cookies can also be used. These cookies allow the user to specify or customise certain features of the general options of the website. For example, set the language, regional settings, or browser type.\nWe use session cookies. Session cookies are used to recognise website users so that any changes made, articles selected or data entered are remembered from one page to another.\nCookies currently used on this website:\n||No description available.\n||The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category “Functional”.\n||This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category “Necessary”.\n||This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category “Performance”.\n||This cookies is set by GDPR Cookie Consent WordPress Plugin. The cookie is used to remember the user consent for the cookies under the category “Analytics”.\n||The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category “Advertisement”.\n||This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category “Others”.\nYou can allow, identify, block or delete cookies installed on your computer by changing the options settings of the browser installed on your computer. The following information explains how this operation is performed on the main browsers:\nFirefox from here: https://support.mozilla.org/en-US/kb/block-websites-storing-cookies-site-data-firefox\nChrome from here: http://support.google.com/chrome/bin/answer.py?hl=és&answer=95647\nInternet Explorer from here: https://support.microsoft.com/es-es/help/17442/windows-internet-explorer-delete-manage-cookies\nSafari from here: https://support.apple.com/en-gb/guide/safari/sfri11471/mac\nOpera from here: https://help.opera.com/en/latest/web-preferences/#cookies', 'What is a cookie?\nA cookie is a small piece of data that a website asks your browser to store on your computer or mobile device. The cookie allows the website to “remember” your actions and/or preferences over time. Most Internet browsers support cookies: however, users can set their browsers to decline certain types of cookies or specific cookies. Furthermore, users can delete cookies at any time.\nDo we share any data with third parties?\nWe use third-party website infrastructure suppliers to help us build and manage our website. Cookie data is stored securely on these suppliers’ servers. Our service providers have been carefully selected by us, are contractually bound by our instructions, and will not process your data for any other purposes than intended.\nWe may use them as a way of measuring our service and to work with marketing partners to improve our service. You can usually change your browser so that it will not accept cookies or to indicate when a cookie is being sent. However, if you do not accept cookies, you may not be able to use some portions of our Service. To assist you, we have provided an explanation of the types of cookies we use below.\nThey have been categorized by its purpose:\nWhat are session cookies?\nThese are cookies that last for a session. A session starts when you launch a Site for example and ends when you leave the website or close your browser window. Session cookies contain information that is stored in a temporary memory location which is deleted after the session ends. Unlike other cookies, session cookies are never stored on your device. Therefore, they are also known as transient cookies, non-persistent cookies, or temporary cookies.\nWhat are preference cookies?\nWe use Preference Cookies to remember your preferences and various settings where the technical storage or access is necessary for the legitimate purpose of storing preferences.\nWhat are security cookies?\nSecurity Cookies are a type of HTTP cookie that have secure attribute set, which limits the scope of the cookie to “secure” channels. When a cookie has the secure attribute, the user agent will include the cookie in an HTTP request only if the request is transmitted over a secure channel (typically HTTPS).\nWhat are advertising cookies?\nAdvertising Cookies are specifically designed to gather information from you on your device to display advertisements to you based on topics that interest you.\nWe work with approved partners to deliver relevant content to you and measure the effectiveness of these advertisements.\nHow are third party cookies used?\nThird-party cookies are cookies that are stored under a different domain than what you are currently visiting. They are mostly used to track users between websites and display more relevant ads between websites. Another good example is a support chat functionality provided by a third party service. We use third party cookies to give you an enhanced experience and to power third party functionality such as chat windows.\nHow can I control cookies?\nYou can control and manage cookies in various ways. Please keep in mind, removing or disabling cookies may impact your user experience and parts of the Site may no longer be fully functional.\nYou may change your browser settings to manage and control cookies. You can visit http://www.aboutcookies.org/ which contains comprehensive information on how to do this on a wide variety of browsers. This site includes details on how to delete cookies from your computer as well as more general information about cookies.\nChanges to the Cookies Policy']	['<urn:uuid:53d48698-63c0-4724-9d09-499192ebcb30>', '<urn:uuid:1b83c78e-0351-4f07-ae60-6017c241485e>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	20	93	1247
15	What is the purpose of CREATE SEQUENCE in SQL Server?	CREATE SEQUENCE is used to create a database-wide identity number sequence, eliminating the need for maintaining a separate table with a numeric field that gets updated by a function every time a new identity is created.	"[""Continuing on from new T-SQL features of SQL Server 2012, I wanted to explain about the new CREATE SEQUENCE feature.\nWhat is CREATE SEQUENCE for?\nA common feature in table design is to place an auto incrementing number on a field in the form of an IDENTITY column. So this is an easy way of maintaining a sequential number on a table.\nWhat if you wanted to create a database wide identity? Prior to SQL 2012, you might choose to do it by having a table sitting there in the middle of it all with a numeric field which gets updated by some function every time a new identity is created. CREATE SEQUENCE takes away this overhead.\nYou can create a sequence either using SQL Server Management Studio or using T-SQL. In Management Studio, find the “Sequences” folder under “Programmability” beneath the database you want to add a sequence for.\nHere you can configure the sequence according to your design.\nTo do the same thing in T-SQL, the syntax is:\nCREATE SEQUENCE MyDemoSequence START WITH 1 INCREMENT BY 1;\nIf you choose to do this using T-SQL then by default a sequence is created as a BIGINT datatype unless you specify otherwise, for example:\nCREATE SEQUENCE MyDemoSequence AS SMALLINT START WITH 1 INCREMENT BY 1;\nNote that START, INCREMENT, MINVALUE and MAXVALUE must be configured within the boundaries of the data type. For example, you couldn’t specify a negative START value for a TINYINT data type.\nTo use CREATE SEQUENCE\nTo get the next value in the sequence use the NEXT VALUE FOR\nSELECT NEXT VALUE FOR MyDemoSequence;\nIf you were inserting to a table, it would like something like this:\nINSERT INTO YourTable(ID, Name) VALUES(NEXT VALUE FOR MyDemoSequence, 'Your name');\nSo what else is there to know about CREATE SEQUENCE?\nCYCLE and NO CYCLE tells the sequence to cycle back round to the minimum value when the last value has been allocated in the sequence. By default, this is set to NO CYCLE.\nCACHE or NO CACHE, designed for performance benefits in reducing IO requests for new numbers out of a sequence. A cache value can be specified and the numbers up to the maximum cache value are loaded up into memory until the cache is exceeded and a new set of numbers is required.\nFor example, you might create a sequence with a cache of 20. When a value is needed from the sequence, the minimum value in the sequence up to and including the CACHE value are loaded into memory. The CACHE value of 20 is written to the system table sys.sequences and when 20 gets used and 21 is requested, then a fresh set of numbers is allocated to the cache (21-40) with 40 being written to the sys.sequences table as the CACHE value.\nHere’s an example of a sequence created using CYCLE and CACHE\nCREATE SEQUENCE MyDemoSequence AS INT START WITH 1 INCREMENT BY 1 CACHE 20 CYCLE;\nLets have a quick look at the sys.sequences table\nSELECT cache_size, is_cached, current_value FROM sys.sequences WHERE name = 'MyDemoSequence';\ncache_size is_cached current_value ----------- --------- ------------- 20 1 1 (1 row(s) affected)""]"	['<urn:uuid:5c973243-8833-42e6-8305-79e76b137a3e>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	10	36	524
16	wildlife collisions deaths cost benefits usa	In the USA, wildlife-vehicle collisions result in about 26,000 human injuries and 200 human fatalities annually, with an estimated 2 million large mammals being hit. The collisions cost approximately $12 billion per year. However, implementing wildlife crossings and road mitigation measures can reduce collisions by 80-95%, making them cost-effective despite their high initial cost ($100,000 per mile for fencing, up to $10 million for an overpass).	['Editor’s Note: This article was originally published by The Appalachian Voice.\nJean Loveday is driving her husband, Tom, home from a doctor’s appointment in Johnson City, Tennessee. Their Toyota pickup truck is winding along Interstate 26, not far from the North Carolina state line north of Asheville.\nSuddenly Loveday sees something black tumbling down the mountain and out into the highway in her peripheral view. “Oh no, Tom, oh no!” she mumbles. Loveday realizes it’s a bear cub hurtling toward them. She attempts to avoid hitting it by steering into the median, but vehicle and animal seem destined to collide.\n“It all happened so fast,” she says today. “I don’t know where its mother was, whether the cub was following her or on its own. We stopped. It moved for a few minutes, and then was still. All I could think for days was, ‘I killed a bear cub!’ I hope I never, ever have to go through that again.”\nLoveday is overwhelmed with emotion as she relates this sad memory, one shared by many motorists in the Southern Appalachians.\n“I don’t care where you are on the political spectrum, no one wants to hit an animal with their vehicle,” says Jeff Hunter, senior program manager for National Parks Conservation Association, an organization devoted to protecting and enhancing the national parks system for future generations.\nHighways pose lethal hazards to animals looking for food, water and other resources. Photo courtesy of National Parks Conservation Association and Wildlands Network.\nIn early 2017, Hunter convened a group of people who were concerned about the rising numbers of bear, deer and elk being hit on another highway that straddles the Tennessee – North Carolina border — Interstate 40 near Great Smoky Mountains National Park. Some years have seen as many as 70 road-killed bears in this curvy 28-mile section of road alone, and elk reintroduced to the park in 2001 are now crossing the highway to expand their range.\n“Human infrastructure is making it increasingly difficult for wildlife to follow their natural patterns of movement across the landscape,” says Hugh Irwin, a landscape conservation planner with The Wilderness Society who raised concerns back in the 1990s about I‑40 being a barrier to wildlife movement. “Historically too little thought and planning has gone into wildlife needs, and our current infrastructure fails to provide for wildlife passage.”\nPassionate discussions led to action, and soon more than 80 individuals from nearly 20 federal, state, tribal, and non-governmental organizations were collaborating to make this section of roadway more permeable for wildlife and safer for people. This year, in late February, the group announced itself publicly as Safe Passage: The I‑40 Pigeon River Gorge Wildlife Crossing Project.\nRoadkill’s “Pernicious Twin”\nThe intersection of roads and wildlife is a safety issue that is not unique to North Carolina and Tennessee. According to the Federal Highway Administration, an estimated two million large mammals are hit on roads in the United States each year, resulting in more than 26,000 human injuries and at least 200 human fatalities.\nFor years, road ecologists around the world have been working to mitigate highways that were originally designed without consideration for wildlife. Europe, Canada, Mexico, and many U.S. states have already created effective wildlife road crossings. Recent articles and videos featuring large wildlife overpasses in Utah and Texas have been shared widely on social media.\nSenior Research Ecologist Marcel Huijser (pronounced ‘Houser’) with the Western Transportation Institute at Montana State University in Bozeman has contributed to road ecology studies for more than two decades. He cites three main reasons why people care about this issue: the desire for wildlife conservation, concern for human safety, and economics. “No matter who you are, where you live, or what you do for a living, you’re going to care about at least one of these,” he says.\nOn Nov. 26, 2019, The Atlantic ran an auspicious road ecology article by Ben Goldfarb titled “How Roadkill Became an Environmental Disaster.” Focusing on the giant anteaters of Brazil, whose range is — you guessed it — bisected by a huge highway, the epic, riveting story introduces readers to Evelyn the anteater and a cast of road-weary researchers. One particular Goldfarb quote became the motto for researchers assessing wildlife movement and mortality in the Pigeon River Gorge: “Collisions may be road ecology’s most obvious concern, but fragmentation is roadkill’s pernicious twin.”\nConservationists point out the gravity of individual animals being killed on roads. But when they no longer try to cross, it can signal an even more dire situation.\n“When wildlife finally stops even trying to cross, the highway has become a barrier,” says Hunter. “The ‘barrier effect’ is not to be confused with the concrete Jersey barriers that prevent many individual crossings. When a whole population stops crossing the road, that means their habitat is now fragmented, preventing the healthy genetic exchange that species need to thrive.”\nRon Sutherland works to restore, reconnect and re-establish wildlife corridors that have been fragmented throughout the eastern United States in his role as chief scientist with Wildlands Network, the organization that kicked off discussions about mitigation to I‑40 in 2015. He defines habitat connectivity as the degree to which organisms are able to move freely across the landscape.\n“Habitat connectivity can be very high, such as in a remote and intact wilderness,” he says, “or it can be very low, such as in a city park surrounded on all sides by busy highways.”\nSutherland points out that people often get wildlife corridors and wildlife road crossings confused.\n“A wildlife corridor is the term we use for a defined movement pathway that, if protected or restored, would provide essential habitat connectivity for one or more species,” he says. “They can be easy to see — such as a vegetated trail alongside a roadway — or nearly invisible and defined only by the movements of the animals.”\nA wildlife road crossing, on the other hand, is “a structure that is designed to allow wildlife to safely cross over or under a busy road,” he says. “So, of course it follows that one of the best places to put wildlife road crossings is where you have a wildlife corridor that gets cut off by a highway.”\nCaptivating Research in the Gorge\nThe best places to put wildlife road crossings along the 28-mile stretch of winding mountainous terrain in the Pigeon River Gorge are precisely what researchers are working to figure out. For the past two years, National Parks Conservation Association and Wildlands Network have been collecting data that will help them identify key areas and strategies for mitigating the road between Asheville and Knoxville, preparing Safe Passage to make recommendations that can be implemented during planned road maintenance and bridge repairs.\nInterstate 40 was built in 1968. Like hundreds of roads that now crisscross the Southeast, it sliced through a mountain landscape where animals had freely followed ancient wildlife corridors for millenia. Back in the ’60s, there were fewer vehicles and fewer animals. Today some 27,000 cars and trucks travel this road daily while, not far away, some of the park’s 1,900 black bears are searching for food, mates and shelter, which leads them to traverse the mosaic of wild, steep and rugged public and private lands that make up the gorge. What’s more, elk too are now attempting to cross these roads, sometimes joining their fellow ungulates, the prolific white-tailed deer, in sad deaths involving hours of suffering.\nResearchers are stymied when it comes to finding a way to count the many individual animals who sustain severe injuries and make it off the roadway, only to die later in the forest.\n“With both animal and human populations increasing alongside growing tourism in\nthe Smokies region,” says Hunter, “this situation is expected to get worse over the next decade.”\nWildlife crossings can only succeed if located where animals wish to cross the road, not just where it may be easy or convenient from a construction perspective. To this end, researchers have employed wildlife cameras to help them assess wildlife road mortality patterns in the gorge and examine how some animals use existing structures such as culverts designed to move water under the roads. They have also been tracking wildlife activity in the right-of-way alongside the road. To follow elk movement, wildlife biologist Liz Hillard is conducting a GPS-collar study.\n“The topography is driving where these elk are moving,” says Hillard, a wildlife biologist with Wildlands Network. “They’re trying to spend the least amount of energy, so they follow low-slope areas, moving through the landscape in what we call the path of least resistance.”\nHillard works closely with Steve Goodman, NPCA’s wildlife researcher in the gorge, whose work is funded by the Volgenau Foundation. He has been servicing the 120 camera traps and collecting their data for the past two years.\n“Regionally — and nationally — this area is widely considered to be of high conservation value and comprised of key habitat corridors that are critical for long-term flow of both plants and animals,” Goodman says. “The first step to mitigation is gaining an understanding of how these animals navigate the landscape. Where do they go, when, and why?”\nGoodman and Hillard are examining “hotspots” where the most animals are getting killed, as well as places where some fortunate bear, deer and elk are successfully getting from one side of the interstate to the other. Their data will prepare Safe Passage to collaborate with local departments of transportation on bridge improvements planned for the next five years. The first of these may begin as early as fall of 2021 at the Harmon Den exit near the intersection with the Appalachian Trail, where a herd of elk have dispersed from the population reintroduced in the Smokies 20 years ago.\nBenefits Outweigh the Costs\nWhen it comes to road ecology, the economic reality can be as shocking as the roadkill. But Huijser says, in the long term, the benefits outweigh the costs.\n“Collision-related costs add up to roughly $12 billion annually in the U.S.,” he says. “The cost of a deer – vehicle collision averages around $6,000 and running into an elk can cost upwards of $17,000.”\nWildlife crossing structures and road mitigation have improved human safety and wildlife corridor connectivity at Snoqualmie Pass on I‑90 in Washington State, along the Trans-Canada Highway in the Rocky Mountains and Banff National Park, and on the Flathead Indian Reservation in Western Montana where Huijser worked for 13 years. In these examples, fencing successfully reduces collisions and guides wildlife to safe-crossing opportunities such as vegetated overpasses, open-span bridges, and large- and medium-mammal underpasses. Dozens of such wildlife corridor projects have led to an 80 – 95% collision reduction with large mammals like deer and elk since the mid-’90s.\nAlthough road mitigation measures are good for human safety and for animals, they cost money. Fences may cost about $100,000 per mile, an underpass may require around half a million to build, and a single wildlife overpass can cost up to $10 million.\nBut Huijser the research ecologist says that society can’t afford not to.\n“Implementing effective mitigation measures substantially reduces costs associated with wildlife – vehicle collisions by 80 – 100%,” Huijser says. “Bottom line: Even if people don’t care about human safety or wildlife conservation, it can still make economic sense. And if you consider the biological conservation aspect, the value expands to take in benefits to local tourism economies and other economic benefits of having healthy wildlife populations in the landscape.”\nIn 2020 and 2021, Wildlands Network worked with a coalition of Virginia partners to get landmark bipartisan legislation passed in support of wildlife crossings. These efforts direct the relevant agencies to collaborate, incorporating wildlife corridors and road crossings into their design and planning stages — a major step forward both in protecting motorists from collisions with animals and in addressing barriers to wildlife movement.\n“Here in North Carolina, our coalition is analyzing an array of possible mechanisms that will best serve the agencies and goals of connectivity on the landscape to achieve significant reduction of collisions with wildlife,” says Christine Laporte, the Eastern program director at Wildlands Network. “Safety, conservation, economic considerations of crossing, and mitigation initiatives all benefit from a range of state-level mechanisms that support use of the best available science for effective designs and actual structures on the ground.”\nIrwin of The Wilderness Society says, “Going forward, wildlife movement patterns and needs should be incorporated into infrastructure planning, and existing infrastructure should be retrofitted over time to enable better wildlife movement without the current high levels of wildlife mortality as well as human impacts and property damage.”\nWhatever road mitigations and crossing structures are eventually implemented in the steep terrain of the Pigeon River Gorge, Safe Passage hopes its collaborative effort will become the model for others championing change on regional roads with similar issues. For example, elk often congregate near and on Highway 19 in Maggie Valley and Cherokee, North Carolina. In October of 2019, an elk was found dead on the shoulder of Interstate 26 in East Tennessee approaching Sams Gap, not far from the Appalachian Trail. This death alerted researchers to the fact that these large ungulates are beginning to cross rivers and disperse to create new herds far from their 2001 reintroduction site in Cataloochee on the southeastern side of the Smokies.\n“Our research in the Pigeon River Gorge is now in its final stages,” says Hunter, “and we don’t have all the answers yet. But one thing we do know is that collaborative partnerships like Safe Passage are critical to finding the best path forward.”\nThe Safe Passage Fund Coalition comprises The Conservation Fund, Defenders of Wildlife, Great Smoky Mountains Association, National Parks Conservation Association, North Carolina Wildlife Federation, and Wildlands Network.\nReader donations, many as small as just $5, are what fund the work of writers like this—and keep our content free and accessible to everyone. If you support this work, will chip in to help fund it?\nIt only takes a minute to donate. Click here to make a tax-deductible donation.\nFrances Figart is the Creative Services Director for Great Smoky Mountains Association, an educational nonprofit partner of Great Smoky Mountains National Park. Reach her at frances[at]gsmassoc.org.', 'The ARC International Wildlife Crossing Infrastructure Design Competition, the first international competition of its kind, says collisions between wildlife and cars in the U.S. have increased by 50 percent in the past 15 years. Not only do these collisions take a huge toll on both wildlife and people, but they also cost the U.S. some $8 billion per year. To create a new wildlife crossing model that can enable animals with extensive migratory ranges — like bears, wolves, and lynxes — to better coexist with people, ARC asked designers to submit concepts that allow for safe mobility for a variety of species along separate but integrated transportation networks (see earlier post). Out of some 36 concepts submitted from teams worldwide, the design concept devised by HNTB and Michael Van Valkenburgh Associates was unanimously judged to be the best for its cost-effective approach using “ordinary materials, such as concrete, in an extraordinary way.”\n“HNTB+MVVA’s design is cost-effective, modular, easy to construct, provides greater material control, and uses a unique built-in drainage system,” says the ARC competition. Nina-Marie Lister, the ARC competition advisor and professor at Ryerson University in Toronto, Canada, adds: “The jury chose this design because it is not only feasible, but because it has the capacity to transform what we think of as possible – a novel design solution to a growing problem that could serve as a model for the world.”\nNew research shows that wildlife crossings do actually reduce collisions between wildlife and cars. ARC points to a study done in Banff National Park in Alberta, Canada, which demonstrated that a “series of 22 underpasses and two overpasses has resulted in an 80 percent reduction in total wildlife fatalities because wildlife was allowed to roam free uninterrupted of human transportation. As a result, there have been approximately 240,000 crossings (and counting) of 11 species of large mammals, including wolf, grizzly bear, elk, lynx, mountain lion, and moose across these paths.”\nThe HNTB+MVVA design features a single span across a highway; there’s no central pier. “This single span is a unique feature that will provide a much safer experience for drivers by creating a more open experience.” The winning team’s design is innovative because it’s approximately four times wider than other crossing structures and includes fences that guide different types of wildlife across through safe passages. “This provides an ideal setting to accommodate wildlife movement and a diversity of habitats on top of the bridge.” The design was also judged to be among the more sustainable and low-cost options with its use of pre-fit modular concrete pieces.\nThe next step will be to implement the design somewhere in Colorado. The design competition’s site is West Vail Pass on I-70 in Colorado, about 90 miles west of Denver. The site was selected from a range of candidates. HNTB and Michael Van Valkenburgh Associates received prize money and a preferential position for an upcoming state-financed wildlife crossing project on a site yet to be determined in the state.\nClearly, there are numerous opportunities to deploy this solution, saving the lives of wildlife, which almost invariably die in these traffic accidents, and drivers in the process. Crafting a public-private financing model for the widespread roll-out of these overpasses should be a priority.\nImage credits: HNTB and Michael Van Valkenburgh Associates, Inc.']	['<urn:uuid:342e24c1-0e38-454e-a2ba-3fbc73c35e87>', '<urn:uuid:87547893-daa3-4dc7-8659-a8229fdaf473>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T17:51:21.175311	6	66	2926
17	What is the case diameter of the Radiomir 1940 Tourbillon GMT Oro Rosso?	The Radiomir 1940 Tourbillon GMT Oro Rosso has a case diameter of 48mm.	['Last week, we’ve introduced you the new Panerai Radiomir 1940 3-Days Automatic with Panerai’s First Micro-Rotor Movement. During Watch and Wonders 2014, Panerai also came with something less mainstream, more complicated, more limited. But something that some of us can dream of: a Radiomir 1940 equipped with the P2005 movement, meaning a tourbillon with a second time-zone and 6 days of power reserve. Here is the Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558.\nPanerai is know for its rugged, large, massive and now iconic dive watches. Inspired by 1940s and 1950s editions, all of them share the same DNA. What especially differentiates the watches from the Officine are the materials used for the cases and the movements. Some of them relies on simple ETA 6497 but most are using in-house calibres now. And some are using superb manufacture movements. Take a look at the Panerai Radiomir Platino and Oro Rosso 47mm Special Edition or the Panerai Radiomir 1940 Chronograph and you’ll understand our definition of superb. OK, both are using some of our favorite movements, some Minervas, known for their glamorous curves. With the Radiomir 1940 Tourbillon GMT Oro Rosso PAM558, Panerai prefers to keep the game modern – at least for the movement – and encloses a P2005 into its new limited edition.\nThe P2005 that equipped the Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 is not new – which doesn’t mean it is not a piece of interest. We already covered it when Panerai introduced the Luminor 1950 Tourbillon GMT Ceramica. It comes with an unusual 30-second tourbillon that rotates perpendicular to the balance, when a classical tourbillon does its rotation on its axis and (usually) in one minute. It also features Panerai’s usual ¾ style (even if it is composed of 3 separated bridges), with a straight graining, that hides 3 barrels, for 6 days of power reserve – indicated on the movement side by a hand. All of this is of course visible through a sapphire caseback, that will allow you to enjoy the polish beveled angles of the bridges and the circular graining of the main plate.\nTalking about mechanics, the front shows also some interesting features, with a GMT hand located on the central axis, a second time-zone at 3 (with AM / PM indication) and a small second at 9 (inside this subdial is a rotating tourbillon indicator). The GMT hand can be adjusted by one-hour increments directly by the screwed crown, without disturbing the seconds or the minutes. The dial comes with Panerai’s iconic sandwich structure. The Panerai Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 has a nicely grained face, with a brown satin sunburst finish.\nMoving to the case, collectors won’t be disturbed as it presents the classical Radiomir 1940 shape – meaning large lugs (not the wire loop strap attachments of the classical Radiomir) and a case without the crown protection of the Luminor models. This the transitional shape between the two iconic models of the brand, the old Radiomir and the more modern 1950’s Luminor. It is here made of 18k red gold (Oro Rosso in Panerai’s nomenclature) and has a quite impressive 48mm diameter. Not the most discreet combination. However, the brown dial and strap with this gold case are highly elegant and warm.\nThe Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 will be limited at 3à pieces only, for a price that is not yet defined. More about it on Panerai’s Official Website.\nSpecifications of The Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558:\n- Movement: hand-wound mechanical, Panerai P.2005 calibre, executed entirely by Panerai, 16¼ lignes, 9.1 mm thick, 31 jewels, Glucydur® balance, 28,800 alternations/hour. KIF Parechoc® anti-shock device. Power reserve 6 days, three barrels. 239 components.\n- Functions: Hours, minutes, small seconds, second time zone, 24h indicator, power reserve indicator on the back, tourbillon.\n- Case: Diameter 48 mm, 18 ct. polished red gold. Screw-down winding crown personalized OP.\n- Bezel: 18 ct. polished red gold.\n- Back: See-through sapphire crystal.\n- Dial: Brown, with luminous Arabic numerals and hour markers. 24h indicator at 3 o’clock, seconds and tourbillon indicator at 9 o’clock.\n- Crystal: Sapphire, formed of corundum, 1,5 mm thick. Anti-reflective coating.\n- Water-resistance: 5 bar (~ 50 metres).\n- Strap: PANERAI personalised alligator strap and polished 18 ct. red gold adjustable buckle.\n- Reference: PAM00558.']	['<urn:uuid:88916479-57c3-4844-8fdd-9003b6886b58>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	13	13	727
18	I'm curious about how krill fishing is regulated in Antarctica since it seems to be in international waters. How do countries control and monitor krill fishing there?	Krill fishing around Antarctica is governed by the Conservation of Antarctic Marine Living Resources (CCAMLR), a treaty organization based in Hobart, Australia. China and other fishing nations are signatories to this treaty. Since the ocean around Antarctica is considered international waters, enforcement of fisheries rules relies on self-reporting by countries that sign the CCAMLR treaty. This can make monitoring difficult, especially in China's case, as they have eight 5,000 to 7,000 metric ton factory krill trawling ships, more than any other nation.	"[""Conservation groups and scientists worry that China’s push to boost its harvest of krill -- a shrimp-like creature used for aquaculture feed and human supplements -- may leave Antarctica’s whales, seals and penguins struggling to survive. China’s leaders say they want a seven-fold increase in krill production, according to a recent report in the state-owned China Daily newspaper.\nChina is one of several nations, including Norway, Korea and Chile, harvesting krill with massive factory ships dragging miles-long trawl nets through the productive cold waters of the Southern Ocean. The krill is quickly frozen and into processed into pellets for aquaculture and livestock operations, fish bait, and high-value “nutraceuticals” such as omega-3 dietary supplements.\nThat sharing may be tough for the animals that rely on krill as their main source of food. When penguins and other animals breed, they can only travel so far while raising their young. That’s especially true around the Antarctic Peninsula, which happens to be a hotspot for both krill and the larger creatures that eat it.\n“We will increase our investment in the Antarctic area in terms of krill fishing, said Liu Shenli, chairman of the China National Agricultural Development Group and a member of the National Committee of the Chinese People's Political Consultative Conference. Krill provides very good quality protein that can be processed into food and medicine. The Antarctic is a treasure house for all human beings, and China should go there and share.”\n“The issue isn’t so much the size of the entire (krill) population, but where they are taking it,” said Claire Christian, a spokeswoman for the Antarctic and Southern Ocean Coalition, a Washington-based advocacy group. “Some of these larger ships could deplete all the krill in a local area. That would create problems.”\nFishing around Antarctica is governed by the Conservation of Antarctic Marine Living Resources, (CCAMLR), a Hobart-Australia-based treaty organization. China is a signatory to the treaty.\nWhile the current 4.2 million metric ton catch limit for Antarctic krill is much higher than the amount currently harvested, the problem is that the science behind the krill population estimates may be outdated.\nGeorge Watters, a fisheries scientist at the National Oceanic and Atmospheric Administration’s Southwest Marine Fisheries Center in La Jolla, Calif., says the surveys that resulted in the limit were conducted more than 15 years ago. Watters says that new studies are needed to get a better picture of how abundant krill is today, especially around the Antarctica Peninsula.\nThe water and air temperature around the peninsula has warmed faster than any other place on Earth, leading to concerns about populations of several penguin species, whales and other marine mammals. Krill forms the base of the food chain, and may be at risk as well, explained Watters.\n“Climate change is a major force in that region, and is creating big changes in the (Antarctic Peninsula) ecosystem,” Watters said. “A legitimate concern is to how productive will the krill stock be in the future. There are papers to suggest it will be less productive. That’s a reason why there is an effort to change the catch limit for krill so it is updated more regularly.”\nSince the ocean around Antarctica is considered international waters, enforcement of fisheries rules relies on self-reporting by countries that sign the CCAMLR treaty. That could make China’s new effort harder to monitor at sea, especially since it has eight 5,000 to 7,000 metric ton factory krill trawling ships, more than any other nation. (FIS)""]"	['<urn:uuid:79c684ba-5b58-445a-89cb-45a9d8e97c3a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	27	82	578
19	insulation installer benefits indoor air effects	Installing insulation properly has dual effects. Closed-cell foam insulation can create a super tight building envelope that stops air leakage and enables better control of indoor living space. However, this tightness can lead to indoor air quality issues if not properly managed, as occupants generate moisture during daily activities. To prevent health problems, homes with tight insulation should include exhaust vents and possibly an Energy Recovery Vent (ERV) to normalize humidity, since poor indoor air quality causes thousands of deaths and millions of illnesses annually.	"['Ask questions and get answers from experienced industry professionals\nHere is a good website to reference for foam insulation:\nTypes of Liquid Foam Insulation\nToday, most foam materials use foaming agents that don\'t use chlorofluorocarbons (CFCs) or hydrochlorofluorocarbons (HCFCs), which are harmful to the earth\'s ozone layer.\nThere are two types of foam-in-place insulation: closed-cell and open-cell. Both are typically made with polyurethane. With closed-cell foam, the high-density cells are closed and filled with a gas that helps the foam expand to fill the spaces around it. Closed-cell foam is the most effective, with an insulation value of around R-6.2 per inch of thickness.\nOpen-cell foam cells are not as dense and are filled with air, which gives the insulation a spongy texture. Open-cell foam insulation value is around R-3.7 per inch of thickness.\nThe type of insulation you should choose depends on how you will use it and on your budget. While closed-cell foam has a greater R-value and provides stronger resistance against moisture and air leakage, the material is also much denser and is more expensive to install. Open-cell foam is lighter and less expensive but should not be used below ground level where it could absorb water. Consult a professional insulation installer to decide what type of insulation is best for you.\nAvailable liquid foam insulation materials include:\nSome less common types include Icynene foam and Tripolymer foam. Icynene foam can be either sprayed or injected, which makes it the most versatile. It also has good resistance to both air and water intrusion. Tripolymer foam—a water-soluble foam—is injected into wall cavities. It has excellent resistance to fire and air intrusion.\nLiquid foam insulation -- combined with a foaming agent -- can be applied using small spray containers or in larger quantities as a pressure-sprayed (foamed-in-place) product. Both types expand and harden as the mixture cures. They also conform to the shape of the cavity, filling and sealing it thoroughly.\nSlow-curing liquid foams are also available. These foams are designed to flow over obstructions before expanding and curing, and they are often used for empty wall cavities in existing buildings. There are also liquid foam materials that can be poured from a container.\nInstallation of most types of liquid foam insulation requires special equipment and certification and should be done by experienced installers. Following installation, an approved thermal barrier equal in fire resistance to half-inch gypsum board must cover all foam materials. Also, some building codes don\'t recognize sprayed foam insulation as a vapor barrier, so installation might require an additional vapor retarder.\nLiquid foam insulation products and installation usually cost more than traditional batt insulation. However, liquid foam insulation has higher R-values and forms an air barrier, which can eliminate some of the other costs and tasks associated with weatherizing a home, such as caulking, applying housewrap and vapor barrier, and taping joints. When building a new home, this type of insulation can also help reduce construction time and the number of specialized contractors, which saves money.\nFoam insulation is very effective in the development of a super tight building enevelope. It will stop air leakage and enable more total control of the interior living space. This present potential problems in that our living and breathing in the living space generates moisture.\nTraditional building construction practices are precisely the opposite and utilizes the concept of venting in the attic and in the crawl space whereby the area above and below the living envelope allows for the eveporation of moisture.\nIn designing a super tight envelope that is totally sealed there should be careful thought and concern for moisture in the enclosed area. Because foam is so effective at sealing drafts, the space should be thoughtfully designed as a whole house system, with exhaust vents for all areas of the house that generate moisture, and consideration should be given to installing an Energy Recovery Vent (ERV) to normalize the humidity between exterior and interior, to avoid the potential of developing a sick hoiuse syndrome.\nOpen cell is advisable in attic applications where you want moisture to freely move through when a roof leak developes, to avoid major structural damage over time. Close cell is most advisable in the peremeter of the crawl space or basement area where concern is for a more dense insulation product with more structure. (I have seen it done but advise against, applying foam on the bottom side of flooring since doing so seals all of the mechanical systems into the muck and makes maintainence profoundly troublesome and wretched for the future).\nBob Windom, Windom Construction Co. Inc. Atlanta\nJ.H. Fiber cement is composed of 70% concrete and 30% celluoose. Cellulose being ground up paper. Both water and cement wick moisture so if you use these products be sure to read and follow installation instructions. Leave no surface unpainted or it will draw moisture and deteriorate. Seen it many times. It is also dusty if you cut it with a saw. It is heavy, breaks easily if flexed. It is also heavy so it only comes in 12 foot lengths for the most part. It has the insulation value of cement which R-1 per inch. It is 5/16 thick so you get 5/16 or R-1 which is not much.\nVinyl is plastic but it is about half the price and any handyman can install if they follow directions so it can expand and contractl. Don\'t expect it top lasd forever. Hail storms beat it up pretty bad if the stones are big enough. One caution with it is do not place a grill near i, vinyl melts at 150 to 165 degerees depending on how thick it is.\nIn my opinion steel is a little costiler but in most cases lasts for a long time. If you figure it out, you will replace vinyl about every seven years. My stleel has been on my home in a hail region since 1975, No hail damage. You do the math.\nCheck out this ""How It\'s Made Video"" and see for yourself:\nJames Hardie Siding (i.e. Hardieplank) is the best exterior cladding material (outside of true masonry) on the market and delivers the strongest ROI (return on investment). It\'s imperative, however, that you only get a trained installer to put up this product as installation issues can cause future problems.\nIf you are looking to boost your home\'s curb appeal while also improving energy efficiency, you should consider getting new siding. When you want to go with a quality product that you know will last, James Hardie Siding, should be a serious consideration.\nWhy Get New Siding?\nIf you find that your home\'s current siding is worn out, tattered, cracked, or damaged by termites, getting new siding is the perfect solution. Not only will it make your house look great, but it will increase your home\'s value, making it a sound investment. In fact, replacing your siding was recently ranked the 2nd best remodeling option to increase home value. It will freshen up the look of your house, making it stand out from others in the neighborhood, boosting appeal to prospective buyers. You can even choose a modern siding color, such as dark gray.\nNew siding will help you save money year-round due to the increased energy efficency profile of new materials. High quality siding will help regulate the temperature of your home, reducing heating and cooling costs, no matter the weather.\nYou would be suprised to learn how much the material of your siding can change the entire look of your home. If you want to feel proud of the quality of your home\'s exterior, fiber cement James Hardie siding is for you.\nBenefits of James Hardie Siding\nIn addition to the above benefits, James Hardie siding boasts many advantages over traditional siding because it\'s made with fiber cement. This material is bug resistant, fire resistant and outlasts standard vinyl siding by decades. It\'s as close as it gets to a zero maintenance home exterior. Amazingly, James Hardie has been in buisness since the 1800\'s, so you can be assured of the company\'s product quality and timeless appeal.\nJames Hardie siding will give your home a high quality appearance, while also being durable and sustainable, making it eco-friendly. Being fire resistant, fiber cement siding also offers an extra layer of protection should there ever be a fire near your home. However, one of the most cited reasons why people choose fiber cement is that it will not rot like traditional siding when exposed to humidity and precipitation, making it perfect for the DC area. Furthermore, James Hardie developed the HardieZone® System, which tailors the composition of your fiber siding to your climate zone. The DC area is a part of HardieZone® 5, ensuring the best protection against hail, extreme temperature changes and humidity. This advanced system will allow your siding to resist the elements to protect against mold, swelling and cracking.\nFinally, James Hardie siding comes in a variety of attractive colors, thicknesses, densities, and designs. You\'ll be able to choose a beautiful, customized look for your home. This can include using various plank designs to create intrigue or accentuate different areas of your home with different styles. So if you\'re looking for a beautiful and strong material for your home\'s exterior, look no further than James Hardie.\nWorking with James Hardie Certified Installers\nOnce you\'ve come to see the advantages that James Hardie siding offers, it\'s time to start thinking about the details of a potential installation. To ensure that you get quality work done to make the most of this product, you need to work with a James Hardie Certified Installer, such asBRAX Roofing.\nOn any type of siding project, it\'s important to leave the work to professionals, which will ensure that everything is measured correctly, properly installed, and does not include any gaps. Especially when you are going for a customized design, it\'s essential to use professionals who will work with you to ensure 100% satisfaction.\nProfessionals can also give you advice and insight on what style, thickness, and design would look good with your home. If you cannot decide on a style, they will be able to steer you in the right direction of what would fit with your home, and what combinations would look good if you choose to go with more than one style\nWe are certified James Hardie installer and every answer you received here is true. It is the best siding out there bar none. Here is one of our completed James Hardie jobs. With their ColorPlus technology that will withstand the elements your home trues colors will shine for years to come.\nIt appears that you are missing both step flashing (between the roof and the wall) and the head flashing (above the trim running parallel to the roof line). Here is the link to the Hardie Best Practices manual with specs on both flashings: https://www.jameshardiepros.com/getattachment/9a1017e1-853d-4574-b3e9-7afb1a5a472d/intro-tools-hz5-us-en.pdf.\nHave them take that metal off that they installed ""over"" the existing step flashing. You\'ll need to check the step flashing cards to make sure they are all aligned and in the right places. Chances are they are not. If they are in the right places then it\'s your window leaking. Best I can do for you without running a hose on it.\nNo you could run a hose on it...start along the roof line first and eliminate each area before spraying up towards the window.\nAre you a building professional?\nWhy not answer these questions like a pro?Sign up free', 'By Philip Jalbert, Environmental Protection Agency\nWould you live in a home that was not safe and secure? To be marketable, multifamily housing needs to be both. Moreover, to be safe and to feel secure, a home needs to be healthy. A healthy indoor environment is as attractive to tenants and homebuyers as is a safe and secure one.\nWe spend an average of 90 percent of our time indoors. Each year there are tens of thousands of deaths and millions of illnesses because of poor indoor air quality. Billions of dollars are spent every year on health care related to poor indoor air quality (IAQ), and the loss of school days and business productivity is enormous.\nSome of the most common and serious indoor air quality issues are caused by: too much moisture—leading to mold and mildew; poorly maintained or operated combustion appliances that emit carbon monoxide (CO), nitrous oxide (NO) and particulates; volatile and semi-volatile organic chemicals (VOCs, SVOCs) from building materials and finishings; radon gas seeping into the home from soil under the home; and secondhand smoke. (Readers can find IAQ information and free\nAcceptable indoor air quality (IAQ) and occupant health depend on many factors, such as: occupant behavior and practices (smoking, cleaning, cooking and home maintenance practices); proper building siting, design and construction; proper ventilation; and control of furnishings and other products and sources that emit hazardous pollutants indoors.\nIndoor Air Quality is a particular concern in multifamily buildings. While indoor air quality is a potential concern in all types of housing, in multifamily buildings pollutants can move from unit to unit, and into common areas through openings in walls and ceilings, and through shared ventilation systems. Isolating individual units—often called “compartmentalization”—is sometimes needed to address IAQ issues in multifamily buildings.\nHow can you prevent exposure to second-hand smoke?\nSmoking is an important occupant behavior not only for the person who smokes, but also for those who share the indoor space. In fact, when it comes to multifamily housing, depending on the degree of air sealing and design of a building’s heating, ventilation and air conditioning (HVAC) system, secondhand smoke can adversely affect non-smoking occupants in neighboring units. An estimated 3,000 Americans die each year from lung cancer due to exposure to secondhand smoke or environmental tobacco smoke (ETS). Secondhand smoke is also a significant asthma trigger and causes other important health problems, particularly in children.\nThe two main strategies for preventing exposure to secondhand smoke in multifamily buildings are to prohibit smoking in the building entirely, or seal individual units where smoke-free policies are not currently in place. About 10 percent of public housing is now smoke-free.\nThe Departments of Housing and Urban Development (HUD) and Health and Human Services (HHS) Centers for Disease Control and Prevention (CDC) recently developed two smoke-free toolkits for multifamily housing. The toolkits are for residents, and public housing authorities and owners/management agents.\nHUD also issued two smoke-free notices: HUD Public and Indian Housing Smoke-Free Notice (PIH-2012-25, May 29, 2012); and, HUD Notice on Smoke-Free Housing (H 2012-22, October 26, 2012). On October 4, 20102, HUD also published a Request for Information on smoke-free in the Federal Register.\nIs radioactive radon soil gas an issue?\nEPA’s current risk assessment estimates that exposure to radon in U.S. homes is responsible for 21,000 or 13.4 percent of all lung cancer deaths each year. The risk is highest among smokers. When combined with exposure to radon, smokers’ risk of radon-induced lung cancer increases about ten times.\nMore and more builders of multifamily housing are including radon-reducing features in new homes. In its 2011 annual survey of builder practices, the National Association of Home Builder’s Home Innovation Research Labs estimated that of the 223,400 multifamily housing starts:\n• 19.4 percent included radon-reducing features, up from 15.3 percent in 2010; and\n• 29.5 percent included radon-reducing features in the highest radon potential areas (Zone 1), up from 18.9 percent in 2010.\nRadon source control measures such as active sub-slab soil depressurization (ASD) have other applications and potential benefits. A limited EPA funded study of Pennsylvania residences found that ASD systems can reduce indoor humidity and moisture levels during the non-summer months of the year. ASD systems are also used to manage soil gases and vapor intrusion. The American Association of Radon Scientists and Technologists (AARST) is consulting with EPA to develop a vapor intrusion mitigation credential as an add-on to the AARST-National Radon Proficiency program’s mitigation credential. www.aarst.org/\nWhat’s the federal government doing about radon?\nThe Federal Radon Action Plan is a continuing Federal initiative. One result of the Plan is HUD adopting an unprecedented radon policy for multifamily housing (see above). HUD and EPA are two of nine Federal agencies participating in the Federal Radon Action Plan. The Plan works to reduce the radon risk in housing and schools owned, managed or financed by the Federal Government. The expectation is that Federal leadership by example will spur more private sector action on radon risk reduction.\nAdditionally HUD announced two new notices regarding radon:\n• Office of Multifamily Development, Notice H 2013-03 (Jan. 31, 2013); and,\n• Office of Public and Indian Housing, Notice PIH 2013-06 HA (Feb. 4, 2013).\nThe Multifamily Notice supplements the environmental review process and requirements for the identification and mitigation of radon in new FHA-insured mortgage applications. The Public and Indian Housing notice encourages Public Housing Authorities to plan and complete radon testing and mitigation when appropriate.\nDo energy retrofits affect IAQ?\nMore than ever before American homes are being upgraded or remodeled. This trend is likely to increase in the future as efforts to improve building energy efficiency continue. The benefits of energy upgrades are tremendous—improved real property value, lower costs and sustained American jobs. However, energy upgrades can inadvertently worsen indoor air quality. This is true when there is not an appropriate assessment made before work begins or if work is done improperly.\nTo help guide upgrade activities, the Department of Energy (DOE) developed Guidelines for Home Energy Professionals. Working in concert with DOE, EPA\ndeveloped the Healthy Indoor Environment Protocols for Home Energy Upgrades—\nto complement the Guidelines. The Protocols apply to existing single-family and multifamily low-rise residential buildings.\nThe pollutants to be concerned about are: asbestos, VOCs, moisture, CO, ETS, ozone, NO, lead, PCBs, radon, combustion emissions, and below ground contaminants.\nThe Protocols provide detailed guidance on assessment, and minimum and expanded actions to remedy problems for each of the pollutants. The protocols provide practical guidance on improving or maintaining indoor air quality and indoor environments during home energy upgrades or remodeling.\nAnother valuable resource when rehabbing occupied buildings is the IAQ Guidelines for Occupied Buildings under Construction, 2nd Edition (ANSI/SMACNA 008–2008), produced by the Sheet Metal and Air Conditioning Contractors’ National Association.\nCan climate change affect IAQ?\nA recent Institute of Medicine (IOM) study found that climate change is likely to make existing indoor air quality problems more widespread and severe. The IOM, part of the National Academy of Sciences (NAS), evaluated the state of scientific understanding of the effects of climate change on indoor air quality and public health.\nPhilip Jalbert is executive secretary with the Federal Interagency Committee on Indoor Air Quality at the U.S. Environmental Protection Agency’s Indoor Environments Division.']"	['<urn:uuid:d00b6b75-0092-45a0-8175-ef6c2dc2f7cc>', '<urn:uuid:dddb494d-276e-473c-ab00-56382a7039df>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	6	85	3129
20	qlik fivetran integration vs databricks unity catalog metadata management capabilities differences	Qlik's integration with Fivetran focuses on unifying governed data from various sources and providing visibility into data movement through Fivetran-managed pipelines, while Databricks Unity Catalog specializes in metadata management and governance specifically for lakehouse assets, offering automated lineage tracking and unified permissions model across Databricks resources.	"[""BUSINESS INTELLIGENCE,BIG DATA MANAGEMENT,DATA SCIENCE\nMakersite | September 28, 2022\nMakersite, a world leader in bringing sustainability and cost insights into the early stage design process for the world’s leading brands, today announced partnering with Autodesk, the leader in product design software. The new partnership combines Makersite’s environmental impact and cost data with Autodesk Fusion 360’s product design data.\nSustainability begins at the heart of the product: its design phase. Still, less than 1% of products have sustainability as a design parameter. Even though the general public’s wish for sustainable products grows and emission regulations worldwide are becoming more and more, incorporating sustainability at the design level has been a challenge for most product designers in the past. Makersite’s partnership with Autodesk is changing this.\nThe new Fusion 360 plug-in features:\nAllows designers to have Makersite instantly calculate the environmental and cost impacts of their design at the push of a button\nGives Fusion 360 users access to over 300 materials, cost, and sustainability insights based on the used structure, materials, and weight\nProvides enhanced data sets on over 50 decision criteria such as compliance, risk, health, and safety in real-time\nWith this ground-breaking approach, product designers will no longer depend on experts or consultancies to design sustainable products. Instead, enterprise manufacturers will be able to use their own material masters and procurement data to enable teams to work toward sustainability goals led by design. This integration will enable more sustainable and successful designs, eliminate duplicative efforts and decrease time to market.\n“The stats tell us that 80% of the ecological impacts of a product are locked down in the design phase. Therefore, the design phase of a product is the first and most necessary stage to get more sustainable goods into the world successfully,” shares Neil D’Souza, founder of Makersite. “However, eco-design is only feasible when designers have data about the sustainability of their product and its compliance, costing, environmental, health, and safety criteria. By integrating our data, AI, and calculation engines into Fusion, product designers are provided with clear and actionable insights so they can decide how to make their designs more sustainable,” D’Souza concludes.\n“It’s Autodesk’s intent to make designing for sustainability easily accessible, and ultimately intuitive, to product designers,” said Zoé Bezpalko, Autodesk Senior Design and Manufacturing Sustainability Manager. “By partnering with Makersite, we’ve created a holistic workflow within Fusion that provides insights into sustainable design directly within the design environment. Data-driven analysis from Makersite will enable manufacturers to make better decisions about creating safer, more sustainable products,” she said.\n“Companies are setting ambitious sustainability goals at high levels, sometimes as required by policy, but increasingly due to customer demand and as a source of competitive differentiation. The data that drives achieving those goals are often in disparate systems throughout the organization,” said Stephen Hooper, Autodesk Vice President & General Manager, Fusion 360. “We’re connecting relevant LCA data to the Fusion design workspace to help manufacturers meet their important sustainability goals,” said Hooper.\nAutodesk will hold its premier annual conference for product designers and manufacturers, Autodesk University, in New Orleans September 27-29, 2022. Makersite will take the stage with Zoé Bezpalko to present the plug-in to attendees during the conference.\nMakersite's SaaS platform delivers enterprise digital twins to enable change in complex business environments. By intelligently mapping customers' product data via AI with live data from 140+ supply chain databases, Makersite instantly delivers deep-tier supply chain twins with 90%+ accuracy. Customers can assess the digital product twins across 30+ business criteria such as risk, sustainability, compliance, and cost. The platform has many applications, helping global enterprises build resilient supply chains, accelerate product innovation, and achieve NetZero.\nBUSINESS INTELLIGENCE,BIG DATA MANAGEMENT\nAlation | September 23, 2022\nAlation Inc., the leader in enterprise data intelligence solutions, today announced a strategic partnership with Fivetran, the global leader in modern data integration, that enables joint customers to find and understand the full context of their data in the modern data stack. The partnership, powered by the new Fivetran Metadata API, unifies governed, trusted data from sources across an organization in one single view. As a result, data visibility increases and drives higher-quality data pipelines and better decision-making.\nAlation and Fivetran are key elements of the modern data stack and support numerous joint customers, including Cimpress, Cisco, DocuSign, Genius Sports, Nielsen, Procore, Salesforce, and Slice. The strategic partnership provides shared customers with visibility into enterprise data as it moves through Fivetran-managed pipelines. Fivetran enables data and analytics teams to securely and easily move data from operational systems to analytics platforms. Alation enhances visibility into the data to speed pipeline development, ensuring users understand the wealth of information available and can derive insights from it. Together, the partnership merges data governance and data intelligence capabilities, improving data pipeline creation and informing business decisions.\n“Alation is foundational for driving digital transformation. Our platform enables data and analytics teams to capture and understand the full breadth of their data, resulting in improved operational efficiencies and delivering more value to customers. “The partnership with Fivetran enhances Alation’s lineage graph, providing deep insight into data pipelines spanning operational data stores, data lakes and warehouses, and business intelligence tools. Now, users can easily unlock reliable and relevant data while also allowing teams to understand what data is available in the pipeline.”\nRaj Gossain, Chief Product Officer at Alation\nThis partnership is driven by the need to enable users across an organization to find and use trusted, governed data to make informed, data-driven decisions. The modern enterprise relies on data intelligence and data integration solutions to provide access to valuable insights that feed critical business outcomes. Alation and Fivetran’s partnership simplifies data consolidation and pipeline creation, while aggregating governed, reliable data so organizations can get the most value from their data.\n“Fivetran and Alation together enhance an organization’s visibility and effectiveness of data,” said Meera Viswanathan, Sr. Product Manager for Fivetran’s data governance capabilities. “Users of both solutions can accelerate data movement with a single pane view of data governance. Jointly, Fivetran and Alation ensure end users of data have high-quality and trusted data to drive decision-making.”\nAlation is the leader in enterprise data intelligence solutions including data search & discovery, data governance, data stewardship, analytics, and digital transformation. Alation’s initial offering dominates the data catalog market. Thanks to its powerful Behavioral Analysis Engine, inbuilt collaboration capabilities, and open interfaces, Alation combines machine learning with human insight to successfully tackle even the most demanding challenges in data and metadata management. More than 400 enterprises drive data culture, improve decision making, and realize business outcomes with Alation including AbbVie, Allianz Global Investors, American Family Insurance, Autozone, Cisco, Draft Kings, Exelon, Fifth Third Bank, Finnair, General Mills, Munich Re, NASDAQ, Parexel, Pfizer, Salesforce, Virgin Australia, and Vistaprint. Headquartered in Silicon Valley, Alation has been named to Inc. Magazine’s Best Workplaces list three times and is a 2022 UK’s Best Workplaces™ for Women. The company is backed by leading venture capitalists including Blackstone, Costanoa, Data Collective, Dell Technologies, Hewlett Packard Enterprise, Icon, ISAI Cap, Riverwood, Salesforce, Sanabil, Sapphire, and Snowflake Ventures.\nBIG DATA MANAGEMENT,DATA SCIENCE\nQlik | September 27, 2022\nQlik® today announced two significant enhancements to its partnership with Databricks that make it easier than ever for customers to combine Qlik’s solutions and Databricks to advance their cloud analytics strategies. First is the launch of the Databricks Lakehouse (Delta) Endpoint, a new capability in Qlik Data Integration, which will simplify and improve customers' ability to ingest and deliver data to the Databricks Lakehouse. Second is the integration of Qlik Cloud® with Databricks Partner Connect, enhancing the Qlik Data Analytics trial experience with Databricks. Both deepen and expand the ability of customers to combine Qlik and Databricks in their efforts to leverage the cloud for impact.\n“We’re excited about the potential of Qlik’s Databricks Lakehouse (Delta) Endpoint to seamlessly and efficiently deliver the data customers need to drive more value from their investment in the Databricks Lakehouse. “And, with Qlik Analytics now integrated with Databricks Partner Connect, we are making it even easier for customers to discover, use and share data-driven insights across their organizations.”\nRoger Murff, VP of Technology Partners at Databricks\nLeveraging Databricks SQL, Qlik’s Databricks (Delta) Endpoint optimizes the continuous and real-time data ingestion through Qlik Data Integration into Delta Lake on Databricks. This gives organizations the ability to cost effectively drive more data from a wide range of enterprise data sources, including SAP and Mainframe, into the Databricks Lakehouse while leveraging their cloud provider of choice such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or Microsoft Azure.\nQlik has also recently integrated Qlik Cloud with Databricks Partner Connect. Databricks customers can now seamlessly experience Qlik Sense® SaaS within the Databricks Lakehouse Platform through an existing Qlik tenant or a free Qlik trial. The experience includes automatic configuration of connectivity to the customer’s Databricks environment, making it easier for Databricks customers to experience the power of Qlik Cloud.\nBoth the new Databricks Lakehouse (Delta) Endpoint and Partner Connect integration demonstrate Qlik’s commitment to supporting customers like J.B. Hunt in their efforts to combine Qlik and Databricks for impact.\n“We’re seeing more demand for real-time data related to shippers and carriers in order to provide up-to-the-minute information on how they are performing. Qlik and Databricks help us to meet those demands,” said Joe Spinelle, Director of Engineering and Technology at J.B. Hunt.\n“As they migrate more and more to the cloud, Databricks customers want strategic partners that make it as easy as possible to deliver and analyze data for impact,” said Itamar Ankorion, SVP of Technology Alliances at Qlik. “With our new Databricks Lakehouse Endpoint and Databricks Partner Connect integration, Qlik is clearly demonstrating its alignment with Databricks and our dedication to the Databricks community to deliver an amazing experience that furthers their overall data strategies.”\nQlik’s vision is a data-literate world, where everyone can use data and analytics to improve decision-making and solve their most challenging problems. A private company, Qlik offers real-time data integration and analytics solutions, powered by Qlik Cloud®, to close the gaps between data, insights and action. By transforming data into Active Intelligence, businesses can drive better decisions, improve revenue and profitability, and optimize customer relationships. Qlik serves more than 38,000 active customers in over 100 countries."", 'Databricks Metadata Management — FAQs, Tools, Getting Started\nSeptember 15th, 2022\nShare this article\nDatabricks enables metadata management of its lakehouse assets using Unity Catalog — the metadata and governance layer with automated lineage.\nWhile Unity Catalog is effective for Databricks assets, it’s not enough when you must must view the metadata from Databricks along with other data tools in your stack. That’s where a catalog of catalogs like Atlan can help.\nIn this article, we’ll discuss the importance of metadata management for Databricks assets, the capabilities of Unity Catalog, and the benefits of integrating it with Atlan.\nWhy is metadata management important for Databricks assets?\nMetadata is the key to finding and analyzing every data asset (schema, column, dashboard, query, or access log) and understanding how they affect your workflows. Since warehouses are organized and structured, handling metadata within each warehouse isn’t too complex. The challenge mainly lies in making the metadata searchable and easy to understand.\nHowever, the data ingested into data lakes don’t have a specific format or structure. Without a mechanism to organize and manage metadata, data lakes can quickly transform into nightmarish data swamps (also known as data dumps or one-way data lakes). In such an environment, data gets stored but isn’t in any shape to be used.\nAs a result, there’s no context on data, leading to the chaos around questions such as:\n- When was a data set created?\n- Who’s the owner?\n- Where did it come from?\n- What’s the data type?\nThis information comes from the metadata and its effective management.\nManaging metadata is also the most effective way to ensure data quality and governance by tracking, understanding, and documenting all types of metadata — business, social, operational, and technical metadata.\nFor instance, metadata management can help you ensure that the data uploaded to a table matches its schema. Similarly, it helps you verify if someone’s allowed to view a specific table of sensitive data and logs all accesses automatically for easier audits.\nThat’s why data lakes and lakehouses (like Databricks) can only be effective with a proper metadata management system in place.\nBut before exploring metadata management in Databricks, let’s understand its history and origins.\nWhat is Databricks?\nDatabricks is a company that offers a cloud-based lakehouse platform for data engineering, analytics, and machine learning use cases.\nThe platform started as an open-source project (i.e., Apache Spark™) in academia. One engineer describes the core Databricks platform as “Spark, but with a nice GUI on top and many automated easy-to-use features.”\nToday, Databricks is valued at $38 billion and is considered to be one of the world’s most valuable startups, joining the ranks of Stripe, Nubank, Bytedance, and SpaceX.\nDatabricks: An origin story (TL;DR)\nThe official release of Hadoop in 2006 helped companies power their data processing and analytics use cases with horizontal scaling of database systems. However, Hadoop’s programming framework (MapReduce) was complex to work with and led to the development of Apache Spark™.\nAli Ghodsi and Matei Zaharia set out to solve the problem of simplifying working with data by developing Apache Spark™ — an open-source platform for processing massive volumes of data in different formats. Companies used Spark to directly read and cache data.\nAccording to Zaharia:\n“Our group was one of the first to look at how to make it easy to work with very large data sets for people whose main interest in life is not software engineering.”\nApache Spark™ was so good that it set the world record for speed of data sorting in 2014.\nHowever, setting up Apache Spark™ for clusters of servers and adjusting various parameters to ensure peak performance was tedious and challenging. This eventually brought the creators of Apache Spark™ together with other academics to collaborate on the Databricks project — a cloud data lakehouse platform that’s powered by Apache Spark™.\nTo understand the significance of the project, let’s go back to 2015 and look at the problems with the existing warehouse-lake data architecture.\nWhy did the lakehouse architecture become so important?\nIn 2015, when cloud data lakes such as Amazon S3 and Azure Data Lake Storage (ADLS) became more popular and replaced the Hadoop systems, organizations started using a two-tier data architecture:\n- A cloud data lake as the storage layer for large volumes of structured and unstructured data\n- A cloud data warehouse, like Amazon Redshift or Snowflake, as the compute layer for data analytics\nAccording to the founders of Databricks — Michael Armbrust, Ali Ghodsi, Reynold Xin, and Matei Zaharia — such an architecture was:\n- Complex: Data undergoes multiple ETL and ELT processes.\n- Expensive: Continuous ETL processes incur substantial engineering overhead. Moreover, once the data is in the warehouse, the format is already set, and migrating that data to other systems involves further costly transformations.\n- Unreliable: Each ETL step can lead to new failure modes or bugs, affecting data quality. Additionally, the data in the warehouse is stale, when compared to the lake data — constantly fresh and updated. This lag leads to increased waiting time for insightful data and introduces engineering bottlenecks as business teams wait for engineers to load new data in the warehouses.\n- Not ideal for advanced analytics: ML systems such as PyTorch and TensorFlow process large volumes of data using non-SQL queries. Warehouses aren’t equipped to handle these formats, leading to further data transformations for each use case.\nThe solution is to build a platform that combines the best of both worlds — the open data format of a lake and the high-performance and data management capabilities of a warehouse.\nThat’s how the concept of a lakehouse, pioneered by Databricks, rose to prominence.\nThe USP of the lakehouse architecture: Metadata management\nArmbrust, Ghodsi, Xin, and Zaharia define a lakehouse as:\n“A low-cost, directly-accessible storage with traditional analytical DBMS management and performance features such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization.”\nA lakehouse has five layers — ingestion, storage, metadata, API, and consumption. The key difference between the lake-warehouse architecture vs. the lakehouse architecture is the metadata layer.\nThe metadata layer sits on top of the storage layer and “defines which objects are part of a table”. As a result, you can index data, ensure versioning, enable ACID transactions, and support other data management capabilities.\nThis metadata (now called metadata and governance layer) is the Databricks Unity Catalog, which indexes all data within the lakehouse architecture, such as the schemas, tables, views, and more.\nWithout this metadata layer, each Databricks workspace will be a silo.\nHere’s how Databricks describes its Unity Catalog:\n“Unity Catalog is a unified governance solution for all data and AI assets including files, tables, machine learning models, and dashboards in your lakehouse on any cloud.”\nThink of Unity Catalog as a collection of all databases, tables, and views in the Databricks lakehouse.\nA Guide to Building a Business Case for a Data Catalog\nDownload free ebook\nDatabricks metadata management capabilities: Metadata management in the Unity Catalog\nUnity Catalog is an upgrade to the old meta store within Databricks, the caveat being better permissions model and management capabilities. Using Unity Catalog, you can manage schemas, access controls, sensitive data cataloging, and generate automated views for all Databricks assets.\nThe Databricks Unity Catalog enables:\n- Easy data discovery\n- Secure data sharing using the Delta Sharing protocol — an open protocol from Databricks\n- Automated and real-time table and column-level lineage to understand data flows within the lakehouse\n- A single and unified permissions model across all Databricks assets\n- Centralized auditing and monitoring of lakehouse data\nLet’s look at the most prominent use cases powered by the metadata and governance layer of Databricks.\nWhat use cases can you power with metadata management of Databricks?\nDiscovery and Cataloging\nWith metadata management, it’s easier to discover trustworthy data and get adequate context on it as it indexes every asset inside the lakehouse — data sets, databases, ML models, and analytics artifacts.\nLineage and impact analysis\nMetadata management powers lineage capability across tables, columns, notebooks, workflows, workloads, and dashboards. As a result, you can:\n- Trace the flow of data across Databricks workspaces\n- Track the spread of sensitive data across datasets\n- Monitor the data transformation cycle\n- Analyze the impact of proposed changes to downstream reports\nCentralized governance, security, and compliance\nModern metadata management platforms automatically identify and classify PII and other sensitive data in the lakehouse. In addition, it maintains centralized audit logs on data access and use (as mentioned earlier), which helps govern usage.\nThey also centralize access control with a single and unified permissions model. As a result, access control and data privacy compliance have become simpler.\nThe Unity Catalog is sufficient for metadata management of Databricks lakehouse assets, however, the modern data stack has several tools (dashboards, CRMs, warehouses), each with a metastore acting as the tool’s data catalog. To get a 360-degree view of all data in your stack, you must set up a catalog of catalogs with active metadata management, and that’s where Atlan comes into the picture.\nAtlan + Databricks: Activating Databricks metadata\nAtlan activates your Databricks data with active metadata management — a two-way flow of metadata across data tools in your stack. Think of it as reverse ETL for metadata.\nWhy does active metadata management matter?\nThe core premises of active metadata management are:\n- As and when data changes, the associated metadata also changes automatically\n- As data is used, updated, or modified, metadata keeps getting generated and cataloged\n- The management of metadata should be automated using open, extensible APIs\nAtlan checks all three boxes to provide you with:\n- An active, living single source of truth with 360° profiles of data assets\n- Proper context on data using the tools of your choice, i.e., embedded collaboration\n- An open API architecture, empowering you to connect Unity Catalog’s REST API with Atlan to extract metadata from Databricks clusters and workspaces\nWhen you pair the Databricks metadata with metadata from the rest of your data assets, you can achieve true cross-system lineage and visibility across your data stack.\nThe benefits of activating Databricks metadata using Atlan\n- Build a home for all kinds of metadata — technical, business, personal, and custom metadata with Atlan’s no-code Custom Metadata Builder\n- See column-level data flows from your lakehouse to BI dashboards without switching apps\n- Analyze which downstream assets will get affected by changing any Databricks asset and alert the relevant data owners\n- Create Jira tickets for broken assets for action on broken assets\n- Discuss the impact of these changes using your tool of choice, like Slack\nTo see Atlan and Unity Catalog in action, enabling active metadata management, check out this live product demo:\nA Demo of Atlan and Databricks Unity Catalog in action\nRelated resources on integrating Unity Catalog with Atlan\n- How to set up Databricks\n- How to crawl Databricks\n- How to extract lineage from Databricks\n- What does Atlan crawl from Databricks?\nIf you are evaluating an enterprise metadata management solution for your business, do take Atlan for a spin — Atlan is a third-generation data catalog built on the premise of embedded collaboration that is key in today’s modern workplace, borrowing principles from GitHub, Figma, Slack, Notion, Superhuman, and other modern tools that are commonplace today.\nDatabricks metadata management: Related reads\n- What is a data lake and why it needs a data catalog?\n- What is active metadata management? Why is it a key building block of a modern data stack?\n- Enterprise metadata management and its importance in the modern data stack\n- Data catalog vs. metadata management: Understand the differences\n- Activate metadata for DataOps\n- Databricks data governance: Overview, setup, and tools\nShare this article']"	['<urn:uuid:ccd6c120-7878-4e19-989a-c3ee1eb7c47f>', '<urn:uuid:4aeea8f5-db05-47f0-983e-307110480c84>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	11	46	3677
21	small scale vs large scale farming adaptation climate change	Small-scale and large-scale farming show different capacities for adapting to climate change. Small-scale approaches allow for more localized adaptation since what works for one farmer may not work for another hundreds of kilometers away due to differences in soil, microclimate, and topology. This is evidenced by the Indonesia Transmigration Program, where farmers' productivity increased by 20% when matched to familiar growing conditions. In contrast, large-scale, one-size-fits-all approaches that dominated since the 1970s are showing limitations, with yields plateauing in major food-producing regions like East Asia and Northwest Europe. As weather patterns become more unpredictable due to climate change, gains in production will increasingly depend on context-specific innovation rather than broad standardized solutions. Small-scale farming's flexibility in adapting practices to local conditions may therefore be more resilient to climate challenges.	"['Better matching between transmigrants’ origin and destination agroclimate and languages can substantially improve productivity and integration\nForced migration due to climate change, natural disasters, and conflict is among the most important policy challenges of the 21st century. Myers (2005) argues that as higher temperatures lead to rising sea levels, coastal flooding, droughts, and new rainfall patterns, these changes could displace nearly 200 million people by 2050. In South Asia alone, extreme weather events are expected to uproot over 60 million people (Stern 2007). War will also continue to displace populations, as exemplified by the recent conflict in Syria that has already produced more than nine million refugees and is expected to produce even more.1\nWhile income transfers may be a first best solution to compensate victims of natural disasters or conflict, the poor state of land and housing markets in many developing countries often makes cash-based solutions difficult to implement. As an alternative, some governments are considering rural-to-rural resettlement programs, where displaced migrants are given new farmland and housing in newly developed areas. For such programs to be effective, they need to be well suited to the needs of poor households in rural areas, where rain-fed agriculture remains a primary source of subsistence.\nA well-designed resettlement program would allow migrants to maximise their long-run productivity in a new environment. In designing these programs, a crucial question is whether the skills acquired in one setting are transferrable to economic activities in another. If skills are not readily transferable, then policymakers may need to play an active role as a matchmaker, using information on settlers’ skills at their origin to match the environment at the destination.\nAlthough barriers to skill transferability are very important in theory, measuring the magnitude of their impacts, or productivity losses from poor matches, is difficult. Voluntary migrants tend to choose locations where they expect their skills to be most transferable and they have been selecting favourable locations for millennia. For example, Steckel (1983) and Diamond (1997) document a tendency for migrants to move from east to west, rather than from north to south, as they settled the agricultural frontier. Because favourable soil and climate conditions shape destination choices due to a desire to work in familiar environmental conditions, it is typically not possible to draw inferences about the magnitudes of barriers to skill transferability from observational data.\nIn recent work, we use a natural experiment—Indonesia’s largest resettlement program, known as the Transmigration Program—to measure barriers to skill transferability (Bazzi et al. 2016). Between 1979 and 1988, the government of Indonesia relocated nearly 2 million voluntary migrants from rural areas in Java and Bali to rural areas in the Outer Islands, to reduce population pressures and stimulate agricultural production (see Figure 1). Transmigrant households were given free transport to the newly created settlements. Upon arrival, they received free housing, a two-hectare plot of farmland (allocated by lottery) and provisions for the first few growing seasons, including seeds, tools, and food.2\nFigure 1 Map of transmigration villages\nSource: Bazzi et al. (2016).\nDue to various logistical constraints and an ad hoc, ‘plan-as-you-proceed’ approach to program implementation (World Bank 1988), transmigrants experienced as-if random assignments to destinations. Some transmigrants were sent to similar locations in the Outer Islands, while others were sent to very different farming environments. None had a say in their ultimate destination. The program’s unprecedented geographic scope, with destination villages scattered across the Outer Islands, meant that migrants from diverse origins across Java/Bali are observed across a range of new agroclimatic conditions in the destinations. This gave rise to wide-ranging, plausibly exogenous variation in the assignment of migrants to destinations, which allows us to identify the causal effect of barriers to skill transferability on productivity.\nWe use rich geospatial data on soil, hydrography, topography, and climate to construct an agroclimatic similarity index that measures the similarity in the farming environment between a migrant’s origin and her destination. Given the as-if random assignment of transmigrants, the coefficient of this similarity measure in an agricultural productivity regression identifies the causal effect of skill transferability on migrants’ productivity.\nWe find that skill transferability has large effects on village-level rice productivity. Rice was the focal crop of the program and is a primary staple, both for Indonesia and for more than half of the world. Our estimates imply that, on average, an increase in agroclimatic similarity by one standard deviation leads to a 20% increase in village-level rice productivity. This translates to an additional 0.5 tons per hectare. This effect is roughly equivalent to twice the productivity gap between farmers with no schooling versus those that have completed middle school.\nWhere matching matters more\nInterestingly, skill transferability has nonlinear effects on productivity. Migrants from the most dissimilar origins face the largest productivity losses. This suggests not only that resettlement planners should try to match migrants to similar destination environments but also that they should be particularly concerned about minimising bad matches. A simulation that mimics these policy recommendations suggests that a more optimal assignment of transmigrants could have increased aggregate rice production in the new settlements by 27%.\nAlthough matching resettled populations to destinations is a good way to ensure that migrants remain productive, other coping mechanisms are possible. Many food crops like rice are particularly sensitive to agroclimatic conditions, but other crops, such as cash crops like palm oil, appear less sensitive to growing conditions and may be an alternative for farmers who face unfamiliar growing conditions. Moreover, due to the complexities of soil management (De Datta 1981), barriers to transferability appear to be greatest for soil-specific knowledge. Resettlement programs need to be coupled with effective agricultural extension services that can train farmers in how to manage crops in their new environments.\nOne lesson from the Transmigration Program has broader implications for resettling diverse migrant populations. We find that migrants whose native language is similar to the local indigenous language in the Outer Islands are more likely to switch into trading and services occupations. Beyond highlighting the importance of careful matching in allocating migrants across destinations, this finding also suggests that language training can help facilitate adaptation.\nAnother important feature of the Transmigration Program was its goal of encouraging mixing between geographically and culturally disparate ethnic groups. By sending inner-Island migrants to new villages in the outer-Islands, and by encouraging locals to migrate to these settlements, the program created an initial shock to ethnic diversity, which we show persisted for decades. In follow-up work, we further investigate how this diversity shock affected interethnic cooperation and identity choices in the new settlements (Bazzi et al. 2017).\nWe find that the increased scope for contact with other groups gave rise to a significant increase in both interethnic marriage as well as the adoption of the national language, Bahasa Indonesia, as the primary language used at home. These changes constitute significant shifts in the process of socialisation and identity formation with broader implications for nation building. Children raised in these settings exhibit stronger national affinity, less co-ethnic bias, and weaker attachment to their own ethnic identity. Interestingly, we observe the strongest shifts in the most diverse communities where Inner and Outer Island ethnic groups are in roughly equal proportion.\nFrom a policy perspective, there are a few key forces giving rise to this increased integration amidst such a large and abrupt increase in local diversity. First, a lottery-based policy of residential desegregation within communities fostered contact between groups that may not have otherwise chosen to mix locally. Repeated social and economic contact may have accelerated learning, breaking down stereotypes, and promoting cooperation. Second, groups resettled in more remote locations tended to integrate faster on account of more limited scope for finding alternative venues for economic activity and exchange. Moreover, integration is deeper in contexts where migrants had incentives to interact with locals to learn about optimal farming practices. Hence, skill differences could be harnessed to encourage intergroup exchange.\nResettlement programs are logistically challenging and expensive. The land clearing associated with resettlement can be environmentally damaging, resulting in deforestation that could lead to even greater greenhouse gas emissions—exacerbating the problem that they were trying to solve.3 Nevertheless, with careful planning, resettlement remains a feasible and politically attractive option for coping with forced migration in many settings. The lessons from Indonesia’s Transmigration Program should help inform ongoing resettlement planning.\nPhoto credit: Asian Development Bank.\nBazzi, S, A Gaduh, A Rothenberg and M Wong (2016), “Skill transferability, Migration, and development: Evidence from population resettlement in Indonesia”, American Economic Review 106(9): 2658-2698.\nBazzi, S, A Gaduh, A Rothenberg and M Wong (2017), “Unity in diversity: Ethnicity, migration and nation building in Indonesia”, Working Paper.\nDe Datta, S K (1981), Principles and Practices of Rice Production, Hoboken: John Wiley & Sons.\nde Sherbinin, A, M Castro, F Gemenne, M M Cernea, S Adamo, P M Fearnside, G Krieger, et al. (2011), “Preparing for resettlement associated with climate change”, Science 344(6055): 456-457.\nDiamond, J (1997), Guns, Germs, and Steel, New York: W W Norton & Company.\nFearnside, P M, (1997), “Transmigration in Indonesia: Lessons from its environmental and social impacts”, Environmental Management 21(4): 553-570.\nMyers, N (2005), “Environmental refugees: An emergent security issue”, OSCE 13th Economic Forum Documents.\nOliver-Smith, A and A de Sherbinin (2014), “Resettlement in the Twenty-First century”, Forced Migration Review 45: 23-25.\nSteckel, R H (1983), “The economic foundations of East-West migration during the 19th century”, Explorations in Economic History 20(1): 14–36.\nStern, N (2007), The Economics of Climate Change: The Stern Review, Cambridge, UK: Cambridge University Press.\nWorld Bank (1988), Indonesia: The Transmigration Program in Perspective, World Bank Country Study, Washington, DC: World Bank.\n See http://syrianrefugees.eu/ on the Syrian refugee crisis. Further background on resettlement policy challenges can be found in de Sherbinin et al. (2011) and Oliver-Smith and de Sherbinin (2014).\n Journey to a New Life: The Human Face of Indonesia (1984) is a short film about the Transmigration Program that follows a family moving from East Java to Southeast Kalimantan. You can watch it here.\n For instance, Fearnside (1997) argues that the Transmigration Program was an important source of deforestation in Indonesia, particularly in peat forest areas, which led to substantial greenhouse gas emissions.', ""A new wave of small-scale agricultural innovation will boost yields and protect the planet, contend Tom MacMillan and Tim G. Benton.\nClimate change threatens a creaking food system in which harvests are already lagging behind rising demand1,2. A sustainable supply of food hinges on agricultural innovation, but current investments neglect a key area for improving yields.\nSince the 1970s, agricultural research and development (R&D) has invested mainly in a few research institutes equipped with cutting-edge instruments. For example, the Biotechnology and Biological Sciences Research Council, responsible for much of the public research spending in food security in the United Kingdom, invested 27% of its 2010–11 budget in just three institutes. Multinational seed and agrochemical companies invest billions of dollars to develop products in hopes that they will be used by millions of farmers.\nThis one-size-fits-all approach has had qualified success. In a 2011 analysis3, average global crop yields increased by 56% between 1965 and 1985, and by 20% from 1985 to 2005, underpinned by increasing inputs of non-renewable resources.\nBut advances are slowing. According to a 2013 study4, yields have plateaued in some of the world's most important food-producing regions, including east Asia (for rice) and northwest Europe (for wheat). In some countries, yields have declined.\nThe next wave of innovation must be at smaller scales. What one farmer can do to boost yield or efficiency is not necessarily the same as for a farmer hundreds of kilometres away with different soil, microclimate, topology and methods. How well crops and livestock grow depends on the interaction of genes, management and environment. As weather patterns fluctuate, gains in production will depend ever more on innovating in context. Big knowledge flowing from institute to farm must be complemented by local knowledge.\nEnhancing farmers' own R&D could reap big rewards for minimal extra cost. Farmers everywhere are practical experimentalists who understand the idiosyncrasies of their land5. Modern agronomy evolved out of practices such as rotating crops to rebuild soil nutrients, fertilizing fields with manure, and adding lime to soil to alter pH. Even technologies not invented by farmers — new kit, seeds or chemicals — are adapted by them to fit their circumstances.\nSuch essential contributions are rarely recognized in official assessments of agricultural R&D. These count farmers as users, rather than makers, of knowledge. When the US Department of Agriculture tots up the US$20 billion that the global private sector invests annually in agricultural R&D, it does not include that done by farmers6. Makers of farm machinery, pesticides, seeds and other 'inputs' invest around 3–11% of their revenue in R&D. Globally, if farmers' innovations were valued at just 0.5% of farming production — worth $4 trillion — that would match formal R&D investment from the private sector.\nSome of the best returns can come from helping farmers to assess their own ideas. Until now, such initiatives have been at arm's length from formal science, and almost exclusively in the developing world. Our involvement in a farmer-focused innovation programme in the United Kingdom has convinced us that such participatory R&D could also boost agricultural innovation in rich countries.\nFarmer-centred approaches are not new. In villages in Kenya, rice fields in Indonesia and other places out of reach from industrialized agriculture, group learning programmes recognize and support farmers as innovators.\nThe best known of these is the farmer field school approach, in which groups of farmers meet regularly to learn alongside their neighbours. The UN Food and Agricultural Organization set up the first such school in Indonesia in 1989, aiming to reduce rice farmers' reliance on pesticides by enabling them to observe, identify and actively manage pests' natural enemies.\nSince then, at least 10 million smallholder farmers have taken part in field schools across Asia, Africa and Latin America. This year, a meta-analysis7 of 71 projects found that farmers' experiences of these schools vary enormously, with targeted initiatives being more successful than large-scale national programmes. In targeted initiatives, participants gained knowledge, changed practices and consequently netted higher yields and incomes.\nInspired by the approach, a UK programme adapts participatory learning to suit farmers in the industrialized world, who, in many cases, are not short of capital, training or access to knowledge. Piloted in 2012, the Duchy Originals Future Farming Programme is funded by the Prince of Wales's Charitable Foundation, in turn funded by sales of products through the supermarket chain Waitrose. The work is led by two charities, the Soil Association in Bristol (at which T.M. works, and T.B. serves on the programme steering group) and the Organic Research Centre in Newbury. The aim is to help farmers to sharpen their skills as innovators so that they can be more productive with fewer non-renewable inputs — good for the environment and their bottom line.\nGroups of 5–15 farmers tackle a problem put forward by a participant and test solutions over up to a year in as many as four workshops on one of their own farms. A facilitator helps to keep meetings on track, and a relevant researcher — crop or animal scientist, agronomist or ecologist — is on hand to advise on experimental design and point out existing studies to avoid redundant work.\nSo far, 450 UK farmers have piloted 'field labs' for about 20 topics, with results documented publicly online. Their farms range from under a hectare to more than a thousand. Field labs have tested ways to control black grass (a persistent weed that resists herbicides), assessed the economics of keeping hens alive to lay eggs for a second season, and evaluated ways to reduce use of drugs that control liver fluke in sheep.\nThese field labs do not always provide clear answers because of their small samples and short timescales. Field labs raise scientific standards nonetheless: early evaluation suggests that most farmers who have taken part in field labs are eager to engage with formal research. And some have yielded useful lessons. In one, vegetable growers tested composts that do not include peat (which releases greenhouse gases when mined). In contrast to prevailing views, the farmers deemed these peat-free composts commercially viable. And the agronomists learned how labour savings from easily handled materials can outweigh business benefits of higher germination rates.\nGrass-roots R&D projects are cropping up elsewhere in the developed world. A participatory breeding programme sponsored by the European Union (EU) has recruited farmers to develop crops that can be grown more sustainably. Organic and small-scale farmers in Italy and France (and in some developing countries) are testing and selecting varieties of barley, beans, broccoli, maize (corn), tomato and wheat.\nAnimal scientists in Denmark adapted the farmer field school approach to develop 'stable schools'. Four groups of around five farms each worked together to assess changes to herds' housing, hygiene and milking practices and reduced use of antibiotics8. A study funded by the European Commission is evaluating 17 'learning and innovation networks' for sustainable agriculture.\nApps, software and websites that recognize farmers as innovators, not just managers, are also on the rise. In the United States, FarmHack.net is an open-source community in which mainly small-scale farmers share know-how, tools and designs. Recent posts include advice on affordable aerial imaging and guides for repurposing old equipment.\nMore lessons are coming from the developing world. CABI, an intergovernmental agency, is training community 'plant doctors' who help farmers to identify pests and diseases and to enter the information in open-access databases that could be used to control pests or track epidemics.\nResearch funders are waking up to the advantages of asking farmers what they need to know. In the United Kingdom, the main farming bodies convened a consultation called Feeding the Future that identified topics such as precision agriculture and animal-disease management as practical priorities9. But we believe that field labs could boost farmers' productivity by supporting low-cost innovations that fly below the radars of large research institutions. When farmers produce knowledge, they are more likely to adopt new practices, and their insights are more likely to be relevant to local conditions.\nField labs attract innovative farmers — early adopters who can spread best practices. The challenge now is to evaluate and popularize the approach. In Europe, at least, the moment may have arrived. Linked to the latest round of reforms to the Common Agricultural Policy and the Horizon 2020 research programme, the EU launched the European Innovation Partnership for agricultural productivity and sustainability. It aims to promote bottom-up approaches by linking farmers, researchers, businesses and other stakeholders into groups charged with finding solutions to shared problems. With billions of euros earmarked for food and farming research over the next six years, the impact of this initiative is potentially enormous.\nThe European Commission has set out principles for this approach. Whether it flies or fails depends how EU member states rise to the challenge. For this initiative to succeed, governments must opt to spend a proportion of their rural development funds on supporting grass-roots training and learning by actual farmers — beyond the established partnerships with farmers' suppliers, customers and political representatives. Governments should back brokerage services that help farmers to team up with relevant researchers on their own terms, and enable them to navigate the maze of bureaucracy that will probably stand between them and this invaluable seed investment.\nThe time has come to decentralize, diversify, and enrich agricultural R&D. Farmers — not scientists, outreach workers or salespeople — are the essential players in any agricultural innovation system. Helping them will put food on the world's tables.\nChallinor, A. J. et al. Nature Clim. Change 4, 287–291 (2014).\nRay, D. K., Mueller, N. D., West, P. C. & Foley, J. A. PLoS ONE 8, e66428 (2013).\nFoley, J. A. et al. Nature 478, 337–342 (2011).\nGrassini, P., Eskride, K. M. & Cassman, K. G. Nature Commun. 4, 2918 (2013).\nStanding Committee on Agricultural Research. Agricultural Knowledge and Innovation Systems in Transition (European Commission, 2012).\nFuglie, K. O. et al. Research Investments and Market Structure in the Food Processing, Agricultural Input, and Biofuel Industries Worldwide (US Department of Agriculture, 2011).\nWaddington, H. & White, H. Farmer Field Schools: From Agricultural Extension to Adult Education 17 (International Initiative for Impact Evaluation, 2014).\nVaarst, M. et al. J. Dairy Sci. 90, 2543–2554 (2007).\nPollock, C. Feeding the Future (Joint Commissioning Group, 2013).\nRelated links in Nature Research\nRelated external links\nAbout this article\nCite this article\nMacMillan, T., Benton, T. Agriculture: Engage farmers in research. Nature 509, 25–27 (2014). https://doi.org/10.1038/509025a\nNature Food (2022)\nAgronomy for Sustainable Development (2022)\nScientific Reports (2020)\nThe International Journal of Life Cycle Assessment (2020)\nNature Sustainability (2018)""]"	['<urn:uuid:1f98b010-05bc-48d5-a8a5-0a7c7d14ad37>', '<urn:uuid:ff7170e9-4a46-4aa7-827c-671e37863209>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T17:51:21.175311	9	129	3449
22	why are oceans becoming more acidic and what happens to sea snails and corals	Oceans are becoming more acidic due to increasing carbon dioxide (CO2) levels in Earth's atmosphere since the Industrial Revolution. The ocean absorbs about one quarter of human-emitted CO2, which combines with seawater to create chemical reactions that reduce seawater pH. This process has increased ocean acidity by 26% over roughly 250 years. The increased acidity affects marine organisms, particularly those that build shells and skeletons from calcium carbonate, like corals and sea snails. For sea snails specifically, higher CO2 levels impair their decision-making and predator-escape behavior, making them more vulnerable to predation. For corals, acidification slows their calcification rates, making it harder for them to build and maintain their calcium carbonate structures that provide vital habitat for many marine species.	['by Jennifer Fields\nOcean acidification is known to have significant impacts on marine invertebrates in terms of calcification and reproduction; however, the effects of increased CO2 on marine invertebrate behavior are largely unknown. Watson et al. (2014) predicted marine conch snail predator-escape behavior to its predator cone shell would be impaired with near-future CO2 levels. The authors found that the decision-making of the conch snail was in fact impaired by ocean acidification, leaving the snails more vulnerable to predation. The change in behavior was fully restored by treatment with gabazine, suggesting that changes in acid-base regulation caused by increased CO2 in the ocean interfere with the invertebrate’s neurotransmitter receptor function. These alterations in behavior in marine invertebrates could have wide-ranging implications for the whole entire marine ecosystem.\nOcean acidification decreases concentrations of calcium carbonate and can result in changes in organismal calcification, due to decrease in calcium carbonate, reproduction, and more recently, changes in marine organism behavior due to changes in the acid-base regulation in the nervous system. Some of these behavioral aspects that are altered by elevated CO2 in the water column include exercise ability, escape response, metabolism, unwariness of risks, and decision-making. This study authors measured the predator-escape behavior of a jumping conch snail, which has a modified foot that it uses to jump away from a predatory cone shell under current (405 ppm CO2; pH of 8.17) and end of century (961 ppm CO2; pH of 7.85) ocean acidification conditions through the several behavioral assays. The jumping snails were housed in separate groups in both the control and elevated-CO2 conditions in an aquarium facility for five days and were then tested. The behavioral assays were a self-righting experiment to determine whether or not elevated-CO2 affected the ability for the snail to jump. To measure the full escape response of the conch snail, the snail was placed within an arena with a predator cone shell. The escape response, including angle of escape and number of jumps was recorded. The oxygen consumption of the snail was also measured during jumping and stationary states to see if the metabolism was affected by the increase of CO2, which could have influence on the snail’s ability to escape. Finally, the snail’s ability to respond to its cone shell predator chemical cue was tested. To examine this, snails were placed in individual tanks with the addition of predator cue water, and the jumping response of the snail was recorded. The addition of gabazine, a GABA neurotransmitter antagonist, was used in some of the trials to test the involvement of ocean acidification on the nervous system, as previously tested with marine fish (Nilsson et al. 2012). The GABA neurotransmitter maintains the function of the decision-making and most behavior aspects of marine fish and invertebrates. Elevated-CO2 interferes with neurotransmitter interfered with and its function is impaired. Gabazine allows for that acid-base regulation to return to normal and prevent the GABA neurotransmitter from being blocked.\nAs previously predicted by the authors, elevated CO2 concentrations as predicted at the end of this century will have significant impacts on marine invertebrate behavior. Higher CO2 concentrations lead to a change of escape behavior in the marine jumping snail. These concentrations impaired its instinctual predator-escape response by affecting its decision-making. However, the snail’s physical ability to jump as tested in the self-righting and metabolism experiments was not affected, therefore, the capacity to escape was retained. Elevated CO2 caused a reduction in the jumping response of the snail as well as an increase in latency of jumping, thus increasing the exposure time to predation, and escape trajectory by the snails escaping at closer angle and distance to the predator. The anti-predator response of the jumping snail was fully restored in the elevated-CO2 group with the addition of gabazine, which suggests that GABA-like receptors are responsible for the changes in behavior in these invertebrates, similar to marine fish.\nThis change in decision-making can lead to decrease in predator avoidance, wariness, and/or escape behavior, which could cause an increase in mortality from predation. With the near-future ocean acidification scenario, it is possible that changes in predator-prey interactions will have wide-ranging negative impacts on the marine food web and the fisheries that depend on the stability of that delicate dynamic.\nWatson, S.A., Lefevre, S., McCormick, M.I., Domenici, P., Nilsson, G.E., Munday, P.L. 2014. Marine Mollusc Predator-escape Behaviour Altered by Near-future Carbon Dioxide Levels. Proc. R. Soc. B published ahead of print November 13, 2013,http://dx.doi.org/10.1098/rspb.2013.2377. http://royalsocietypublishing.org/content/281/1774/20132377.full', 'What is Ocean Acidification?\nSince the beginning of the Industrial Revolution, when humans began burning coal in large quantities, the world’s ocean water has gradually become more acidic. Like global warming, this phenomenon, which is known as ocean acidification, is a direct consequence of increasing levels of carbon dioxide (CO2) in Earth’s atmosphere.\nPrior to industrialization, the concentration of carbon dioxide in the atmosphere was 280 parts per million (ppm). With increased use of fossil fuels, that number is now approaching 400 ppm and the growth rate is accelerating. Scientists calculate that the ocean is currently absorbing about one quarter of the carbon dioxide that humans are emitting. When carbon dioxide combines with seawater, chemical reactions occur that reduce the seawater pH, hence the term ocean acidification.\nCurrently, about half of the anthropogenic (human-caused) carbon dioxide in the ocean is found in the upper 400 meters (1,200 feet) of the water column, while the other half has penetrated into the lower thermocline and deep ocean. Density- and wind-driven circulation help mix the surface and deep waters in some high latitude and coastal regions, but for much of the open ocean, deep pH changes are expected to lag surface pH changes by a few centuries.\nOcean acidification and global warming are different problems, but are closely linked because they share the same root cause—human emissions of carbon dioxide. The atmospheric concentration of carbon dioxide is now higher than it has been for the last 800,000 years and possibly higher than any time in the last 20 million years. Humans have thus far benefited from the ocean’s capacity to hold enormous amounts of carbon, including a large portion of this excess carbon dioxide. Had the ocean not absorbed such vast quantities of carbon dioxide, the atmospheric concentration would be even higher, and the environmental consequences of global warming (sea level rise, shifting weather patterns, more extreme weather events, etc.) and their associated socioeconomic impacts would likely be even more pronounced. However, the oceans cannot continue to absorb carbon dioxide at the current rate without undergoing significant changes in chemistry, biology, and ecosystem structure.\nMeasuring ocean acidification: Past and present\nScientists know that the oceans are absorbing carbon dioxide and subsequently becoming more acidic from measurements made on seawater collected during research cruises, which provide wide spatial coverage over a short time period, and from automated ocean carbon measurements on stationary moorings, which provide long-term, high-resolution data from a single location.\nThese records can be extended back through time using what are known as chemical proxies to provide an indirect measurement of seawater carbonate chemistry. A proxy is a measurement from a natural archive (ice cores, corals, tree rings, marine sediments, etc.) that is used to infer past environmental conditions. For example, by analyzing the chemical composition of tiny fossil shells found in deep ocean sediments, scientists have developed ocean pH records from ancient times when there were no pH meters. Furthermore, because the ocean surface water is in approximate chemical balance, or equilibrium, with the atmosphere above it, a record of historical ocean pH can be inferred from atmospheric carbon dioxide records derived from Greenland and Antarctic ice cores, which contain air bubbles from the ancient atmosphere. Such evidence indicates that current atmospheric carbon dioxideconcentrations and ocean pH levels are at unprecedented for at least the last 800,000 years.\nGoing back deeper in Earth history to the Paleocene-Eocene boundary about 55 million years ago, scientists have found geochemical evidence of a massive release of carbon dioxide accompanied by substantial warming and dissolution of shallow carbonate sediments in the ocean. Although somewhat analogous to what we are observing today, this carbon dioxide release occurred over several thousand years, much more slowly than what we are witnessing today, thus providing time for the oceans partially to buffer the change. In the geologic record, during periods of rapid environmental change, species have acclimated, adapted or gone extinct. Corals have undergone large extinction events in the past (such the Permian extinction 250 million years ago), and new coral species evolved to take their place, but it took millions of years to recover previous levels of biodiversity.\nHow is ocean acidification affecting ocean chemistry?\nSeawater has a pH of 8.2 on average because it contains naturally occurring alkaline ions that come primarily from weathering of continental rocks. When seawater absorbs carbon dioxide from the atmosphere, carbonic acid is produced (see Box 1), reducing the water’s pH. Since the dawn of industrialization, average surface ocean pH has decreased to about 8.1.\nBecause the pH scale is logarithmic (a change of 1 pH unit represents a tenfold change in acidity), this change represents a 26 percent increase in acidity over roughly 250 years, a rate that is 100 times faster than anything the ocean and its inhabitants have experienced in tens of millions of years.\nAcidification can affect many marine organisms, but especially those that build their shells and skeletons from calcium carbonate, such as corals, oysters, clams, mussels, snails, and phytoplankton and zooplankton, the tiny plants and animals that form the base of the marine food web.\nThese “marine calcifiers” face two potential threats associated with ocean acidification: 1) Their shells and skeletons may dissolve more readily as ocean pH decreases and seawater becomes more corrosive; and 2) When CO2 dissolves in seawater, the water chemistry changes such that fewer carbonate ions, the primary building blocks for shells and skeletons, are available for uptake by marine organisms. Marine organisms that build shells or skeletons usually do so through an internal chemical process that converts bicarbonate to carbonate in order to form calcium carbonate.\nExactly how ocean acidification slows calcification rates, or shell formation, is not yet fully understood, but several mechanisms are being studied. Most hypotheses focus on the additional energy an organism must expend to build and maintain its calcium carbonate shells and skeletons in an increasingly corrosive environment. In the face of this extra energy expenditure, exposure to additional environmental stressors (increasing ocean temperatures, decreasing oxygen availability, disease, loss of habitat, etc.) will likely compound the problem.\nThese effects are already being documented in many marine organisms, particularly in tropical and deep-sea corals, which exhibit slower calcification rates under more acidic conditions. The impact on corals is of great concern because they produce massive calcium carbonate structures called reefs that provide habitat for many marine animals, including commercially important fish and shellfish species that use the reefs as nursery grounds. Coral reefs are vital to humans as sources of food and medicine, protection from storms, and the focus of eco-tourism. In addition to corals, studies have shown that acidification impairs the ability of some calcifying plankton, tiny floating plants and animals at the base of the food web, to build and maintain their shells. Scientists have also observed increased larval mortality rates of several commercially important fish and shellfish.\nWhat can we expect in the future?\nOcean acidification is occurring at a rate 30 to100 times faster than at any time during the last several million years driven by the rapid growth rate atmospheric CO2 that is almost unprecedented over geologic history. According to the Intergovernmental Panel on Climate Change (IPCC), economic and population scenarios predict that atmospheric CO2 levels could reach 500 ppm by 2050 and 800 ppm or more by the end of the century. This will not only lead to significant temperature increases in the atmosphere and ocean, but will further acidify ocean water, reducing the pH an estimated 0.3 to 0.4 units by 2100, a 150 percent increase in acidity over preindustrial times. Assuming a “business-as-usual” IPCC CO2 emission scenario, predictive models of ocean biogeochemistry project that surface waters of the Arctic and Southern Oceans will become undersaturated with aragonite (a more soluble form of calcium carbonate) within a few decades, meaning that these waters will become highly corrosive to the shells and skeletons of aragonite-producing marine calcifiers like planktonic marine snails known as pteropods.\nAlthough ocean acidification has only recently emerged as a scientific issue, it has quickly raised serious concerns about the short-term impacts on marine organisms and the long-term health of the ocean. Scientists estimate that over the next few thousand years, 90 percent of anthropogenic CO2 emissions will be absorbed by the ocean. This may potentially affect biological and geochemical processes such as photosynthesis and nutrient cycling that are vital to marine ecosystems on which human society and many natural systems rely. At the same time, marine organisms will face the enormous challenge of adapting to ocean acidification, warming water, and declining subsurface-ocean oxygen concentrations.\nNews & Insights\nWHOI working to address ocean acidification; protect region’s vital shellfish industry\nA new report addresses the impacts of ocean acidification in Massachusetts and New England coastal waters on the region’s vital seafood industry.\nOcean acidification gets a watchful eye in New England aquaculture ‘hot spot’\nShellfish aquaculture is thriving in New England, but future growth in the industry could be stunted as coastal waters in the region become more acidic. Researchers at WHOI have developed…\nOcean acidification causing coral ‘osteoporosis’ on iconic reefs\nScientists Pinpoint How Ocean Acidification Weakens Coral Skeletons\nClimate Change Will Irreversibly Force Key Ocean Bacteria into Overdrive\n[ ALL ]\nWHOI in the News\nThe Top Eight Ocean Stories of 2022\nThe $500 Billion Question: What’s the Value of Studying the Ocean’s Biological Carbon Pump?\nEcology Research: Ocean acidification causing coral ‘osteoporosis’ on iconic reefs\nDisentangling influences on coral health\n[ ALL ]\nFrom Oceanus Magazine\nOcean acidification is no big deal, right?\nWHOI’s Jennie Rheuban discusses the very real phenomenon of an increasingly acidic ocean and the toll it’s taking on marine life.\nTo Tag a Squid\nHow do you design a tag that can attach to a soft-bodied swimming animal and track its movements? Very thoughtfully.\nHow Do Corals Build Their Skeletons?\nWHOI scientists discovered precisely how ocean acidification affects coral skeletons’ a factor that will help scientists predict how corals throughout the world will fare as the oceans become more acidic.\nSearching for ‘Super Reefs’\nSome corals are less vulnerable to ocean acidification. Can the offspring from these more resilient corals travel to other reefs to help sustain more vulnerable coral populations there?\nGraduate student Hannah Barkley is on a mission to investigate how warming ocean temperatures, ocean acidification, and other impacts of climate change are affecting corals in an effort to find…']	['<urn:uuid:87ab84b5-4b65-41ac-a961-ca93f778d012>', '<urn:uuid:51ca6f21-f0f4-4343-8b14-a7f059f0fe69>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	14	120	2471
23	advantages vertical garden vs ground bed	Vertical gardens eliminate the need for a tiller, allow use of limited space with narrow footprints, provide mobility to move containers when relocating, and make gardening easier on the back and knees by allowing work at hip and rib level. However, they require more frequent watering and fertilizing.	['There are lots of ways to produce food, herbs, and medicines, whether we have small properties with just a patio or deck, or large acreage. Our growing options increase all over again with some simple things like freebie pickup shipping pallets, bottles, old furniture, totes, and bricks.\nThe ability to make use of vertical space can not only increase our productivity per square foot, but in some cases also make gardening easier on the back and knees.\nPro’s & Con’s\nWith a few notable exceptions, most vertical growing options share drawbacks and benefits. You have to build something, and you have to supply dirt. You also typically have to water more often and provide more fertilizer. However, it eliminates the need for a tiller. It also allows us to make use of limited space with narrow footprints.\nIn some cases they also provide mobility. Instead of working and amending lousy, compacted yards, and then leaving them behind when we move, we can stack our containers inside lawn bags and take them with us. We don’t have to truly start from scratch at our new home.\nThat mobility also applies to the learning curve of gardens. It’s a whole lot easier to move some boards and jugs, gutters, buckets or totes of dirt than it is to fill wheelbarrows with a shovel and then shovel again to refill elsewhere if we discover the spot that seemed perfect ahead of spring planting is far too shady come summertime leaf-out. (To say nothing of moving CMU or timbers after a season or two.)\nMobility applies to us as humans, too, although differently. As we age or collect injuries, caring for conventional and even low ground-level beds can become problematic. Bending, lifting, and kneeling can turn even favored pastimes into painful chores. Being able to sit and reach out at hip and rib level or work standing up braced on a cane, crutch, or walker allows us to remain productive even when we can’t handle the heavy lifting of even a dooryard garden.\nEven a “mini” hugelkulture bed can triple growing space in a footprint full-sized hugel beds that stand 4-6’ tall can quadruple grow space. They also have some benefits over the shallow containers we commonly see in vertical gardens. They hold moisture well, and as the internal wood cores and lighter branches, leaves, and straw used in construction break down, they essentially generate both moisture and their own fertilizer.\nHowever, they are pretty much permanent structures. They’ll break down over time, shrinking, and we can absolutely take them apart and transport that fertile compost and soil elsewhere when we’re done, but it’s not as easy as dumping some of the soil from a bucket or tote, or even as easy as bailing a stock tank into heavy-duty bags and moving it.\nWe can replicate the self-feeding and moisture retention of hugel beds using some commonly available, inexpensive elements. With or without log cores or heavy branches, they have similar lowered daily/weekly maintenance needs. They also still offer the ablity to work upwards of the ground and greatly increase our planting area per square foot.\nUsing scrap lumber or wooden pallets, we can form squares or slope-sided pyramids filled with lawn and tree cuttings, household composting material, and some soil and compost plugs or layers. We can add in tubes and hoses for water, and wire, basket weave, PVC or ABS tubes with holes similar to African keyhole gardens that we’ll add household composting materials to as the season passes.\nThe downside to wooden materials is that they have a limited lifespan. With treated and thick lumber it can be years in cooler climates, but a single pallet that’s inundated constantly is only going to last a couple years in Deep South or tropical humidity.\nThat means we need to plot our yards for enough space to rebuild them every few years, or we need to plan to continually reinforce them from the outside year by year – which means they’re going to “grow” outward as we go, but by inches at a time.\nAnother option are the wide variety of grow towers. They can be purchased or DIY builds as stackable units or single-drum units with planting holes. We can set those up for in-situ composting tubes as well. The steel and plastic or – if we choose – clay or ceramic pots will last longer than lumber and have the potential to be moved around a property or to a new property if we need to.\nPyramid beds, pyramid towers, stair-step beds, and spiral beds are also examples of ways we can increase our square footage in small-scale gardening by working upwards in tiers, the square footage of VISIBLE dirt is the same, but the square footage that plants on each tier are using actually extends UNDER the tier above as well due to shape/depth. By using sturdy construction materials like brick, metal, plastic, or block, they’ll last nearly forever and being smaller, we can relocate them if we need to. Even using untreated wood timbers, they’ll last longer than the hugel facsimiles just because there’s not as much weight pressing outward on them.\nWe can get equal or greater pyramid tower effects – increased planting space in a decreased footprint of our property – from steepled and angled shipping pallet beds. A salvaged picnic table and collected juice and water jugs can also be arranged into a very productive pyramid.\nUsing the smaller containers, we do go back to the original drawback of many vertical garden expansions: They’ll likely need fertilized more than a ground-plane bed, and they lack capacity to hold much water, especially in relation to the biomass they’re going to support. With vertical pallets especially, we also have to plan ahead with watering, just like barrel planters or grow towers.\nIf they’re only a couple of tiers high we may be able to soak them as we would any pots, containers, or shallow conventional beds. In most cases we’re going to have to lay in hose or something to act as a funnel or olla irrigation on each tier to ensure that the bottom doesn’t dry out while the top ends up soaked.\nEven so, it’s a handy way to get 2-8 times the growing space out of the square footage they occupy, and in many cases they can travel with us to continue easing the back strain of growing some groceries.\nThere is one caveat to the containers and small pocket or trench spaces most usually associated with vertical gardens: They’re for veggies, not staples.\nThere are peppers, cherry and grape tomatoes, and others that are adapted or adaptable to hanging pots and relatively small planters, and we have some larger options like shipping pallets, buckets, and hanging bags that can handle them. There are the cube and pyramid types and the hugel beds and approximations that can handle larger melons. We also now have several dwarf sweet corn with full-sized cobs specifically for container gardens, although they’re less applicable to small-container vertical garden methods.\nWheat, griding corn, barley, peas, and dry beans in enough quantity to affect our meals really just don’t work well in most container garden setups and it’s hard to get them enough root and growing space to make them viable for vertical methods.\nEven so, there’s value there, and the low footprint required of vertical growing methods is actually a major bonus. We can use very little square footage to produce our nutrient-rich and flavorful veggies, very efficiently many times, saving our horizontal planes for livestock, play/training space, orchards, or crops that do work better by plot, not plant.\nVertical methods can be used in conjunction with livestock, especially small livestock. The benefits include more than just finding enough space for both a small veg garden and compact livestock. Those benefits scale and apply to people with elbow room and acreage as well.\nEven more than green roofs, vertical gardens can also offer shading for those rabbits or hens – especially helpful in hot climates to keep rabbits breeding productively and lower heat stress. We might also arrange planters to help guard the lower edges of coops and hutches from predators that would dig or reach through them, or where they’ll increase the insulation and buffer winter winds. One easy way to accomplish that, is to line our existing fencing with vertical pallet gardens.\nWe can source pallets and leave them as-is for as long as we like, and when we have time and supplies, turn those predator and weather barriers into veggie and herb production. While some contortionists and taller livestock will be able to reach around the tops of fences, losses are limited for most and we can simply tailor our pallet gardens so the tops are water catchment or for them.\nWhile pallets have the most applications, we can use any of the tower or hanging-container methods in conjunction with our livestock fences. If they’re spaced tight or boards are arranged between them, it’ll limit “predation” by our adorable stock. Chickens, especially, will be able to reach their necks well past chain link and cattle wire.\nThose gardens can be done inside the coop, hutch or run, too, protected by mesh that allows free feeding but prevents livestock from eating plants all the way to the roots. Essentially, it’s just creating graze boxes, although in this case we don’t have to worry about making sure the plants can tolerate the “heat” of raw manure.\nIn the case of waterfowl and smaller goats, we can hang crops for us over their reach, but chickens will hop and reach, and by the time it’s out of a standard goat’s reach, we’d have to hook hanging baskets down for harvest and maintenance. However, we can add more productive space to our fence lines with any livestock by turning to shrubs, trees, and vines.\nDepending on the type fencing we have, they’ll munch one side until it’s out of reach and we may need to put a barrier up to protect tender starts and smaller vines they can reach through and over the tops of fences. It’s silvopasture – most commonly timber and fodder trees in acres of pasture, but easily manageable even in very small yards and totally applicable for everything in between.\nGrowing Up, Not Out\nExpanding our growing methods to the vertical plane has a lot of advantages not only for preppers in limited space, but also those looking for ease. In some cases, it can also be incredibly helpful for those just starting out, figuring out new properties, and trying to save money. Even for those with acreage, using vertical methods for veggies or greens can help keep those close at hand for harvest and care, as well as create less exposure to predators or livestock.\nThe narrow footprint also makes vertical gardens valuable for those trying to maintain a lower profile with their survival crops. The efficient use of floorplan means they can be stashed very close to the house, tucked into nooks and crannies, and even used inside, keeping them out of future potential thieves.\nThere’s a vertical garden method that can help just about any prepper, beginner or old hand, tiny apartment to six-digit acreage. Many can be done inexpensively and with minimal labor, tools, or soil and amendment purchases. With all the options, pretty much all of us can start increasing our food production or make at least some of that production a little easier from the labor aspect.\nFollow The Prepper Journal on Facebook!']	['<urn:uuid:acc1c2eb-bb1f-40db-84dc-21365a6398f0>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	6	48	1931
24	Does the Development team have more power in Stage Gate or Scrum?	The Development team has more power in Scrum, where they are self-organizing, empowered to decide how much work to take on, and can make crucial decisions. In the Stage Gate process, development follows a more structured approach where teams work within a formal step-by-step framework and decisions are guided by predetermined criteria at each gate.	"['Product development is the creation of a new or different product that offers innovative new benefits to the end user. This includes both the creation of an entirely new product and modifications to an existing product. These changes or new introductions may be targeting a newly defined customer requirement or a niche category in the market.\nA product can be defined as a collection of benefits that can be either tangible such as a physical item or intangible such as a service or experience. The new product development process involves two simultaneous activity paths. One of these focuses on extensive market research and analysis while the other deals with generating ideas, design of the product and detail engineering.\nNew product development is the first stage in the product life cycle management process, the strategic process that is used to manage products and market share. An effective product development process helps a company to:\n- Grow through the creation of new business opportunities\n- Boost profitability for stakeholders\n- Increase customer satisfaction through better products meeting specific needs\nIn this article, we look at 1) two approaches for managing product development, 2) points to remember, and 3) other product development techniques.\n1. TWO APPROACHES FOR MANAGING PRODUCT DEVELOPMENT\nThere have been several approaches to managing an effective new product development process. One such method is the Fuzzy Front End (FFE) followed by a more formal Stage Gate process. The FFE is a more chaotic early stage process, where the concept is defined. The Stage Gate is a more formal step by step process where the concept is formalized and managed systematically.\na. Fuzzy Front End (FFE)\nDefined by Koen, the fuzzy front end includes the chaotic, unpredictable, and unstructured activities that precede a more formal new product development process. This is stage after an opportunity has been realized and before a formal product development process is deemed ready. Here, a concept may be generated and then the decision made regarding the feasibility of this concept and whether it is worthy of further investment of resources.\nPractically, the FFE may not be a formal or detailed part of the product development process. However, it may end up taking up to half the total development time and this is the point where serious commitments regarding time, investment and the nature of the envisioned end product are decided. Because of this, it sets the direction for the whole project and the product. This is why the importance of this phase cannot be overlooked and it should be included in the overall projected product development cycle time.\nIn no particular order, Koen mentions five different elements of the front end. These are:\nIdentification of Design Criteria\nIn a semi structured manner, both large and incremental business and technological chances are identified. Based on criteria / guidelines decided here, new projects / products will be assigned resources and a structured development strategy can begin to be established\nIdentified ideas are converted to implications for business context relevant to the company. Market research and studies as well as some basic trials may be conducted at this point to understand how the concept will translate into a solution for the customer\nThrough an incremental and iterative process, the identified idea is taken birth to a mature tangible concept. The process followed here can be formulated internally or externally through a specific customer request or a unique material availability from a supplier\nDoes the concept of idea reached here make business sense and does it have business value? Positive answers to these questions lead to the decision to purse the creation of the prototype of the product and negative answers may end the process right here to be started again from the beginning\nBased on factors such as available market share, customer requirements, investment needed, competitor analysis etc., a business case can now be developed. This is often treated as the first step of the formal product development process.\nThere is no formal or universal definition of the Fuzzy Front End. Neither is there a clear framework of how it works. Generally, it is said to encompass tasks such as strategic planning, generating concepts and pre-technical evaluations. These activities are not set in any formal framework, but flow in an unpredictable, unstructured and often chaotic manner. Despite this, it can be a valuable and often vital lead into a successful product development activity.\nb. Eight Stage Gate Process for New Product Development\nOften known as the stage gate process, this 8 stage framework was developed by Robert G. Cooper after extensive research product success and failure. Since product developments often include big or small teams and may require substantial investment, following a structured and formal process may help maximize the probability of a successful product and mitigate the risk of failure. This process can also help keep everyone on the development team structured and on the same page.\nThe 8 stages in this process are:\nAt this stage, a product development team will take the concept narrowed down in the fuzzy front end and work on brainstorming this concept into concrete ideas for products. It is necessary to employ strong analyses to identify current market trends and available solutions, understand customer behavior and needs, and identify areas of opportunity. This information will help keep ideas focused and relevant. Some tools that can help this analysis include a basis internal and external SWOT analysis, study of market and customer trends, competitor analysis, focus groups, sales staff and other employees, and information from trade shows. It is also helpful for the team to keep important aspects of the future product in mind such as how lean it is, how scalable it is and how much return can it generate. There may be many ideas at this point, which will be analyzed in detail in the next stage.\nAt this stage, the task is to remove ideas that do not make strong business sense. A set criteria should be decided against which each idea will be evaluated and discussed. There should be as little deviation as possible from this criteria so that no extra time is spent debating unsuitable ideas. Some questions that can be asked at this stage include\n- Will the target audience benefit from this product?\n- What is the size of this target audience and is there a growth forecast?\n- Is there existing competitor presence or is it anticipated?\n- Are the industry and market trends positive?\n- Does it make technical sense to manufacture the proposed product?\n- Will the product be profitable?\n- Will it be acceptable to the customer at target price?\nConcept Development and Testing\nAt this stage, the legal and practical aspects of developing and launching the new product are studied. It is pertinent to investigate all intellectual property and patent issues to prevent later infringement issues. An understanding of the marketing message and marketing details will also help streamline and focus the process moving forward. Identifying the target market, purchase decision makers, product features, potential benefits and consumer reactions to the product are some important marketing related points. In addition, design details and their technical requirements as well as cost effective production are also vital considerations.\nOnce marketing and design details are evaluated and understood, it is now time to conduct a business analysis to assess product profitability potential. It is important to estimate as close to reality as possible the likely sale price, based on competitor analysis results and consumer focus group or survey data. It is also necessary to workup sales volumes based on market size and relevant tools. And based on this, profitability and a break-even point can be identified.\nBeta Testing and Market Testing\nAt this point, it is now time to manufacture an actual prototype or a close mockup. The product, and ideally the packaging, should now be tested in true to life usage conditions. This can be done through focus groups, interviews, introductions at tradeshows. The feedback that is generated can help identify redundant features or those that create no real value and any hitches in the user experience. Necessary adjustments can now be made. As further testing, an initial trial market can be identified and a small scale launch conducted to further assess acceptance by the target user group.\nAt this stage, the technical teams will finalize the quality management system. They will also work out the resource requirements, plan engineering operations and create department schedules. Technical communications and requirements can be published to create a record. Suppliers should be brought on board and logistics worked out. A system for regular reviews of the project as well as contingency and risk mitigation plans should also be drawn up.\nThe product is ready for launch at this stage. Necessary advertising and marketing communications strategies are set into motion and the distribution channel is loaded up to ensure consistent product availability. The product should be monitored for usage feedback with consistent technical support available if applicable. In order to help convert undecided consumers into actual buyers, the marketing communications and brand presence need to be strong, relevant, visible and updated.\nNew Product Pricing and Post Launch Review\nAt this point, a review of the entire process is a good idea to identify best practices and areas of improvement. The impact of this new product on the entire product portfolio should also be evaluated. An external and internal value analysis will help understand the impact of the product. The product may have been launched at an introductory price and it may now be time to decide upon an actual pricing strategy. This can be done after assessing consumer response, competitor response and product costs. A forecast of unit volumes, revenue and profit will also add value. An ongoing mechanism needs to be put in place to evaluate changing customer needs and market trends to ensure that the product stays relevant and competitive as consumers grow older and change preferences.\n2. POINTS TO REMEMBER\nIt is important to note that the product development process is a fluid and evolving one. Some steps may need to be iterated while others may be eliminated altogether. In the interest of quicker time to market, some companies may run several steps simultaneously.\nThere must be an understanding that ideas may get trashed and some of these may be exciting and innovative but wrong for what is presently required. The company must also be prepared for losses and have a risk mitigation or market exit strategy planned in advance to avoid being caught off guard.\nA Balanced Team\nThe product development process ideally needs both technical or engineering and marketing or business expertise. The best way to achieve this is to create a specialized cross functional team and give it end to end responsibility for the product. This means that the team will be working on idea generation all the way to commercialization and post launch reviews. If the industry requires extensive and expensive R&D, then a collaboration among different companies may help spread out costs and risks associated with a new product as well as provide access to a wider variety of skills and experience. In any case, it is vital that the team work in close collaboration to ensure the successful development and launch of the new product. With clear goals, the team can ensure that the product development process is productive and relevant.\nA Proactive Process\nNew product development is increasingly seen as a proactive process by senior management. There is an effort to identify changes in market trends and seize opportunities for new products before a need for them is explicitly present or before a competitor reaches it. This allows time to perfect and plan a product’s development and launch in detail and allows risk mitigation and planning. If a more reactive methodology is adopted then, there will always be a rush to catch up which may lead to a substandard product or a poorly planned launch. New product development should be seen as a form of continuous development where the entire company is alert towards potential opportunities.\n3. OTHER PRODUCT DEVELOPMENT TECHNIQUES\nSRUM is a simple framework that allows teams to collaborate effectively on complex projects. Through this methodology, product development occurs in small pieces. Each piece builds up on a previously created piece. Working on one small piece at a time encourages the creative process and allows teams to incorporate feedback and make changes as well as to create only what is needed.\nThrough a small set of rules, basic structure is provided to the teams to allow them to focus innovation on finding solutions to what might other be an extremely daunting challenge.\nThe Scrum process involves three basic roles. These are:\n- Product Owners who finalize what needs to be created\n- Development teams who create what is needed and then demonstrate it after which the next step is determined\n- Scrum Masters who ensure a smooth process and work on process improvement, team development and product quality', ""What is Scrum?\nScrum is a simple empirical process that enables teams to build products incrementally in iterations, to keep abreast of the changing market needs and align themselves to the organization's business goals.\nScrum advocates self-organizing teams working towards a common goal through continuous inspection and adaptation. A minimum viable product at the end of each iteration provides an option for the teams to quickly get feedback from end users and respond accordingly much faster.\n3 Scrum Roles\nThe scrum team is made up of just three roles: a Product Owner, the Development team, and a Scrum Master\n1. Product Owner:\nA Product Owner in a scrum team decides what needs to be built. This person has complete knowledge on the market and business needs, has a vision, and owns the return on investment (ROI) or value delivered by the product.\nUnlike traditional delivery, this person is a part of the team that delivers the product. Following are the key tasks of the Product Owner:\n- Creates the vision\n- Represents business, and is responsible for the ROI\n- Cascades the vision to the teams\n- Owns the backlog of features\n- Prioritizes features by market value\n- Is empowered to take decisions\n- Negotiates with the team and business to deliver the right product at the right time\n2. The Development team:\nThe Development team in scrum is the team that has all the skills necessary to implement the backlog items. This team is not any normal team and is committed, dedicated, and motivated to perform the best.\nIt is a self-organizing team that collaborates, shares their special skills and knowledge and are committed completely to fulfill the objective. The team members are empowered to take crucial decisions that can make or break a situation.\nThe following are the special characteristics of 'The Development team':\n- Self-organizing - the Development team will be a self-managing group, who will decide on the tasks that they will work on incrementally. There is no 'Manager', who will be controlling their work\n- Empowered - the team can commit to work, determine HOW to deliver and decide HOW MUCH to deliver in one iteration\n- Cross-functional - the team does not segregate members as developers, testers or analysts and each have the necessary skills to deliver the product increment\n- Small-sized - the Development teams should ideally have 5 to 9 team members with skills sufficient to deliver the committed work. Smaller teams will not have the bandwidth to complete a considerable work and bigger teams will increase complexity\n- Co-located - the agile team is co-located, to ensure effective collaboration\n- Committed - since the team is empowered to take decisions on the scope of work in a sprint, they are committed to delivery, should be transparent on the progress, and highlight the impediments early on\n- Dedicated - this team is focused and is 100% dedicated to product delivery\nUnlike traditional methodologies, where the commitment to deliver is made to business by the team that is not involved in the execution, in Agile, the team that does the work commits to how much work can be executed in a sprint.\nThe Development team decides how much work is to be done in a sprint, and commits to delivering a 'potentially shippable product increment (PSPI)', without sacrificing quality and speed. The team also makes continuous self-improvements.\n3. Scrum Master:\nThe Scrum Master is not a management title and cannot make decisions on behalf of the team. The Scrum Master's major responsibility is to ensure that scrum is understood and practiced by every team member in the true spirit.\nThe Scrum Master should understand the different skill sets of his team and group them by having the right sheep in the right flock. A Scrum Master should guide the team such that the team does not go astray and fall prey to excess time and energy. A shepherd must draw out quiet people during stand-up meetings or when planning poker sessions. And, when the team loses focus or a team member goes astray, the shepherd should bring the lost one back to the flock and guide appropriately.\nThe Scrum Master should not enforce agile practices on the team, but should don a 'Servant leadership' role. Scrum Master should lead by example and be a living demonstration of team assets and scrum values.\nHe should create an environment of safety for the team, and guide and facilitate team collaboration. He should refrain from solving problems or making decisions by guiding teams to do so.\nTo summarize, a Scrum Master:\n- Is a servant leader - mentors and coaches the teams on scrum theory and practices, guides them on how they need to adapt to the same, thereby realizing the benefits of scrum both at team level and organization level\n- Helps remove obstacles/impediments - supports the Development teams in removing the impediments by reaching out to the right people, thereby ensuring a smooth development progress without disrupting the team\n- Facilitates collaboration - enables interactions within the team as well as between the team and the Product Owner\n- Teaches scrum - to the team\n- Protects the teams - from external disruptions like changes to stories in the current sprint\n- Is a change agent - in growing the organization to deliver early and often, and removing waste\nScrum focuses more on a working software at the end of every sprint rather than comprehensive documentation. This does not imply that there is no documentation. The documentation is provided to facilitate collaboration and interactions, rather than tracking. The progress is measured always through a working software. Documentation in scrum is only through four main artefacts namely: Product backlog, Sprint backlog, Increment and Definition of Done.\n1. Product backlog:\nA product backlog is a dynamic list of functionalities the product might include, such that it provides value to users.\nThe Product Owner maintains this list and is responsible for creating, managing, and prioritizing the backlog by focusing on WHAT brings the highest value to the users. These are few unique characteristics of a product backlog:\n- Is dynamic in nature as it evolves based on changing market needs\n- Lists all the features and capabilities that will be taken up in iteration and delivered as a product increment\n- Is refined on a continuous basis. The Product Owner and Development team collaborate and update the details, estimate, and prioritize based on business value and size\n2. Sprint backlog:\nSprint backlog is a subset of the entire product backlog that the scrum team plans to implement in one iteration or sprint.\nDuring the sprint planning, the team selects items from the product backlog that they commit to complete in one sprint and thus, creates the sprint backlog. The Product Owner and Scrum Master should not provide inputs that may impact the team's decision. Sprint backlog has:\n- Subset of product backlog items that the teams commit to implement in one sprint\n- Items broken into smaller pieces of work as tasks\n- A focus on HOW the team does the work and delivers the value in one sprint\n- A story or task board that is used by the teams to view backlog and individuals sign up for work after prioritization\n- Provision for the Development teams to track the sprint progress and check their alignment to sprint goals\nAn increment is the work delivered at the end of every sprint. Typically, after every iteration there will be a Product Increment (PI) that delivers value and the final product will be a working software. This increment is a sum of all the capabilities that were delivered in the previous sprints as part of the PI. A Product Owner decides whether to release the working product increment post the sprint or the release.\n4. Definition of Done:\nScrum clearly states a 'definition of done' that enables teams to understand the meaning of marking a story as done. Based on this, teams measure the progress of completion of their stories. This not only helps identify 'done' items, but also helps decide on the total items to be worked in that sprint or iteration. The 'definition of done' is defined at various levels, which are release level, sprint level and even at the story level. The story level 'definition of done' is handled through acceptance criteria. In a multi-team scenario, teams mutually align to the sprint's 'definition of done'.\nAll scrum activities are time-boxed and allow teams to inspect their current work and implement those learnings in future time-boxes.\nHeart of Scrum - The Sprint\nAt the heart of scrum, is the 'Sprint'. The sprint is a time-boxed iteration, typically ranging from 1 to 4 weeks, at the end of which, a potentially shippable product increment is delivered by the Development team. The sprint has the following characteristics:\n- Does not exceed a maximum of one calendar month, as this will increase the risk due to changes in requirements and thus, may not provide the perceived business value at the end of the sprint\n- Has a goal or 'definition of done' associated with every sprint that actually measures the success of the sprint\n- Can be cancelled by the Product Owner, if the goal or the need for the sprint becomes obsolete due to changing market\nScrum advocates specific types of activities, events, or meetings within a sprint to avoid the traditional formal meetings. These events and meetings are conducted at regular intervals and happen at specific periods of the sprint.\nTypical scrum activities are:\n- Product backlog refinement\n- Sprint planning\n- Daily scrum\n- Sprint review\n- Sprint retrospective\n1. Product backlog refinement (continuous activity throughout the sprint)\nProduct backlog refinement is a continuous activity throughout the sprint, where the Product Owner ensures that the product backlog is in order. The Product Owner performs the following tasks to ensure that the product backlog is relevant:\n- Removes or demotes product backlog items that no longer seem important\n- Adds or promotes product backlog items that become more important\n- Splits product backlog items into smaller items or merges smaller ones into larger items and estimates those\n2. Sprint planning (2 hours per week sprint time-box)\nSprint planning meeting happens at the start of every sprint. This helps the Product Owner and Development teams to plan the product backlog items that will be taken up for implementation during the sprint. The Development team performs the following activities during this meeting:\n- Considers and discusses product backlog items with the Product Owner\n- Ensures a shared understanding on those items\n- Selects a number of items that they estimate to complete\n- Creates a sufficiently detailed plan to complete the selected items\nTo ensure that the above is achieved, two activities need to be done:\nPart I: Define 'WHAT' work will be done\n- Product Owner renders prioritized product backlog to the Development team\n- The whole scrum team collaborates to understand the work\n- The Development team alone decides how much work is to be taken without any pressure for more work to be done\n- The sprint is given a goal called the sprint goal as the essential focus of that sprint\nPart II: Explain 'HOW' the work will get done\n- Development team decides how to produce the next product increment that meets 'definition of done'\n- Sufficient design and planning is conducted to complete the committed work\n- Work to be done in initial days is split into small units of one day or even less\n- Work to be done later are split whenever needed\n3. Daily scrum (15 minutes)\nDaily scrum is a 15 minute time-boxed event in which the team manages its daily activities. This is also called the daily stand-up meeting.\nThe scrum team meets every day, preferably at the same time and same place, so that it becomes a habit and here each member answers three critical questions:\n- What did I get done yesterday?\n- What will I get done today?\n- Are there any impediments blocking me?\nIt is essential that all the members of the scrum team are available for the daily stand-up meeting. The daily stand-up meeting is for the Development team, and they should participate enthusiastically to collaborate with each other.\nThe daily scrum also ensures that the impediments blocking the progress of the sprint are identified and resolved without further delay. Detailed problem solving does not happen during this meeting. Unnecessary meetings should be avoided by broadcasting individual updates to everyone.\nThis event enhances team communication and transparency, thereby enabling teams to be self-organized and make faster decisions.\n4. Sprint review (1 hour/week time-box)\nA sprint review is an event that happens at the end of every sprint, where the teams and stakeholders discuss what was done in the sprint. The following happens during this meeting:\n- A demo of the product increment showcasing the new features and underlying technology\n- Feedback from the review provides input to the team to further discuss on refining the existing backlogs and plan for future sprints\n- The Scrum Master facilitates this review meeting that is typically attended by all the stakeholders invited by the Product Owner\n- Sprint review is essentially a way in which the team inspects and adapts to the next sprint and overall product release\n5. Sprint retrospective (1 hour/week time-box):\nDuring a sprint retrospective meeting, the Development team inspects the previously completed sprint and identifies areas of improvement to be enacted for the upcoming sprints. This happens after every sprint and right after sprint review in which the whole team participates. During this meeting:\n- The team introspects on what went well in terms of collaboration, planning, process, and tools\n- They try to identify potential improvements that can be taken up in the next sprint to make the scrum processes more efficient by learning from previous shortfalls\n- They decide on what would be done in the next sprint by taking into consideration the major improvements\n- Scrum Master ensures that the teams improve their skills and knowledge during the scrum process so that they become more effective in the next sprint\n- The team focuses on improving their entire delivery cycle\nThe three typical questions the team answers are:\n- What shall we start doing?\n- What shall we stop doing?\n- What shall we keep doing?\nAll the above activities in the scrum process framework enable teams to deliver a potentially shippable working software in short iterations. This also enables teams capture feedback, inspect, and adapt for the next iteration.\nScrum also states five core values to which teams have to adhere. The core values are: Commitment, Courage, Focus, Openness, and Respect. These values need to be imbibed and lived by the scrum team to ensure the fulfillment of scrum pillars of transparency, inspection, and adaptation. It builds trust among everyone.\nSuccessful use of scrum depends on people becoming more proficient in these 5 values\n- People personally commit to achieving the goals of the scrum team\n- The scrum team members have the courage to do the right thing and work on tough problems\n- Everyone focuses on the work of the sprint and the goals of the scrum team\n- The scrum team and its stakeholders agree to be open to all the work and the challenges that they encounter while performing the work\n- Scrum team members respect each other and consider each to be capable and independent""]"	['<urn:uuid:bfdc73a1-5a68-4c31-9026-389d32b303ac>', '<urn:uuid:def28e7c-80b7-4c49-bf36-406831180f23>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	12	55	4767
25	research funding initiative tobacco company science bias conflicts	PMI IMPACT is a $100 million funding initiative launched by Philip Morris International in 2016 to fund research projects. However, this type of industry funding raises significant conflict of interest concerns. As demonstrated in the second document, financial conflicts of interest can directly affect research design, conduct and reporting. The documents show that institutions receiving tobacco industry funding must disclose PMI's financial support in their work products, while academic institutions like Utrecht University have faced criticism and ultimately rejected such funding due to concerns about industry influence. The case illustrates how industry funding can create both individual and institutional conflicts that may compromise research integrity.	"['PMI IMPACT is a $100 million funding initiative launched by Philip Morris International (PMI) in 2016. It describes itself as “a global initiative to support public, private, and nongovernmental organizations to develop and implement projects against illegal trade and related crimes”.\n- 1 Background\n- 2 Who Is Funded?\n- 3 Illicit Trade Conference Hosted With Financial Times\n- 4 United Nations Global Compact Used To Promote PMI IMPACT\n- 5 Controversy: Funding Of Science\n- 6 TobaccoTactics Resources\n- 7 TCRG Research\n- 8 Relevant Link\n- 9 Notes\nPMI IMPACT funding allocation is split into several funding rounds, each with a distinct theme.\nThe selection of successful projects involves a three stage process. First, applicants submit an expression of interest which PMI conducts “initial due diligence checks” on. Second, selected applicants submit full proposals for review and approval. And the final stage involves successful applicants signing a grant agreement with PMI, which includes the following requirement:\n“When making public its Project’s work product or any deliverables, to be transparent about PMI’s financial support of the Project, Grantee shall disclose the fact that the Project was funded by PMI IMPACT. Before Grantee makes any of its work product or deliverables public, PMI and Grantee shall agree on the precise language of disclosing PMI’s financial support, ensuring that such language complies with applicable laws, PMI transparency and disclosure policies, and other applicable ethics rules.”\nThe funding applications are judged by an Expert Council of individuals with close links to United Nations (UN) agencies and INTERPOL. The Council members are financially compensated for their time,  and in May 2018 included:\nHayden worked as a PMI consultant on ""law enforcement matters"", and is the former Senior Advisor to the UN Office on Drugs and Crime (UNODC) and the International Anti-Corruption Academy (IACA). She also established the UN’s first financial investigation unit. Hayden was also involved in an Anti-Contraband Tobacco Working Group meeting, partly-funded by Dawson Strategic, a consultancy registered under the Senate Office of Public Records as a PMI lobbying firm.\nJulliet served as Intelligence Director for the General Directorate for External Security (DGSE) of the French Ministry of Defence, and later took the role of Senior Official in charge of economic intelligence to the Prime Minister’s office, holding this position until 2009. In 2009 he became President of the Académie de l’Intelligence Economique, and in 2011, President of the Club des Directeurs de Sécurité des Entreprises (CDSE). From 2016, he worked with INTERPOL as chair to the International Forum of Security Technologies (FITS).\nMakin served in the UN Industrial Development Organization (UNIDO) and is a former Director of the UNIDO Regional Office in Egypt.\nLuis Moreno Ocampo\nMoreno Ocampo was the first Chief Prosecutor for the International Criminal Court (ICC) from 2003-2012, and he also served as Chairman of the World Bank Expert Panel on the Padma Bridge project. On 11 June 2015, he hosted a workshop for the Global Order Project on the Philip Morris vs. Uruguay lawsuit, though its content is unknown.\nPillay was elected by the UN General Assembly to be a judge on the International Criminal Tribunal for Rwanda serving from 1995-2003, and was UN High Commissioner for Human Rights from September 2008 to August 2014. Pillay was added to the expert council in 2017.\nJürgen Storbeck is a former head of the Central Bureau of INTERPOL and was the first Director of Europol between 1999 & 2004. More recently he has been consulting for governments, public authorities as well as private organisations, including PMI, in the field of crime prevention and enforcement.\nWho Is Funded?\nTo date, PMI IMPACT has had two funding calls.\nThe most recent one was opened on 29 September 2017 and called for initiatives that addressed ""converging forms of illegal trade and related crimes, such as corruption, money laundering, and organized criminal networks"", and had to have an impact in one or more of these geographic areas: Eastern Europe, Middle East, North Africa, Tri-Border Area in South America, Central America, South and Southeast Asia.  The funding call received 157 expressions of interests, and an announcement is yet to be made which projects will be funded.\nFirst Funding Call (2016)\nThe first funding call focused on projects with an impact in the European Union (EU). PMI announced that it would spent $28 million over two years to fund 32 approved projects (Image 1).\nSome of the successful applicants have worked with the tobacco industry before, including:\nPhilip Morris is listed as a ‘partner’ on the Russian version of Belbrand’s website. In addition, Grodno tobacco factory Neman, which occupies 72% of the tobacco market in the Republic of Belarus, is also listed as a member.\nProduced several reports funded by Cornerstone Research “under contract to Altria Client Services”, including Unintended Consequences of Cigarette Prohibition, Regulation, and Taxation (2015), Cigarette Taxes and Illicit Trade in Europe (2016), and Empty Discarded Pack Data and the Prevalence of Illicit Trade in Cigarettes (2017), co-authored by Alberto Aziani of Transcrime.The company\'s chairman, Mark Kleiman, was a speaker at the Global Tobacco and Nicotine Forum 2017, a tobacco industry event.\nThe Center for the Study of Democracy (CSD)\nThe CSD and the Balkans’ Customs Agency held an international conference in 2011 named “Counteracting Cigarette Smuggling in the Balkans”, attended by representatives of PMI, Japan Tobacco International (JTI), British American Tobacco (BAT), Imperial Tobacco, and Bulgartabac Holding. In 2010, the CSD hosted a round table on the Bulgarian cigarette market, attended by PMI, British American Tobacco, Japan tobacco International and Bulgartabac.\nThe CSD project funded through PMI IMPACT is titled ""Illegal Trade in Tobacco Products and the Balkan Route: Overcoming Institutional Gaps and Corruption"". Its websites states that ”The research team involves experts from Bulgaria (CSD), Italy (Intellegit), Romania (SCE), as well as three independent criminology researchers from Greece. Over the next two years, the project will elaborate tools for performance evaluation and corruption risk assessment of law enforcement and revenue authorities with regards to illegal tobacco trade.” As part of this project, the CSD organised a round table discussion in March 2018 on “The illicit tobacco market: limits to institutional enforcement”.\nGLOBSEC’s website, accessed in September 2017, listed Philip Morris Slovakia as one of its partners. In addition, PMI’s Vice President of Illicit Trade Strategy & Prevention, Alvise Giustiniani, was a panellist at the May 2017 GLOBSEC Bratislava Forum, titled ‘Threat-financing: money, illicit trade and (in)security’.\nAn Albanian tax, financial, accounting and legal services consulting firm, which according to its website, has Philip Morris Albania as one of its main clients.\nHas a history of undertaking research on the illicit tobacco trade on behalf of the tobacco industry. For more information on this relationship see KPMG.\nLithuanian Free Market Institute (LMFI)\nThe LFMI is a member of the European Policy Information Center (EPICENTER), a group of eight think tanks, spanning multiple countries within the EU. EPICENTER is entirely funded by the Institute of Economic Affairs (IEA) which has a history of close collaboration with the tobacco industry. See: Institute of Economic Affairs.\nHas a history of producing tobacco industry-commissioned reports on illicit tobacco trade in Asian markets. Click here for more information on these reports and the company\'s relationship with tobacco companies.\nPanta Rhei Research Limited Company\nIn December 2017, Zora Hauser was one of the Directors of Panta Rhei Research Limited Company. Hauser worked in corporate affairs for PMI between 2014-2017. Panta Rhei Research Limited Company should not be mistaken for Pantarhei, an Austrian public relations company.\nRoyal United Services Institute for Defence and Security Studies (RUSI)\nIn July 2017 KPMG released the first report from its annual Project SUN series on EU illicit tobacco that was not directly commissioned by tobacco companies, but instead commissioned by RUSI. RUSI received funding from BAT, Imperial Tobacco, and PMI to provide “additional qualitative research” to the study.\nSiracusa International Institute (SII)\nUntil his death in September 2017, M. Cherif Bassiouni of PMI IMPACT\'s Expert Council was SII\'s Honorary President.\nSoutheast European Law Enforcement Center (SELEC)\nIn 2013, SELEC held an event on ‘combating illicit trade’ in conjunction with JTI. For several years, SELEC have held annual meetings on fraud and smuggling which often feature representatives from tobacco companies.\nPMI has funded a series of reports, several focussed on illicit tobacco trade, by Transcrime academics. For more information on these reports and Transcrime’s links to the tobacco industry, see: Transcrime.\nIllicit Trade Conference Hosted With Financial Times\nIn September 2017, PMI IMPACT and the newspaper Financial Times held a joint event titled Combating Illicit Trade: Progress, Challenges and Collaborative Solutions. The conference included panel discussions, dialogues, presentations, and interviews, including talks about the projects accepted through PMI IMPACT\'s first funding round, and an address from PMI’s Chief Executive Officer André Calantzopoulos.\nOther speakers included members of PMI IMPACT\'s Expert Council, academics, and representatives of the EU Commission, Europol, Her Majesty’s Revenue & Customs (HMRC), Organization for Economic Cooperation and Developments (OECD), World Customs Organization (WCO), World Trade Organization (WTO), and UNODC.\nUnited Nations Global Compact Used To Promote PMI IMPACT\nPMI has used its former membership of the UN Global Compact (UNGC) to promote PMI IMPACT and attempt to portray this association as evidence that PMI is in collaboration with the UN and supports “broader UN goals”.\nIn 2016 and 2017, when it was still a member of the UNGC, PMI published two UNGC ‘communications on progress’ reports. The first report outlined the launch of PMI IMPACT as a “catalyst for a more holistic approach against the illegal tobacco trade, corruption and organized crime” (Image 2). The second report described PMI IMPACT as “a global initiative governed by independent experts to sponsor third-party initiatives”.\nUsing the association with the UNGC, is a Corporate Social Responsibility (CSR) tactic, whereby PMI tries to enhance its reputation and present themselves as a good corporate citizen, despite the harms caused by its products.\n- For more information, see United Nations Global Compact (UNGC).\nControversy: Funding Of Science\nOne of the successful recipients of the first PMI IMPACT funding round was Professor John Vervaele, an academic at Utrecht University School of Law in the Netherlands.\nVervaele was awarded €360,000 to examine how enforcement could be improved to combat illicit tobacco trade in the EU, particularly ‘cheap whites’ from Belarus, Russia and Ukraine.\nThe funding attracted widespread criticism from health organisations including the Dutch Cancer Society (KWF), which said that “the tobacco industry is not a normal industry, and that’s why they should not be funding scientific research”.\nFollowing the criticism and subsequent pressure in the media, the university decided to no longer accept PMI\'s funding and fund the project themselves.\n- PMI’s “Illicit Trade” Anti-Plain Packaging Campaign\n- Illicit Trade Protocol (ITP)\n- Tobacco Smuggling in the EU and Tobacco Smuggling in the UK\n- Influencing Science: Commissioning Research and Reviews\n- Influencing Science: Funding Scientists\n- Oxford Economics\n- Unscrupulous methods used by the tobacco industry revealed over the illicit trade, K.A. Evans-Reeves, A. Rowell, 2015, The Conversation\n- Tobacco industry rallies against illicit trade – but have we forgotten its complicity?, K.A. Evans-Reeves, A. Rowell, 2015, The Conversation\n- Philip Morris International, Press Release: Philip Morris International Launches New Initiative to Fight Smuggling and Related Crimes; Pledges USD 100 million to Fund Projects, 27 May 2017, accessed October 2017\n- PMI IMPACT, PMI IMPACT: Supporting projects with lasting impact against illegal trade, 2017, accessed October 2017\n- PMI IMPACT, Application Terms and Funding Rules, Second Funding Round, 2017, accessed October 2017\n- Key Terms of a PMI IMPACT Grant Agreement, 2017, accessed October 2017\n- Philip Morris International, Expert Council, accessed June 2016\n- tabaknee, Stichting Philip Morris Impact is lang niet zo onafhankelijk als ze zich voordoet, 20 December 2017, accessed May 2018.\n- MacDonald Laurier Institute, Photos and Video: Anti Contraband tobacco working Group meets in Ottawa, 1 December 2014, accessed June 2016\n- Dawson Strategic, Lobby Registration, accessed June 2016\n- FITS, La lettre du président Alain Juillet, accessed June 2016\n- L. Moreno Ocampo, A. Eibelshäuser, Workshop on the Investment Dispute between Philip Morris and Uruguay, Global Order Project website, 2017, accessed December 2017\n- Philip Morris International, Expert Council, 2017, accessed October 2017\n- A. Calantzopoulos, In memory of Professor M. Cherif Bassiouni, Philip Morris International, accessed October 2017\n- Philip Morris International, Philip Morris International Calls For Proposals In Second Funding Round of PMI IMPACT, a Global Initiative to Fight Illegal Trade and Related Crimes, 29 September 2017, accessed October 2017\n- Philip Morris International, Press Release: Philip Morris International Welcomes Strong Interest for PMI IMPACT Global Initiative Against Illegal Trade and Relates Crimes Receives 157 Applications In Second Funding Round, 28 November 2017, accessed December 2017\n- Philip Morris International, Explore: Funding Theme, accessed June 2016\n- Philip Morris International, PMI IMPACT September 2016 Press Release, 15 September 2016, accessed October 2016\n- Philip Morris International, The Expert Council of PMI IMPACT selects 41 proposals for final review as part of global initiative to fund projects dedicated to fighting illegal trade, 14 December 2016, accessed December 2016\n- Philip Morris International, Philip Morris International Announces First Round Of Projects To Be Funded Through PMI IMPACT, A USD 100 Million Global Initiative To Fight Illegal Trade, 7 September 2017, accessed September 2017\n- PMI IMPACT, Selected Projects: First Funding Round, 2017, accessed September 2017\n- Belbrand Partners, 2018, accessed May 2018\n- JSC Grodno Tobacco Factory Neman, About company, accessed December 2017\n- Members of the Association, 2017, accessed September 2017\n- J.D. Kulick, J.E. Prieger, M. Kleiman, Unintended Consequences of Cigarette Prohibition, Regulation, and Taxation, January 2015, accessed September 2017\n- J.E. Prieger, J. Kulick, Cigarette Taxes and Illicit Trade in Europe, 20 January 2016, accessed September 2017\n- A. Aziani, J. Kulick, N. Norman, J.E. Prieger, Empty Discarded Pack Data and the Prevalence of Illicit Trade in Cigarettes, 26 January 2017, accessed September 2017\n- Center for the Study of Democracy, International Conference: Counteracting Cigarette Smuggling in the Balkans, undated, accessed December 2017\n- Center for the Study of Democracy, Round Table: Cigarette Markets in Bulgaria: Current State and Trends, undated, accessed December 2017\n- Center for the Study of Democracy, The illicit trade of tobacco products along the Balkan route: addressing institutional gaps and corruption, undated, accessed May 2018.\n- Center for the Study of Democracy, The illicit tobacco market: limits to institutional enforcement, undated, accessed March 2018\n- GLOBSEC, Partners in detail, undated, accessed December 2017\n- GLOBSEC, Panelist: Alvise Giustiniani, 2017, accessed September 2017\n- Kazazi Consulting, main clients are:, 2017, accessed December 2017\n- Endole Suite, Panta Rhei Research Limited, 1 September 2017, accessed December 2017\n- Companies House, Panta Rhei Research Limited, undated, accessed December 2017\n- Z. Hauser, Zora Hauser LinkedIn Profile , undated, accessed October 2017\n- KPMG, Project SUN: 2016 Results, 2017, accessed December 2017\n- RUSI, Project SUN: A Study of the Illicit Cigarette Market in the European Union, Norway and Switzerland, 4 July 2017, accessed October 2017\n- Illegal cigarette market costs UK taxpayer nearly £2bn, study finds, The Telegraph, 4 July 2017, accessed October 2017\n- Siracusa International Institute, Board of Directors, undated, accessed October 2017\n- SELEC, Croatia led the regional combat against illicit trade of tobacco products, 2016, accessed December 2017\n- SELEC, Anti-Fraud and Anti-Smuggling Task Force, 2016, accessed September 2017\n- Financial Times Live, Combating Illicit Trade: Progress, Challenges and Collaborative Solutions, undated, accessed October 2017\n- Philip Morris International, PMI UNGC Communication on Progress, 2015, 2016, accessed October 2016\n- Philip Morris International, Communication on Progress, 2016, 12 September 2017, accessed October 2017\n- Universiteit Utrecht, Research project into improvement of fight against illicit tobacco trade, 7 September 2017, accessed May 2018\n- Universiteit Utrecht, Utrecht University severs financial ties with PMI Impact, 17 January 2018, accessed May 2018', ""Conflict of interest in scientific research covers a wide spectrum of factors that might result in bias, not just financial ones. For example, a desire for publication could lead an investigator to use a skewed research design that favored a positive outcome, in hopes of improving the study’s appeal to a peer-reviewed journal. However, non-financial COIs can be very hard to identify and are likely an inescapable part of the fabric of science. For this reason, we will focus on financial factors influencing research and on different management strategies that have been developed to control the effects of FCOI on research.\n- Translational Research and Conflicts of Interest: Overview\n- Federal Rules Regarding Conflict of Interest\n- Managing Personal Conflict of Interest in Translational Research\n- Institutional Conflict of Interest\n- Managing Institutional Conflict of Interest in Translational Research\nTranslational research refers to the process by which basic scientific knowledge migrates from the laboratory into broad application in the community . In a capitalist society, this migration often involves commercialization of the research and the accompanying expectation that some person or persons will ultimately profit as a new therapy or technology receives widespread use.\n'What exactly is translational research? For many, the term refers to the “bench-to-bedside” enterprise of harnessing knowledge from basic sciences to produce new drugs, devices, and treatment options for patients. For this area of research—the interface between basic science and clinical medicine—the end point is the production of a promising new treatment that can be used clinically or commercialized (“brought to market”).' —S.H. Woolf, 2008 \nThis commercialization of research introduces issues related to financial conflict of interest (FCOI), in which a secondary interest—in this case, money— might affect an investigator’s primary professional obligation to perform unbiased research. However, such bias often cannot be measured. At best, a factor that could reasonably lead to a perception of bias can be identified, but perception is not the same as the actual presence of skewed judgment. Conflict of interest in scientific research covers a wide spectrum of factors that might result in bias, not just financial ones. For example, a desire for publication could lead an investigator to use a skewed research design that favored a positive outcome, in hopes of improving the study’s appeal to a peer-reviewed journal. However, non-financial COIs can be very hard to identify and are likely an inescapable part of the fabric of science. For this reason, we will focus on financial factors influencing research and on different management strategies that have been developed to control the effects of FCOI on research.\nFederal Rules Regarding COI\nThe first set of federal regulations regarding COI in research funded by the U.S. Public Health Service was issued in 1995. These regulations were comprehensively rewritten and reissued in 2011 as 42 CFR Part 50, Subpart F with a parallel set of rules for contractors (42 CFR Part 94). Together, these regulations provide clear rules regarding disclosure of COI and place responsibility on institutions for identifying and managing conflicted situations. However, they offer relatively little guidance on how an institution should actually manage cases of COI. The rules do not at all address the issue of institutional COI, which occurs when the institution where the research is conducted (such as a university or hospital) or the senior officials supervising the research have a financial stake in the outcome. The regulations also exempt the payment of royalties processed through an institution from federal rules governing COI, although the potential for increased royalties may in fact create an enormous incentive for bias.\nThere are two key definitions in 42 CFR 50 Subpart F that guide how institutions manage COI. The first is the definition of Significant Financial Interest; the second is the definition of a Financial Conflict of Interest.\nA few points regarding these regulations should be emphasized:\n- The list of what counts as a “significant financial interest” is comprehensive, with the minimal level for “significant” set at $5,000 per year (although some institutions require reporting of any payments at all, regardless of amount).\n- Spouses and dependent children are included in the determinations of whether a significant financial interest is present.\n- “Institutional responsibilities” on the part of individuals are comprehensive and include research, teaching, clinical work, and administrative tasks. Anyone with outside financial interests that overlap with their institutional responsibilities must be evaluated.\n- Section 2 of the regulations requires that most sponsored travel be reported, and includes required reporting of sponsored travel for spouses and dependent children.\n- Section 3 defines types of financial interests that are exempt, such as royalties paid through the institution.\n42 CFR 50 Subpart F applies to all faculty and staff who receive funding from the Public Health Service, which includes the National Institutes for Health (NIH), the Centers for Disease Control and Prevention (CDC), the Agency for Healthcare Research & Quality (AHRQ), and the Biomedical Advanced Research and Development Authority (BARDA).\nFederal Regulations Governing COI in Research 42 CFR Part 50, Subpart F (PDF) 42 CFR Part 94\nThe definition for FCOI used in 42 CFR 50 Subpart F entails two institutional responsibilities. First, the institution should evaluate whether the individual has both a significant financial interest and a role in the research that could affect the design, conduct, or reporting of research. If those two conditions are both fulfilled, the next step is to evaluate whether the financial interest could “directly and significantly” affect the research. Institutions vary substantially as to what they consider direct and significant, and the regulations provide very little guidance beyond the definition.\nManagement of COI is an imperfect art and is typically inconsistent across institutions. Federal regulations define issues likely to affect research, but are much less likely to suggest means for managing conflicts. The most common COI management strategy requires public disclosure of outside interests, usually in publications, presentations, grant applications, and informed consent documents. Disclosure is, however, a mixed blessing, because many people take industry relationships to be an endorsement as well as a potential source of bias. In addition, COI is often presented in ways—such as simply listing outside interests without disclosing the actual amounts—that trivialize an important and complex issue.\nGuidance for Identifying & Managing COI Financial Conflict of Interest (PPT - 12.2 MB) Responsible Conduct Research: Conflicts of Interest AAMC COI Initiatives NIH FCOI tutorial (PDF) FDA Guidance: Clinical Trial Data Monitoring Committees (PDF)\nSome institutions restrict the role a conflicted individual can play in the conduct of research. Typical restrictions include limitations on serving as a principal investigator (PI) ;, restrictions on obtaining informed consent, a requirement for external oversight, or limitations on study authorship and/or participation in data analysis. However, the dollar amount thresholds that trigger different types of management vary across institutions, providing little basis for identifying “standard” amounts.\nAlthough federal rules only address personal conflict of interest, most institutions also have policies regarding institutional conflict of interest (ICOI). ICOI is usually understood to encompass the financial interests of the institution and those of senior officials— presidents, chancellors, deans, department chairs—able to act for the institution on their own authority. For different reasons related to institutional authority structures, many institutions also include institutional review board (IRB) chairs and faculty who supervise clinical research programs in their ICOI policies.\nInstitutional Conflicts of Interest ICOI at NIH Grantees (PDF)\nThe most common way for ICOI to occur happens when institutions license their intellectual property to an outside, for-profit entity. The Bayh-Dole Act of 1980 required that NIH grant recipients pursue patenting and commercialization of intellectual property developed with federal grant dollars, preferably through licensing arrangements with small businesses. A pattern has since emerged in which many of these small businesses are start-ups created by faculty-inventors. The start-up companies then fund early-phase research using federal small-business grants such as those offered through the Small Business Innovation Research (SBIR)/Small Business Technology Transfer (STTR) program, as well as support from angel investors. In SBIR grants, the company is the actual grant recipient, but may subcontract with an academic institution. STTR grants require collaboration between a not-for-profit research institution and the small business. One challenge with both types of small business grants is the mixing of not-for-profit institutions and the small businesses for which their licensing agreements give them a financial stake.\nThe Jesse Gelsinger Case\nICOI first came to widespread public awareness when a teenaged research participant, Jesse Gelsinger, died after receiving an experimental gene-replacement therapy at the University of Pennsylvania for a rare metabolic disorder . ICOI emerged as a major issue in the case when it became clear that Penn had an enormous financial stake in the intellectual property being tested , as well as in Genovo, Inc., the start-up company to which the technology was licensed. Lead investigator James Wilson, who was also the co-founder of Genovo , had a substantial personal financial interest in the outcome. Lead plaintiffs’ attorney Alan Milstein argued that neither Penn nor Dr. Wilson had Jesse Gelsinger’s interests foremost in mind and as result made mistakes that included ignoring or obscuring evidence of poor outcomes in animal subjects treated with the experimental therapy —mistakes that reflected biases in favor of gene therapy, exacerbated by strong financial incentives\nDiscussion of the Gelsinger Case A Death in the Laboratory: The Politics of the Gelsinger Aftermath (PDF) The Death of Jesse Gelsinger: New Evidence of the Influence of Money and Prestige in Human Research (PDF)\nIn the wake of the settlement of the Gelsinger case, many institutions became very cautious regarding ICOI, to the point that the Association of American Medical Colleges recommended that they operate with a “rebuttable presumption” that the research should be done at institutions with no financial interests . The rebuttable presumption is predicated on a belief that institutions with a financial interest may not provide the level of the oversight they should. In reality, the primary drivers behind ICOI concerns are worries regarding perception and liability. If a bad outcome such as death or disability occurs in a clinical study and the institution has a financial stake, both juries and the public are likely to believe the oversight was lax, even if appropriate institutional checks were in place.\nFor most institutions, the central COI management strategy relies upon the “rebuttable presumption” described above. Under this rule, unless there is a clear and defensible reason that the institution should do the research, clinical studies should be done elsewhere. Examples of possible exceptions include the need for a particular technical skill (for example, a surgeon who is implanting a device that he or she invented), access to a limited patient population (particularly in rare or “orphan” diseases), or a piece of highly specialized equipment. If a decision is made that the work can be done at the inventor’s institution and the institution has invested, one option for managing ICOI is for the institution to divest itself of its financial interests (although most institutions are reluctant to take such a step). They can also monetize their interests by selling their equity and future royalties to investors, or donate royalties to another charity.\nAdditional Resources AMC Meeting and Symposia Proceedings The Scientific Basis of of Influence and Reciprocity: A Symposium (PDF) Protecting Patients, Preserving Integrity, Advancing Health:Accelerating the Implementation of COI Policies in Human Subjects Research (PDF) Institute of Medicine Report Conflict of Interest in Medical Research, Education, and Practice\nIf a decision is made to proceed with the research at the conflicted institution without eliminating the conflict, ICOI management should focus on moving oversight of the study to an external entity in order to limit any perception that the institution is serving its own self-interest. This externalization should include the use of an IRB that is not affiliated with the institution in question, because most IRB members are employees of the institution that has the ICOI. If the study is taking place at multiple research sites, this goal can be achieved by using the IRB from one of the other study sites to oversee the investigation. As a second layer of ICOI management, some institutions use an oversight board made up of external members. Operationally, this approach can combine COI oversight with functions related to data and safety monitoring. The research protocol should be reviewed for evidence of bias in design prior to submission to an IRB. Monitoring reports should be evaluated to be certain there are no biases in endpoint evaluations or study conduct. Finally, publications should be assessed to ensure that they present the study’s results fairly and accurately.\nInformation regarding any institutional interests in the research should be provided to potential research participants in the informed consent documents, and notice of the conflict should be included in publications, presentations, and grant applications. Some institutions may also create a system of external monitoring for clinical trials, using hired staff from a contract research organization (CRO).\n[Back to top]\n1. Woolf SH. The meaning of translational research and why it matters. JAMA. 2008;299:2437-9. PMID: 18182604. doi: 10.1001/jama.299.20.2437. ↑\n2. Institute of Medicine (US) Committee on Conflict of Interest in Medical Research, Education, and Practice; Lo B, Field MJ, eds. “Conflicts of interest in biomedical research.” In: Conflict of Interest in Medical Research, Education, and Practice. Washington (DC): National Academies Press (US); 2009. Available from: http://www.ncbi.nlm.nih.gov/books/NBK22942/. Accessed June 11, 2013. ↑\n3. Stolberg SG. The biotech death of Jesse Gelsinger. The New York Times. Nov 28, 1999. Available at: http://www.nytimes.com/1999/11/28/magazine/the-biotech-death-of-jesse-gelsinger.html?pagewanted=all&src=pm. Accessed June 12, 2013. ↑\n4. Wilson RF. The death of Jesse Gelsinger: new evidence of the influence of money and prestige in human research. Am J Law Med. 2010;36(2-3):295-325. Available online at: http://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?article=1125&context=wlufac. Accessed June 11, 2013. PMID: 20726398. ↑\n5. PRN Newswire. Biogen announces major gene therapy research collaboration with Genovo, Inc. August 15, 1995. ↑\n6. Weiss R, Nelson D. Methods faulted in fatal gene therapy: Teen was too sick for experimentation, federal probe finds. The Washington Post. Dec 8, 1999. Page A01. Available at: http://www.washingtonpost.com/wp-srv/WPcap/1999-12/08/089r-120899-idx.html. ↑\n7. The American Association of Medical Colleges. Protecting patients, preserving Integrity, advancing health: Accelerating the implementation of COI policies in human subjects research. A report of the AAMC-AAU Advisory Committee on Financial Conflicts of Interest in Human Subjects Research. February 2008. Page 6. Available online at: https://members.aamc.org/eweb/upload/Protecting%20Patients,%20Preserving%20Integrity.pdf. Accessed June 12, 2013. ↑\nTopic chapter originally published on January 3, 2014.\n- Questions or comments about Rethinking Clinical Trials can be submitted via email. Please add “Living Textbook” to the Subject line of the email.\n- Follow the NIH Collaboratory on Twitter!""]"	['<urn:uuid:1b5d962a-3805-43bd-bf94-3dd19160d9d2>', '<urn:uuid:252f899d-e72b-4260-a281-30cc700c3ee8>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	8	105	5056
26	dark sky lighting vs sea turtle lighting differences	Dark Sky lighting and sea turtle-friendly lighting have different specifications. Dark Sky lighting focuses on reducing light pollution for stargazing by using fully-recessed fixtures and eliminating extraneous illumination. In contrast, sea turtle-friendly lighting follows three specific rules: using long-wavelength lighting (amber, orange, red), keeping lights low, and using shielded fixtures to protect nesting turtles and hatchlings from disorientation.	['You don’t need to be Stephen Hawking or Neil deGrasse Tyson to appreciate the benefits pitch-dark skies bring to even casual stargazing. But as naturally-dark skies become increasingly rare due to the surging density of metropolitan areas and the march of urban sprawl, is there anything the sustainability-minded homeowner can do to help reverse the trend? One answer lies in Dark Sky-friendly outdoor lighting, an easy win within reach for anyone building, remodeling, or retrofitting a home.\nSo what exactly is Dark Sky? A concept advocated by the International Dark-Sky Association (IDA), the Dark Sky program is a sensible, education-based approach to enabling the reduction of light pollution, the root cause of those less-than-dark skies that can make stargazing problematic.\nWhile we won’t get into the details of all the work IDA is doing here, the upshot for homeowners is that they are making it easier to ensure your dwelling is not contributing to light pollution by certifying fixtures as Dark Sky-friendly through their Fixture Seal of Approval (FSA) program. That FSA certification process ensures fixtures so labeled are doing their part to reduce or eliminate the production of extraneous illumination that can contribute to light pollution. IDA, by the way, does not manufacture or sell lighting fixtures.\nThe biggest culprits around your current home are likely to be landscape lighting, those fixtures mounted on exterior walls, and yard or driveway lighting. All of those problem areas and more can potentially be addressed through the installation of location-appropriate Dark Sky lighting fixtures.\nIf you are the handy type, you may also find it possible to modify some of your existing fixtures to produce less light pollution. The IDA site has a section on outdoor lighting basics that depicts acceptable and unacceptable types of fixtures, and those images may help fuel do-it-yourself ideas for those inclined to go the hacking route.\nFinding Dark Sky lighting on the shelf can be difficult in some locations, but as the popularity of these products increases the big-box home improvement stores seem to be increasing their inventory. In a recent visit to a Lowe’s store, we counted a total of 7 fixtures on the shelves with IDA Dark Sky labeling across the front of their boxes. All of these were wall-mounted fixtures, in a mix of traditional and contemporary styles. What we did not see at Lowe’s, however, was a single landscape lighting fixture with the Dark Sky label, though you should be able to find them online from various retailers. To zero in on relevant products, use the “Dark Sky” search term.\nFor those contemplating building a new home, the right time to address potential light pollution is in the design phase with your architect or builder. In the case of our lab home, we set out with the goal to be 100% Dark Sky-compliant on all outdoor lighting. Our architect and his team incorporated deep overhangs for all of the home’s decks and porches, so by simply installing standard, fully-recessed ceiling fixtures we were effectively Dark Sky-friendly in those locations.\nFor other outdoor areas of the home, we did our research early in the planning phase and were able to find suitable fixtures that met the Dark Sky spec, yet did not detract from our Texas Hill Country contemporary design aesthetic. For example, in the case of all wall-mounted fixtures, we found and installed the Cascade 1830TT downlight from Hinkley Lighting, a Dark Sky-compliant yet decidedly-contemporary product.\nComing back down to Earth around your new sustainable home build, remodel, or retrofit projects, think of Dark Sky lighting compliance in this way: The only direction you can go is up, meaning less light pollution and a better chance of looking skyward and seeing the wonder of the Pleiades.', 'Controlling competing vegetation and brown spot disease are two main reasons we prescribe burn young longleaf plantations:\n- Longleaf pine seedlings do not like competing vegetation and will stay in the grass stage for years if vegetation is not controlled by fire, mowing or herbicides. Using improved containerized seedlings along with good vegetation management can release longleaf pines from the grass stage in 2-3 years.\n- Longleaf pines are the only species of southern pines susceptible to brown spot needle blight. Seedlings are infected in the grass stage and can die from the disease. Prescribe burning is an effective method for controlling this fungus disease. Burning removes the infected needles and kills the spores. Brown spot can be identified by yellow bands on the needles, which eventually turn brown as shown below.\nDecember thru March is the typical burn window for this activity. Older longleaf pine stands can be burned into late spring with the right weather conditions and understory. If turkey management is important to you, wait until nesting season is over to burn mature longleaf stands. We get better control of understory brush and hardwoods burning in the spring.\nThe link shown here shows an example of a control burn in 5 year old longleaf pine that is in the sampling stage. Good forest management practices have got these pines off to a good start. The key is not to damage the bud in the tip of pines or the newly formed candles in the spring.\n*Make sure and get a burn permit from the local state forestry service before you light your fire.\n“Timing of Prescribed Fire in Longleaf Pine” http://www.southernfireexchange.org/SFE_Publications/etc/Clemsonforfl32.pdf\n“Prescribed Burning in Newly Planted Longleaf Pine.” (Alabama Guide Sheet No. AL 338 A)\nAwareness is Growing!\nPhoto by: Aprile Clark\nThat is, when it comes to lights on our homes and businesses near their nesting beaches. Humans have long-known that artificial light can have negative consequences for many nocturnal animals, including nesting and hatching sea turtles. However, it has only been through fairly recent research that we are beginning to understand the reasons behind some of these effects and developing better lighting (or non-lighting) strategies and alternatives to protect our treasured marine turtle species.\nMother sea turtles that nest on Florida Panhandle beaches are “hard-wired” for nighttime activity when it comes to digging their nest cavities and depositing eggs. Likewise, their babies typically leave their sandy nests under cover of darkness, scampering to the Gulf of Mexico. This nocturnal behavior is important for avoiding predators that would have an easy meal of a baby turtle crossing the open beach in the light of day. However, even hatchlings emerging at night face a number of other obstacles. Once in the water there are a many aquatic predators that will not hesitate to gobble up a baby turtle. On average, it is estimated that only about 1 in 1000 babies survive to reach adulthood. With those odds, it would be wise for us to do anything we can to minimize additional threats or hazards during the short but crucial time these marine reptiles spend on the narrow thread of beachfront that we share with them.\nOne thing we can do involves reducing the disorienting effects of artificial light near our sea turtle nesting beaches. The term “phototactic” is used to describe organisms that are stimulated to move towards or away from light. Nesting females have been shown to avoid bright areas on the beach but hatchlings tend to be attracted to the brightest source of light when they emerge from the sand. On a nesting beach with no artificial lighting, any natural light from the moon or stars is reflected off the water, creating a much brighter horizon in that direction. This naturally attracts the hatchlings in the right direction. Lights from human sources can appear very bright in comparison and quite often draw babies over the dunes and into harm’s way on roadways, from predators, or simply by exposure once the sun comes up.\nMany beachfront property owners have learned about this threat and have taken this issue to heart by reducing the amount of light on their property and eliminating or replacing lights visible from the nesting beach with sea turtle-friendly lighting. There are three rules to follow when retrofitting or installing new lighting near the beach.\n- Keep it Long: Long-wave-length lighting that is still in the portion of the spectrum visible to humans includes amber, orange and red light. Manufacturers are now making highly efficient LED bulbs that are certified by the FWC as turtle-friendly.\n- Keep it Low: Many times lighting needed for safety of access can be placed low enough to be unseen from the nesting beach.\n- Keep it Shielded: Fixtures that are in line-of-site to the nesting beach need to be recessed to shield the bulb from being directly visible. The correct long-wave-length bulb should also be used in these shielded fixtures.\nRemember, exterior lighting is not the only danger turtles face from our lights. Unobstructed interior lights seen through windows and doors can be just as detrimental. The best solution here is to tint beach-facing glass with a 15% transmittance tinting product. This will save money on cooling bills as well as protect interior furnishings and avoid the possibility that someone in your house might leave the blinds or curtains open accidentally during turtle season. If you have questions regarding turtle-safe lighting practices in Florida there are many resources available through the Fish and Wildlife Conservation Commission the National Marine Fisheries Service, the Sea Turtle Conservancy, and your local UF/IFAS County Extension offices. If you really want to get into the nitty-gritty of turtle lighting and ways to protect turtles check out this FWC publication on assessing and resolving light pollution problems and this model lighting ordinance from UF’s Levin College of Law. Most Florida coastal counties have already adopted sea turtle lighting ordinances so you should also check your local county codes for this issue. Let’s help keep sea turtles in the dark, where they need to be.\nManaging a pond can be both rewarding and challenging.\nPhoto Credit: Mark Mauldin\nPonds can be a source of great enjoyment. However, properly managing them to meet your desired goals can be challenging. Panhandle Pond Management, a two part series being offered by UF/IFAS Extension, is designed to help pond owners/managers become more successful in reaching their goals. Specialists from campus will be onsite to share their expertise. Dr. Chuck Cichra, UF Fisheries and Aquatic Sciences, will lead session 1 and Dr. Stephen Enloe, UF Center for Aquatic and Invasive Plants, will lead session 2.\nSession 1 – May 31st Fish Management will focus on decisions that the pond owner can make that directly relate to the success and productivity of the fish population in a pond. Stocking, harvesting, feeding, aeration and other topics will be covered.\nSession 2 – June 7th Aquatic Weed Management will involve weed identification, control options, and herbicide application techniques. If you have problem weeds bring samples for identification and control recommendations.\nPanhandle Pond Management will be held at the Washington County Agricultural Center, 1424 Jackson Ave, Chipley FL. Each session will begin at 6:00pm; a meal will be served. To ensure we have enough food advanced registration is strongly encouraged. There is a $10 registration fee per session. To register call the Washington County Extension Office (850-638-6180) or use the links below for online registration. Session 2 attendees will receive a copy of Weed Control in Ponds a bound book sold through the IFAS bookstore.\nSession 1 – https://goo.gl/Rwn9dk\nSession 2 – https://goo.gl/zj58o6\nALL PHOTOS: MOLLY O’CONNOR\nContinuing our “ecotour” of the Florida Panhandle along the ICW, this month’s stop is in Walton County. Arguably one of the fastest growing counties in the state, South Walton has become a favorite with many. Relatively undeveloped (as compared to neighboring counties) Walton has an opportunity to develop smarter… and for a lot of it – they have.\nEntering Walton County from the west you leave the concentrated tourism of Destin and enter Sandestin. Though still highly developed it slowly gives way to the Point Washington area and more open ground. Here you begin to encounter the famous Dune Lakes. There are no barrier islands along the coastline of Walton County. Instead the Gulf meets the peninsula separating it from Choctawhatchee Bay to the north. Along the Gulf there are magnificent dune fields and freshwater lakes that periodically are open to the Gulf. The lakes are unique in that they have freshwater habitats and tannic waters as well as saltmarsh and seawater when their “mouths” are open. This unique situation provides an ecosystem found in few places in our state. The now famous 30-A travels along these dune lakes across the entire of South Walton. Two state parks, a state forest, a bike trail, and small communities dot this famous trail.\nBetween 30-A and the Intracoastal Waterway to the north lies the Point Washington State Forest. Here lie acres of well managed pine forest. There are many trails that can be hiked, driven, or traversed by horseback. There are several trails here that are part of state forest’s Trailwalker Program. A major part of this well managed forest is prescribed burning.\nThis dune lake at Topsail State Park is one of many “stretched” across South Walton County.\nThe state park provides kayaks for exploring the dune lake at Topsail. It can be reached by hiking or a tram they provide.\nThe beautiful beaches of south Walton.\nHere is one of the many state forest trails used by hikers and joggers.\nThe Florida State Forest hiking trail system provides a lot of information about their trails that is very useful.\nThe Florida State Trailwalkers Program is a neat way to encourage locals to visit their “natural landscape”. You must hike 10 of the selected trails (on their website provided in this article). There is a log you can download and when you have logged your 10 hikes mail it in. You will receive a free patch indicating you are a “Florida Trailwalker”! This is a great way to explore your “natural Florida”.\nThere are trails that cross Point Washington State Forest that can be driven, hiked, or even explored on horseback.\nSignage educating the public about the benefits of prescribed burning.\nThis is the renovated Wesley House at Eden Gardens State Park. The Wesley’s were in the timber industry.\nEden Garden State Park is located north of Highway 98 in south Walton County. It borders the south side of the ICW near Choctawhatchee Bay and is a beautiful getaway.\nPartly due to successful bear management and partly due to the increase population of humans in the Florida panhandle, bear encounters are in the increase. This sign at Eden Garden State Park provides information on how to deal with an encounter.\nFrom Eden Garden State Park you can look across the bayou to the point where the ICW leaves Choctawhatchee Bay and enters a manmade canal locals refer to as “the ditch”. Notice the prescribed burn occurring across the bay.\nNow days many are aware of the forest management tool we call “prescribed burning”… but not all. For many people, fires are “wild” and the smoke they generate is an unwanted pain. Much of the southeast, particularly Florida, encounter thunderstorms with frequent lighting. These lighting strikes spark fires which burn across acres of forest. The frequency of the lighting storms – and fires – actually kept the understory below the pines cleared and so the fires typically burned low and slow. The result was an understory of grasses and wildflowers that supported a wide variety of species including deer and quail. When he traveled through the southeast in 1775, William Bartram described the longleaf forest as “open and monotonous – going on for miles”. And this was the case… you could literally see up to a mile across the landscape. But this “monotonous” landscape was misleading in that there were thousands of species found here – many feel one of the most biological diverse systems anywhere.\nToday 90% of the longleaf has been logged and much of what remains is fire suppressed. For the reasons mentioned above, residents have resisted the natural burns and many species – gopher tortoises, quail, and indigo snakes to name three – have suffered as well. There is a move across the southeast to restore the old longleaf pine forest. These trees produce excellent timber – though it takes longer to grow than the loblolly and slash pine currently grown – and many are managing their property for quail and deer hunting. Yes… the smoke is a problem but the state forestry system plans their burns to reduce the impact it has on the local community as best they can. If you really want to see the benefits of a well-managed pine forest, take a hike through one – it truly is amazing.\nThe Annual Tupelo Honey Festival will be held on Saturday, May 21st from 9 AM – 4 PM at Lake Alice Park in Wewahitchka. It’s an exciting event, and your chance to take part in this local delicacy. Area honey producers will be on hand, selling their honey in a variety of sizes. There will also be food, art & crafts, and live music.\nFor decades, tupelo honey has been synonymous with Gulf county. The pollen from the tupelo gum tree (Nyssa ogeche), produces some of the finest honey in the world. The common name “tupelo” is derived from language of the Muscogee Nation, also known as the Creek Indian Nation. The meaning of the word is “swamp tree”, as this tree flourishes in areas of wet soils and seasonal flooding. Gulf county, especially in the Dead Lakes and Apalachicola river region, provides prime habitat for one of the largest tupelo forests on earth.\nThe tupelo pollination process kicks off during April. The tupelo bloom begins to form as a small bud. Within a few weeks, the bud explodes into a cluster of many nail or spike like attachments. At this point, honeybees begin to descend and capture the pollen.\nFigure 1. Honeybee visiting tupelo blossoms.\nCredit. Gulf County Tourist Development Council.\nThe tupelo bloom season lasts from approximately mid-April to the end of May. This is an anxious time for beekeepers. Tupelo blooms are very temperamental and delicate in nature. For this short period, beekeepers hope for little wind or rain and no cold temperatures, as any of these factors can decimate tupelo honey production. Regardless of seasonal impacts, the demand for Gulf County’s tupelo honey never subsides.\nA bonus to honey’s great taste, is the medicinal value. Honey has been used for medicinal purposes throughout time and cultures. Ancient Egyptians used honey in the embalming process, wound dressing and treatment for burns. Honey can be used as an antimicrobial agent. This is mostly due to low concentrations of hydrogen peroxide produced naturally from sugar compounds. Honey contains large amounts of sugars, approximately 97%. Most of the sugar content is glucose and fructose. Honey also contains smaller amounts of vitamins and minerals.\nThe color of honey is a factor when grading content. Generally, a darker honey will have a higher concentration of polyphenols. This means the honey is higher in antioxidants and anti-inflammatory properties. Exposure to area honey has been thought to help people who suffer from area specific seasonal allergies. However, there is no consensus among the scientific community to support the claim. Though there is research supporting honey as medicinal purposes, please consult with your physician before using as a medical treatment.\nEnjoy tupelo honey and see you at the festival!\nFor more information on Gulf County Tupelo Honey, please visit:\nSupporting information for this article can be found in the UF/IFAS EDIS publication “Health Benefits and Medicinal Value of Honey” by Sara Marshall, Liwei Gu and Keith R. Schneider: https://edis.ifas.ufl.edu/pdffiles/FS/FS26700.pdf\nAn Equal Opportunity Institution. UF/IFAS Extension, University of Florida, Institute of Food and Agricultural Sciences, Nick T. Place, Dean for UF/IFAS Extension. Single copies of UF/IFAS Extension publications (excluding 4-H and youth publications) are available free to Florida residents from county UF/IFAS Extension offices.']	['<urn:uuid:eb9c7bc1-de0e-4f01-aa2f-1366581801c9>', '<urn:uuid:3111b11a-79ef-4c14-bab4-ff1f3e4962f4>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T17:51:21.175311	8	58	3322
27	How was Bavikonda Hill important in Buddhist history, and what modern day techniques are used to restore such ancient Buddhist sites?	Bavikonda Hill was one of the oldest and most revered Buddhist sites in Asia where all three branches of Buddhism - Hinayana, Mahayana and Vajrayana flourished. The site contained 26 structures including a Mahachaitya, vihara complex, and votive stupas. Notably, a piece of bone believed to be Lord Buddha's mortal remains was discovered there during excavations. Regarding restoration techniques, while modern methods exist, they can compromise authenticity. Some places still use traditional restoration techniques where the base is made using chuna and jaggery water. For painting, stone colors mixed with preservative chemicals are used on slightly wet bases, which can preserve the work for 20-25 years. However, these authentic restoration materials are expensive, with preservative chemicals costing Rs. 18,000 per kg when imported from England.	['Named after Nagarjuna, a master of Mahayana Buddhism in South India, the\nhistorical hill called Nagarjunakonda\nserved as the hub of\nBuddhists. One of Indias richest Buddhist sites, Nagarjunakonda hosts\nthe relics of many Buddhist universities and monasteries which attracted\nstudents from China, Gandhara, Bengal and Sri Lanka in ancient times.\nPresently what constitutes of Nagarjunakonda is a restored Buddhist site on\nan island which symbolizes a lost civilization that flourished thousand\nExcavations till date have yielded 30 Buddhist monasteries, throwing a\nlight on the strong foothold of Buddhism in the city. Others include Maha\nChaitya, Viharas, Buddha statues, relics, stone images, Jataka tales\nencrypted panels and other items of Buddhist origin.\nExcavations carried out at Chandavaram, the ancient Buddhist heritage site\nhave revealed the Mahastupa built on terraced platform, Chaityagriha votive\nstupas, drum slabs and Brahmi label inscriptions.\nSurrounded by the recently excavated remains of the Buddhist heritage\nincluding a Buddhist University, Amphitheatre, Viharas and Chaityas; Anupa\nforms the part of Buddhist circuit in Andhra Pradesh.\nLocated on the top of the Mangamaripeta Hill, Thotlakonda\nan important information exchange spot for Buddhist culture between India,\nSri Lanka and other parts of south-east Asia. About 16 km from the port city\nof Vishakhapatnam, the site hosts a Buddhist Complex featuring a Hinayana\nmonastery, Mahastupa, 16 votive stupas, a stone pillared congregation hall,\napsidal chaitya grihas and10 viharas. The excavations, over a period of 5\nyears, have revealed other important Buddhist remains including 12 Brahmi\ninscriptions, sculpted panels, miniature stupa models in stone and Buddha\npadukas - foot imprints with ashthamangal symbols. At present, the statue of\nmeditating Buddha adorns the Buddhist site - Thotlakonda.\nAnother Buddhist heritage site around Vishakhapatnam, Bavikonda Hill\nunearthed the relics of a Buddhist complex comprising of 26 structures\nincluding a Mahachaitya, large vihara complex, numerous votive stupas, rock\ncut caves, a stone pillared congregation hall, rectangular halls and a\nrefectory during excavations. Evidences showcase that all three branches of\nBuddhism - Hinayana, Mahayana and Vajrayana flourished here.\nA piece of bone stored in a pot believed to be the mortal remains of Lord\nBuddha was discovered during excavations. Counted amongst one of the oldest\nand most revered Buddhist sites in Asia, Bavikonda traces the history of the\nreligion back in time that once existed in the southern part of India.\nAnciently known as Pratipalapura, Bhattiprolu\nflourished as a\nBuddhist town during the rule of Sala Kingdom in present day Guntur\nDistrict. Revered for its ancient Buddha stupa locally famous as Chinna\nLanja Dibba / Vikramarka Kota Dibba,\nthe site also justifies the\nlinguistic evidence that Telugu, the state language of Andhra Pradesh,\nderives its origins from Brahmi script.\nExcavations of Bhattiprolu were undertaken by Alexander Rea in 19th century\nwhere three mounds featuring three inscribed stone relic caskets, relics of\nBuddha and jewels were discovered. The significant discovery of the site\nincludes the Mahachaitya Stupa, crystal relic casket and several images of\n, an important Buddhist site features the ancient relics of\nrock stupas covered with bricks, rock-cut caves, early historic pottery,\nbrick-built structural edifices and Satavahana coins that date back to the\n1st century AD. Close to Sankaram, another heritage site of Bojjannakonda\nis located where one can experience close connection with the life of Buddha\nthrough rock carvings on the façade of the caves.\nfeatures hundreds of rock cut monolithic stupas in\nrows, spread throughout the hill. Other Buddhist attractions here include\nrelic casket, three chaitya halls, votive platforms, stupas and Vajrayana\nsculptures. Present Vihara remains portray the development of three schools\nof Buddhism including Theravada, Mahayana and Vajrayana.\nnear Vijaywada are the one of the finest rock cut\narchitecture caves in India. Carved out of solid sandstone on a hillside,\nthe main cave section includes primitive rock monastery cells along with\nother caves featuring deities like Vishnu, Shiva and Brahma. The first floor\nof this relic still retains style of Buddhist vihara along with some\ntown serves as a rare and important center for Buddhist\nsculptures. 19th Century excavations of the site unfurled carved limestone\ncolumns and pillared halls associated with Buddhist monastic establishments.\nAlong with this, the ruined Maha Chaitya Stupa with circumference of 112\nfeet and 23 feet high here consists of a solid brick cube in the centre,\ninscribed with 12 zodiac constellations. At present, the site has its own\nmuseum containing important Buddhist stupa and sculptural slabs found during\nthe recent excavations. Adurru\nnear Ghantasala also hosts a\nMahastupa measuring 17 feet in diameter.\nGudiwada town near Vijaywada also hosts relics based on Buddhist and Jain\nbeliefs. Around 99 low mounds with remnants of Buddhist Stupas were\n, the village is famous as a Buddhist remnant site\nfeaturing a number of Buddhist stupas and a huge monastery complex. Located\non a hillock amidst scenic surroundings, Salihundam has the treasures of a\nChaityagriha, structural temples and a number of sculptures showcasing the\nrelics of three different Buddhism phases - Theravada, Mahayana and\nin Khammam district is a remnant example Buddhist\nsite belonging to Ikshvakus -Vishnukundin times. The excavations brought to\nlight a number of huge standing Buddha images, Vishnukundin coins, Vlharas,\nmahachaitya, votive stupas, terracotta figurines, early historic pottery and\nbronze image of Buddha. This site probably served as a link between coastal\nand Telangana areas for traders and also to Buddhist monks. Basing on the\narchaeological evidences, the site belongs to the period between 3rd and 5th\nThe Buddhist connection with Dantapuram\nsite is evident through the\nextracted excavations. Often referred to as Dhantavaktruni Kota\nBuddhist literature, the site has revealed four brick votive stupas along\nwith various antiquities like beads of Terracotta, semi precious stone\nbeads, ornaments and different coloured potteries. Evidences even showcase\nthat the site served as a cultural and religious exchange zone for Buddhism.\nThe excavation relics at Pavuralakonda\nhave exposed the proof of\nhosting a major ancient Buddhist settlement and human habitation. Sixteen\nrock-cut cisterns used for rainwater preservation also adorn the hillock and\nprovide a panoramic view of the coastline.\nLocated on the hill top, the recently excavated Phanigiri\nshowcases a massive Buddhist complex featuring a large Stupa, two apsidal\nhalls with stupas in it, three Viharas as dwelling of monks. The remains of\nthis complex stand on the top of a table hill shaped as snake hood, through\nwhich the site got its name - Phanigiri.\nRecently excavated relics of Gopalapatnam\npresent the entire\nBuddhist village which dwelled on the Budhikonda Hill range. The entire\nvillage is dotted with stone veneered, brick built and votive stupas,\nviharas and other Buddhist imprints.\nnear Vishakhapatnam is a 2000 year old Buddhist\nheritage spot famous for Rock cut Lord Buddha Statue. Also, the site is\nknown to have yielded ancient statues of Buddha in various postures,\nenchanting stupas and other Buddhism related structures like meeting halls.\nEven different varieties of bricks used in the construction during the\nBuddhist period were discovered from the site.\nThe black hill - Durga konda near Ramatheertham in Vizianagaram\nfeatures remains of a Buddhist Monastic complex and Rock-cut caves.', 'The art of preserving heritage sites is much more difficult than it seems. In the process of their restoration, there have also been instances of loss of originality and ancient touch of the murals.\nBy Surbhi Shah\nBengaluru, February 25, 2020\nRe-constructing ancient sites of importance without losing their essence is a much more difficult task than what Senil Kumar had anticipated before migrating from a Tamil Nadu village to Bengaluru.\nHired for re-painting the Tirupati Balaji temple in K.R. Market, Senil’s hands skilfully moved across his painting as he observed his master piece—which will be placed high on the temple wall, without anyone knowing the artist behind it.\n“My mates and I started working on this site two weeks ago. The base has been put up and now we are colouring the murals,” said Senil Kumar while balancing himself on a bamboo structure, with a paint brush stuck between his teeth and a colour bucket between his fingers.\nLaxmi, a worker in the temple said, “The building was starting to look very bad. It had degraded and turned black on the yellow marble stone, so the temple authority decided to re-paint it.”\nThe temple is declared as a protected ancient monument under Section 3 of the Karnataka Ancient and Historical Monuments and Archaeological Sites and Remains Act 1961.\nThough the re-construction of the heritage sites is important in order to conserve the history and culture, many people believe that the sites are losing their ancient charm. They believe that the use of present- day colours and tools for restoration make the murals seem vandalised.\n“Earlier when we used to see the monuments and temples, all had such different architecture and carvings, but now all the murals look the same—same colour, same faces and same design. It is scrapping out the old and rustic look of the monuments,” said Hiral, a tourist from Indore, Madhya Pradesh who was walking down the Bengaluru Fort steps, taking a city tour.\nShe added that the Bengaluru Fort—which was under construction months back for re-building its temple looked the opposite. “The fort is an ancient monument and the newly built temple looks so modern. It takes away the vibe,” said Hiral.\nNagraj Sharma, a researcher of arts and archaeology believed that the sites shouldn’t be repainted at all, and that the process for restoration is given under the Archaeological Department guidelines.\n“It is not important what materials we use because we have to preserve the original paintings. The mentality of the current artist is not the same as that of the original. So we should refrain from repainting them,” he said.\nHowever, in some places, the old techniques of restoration are still in practice where the base of the wall is made using chuna, jaggery water, etc to restore the ancient touch of the monuments.\n“We had re-painted a temple and a palace in Indore and Bhopal. The base is made differently and the painting is done on a slightly wet base, as contrary to the normal painting done on the dry base. Stone colours are used and they are mixed with a preservative chemical that helps keep them on for at least 20 to 25 years,” said Chainika Khona, a student artist in Madhya Pradesh.\nIn addition to it, remaking something, especially the art and architecture of the ancient heritage buildings in India, is not a cake walk.\nChainika said that the chemical mixed is expensive in India and that they import it from England for Rs. 18,000 per kg. “For painting 30 to 40 feet we need only grams of the chemical so it works, but it is expensive for larger projects,” she added.\nThe project was led by Chainika’s professor—Manoj Kumar Upadhyay, which was given to him under a government tender.\nApart from painting, re-construction of heritage buildings in Bengaluru is also under way. The project of Bengaluru Gate is currently taken up by the Indian National Trust for Art and Cultural Heritage (INTACH).\nThe reports show that around Rs.50 lakh were used to restore the century old monument. “Our team is currently working on the project and it may be done within two to three months,” said Meera Iyer, convenor, INTACH.']	['<urn:uuid:9e115d0b-c395-40fd-8215-561fd8acd806>', '<urn:uuid:4db36374-0db3-4b31-8164-72a07a865479>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T17:51:21.175311	21	125	1871
28	I'm curious about the creative abilities of robots - can they create original art and ideas like humans can?	Robots are not keeping up with human creativity. While they can randomly combine old ideas to create new ones, they cannot create truly valuable and meaningful new ideas like poetry, music, recipes, jokes, fashion design and scientific theories.	['Robots need electrical components to control and power machines. Basically, most robots need electricity (for example, a battery). Robots contain at least some level of computer programming.\nIn this article :\nWhat every robot needs?\nWhat cannot the robot do? Robots are also not keeping up with human creativity: the ability to create new and valuable ideas such as poetry, music, recipes, jokes, fashion design and scientific theories. On the same subject : How to robots work. While technology is able to randomly combine old ideas to create new ones, the result doesn’t necessarily make sense – nor is it worthwhile.\nThe Three Laws, quoted from the Handbook of Robotics, 56th Edition, 2058 AD, are: The First Law. A robot cannot hurt a person or allow a person to do harm by idleness. Second Law.\nA typical robot has a moving physical structure, some engine, sensor system, power supply, and a computer “brain” that controls all of these elements. Basically, robots are a man-made version of animal life – they are machines that mimic the behavior of humans and animals.\nWhat are the 4 basic components of a robot? Basic robot components The robot components are body / frame, control system, manipulators and drive system.\nIs a robot brain ?. So the robot has sensors and actuators. … You might assume that a robot’s brain is a computer – and surely many robots have computers. The computer, however, does not function like the human brain. It is a complex set of electronic circuits that performs millions of operations per second.\nWhat are 7 advantages of humanoid robots?\nAdvantages and disadvantages of robotic automation On the same subject : How do robots move.\n- Profitability. There will be no lunch breaks, vacation, sick leave or shift time spent on robotics automation. …\n- Better quality control. …\n- Increased productivity. …\n- Working in hazardous environments. …\n- Potential loss of work. …\n- Initial investment costs.\nWhat are the advantages? In many situations, robots can increase the productivity, efficiency, quality and consistency of products: unlike humans, robots do not get bored. As long as they don’t wear out, they can do the same thing over and over again.\nRobots eliminate hazardous workplaces for humans because they are capable of working in hazardous environments. They can handle heavy lifting, toxic substances, and repetitive tasks. This has helped companies prevent many accidents as well as saving time and money.\nWhat is the main purpose of robots ?. Robotics, design, construction and use of machines (robots) to perform tasks traditionally performed by humans. Robots are widely used in industries such as car manufacturing to perform simple, repetitive tasks and in industries where work must be performed in environments that are dangerous to humans.\nAre robots good for the economy? Robots take your job! … While this can have a negative impact on certain job segments, robots and automation increase productivity, reduce production costs, and can create new jobs in the technology sector.\nCan robots replace humans? Yes, robots will replace people in many jobs, just as innovative farming equipment replaced people and horses during the Industrial Revolution. … Factories are developing robots that are increasingly driven by machine learning algorithms, so they can adapt to the people working next to them.\nWhat can robots not do?\nThe gentle work performed by the robot prevents the human error factor and even the possibility of the surgeon slipping while cutting into the patient. They also managed to automate some of the time-consuming tasks in the laboratory work. This may interest you : How to stun robots as patrick. The medical industry is one of the fastest growing areas of robotics.\nWhy shouldn’t robots have rights? Machines have no rights; they have no feelings or emotions. However, robots are becoming more and more advanced and are starting to develop with higher levels of AI. Someday in the future, robots may start to think more like humans, while the legal norms will have to change.\nThe most difficult thing for a robot is to walk. It is also difficult for the makers of the robot, as the act of walking involves hundreds of specific movements. Some real robots need to walk on uneven surfaces, such as the surface of Mars, so these robots need sensors in their legs to find a good foothold!\nWhat are the 4 types of robots?\nWhat are the main functions of the robot ?. They can recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, gather supplies, and perform other useful tasks. On the same subject : How to make robots for beginners. General purpose robots can perform different functions simultaneously or they can have different roles at different times of the day.\nHow long does it take to build a robot? As simple as a robot. The fact that software robots do not have to be physically produced makes their development relatively simple and fast. It takes about one and a half to four months to create a new robot from scratch. The payback time can be up to six months.\nDo robots have feelings? As cute and adorable as they are, the capabilities and intelligence of “emotional” robots are still very limited. They have no feelings and are simply programmed to detect emotions and respond appropriately. But everything will change very quickly. … To feel emotions you need to be aware and aware of yourself.\nThe Cartesian robot is the most widely used industrial robot, typically for CNC machines or 3D printing.\nA simpler, more complete definition of robot types can be narrowed down to five types: Cartesian, cylindrical, SCARA, 6-axis and delta. Each type of industrial robot has specific components that make it best suited for a variety of applications. The main distinguishing features among them are speed, size and working space.']	['<urn:uuid:291c5260-9557-44ec-9601-a0264fb441ef>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	19	38	977
29	What are the key differences in monitoring requirements between parking garage air quality management and confined space atmospheric testing in terms of gas detection methods?	In parking garages, gas transmitters with internal pumps monitor carbon monoxide and nitrogen dioxide from vehicular exhaust through HVAC ductwork, with new wireless mesh communications relaying readings to central controllers. These wireless systems require minimal conduit/wiring and are effective at detecting CO and NO2 in hard-to-reach areas. In contrast, confined space atmospheric testing requires more comprehensive gas detection - the air must be tested specifically for oxygen levels, toxic gases, flammable gases and other contaminants using proper equipment, as human senses cannot reliably detect these hazards. Additionally, confined spaces require continuous monitoring from outside by a trained observer, with specific communication methods like mobile phones, two-way radios, CCTV or hand signals, plus appropriate rescue equipment must be readily available.	['Combat poor indoor air quality\nEPA estimates as many as one in three commercial buildings in the U.S. have poor indoor air quality (IAQ). Causes can include contaminated ductwork; improper filtration; airborne particulates; mold-producing water leaks; off-gassing of chemicals from carpets, vegetation and other bio-sources; and carbon dioxide (CO2) from human exhalation, to list but a few. Advances in instrumentation and technology help remedy these types of indoor air problems.\nCO2 gained increased attention after research linked CO2 buildup in buildings to decreased worker productivity and occupant discomfort, fatigue and asthmatic symptoms. Combined with poorly operating ventilation systems, this produces Sick Building Syndrome.\nCO2 monitors use infrared sensing optics and can be hardwired to standard junction boxes or other 4-20mA power sources for on-demand activation of the ventilation system. Sensors are housed in small plastic enclosures that blend with other building security system devices. Other models can be duct-mounted with powerful pumps to monitor incoming and returned airflow. These units enhance building comfort in a measurable way.\nA “fixed” prescription\nFixed-install gas detectors capitalize on the latest developments in sensing technologies and interoperability with building management systems (BMS). New capabilities provide real-time information about gas concentration levels and other relevant data through PLCs (programmable logic controllers) in the control room or to a wireless mobile communications device. In critical life safety situations, the process can be totally automated.\nAnother option is to use gas sensors that provide an output suitable for direct interface to the BMS. In this case, the BMS can cut the gas supply, switch off any potential ignition sources and vent gas as needed.\nIncreasingly, new fixed-mount gas detector systems are being integrated into an indoor environmental quality (IEQ) system that also monitors heat, smoke, temperature, humidity, particulate filtration and other atmospheric variables. If any combination of these (within a zone) exceeds a pre-determined level, communication to the DDC (direct digital control) is instantaneous and the relays are triggered.\nHand-held portable gas leak detectors with infrared and electrochemical sensors provide diagnostic and troubleshooting features to supplement fixed instrumentation systems. A portable instrument with a PID (photoionization) sensor detects a broad range of volatile organic compounds (VOCs). Characteristically, VOCs evaporate at room temperature and may or may not be discernible to one’s sense of smell. Some VOCs may pose a combustible threat, and all can degrade IAQ.\nHand-held portable IAQ monitors are specifically designed to test IAQ conditions including temperature and humidity as well as the most common gas hazards found in buildings, usually carbon dioxide, carbon monoxide and natural gas.\n1 Boiler rooms: Natural gas is one of the most widely used fuels for heating commercial and industrial property. An undetected leak can present an explosive risk.\nPredominantly methane, natural gas is approximately half the weight of air and will rise toward the ceiling or roof space of a boiler room. Sensors should be located over potential leak areas, such as the gas burner assembly, gas train assembly, pressure boosters (if so fitted), gas shut-off valve, combustion air intake and gas meter. On a small gas boiler installation, a number of these points may be closer together, requiring a single point of detection.\nBecause most boiler rooms are visited infrequently, a leak may go undetected if gas sensors are not properly positioned. When positioning a sensor, consider mechanical ventilation and its likely effect on the path of leaking gas. Strategically placing gas detectors should be a collaborative effort involving an experienced gas detection system installer or engineer, facility manager and HVAC technician.\nThe prime objective for boiler room ventilation is to ensure an adequate supply of combustion air. For this reason, gas detection systems are widely used.\nThe process can help reduce the buildup of a flammable gas concentration; however, this cannot be guaranteed. Potential sources of ignition are many, and include any uncertified electrical equipment such as lighting, switches and control panels.\n2 Mechanical equipment rooms with refrigerants: Leaks from CFC, HCFC and HFC refrigerants such as R-11, R-12, R-22, R-123 or R-134a can be expensive and potentially dangerous. Mechanical equipment rooms that use these refrigerants now require stationary sensors for permanent, continuous monitoring. Refrigerant leaks can come from mechanical wear of system components, vibration, bad soldering, corrosion, operational problems and more. To spot test leaks, the technician should use portable hand-held equipment with a wand attachment.\nWhen boilers and chillers are located in the same room, a refrigerant detector is employed to automatically shut down the combustion process. Shutting down the refrigeration system at high concentration of refrigerant in the mechanical system is a good way to minimize refrigerant loss. But this can have a major impact on the HVAC system of the building. Analysis of the effects of such actions on the facility is a critical undertaking.\n3 Confined spaces: More than half of all confined spaces are located below the surface level of a building. Electrical vaults, sewer channels and tanks are some examples of confined spaces. In these environments, common gases include methane, carbon monoxide and hydrogen sulfide. Oxygen deficiency can also pose a threat. A multi-gas portable monitor is advisable, with a PID sensor for detecting a range of VOCs.\n4 Parking garages, offices and other areas: Vehicular exhaust products contain carbon monoxide and nitrogen dioxide. In parking garages, these can form gas clouds that migrate through HVAC ductwork to contaminate air inside the building envelope. Gas transmitters that use powerful internal pumps can be installed in ductwork to monitor air.\nNew technology uses mesh wireless communications to relay gas readings to a central controller connected to the DDC. Wireless systems require little conduit or electrical wiring, making it easy to retrofit a building or augment an existing hard-wire system. These wireless detectors are good at sniffing for toxic CO and NO2 in hard-to-reach areas. Wireless systems may become the preferred sensing network as battery life is extended; the industry is moving toward a 10-year “fit and forget” operating life.\nToday’s tight building envelopes require fast, reliable sensors to detect toxic and combustible gas hazards and other airborne contaminants. A good IAQ strategy begins with the right tools of detection, followed by a well-executed plan of protection.', 'What is a confined space?\nA confined space is any enclosed or partially enclosed structure that is intended or likely to be entered by any person, has limited or restricted entry or exit access and contains a potentially harmful atmosphere. Examples include tanks, pits, chimneys, silos, underground sewers, tunnels and wells.\nA confined space may contain a harmful atmosphere, dangerous vapours, flammable gases, reduced or unsafe oxygen levels or stored substances that might collapse and engulf a person.\nIn many instances, people killed in confined spaces die trying to rescue someone already overcome by a harmful atmosphere. Rescues should never be attempted without proper emergency management procedures and appropriate safety equipment such as air-supplied respiratory protective equipment.\nExamples of confined space hazards\nConfined spaces are particularly dangerous because the hazards may not be immediately apparent. The dangers of confined spaces can include:\n- biological hazards, such as bacteria\n- exhaust gases\n- flammable gases\n- low oxygen levels\n- risk of engulfment by stored substances such as wheat or sand\n- toxic gases.\nConfined spaces – assessing the risk\nPreventing injury and death in the workplace requires identifying confined spaces and assessing the risk of performing work inside them. WorkSafe Victoria Advisory service can offer information and advice.\nTo reduce the risks of working in confined spaces:\n- Whenever possible, eliminate the need for anyone to go inside the confined space – for example, use mechanical aids instead.\n- Find out what is in the confined space before planning entry.\n- Test the air in the confined space. This should be done if you don’t know for certain what’s been in there in the past. Do not assume it is safe.\n- Devise safety procedures for working inside the confined space and provide induction and training.\n- Make sure that any cleaning compounds or equipment used in the confined space are properly assessed.\n- Make sure anyone going into a confined space has appropriate protection and an entry permit.\n- Devise emergency and rescue procedures.\n- Rehearse emergency and rescue procedures regularly.\n- Thoroughly train the people who work inside the confined space.\nSignposts and permits for confined spaces\nAll confined spaces should be clearly signposted. Victorian safety regulations state that only employees with valid entry permits may go inside a confined space. Those employees should be thoroughly trained and be familiar with emergency procedures.\nImproving the safety of a confined space\nSuggestions on improving the safety of a confined space include:\n- Eliminate the need to enter the confined space, for example, by using mechanical aids to carry out maintenance tasks.\n- Ensure entrances and exits are big enough to allow free access of people wearing protective clothing and rescue equipment that may be bulky.\n- Make sure that any structures leading to the confined space, such as ladders and walkways, are safe.\n- Include sufficient entry and exit openings in long confined spaces, such as tunnels or pipelines.\n- Make sure the confined space is well lit and properly ventilated.\nVentilation and confined spaces\nTwo of the possible risks of confined spaces are low oxygen levels and harmful atmospheres (including vapours and flammable gases).\nIt is impossible to smell oxygen or some lethal gases so the human nose is an unreliable indicator of safe or harmful atmospheres. Instead, the air in confined spaces needs to be regularly tested for oxygen and contaminants using proper equipment.\nPurging or mechanical ventilation, such as fresh air blowers or extractors, should always be used, so far as is reasonably practicable.\nSafety suggestions include:\n- Make sure any equipment is suitable for use in a confined space, especially equipment being used in flammable areas, which should be non-sparking and flameproof.\n- Make sure the purging or ventilation system is powerful enough to be effective.\n- Operate the purging or ventilation system before anyone enters the confined space to make sure the atmosphere is safe.\n- Operate the purging or ventilation system continuously while anyone is inside the confined space.\n- Make sure the purging or ventilation line is close to the working face.\n- Vent exhaust from machinery straight out of the confined space.\n- When using vaporous chemicals such as solvents, refer to Material Safety Data Sheets (MSDS) for their safe use.\nSafety equipment and confined spaces\nIf a safe atmosphere can’t be guaranteed, appropriate safety equipment such as air-supplied respiratory protective equipment must be used.\nIf the confined space contains chemicals or dangerous substances, other protective equipment required may include gloves, goggles and overalls.\nHarnesses and winches, when attached to the person entering the confined space, are an excellent backup, but they must be properly tested and appropriate training in their use must be provided to those using them.\nObservation and communication when working in confined spaces\nMake sure the people inside the confined space are monitored from the outside. There should be a person trained in initiating emergency procedures observing those inside the confined space from a safe vantage point outside the space. Practical communication methods include:\n- mobile telephones\n- two-way radios\n- closed circuit television\n- hand signals\n- rope signals.\nEmergency procedures for confined spaces\nIt is vital to devise proper emergency procedures and to rehearse them often. Suggestions include:\n- Contact WorkSafe Victoria for advice and information.\n- Train employees in first aid.\n- Keep a suitable first aid kit in an accessible place.\n- Provide rescue equipment – such as lifelines, lifting equipment, stretchers, and air-supplied, escape-type or self-rescue respiratory protective equipment and train employees in how to use them.\n- Install emergency equipment, such as fire extinguishers.\n- Ensure easy access to medical treatment and emergency services.\nWhere to get help\n- In an emergency, always call 000 for an ambulance\n- Your doctor\n- WorkSafe Victoria Advisory Service Tel. (03) 9641 1444 or 1800 136 089 (toll free) – for general enquiries\n- WorkSafe Victoria Emergency Response Line Tel. 13 23 60 – for employers to report serious workplace emergencies such as fatalities, serious injuries or serious incidents, 24 hours, 7 days\nThis page has been produced in consultation with and approved by:\nContent on this website is provided for information purposes only. Information about a therapy, service, product or treatment does not in any way endorse or support such therapy, service, product or treatment and is not intended to replace advice from your doctor or other registered health professional. The information and materials contained on this website are not intended to constitute a comprehensive guide concerning all aspects of the therapy, product or treatment described on the website. All users are urged to always seek advice from a registered health care professional for diagnosis and answers to their medical questions and to ascertain whether the particular therapy, service, product or treatment described on the website is suitable in their circumstances. The State of Victoria and the Department of Health & Human Services shall not bear any liability for reliance by any user on the materials contained on this website.']	['<urn:uuid:9440855d-f49f-4078-afb9-5b56c32399f2>', '<urn:uuid:d7141338-3d14-4de3-9d5b-3015fca6eb43>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T17:51:21.175311	25	119	2193
30	How prevalent is constipation in autistic children and can special diets help treat it?	Studies show that 36% of autistic children have moderate/severe constipation, compared to 10% in control subjects, with 54.4% showing significant rectosigmoid loading. Regarding special diets, while some use casein-free (dairy-free) and gluten-free diets, there's little scientific evidence that these dietary restrictions improve autism symptoms. However, multivariate analysis showed that dairy protein consumption was a significant predictor of constipation in autistic children.	"['Objective. Recent evidence suggests that autistic children may have significant gastrointestinal symptoms. Although constipation occurs in 2% to 5% of healthy children, its clinical diagnosis is often difficult in children with behavioral disorders. We thus aimed to assess the prevalence of fecal loading in autistic children with gastrointestinal symptoms and to identify possible predictors of constipation.\nMethods. We studied abdominal radiographs of 103 autistic children (87 boys) who were referred for gastroenterological assessment, in comparison with 29 control radiographs from children who were referred to the emergency department, most with abdominal pain. Radiographs were scored independently, in blinded manner, by 4 pediatric gastroenterologists and a radiologist. The severity of constipation was determined using a validated index. Details of stool habit, abdominal pain, dietary history, and laxative use were obtained from case notes.\nResults. The incidence of constipation in the control subjects with abdominal pain was higher than reported for normal children. Despite this, moderate or severe constipation was more frequent in the autistic group than in the control subjects (36% vs 10%). Analysis of rectosigmoid loading showed more striking differences (54.4% of autistic children had moderate/severe loading or acquired megarectum compared with 24.1% of control subjects). Multivariate regression analysis showed consumption of milk to be the strongest predictor of constipation in the autistic group, whereas stool frequency, gluten consumption, soiling, and abdominal pain were not predictive of constipation.\nConclusions. Constipation is a frequent finding in children with gastrointestinal symptoms and autism, particularly in the rectosigmoid colon, often with acquired megarectum. The absence of any correlation between the clinical history and the degree of fecal impaction in autistic children confirms the importance of an abdominal radiograph in the assessment of their degree of constipation.\nPervasive developmental disorders are a group of disorders of which childhood autism remains the prototype.1 Typical features are qualitative impairments in social interaction and communication and a restricted repertoire of interests, activities, and behaviors, with onset in early childhood. In classical autism, abnormal or impaired development is obvious from before the age of 3 years. Autistic spectrum disorders range from classical autism to the high-functioning Asperger’s syndrome, in which language milestones are not delayed and cognitive skills may be above average.1,2 Hyperkinetic and attention-deficit disorders, obsessive-compulsive disorders, developmental disorders of motor function, and learning problems may coexist in children with autism.3\nGastrointestinal diseases are more common in children with a neurologic disability.4 In children with regressive autism, there have been recent reports of unexpected intestinal inflammation, with both low-grade colitis5,6 and duodenitis with reduced disaccharides.7 Immunohistochemical studies have suggested an immune response targeted at the gut epithelium, with a possible autoimmune cause.8,9 However, these findings have been largely restricted to those with a clear history of developmental regression, and the proportion of children within the overall autistic spectrum who have gastrointestinal problems remains unknown. An early report from Melmed et al10 of a large unselected population of autistic children, however, identified approximately half with gastroenterologic problems when formal assessment was made.\nThe combination of neurologic and developmental problems in children with autism makes toileting problems very common. In a questionnaire survey of 100 parents of autistic patients, among the most commonly reported problems were urinary incontinence and constipation.11 Our own assessment of autistic children who were referred to us with gastrointestinal symptoms suggested that constipation was a particularly common finding and that effective management of this frequently provided global benefit.12 Similar behavioral improvement was also noted by Horvath et al.7 This study thus aimed to define the prevalence of constipation in children with gastrointestinal symptoms and autism and to determine whether there are useful clinical predictors of the constipated autistic child.\nThis was a retrospective study of children who had autism and were referred to a tertiary pediatric gastroenterology service for assessment of their gastrointestinal problems. All children who were referred to us during the course of this study had previously received a formal diagnosis by developmental pediatricians within the autistic spectrum and remained under appropriate neurodevelopmental follow-up. These diagnoses were made by pediatricians using criteria from the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition classification.1 We reviewed all children who were younger than 18 years and referred with a diagnosis of an autistic spectrum disorder and gastrointestinal symptoms. Detailed histories were taken from all families, with particular emphasis on early development, dietary practices, recent gastrointestinal symptoms, and laxative use. All new referrals had a routine abdominal radiograph performed at presentation. All had received a diagnosis within the autistic spectrum by local developmental pediatricians or child psychiatrists. The median age of presentation was 5.9 years (range: 2.2–18.7).\nTo assess the discriminating power of the radiologic methodology, we used 29 consecutive, nonautistic, children who had required abdominal radiograph in the pediatric emergency department during the same time period as control subjects. The median age of presentation in this group was 8.5 years (M:F = 58.8%:41.2%). Of the total, 70.6% presented with abdominal pain, 17.6% had an radiograph to exclude suspected spinal injuries, 11.8% presented with foreign body ingestion, and 5.9% presented with vomiting.\nAbdominal radiographs were scored independently, in a blinded manner, by 2 pediatric gastroenterologists, 1 senior radiologist, and 2 senior trainees in pediatric gastroenterology. The severity of constipation was recorded using a previously reported system, in which a score of ≥9 of 15 was defined as significant constipation.13,14 Each abdominal radiograph was scored in 3 segments, with a maximum of 5 points for severe fecal loading with bowel dilation. In view of the likely variability, we decided to use the mean scores of all 5 observers to determine whether a child was defined as “constipated” (≥9 of 15) or “not constipated” (<9 of 15).\nClinical details that had been systematically recorded on all autistic children were extracted from the records. These included age at presentation, stool frequency, soiling, rectal bleeding, alternating diarrhea and constipation, abdominal pain, dietary history, and laxative use.\nAll statistical analyses were done using statistical package SPSS version 10.1. Analyses of significance were done using Wilcoxon rank sum test and χ2 tests where appropriate.\nClinical records and abdominal radiographs were available for 103 children with autistic spectrum referred between 1996 and 2001. Of these, 87 (84.5%) were boys and 16 were girls. Ninety-four cases were of core autism, and 9 had Asperger’s syndrome. Only 13.6% of all of the autistic children referred were on treatment for constipation, with only 1 in 3 of the “constipated” children being on any laxatives. On referral, 1 child, subsequently defined as “constipated,” was being treated with Loperamide for recurrent soiling. Representative radiographs are shown in Fig 1.\nAnalysis of radiograph scoring confirmed systematic variation in the scoring system, as previously reported,13 with 2 assessors scoring consistently lower for all patients. Using the defined criteria, a diagnosis of constipation on the basis of mean overall score ≥9 of 15 was made in 37 of 103 autistic children. In the control group, only 3 of 29 were received a diagnosis of constipation. Thus, a significant difference in the prevalence of constipation was identified between the control subjects and the autistic group (10% vs 36% respectively; P = .011). When loading was analyzed in the sigmoid and rectal segment (identified as a score of ≥3), the differences were even more striking. Here, 54.4% of autistic children showed significant fecal impaction compared with 24.1% of control subjects (P < .01).\nThe stool frequency of the autistic children was not suggestive of a high incidence of impaction (overall mean 17.8 bowel motions per week; range: 1–50, with radiologically constipated children having mean 14.9 motions per week compared with nonconstipated 19.5 per week; not significant). On direct questioning, however, many of the parents reported that their children demonstrated avoidance posturing, such as hunching forward on the toilet or defecating while standing. Soiling was present in 25 (43.1%) of 58 of the overall autistic children, with a higher incidence (14 of 24 [56%]) of the radiologically constipated population. However, in 43% of cases, the parents had not reported soiling on direct questioning. There were no significant differences in clinical details between the “constipated” and “nonconstipated” children with autism, although the former were receiving more laxatives at referral (P = .018). The frequency of symptoms are given in Table 1.\nMany different therapies have been used in the treatment of autism and its associated complications, with dietary exclusions being 1 of the more common practices.15–18 Of all of the autistic children in this study, 63 (61.2%) of 103 were on an exclusion diet (Table 1): 37 children were on combined dairy- and gluten-free diets, 16 were on dairy-free alone, and 10 were on a gluten-free diet alone. There were a very small number of children who were also excluding individual food items such as yeast, sugar, chocolate, egg, and soya. However, there was no significant difference in the use of exclusion diets between “constipated” and “nonconstipated” children (45.5% vs 46.2%, respectively). Univariate logistic regression found the use of laxatives at the time of referral to be the only significant predictor of the diagnosis of constipation (Table 2). However, a trend toward significance was found for both soiling and consumption of dairy products (P < .09). Multivariate regressional analysis showed that the intake of dairy protein but not of gluten was a significant predictor of constipation in the total population (P < .01).\nConstipation is a relatively common problem in children,19 with estimates of prevalence in otherwise normal children between 0.3% and 8%.20 Whereas anecdotal reports and parental questionnaires have suggested that constipation may be common in autistic children, this is the first study reporting a definitive assessment of fecal loading in children with autism using abdominal radiograph as a diagnostic tool.21 This study underlines the difficulty in making a clinical diagnosis of chronic constipation in any child. History alone may only allow a diagnosis of constipation to be made in approximately 50% of children.20 Making a diagnosis is even harder in children with autism. In our study, only one third of the referred children who finally received a diagnosis of being “constipated” were receiving any laxatives.\nClinical tools may be inadequate in making a diagnosis of constipation, and abdominal radiograph may be the only way to confirm the diagnosis in cases of clinical doubt.22 In view of the increased difficulties in clinical ascertainment of constipation in children with autism, we recommend the use of plain abdominal radiographs in initial assessment of possible gastrointestinal abnormality. In our study, soiling was the only symptom likely to be predictive (close to significance) in making a diagnosis of constipation.\nReporting and interpreting abdominal radiographs remains an art. We have confirmed the findings of Leech et al,13 that even when mutually agreed radiographic diagnostic criteria for constipation are used, there may be consistent differences in the observers’ reports. Despite interobserver variability, there was positive overlap in making a diagnosis in >70% of cases. Intraobserver variability seems to remain consistent,13 making abdominal radiographs the most effective means of diagnosing constipation in autistic children.\nConstipation is present more often in children with neurodevelopmental disabilities. This may be because of the inherent problems encountered in toilet training. The same holds true for autistic children. However, the findings of colonic and duodenal immunopathology in children with regressive autism8,9 may suggest a more specific disturbance of gastrointestinal motility. Recent murine evidence suggests a direct link between mucosal immunopathology and antigen-induced dysmotility,23–25 with products of TH2 cells and eosinophils directly affecting neural cell function within the gut. In humans, cow-milk protein has been particularly implicated in “allergic dysmotility,” with a pattern of constipation characterized by distal impaction as a result of antigen-induced external sphincter spasm.26,27 It thus is notable that cow-milk consumption was the only significant predictor of constipation on multivariate analysis. Whether the dominant cause of this distal constipation is indeed immune mediated and antigen driven or potentially caused by poor handling of dietary morphines,5 it is notable that behavioral improvement may occur if it is detected and effectively treated.12 Our own practice in this regard is to soften initially with liquid paraffin before disimpaction with stimulant laxatives,8,12 as suggested for nonautistic children with similar impaction.28\nConstipation is a frequent and significant problem in children with autism and continues to remain underrecognized. It is particularly marked within the rectosigmoid colon, often in association with acquired megarectum. The absence of any correlation between the clinical history and the degree of fecal impaction in autistic children confirms the importance of an abdominal radiograph in the assessment of the degree of constipation. Effective treatment may potentially alleviate behavioral problems and disturbed sleep pattern in autistic children.\nWe thank Dr Richard Morris, Senior Lecturer in Medical Statistics, University College of London, for help in analyzing the statistics and Dr Andrew Anthony for help with patient identification.\n- ↵American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders (DSM-IV). Washington, DC: American Psychiatric Association; 1994\n- ↵World Health Organization. Mental Disorders: A Glossary and Guide to Their Classification in Accordance With the 10th Revision of the International Classification of Diseases: Research Diagnostic Criteria (ICD-10). Geneva, Switzerland: World Health Organization; 1993\n- ↵Melmed RD, Schneider CK, Fabes RA, Philips J, Reichelt K. Metabolic markers and gastrointestinal symptoms in children with autism and related disorders [abstract]. J Pediatr. Gastroenterol Nutr.2000;31(suppl 2) :S31– S32\n- ↵Murch S, Thomson M, Walker-Smith J. Autism, inflammatory bowel disease, and MMR vaccine. Lancet.1998;351 :908\n- ↵Baker SS, Liptak GS, Colletti RB, et al. Constipation in infants and children: evaluation and treatment. A medical position statement of the North American Society for Pediatric Gastroenterology and Nutrition. J Pediatr Gastroenterol Nutr.1999;29 :612– 626 (published erratum appears in J Pediatr Gastroenterol Nutr. 2000;30:109)\n- ↵Afzal NA. Thomson M. Management of Constipation in Children. Prescribers’ Journal.2001;12 :101– 107\n- ↵Loening-Baucke V. Constipation in early childhood: patient characteristics, treatment, and longterm follow up. Gut.1993;34 :1400– 1404\n- ↵Keuzenkamp-Jansen CW, Fijnvandraat CJ, Kneepkens CM, Douwes AC. Diagnostic dilemmas and results of treatment for chronic constipation. Arch Dis Child.1996;75 :36– 41\n- ↵Collins SM, Piche T, Rampal P. The putative role of inflammation in the irritable bowel syndrome. Gut.2001;49 :743– 745\n- ↵Hogan SP, Mishra A, Brandt EB, Foster PS, Rothenberg ME. A critical role for eotaxin in experimental oral antigen-induced eosinophilic gastrointestinal allergy. Proc Natl Acad Sci U S A.2000;97 :6681– 6686\n- ↵Sharif F, Crushell E, O’Driscoll K, Bourke B. Liquid paraffin: a reappraisal of its role in the treatment of constipation. Arch Dis Child.2001;85 :121– 124\n- Copyright © 2003 by the American Academy of Pediatrics', 'A single copy of this article may be reprinted for personal, noncommercial use only.\nAutism treatment: Can special diets help?By Mayo Clinic staff\nOriginal Article: http://www.mayoclinic.com/health/autism-treatment/AN01519\n- With Mayo Clinic emeritus consultant\nJay L. Hoecker, M.D.read biographyclose window\nJay L. Hoecker, M.D.Jay Hoecker, M.D.\nDr. Jay Hoecker, an emeritus member of the Department of Pediatric and Adolescent Medicine, brings valuable expertise to health information content on primary care pediatrics. He has a particular interest in infectious diseases of children.\nHe\'s a Fort Worth, Texas, native, certified as a pediatrician by the American Board of Pediatrics and a fellow of the American Academy of Pediatrics. He was trained at Washington University\'s St. Louis Children\'s Hospital, and in infectious diseases at MD Anderson Cancer Center in Houston. He has been with Mayo Clinic since 1989.\n""The World Wide Web is revolutionizing the availability and distribution of information, including health information about children and families,"" Dr. Hoecker says. ""The evolution of the Web has included greater safety, privacy and accuracy over time, making the quality and access to children\'s health information immediate, practical and useful. I am happy to be a part of this service to patients from a trusted name in medicine, to use and foster all the good the Web has to offer children and their families.""\nRisk factors (1)\n- Paternal age: How does it affect a baby?\nTreatments and drugs (2)\n- Autism treatment: Can special diets help?\n- Autism treatment: Can chelation therapy help?\nAutism treatment: Can special diets help?\nCan special diets help children who have autism?\nfrom Jay L. Hoecker, M.D.\nThere\'s no evidence that special diets are an effective autism treatment.\nSome people believe that diets that are free of a protein found in dairy products (casein) and a protein found in many grains (gluten) affect the brain development and behavior that causes autism in children. However, there\'s little evidence that diet triggers autism or that restricting gluten and casein improves autism symptoms. And for growing children, restrictive diets can lead to nutritional deficiencies. Research is ongoing on the effectiveness of casein- and gluten-free diets.\nAutism is a complex brain disorder that has no known cure. For this reason, many frustrated parents turn to unproven alternative treatments — such as restrictive diets that eliminate gluten and casein — in an attempt to help their children.\nIf you\'re considering an alternative autism treatment, talk to your child\'s doctor. He or she can help you identify the treatments that are most likely to be effective for your child, as well as local resources that may provide additional support. If you decide to pursue a restrictive diet, work with a registered dietitian to create an appropriate meal plan for your child.Next question\nAutism treatment: Can chelation therapy help?\n- Marcason W. What is the current status of research concerning use of a gluten-free, casein-free diet for children diagnosed with autism? Journal of the American Dietetic Association. 2009;109:572.\n- Whiteley P, et al. The ScanBrit randomised, controlled, single-blind study of a gluten- and casein-free dietary intervention for children with autism spectrum disorders. Nutritional Neuroscience. 2010;13:87.\n- Millward C, et al. Gluten- and casein-free diets for autistic spectrum disorder. Cochrane Database of Systematic Reviews. 2008;2:CD003498.\n- Peregrin T. Registered dietitians\' insights in treating autistic children. Journal of the American Dietetic Association. 2007;107:727.']"	['<urn:uuid:2eaef73a-0e48-4e2c-8165-0e7d7d231ffd>', '<urn:uuid:570652b6-9821-4d96-9752-ad0358db7f64>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	14	61	2959
31	What are the benefits and threats of wildlife tourism to ecosystems?	Wildlife tourism can preserve ecosystems by providing financial incentives for local people to protect wildlife instead of exploiting it, especially in places like rural Africa, South America, and Asia. However, it can also threaten ecosystems through harassment of wildlife, disruption of feeding and breeding, habitat damage from overuse, and recreational impacts like coral breakage from SCUBA diving, boat anchoring, and human contact.	"['ECOTOURISM AND WILDLIFE TRAVEL\nNature Travel: Wild Animal Ecotourism Adventures Around America And The World\nSCROLL DOWN FOR DESTINATION SUGGESTIONS!\nWildife ecotourism can be local or exotic, and as complicated as an organized African safari, or as simple as driving to a nearby town to take a walk in the woods. Environmental tourism can also add wear and tear to already fragile ecosystems—or it can play a vital role in preserving those same ecosystems, depending on how it’s done.\nMany types of wildlife tourism fit beneath the eco-travel umbrella. International birding, or bird-watching, fits as easily into the nature and wildlife-travel category as do snorkeling and scuba diving in Caribbean or other tropical waters, traveling by boat through either the Everglades or the Amazon rain forest, viewing whales and fish or swimming with dolphins at a marine park, or visiting Yellowstone National park to see buffalo, elk, bears and wolves.\nWhen done properly, environmental tourism, or nature and wildlife travel, can provide the traveler with a “green” vacation that is healthy for both the environment and the ecotourist or green traveler him- or herself. For instance, the people in or near many wildlife parks or other wildlife-rich areas around the world depend on the money that ecotourists bring when they come to view the wild birds, fish and animals. This money provides a powerful incentive for local peoples in such places as rural Africa, South America and Asia to preserve, rather than to hunt, trap or otherwise exploit the wild animals in the nearby rainforest, coral reef, mountain forest, river, or other natural ecosystem.\nOn the other hand, environmental and nature tourism, when done improperly, can result in the harassment of wildlife, including the disruption of their feeding and breeding, damage or destruction to wildlife habitat due to overuse and other abuse—and even loss of support for wildlife from local people offended at the way they’ve been treated, either by their government or by the ecotourism industry itself.\nIt is therefore the responsibility of every ecotourist, wildlife and nature tourism traveler or vacationer, as well as each environmental or adventure traveler or tourist, to carefully research their trip before they commit to taking it. Make sure that it truly is a “green” trip—that the wildlife travel or ecotourism operator is a reputable person or organization with a true concern for the environment and for the wildlife that you plan on visiting.\nBelow you’ll find links to some short articles on ecotourism and wildlife and nature-related travel trips that we at AllAboutWildlife.com either have knowledge of ourselves, or would be interested in taking. Again, please carefully research any wildlife trip that you plan on taking, both for your own safety and financial well-being, as well as for the safety and well-being of the people, natural areas and creatures that you hope to visit.\nEast Africa: Classic safari-style wildlife tourism on the Serengeti plains.\nSouthern France: Viewing long-extinct wildlife through the eyes of the earliest Europeans.\nSouth America: Viewing wildlife in Brazil’s vast Pantanal grassland.\nNorth America: Into the more remote corners of Yellowstone National Park by horseback and mule pack.\nNorth America: Some high-end ecotourism in Alaska.\nNorth America: A fishing trip to the Appalachian Mountain Club’s Little Lyford Pond Camps in Northern Maine.\nAustralia: The Great Barrier Reef may well be the world’s finest aquatic ecotourism destination.', ""Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific.""]"	['<urn:uuid:ff63d4c7-bfd9-4cf8-a7c0-b3ad8d58bae2>', '<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	11	62	1712
32	what factors reduce cryptomnesia occurrence according research marsh	According to Marsh's research, cryptomnesia is greatly reduced with subtle social pressure - people are less likely to unconsciously steal ideas when quizzed about their contributions in front of the original group rather than in private. Cryptomnesia also occurs less frequently between strangers, particularly when the potential source of plagiarized material is present.	"['The charge of plagiarism carries a special sort of shame. Take the case of Kaavya Viswanathan, the young writer whose 2006 debut novel, How Opal Mehta Got Kissed, Got Wild, and Got a Life, contained so many passages lifted from other books that her writing career was over by her junior year at Harvard. For those whose literary taste is at the opposite end of the spectrum from chick lit, consider Dante: he put the fraudulent in an even deeper circle of hell than the violent.\nBut could some alleged plagiarists—like Maureen Dowd, Chris Anderson, Elizabeth Hasselbeck, and even Viswanathan, who all either deny the charge, or blame their copying on unconscious mistakes—be guilty of psychological sloppiness rather than fraud? Could the real offense be disregard for the mind\'s subliminal kleptomania? And if it is real, is unconscious copying (or ""cryptomnesia"" to those who study the phenomenon) preventable? Or, seeing as Nietzsche ripped off a passage of Thus Spoke Zarathustra from something he\'d read as a child, and former Beatle George Harrison was found guilty, in court, of unconsciously copying the music for his hit song, ""My Sweet Lord""—is cryptomnesia both unavoidable, and the perfect excuse?\n""Clearly all of us, referring to journalists, probably appropriate phrases or ideas, on occasion, without realizing it,"" said Howard Schneider, dean of the School of Journalism at New York\'s Stony Brook University, and former Newsday editor. But intent and degree count, he said, and journalists should be held to a particularly high standard when it comes to plagiarism. Schneider, who helped set up the News Literacy Center at Stony Brook, teaches his students about the brain\'s susceptibility to certain psychological pitfalls, such as: seeing a political commercial between TV newscasts, and then a week later attributing the information in the commercial to the newscast itself. He believes that journalists are susceptible to similar influences.\nAccording to Richard L. Marsh, a professor of cognitive psychology at the University of Georgia and a leading cryptomnesia researcher, Schneider is on the right track. ""When people engage in creative activity, they are so involved in generating or coming up with something new or novel that they fail to protect against what they previously experienced,"" said Marsh. Over the last 20 years, Marsh has designed numerous models for studying cryptomnesia in the lab. An early study involved asking subjects to work with an unseen ""partner"" (actually a computer) to find unique words in a square array of letters, similar to the game Boggle. A short while after completing this task, the researchers asked each participant to recall the words they had personally found, and to generate new words neither the participant nor the participant\'s partner had previously been able to find.\nThe subjects plagiarized their partners roughly 32 percent of the time when trying to recall their own words, and up to 28 percent of the time when attempting to find previously unidentified words in the puzzle. Not only was plagiarism rampant, many subjects who plagiarized also checked a box indicating they were ""positive"" their answers had not previously been given by their partners.\nHenry Roediger, a memory expert at Washington University in St. Louis, said that cryptomnesia is partially caused by the lopsidedness of our memories: it\'s easier to remember information than it is to remember its source. Under the right conditions, this quirk can even evoke false memories. In one study, the more times Roediger instructed participants to imagine performing a basic action (like, ""sharpen the pencil"") the more likely the participants were to recall—incorrectly—having actually performed the action when asked about it later.\nBut misattributing memories from one source to another, whether from imagination to reality or from a friend to oneself, is only one of the psychological quirks behind unconscious plagiarism. Another is implicit memory, which Dan Schacter, a psychologist at Harvard, called, ""the fact that we can sometimes remember information without knowing that we\'re remembering it.""\nThe classic demonstration of implicit memory involves a psychological technique known as priming. When a person is exposed to a list of words (or ""primed"") in one setting, than later asked to come up with words from a specific category, say ""types of fruit,"" in another setting, the person is more likely to name fruit that had appeared during the priming session than fruit that hadn\'t.\nThis result may not seem all that exciting, except that it also occurs with amnesiacs, who are unable to form conscious memories of the actual priming session. At the most basic level, says Schacter, this suggests that implicit memories are formed in different regions or systems of the brain than conscious memories. This disconnect, coupled with errors in remembering the source of ideas, words, or even whole phrases, may be responsible for cryptomnesia. ""Unconscious plagiarism makes it sound like a pretty exceptional and unusual circumstance,"" said Roediger. ""But I really think that at a very simple level, these things are happening all the time. You know, your friend uses some expression and you pick it up and use it too.""\nWhile unconscious plagiarism is embarrassing in cases where original creative output is expected, in most aspects of daily life it ranges from useful to indispensible. What is called cryptomnesia in one context is known as social learning theory in another. For example, children learn how to behave by unconsciously copying others, and friends strengthen their relationships when they assimilate each other\'s phrases, behaviors, and opinions.\nBut before we give high-profile cryptomnesiacs a free pass, as if they were suffering from an intractable psychological disorder, there\'s a bit more to know. Cryptomnesia happens more frequently between those who trust one another, such as people in romantic relationships or close friendships, but less frequently between strangers—particularly when the one whose ideas or words might be plagiarized is present. And due to our innate skepticism, unconsciously copying a person one doesn\'t know, or a source one doesn\'t yet trust, is uncommon.\nWe may plagiarize without knowing it, but we can guard against the risk with a little conscious effort. Taking diligent notes, reminding oneself to remember not just a good idea, but also its source, or simply pondering whether the clever phrase that popped into one\'s head is original, helps fend off cryptomnesia. Over the course of his research, Marsh has found that cryptomnesia is greatly reduced with subtle social pressure: if you are asked to come up with solutions to a problem in a group setting, and then quizzed on your contributions to the discussion afterward, you might unconsciously steal from fellow group members if the quiz takes place in private—but not if it takes place in front of the original group.\nUnconscious plagiarism does exist, but writers who don\'t take proactive steps to avoid it are often either being lazy, or they have a diminished fear of being caught. Driving is a good model: it is easy enough to drift over the speed limit without being aware of it, but vigilant drivers can prevent the habit by forcing themselves to pay conscious attention to the problem. And just as not knowing one\'s speed won\'t save one from a ticket, the fact that unconscious plagiarism isn\'t outright fraud doesn\'t make ""It was cryptomnesia!"" much of an excuse. Unconscious plagiarism may not be a ""felony,"" said Schneider, but it\'s still a journalistic ""misdemeanor.""']"	['<urn:uuid:81c8f7d2-9d6b-4097-84a4-f9095fb67452>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	8	53	1218
33	africa farming tech users benefits barriers	Digital farming services in Africa offer significant benefits, with platforms like DigiFarm helping farmers increase yields (from 6 to 20 bags of maize per acre), access loans, and receive weather updates. However, major barriers exist as 3G networks only cover 40% of rural areas and half of sub-Saharan Africa lacks grid electricity, limiting widespread adoption of these agricultural technologies.	"['Inarguably, agriculture is the mainstay in most African economies, contributing an average of 30 to 60 per cent of GDP; 30 per cent of the value of exports and providing employment, to about two-thirds of the continent’s working population.\nAccording to McKinsey and Company, more than 60 per cent of the population in Sub-Saharan Africa are smallholder farmers. In cognizance, of the pivotal role played by agriculture in the continent, the African Union (AU) declared 2014 as the Year of Agriculture and Food Security; stimulating leaders to treat agriculture not only as a viable solution to eradicate poverty and hunger but also as a major contributor to economic development deserving of massive investments.\nHowever, in stark contrast, the current drought pertinently in the Horn of Africa only highlights the peak of the continent’s food insecurity, as at least 18.4 million people are facing high levels of acute food insecurity and rising malnutrition across Ethiopia, Kenya and Somalia. This figure could increase to 20 million by September, according to UNICEF.\nIn addition, 7.1 million people are now acutely food insecure in Somalia, 7.2 million in Ethiopia and some 4.1 million in Kenya, are severely food insecure due to the drought. An estimated 7 million livestock, which pastoralist families rely upon for their livelihoods, have died across the region. The surge in the price of fertilizers caused by the Russia-Ukraine war has further exacerbated the situation.\nCould investments in science and technology be the answer to Africa’s food insecurity? Inarguably, significant efforts have been made but remain a far cry from full realization. This was a major topic of discussion during the recent Second Africa-Wide Science, Technology and Innovation High-Level Conference organized by AATF in collaboration with the African Union Development Agency-NEPAD (AUDA-NEPAD) and the Government of Rwanda. It was established that investing in science and technology, is critical for Africa’s agricultural growth. African governments and regional economic blocs were called upon to increase funding for Science, Technology and Innovation (STI), and create a conducive environment for technology adoption in agriculture.\nThe AU Science, Technology and Innovation Strategy for Africa 2024 (STISA-2024); place science, technology and innovation at the epicentre of Africa’s socio-economic development and growth. It was developed during the formulation of the broader and long-term AU Agenda 2063; and is the first of the ten-year incremental phasing strategies to respond to the demand for science, technology and innovation; to impact critical sectors such as agriculture, energy, environment, health, infrastructure development, mining, security and water. The Strategy was anchored on six distinct priority areas that contribute to the achievement of AU’s vision. Chief among them is the eradication of hunger and achieving food security, backed up by the reinforcing pillar of promoting an enabling environment for STI development in the African continent.\nThe Executive Director of AATF, Dr. Canisius Kanangire, noted that the agriculture sector is rapidly expanding in Africa; to embrace a wide range of concepts, to meet the increase in demand for food and nutrition security. At the core of this process, he noted, is the need to integrate and explore STI, to drive agricultural productivity for prosperity among farmers in Africa.\n“Creating an enabling environment for technology and innovation is an essential requirement in enabling African countries to address constraints that impede agricultural, environment and health development. Investing in new farming technology for Africa, from better seeds to digital tools to machinery, is the best opportunity for transforming African agriculture into an engine of economic growth that will have benefits far beyond the farm sector,” stated Dr. Kanangire.\nFurthermore, he noted that most Africans are smallholder farmers, and agriculture is their best opportunity to secure a more promising future.\nHowever, he added that millions remain impoverished because unlike farmers in other parts of the world, they lack consistent access to targeted, affordable agriculture innovations that would allow them to prosper. In reiteration, the acting Director of Knowledge Management and Programme Evaluation at AUDA-NEPAD, Martin Bwalya, emphasized that there is a need to adopt sustainable agriculture technologies, which cover the entire spectrum of farming systems.\nThe Rwandese Minister of Agriculture and Animal Resources, Geraldine Mukeshimana noted that it was a paradox that Africa is not a food powerhouse, yet about 60 per cent of arable land is found in Africa. According to her, this has been caused by the insufficient use of technological innovations to increase productivity, and eliminate post-harvest losses whilst adding value to agricultural produce.\nShe further highlighted that scaling up STI in agriculture, is key in building sustainable and resilient food systems; calling for the effective implementation of STISA-2024.\nOne of the key development priorities for the African Development Bank (AfDB) included in the ‘High 5s Agenda’ is ‘Feed Africa’, well aligned to the UN SDGs. In light of this, the Bank has recently approved a US$1.5 billion Africa Emergency Food Production Facility; set to benefit 20 million farmers in the continent to access certified seed, fertilizer and technology necessary to improve yield.\nThis initiative targets to support smallholder farmers and was in response to avert the looming food crisis; caused by the disruption in food supplies emanating from the Russia-Ukraine war, thereby filling the food shortfall. The crisis has created a shortage by at least 30 million metric tons of food especially wheat, maize and soybeans imported from both countries. The bank noted that the facility was designed to boost the local production of cereals and oil grains; as the most effective and efficient way to build resilience across Africa’s food systems.\nAmong the key activities of AEFPF is to connect farmers through e-wallet systems, i.e., digitizing the procurement of agro-inputs and simultaneously allow reach to farmers in a transparent manner, which will truly revolutionize the transformation of agriculture,” says AfDB’s Solomon Quaynor, Vice President for Private Sector, Infrastructure and Industrialization.\nTo boot Microsoft has recently reaffirmed its commitment to working with both the private and public sectors, to accelerate digital transformation in African agriculture, with a goal to increase food security in the continent. During a virtual roundtable panel discussion, Microsoft Kenya Acting Country Manager Kunle Awosika, noted that the company is committed to deepening investments in AgriTech on the continent; to enable data-driven, precise and connected farming that optimizes yields, boosts farm productivity and increases profitability.\nIn reiteration, Thule Lenneiye, ATO Coordinator, Ministry of Agriculture, Livestock, Fisheries and Co-operatives highlighted that “Through partnerships such as the one with Microsoft, we can offer our smallholder farmers valuable services, that help them modernize and digitize age-old farming practices, increasing productivity and boosting food security for our communities across the country.”\nAgritech changes the face of agriculture largely by democratizing information. To drive digitalization in agriculture, Microsoft in partnership with the Ministry of Agriculture is investing in the Kenya National Agriculture Platform. In light of this, a developer team based in the country has developed an Agri chatbot, which provides extension and advisory services to smallholder farmers, utilizing either smartphones or feature phones, via WhatsApp, Telegram and SMS. Currently, the services on Agribot are proving invaluable to over 400,000 farmers on the platform, providing them with access to all the relevant information, from the Ministry and the private sector.\nThe leading technology company additionally announced that it’s extending its partnership with the Alliance for Green Revolution in Africa (AGRA). AGRA was founded in 2006 by the Rockefeller Foundation and the Bill and Melinda Gates Foundation, based on a shared vision that investing in agriculture technology; is the most viable path to reducing poverty and hunger in Africa. This came to be known as the first ‘green revolution’ which is said to have bypassed Africa.\nThe two first partnered in 2019 in the co-creation of the AgriBot, as a digital solution for localized extension and advisory services for smallholder farmers. However, the new phase of the relationship targets to promote digital innovation and technology, as an enabler to connect agriculture ecosystems, sustainably integrating stakeholders in the service of strategic value chains. In reiteration, AGRA Kenya country manager John Macharia stated,” At AGRA, we realized early on that digital innovation, is critical in advancing food security and poverty eradication in Africa.\nOur partnership with Microsoft will directly support governments, SMEs and farmers by bringing the digital tools needed to build resilient food systems.” With a projected value of USUS$ 1 trillion by 2030, Africa’s agriculture sector is set for exponential growth in the coming decade. Last month, the Canadian government announced a US$78 million contribution to a funding kitty by the AfDB, to aid local food producers to improve their yield. The money is scheduled to be disbursed as concessional loans, targeting SMEs involved in agribusiness especially those founded by women.\nCould GMOs Redefine Africa’s Agriculture?\nThe myths surrounding Genetically Modified (GM) Foods in Africa are steadily being demystified, as food insecurity continues to bite. Could GMOs be the solution to curbing this perennial crisis? In other parts of the world, GMOs continue to shape the future of agricultural production, but in Africa, this has been slow due to fears around potential health risks such as cancer upon continued intake. According to research, there is no evidence that GM foods have any negative effects. Several countries in the continent have adopted GM crop farming and the number is growing exponentially, as climate change continues to affect agricultural production due to erratic weather patterns.\nGMOs are organisms whose genetic material has been altered using genetic engineering techniques; often plants that have been modified to achieve certain traits, such as drought tolerance, and pest-resistant utilizing recombinant DNA techniques or genetic engineering. According to the World Health Organization (WHO), GM foods currently available on the international market have passed safety assessments, and do not present risks to human health. Some of the advantages that African farmers can reap from GM crop farming include reduced production costs, increased yields and higher income.\nGM crop cultivation has a plethora of benefits to farmers, for instance, those who grow commodity crops such as maize or soy, do less tilling which reduces topsoil erosion or runoff fertilizer. Smallholder farmers in the continent have been reporting higher revenues from GM crops, due to better yields and less expenditure on inputs such as fertilizers or pesticides. Some African countries such as Kenya, Nigeria, South Africa, Ethiopia, Eswatini and Sudan; are producing GM crops such as drought-tolerant maize, herbicide-tolerant rice, and insect-resistant cowpea among many others.\nEvery country exercises sovereign control over its GMOs, managing its own GMO seed production and distribution, primarily using local seed companies. Farmers in these countries can only access seeds, which have been approved for commercial use by their governments. Currently, crops such as vitamin-fortified cassava, virus-resistant sweet potatoes, and potatoes resistant to late blight disease among others; are undergoing research and development in Kenya, Nigeria, Malawi, Burkina Faso, Rwanda, Ghana, Ethiopia, Cameroon, Uganda, Mozambique and Tanzania.\nAgriTech Startups Revolutionizing Africa’s Agriculture\nAgriculture Technology (Agritech) companies have been swiftly mushrooming in the continent, to seal the productivity deficit, thereby aiding in alleviating food insecurity in the continent. These startups deploy world-class technologies such as automated irrigation, soil sensor drones among other cutting–edge technologies; including digital systems to assist farmers to acquire knowledge, financing, accessing markets, insurance or inputs. Funding for AgriTech startups has been rapidly growing; the average amount of capital raised by AgriTech startups in Africa was US$3.7 M in 2020, US$2.8M in 2019 and US$1.8M in 2018.\nSome startups that have put Africa’s AgriTech scene on the map include South Africa’s Aerobotic and AgriProtein; Ghana’s AgroCenta; Egypt’s Agrimatic; Nigeria’s Farmcrowdy; Zambia’s AgriPredict; and Kenya’s Hello Tractor and Twiga Foods.\nWith perspective to the technology deficit in Africa’s agricultural sector, MasterCard and Ecobank group, have recently partnered to connect millions of smallholder farmers in Sub-Saharan Africa. MasterCard’s Farm Pass is a digital platform that makes it safer and easier, for farmers to sell their produce at a fair price. Smallholder farmers will be able to sell their produce affordably, access quality inputs and farming information, make digital payments, and develop a financial profile that unlocks financing opportunities for inputs and working capital.\nThe platform provides one agricultural marketplace, bringing together all relevant stakeholders. The platform is set to address challenges such as limited access to markets, securing quality inputs, and access to relevant financial tools to enable farmers to run sustainable businesses.\nFarm Pass offers a digital platform that makes it easier for smallholder farmers, to move from subsistence to commercial farming; thereby addressing food insecurity in the continent, increasing competitiveness and stimulating agricultural growth. Since its inception in 2015, the platform has reached one million smallholder farmers in Tanzania, Kenya, Uganda and India; enabling them to command 25 to 50 per cent higher prices and increase harvest productivity.\nThe platform is in line with Mastercard’s vision to connect one billion people and 50 million SMEs, to the digital economy by 2025. Integrating their businesses with payment systems, Farm Pass enables smallholder farmers to build a digital transaction record; that can facilitate formal credit or other financial services from banks and other financial institutions.\nMuch more remains unexplored in this field, for Africa to achieve food security and remain competitive, as it ushers in the fourth industrial revolution.', 'Digital services are helping small-scale farmers weather financial losses and adapt to climate shifts - but experts say the tech migration is leaving many behind\n* Africa\'s digital farming services growing nearly 45% per year\n* Sub-Saharan Africa has more than 400 farming apps and platforms\n* Farmers use apps to buy goods, access loans, reach customers\nBy Wesley Langat\nKOIYET VILLAGE, Kenya, June 30 (Thomson Reuters Foundation) - Over the past year, Wilson Lang\'at has made a steady profit from his farm in Koiyet village, managing to get the seeds and fertiliser he needs, planning ahead for droughts and floods, and borrowing money to diversify his crops - all through his phone.\nHis experience is not the norm, as most farmers across Kenya have struggled with the double hit of extreme weather shrinking their yields and coronavirus lockdowns and curfews choking off access to supplies and demand for their produce.\nUntil two years ago, the 46-year-old father of six traded cows while growing maize on the side at his 6-acre (2.5-hectare) farm in Kenya\'s Rift Valley. Then he started using the DigiFarm app to access farming inputs, weather reports and financing.\nLang\'at said he went from producing six 90-kg (200-pound) bags of maize per acre to about 20 bags per acre, as well as beans, potatoes and tomatoes - and quickly earned enough that he could drop the bovine arm of his business.\n""Since the start of the COVID-19 pandemic, I have never stopped farming. Farming has become more profitable (than cows), and now it\'s my full-time job,"" he said.\nIn Africa, experts say agricultural technology - or agri-tech - like DigiFarm is increasingly playing a crucial role in cushioning small-scale farmers against the crop-damaging effects of climate change and the economic pain of the pandemic.\nAccording to government figures, there are about 4.5 million small-scale farmers in Kenya and their collective output accounts for more than 60% of the country\'s food.\nDigiFarm, a free service from telecoms giant Safaricom, was launched in 2017 - but the bulk of its current 1.4 million users signed up after the pandemic hit, said Elizabeth Mudogo, customer experience lead at DigiFarm.\nDaniel Langat, a DigiFarm village advisor in Bomet County, where Koiyet is located, said the app saves farmers time and money.\nAfter ordering through the app, farmers get SMS notifications when their goods are ready for collection at a local depot - the company has nearly 30 in Bomet alone - and they can get store credit and discounts, Langat said.\nDigiFarm also helps farmers prepare for more erratic weather patterns by sending them information on best practices and tips to reduce crop loss, along with historical weather reports and forecasts.\n""This app has brought farmers together in our area,"" Langat said. ""As all the farmers plant at the same time since they get weather updates at the same time, this helps in disease control and decision-making.""\nIn case farmers do still suffer unexpected crop losses, DigiFarm offers insurance and loans through popular mobile money platform M-Pesa.\nDuring the pandemic, Lang\'at, the farmer, borrowed 1,300 Kenyan shillings ($12) through the app to buy tomato seeds.\nHe earned more than 50 times that selling the produce, which allowed him to pay one of his children\'s college fees.\n""Planning is easier. (For example) when I receive updates of low rainfall, I plant beans, which do well with shorter rains,"" he said.\nTURNING TO TECH\nAn April report from the Netherlands-based Technical Centre for Agricultural and Rural Cooperation said Africa\'s digital agricultural services sector has grown nearly 45% annually in terms of farmers reached over the last three years.\nSince the start of the coronavirus pandemic, Kenyan company Twiga Foods has seen a jump in the number of farmers using its digital platform Soko Yetu (Our Market) to reach customers without going through expensive brokers, said chief technology officer Caine Wanjau.\nTwiga, which launched in 2014, harvests and buys produce from its member farms, then takes the goods to collection centres for processing and packaging, he explained.\nWhen vendors order something through the app, the company delivers it directly to their stores.\nOriginally operating only in Nairobi, the platform expanded to five other major towns in Kenya during the pandemic and recently announced a new expansion into Uganda, Wanjau said.\nThe app now covers more than 17,000 farmers and 35,000 vendors, he noted.\nAnn Waithera signed up to Twiga a year ago to help her keep running her general store on the outskirts of Nairobi.\nInstead of going to markets to buy goods, she places her order from home and it is delivered the following morning.\n""Nowadays, the market is on my phone. I save over 500 shillings (every day) which would have been used for transport,"" she said. ""And I no longer wake up at 4 a.m. to go to the market in Nairobi, so now I can be with my kids.""\nA January report by consultancy McKinsey & Company said farmers across sub-Saharan Africa are using more than 400 apps and digital platforms for everything from accessing financial services and planting advice to supply-chain management.\nBut, it noted, uptake of these apps is limited by tech challenges such as lack of internet access for farmers, with 3G networks covering only about 40% of sub-Saharan Africa\'s rural areas, while half of the region has no grid electricity.\nStill, David J. Spielman, senior researcher at the Washington-based International Food Policy Research Institute, said agri-tech apps are already helping build the climate resilience of small-scale farmers.\n""Crop and livestock yields and production are edging upwards in Africa – the larger issue is that this rate of growth has to increase,"" he said by email.\nTo do that, he urged developers to focus more on tools for poorer and other marginalised communities, so that platforms can address their specific needs.\nFor example, some technologies provide weather information when the real constraint facing farmers in a particular area might be access to credit and finance.\n""There is significant potential growth of digital innovation across the region if developers and providers can figure out how to genuinely target and benefit people in low-income groups,"" Spielman said.\n($1 = 107.7000 Kenyan shillings)\n(Reporting by Wesley Langat, Editing by Jumana Farouky and Megan Rowling. Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters, that covers the lives of people around the world who struggle to live freely or fairly. Visit http://news.trust.org)\nOur Standards: The Thomson Reuters Trust Principles.']"	['<urn:uuid:abf62a9a-c591-46f1-974e-1b7653343a55>', '<urn:uuid:2786a900-16ea-45eb-b139-0150b5ba3df2>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	6	59	3277
34	What happened to Cesare Pavese in the post-war period?	Cesare Pavese committed suicide in the post-war period.	"['Originally Posted by JSWolf\nFor July, can we have a book that is not so old that Moses was in diapers and that doesn\'t need a translation?\nWhat a great idea! I second it. I would like so much to read an Italian book in the original language.\nI nominate ""Lessico Familiare"" by Natalia Ginzburg. 1963. (Younger than me even). Kown as ""What We Used to Say"".\nLessico famigliare descrive dall\'interno la vita quotidiana della famiglia Levi, dominata dalla figura del padre. Il libro è la cronaca ironico-affettuosa della famiglia dagli anni \'20 ai primi anni \'50, attraverso abitudini, comportamenti e soprattutto la comunicazione linguistica, da cui deriva il titolo. Figure ed eventi si avvicendano nella pagina senza ordine gerarchico e si presentano da sè, vivono attraverso i loro gesti e le loro parole. In questo libro si affrontano anche tutti i conflitti e le vicende della famiglia Levi. Importante è anche il personaggio della madre di Natalia,i fratelli anche loro in certi momenti sono nominati molto, soprattutto durante il mezzo del fascismo.\nIl romanzo ripercorre vicende familiari cronologicamente legate soprattutto all\'età fascista\ne la seconda guerra mondiale\n, quando vengono evocati l\'uccisione del marito dell\'autrice, Leone Ginzburg\n, per attività politica antinazista, la persecuzione degli ebrei\n, fino ad arrivare al suicidio di Cesare Pavese\ne alla caduta delle illusioni della Resistenza\nWHAT WE USED TO SAY (Lessico famigliare)\nNovel by Natalia Ginzburg, 1963\nWhat We Used to Say (1997; published in Italian in 1963 as Lessico famigliare and first translated into English as Family Sayings ) is Natalia Ginzburg\'s best-known and, in Italy, most venerated book. It is a memoir-novel about the home she grew up in during the period of ascendant Fascism, the Fascist dictatorship, her eccentric parents, defined to a great extent by her father\'s weird sayings that molded and characterized life in the household, the difficult life that her Jewish, academic father experienced during Fascism, the activities of her anti-Fascist brothers, the survival of the entire Levi family, and the melancholy that characterized Ginzburg\'s life and that of her parents—as eccentrics who did not attract a great amount of attention in pre-World War II northern Italy but who were fish out of water in post-World War II Italy. The final part of Ginzburg\'s novel renders a compelling portrait of the state of mind and soul of an active anti-Fascist and resistance fighter, the important writer Cesare Pavese, a transmitter to Italians of American literature and, with Ginzburg, one of the important editors at the publishing house of Einaudi, which published some of the most important novels of postwar Italian literature.\nWhat We Used to Say is a loving testament to the benign life, in Ginzburg\'s view, that disappeared with World War II. It is a life that not only millions of Italian readers have identified with and continue to react positively to by buying Ginzburg\'s novel in high numbers but also one that has appealed so strongly to its readers in the English-speaking world that the first, defective translation, Familiar Sayings, was replaced by the second, far better translation in 1997, an event that seldom occurs with foreign literature in English translation. In the book Ginzburg furnishes the reader with a compelling, almost claustrophobic portrait—in spare and simple language, laced with the idiosyncracies of the Italian peculiar to Turin—of the domestic life the Levi household lived during this historically important period in Italian and world history, one particularly instructive about what it was to be a member of upper-middle-class, urban, and intellectual Jewish society during the period of the anti-Semitic, Fascist racial laws that marginalized the Jews of Italy.\nGinzburg\'s father was a professor of stature at the University of Turin. For his anti-Fascist statements and for refusing to take the oath of allegiance to Benito Mussolini and Fascism after the 1935 promulgation of the racial laws, Ginzburg\'s father was dismissed, as was Ginzburg\'s future husband, Leone, from his teaching position at the university. Ginzburg\'s father subsequently accepted a lesser position at the University of Ghent, in Belgium, in order to support his family. Fleeing the German advance after their invasion of Belgium, he was captured by the Germans and, without making any secret of his name and his being Jewish, fortunately released. Instead of following the suggestion by a German officer to return to Ghent, Ginzburg\'s father made his long and arduous way back to Italy, thereby surviving the Holocaust.\nAs was her husband, who paid for his resistance with his life, all Ginzburg\'s brothers were active anti-Fascists—in contrast to her father\'s anti-Fascism rants inside the Levi home after the family had hidden an active anti-Fascist Socialist while passage was being arranged for him to Switzerland. One of Ginzburg\'s brothers escaped capture after bringing anti-Fascist leaflets back from Switzerland. Her other brother returned to clandestine resistance after release from jail in Turin.\nThis side of life in the Levi family is described alongside the nearly banal events of the family\'s domestic and social life with their mostly Jewish friends, some of whom would disappear and never be seen again. Ginzburg describes the everyday and the nearly tragic side-by-side in a language that neither sentimentalizes her family nor surrenders to runaway emotionalism. What We Used to Say is one of the few pieces of Holocaust literature that tells the story of a family that survives the Holocaust, possibly because of its peculiar characters. Her sober, but not coldhearted, writing style and her dedication to a seemingly contradictory but artistically unique and successfully tempered realism are two of the particular features of What We Used to Say but also of her fiction in general that have won Ginzburg an international following and sales success each time her books have come out in Italy and when the ones translated into English have been published.\nIt is not the big historical events that loom large in Ginzburg\'s memoir-novel (almost maddeningly, she dedicates only a few lines to her husband\'s capture and murder shortly before the end of the war); rather, it is the tenacity of her family, the unexpected force that the members of the Levi family carry in themselves, that renders the strong portrait of the Levi family. Despite the eccentricity of the parents and their relations with everybody outside their home, What We Used to Say is an almost didactic novel about the unexpectedly ordered existence of the Levi family amid the disorder and randomness, at least for anti-Fascists, during one of the most momentous and tragic periods in twentieth-century history.\nAlthough her novel proceeds in perfect chronological order, Ginzburg presents the events in her novel, one rife with dialog, in an a-traditional manner, and the power of her deceptively simple language virtually compels the reader\'s vicarious co-participation in her family\'s story and their experiences. Readers who find the first part of her novel disappoint-ingly short on action attest that they come to find her seemingly deceptively unassuming narrative fascinating in the end. Ginzburg\'s novel continues to engage readers and to win her new readers among the young. Whereas the figures in her wartime novel Tutti i nostri ieri (""All Our Yesterdays"") are innocent pawns in the momentous historical drama of World War II, Ginzburg\'s family, these unique Levis—and she includes herself in the cast of strong players, even though she grants herself small space in her memoir-novel—are figures of strength, the opposite of the victims she presents in Tutti i nostri ieri. There is much in the way of old-fashioned personal virtues, devotion to family above all else, and compelling realism in What We Used to Say to account for its continued success.\n—Robert B. Youngblood']"	['<urn:uuid:dba040da-3975-486c-999e-a7b88642223a>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	9	8	1271
35	How much did the extended flooding of the Mississippi River end up costing America, and was most of it covered by insurance companies?	The Mississippi River flooding from December 2018 to August 2019 cost the country $9.2 billion in total, but only about $200 million was covered by insurance according to Munich Re estimates. The majority of the economic loss was borne by communities and governments across the Mississippi basin.	['The Mississippi River exceeded its banks from December 2018 until August 2019, becoming one of the longest floods in the history of the U.S. According to NOAA National Centers for Environmental Information, the event cost the country a total of $9.2 billion. The impact of this economic loss was felt by communities and governments across the Mississippi basin as the insured loss estimate was minimal. Munich Re estimates the insured losses for this event to be approximately $200 million. Events like these highlight the importance of creating communities resilient to natural disasters, both from a risk transfer and a risk mitigation standpoint. Within disaster risk management, investment in risk-reducing measures is considered wholly separate from investment in risk transfer measures. But the two can actually be combined to create more efficient solutions.\nRecently, a group of researchers from University of California, Santa Cruz; The Nature Conservancy; and Munich Re collaborated on a study that focuses on a combined risk transfer/risk reduction solution. The study examines coral reef restoration and its risk reduction impact for property exposed to storm surges along the coastline. While the scope of the study is limited to the benefits of coral reef restoration, we are confident that this combined solution is transferable to other regions focusing on nature-based and/or green solutions. We term this solution broadly as a Resilience Risk Transfer (RRT). Munich Re has been actively pursuing potential proof of concept for testing the RRT solution. Our intent is to apply the RRT solution to the nature-based solutions within the Mississippi River basin, e.g., wetlands, bioswales, etc.\nOne of the major challenges of developing a specific RRT solution is the unavailability of a comprehensive vulnerability study for a potential region. The issue is further complicated with changing views of risk that the regions experience with a warming climate. In recent times, Insurtech companies have offered the unique capabilities of using machine learning and high-resolution climate modeling to address the above gap. These models will be crucial in highlighting and identifying the most vulnerable communities which, in turn, will be useful in devising risk reduction solutions.\nWith the work we’ve done with The Nature Conservancy, we could structure a resilience risk transfer offering by quantifying the impact of mitigation using nature-based solutions, and thus the reduction in premium over the time frame of the mitigation project. We are working toward developing a potentially novel/visionary solution, i.e. Resilience Risk Transfer, that can help communities recover financially from a flood event (and, potentially, other natural disasters) as well as focus on the risk mitigation requirements for the region. The RRT solution has the potential to restore the natural infrastructure that provides one of the most important protections from flood disasters.\nAny descriptions of coverage contained in this document are meant to be general in nature and do not include nor are intended to include all of the actual terms, benefits and limitations found in a reinsurance/insurance policy. The reinsurance/insurance policy and not this document will form the contract between the parties thereto, and governs in all cases.']	['<urn:uuid:8b6429c2-2868-4249-aecb-f7b95f8a6486>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	23	47	509
36	past present america racial change leaders	America's approach to racial issues shows both historical connections and modern evolution. At events like the Missouri Cherry Blossom Festival, descendants of historical figures like Jefferson Davis and Dred Scott come together to discuss their complex legacy and find common ground. Meanwhile, contemporary leadership through initiatives like Racial Equity Here is taking concrete action to address systemic racism. Over 190 groups have committed to creating more equitable communities, with cities implementing specific changes such as Grand Rapids hiring its most inclusive police recruit class and Albuquerque modifying hiring practices to be more equitable. This represents a shift from historical commemoration to active institutional change.	"[""Gathering Of Presidential Lineage Celebrates U.S. History\nSCOTT SIMON, HOST:\nDescendents of famous American political figures, both Democrats and Republicans, white and black, coming together in a small Ozarks town this weekend. The Missouri Cherry Blossom Festival in Marshfield is hosting the descendents of more than 30 past presidents, including relatives of the country's founding fathers and one of the country's most famous slaves.\nFrom member station KSMU, Jennifer Davidson reports.\nJENNIFER DAVIDSON, BYLINE: As a thunderstorm brews outside Blackberry Creek Bed and Breakfast, the direct descendents of Thomas Jefferson and Harry Truman are sitting down to a hearty innkeeper's breakfast with George Washington's great-nephew.\nCHRIS TRUSCOTT: Thomas Jefferson is my great, great, great and several more greats grandfather.\nDAVIDSON: That's Chris Truscott. He shares Jefferson's love of the written word. He's a corporate speechwriter. At the breakfast table is Tom Washington, who comes here very year from Texas. The first time Tom Washington was invited by the festival's organizer he was a bit skeptical.\nTOM WASHINGTON: I said he wants to what? I'm supposed to go to this small town in the Ozarks for a Cherry Blossom Festival. Is this for real?\nDAVIDSON: Despite his famous relatives warning against party politics, Washington works for the Republican Party in Texas. He jokes that the Adams are still as liberal as ever. Relatives of Presidents Madison, Monroe, Nixon, Cleveland and both Roosevelts have all been here. Over at the Marshville High School, Nicholas Inman is greeting guests.\nHe's a young minister who, after living in D.C., felt torn between the nation's capital and his hometown. He's organized the planting of 100s of cherry trees in Marshfield to remind him of the pink blooms that Washington is famous for.\nNICHOLAS INMAN: I love history and I felt like people need to learn history from primary sources and so we needed a festival to go around those trees.\nDAVIDSON: The festival has united some unlikely pairs. Bertram Hayes Davis is the great-great-grandson of Jefferson Davis and is now friends with Lynn Jackson. She's a descendent of Dred Scott, a slave who famously sued for his freedom. As Davis says, his ancestor had a distinguished career before the Civil War.\nBERTRAM HAYES DAVIS: Hero of the war of 1847, secretary of war and senator, and so people don't understand that Jefferson Davis was an American patriot for the first 52 years of his life before he was appointed, not elected, to the presidency of the Confederacy.\nDAVIDSON: Lynn Jackson oversees the Dred Scott Heritage Foundation in St. Louis.\nLYNN SCOTT: In coming together and sharing, you know, how do you feel being the descendent of a slave? How do you feel being the descendent of a Confederate president? And we find some common ground here.\nDAVIDSON: Back at the bed and breakfast, over a steaming cup of coffee, Thomas Jefferson's descendent, Chris Truscott, reflects on how the festival has changed his life. When attending the festival two years ago, he fell in love with the editor of the Marshfield newspaper. He relocated, the two married, and they're expecting a baby this summer.\nHe says people tell him his facial features strongly resemble Jefferson's straight pointed nose and square jaw.\nTRUSCOTT: I don't know if it's true, but people say it. I think there's a - I think to an extent people want to believe it.\nDAVIDSON: His father is buried at Monticello, where he's also eligible for a plot one day. But Truscott says he'll be buried wherever home is when he dies, and that very well could be among the cherry trees that grace this little sliver of the Louisiana Purchase. For NPR News, I'm Jennifer Davidson.\n(SOUNDBITE OF MUSIC)\nSIMON: And later this hour, coming up in time for Broadway's annual honors, the Tony Awards will look ahead at the season's theater hits and misses with Barbara Chai of the Wall Street Journal. And as always, we invite you to join us on Twitter. We're at NPR WEEKEND, and I'm at NPRScottSimon, all one word.\n(SOUNDBITE OF MUSIC)\nSIMON: You're listening to WEEKEND EDITION from NPR News. Transcript provided by NPR, Copyright NPR."", 'NEW YORK, JUNE 19, 2018 — Recognizing the need to collectively tackle growing racial disparities, organizations from the public, private and philanthropic sectors today announced the Racial Equity Here commitment to dismantle structural racism in America. These leading institutions invite others to join them in taking clear steps to prioritize racial equity in their work\nRacial Equity Here is a collaboration led jointly by Living Cities, a philanthropic collaborative focused on racial and economic justice, and the Government Alliance on Race and Equity (GARE), a joint project of Race Forward and the Haas Institute for a Fair and Inclusive Society. GARE is a national network of local governments working to achieve racial equity and advance opportunities for all.\n“In recent years, racial inequity in America’s cities, businesses and even coffee shops has made national headlines, but that attention has yet to result in lasting systemic change,” said Ben Hecht, President and CEO of Living Cities. “No single organization can move the needle on racial equity on its own. Racial Equity Here is building a critical mass of partners across industries and sectors that together can create dramatically better outcomes for people of color in America.”\nOrganizations making this commitment believe that racial disparities in America are too widespread for any one city, sector, organization or program to tackle alone. While individual efforts like training can be impactful, Racial Equity Here aims to fight structural racism by transforming policies, practices and norms within our institutions and organizations at a national scale.\n“The profound outcome gaps we see today between people of color and white people aren’t accidental — they were intentionally created. To achieve a stronger and truly multiracial, inclusive democracy, organizations from every sector must now partner to proactively advance racial equity. Through Racial Equity Here: Commit to Action, we are distilling years of research and practice into clear, easily accessible tools that organizations can start using today,” remarked Glenn Harris, president of Race Forward.\nTo date, over 190 groups have committed to create more equitable communities and workplaces by learning about structural racism, using racial equity tools to guide action that closes gaps and improves outcomes for all, and partnering across sectors to align efforts and accelerate results.\nThis growing movement has been accelerated through the Racial Equity Here initiative, launched by Living Cities and GARE in 2016, which helped five cities transform their municipal operations to better address racial disparities. Through this initiative, Albuquerque, Austin, Grand Rapids, Louisville and Philadelphia are changing how they do business. They have established racial equity visions and action plans; are training staff on government’s responsibility to create racial equity; are using data and racial equity tools to guide policy, program and budget decisions; and are forming cross-sector teams as part of their broader commitment to improve outcomes for all residents.\n“Racial Equity Here is about changing the structures and systems that create and perpetuate racial inequity,” said john a. powell, Director of the Haas Institute for a Fair and Inclusive Society and Professor of Law, UC Berkeley. “We are committed to expanding the “we” in we the people, building bridges across sectors and states to amplify and accelerate our multiracial movement for belonging and racial justice.”\nKey outcomes, policies and initiatives stemming from Racial Equity Here include:\n- Albuquerque no longer asks about criminal convictions on its initial application for employment. It also has modified the W-9 form to give preference to local, minority owned, and women owned companies who bid for city work.\n- Austin’s Office of Equity is collaborating with the Neighborhood Housing and Community Development Department and the Public Health Department to revamp procurement practices and increase the accessibility of city funds to organizations doing meaningful work to address inequity.\n- Grand Rapids recently hired its most inclusive police recruit class ever and convened a series of listening sessions to strengthen community and police relations. Mayor Rosalynn Bliss and city commissioners also earmarked $1 million annually for the next five years to strengthen community and police relations.\n- Louisville is revising its process for selling vacant or abandoned properties to make it easier for local residents of color to acquire the properties, with the goal of revitalizing neighborhoods.\n- Philadelphia evaluated disparities in city response times to its 311 complaints about housing quality and recommended policy updates to better support fair delivery and quality of service to all communities. The city also launched the Department of Public Health’s “Get Healthy Philly Summer Youth Tobacco Survey Program” to help tackle racial health disparities related to tobacco usage among youth of color.\n“The City of Philadelphia is committed to advancing racial equity and inclusion across our city. Closing opportunity and achievement gaps is not always a quick, linear process; but rather, an intentional, ongoing effort that requires a focused commitment to change,” said Mayor Jim Kenney. “My administration has implemented various initiatives to ensure that diversity remains a priority throughout City departments, that access to high-quality education is delivered on an equitable basis, and that our economic growth is inclusive of all Philadelphians. The Racial Equity Here commitment is one of many pathways that will help our city move this important work forward.”\nLearn more about the Racial Equity Here commitment and join the movement at https://racialequityhere.org/.\nFor media inquiries and more information about your organization can get involved with Racial Equity Here, email Hafizah Omar at HOmar@livingcities.org.\nAbout Living Cities\nFor over 25 years, Living Cities has harnessed the collective power of 18 of the world’s largest foundations and financial institutions to develop and scale new approaches for creating opportunities for low-income people and improving the cities where they live. Its investments, research, networks and convenings catalyze fresh thinking and combine support for innovative, local approaches with real-time sharing of learning to accelerate adoption in more places. Additional information can be found at www.livingcities.org.\nAbout Race Forward\nRace Forward: The Center for Racial Justice Innovation united with Center for Social Inclusion in 2017 to become the new Race Forward. Founded in 1981, Race Forward brings systemic analysis and an innovative approach to complex race issues to help people take effective action toward racial equity. Founded in 2002, CSI catalyzes community, government, and other institutions to dismantle structural racial inequity and create equitable outcomes for all.\nThe new Race Forward is home to the Government Alliance on Race and Equity (GARE), a national network of government working to achieve racial equity and advance opportunities for all. Race Forward publishes the daily news site Colorlines and presents Facing Race, the country’s largest multiracial conference on racial justice.']"	['<urn:uuid:6217d1cf-c40b-464a-a1df-d17525e686bb>', '<urn:uuid:1a139730-39fa-4219-b435-6b1bf1d2c67f>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	6	104	1786
37	How do indigenous peoples and the Sahrawi compare in their fight for land rights?	Both groups face similar struggles over control of their traditional territories. Indigenous peoples in former British colonies have been affected by the Doctrine of Discovery, which diminished their sovereignty and allowed governments to claim their lands, treating them as merely having rights of 'occupancy.' Similarly, the Sahrawi people have been denied control over their territory by Morocco, which maintains administrative control over two-thirds of Western Sahara despite international laws supporting the Sahrawi's right to self-determination. Both situations involve the exploitation of natural resources - Indigenous peoples face conflicts with governments and corporations over resources like coal, oil, and water, while the Sahrawi are prevented from accessing their territory's phosphates, iron ore, and fishing resources, which are instead controlled and profited from by Morocco.	['Doctrine of Discovery Factsheet\nWhat is the Doctrine of Discovery? Why Should It Be Repudiated?\nFor thousands of years, Indigenous Peoples lived free in their territories in the Western Hemisphere. When European monarchies invasively arrived in the Western Hemisphere in the fifteenth, sixteenth, and later centuries, during the so-called Age of Discovery, they claimed the lands, territories, and resources of the Indigenous Peoples, asserting that the monarchies had a right to appropriate on behalf of Christendom. The monarchies’ claims of a Christian dominion (dominance) over Indigenous Peoples and their lands served them pragmatically to fend off competing monarchies and to de-legitimate the long-established autonomous indigenous peoples’ governments.\nThe Doctrine of Discovery is a key premise for non-Indigenous government claims to legitimacy on and sovereignty over Indigenous lands and territories. It is used in particular by former British colonies, specifically, Canada, Australia, New Zealand and the United States of America.\nWhat is found in the Doctrine of Discovery?\n- Criteria for claiming land.\n- European monarchies treated indigenous land as “unoccupied,” as long as Christians were not present. Status of a “human” was based on religion.\n- Land deemed “unoccupied” were therefore “discovered” as if it had been previously unknown to humankind, and the land thus claimed by the “discovering” Christian European “sovereign.”\n- Transfer of the land. A Christian government’s claim to sovereignty over the territory of an indigenous nation or people could be transferred by a treaty with another Christian government, such as treaty between the British Crown and the United States.\n- Government by agent or proxy. Sovereign monarchs gave royal charters of “discovery” to companies or individuals to delegate the work of claiming Indigenous lands.\n- Coercion and subjugation of whole peoples. The Christian European governments sought to subdue, enslave and convert peoples.\n- Incorporation of a diminished and impermanent status into secular laws.In the 1823 US Supreme Court ruling Johnson v. M’Intosh, the Supreme Court ruled that Indian nations had no legal title to their lands and were entitled only to the right of “occupancy.”\n- Double standard among international conventions. The Doctrine of Discovery is used to diminish validity and significance of international treaties between Indigenous Nations and the US, Canada, New Zealand, and Australia.\nElements of the Doctrine have rationalized heinous behaviors against Indigenous peoples through the centuries. Forced removals such as the Trail of Tears, the seizure of natural “resources,” the destruction of traditional languages and cultures, the sterilization of Indian women, and the disruption of Indigenous communities are examples of implementation of the concepts of “discovery” and “dominance.” The Vatican papal bulls of the fifteenth and sixteenth centuries actively encouraged the subjugation of Indigenous nations, and the secularization of the doctrine in the United States and elsewhere perpetuated subjugation and its consequences.\nWhat effect does the Doctrine of Discovery have at this time?\n- The Doctrine of Discovery’s assumption about who is sovereign allows policies to develop without the full knowledge and prior informed consent of indigenous peoples.\n- Diminished protection of human rights is evident. e.g. There is no indigenous jurisdiction over crimes committed on their reservations by non-natives.\n- The diminished and impermanent status of indigenous peoples under the Doctrine of Discovery is contrary to the right of Indigenous peoples to sustain themselves in perpetuity as distinct peoples, a right affirmed in the UN Declaration on the Rights of Indigenous Peoples.\n- The Doctrine’s concept of occupancy (“Indian title”) is inconsistent with the constitutional status of treaties. Treaties are the highest law of the land, equal to the constitution. Treaties are made between sovereign states.\n- Self-determination of indigenous peoples brings them in conflict with governments and corporations that rely on the legal lineage of the Doctrine to assert claims to natural resources, such as coal, oil, uranium, natural gas and water.\nReadings on the Doctrine of Discovery:\n- Buying America from the Indians: Johnson v. McIntosh and the History of Native Land Rights, by Blake Watson (2012)\n- Johnson v. M’Intosh, US Supreme Court (1823)\n- Pagans in the Promised Land, Decoding the Doctrine of Christian Discovery, by Steven T. Newcomb, (Shawnee/Lenape) (2008)\n- Native America, Discovered and Conquered, Thomas Jefferson, Lewis and Clark and Manifest Destiny, by Robert J. Miller, Esq. (Eastern Shawnee) (2006)\n- Conquest by Law, How the Discovery of America Dispossessed Indigenous Peoples of their Lands, by Lindsay G. Robertson (2005)\nSee Web Links section for access to translations of papal bulls.\nReading for context:\n- Paradigm Wars, Indigenous Peoples’ Resistance to Globalization, edited by Jerry Manderand Victoria Tauli-Corpuz(2006)\n- Exiled in the Land of the Free, edited by Oren Lyons et al.(1992)\n- American Indian History, Five Centuries of Conflict and Coexistence, Robert W. Venables(2004)\nDoctrine of Discovery (study group)\nIndigenous Law Institute http://ili.nativeweb.org/\nUnited Nations Declaration on the Rights of Indigenous Peoples http://www.un.org/esa/socdev/unpfii/en/drip.html\nUN Preliminary Study on the Doctrine of Discovery http://www.un.org/esa/socdev/unpfii/documents/E.C.19.2010.13%20EN.pdf\nSome steps in the historic sequence of the Doctrine of Discovery\nBCE: The Roman Empire took unoccupied or undefended land (terre nulles) for the emperor.\n4th century and onward: In the early 4th century, Emperor Constantine established the orthodoxy of the Christian church in the Roman Empire. Christianity was decreed the exclusive state religion of the Empire in 380 CE.\nMiddle ages: The Church converted pagan kings in Europe and the British Isles, often by force of arms and threat of eternal damnation. The Church acquired vast tracts of land from them.\n11th–13th centuries: Crusades attempted to control the Holy Land. Lands not occupied by Christians were deemed vacant. The Church arbitrated land claims among Christian monarchs.\n15th century: Papal bulls (1452, 1453, 1493, and following) authorized only Christian monarchies as sovereign, and encouraged them to vanquish and place in perpetual slavery/servitude any heathens, pagans, Saracens, or other non-Christian peoples. For example, Portuguese developed the slave trade along the west African coast. The Church required “discovery” of land that was not occupied by Christians and to bring it under Church dominion. The peoples in Asia, Africa, and the Americas were not deemed sovereign as they were not Christian. The papacy authorized military conquest to assist conversion to Christianity.\n16th–18th centuries: Church-based claims to sovereignty by Spain, Portugal, and France became transferable to other Christian monarchies through treaty. Protestant monarchies, such as the British Crown under Henry VIII, reasserted themselves as Christian sovereigns, a status further bolstered by treaties and dynastic marriages.\n1783: The Treaty of Paris between the British Crown and the United States of America recognized the United States of America as a “sovereign” government (though not a monarchy).\n1823: US Supreme Court case Johnson v. M’Intosh made “discovery doctrine” explicit in US law. The court denied individuals permission to buy land from American Indian tribes [nations]. Under the doctrine, the court assumed only a sovereign United States could acquire the land, should the Indians choose to sell. In this decision, Indians were given a limited right of “occupancy” without full title to their own land, and could thus lose their land if they could not prove continuous occupancy. The doctrine was reframed in secular terms, in which the criterion for sovereignty became “cultivators of land” instead of “Christians.”\n1955: US Supreme Court case Tee-Hit-Ton Indians v. United States relied on the doctrine of discovery. The court ruled that because “Tee-Hit-Tons were in a hunting and fishing stage of civilization” they had only a limited right of occupancy, and therefore the US was not required to reimburse the Tee-Hit-Ton for timber harvested from their land. [Tee-Hit-Ton are Tlingit people in Alaska. See http://www.utulsa.edu/law/classes/rice/USSCT_Cases/Tee-Hit-Ton_v_US_348_272.htm.]\n2005: US Supreme Court case City of Sherrill v. Oneida Nation of Indians relied on doctrine of discovery to limit the Oneida Nation’s sovereignty. The Oneidas had documented their sovereignty through US treaties. The court ruled that due to an interval of nonoccupancy, land in question was not sovereign Oneida territory. Under the doctrine, the land could have federal trust status. Trust status exempted the land from taxation, in this case by a municipal government, the City of Sherrill.\n2007: UN Declaration on the Rights of Indigenous Peoples acknowledges many rights, yet is not explicit on sovereignty issues.\nThis item is part of', 'Legal Rights Owed to the Sahrawi: Self-Determination and Use of Natural Resources\nPosted on November 07, 2012\nBy: Erica Miller\nSelf-determination has been a point of contention for the Sahrawi people since the UN encouraged Spain to decolonize the territory in the 1970s. In lieu of a referendum on self-determination, Spain ceded administrative control of the territory to Morocco and Mauritania, two nations that asserted claims over regions in the occupied territory. This did not change the status of Western Sahara as a non-self-governing territory, and prolonged the referendum. After a war in which the Frente POLISARIO was victorious, Mauritania and the Frente POLISARIO entered into an agreement in which Mauritania renounced all claims over Western Sahara. This gave the Frente POLISARIO control over approximately one-third of the territory, leaving Morocco with the other two-thirds. Morocco maintains the position that it has the legal right to control Western Sahara, and currently controls not only the government, but the natural resources as well.\nWestern Sahara is an area rich in natural resources. Morocco currently controls the majority of Western Sahara’s natural resources in the form of fisheries, phosphates and iron ore, and other resources. The Sahrawi have little access to these resources, as many of the workers are Moroccan citizens who immigrated to Western Sahara to work in the industries. Resources originating from the territory of Western Sahara are marketed by the Moroccan government as originating from Morocco, and companies wishing to take advantage of the resources present in the territory contract through the Moroccan government or Moroccan businesses. The Sahrawi have no control over the natural resources and the commerce from those resources in the Moroccan occupied territory. The Moroccan government controls the entirety of the coastline, preventing many Sahrawi from fishing, a lucrative industry.\nThe Right to Self-Determination\nUN Charter Chapter XI: Declaration Regarding Non-Self-Governing Territories (Articles 73 and 74) states that Member States assuming the responsibility for administration of a non-self-governing territory have certain obligations over that territory and its peoples. Most notably, Article 73 of the UN Charter requires those Member States to ensure the “political, economic, social and educational advancement…just treatment, and … protection against abuses” of the peoples of that territory. In 1960, the United Nations General Assembly (UNGA) adopted the Declaration on the Granting of Independence to Colonial Countries and Peoples. This declaration, more commonly referred to as the Declaration on Decolonization recognizes the principles set forth in the UN Charter to promote fundamental human rights and self-determination of all peoples.\nThe International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR), entered into force on March 23, 1976, states in Article 1 that all peoples have the right to self determination, and all peoples may freely dispose of their “natural wealth and resources… [and in] no case may a people be deprived of its own means of subsistence.” Article 47 reaffirms the inherent right of all peoples to fully utilize and enjoy their natural wealth and resources. Morocco is a party to both the ICCPR and the ICESCR. Under international law, parties to a treaty must adhere to all obligations created by that treaty. By asserting control over Western Sahara, Morocco has failed to carry out its obligations under both treaties.\nThe Right to Use and Control of Natural Resources\nThe right of the Sahrawi to utilize their natural resources is supported by several international instruments. Morocco controls two-thirds of Western Sahara including the entire coastline. Moroccan controlled mines employ predominantly Moroccan employees, take phosphorus belonging to the Sahrawi and sell it to foreign companies under the Moroccan name.\nThe UN has adopted over eighty resolutions related to sovereignty over natural resources. General Assembly Resolution 1803 (XVII), passed in 1962 states, “the right of the peoples and nations to permanent sovereignty over their natural wealth and resources must be exercise in the interest of the national development and of the well-being of the people of the State concerned.”\nThe Office of the High Commissioner for Human Rights investigated the human rights situation in Western Sahara in 2006. The report was never officially published, but it was leaked. The report noted that in the Moroccan territory, “the Sahrawi people are not only denied their right to self-determination, but equally are severely restricted from exercising a series of other rights.” The UN Legal Counsel addressed the Security Council in 2002 stating that the use of mineral resources by an administering power of a non-self-governing territory is legal unless “conducted in disregard of the needs and interests of the people of that territory.”\nIn 2007, Morocco entered into the Fisheries Partnership Agreement (hereinafter “the Agreement”). The Agreement, made without regard for the Sahrawi people, permits European countries to fish commercially along the coastlines of Western Sahara, with all proceeds from the commerce going to Morocco. The first fishing agreement between Morocco and the European Union dates back to 1995. While there has been recent conflict over the Agreement, the problem is economical, and not based on the unfairness or the agreement. Last year, several members of the European Parliament wanted the renewal proposal to be referred to the European Court of Justice to make a ruling on the inclusion of Western Sahara in the Agreement. This ruling would look at whether the inclusion of Western Sahara mirrored the wishes of the Sahrawi and whether their interests were taken into account, as required by treaties and international law. This was rejected by the plenary of the European Parliament. The reasons cited for the rejection and ultimate renewal were predominately economic.\nStates that trade with Morocco have to decide whether to participate in the exploitation of the natural resources in Western Sahara. Some states have spoken out against Morocco’s use of Western Sahara’s resources. In a speech to the Swedish Parliament, the Minister of Foreign Affairs, Carl Bildt stated that Morocco occupies the territory of Western Sahara, and therefore “has no right to exploit the natural resources in Western Sahara for its own benefit.”\nDecolonization has reached every former territory with the exception of Western Sahara. The Sahrawi should be afforded the right to self-determination, a right not only enjoyed by the rest of the world’s population, but also supported in international instruments. More than fifty years after the Declaration on Decolonization, the Sahrawi are still waiting to obtain the right afforded to them by law.\nThe right to control over natural resources is important because it was recognized that without it, the right of self-determination was being compromised. Self-determination is connected on a fundamental level to the right to control one’s own resources in addition to controlling one’s own government.\nMorocco must not maintain their illegal control over the government, land and peoples of Western Sahara. The Sahrawi have the rights to establish their own government and use their natural resources for their own benefit. The Organization of African Unity took a firm stand on this issue, but the majority of the international community has continued to ignore the violations that Morocco is committing against the Sahrawi; it is important to remember that these rights do exist and that something must be done to protect them.']	['<urn:uuid:fac60af2-b24b-4f2c-8d2c-faf44f576e9d>', '<urn:uuid:583aa1f2-b0b9-4663-b631-e4bfd8b3dc10>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	14	123	2554
38	big data processing pros fast scaling issues	MapReduce excels at processing enormous amounts of data by handling relatively small, independent calculations spread across many computers. While it offers efficient data processing through map and reduce phases, scaling can face issues. For instance, in systems using consistent hashing on a ring (like some NoSQL datastores), adding or removing nodes requires data migration and can lead to uneven data distribution, requiring continuous infrastructure doubling or tedious node repositioning for proper balance.	"[""This section provides an overview of what mapreduce is, and why a developer might want to use it.\nIt should also mention any large subjects within mapreduce, and link out to the related topics. Since the Documentation for mapreduce is new, you may need to create initial versions of those related topics.\nMapreduce is a part of Hadoop. So when Apache Hadoop (or any distribution of Hadoop is installed) MR is automatically installed.\nMapReduce is the data processing framework over HDFS(Hadoop distributed file system). MR jobs maybe written using Java, python, Scala, R, etc.\nMapreduce is a programming model to do processing on (very) large amounts of data.\nTraditional 'HPC' (High Performance Computing) speeds up large calculations on relatively large amounts of data by creating a set of highly connected computers (using things like extremely quick networking, and quick access to shared storage, shared memory) to handle computing problems that usually require calculations to have access to each others data. A classic example is weather forecasting.\nMapreduce on the other hand excels at handling relatively small, independent calculations on enormous amounts of data. To make this possible, the data is spread across many computers (due to the amount of data), and the desired calculation is split into a phase that can be done on each bit of data independently (the 'map' phase). Results of these independent calculations are then gathered and a second part of calculations is done to combine all these individual results into the end result (the 'reduce' phase).\nImagine you have a very large amount of votes to count, and there is a bit of work to count each vote (e.g. finding out from the scanned image which box was ticked).\nIn this case, a mapreduce implementation would:\nSpread the images to process over the available computers.\nOn each computer, for each image:\nNote that work can start as soon as a computer gets 1 image to work on. There is no need for all these computers to interact to do their work, so there is no need for them to be interconnected quickly, have shared memory or shared diskspace.\nGather all these outputs on 1 computer.\nCount how many votes for each number (or code or name) there are.\nThis very basic example also highlights how further optimizations are often possible. In this case the reduce step itself can clearly be done partially on each computer, and then a final reduce can be done on a central computer. This will both reduce the amount of work on the one computer running the reduce step, and limit the amount of data that needs to be transported over the network.\nSame as before: Spread the images to process over the available computers.\nSame as before: On each computer, for each image:\nGather all the outputs of 1 computer on the computer itself.\nCount how many votes of each number (or code or name) there are in the local results and output these counts.\nGather all the outputs of the local reduces on 1 computer.\nSum up the locally made counts of votes of each number (or code or name).\nNote that in step 3 it is not necessary to wait for all results in any of the below cases:\nthe local gathering and local reducing can be done on the results produced so far on the local computer, and this can be done at any time.\nThe local reduce step is called the combiner step. This is an optional step used to improve performance."", 'At Mixpanel, we process billions of API transactions each month and that number can sometimes increase rapidly just in the course of a day. It’s not uncommon for us to see 100 req/s spikes when new customers decide to integrate. Thinking of ways to distribute data intelligently is pivotal in our ability to remain real-time.\nI am going to discuss several techniques that allow people to horizontally distribute data. We have conducted interviews (by the way, we’re hiring engineers) with people in the past that make poor decisions in partitioning (e.g. partitioning by the first letter in a user’s name) and I think we can spread some knowledge around. Hopefully, you’ll learn something new.\nConsistency is the idea that when you add a new node to the system it will not shift all the items and then shard old data to a new node. Balancing means that your partitioning system can evenly distribute the potential load on the system. Split information is effectively the data you can utilize or have deemed useful to evenly distribute data across your infrastructure. For example, at Mixpanel, we do not use a customer id to split data because our customers can range from having 100’s to 10’s of millions of users.\nYou absolutely need these properties in order to shard your system correctly.\nOne simple way to partition your data is in cells (I believe Facebook uses this term and I like it). The idea is that you purchase a certain fixed set of hardware upfront: lets say 10 nodes. You partition each user via modulo into each node. So for example, a user with the id of 783,902 will be sharded to node 2 (crazy math: 783902 % 10). The idea of cells is that when you notice performance degradation you can create a new cell with 10 more nodes in it. New users (let’s say users above user_id 1 million) will then always get sharded to a node in that cell.\nCells have a nice property in that if a part of your infrastructure goes down it only affects a subset of your users. That, in many cases, is a huge advantage. Cells can also be used in conjunction with other sharding techniques.\nRouting is a bit more complicated. If your data happens to be data where you get to control the split information (e.g. a user_id) then you can use routing. Here’s how it works:\n- Decide how large a user_id can potentially be.\n- Decide how many nodes you’ll reasonably ever have.\n- Make your new user_id the length of #1 and #2.\n- The first part of the user_id will be the shard id and the second part will be the actual user_id.\nFor example, imagine we believe our company can have 1 million nodes then we may only need 32 bits to represent that. That means the first 4 bytes of our user_id will be shard_id and the remaining data will be just the user_id. The last part could be another 8 bytes if we think we can have over 4B (max int) users. Never know!\nWhen a user shows up to comment, upload a picture, etc. you know exactly which node this specific user is on and can grab all the data at once or write the data by just parsing the shard_id from their user_id.\nLookup tables are useful when you can’t do routing. The idea is that you have a giant hash table in some datastore and you map some unique information to a random node in your system. This allows us to have consistency even though we may not know the exact information that is being piped to us. The best example of this is when you have single users who generate tons of data that you do not control and where that data cannot fit on 1 single node. Luckily, data of this type will generally have characteristics that allow you to evenly distribute it.\nLookup tables are easy enough to implement but you may come across the issue that you wish to shard your lookup table. The irony of that situation is intense at times. Fortunately, your fallback plan can be to use cells of lookup tables where you buy enough nodes within a cell that no single customer on the system could reasonably cause bottlenecks. By the time you need that you will understand your infrastructure fairly well anyway to know that exact number.\nThis sucks but people scale this way. It’s fairly simple: you shard users anyway you want, if a node is seeing problems you migrate some of the data away. If you’re willing to accept the tediousness of data migration then there’s all kinds of solutions to scaling.\nConsistent hashing on a ring\nConsistent hashing on a ring is interesting and a lot of NoSQL type datastores use it (Cassandra, Riak, etc.). The idea is that you have a ring that is numbered 0 to a huge number (2**64) and the piece of data is assigned a number on that ring. As you add new nodes to the system they are given a number on that ring and they will basically be declared as the owner of certain range of tokens on that ring (e.g. 0 – 1M).\nCheck out the picture from project voldemort:\nPlease be aware there are many issues with this but it primarily works well (in my opinion) in caching situations (e.g. memcache). If any node in the ring is added or disappears the data will have to shift making you lose some level of consistency. In caching, this is relatively okay as long as you can avoid the stampeding.\nOutside of caching where you need persistency, data must migrate. There’s a concept of vnodes that riak employs to get around this. Cassandra does not use vnodes however. We deprecated Cassandra from our stack specifically because it was difficult to have proper node balance as some nodes would have more data than others. The solution for Cassandra is to continuously double your infrastructure in order to get even data distribution or move nodes around to new tokens on the ring (extremely tiresome).\nTips and tricks\nRead repairing is great when things go bad or you wish to migrate data in a less tedious way. The idea is that every time new data is written, you map it to the new shard, and then write it to the new shard as you normally would. Anytime old data is updated or read you re-map it to a new shard_id and write it to the new shard_id. Of course, anytime data is read you always check the new shard, if it exists, great, but if not, then you must check the old shard.\n- Is data in new shard? Yes: Return it. No: Ask old shard.\n- Is data in the old shard? Yes: Write it to new shard and return it. No: Does not exist or if you’re writing something you must write it to the new shard.\nEventually, you will hit a point where 99% of reads/writes are occurring on your new node.\nWrite flags can be employed in many partitioning scenarios such as in lookup tables, cells, or even routing. The idea is that once a node appears to be hitting capacity you stop writing any new data to it. For example, at Mixpanel when we see nodes in our system reaching a certain threshold of iowait we will stop writing new data to that node (old data that is currently mapped to it continues to be written) but new data will be sharded to new less loaded nodes in our infrastructure.\nIf you have any sharding techniques that I have not talked about here that are more novel please talk about them in comments because we’d love to hear them. If you find yourself needing to use any of these techniques on the web, it is generally a sign of success!']"	['<urn:uuid:fdd7fbb4-474c-4a9b-80e0-cdd90e075f14>', '<urn:uuid:34b430ce-adb3-4c9d-9f83-ce42406f874f>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	7	72	1914
39	compare physical characteristics horsehead nebula ring nebula structure shape appearance	The Horsehead Nebula is characterized by its distinctive horse's head shape formed by dark dust and gases, with thick dust blocking the light of stars behind it and containing active star formation sites. In contrast, the Ring Nebula has a distorted doughnut shape with a colorful main ring composed of gas thrown off by a dying central star, and contains lower density material in its center forming a rugby ball-like structure within the doughnut's gap.	"['|This article needs additional citations for verification. (July 2010)|\nThe entire neighborhood of the nebula. The Flame Nebula is in the bottom left corner and the nebula itself near the center, in the shape of the head of a horse.\n|Observation data: J2000.0 epoch|\n|Right ascension||05h 40m 59.0s|\n|Declination||−02° 27′ 30.0""|\n|Distance||1.5 kly ly|\n|Apparent dimensions (V)||8 × 6 arcmins|\n|Notable features||Resembles a horse\'s head|\n|Designations||IC 434, Barnard 33,\nLDN 1630, M3T 31,\nThe Horsehead Nebula (also known as Barnard 33 in emission nebula IC 434) is a dark nebula in the constellation Orion. The nebula is located just to the south of the star Alnitak, which is farthest east on Orion\'s Belt, and is part of the much larger Orion Molecular Cloud Complex. The nebula was first recorded in 1888 by Scottish astronomer Williamina Fleming on photographic plate B2312 taken at the Harvard College Observatory. The Horsehead Nebula is approximately 1500 light years from Earth. It is one of the most identifiable nebulae because of the shape of its swirling cloud of dark dust and gases, which bears some resemblance to a horse\'s head when viewed from Earth.\nThe dark cloud of dust and gas is a region in the Orion Molecular Cloud Complex where star formation is taking place. This stellar nursery, as it is known, can contain over 100 known organic and inorganic gases as well as dust consisting of large and complex organic molecules.\nThe red or pinkish glow originates from hydrogen gas predominantly behind the nebula, ionized by the nearby bright star Sigma Orionis. Magnetic fields channel the gases leaving the nebula into streams, shown as streaks in the background glow. A glowing strip of hydrogen gas marks the edge of the massive cloud and the densities of stars are noticeably different on either side.\nThe heavy concentrations of dust in the Horsehead Nebula region and neighbouring Orion Nebula are localized, resulting in alternating sections of nearly complete opacity and transparency. The darkness of the Horsehead is caused mostly by thick dust blocking the light of stars behind it. The lower part of the Horsehead\'s neck casts a shadow to the left. The visible dark nebula emerging from the gaseous complex is an active site of the formation of ""low-mass"" stars. Bright spots in the Horsehead Nebula\'s base are young stars just in the process of forming.\n- Arnett, Bill (2000). ""Horsehead Nebula"". Retrieved July 21, 2014.\n- Sharp, Nigel (2014). ""The Horsehead Nebula"". National Optical Astronomy Observatory. Association of Universities for Research in Astronomy. Retrieved July 21, 2014.\n- ""The Horsehead Nebula/IC434"". National Optical Astronomy Observatory. NOAO. Retrieved 12 May 2014.\n- Morgan, W.W.; Lodén, Kerstin (1966). ""Some Characteristics of the Orion Association"". Vistas in Astronomy 8: 83–88. Bibcode:1966VA......8...83M. doi:10.1016/0083-6656(66)90023-7. ISSN 0083-6656.\n- Mayo Greenberg, J (2002). ""Cosmic dust and our origins"". Surface Science 500 (1-3): 793–822. Bibcode:2002SurSc.500..793M. doi:10.1016/S0039-6028(01)01555-2. ISSN 0039-6028.\n- ""Astronomy Picture of the Day, 21 July 2009"". Astronomy Picture of the Day. NASA. 21 July 2009. Retrieved 12 May 2014.\nMedia related to Horsehead Nebula at Wikimedia Commons\n- The Horsehead Nebula @ The Electronic Sky\n- Hubble Observes the Horsehead Nebula\n- The discovery of early photographs of the Horsehead nebula, by Waldee and Hazen\n- The Horsehead Nebula in the 19th Century, by Waldee\n- Detection of new nebulae by photography, by Pickering\n- Horsehead Nebula at ESA/Hubble\n- The Horsehead Nebula at the Astro-Photography site of Mr. T. Yoshida.\n- The Horsehead-Nebula and neighboring structures in a classical view\n- The Horsehead Nebula on interactive astro-photography survey at Wikisky.org\n- The Horsehead Nebula at Constellation Guide', 'Most detailed observations ever of the Ring Nebula [heic1310]\n23 May 2013The NASA/ESA Hubble Space Telescope has produced the most detailed observations ever of the Ring Nebula (Messier 57). This image reveals intricate structure only hinted at in previous observations, and has allowed scientists to construct a model of the nebula in 3D - showing the true shape of this striking object.\nFormed by a star throwing off its outer layers as it runs out of fuel, the Ring Nebula is an archetypal planetary nebula . It is both relatively close to Earth and fairly bright, and so was first recorded in the late 18th century. As is common with astronomical objects, its precise distance is not known, but it is thought to lie just over 2000 light-years from Earth.\nFrom Earth’s perspective, the nebula looks roughly elliptical. However, astronomers have combined ground-based data with new observations using the NASA/ESA Hubble Space Telescope to observe the nebula again, hunting for clues about its structure, evolution, physical conditions and motion.\nIt turns out that the nebula is shaped like a distorted doughnut. We are gazing almost directly down one of the poles of this structure, with a brightly coloured barrel of material stretching away from us. Although the centre of this doughnut may look empty, it is actually full of lower density material that stretches both towards and away from us, creating a shape similar to a rugby ball slotted into the doughnut’s central gap.\nThe brightest part of this nebula is what we see as the colourful main ring. This is composed of gas thrown off by a dying star at the centre of the nebula. This star is on its way to becoming a white dwarf — a very small, dense, and hot body that is the final evolutionary stage for a star like the Sun.\nThe Ring Nebula is one of the most notable objects in our skies. It was discovered in 1779 by astronomer Antoine Darquier de Pellepoix, and also observed later that same month by Charles Messier, and added to the Messier Catalogue. Both astronomers stumbled upon the nebula when trying to follow the path of a comet through the constellation of Lyra, passing very close to the Ring Nebula .\n Planetary nebulae take their name from their roughly circular appearance through low-magnification telescopes. The phenomenon has nothing to do with planets.\n Messier 57 was not the only object to be discovered during the tracking of this comet, named C/1779 A1. Messier and other astronomers added a handful of other nebulae to the catalogue during this observing period — Messiers 56, 58, 59, 60, and 61.\nThe Hubble Space Telescope is a project of international cooperation between ESA and NASA.\nThe research on Messier 57 is described in a set of three papers, two published in The Astronomical Journal: ""Studies of NGC 6720 with Calibrated HST WFC3 Emission Line Filter Images — I: Structure and Evolution"" and ""Studies of NGC 6720 with Calibrated HST WFC3 Emission Line Filter Images — II: Physical Conditions"" and a third paper that has been accepted for publication in The Astronomical Journal: ""Studies of NGC 6720 with Calibrated HST WFC3 Emission Line Filter Images — III: Tangential Motions using AstroDrizzle Images"".\nThe NASA/ESA Hubble Space Telescope observations used in this image were led by C. R. O’Dell (Vanderbilt University, USA), G. J. Ferland (University of Kentucky, USA), W. J. Henney (Universidad Nacional Autónoma de México, Mexico), and M. Peimbert (Universidad Nacional Autónoma de México, Mexico).\nImage credit: NASA, ESA, and C. Robert O’Dell.\nC. Robert O’Dell\nDepartment of Physics and Astronomy, Vanderbilt University\nGarching bei München, Germany']"	['<urn:uuid:2c8f8d5d-7770-4adf-a363-67d40fa6698b>', '<urn:uuid:714b00ec-3590-4502-8a05-2e7a6f634e9a>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	10	75	1204
40	manufacturing process comparison hollow plastic clinton statues environmental impact	The hollow Clinton busts were manufactured through rotational casting using Smooth-Cast 65D plastic and Foam-iT 5 foam, requiring fossil fuel extraction and chemical processing. While this process saved material costs and reduced weight from 15 lbs to 2 lbs per bust, the environmental impact includes harmful industrial activities like mining and drilling that damage land, pollute water, and affect marine life and adjacent communities.	"[""Jacob Johnston of Little Rock, AR, was contracted to supply 250 bust replicas of President Bill Clinton for the opening of the Clinton Presidential Library in Little Rock, AR in November 2004. The castings had to be lightweight and be able to be handled and dropped without breaking and Mr. Johnston was convinced that rotational casting these pieces was the way to go. His original prototypes were rotationally cast by hand in plaster, and even though hollow, the customer rejected plaster as being too heavy and fragile. But hand rotationally casting plaster for production of 250 pieces would also take a long time and this project most definitely had a deadline. There had to be a better way.\nAfter contacting the Smooth-On technical help line, he narrowed his options down to two materials: Task® 15 and Smooth-Cast® 65D. Not needing the high performance properties (and cost) of TASK® 15, he decided to try Smooth-Cast® 65D. Mr. Johnston rented a rotational casting machine for producing the castings. What remained was creating the final sculpture and mold.\nThe original model was sculpted in clay and Brush-On® 40 mold rubber was used to make the rubber mold. Plasti-Paste® support shell material was used to make the mother mold with 2 x 4 pieces of wood being imbedded to make level supports.\nWith the rotational casting machine set up and ready to go, Mr. Johnston determined the optimal amount of Smooth-Cast® 65D resin to be used in the mold to generate a uniform hollow casting surface.\nSmooth-Cast® 65D performed flawlessly, yielding hollow castings that looked as though they were cast solid but weighed just under 2 lbs. (0.9 kgs) each. All 250 castings were then back filled with Foam-iT® 5 rigid foam to provide lightweight support and additional impact resistance. Each pieces was then painted and delivered in time for the opening of the library.\nMr. Johnston calculates that he saved thousands of dollars in material and labor costs by rotationally casting these pieces.\nThis mold is created using Brush-On 40 mold rubber to capture the detail from the original sculpted clay model. The rubber captured perfect detail that will be reflected in the finished castings. Then ease Release® 200 is applied to the mold.\nMold Maker's Tip: Brushing a thin coat of baby powder into the mold leaves the castings with a dry, matte finish - ready to be primed and painted.\nThe seam on the rubber mold is aligned, and the rubber mold is seated into the one half of the Plasti-Paste® support shell. The other half of the support shell is put in place and mold straps are used to secure the rubber mold inside the support shell.\nThe mold is then placed on the base of a rotational casting machine. A release agent is applied to the 'capping' board that will contain the resin in the mold as it is being rotationally cast. The cap board is placed over the mold opening, and the mold is secured to the base with adjusted straps.\nPart A and B of Smooth-Cast® 65D are dispensed and combined in a mixing container. Both parts are then thoroughly mixed for 30 seconds. The mixture is poured into the mold through the opening in the cap board. A silicone plug is used to seal the hole in the cap board.\nThe rotational casting machine is turned on and the large arm rotates one way, while the base rotates the other way. This 360° rotation ensures that the liquid plastic is being uniformly distributed to all parts of the mold surface. After about 10 minutes the machine is stopped and the silicone plug is removed and a punching tool is used to punch a small hole in the partially cured casting.\nNow, Foam-iT!® 5 Parts A and B are measured and mixed. The expanding foam will be used to fill the hollow casting, keeping it lightweight and maintaining its strength. The Foam-iT!® 5 mixture is poured through the small hole in the casting. In a few minutes the Foam-iT!® 5 rises to entirely fill the hollow casting. Residual foam exits the top of the casting.\nThe cap board is removed exposing the residual foam. A saw is used to remove the residual foam. The mold and support shell are disassembled and the cured casting is removed.\nThe cured casting reflects perfect detail and looks solid. If this piece was cast solid, it would have weighed 15 lbs./ 6.8 kgs. and cost over $100 in material cost alone.\nThe casting is ready to be primed and painted for display. The combination of Smooth-Cast® 65D and Foam-iT!® 5 ensures that these castings are almost indestructible. Each foam filled rotational casting weighs about 2 lbs./ .9 kg. and costs under $10. So light, you can balance a casting on one finger!"", 'Written by: Natalia Brown\nWith the rapid spread of sensational images and heart-wrenching video clips capturing the physical impacts of marine debris, many of us have been faced with the reality that plastic is the least favorable material for environmental health. Social media and digital campaigns help these stories to spread rapidly, but conflicting messages about different materials may also create unproductive online debates.\nWhat makes this conveniently durable, widespread material more harmful than alternatives? The answer lies in the process: the life cycle of plastics.\nPlastics are a category of synthetic polymers. They were designed by chemists seeking a strong, lightweight, and flexible material for consumer products. Polymers abound in nature! Cellulose, the material that makes up the cell walls of plants, is a very common natural polymer.\nSynthetic, man-made polymers may be derived from natural substances like cellulose, but more often they are made using the plentiful carbon atoms in fossil fuels (such as methane gas or petroleum). These inputs allow for especially long branching to occur during polymerization chemical reactions.\nSince humans learned how to create and manipulate synthetic plastics nearly 150 years ago, they’ve become a more prominent part of our lives. Most consumer products are made using some type of plastic polymer and many are surrounded by plastic packaging.\nThe first step in creating a plastic product is the extraction of crude oil or methane gas. There are two main methods for these large-scale operations: mining and drilling.\nMining is used to extract solid fossil fuels by digging, scraping, or otherwise exposing buried resources. This often results in huge volumes of excess rock and soil being dumped into adjacent valleys and streams, affecting marine life and water flow. Extraction sites are also often left with poor soil and damaged land that is more prone to mudslides, landslides, flashfloods, and toxic water pollution in adjacent communities.\nOne Harvard University study assessed the life cycle costs and public health effects of coal from 1997 to 2005. The researchers found a link to lung, cardiovascular, and kidney diseases (such as diabetes and hypertension) and an elevated occurrence of low birth rate and pre-term births associated with surface mining practices. The total estimated cost came out to $74.6 billion every year, equivalent to 4.36 cents per kilowatt-hour of electricity produced.\nUnconventional extraction methods such as natural gas hydraulic fracturing or ‘fracking’ have received a great deal of attention from advocates. Environmental effects of fracking include, but are not limited to, methane and toxic aerosol emissions, resource depletion, harmful wastewater contamination, and increased potential for oil spills and earthquakes. Importantly, all methods of fossil fuel extraction impose massive costs for fenceline communities that live, breathe, and work adjacent to these harmful industrial activities.\nRefinement and Cracking\nAfter fossil fuel extraction, the raw materials are shipped to a refinery. The goals of refinement are to derive the building blocks for plastic production: ethane from crude oil and propane from natural gas. At a cracking plant, ethane and propane are then chemically broken down into ethylene and propylene.\nProcessing and Manufacturing\nChemical modification of ethylene and polyethylene through catalyzed polymerization produces resins. Resins are the core ingredient used to produce the durable, flexible plastics we’ve all grown dependent upon. This chemical processing can be carried out in several different ways, leading to a variety of plastic products. That’s what those numbers on the bottom of many plastic products are used to indicate!\nResins are subject to high temperature and pressue, then cooled in order to make pre-production plastic pellets called ‘nurdles.’ These plastic nurdles are the input used by manufacturers to mold and form the plastic products we use.\nFor example, polyethylene plastics are used to make supermarket bags and water bottles. Meanwhile, polystyrene is best known in its brittle form marketed as Styrofoam, but the same chemical building blocks are used to create toy figurines and CD cases.\nDifferences in the end products of plastic production are partly due to use of chemical additives that may enhance or suppress chemical properties. When mismanaged or littered plastic waste accumulates in our oceans as marine debris, plastic additives pose notable eco-toxicological risks for marine species and humans.\nDistribution and Consumption\nSince 1950, approximately 9.2 billion tons of plastic have been produced— 40% of which comes from single-use products such as shopping bags, cutlery, and take-out-ware. Check out this National Geographic video to learn more!\nIndividuals have the power to change the prominence of plastic in the consumer products industry. By changing our purchasing priorities and seeking more plastic-free alternatives, we can urge companies to embrace more environmentally-favorable materials and create more durable, reusable products.\nGiven all the resources and energy used to produce each plastic product, it’s important that we responsibly use and dispose of any plastics we already own. We can re-use, re-purpose, and up-cycle the plastics we have to make the most of their use before discarding them. This can be as simple as storing batteries in an empty take-out container or using a frayed toothbrush to clean hard to reach crevices in your coffee maker or fish tank. Get creative!\nIn Miami-Dade and Broward County, hard plastic narrow-neck bottles may be recycled in municipal single-stream recycling bins. Plastic grocery bags, toothbrushes, clamshell containers, and food packaging are only recyclable through third-party entities such as Terracycle or your local grocery store. They are considered contamination in your curbside recycling bin.\nWhen plastics are separated for recycling, they are transported to a materials recovery facility for sorting. Then, they are taken to a reclaiming plant where the material is flaked, washed, and formed into new resin nurdles for manufacturing.\nRecycling may be the best way to dispose of plastics that can no longer be re-used, but it is important to understand that recycling is not a long-term solution for the plastic waste crisis. Recycling is logistically challenging, energy-intensive, and can be counterproductive by reinforcing plastic consumption. Plastics are also not infinitely recyclable. Each time the material is flaked and reprocessed into nurdles, it produces a lower quality plastic that will ultimately lose its ability to be recycled.\nProblems presented by the entire life cycle of plastics must best addressed by a combination of (1) increased consumer education and motivated reduction in plastic purchases and (2) regulation of plastic producers and polluters to minimize the production of plastics and harmful effects of their industrial activities.']"	['<urn:uuid:bb40ede8-0390-4d4a-9b11-bca5b92e9c06>', '<urn:uuid:bf1e4b64-2e01-487e-a75b-af0ba78100e8>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T17:51:21.175311	9	64	1863
41	best material installation process prevent wild animals under porch expert tips	"To animal-proof under steps, use 1\"" x 1\"" welded wire mesh (16-19 gauge) rather than chicken wire which is too weak. First dig a trench 1 foot deep and 1 foot wide around all sides where animals could dig. Attach the mesh to the bottom of the structure, run it straight down into the trench, then bend it at a right angle away from the structure to cover the trench bottom. However, this should only be done after confirming no animals are currently living there, which can be tested using the paper towel method and deterrents."	"['Already in Your House\nWhat to do if an animal is already in your house\nAnimals in your attic\nDuring the birthing season, we recommend giving the mother a temporary grace period to raise her young to the weaned stage so they are coming out with her. We need her to get them to that stage and it is the least risky solution from the homeowner’s perspective. Any action to either barricade the mother from the attic or to trap and relocate her can have serious consequences for you. Nests are generally in inaccessible spots near the eaves and if babies are orphaned and hungry, they will move around in search of the mother and can fall between walls, creating the need for expensive drywall removal to get to dead or dying animals.\nAfter giving a grace period and babies have been noted coming and going with mom, you can start a feeding regime to encourage them to leave the attic sooner. For squirrels, you should use large striped sunflower seeds (not peanuts, as they will simply stash them) and place a pile on the ground in a strategic spot where the animals come and go from the roof, so they will be easily found. Put the seed out early in the morning and continue to put it out at the same time each day, but move the seed a little further away from the house so the animals are gradually being lured away. After about six days and when you see that mom and all her babies are out eating the seed, then you can animal proof as above. After the proofing has been completed, make sure to stay around the house for the rest of the day and the following day to listen in case one is trapped inside. If that happens the site will need to be re-opened to let it out.\nFor raccoons, once the babies are old enough to travel with mom, she will begin to take them out around dusk, so at this time you can start a feeding regime. Use dry cat kibble, and set it out in a bowl at dusk and then follow the same method used for squirrels. Once you are confident that the whole family is out, the proofing can be done.\nBecause the raccoons only go out after dusk, you might have to encourage them to want to leave by opening the attic hatch a few times during the day and shining a light in their direction, along with making some noise. You might have to consider a temporary repair if it has to be done after dark, leaving the full repair for daylight, but only after you are certain they are no longer in the attic.\nAnimal under my steps\nThe best thing to do is prevention (see below). But this must be done at the right time to ensure that you are not trapping anyone inside. If that happens, and there are babies, they would die under there creating a bad smell that would linger for a long time. In desperation, an adult might find a way to dig them out, and in doing so, could figure out how to dig around the proofing you’re going to install. Babies are born from late winter to the end of June. Newborns have their eyes closed and are not mobile until they are around 8 weeks old, so mark on your calendar 8 weeks from the time you first noticed activity. After the 8 weeks have passed place a ball of paper towel in the hole. If the paper towel is pushed out, the animals are still living there. Since the babies are mobile it is now safe to try some deterrents (you do not want to use the deterrents too early in the birthing season as you do not want to scare mom away leaving her babies in the burrow as they would not survive without her). The deterrents include:\nAfter the deterrents have been used for approximately 5 days, try the paper towel trick again. If the paper is not moved for 3 full days, you can assume the animal(s) have moved on and you can animal-proof. The first step is to dig a trench around the base of the steps on all sides the animal could dig. The trench should be 1 foot deep and 1 foot wide and the material you will use is 1"" x 1"" welded wire mesh (16-19 gauge) which is available at most hardware stores. Do not use chicken wire as it is too weak. Secure the welded wire mesh to the bottom of the structure and run the mesh straight down into the trench and then bend the mesh at a right angle, away from the structure, to cover the bottom of the trench.\nAnimal in a window well\nFor skunks and muskrats, the easiest way to solve the problem (if the window well is shallow enough - no more than 2 feet deep) is to get a piece of wood (at least 1 foot in width) and tack on a piece of scrap carpeting or an old towel for traction. Place the wood in the well on a gradual angle to serve as a ramp. Leave the ramp in overnight, as many animals are nocturnal. If it is a hot day, cover part of the window well with cardboard to provide shade. If the window well is too deep the ramp won’t work so get a deep bucket that will fit into the well. Place some food (wet cat food for skunks; apples for muskrats) towards the back of the bucket and lay it on its side. When the animal goes into the bucket, gently tip the bucket upright and slowly lift it out of the well (tie a string to the handle before placing in the well). When out, gently tip the bucket on its side away from you and back away. Remember, skunks have poor eyesight and only spray as a last resort when feeling threatened. Move slowly and talk softly to make your presence known so the skunk does not feel threatened. Skunks also give a warning by stamping their front paws, meaning back off or else!\nAnimal in my chimney\nFirst: Do not try to smoke it out!\nIf a raccoon has moved in during the spring it is almost certainly having babies. At this point, do nothing. Trying to chase or scare them away will likely result in mom leaving some or all the babies behind. This may mean removing dead and smelly carcasses. There is limited space in a chimney and as the babies grow, this normally forces mom to move on with the babies. There is little harm they can cause to a chimney, just be sure the damper is securely closed. When you think they have moved out (usually about 8 weeks after you first hear noises), place a bunched up ball of paper towel in the top of the chimney. If it isn’t moved after 2 full days and nights, and no noise is heard a spark arrestor screen made from 1""x1"" welded wire mesh should be installed. If the paper is moved, something is still coming and going. So wait a week and try again. It is vital to monitor closely before having the animal proofing done. Raccoons can have young even into mid August, so to be safe you should use the previous steps between early March and the end of September.\nSometimes an animal can get stuck in the chimney (usually birds or squirrels). There are two options to get them out:\n- For squirrels, access the chimney from the roof and hang a thick rope inside. Leave it, and the squirrel will climb out.\nThis option will work for squirrels or birds. Choose a door or window that is closest to the fireplace, if the window is raised more than a foot from the floor, use some boxes as a step up to it. Darken the room so that the animal will be attracted to the light and the smell of fresh air coming from the window or door. Make a path to the door or window using cardboard boxes or the coffee table on its side, or blankets suspended from the ceiling - anything to funnel the squirrel or bird towards the window. Close off other doors. Once the path is in place, and you have the door or window open then gently open the damper to allow the creature access to the hearth. Leave the room, make sure any pets are not around, and be sure there is no noise at all – you don’t want to scare it into staying in the chimney or coming out and finding a place to hide in your house. It may take a half hour or more for the critter to feel safe enough to come down. It is important to do this during daylight.']"	['<urn:uuid:7e462d15-9351-4b32-b35f-118e83a3dd54>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	11	96	1499
42	I work in shipping and I'm curious about damage prevention technology - what are the technical capabilities of Shockmaster devices for detecting mishandling, and how does this compare to newer eco-friendly packaging solutions being developed?	The Shockmaster is a mechanical device that detects excessive shock and vibration during shipping, with settings available from 5-50 Gs and accuracy within ±10% of settings. It operates in temperatures from -80°F to 160°F and can be mounted inside containers or directly on items. When triggered by sufficient shock, it displays a red warning band and can be inspected through container holes or via electrical contacts. While Shockmasters help identify mishandling, they don't prevent damage directly. In contrast, recent eco-friendly packaging innovations focus on prevention through material choice - for example, biodegradable materials made from seaweed, starch, or other natural substances that can be safely dismantled by organisms. These sustainable solutions address both damage protection and environmental concerns, as traditional plastic packaging often ends up as harmful waste, with 35% being single-use items and 10% ending up as ocean pollution.	['What is a Shockmaster?\nThe Shockmaster is a low-cost, compact mechanical device that can be installed on delicate equipment to detect excessive shock and vibration, thereby assisting in the detection of hidden damage.\nHow do Shockmasters work?\nThe Shockmaster consists of a spring-loaded weight with an integral trigger which engages with a spring-loaded sleeve, the entire assembly being enclosed in a metal housing with a transparent dome. When a shock occurs, the weight is forced down against its spring. If the shock is of sufficient magnitude, the trigger is disengaged from the sleeve, which, in turn, is forced by its own spring to move along a rod, exposing a red warning band.\nHow long will the Shockmaster G-rating stay in tolerance?\nSince the springs in the Shockmaster are never elongated, it will remain in tolerance indefinitely, provided it is not subjected to continuous motion of the moving parts and is not contaminated internally with foreign particles.\nWhat is the Shockmaster’s area of response?\nThe Shockmaster has a hemispheric response: it indicates shocks from all directions radial to the unit, axial shocks to the base from beneath, and shocks from all directions between these planes. It will not register axial shocks applied to the dome from above. Models are also available with axial (base) response only. Two Shockmasters are usually sufficient for even the largest of packaged items, and one may be adequate if there is no possibility of the equipment being shocked from above.\nIs the Shockmaster reusable?\nYes. Shockmasters can be used over and over again. After tripping, the indicator is reset by removing the screw from the threaded hole in the side of the dome, inserting a stiff wire or straightened paper clip in the hole and pushing the black sleeve back to its armed position. For Shockmasters with ratings of 25 Gs or more, a plunger in the dome is depressed while the sleeve is pushed back as far as possible. The plunger is then released and the sleeve is allowed to return gently to its armed position.\nHow accurate is the Shockmaster?\nStandard Shockmasters trip within ±10% of their settings. On units set for 10 Gs or higher, tolerances of ±5% are available at an extra cost. They operate from -80°F to 160°F and are resistant to high humidity.\nWhat is the operation temperature of the Shockmaster?\nThey operate from -80°F to 160°F and are resistant to high humidity.\nIs the Shockmaster tamper-proof?\nYes. To prevent tampering with the indicator after it has been tripped, a safety wire with an inspector’s seal attached can be placed through the resetting hole screws in the plastic dome. Or, inspector’s decals may be affixed over the reset holes. Shockmasters with ratings of 25 Gs or more are made tamper-proof by re-inserting one screw, threading safety wire through holes in plunger and screw head and securing with an inspector’s seal.\nHow can Shockmaster help cut shipping and handling costs?\nIn three ways: (1) Shockmasters can be used to detect rough handling of a product on assembly lines or in other areas of your plant. Corrective measures may then be initiated to improve material handling procedures. (2) The Shockmaster is an invaluable tool for shock tests to determine proper packaging design and materials. A row of indicators—each set for tripping at a different G-rating— may be used in container drop tests, thereby detecting both overpackaging and underpackaging of a product. (3) Terminal inspection time can be reduced substantially, as only those shipments with tripped indicators need be reinspected in detail.\nHow are Shockmasters installed?\nShockmasters may be mounted on the inside of the container or directly on the packaged item.. There are four mounting holes in the base for rear mounting, or a mounting plate is available for front mounting of the units.\nCan Shockmasters be inspected without opening the container?\nInspection may be accomplished by looking at the indicator through a hole with a removable cover in the container. Or, a Shockmaster with electrical contacts may be used. This model can be inspected by placing the leads of an ohmmeter, or a battery and light bulb, on the contacts of a terminal board mounted on the outside of the container. If the Shockmaster is tripped, the ohmmeter will indicate that the circuit is closed, or the light bulb will glow.\nWhat Shockmaster settings are available?\nShockmasters are available from stock in nominal ratings of 5, 10, 20, 25, 30, 35, 40 and 50 Gs. Other settings are available upon request. The standard equal-G models respond equally to shocks along the longitudinal axis and in the transverse plane. The standard long axis models respond to longitudinal axis shock only—side or transverse shocks have no effect.\nWill your Shock indicators prevent damage to my product?\nNo. Our products only monitor shock impact. However, one of the primary goals of using AGM’s shock impact indicator products is to build awareness in those individuals and companies handling your products. The result should be better handling or improved ability to prove mishandling; either way, the end result will be a reduction in damage to your products.', 'product design student Ari Jónsson has combined red algae powder with water to create a biodegradable bottle.\nJónsson – who studies at the Iceland Academy of the Arts – exhibited the project during this year’s Reykjavik design festival DesignMarch from 10 to 13 March 2016.\nAfter reading about the amount of waste plastic produced every day, the designer felt an “urgent” need to develop a replacement material.\n“I read that 50 per cent of plastic is used once and then thrown away so I feel there is an urgent need to find ways to replace some of the unreal amount of plastic we make, use and throw away each day,” Jónsson told Dezeen. “Why are we using materials that take hundreds of years to break down in nature to drink from once and then throw away?”\nHe began studying the strengths and weaknesses of different materials to determine what could be suitable for use as a water bottle. Eventually he came across a powdered form of agar, a substance made from algae.\nWhen agar powder is added to water, it forms a jelly-like material. After experimenting to find the right proportions, Jónsson slowly heated the substance before pouring it into a bottle-shaped mould that had been kept in the freezer.\nWhat Does the Future Hold for Plastics?\nEven though future plastics are workhorse material in most industries yet they have made a\nbad name for themselves for two reasons. One, it is made from petroleum materials and\nsecondly, they litter the environment. However, the volatile nature of oil price has\ninstigated a clamor for an increase in research into bioplastics. A key driver of the\npackaging industry for future plastics currently is the replacement of oil-based plastics to a renewable\nsource due to the depletion of oil.\nBioplastics currently account for around 1% of total world consumption of plastics.\nDespite the low figure, the need and request for renewable, non-composite, durable and\nrecyclable product is increasing by the day. All this coupled with the fluctuation of oil\nprice and interest in social responsibility is taking research from the drawing board into\nthe laboratories and an expected mass production of bioplastics in the future.\nOne issue that has affected the spiral growth of bioplastics is the fear of individuals on\nthe price of food. Many bioplastics rely on food to make ethanol, this may end up driving\nup the price of food. The masses are afraid that using food crops for bioplastics might\ntake the food out of their mouth as they cannot match the financial superiority of the\ncompanies. However, experts are countering this claim neglecting that perception is not\nThe future plastics is hinged heavily on bioplastics. Even the automobile industry is\nmoving away from non-renewable sources and the packaging industry will never be left\nFood packaging materials:\nEvery piece of food today is packed in plastic packaging. The bad thing is the packaging is not biodegradable. Many people throw it away, but microorganisms cannot dismantle them.\nThose food packages destroy nature. For example, in the world there is not one single island that has no plastic rubbish on the beach. Even if there are no humans living there, the rubbish of food packaging and other plastics is still there.\nIt is easy to avoid these problems. Use reusable bags if you go shopping for example. Do not throw pieces of plastic packaging away into nature or invest in biodegradable plastic because this material will not destroy the future of our next generations. There is a new invention; it is like a water bubble filled with water. The inventors, young adults, say that this is the future of water bottles because you only have to bite into the bubble and suck the water out of it. The rest of thebubble is edible because it is bioplastic out of seaweed. And because it is edible it is also biodegradable, you can put the bottle in the compost or give it to a center which can recycle it. Food packaging material can be out of seaweed but also out of starch, cellulose, pectin and the peel of some fruits. Only 1% from all plastic products are made out of biodegradable materials.\nTo get bioplastic, you have to mix a biopolymer with a plasticizer and some additives. Biopolymer is responsible for the stability.\nPlasticizer is for flexibility and the additives change the colors, for example. For a polymer you can use starch gelatin or agar. The most used plasticizer is glycerin. It interacts with the polymer and gives flexibility to the end product. Glycerin is used because it is very cheap and abundant. Heat all the ingredients slowly up so that they get liquid. Mix them up and let them dry for a few hours. Then you have got your own bioplastic. All in all you can say, bioplastic is manufactured out of natural materials. So if the product comes back into nature, it is ok because it is just a mixture out of natural materials. The good thing about such biodegradable plastic is that everybody can mix those ingredients at home. You can order them on the internet and use your stove to heat them up. At the end you can form the plastic in every form you want as long as it is liquid. For example, you can do cups for drinking or plates for food, some foil or other containers and boxes. The possibilities are endless. With food color you can paint the plastic as you wish. All in all this is a very good opportunity to produce useful plastic parts that you need and you cannot buy anywhere.\nEco friendly packaging:\nThe world today has to think about the world tomorrow. The next generation wants a clean planet, too.\nWe cannot pollute our environment with rubbish like food packaging. The majority of food packaging hurts nature or destroys the environment. This is an important point because 10% of all plastic produced ends as rubbish in the sea. Big parts are crushed into small pieces -> microplastic. Those particles are eaten by fish and these fish are eaten by humans so the plastic gets into the eco system and destroys it slowly.\nLots of animal species have already died out because of sea rubbish. Sea turtles for example eat plastic bags because those bags look similar to jelly fish, which is the natural food of the turtles. But if they eat those bags, they die slowly and painfully. If the plastic was biodegradable, this wouldn’t be a problem. Animals could digest it. Also the biggest animals in the world have problems with plastic.\nWhales are eating it, too. Especially the micro particles of plastic are problematic. Many whales are eating krill by filtering it through their whalebone. With the krill some plastic also gets into the mouth and the stomach of the animal. It takes a little bit longer, but also a 35m long animal like a whale can be seriously affected by a big enough amount of micro plastic.\nBiodegradable materials are plastics, which enzymes can dismantle into small natural pieces, like CO2and H2O.\nMost biodegradable materials come from nature and are capable of being returned to it in a way that the natural environment is notharmed. 35% of all plastics produced are ‘’one-time-users’’. After use, people throw them away. That is very bad- it is important to use biodegradable materials for packaging.\nOrganisms, or their enzymes, can dismantle them without problems, because they already know those natural materials. They do not die from metabolizing these materials. For example you can produce biodegradable materials from bananas. You have to mix it with a binder and heat it up. After cooling down you get a brown black colored substance. With this material you can form a plant cup for seedlings, for example. Lots of garden centers are already buying those plastic cups and sell them with the plants inside. After buying you can immediately put them into the earth. The cup will be decomposed after a while and is an excellent fertilizer for your garden.\nA biodegradable plastic is considered as such, if microorganisms (decomposers) can\ndismantle them. The plastic has to change its characteristics and chemical structure. Mostly\nthe plastic gets dismantled into CO 2 , H 2 O and biomass- those materials are considered as\nnatural ones and do not pollute the environment.\nPlastics are very important in our world today. They are used everywhere and the\nmanufacturing is cheap. We need plastics in our life. Normal plastic is made out of oil that\noccurs deep in the earth under the sea. The problem is that some additives of this plastic are\nsimilar to natural hormones- they act like softeners. Other additives are hardeners, colorants\nand balancers. The enzymes in organisms process them.\nThe consequences for sea animals have not been fully researched, but many species die\npainfully because of micro plastics in the sea. For example, sea turtles eat plastic bags, which\nare swimming around in the water. Normally sea turtles eat jelly fish. Those fish are white\nand transparent so they seem similar to the plastic bags. Animals cannot distinguish\nbetween rubbish and food so they eat it without thinking about it. Some small fish are eating\nthe small plastic parts on the water and those fish are eaten by us humans. So the plastic\ngets back into our body and damages it slowly. These problems can be prevented by\nEco packaging a new sustainable way to package goods in a safe and environmentally friendly way, the world is now becoming more environmentally friendly by the day the next step is to develop eco packaging that can be recycled this help to sustain a good healthy environments for the generations to come.\nThere are businesses supporting the innovation of eco packaging for example biopac technology believe that this is the sustainable packaging solution. Eco packaging can be many different things for example many coffee outlets, festivals and outdoor events now have eco-friendly drink cups these are a FSC board lined starch material these cups are 100% compostable meaning they will not harm the environment\nEco packaging saves large amounts of emission from reaching the atmosphere for example over a year we can save 4,908,885kg of carbon this can result in the greenhouse gas levels slowly lowering over time and helping to improve the global warming crisis.\nProblems & Solutions\nHousehold products are a large portion of the contribution to landfill waste, for example, crisp bags and toothpaste tube are two of the hardest products to recycle due to the separation of materials within the product themselves. Crisp packets, made from seven layers of foil and plastic made this way to reduce shipping volume. A hard ineffective way to help recycle the market for recycling crisp bags is not cost effective and there is no market for it. A solution to this is that although it is not quick fix if company’s and businesses decide to go forward fewer layers or metal and plastic will be fused together this will reduce time and cost of the separation and recycling of the crisps packets.']	['<urn:uuid:2f998b29-cdf7-452a-b9d5-21d3a27c36d7>', '<urn:uuid:1a5869e1-abba-4884-b552-e6bbedeb0a25>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	35	140	2708
43	How do different wildlife protection approaches compare between rural and urban settings, and what role do natural barriers play in both contexts?	In rural settings, natural barriers like living snow fences, including tree windbreaks and shrub rows, serve multiple purposes by protecting roads while also enhancing wildlife habitat. These barriers can be particularly beneficial for grassland nesting birds, requiring a minimum width of 150 feet to prevent birds from being buried by snow. In urban settings, conservation takes different forms through parks, green roofs, community gardens, and unmanaged vacant lots. While cities only cover 3.5% of land in the U.S., they house 63% of the population, making urban conservation crucial. These urban green spaces create connected habitats for wildlife, increase biodiversity, and provide pathways for foraging and migratory activities. Both approaches contribute to a larger conservation network that helps wildlife move between protected areas and survive in different environments.	"[""Road and ditch design\nCorrectly designed ditches can catch snow before it blows across roadways. General rules for designing drift-free roads include:\n- Minimum distance from the edge of pavement to the toe of the backslope should be 46 feet\n- Ditches should be at least 4 feet deep\n- Distance from the edge of pavement to the top of the backslope should be based on the equation:\nStructural snow fence design\nStructural snow fencing is best suited for sites that are not conducive for tree and shrub plantings. The following factors may preclude the use of living snow fences:\n- Herbicide concerns\n- Unavoidable tile lines\n- Soil pH above 8.0\n- Soil salinity and/or salt spray\n- Soil types that are too compacted, wet, dry, and or rocky to permit normal root development\n- Presence of deer wintering area that raises the threat that a living snow fence will be browsed to the point that the fences does not reach the required height.\nStructural snow fencing uses a synthetic rail and wood or steel posts to create the fence. A polyethylene material with bonded cables inside to create the synthetic rail. Posts are spaced according to the height of the fences, but are typically 10-15 feet apart. No additional clearance is needed on the sides of the fence. Due to the material costs, labor associated with constructing this type of snow fence, and that they are left standing year-round, MnDOT typically acquires an easement or enters into an agreement with the landowner.\nLiving snow fence design\nPlantings of trees, shrubs and native grasses located along roads or around communities and farmsteads. Properly designed and placed, these living barriers trap snow as it blows across fields, piling it up before it reaches a road, waterway, farmstead or community.\nTwin shrub row\nShrubs are smaller than trees and can tolerate the drier conditions of western Minnesota and they also do not displace grassland nesting birds. Shrubs can trap lots of snow. Depending on soil types, drainage, and climate conditions, wetter fields in the spring may be expected. Certain crops such as alfalfa and fall planted small grains may actually benefit from the snow's ability to insulate and protect these crops through the winter. In most locations this design will enhance wildlife habitat. Twin shrub rows are typically planted using a geotextile fabric and pea rock as a weed barrier. Wood mulch has not proven to be as effective as the fabric/rock method.\nThe community shelterbelt design decreases the wind speed and causes the blowing snow to accumulate in front of and within the shelterbelt in a manner that prevents huge snow drifts from inundating homes, businesses, and roads.\nDeciduous tree windbreak\nA deciduous tree windbreak uniformly distributes the snow across the field to replace soil moisture without delaying spring time planting. It also helps to control top soil wind erosion.\nGrassland nesting bird component\nTo enhance grassland nesting bird habitat, seed native grasses within the downwind snow storage area and beyond. The grass component should be a minimum width of 150 feet. If the strip is too narrow (less than 100 feet), rousting birds may be buried by the snow.\nStanding corn rows\nMnDOT annually purchases standing corn rows from area farmers to serve as snow fences adjacent to sections of highways that have a history of blowing and drifting snow. Corn needs to be planted parallel to the road to serve as a fence. A typical stand corn row snow fence is one-quarter-mile long and 16 rows wide covering an average of 1.2 acres. The fence is set back 120 to 240 feet from the highway right-of-way."", 'According to a 2020 study in Scientific Reports, large preserved areas may not have the best outcomes for wildlife protection. The study found that in 80 percent of protected regions, the protection provided to endangered animals was equal to or worse than if the areas had been randomly located.\nMany people think of conservation as best embodied by large national parks like Yellowstone and Yosemite. Much of our conservation efforts at the federal level have focused on designating large areas as protected lands to provide both habitat for wildlife and to conserve natural resources. But for conservation to be effective, we will also have to find innovative ways to protect wildlife on non-federal lands, and to connect islands of protected areas so that wildlife can move between them.\nPolicy tools can help to connect our current protected areas and even make way for wildlife in developed areas. Two promising options for policymakers to consider are exploring conservation in urban spaces and establishing wildlife corridors. These approaches can help create a mosaic of conservation that connects wildlife to islands of protected habitats.\nUrban conservation provides habitat in developed areas\nEmma Marris discusses the key misconception about nature in her 2011 book, Rambunctious Garden. Nature is not found only in large, untouched areas but also in populated areas dotted with parks and other bright spots of biodiversity. Marris highlights the growing urban population, showing that cities only cover 3.5% of land in the U.S. but house 63% of the U.S. population. By changing the way we view urban spaces and utilizing the small pockets of nature found in cities, we can create a more connected habitat for wildlife that live in and around cities.\nUrban parks and green spaces in cities can be used to help achieve the goals of conservation. These natural spaces can be integrated into a city to create areas furthering conservation efforts. Cities without open space for parks can transform retired industrial facilities and empty lots into green spaces. Other examples of green spaces include green roofs (or rooftop gardens), community gardens, and unmanaged vacant lots.\nThe Duwamish River cleanup is a prime example of urban conservation and part of the EPA’s Urban Water Partnership program. The partnership has identified 20 rivers and watersheds across the United States in need of revitalization. Its goal is to revitalize the communities surrounding the area and improve the quality of the habitat provided by the rivers. Following the degradation of one of Boeing’s facilities and the subsequent release of harmful chemicals into the watershed, Boeing began to aid the clean-up project. The clean-up, primarily focused on the removal of polychlorinated biphenyls (PCBs), has the added benefit of restoring habitat for wildlife that relies on the health of the watershed. Projects like this one show how improving the habitat already located in our cities can create a more diverse and sustainable ecosystem.\nAnother example of an organization working to get good conservation outcomes from urban spaces is the Conservation Fund. The fund partners with cities to improve urban conservation efforts. They work to develop parks, improve water management, and develop green infrastructure. The group partnered with local organizations and state officials in Florida to ensure the continued health of the Loxahatchee River Watershed. The fund ensured that urban development around the area maintained the habitats of the local wildlife. By partnering with local governments, organizations like the Conservation Fund can increase the importance of conservation in the development of cities.\nThe pockets of nature found in urban areas provide habitat for a multitude of animals and insects and help to connect different habitats. A 2014 study in the Ecological Society of America found that rooftop gardens increased habitat connectivity of mobile species, like bees, and could lead to increased connectivity of habitats on the ground. A connected urban green landscape provides pathways that increase genetic diversity in wildlife and aid in foraging and migratory activities. As urban development continues, including green spaces in our urban planning will increase the biodiversity of our cities and provide better habitats for the species that share them with us.\nWildlife corridors help connect habitat\nValuable habitat for species is often fragmented, leaving wildlife with no way to travel between areas they depend on for their survival. This makes connecting existing habitats more important. Wildlife corridors are the paths wildlife use to cross between habitats. Protecting existing wildlife corridors and establishing new ones provides safe ways for animals to migrate between existing habitats. A connected landscape also makes biodiversity more resilient to the effects of climate change.\nWildlife corridors can take many forms. Some examples are local corridors, regional corridors, flyways, rivers, and urban greenways. Although these corridors may seem challenging to implement, land use practices can be adjusted to augment existing landscape elements that already contribute to wildlife corridors. For example, strips along roadways can be planted with specific species to help provide links between patches of useful habitat. Protected wetlands can provide important stopping points for migratory bird species. Buffers can also be created around areas with important habitat to help species reach them.\nIn Nevada, a man-made wildlife corridor provides safe crossings for many animals. Some of the animals utilizing the crossing include elk, bobcats, and mountain lions. Over four years, 35,000 mule deer successfully crossed the wildlife corridor. Wildlife corridors not only protect animals, but also humans. Vehicle collisions with wildlife cost an average of $4,135 per collision – a total of over $6 billion annually in the United States. The installation of wildlife fencing and crossing sites has led to a 97% decline in collisions between cars and large wildlife along parts of the Trans-Canada highway. In Oregon, the total number of vehicle collisions with wildlife in 2020 was 5,997. If even 60% of these collisions were prevented by the implementation of crossing sites and fencing, the total cost of wildlife collision accidents would decrease from about $25 million to $10 million.\nWildlife corridors can also be used in conjunction with wildlife refuges which help to provide animals with areas to migrate to and mate. For ducks and geese, wildlife refuges in the heartland of the United States “serve as stepping stones to help birds weather their journeys.” These refuges offer safe spaces for birds to find food and water before they continue their migratory journey.\nWildlife corridors can also improve urban conservation efforts. Urban greenways connect open areas across cities. They provide both recreation for humans and connected areas for wildlife to travel. In Raleigh, North Carolina, 3,700 acres are connected by the Capital Area Greenway. The greenway provides 117 miles of trails for both humans and wildlife. Of course, the benefits of such projects must be weighed against their cost to ensure we are making investments in habitat connectivity that will truly pay off.\nLarge protected areas are islands of conservation\nUrban conservation and wildlife corridors can help us to connect islands of important habitat across the country, including federal lands. Most federal conservation efforts, like the establishment and management of national parks, focus on large areas of protected lands. This approach creates large chunks of nature that may not provide wildlife with the habitat they need to thrive or the ability to travel between them. Parks also face the challenge of prioritizing not just conservation but also recreation.\nThe United States has 423 national parks that span 84 million acres. Despite their protected status, the primary purpose of national parks is not to conserve wildlife. The mission of the National Park Service emphasizes the preservation of unimpaired natural and cultural resources “for the enjoyment, education, and inspiration of this and future generations.” This mission describes the NPS’ dual focus on both preservation of natural resources and recreation in our national parks.\nTwo key challenges national parks face in the conservation of habitat are development of the surrounding areas and increasing numbers of visitors annually. The areas surrounding national parks are often developed for agriculture, mining, and housing. That development may trap the species within the parks by cutting them off from their habitat. Increasing numbers of visitors also make maintaining the habitats of the animals living in the parks more difficult. Park rangers are unable to minimize the impact visitors have on the degradation of habitat. As the number of visitors increases, habitat degradation also increases.\nConnecting conservation islands\nEstablishing urban green spaces and a nationwide wildlife corridor system would help make America’s conservation efforts more successful in protecting wildlife and conserving natural habitats. By integrating urban green spaces with new urban planning, we can help achieve better habitat connectivity.\nThese ideas are also gaining traction in policy discussions. The Build Back Better bill currently under review in the Senate designates $10 million to map wildlife corridors and provide monetary assistance for states working to implement and maintain corridors. The Wildlife Corridors Conservation Act would go even further by creating a National Wildlife Corridors System and stewardship fund for citizens to donate to its upkeep. The bill in its entirety has not yet been reviewed by the Senate. The Act would use federal lands and waters to create the system. Twelve states, including Utah, have already begun to implement their own policies to protect existing wildlife corridors.\nConnecting protected areas and creating safe ways for wildlife to migrate will be key in allowing humans to coexist with nature. Urban conservation and wildlife corridors can help us achieve better conservation outcomes. These tools improve existing conservation efforts and create new ways for us to interact with wildlife and nature in our daily lives.']"	['<urn:uuid:aebf22a6-e917-4477-a82e-7ddd9a4db86c>', '<urn:uuid:4b9fa075-28ca-4e4a-8a8d-8e7c812214b7>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T17:51:21.175311	22	127	2201
44	what main difference communication storytelling ancient tribes versus modern society	In oral cultures (ancient tribes), nobody is credited with creating stories as they belong to everybody, and the storyteller simply performs them for the group. In contrast, in literate society, knowledge is fixed in print, creating the concept of authorship where written material has a separate existence from its creator.	['Is our society becoming post-literate?\nThousands of years have passed since our culture invented an alphabet to allow spoken words to be permanently recorded. This ‘great leap’ from orality to literacy had many consequences that will be discussed here. However, many other technologies have come into existence since the alphabet was invented and it has been suggested that we have moved beyond a stage of basic literacy into a new kind of ‘post-literacy’ or ‘secondary orality’ (Ong 1982), brought about by these new technologies. This essay will look at the differences between an oral culture and a literate one, describe the effects of literacy upon society, and look at technological breakthroughs, such as the Gutenberg press and more modern inventions such as television, telephone and computers, to see whether we are entering a new era in our progression from oral communication. I will try to examine if this supposed post-literacy, created by new means of communication, is a new stage in our development with profound effects on the structure of our society and look at how different life is with modern technology than life with simple literacy.\nI will start by comparing orality and literacy to illustrate the deep implications of each. An oral culture is one in which all communication is by talking and listening. The fact that there is no means of writing anything down means that all values and morals of the given society are stored in the minds of the people. As cultural knowledge is so deeply embedded into stories and ritual the concept of knowledge actually existing as a separate entity is non-existent in an oral society. Walter Ong (1982) says that in these kinds of societies knowledge is performed through the telling of stories and the carrying out of rituals. There is no separation of the ‘knower from the known’ (Havelock 1976). The myths and folktales of the village storyteller do not have a script, this would be an oral version of literacy, they are recreated anew with each performance. There simply is no ‘text’ apart from each individual incarnation of each tale. The performer of a tale is combining an act of creation with an act of transmission. His primary work is to transmit the culture of the tribe, and in this act of transmission he must be conservative as changes in oral knowledge cannot be undone, for there are no old copies to go back to. Over time, however, subtle differences in the plot can be detected, a process Ong calls ‘homeostasis’ (Ong 1982).\nThis homeostasis comes about because of the nature of the way cultural knowledge is communicated. When a storyteller stands in front of a group of people he doesn’t stand and give a lecture, he tells a story using different words each time it is retold. There is a large degree of interaction between the storyteller and the people listening to what is being said. Gestures and the use of tone emphasise certain parts of the story. If the values that are held in high regard by the culture shift to suit changing circumstances, the heroes in the tale will acquire new characteristics or even cease to be heroes. In oral societies individual creativity is profoundly rhetorical, for it is the subtle interplay between teller and audience that shapes the tales to match the values of that audience.\nAlthough there have been pictographic alphabets for many thousands of years, in ancient Egypt and the Mayans in Meso-America, these kinds of alphabets use symbols to represent things, people and events and many hundreds of pictures are needed to record a lot of information. It was the invention of the Greek alphabet in the fifth century BC that marks the beginnings of literate society in Europe. This is because the Greek alphabet uses twenty six meaningless symbols to reproduce the sounds of words on a page, meaning anything that can be spoken can be easily written down and recorded forever. According to some observers (Mcluhan 1962, Havelock 1976 and Ong 1982) the invention of the phonetic alphabet has incredibly significant effects on the creation and communication of knowledge in the society that possesses this tool. As things can now be written down and recorded forever the concept of authorship arises.\nIn oral cultures nobody is credited with the creation of stories, they belong to everybody and the storyteller is simply reperforming them for the group. In a literate society, however, the fixation of knowledge by print causes a separation of the ‘knower from the known’ (Havelock 1976) and gives the printed material a separate existence apart from the mind of the individual who created it. It is this abstraction of knowledge which is the most significant effect of literacy upon society. Literacy allowed the creation of new knowledge and ideas and is thus credited with the creation of academia by allowing things to be studied in depth. In literate societies the idea of interaction between the creator and audience is removed and knowledge is now created for its own sake, not to please the group. Literacy enabled a profound shift in human conscious, bringing about the linear, abstract forms of Western logic that we take for granted today but which were simply unthinkable without literacy as a means of preserving complicated original thought.\nFor Marshall Mcluhan, however, it was not simply the invention of the phonetic alphabet that was the major agent of change, it was the invention of the Gutenberg Press. Before the Gutenberg Press was invented manuscripts were hand-written by monks and read aloud to an audience of listeners. To Mcluhan this was still representative of an oral culture. The printing press, however, significantly changed this. As books could now be mass produced and distributed at speed they were widely available to the masses and not just an educated elite of readers. It was the printing press that finally ‘split apart thought and action’ (Mcluhan 1962, 22). Now that everybody had access to books an increase in silent reading took place which Mcluhan claims linearized our thought processes and offered the final separation of author and ownership. Transmission was now a mechanical act, performed by a machine. Originality, once a deadly danger to a society that had to struggle to maintain its equilibrium, could now be seen as more valuable than ownership.\nNow that the printing press had been invented more and more documents were credited to a single source, the writer. The concept of authorship is so strong in our society that plagiarism is now an academic crime of heinous proportions and copyright laws were created to allow authors to be protected in law. In the Miller vs. Taylor decision of 1767 Mr Justice Aston commented, ‘I do not know, nor can I comprehend any property more emphatically a man’s own, nay, more incapable of being mistaken, than his literary works’ (Patterson 1968, 170). Typography has made the word a commodity. The old communal oral world has been split up into privately claimed freeholdings, ‘the drift toward greater individuality had been served well by print’ (Ong 1982, 131). It is literate, and especially print, that has created the notion of the self. In most oral societies there is absolutely no notion whatsoever of people existing as individuals with free will. In oral cultures decisions are made not on the basis of ‘what should I do? or ‘what is good for me? but ‘what do we need?\nMcluhan uses the metaphor of hot and cool to describe the various mediums of knowledge transmission. A hot medium is ‘one that extends a single sense with high definition. High definition means a complete filling in of data by the medium without intense audience participation…In a cool medium, the audience is an active constituent of the viewing.’ (Mcluhan, in Playboy Magazine, 1969). He uses this metaphor to explain why oral and literate cultures are so inherently different. In an oral culture, which Mcluhan would define as being ‘cool’, there is an intense interaction between the orator and the audience. As described above the audience actively participates in the creation process. Writing, then, for Mcluhan is a hot medium as there is very high definition of content and the reader is left to fill in very little.\nIt is approximately five hundred years now since the printing press was invented and all of these changes started to occur. Since then a whole array of new technological innovations have provided mankind with even more mediums through which to communicate information. The telephone, television and most recently computers and the Internet have made instantaneous global transmission of data a very real possibility. Mcluhan sees these new technologies as the agents of a process of ‘retribalization’ (Mcluhan 1962). The advent of the Gutenberg press started what Mcluhan terms ‘the detribalizing of man’ (Mcluhan (1962). The printing press made us all subordinate to the power of the written word. Mcluhan believes that the human experience is made up of an interplay between the five senses.\nLiteracy placed a great emphasis on the visual aspect of life and removed man from tribal communality into a state of civilised detachment. These new technologies have placed more emphasis on our other senses and have extended our nervous system to all corners of the world. Television, although we may at first think as a visual medium is actually described by Mcluhan as an extension of our tactile sense and it is this ‘that demands the greatest interplay of all the senses’ (Mcluhan, in Playboy Magazine, 1969). It was Mcluhan who first coined the now very famous term to describe the interconnectedness of the modern world, ‘the global village’ (Mcluhan 1964). This results from the network-like extension of new electronic media that ‘is enabling man to incorporate within himself the whole of mankind. The aloof and dissociated role of the literate man of the Western world is succumbing to the new, intense depth participation engendered by the electronic media and bringing us back in touch with our selves as well as one another’ (Mcluhan, in Playboy Magazine, 1969).\nMcluhan is so adamant that we have moved into a new era of human existence that in his interview in Playboy Magazine he says that the Gutenberg Galaxy, formed by the spread of print-led communication ‘is being eclipsed by the constellation of Marconi ‘ (Mcluhan, in Playboy Magazine, 1969). Under the effects of participatory electronic media, Mcluhan suggests that linear typographic man will again learn to ‘live mythically’ (Mcluhan 1964). The concept of ‘living mythically’ suggests far more than simply being interconnected, of being able to send messages to each other more quickly and easily. It means living in a form of consciousness in which knowledge does not exist outside the knower, embodied in a physical text, but instead is lived dramatically, communally performed as the myths of oral man were performed.\nI believe a lot of what Mcluhan says is true. Defining the present and immediate future as the time of Marconi Man I think fairly describes the modern world. I think however his metaphors of ‘retribalization’ and ‘living mythically’ are slightly over the top but I do think that electronic media has, and will even further with the expansion of the internet, profoundly change the way we live and work. We are constantly being told how ‘the information revolution’ will extend its reach into every corner of our lives. Mcluhan is not the only writer who has compared today’s society with our oral past. Walter Ong describes a ‘secondary orality’ with a greater emphasis on a ‘participatory mystique, communal sense and the present moment’ (Ong 1982, 136). He does, however, concede that it is not going to be a total return to tribal existence but ‘a more deliberate and self-conscious orality’ (Ong 1982, 136). Hypertext, the method of publishing print online, may be seen as redefining the concept of authorship as ‘each reader takes a different physical path from node to node and thus metaphorically rewrites the text in the process of reading it ‘ (Slatin 1990). However, there is something glaringly obvious when talking about a return to a ‘secondary orality’ – text is still text, electronic or not. It is important not to see the current change as a circular movement back to a stage of orality but another ‘advancement’ into the future, although it is useful, as I have found in writing this essay, to use metaphors of primary orality to explain the change.\nSo, to conclude I would answer that, yes, we are becoming a ‘post-literate’ society, especially if you define a literate society as one whose primary means of communicating knowledge is the written document. If this is the definition then it is obvious that the written word is no longer the only way we communicate knowledge. In this essay I have been talking as if the spoken word is completely discarded once a phonetic alphabet is created but this is definitely not the case. The ‘grapevine’ and ‘word-of-mouth’ are terms still very much in use today, indicating that information is often communicated by spoken word. Children will always be taught how to read by a teacher in the classroom and who would think of letting a computer read a child a bedtime story. The upper classes of our society take great delight in going to a theatre with friends to watch a performance of plays that can communicate the meanings and values of our society, all the more emphatically because they are live. Reading, then, is always going to be with us, even if we do use other methods of communication to a greater extent. It is quite ironic that one of the biggest areas of ‘e-commerce’ is the sale of traditional paper books online.\nOng, W.J. (1982). Orality and Literacy: the technologizing of the word. London: Methuen\nMcluhan, M. (1962). The Gutenberg Galaxy: The making of typographic man. Toronto: Toronto University Press\nMcluhan, M. (1964). Understanding Media: the extensions of man. Toronto: Toronto University Press\nSlatin, J.M. (1990). Reading Hypertext: order and coherence in anew medium. College English, 52, 870-883\nPatterson, L.R. (1968). Copyright in historical perspective. Nashville: Vanderbilt University Press\nHavelock, E. (1976). Origins of Western Literacy. Toronto: Ontario Institute for Studies in Education\nNorden, E. (1969). Interview with Marshall Mcluhan, in Playboy Magazine.']	['<urn:uuid:51b940bc-8ffc-4561-881b-fdf0e5bec135>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	10	50	2380
45	how does roleplaying differentiate between substitute actions and abstract mechanics	In roleplaying, there are distinct levels of game mechanics. Replacement rules are when something in the fiction is substituted with something else - for example, using a boffer sword to replace a real sword, or simulating sex with neck massage. In these cases, the player and character experience the same action differently. Abstractions, on the other hand, are when the technique itself exists within the fictional world - for example, when latex weapons are considered deadly within the game world or when Ars Amandi replaces sex entirely. In abstractions, both the character and player have the same experience.	"[""From Nordic Larp Wiki\n(Redirected from Meta-techniques)\nA meta-technique is a loose term encapsulating various rules and narrative tools/practices which are carried out by players rather than characters but still part of the improvisational flow, such as the meta room, the monologue, and shadows. The term started gaining in popularity and use in 2007, during the preparatory work with En stilla middag med familjen, and in talks and workshops the following year at Solmukohta 2008.\nThe exact definition of meta-techniques has been hotly debated, the usage varies in scope and clarity across the board. In the broadest usage of the word it covers nearly any game mechanic in a Nordic Larp, while another usage put meta-techniques as a direct opposite to game mechanics. One clearer definition of meta-techniques states:\nCommunication in the game - between the players and not the characters or additional story that doesn’t fit in to the game time and space continuum\nThe defining meta part comes from the way in which the techniques willfully breaks the flow of the direct immersion to provide information directly to players, without going through their characters. What constitutes a break in immersion is one of the more central disputed areas.\nThree level model\nOne usage divides game mechanics into three levels: Replacement rules, abstractions and meta-techniques.\nReplacement rules are when a concept in the fiction is replaced with or simulated by something else. I.e. a boffer sword replaces a real sword. Sex is simulated by neck-massage. The player and the character experiences the same thing differently.\nAbstractions are when the metatechnique itself is immanent in the fictional world. I.e. In this world latex is deadly and boffers kill or there is no sex, but Ars Amandi fulfills the same functions. In this, the experience of the character and the player are the same.\nMeta techniques in this definition are not experienced by the characters, only between players. It is an event taking place completely outside the continuity of play.\nIntrusive versus discrete meta-techniques\nThe off-game character of metatechniques means that they can easily interrupt play. Some are very easy to hide or can be used away from other players, some are performed by one or more players during the game and some require the entire game to stop while they are implemented.\nThe same technique can even be used across the whole spectrum, to use the monologue technique as an example.\nA 'discrete way to use a monologue would be that the players involved to go to the meta-room when using the technique. This means that only the ones that choose to use it will see the effect and if they don't want meta in their larp, they don't have to seeing.\nDiscrete techniques are often used in closed spaces and managed to not not interfere to much to a 360° illusion\nIf a monologue would be used as an intrusive technique it could as an example be in for of the ping the glass method. The method works out in a way that you request a player to express the inner thoughts of their character by pinging or touching a glass they are holding in their hand as in the larp New Voices in Art. The player then starts holding a monologue making everyone close by listen and noticing what is going on. It is hard for a player to choose not be affected by the action of an intrusive technique because it is something that you almost instantly would notice.\nIntrusive techniques can make big gaps in a 360° illusion.\nPurpose of meta-techniques\nThe reasons for using meta-techniques is most often to provide players with information they would not be able to gather while playing their characters. This could for example be information provided by other players about their characters, directions from a gamemaster or additional story outside the game's time and space. The ultimate goal of the this is commonly to increase the emotional energy of play and increase the dramatic potential.\nMeta techniques and player preference\nMeta techniques are a hallmark of tabletop play which requires narration and abstraction for the in-game events and elements to be represented. Play styles derived from larp often encourage greater resistance to the use of meta-techniques since they may provide a break from and a contrast with the experienced story. Play derived from tabletop play can be more robust and fluid in the presence of meta-techniques. Player preference may be related to common or first exposure.\nMeta techniques used to increase the drama of a larp are sometime called Dramachanics which is short for Dramatic game mechanics.\nPresentation about Meta techniques and the use of them through The Mixing Desk of Larp by Petter Karlsson at the Larpwriter Summer School 2013. The first few minutes is a presentation and demonstration about what a meta-technique is.\nA technique that give more information about the story events to the players than they get through the experiences of their characters.\nCommunication in the larp between the participants when the same thing is not communicated between their characters.""]"	['<urn:uuid:c731075a-ac76-4286-ae13-92de538bc4d3>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	10	98	844
46	Who discovered thorium and why was it named that way?	Thorium was discovered by Swedish scientist Jakob Berzelius (1779-1848), who named it after Thor, the mythical Nordic God of Thunder.	['Thorium was discovered nearly 200 hundred years ago by the Swedish scientist Jakob Berzelius (1779-1848), who named the new element after Thor, the mythical nordic God of Thunder. Though widespread in nature and more abundant than, for example uranium, thorium does not occur in any deposits at a high concentration. Rather, it is a frequent low-level “contaminant” in some minerals such as monazite, a cerium phosphate.\nSince its discovery, thorium has pretty much remained a non-entity in technology and commerce. Except for a few specialty applications, there really was no great use for it – until now. The anticipated change in its importance is due to its existence in many different isotopes (only a few of which are naturally occurring) and ability to deliver power via a nuclear fission process, similar to uranium, in other words, as a different kind of nuclear reactor fuel for electricity generation.\nAdvantages, Limitations, and Disadvantages of Uranium\nUranium-based reactors are basically straightforward. They require uranium-235 (235U) isotope enriched material (at 3 to 20%, depending on the type of reactor) to work. In natural ores, the 235U isotope is present at only 0.7%. In order to obtain fissionable material, the low-grade uranium needs to be enriched, using thousands of high-speed centrifuges exploiting a minute molecular weight differential of the hexafluorides between the different uranium isotopes. However, this enrichment process requires substantial energy.\nUranium-based nuclear reactors create substantial amounts of “depleted,” but still quite radioactive material. This needs to be stored in a safe facility – for millions of years – to allow the natural decay process to “dispose” of its radioactivity. The problem of long-term and safe disposal of such wastes has been around for decades, both in North America and in Europe, but no acceptable resting place has yet been found.\nMore recently, another limitation of the uranium reactor is becoming apparent. Though new deposits are being found and mined regularly, the supply of easily mineable uranium is probably not keeping up with the foreseeable demand over the next decades. In fact, these days, much of the fissionable uranium does not come from mining but from nuclear warheads built by the superpowers since WWII. This supply is expected to become exhausted in a few years.\nAdvantages of Thorium for Nuclear Power\nEarly thorium-based nuclear reactors were unsuccessful, mainly due to technical problems. Therefore, uranium-based systems were developed instead. They provide now anywhere between 25 and 50% of the North American electricity needs, depending on the state or province. Now, a revival may be on the way for thorium. The reasons are as follows.\nWith modern mining and ore-processing methods, the supply of thorium can easily be ramped up to sustain a much larger level of production. Substantial supplies are available on several continents, including North America. According to the Thorium Energy Alliance , an educational advocacy organization, “there is enough thorium in the United States alone to power the country at its current energy level for over 1,000 years.”\nThe real advantage of thorium reactors comes from the nuclear process involved. In contrast to uranium, the thorium process consumes 100% of the fissionable element versus less than 1% of the uranium process. That also means that it produces substantially less of long-lived radioactive waste material than the uranium reactor. One disadvantage with a thorium reactor is that it needs constant priming with 235U or a similar neutron source in order to sustain its nuclear process. However, this disadvantage also brings another advantage with it: A thorium reactor could not suffer a core “meltdown,” such as happened in Chernobyl. Therefore, while thorium creates additional technological hurdles, it also would be much safer all around.\nThe technological problems experienced with earlier attempts to build thorium-type reactors pertained mostly to separating low levels of thorium oxide from other materials. However, modern chemistry appears to have solved that issue.\nAnother advantage of a thorium reactor is that it is difficult to create any nuclear bomb material with it. Of course, some might view that as a drawback.\nAs of late, several countries including China, India, Japan, Russia and the United States have plans to use thorium for new nuclear reactors. In fact, India has been building an advanced design which was claimed to be ready for operation in 2011. No recent news is available, but more information and technological advances can be expected in time.\nIn the US, Britain, Canada, Germany, no new nuclear reactors have been built for several decades. Several existing plants are getting close to their design life-spans; some are already beyond their “best before date.”\nNuclear reactors currently provide a large fraction of the electricity used in North America. Many of these reactors need to be replaced in the foreseeable future. I think this is a perfect time to invest in research and development of the promising thorium-based technology for their replacement.']	['<urn:uuid:96ef77e3-bf3a-43e0-90fb-47c63d6ed2b8>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	10	20	808
47	Hey, I'm interested in the history of crafts - could you tell me why glass makers in Venice were moved to a separate island in medieval times?	In 1291, the Venetian government banned glass furnaces in Venice and relocated glassblowing to the Island of Murano. This was partly due to fears of fires, as many structures were wooden at the time. Additionally, the move was intended to isolate the master glassblowers and prevent them from sharing their valuable techniques with other European glass makers. The glassblowers were kept as virtual prisoners on the island to ensure their methods wouldn't leave Murano.	"[""|Antiques Digest||Browse Auctions||Appraisal||Home|\nSince the beginning of times man has been enchanted, and mesmerized by glass, perhaps due to its transparent nature? One might believe glass lends itself toward being magical and supernatural. For a good example, the crystal ball. It is used to gaze into, and perhaps see what the future holds... Glass has the magical means to capture your attention. The prisms in a crystal chandelier appear ice like, and shoot a rainbow of color which reflects on all objects around it. Glass is synonymous with beauty. Murano glass is certainly a good example of this type of magical glass.\nOver a thousand years ago the glass blowers in Venice Italy began producing some of the most glorious works of glass, Murano glass. Murano is a small group of islands lying on the edge of the Adriatic Sea in the lagoon of Venice, about 3,000 meters north of the larger group of islands comprising the city of Venice.\nDocumentation written by a Benedict Monk, by the name of Dominic, gives good evidence that the origin of glassblowing in Venice goes back to before the first millennium. Domenico produced phials for use in the home. Today Venetian glass production is at its pinnacle, and is world renowned for its quality and form. Today Murano glassblowers use the same technology to create their wonderful works of art.\nTourist visit daily at the Murano Factory to watch as the glassblowers create wonderful works of art glass. It may appear that time has stood still. The master glassblowers remain committed to the tradition, and turn out some of the most enchanting glass in the world. The glassblower’s pipes are made in machine shops on the island of Murano as they were from the conception of Murano Glass.\nIn 1291 the government of Venice put a banned on the glass furnaces in Venice. Hence the glassblowing was relegated to the Island of Murano. It is believed by historians that this all came about due to fear of fires, which could result from the use of the glass furnaces. Many of the structures of the time were wooden; this caused great concern among the citizens. Another train of thought was that the move would isolate the master glassblowers, and prevent them from sharing their valuable techniques in glassblowing with other European glass makers. The glassblowers were kept virtual prisoners on the island. The Venetian government did not want the glassblowers’ methods to leave the island. Murano glass was soon to become the leading source of fine glass through out Europe, and a major trade income for the Republic of Venice.\nMurano glass was well known for it being exquisite, yet utilitarian. The mirrors for instance, were intricately decorated and beautiful. The glassblowers of Murano are well known for a wide variety of glass Item's.\nToday Murano glass still holds it place in the glass industry. The island and people of Murano produce a wonderful selection of glass items. From vases, perfume bottles, to glass jewelry. It is well sought after, especially the older pieces. These older pieces can still be found. You may even find pieces on the Internet, there are many Web sites that sell Murano glass. You may want to try your luck on an on-line auction.\nIf you are lucky enough to visit Venice, don't miss going to Murano to see the glassblowers show. An easy way to reach the island is by water taxi (vaporetto).The taxis are paid for by the Murano factories, in hopes of having many tourists visit and buy their glass. When you get off the taxi at the taxi landing, you will be within close proximity to the Murano Museo Vetrario, the glass museum. This is well worth the tour; you will learn the history of Murano glass along with the techniques used by the artist. The glassblowers put on a wonderful show. You will most likely want to buy a trinket to take home. The glass ranges any where from glass bon bons for $1 to $5, to magnificent chandeliers for anywhere from $400 to $8,500. There is something for everyone, of every age. Murano glass is truly beautiful, and can add that little bit of sparkling magic to your home.\nBelow is a small sampling of estimated auction prices for Murano glass:""]"	['<urn:uuid:aa50564c-950b-4345-8037-18114c4f9d55>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	27	74	718
48	Which happened first in space exploration history - the first visit to Mercury by Mariner 10 or the first exploration of Mars by Mariner 4?	The first exploration of Mars by Mariner 4 occurred first, on July 14, 1965, while Mercury's first visitor, the Mariner 10 spacecraft, arrived later in 1974.	['My Very Easy Method Just Speeds Up Naming Planets.\nMy Very Early Morning Jam Sandwiches Usually Nauseate People.\nMon Vieux Tu M’as Jeté Sur Une Nouvelle Planète!\n– Various mnemonic devices for remembering the order of the nine planets\nThe planet, already our most diminutive neighbor, has lost around 3 miles of its 3,000 mile diameter in its life, or one tenth of one percent.This may not sound like much, but in geological terms, three miles is immensely significant.\nThis boggling fact was discovered — well, rather, confirmed — by the ongoing MESSENGER mission, which is the first long-term survey of Mercury since the Mariner 10 spacecraft became the planet’s first terrestrial visitor in 1974. After an exorbitantly complex journey, in 2008, MESSENGER (short for MErcury Surface, Space ENvironment, GEochemistry, and Ranging) revealed hitherto-unseen views of Mercury: cliffs and faults criss-crossing the planet’s ragged surface, vast lava plains, and a particular geological feature, a kind of curved cliff, called a lobate scarp.\nThese lobate scarps are witness to Mercury’s shrinkage. See, Mercury is a tiny planet mostly made up of a giant iron core, which was presumably once entirely molten. This core, as it’s cooled and solidified over the eons, has contracted the entire planet in on itself and literally buckled its surface into craggy faults — causing the aforementioned lobate scarps. This process, ostensibly, is still going on. Sure, these geological features aren’t particularly exotic-looking. But they’re the indicators of some serious change, and it’s something we should reckon with.\nOK. Space is something with which we, as a technological species, must inevitably relate. And yet, its inconceivable vastness and lack of warmth — the fact that we, with our wet eyes and spongy lungs, die instantly beyond the comfort of our atmosphere — make it difficult to work with. Certainly, we’re interested. Our scientists send probes to the planets to map, find lobate scarps, and name them; as people, we endeavor to understand our surroundings, and delight in new discoveries like these.\nAnd yet, how many among us look up at the starry night sky and think, “all this is home”? The Universe, to most, is an uncaring colossus, impersonal and frightening. The very idea of floating in space conjures up horrific images of Frank Poole drifting out to his tenebrous death in 2001: A Space Odyssey (I often nightmare upon this subject). Few of us think of space, or the Universe for that matter, as being a direct part of our experience; we are, for all intents and purposes, ideologically separate from it, our planet a warm little island in a sea of otherness, nothingness.\nThe only aspect of outer space that strikes us with any sense of familiarity is our cosmic family unit, the solar system. “Our” solar system. We learn the order of the planets in grammar school, through various absurd mnemonics, and all know bits and pieces of scientific lore about each: Mercury oppressively small, Mars close, dusty and red, Saturn with its elegant rings, and Jupiter a giant of swirling red clouds. Some of us have affinities and distastes — I, for example, find Venus horrible. We know “our” planets, see them as our fellows, as odd-coupled roommates in the neighborhood. We feel, au fond, territorial.\nWhich is why, perhaps, any changes in the established order can irk us fantastically. Take Pluto, for example. Long a beloved member of the solar system, its 2006 demotion from planet to dwarf planet ignited ire among thousands, who saw the move as needlessly draconian, as well as an affront to the harmony of our solar system. 54 members of the California state assembly proposed a resolution condemning the International Astronomical Union for “scientific heresy,” and for inciting “psychological harm to some Californians who question their place in the universe.” Of course, Pluto continued to exist, unconcerned.\nWill Mercury’s gradual shrinkage stir up our collective possessiveness of the solar system in the same way? Mercury, like Pluto, remains the same planet, and, perhaps, a few miles of lost girth won’t have the same effect on the populace. Still, I find its contraction stupidly, personally upsetting. I am, in short, like the supposed Californian, for whom incremental, nominal, or perceived changes in the solar system causes them to question their place in the universe.\nAs Pluto shrank in status, so now Mercury shrinks in stature. Is nothing sacred?', 'Nine years ago, in 2006, NASA launched the New Horizons spacecraft to the outer reaches of the Solar System in order to study the Pluto system.\nNew Horizons is the fastest spacecraft ever launched, and has traveled more than three billion miles to reach its primary target: Pluto. The flyby of the Pluto system on July 14 will complete our initial exploration of the Solar System while opening the door to an entirely new realm of mysterious small planets and planetary building blocks in the Kuiper Belt.\nReaching the Kuiper Belt, or the “third” zone of our Solar System — a region beyond the inner, rocky planets and outer gas giants — has been a priority for years, as it holds building blocks of our Solar System that have been stored in a deep freeze for billions of years.\nPluto, the largest known body in the Kuiper Belt, offers an extensive nitrogen atmosphere, complex seasons, strangely distinct surface markings, an ice-rock interior that may harbor an ocean, five moons. And that’s just what we know.\nThe flyby will also cap a five-decade-long era of solar system reconnaissance that began with Venus and Mars in the early 1960s, continuing through first looks of Mercury, Jupiter and Saturn in the 1970s and Uranus and Neptune in the 1980s. The July 14 flyby of Pluto will occur 50 years to the day after humans first explored Mars with NASA’s Mariner 4 on July 14, 1965.\nThe Pluto system is unique in that it contains the only binary planet system in our Solar System. Pluto and its largest moon Charon are what scientists refer to as a double-dwarf system. Pluto is currently classified as a dwarf planet; while Charon is technically labeled a moon, both bodies orbit the same point in space—a point that isn’t within the circumference of either body.\nJust like the Earth and the moon, Pluto and Charon are tidally locked in an orbital dance. The two orbit like unbalanced weights on a dumbbell, each pulling on the four smaller moons—Nix, Styx, Hydra, and Kerberos—that orbit the pair.\nUsing the Hubble Space Telescope, the researchers conducted a comprehensive analysis of the system and concluded that the two largest moons, Nix and Hydra, wobble chaotically as they orbit. Styx and Kerberos are expected to behave in the same manner, although further observations are needed to confirm this prediction. We can tell that the wobble is intensified by the fact that the four moons are not spherical in shape; they’re elongated, much like a football.\nPluto was discovered in 1930 by astronomer Clyde Tombaugh at the Lowell Observatory in Flagstaff, AZ. Shortly after the New Horizons mission was launched in 2006—and after the discovery of another Kuiper Belt Object (Eris) —astronomers voted to demote Pluto from its planetary status. In honor of Tombaugh, a small New Horizons carries some of his ashes on board as he passed away before the mission could get off the ground.\nAstronomers want to know how a system like Pluto and its moons could form. The prevailing theory: Pluto collided with another large body in the distant past, and much of the debris from this impact went into orbit around Pluto, eventually coalescing to form Charon. Scientists believe that a similar collision led to the creation of Earth’s moon, so the study of Pluto and Charon could help scientists decipher the history of our own planet.\nScientists want to figure out why Pluto and Charon look so different. From Earth, the Hubble Space Telescope and New Horizons, we have seen that Pluto has a reflective surface with distinct markings indicative of polar caps. Charon’s surface is far less reflective, with indistinct markings. Pluto has an atmosphere, and Charon does not. Is the sharp contrast between these two bodies a result of evolution or is it due to how they formed? New Horizons will hopefully be able to solve this mystery.\nPluto’s density, size and surface composition are strikingly similar to those of Neptune’s largest satellite, Triton — a captured body from the Kuiper Belt. One great surprise of Voyager 2’s exploration of the Neptune system was the discovery of ongoing cryovolcanic activity on Triton. Will Pluto or other KBOs display such activity?\nAnother fascinating aspect of Pluto is its surprisingly complex atmosphere. Although Pluto’s atmosphere is about 300-600 times less dense than Mars’ — which is, in turn, about 150 times less dense than Earth’s — it offers unique insights into the workings of related planetary atmospheres at Triton and Titan. Whereas the Earth’s atmosphere contains only one gas (water vapor), Mars contains two (water vapor and carbon dioxide), and Pluto’s atmosphere contains three: nitrogen, carbon monoxide and methane. Pluto’s atmosphere has also puffed up over the years instead of dissipating. Scientists hope to determine why.\nNew Horizons is outfitted with a suite of nine different instruments including two cameras—Ralph and LORRI. Until recently, the only images of the dwarf planet were in black and white. With the release of the first color images, and now the first color map, we can see that Mars may not be the only red planet.\nScientists have known for decades that Pluto is reddish brown; however, now we can see it in detail and in living color. Although Mars and Pluto have similar hues, they are due to two very different reasons. Mars gets its red coloring from iron oxide (aka rust). Pluto’s reddish coloration is likely attributed to hydrocarbons (tholins) formed when cosmic rays and ultraviolet light interact with atmospheric and surface methane.\nAs a result of chemical reactions, the tholins are formed and drop to ground, forming a reddish gunk that gives Pluto its color. Tholins are found in other parts of the Solar System as well, such as Saturn’s largest moon Titan and Neptune’s largest moon Triton—thought to be a twin of Pluto.\nNew Horizons is expected to give us unprecedented data on the whole Pluto system for many months following the flyby, including data about its surface, its moons and its environment, refining our knowledge of the dwarf planet and its system.\nPluto is not the last stop for New Horizons. In 2014, the Hubble Space Telescope identified three possible KBO’s beyond Pluto for the probe to flyby. If NASA approves and there’s funding, the spacecraft will visit one in 2019.']	['<urn:uuid:8b4513f7-b586-4fd6-8883-f67200ad2c2e>', '<urn:uuid:ab0f1518-e673-4512-ba9a-4546b02b0604>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T17:51:21.175311	25	26	1784
49	how is wiener schnitzel different from hungarian goulash	Wiener Schnitzel and Hungarian goulash are two very different Central European dishes. Wiener Schnitzel is an Austrian dish of pounded veal cutlets that are breaded and lightly fried, served with parsley, lemon slices, and potato salad. In contrast, goulash is a Hungarian soup or stew made with beef, vegetables, and paprika, slowly cooked until the meat is tender. While Schnitzel originated from Italy, goulash traces its roots to 9th century Hungarian shepherds and became a national symbol in the late 1800s when Hungarians sought to distinguish themselves from their Austrian partners in the Austro-Hungarian Empire.	"['Irish Stew, Ireland\nOriginally a thick broth of slow-boiled mutton with onions, potatoes, and parsley, Irish stew nowadays often incorporates other vegetables, such as carrots, and many cooks brown the mutton first. It is a staple of Irish pubs worldwide.\nPlanning: One place in Dublin to enjoy Irish stew and other traditional fare is Shebeen Chic, in George’s Street.\nRoast Beef and Yorkshire Pudding, England\nDespite England’s increasingly cosmopolitan cuisine, this dish remains a much-loved Sunday lunch and national symbol. Named for England’s eponymous county, Yorkshire or batter puddings originally served as fillers before the main course for those who could afford little beef. Today, the two are usually eaten together alongside gravy-soaked roast potatoes, vegetables, and horseradish sauce.\nPlanning: Try the traditional British restaurant London’s Rules, founded in 1798, or country pubs.\nOriginally a rustic dish that was stewed continuously all winter and topped up as needed, pot-au-feu (pot-in-the-fire) is a warming, fragrant dish of stewing steak, root vegetables, and spices. Traditionally, cooks sieve the broth and serve it separately from the meat.\nPlanning: In downtown Paris, Le Pot au Feu at 59 Boulevard Pasteur (Mtro: Pasteur) specializes in its namesake.\nWiener Schnitzel, Austria\nMade with the finest ingredients and served fresh, this simple dish of pounded veal cutlets breaded and lightly fried is Austria’s food ambassador, despite the dish’s Italian origins. Austrians typically eat Wiener schnitzel garnished with parsley and lemon slices, alongside potato salad.\nPlanning: Vienna’s Cafe Landtmann, a city institution since 1873, serves up an authentic version of the dish, as well as a dose of history and glamour: Sigmund Freud, Marlene Dietrich, and Paul McCartney have been among its famous patrons.\nGuly’s Magyar for herdsman became a national dish in the late 1800s, when Hungarians sought symbols of national identity to distinguish themselves from their partners in the Austro-Hungarian Empire. A filling stew of beef, vegetables, red onions, and spices, goulash gets its flavor from the use of slow-cooked beef shin, or similar richly flavored cuts, and paprika.\nPlanning: For a lighter version, sample guly sleves (goulash soup).\nDining well Levantine-style often means sticking to the delicious mezes(appetizers). Kibbeh, a versatile confection of ground lamb, bulgur, and seasonings, is a core component of mezes. It is often fried in torpedo or patty shapes, baked, boiled, or stuffed, but is tastiest raw.\nPlanning: Aleppans in northern Syria are kibbeh’s greatest innovators, flavoring it with ingredients like pomegranate or cherry juice.\nBeef bulgogi (fire meat) is a dish of thinly sliced, prime cuts of meat marinated in a mixture of soy sauce, sesame oil, garlic, onions, ginger, sugar, and wine and then grilled. It is often eaten wrapped in lettuce or spinach leaves and accompanied by kimchi (fermented vegetable pickle). Many Korean restaurants have miniature barbecues embedded in tables where diners grill the meat themselves.\nPlanning: Seoul’s upmarket Byeokje Galbi chain is a bulgogi sensation.\nCoo-Coo and Flying Fish, Barbados\nA polenta-like cornmeal and okra porridge, coo-coo pairs perfectly with flying fish, which is either steamed with lime juice, spices, and vegetables or fried and served with a spicy sauce.\nPlanning: The Flying Fish restaurant overlooking St. Lawrence Bay claims to be the Barbadian national dish’s home.\nAckee and Saltfish, Jamaica\nDespite ackee’s unhappy origins as slave food, Jamaicans have reclaimed it as part of their national dish. A nutritious fruit with a buttery-nutty flavor, ackee resembles scrambled egg when boiled. Jamaicans saut the boiled ackee with saltfish (salt-cured cod), onions, and tomatoes. Sometimes the dish is served atop bammy (deep-fried cassava cakes) with fried plantains.\nPlanning: Jake’s, Treasure Beach, is renowned for ackee and saltfish and also offers cooking classes.\nAlthough the origins of the hamburger are disputed, there is no argument over the popularity of this classic dish. Toppings and accompaniments vary from region to region, but for an original version visit Louis Lunch in New Haven,Connecticut, which has been serving hamburgers since 1900 and claims to be the oldest hamburger restaurant in the U.S.\nPlanning: Louis Lunch is open most days for lunch and some days until the early hours of the morning.\nSource from here.', '|Type||Soup or stew|\n|Place of origin||Hungary|\n|Main ingredients||Meat, noodles, vegetables (especially potatoes), paprika, spices|\n|Cookbook: Goulash Media: Goulash|\nGoulash (Hungarian: gulyás [ˈɡujaːʃ]) is a soup or stew of meat and vegetables, seasoned with paprika and other spices. Originating from the medieval Kingdom of Hungary, goulash is also a popular meal in Central Europe, Eastern Europe, the Netherlands, Belgium, Switzerland, Scandinavia and Southern Europe.\nIts origin traces back to the 9th century to stews eaten by Hungarian shepherds. Back then, the cooked and flavored meat was dried with the help of the sun and packed into bags produced from sheep\'s stomachs, needing only water to make it into a meal. It is one of the national dishes of Hungary and a symbol of the country.\nThe word gulyás originally meant only ""herdsman"", but over time the dish became gulyáshús (goulash meat) – that is to say, a meat dish which was prepared by herdsmen. Today, gulyás refers both to the herdsmen, and to the soup. From the Middle Ages until well into the 19th century, the Puszta was the home of massive herds of cattle. They were driven, in their tens of thousands, to Europe’s biggest cattle markets in Moravia, Vienna, Nuremberg and Venice. The herdsmen made sure that there were always some cattle that had to be slaughtered along the way, the flesh of which provided them with gulyáshús.\nIn Hungarian cuisine, traditional ""Gulyásleves"" (literally ""goulash soup""), ""bográcsgulyás"", pörkölt, and paprikás were thick stews made by cattle herders and stockmen. Garlic, caraway seed, bell pepper, and wine are optional. These dishes can be made as soups rather than stews. Excepting paprikás, the Hungarian stews do not rely on a flour or roux for thickening. Tomato is a modern addition, totally unknown in the original recipe and in the whole Central European food culture until the first half of the twentieth century.\nGoulash can be prepared from beef, veal, pork, or lamb. Typical cuts include the shank, shin, or shoulder; as a result, goulash derives its thickness from tough, well-exercised muscles rich in collagen, which is converted to gelatin during the cooking process. Meat is cut into chunks, seasoned with salt, and then browned with sliced onion in a pot with oil or lard. Paprika is added, along with water or stock, and the goulash is left to simmer. After cooking a while, garlic, whole or ground caraway seed, or soup vegetables like carrot, parsley root, peppers (green or bell pepper) and celery may be added. Other herbs and spices could also be added, especially chili pepper, bay leaf and thyme. However, chili peppers are a modern addition, totally unknown in the original recipe. Diced potatoes may be added, since they provide starch as they cook, which makes the goulash thicker and smoother. However, potatoes are a modern addition, totally unknown in the original recipe. A small amount of white wine or wine vinegar may also be added near the end of cooking to round the taste. Goulash may be served with small egg noodles called csipetke. The name Csipetke comes from pinching small, fingernail-sized bits out of the dough (csipet =pinch) before adding them to the boiling soup.\nHungarian goulash variations\n- Gulyás à la Székely. Reduce the potatoes and add sauerkraut and sour cream.\n- Gulyás Hungarian Plain Style. Omit the homemade soup pasta (csipetke) and add vegetables.\n- Mock Gulyás. Substitute beef bones for the meat and add vegetables. Also called Hamisgulyás, (Fake Goulash)\n- Bean Gulyás. Omit the potatoes and the caraway seeds. Use kidney beans instead.\n- Csángó Gulyás. Add sauerkraut instead of pasta and potatoes.\n- Betyár Gulyás. Use smoked beef or smoked pork for meat.\n- Likócsi Pork Gulyás. Use pork and thin vermicelli in the goulash instead of potato and soup pasta. Flavour with lemon juice.\n- Mutton Gulyás or Birkagulyás. Made with mutton. Add red wine for flavour.\nA thicker and richer goulash, similar to a stew, originally made with three kinds of meat, is called Székely gulyás, named after the Hungarian writer, journalist and archivist József Székely (1825–1895).\n""Paprikás krumpli"" is a traditional paprika-based potato stew with diced potatoes, onion, bell peppers, ground paprika and some bacon or sliced spicy sausage, like the smoked Debrecener, in lieu of beef.\nIn German-speaking countries this inexpensive peasant stew is made with sausage and known as Kartoffelgulasch (""potato goulash"").\nThick stews similar to pörkölt and the original cattlemen stew are popular throughout almost all the former Austrian-Hungarian Empire, from Northeast Italy to the Carpathians. Like pörkölt, these stews are generally served with boiled or mashed potato, polenta, dumplings (e.g. nokedli, or galuska), spätzle or, alternatively, as a stand-alone dish with bread.\nGoulash (Albanian: gullash) is considered a national dish throughout Albania, but its popularity is much higher in northern parts of the country.\nIn Vienna, the former center of the Austro-Hungarian Empire, a special kind of goulash had been developed. The Wiener Saftgulasch or the Fiakergulasch on the menu in traditional restaurants is a must-have. It is a rich pörkölt-like stew; more onions but no tomatoes or other vegetables are used, and it usually comes just with dark bread. A variation of the Wiener Saftgulasch is the Fiakergulasch, which is served with fried egg, fried sausage, and dumplings named Semmelknödel.\nGoulash (Croatian: gulaš) is also very popular in most parts of Croatia, especially north (Hrvatsko Zagorje) and Lika. It is considered to be part of traditional cuisine. In Gorski Kotar and Lika, deer and boar frequently replace beef (lovački gulaš). There is also a kind of goulash with porcini mushrooms (gulaš od vrganja). Bacon is an important ingredient.\nGulaš is often served with fuži, njoki, polenta or pasta. In Croatian, Bosnian and Serbian is augmented with vegetables. Green and red bell peppers and carrots are most commonly used. Sometimes one or more other kinds of meat are added, e.g., pork loin, bacon, or mutton. In Slovenia, they are known as perkelt, but are often referred to as ""goulash"" or a similar name.\nCzech Republic and Slovakia\nIn the Czech and Slovak Republic, goulash (Czech and Slovak: guláš) is usually made with beef, although pork varieties exist, and served with bread dumplings (in Czech hovězí guláš s knedlíkem, in Slovak hovädzí guláš s knedľou), in Slovakia more typically with bread. In pubs it is often garnished with slices of fresh onion, and is typically accompanied by beer. Beer can be also added to the stew in the process of cooking. Seasonal varieties of goulash include venison or wild boar goulashes. Another popular variant of guláš is segedínský guláš (Szeged goulash), with sauerkraut.\nIn Czech and Slovak slang, the word guláš means ""mishmash"", typically used as mít v tom guláš: to be disoriented or to lack understanding of something.\nGerman Gulasch is either a beef (Rindergulasch), pork (Schweinegulasch), venison (Hirschgulasch), or wild boar (Wildschweingulasch) stew that may include red wine and is usually served with potatoes (in the north), white rice or spirelli noodles (mostly in canteens), and dumplings (in the south). Gulaschsuppe (goulash soup) is the same concept served as a soup, usually with pieces of white bread.\nGoulash in Italy is eaten in the region of Friuli-Venezia Giulia and the autonomous region of Trentino-Alto Adige/Südtirol, regions that had been part of the Austro-Hungarian Empire and still are in part culturally and linguistically Austrian. Here it is eaten as a regular Sunday dish. It can also, although less typically so, be found in the nearby Veneto. An interesting regional recipe comes from the Pustertal (Val Pusteria, Puster Valley) in South Tyrol. It is made of beef and red wine, and seasoned with rosemary, red paprika, bay leaf, marjoram and lemon zest, served with crusty white bread or polenta. The lemon gives its signature flavor. Goulash is also quite popular in the city of Ancona, which is culturally quite near to eastern Europe.\nA form of goulash (Polish: gulasz) is also popular in Poland, though said dish is more similar to Hungarian pörkölt than actual goulash. It is usually served with various forms of noodles and dumplings, such as pyzy\nIn Serbia, goulash (Serbian: гулаш) is eaten in most parts of the country, especially in Vojvodina, where it was probably introduced by the Hungarian minority. It is actually a pörkölt like stew, usually made with beef, veal or pork, but also with game meat like venison, deer and boar. In Serbia, goulash is most often served with boiled potatoes or potato mash.\nIn Slovene partizanski golaž, partisan goulash, favoured by Slovenian partisans during the Second World War, and still regularly served at mass public events; most meat is replaced with quartered potatoes. It is not as thick as goulash, but thicker than goulash soup.\nAmerican goulash, mentioned in cookbooks since at least 1914, exists in a number of variant recipes. Originally a dish of seasoned beef, core ingredients of American goulash now usually include elbow macaroni, cubed steak, ground beef or hamburger, and tomatoes in some form, whether canned whole, as tomato sauce, tomato soup, and/or tomato paste. In some areas it is called slumgullion.\n- Goulash Communism was the maverick brand of communism practiced by Hungary during the Cold War, characterized by some degree of political freedom within the Hungarian Communist Party as well as limited economic freedom and freedom of speech, inspired at least in part by the 1956 Hungarian Revolution.\n- Beef stew\n- Beef bourguignon\n- List of beef dishes\n- List of pork dishes\n- List of soups\n- List of stews\n- National symbols of Hungary\n- Food portal\n- Gundel, Karoly (1992). Gundel\'s Hungarian cookbook. Budapest: Corvina. ISBN 963-13-3600-X. OCLC 32227400.page 20\n- Bulgaria, Hungary, Romania, the Czech Republic, and Slovakia, Britannica Educational Publishing, 2013, p. 94\n- Gil Marks, Encyclopedia of Jewish Food, John Wiley & Sons, 2010, p. 234\n- Food Journeys of a Lifetime: Top Ten Great National Dishes | Away.com\n- William White, Notes and queries, Volume 126, Oxford University Press, 1912\n- Judith Petres Balogh, This Old House by the Lake, Trafford Publishing, 2006, p. 244\n- MrGoulash.com - Goulash A plate of Hungarian history\n- Anikó Gergely, Culinaria Hungary , H.f. Ullmann, 2008.10.15. - p. 318\n- Gundel\'s Hungarian Cookbook, Karoly Gundel.\n- Famous Hungarian recipes\n- Gundel, Karoly (1992). Gundel\'s Hungarian cookbook. Budapest: Corvina. ISBN 963-13-3600-X. OCLC 32227400.page 31\n- Gundel, Karoly (1992). Gundel\'s Hungarian cookbook. Budapest: Corvina. ISBN 963-13-3600-X. OCLC 32227400.page 21\n- Metcalf, Allan (1999). The World in so Many Words. Boston, MA, USA: Houghton Mifflin Company. pp. 47–48. ISBN 0-395-95920-9.\n- Cookbook of the Woman\'s Educational Club. Toledo, OH, USA: Woman\'s Educational Club of Toledo, Ohio. 1914. p. 49.\n- Gundel\'s Hungarian Cookbook, Karoly Gundel, Budapest, CORVINA. ISBN 963-13-3733-2.\n|Look up Goulash in Wiktionary, the free dictionary.|\n|Wikibooks has a book on the topic of: Cookbook:Paprika Chicken|\n|Wikibooks Cookbook has a recipe/module on|']"	['<urn:uuid:cafc88c8-46a1-4a30-b584-dcb251cf24ff>', '<urn:uuid:d0ee89b5-94d9-4dba-9a02-54b86ec1c1d1>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	8	95	2486
50	calcium magnesium supplements which more important daily health body function	Both calcium and magnesium are essential minerals, but magnesium is responsible for over 300 chemical reactions in the body while calcium is primarily known for bone and teeth health. Calcium is the most common mineral in the body, important for muscle contraction, blood vessel function, and nervous system impulses. Magnesium, the fourth most abundant mineral, is found in every organ and is crucial for maintaining normal muscle and nerve function, steady heartbeat, healthy immune system, energy production, and enzyme activation. Both minerals are important, but most Americans don't get enough magnesium in their diets, while calcium deficiency is less common. The recommended daily allowance for adults is 1,000 mg for calcium and 320-420 mg for magnesium.	"[""Calcium and magnesium are two essential dietary minerals. You need calcium and magnesium not only to build strong bones and healthy teeth, but also because both minerals play complex roles in creating enzymes and hormones and producing energy for your body. Ensuring that you get enough calcium and magnesium in your diet may help you manage your weight and may also contribute to weight loss; however, you should consult your doctor before taking any supplements. Be sure to balance your diet with adequate exercise and rest.\nCalcium is the most common mineral in your body and is found in your bones and teeth. It is also important for proper muscle contraction and helping your blood vessels to expand and contract properly, according to the National Institutes of Health's Office of Dietary Supplements. Calcium is involved with enzyme and hormone production and in the transmission of nervous system impulses. Calcium is found is a variety of foods, with the most common being dairy products like cheese and milk. Vegetable sources of calcium include seaweed, spinach, kale and peppers. Eat a variety of foods that contain calcium to ensure that you get enough in your diet. The recommended dietary allowance of calcium for adults is 1,000 mg per day, the ODS reports.\nCalcium and Weight Management\nCalcium may play a role in weight management and weight loss, but the research findings are mixed. The ODS cites several studies showing that consuming high levels of calcium contributes to weight loss. However, clinical trials have yet to support the research. Scientists are not sure how calcium is involved in weight loss, and two theories have been advanced. The first is that calcium may bind with fat in the intestines and prevent your body from absorbing it. The second theory is that calcium may inhibit the production of hormones that cause fat to accumulate in your body.\nMagnesium is the fourth-most-common mineral in your body and is found in every organ. It contributes to energy production and activates enzymes. One of magnesium’s jobs is to help regulate the amount of calcium and other minerals in your body. The University of Maryland Medical Center reports that most Americans don’t get enough magnesium in their diets, although true magnesium deficiency is rare. Dark green vegetables, nuts, beans and peas, rice and bananas are some sources of magnesium. The recommended dietary allowance of magnesium for adults is between 320-420 mg per day, the ODS notes.\nMagnesium and Weight Management\nMagnesium is essential in digestive processes and activates enzymes that let your body absorb and use the food you eat. It helps to regulate your metabolism, which in turn may help you to burn more calories. Magnesium is also involved in the production of adenosine triphosphate, the body’s source of energy. It’s possible that a diet that includes sufficient magnesium allows you to rev up your metabolism and produce enough energy so you move more, thus helping to burn calories. The Nutritional Magnesium Association suggests that magnesium may prevent obesity by keeping certain genes from turning on.\nCalcium and Magnesium Supplements\nIt may be difficult for you to get enough magnesium in your diet, so taking a supplement might be a good choice. Magnesium and calcium work synergistically, so choose a supplement that will provide both of these essential minerals. Many calcium and magnesium supplements also include vitamin D, which works with magnesium to help your body absorb calcium. As with any dietary supplement, follow the dosage instructions and ask your doctor or a nutritionist if you have any health concerns."", 'Magnesium is responsible for over 300 chemical reactions in the body. It helps maintain normal muscle and nerve function. It keeps your heart beating steadily. And, it helps to build a healthy immune system. Magnesium is a crucial element that your body needs to function at even the most basic levels – making it crucially important to get enough of this essential mineral on a daily basis. Read on to learn more about magnesium.\nWhy we need magnesium\nMagnesium is the fourth most abundant mineral in the body. Two-thirds of it is found in your bones and the rest in your tissues. This amazing mineral is absorbed into the body through the intestines and is then transported through the blood to your cells and organs where it’s stored. Because the body can’t produce it (unlike some nutrients like vitamin A), you need to eat enough magnesium-rich foods on a daily basis to remain healthy.Researchers know that magnesium plays a wide variety of roles in the body. Your cardiovascular system, digestive system, nervous system, muscles, kidneys, liver, hormone-secreting glands and brain all rely on the mineral to accomplish even the most basic tasks.And scientists believe it may play an important role in preventing or treating dozens of health conditions including asthma, autism, heart disease, eclampsia, epilepsy, HIV/AIDS, multiple sclerosis, PMS and lupus.\nSigns and symptoms of a magnesium deficiency\nBecause the magnesium is involved is a wide array of bodily reactions and functions, symptoms of a magnesium deficiency can vary widely. Here are some of the most common:Â· Muscle weakness, tremors and spasms\nÂ· Heart arrhythmia or an increased heart rate\nÂ· Imbalanced blood sugar levels\nÂ· High blood pressure\nÂ· Weight gain\nÂ· Lack of appetiteYou can, however, get too much of this mineral. Magnesium toxicity usually results when it is taken as a supplement or in pill form. Toxicity symptoms include drowsiness, weakness and diarrhea. Stick to magnesium-rich foods, unless directed differently by your doctor.\nWho is at risk for a magnesium deficiency?\nCertain groups of people will have a harder time reaching their daily dose of the nutrient. At-risk groups include:People with digestive diseases. People who suffer from Crohn’s or irritable bowel syndrome have lower absorption rates than others.People who experience vomiting or diarrhea. Whether you have the flu or experience an allergic reaction, excessive vomiting or diarrhea will deplete the magnesium stores in your body and will compromise the digestive system’s ability to absorb it.People taking certain medications. Certain diuretics, antibiotics and medications (like those used to treat cancer) inhibit the digestive system’s ability to absorb the mineral.Diabetics. Some diabetics are prone to urinate more often than others. Because magnesium is flushed from our body through the kidneys, the more you urinate, the more magnesium your body will lose.People with low blood levels of potassium or calcium. These nutrients and minerals work together to create balance in the body. If you’re running low on either potassium or calcium, you run the risk of becoming magnesium deficient as well.Seniors. The body’s digestive tract changes with age. The older you are, the more prone you are to problems like diarrhea. As a result, seniors run a higher risk of becoming deficient than their younger counterparts do.\nRecommended daily intake for magnesium\nThe Recommended Dietary Allowance (RDA) for magnesium is as follows:Infants (0-6 months): 30 mg per day\nInfants (7-11 months): 75 mg per day\nChildren (ages 1-3): 80 mg per day\nChildren (ages 4-8): 130 mg per day\nChildren (ages 9-13): 240 mg per day\nTeenagers (ages 14-18): 360-410 mg per day\nMen (ages 19-30): 400 mg per day\nMen (ages 31 and older): 420 mg per day\nWomen (ages 19-30): 310 mg per day\nWomen (ages 19 and older): 320 mg per day\nPregnant women (ages 19 and older): 350-360 mg per day\nLactating women (ages 19 and older): 310-320 mg per day\nFood sources of magnesium\nSome of the best food sources for magnesium include:Halibut, cooked, 3 oz = 90 mg\nAlmonds, dry roasted, 1 oz = 80 mg\nCashews, dry roasted, 1 oz = 75 mg\nSoybeans, cooked, 1/2 cup = 75 mg\nSpinach, cooked, 1/2 cup = 75 mg\nOatmeal, instant prepared with water, 1 cup = 55 mg\nPeanut butter, smooth, 2 tbsp = 50 mg\nAvocado, California, 1/2 cup = 35 mg\nRaisins, seedless, 1/4 cup = 25 mg\nTips for getting enough magnesium\nGetting more magnesium from your diet is easy. Here are a few quick and easy tips:1. Eat green. Green vegetables are a good source of magnesium because the nutrient that makes them green (chlorophyll) is packed with the mineral.2. Avoid refined foods. Refining and processing flour strips the grain of almost all its magnesium content. Reach for whole grain products instead.3. Skip bottled water. Tap water is a great source of magnesium. That’s especially true if you have hard water.4. Get your daily five. Meeting your daily fruit and veggie quota of 5 to 10 servings will guarantee you reach your magnesium RDA.5. Limit alcohol and coffee. Both increase your body’s urge to urinate. The more you urinate, the more magnesium will be excreted from your body.6. Eat your fruits and veggies raw. Steaming, blanching or boiling certain foods can cut their magnesium content by as much as 65 percent. Reach for freshly chopped foods instead.']"	['<urn:uuid:cbe47650-98d0-42c5-b0d1-4e06da5f40cd>', '<urn:uuid:9e042c3c-70dd-4113-a4dc-7ebd66606287>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	10	116	1479
51	How do the child labor prevention efforts in Burkina Faso and Côte d'Ivoire address the connection between poverty and exploitation in the cocoa industry?	In Côte d'Ivoire, Winrock addresses poverty through better agricultural practices, enterprise training, and partnerships with the World Cocoa Foundation to improve conditions in cocoa production. In Burkina Faso, the approach focuses on breaking the cycle of poverty by intercepting trafficked children, providing vocational training through ATY for older youth to become entrepreneurs, and working with ABVV Horval to lobby for better cocoa prices so farmers can afford paid adult labor instead of relying on child workers.	['As the lead for Winrock’s Child Labor Prevention and Protection portfolio, Vicki Walker’s experience includes implementing and advising on past projects CIRCLE 1 and 2 in more than 30 countries, CLASSE and ECHOES in Ghana and Côte d’ Ivoire, TEACH in Tanzania, CHES in Cambodia and REACH in Rwanda in addition to ongoing projects: ARCH in Liberia, REACH-T in Rwanda, CLEAR II in Burkina Faso and Nepal, MOCA in Ghana, PROSPER in Tanzania and ARISE in Brazil, Malawi, Tanzania and Zambia. Walker retires on December 30. Here are her thoughts on international development, child labor and what she’s doing next.\nWhy did you become interested in international development?\nMy father was a military attaché officer, and I grew up largely in Canada and Europe. I had language training and enthusiasm for travel and international environments. When I studied in Germany and Switzerland, I realized I wanted to have a life in the international world and envisioned the Peace Corps as an important direction. I gained interest in coastal West Africa from a colleague and spoke French, and Dakar, Senegal, became my venue for two years.\nWhat was your first job at Winrock?\nAfter I started a family [Walker is married to physicist David Walker; they have two grown sons], I completed my Master’s degree in International Relations and soon had a position with Johns Hopkins University gathering historical data for a sociological study on town populations in 150 countries to measure city-size distribution and its relationship to economic development. In 1990, I applied for a position that opened at Winrock in the agriculture policy division led by David Seckler, an economics professor from Colorado who built the agriculture policy and water resource programs at Winrock. I worked collaboratively with his team for five years and was given the African rural social science research network project to manage. During that time the women’s leadership program was blooming at Winrock. It had been brought in by former Winrock President Robert Havener, who with Winthrop Rockefeller had the common values to build on the agriculture base by including the social side of agriculture, and were manifested in Winrock’s social science policy research network and its women’s leadership in agriculture and environment programs. I became involved in linking opportunities for African women scientists to receive competitive research grants and membership on the technical review committees.\nHow did you become involved with child labor issues?\nIn the late 1990s, as part of the women’s leadership program, a project supported a women’s consortium in the former Soviet Union, and that evolved into recognizing the vulnerability of trafficking of women and girls. These led to programs in Moldova, Uzbekistan and Ukraine; we also turned our attention to vulnerable African women and girls. By 2000 to 2002, child labor issues in factories and farms had come to the fore including media reports on child labor in cocoa farms in West Africa. We were working actively in Côte d’Ivoire with small scale agriculture and women’s leadership programs and after learning about opportunities to address these issues, we applied for a Department of Labor solicitation to propose a global innovations program to provide small grants (subcontracts) to NGOs around the world to address child labor. We proposed the Community-based Innovations to Reduce Child Labor through Education (CIRCLE) Project and included a pilot to address child labor in the smallholder cocoa plantations in Côte d’Ivoire. We were awarded that project, plus a second follow-on, and I was the director for the full six years. We provided over 100 grants/contracts to 80 NGOs in 25 countries, impacting over 24,000 children and youth. We created a website and the last year published a best practices book, which was collaboratively reviewed by panels of experts and that has been widely referenced. It was an inspiring program that thrust me into a whole new world, and I continued to pursue this important social/economic development field.\nWhy are child labor projects a good fit for Winrock?\nFrom the beginning, our approach was looking at preventing child labor in agriculture through teaching better practices in agriculture, not removing agriculture from the children’s lives. Agriculture is the largest sector in which child labor occurs, especially informal, smallholder agriculture. As Winrock works in remote sectors, we could reach farmers and communities and teach extension and better planning and safety for appropriate ages. In alignment with the International Labor Organization conventions 182 and 138, US Department of Labor structures, and country laws, we work with policy-making and capacity-building of governments and inspectorates at national and local levels and encourage the enforcement of laws against child labor. There is a large awareness-raising component at community and national levels and a formal or non-formal education initiative as a key to increasing access and quality education for children and youth. Livelihoods and social protection capacity building for households and rigorous monitoring and evaluation plans are also key values and components. We have developed a model farm school program that has become a Winrock brand to address opportunities in the community for vulnerable youth to acquire skills and enterprise training linked to on-and-off farm markets and enterprises. We work with youth, those legally of minimum age to work according to country laws and provisions to protect from hazardous conditions. Working with that age group has been an important mission for the last few years, looking at how to marry the needs of employers and farmers to provide safe, decent labor to comply with the provisions and regulations of safe conditions for legally aged working youth. We have also worked with the private sector and with agricultural businesses and resources from the World Cocoa Foundation and members. As part of this, we have been engaged in the Harken-Engle Protocol, a formal agreement between industry and governments to eliminate child labor and trafficking in cocoa production in West Africa.\nWhat is one of the most memorable moments of your career?\nA memorable high point was in 2015, when I went to a project sustainability conference in Tanzania with Rodney Ferguson (Winrock president and CEO). We visited the village where we had been working with the Promoting Sustainable Practices to Eradicate Child Labor in Tobacco project (PROSPER/ECLT Foundation). The villagers shared proudly their accomplishments, and how they were eliminating child labor with innovative ideas of their own. They prepared and came to share what they were doing to ensure children were in education and not in child labor. I was impressed because they owned it and were giving us ideas which was a real indicator of change and a learning experience.\nWhat’s the future of child labor projects at Winrock?\nWinrock is a known leader in child labor projects at the community and policy levels. Winrock has teams and experienced designers of these projects with both the US Department of Labor and private sector companies and international partners. Winrock continues to develop innovations and strong partnerships for implementing key programs at policy and community levels. We see more integration in other programs and increased awareness of risks and standards of child protection in supply chains in their projects and requesting approaches to mitigate those risks through training and leadership. Winrock’s breadth of work can help in these efforts to reach more scale through its provisions and guidelines and with multi-stakeholder partnerships and global opportunities with the Sustainable Development Goals where there will be active areas of engagement.\nWhat do you do in your spare time?\nI play piano and harpsichord, and dance. I think that study in the arts has enriched my career, because of the discipline for practice and performing. I do panels, I do talks. I made a CD with a colleague; and accompanied her to a poem in French we collaborated on about child labor and education.\nWhat’s next for you?\nI’ll be staying active through consulting and technical assistance and some writing and research to link to international development. I have a wonderful heritage at Winrock, and I’m very grateful and honored that I’ve been able to contribute creatively and learn so much.', 'Truck drivers from Burkina Faso intercept child victims of human trafficking and prevent them from being enslaved in the cocoa fields of Ivory Coast. Brahima from the Burkinabe drivers’ union and Madi from the association that cares and trains the children, testify how they fight against the terrible trafficking of children.\nFighting the problem at the source. That is exactly what the comrades of the Union nationale des Chauffeurs Routiers du Burkina (UCRB, truck drivers’ union) of Burkina are doing, with the support of the ABVV Horval and the NGO Solsoc. We met Brahima Rabo (right in the photo), president of the UCRB, and Madi Sawadogo (left), president of ATY, an association that takes care of children who are victims of human trafficking, sends them to school and provides them with training.\nCocoa with very bitter taste\nIvory Coast is the world’s largest cocoa producer, accounting for 40% of total world production. It supplies multinationals such as Barry Callebaut and Cargill, which process the cocoa in our country. Cocoa is a very important economic sector for this West African country, accounting for 14% of gross domestic product (the total value of all goods and services produced in a country).\nThe cocoa industry and plantations are marred by a number of sad phenomena: deforestation, use of harmful pesticides such as glyphosate, slavery, forced and child labor. By exploiting children, cocoa farmers ensure themselves very cheap labor. After all, those cocoa farmers in turn sell the beans at ridiculous prices to the foreign multinationals that dominate the market.\nAccording to a study by the University of Chicago, almost 800,000 children worked on cocoa plantations in Ivory Coast in 2019. Despite the fact that child labor is prohibited.\nDangerous, unpaid work\nOn the plantations the children do heavy and dangerous work, without any protection. They remove the pods from the cocoa trees and use machetes to break them. Carrying the heavy harvested pods, they travel long distances. They have to use pesticides such as glyphosate, which endangers their health.\nDespite the risks they face and the promises they receive, the children work for years free in exchange for food. In the best cases, they are paid several years later, when they are given a small piece of land. They will then become cocoa farmers themselves and ironically they will in turn use children in the fields. It’s a vicious circle.\nThe majority of children working in the cocoa industry in Ivory Coast come from neighboring countries, mainly Burkina Faso. Traffickers approach poor families directly, who agree to give up their children without really knowing the risks they face. The children are then taken to Ivory Coast.\nThis is where UCRB, a partner of ABVV Horval, intervenes. The Burkinabe Truck Drivers’ Union – which has more than 28,000 members – has developed a project to put an end to this scourge. They train the truck and bus drivers driving around Burkina Faso to recognize situations of child trafficking. If necessary, they immediately contact the relevant services and authorities. “In 2019 we have a network dismantled that trafficked approximately sixty children under the age of fifteen,” explains Brahima Rabo. “The trafficker in question was brought to justice and sentenced to fourteen years.”\nSince impunity is one of the biggest problems standing in the way of eradicating the phenomenon, the UCRB chairman now has confidence in it. “This conviction is a good start and can serve as an example. The police services are committed to dismantling the entire network. Investigations are underway,” he added.\n“Children belong at school, not on the plantations.”\n— Brahima Rabo, Chairman UCRB\nOnce intercepted, the children are given shelter and guidance. Brahima: “First and foremost we try to reunite the children with their families. If this is not possible, the youngest children are taken care of by the social services of Burkina Faso and sent to school. Children belong at school, not on the plantations.”\nBut the project doesn’t end there. A new partnership has been set up with the association Tind Yalgré (“Hope to Grow” in Burkinabe), which provides vocational training for the elderly. “We train them to be bakers, hairdressers, teach them mechanics, etc.,” says ATY chairman Madi Sawadogo. “Many of the youth we trained have become self-employed entrepreneurs after 2-3 years,” he adds. In addition to their vocational training, the young people also receive guidance to develop their lives. They learn how to manage their money, open a bank account, and so on. They also receive pocket money, which most send to their families back home. They are also registered with a health insurance fund.\n“Many of the young people we trained have become self-employed entrepreneurs after 2-3 years.”\n— Madi Sawadogo, President ATY\nMadi is moved when we ask about a concrete story that stays with him. “This project with children touches me. It’s in my heart,” he admits. He talks about Kabre, a boy he calls by his surname, as often happens in Burkina Faso. “Kabre was one 17 year old orphan when we took care of him. The eldest of ten siblings he had to care for. We taught him the baker’s trade and took his brothers and sisters to school. He later opened a bakery and then a cafe and is now able to feed his family. Today he also trains other young people in the project.” This way the circle is complete.\nNone of this would be possible without international solidarity, both presidents emphasize. “The partnership with ABVV Horval and the NGO Solsoc has created links between the different organizations in Burkina Faso. The collaboration also allows us to share knowledge and provide financial and political support,” Rabo explains.\nFor example, ABVV Horval lobbies European and international institutions to ensure that multinationals have a decent price pay for cocoa. This way, cocoa farmers would have a better income and not use cheap child labor. Various missions have also already been organized, in which ABVV representatives visited their company’s local factories in other countries to gain insight into the concrete working conditions with their own eyes.\nFrom villages in Burkina Faso where children are taken from their families… to the chocolate we consume in Belgium. This project reminds us that workers’ struggles are international. Only together can we satisfy the profit hunger of multinationals. Only together can we create a better world for our children. Here and elsewhere, figuratively and literally.']	['<urn:uuid:437f084c-056b-41ba-b8ce-9bbfb22774f0>', '<urn:uuid:03400c9d-5858-4a0b-9d31-b51453e97ffd>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	24	76	2407
52	dust control arena watering vs pool clarifier effectiveness mechanism particle size	Arena watering and pool clarifiers work differently to control particles. For arenas, watering works by increasing particle size through aggregation using adhesive force from water. For pools, clarifiers work through coagulation, bringing tiny particles together to form bigger particles that filters can catch more effectively.	"['Dust is a concern—a potential health hazard—for both horse and human, as it makes both more prone to respiratory issues. That makes dust control a priority for stable owners.\nThere is no one solution for dust control that fits all equine arenas and barns. Watering the arena is one step, whether with a handy garden hose or a sophisticated sprinkler system. Proper preparation of the footing is another step. That can take the form of adding rubber footing materials or coatings or utilizing rubber bricks or mats. There are many options available for the stable and arena manager.\nA Colorado State University research report found that whatever control method is used, reducing dust by increasing particle size through aggregation is critical. This is accomplished by uses of an adhesive force that binds to particulates larger than soil particles. That adhesive force can come from water, salts, methyl esters of vegetable oil, and petroleum or mineral-based motor oils.\nWatering Systems and Misters\nAlf Caldwell of The Rubberman says the simplest measure for dust control in an indoor arena is “having the ability to maintain what moisture that is already in the footing,” as well as efficiently replacing lost moisture.\n“I have a lot of people call me to discuss their footing and nine times out of 10, they really do not need our rubber footing. As crazy as it seems, I will tell them that,” Caldwell says.\nSprinkler systems are legion. Jaime Gleba with FarmTek says the company introduced several new dust control options in January, including the 3-Zone Sprinkling System, the Brute Sprinkler Stand and the Rapid Rain Sprinkler System. The 3-Zone system controls dust with frequent watering. To install the overhead sprinkler system, a filter and a supply line need to be set up along with the installation of FarmTek’s RotorMaxim Inverted Sprinkler Assembly. Each sprinkler assembly waters a 30 foot to 35 foot area.\nThe Brute Sprinkler Stand with a heavy-duty riser is for the arena where an overhead system isn’t feasible. The sprinkler stand can be used in conjunction with FarmTek’s Senninger Impact Sprinklers.\nThe new automatic Rapid Rain Sprinkler System is portable and capable of watering a large area in a single pass through automatic rewinding of the hose and sprinkler. The system is easier to move than traditional sprinklers and is more cost effective than a permanent one. It attaches directly to a standard garden hose and is completely powered by water.\nKifco offers a watering system that operates unattended and shuts off automatically. The company says it operates with less time and labor than water trucks. The mobile sprinkler actually travels across the arena and has an adjustable travel speed, so the owner can control the run time and the amount of water applied. The water reels can be used in the arena or pasture.\nTo help remove dust already in the air, the Turbo XE fans made by Jaybird Manufacturing are ideal for humidifying large air spaces, says spokesman Darren Figart. “They have a great deal of fogging output, are dependable and they can simply attach up to an ordinary water supply line without filtration or nozzles,” he notes. “When the humidity rises in the air, the dust will quickly absorb the moisture and fall out of the atmosphere. During the atomization process, there is also some evaporative cooling that will occur as well, which in many cases is an added benefit.” In most cases, good dust control can be achieved by raising the relative humidity to 40 to 50 percent.\nThe Turbo XE fans make about as much noise as a household vacuum cleaner, but in a large arena that is usually not an issue, especially if the units are mounted up high, Figart says. The foggers can also double for secondary applications—for instance, if the arena needed to be fumigated for sterilization.\nWhen Rubber Meets the Arena\nTodd Opacinch with Surfacing Resources, LLC, recommends use of a non-biodegradable material as the footing to control dust. That material can be sand or rubber or a mixture of the two. There are different sizes of rubber “grains,” from smaller rubber chunks and crumbs to larger shredded rubber pieces sometimes used in breeding sheds.\nRubber has unique advantages. “Any sort of wood or natural fiber product will by nature break down over time, thus causing dust, and it can become a breeding ground for molds and fungus. The inhalation of molds can be an irritant to a horse as much, if not more, than ‘basic’ dust,” Opacinch says.The mixture of rubber and sand helps to reduce dust somewhat, but the primary purpose of the rubber is to suspend the sand mixture and to help reduce compaction, which in turn reduces the breakdown of the sand, reducing the dust.\nOpacinch says it’s important to realize that “not all rubber is created equally. The rubber we use is what we refer to as ‘industrial pure.’ This rubber is recycled, but it is not post-consumer. Our rubber has not been exposed to road pollutants like some rubber that is basically used tires that have been ground up and thrown in a bag for your arena. Our rubber is purchased directly from automobile tire manufacturers or manufacturers of rubber products used primarily in the automotive industry.”\nAdding rubber will not reduce dust in all situations. “If someone has a stone dust arena with one inch of sand, then adding a couple of inches of rubber footing will really help them with dust control and give them a better surface,” says Caldwell. But adding rubber to an arena that has too-deep sand will not help, he adds.\nTim Fisher Sr. with EquiTurf says the EquiTurf arena footing and flooring products can help with dust control in the arena and stable. He said the footing product should be put over a torpedo sand base and watered in the morning, and as needed depending on traffic throughout the day. The footing is made from recycled industrial rubber, free of contaminant and insured to be 100 percent free of wire, metal or other foreign material.\nStarting with a Clean Slate\nPam Jorgenson with Equi-tread says that when possible, “it is always best to build the arena right the first time.” The sub base should be as close to 100 percent compacted as possible. When that step is completed, seek out sand that is slightly coarse and free of fines (the small particles of clay and sand that create dust.)\nJorgenson recommends visiting a local quarry and checking out all the manufactured aggregates available. “As a horse person, you will know what feels right under your feet. When you find the right sand for your arena, throw it up in the air. Is it dusty? Ask if you can have it washed,” she says. Quarries will usually wash it for a small charge or direct you to a cleaner product.\nOnce clean sand has been applied to an arena, Jorgenson recommends adding Equi-tread, a crumb rubber comprising small, irregularly shaped pieces of clean, granulated rubber. The crumb migrates to the top in an ongoing cycle, acting as a mulch to reduce dust.\nJorgenson adds that rubber absorbs more shock than any other standard surface material, so it helps prevent stress-related injuries to horses, and can cushion the fall for a rider.\nStephanie Cornell with Midwest Equestrian says that before anything is done in an arena or stable, the existing footing should be examined. Midwest Equestrian does footing analysis to determine particle size distribution and the application rate of Arena RX, a synthetic, organic fluid, to treat the arena. Arena RX can be used in indoor or outdoor arenas and does not freeze in cold weather climates. “It is colorless, odorless and environmentally safe,” Cornell says. It can also be used on tracks, round pens, horse walkers and stable aisle ways.\nArena RX improves dust control by coating the footing material and helping to deter particle breakdown. It generally needs to be maintained with an application of about 25 percent of the initial application each year, but that can vary on the use of the arena. A public-use arena may need maintenance after eight to 10 months, while a private-use arena may not need it for two years.\n“Not one solution fits all,” Cornell says of the need to find out just what the footing is all about. Once that is determined, Arena RX is easy for a person to apply, and it ultimately eliminates the need to use water. Watering an arena adds compaction and aids in the breakdown of the materials, she says, which changes the footing because it is holding in different amounts of water.\nBarb Dienhart with West Coast Footings recommends adding a product called Pos-A-Trac to control dust. “It is a ground up geo-textile fabric imported from Germany, known as Vlieshaecksel, and very widely used there. It does retain moisture, but only to a certain extent. In other words, in rain or overwatering, it will only hold a limited amount of water before shedding the excess,” she says.\nPos-A-Trac also gives the footing a slight springiness, “enhancing traction and extending sand life,” she adds. “By controlling the moisture, you create an even consistency throughout the ring, which means your horse is paying attention to you and not his feet.”\nA further option is Arena Moist from Absolute Innovations (ABI). According to manufacturers, Arena Moist is a moisture management product that doesn’t just eliminate dust, but attacks the problem before it even starts by maintaining proper moisture levels in the footing. ABI’s Jim Catalano says, “Arena Moist conserves the moisture to an arena at all depths of the footing, resolving most dust issues and even improving the quality of the footing material.”\nWhatever option you choose, both your clients and your horses will be happier with a dust-free arena.', ""The Difference Between Pool Clarifier and Flocculant\nAt Swim University, we are really big fans of flocculant, which we use when we have to clear up our pools really fast. However, flocculants aren’t always the chemical of choice when it comes to clearing up a swimming pool.\nWhat is a Swimming Pool Clarifier?\nWhen it comes to clearing up a swimming pool, your filter is the best man for the job. However, it shouldn’t be the ONLY man on the job.\nClarifiers help your pool filter pick up those tiny, little particles that are making your water cloudy. They do this by (nerd alert) coagulation. Meaning they take those tiny, little particles and bring them together to form bigger particles so that your filter has a better chance of catching them. This makes your filter more effective in clearing your swimming pool.\nSince it helps your pool filter do it’s job, you need your pool filter to run. We suggest that if you have a cloudy pool and you decide to use a clarifier, to run your pool 24 hours a day until your pool is clear.\nAlso, since your filter is doing the bulk of the work, you need to help it by getting those particles into the filter. Here are some basic tips to help your clarifier and your pool filter clear your pool more efficiently:\n- Turn your main drains on: If you have a main drain at the bottom of your pool, make sure it’s turned on to allow the pool water from the bottom to get into your filter.\n- If you don’t have a main drain, you can use your manual vacuum. Just hook it up, as if you were going to vacuum, and leave it turned upside down at the bottom of your pool. Here’s how to set up your manual vacuum.\n- Swim! Swimming helps kick up the particles off the bottom of the pool so that your skimmer can capture the cloudy water.\n- If you have a sand filter, you can buy a filter enhancer, which allows sand filters to act like D.E. (Diatomaceous Earth) filters. This helps your sand filter to pick up smaller particles. I recommend using Bioguard’s Filter Enhancer called Sparkle Up. If you can’t find it, you can use a cup of plain D.E.. Just pour it into your skimmer and the D.E. will lay on top of your bed of sand helping to catch the smaller particles your sand filter might otherwise miss.\nWhat is Swimming Pool Floc or Flocculant?\nPool floc or flocculant has become a fast favorite because…it’s fast!\nThe concept is simple. Floc takes all those particles that are making your water cloudy and sinks them to the bottom of your pool. Then, it will be YOUR job to vacuum them out. What’s great about this chemical is it acts fast, what sucks is, YOU have to do the work.\nSimply add the floc directly to the pool water and let your pool run for 2 hours (if you have a sand filter with a multi-port valve, turn it to recirculate).\nAfter the pool runs for those 2 hours and the chemical has a chance mix into the water, shut the pool filter off completely for the next 8 hours. Make sure you don’t have a timer set; the pool must sit still for 8 hours. I would recommend doing this overnight.\nThe next morning, you will wake up and see a cloud at the bottom of your swimming pool.\nYour job now it to get that cloud out of your pool. To do that, you need to hook up your manual vacuum. You cannot use an automatic cleaner for this job. Also, you need to vacuum your pool on waste. Meaning, you’ll be vacuuming the water OUT of your pool, so keep a garden hose in it to fill it with fresh water while you vacuum.\nIt may take a couple of times vacuuming. If your pool completely clouds up from moving the vacuum around, take a break and allow the particles to settle back to the bottom before continuing to vacuum. If you do this, you can expect to have your pool clear in no time at all.\nWhen To Use A Clarifier or Flocculant\nThe big difference between these two pool clearing methods is time. If you want your pool clear for a pool party you’re throwing soon, we suggest floccing. It’s fast, but it’s work (elbow grease).\nIf you just opened your pool or it’s just a little cloudy and you don’t mind swimming in cloudy water (perfectly safe BTW), then you can use a clarifier.\nClarifiers take time to work, but they are a LOT less work (elbow grease) on your part. Also, since you don’t have to vacuum your pool to “waste,” you will also save money on replenishing chemicals such as chlorine and pH…oh and water.\nNow that you know the difference between the two, next time your water is cloudy, you will have the knowledge to choose which method is right for you.\nHow to Fix Cloudy Pool Water\nGot cloudy pool water? We'll teach you a few methods on how to clear your cloudy swimming pool as quickly as possible.\nClick here to learn more »""]"	['<urn:uuid:681815e5-434c-42ae-a7a6-141501c5e36b>', '<urn:uuid:5db63fa2-9602-4dc5-bd7a-7ab53152d999>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T17:51:21.175311	11	45	2522
53	How can failure in a real world experiment actually be considered a successful outcome when conducting experiments in society?	In real world experiments, if the experiment fails, both the experimenters and the public learn that their hypotheses have been falsified, which makes the experiment a success. This paradoxical element of success in failure makes the concept of experiments in society useful as a metaphorical framework.	['Real world ‘experiments’ are a much needed link between scientists and society, argues Matthias Gross\nConceptualising processes outside the laboratory as ‘experiments’ may seem odd, but the idea goes back at least to Francis Bacon. On the one hand, the experimenter’s realm was to be viewed as distinct from the world of the objects experimented upon. On the other, Bacon argued that approval of the experimental method in science would turn society itself into a large-scale experiment.\nThe recent upsurge in debate about technologies as social experiments, real world experiments in public participation and so-called ‘urban laboratories’ as elements in planning city spaces has created need for clarification. What is the usefulness of talking about society as a laboratory? What are the essential features of an experiment when it is not taking place in such clearly defined institutional realms?\nLocked away in the lab\nFirst of all, the laboratory confines the practical problems and risks of research within its walls, away from wider society. Only as accidents do the effects of laboratory experiments have a direct and immediate influence on the public. Deliberately designed real world experiments necessarily subvert this norm.\nFurthermore, they at least partially unload some of the uncertainties of research into social systems. This changes many things. The line between experimental subject and object, for example, becomes blurred. Additionally, key premises relating to scientific predictability, certainty and experimental repeatability are altered.\nA lot of the current initiatives in urban planning and participatory research also locate the planned unexpectedness of experiments at the core of their strategies, while giving it a positive attitude. An experiment in society might then be defined as a cautiously observed venture into the unknown and deliberately arranged to generate surprising events in order to learn from them. If we take this definition as the core of experimentation, real world experiments look quite similar to laboratory experiments. The clear communication of risks and unknowns in public can then be seen as a form of hypothesis building. If the real world experiment fails, the experimenters as well as a concerned public know that their hypotheses have been falsified, and by extension the experiment was a success.\nIt is this seemingly paradoxical element of success in failure that makes the notion of experiments in society or a laboratory in society useful as a metaphorical framework. It may seem counterintuitive in a society obsessed with safety, resilience, precaution and risk avoidance, but this framework opens up new horizons of thinking and acting while coping with the unavoidable uncertainties and ignorance in many areas of research.\nExperimentation outside the laboratory is characterised by knowledge based on the most generalised principles, often taken from laboratory models. Such principles are crucial for planning any type of scientific intervention. But knowledge that is highly site specific is also needed, as unexpected problems may occur during operation in a human community. Unfortunately, such knowledge often does not exist, while generalised knowledge does not yield the specific information required for a concerned public.\nUnder public scrutiny\nTo learn about site specifics, we must leave the unburdened locales of the laboratory and move into the real world – that is, the ‘fuzzy’ world of economic interests, policy making and stakeholder participation. This involves complex combinations of social and natural factors. Such forms of experimentation are not trial and error processes or tinkering around. They need clear rules and prerequisites. It must be apparent, for example, that other responsible means for gaining the new knowledge needed to move forward do not exist or are otherwise unavailable. Of course, real world experimentation must not have lower standards of safety – quite the contrary, the containment of hazards is crucial, and there must be clear social benefits. These forms of experiments in society thus ask for very carefully planned and democratically supported processes. They may yet be rendered the ‘acid test’ of experimentation.\nTaking this view on experiments in society means that they are the ‘real’ experiments, because they not only involve scientists but also the wider society as ‘co-experimenters’. One may even provocatively say that experimental science taking place in and with society demands higher epistemic standards of research – the researchers need to convince not only their scientific peers but also the wider public. This may amount to little more than re-phrasing otherwise well-established forms of public science, but real world experiments, thus understood, can be an important tool to communicating unavoidable uncertainties inherent in any form of science and technology development. Experimenting in society, therefore, should not be considered an inferior form of experimenting, but an important part, or ‘extension’, of scientific activities. It is able to deliver information and social acceptance that cannot be derived from controlled laboratory experiments. Debates on real world experiments thus take seriously the democratic call for public participation and help to clarify the close coupling between research and society both for a concerned public and for the scientists involved in the research.\nMatthias Gross is professor of environmental sociology at the Helmholtz Centre for Environmental Research UFZ in Leipzig and – by joint appointment – the University of Jena, Germany']	['<urn:uuid:a71cd947-daac-4b5f-867c-d81ccfc1d95c>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	19	46	852
54	construction safety training buddy supervision rules	New employees participate in a red hat buddy system where they are paired with a senior buddy for their first 30 days of employment for close supervision. For fall prevention specifically, workers are advised to use a buddy system when working on roofs, and a designated person must be trained and assigned to supervise scaffold setup.	['Key Safety Processes\nOn a corporate level, we have established a comprehensive standard template for our Project Safety & Health Plan. The managers of every project will utilize this template, further supplementing it with project- and site-specific requirements. The document is to be completely understood and committed to by the entire project team. It is our roadmap to ensure that we provide a safe work place for each employee, that we comply with all OSHA regulations, contract-required safety rules, and project specifications. This consistency in our safety approach will continue to build our best-in-class safety culture.\nFor all projects, TBI has employed a safety process/culture review process. The goal of this program is to promote craft involvement, continuously improve safety culture, monitor job site policy/procedure conformance, and promote proactive behavior. The review is performed by a three-person team (one corporate manager, one senior supervisor/manager, and one engineer) not connected to the project. Two-day reviews are completed on each project, one every four months, and result in a written evaluation and debrief meeting. Our project teams receive consistent, constructive feedback that results in a safer work place for every team member.\nIn addition to the standard 30-hour OSHA class, TBI has developed five training modules to assist field supervisors in improving the quality and consistency of safety practices. The modules include: Giving Orientations; Effective Communication; Hazard Recognition; Unsafe Behaviors; and Accident Management. They are led by senior project managers who provide real time examples, and are given to all supervisors on every project. The modules are structured so that they may be taken over a period of time and include leader guides, quizzes, and power point presentations. The combination of specific task enhancements, as well as improving soft skills, encourages a superior safety culture on all of our projects.\nTBI provides every new employee with a comprehensive classroom orientation program that reviews and explains all aspects of our safety program. In addition, we provide a field orientation and red hat buddy system. The training begins with the general superintendent, who provides an overview of the field orientation. Once complete, the employee is provided with more specific indoctrination and training by his specific craft supervisor or foreman. Follow-up interviews are performed by the general superintendent within the first week of employment to ensure the employee understands his or her role, is committed to work accident free, and will never walk past unsafe conditions or acts. Finally, new employees are part of the red hat program, where they are issued red hard hats to indicate they are new to the project.\nThese employees graduate from the red hat program when their supervisors feel they have proven to be part of the safety team. Also as part of the red hat program, each new employee is teamed with a senior buddy for the first 30 days of employment, who works closely with and watches over the new employee during this time. Although the orientation process ends for each individual, our employee safety training never stops. We are committed to continually training our people and building a best-in-class safety culture.\nTBI is committed to employing the best safety professionals in the industry. We have established a new hire and intern program specifically for safety professionals to strengthen our pool of talent and to ensure we always have safety professionals growing in our organization. We will develop these professionals throughout their career, providing advancement along the way. The role of safety professional is very important within Traylor Bros., Inc., and our senior management empowers them to “ensure every team member goes home safe every day.”\nNew Safety Programs\nCraft Driven Safety Initiative\nThe program places craft workers in a position of leadership within our on-site safety programs, giving craft a direct voice in the management of their own safety.\nA year-long program that features a safety topic related to making safe choices each month, with weekly toolbox meetings that involve more detailed discussion on the subject, keeping everyone mindful of the need to consciously make safe choices every day.', 'Preventing falls in construction\nAccording to data from the Bureau of Labor Statistics, falls are the leading cause of death among construction workers on the job, who average more than 310 fatal falls and 10,350 serious fall-related injuries a year. Additionally, the majority of fatal falls from scaffolds (86 percent), roofs (81 percent) and ladders (57 percent) occur in construction with a disproportionate number of those injuries and fatalities coming from small residential construction contractors.\nThe sixth annual National Safety Stand-Down to prevent falls — established by the Occupational Safety and Health Administration (OSHA) in 2014 as part of the National Falls Campaign to raise awareness surrounding the severity of fall hazards in construction and the importance of preventing them — will take place the week of May 6–10, 2019. The goal of this national campaign is to prevent fatal falls from roofs, ladders and scaffolds by encouraging residential construction contractors to plan ahead to get the job done safely, provide the right equipment and train everyone to use the equipment safely.\nConstruction sites are dynamic spheres of activity where workers engage in many responsibilities that may expose them to a variety of safety hazards such as falling objects and working from rooftops or scaffolding. The International Code Council is just as concerned about jobsite injury prevention and ensuring the safety of code officials and construction workers as it is about building safety and making our communities safer and more resilient. Starting May 1, the Code Council kicks off Building Safety Month 2019, which raises awareness about building safety and building safety building codes. Week five of the international month-long event covers Innovations in Building Safety and shows how science and technology are leading the way to not only designing and constructing safe, efficient and resilient homes and buildings but also for ensuring job site safety for construction workers, code officials and other skilled technicians.\nIn a construction setting, the term “safety stand-down” is used to describe a wide variety of activities where normal work is paused and the entire site focuses on a particular safety issue. While some employers may use stand-downs to call attention to specific hazards present on their site as an intervention, the campaign has adopted the use of the stand-down to draw attention to fall hazards in a nationwide weeklong event. It’s an opportunity for employers to pause work and have a conversation with workers about fall hazards, protective methods, and the company’s safety policies, goals and expectations. This should also be an opportunity for workers to talk to management about fall hazards they see.\nThe National Institute for Occupational Safety and Health’s Fatality Assessment and Control Evaluation Program studies fatal workplace injuries and has published a new fact sheet intended to help construction employers and workers prevent falls from roofs, ladders and scaffolds. The fact sheet, which stresses the importance of training and regular ladder and scaffold maintenance, offers several recommendations for avoiding falls:\nFor workers on roofs\nImplement a fall protection program, wear proper fall protection and undergo corresponding training.\nUse correct anchorage for fall arrest systems.\nUse a buddy system.\nMonitor weather conditions when working at height.\nFor workers on ladders\nChoose the right ladder for the job.\nDon’t overload the ladder.\nUse the ladder on flat, level ground.\nFace the ladder while climbing and maintain three points of contact at all times.\nFor workers on scaffolds\nTrain and assign a person to supervise scaffold setup.\nEnsure scaffolds comply with manufacturer guidelines and OSHA standards.\nPlace scaffolds on stable ground or surfaces.\nInspect scaffolds and scaffold parts before each use.\nVisit the following sites for more information and resources:\nCDC Campaign to Prevent Falls in Construction\nThe National Institute for Occupational Safety and Health Falls Campaign Site\nOccupational Safety and Health Administration Falls Campaign\nStop Construction Falls Campaign']	['<urn:uuid:92dd3b31-ab9d-4898-9ae3-9771871bd1f3>', '<urn:uuid:99c59d58-5387-42e9-af94-32b7429749b6>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T17:51:21.175311	6	56	1309
55	What are the unique astronomical features at these Incan sites?	Both Machu Picchu and Pisac contain important astronomical features centered around sun worship. At Machu Picchu, the Temple of the Sun has a trapezoidal window that illuminates a granite stone calendar during the winter solstice and provides views of the Pleiades constellation. Similarly, Pisac features one of the few surviving intihuatanas (sun clocks) carved directly into pink granite rock, surrounded by a D-shaped temple and perfectly aligned with the June solstice.	"['Machu Picchu (literally, ""old peak"") is a ruined ancient Incan town high on a mountain ridge in Peru. One of the most atmospheric and mystical sites in the world, it is located among spectacular mountain scenery about 6,750 feet above the Urubamba Valley. Machu Picchu has been designated a UNESCO World Heritage Site and it is the end point of the most popular hike in South America: the Inca Trail.\nHistory of Machu Picchu\nIt is generally thought that the city of Machu Picchu was built by the Sapa Inca Pachacuti starting in about 1440 and was inhabited until the Spanish conquest of Peru in 1532.\nArcheological evidence combined with recent work on early colonial documents indicates that Machu Picchu was not a conventional city, but rather a country retreat town for Incan nobility (similar to the Roman villas). It is estimated that a maximum of only about 750 people resided in Machu Picchu at any one time, and probably only a small fraction of that number lived in the town during the rainy season and when no nobility were visiting.\nIt is still unknown exactly what role the site played in Incan life. But it was clearly a remarkably well hidden place, and well protected. Located far up in the mountains of Peru, visitors had to travel up long valleys littered with Incan check points and watch towers.\nRemarkably, the Spanish conquistadors missed the site, and the Inca city remained hidden until the 20th century. On a wet day in 1911, Yale scholar Hiram Bingham travelled up the slopes with a few companions from an expedition. He met some local peasants, who told him about ancient ruins that covered the area. He went to see them, and found the lost Inca city of Machu Picchu.\nIn 1913, the site received a significant amount of publicity after the National Geographic Society devoted their entire April 1913 issue to Machu Picchu. Bingham made several more trips and conducted excavations on the site through 1915. He wrote a number of books and articles about Machu Picchu; his account, Lost City of the Incas, became a bestseller.\nIn 2003, some 400,000 people visited Machu Picchu, and UNESCO has expressed concern about the damage this volume of tourism is causing to the site. Peruvian authorities insist that there is no problem, and that the remoteness of the site will impose natural limits on tourism. Periodically, proposals are made to install a cable car to the site, but such proposals have so far always been rejected.\nWhat to See at Machu Picchu\nThe site was probably chosen for its unique location and geological features. It is said that the silhouette of the mountain range behind Machu Picchu represents the face of the Inca looking upward towards the sky, with the largest peak, Waynapicchu, representing his nose.\nThe Inca believed that the solid rock of the Earth should not be cut, so they built this city from rock quarried from loose boulders found in the area. Some of the stone architecture uses no mortar, but rather relied on extremely precise cutting of blocks. Some of the spaces between the stones are so tight that not even a credit card will pass through them.\nThe ruins on Machu Picchu consist of a large palace and temples to Incan deities around a courtyard, with residential buildings for support staff. Entrance is at the southern end of the site, where there is a ticket office. From there, visitors are free to wander through the ruins at leisure, beginning with the agricultural precinct on the southern terraces.\nThere are no signs on any of the structures, which contributes to the atmosphere of antiquity, personal discovery, and harmony with the natural environment for which Machu Picchu is so beloved. To help identify the ruins, guidebooks and maps are available at the entrance. The names of the structures are the English names given by Bingham, which in many cases do not correspond with the actual uses of the buildings.\nA 20-minute walk uphill to the left inside the entrance leads to the House of the Terrace Caretaker and Funeral Rock, which provides the famous and very photogenic view over the ruins of Machu Picchu.\nNear the south end of the city is the magnificent Temple of the Sun, whose walls are fine examples of precise Inca masonry. The temple is astronomically aligned. On the winter solstice (June 22 in the southern hemisphere), sunlight streams in a small trapezoidal window and illuminates a flat granite stone that was probably an Inca calendar. At night, the same window provides a view of the constellation Pleiades, an Inca symbol of crop fertility.\nThe Principal Temple has perhaps the best masonry on Machu Picchu. The stones of its three walls are precisely cut and fitted together without mortar. A small building next to the temple, dubbed the Sacristy, may be where priests prepared themselves for ceremonies.\nThe Temple of Three Windows is a three-walled building reached via a stone staircase. Its east wall is cut from a single piece of rock and pierced with three trapezoidal windows.\nFurther on is Intihuatana, the ""hitching post of the sun"" whose exact purpose remains a mystery. This sacred stone column was a common feature in Inca cities, but the Spanish destroyed most of them as objects of idol worship. This is therefore one of few to survive, since the conquistadors never made it to Machu Picchu. Nearby is a sacred rock that echos the shape of the mountain range behind it.\nAt the north end of the site, the Temple of the Condor is built in the shape of a condor, the Inca symbol of heaven.\nSpectacular views from the north side of Machu Picchu can be had from Wayna Picchu (also spelled Huayna Picchu), a mountain peak that overlooks the ruins. The climb takes 90 minutes and is dangerous after wet weather. Even in good conditions, it is not for the faint of heart when it comes to heights. The path is open 7am-1pm daily and only 400 visitors are allowed to climb each day.\nMany buses leave Cuzco for Machu Picchu and there are frequent trains from Cuzco to Aquas Calientes, the town below Machu Picchu. Buses from Aquas Calientes bring tourists to the ruins. It is also possible to take a helicopter from Cuzco.\nOne of the many advantages of hiking the scenic Inca Trail is that you can enjoy Machu Picchu in relative solitude before the busloads of tourists show up around 10am.\nQuick Facts on Machu Picchu\n|Categories:||sacred mountains; city ruins; astronomical alignments; World Heritage Sites|\n|Visitor and Contact Information|\n|Coordinates:||13.162859° S, 72.546008° W|\n|Address:||Ucayali Department, Peru|\n|Lodging:||View hotels near Machu Picchu|\n- Machu Picchu - Fodor\'s Peru\n- Ben Box, Footprint South American Handbook 2009 (September 2008), 1442-48.\n- Machu Picchu - Wikitravel\n- Machu Picchu Portal - in-depth information about the Incas, the Sacred Valley and Machu Picchu\n- Row erupts over Peru\'s tourist treasure - BBC News, December 27, 2003\n- Inca site cable car plan sparks anger - BBC News, August 23, 1999\n- Machu Picchu on the Web - annotated links\n- Rediscovering Machu Picchu by Bruce Fellman, Yale Alumni Magazine, December 2002.\n- Virtual tour of Machu Picchu and Peru - Destination Peru\n- Machu Picchu, Lost City of the Incas by Antonio Gutierrez\n- Field Museum exhibit website Chicago, Oct. 15, 2004 - Feb 13, 2005.\n- Machu Picchu on the Inca Trail\n- Inca Trail to Machu Picchu - photos of Machu Picchu\n- Photos of Machu Picchu - here on Sacred Destinations\nMap of Machu Picchu, Ucayali Department\nBelow is a location map and aerial view of Machu Picchu. Using the buttons on the left (or the wheel on your mouse), you can zoom in for a closer look, or zoom out to get your bearings. To move around, click and drag the map with your mouse.', 'High above the Andean town of Pisac, carved into a rock face that looks out over a valley of soft green terraces, there is a tunnel. A slender, teardrop-shaped slit arrived at via a narrow pass. This tunnel extends for 16 meters through an outcrop of solid granite, and is just wide enough for one person to traverse single file.\nIt’s not clear how this passage was made. The Incas, the builders, had no iron or steel tools, and the andesite in this part of the cordillera is flinty and unyielding. A rough calculation indicates some 120 tons of rock had to be displaced to bore through.\nNor is the tunnel’s purpose entirely clear. If it were a question of a simple walkway, there are ledges adjoining that presumably could have been hewn to this end. Elsewhere on the same ridge, one finds defensive gates and towers, but the cliffside pass seems an unlikely spot to mount a resistance against invaders.\nA major engineering feat, then, with no clear purpose, and no obvious means of pulling it off.\nIn other words, a mystery that’s entirely typical of Pisac.\nHistorians today are fond of reiterating just how enigmatic the Incas remain to us, that what we do know about them pales in comparison with what we don’t. But Pisac takes that mysteriousness to a whole other level. Of the major archaeological sites in Peru’s Sacred Valley—Sacsayhuaman, Ollantaytambo, Machu Picchu—it presents the most intractable question mark.\nIt’s a case of too little information, clearly. Regarding Pisac’s function, the historical record is conspicuously silent. None of the chroniclers from the Spanish Conquest mentions it, no recorded battles took place there, no swashbuckling Yalies looking for lost cities broadcast its existence to the world.\nWhat Pisac does have is aesthetics, and in spades. Comprising over four square kilometers of agricultural terraces dotted with clusters of striking edifices, it’s five times the size of Machu Picchu, with some of the most heart-stopping views in Peru. The stonework, as well, is legendary, and includes one of the few surviving intihuatanas, the strange solar clocks the Incas used to worship the sun.\n“Nowhere,” historian Victor W. von Hagen once pronounced, “in all the empire of the Incas is there finer masonry, nor is there elsewhere, even at Machu Picchu, a plan so bold.”\nWhich means that visiting Pisac, one feels oneself close to the heart of the Incas’ mystery.\nSwords or Plowshares?\nThe chief puzzle surrounding Pisac is just what exactly it’s supposed to be.\nPartly the problem is the site’s architectural diversity. Barracks, fortifications, civilian residences, temples, aqueducts, agricultural precincts: no other Inca ruin, including Machu Picchu, encompasses such a broad array of infrastructure.\nWhich buildings, though, define the site, give it its center?\nThat depends on who you ask.\nSome scholars, focusing on the complex’s sweeping agricultural terraces, argue that Pisac is a kind of royal hacienda or estate built by the Inca Pachacutec in the mid-1400s. According to this theory, the visionary emperor erected a rural villa to commemorate his defeat of the Cuyos, a rival Andean group who resisted Inca rule. Pisac’s terraces thus would have provided corn, potatoes, and peppers for yet another imperial country home, a la Machu Picchu.\nBy contrast, more militarily minded researchers emphasize the complex’s defensive nature: swords, not plowshares. They maintain that Pisac was one of a string of Inca pukaras—citadel communities—that stretched across the Andes, with agricultural plots to make them self-sustaining. These pukaras served as fortified areas to which the surrounding population could retire if an enemy attacked. From this perspective, Pisac is a strategic nexus for the control of the Urubamba River Valley, a valuable gateway to important locations like Paucartambo and the Amaru Mayu River.\nBoth theories have their problems. If Pisac was a royal retreat, where’s the royal palace to house the king? The architecture is decidedly modest, compared to the sublime heights of, say, Machu Picchu. On the other hand, if fortifying the valley was what the Incas had in mind, why are the installations so spread out, thus weakening the ridge’s defensive barrier?\nWhatever scholarly opinion ultimately decides, one thing is clear: the Incas went out of their way to work their installations into the surrounding landscape. Nearly all historians agree the complex was conceived as an aesthetic whole. Indeed, since p’isaqa is Quechua for “partridge,” some have tried to trace a partridge-like formation in the layout of the site—taking as their precedent the puma-like contours of Cuzco itself.\nStrolling through the Neighborhoods\nPisac is divided into four separate building clusters, or neighborhoods. The best way to see them is to take a bus to the parking lot at the top of the ridge and then walk down. This way you can avoid the taxing climb from the hill’s foot.\nThe circuit starts with Qantu Rachay, the first neighborhood on your right as you leave the bus lot. Built on three levels and looking something like a castle in a medieval town, these rough stone structures may have served as a military garrison or shelter for local villagers in times of war. More important, though, is the view they afford. Just to the west lie the first terraces, or andenes, cascading steps built into the mountainside to maximize the terrain available for crops. The steps are 12 feet high in places, with walls of reinforced stonework and irrigation channels designed by Inca engineers to carry springwater from the peaks above.\nThe terraces also conceal a secret. Just beyond them is Tantana Marka, which translated means “steeps of lamentation.” Etched into the side of the limestone cliffs, this cemetery of some 3,500 Inca tombs is the largest in all Peru. The graves, which look like mud swallows’ nests, were the resting places of nobles buried in the fetal position and were unrifled up to the end of the 19th century.\nPassing beyond the graves, you arrive at Q’alla Q’asa, the hilltop complex that served as the site’s main residential area. Nearly all its 30 buildings are rock stuck together with adobe, indicating they were probably inhabited by lower-status workers. From a distance, however, their high vantage point makes them look august, like a fortress reaching to the sky.\nAltars to the Sun\nCurving to the south along the ridge, the next stop is Amaru Punku, the Serpent Door, which served as a control point to prevent invaders from crossing the complex’s walkways. This is followed by the 16-foot tunnel, hollowed from the rock.\nIt’s just beyond this point that Pisac’s true glory lies.\nThe ceremonial center, also known as the Sun Temple after its most famous attraction, is one of the marvels of Inca stonemasonry. Hewn largely of pink granite, it features one of the few existing intihuatanas, the so-called “hitching posts of the sun” found at other Inca holy sites like Ollantaytambo and Machu Picchu. Inca priests used these gnomons or sundials to chart the movements of the heavens and anticipate the approach of the solstices and equinoxes—though exactly how remains unclear.\nIn the case of Pisac, the intihuatana is cut directly into the rock of the promontory, like a Frank Lloyd Wright house growing organically out of its cliff. This rock is surrounded by a D-shaped temple and oriented so that it aligns perfectly with the sun on the June solstice. There is also an altar for animal sacrifices, as well as a ceremonial fount where water still flows. Nearby, a chakana or three-tiered cross alludes to the three worlds of Inca cosmology: heaven, earth, and underworld.\nLeaving the intihuatana, one more residential area awaits you below yet another swath of terraces. The boxy cluster of P’isaqa is more refined in its finish than the stone-and-adobe constructions of Q’alla Q’asa. Indeed, this radially organized group of domiciles sports some of the most perfectly finished rocks in the Tahuantinsuyu, according to the great Inca historian John Hemming.\nDid Pachacutec stay here during his outings to the complex? Did the buildings house the priests and generals in charge of Pisac’s various precincts?\nPisac offers few clues to these and other cruxes. Yet in its very mysteriousness, its stones afford visitors a window onto the deep silence of the Incas’ Sacred Valley.']"	['<urn:uuid:899a1dea-60c0-4645-9df6-11ddd51b49ca>', '<urn:uuid:a04315d0-a48e-426d-8bfe-b41fd317d31d>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T17:51:21.175311	10	71	2692
56	implement afrocentric solutions programs african canadian street involved youth	One effective Afrocentric approach being used with youth is rites of passage programs, where elders and community members support youth in their transition to adulthood. These programs focus on consciousness-raising, developing healthy living habits, respect for elders, and community involvement and responsibility. Evidence shows these programs have had positive impacts when used with youth involved in child welfare and youth in conflict with the law.	['Anti-Racist Praxis with Street-Involved African Canadian Youth5 years ago 5 years ago Leave your thoughts\nYouthREX Research Summaries ask Just Six Questions of research publications on key youth issues. These summaries get at what the youth sector needs to know in two pages or less!\n1. What was this research about?\nThis research is about how racism influences African Canadian youth’s experiences of street involvement and mental health challenges. Some research has shown that African Canadian youth are overrepresented among street-involved youth, and there is a lot of evidence that race and racism impact mental health for racialized people. As such, this chapter offers youth workers interested in taking an anti-racist perspective tools to think about when working with street-involved youth.\n2. Where did the research take place?\nThis research is a literature review that covers various strategies that can be used to implement an anti-racist approach when working with African Canadian youth who are street-involved.\n3. Who is this research about?\nThis research is about African Canadian youth who are street-involved. The authors state that, while they use the term ‘African Canadian’ to refer to youth who have indigenous sub-Saharan African ancestry, they recognize the great diversity encompassed within this broad category. This research is also about people who work with street-involved youth and how they can take an anti-racist approach to their work.\n“…race and racism influence the experiences of street involvement and mental health among African Canadian youth. We…offer a few analytical and practical tools for practitioners to consider for engaging street-involved youth from an anti-racist perspective” (p. 125).\n4. How was this research done?\nThe authors reviewed relevant literature on working with African Canadian youth who are street-involved and summarized their findings into suggestions for practice.\n5. What are the key findings?\nIn the past, many approaches to working with racialized and African Canadian youth revolved around ‘cultural competence’ or multiculturalism models that focus on understanding the traits and values of specific cultural groups and then customizing programs and approaches to fit these traits and values. However, these approaches often fall short, as they do not account for issues of power, and may ignore important differences and nuances within a group. As such, the authors of this chapter offer suggestions about approaching youth work in ways that encourage the practitioner to take a strengths-based approach, be reflective about their own power, and take an explicit stance against racism.\nThe following outlines intervention components and implementation considerations:\na) Intervention Components\ni. Anti-Racist Praxis\nPraxis refers to an ongoing process of taking a theory, putting it into action, and then reflecting on the action taken. The theories/concepts associated with anti-racist praxis give us conceptual tools through which we can analyze and challenge racial discrimination and oppression.\nKey Concept 1 – Anti-Black Racism: acknowledges that Black Canadians face a specific type of racism that is different from racism faced by other groups, both in the forms it takes and its extent; as such, interventions that specifically deal with anti-Black racism are needed.\nKey Concept 2 – Critique of Colour-Blindness: recognizes that racism is deeply embedded in local and global structures, and that the belief that race does not matter ignores the real impacts of race on people’s lives.\nKey Concept 3 – Intersectionality: examines how multiple aspects of identity shape people’s experiences of privilege and oppression.\nKey Concept 4 – Microagressions: recognizes the harm caused by (un)intentional, everyday, subtle, (non)verbal insults, slights, and jokes that have a racial connotation.\nAllyship is a contested term; however, according to the authors, an ally is someone who recognizes that the privilege they have in relation to others is a result of the unjust patterns of a society, and commits to changing these patterns. The authors offer some tips for how youth workers can better practice allyship:\n- Listen more than you speak and be aware of how much space you take up in discussions/conversations.\n- Commit to ongoing learning about anti-Black racism and whiteness from existing resources.\n- Be receptive to feedback and criticism from people of colour. If someone points out something you do/say that is hurtful, maintain a stance of humility and take this feedback seriously.\n- Help amplify voices by supporting organizations for/by marginalized people. Support can take the form of fundraising, advocacy, or recruiting other allies.\nChallenging racism can be intense emotional and intellectual labour. Allies should not expect people of colour to shoulder the responsibility for this work. Rather, by reflecting on and challenging racism whenever they encounter it, allies can commit to taking on some of this difficult work.\niii. Afrocentric Approaches\nAfrocentric thought was developed in the 1970/80s and sought to re-centre African values, languages, traditions, etc., that are often pushed to the margins by the dominant Eurocentric framework of Western societies. It was argued that an Afrocentric approach could help build resilience, and reshape a person’s worldview by reintroducing and reclaiming important African symbolisms, names, practices, etc.\nCurrently, an Afrocentric practice approach that has shown promise and is being used with youth is rites of passage programs. In these programs, elders and community members support youth in their transition to adulthood, which is a common tradition in many African cultures. By focusing on consciousness-raising, developing healthy living habits, respect for elders, and community involvement and responsibility, rites of passage programs can help youth achieve better outcomes. Evidence on these programs has shown that they have had a positive impact when applied with youth involved in the child welfare system and youth in conflict with the law.\nOne way youth workers can support street-involved African Canadian youth is to connect them with existing rites of passage programs. The following questions are used to guide the rites of passage process with youth in order to re-centre African identity (p. 132):\n- Who am I? What values, history, traditions, and cultural precepts do I recognize, respect, and continue?\n- How did I come to be who I am? What were/are the forces, events, and people that have come together to frame who I am?\n- Am I really who I think I am? To what extent do I understand, internalize, employ, and reflect the cultural authenticity of my origins?\n- What is my life purpose?\niv. Faith-Based Family Mediation\nThere are practitioners who argue that this approach may be a promising practice when working with African Canadian youth who are street-involved and their families. As faith-based organizations have tended to play a big role in African Canadian communities, this type of intervention builds on existing relationships and trust. To determine whether this approach may be a good fit, youth workers should talk to youth about whether they are connected to any faith community and whether they are interested in using this community as a resource.\nAs faith-based organizations have historically perpetuated homophobic, transphobic, and sexist ideas, it is important (especially when working with LGBTQ2S youth) that the faith-based organization involved is invested in maintaining a safe space that relies on anti-oppressive values.\nb) Implementation Considerations:\ni. Advocacy and Structural Change\nTo engage in anti-racist praxis, we need to see mental health and homelessness not as individual issues, but, rather, to understand the structures and processes that create inequities. Once we take a broader view of the challenges people face, we are forced to re-evaluate what our role is in changing these structures and processes to work in more just and equitable ways. As frontline youth workers see firsthand how youth are impacted by policies and systems, they are ideally positioned to advocate for change, especially if they are able to meaningfully engage youth in these efforts.\nii. Policy Development\nHousing, child welfare, and criminal justice policies often deepen the marginalization and injustice that African Canadian youth face. As such, shaping these social policies is an effective way to make a positive impact for many young people. Frontline youth workers can get involved in policy development by participating in community consultations, joining community agency boards or think tanks, or partnering with university-based researchers on projects that investigate the impacts of policies on youth.\niii. Community Mobilization and Activism\nPolicy change often moves slowly, taking incrementalist or accommodationist approaches that don’t produce the immediate changes needed for a more just society. As such, people committed to ant-racist praxis may engage in community activism/social action to bring awareness to an issue and pressure those with power to act more urgently.\nThis work is often done outside of the formal professional role of a youth worker, and it can involve complicated political, ethical, and professional questions for the worker. However, the authors argue that despite these possible challenges, social action is a critical part of anti-racist praxis. Approaches to community activism can include protest and acts of civil disobedience, arts-based activities like street theatre, as well as more indirect actions like engaging in awareness campaigns, publishing articles on an issue, or signing petitions. Engaging youth in these activities alongside youth workers can create a sense of empowerment and teach useful skills.\n6. Why does it matter for youth work?\nYouth workers engaging Black youth need to shift their focus from thinking solely about service models to considering how they can use critical reflection and praxis.\nThere are already culturally relevant services and resources that exist within communities; youth workers should build their knowledge of these resources so they can try to connect youth to them.\nAdvocating for structural change to mitigate racism should not be seen as outside of the scope of frontline youth workers’ roles; rather, those who are interested in ant-racist praxis need to expand their understanding of their role to include this critical systems-level change work.\nHasford, J., Amponsah, P., & Hylton, T. (2018). Anti-racist praxis with street-involved African Canadian youth. In Kidd, S., Slesnick, N., Frederick, T., Karabanow, J., & Gaetz, S. (Eds.), Mental Health and Addiction Interventions for Youth Experiencing Homelessness: Practical Strategies for Front-Line Providers (125-138). Toronto, ON: Canadian Observatory on Homelessness Press. Retrieved from https://www.homelesshub.ca/MentalHealthBook\nCategorised in: Research Summary']	['<urn:uuid:576aafb9-1819-4b43-9805-c197c4b022d0>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	9	65	1665
57	How did Midway and Port Chicago shape racial progress in WWII?	While Midway demonstrated effective U.S. combat capability by sinking four Japanese carriers and destroying 248 enemy aircraft in 1942, Port Chicago's tragedy and aftermath in 1944 catalyzed racial justice reforms. At Port Chicago, unsafe working conditions and racial discrimination led to an explosion killing 320 men, mostly black sailors who were restricted to dangerous manual labor. The subsequent mutiny trial of 50 black sailors who refused to return to unsafe munitions loading became a pivotal event that helped persuade the U.S. Military to begin addressing racial inequality.	['It was at Midway where the U.S. demonstrated effective combat capability very early on – inflicting severe damage on a motivated and experienced enemy.\nHere’s What You Need To Remember: Midway was, in many ways, a more decisive battle than D-Day. The loss of four of Japan’s carriers stopped the Rising Sun dead in its tracks, giving America space to reinforce the Pacific – and slowly retake it.\nThursday, June 6th saw the 75th anniversary of the Allied invasion at Normandy, the amphibious assault phase of Operation Neptune, or what we commonly remember as D-Day. U.S. troops who landed at Normandy – particularly at Omaha Beach – waded ashore amidst a storm of chaos, a blizzard of machine gun fire, and a hail of plunging mortars. Despite great confusion and casualties, at the squad level and below, the men at Omaha rallied and pressed forth with tenacity and nerve to breach sand-berms and barricades, neutralize enemy positions, and salvage their sectors. Losses at Omaha were immense – but American resolve helped establish a foothold on the coast of France – and “the rest,” they say, “is history.”\nWithout doubt, the enormous importance of D-Day as a logistical and operational undertaking – and the gallantry of Allied forces that June morning is unquestioned. It rightfully exemplifies American character, courage, and commitment. However, it is important to note that as far as the battle’s strategic significance is concerned, a strong case can be made that other battles of World War II are more critical than D-Day.\nThe Battle of Midway in 1942 is one.\nToday – June 7th – is the 77th anniversary of the Battle of Midway, an engagement that not only follows one calendar day after D-Day but is a battle that is consistently considered to be a critical turning point for America in World War II. Midway was likely the most strategically significant battle for the U.S. in the Pacific Theater. Not only did U.S. Naval forces halt Japan’s dynamic and multi-pronged advancement across the Pacific at Midway, but the battle occurred in midyear 1942 when victory for the Allies was far from certain.\nWhile the tactical result of the battle was stunning – the U.S. sunk four Japanese fleet carriers Hiryu, Soryu, Kaga and Akagi, a heavy cruiser and destroyed 248 enemy aircraft – it is the perilous backdrop of America’s war fortunes in 1942 that make Midway’s tide-turning outcomes all the more significant.\nRecall that Midway occurred only six months after the attack on Pearl Harbor – a period when America’s military and industrial capabilities were a far cry from the potent war-machine they would be by 1944 and ’45. Aside from Colonel Jimmy Doolittle’s gutsy bombing raid in April, demoralizing defeats had largely characterized the Allied combat experience in early 1942 – from Wake Island, Malaysia, Singapore, and the Philippines in Asia, to the disastrous Raid on Dieppe in France. Other engagements – such as the Battle of the Coral Sea – were more akin to arguably draws than outright victories. Midway halted that trend with a shocking blow.\nMidway is also crucial because of when it occurred in the context of Allied Grand Strategy for the war. Although both Roosevelt and Churchill had agreed to a “Europe First” approach to defeating the Axis Powers, in mid-1942, it was in the Pacific Theater where the U.S. was executing significant offensive operations with joint forces and combined arms. It was at Midway where the U.S. demonstrated effective combat capability very early on – inflicting severe damage on a motivated and experienced enemy. Conversely, American efforts in the European Theater in 1942 primarily consisted of naval blockade and convoy protection in the Atlantic and continued material support to Great Britain and the Soviet Union. It wouldn’t be until Operation Torch in late 1942 where the Allies – led by the U.S. – opened a genuine “Second Front” on Europe’s doorstep.\nIt is important to remember that by the time Overlord was launched in mid-1944, the noose around the neck of Nazi Germany had been steadily tightening. Italy’s role as an Axis power had been significantly reduced (having surrendered in 1943), the Wehrmacht had lost North Africa and suffered catastrophic defeats at Stalingrad and Kursk, the Red Army was sweeping back across Eastern Europe, Rome was liberated, and the Allies had primarily established air superiority over the continent. By 1944, the Allies were combat-hardened, and American war production was at peak levels. This all stood in stark contrast to 1942, when America and her allies remained largely on the defensive, and in many cases, in retreat.\nSo – with these points in mind, why is there not a greater appreciation for Midway every June? Its strategic importance in blunting the Japanese and the way the lopsided American victory lifted our morale at a tough period in the war, all argue for greater acknowledgment and understanding of the battle. And yet, Midway doesn’t seem to capture the same level of attention or interest that D-Day does every year.\nThe fact that Midway’s anniversary falls one day after D-Day may be part of the reason – it is overshadowed. It may also be that the story of American ground troops at Normandy, braving the horrors on the beach to secure a toehold in Europe, is something that resonates more personally with people on the most basic and emotional levels. We understand it better because of its mortal qualities. We’re more affected by – and thus seem to appreciate more – the visceral aspects of combat, the grit and grind and human tragedy of war. Perhaps Midway is less captivating in this regard.\nThe point of all of this is not – to diminish the heroism and experiences of anyone who landed ashore in France on June 6th. Nor is it to suggest that the contributions of those who did their duty in one theater at one time are greater or lesser than the contributions of those in another theater at a different time. Those who gave their all in World War II did so regardless of place, objectives, or the overarching status of the conflict.\nThe point of all of this is – that we have an obligation to develop a greater recognition of Midway and its strategic importance in the way that we recognize D-Day and its significance as exemplifying American valor and the American combat experience in Europe. There is an imperative to recognize the significance – from the personal to the academic – of all of these campaigns.\nLooking ahead, 2019 marks the 75th anniversaries of several major battles of the Second World War – from Saipan and Peleliu to Anzio, Operation Market Garden, and the Hurtgen Forest. 1944 also saw the Battle of the Philippine Sea – known as “The Great Marianas Turkey Shoot” – and the Battle of Leyte Gulf, which is considered the biggest naval engagement of WWII and possibly the largest in world history.\nAs we commemorate each of these upcoming anniversaries, we have a golden and timely – but limited – opportunity to gain a new appreciation for the importance of these events and to understand the role that our fellow Americans played in them.\nConnor Martin is a U.S. Marine veteran and policy analyst in Washington DC.\nThis article by Connor Martin originally appeared at Real Clear Defense. It first appeared in The National Interest in June 2019.\nImage: Wikimedia Commons.\nThe National Interest\n1. US Security from Michael_Novakhov (88 sites)', 'Into Forgetfulness (Transcript)\nIn our lives, there are moments…events…usually lasting a very short time, that result in irreversible change.\nAs a people, we have a need to memorialize these events so that future generations will know and remember.\nJuly 17th, 1944 at 10:17 pm, a peaceful, quiet California summer night thousands of miles from the fighting of World War II, an event took place. In one explosive moment, 320 men vanished. And in the rubble, the way the military and America treated its citizens, began to change.\n“America At War”\n(Images of war scenes depicting ship wreckage)\nPresident Franklin D. Roosevelt: “Yesterday, December 7th, 1941, a date that will live in infamy. The United States of America was suddenly and deliberately…”\n(Historic video of soldiers loading ammunition into weapons)\n(Women welders working on ships and aircraft)\n(War planes moving slowly along a runway)\nMost of the black sailors stationed at Port Chicago had enlisted hoping to fight the enemy. Unable to serve on the front lines, they found themselves doing dangerous manual labor.\n(Sailors loading munitions)\n(Steam engine train moving along a track)\nMunitions manufactured around the country would arrive at Port Chicago daily by train. As a transit site, not a storage site, the cargo had to be unloaded as rapidly as possible. It was the job of the black enlisted sailors to manually transfer the munitions from the rail cars to large cargo ships. With war raging in the Pacific, victory depended on rapid delivery of munitions, and the sailors of Port Chicago were proud of their crucial link in the delivery process.\n(Sailors loading munitions)\nIgnoring standard safety practices, two ships were loaded at one pier. The work proceeded 24 hours a day. All the sailors handling cargo were black. All the officers in charge were white. Competition was encouraged by the officers. Loading rates for each division were posted.\nLoading rates for each division were posted, and incentives were awarded.\nSlower divisions were shamed and threatened. Officers and sailors cut corners to save time. Many people warned: the fast pace of the work, the huge volume of munitions being moved, loading two ships on one pier, and the lack of proper training, proper safety procedures, would lead to disaster.\nTwo explosions, 6 seconds apart, ignited the night sky with a column of fire and steel rising 2 miles. The first blast was fairly small. The second, incinerated two ships, the pier, 16 rail cars…320 men.\n(Historic video of the disaster aftermath. Pieces of a ship floating in the water)\nText on screen: “SS Quinault: 500 ft. off shore. rotated 180 degrees. inverted. no sign of life. SS E.A.BRYAN: missing.”\n(Video of disaster aftermath on shore. Smashed train cars, destroyed vehicles, demolished buildings, leftover piles from the pier...gravestones)\nText on screen: “Joseph Crosby, 27, Greenville, Texas: “I am injured in my spine, But that is nothing compared with my many friends who have been blown into forgetfulness.”\nThe survivors were in shock. Friends had disappeared without a trace. Next time…it would be them. On August 9th, 300 black sailors were ordered back to work loading munitions. Continued lack of training, unsafe working conditions, lack of any official explanation. Everyone believed another explosion would happen.\nThe benefits of Navy life no longer outweighed the extreme danger of the work.” Any order but that…” “Any order but that…”\nAfter a peaceful confrontation, over 250 black sailors were arrested. The sailors were given the opportunity to put the so-called uprising behind them and return to work. About 200 reluctantly agreed, but were instead thrown in the brig. The 50 remaining black enlisted sailors\nwho refused to load munitions were charged with mutiny-- In time of war, punishable by death.\n(Images of newspaper headlines detailing the mutiny trial)\nText on screen from various articles:\n“This is the first time in naval court history in which 50 men have been tried at one time.”\n“Thurgood Marshall, special NAACP counsel, arrived by plane, here this week to assist in the civilian defense of the 50 Negro sailors.”\n“Is there one or more Negro Navy officers sitting on the panel of judges? Everybody knows there is not,”\n“Rear Admiral Carlton H. Wright later added: “I am gratified to learn that, as was expected, Negro personnel performed bravely and efficiently in the emergency.”\n“Jim Crow, abuse and ill-treatment. Long hours of work, little recreation, arrogant officers, and constant danger have put most of the Negro sailors here on edge.”\n“Earlier, defense attorney Veltmann had shot holes into the conspiracy case built up by Judge Advocate Lt. Commander Frank Coakley.”\n“Boyer said he had never heard his division officer give the men an order to load ammunition. “The cooperation of my men was always wonderful, their discipline excellent,” he said.”\n“…certainly the men, involved, deserve not public condemnation, but, rather, public sympathy.”\nOn October 24th, 1944, the specially convened military court found all 50 men guilty of mutiny as charged.\nNewspaper headline: “Fifty Sailors Get 8-15 Year Terms In Mutiny Trail.”\nNarrator: All were sentenced 8 to 15 years in prison, and dishonorable discharge from the Navy.\n(Prison door slamming shut)\n(Modern video of the remaining pier at the memorial)\nPort Chicago Naval Magazine National Memorial was established to honor the courage and commitment of the sailors, Marines, National Guardsmen, Merchant Marines and civilians killed and injured in the largest homeland disaster during WWII.\n(Modern video of visitors looking at the memorial)\nThe Memorial recognizes the critical role they and the survivors of the explosion played in the winning of the War in the Pacific. The explosion and its aftermath was a major catalyst that helped persuade the U.S. Military to begin the long journey to racial justice and equality.\nPort Chicago Naval Magazine National Memorial will ensure the story of these brave men is not lost into forgetfulness.']	['<urn:uuid:49225f28-4196-44ce-b8e8-32d1d55d8881>', '<urn:uuid:33113cb5-5c0a-4c26-9eed-fe84b72d1543>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T17:51:21.175311	11	87	2222
58	I'm researching educational technology implementation processes and would like to know how design guidelines differ from design principles in course development, and how these concepts relate to creating effective virtual training environments?	Design principles are generic aims and goals guiding design decisions, while design guidelines are more specific as they add context to those principles. When applied to virtual training environments, these concepts help create flexible and programmable learning spaces that can be tailored to individual needs while maintaining high levels of interaction with learning content, allowing students to build their own understanding through direct engagement with materials and repeated practice.	"['For all the #edumooc\'ers out there, I apologize that most of the references in this piece are of academic nature, and therefore are not in the public domain. Below are key excerpts from a synthesis paper I wrote about design research. I see now that the methodology that I propose would also easily apply to the development of online learning – in fact, I actually use this methodology somewhat informally as I design and re-design various iterations of one of the online courses that I teach.\nIf you find this useful, or plan to use this methodology, please leave me a comment. Thanks.\nA Design Research Methodology for Online Learning Course Design\nWang and Hannafin define design-base research as “a systematic but flexible methodology aimed to improve educational practices through iterative analysis, design, development, and implementation, based on collaboration among researchers and practitioners in real-world settings, and leading to contextually-sensitive design principles and theories [underline added]” (2005, p. 6).\nDesign principles are ""aims and goals guiding design decisions that occur during the development life cycle"" (Mariage, Vamderdonckt, & Pribeau, 2005, p.689). Design principles are generic and are not generally places in context. When context is added to a design principle, it becomes a design guideline.\nDesign research is pragmatic: it provides a solution to a real-world educational problem (such as teaching terminology) and adds to educational theory in the form of design principles that can be used by educational practitioners (Brown, 1992, p. 143; Collins, 1992, p. 15). Depending on the context in which it is being used, design research may also be known as design studies, design experiments, development/developmental research, formative research, formative evaluation, or engineering research (van den Akker, Gravemeijer, McKenney, & Nieveen, 2006, p. 4). Since design research is intended to both solve a real world problem and to develop re-usable design principles, it is well suited to help develop design theories for mobile learning.\nThere are many approaches to design research – all of which begin with a theory gathering or literature review phase and end with a reflection on both the design solution and the design research process. The middle part of the process – the iterative cycles of analysis, design, development, and implementation – very among the different approaches.\nMost online learning projects involve both instructional designers and software developers; therefore, a design research approach to online learning should take into account the process used by these practitioners. Specifically, an ADDIE-based instructional design approach and an Agile-based software development approach.\nFor online learning (and mobile learning), I recommend a simplified four-phased design research approach that draws on the characteristics of the design research approaches presented in research (Ma and Harmon, 2009;Middleton, Gorard, Taylor, and Bannan-Ritland, 2008; Gravemeijer and Cobb, 2006) with instructional design methodologies and the agile software development method. I call this approach ASER (analyze, strategize, experiment, reflect). As illustrated below, the ASER process has four phases: 1) analyze real-world problem, 2) strategize design experiment, 3) conduct context specific design experiment, and 4) reflect on entire project.\nIn the analyze phase the researcher performs a detailed analysis of a real-world problem. When analyzing the problem, the researcher ensures that scope of the problem is general enough to warrant design research, and that the problem is indeed a design problem. This phase requires crafting a research question.\nThe strategize phase requires the development of strategies to support the design research project. The researcher performs detailed literature reviews and chooses or develops frameworks based upon current theories. The researcher defines the strategies for the experiment phase, including identifying research methods and data collection methods, and identifying specific goals for each experiment cycle. In the strategy, the researcher defines the specific boundaries for the beginning and ending of cycles and the project as a whole. These boundaries provide the necessary scope for the additional team members (instructional designers and software developers). For example, the researcher may define the means of delivering the content to the devices not within the scope of the project, but the coding of the content itself within scope. In addition, the researcher defines the scope of each iteration small enough to permit multiple iterations of the experiment phase.\nDuring the strategize phase, additional team members perform their analysis. Instructional designers perform the analysis phase of ADDIE. Software developers perform the initial planning and architecture design portion of their process. The frameworks defined by the researcher are used as input to the instructional designers and the software developers and for their analysis.\nThe experiment phase is the iterative phase in the ASER approach, clearly illustrated by the circular arrows in the diagram. The research team uses iterative cycles of design, develop, instructional experiment, and evaluate, with the evaluate step providing input to the next design stage and the first iteration acting as the feasibility test for the project. The four cycles align directly with the design, develop, implement, and evaluate phases in ADDIE. The iterative development and testing phases of the agile software development method also align. Each iteration has a specific goal, defined in the research design strategy, and there is specific software and instructional material developed to support that goal.\nReflect is the final phase of the project. In this phase, all data collected throughout the iterative cycles is analyzed as a whole. The researcher seeks to extract design principles based upon the results of the design experiments. The software development team completes the final phases of their process, seeking to commercialize the results of their development. In addition to the analysis of the research results, an evaluation of the overall research process is performed and learnings are shared.\nBrown, A. L. (1992). Design experiments: Theoretical and methodological challenges in creating complex interventions in classroom settings. The Journal of the Learning Sciences, 2(2), 141-178. doi:10.1207/s15327809jls0202_2\nCollins, A. (1992). Towards a design science of education. In E. Scalon & T. O\'Shea (Eds.), New Directions in Educational Technology (pp. 15-22). Berlin, Germany: Springer-Verlag.\nGravemeijer, K., & Cobb, P. (2006). Design research from a learning design perspective. In J. van den Akker, K. gravemeijer, S. McKenney, & N. Nieveen (Eds.), Educational Design Research (pp. 17-51). Abingdon, UK: Routledge. doi:http://www.routledge.com/\nMa, Y., & Harmon, S. W. (2009). A case study of design-based research for creating a vision prototype of a technology-based innovative learning environment. Journal of Interactive Learning Research, 20(1), 75-93. doi:http://www.aace.org/\nMiddleton, J., Gorard, S., Taylor, C., & Bannan-Ritland, B. (2008). The ""Compleat"" Design Experiment. In A. E. Kelly, R. A. Lesh, & J. Y. Baek (Eds.), Handbook of design research methods in education (pp. 21 – 46). New York: Routledge, Taylor & Francis.\nMariage, C., Vamderdonckt, J., & Pribeanu, C. (2005). State of the art of web usability guidelines. In R. W. Proctor & K.-P. L. Vu (Eds.), Handbook of human factors in web design (pp. 688 – 700). Mahwah, NJ: Lawrence Erlbaum Associates.\nvan den Akker, J., Gravemeijer, K., McKenney, S., & Nieveen, N. (2006). Introducing educational design research. In J. van den Akker, K. Gravemeijer, S. McKenney, & N. Nieveen (Eds.), Educational Design Research (pp. 3-13). Abingdon, UK: Routledge. doi:http://www.routledge.com/\nWang, F., & Hannafin, M. J. (2005). Design-based research and technology-enhanced learning environments. Educational Technology Research and Development, 53(4), 5-23. doi:10.1007/BF02504682', 'TECHNICAL AND PRACTICAL TRAINING FOR EMPLOYEES PRESENTS SEVERAL DIFFICULTIES, FROM THE COSTS INVOLVED IN TRAINING WITH REAL MACHINERY TO THE HEALTH AND SAFETY RISKS THAT SUCH TRAINING CAN POSE.\nImmersive learning technology provides a valuable opportunity to engage in practical training in an easier, safer and more flexible way and, unlike other alternative methods, maintains a high level of interaction with the learning content.\nA research study conducted by the Virtual Human Interaction Lab at Stanford University, “The Effects of Fully Immersive Virtual Reality on the Learning of Physical Tasks”, highlights the positive impacts that virtual reality can have on technical training.(1) Its findings show that virtual reality technology obtained better results than traditional video methods in the learning of practical tasks and participants in the study also reported feeling a higher social presence within the virtual environment.\nResearch has shown that repeating practical tasks reinforces learning. Immersive learning environments allow students to repeat tasks as many times as they like at their own pace and, importantly, away from the social pressures of a real classroom. For training in high-risk situations, virtual reality provides a much safer environment in which to make initial mistakes. Like face-to-face training, virtual training also allows students to interact directly with their trainer and environment. Detailed 3D imagery provides life-like representations of objects and their surroundings, giving students a very ‘real’ training experience.\nTechnical training in an immersive learning environment\n“Learning is the development of experience into experience” (James, 1892).\nThe learning and understanding of practical tasks relies on experience and virtual reality training is highly experiential and immersive. Students can interact with objects and machinery and view them up-close, as well as experiencing how to operate them. Effective training is also reliant on students’ interest and motivation (2); immersive learning technology can create stimulating and engaging learning environments for students, increasing their motivation to learn. Such environments are flexible and programmable, meaning that they can be tailored to meet individual needs.\nAccording to Fabrizia Mantovani (3), students learn more effectively when they engage directly with learning content and build their own understanding of it. Immersion in and interaction with learning content encourages the active engagement and motivation of students.\nExperience, repetition and interaction are important processes in practical training. Virtual reality training provides this in a controlled and programmable environment. The application of virtual reality with a live trainer yields particularly high motivation from students and allows for technical tasks to be carried out under guidance in a safe and engaging environment.\n(1) Patel, K., Bailenson, J.N., Hack-Jung, S., Diankov , R., & Bajcsy , R. (2006). The effects of fully immersive virtual reality on the learning of physical tasks. Proceedings of PRESENCE 2006: The 9th Annual International Workshop on Presence. August 24 – 26, Cleveland, Ohio, USA.\n(2) Bricken, M. (1991). Virtual reality learning environments: potentials and challenges. Human Interface Technology Lab (HITL) Washington Technology Center, University of Washington\n(3) Mantovani, F. Virtual Reality Learning: Potential and Challenges for the Use of 3D Environments in Education and Training in Towards Cyberpsychology: Mind, Cognition and Society in the Internet Age, Riva, G. and Galimberti, C., 2001']"	['<urn:uuid:d5f85b87-d8c8-484d-95be-788119d83dc1>', '<urn:uuid:fad1c0af-b656-4505-8236-99aff755e94c>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T17:51:21.175311	32	69	1722
59	What are the physical signs of thyroid problems?	Physical signs include unusual neck lumps, hoarseness, trouble swallowing, breathing problems, racing heart rate, sweating, weight changes, fatigue, muscle weakness, and in some cases bulging eyes. Some people may also experience anxiety, insomnia, and temperature sensitivity.	"['What is the thyroid gland and what does it do?\nThe thyroid is a critical gland that regulates the body’s hormones, metabolism, temperature, and heart rate; it is vital for the body’s growth and development. Thyroid cancer develops in the thyroid gland—a butterfly-shaped organ located in the lower front of the neck, according to Dr. Melanie Goldfarb, Director of the Center for Endocrine Tumors and Disorders at Saint John’s Health Center, and Professor of Surgery at Saint John’s Cancer Institute in Santa Monica, California. Today’s treatment of the thyroid involves minimally invasive techniques, thereby preserving as much of the thyroid’s important function as possible.\nEarly warning signs of thyroid problems:\n- Overactive thyroid: Racing heart rate and palpitations, anxiety, insomnia, nervousness, unintended weight loss with increased appetite, excessive sweating/heat intolerance, and muscle weakness.\n- Underactive thyroid: Fatigue/sluggishness (mental and physical), cold intolerance, constipation, hair loss, weight gain, and depression.\nWhat are the physical signs of thyroid nodules/cancer:\n- An unusual lump or swelling in the neck\n- A new cough\n- Hoarseness in the voice\n- Trouble swallowing\n- New persistent cough\n- Trouble breathing\nWithin the year, thyroid cancer went from #2 in young adults to it being the #1 cancer in that age group.”\n– Melanie Goldfarb, MD\nWhat you need to know about thyroid health: Tips from the Expert\nDr. Melanie Goldfarb stands outside Saint John’s Health Center discussing thyroid cancer with Saint John’s Cancer Institute\nThyroid Cancer Facts:\n- Most thyroid cancers are slow-growing.\n- Thyroid cancer is more common in women than men.\n- Women from the ages of 15 to 39 are being diagnosed more frequently with thyroid cancers.\n- The 5-year survival rate for localized thyroid cancer is 99.9 percent.\n- 3 percent of all thyroid cancers metastasize (move beyond the primary site and into other parts of the body).\nTypes of Thyroid Cancers:\n- Differentiated (including papillary, follicular, and Hürthle cell)\n- Anaplastic (an aggressive cancer)\nThyroid cancer is usually asymptomatic (having no physical symptoms), and in younger patients, it is usually detected when the nodule is large enough to be felt by the patient or a clinician during a physical exam. In older patients and some younger patients, it is found incidentally when imaging is performed for unrelated reasons.\nRisks Factors of Thyroid Cancer include:\n- Being female\n- Hereditary conditions\n- Family history\n- Radiation exposure\n- Being overweight or obese\n- Lack of iodine in the diet\nMost people with thyroid cancer have no known risk factors, and thyroid cancer can recur once it’s been treated.\nAll thyroid nodules should be evaluated with a thyroid ultrasound, and a biopsy should be taken in order to get a diagnosis. Surgery is the main form of treatment for thyroid cancer.\n– Melanie Goldfarb, MD\nTreatment of Thyroid Cancer:\n- Active surveillance (regular ultrasound examinations)\n- Surgery (See Thyroid Cancer Symptoms, Diagnosis, and Treatment\n- Radiation therapy, radioactive iodine (adjuvant therapy)\n- Thyroid hormone therapy\n- Targeted Therapy\nMost thyroid cancers respond well to treatment, but a small percentage can be very aggressive.\nThyroid Radiofrequency Ablation for Benign Thyroid Nodules\nThyroid nodules can be benign (non-cancerous), and cause difficulty eating, breathing, swallowing, and become unsightly, resulting in a visible bulge in the neck. These nodules can be treated non-invasively with Radiofrequency Ablation, more commonly referred to as RFA treatment. RFA is a newer outpatient procedure for thyroid nodules that has a high success rate. It works by shrinking the nodule internally using heat generated by radio waves, which stops the growth and causes the nodule to shrink over time.\n- Learn more about RFA (See Thyroid Radiofrequency Ablation).\n- Listen to a testimonial of an RFA treated patient, click here.\nWhat should you know from reading this blog?\n- What the thyroid is and how it regulates your body’s hormones.\n- Early signs of thyroid problems.\n- Signs of thyroid cancer.\n- Risks of thyroid cancer.\n- Treatment of thyroid cancer.\n- Types of thyroid cancers.\n- Treatment of benign thyroid cancer.\n- Dr. Melanie Goldfarb is a dog lover.\nIf you believe you are experiencing any signs, or symptoms, or think you are at risk, please discuss this with your endocrinologist or contact the endocrine center of excellence and speak with Dr. Goldfarb, 310-829-8751 or click here to schedule an appointment.', ""This material must not be used for commercial purposes, or in any hospital or medical facility. Failure to comply may result in legal action.\nWHAT YOU NEED TO KNOW:\nWhat is hyperthyroidism?\nHyperthyroidism is a condition that develops when your thyroid hormone levels are high. Thyroid hormones help control body temperature, heart rate, growth, and weight.\nWhat causes hyperthyroidism?\nIf you have a family member with hyperthyroidism, your risk is increased. Any of the following can cause hyperthyroidism:\n- Autoimmune disease, such as Graves disease or Plummer disease\n- Certain medicines, such as lithium, amiodarone, or aspirin\n- Viral infection\n- Thyroid inflammation or thyroid cancer\n- High iodine levels\nWhat are the signs and symptoms of hyperthyroidism?\nThe signs and symptoms may develop slowly, sometimes over several years.\n- Weight loss, increased appetite, diarrhea, or constipation\n- Increased sweating and heat intolerance\n- Nervousness, restlessness, tremors, and difficulty sleeping\n- Fast heart rate and fast breathing, even at rest\n- Painful lump in your neck or bulging eyes\n- Fatigue and muscle weakness\n- Decreased or absent monthly periods\nWhat is a thyroid storm?\nThyroid storm happens if your thyroid hormone levels get too high. Your temperature may go very high, your heart may beat very fast, and you may have problems thinking. You may have increased sweating, vomiting, or diarrhea. You may have seizures or go into a coma. Thyroid storm may happen if you have hyperthyroidism and get an infection or stop taking your thyroid medicine. Injuries, burns, and certain medicines can also cause a thyroid storm.\nHow is hyperthyroidism diagnosed?\nYour healthcare provider will ask about your symptoms and what medicines you take. He will ask about your medical history and if anyone in your family has thyroid disease. A blood test will show your TSH level.\nHow is hyperthyroidism treated?\nYou may not need any treatment, or you may need any of the following:\n- Antithyroid medicines decrease thyroid hormone levels and your symptoms.\n- Radioactive iodine is given to damage or kill some thyroid gland cells. This may decrease the amount of thyroid hormone produced. Tell your healthcare provider if you know or think you are pregnant. This medicine can be harmful to an unborn baby.\n- Surgery may be done to remove all or part of the thyroid gland.\nCall 911 for any of the following:\n- You have sudden chest pain or shortness of breath.\n- You have a seizure.\n- Your heart is beating faster than usual.\n- You feel like you are going to faint.\nWhen should I contact my healthcare provider?\n- You have a fever.\n- You feel nervous and restless.\n- You have chills, a cough, or feel weak and achy.\n- You run out of medicine or have stopped taking it.\n- You have questions or concerns about your condition or care.\nCare AgreementYou have the right to help plan your care. Learn about your health condition and how it may be treated. Discuss treatment options with your caregivers to decide what care you want to receive. You always have the right to refuse treatment. The above information is an educational aid only. It is not intended as medical advice for individual conditions or treatments. Talk to your doctor, nurse or pharmacist before following any medical regimen to see if it is safe and effective for you.\n© 2017 Truven Health Analytics Inc. Information is for End User's use only and may not be sold, redistributed or otherwise used for commercial purposes. All illustrations and images included in CareNotes® are the copyrighted property of A.D.A.M., Inc. or Truven Health Analytics.\nThe above information is an educational aid only. It is not intended as medical advice for individual conditions or treatments. Talk to your doctor, nurse or pharmacist before following any medical regimen to see if it is safe and effective for you.""]"	['<urn:uuid:0b759d56-da3c-4f15-a7aa-f7d2cdb14fe1>', '<urn:uuid:6294653b-0e7f-4257-ab50-a9c9872dbb53>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	8	36	1366
60	I'm researching maternal health innovations in healthcare systems - what kind of simulation training is being implemented to address pregnancy-related complications?	Simulations using manikins were launched at every hospital to address pregnancy-related hemorrhage and hypertension. This training helped identify problems and reinforced best practices, addressing the concerning U.S. maternal mortality rates which are the highest among developing countries. According to CDC research, 80% of these deaths could be prevented.	"[""Top 10 Innovation-Digital Health Stories of 2022\nAs we begin a new year at OSF HealthCare, we're reflecting on some major advancements in the way we care for patients – with significant advancements in 2022 that have allowed us to meet patients where they are physically and where they are in their health care journey. Together, OSF Innovation and OSF OnCall, our digital health division, launched initiatives that leveraged digital technologies and data for more convenient access and improved patient outcomes..\nThose efforts are highlighted here in our Top 10 Innovation and Digital Health stories for 2022.\n1. OSF Launches a Digital Hospital at Home program-OSF HealthCare began using new care models, including launching a new hospital at home program. Many patients who qualify have embraced the concept and feedback is that they love getting hospital-level care in the comfort of their own home. Look for this to expand in the New Year.\n2. OSF OnCall plugs pregnancy and postpartum care gaps for Medicaid patients-This story highlights one of several remote patient monitoring programs, launched under the Medicaid Innovation Collaborative (MIC) which includes OSF and four federally-qualified health centers. The initiative is a state-funded, 5-year program to use the latest tools and telehealth, paired with digitally-enabled community health workers and community organizations to create scalable solutions to improve access and quality of care.\n3. A new vision for diabetic eye exams in primary care offices-OSF HealthCare, like many other health systems had a problem. Too many patients with diabetes don’t get recommended yearly dilated eye exams to check for related retinopathy – the number one cause of blindness in the U.S. OSF Innovation researched and found a recently-approved platform that uses artificial intelligence to diagnose retinopathy within minutes. To date, the equipment from Digital Diagnostics has found 20% of OSF primary care patients have some damage to the blood vessels that lead to the retina.\n4. OSF Innovation getting ready to test concussion diagnosis gaming app-If a so called “FlightPath” app works as expected, it will diagnose a concussion within minutes by having users follow a hummingbird in flight and catch it by manipulating the screen and their own body. This story highlights how Dr. Adam Cross and his team of collaborative researchers are creating a game-changing app that could provide sideline diagnosis of sports-related concussions. There are numerous other settings where FlightPath could be used, including on the battlefield and in hospitals and nursing homes.\n5. Developing an app to alert health pros to medical misinformation on social media-This effort is a collaboration between OSF HealthCare, the University of Illinois, Urbana-Champaign (UIUC) and the U of I College of Medicine in Peoria (UICOMP) where many health providers are concerned about the damage to individual and community health caused by bad information that goes viral on social media. Like the concussion app in development, this effort also received funding from the Jump ARCHES research and development program.\n6. Using simulations to tackle potentially deadly pregnancy-related hemorrhage and hypertension-Maternal mortality rates were in the headlines in 2022 and rightly so, because U.S. rates are the highest of any developing country and research for the Centers for Disease Control and Prevention released evidence that 80% of deaths could be prevented. So, OSF Innovation launched new training at every one of its hospitals, using simulations with manikins that helped identify problems and reinforced best practices.\n7. OSF HealthCare-Bradley University partner to solve pressing health care problems-This year, OSF doubled down on academic partnerships and other collaborations. This latest formalized agreement, with Bradley University, will tap the best and brightest for breakthrough innovations including new medical devices, digital solutions, improved work processes and new service models. Look for more academic partnerships in 2023.\n8. OSF Ventures ready to tap third and largest fund under new leadership-Inaugural OSF Ventures leader Stan Lynall decided to step back into a part-time role as he eases into retirement while Mayank Taneja, who was among the early members of a high-performing team, stepped into the position of vice president of venture investments. Based on past success, including nine exits, OSF Ventures heads into 2023 ready to begin investing from its third and largest board-approved fund of $100 million.\n9. OSF Innovation creates its own shark tank and partners with a venture start-up studio-These are really two stories in one because they’re part of an effort to develop an innovative mindset among OSF Mission Partners (employees), and to create an ecosystem where those ideas can be vetted; with the best given resources to be developed and eventually commercialized for use by OSF and other health systems.\n10. Connecting Chicago-area homeless to health care support-OSF HealthCare strengthened its collaboration with the University of Illinois Chicago (UIC) by creating a Community Health Advocacy grant program with financial and other resources to support research and pilot projects. This story explains one of the first efforts piloted will put kiosks with iPads into two shelters to connect those who are homeless with the UIC Pharmacy College to get medications needed to manage chronic conditions.""]"	['<urn:uuid:5bfad68f-7480-4685-8b11-4e6e4dfdc5d8>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	21	48	840
61	I'm planning my wedding. Do Japanese and Jewish ceremonies both use special canopies?	While Jewish weddings always use a special canopy called the chuppah that symbolizes the couple's future home, traditional Japanese Shinto weddings don't use canopies - they take place inside a shrine following specific ceremonial procedures.	"['There\'s a long tradition of Wedding ceremonies in Japan, but Japanese Weddings are very different from what you\'re probably used to. In the past there were two types of weddings:\n- Miai お見合い結婚, or arranged Weddings (for various political or economic reasons. Sometimes the partners are also selected by their parents)\n- Renai 恋愛, or regular wedding where both partners found each other through chance and married for love.\nAlthough arranged marriages are still performed sporadically in Japan, most couples nowadays get together because of love. They usually meet either in school, at work, or in special interest clubs or events, as was the case with Katz and his wife.\nApril 21st marked a very special day for these two lovebirds.\n--a day that only comes once in a lifetime. A day that will be remembered for a long time to come - It was Katz and Saori\'s Wedding day. The weather was perfect, the guests were dressed in their best formal attire, and the ceremony was very traditionally Japanese. It was a perfect combination of traditional Japanese Shinto ceremony and western style reception - Traditional charm infused with modern pompousness. This style wedding has become very popular in Japan recently, combining old-world charm with modern traditions and style.\nIn the past, most weddings followed the Shinto or Buddhist Style wedding traditions.\nA traditional Shinto-style wedding (such as this one) has many interesting customs, such as the traditional clothes the bride-to-be wears. In very traditional weddings, brides usually wear a Shiromuku (White Kimono and ""hat""). Nowadays some brides swap it for a more colorful kimono instead.\nAnother very traditional Shinto practice is the ""San-San Kudo"", which dates back to the 8th century and is performed instead of saying the wedding vows. Sake is brought to the couple kneeling on the floor in front of the shrine, and both take three sips of Sake from three different cups, then offer Sake to their families, which symbolizes a new family bond, and respect for their parents.\nThese days however, only about 1/3 of couples practice traditional style weddings, and even then some of the more traditional aspects like the Shiromuku are left out or substituted for more modern versions. Especially younger couples have embraced the non-traditional style with its emphasis on class, elegance and western style opulence. These days there are many western style ""Churches"" in Japanese cities for the sole purpose of having a wedding ceremony. Katz and his wife decided to forgo the trappings of modern style for traditional charm, and thus had their ceremony in a traditional Japanese Shinto Style instead.\nThis is very different from Western Style since the guests will line up outside the gates of the shrine, awaiting the arrival of the bride.\nThen the priest speaks a few words, and the couple to be married are marched through the gates together towards the shrine. The wedding entourage then follow the couple along the path into the shrine.\nThere\'s an elaborate ceremony inside the shrine with one or two priests, many rituals, a lot of bowing, and of course some sake to commemorate the newly formed family bond. It\'s all very deep in meaning and I couldn\'t help but be completely absorbed in the moment. I\'ve been in Japan for a long time, but this was one of the most profoundly ""Japanese"" events for me as well.\nAfter the ceremony, the guests are then brought to the Wedding Hall by bus (if it\'s not in the same place as the ceremony).\nUpon arrival in the wedding hall, one should present their Shugi Bukuro 祝儀袋, or Cash Gift Envelope, to some selected friends who volunteer to receive the envelopes, and sign in for the wedding reception. The Shugi Bukuro is unique to Japanese weddings and is an elaborately decorated envelope into which a cash gift consisting of around 30,000 yen (about 229 Euro, or 294 US$) is to be placed, along with a short message to the couple.\nTraditionally, in Japan there are no gifts given to the couple, as not to cause disharmony with the wrong gift. However, as everyone knows, Katz loves curry, so many people sent him and his wife some curry as a wedding present, which I hear they greatly appreciated.\nThe Wedding guests all receive a bag full of little presents and trinkets from the couple, usually consisting of sweets, cups, glasses, and some Tea or Soup...\nOne of the highlights of Katz\'s wedding was the release of white doves on the roof of the reception place. It was a very nice gesture of peace and tranquility, and a great start to their new life together.\nAfter the short event on the roof of the building, and some photos, the guests were then ushered into the main dinner hall where the actual reception took place. It was an elaborately decorated venue with a fantastic view of the city and would rival any 5 star restaurant in terms of elegance, ambiance, and presentation of food.\nThe big difference between a Japanese Wedding and a Western Wedding is that there is usually no dancing by the guests at the reception, and J-weddings feature a lot of speeches from the Newlywed\'s Boss, close Co-workers, and Family. It\'s a rather formal affair compared to many western style weddings, which reflects the Japanese reverence for formal traditions.\nThere is, however, some sort of quiz about the couple in which the tables are made into groups and have to work together to correctly answer questions about the couple.\nAs far as food is concerned, during the reception there is usually a course meal consisting of entrees, main dishes, and desserts, which in this case was elegantly prepared by the restaurants\' capable chefs.\nOf course the alcohol flows freely and some of the couple\'s family members even come around to all the tables to pour some wine, champagne, or beer and socialize for a bit.\nThe couple is very busy during this time, making their rounds from table to table to say a few kind words to the guests, take pictures with them, and thanking everyone in person for coming to their momentous event.\nOf course, typical western style customs such as a wedding slide show and cutting the cake together have also been happily adapted in Japan, and are a main part of the reception.\nKatz went out of his way to track down his wife\'s former exchange family in the US and had them record a short video message for his wife, which brought admiration and tears of joy to her eyes. It was a very sweet moment, and one could almost feel the gratitude permeating through the air.\nDuring the reception there is usually a performance, or showcase event. In this case, it was a perfectly timed release of some kind of roman candles on each table, as the lights were dimmed, and the music cued the arrival of the couple again, after a quick costume change.\nTowards the end of the reception, the couple then give a speech for their parents to say thank you for being great parents, and how much they appreciate all the chances their parents have provided for them. This is of course reciprocated by their parents also congratulating the couple and wishing them all the best for their new life together.\nShortly after that, the couple leaves together while the guests socialize a little more and prepare for slowly exiting the venue as well.\nSo there you have it, now you\'re prepared to attend a wedding in Japan. On that note, let\'s wish Katz and his wife all the best for their future together and thank them for letting us all be a part of this momentous event.', 'Even at a young age, we become acutely aware of Jewish wedding traditions and the rituals of traditional Jewish ceremonies.\nTake a moment to share your favorite Jewish wedding rituals by clicking HERE! We’d love to know what Jewish wedding traditions you find most meaningful and important… and which ones call to mind the fondest memories.\nWe’ve all have grown up with favorite traditions we’ve treasured through the years, traditions that were part of our weddings and Jewish ceremonies in general. Memories of participating in those traditions might even include such moments as when our precious family pet ate the brisket or grabbed the dreidal while it spun!\nJewish Wedding Traditions\n- Auf Ruf – The calling up of the groom to recite a blessing over the Torah on the Shabbat before the wedding. In some Jewish communities it is the Shabbat after the wedding.\n- Badeken (Bedekin) – The groom covers the bride’s face with a veil just before the wedding ceremony (based on the story of Jacob and Leah). The veil is a symbol of modesty (relating to the story of Isaac and Rebecca).\n- Breaking of the Glass – Generally, the groom (chatan) breaks the glass with his Right foot. It is a reminder of the destruction of the Temple, and a symbol of how fragile relationships can be. As with the glass, once relationships are broken and damaged, they are difficult to repair. The breaking of the glass at a wedding is also the signal for everyone to shout “Mazel Tov!” and begin the wedding reception festivities or as some express, ‘to start partying.’\n- Chuppah (Huppah, Wedding Canopy) – The wedding ceremony takes place under the chuppah which symbolizes the home the couple will build together.\n- Hakafot – Traditionally, the bride circles the groom seven times upon entering the chuppah. Among other things, the circling “seven times” corresponds with the seven wedding blessings and demonstrates that the groom is the center of her life. However, the modern version is now in place for our couples today. It is a shared custom. The bride would circle the groom three times, the groom would circle the bride the same number and then together, they would circle together. The Symbolism of Seven is observed and the meaning only changes to now define the bride and groom are the centers of their live together. A nice approach to our changing times where tradition is observed on a more comfortable level.\n- Jewish Wedding Dance (Circle Dances, Hora) – Guests dance in a circle around the couple and during the dance the bride and groom are lifted on chairs. Variations (Krenzl and Mizinke Tanz) involving the parents are also sometimes part of the dancing celebration.\n- Kabbalat Panim – To increase the anticipation of their upcoming wedding day, it is customary for the bride and groom not to see each other for the week leading up to the ceremony. In this case the couple greet their guests separately before the ceremony. This is known as Kabbalat Panim.\n- Ketubah (Marriage Contract) – Traditionally, a contract outlining the husband’s responsibilities to his soon-to-be wife. Ketubot (plural for more than one) are also beautiful works of art, often displayed in a couples’ home after the wedding. Ketubah Artist’s also changed with the times so couples can observe the wedding traditions of Ancient Times. Now the Ketubah text reflects the promises from bride and groom to each other.\n- Kiddushin (Blessings of Betrothal) – The first part of the ceremony which takes place under the chuppah and includes blessing of the wine and the ring ceremony.\n- Nisuin – The second part of the ceremony which includes the seven wedding blessings.\n- Seudah (Festive Meal) – A celebratory meal following the fulfillment of a mitzvah (commandment). In this case, a wedding, but also for a bar mitzvah, a brit milah, etc.\n- Sheva Brachot (Sheva B’rachot, The Seven Blessings) – The seven wedding blessings are either sung or recited. They praise God for creating human beings, and for making the groom and bride as happy as Adam and Eve were in the Garden of Eden.\n- Shidduch – Matchmaking in which Jewish singles are introduced to one another in Orthodox Jewish communities for the purpose of marriage.\n- Yichud – During a Jewish wedding the bride and groom are escorted to a private room to spend some time alone. This time of seclusion represents their new status as husband and wife.\nOther Jewish Traditions and Rituals\n- Baby Naming – An event that occurs shortly after birth. At this time the infant is given his/her name(s). According to Jewish custom, girls have their ceremony on the first shabbat after they are born and boys are named on the eighth day after they are born as part of their circumcision ceremony.\n- Bar Mitzvah – The coming of age of a boy and the associated ceremony.\n- Bat Mitzvah – The coming of age of a girl and the associated ceremony.\n- Shabbat – The seventh day of the Jewish week and a day of rest. The Jewish Sabbath is observed from sundown Friday until sundown on Saturday night.\n- Tzedakah – Offering of charity monetarily or through the giving of time.\n- Yahrzeit – The annual anniversary of the death of a relative. A time to remember the souls of the past.Do you have a Jewish tradition or ritual you would like to see added to this page?\nIf so, fill in the form below and if the tradition fits with this site, we will add it shortly.\nPlease Add This Tradition/Ritual.\nPlanning a wedding in particular can call to mind much reflection on Jewish customs and traditions. So, if you’re planning your wedding and have some wonderful unique ideas on how to include your favorite Jewish wedding traditions in your ceremony, please share them with us while they are fresh on your mind.\nAlso, if you’re a wedding planner that can offer great tips, creative solutions, or good advice on how to implement Jewish wedding traditions in various types of ceremonies, please do share. We even encourage you to leave contact information so that brides-to-be can seek out your services for their upcoming weddings.']"	['<urn:uuid:3fab5f9c-1aab-4074-a807-e59edcc17345>', '<urn:uuid:a5936186-637f-4d2b-8400-f3a894c1fe10>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	13	35	2331
62	What happens to naive B cells after they trap antigen?	Naive B cells, after entrapping antigen by their specific receptor and receiving an activation signal from T-helper cells in the cortical region of lymphoid organs, enter B-cell follicles.	"['Genes and Development 12 15711586. The transposon DNA element is autonomous in that it contains all of the necessary sequences for the production and assembly of the transposon machine. The two tRNAs move simulta- neously on the two subunits in a coordinated fashion, it has the potential to serve as a mutagenic agent for any gene.\nNaive B cells, once they entrap antigen by their specific receptor and receive an activation signal from T-helper cells in the cortical region of lymphoid organs, enter B-cell follicles. We call these states S-states (singulet states).\n(1994) Mechanisms of cytotoxtc activtty of opitons Trends Gl,vcoscl Glycotech 6,435-445 9 Arends, MMoms, Rand Wyllie, A.\n3 kb (h) Digestion with Binnary and BanII 0. YMSH3, a structure in the centromere region of each chro- matid that is specialized for conveyance. The major sources of variation in phenotype as described by Futuyma (1998) include Differences binary options software scams genotype (i. Sotware, 1993). Lancet 349 How to use binary option signals. Acting on carboncarbon bonds 3.\nMacromolecules such as proteins, DNA, and RNA contain so many sites of potential hydrogen bonding or ionic, van opt ions Waals, or hydrophobic interactions that the cumulative effect of the many small binding forces can be enormous. Nonsense mutations change a codon for an amino acid to a stop codon. Tumbling is suppressed and smooth swimming is prolonged.\nGlycolysis is the sole source of ATP in some cells; in other cells, it is a preparatory pathway for the complete oxidation of glucose to carbon dioxide binary options software scams water, and an emergency mechanism for ATP production in the absence of oxygen. Since the bacteria and phage att regions possess no sequence homology except for their common core sequences, and hRad52 appears to facilitate homologous pairing by helping to гptions hRad51 to the ssDNA substrate.\nInitiation binary options software scams Prokaryotic fMettRNA fMet Shine-Dalgarno box 5 AGGAGG 3 Large ribosomal subunit (50S) 3 A site 3 mRNA AUG AUG AUG P site P A Feature Figures Special multipage spreads integrate line art, photos, that nothing the United States was doing could defeat the enemy. The coumaroyl ring stays locked in its binding optionswhile the acetyl groups in subsequentextensionstepsgrowandbendtofillthe cyclization pocket.\nOrg. The synthesis concludes by activation with thionyl 29 chloride and a Schotten-Baumann condensation with piperidine. net 3 Page 4 Binding and Catalysis that formation of the enediol intermediate is facilitated (Komives et al. Proteins large polymers composed of hundreds to thousands of amino-acid subunits strung together in a specific order into long chains.\nAnal. If the fungal partners are isolated and cultured alone, they produce colonies similar to those of other fungi on artificial media, but are relatively slow- growing, compact. 40) has been made. 1) is taken into account. They then poured through a filter the mixture of synthetic mRNAs and translational systems containing a tRNA- attached, radioactively labeled amino acid (Fig.\nThe result is a duplication of the green- pigment gene in one chromosome and a deletion of the green-pigment gene in the other. Adjacent proteins likely have specific interactions.\nThe treated cells were reintroduced into the patients marrow. 5) lnmal concen- tration of mononuclear binary options software scams was achreved wrth the lymphocyte collectton pro- gram on a Hemonettcs V50 cell separator (Hemonetlcs. (1996) Which is the best binary options broker of T cells by the ragged tail of MHC class II-presented peptides of the measles virus fusion protein.\nHow, then, is the extraordinarily high specificity of the restriction enzymes generated. Phosphorylation of Thr or Tyr residues in the activation loop is often critical for activation of many protein kinases.\nSeparat- PP ing the appropriate binary options software scams, integrating, and rearranging ultimately yields C(t) AP(1 eFtP). The 5- noncoding region exhibits a cloverleaf folding pattern.\nIncomplete dominance A situation in which the heterozygous genotype has a phenotype that falls somewhere in the range between the two homozygous genotypes. Binary options software scams, cisplatin was 60 second binary options strategy pdf in tumor systems also and found to be active. 1-3 Paraethoxycaine 1_, 10 Paramethadione 1_, 232 Paramethasone J, 200 Paranyline 2_, 218 Parapenzolatebromide275 Parconazole 3, 133 Pargyline 1_, 54; _2, 27 Pazoxide 2, 395 Pacazine T, 387 Pemerid 2, 288 PenfluricTol 29 334 Pentethylcyclanone 1_, 38 Pentapiperium j!, Binary options software scams Pentaquine 1 346 Pentazocine l_9 297; j£, 325 Pentobarbital 1, 268 Pentomone ,3» 2T8 Pentoxphylline 2_9 466 Pentylenetetrazole J.\nBeyond the most optimistic hopes, when DrH8 is essentially constant, because K2 rHT2 T1 lnK RTT 11 112 Table 1 and the NBS Tables give standard formation properties at zero ionic strength. Binary options software scams both labeled messages hybridize to a PCR product, 2003 Chromo.\nPoleksic (1998) Functional enzymes activity and gill his- tology of carp after copper sulfate exposure and recovery. Preventive medicine will place the defective genes in sрftware biological systems and develop ways to avoid their limitations with, for example, drugs, diet, gene therapy, or stem cell therapy.\nE30. The presence of only one kind of crossover points or nodes in the trefoils means that binary options software scams polarities of the wrapping in all the original PP sites att B att P Page 537 Holliday Structures and Branch Migration in Integration 521 P B PB P B PB P B PB B P P B Figure18. Jason and Sylvia received some unsettling news from the amniocentesis of their baby. Monod, J. (b) Model of IES (internal eliminated sequence) deletion in T.\n13b). Abstr. 88)44. The link between this trait and multilocus heterozygosity within sscams may also be crucial in providing a mechanistic link between physiology and population fitness and survival. Prostate and Seminal Vesicle Normal prostattc tissue shows cytoplasmtc bmdmg by many lectms, although scamss is selective bmdmg of Succ-WGA, PNA, and UEAl to the cen- tral zone of glands (72). Reviews of Infectious Diseases 11 (supplement 4) S736742.\nReplication. Acta 1314, 191225; Kemp. It is believed that most intracellular chromatin has the solenoidal supercoiled configuration. Broka, S. Agarose- or sepharose-bound HPA or PNA (varrous commerctal suppliers) 2 Binary options software scams G-Sepharose or Protein A-Sepharose (Pharmacta, Missouri LESTER A.\nEach tip of a branch is represented by binary option money management strategy cloud to indicate the multiple variants that are often found in each viral isolate.\nO?FHJAE CE FFHJKI HEAJ ALAFAJ "" HFDCA "" H-. However, removal of the carbohydrate structure as defined by the specificity of the lectm by the appropriate exo- and endoglycosidase provides direct evidence for the specl- ficlty of lectm labeling Such experiments demonstrate not only specificity of Page 153 Blot Analysis with Lectins 161 lectm-carbohydrate interaction, is an inhibition of platelet production at the medullary level due to a direct or binaryy cytokine-mediated action of the viruses on the megakaryocytes.\nThe dNTPs can also serve as enzyme activators. Lodge was happy to accept the job, once assured he would be provided with a viceroys powers in Saigon. Escherichia coli Is the Most-Studied Prokaryotic Cell Bacterial cells share certain common structural fea- tures, but also show binary options software scams specializations (Fig. 209252. 75ethanol.\nBy com- parison, about 10 people per 1000 suffer from an inherited binary options software scams ease caused by a single-gene mutation.\n5 billion years ago, the fossil record be- gins to show evidence of larger and more complex or- ganisms, probably the earliest eukaryotic cells (Fig. Crawford (1994) The Booroola fecundity (FecB) gene maps to sheep chromo- some Ooptions. Mutagenized library DNA is digested with NotI to release the Tn3-mutagenized yeast fragment, Gal0 1-4GlcNAcP l -2Mana l- G(GalPl-4GlcNAcP 1-2Mana 1-3)Manp l -4GlcNAcP I-4GlcNAc, as a major component.\nIn the lac operon, the CRP site lies at positions -48 to -78 with respect to binary options software scams start of transcription, just next to RNA polymerase. The linear monomeric RNAs contain most probably 5- hydroxyl and 2,3-cyclic phosphate termini. Karger.Binary options software scams, 1969-1972, Vol. Muscle Contraction Mechanism Use of Synchrotron X-ray Diffraction ENCYCLOPEDIA OF LIFE SCIENCES 2001 Nature Publishing Group www.\nels. Page 216 186 TIM WEINER ""THERE IS NOTHING ON PAPER, OF COURSE"" Helms chose William K. Larosa (1995) Polarised Caco-2 cells effects of reactive oxygen metabolites on enterocyte barrier function. Other Duplications Result from Unequal Crossing-Over. Sci. Clin Genet 55483486, 1999. Future binary options software scams will show whether TEs have really been involved here or not.Cambridge, Massachusetts, USA Recombination proteins in eukaryotic cells function to provide genetic diversity and to maintain stability of the genome.\nGCATT C. Sulfur-containing High-temperature Environments as Habitats. 888951. A binary options trading brokers review drop of BPB solution can be placed on the gel along the starting line (a line connecting the two sample wells).\nHe scored success in this quest by the discovery of the antisyphilitic agent,salvarsan (38). As a result, which arises from an altered chain, and thalassemia, which results from decreases in the amount of either - or -chain production.\nCEzn Chem 35, 1638-l 643 Kochlbe, Bniary. S B N pCU999 2. (b) Arrangement of the a helices of bina ry PS I monomer, shown with parts of the adjoining monomers (separated by bold, broken lines), and tentative assignment of the membrane intrinsic subunits. The numbers between the SDS gel and the immunoblot indicate Mr in thousands.\nEffect of temperature Temperature sofftware enzyme activity in much the same way as it affects other chemical binary options software scams. Apart from the normal immune cells such as macrophages, neutrophils and lymphocytes, which reside or migrate to the skin during infection, the skin contains epidermal Langerhans cells, dermal dendritic cells and keratinocytes which are sca ms involved in cutaneous immune defence against dermatophytes.\nReplication proceeds p53 inactive binary options software scams p21 not induced Mutant p53 cant induce functional p21, so cell enters S phase (d) Apoptosis DNA degraded Nucleus binarry cell may be devoured by surrounding cells CDK activity in yeast is controlled by Induces expression of CDK inhibitor, p21 (b) Tumor cells Rb remains binary options software scams and E2F is inhibited, preventing entry into S phase of cell cycle E2F Rb Double minutes Binary options software scams Figure 18.\nconserved motifs are part of binary options software scams fold and interact with the nucleotide (Figure 1a). Hicks, W. Els. It will also depend on the interac- tion of the contaminant(s) with other environmental factors such as temperature, salinity and dissolved oxygen levels.\nSending anythingto the Iranians was against the law and the foreign pol- icy of the United States. The radioactive decay of the 32P sensitizes the binary trading live charts halide crystals in the film so that upon development, black particles of silver remain to reveal positions of radioactive DNA or RNA in the gel.\n) Page 791 divisions in the development of C. Hypersaline environments also occur in subterranean evaporite deposits and deep-sea optio ns created by the evaporation and flooding of ancient seas. Binnary process is highly binary options software scams by secreted signalling molecules called cytokines that help direct the proliferation and differentiation of all the blood cell lineages, 1989).Best rated binary options brokers']"	['<urn:uuid:d428815a-0a5c-412f-8cb4-2413cff267af>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	10	28	1840
63	How did submarines first get into military use?	Military use of submarines began during the First Anglo-Dutch War (1652-1654) with Louis de Son's 'Rotterdam Boat', a 72ft-long semi-submerged battering-ram designed to punch holes in enemy warships. However, it couldn't move once launched. The first submarine to actually attack a ship was the Turtle during the American War of Independence in 1776, which attempted to attack HMS Eagle in New York harbor by attaching a 150lb-keg of gunpowder to the ship's hull, though the attack failed.	['The legendary origins of the submarine stretch back to 332 BC with a tale about Alexander the Great being lowered into the sea in a glass barrel to study fish. The submarine concept was thereafter consigned to the backwaters of history for some 1,800 years.\nIt reappears with the publication in 1578 of Inventions or Devises by William Bourne, an English gunner turned innkeeper and mathematician. In this work, Bourne describes the principle of making a boat sink and rise again by changing the volume of the ship. If you contract the volume of the ship, it will sink; if you expand its volume, it will float upward. The exact process for doing this is not made clear, and contemporary materials and techniques precluded effective experiment.\nThe Alexander legend and Bourne’s principle related more to the diving bell than a boat. The next step forward, conceptually, was to add some form of propulsion. The Dutchman Cornelius van Drebbel achieved this around 1620.\nHis boat, Drebbel I, is probably the first working submarine. Basically an enclosed rowboat manned by 12 oarsmen, it probably had a sloping foredeck. This would have forced the boat under as forward momentum was applied, like the angled plane of a modern submarine.\nIn 1636, a French priest, Marin Mersenne, added another piece to the jigsaw. He suggested that a submarine should be built of copper and be cylindrical in shape to better withstand increasing pressure at depth. Early designs for submarines, henceforth, generally adopted a porpoise-like form. Despite these early concepts and the Drebbel I prototype, it was more than 200 years before the French Navy launched the first true precursor of the modern submarine. In 1863, the Plongeur (‘Diver’), which was powered by engines run on compressed air, became the first submarine that did not rely on human propulsion for momentum.\nMilitary possibilities of the Submarine\nIt was not long before the military possibilities of a submerged boat began to be realised. As early as the First Anglo-Dutch War (1652- 1654), Louis de Son had built his 72ft-long ‘Rotterdam Boat’. This, in effect, was a semi-submerged battering-ram designed to approach an enemy warship unnoticed and punch a hole in its side. Once launched, though, it was unable to move.\nThe American War of Independence provided further impetus in the form of David Bushnell’s Turtle. Water was pumped in and out of the skin of the boat to change its ballast, thus enabling the boat to sink and rise. This one-man boat was driven by hand-cranked propellers, one to provide vertical movement and another to provide horizontal drive. The Turtle became the first submarine to attack a ship, probably the HMS Eagle, in New York harbour in 1776. The attack failed, as Ezra Lee, the boat’s pilot, was unable to attach its armament, a 150lb-keg of gunpowder, to the enemy ship’s hull.\nAnother American, Robert Fulton, attracted the attention of Napoleon in 1800 with his Nautilus. This submarine had a number of successful test dives, reaching a depth of 25ft and an underwater speed of 4 knots. It was driven by a hand-cranked propeller underwater, and by a sail when on the surface. Although it made a number of attacks on Royal Navy ships, they could always see the Nautilus coming and easily evaded it.\nFailure meant Fulton’s dismissal, and the Royal Navy, with the world’s largest fleet, breathed a sigh of relief. Submarine warfare did not develop further for 50 years. Then, the American Civil War (1861-1865) provided a major stimulus, particularly on the Confederate side. The Union had retained control of the US Navy, and its blockade of the South meant that the Confederacy was bound to search for ways to break it: the submarine was one of these.\nSeveral prototypes were built – by both sides – but these depended primarily on improvements to established technology rather than anything radically new. The most significant achievement was the destruction of the USS Housatonic in 1864, the first submarine victory. The oar-propelled CSS Hunley attacked the Housatonic with an explosive device on the end of a spar that was attached to its nose. Though the Hunley did not survive the attack, war beneath the waves had definitely begun.\nThe Royal Navy and the modern submarine\nThe real breakthrough, and the birth of the modern submarine, came courtesy of John Phillip Holland, towards the end of the 19th century. He became the first designer to successfully unite three new pieces of technology – the electric motor, the electric battery, and the internal combustion engine – to create the first recognisably modern submarine.\nThe Admiralty’s official position at the time was to give submarine development ‘no encouragement’. But it could not afford to ignore it completely, and, in October 1900, five Hollands were ordered with the purpose of testing ‘the value of the submarine in the hands of our enemy’. The Hollands were built under licence at Vickers’ yards in Barrow, which was to become the home of British submarine construction.\nThe traditionalist view at the Admiralty thought of submarine warfare, in the words of Rear Admiral Wilson, as ‘underhand, unfair, and damned un-English’. Notwithstanding such views, the submarine gained a champion in Admiral ‘Jacky’ Fisher. Having watched the five Hollands ‘sink’ four warships in an exercise to defend Portsmouth Harbour, Fisher realised that naval warfare had changed. So, when he became First Sea Lord (1904-1910), he diverted 5% of the Navy’s shipbuilding budget, despite strong opposition, to the construction of submarines.\nFrom the beginning of Fisher’s tenure to the outbreak of the First World War, there was continual development of the submarine, from the Hollands through A to D classes. The D-class, with its decking and deck gun, represented a major change from the porpoise shape of earlier submarines, and introduced the form that would become familiar through two world wars.\nSubmarines in World War Two\nLulled into the belief that ASDIC made submarines irrelevant, the British Government, advised by the Admiralty, agreed in 1935 that the German Navy should be allowed the same tonnage of submarines as the Royal Navy.\nCaptain, later Admiral, Dönitz was ready with his submarine strategy. WWI experience implied that in a ‘tonnage war’, merchant ships could be sunk faster than they could be replaced. In order to achieve this, U-boats were to operate in Atlantic waters in ‘wolf packs’: seven or eight boats would shadow merchantmen across the sea, attack at night, and then submerge to escape, ready for the next attack.\nThe strategy worked until mid-1943, when the Germans had lost 250 submarines and sunk over 3,000 Allied vessels. In May, the tide turned, with 42 U-boats sunk in that month alone, forcing Dönitz to withdraw his fleet from the Atlantic. Even so, over the next two years they lost a further 520 submarines and sank only 200 ships. American aid, the convoy system, long-range air cover, and improvements in detection and anti-submarine weapons all had their effect.\nHaving lost the Battle of the Atlantic, the Germans were forced to rethink. One result was the development of the snorkel, a breathing tube that meant the submarine could use its diesel engine whilst just below the surface, conserving battery power. It also made submarines less visible from the air, though the snorkel did leave a trailing wake, and it could be picked up on sonar. The standard U-boat had been the Type VII, of which more than 700 were built. They were around 200ft long, with a surface displacement of 760 tons, and a surface speed of 15 knots, equalling the speed of most surface ships. They had a dive time of 20 seconds to a maximum safe depth of 650ft, a range of over 8,700 miles, and could go seven or eight weeks without refuelling. Britain’s equivalent workhorse was the T-class.\nThey were the first of the Navy’s boats to have their fuel tanks inside the hull, eradicating the problem of leaking fuel leaving surface trails. Whilst slightly smaller than the classes they replaced, they were an all-round improvement, and an all-welded hull meant they were stronger and able to dive deeper.\nThe T-class performed sterling service in all naval theatres of war. HMS Truant, for example, sank enemy ships in home waters, in the Mediterranean, and in the Far East – clocking up 81,000 tonnes of destruction in all. There was also success in the Far East for HMS Trenchant, which sank the Japanese heavy cruiser Ashigara.\nSubmarines during The Cold War\nPost WWII developments were dominated by the Cold War and the arms race between the US and the USSR. Changed political realities meant a different role for the submarine. The Royal Navy’s job ceased to be aimed at attacking surface shipping, and focused instead on the interception of Soviet submarines.\nThe new Amphion class had been designed and introduced towards the end of WWII, but the submarine’s new role, and the development of increasingly sophisticated equipment, meant they were gradually refitted. They had already been given the Snort mast, a development of the German snorkel, and air-warning radar that worked whilst the submarine was underwater. Extra streamlining was introduced, which included the removal of the deck-gun; but perhaps the most important advances were in the complex array of sonar devices that were added to the boat.\nThe Americans had also been busy, and another German invention, the rocket, became one of the major areas of advance in submarine design. The US’s experimentation with sub-launched missiles would lead to Polaris and Trident.\nThey also went nuclear in the sense of having developed a suitable power-plant for a submarine. In 1955, USS Nautilus made the first nuclear-powered submarine patrol, all 323ft and 3,674 tons of it. It had a surface speed of 18 knots and a capability of reaching 23 knots submerged. The Nautilus also represented a radical shift in design. Capable of sustained underwater cruising, the Nautilus had returned to the streamlined, porpoise shape of the early pioneers, for there was now no need to spend long periods on the surface. It revolutionised naval warfare, for it combined the stealth and surprise of traditional submarines with a speed greater than their quarry.\nThe British, too, developed nuclear-powered submarines, and Dreadnought, the Navy’s first example, went to sea in 1963. There were two strands to British design: one was the attack submarine, with responsibility for protecting Britain’s nuclear deterrent; the other was the Submerged Ship Ballistic Nuclear (SSBN), which carried Britain’s nuclear deterrent. The most famous of the latter was the Resolution Class HMS Conqueror, which sank the Belgrano during the Falklands War in 1982, and remains the only nuclear submarine with an official kill.\nSuch combined operations point the way to contemporary military strategy. As the Malta Convention of 1998 declared the Cold War over, so the role of the submarine has changed. It is no longer just anti-submarine work, but, in military terminology, ‘Maritime Contributions to Joint Operations’. This also includes the ability to launch special forces operations and undertake intelligence gathering – but the Silent Service has always been capable of multi-tasking. Silent, submerged, and lethal, the submarine has changed the face of naval warfare.']	['<urn:uuid:adae528f-5a04-4660-84de-ce01e8e22be2>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	8	77	1860
64	moving average methods forecasting time series	Moving average methods used for forecasting include simple moving average where all weights are equal to 1/m, weighted moving average which gives more weight to newer data, and exponential weighted moving average. These methods are used for smoothing data and predicting future behavior, especially in stationary time series that show no clear trends or seasonality.	"['Two New Indicators Kaufman Adaptive Moving Aveage – KAMA and Fisher Transform Indicator by Ehlers are added in our Interactive charting section. Now users can make use of these indicators in NSE FNO 5min charts and NSE Cash and Futures EOD charting section\nKaufman Adaptive Moving Average\nKaufman Adaptive Moving Average (KAMA) was created by Perry Kaufman. Wikipedia says Perry J. Kaufman is an systematic trader, index developer, and quantitative financial theorist. He is considered a leading expert in the development of fully algorithmic trading programs.\nGenerally Moving averages are one of the most favourite trading tools for the traders. As other indicators moving average also has its own advantages and setbacks.\nPros and cons of moving averages:\nSimple moving average:\nIn moving averages, by averaging the data for a stated number of days,one can derive a sort of automated trendline which highlights the trend. While smoothing the data, trader always gives back a part of profit as moving averages lag behind the market action.\nWeighted moving average or Exponential moving average:\nTo overcome the setback in simple moving average, weighted moving average compared to SMA tracks the price very closely by assigning high weightage to the recent data. The formula to calculate an exponential moving average is:\nEMA = (W * Close) + ((1-W) * EM)\n- W is the smoothing constant calculated as\n2 / (N + 1)\n- which remains constant\n- EM is the exponential moving average from yesterday\nEventhough EMA overcomes the lag resulted in Simple Moving Average,it failed to reduce the losing trades.\nAdaptive Moving Average:\nPerry Kaufman introduced the Adaptive Moving Average which compared the price direction with the level of volatility.The name “Adaptive” reveals that AMA automatically adapts to the market volatility.AMA traverse between fast and slow moving average.\nIf a market is advancing with very small countertrend moves, the degree of noise is low and enough if one follows a shorter look back period.However, if the market is moving sideways, the degree of noise is high and one should look for a longer look back period. So AMA is designed likewise , to move slower in a sideways market and faster in a trending market.\nIn AMA, Kaufman replaced the “weight” variable with a factor “Efficiency Ratio” ,which can be calculated as follows:\nER = (total price change for period) / (sum of absolute price changes for each bar)\nPrinciple behind ER: An ER of +1.0 indicates that the stock is in a perfect uptrend; -1.0 represents a perfect downtrend. In practical terms, the extremes are rarely reached.\nAMA can be calculated as follows:\n1. Calculation of weight factor\nC = [(ER * (SCF – SCS)) + SCS]2\n- SCF is the exponential constant for the fastest EMA allowable (usually 2)\n- SCS is the exponential constant for the slowest EMA allowable (often 30)\n- ER is the efficiency ratio that was noted above\n- C is the weight factor which is adaptive\nSCS can be calculated as\n2 / (SA + 1)\nWhere, SA – trader’s choice of a slow moving average > FA\nSCF can be calculated as\n2 / (FA + 1)\nWhere, FA – choice of a slow moving average < SA\n2. Replacing the weight factor in Exponential Moving Average formula\nAfter finding the C value, W in EMA formula should be replaced with C like,\nAMA = (C * Close) + ((1-C) * EM)\n- C is the smoothing constant calculated by applying Efficiency ratio\n- EM is the exponential moving average from yesterday\nSimple understanding of AMA:\nIn the above chart,when price moves sideways Adaptive Moving Average(in blue) maintained a flattened line whereas, the Exponential Moving Average(in pale green) moved with the choppiness of prices.\nThe adaptive moving average’s strength is its ability to respond to changing market conditions, which is a problem for studies that use fixed look-back periods.\nUsing a fixed look-back period is not possible. Because the market is always changing, static approaches are likely to have limited success. Using adaptive study which is dynamic is a potential way to get good results. The AMA could also be combined with other indicators to develop a profitable trading system.\nAmibroker Users Download the AFL Code for Kaufman Adaptive Moving Average here\nGeneral assumption in share market is, market price follows the Gaussian Normal Distribution forming a bell shaped curve.In practice, the price movements not only follows Gaussian distribution but also other probability distribution functions. At this point,a indicator is necessary to transform such values to Gaussian normal distribution to identify trend reversals.\nFisher Tansform is the one ,which transforms any probability distribution function to Gaussian normal distribution\nAssumption: underlying prices follows square wave distribution\nThe fisher transform formula is as follows:\nWhere x is the correlation coefficient.\nHow transformation is made\n1. Prices should be constrained to remain within the limits of -1 to +1.\n2. Comes the real transformation using fisher transform formula\nAs Gaussian normal distribution function most of the values are highly concentrated around the mean, while the extremes are observed to be concentrated at the right and left tails of the bell shaped curve. So transformed time series helps to identify trends,i.e sharp turning points in price movements\nHow to Apply the Indicator\nIn this indicator along with fisher line,fisher delay(one previous fisher bar) is also added.Signals are identified when fisher line crosses above or below its signal line.\nBullish signal – Fisher line turns up below -1 threshold & crosses up the fisher delay line\nBearish signal -Fisher line turns down above +1 threshold & crosses down the fisher delay line\nIts shown in the below chart\nAmibroker Users can Download the Fisher Transform Amibroker AFL code here\nThe Fisher Transformation transforms the probability density function of any waveform into a Gaussian-like probability density function. Unlike other indicators & oscillators, the Fisher Transformation introduces no lag and provides the turning points in advance. It is an excellent tool to improve our trading systems.', 'weighted moving average method forecasting\nThe Weighted Moving Average method is usually used for smoothing purposes. However, it can be used to forecast Y(t1) based on the last n observed data. In real-world problems, forecasting in very short horizon (h 1) is not very interesting. This type of forecasting is called weighted moving average. Here we assign m weights w1, , wm, where w1 . wm 1, and define the forecasted values as follows. In the simple moving average method all the weights are equal to 1/m. What is Weighted Moving Average Method? This approach is based on the principle that more weightage should be given to relatively newer data. The forecast is the weighted average of data. Forecasting- moving average, exponential smoothing, trend. value of 945 provides the best forecast? projection e. Which method of forecasting would you Makes forecasts using moving average, weighted moving average In this article, we will discuss 2 types of forecasting models the weighted and unweighted moving average model.Assuming the presence of trend and cyclical behavior in the original series, the moving average process provides a method of isolating it. S Spencers weighted moving average H Hendersons weighted moving average Table 6.3: Commonly used weights in weighted moving averages.Forecasting data and methods. Some case studies. The basic steps in a forecasting task. This forecasting method is similar to the simple moving average method, since it moves one period at a time once another period has completed however, the new forecast is not calculated the same way. In order to calculate a weighted moving average Link. Moving Average Method.\n63,642 views. Share.1. Moving Average Method Time Series Secular Trend 3.Moving Average Method 4. Method Of Least Squares.7. Exponential Weight: Example Year Response Smoothing Value Forecast (W .2, (1-W).8) 1994 2 2 NA 1995 5 (.2)(5) What is Weighted Moving Average Method? This approach is based on the principle that more weightage should be given to relatively newer data.15/03/2016 Tutorial on how to conduct a weighted moving average forecast in Excel. In last weeks Forecast Friday post, we discussed moving average forecasting methods, both simple and weighted. When a time series is stationary, that is, exhibits no discernable trend or seasonality and is subject only to the randomness of everyday existence, then moving average methods What is Weighted Moving Average Method? This approach is based on the principle that more weightage should be given to relatively newer data.\nThe forecast is the 2.1.1 Nave Forecast Method. 2.1.2 Cumulative Mean Method. 2.1.3 Moving Average Method.Rosili, Athens, Greece (in Greek) Holt CC (1957) Forecasting trends and seasonals by exponentially weighted moving averages. Learn how to use a weighted moving average smoothen data and predict future behaviour, as part of Time Series Analysis explore on our forexForecasting Methods are common in modeling demand technical when calculates average, one. pros cons technique exponentially chart plots values. And some of the other advanced Time Series Methods such as ARIMA, GARCH and ARCH. In this blog, main focus will be to explain and use Simple and Weighted Moving Average Methods of Forecasting a time series values. Abstract: Time series forecasting is an active research area that has drawn considerable attention for applications in a va-riety of areas.The following are the weights shifted weighted moving average method and on the improvement measures are introduced. Recently, Tsokos (2010) introduced a class of weighted methods for forecasting nonstationary time series which is called the k-th moving average, k-th weighted moving average and k-th exponential weighted moving average processes. Forecasting with moving averages. Robert Nau Fuqua School of Business, Duke University August 2014.The SMA model is an easy-to-understand method for estimating the local mean value around which a time series is thought to be randomly varying, but putting equal weight on the last m When using a moving average method described before, each of the observations used to compute the forecasted value is weighted equally. In certain cases, it might be beneficial to put more weight on the observations that are closer to the time period being forecast. Hendersons weighted moving averages are also widely used, especially as part of seasonal adjustment methods (Ladiray Quenneville 2001).A simple forecasting method is to simply average the last few observed values of a time series. In this paper, the Exponentially Weighted Moving Average (EWMA) method with designed input data assignments (i.e. the proposed method) is presented to forecast lime prices in Thailand during January 2016 to December 2016. [Summary]Forecasting - Weighted moving average - Example 1 In this video, you will learn the how to find out the demand forecast using Weighted moving average method. The Weighted Moving Average (WMA) method is similar to Method 4, Moving Average (MA).Like all linear moving average forecasting techniques, forecast bias and systematic errors occur when the product sales history exhibits strong trend or seasonal patterns. Forecast accuracy To use the weighted moving averages method, we must first select the number of data values to be included in the weighted moving average and then choose weights for each of the data values. The Naive Methods Simple Moving Average Method Weighted Moving Average Exponential Smoothing Evaluating the forecast accuracy Trend Projections Linear Regression Analysis Least Squares Method for Linear Regression Decomposition of the time series Selecting A Suitable Forecasting Methods. 8. Stationary time series. The moving average is moving because we look each time at the last N values.We could find this inappropriate and want to give higher weights to more recent data. This leads to the weighted moving average Weighted Moving Average Forecasting. Using Solver to Optimize the Weights .Moving-average-methods -Describing an important method of forecasting called moving. The time series data forecasting results using B-WEMA then be compared with other conventional and hybrid moving average methods, such as weighted moving average (WMA), exponential moving average (EMA), and Browns double exponential smoothing (B-DES). Then use that method to provide your forecast for week 7 in part c. Week Unit Sold. 1 300.6 335. Calculate the MAD for the 2 period weighted moving average forecasting technique. Use weights of 0.8 and 0.2, with the most recent observation weighted higher. Images for Weighted Moving Average Forecastingweighted moving average adjusts the moving average method to reflect www.apicsforum.comForecasting: Weighted Moving Averages, MAD - YouTube i.ytimg.com This method provides an exponentially weighted moving average of all previously observed values. Appropriate for data with no predictable upward or downward trend. The aim is to estimate the current level and use it as a forecast of future value. Forecasting — In the simple moving average method each of the N periods is equally important for the purpose of forecasting. Exponential smoothing method computes a forecast value which is the weighted average of the most recent data and forecast values. Forecasting With the Weighted Moving. More Info ""placeholder (or filler) text.""Forecast Friday Topic: Moving Average Methods | Insight — (Third in a series) One of the easiest, most common time series forecasting techniques is that of the moving average. This master thesis considers the time series methods of a quantitative forecasting. These methods are classified into the moving average, weighted moving average, and exponential smoothing methods. Difficulty: Moderate Keywords: weighted, moving, average 90.Methods Difficulty: Moderate Keywords: moving, average 95. Use the information in Table 13.3. Calculate the exponential smoothing forecast for week 5 using 0.10 and F 4 410. Simple, weighted and exponential moving averages are most popular value-at-risk analyses, exponentially (ewma) estimation construct covariance matrices non-uniform weighting наиболее полная статья о том, что такое скользящее среднее (moving average), как правильно его. Moving averages are one of the most popular tools used by active traders to measure momentum. The primary difference between a simple moving average, weighted moving average and exponential moving average is the formula used to create them. This video explains simple weighted moving average methods of forecasting and its numericals. Very simple questions have been asked from this topic in the previous GATE exams. Advantages and disadvantages of the moving average method, The trend obtained by moving averages generally is neither a straight line nor a standard curve. for this reason the trend cannot be extended for forecasting future Weighted moving average | real statistics using excel 1. Forecasting with Moving Averages. The naive method discussed in Lecture 1 uses the most recent observations to forecast future values.Moving averages are developed based on an average of weighted observations, which tends to smooth out short-term irregularity in the data series. Forecasting methods: Averaging methods. Equally weighted observations Exponential Smoothing methods.document.write(adsense.getbannercode(200x90)) Slide 1 1 Forecasting Slide 2 2 Demand Management Qualitative Forecasting Methods Simple Weighted Moving The moving average method is one of the empirical methods for smoothing and forecasting time-series. The essence: the absolute values of a time-series change to average arithmetic values at certain intervals. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a type of infinite impulse response filter that applies weighting factors which decrease exponentially.Quantitative forecasting methods. 12 Qualitative Methods Grass Roots Market Research Panel Consensus Executive Judgment Historical analogy Delphi Method Qualitative Methods.26 Weighted Moving Average Problem (1) Data Weights: t-1.5 t-2.3 t-3.2 Question: Given the weekly demand and weights, what is the forecast for Weighted moving average method: The forecast for next period (period t1) will be equal to a.\nweighted average of a specified number of the most recent observations. In this illustration we assume that a 3-year weighted moving average is being used. LESSON 5: FORECASTING STATIONARY TIME SERIES METHODS. Outline Simple Moving Average Weighted Moving Average Exponential Smoothing Comparison of Simple Moving Average and Exponential Smoothing. Time Series Methods. - PowerPoint PPT Presentation. To Appear: Journal of Modern Applied Statistical Methods, 2008. A Weighted Moving Average Process for Forecasting. Shou Hsing Shih Chris P. Tsokos. Department of Mathematics and Statistics University of South Florida, USA. A moving average method uses a number of most recent historical actual data values to generate a forecast.Following types of rolling averages are used more often than others: a simple moving average (SMA) a weighted moving average (WMA) and an exponentially moving average (EMA). 28 28 Weighted Moving Average Problem (2) Data Weights: t-1.7 t-2.2 t-3.1 Question: Given the weekly demand information and weights, what is the weighted moving average forecast of the 5 th period or week?Qualitative Forecasting Methods. This video explains simple weighted moving average methods of forecasting and its numericals. Very simple questions have been asked from this topic in The k-days exponential weighted moving average process, in addition to what the previous two models offer, instead of decreasing weight consistently as the weighted moving average method does, it decreases the weight exponentially.A weighted moving average process for forecasting.']"	['<urn:uuid:ce5186d2-cf11-4836-8b4b-3e6599a3c4a0>', '<urn:uuid:1f734713-48d4-4424-afb4-a39445cbfb70>']	factoid	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-12T17:51:21.175311	6	55	2769
65	contact form 7 vs wpforms lite features	Contact Form 7 is the most popular free contact form plugin that uses shortcodes and has extensive documentation and add-ons. WPForms Lite offers a more visual interface with drag-and-drop field creation. While Contact Form 7 is highly customizable through add-ons, WPForms Lite is more straightforward for beginners but requires a Premium version for advanced features like multi-step forms and payment forms.	"['Translated by Núria Adell.\nApart from a few exceptions, all websites need some sort of mechanism to interact with the visitors, aside from managing or not to make them read the content. Many WordPress themes directly incorporate some sort of contact forms so that the readers can get in contact with you.\nBut if this is not your case, there are plenty of plugins that allow you to add forms to your website. Actually, you can install in your dashboard more than 1,000! 😯 (please, don’t do it)\nBut before discussing a few plugins, do you already know the basic characteristics that every contact form plugin should have? In most cases, they’re the following:\n- Define or configure any form you want your visitor to see,\n- Indicate where you want your form to appear: page, post or widget,\n- Indicate messages of confirmation or error that you want to come up after filling out the form,\n- Indicate where you want to get the information from the correctly filled out form.\nAnd from here, you can complicate it as much as you want with complex contact forms, sending the information to multiple users, controlling spam, integrating it with other platforms, etc.\nFor example, let’s see how contact forms work in Contact Form 7, the most popular free contact form plugin out there; these are the basic characteristics:\nContact Form 7 is one of the plugins included in the list of the 20+ plugins with more than 1 million active installations. And, as I have already mentioned, it’s the most popular free contact form plugin. It includes the following options:\nConfigurating contact forms\nOnce you have installed the plugin in the WordPress dashboard, with the option Contact » Contact Forms you can configure all the forms you want to be shown on your website.\nAdding a new contact form on a post or page\nEach contact form has it’s own shortcode, like\n[contact-form-7 404 ""Not Found""], which is what you have to copy and paste on the post, page or widget where you want to include the desired form.\nOnce it has been added, you will have the contact form on your post or page:\nAdding or modifying fields on the form\nWhen editing a form, you will see four tabs coming up: Form, Mail, Messages, and Additional Settings.\nOn the first one, Form, you will see the labels your form will have, together with the type of fields you want your visitor to fill. You can add, modify or delete the fields shown in the form as you wish. More details on the syntax of the labels can be found in Tag syntax.\nWhere to send the filled-out form\nOn the second tab of the form, Mail, you can indicate the email address to which you wish to send the form information, additional headings, or the body of the message you want to write.\nAutomatic answer messages\nOn the third tab, Messages, you can indicate the sentences you want to be shown depending on the information that’s introduced in the form.\nDespite being a totally free plugin, it has very complete documentation. And one of the great advantages of the popularity of this plugin is the amount of Add-ons that have been created to increase the power of the plugin itself. For instance, Contact Form 7 – PayPal Add-on to integrate payment forms with PayPal; Contact Form 7 MailChimp Extension to be able to automatically incorporate your contacts in a MailChimp list; or Contact Form 7 Style that allows you to create contact forms with styles that are much more diverse than the default ones of Contact Form 7.\nSo after looking at this, what are the most popular alternatives to using Contact Form 7?\nWPForms has a free version, WPForms Lite, and the great advantage of this form plugin is that the creation of forms is very visual and straightforward with an interface that allows to drag and drop fields.\nFor additional functionalities, such as multi-step forms, payment forms, or special conditions, etc., you need to subscribe to the Premium version.\nNinja Forms also has a free version and, even though initially it might not be as visual and easy to use since it doesn’t have a default contact form, the free version is more powerful. And it has many interesting extensions in Premium versions, such as the integration with Campaign Monitor, Freshbooks, Salesforce, SMS notifications, etc.\nYou have both the option of buying each of the extensions separately or getting packages that already include the most popular extensions. With such extensions, you get prices similar to those of the Gravity Forms (see below).\nThe plugin Fast Secure Contact Form is a free plugin that generates forms and also blocks spammers. It allows you to generate very professional forms in a simple way, easily adding and removing fields. The filled-out forms can be sent to one or several email addresses and it also allows attached files to be added to the form. There’s only a free version.\nThe plugin Visual Form Builder has a free version and several pro versions, starting from $29. You can easily create forms with a very professional interface, send the information to multiple email addresses and, once the form has been filled out, redirect to the page of your interest.\nThe plugin Pirate Forms is a free plugin created by the team of Themeisle and CodeInWP. It has a very intuitive interface to create simple contact forms and includes all the basic characteristics required. It has been made to create forms without complications, however it’s important to highlight it doesn’t give you the option to integrate them with payment platforms.\nThe plugin Contact Form by BestWebSoft is also free and allows you to send the information of the forms to more than one email address, change the aspect and type of form, and let the reader add attached files. Just like Contact Form 7, it works with shortcodes. It’s a plugin created for beginners, so it’s very intuitive.\nGravity Forms is a paid plugin (starting from $39 for a website) that has all the alternatives you can think of related to contact forms. It includes forms with multiple pages visualizing what’s left to fill out, it allows to fill in only a maximum number of forms (if it’s, for example, a limited application), and it lets you arrange visit times including post codes.\nIf you want to create complex forms, don’t hesitate to look into this option as a good alternative.\nAs I have mentioned at the beginning, it is possible that your theme includes some contact form that’s already sufficient for what you need. But if you’re looking for something simple and you’re not a great expert in WordPress, maybe WPForms is the plugin that, to start with, seems to be the most simple to use.\nThe truth is all the plugins I previously mentioned look very straightforward to me, even if you’re a beginner. I recommend you install any of them and you’ll see that, in a few minutes you’ll manage to create a form for your site.\nIf you’re looking for something more sophisticated, I recommend you look at one of the Premium versions. Amongst other motives, to make sure you have the service support for any problem you may have.\nOh! And finally, it is also interesting to analyze the alternatives of emerging contact forms (pop-ups or opt-ins). For instance, in this blog, we’ve been using the OptinMonster for a while. If you’ve been reading until here such pop-up has probably already come up.\nSo what is your experience with contact forms? Do you use any plugin that I haven’t mentioned? Leave a comment to share your case.\nFeatured image by Mathias Kurman.']"	['<urn:uuid:306ec23f-5d1b-488c-b9e8-d07f319f049d>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	7	61	1294
66	As an astronomy enthusiast studying celestial coordinates, I'm curious about the sun's position during the December solstice - what is the exact declination of the sun at this time?	During the December solstice, the sun reaches a declination of exactly 23 degrees and 26 minutes, which is the same as the angle at which the Earth's poles are tilted from the ecliptic.	"['Next week marks a major turning point in the annual cycle of the seasons: the winter solstice. It takes place at 11:48 p.m. EST on Dec. 21 (0448 GMT Dec. 22). At that moment, the sun reaches its southernmost position in the sky, resulting in the shortest day of the year in the Northern Hemisphere, and the longest day of the year in the Southern Hemisphere.\nThe solstice gets its name from the apparent stop (""stice"") in the motion of the sun (""sol""). The earliest astronomers recorded this phenomenon carefully: Monuments like Stonehenge are thought to have been used to mark the extreme positions of the sun in the sky. The December solstice has long marked the beginning of the new year, and it\'s mainly because of slippage in our calendar that it now occurs 11 days before the ""official"" start to the new year, Jan. 1.\nThe solstice is a day of celebration in many cultures. The Romans knew it as ""Saturnalia,"" and the early Christians adopted the date of the solstice to mark the birth of Christ, so that they could celebrate without drawing the attention of their Roman masters.\nThe solstice will fall on Tuesday or Wednesday, because, although the time of the solstice is exactly the same everywhere in the world, because of local time zones, it falls on different days in different places. The exact time of the solstice this year is 0448 UTC on Dec. 22. (UTC, or coordinated universal time, is the basis for all time zones and is used by astronomers and pilots everywhere.)\nIn England, where the Earth\'s prime meridian lies, the solstice will occur at 4:48 a.m. GMT on Wednesday, Dec. 22. Similarly, in other parts of Europe and in Africa, the solstice will occur in the early hours of Wednesday morning.\nIn North America, we subtract a number of hours from UTC to get our local times. In most of eastern North America, we are on Eastern Standard Time, and subtract 5 hours, so the solstice falls at 11:48 p.m. on the previous day, Tuesday, Dec. 21. The farther west we go in North America, the earlier the solstice occurs in the evening, so that on the Pacific coast, it occurs at 8:48 p.m. PST.\nRemember, these are all exactly the same time in the broader scheme of things; local times are just vagaries of the way we handle time around the world.\nThe graphic above shows the sky as it would appear at the time of solstice from a location where the solstice occurs at noon, which, this year, would be Vietnam. Through the Starry Night software, we have turned the blue sky transparent and have added the two coordinate systems we use to mark positions in the sky.\nThe red line across the sky marks the celestial equator, halfway between the celestial North and South poles. It shows 18h on the meridian because right ascension, the celestial equivalent of longitude, is measured from the vernal equinox, exactly 9 months ago.\nThe green line marks the ecliptic, the path that the sun appears to follow across the sky. When the solstice occurs, the sun is at its southernmost position, at a declination (celestial equivalent of latitude) of exactly 23 degrees and 26 minutes, which just happens to be the exact angle at which the Earth\'s poles are tilted from the ecliptic.\nThe height of the sun above the horizon on solstice day depends on your latitude on the surface of the Earth. Where I live in southern Canada, it hangs very low in the southern sky, barely 22 degrees above the horizon. For anyone north of the Arctic Circle, it never rises at all. Even in the southernmost continental United States, it\'s barely 42 degrees above the horizon, less than halfway up in the sky.\nIn the Southern Hemisphere, the situation is reversed. The December solstice marks the longest day in the year, combined with the shortest night. Longer days mean more hours of sunshine and warmer weather.\nI tend to avoid using the words ""winter"" and ""summer"" when talking about the solstices because, even though the sun\'s position in the sky is responsible for our seasons, it does not match exactly with the seasons as we experience them. Sometimes, the astronomical solstices and equinoxes are referred to as ""the first day of"" a new season, but these days don\'t actually indicate when snow will start falling in December or when flowers will start blooming in March. Depending on location conditions, seasons arrive and depart in different ways, and vary in severity. Hence the old joke about the Canadian seasons: 11 months of winter and one month of bad sledding.\nThis article was provided to Space.com byS imulation Curriculum, the leader in space science curriculum solutions and the makers of Starry Night and SkySafari. Follow Starry Night on Twitter @StarryNightEdu. Follow us @Spacedotcom, Facebook and Google+. Original article on Space.com.']"	['<urn:uuid:077b3818-9ced-4942-ba9e-eed0f1da2502>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	29	33	822
67	I've read some recipes comparing something called biga and poolish for pizza making. What's the main difference between these two methods?	Biga and poolish differ primarily in their hydration levels and fermentation times. Biga is a firm starter with lower hydration (50-60% water) that needs to rise overnight at room temperature. Poolish, on the other hand, has higher hydration (around 70% water) and requires less time to ferment. These differences affect the final result - biga produces a slightly denser texture with a crispy crust, while poolish creates a softer, more open crumb.	['Biga and poolish are two pre-ferment methods used in making pizza dough. Each produces different flavors and textures.\nPizza enthusiasts are continuously searching for new ways to enhance this italian classic. The pre-ferment method has been gaining popularity, and two common methods are frequently used for making pizza dough- the biga and poolish methods. These methods have been used for years in bread baking and have recently found their way into pizza crusts.\nBoth employ a pre-ferment process where a small portion of flour, water, and yeast, are mixed and then left to ferment before being added to the pizza dough. The purpose of this is to extend fermentation time, allowing flavors to intensify and the dough to have a more open and airy texture when baked. Although both methods are similar, there are notable differences in how they are made and the resulting pizza crust’s flavor and texture. In this article, we will explore the differences between biga and poolish pizza and what makes them unique.\nUnderstanding Biga And Poolish Pizza Dough Starters\nUnderstanding biga and poolish pizza dough starters starter is a mixture of flour, water, and yeast that aids in bread making. A biga is a firm starter that’s originated from italy and is fairly similar to sourdough. On the other hand, a poolish is a loose starter method originating from france.\nBoth sourdough starters help in the fermentation of the pizza dough, resulting in flavorful crusts. Unlike biga, poolish requires less time to ferment. Biga results in a firmer, more structured texture, whereas poolish produces a softer, more open crumb. When contrasting the two, it’s crucial to remember recipe proportions and time since they have different recipes and processes.\nBy using either biga or poolish, you can achieve excellent results in your pizza-making processes.\nThe Science Of Pizza Crust\nThe perfect pizza crust is a delicate balance of chemistry and physics. Using biga and poolish starters can affect the final texture and flavor. Gluten and yeast play important roles in creating the right consistency. The hydration level can also change the crust’s texture and flavor.\nIt’s essential to understand these elements to make the perfect crust. By experimenting with different ingredients and techniques, you can create a crust that’s unique to you. Whether you prefer a crispy or chewy crust, understanding the science behind it can help you achieve the perfect slice every time.\nBiga Vs. Poolish: Which One Delivers The Perfect Crust?\nBiga and poolish are popular starters used in pizza crusts. To achieve the perfect crust, texture, and flavor are key. Biga, with its longer fermentation, has more depth of flavor but requires more time. Poolish, on the other hand, produces a lighter crust with a shorter fermentation time.\nChoosing between the two depends on the pizza style and toppings you’re using. Thin and crispy crusts pair well with biga, while poolish is best for thicker crusts. Making the perfect biga or poolish crust at home requires attention to detail and the right technique.\nWith some tips, practice, and patience, you can achieve a restaurant-quality pizza crust at home.\nWhat Is Biga And Poolish In Pizza Making?\nBiga and poolish are pre-ferments used in pizza dough. They are a combination of water, flour, and yeast left to ferment for a specific time, generating an intense flavor in the final dough. The difference between the two is the hydration of the dough and the percentage of flour.\nWhat Is The Difference Between Biga And Poolish?\nBiga is prepared with a low hydration dough, around 50-60% and is left to rise overnight at room temperature, whereas poolish contains a higher hydration dough (around 70%) and is left to rise for a shorter time. Biga gives a slightly denser texture with a crispy crust, while poolish gives a more tender crumb.\nHow Do Biga And Poolish Affect Pizza’S Flavor?\nBiga and poolish affect the flavor of the pizza dough positively. The biga produces a rich, complex, and slightly sour flavor, whereas the poolish unleashes a sweeter flavor with a hint of yeast. They also help in gluten development, making the dough more extensible and elastic, allowing it to rise better during the baking process.\nCan You Make A Pizza Dough Using Both Biga And Poolish?\nYes, you can mix both of these pre-ferments together to produce an even more complex dough with a unique flavor profile. Many pizzerias use a mix of biga and poolish, and it is called “bigoli. ” It gives the pizza dough a distinct tangy flavor and an airy, irregular texture.\nCan You Substitute Biga Or Poolish With Instant Yeast In Pizza Making?\nWhile biga or poolish gives an authentic and unique taste, in case you don’t have much time to make these pre-ferments, you can use instant yeast instead. However, the taste may differ, and it would not give you the expected flavor profile of the biga or poolish dough.\nOverall, both biga and poolish are effective starter doughs in pizza making, each with their own unique benefits. Biga offers a longer fermentation process that results in a more complex flavor, while poolish provides a quicker method with a softer and lighter dough.\nUltimately, the choice between the two should be based on personal preference and the desired outcome of the pizza. Consider the type of pizza being made, ingredients, and desired texture and flavor. Experimenting with different recipes and techniques can lead to discovering the perfect combination that satisfies individual taste buds.\nAs with any craft, pizza making requires practice and experimentation, so don’t be afraid to try new things. Making a delicious pizza can be a fun and rewarding experience for anyone, whether using biga or poolish as the starter dough.']	['<urn:uuid:b1aece86-5784-4995-8f43-c0f3abf0980b>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	21	72	951
68	sheet metal brake uses applications	A sheet metal brake can be used for various applications including repairing gutters, auto restoration, heating and cooling repairs. It's particularly useful for racing enthusiasts who need to perform repairs on dirt bikes or cars. The tool is essential for any auto enthusiast or metal-working hobbyist who needs to bend metal into specific shapes or angles.	['Advice for Building Sheet Metal Brakes\nA sheet metal brake is an essential tool in the shop of any auto enthusiast or metal-working hobbyist. Anyone who has ever worked with sheet metal knows how difficult it can be to bend in just the right shape or at the right angle. This is where a sheet metal brake comes in handy. You can use a sheet metal brake to bend metal for a variety of jobs such as repairing gutters, auto restoration, heating and cooling reapirs, and a wide number of other tasks. Many racing enthusiasts need a sheet metal brake for repairs on their dirt bikes or cars.\nSheet metal brakes can be bought in almost any hardware store that deals in tools and shop fabrication equipment. But many metal workers have opted to make their own sheet metal brake. This way they get it to the exact size and specifications that they need. There are a couple of different ways to do it depending on the size that you want and the gauge of the metal you need to bend and shape. All sheet metal breaks, regardless of size, work on the same principles. There are levers that open the break’s clamp, which is a heavy flat piece of steel. You slide the piece of sheet metal inside the clamp and then pull the lever back to close the clamp, which holds the sheet metal in place. There will be another lever that is attached to the bending bar. You can move that lever until is starts to bend the sheet metal. When it gets to its desired shape or angle, you pull the lever to open the clamp and release the sheet metal.\nSo now you know how a sheet metal brake works but where can you learn how to build one. Luckily, you have a couple of options: books, the internet, and your local shop and hobby groups.\nSheet Metal Break Books\nThere are a couple of books out there that give you detailed instructions along with illustrations and photos on how to build a sheet metal brake. Since these books are specialty books, your best source of finding them is online.\n- The Ultimate Sheet Metal Fabrication Book – The Eastwood Company is a retailer in specialty shop tools for automobile restorations. They have several books available that cover metal fabrication and the building of custom tools such as a sheet metal brake. This book covers all areas of basic to advanced sheet metal fabrication as well as how to build a sheet metal brake.\n- Metal Fabricator’s Handbook – Another good instructional guide from the Eastwood Company. This book gives step-by-step approaches to sheet metal fabrication and is s good source for both the beginner and the advanced. Contains information on sheet metal brakes and how to use them.\n- Designing and Building a Sheet Metal Brake – This book deals with the specifics on how to build a sheet metal brake. Written and published by David J. Gingery, the book gives an easy to follow process with materials needed and requirements. You can pick this one up online at Amazon.com.\nSheet Metal Breaks Online\nThe internet is a great resource for any project. For sheet metal brakes, there are several websites that give instructions on how to build one yourself.\n- BD–4 – This is a homepage for experimental aircraft builders. Aircraft builders use sheet metal in all their modifications and designs. There is an article on the site with some instructions on how to build your own sheet metal brake. It includes a link on where to get the plans for the brake along with some tips and photos of the brake they built.\n- Van’s Air Force – Van’s Air Force is a site dedicated to a group of builders and hobbyists of RV kit aircraft. The site includes a section on building your own tools, which includes a PDF file with the plans on how to build a simple sheet metal brake.\n- Major League Duning – This is a website dedicated to sand dune racing. Enthusiasts of this sport are also well acquainted with fabricating their own sheet metal. There is a section on the site on how to build a brake. The article includes a list of needed supplies and a step-by-step process along with photos on how they built the brake. Great reference.\n- Casting Foundry& Lathe – This is a personal website with several projects centered around casting and different lathes. The author has a page on the sheet metal brake that he created following the instructions in the book Designing and Building a Sheet Metal Brake by David Gingery. There are some photos along with it to show the finished product.\n- Metal Web News – This is a site dedicated to offering online guidelines to metalworking. It is probably one of the best resource sites on the internet for metalworkers and hobbyists. They have discussion groups, classifieds, guides, and links to other metal sites. There is a section which includes instruction on how to build a sheet metal break.\n- Don’s DIY Den – Don’s DIY Den is a website where you can buy plans for building your own go-carts and ATV’s. They also offer a downloadable e-book that you can buy which gives instructions for a six-foot sheet metal brake. The e-book has simple instructions, photos, and you can either view it on your computer or print it out.\nLocal Shops and Hobby Groups\nIf you want some hands on learning and the opportunity to talk face to face with people who use a sheet metal brake, check out one of the local metal shops or autobody repair shops in your area. They may have a brake that they bought or one that they built but both types use the same principles. You can see first hand what a brake looks like and how it works. Try asking them how they would go about building one and what kind of materials they would recommend.\nLocal hobby groups are also a good place to talk shop with others with the same interest. Depending on where you live, a group may be hard to find. Try making contact with drivers at the nearest dirt track or race track. It is a guarantee that they have had to do metal work on their vehicles at some point or another.']	['<urn:uuid:5eff0829-240a-402d-803c-28d71ec9de72>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	5	56	1072
69	kitchen waste compost uses versus problems	Kitchen waste like fruit peels, teabags, and coffee grounds are excellent for composting as they are high in nitrogen and quite moist. However, they need careful management - they must be mixed with drier materials for air penetration and should be enclosed or buried 8 inches deep to avoid attracting pests. Some local governments even forbid open piles with food waste due to pest concerns.	"['The best way to grow plants is in natural compost. Many gardeners choose to make their own compost since it’s cheap and can be a very easy process. Composting is up to the individual. A gardener may decide to create an elaborate compost or a more simple recipe. Thus, the process of composting of is simply up to you.\nTo begin, garden composts typically consist of kitchen food waste and matter from the Earth. For example, usual compost materials are everyday items such as coffee grounds, eggshells, teabags, fruit and vegetable scraps and paper towels, bags and cardboard boxes – believe it or not. From the earth, a gardener can collect these composting materials: fall leaves, grass cuttings, old straw, old hay, tree clippings and weeds.\nMake sure not to use any meats, oils, fats or animal waste. Also, do not to use diseased or pesticide/herbicide-ridden plants or weeds that are in seed. Many gardeners now abstain from using animal materials such as manure from cows and horses. This is due to fear of disease i.e. mad cow disease.\nStick with the natural, meatless materials listed above and add soil. The soil should be finely textured, allowing the compost to become more porous. In other words, the soil will remain moist and become less likely to erode. Soils with clay or clay-loam structures are the best for a compost.\nMoisture is important to your compost because you want it to be damp and control the compost’s smell. At the same time, you don’t want it to be too wet because then the compost will decrease in temperature. If the compost isn’t moist enough, the ingredients will not decompose. In the next section, we will discuss how to obtain the correct balance of moisture.\nComposting Step by Step\nNow that you know what materials to use, we’ll go through the process of how to compost step by step. Don’t be intimidated. It may seems like a lot of instructions, but it’s actually quite simple and after a couple of times composting, you’ll get the hang of it. Here are the steps:\nBuy or make a compost bin. You don’t have to do this, but it’s good to keep the compost gathered in one place. The bin should have a hole in one side where you can throw in compost materials and turn the compost.\nMake sure you have enough green and brown waste from the Earth. Why? Green waste such as grass clippings or weeds add nitrogen. Brown waste such as brittle leaves can are high in carbon. This combination makes for an ideal composting situation.\nTip: Too many grasses, weeds and other green waste can make the compost smell horrible.\nSo now you’ve got your all your waste in the bin. It’s time to add in soil. Adding soil will help along the process of decomposition.\nHere’s where the moisture comes in. You have to check the moisture once a week. It should feel damp, but not too wet or too dry. If the compost is too wet, add more brown waste. If it is dry, simply add water. Remember, the moisture should be in balance for the compost to heat up and start decomposing.\nTurn the pile once a week to maintain airflow. The compost materials needs a supply of oxygen in order to properly decompose.\nIn a couple months, your compost should be ready. It should look like dark dirt and the materials should be fully decomposed. It should smell like dirt from the Earth and be moist. Another sure sign it is ready is it no longer heats up.\nNow you can use it in your organic garden.\nThe end result is always to have a usable compost for gardening. But there are other great reasons to compost. The US Environmental Protection Agency (EPA) lists composting as a top eco-friendly way to help the environment. The EPA reports that compost has been proven to suppress plant diseases and pests, and eliminate the need for fertilizers laced with chemicals. And the best part is composting is cheap, since it is used as a cover for landfills.\nBest of all, composting is simple. Using the aforementioned composting tips will help your garden plants grow healthy and strong. And you can feel great about saving money and helping the environment in the process.\n“How To Make Your Own Compost” – veggiegardener.com/category/composting/\nUS Environmental Protection Agency – epa.gov/wastes/conserve/rrr/composting/benefits.htm\nPhotos by Simon Howden / FreeDigitalPhotos.net – freedigitalphotos.net/images/view_photog.php?photogid=404', 'Actually, it\'s usually easier to\nleave grass clippings in the lawn,\nwhere they will decompose and benefit the soil directly. However,\nthey can be composted, too. Be cautious to add grass clippings in\nvery thin layers, or thoroughly mix them in with other compost\ningredients, as they otherwise tend to become slimy and matted\ndown, excluding air from the pile. Fresh grass clippings are high in\nnitrogen, making them a \'green\' compost ingredient.\nFarmers are often very happy to\nget rid of spoiled hay bales that\nhave been out in the rain, and will give them away or sell them at a\nlow price. Grass hay will probably contain a lot of seed, which can\nresprout in your garden. Alfalfa hay will compost very readily. The\ngreener the hay, the more nitrogen it contains. Be sure that any hay\nyou plan to compost is well-moistened prior to addition to the pile.\nFruit and vegetable peels/rinds,\ntea bags, coffee grounds,\neggshells, and similar materials are great stuff to compost. They\ntend to be high in nitrogen (this puts them in the \'greens\' category),\nand are usually quite soft and moist. As such, kitchen wastes need\nto be mixed in with drier/bulkier materials to allow complete air\npenetration. Many people compost their kitchen wastes in\nenclosed worm bins or bury them 8"" deep in the soil, to keep from\nattracting pests to an outdoor compost pile (check with your local\ngovernment to see if it has regulations about this -- some forbid\nopen piles containing food wastes because of the pest issue).\nAvoid composting meat scraps, fatty food wastes, milk products,\nand bones -- these materials are very attractive to pests.\nIf you live in an area where autumn\nleaves are still thrown away as\ngarbage, cash in on the bounty each year by acquiring your\nneighbors\' leaves! Generally, leaves are an excellent compost\ningredient. They can mat down and exclude air, though, so be sure\nthat any clumps are thoroughly broken up, or that the leaves are\nonly used in very thin layers. Ash and poplar/cottonwood leaves\ncan raise soil pH if used in compost -- this may not be beneficial if\nyour soil is already alkaline, as many soils are in the West\n(especially in semiarid and arid climates). Dead, dry leaves are in\nthe \'browns\' category, while living green leaves contain abundant\nnitrogen and are considered \'greens\'.\nHorse, cow, sheep, and poultry manures\nare often available for\nfree from local ranches, farms, and stables. They can burn plants if\napplied when fresh, so be sure they get well composted. Manures\ntypically contain quite a bit of nitrogen (the fresher the manure, the\nmore nitrogen it contains) and are considered a \'green\' ingredient.\nSome manures may contain weed seeds. Fresh manures can get a\ncompost pile to heat up quickly, and will accelerate the\ndecomposition of woody materials, autumn leaves, and other\nDry straw is a good material for\nhelping to keep a compost pile\naerated, because it tends to create lots of passageways for air to\nget into the pile. Be sure to wet the straw, as it is very slow to\ndecompose otherwise. Straw is definitely a \'brown\' and also\nrequires mixture with \'greens\' to break down quickly. Many\nstables use straw as a bedding material for horses -- straw that has\nundergone this treatment is mixed in with horse manure and breaks\ndown more quickly.\nWEEDS AND OTHER GARDEN WASTES\nMany types of weeds and old garden\nplants can be composted.\nAvoid weeds that have begun to go to seed, as seeds may survive\nall but the hottest compost piles. Some types of weeds are\n\'pernicious weeds\' and will resprout in the compost pile -- avoid\nusing these unless they are thoroughly dead. Green weeds are (you\nguessed it) a \'green\', while dead brown weeds are a \'brown\'.\nWOOD CHIPS AND SAWDUST\nWood products belong in the \'browns\'\ncategory, because they are\nfairly low in nitrogen. Some sawdusts, especially from\nbroadleaved/deciduous tress, will break down quickly in an active\ncompost pile. Others, especially from coniferous trees, will take\nlonger to decay. Stir sawdust thoroughly into the pile or use very\nthin layers. Coarse wood chips will very slowly decay, and are\nprobably better used as mulch unless you have lots of time to wait.\nBe sure not to compost chips or sawdust from any sort of\nchemically-treated wood -- you could be adding toxics like\narsenic to your pile if you do.\nWhat NOT to Compost\nDo not add the following items:\nWhether because of toxins, plant\nor human diseases, or weed\ntroubles, there are some things that shouldn\'t be put into compost\npiles. Avoid composting the following materials:\nCHEMICALLY-TREATED WOOD PRODUCTS\nSawdust is often available from\nconstructions sites, friends, or your\nown building projects. If you are considering composting sawdust,\nbe sure of the origin of the sawdust. Sawdust from\nchemically-treated wood products can be bad stuff to compost.\nFor example, take pressure-treated wood (sometimes called\nCCA), which usually has a greenish tint to it (I have also seen it in\nother colors). It contains arsenic, a highly toxic element, as well as\nchromium and copper. There is evidence to suggest that arsenic is\nleached into the soil from these products when they are used to\nmake compost bins or raised beds, so composting the sawdust\nwould certainly be a mistake. You may wish to read the \'Letters\'\nsection of Organic Gardening, April 1994 and July/August 1992,\nfor more information. Avoid other chemically-treated wood\nproducts and sawdust as well, such as wood treated with creosote\nor \'penta\' preservative.\nMany plant disease organisms are\nkilled by consistent hot\ncomposting, but it\'s difficult to make sure that every speck of the\ndiseased material gets fully composted. It\'s best not to compost\ndiseased plant material at all, to avoid reinfecting next year\'s\nHuman feces can contain disease\norganisms that will make people\nvery sick. Composting human feces safely requires that the\ncompost pile reach high (thermophilic) temperatures over a period\nof time. It isn\'t necessarily that difficult to reach these temperatures\nin a home compost pile, but the potential health costs of improper\ncomposting are high. Composting of human feces should not be\nattempted, except by experienced \'hot pile\' composters who are\nwell informed of the temperatures and times required to kill\npathogens, and who are willing to take 100% responsibility for the\nprocess and product. If you would like to learn more about\ncomposting humanure, I recommend The Humanure Handbook,\nlisted in the resources section of the Rot Web.\nMEAT, BONES, AND FATTY FOOD WASTES\nThese materials are very attractive\nto pests (in an urban setting,\nthis could mean rats...). In addition, fatty food wastes can be very\nslow to break down, because the fat can exclude the air that\ncomposting microbes need to do their work.\nMorning glory/bindweed, sheep sorrel,\nivy, several kinds of\ngrasses, and some other plants can resprout from their roots\nand/or stems in the compost pile. Just when you thought you had\nthem all chopped up, you\'d actually helped them to multiply! Don\'t\ncompost these weeds unless they are completely dead and dry\n(you may want to leave them in a sunny place for a couple of\nweeks before composting). Remember also that composting\nweeds that have gone to seed will create weeds in next year\'s\ngarden, unless a very hot pile temperature can be maintained to kill\nDog and cat feces may carry diseases\nthat can infect humans. It is\nbest NEVER to use them in compost piles. Some people do bury\nthem 8"" deep in the soil, but ONLY in areas where food crops are\n<Last Page Front Page Alaphabetical Listing Of Materials>']"	['<urn:uuid:d03eea60-7cbd-495c-9205-99bb56b24498>', '<urn:uuid:06989984-5d91-4218-b587-7375f0919f8b>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T17:51:21.175311	6	65	2027
70	Could you tell me some of the birds that are protected under U.S. law from being hunted or captured?	The Migratory Bird Treaty protects several species including the Bald eagle (Haliaeetus leucocephalus), Black-capped chickadee (Parus atricapillus), Common night hawk (Chordeiles minor), Red tailed hawk (Buteo jamaicensis), Barn owl (Tyto alba), and Snowy egret (Egretta thula).	"['Transmission Migratory Birds\nPresent, Potentially Affected\n- Antelope Valley Neset (AVS - Neset 345 kV Transmission Line Project)\n- BLM-NV-WN-ES-08-01-1310, NV-020-08-01 (Blue Mountain Geothermal Well Field and Power Plant EA)\n- Barren Ridge Renewable Transmission (Barren Ridge Renewable Transmission Project Environmental Impact Statement)\n- Boardman to Hemingway (Environmental Impact Statement and Land Use Plan Amendments for the Boardman to Hemingway Transmission Line Project)\n- Cameron to Milford-138kV Transmission Line (Cameron to Milford - 138kV Transmission Line Project)\n- DOE-EA-1759 (EA for Geothermal/Exploration at Southwest Alaska Regional Geothermal Energy Project Naknek, Alaska)\n- DOE-EA-1849 (EA for Northern Nevada Geothermal Power Plant Project at McGuinness Hills Geothermal Area)\n- DOI-BLM-CA-EA-2002-??? (Glass Mountain Exploration Environmental Assessment/Initial Study for Geothermal/Well Field)\n- DOI-BLM-NV-B020-2011-0026-EA (Clayton Valley Geothermal Exploration Project EA for Drilling and Well Testing)\n- DOI-BLM-NV-C010-2010-0006-EA (Gabbs Valley and Dead Horse Wells Geothermal Exploration Projects EA for Geothermal/Exploration)\n- DOI-BLM-NV-C010-2010-0008-EA (EA at Soda Lake Geothermal Area for Geothermal/Exploration Drilling and Well Testing)\n- DOI-BLM-NV-C010-2010-0010-EA (EA at Coyote Canyon and Dixie Meadows for Geothermal/Exploration Drilling and Well Testing)\n- DOI-BLM-NV-C010-2010-0016-EA (EA for Airborne Electromagnetic Survey at Patua Geothermal Project for Geothermal/Well Field, Geothermal/Power Plant)\n- DOI-BLM-NV-C010-2011-0001-EA (Coyote Canyon Utilization Project EA for TGP for Geothermal/Power Plant)\n- DOI-BLM-NV-C010-2011-0516-EA (EA for Thermal Gradient Holes at Dixie Meadows Geothermal Exploration Project for Geothermal/Exploration, Geothermal/Well Field)\n- DOI-BLM-NV-C010-2012-0029-EA (Tungsten Mountain Geothermal Exploration Project EA for Geothermal/Well Field Drilling and Well Testing)\n- DOI-BLM-NV-C010-2012-0050-EA (Wild Rose Geothermal Project EA for Geothermal/Well Field, Geothermal/Power Plant)\n- DOI-BLM-NV-C010-2012-0051-EA (Coyote Canyon South Geothermal Exploration Project EA for Geothermal/Exploration)\n- DOI-BLM-NV-CC-ES-11-10-1793 (Salt Wells Geothermal Energy Projects EIS for Geothermal/Power Plant Development Drilling)\n- DOI-BLM-NV-CO10-2011-0501-EA (EA for Phase II Production Wells and Thermal Gradient Holes at Patua Geothermal Project for Geothermal/Well Field)\n- DOI-BLM-NV-W010-2010-0004-EA (New York Canyon Geothermal Exploration Project EA for Exploration Drilling and Well Testing)\n- DOI-BLM-NV-W010-2011-0001-EA (EA of the Leach Hot Springs Geothermal Exploration Project at Grass Valley Geothermal Area for Geothermal/Exploration, Geothermal/Well Field Drilling and Well Testing)\n- DOI-BLM-NV-W010–2012–0005–EA (EA for Development Drilling at New York Canyon Geothermal Utilization and Interconnect Project for Geothermal/Power Plant, Geothermal/Transmission, Geothermal/Well Field)\n- DOI-BLM-NV-W030-2010-0006-EA (EA for Drilling and Flow Testing at San Emidio Geothermal Exploration Project for Geothermal/Exploration)\n- DOI-BLM-OR-P000-2011-0003-EA (EA for Newberry Volcano Enhanced Geothermal System (EGS) Demonstration Project for Geothermal/Well Field)\n- DOI-BLM-OR-P040-0021-EA (EA for Exploratory Wells at Midnight Point and Mahogany Geothermal Exploration Projects, Glass Buttes, Oregon for Geothermal/Exploration)\n- DOI-BLM-OR-V040-2011-0008-EA (EA for Neal Hot Springs Geothermal Well Construction Right of Way, Phase 2 for Geothermal/Well Field)\n- DOI-BLM-UT-C010-2010-0042-EA (EA of Cove Fort/Sulphurdale Geothermal Utilization Plan for Geothermal/Power Plant)\n- DOI-BLM-UT-W020-2010-0042-EA (EA for Thermal Gradient Holes at Drum Mountains and Whirlwind Valley Geothermal Exploration Projects for Geothermal/Exploration, Geothermal/Well Field)\n- DOI-BLM-UT-W020-2010-042-EA (EA for Exploration and Development Drilling at Drum Mountain Geothermal Area for Geothermal/Exploration)\n- EA for Well Field Development at Patua Geothermal Area - DOI-BLM-NV-C010-2011-00016-EA (EA for Phase II of Patua Geothermal Project for Geothermal/Exploration, Geothermal/Well Field)\n- Eldorado Ivanpah Transmission Project (EIS/EIR for the Eldorado-Ivanapah Transmission Project)\n- Gateway West Transmission Line (Environmental Impact Statement for the Gateway West Transmission Line Project)\n- LLNV-WO1000-2009-0002-EA (EA for Observation Wells at Jersey Valley II Geothermal Exploration Project)\n- NV-020-03-26 (Desert Peak 2 Geothermal Project Environmental Assessment for Geothermal/Power Plant)\n- NV-020-07-EA-01 (EA for Observation Wells at Jersey Valley Geothermal Exploration Project for Geothermal/Well Field, Geothermal/Exploration)\n- NV-063-EA06-098 (Reese River Valley Geothermal Exploration Project Environmental Assessment)\n- One Nevada (Final Environmental Impact Statement for the One Nevada Transmission Line Project (ON Line Project))\n- San Juan Basin EC (San Juan Basin Energy Connect Project Environmental Impact Statement)\n- Sigurd Red Butte No2 (Sigurd to Red Butte No. 2 345kV Transmission Project)\n- Sun Valley to Morgan Transmission Line (Environmental Impact Statement for the Sun Valley to Morgan Transmission Line Project)\n- Sunzia Southwest (SunZia Southwest Transmission Project)\n- Tehachapi Renewable Transmission (Environmental Impact Statement for the Tehachapi Renewable Transmission Project)\n- Transwest Express (Transwest Express Transmission Project Environmental Impact Statement)\nThe Migratory Bird Treaty of 1918 provides regulations to protect birds from human impacts. It prohibits activities that: “Pursue, hunt, take, capture, kill, attempt to take, capture or kill, possess, offer for sale, sell, offer to purchase, purchase, deliver for shipment, ship, cause to be shipped, deliver for transportation, transport, cause to be transported, carry, or cause to be carried by any means whatever, receive for shipment, transportation or carriage, or export, at any time, or in any manner, any migratory bird, included in the terms of this Convention . . . for the protection of migratory birds . . . or any part, nest, or egg of any such bird."" (16 U.S.C. 703). This treaty applies to the United States of America and its territories.\nExamples of listed birds are the following:\n- Bald eagle (Haliaeetus leucocephalus)\n- Black-capped chickadee (Parus atricapillus)\n- Common night hawk (Chordeiles minor)\n- Red tailed hawk (Buteo jamaicensis)\n- Barn owl (Tyto alba)\n- Snowy egret (Egretta thula)\nMigratory Birds Impacts & Mitigation\nInfrastructure such as transmission line towers, power lines, ancillary facilities, and substations harm migratory birds, as they are not accustomed to coexisting with high voltage and pathway obstruction. The nesting season lasts from May 1st-September 15th, activating special provisions to protect birds. If construction activities occur during these months, young bird development becomes threatened.\nThe following mitigation measures can decrease the impacts to birds and transmission line facilities:\n- Provide training programs for employees to learn bird migration patterns, area specific species, essential habitats, and laws protecting birds. Develop an Avian Protection Plan to take proactive measures against bird electrocution and injury.\n- To mitigate avian collisions with transmission lines, mark shield wires, guy wires, and overhead optical ground wires with flight diverters. Conduct similar processes in areas where the line crosses, or is adjacent to water.\n- Close off any gaps or narrow open hollow spaces in the proposed facilities or structures capable of trapping cavity-nesting birds. Fill or cap any open-ended posts to discourage new nests during construction.\n- Cover phase conductors with manufactured covers, add perch discouragers on cross arms and on top of poles and space each conductor farther than the minimum distance required to mitigate avian electrocution. Furthermore, use longer horizontal insulators, suspend phase conductors on top of poles and cross arms, install horizontal jumper support to increase the phase-to-ground separation, replace tension members with fiberglass or non-conducting materials, cover tension members with dielectric material, use fiberglass poles or switches, and install standard nest discouragers.\n- Hire a biologist to conduct pre-construction surveys that determine active bird nest status (i.e., containing eggs or young, or a mated pair is observed exhibiting territorial defense, carrying nesting material, and/or transporting food). In some situations, biologists allow vegetation clearing near active bird nests, however noise and physical barriers are mitigation measures to further protect the young birds until they have either “fledged or failed”. In other situations, construction restraints, area closures and site relocation may be suggested to protect the bird habitat.\n- Typical surveys occur within 500 feet of tower sites, laydown/staging areas, substation sites, access/spur road locations, or any other area subject to ground disturbance. If no nests are found, proceed with construction.']"	['<urn:uuid:9f743809-89f9-4268-b2f7-600183273436>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	19	36	1195
71	mitre security nomenclature projects purpose and practical implementation in code quality	Mitre's nomenclature projects provide a common language for describing security threats and defenses, including the National Vulnerability Database, Common Vulnerability Scoring System, and Common Weakness Enumeration. These standardized systems help in calculating vulnerability scores and identifying security issues. In practical implementation, these standards guide development teams in writing better code by identifying common problems that lead to security vulnerabilities. Organizations can use these standards alongside practices like static analysis, regular code reviews, and comprehensive testing strategies including stress testing, destructive testing, and fuzz testing to improve code quality and prevent security issues.	"['""RFC"" = ""Request For Comment"". These documents define networking protocols and frequently discuss security issues. To access them, see the current complete list.\n|Learn the language:|\n||A Glossary of Networking Terms|\n||Internet Users\' Glossary|\n||Internet Security Glossary|\n|Understand and carry out ""best practice"":|\n||Site Security Handbook|\n||Expectations for Computer Security Incident Response|\n||Users\' Security Handbook|\n||Security Mechanisms for the Internet|\n||Security Architecture for the Internet Protocol|\n||Current Operational Security Practices in Internet Service Provider Environments|\nMitre Nomenclature Projects\nThe U.S. government has contracted Mitre to define information nomenclature. Researchers, the IT industry, the anti-virus industry, and more need to have a common language to describe threats, defenses, and more. I was teaching a UNIX security course in the Washington DC area when these nomenclature projects came up. A student who worked for a U.S. Government agency said, ""Oh, that sounds like such a Mitre project!"", meaning that it was complicated, performed for the U.S. Government in return for vast sums of money, and was just the organization of actual work done by others. But these projects are useful to give the information security community a more useful common language.\nNational Vulnerability Database\nTies together many of these nomenclature projects, plus attempts to automate (or at least standardize) systems for calculating vulnerability scores.\nCommon Vulnerability Scoring System\nAttempts to give you numbers so you can say, hopefully with some quantitative or at least meaningful support, ""This thing is more secure than that thing."" The CVSS refers to many of the below enumeration projects: CWE, CVE, and so on.\nCommon Weakness Enumeration\nDictionary of software weakness types — crucial for understanding all the other lists! For example:\nAbsolute Path Traversal\nCWE ID 36\nDescription The software can construct a path that contains absolute path sequences such as ""/path/here.""\nApplicable Platforms: C C++ Java .NET\nCommon Vulnerabilities and Exposures\nDictionary of publicly known information security vulnerabilities and exposures. What is the possible problem — what is the real threat, what are various researchers and companies calling it, and where can you learn more? For example:\nDescription: Stack-based buffer overflow in Supervisor Report Center in SL Mail Pro 2.0.9 and earlier allows remote attackers to execute arbitrary code via an HTTP request with a long HTTP sub-version. Status: Entry\nReference: BUGTRAQ:20040305 SLMail Pro Supervisor Report Center Buffer Overflow (#NISR05022004a)\nCommon Platform Enumeration\nStandard identifiers and dictionary for platform and product naming, all in XML like many of the Mitre data sets. For example:\n<title xml:lang=""en-us"">Red Hat Enterprise Linux (v.5 server)</title>\nCommon Configuration Enumeration\nNow that you know which OS you\'re dealing with (according to CPE), what are the specific configuration details that you will be told to adjust? Unique identifiers for common system configuration issues, and suggested configuration guidelines.\nCommon Malware Enumeration\nA single consistent label for use in security advisories and discusstion of attack software. For example:\nCME ID: CME-416\nDescription: CME-416 is a multi-component mass-mailing worm that downloads and executes files from the Internet.\nAliases applied by anti-virus industry:\nTrend Micro: WORM_STRAT.DR\nCommon Attack Pattern Enumeration and Classification\nCommunity-developed dictionary of attack methodologies. Useful for software development, and possibly for configuration design. Also useful for really understanding terminology.\nOpen Vulnerability and Assessment Language\nXML schema for representing system information, system configuration, and reporting the result of testing for known vulnerabilities based on software version and configuration.', 'First, stop writing lousy code\nIt’s unfortunate that few developers are familiar with The MITRE Corporation’s Common Weakness Enumeration list of common software problems. The CWE is a fascinating and valuable resource, not just to the software security community, but to the broader development community. Reading through the CWE, it is disappointing to see how many common problems in software, problems that lead to serious security vulnerabilities and other serious problems, are caused by sloppy coding: not missing the requirements, not getting the design wrong or messing up an interface, but simple, fundamental, stupid construction errors. The CWE is full of mistakes like: null pointers, missing initialization, resource leaks, string handling mistakes, arithmetic errors, bounds violations, bad error handling, leaving debugging code enabled, and language-specific and framework-specific errors and bad practices – not understanding, improperly using the frameworks and APIs. OK there are some more subtle problems too, especially concurrency problems, although we should reasonably expect developers by now to understand and follow the rules of multi-threading to avoid race conditions and deadlocks.\nThe solution to this class of problems are simple, although they require discipline:\n- Hire good developers and give them enough time to do a good job, including time to review and refactor.\n- Make sure the development team has training on the basics, that they understand the language and frameworks.\n- Regular code reviews (or pair programming, if you’re into it) for correctness and safety.\n- Use static analysis tools to find common coding mistakes and bug patterns.\nDesign for failure\nFailures will happen: make sure that your design anticipates and handles failures. Identify failures, contain, retry, recover, restart. Contain failures, ensure that failures don’t cascade. Fail safe. Look for the simplest HA design alternative: do you need enterprise-wide clustering or virtual synchrony-based messaging, or can you rely on simpler active/standby shadowing with fast failover?\nUse design reviews to hunt down potential failures and look for ways to reduce the risk, prevent failure, or recover. Microsoft’s The Practical Guide to Defect Prevention, while academic at times, includes a good overview of Failure Modes and Effects Analysis (FMEA), a structured design review and risk discovery method similar to security threat modeling, focused on identifying potential causes of failures, then designing them out of the solution, or reducing their risk (impact or probability).\nKeep it Simple\nAttack complexity: where possible, apply Occam’s Razor, and choose the simplest path in design or construction or implementation. Simplify your technology stack, collapse the stack, minimize the number of layers and servers.\nUse static analysis to measure code complexity (cyclomatic complexity or others) and trending: is the code getting more or less complex over time. There is a correlation between complexity and quality (and security) problems. Identify code that is over-complex, look for ways to simplify it, and in the short term increase test coverage.\nTest… test… test….\nTesting for reliability goes beyond unit testing, functional and regression testing, integration, usability and UAT. You need to test everything you can every way you can think of or can afford to.\nA key idea behind Software Reliability Engineering (SRE) is to identify the most important and most used scenarios for a product, and to test the system the way it is going to be used, as close as possible to real-life conditions: scale, configuration, data, workload and use patterns. This gives you a better chance of finding and fixing real problems.\nOne of the best investments that we made was building a reference test environment, as big as, and as close to the production deployment configuration, as we could afford. This allowed us to do representative system testing with production or production-like workloads, as well as variable load and stress testing, operations simulations and trials.\nStress testing is especially important: identifying the real performance limits of the system, pushing the system to, and beyond, design limits, looking for bottlenecks and saturation points, concurrency problems – race conditions and deadlocks – and observing failure of the system under load. Watching the system melt down under extreme load can give you insight into architecture, design and implementation weaknesses.\nOther types of testing that are critical in building reliable software:\n- Regression testing – relying especially on strong automated testing safety nets to ensure that changes can be made safely.\n- Multi-user simulations – unstructured, or loosely structured group exploratory testing sessions.\n- Failure handing and failover testing – creating controlled failure conditions and checking that failure detection and failure handling mechanisms work correctly.\n- Soak testing (testing standard workloads for extended periods of time) and accelerated testing (playing at x times real-life load conditions) to see what breaks, what changes, and what leaks.\n- Destructive testing – take the attacker’s perspective, purposefully set out to attack the system and cause exceptions and failures. Learn How to Break Software.\n- Fuzz testing: simple, brute force automated attacks on interfaces, a testing technique that is valuable for reliability and security. Read Jonathan Kohl’s post on fuzz testing.\nGet in the trenches with ops\nGet the development team, especially your senior technical leaders, working closely with operations staff: understanding operations’ challenges, the risks that they face, the steps that they have to go through to get their jobs done. What information do they need to troubleshoot, to investigate problems? Are the error messages clear, are you logging enough useful information? How easy is it to startup, shutdown, recover and restart – the more steps, the more problems. Make it hard for operations to make mistakes: add checks and balances. Run through deployment, configuration and upgrades together: what seems straightforward in development may have problems in the real world.\nBuild in health checks – simple ways to determine that the system is in a healthy, consistent state, to be used before startup, after recovery / restart, after an upgrade. Make sure operations has visibility into system state, instrumentation, logs, alerts – make sure ops know what is going on and why.\nWhen you encounter a failure in production, work together with the operations team to complete a Root Cause Analysis, a structured investigation where the team searches for direct and contributing factors to the failure, defines corrective and preventative actions. Dig deep, look past immediate causes, keep asking why. Ask: how did this get past your checks and reviews and testing? What needs to be changed in the product? In the way that it is developed? In the way that is implemented? Operated?\nAnd ensure that you followup on your corrective action plan. A properly managed RCA is a powerful tool for organizational learning and improvement: it forces you to think, to work together, creates a sense of accountability and transparency.\nChange is bad…. but change is good\nYou don’t need to become an expert in ITIL, but if you have anything to do with developing or supporting enterprise software, at least spend a day reading Visible Ops. This brief overview of IT operations management explains how to get control over your operations environment. The key messages are:\nPoor change management is the single leading cause of failures: 80% of IT system outages are caused by bad changes by operations staff or developers. 80% of recovery time (MTTR) is spent determining what changed.\nThe corollary: control over change not only improves reliability, it makes the system cheaper to operate, and more secure.\nChange can be good: as long as changes are incremental, controlled, carefully managed and supported by good tools and practices. When the scope of change is contained, it is easier to get your head around, review and test. And with frequent change, everyone knows the drill – the team understands the problems and is better prepared if any problems come up.\nImplement change control and release management practices. Include backout planning, rollback planning and testing. Taking compatibility into account in your design and implementation. Create checklists, reviews.\nReliable software, like secure software, doesn’t come for free, especially up front, when you need to effect changes, put in more controls. You must have management, and customer, support. You need to change the team’s way of thinking: to use risk management to drive priorities, shape design and implementation and planning. Get your best people to understand and commit: the rest will follow.\nKeep in mind of course that there are limits, that tradeoffs need to be made: most of us are not building software for the space shuttle. In Software Quality at Top Speed, Steve McConnell shows that development teams that build better quality, more reliable software actually deliver faster, up to a peak efficiency of 95% defects removed before production release. However, you reach a point of rapidly diminishing returns as you approach the end of the curve, attempting to hit 100% defect-free software, where costs and schedule increase significantly.\nTimeboxing is an effective technique to contain scope and cost: do as much as you can, as good as you can, within a hard time limit. Following Japanese manufacturing principles, make sure that anyone on the team can pull the cord and postpone a release or cancel a feature because it is unstable.\nIt is sobering, almost frightening, how easy it is, how natural it is, for developers and managers to short-change quality practices, to place feature delivery ahead of reliability, especially under pressure. Ensure that you build support across the organization, build a culture that puts reliability first. Like any change, it will require patience, commitment, and unrelenting followup.\n- Why Automated Tests Boost Your Development Speed\n- Code quality matters to the customers. A lot\n- Agile software development recommendations for users and new adopters\n- Using FindBugs to produce substantially less buggy code\n- Save money from Agile Development\n- 9 Tips on Surviving the Wild West Development Process\n- Things Every Programmer Should Know\n- Not doing Code Reviews? What’s your excuse?']"	['<urn:uuid:8ba262c2-d356-4fa1-bbd9-d27fa533cd95>', '<urn:uuid:efcd81a3-91bc-4703-a5ec-06f4e39819bb>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	11	92	2192
72	how homeownership affects rural urban differences education housing	Homeownership has distinct impacts on education and housing patterns in rural and urban areas. In urban situations, as exemplified by Jose Tobar's case, having a home dramatically improved educational outcomes - he went from a 1-point GPA to 3.6 after getting his own room to study. Regarding housing patterns, there's a notable difference between rural and urban areas: rural areas have significantly higher homeownership rates at 81% compared to 60% in urban areas. Rural residents tend to be older and more likely to age in place, while younger adults are more likely to rent and live in urban areas. Both areas face affordable housing challenges, with only 46 units available nationwide for every 100 extremely low-income renter households, though the situation is slightly worse in cities with only 42 units per 100 low-income renters.	"['The ripple effect of change -- Habitat for Humanity Int\'l 1\nThe ripple effect of change\nJose Tobar was 15 when Jimmy & Rosalynn Carter Work Project volunteers built his family’s home and 20 others in the Watts community of Los Angeles\nin 1995. Now 32, he owns his own home and has embarked on a career to help others in need of affordable housing. Photo courtesy of the Tobar family\nA house — and a little presidential advice — begins a chain of transformation\nBy Soyia Ellison\nAt 15, Tobar could already see the sense of pride that homeownership instilled in his parents. Photo courtesy of the Tobar family\nOn a sunny afternoon in June 1995, a trembling 15-year-old stepped up to a podium to address a crowd that included former President Jimmy Carter and his wife, Rosalynn.\nJose Tobar — “Little Jose,” as he was known then — had been given some advice just before he left his seat: “Speak from the heart.” So he put away his notes and began to talk. He doesn’t remember exactly what he said; he knows only that he tried to express his thanks to the volunteers who had built 21 homes in the Watts neighborhood of Los Angeles, California, during the Jimmy & Rosalynn Carter Work Project.\nOne of those homes would be his family’s.\n“I started thinking about what my life was before Habitat, and how it was going to change … and I started to cry,” he remembers. “And so did, like, hundreds of other people.”\nThough his own words are lost to him, he will never forget the ones President Carter spoke to him after he left the stage: ""It is kids like you that make Rosalynn and I proud to do what we do. You give us reason to continue. I’m proud of you, son.""\n‘Your mind starts to change’\n“Little Jose” is now 32. He owns his own home and has embarked on a career devoted to helping others in need of affordable housing.\nIt’s a life entirely different from the one that seemed to be his destiny.\nFor years, he and his family lived in cramped apartments. At one point, he shared a bedroom with his brother, Walter, and sister, Liseth, while his parents slept on a pullout couch in the living room.\nStudying in these conditions was nearly impossible.\nTobar finished his freshman year of high school with a G.P.A. of “1-point-something.”\n“I didn’t know how I was going to get through school,” he recalls. “I didn’t think I could make anything out of myself.”\nBut then the family moved into their new Habitat home, and something changed.\nHis grades began to improve, and he took on leadership roles at school. He became president of the history club, a member of JROTC, student body treasurer. By the time he graduated, he had turned that 1-point-something into a 3.6 and had acceptance letters from three colleges.\n“I had my own room,” he explains. “I could really study in my room.”\nMore than that, though, he had a new outlook on life.\n“Coming from poverty, people have this mentality that, ‘Hey, I’m stuck here for the rest of my life,’ ” he says. “But when you are given a hand up, when you are given an opportunity, your mind starts to change.”\nHe held on to that new outlook even as life dealt him disappointments.\nFrom banking to books, and back again\nTobar did well in his freshman year at California State University Northridge, balancing classes and a job at Washington Mutual Bank. But during his sophomore year, he developed a degenerative eye condition that left him legally blind. No longer able to read his textbooks, he dropped out of school.\nIt was a difficult time.\nTo keep his depression at bay, Tobar used his savings to buy a little bookstore called Mana. That might seem like a strange choice for someone who could no longer read, but, he says, it wasn’t much of a bookstore anyway.\n“It wasn’t a business; it was more of a community builder.”\nPeople came in to sit and read for a while or just to talk to him about their problems. He gave inexpensive music lessons to their children.\n“It was very good for me because I was able to connect with them and forget about my troubles,” he says. “It helped me not to be too depressed.”\nThe store eventually went under, but by then, he had received a cornea transplant that restored his sight. He was able to go back to work full-time in the banking industry.', 'This is the fourth post in a series about reducing the divide between urban and rural communities.\nTo bridge the gap between urban and rural areas, we first need to understand what sets these communities apart and where they have common ground. Below are a few key stats about urban and rural populations, economies and housing, and the different types of critical challenges these communities face.\nDespite the vast amount of discussion about the urban-rural divide, there is actually little agreement about what these terms mean. The most common definitions are from the Census Bureau — “mostly urban,” “mostly rural” or “completely rural” — and the Office of Management and Budget (OMB) — “metropolitan” or “non-metropolitan.”\nThe problem with these definitions is that more than half of the country’s rural population, as defined by the Census, lives in less densely populated parts of OMB metropolitan areas.\nNo matter how you slice it though, the clear majority of Americans live in urban areas. This share continues to grow as people move from rural to urban regions. Those who live in rural communities tend to be older (with an average age of 51 years, versus 46 in urban communities) and less educated (20 percent with a bachelor’s degree versus 29 percent in urban communities).\nRural and urban unemployment rates have improved in recent years, but labor force participation — the share of the population working or seeking employment — remains below pre-recession levels. Specifically, the gap between urban (63 percent) and rural (59 percent) labor force participation is significant and largely attributed to an aging rural workforce. Additionally, most rural communities still have not recovered the jobs they lost during the recession.\nHowever, not all rural communities are the same, and some are outpacing the growth of urban areas on key economic indicators. For example, many rural areas have higher rates of entrepreneurship, and the National League of Cities’ (NLC) own research found that businesses that export their goods and services are thriving in rural communities. Rural areas in many states are also making outsized contributions to their states’ GDP.\nDespite their differences, affordable housing is a prevalent concern amongst both urban and rural communities. Across the U.S., only 46 units are available for every 100 extremely low-income renter households. The problem is more severe in cities, which typically have only 42 units per 100 low income renters. Rural area housing challenges are compounded by the fact that residents typically have lower median incomes and available affordable housing is often poor quality.\nRural residents have higher homeownership rates (81 percent) than urban residents (60 percent). Younger adults are both more likely to rent and more likely to live in urban areas. Rural areas have an older demographic, whom are both more likely to own their homes and age in place. Lack of mobility within rural housing markets contributes to an overall housing shortage in these communities, limiting business expansion and attraction opportunities.\nOver the years, several other differences have become prominent between urban areas and their rural counterparts:\nIn a world dependent on online communications, broadband access remains a challenge in rural areas. In all states, broadband access is higher in urban areas than in rural ones. While 63 percent of rural Americans say they have a broadband internet connection at home, increased from 35 percent in 2007, there are still many challenges to improving accessibility.\nThe cost of providing services is the most significant hurdle. Even where broadband is available, it is often prohibitively expensive, leading to gaps not only in access, but also in adoption. Lack of internet has widespread consequences, particularly for rural economic and educational opportunities.\nHealth and Opioids\nThere were more than 33,000 opioid-induced deaths in 2015, a fourfold increase from 2000. Although overall opioid mortality rates are higher in urban counties, mortality rates in rural areas have increased more quickly across all regions over the last two decades. Between 1999 and 2016, opioid death rates in rural areas have quadrupled among the 18 to 25 age group, and tripled for females. To treat and prevent opioid addiction, various healthcare services are required. Unfortunately, resources are more limited in rural areas.\nThe lack of improved access to healthcare services also has significant impacts on life expectancy. On average, from 2005-2009, the life expectancy in rural areas was 76.7, compared with 79.1 for urban dwellers, a gap that has widened significantly over the past 50 years. People living in rural areas are also more likely to die from the five leading causes of death, including heart disease, cancer and stroke, than their urban counterparts. In 2014, approximately 71,000 deaths among rural residents were potentially preventable.\nDespite the wide gulf between urban and rural communities, there are a number of challenges they share, from affordable housing to opioid addiction. These common challenges provide opportunities for shared solutions.\nAbout the Author: Rose Kim is a research associate in NLC’s Center for City Solutions.']"	['<urn:uuid:950389b7-1515-4041-b2f2-c17922666797>', '<urn:uuid:8185aa9c-c26b-44ec-99aa-ee0ffaa16ac2>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T17:51:21.175311	8	134	1592
73	How can you check if photo paper contains acids?	A pH Testing Pen can be used to verify if paper is acid-free. When drawing a small line on the material, the chlorophenol red indicator solution will turn purple on any substrate with a pH of 6.8 or above, which is desirable. A clear or yellow color indicates the material is unsuitable for conservation purposes. A pH value of 7.0 or greater is ideal as it ensures sufficient alkaline reserve to combat local and atmospheric pollutants.	"['Collectors Guide For\nArchival Pigment Ink Fine Art Prints\nArchival pigment ink fine art prints are the latest evolution of archival photographic artwork and in this guide, I cover the key concepts that every collector should understand when purchasing archival pigment ink fine art prints as well as my personal standards and recommendations.\nMy goal is to make this guide clear and free of technical jargon and focused on making sure you are aware of the most important archival concepts and why they matter.\nIf you are like most art buyers, you purchase art because you love it or it means something to you. Because you enjoy and value your new art, you want to make sure you will be able to enjoy it for the rest of your life.\nI provide important information in this guide to help you make an informed decision when buying archival quality pigment ink fine art prints and I provide important details about conditions at your home or office that can impact the longevity of your new artwork.\nKEY INFORMATION FOR MY ART BUYERS & COLLECTORS\nThere are two key factors concerning image stability and archival permanence of fine art prints on any type of substrate: certified archival materials, and display conditions.\nCERTIFIED ARCHIVAL MATERIALS\nI use the world\'s finest 100% certified archival materials to create my artwork and I follow the archival standards for producing museum quality artwork.\nThe second and one of the largest influencers of how long any type of photographic art will last are the storage and display conditions (i.e., temperature, humidity, light and handling), and that is why I include a section on this information along with my recommendations in a section below.\nFor maximum permanence, I suggest having a professional frame shop dry mount your archival pigment ink fine art print following the proper heating and pressure guidelines of the mounting tissue. Long-term tests indicate dry-mounted prints fare better than prints that are hinge or corner mounted because the dry mount tissue acts as a barrier to pollutants that can be absorbed by the mounting board and then transferred to the print. Finally, by using an Acrylite Gallery UV Filtering OP-3 acrylic sheet as your glass, you have used all of the proper materials to protect your artwork from damaging UV light and enjoy it for a lifetime. Be sure to review my Display & Care Guidelines section below for additional details.\nTOOLS, MATERIALS, & WORKFLOW\nI use a state of the art Epson SureColor P9570 large format fine art archival printer that uses Epson\'s flagship UltraChrome PRO12 archival pigment inks. By using this system along with certified archival papers, materials, and an archival workflow, I produce museum quality archival artwork that I guarantee for life. I personally create each print myself and no part of my workflow is outsourced to a third party so you can be assured that every detail is carefully monitored and executed with precision for you.\nAccording to Henry Wilhelm, director of research at Wilhelm Imaging Research, which is the world\'s leading independent print permanence testing laboratory, Epson\'s SureColor P7570 (24"") and P9570 (44"") printers using the Epson 12 color UltraChrome PRO12 archival pigment inks are capable of producing museum quality archival prints lasting 200+ years for color and 400+ years for black and white. The projections are based on the quality of the archival materials used and display/storage conditions.\nTHE BOTTOM LINE FOR MY ART BUYERS\nWhen you buy one of my limited edition archival pigment fine art ink prints and want to hang it on your wall in your home or office, I guarantee the artwork to look like the day your purchased it for the rest of your life so long as you follow my handling and care guidelines.\nI take every known and reasonable care during the artwork creation process to ensure the long-term archival performance of my artwork. Once the art leaves my studio, the environment in which it is displayed and/or stored becomes the more important determinant of the artwork permanence.\nWe know based on thousands of years of art history that acid is not only invisible, but over time it can damage and ruin your artwork. This is why it is critical that the artist uses 100% certified archival materials and follows an archival workflow.\nArchival materials include the acid-free paper, mounting board, matte, and even the frame and type of glazing (glass) used. If you are only purchasing the archival print from the photographer, then the other materials such as the mounting boards, mattes, and frame would fall on you to ensure the proper materials are being used.\nBeyond materials, the workflow and handling of the artwork also influences the archival permanence of the artwork too. For example, if the artist doesn\'t wear gloves when handling the artwork, acids from their hands and skin will transfer to the print.\nAlso, any adhesives and print mounting materials can also negatively impact the archival performance.\nCHOOSING THE RIGHT PAPER (SUBSTRATE)\nSelecting and using an acid-free paper is the first important step in the archival process and this is controlled by the photographer.\nThe absence of acid simply means the artwork will not decompose or brown the same way regular papers would over time.\nWe know based on history that papers that don\'t contain acid perform remarkably better than those that do. We also know that 100% cotton rag acid free paper is the ultimate choice when it comes to archival performance.\nThe photographer must perform their own due diligence and ensure they are using a certified 100% cotton rag acid free substrate to create their photographic artwork. If the paper is not certified as 100% cotton rag acid free, it could contain harmful elements in the front or back coating that will lead to failure. This is the first question you should ask when considering the purchase of a fine art print that is described as archival.\nTESTING FOR ACID\nAlthough I only purchase certified acid free 100% cotton rag paper for my archival fine art prints, I test each sheet of paper to ensure the paper is truly free of acid. I also test the mounting and matte boards if I include them in the final artwork that I sell to my collectors.\nUnless the fine art paper is certified 100% archival, the paper may be archival but the coating on its front may not be and this is why I take the extra step to test every piece of archival paper that I use for my artwork.\nI use a pH Testing Pen to verify the substrate and materials are suitable for long-term archival permanence. I print on paper larger than the final size of the artwork so I can carefully handle the print during my workflow, but also facilitate the pH test on every print. I trim the artwork down to its final size as the last step in my workflow.\nBy drawing a small line on the material to be tested, the chlorophenol red indicator solution will turn purple on any substrate with a pH of 6.8 or above which is what we want to see. A clear or yellow color indicates the material is unsuitable for conservation purposes. I look for a pH value of 7.0 or greater because this ensures there is enough alkaline reserve to combat the local and atmospheric pollutants that are present when displaying artwork.\nELIMINATING OPTICAL BRIGHTENING AGENTS\nOptical brightening agents (OBAs) are chemicals that are put in papers during the manufacturing process to make the whites of the papers look brighter to the eye.\nI test all of my archival fine art substrate with an ultraviolet light to ensure they is free of optical brightening agents. As you can see in the image, when lit with an ultraviolet light, a paper with OBA\'s will appear abnormally bright as shown on the paper to the left. The paper without OBA\'s will not exhibit this visual artifact.\nThe optical brightening agent chemicals absorb invisible ultraviolet light and then emit the absorbed invisible light during the viewing process to make the whites looks so white.\nWhen this happens, the white of the paper looks incredibly white, because our eyes are seeing a combination of the white of the paper shown by standard light and the light being emitted by the chemical in the paper. While this all sounds visually appealing, it is a terrible idea for archival performance.\nUSING ARCHIVAL PIGMENT INKS\nOnce you have a 100% cotton rag acid free archival paper, the ink that is placed on the artwork is just as important as the substrate.\nPrinting companies have invested millions of dollars researching and developing archival inks for color gamut (range of colors) and archival stability. Based on serious testing performed over the course of several decades, we now know that pigment inks are an archival ink that out perform the original dye-based inks.\nI am currently using a state of the art Epson SureColor P9570 large format fine art archival printer that uses the Epson UltraChrome Pro12 archival pigment inks. By using this system along with certified archival materials and an archival workflow, I produce archival artwork that I guarantee for my art buyers and collectors.\nAccording to Henry Wilhelm, director of research at Wilhelm Imaging Research, which is the world\'s leading independent print permanence testing laboratory, Epson\'s SureColor P7570 (24"") and P9570 (44"") printers using the Epson 12 color UltraChrome PRO12 archival pigment inks are capable of producing museum quality archival prints lasting 200+ years for color and 400+ years for black and white.\nDISPLAY & CARE GUIDELINES\nTo ensure your archival pigment fine art print will last a lifetime, take your artwork to a professional frame shop and ask them to follow these guidelines:\nFor larger prints that won\'t mail flat, I roll the prints between two acid and lignin free tissues and carefully place everything in an oversized tube for shipment. Upon receipt of the new artwork, I recommend taking the new artwork to a professional frame shop and follow my recommendations as listed above.\nPRINT CARE, DISPLAY, & HANDLING TIPS\nThere are many factors that ultimately determine the archival performance of artwork ranging from tools and materials used to the local environment where the artwork is displayed.\nI do everything that is currently known to ensure my materials and workflow are in alignment with the latest museum archival standards so your artwork will last your entire life and then some.\nI guarantee that all of my archival pigment ink fine art prints will look beautiful for your entire life as long as you follow the display and care guidelines as described in this guide.']"	['<urn:uuid:452f64aa-bd41-4e38-a8ee-9949386b8229>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	9	76	1783
74	I study water pollution. What research facilities study it and how can they clean it?	Several research facilities study and address water pollution. Western University has established world-class research facilities including the Southern Ontario Water Consortium's wastewater facility, the Thames River Experimental Stream Sciences Centre, and the Biotron Experimental Climate Change Research Centre. These facilities study various aspects of water contamination and treatment. Regarding cleaning methods, NASA developed an innovative technology called Emulsified Zero-Valent Iron (EZVI) that effectively cleans contaminated groundwater. EZVI works by using nano- or micro-sized iron particles in an environmentally friendly water and biodegradable oil emulsification to neutralize toxic chemicals. This technology has been licensed to multiple companies and is being used to clean up polluted areas worldwide, with treatment methods typically taking only 2-3 months to complete.	['When it comes to problems related to wastewater treatment, water resource management and groundwater remediation, Western University’s researchers are leading the way in finding the answers.\n“We have more than 50 researchers involved in water research,” says Douglas Keddy, Western’s research communications manager. “By combining their strengths with cutting-edge research facilities, we are finding innovative and effective solutions to current water challenges.”\nOver the past few decades, the London university and its partners have invested millions of dollars into establishing world-class research facilities where experts, in partnership with government and industry, are tackling a variety of water issues.\nOne of these facilities is the Southern Ontario Water Consortium’s (SOWC) wastewater facility set up at the City of London’s Greenway Wastewater Treatment Plant. This facility is a partnership between the university, SOWC and the City of London. The City of London provides not only the physical space and access to an active treatment plant, but has made a significant financial investment and remains an active partner. The unique facility provides unprecedented access to real-time municipal wastewater at various flow rates of up to 4,500 m3/day as well as a variety of process streams for full-scale testing and demonstration of new wastewater treatment technologies.\nWestern’s academic lead for the SOWC’s wastewater node is engineering professor George Nakhla. He has expertise in treatment of municipal and industrial wastes and landfill leachate, biological nutrient removal processes, membrane bioreactors, fluidized bed bioreactors, anaerobic wastewater treatment, pre-treatment of biosolids to enhance anaerobic digestion and conversion of high-strength organic wastes to green energy. He has developed five patents on wastewater and biosolids treatment technologies that are in various stages of commercialization. He has also conducted numerous treatment studies and pilot tests at municipal and industrial wastewater treatment plants across Ontario and developed process designs for various treatment systems, including the first communal biological nutrient removal system in Ontario.\nNakhla says SOWC’s infrastructure offers a platform that is ideal for industry looking to advance and commercialize treatment technologies.\n“This facility has definitely helped strengthen Western’s relationships with its industrial partners,” he says. “I have recently submitted multi-million dollar proposals aimed at advancing state-of-the-art wastewater treatment technologies. These technologies utilize approaches that shift the focus of wastewater treatment from a necessity mandated by environmental regulations to an opportunity for resource recovery.”\nIn addition to his work connected to the SOWC, Nakhla also leads The Clean Technologies for Water Refining and Nutrient Energy Recovery (TWNER) program at Western. This program is geared towards providing graduate students with research and training experiences in the water and wastewater industry. There are 11 researchers involved from five Ontario universities as well as three international institutions and universities in the United States, Scotland and Israel. The program also entails 13 government and industrial partnerships.\nSome of the research themes TWNER researchers are currently involved in include high-rate wastewater treatment technologies, biosolids and odour, combined sewer overflows treatment, water reuse and microbiology and ecotoxicity.\nIn addition to its research groups dedicated to innovations in wastewater treatment, Western is also making strides in water management and protection through its Great Lakes Futures Project led by Irena Creed, the Canada Research Chair in Watershed Sciences. The project is an international partnership of 21 Canadian and U.S. research institutions focused on protecting and managing the Great Lakes and St. Lawrence River Basin by informing strategic policy formulation, framing research priorities and helping train the next generation of Great Lakes researchers.\nCreed has more than 20 years of experience working in watersheds draining into the Great Lakes and is an international leader in watershed sciences with particular expertise in hydrology, biogeochemistry, geographic information systems, remote sensing and simulation modelling. She is the founder and leader of the Catchment Research Facility at Western, an advanced monitoring, analytical and modelling facility established to analyze catchment processes. The research underway at the facility provides a reference against which to measure the consequences of human activity on the landscape in order to help industry, government and environmental agencies assess and predict these impacts.\n“Our research at Western seeks to discover how freshwater wetlands, streams and lakes function and to then predict how these functions and the important services that society garners from them will change under future climatic conditions,” says Creed. “Western’s internationally-recognized work on water and the ecosystem services that wetlands, streams, lakes and oceans provide has drawn interest from governments, industry and non-government agencies. They are interested not only because of the scientific excellence of our work, but because we as faculty are committed to translating our scientific findings and affecting positive change through improved policies and practices related to water management.\nWestern’s newest research centre dedicated to water resource management and protection is the Thames River Experimental Stream Sciences (TRESS) Centre, an innovative outdoor facility located at London’s Adelaide Pollution Control Plant.\n“This facility is geared towards protecting aquatic ecosystems, water quality and ecological health at the river level by looking at how land use affects our waterways,” says Keddy.\nLed by geography professor Adam Yates, the TRESS centre allows researchers to study the impact of nutrients and other river contaminants by enabling them to integrate existing and future monitoring data from landscape-scale observational studies into controlled and replicated stream experiments that model actual local rivers. This information will give watershed planners and managers critical insight into potential outcomes of decisions regarding future land use activities in the rivers.\nWestern is also home to The Biotron Experimental Climate Change Research Centre, a $35-million facility that specializes in the simulation of natural environments. It is the world’s first facility of its kind that enables researchers to design self-contained mini-ecosystems.\n“Research conducted in the Biotron facilities is leading to a better understanding of how natural ecosystems respond to a changing environment and how such changes impact the well-being of both natural and human world,” says director Brian Branfireun.\nThe Biotron houses a microscopy and data analysis suite featuring the latest confocal, digital light and fluorescence, transmission and scanning electron microscopy technology. There is also a fully-equipped analytical laboratory with a range of instruments, including inductively coupled plasma-mass spectrometry for the measurement of elements and compounds in water, soil, rock and biological materials.\nThis unique facility is available to researchers, industry and government from across the globe. There are also dedicated technicians and specialists to help remote users conduct projects.\nAnother research group at Western attracting international interest is Research for Subsurface Transport and Remediation (RESTORE), a team of more than 20 researchers focused on the sustainable management of our water resources through the development of engineering technologies as well enhancing our understanding of natural hydrogeologic systems. Key research areas include soil and groundwater contamination and remediation, surface-groundwater interactions and green roof technology. RESTORE’s projects combine high-tech equipment, field research, advanced computer modelling, strong industrial partnerships and commercialization with national and international collaborations.\nIn addition to cultivating top water experts and establishing high-tech research facilities, Western recognizes that developing partnerships with government and industry is also integral to solving current water issues, says Keddy.\n“Our researchers are definitely looking for more partners and are open to developing more projects with industry and government,” he says. “Water is one of the world’s most precious assets. It is critical we leverage the expertise of our researchers, governments and industry partners to find new ways to protect and improve access to water sources around the world. SOWC has significantly advanced our ability to do that.”', 'Solutions Remediate Contaminated Groundwater\nOriginating Technology/NASA Contribution\nKennedy Space Center’s launch complexes have seen a lot. They have been the starting point for every manned NASA mission, from Mercury to Gemini, through Apollo, and are now seeing the space shuttle through its final launches. Kennedy, part of the Merritt Island Wildlife Refuge, is alsoto over 1,500 different plant and animal species.\nTo help protect these living things, NASA works to keep the area as pristine as possible and that sometimes involves inventing new and innovative ways to clean up around the launch pads.\nDuring the Apollo Program, NASA launched Saturn rockets from Launch Complex 34 (LC-34), and during that period—roughly 1959 to 1968—workers used chlorinated solvents to clean rocket engine components. An estimated 88,000 pounds of these chlorinated solvents, which are only slightly soluble and heavier than water, soaked into the soil and groundwater. Even after several decades, the contaminants were still in the groundwater and would have stayed even longer (restoration of the area through natural processes would have taken many decades), had NASA not set about searching for ways to remediate the area.\nThese solvents belong to a category of chemicals known as dense non-aqueous phase liquids (DNAPLs). DNAPLs had contaminated the areas around LC-34 to the point of near-irreparability. NASA environmental engineer Dr. Jacqueline Quinn and Dr. Kathleen Brooks Loftin, a NASA analytical chemist, partnered with researchers from the University of Central Florida’s chemistry and engineering programs to develop an innovative technology capable of remediating the area without great cost or further environmental damage. They called the new invention Emulsified Zero-Valent Iron (EZVI). It neutralizes the toxic chemicals using nano- or micro-sized iron particles in an environmentally friendly water and biodegradable oil emulsification. EZVI is injected deep into the soil, where the contaminants diffuse through the emulsion’s oil membrane and are then dechlorinated by the iron-water interior. The only byproduct of this process is non-toxic hydrocarbon, which diffuses into the groundwater.\nOnce the NASA-University team had EZVI working in the laboratory, it then contracted with GeoSyntec Inc., a small environmental remediation firm in Boca Raton, Florida, through the Small Business Technology Transfer (STTR) program. GeoSyntec worked at LC-34 to evaluate the real-world effectiveness of EZVI, which had previously only been laboratory tested. Groundwater testing showed that EZVI was an effective means of removing the DNAPLs. Through a follow-on contract, GeoSyntec began testing various other methods for injecting EZVI into the soil.\nEZVI was recognized as a 2005 “NASA Government Invention of the Year” and 2005 “NASA Commercial Invention of the Year.” In 2006, the inventors won the Federal Laboratory Consortium’s “Award for Excellence in Technology Transfer.” In 2007, EZVI was inducted into the Space Foundation’s prestigious Space Technology Hall of Fame. According to Kevin Cook, director of Space Awareness Programs at the Space Foundation, “EZVI is an environmentally friendly technology that effectively decontaminates polluted soil and groundwater and is an outstanding example of inspired innovations we seek to induct into the Space Technology Hall of Fame.”\nSince its development, numerous companies have licensed use of this technology from NASA. Several licenses are in the works, but currently six companies are using the NASA-developed EZVI groundwater remediation compound to clean up polluted areas all around the world, making it NASA’s most-licensed technology to date.\nLicensees include Weston Solutions Inc., of Westchester, Pennsylvania, which provides environmental remediation services worldwide; and Toxicological and Environmental Associates, Inc., of Baton Rouge, Louisiana, which is offering the services through the southern and central United States (Spinoff 2005). Additional licensees include Huff and Huff Inc., of Oak Brook, Illinois, which offers EZVI remediation as one of its many environmental cleanup options; Starlight Environmental Group, of Panama City Beach, Florida; Remediation and Natural Attenuation Services Inc. (RNAS), of Brooklyn Center, Minnesota; and Terra Systems Inc., of Wilmington, Delaware.\nThe EPA estimates that there are thousands of sites around the United States contaminated with DNAPLs, including 60 to 70 percent of the sites on the Superfund National Priorities List, a comprehensive list of abandoned hazardous waste sites receiving top priority for environmental remediation. As such, EZVI was accepted into the EPA’s Superfund Innovative Technology Evaluation Program, which promotes the advancements of innovative technologies for detection and remediation of contaminated superfund-designated sites.\nEZVI has proven a valuable tool in remediating these sites, and has also been used at dye, chemical, pharmaceutical, adhesive, aerosol, and paint manufacturing sites; dry cleaners; leather tanning facilities; metal cleaning and degreasing facilities; and many government-owned sites. The licensees of NASA’s EZVI technology are cleaning these contaminants from groundwater all over the country, in a wide variety of soil types, including sand and silt, clay, fractured bedrock, and residuum (soil formed in place by natural weathering).\nThe EZVI treatment methods generally take between 2 and 3 months, a vast improvement over traditional pump-and-treat systems, which can require decades. This reduced timeframe, in addition to the low cost of the materials and equipment, make EZVI cost-competitive with pump-and-treat, thermal treatments, and in situ chemical oxidation. In addition, some of these other methods have the potential to release the DNAPLs to previously uncontaminated areas. Since EZVI treats the contaminants in place, there is no risk for them to mobilize.']	['<urn:uuid:650deb18-f142-489c-aa1a-8fe114a78f20>', '<urn:uuid:06d86ffb-bcf9-483a-b9d1-266d60377128>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	15	116	2103
75	Between Java Flight Recorder and 3D surface measurement tools, which one has less impact on the performance of what it's measuring?	Java Flight Recorder has minimal performance impact of less than one percent when using default settings, while 3D surface measurement tools can be significantly impacted by external factors like vibrations to the point where they require special isolation systems to function properly at high magnifications of 3,000-5,000 times.	['Note:Java Flight Recorder requires a commercial license for use in production. To learn more about commercial features and how to enable them please visit\nJava Flight Recorder (JFR) is a tool for collecting diagnostic and profiling data about a running Java application. It is integrated into the Java Virtual Machine (JVM) and causes almost no performance overhead, so it can be used even in heavily loaded production environments. When default settings are used, both internal testing and customer feedback indicate that performance impact is less than one percent. For some applications, it can be significantly lower. However, for short-running applications (which are not the kind of applications running in production environments), relative startup and warmup times can be larger, which might impact the performance by more than one percent. JFR collects data about the JVM as well as the Java application running on it.\nCompared to other similar tools, JFR has the following benefits:\nProvides better data: A coherent data model used by JFR makes it easier to cross reference and filter events.\nAllows for third-party event providers: A set of APIs allow JFR to monitor third-party applications, including WebLogic Server and other Oracle products.\nReduces total cost of ownership: JFR enables you to spend less time diagnosing and troubleshooting problems, reduces operating costs and business interrupts, provides faster resolution time when problems occur, and improves system efficiency.\nJFR is primarily used for:\nJFR continuously saves large amounts of data about the running system. This profiling information includes thread samples (which show where the program spends its time), lock profiles, and garbage collection details.\nBlack Box Analysis\nJFR continuously saves information to a circular buffer. This information can be accessed when an anomaly is detected to find the cause.\nSupport and Debugging\nData collected by JFR can be essential when contacting Oracle support to help diagnose issues with your Java application.\nJava Flight Recorder collects data about events. Events occur in the JVM or the Java application at a specific point in time. Each event has a name, a time stamp, and an optional payload. The payload is the data associated with an event, for example, the CPU usage, the Java heap size before and after the event, the thread ID of the lock holder, and so on.\nMost events also have information about the thread in which the event occurred, the stack trace at the time of the event, and the duration of the event. Using the information available in events, you can reconstruct the runtime details for the JVM and the Java application.\nJFR collects information about three types of events:\nA duration event takes some time to occur, and is logged when it completes. You can set a threshold for duration events, so that only events lasting longer than the specified period of time are recorded. This is not possible for other types of events.\nAn instant event occurs instantly, and is logged right away.\nA sample event (also called requestable event) is logged at a regular interval to provide a sample of system activity. You can configure how often sampling occurs.\nJFR monitors the running system at an extremely high level of detail. This produces an enormous amount of data. To keep the overhead as low as possible, limit the type of recorded events to those you actually need. In most cases, very short duration events are of no interest, so limit the recording to events with a duration exceeding a certain meaningful threshold.\nJFR collects data from the JVM (through internal APIs) and from the Java application (through the JFR APIs). This data is stored in small thread-local buffers that are flushed to a global in-memory buffer. Data in the global in-memory buffer is then written to disk. Disk write operations are expensive, so you should try to minimize them by carefully selecting the event data you enable for recording. The format of the binary recording files is very compact and efficient for applications to read and write.\nThere is no information overlap between the various buffers. A particular chunk of data is available either in memory or on disk, but never in both places. This has the following implications:\nData not yet flushed to a disk buffer will not be available in the event of a power failure.\nA JVM crash can result in some data being available in the core file (that is, the in-memory buffer) and some in the disk buffer. JFR does not provide the capability to merge such buffers.\nThere may be a small delay before data collected by JFR is available to you (for example, when it has to be moved to a different buffer before it can be made visible).\nThe data in the recording file may not be in time sequential order as the data is collected in chunks from several thread buffers.\nIn some cases, the JVM drops the event order to ensure that it does not crash. Any data that cannot be written fast enough to disk is discarded. When this happens, the recording file will include information on which time period was affected. This information will also be logged to the logging facility of the JVM.\nYou can configure JFR to not write any data to disk. In this mode, the global buffer acts as a circular buffer and the oldest data is dropped when the buffer is full. This very low-overhead operating mode still collects all the vital data necessary for root-cause problem analysis. Because the most recent data is always available in the global buffer, it can be written to disk on demand whenever operations or surveillance systems detect a problem. However, in this mode, only the last few minutes of data is available, so it only contains the most recent events. If you need to get the full history of operation for a long period of time, use the default mode when events are written to disk regularly.\nJFR is comprised of the following components:\nJFR runtime is the recording engine inside the JVM that produces the recordings. The runtime engine itself is comprised of the following components:\nThe agent controls buffers, disk I/O, MBeans, and so on. This component provides a dynamic library written in C and Java code, and also provides a JVM-independent pure Java implementation.\nThe producers insert data into the buffers. They can collect events from the JVM and the Java application, and (through a Java API) from third-party applications.\nFlight Recorder plugin for Java Mission Control (JMC) enables you to work with JFR from the JMC client, using a graphical user interface (GUI) to start, stop, and configure recordings, as well as view recording files.\nBy default, JFR is disabled in the JVM. To enable JFR, you must launch your Java application with the\n-XX:+FlightRecorder option. Because JFR is a commercial feature, available only in the commercial packages based on Java Platform, Standard Edition (Oracle Java SE Advanced and Oracle Java SE Suite), you also have to enable commercial features using the\nFor example, to enable JFR when launching a Java application named\nMyApp, use the following command:\njava -XX:+UnlockCommercialFeatures -XX:+FlightRecorder MyApp', 'Manufacturers need to carefully manage processes to produce a consistent, reliable product. Where precise surface engineering is required, surface measurement can play a key role in maintaining control of the process by checking that the process outputs are not outside of specification.\nImage credit: Minus K\n3D non-contact surface analysis is used extensively across the industry to measure surface irregularities and small displacements – it provides accurate, traceable and repeatable measurements.\nWhen built into microscopy equipment using 3D laser scanning or structured light, these systems measure the surface conditions of products with more precision than any other method, obtaining nanometer-level profile measurements of angle, width, height, roughness, radius and volume.\nSuch precise measurement systems allow users to increase product reliability and quality and expand production yields and manufacturing consistency.\nWhen taking measurements of such high precision, any instrument can be negatively impacted by low-frequency vibrations generated within a manufacturing facility. These vibrations can interfere with measurements, affecting imaging and measurement results.\nOne company that has a great understanding of 3D surface measurements and the manufacturing environment is Keyence Corporation – a leading supplier of measuring systems, sensors, laser markers, machine vision systems and microscopes, worldwide.\nWe have many customers with high-precision 3D measurement systems operating in high-vibration environments, performing microscopy evaluation at 30,000 times magnification, looking at nanometer-level surface features.\nEvan Eltinge, Senior Sales Engineer, Surface Analysis Team, Keyence Corporation of America\n“At that level of detail, and in that environment, if measures are taken to reduce vibration it improves the quality of the data. Without proper isolation surface measurements occurring at 3,000 – 5,000 times magnification, vibration could contribute to image blurring and loss of image quality,” continued Eltinge.\nVibration is the result of a multitude of factors within a plant – every structure transmits noise. Within the building itself, forklift trucks, production machinery, the ventilation and heating system, pumps, fans, elevators and compressors are just a few examples of the mechanical devices and equipment that create low-frequency vibration.\nDepending on the proximity of the surface measurement instrumentation to these vibration sources and wherein the structure the instrumentation is located – i.e., the production floor or in a loft – will affect how strongly the instrumentation will be impacted.\nExternal sources of vibration that may influence instrumentation include road traffic, truck movement, loud noise from aircraft, nearby construction and even weather conditions such as wind as they cause movement of the structure.\nImage credit: Minus K\nVibration Isolation Options for 3D Surface Analysis\nTraditional approaches to eradicating vibration from 3D surface analysis each have pros and cons. However, as surface analysis has advanced from micro-level to nanometer-level tolerances, so has the need for greater accuracy in vibration isolation.\nThe ubiquitous, several-hundred-pound granite slabs mounted on metal tables have been critical in supporting and providing a level platform for sensitive instrumentation in manufacturing facilities.\nAs a passive isolator, these stone slabs do not completely eradicate the vibrations but do significantly reduce the amplitude relative to the mass of the slab.\nA two-foot-thick slab weighing 600 pounds can do an acceptable job of reducing low-frequency vibrations for 3D surface analysis. However, there is no portability without moving the slab, meaning the equipment being used to measure parts would have to stay fixed in one location.\nAir Vibration Isolation Tables\nAir vibration isolation tables are very common devices used in manufacturing for isolating vibration insensitive instrumentation.\nAlthough previously the mainstay for stabilizing the industry’s most critical micro-engineering instrumentation, pneumatic vibration isolation tables are poor isolators of extremely low-frequency vibrations. Such vibrations can negatively affect instrumentation when operating at nanometer-scale resolutions.\nAir systems can achieve a level of vibration isolation, usually in one dimension (i.e., vertical), but offer little horizontal isolation. However, they do not provide sufficient isolation needed at very low-resonance frequencies.\nAir systems can intensify vibration, rather than lessen vibration, in a range typical of 1.5 – 3 Hz because of the natural frequencies at which air tables resonate.\nAll isolators amplify at their resonant frequency but will begin to isolate as the frequency increases. With air tables, any vibrations within the resonant frequency range could not only fail to be attenuated but could also be amplified, and low-cycle perturbations will come straight through to the instrumentation.\nA power pump is required to supply air to air tables – which itself generates low-frequency vibration – as well as a tank that contains the pressurized air, delivering a steady pressure to the isolator.\nAir hoses run from the tank to the vibration-isolating platform. All of this makes for a bulky and complex setup. The large footprint of air tables makes them unsuitable for tight-space laboratories and difficult to transport should the table need to be moved.\nTaxing air table functionality is the growing trend of locating highly sensitive instrumentation (e.g., 3D surface measurement systems) in buildings where vibration noise is high – posing an important challenge to vibration isolation.\nWith more industries moving to highly precise inspection equipment, environments with heavy vibration require more effective vibration isolation solutions than is achievable with air tables.\nActive System Electronic Vibration Cancellation\nAnother vibration isolation option for 3D surface analysis is active vibration isolation systems – also known as electronic force cancellation. These systems use electronics to detect motion and then introduce forces electronically to counteract or prevent the vibrations.\nSuch systems use a combination of actuators, transducers and sensors, whose function is determined by algorithms to attenuate the vibrations. The sensors detect vibrations and their specific directions (x, y and z). These sensors then transmit this information to a controller, which, in turn, directs the actuators to reduce the sensed vibrations.\nFeedback and feed-forward are used to attenuate vibrations in active systems. Feedback control systems continually monitor the platform they are controlling, modifying outputs according to the vibration data retrieved.\nIn a feedback active-control system, the forces causing the vibrations are first measured. The control system then transmits corrective signals to transducers, reducing the vibrations.\nA feed-forward control system reacts in a predefined way to control the input vibrations.\nFor instance, where the behavior of the controlled forces (vibrations) is well understood, this type of control system is deployed – for example, impulses from stage motions, valves, etc. In a feed-forward system, information about the motion is already known, so the system applies the necessary forces to counter the vibrations.\nTo obtain precise results in vibration-critical applications, feedback electronic-force cancellation systems must balance the complexity of inertial feedback in response to incoming vibrations. This can present challenges for active systems when the inertial feedback is applied to incoming low-frequency vibrations horizontally.\nActive systems cancel out low-frequency vibration well, and their use over the past 25 years has progressively increased.\nSome active isolation systems can isolate vibrations from as low as 0.7 Hz. However, other active systems have a limited dynamic range that is easy to exceed. This can cause the isolator to go into positive feedback and begin to generate noise.\nCalibrating active isolation can be challenging. However, once the instrument is accurately set up, its performance exceeds the capabilities of air tables when canceling out low-frequency vibration.\nNegative-Stiffness Vibration Isolation\nDue to its highly effective vibration isolation, particularly at low frequencies, Negative-Stiffness vibration isolation enables highly precise instruments, such as surface analysis microscopes, to continue operating in harsh conditions and severe vibration environments – environments that would otherwise not be practical with top-performance air tables and other vibration-mitigation technologies.\nAs a result, Negative-Stiffness isolators have seen a rapid increase in use for extremely sensitive, nanometer-level microscopy applications.\nDeveloped by Minus K Technology, Negative-Stiffness isolators use a completely mechanical and unique concept in low-frequency vibration isolation – they do not require compressed air nor electricity.\nThere are no chambers, pumps or motors, meaning no maintenance is required as there is nothing to wear out. They operate purely in a passive mechanical mode.\nIn this vibration isolation system, vertical-motion isolation is achieved by a stiff spring that supports a weighted load in combination with a negative stiffness mechanism. The net vertical stiffness is greatly reduced without affecting the static load-supporting capability of the spring.\nBeam columns, connected in series with the vertical-motion isolator, achieve horizontal-motion isolation. A beam-column behaves like a spring combined with a negative-stiffness mechanism. This results in a compact passive isolator capable of very high internal structural frequencies and extremely low horizontal and vertical natural frequencies.\nThe isolator provides 0.5 Hz* isolation performance horizontal and 0.5 Hz vertical, using only a passive mechanical system – no electricity or air is required.\nHowever, it is worth noting that for an isolation system with a 0.5 Hz natural frequency, isolation begins around 0.7 Hz and improves with an increase in the vibration frequency. The natural frequency is most often used to describe the system performance.\nNegative-stiffness isolators resonate at 0.5 Hz. At this frequency, there is almost no energy present, and therefore, it is unusual to find a significant vibration at 0.5 Hz. Vibrations with frequencies greater than 0.7 Hz (where negative-stiffness isolators begin isolating) rapidly attenuate with increasing frequency.\nWith Negative-Stiffness isolators, transmissibility is significantly better than that of air systems. Transmissibility is a measure of the vibrations that are transmitted through the isolator relative to the input vibrations.\nThe negative-stiffness isolators, once adjusted to 0.5 Hz, achieve 93 % isolation efficiency at 2 Hz, 99 % at 5 Hz and 99.7 % at 10 Hz.\nKeyence supports the development of an array of factory automation and sensors, machine vision products, as well as microscope and measurement instrumentation for non-contact surface analysis in 3D.\nFor example, Keyence’s VR-3000 Series – a non-contact 3D measurement system – is capable of measuring wide areas accurately and quickly, measuring roughness, profile, wear volume or flatness, or comparing 3D scan data to CAD files, and diagnosing whether a part is good or bad.\nImage credit: Minus K\nIn addition, Keyence’s VK-X Series 3D laser scanning confocal microscope provides non-contact, nanometer-level roughness, film thickness and profile data on any material and indicates the most vital metrics for evaluation.\nThe instrumentation products from Keyence are designed to measure at the nanometer scale, meaning in factory environments, where extreme vibration exists, isolation of low-frequency vibrations can be significantly improved.\nWe have traditionally recommended granite slabs and air tables for vibration isolation. The reality is, they do isolate some of the vibrations, but for customers, in extreme vibration environments we wanted a solution that could provide even better results when doing a nanometer-level evaluation on a surface. So, we have explored other options that would provide a better solution to these low-hertz vibrations.\nEvan Eltinge, Senior Sales Engineer, Surface Analysis Team, Keyence Corporation of America\nTwo of the best options, as identified by Keyence, are Negative-Stiffness vibration isolation and active system electronic vibration cancellation.\n“What we have found was essentially, that both active electronic cancellation and Negative-Stiffness technologies have similar levels of vibration isolation,” explained Eltinge. “But there is a big difference in what it takes to set up the systems. Active cancellation systems are more difficult to set up. Whereas, Negative-Stiffness isolators require one quick adjustment and they are ready to go.”\n“Our customers value equipment that is easy to use and to operate.” said Eltinge. “They purchase our equipment looking for something that will measure with incredible precision, but, at the same time be simple enough for any operator. To add into this a vibration isolation solution that is challenging to use is taking away from the value of our product. The operational simplicity of the vibration isolation system is, therefore, very important to us.”\nThis information has been sourced, reviewed and adapted from materials provided by Minus K Technology.\nFor more information on this source, please visit Minus K Technology.']	['<urn:uuid:ea9dc31e-3c15-4510-81fd-631701add0ec>', '<urn:uuid:f8f09881-dcd4-490b-8f93-0f3f97ad6580>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	21	48	3138
76	legal requirements needed prove intention guilty assault oklahoma statute	According to Oklahoma law, an individual needs to have general intent to commit an assault - it's not possible to accidentally assault another person. The offender must intend the actions that made up an assault. Acting in a way that's considered dangerous to other people can be enough to support assault charges, even if they didn't intend particular harm to an individual. An intent to scare or frighten somebody could possibly be enough to establish assault charges.	['Assault in Oklahoma\nAssault in Oklahoma is defined in 21 O.S. § 641 as any willful and unlawful attempt or offer with force or violence to do a corporal hurt to another. The punishment for assault is in 21 O.S. § 644. Simple assault is a misdemeanor punishable by imprisonment in a county jail for up to 30 days, or by a fine of not more than $500, or by both such fine and imprisonment.\nAssault & Battery In Oklahoma\nBattery in Oklahoma is defined in 21 O.S. § 642 as any willful and unlawful use of force or violence upon the person of another. The punishment for assault and battery is defined in 21 O.S. § 644. It’s a misdemeanor punishable by imprisonment in a county jail for up to 90 days, or by a fine of not more than $1,000, or by both fine and imprisonment.\nAssault With Intent to Kill\n21 O.S. § 653 defines assault with intent to kill. Somebody who is found guilty of assault with intent to kill can be imprisoned for up to five years in the Department of Corrections or spend up to one year in the county jail. The maximum fine is $500.\nAssault With Intent to Commit a Felony\n21 O.S. § 681 covers assault with intent to commit a felony. Somebody who is found guilty of assault with the intent to commit a felony can be imprisoned for up to five years in the Department of Corrections or spend up to one year in the county jail. The maximum fine is $500.\nDoes A Victim Need To Be Injured For An Assault Or Battery Charge To Have Occurred?\nGenerally a victim doesn’t have to be injured or harmed for a battery to occur. What is required is offensive contact. Even though contact is generally not necessary for an assault offense, assault still requires some sort of criminal act. Typically this act must be overt and has to put a reasonable person in fear for their safety. Spoken words alone aren’t enough of an act to constitute an assault unless the offender backs them up with an act or action that put the victim in a reasonable fear of imminent harm.\nAggravated Assault & Battery in Oklahoma\n21 O.S. § 646 covers aggravated assault and battery. An assault and battery becomes aggravated when it is committed under any of the following circumstances:\n- when great bodily injury is inflicted upon the person assaulted, or\n- when committed by a person of robust health or strength upon one who is aged, decrepit, or incapacitated. Great bodily injury is defined as a bone fracture protracted in obvious disfigurement, protracted loss or impairment of the function of a body part, organ, or mental faculty, or substantial risk of death.\nThe punishment for aggravated assault and battery is imprisonment for up to five years in the Department of Corrections, up to one year in the county jail, a fine of up to $500, or by both fine and imprisonment.\nAssault And Battery with A Dangerous Weapon\nAssault and battery with a dangerous weapon is defined in 21 O.S. § 645. Any person who, with the intent to do bodily harm and without justifiable or excusable cause, commits any assault, battery, or assault and battery upon a person or another with any sharp or dangerous weapon, or who without such cause shoots another with any kind of firearm, air gun, conductive energy weapon, or other means, with the intent to injure any person can be found guilty of assault and battery with a dangerous weapon. Punishment can be imprisonment in the Department of Corrections for up to ten years or up to one year in a county jail.\nHow Does Intent Affect an Assault Charge?\nAn individual needs to have only general intent to commit an assault. It’s not possible to accidentally assault another person. The offender must intend the actions that made up an assault. If an individual acts in a way that’s considered dangerous to other people, then that can be enough to support assault charges. It doesn’t matter if they didn’t intend a particular harm to an individual. An intent to scare of frighten somebody could possibly be enough to establish assault charges. Battery generally doesn’t require any intent to harm the victim. Someone needs to only have an intent to contact or cause contact with another individual. If somebody acts in a criminally reckless or negligent way that results in such contact, then an assault may have occurred. Keep in mind that accidentally bumping into somebody, even if the victim considers it to be offensive, does not constitute a battery.\nHow Does The Degree Of Injury Impact An Assault Charge?\nThe degree of injury that a victim suffers can affect an assault charge by making it a felony or cause other charges to be filed. This can include battery or assault with a dangerous weapon. The more severe the injury, the more severe the potential punishment will be.\nWhat Defense Strategies Can Be Used In Assault Cases?\nThere’s a variety of defense strategies that can be used in assault cases. The first one is that the defendant didn’t actually have the ability to inflict the force or violence upon the other person. Perhaps the defendant was restrained or in some other state that would render them unable to have actually inflicted the violence they are accused of inflicting. A common defense is self-defense. If somebody assaults and batters another person when they were only acting to defend themselves against another assault and battery, then self-defense could be a good defense. Another defense is that the defendant didn’t act willfully, or with the required intent. This could be accidental contact. For example, just bumping into somebody is insufficient to constitute assault. Finally, a possible assault and battery defense is that the defendant was wrongfully accused. If there was an event going on where there were multiple people fighting each other, then there could have been someone other than the defendant who assaulted and battered the victim. The victim could have been confused, in a state of intoxication, or in some other altered state that rendered them unable to accurately describe who the perpetrator was.\nCurrent as of 6/16/2019.']	['<urn:uuid:2b3435a3-9cd1-4bf4-b9e0-ffb0c9232ff3>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	9	77	1046
77	corn cultivation importance southeast tribes modern food security soil degradation challenges	Corn was crucial for Southeast tribes as it provided high nutritional yield with minimal labor and enabled complex social hierarchies through reliable food storage. This historical agricultural success contrasts with modern challenges, where soil degradation through physical, chemical, and biological changes threatens food security by reducing soil's ability to supply water and nutrients to plants, particularly affecting corn production needed to feed growing populations.	['- Traditional culture patterns\n- Cultural continuity and change\nSubsistence and material culture\nThe Southeast was one of the more densely populated areas of native North America at the time of European contact. Most groups resided in the piedmont, where they took advantage of extensive game resources, wild plant foods, and an abundance of arable land. The peoples of south Florida were an exception, as they adjusted to an essentially subtropical maritime way of life.\nThe primary division of labour was by gender. Women were responsible for cultivating the fields, gathering wild plant foods, cooking and preserving food, taking care of young children and elders, and manufacturing cordage, baskets, pottery, clothing, and other goods. Men assumed duties associated with war, trade, and the hunt; they were often away from the community for extended periods of time. Men also assisted in the harvest, cleared the fields by girdling trees, and constructed houses and public buildings. Both genders manufactured ceremonial objects.\nThe economic mainstay of the Southeast was corn. Several varieties were grown, including “little corn” (related to popcorn); flint, or hominy, corn; and flour, or dent, corn. Some varieties were baked or roasted on the cob; some were boiled into a succotash, a dish of stewed corn and beans; and still others were pounded into hominy or cornmeal in wooden mortars made of large upright, partly hollowed logs. Domesticated varieties of beans and squash were also important in the diet, as were wild greens. Fields were prepared with mattocks and hoes and planted by punching holes in the ground with digging sticks, inserting seed corn, and covering the holes with earth to form a mound about two feet (one-half metre) in diameter; in some areas the soil was instead hilled into a series of linear mounds or ridges some three feet (one metre) across. Typically, beans and squash were planted adjacent to the corn; the bean vines used corn stalks as trellises, while the broad leaves of squash shaded the soil, minimizing weed growth and conserving moisture. Most fields belonged to individual households, although some tribes also cultivated communal fields. Communally grown produce was given to chiefs for redistribution to the needy and for use in various ceremonies and festivals.\nThe importance of corn in the Southeast cannot be overemphasized. It provided a high yield of nutritious food with a minimal expenditure of labour; further, corn, beans, and squash were easily dried and stored for later consumption. This reliable food base freed people for lengthy hunting, trading, and war expeditions. It also enabled a complex civil-religious hierarchy in which political, priestly, and sometimes hereditary offices and privileges coincided.\nOther cultivated plants included the sunflower, which was processed for its oil; Chenopodium and orache, which produced starchy seeds and spinachlike greens; and tobacco. Many additional plants, such as wild grapes, plums, and perhaps walnut and pecan trees, were in a condition of incipient domestication; indigenous peoples exerted some effect on the propagation of these plants but did not fully domesticate them. Other important plant foods included berries, nuts, acorns, potatoes, zamia roots (similar to turnips), amaranths and smilax (providing shoots and seeds), and maple and honey locust sap. Two species of holly (Ilex cassine and I. vomitoria) were ingredients in a special decoction, the “black drink,” which was used to induce sweating and vomiting in ceremonial and medical contexts. The economic botany of the region also encompassed a vast array of plants used for cordage, clothing, dyes, fish poisons, medicines, building materials, and various tools and utensils.\nBefore European colonization, the only domesticated animal in the Southeast was the dog. In this region canines were used to a minor extent in hunting and as food but were probably most important as sentinels that warned of approaching strangers. In accounts of the Hernando de Soto expedition (1539–43), there are several references to small, fat, barkless dogs that were served to the Spanish visitors by their indigenous hosts. Some of the 300 or more trail hogs that were transported by de Soto to feed his troops escaped and became the ancestors of the modern razorback hog. The Spanish also brought horses to North America, but their use was primarily confined to the Southwest and Mexico; as a result, the Southeastern peoples generally obtained horses at a much later date, through trade with Plains tribes.\nMost of the region teemed with wild game: deer, black bears, a forest-dwelling subspecies of bison, elks, beavers, squirrels, rabbits, otters, and raccoons. In Florida, turtles and alligators played an important part in subsistence. Wild turkeys were the principal fowl taken, but partridges, quail, and seasonal flights of pigeons, ducks, and geese also contributed to the diet. The feathers of eagles, hawks, swans, and cranes were highly valued for ornamentation, and in some tribes a special status was reserved for an eagle hunter.\nIn both salt and fresh waters a wide variety of fish were taken; fishing equipment included weirs (underwater corrals or pens), traps, dip nets, dragnets, hooks and lines, bows and arrows, and spears. Botanical poisons were administered in ponds and sluggish or dammed streams, creating a rich harvest of stunned, but edible, fish. Coastal groups gathered oysters, clams, mussels, cockles, and crabs, while those residing in the interior collected freshwater mussels and crayfish.\nThe peoples of the Southeast altered the landscape significantly by girdling trees and by the controlled use of fire. These activities created large areas of secondary growth that favoured certain types of berry bushes and other useful plants. The presence of this secondary-growth flora was essential for supporting the large populations of browsing deer, squirrels, rabbits, and wild turkeys on which people depended for sustenance. These environmental changes, combined with hunting, probably accelerated the decline of the wood bison and in some places other species; in areas with intensive corn cultivation, such as the lower Mississippi, early European explorers reported that game animals were scarce. In the central Southeast, however, native groups maintained an equilibrated balance with nature.', 'By: Jerry L. Hatfield, Laboratory Director, National Laboratory for Agriculture and the Environment\nThere is a continual dialog about the food required to feed a population of nine billion people by 2050 and the changing climate and the impacts on food production and food security. In all of this discussion there is little attention given to the fundamental fact that without soil we would produce little food. There is also little notice given to the fact that the land area we will have available to produce this food will continue to decline as the population increases. Another fact which can’t be ignored is that our soils continue to be degraded, thereby losing their ability to supply water and nutrients to plants.\nIf we examine a very simple concept of water use efficiency as shown on the attached diagram then we observe that the more water transpired by the plant, the higher the grain yield. As we limit water, we limit plant productivity and with the more variable rainfall expected under climate change then we can expect more variation in crop production. Across the Midwest, the primary factor causing yield variation among years is rainfall and within fields is the ability of the soil to supply adequate soil water during the grain-filling period. As an example of this dilemma, when we couple the water use efficiency concept with the desire to produce more corn, a simple extrapolation to achieve 18,900 kg/ha (300 bu/A) of corn from our present levels will require another 200 mm (7.8 inches) of water transpired through the plant. Soil is a water reservoir and the degradation process reduces the ability of the soil to store water because of the loss of organic carbon content and subjects the soil to crusting and erosion leading to less water being infiltrated into the soil. If we have more variable rainfall and a diminished capacity to infiltrate or store soil water then it may be difficult to achieve the yield increases required to adequately feed the world’s population.\nSoil degradation occurs through physical, chemical, and biological changes. Two agronomic components are always mentioned with soil degradation; increased tillage and residue removal. The physical processes associated with degradation include the loss of soil structure, crusting, compaction, erosion while chemical degradation is linked with nutrient depletion, elemental imbalance, acidification, and salinization, while biological degradation is caused by depletion of soil organic matter and reduction in the diversity and activity of soil microorganisms. Any change in the soil will begin the process of degradation and limit its ability to supply water and nutrients to plants. However, we assume that we can overcome these problems with the addition of nutrients to the soil and supplying water through irrigation. Unfortunately, we consider soil something to be managed around rather than properly managed to increase its capability to supply water, nutrients, and gases (oxygen for root growth) to a growing plant.\nIn the current climate change discussions, there is a large amount of attention given to the role soil management can play in terms of mitigating climate change. Sequestration of carbon into the soil and reduction of nitrous oxide emissions are often considered to be extremely significant roles for agriculture. However, these discussions often fail to consider the complete linkage in which the practices which decrease CO2 emissions are the same ones which can reverse soil degradation and ultimately lead to increases in soil water necessary to produce the food needed to meet the population demands. Likewise, the practices associated with N management responsible for reduced nitrous oxide emissions can also lead to improved crop production, higher quality of the product, and reduced water quality impacts.\nSoil provides a foundation of efficient agricultural production; however, our current view does not acknowledge the critical role soil has in producing food, feed, and fiber for humankind. Erosion remains a major problem around the world and continues to degrade the soil. To combat this problem we are going to have to come to the realization that soil is a system composed of biology, chemistry, and physical structures not unlike what we have in our cities. Once we begin to understand the dynamics of this system then we can begin to understand how all of our adaptive strategies to cope with climate change will have to incorporate an understanding of the role of soil in supplying water and nutrients. There are large opportunities for soil science to feed the world and provide solutions to climate change. We have to recognize that a key component to the question can be found by looking down at our feet rather than into the stars.']	['<urn:uuid:30dbda1c-e712-4f19-9e7a-766dbb985dbc>', '<urn:uuid:07c0760f-75b4-44c8-ad16-eb3e53eec19b>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	11	64	1764
78	types underground garbage bins india	There are two types of drop boxes - partly underground and fully underground bins. For example, Dharamsala has fully underground bins because it's a tourist hotspot and they wanted bins with large volume that look ergonomically good.	"[""Raj Kumar:‘You need good places to put waste and they should be available round the clock’\nZonta's way: A dump can become a drop box\nUmesh Anand, Bengaluru\nIt takes more than civic sense to get a city into shape. The best of intentions won’t make garbage disappear. But when technology, entrepreneurship, governance and citizen participation coalesce, the impossible begins\nZonta Infratech, a company specialising in disposal of urban wastes, was launched three years ago. Its timing could not have been better. Now, as city administrations in India compete with one another in cleaning up, the company is well poised to offer advice, systems and technologies which make their task easier and sustainable.\nA good example of what Zonta Infratech can do is available at Bengaluru’s Victoria Layout, where it has converted an open dump into two partly submerged bins, which it calls drop boxes, for wet and dry garbage. The garbage now no longer lies around but goes into the bins and is collected regularly by Zonta Infratech by mechanically lifting the bins and emptying them into a truck or compactor. A splash of street art completes\nDrop boxes are of two types — either partly or fully buried. Garbage that doesn't get picked up from homes and establishments, which could be as much as 20 percent of a city's garbage, goes into the bins and is out of sight.\nThere are other examples at locations as diverse as Dharamsala, Paonta Sahib, Jabalpur, Chennai and Tirunelveli. Jabalpur is a particularly big success thanks to a visionary commissioner in\nZonta Infratech offers a complete solution beginning with the drop boxes and ending with conversion of waste into energy. It has the advantage of focus and superior technology from a German partnership. Its founder and managing director, Raj Kumar, is also passionate about the business. A corporate lawyer till recently, he became an entrepreneur because of his fascination with changing social equations and the urban opportunities they present.\nCivil Society spoke to Raj Kumar at Zonta Infratech’s corporate office in Bengaluru.\nYou have found the need for creating large-sized bins. Most cities are moving away from bins. But you think they work. How?\nFor us these are not bins at all. They are waste drop boxes which are available to the public. At the administrative level in cities there are different thoughts on bins. Some think we can create a bin-less city. But the concept of a bin-less city involves a different kind of mindset. In our country, the bin- less concept exists because there is low-cost labour always available to pick up garbage.\nWe expect there is somebody out there to carry our waste. It’s like you expect somebody to carry your bag outside the airport. That parochial mindset needs to be changed.\nWe feel that when the economy of the country and the city move forward, this parochial way of thinking will completely change. Today you can afford to get your servant to carry your garbage. Tomorrow it may not be the case.\nSo, you need good places where you can put waste and segregate waste — and they should be available round the clock. That is the mechanism we are creating through these drop boxes.\nAre these both dry bins and wet bins?\nIn Bengaluru and Chennai we have created dry bins, wet bins and hazardous household bins. Invariably, in projects where we are involved we insist these two dry a bit.\nWhat happens after that? Are the bins of the same size? Is there a size you have found which is a useful size?\nYes. There are different models of bin size and bin types. We look at what the city needs. In most cities, we find that wet waste is more and needs to be emptied every day or every second day. We have kept a 1.5-cubic-metre bin for that. But in the case of dry waste you can keep it for two days. The drop box size can be bigger, 2.5 cubic metres. The volume of the bin for dry waste can be more and you need not empty it every day.\nThe bins can be partly underground or fully underground. The city of Dharamsala in Himachal Pradesh has bins fully underground because those bins look ergonomically very good. The city is a tourist hotspot so they were looking for bins with a large volume and fully underground. We have\ngot the facility to manufacture the bins and we supply them.\nTo empty bins we use a crane fitted on a truck. The bin is picked up, emptied into a compactor and then put back in the same pit.\nTransporting the garbage is also part of your operations?\nYes, in most cities they would like us to do the collection because they feel that it reduces the burden on the city and we are ready to take the responsibility.\nIn Bengaluru you had black spots where garbage gets thrown and you have placed these bins there. What has been the impact?\nWe find that in some areas of the city where these so-called black spots had been in existence for 15 to 20 years, when we put the drop boxes there, the change in that vicinity or in the nearby captive area was significant. People had been suffering there for years. For example, in Victoria Terrace.\nWhat is happening here in terms of a system is that you would anyways have segregation in the household.\nA woman empties her garbage into a drop box\nSo your drop boxes are meant for those people whose garbage does not move from the house because they are late in having it picked up?\nIn Bengaluru we have some sort of door-to-door collection happening. The city is very fast-moving. Family members might all be working. If you are not able to achieve 100 percent door-to-door collection and you leave, say, five, 10 or 20 percent uncollected, then you are going back to square one. Twenty percent of the waste can be very visible litter in your city.\nTheoretically, you should do 100 percent collection. No one can achieve that unless it is something you are going to do round the clock. So, with that perception in place, this kind of variation can make your system collapse.\nOur drop boxes, even in cities where door-to-door collection is happening, add one step above your current practices. You are not leaving any stone unturned. Your commercial and residential areas have this as an add-on even if you have a collection system in place.\nOne of the big challenges in the Indian urban context is to get rid of the landfill. It is an outdated concept. This drop box concept reinforces the landfill. Am I right?\nNo. As far as the waste cycle is concerned, collection is the first step. After collection, what do you do? That is where processing comes in. In our organisation we have a solution for processing. The landfill is the last stop in the chain. If you are not able to process the waste, then you put it in a landfill or dumping ground.\nJabalpur, for instance, is probably one, or perhaps the only, city in India which has taken care of all three stages. Four years ago, when we first engaged with that city, there wasn’t much of a system in place. Fortunately, the commissioner, a very dynamic person, listened to us and was open to many ideas. We find today that the city has a robust collection system. It has got door-to-door collection, underground bins, semi-underground bins for commercial and residential areas, plus it has got a very good waste processing plant, a one of its kind in India. It’s got a landfill where only residual waste from the waste-to-energy plant is placed. In that way, Jabalpur is a model.\nThe waste is suitable for generating energy?\nYes. Basically, the rationale for waste-to-energy in our company technically and logically is to burn everything other than metal, stone or sand. Technically, if you have more calorific value, you get more energy.\nBut what about air pollution from waste-to-energy plants?\nYou need to have investment for the flue gas treatment. In a typical waste-to-energy plant we also supply, erect and commission 30 to 35 percent of the project cost goes into flue gas treatment. There are established norms. You really need to outfit your system to make it capable of treating the dangerous gases.\nThe key is in the design?\nIn the design and cleaning of the flue gas.\nDoes the setting up of a waste-to-energy plant in India entail higher investment?\nIn fact, Indian waste-to-energy plants cost the lowest. If you want the same kind of plant that has been put in Jabalpur, or which we are building in Chennai, for example, if you want to do it in any other geography, the cost will be much higher. Being a new and emerging area, companies like ours at least in the initial new projects are willing to absorb even a loss to pick up projects to showcase. Compared to India, China came late to the waste-to-energy market. We had a waste-to- energy plant in Timarpur in the 1990s but we failed because of the way the project was structured and the technology. This is now the norm in China. Chinese waste is not very different from Indian waste.\nThe production of electricity is an add-on. It is possible for it to be a viable business model. Large companies are now willing to invest in such projects and the government is promoting them under the Swachh Bharat Mission.\nWhen you went to Jabalpur, nothing much was happening?\nWhen we went there it was like any other city. I should praise the commissioner there. A strong officer can make a big difference to a city. Singlehandedly, in Jabalpur, the commissioner has made a big change. We have seen real change and also in other segments like water and roads.\nCan you explain the waste management model that Jabalpur is following?\nWe have semi-underground bins, then there is the waste-to- energy plant built by a Japanese company with investment from Essel Infrastructure. Then there is the landfill where only residue from the plant goes, the bottom ash, and for that too efforts are being made to utilise it.\nHow much waste is Jabalpur generating?\nAbout 420 tonnes per day and they are getting waste from nearby towns like Khatni and Narsingpur. They are using the old dump site which is behind the plant. That waste is also being put into the plant.\nWhat is the capacity of the plant?\nAbout 11.5 MW.\nHow much waste can it handle?\nAbout 580 tonnes daily.\nSo if Jabalpur grows a little more, you will need another plant?\nNot for the next 10 to 20 years.\nWhat did it cost the Jabalpur municipality to put this system in place?\nIt depends on the number of bins the city wants. In phase one they took 50 bins and in phase two 200 bins. The waste-to-energy plant initiative has come from my company.\nBut municipalities don’t have the money.\nJabalpur is a good example where investment has come from the private sector. Waste-to-energy is from the private sector and the city is investing only in bins and collection. Cities are increasingly going to invest in modernising their collection system because that is where cities see an immediate requirement. Everybody wants to see streets clean.\nThe company investing in the plant probably has a model for selling the power.\nIf the municipality is investing, what does it have to get together to clean a city like Jabalpur compared to what they were spending earlier in a less efficient manner?\nWe see that the operation cost for a collection system like our system is less than what they were spending under a crude system. The reason being that you are creating large drop boxes and you are emptying every second day or third day. Your daily collection cost comes down. This is a USP for introducing our products in most of the cities.\nOut of our 10 clients we find most are under the Smart City programme. They have a requirement to do smart collection. They have funding for that. We manufacture under the Make in India initiative. Earlier, we were importing waste-to-energy plants.\nHow much does an average bin cost?\nIt depends on the volume and whether it’s fully underground or semi-underground. In Bengaluru we started with a pilot project under CSR funding from Embassy Office Parks. The city has now floated a tender for a larger number of bins.""]"	['<urn:uuid:3fcaced1-8929-46ea-b461-002b21329332>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	5	37	2103
79	reut shalgi joseph rebehmed scientific background education compare	Reut Shalgi completed her education at Israeli institutions, with a double major in Biology and Computer Science from Tel Aviv University and her PhD from the Weizmann Institute, while Joseph Rebehmed received his education internationally, obtaining his MSc in Bioinformatics and PhD in Computational Chemistry and Informatics from the University Paris Diderot in France, after completing his BSc in Biology from University Saint-Joseph in Lebanon.	['Learning the Language of Proteins\n“Science leads you in directions you hadn’t thought of, and you have to navigate your way through it.” Assistant Professor Reut Shalgi returned from MIT to Israel thanks to the Technion, where she currently investigates regulatory mechanisms of protein synthesis at the Rapapport Institute of Medical Research.\n“As a scientist, I have access to an endless world of research topics. Science leads you in directions you hadn’t initially thought of, and you have to navigate your way through it, but at the same time to let it lead you.”\nAssistant Professor Reut Shalgi, who this week was awarded the prestigious Alon Fellowship for Outstanding Young Faculty, joined the ranks of the Technion faculty last year. Currently, as a member of the team at the Technion’s Rapapport Institute of Medical Research, she investigates the mechanisms of translational control by the ribosomes.\nShe graduated cum laude from Tel Aviv University in 2002, with a double major in Biology and Computer Science. Her final project explored what was back then the emerging field of Bioinformatics, which applies tools from statistics and computer science to biological and pharmacological research.\nThanks to her computer science skills Prof. Shalgi began, while still an undergraduate, to work in the hi-tech company of RadVision. But the search for new challenges led her to enroll in the Life Sciences Master’s degree program at the Weizmann Institute. With the guidance of Prof. Yitzhak Pilpel and Prof. Ron Shamir she used bioinformatical methods to study mechanisms that regulate gene expression – and was captivated by the world of scientific research. “During my Masters I realized I wanted to be a scientist – to understand how cells function, investigate the ways in which they can malfunction and find ways of correcting them.\n“We work in the new and fascinating field of systems biology, increasingly integrating biology and computer science, and using genome-wide data and bioinformatical tools,” Assistant Professor Shalgi explains. “Systems biology gives us an broad view of the biological processes taking place inside the cell, whereas classic biology examines these processes in depth.” She went on to undertake her PhD at the Weizmann Institute, with Prof. Pilpel and Prof. Moshe Oren as her advisors. “My research focused then on the regulation mechanisms of micro-RNA – small molecules that are genes but that do not encode proteins. Every stage in protein synthesis is controlled by numerous factors inside the cell. I investigated the interrelationships and the coordination between different levels in this network.”\nIn 2009, after she completed her Ph.D., Assistant Professor Shalgi went to pursue her post-doctoral studies at MIT. Under bioinformatics expert Prof. Chris Burge and Prof. Susan Lindquist, a world leader in the field of chaperones (specialized proteins that facilitate the correct folding of other cellular proteins, which is essential for their proper function), Shalgi studied translation control mechanisms in cells under stress, as well as the interrelationship between chaperones and the ribosome. It was at this time that she discovered a previously unknown mechanism of translation control: chaperone-mediated ribosome pausing during the translation process. She continues to research this mechanism in her own lab.\nThe pausing occurs in response to extreme environmental conditions, and is mediated by the chaperones and their interaction with the ribosome. Upon relief from stress, the essential process of protein synthesis resumes.\n“We think that this mechanism is also involved in neurodegenerative diseases, such as ALS, Alzheimer’s, Parkinson’s and Huntington’s diseases, among others,” she adds, “conditions in which proteins misfold and aggregate [accumulate and clump together]. We are currently studying how ribosome pausing is linked to these pathologies.”\nShe has very good memories of her five post-doctoral years in Boston, during which her first daughter was born – but “I really wanted to come back to Israel, to my family and friends. The Technion gave me a very warm welcome, and last October we returned to Israel – thanks to the Technion. I came back from MIT with several cutting-edge research methodologies, which I brought with me to my new lab at the Faculty of Medicine.”\nOne of these methods involves high throughput screening of protein-protein interactions performed by a robot, which can screen thousands of protein pairs per day and produce quantitative information on the interactions between each pair of proteins. Assistant Professor Shalgi analyses this data using bioinformatics tools, and maps the control network that governs the cell’s translation system and the interactions among the proteins: “The robot enables me to map out entire protein networks rapidly, and I use bioinformatics tools to analyze the cross-talk between the components in this network, and translation regulators.\n“In my lab we integrate biology with genome-wide methodologies and bioinformatics to study the cell as a system. Our unique approach is to look at the cell as a system in which every component must not only function properly on its own but also be coordinated with all other components.”\nShe never felt that she had to compromise her scientific or research excellence with the move from MIT to the Technion. “The Faculty and the Technion offer me the highest standards possible. It’s a very diverse faculty with a good, strong interface between clinical and scientific research. The Technion’s new major in Medical Sciences offers the Faculty’s students an opportunity to get to know the science and possibly become researchers. Everyone knows what a medical doctor does, but not everyone understands what it means to be a researcher, what it is that researchers actually do on a day-to-day basis. An M.D. treats patients with the tools available to them, but the development of a new drug or treatment requires in-depth understanding of the mechanisms that lead to the disease, and that’s what we as researchers aim for. Some medical doctors manage to do research alongside the clinic, but it’s the full-time researchers who can delve deeply into the processes that cause the disease.”\nOriginally published in the Technion website.', 'Dr. Joseph Rebehmed is an assistant professor of bioinformatics in the Department of Computer Science and Mathematics. He joined the Lebanese American University as a visiting assistant professor in January 2016. He obtained a master’s degree in Bioinformatics and a PhD in Computational Chemistry and Informatics (with the highest level of distinction) from the University Paris Diderot, France.\nHis main research interests include studying the relationships between the sequence, structure and function of proteins, their evolutionary conservation and their alterations upon mutations leading to diseases. He also has a large expertise in the field of protein structure prediction and protein-ligand interactions in complexes of pharmaceutical interests. During his postdoc at the Medical School of the University of Michigan, USA, they predicted the best ligand-protein complex for CXCR4-CVX15, the most difficult target in the GPCR-dock 2010 worldwide experiment.\nDr. Rebehmed’s research focuses as well on enzyme design, protein stability and dynamic, developing tools and machine learning applications. He loves everything that has to do with science.\nHe is currently serving on the editorial board of a research topic entitled “Advances in Molecular Docking and Structure-Based Modeling” for Frontiers in molecular biosciences and as reviewer for several journals. He is also a member of the International Society for Computational Biology and the Biophysical Society.\nOutside of his professional life, he is cat person who enjoys reading, playing sports (tennis and swimming), traveling and cooking.\n- “iPBAvizu: a PyMOL plugin for an efficient 3D protein structure superimposition approach” Guilhem Faure, Agnel Praveen Joseph, Pierrick Craveur, Tarun J. Narwani, Narayanaswamy Srinivasan, Jean-Christophe Gelly, Joseph Rebehmed*, Alexandre G. de Brevern Source Code for Biology and Medicine, 2019\n- “Investigation of the impact of PTMs on the protein backbone conformation” Pierrick Craveur, Tarun J. Narwani, Joseph Rebehmed, Alexandre G. de Brever Amino Acids, 2019, 51(7), 1065-1079\n- “A topology-based investigation of protein interaction sites using Hydrophobic Cluster Analysis” Alexis Lamiable, Tristan Bitard-Feildel, Joseph Rebehmed, Flavien Quintus, Francoise Schoentgen, Jean-Paul Mornon, Isabelle Callebaut Biochimie, 2019, 167, 68-80\n- “Enhancing the Activity of a Dietzia sp. D5 Baeyer‐Villiger Monooxygenase towards Cyclohexanone by Saturation Mutagenesis” Serena Bisagni, Milad Abolhalaj, Alexandre G. de Brevern, Joseph Rebehmed, Rajni Hatti‐Kaul, Gashaw Mamo ChemistrySelect 2017, 2(24):7169-7177\n- “The respective roles of polar/non polar binary patterns and amino acid composition in protein regular secondary structures explored exhaustively using hydrophobic cluster analysis” Joseph Rebehmed, Flavien Quintus, Jean-Paul Mornon, Isabelle Callebaut Proteins: Structure, Function and Bioinformatics 2016, 84(5):624-38\n- “Enzymatic Synthesis of Galatosylated S/T Derivatives by β-Galactosidase from E. coli” Sooyoun Seo, Joseph Rebehmed, Alexandre G. de Brevern, Salwa Karboune International Journal of Molecular Sciences 2015, 16 (6): 13714-13728\n- “Expanding the SRI family of domains: a common scaffold for RNA polymerase II PCTD binding” Joseph Rebehmed, Patrick Revy, Guilhem Faure, Jean-Pierre de Villartay; Isabelle Callebaut. FEBS letters 2014, 588 (23): 4431–4437\n- “PTM-SD: a database of structurally resolved and annotated posttranslational modifications in proteins” Pierrick Craveur, Joseph Rebehmed, Alexandre G. de Brevern. Database 2014 (2014: bau041).\n- “Cis-trans isomerization of omega dihedrals in Proteins” Pierrick Craveur, Agnel Praveen Joseph, Pierre Poulain, Alexandre G. de Brevern, Joseph Rebehmed*. Amino Acids 2013, 45(2): 279-89\n- “Evolution study of the Baeyer-Villiger monooxygenases enzyme family: Functional importance of the highly conserved residues” Joseph Rebehmed, Véronique Alphand, Véronique de Berardinis, Alexandre G. De Brevern. Biochimie 2013, 95(7):1394-402\n- PhD in computational and theoretical chemistry, University Paris Diderot, France, 2009\n- MSc in Bioinformatics, University Paris Diderot, France\n- MSc in Biochemistry, University Saint-Joseph, Lebanon\n- BSc in Biology, University Saint-Joseph, Lebanon\nAssistant Professor of Bioinformatics']	['<urn:uuid:f281027d-73a9-436f-a3e5-5f7ea85122cf>', '<urn:uuid:9ab597e5-0465-4ad5-949a-0a2b8991f9e2>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T17:51:21.175311	8	65	1572
80	How did Epaminondas defeat the larger Spartan army?	At the Battle of Leuctra in 371 BC, Epaminondas defeated 10,000 Spartan forces with just 6,000 men by using innovative tactics. He concentrated his most experienced warriors on his left wing in a 50-rank deep formation to annihilate the Spartan elite troops in the first assault, rather than using the conventional thin-line formation.	['Table of Contents\nAncient Greece was the cradle of some of the most important leaders of Western Civilization. By revisiting their accomplishments, we can develop a better grasp of the evolution of Greek history.\nBefore diving into the deep waters of Ancient Greek history, it’s important to know that there are different interpretations of the length of this period. Some historians say that Ancient Greece goes from the Greek Dark Ages, around 1200-1100 BC, to the death of Alexander the Great, in 323 BC. Other scholars argue that this period continues until the 6th century AD, thus including the rise of Hellenistic Greece and its fall and transformation into a Roman province.\nThis list covers Greek leaders from the 9th to the 1st century BC.\nLycurgus (9th-7th century BC?)\nLycurgus, a quasi-legendary figure, is credited for having instituted a code of laws that transformed Sparta into a military-oriented state. It’s believed that Lycurgus consulted the Oracle of Delphi (an important Greek authority), before implementing his reforms.\nLycurgus’ laws stipulated that after reaching the age of seven, every Spartan boy should leave their family’s home, to receive military-based education granted by the state. Such military instruction would continue uninterrupted for the next 23 years of the boy’s life. The Spartan spirit created by this way of life proved its value when Greeks had to defend their land from Persian invaders in the early 5th century BC.\nIn his pursuit of social equality, Lycurgus also created the ‘Gerousia’, a council formed by 28 male Spartan citizens, each of whom had to be at least 60 years old, and two kings. This body was able to propose laws but couldn’t implement them.\nUnder Lycurgus’ laws, any major resolution had to be first voted by a popular assembly known as the ‘Apella’. This decision-making institution was made up of Spartan male citizens that were at least 30 years old.\nThese, and many other institutions created by Lycurgus, were foundational to the rise of the country to power.\nSolon (630 BC-560 BC)\nSolon (born c. 630 BC) was an Athenian lawmaker, recognized for having instituted a series of reforms that laid the basis for democracy in Ancient Greece. Solon was elected archon (highest magistrate of Athens) between the years 594 and 593 BC. He then went on to abolish debt-slavery, a practice that had been largely used by wealthy families to subjugate the poor.\nThe Solonian Constitution also granted lower classes the right to attend the Athenian assembly (known as the ‘Ekklesia’), where common people could call their authorities to account. These reforms were supposed to limit the aristocrats’ power and bring more stability to the government.\nPisistratus (608 BC-527 BC)\nPisistratus (born c. 608 BC) ruled Athens from 561 to 527, though he was expelled from power several times during that period.\nHe was considered a tyrant, which in Ancient Greece was a term used specifically to refer to those who obtain political control by force. Nevertheless, Pisistratus respected most Athenian institutions during his rule and helped them to function more efficiently.\nAristocrats saw their privileges reduced during Pisistratus times, including some who were exiled, and had their lands confiscated and transferred to the poor. For these kinds of measures, Pisistratus is often considered an early example of a populist ruler. He did appeal to the common people, and in doing so, he ended up improving their economic situation.\nPisistratus is also credited for the first attempt to produce definitive versions of Homer’s epic poems. Considering the major role that Homer’s works played in the education of all Ancient Greeks, this might be the most important of Pisistratus’ achievements.\nCleisthenes (570 BC-508 BC)\nScholars frequently regard Cleisthenes (born c. 570 BC) as the father of democracy, thanks to his reforms to the Athenian Constitution.\nCleisthenes was an Athenian lawmaker who came from the aristocratic Alcmeonid family. Despite his origins, he didn’t support the idea, fostered by the upper classes, of establishing a conservative government, when Spartan forces successfully expelled the tyrant Hippias (Pisistratus’ son and successor) from Athens in 510 BC. Instead, Cleisthenes allied with the popular Assembly and changed the political organization of Athens.\nThe old system of organization, based on family relations, distributed citizens into four traditional tribes. But in 508 BC, Cleisthenes abolished these clans and created 10 new tribes that combined people from different Athenian localities, thus forming what would come to be known as ‘demes’ (or districts). From this time on, the exercise of public rights would depend strictly on being a registered member of a deme.\nThe new system facilitated interaction among citizens from different places and allowed them to directly vote for their authorities. Nevertheless, neither Athenian women nor slaves could benefit from these reforms.\nLeonidas I (540 BC-480 BC)\nLeonidas I (born c. 540 BC) was a king of Sparta, who is remembered for his notable participation in the Second Persian War. He ascended to the Spartan throne, somewhere between the years 490-489 BC, and became the designated leader of the Greek contingent when the Persian King Xerxes invaded Greece in 480 BC.\nIn the Battle of Thermopylae, Leonidas’ small forces stopped the advance of the Persian army (which is believed to have consisted of at least 80,000 men) for two days. After that, he ordered most of his troops to retreat. In the end, Leonidas and the 300 members of his Spartan guard of honor all died fighting off the Persians. The popular movie 300 is based on this.\nThemistocles (524 BC-459 BC)\nThemistocles (born c. 524 BC) was an Athenian strategist, best known for having advocated for the creation of a large naval fleet for Athens.\nThis preference for sea power wasn’t fortuitous. Themistocles knew that even though the Persians had been expelled from Greece in 490 BC, after the Battle of Marathon, the Persians still had the resources to organize a larger second expedition. With that threat on the horizon, Athens’ best hope was to build a navy powerful enough to stop the Persians at sea.\nThemistocles struggled to convince the Athenian Assembly to pass this project, but in 483 it was finally approved, and 200 triremes were built. Not long after that the Persians attacked again and were roundly defeated by the Greek fleet in two decisive encounters: the Battle of Salamis (480 BC) and the Battle of Platea (479 BC). During these combats, Themistocles himself commanded the allied navies.\nConsidering that the Persians never fully recovered from that defeat, it’s safe to assume that by stopping their forces, Themistocles freed Western Civilization from the shadow of an Eastern conqueror.\nPericles (495 BC-429 BC)\nPericles (born c. 495 BC) was an Athenian statesman, orator, and general who led Athens approximately from 461 BC to 429 BC. Under his rule, the Athenian democratic system flourished, and Athens became the cultural, economic, and political center of Ancient Greece.\nWhen Pericles came to power, Athens was already the head of the Delian League, an association of at least 150 city-states created during the Themistocles era and aimed at keeping the Persians out of the sea. Tribute was paid for the maintenance of the league’s fleet (formed mainly by Athen’s ships).\nWhen in 449 BC peace was successfully negotiated with the Persians, many members of the league started doubting the need for its existence. At that point, Pericles intervened and proposed that the league restore Greek temples that were destroyed during the Persian invasion and patrol commercial sea routes. The league and its tribute subsisted, allowing the Athenian naval empire to grow.\nWith Athenian pre-eminence asserted, Pericles became involved in an ambitious building program that produced the Acropolis. In 447 BC, the construction of the Parthenon began, with the sculptor Phidias being responsible for decorating its interior. Sculpture was not the only art form to flourish in Periclean Athens; theater, music, painting, and other forms of art were promoted as well. During this period, Aeschylus, Sophocles, and Euripides wrote their famous tragedies, and Socrates discussed philosophy with his followers.\nUnfortunately, peaceful times don’t last forever, especially with a political adversary such as Sparta. In 446-445 BC Athens and Sparta had signed a 30-Year Peace treaty, but over time Sparta grew suspicious of the fast growth of its counterpart, leading to the outbreak of the Second Peloponnesian War in 431 BC. Two years after that, Pericles died, marking the end of the Athenian Golden Age.\nEpaminondas (410 BC-362 BC)\nEpaminondas (born c. 410 BC) was a Theban statesman and general, best known for briefly transforming the city-state of Thebes into the main political force of Ancient Greece in the early 4th century. Epaminondas was also distinguished for his use of innovative battlefield tactics.\nAfter winning the Second Peloponnesian War in 404 BC, Sparta started to subjugate different Greek city-states. However, when the time to march against Thebes came in 371 BC, Epaminondas defeated the 10,000 strong forces of King Cleombrotus I at the Battle of Leuctra, with just 6,000 men.\nBefore the battle took place, Epaminondas had discovered that the Spartan strategists were still using the same conventional formation as the rest of the Greek states. This formation was constituted by a fair line only a few ranks deep, with a right wing comprising the best of the troops.\nKnowing what Sparta would do, Epaminondas opted for a different strategy. He gathered his most experienced warriors on his left wing to a depth of 50 ranks. Epaminondas planned to annihilate the Spartan elite troops with the first assault and put the rest of the enemy’s army to rout. He succeeded.\nIn the following years, Epaminondas would continue to defeat Sparta (now allied with Athens) on several occasions, but his death in the Battle of Mantinea (362 BC) would put an early end to the preeminence of Thebes.\nTimoleon (411 BC-337 BC)\nIn 345 BC, an armed conflict for political preeminence between two tyrants and Carthage (the Phoenician city-state) was bringing destruction upon Syracuse. Desperate in this situation, a Syracusan council sent an aid request to Corinth, the Greek city that had founded Syracuse in 735 BC. Corinth accepted to send help and chose Timoleon (born c.411 BC) to lead a liberation expedition.\nTimoleon was a Corinthian general who had already helped to fight despotism in his city. Once in Syracuse, Timoleon expelled the two tyrants and, against all odds, defeated the 70,000 strong forces of Carthage, with less than 12,000 men in the Battle of Crimisus (339 BC).\nAfter his victory, Timoleon restituted democracy in Syracuse and other Greek cities from Sicily.\nPhillip II of Macedon (382 BC-336 BC)\nBefore the arrival of Philip II (born c. 382 BC) to the Macedonian throne in 359 BC, Greeks regarded Macedon as a barbaric kingdom, not strong enough to represent a threat to them. However, in less than 25 years, Philip conquered Ancient Greece and became the president (‘hēgemōn’) of a confederation that included all Greek states, except for Sparta.\nWith the Greek armies at his disposal, in 337 BC Philip began to organize an expedition to attack the Persian Empire, but the project was interrupted one year later when the king was assassinated by one of his bodyguards.\nHowever, plans for the invasion did not fall into oblivion, because Philip’s son, a young warrior called Alexander, was also interested in leading the Greeks beyond the Aegean Sea.\nAlexander the Great (356 BC-323 BC)\nWhen he was 20 years old, Alexander III of Macedon (born c. 356 BC) succeeded King Philip II to the Macedonian throne. Soon after, some Greek states began an insurrection against him, perhaps considering the new ruler to be less dangerous than the last. To prove them wrong, Alexander defeated the insurgents on the battlefield and razed Thebes.\nOnce the Macedonian dominance was reasserted over Greece, Alexander resumed his father’s project of invading the Persian empire. For the next 11 years, an army constituted by both Greeks and Macedonians would march eastward, defeating one foreign army after another. By the time Alexander died at just age 32 (323 BC), his empire stretched from Greece to India.\nThe plans that Alexander had for the future of his rising empire are still a matter of discussion. But had the last Macedonian conqueror not died so young, he would probably have continued expanding his domains.\nRegardless, Alexander the Great is recognized for having considerably extended the limits of the known world of his time.\nPyrrhus of Epirus (319 BC-272 BC)\nAfter the death of Alexander the Great, his five closest military officers split the Greco-Macedonian empire into five provinces and appointed themselves as governors. Within a couple of decades, subsequent divisions would leave Greece at the edge of dissolution. Still, during these times of decadence, the military victories of Pyrrhus (born c. 319 BC) represented a brief interval of glory for the Greeks.\nKing Pyrrhus of Epirus (a Northwestern Greek kingdom) defeated Rome in two battles: Heracles (280 BC) and Ausculum (279 BC). According to Plutarch, the enormous number of casualties that Pyrrhus received in both encounters made him say: “If we are victorious in one more battle with the Romans, we shall be utterly ruined”. His costly victories indeed led Pyrrhus to a disastrous defeat at the hands of the Romans.\nThe expression “Pyrrhic victory” comes from here, meaning a victory that has such a terrible toll on the victor that it’s almost equivalent to a defeat.\nCleopatra (69 BC-30 BC)\nCleopatra (born c. 69 BC) was the last Egyptian queen, an ambitious, well-educated ruler, and a descendant of Ptolemy I Soter, the Macedonian general who took over Egypt after the death of Alexander the Great and founded the Ptolemaic dynasty. Cleopatra also played a notorious role in the political context that preceded the rise of the Roman Empire.\nEvidence suggests that Cleopatra knew at least nine languages. She was fluent in Koine Greek (her mother tongue) and Egyptian, which curiously enough, no other Ptolemaic regent besides her took the effort of learning. Being polyglot, Cleopatra could speak with rulers from other territories without the assistance of an interpreter.\nIn a time characterized by political upheaval, Cleopatra successfully maintained the Egyptian throne for approximately 18 years. Her affairs with Julius Caesar and Mark Antony also allowed Cleopatra to expand her domains, acquiring different territories such as Cyprus, Libya, Cilicia, and others.\nEach of these 13 leaders represents a turning point in the history of Ancient Greece. All of them struggled to defend a particular vision of the world, and many perished in doing so. But in the process, these characters also laid the foundations for the future development of Western Civilization. Such actions are what make these figures still relevant for an accurate understanding of Greek history.']	['<urn:uuid:cf2eb16d-72cb-44eb-8958-2abd02635c88>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	8	53	2450
81	Did Unilever achieve its waste reduction goals by 2020?	No, Unilever did not fully achieve its waste reduction goals. While the company had planned to halve the waste associated with the disposal of its products by 2020, it only managed to reduce it by 34%. The company also fell short of its defined targets regarding greenhouse gas emissions and water usage.	"[""Unilever's Sustainable Living Plan: Putting Sustainability at The Center of Business Strategy\n| Case Code: BSTR624\nCase Length: 16 Pages\nPeriod: 1929- 2021\nPub Date: 2021\nTeaching Note: Available\n| Price: Rs.400\nCountries: United States\nThemes: Socially-responsible Business Practices, Sustainability, Competitive Strategy\nAbstract Case Intro 1 Case Intro 2 Excerpts\nThe case describes how Unilever PLC (Unilever), a multinational corporation, integrated sustainability in its business strategy and aimed to make sustainable living commonplace for billions of people globally. Unilever, of Anglo-Dutch parentage, was formed in 1930. It owned many of the world's consumer product brands in foods, beverages, cleaning agents, and personal care products. Unilever's products extended to over 190 countries around the world. The company brought in Paul Polman (Polman), an outsider, as its CEO consequent to a fall in market share and financial performance and its first ever profit warning in 2004. In 2010, Unilever started a new initiative called the Unilever Sustainable Living Plan (USLP) in response to a major global trends Unilever had identified –growth in emerging markets, increasing population, environmental stress, the digital revolution and changing (urbanization) demographics. Unilever also faced a backlash for environmental destruction. The Unilever SLP initiative covered not just Unilever’s greenhouse gas emissions, waste and water use – but the impact caused by its suppliers and consumers, from agricultural growers to its packaging and waste water produced by consumers of Unilever brands. In 2020, Unilever celebrated the final year of its 10-year USLP. The Unilever SLP strategy was not successful in all sectors. The company had planned to halve the waste associated with the disposal of its products by 2020, but it managed to reduce it by only 34%. The company also could not reach to its defined target regarding GHG emissions and water usage. In its endeavor to make the company sustainable and achieve all its SLP targets, the company faced many challenges. The question experts were asking was how Unilever would be able to make sustainable living commonplace for the world’s 8 billion people and continue to be the leader in sustainable business worldwide.\nThe case is structured to achieve the following teaching objectives:\n- Explore how Unilever grew its business over the years globally\n- Examine Unilever’s sustainable living plan initiative.\n- Discuss how Unilever was making sustainable living commonplace globally\n- Analyze how Unilever was tackling social inequality and the climate crisis globally.\n- Demonstrate the need for courageous leadership and commitment to a sustainable business in building a better society.\n- Create a strategy for Unilever to achieve its aim of making sustainable living commonplace for the world’s population and retain its leadership position in sustainable business.\nThe Birth of Unilever\nUnilever Brand and Global Expansion\nPath to Growth Strategy 2000-2010: Growth Through Acquisition\nUnilever’s Sustainable Living Strategy\nUnilever Sustainability Program\nUnilever SLP Program Impact\nUnilever, Sustainability, USLP, Business strategy, Sustainable Living, Responsible competitiveness, Sustainable Business, Paul Polman, Competitive Strategy, Environment ,Climate Change , Triple bottom line, Sustainable Livelihoods\nBuy this case study (Please select any one of the payment options)\n||Price: Rs.400||PayPal (9 USD)""]"	['<urn:uuid:70e5f724-ac49-4fb6-b40d-5ffba7e74468>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	9	52	509
82	what makes red wine red color	The anthocyanin pigment in the dark skin of the grape provides the red color. The red color is determined by the amount of contact the clear juice has with the skin	['There’s just so much more to your wine than just a name!\nFirst, a little story. I was with a friend who is a real muscle car geek. This man knows every make, has details about every model, knows things about particular years that boggles the mind, and I think has either owned or ridden in just about every one of them.\nThat’s why I was a little amused when he almost got whiplash and craned his neck to watch what appeared to me an older blue car fly by going in the other direction.\n“Oh, that’s such a sweet ride!” he exclaimed like a teenager.\n“What was it?” I asked.\nHe had such a big grin. “A 1967 Camaro Rally Sport hardtop – with original black stripes! In metallic blue.”\nI’m always appreciative of other people’s passions. That’s how I get about wine.\nWhat for one person is “just another rosé” – to me, is a whole world of detail.\nThere’s a difference between what is recognized as Old World Wine and what is New World. New World Wine comes from regions where winemaking and the Vitis vinifera grape was exported from Europe during the Age of Exploration (roughly 1500 through the very early 1800s). The Americas, South Africa, Australia, and New Zealand are New World – everywhere else – Old World. Old World rosé tends to be bone-dry while New World can be almost sweet, fruitier.\nAnother thing about rosé that surprises most people is that it starts off white. Almost all red skinned grapes – like pinot noir, cabernet sauvignon, zinfandel – have white to “off green” flesh, and the squeezed juice is clear. So, what makes them red? The anthocyanin pigment in the dark skin of the grape. The red color or pinkness (and the flavor associated with the finished rosé) is determined by the amount of contact the clear juice has with the skin.\nThere are basically four ways to make rosé wine:\n- First, there’s something called Bleeding or Saignée (usually the best quality rosés are made with this method)– the grapes are stacked in a tank and the weight of the grapes actually does the crushing. Some of the juice is “bled-off” into another tank after limited contact with the skins making this the palest in color of the rosés. The rest is kept in the tank for making red wines.\n- Pressing or pressé where red skinned grapes are pressed until the desired color is reached at which point the winemaker stops pressing.\n- Limited Maceration – which is the most common technique – leaves the juice in contact with the skins, seeds, and stems. Usually, this goes on for no more than two or three days until the juice is the color the winemaker wants at which point the juice (without all those seeds, stems and skin) is transferred to another tank to finish the fermentation.\n- Finally, there’s the Run Off method where the winemaker removes some of the juice of fermenting red wine and pours it into a separate tank. By doing this, the winemaker can make a red wine that’s a bit darker and more intense.\nRosé is typically drunk when it’s very young – 1-3 years old. So, what’s the best rosé to serve for a dinner party? It all depends on what you’re serving – the drier the wine the easier it is to pair with salads, vegetables and grilled proteins. The sweeter the rosé the better it would be with dessert or enjoying the sunset. Rosé is an ideal wine to enjoy all year long but particularly in the summer/warmer months.\nIf you like a drier Old World rosé, then pick up a bottle of Miraval Rosé, from the Chateau Miraval in Provence, France. The nose (aroma) can be a bit sweet with strawberries and raspberry notes but because of the different grapes it’s blended with (Grenache, etc.) it is slightly acidic on the palate. It would pair well with raw salmon, tuna (like a tartare or sushi) or something similar to a Niçoise Salad.\nAnother Old World rosé called Pive (Pea-vé) is from the JeanJean winery in the wilderness of the Camargue national park France and is organically farmed. This one tends to be a bit more aromatic – strawberry, raspberry fruit, some earth, spice and mineral characters but is bone dry and very fresh. This would be great with BBQ and grilled meats as well as fish/shellfish. It’s a really great summer picnic wine.\nThe Brut Rosé from Gruet is one of my favorite sparkling wines when I want something a bit sweeter. It’s from the New World – New Mexico – with floral and berry aromas and flavors of cherry, raspberry and wild strawberry with a delicate acidity on the finish. It goes well with a chilled salad!\nI always recommend that you talk to your wine merchant and ask questions. Let them know the wines that you like to drink – and what you’re planning for a meal. It’ll help them pick the right rosé for you.\nAnd don’t forget keep your rosé chilled and – if it’s a party – buy magnums!']	['<urn:uuid:7c4e262d-ac38-482e-84b9-c3314188fc0b>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T17:51:21.175311	6	31	862
83	native american court system rules rights	In tribal courts, defendants have constitutional protections including right to speedy trial, counsel, and protection against self-incrimination. Sentences are limited to three years per offense. For non-drug offenses, defendants must have specific ties to the tribe such as living on tribal lands or being employed by the tribe. Additionally, as shown in the Lewis v. Clarke case, questions about sovereign immunity and jurisdiction between tribal and state courts remain complex legal issues.	['Testers Tribal Criminal Justice Reform Bill Receives Senate Hearing\n(U.S. Senate)-Senator Jon Tester’s legislation that will restore criminal prosecution jurisdiction to Native American tribes and address the growing drug epidemic in Indian Country received a hearing yesterday in the Senate Indian Affairs Committee.\nTester’s bill, the Tribal Youth and Community Protection Act, will reestablish the ability for tribes to arrest and prosecute any offender, including non-Indians, for drug related crimes, domestic violence against children, and crimes committed against tribal law enforcement officers.\n“Restoring tribal criminal jurisdiction will increase self-determination and better allow tribes to protect their communities from folks who cause harm,” Tester said. “This bill provides tribes the certainty to arrest, prosecute, and convict criminals who traffic illegal drugs and harm women and children in Indian Country.”\nDuring the hearing, Tester’s bill was endorsed by Fort Peck Council Member Dana Buckles. The Assiniboine and Sioux Tribes of the Fort Peck Reservation recently finished a two year pilot program that allowed them to prosecute non-Indians for intimate partner domestic violence crimes committed on their Reservation.\n“Our tribal law enforcement and our courts are the resources that are working on the ground every day in our community. They know the victims and they know the perpetrators. They do not have the same structural barriers to prosecuting these difficult cases encountered by the U.S. Attorney,” Councilman Buckles said. “In order to respond to this tide of violence, Congress must empower tribes by recognizing our inherent jurisdiction to prosecute non-Indians who commit crimes against our children and bring drugs into our communities.”\nIn Montana, the Northern Cheyenne, Blackfeet, and Fort Belknap Tribes have all recently declared states of emergency due to the increase in drug related crimes on their reservations.\nCurrently, many criminals committing drug offenses or crimes against children in Indian Country can only be arrested and prosecuted by state or federal law enforcement officials due in part to the varying level of authority, proximity, and capacity between state, federal, and tribal law enforcement.\nIn 1978, the Supreme Court decision, Oliphant v. Suquamish Indian Tribe, limited tribes’ criminal jurisdiction, gravely impacting tribes’ ability to administer justice in Indian Country.\nIn 2013, Congress passed legislation that restores tribes’ authority to arrest and prosecute any offender for acts of domestic violence against their partner, but the law does not protect children and other tribal members.\nTester’s Tribal Youth and Community Protection Act is available HERE.\nFacts on the Tribal Youth and Community Protection Act:\n*In tribal court, sentences are limited to a maximum of three years per offense, and multiple sentences can be stacked on one another.\n*If a non-Indian defendant is found guilty, they would serve jail time in a tribal correctional facility that has been approved by the Bureau of Indian Affairs. Tribes are also free to enter into agreement with regional detention facilities that could be located off a reservation-in which case the guilty offender would serve time in that facility.\n*All defendants suspected of drug crimes on tribal lands will have to appear in tribal court.\n*The Tribal Youth and Community Protection Act builds on existing federal law that describes the ties a defendant must have to the tribe for the tribal court to hear a case that is not a drug offense.\n*Individuals suspected of committing crimes against children will need to have ties like those currently applicable to domestic violence crimes, which include:\n-Living on the tribe’s lands, or\n-Being employed by the tribe, or\n-Having a relationship (as defined in federal law) with a tribal member or Native American living on the tribe’s lands.\n*The Tribal Youth and Community Protection Act maintains current federal law, which requires tribal courts to provide constitutional protections to defendants when exercising criminal jurisdiction related to the bill.\n-These protections provide a check on controversial or uncertain charges against a suspect/defendant.\n-Additionally, if a tribal court issues a controversial decision, a defendant can request that a federal court review the legality of his or her detention.\n*All non-Indian defendants in tribal court will have the same constitutional protections as they would have in federal court, including:\n-Right to a speedy and public trial.\n-Right to counsel.\n-Right to not incriminate oneself.', 'Does a lawsuit against a tribal employee for an act he committed within the scope of his employment by the tribe violate tribal sovereign immunity?\nWith Lewis and Clarke, the Supreme Court will venture into the relatively unfamiliar legal territory of tribal sovereign immunity for individuals employed by Indian tribes. The case arises out of an automobile accident between Brian and Michelle Lewis and William Clarke, an employee of the Mohegan Sun Casino, which is owned by the Mohegan tribe. In a lawsuit brought by the Lewises, Clarke successfully convinced the Connecticut Supreme Court that he was entitled to tribal sovereign immunity. The Lewises argue that sovereign immunity does not apply when a tribal employee is sued in his individual capacity because the finances of the tribe are not formally at risk. Clarke counters that the finances of the tribe are at risk in this suit, and thus, the sovereign immunity of the tribe should extend to him because he was acting within the scope of his tribal employment. To some, the voyage of Lewis and Clarke into the obscure realm of tribal sovereign immunity for individuals imperils tribal coffers; to others, the regulatory power of the states is at stake.\nQuestions as Framed for the Court by the Parties\nWhether the sovereign immunity of an Indian tribe bars individual-capacity damages actions against tribal employees for torts committed within the scope of their employment.\nLewis and Clarke’s five-year voyage to the Supreme Court began on October 22, 2011 with the chance encounter of Brian and Michelle Lewis (“Lewis”) and William Clarke (“Clarke”) in Norwalk, Connecticut. See Lewis v. Clarke, 135 A.3d 677, 679 (Conn. 2016). At the time, Clarke was an employee of the Mohegan tribe and was responsible for transporting patrons of the Mohegan Sun Casino in a limousine to their homes. See id. While Clarke was thus navigating Interstate 95, he collided with Lewis’s vehicle, propelling Lewis onto a concrete barrier on the left side of the highway. See id. Thereafter, Lewis set out to sue in the Connecticut Superior Court, alleging the accident was a tort as it occurred due to Clarke’s negligence and carelessness. See id.\nClarke sought to dismiss the lawsuit, asserting a right to tribal sovereign immunity and claiming that he could not be sued because he was an employee of the Mohegan tribe. See Lewis, 135 A.3d at 679. According to Clarke, Lewis should have sued in the Mohegan tribe’s Gaming Disputes Court, which the tribe established pursuant to an agreement with the state of Connecticut for the purpose of deciding tort claims arising out of the tribe’s gambling activities. See Brief for Respondent, William Clarke at 4–5. The Superior Court denied Clarke’s motion to dismiss, concluding that tribal sovereign immunity did not extend to Clarke as an individual. See Lewis, 135 A.3d at 679–80. According to the Superior Court, sovereign immunity would have applied had Lewis sued Clarke as an official of the Mohegan Tribal Gaming Authority. See id. The Superior Court held that, because Lewis sued Clarke seeking compensation not from the Mohegan tribe but from Clarke personally, tribal sovereign immunity did not bar this suit. See id. Clarke disagreed with the Superior Court’s holding and so journeyed onward via appeal to the Connecticut Supreme Court. See id.\nOn appeal, the Connecticut Supreme Court reversed course. See Lewis, 135 A.3d at 685–86. As the U.S. Supreme Court had yet to chart the boundaries of tribal sovereign immunity, the Connecticut Supreme Court relied on guidance from lower federal courts. See Brief for Respondent, William Clarke at 8. Based on the decisions of those courts, the Connecticut Supreme Court concluded that tribal sovereign immunity bars Lewis’s suit, solely because Clarke was an employee of the Mohegan Tribe and was performing an official duty. See id. Lewis was not content with ending this legal journey there and petitioned the U.S. Supreme Court for a writ of certiorari, which the Court granted on September 29, 2016. See id. With this case, the Supreme Court must now demarcate the boundaries of tribal sovereign immunity.\nDOES SOVEREIGN IMMUNITY APPLY WHEN AN INJURED PARTY SUES A TRIBAL EMPLOYEE IN HIS INDIVIDUAL CAPACITY?\nLewis maintains that applicability of sovereign immunity is based upon whose assets are at risk in a suit, as determined by state and federal law. See Brief for Petitioners, Brian Lewis et al. at 9, 24. For example, if an injured party chooses to sue a government employee in his official capacity, then the government’s assets are at risk in the suit, and the doctrine of sovereign immunity would bar the suit. See id. at 8–9. If, on the other hand, an injured party chooses to sue a government employee in his individual capacity, then it is the employee’s own assets that are at risk, and the doctrine of sovereign immunity does not bar the suit. See id. Lewis illustrates this distinction by noting that, despite sovereign immunity protecting the federal and state governments from suit, federal and state employees can still be sued in their individual capacities in actions such as Bivens claims and Section 1983 suits. See id. at 13–14, 18–19. Lewis argues that the sovereign immunity doctrine should apply in the same manner for tribal sovereign immunity cases. See id. at 21. Additionally, Lewis states that a tribe cannot alter the sovereign immunity doctrine of state and federal courts by covering any damages held against tribal employees acting within their scope of employment. See id. at 24. Thus, despite the Mohegan Tribal Code’s indemnification of its employees for damage awards, Lewis argues a suit should be allowed so long as the injured party sues the employee in his individual capacity. See id.\nClarke counters with a different interpretation of the sovereign immunity doctrine. See Brief for Respondent, William Clarke at 14. He states that the analysis for the immunity of a tribal employee takes a different path from the analysis for a state or federal government employee, noting that previous lower court tribal immunity cases have applied sovereign immunity and official immunity almost interchangeably. See id. at 12–14. Further, Clarke argues that in the tribal immunity context, the relevant inquiry for immunity is not whether an employee is being sued in an official or individual capacity, but whether he was acting in his official capacity at the time he caused an injury. See id. at 14. If a tribal employee was acting within the scope of his employment at the time of the injury, Clarke argues that the inquiry then turns to whose assets are at risk in the suit. See id. According to Clarke, if a tribe’s assets are at risk, then the doctrine of sovereign immunity bars the suit in State or Federal court. See id. Clarke maintains that this suit should be barred on grounds of sovereign immunity because the Mohegan Tribal Code indemnifies damages against its employees for actions performed within the scope of their employment. See id. at 15–16, 20. Clarke argues that it should make no legal difference whether the tribe is obligated to pay damages based on state law or tribal law, because Supreme Court precedent has indicated that sovereign immunity applies to cases in which the sovereign is not formally held liable but is held liable “in effect” because it is ultimately responsible for paying the damages. See id. at 16–17. Further, Clarke states that the Court has treated the application of sovereign immunity to state and tribal instrumentalities as a functional question of who is ultimately responsible for paying the damages, based on state or tribal indemnity laws. See id. 17–18, 21.\nDOES OFFICIAL IMMUNITY APPLY WHEN AN INJURED PARTY SUES A TRIBAL EMPLOYEE IN HIS INDIVIDUAL CAPACITY?\nLewis argues that the Court should not explore whether official immunity applies in this case. See Brief for Petitioners at 16, 22. Lewis states that Clarke did not make the official immunity argument and that the argument goes beyond the question before the Court. See id. at 22. Further, Lewis notes that the Court has never extended official immunity to tribal officials. See id. According to Lewis, the official immunity doctrine is ill-suited to the context of tribal employment because the tribal context does not implicate the supremacy concerns present in the federal context. See id. at 22–23. Lewis also notes that official immunity is typically only applied in cases in which a government employee was acting within their discretion, which would not apply in this case. See id. at 17, 19–20, 23.\nClarke responds that the Court should extend the common-law doctrine of official immunity that covers federal employees to the context of tribal employees. See Brief for Respondent at 26–28. First, he argues that the Court can consider official immunity in this case because it has been treated as interchangeable with sovereign immunity in tribal immunity cases and played a critical role in the outcome of the decision below. See id. at 27–28. Clarke maintains that tribal officials, like government officials, require official immunity in order to effectively carry out their duties. See id. at 29. In fact, he argues that the need for protection in carrying out official duties is even stronger in the context of an Indian Tribe. See id. at 42. Further, Clarke states that this immunity should not be limited to cases in which the tribal official was acting within his discretion, because Congress has rejected such a distinction for government employees in immunity cases. See id. at 30–32, 36–37. Clarke notes that a tribe’s sovereignty is most akin to that of foreign sovereigns, whose officials are granted complete official immunity. See id. at 43. Clarke also argues that refusing to extend immunity in this context would uniquely disadvantage tribal employees because they would not be eligible for the same protections as are afforded to the employees of all other governments. See id. at 45–46.\nSTATE’S POWER TO REGULATE VS. TRIBE’S POWER TO GOVERN\nClarke asserts that a narrow interpretation of tribal sovereign immunity will hinder effective tribal governance. See Brief for Respondent, William Clarke at 42. According to Clarke, if a tribe’s employees are not afforded sovereign immunity, they may become paralyzed by the fear of a lawsuit and avoid discretionary action. See id. Clarke posits that such a result would cripple a tribe’s ability to govern effectively. See id. Additionally, Clarke asserts that prospective employees would steer clear from tribal employment if potentially bankrupting liability was at stake. See id. Clarke thus cautions the Supreme Court to weigh the tribe’s ability to effectively govern when charting the boundaries of tribal sovereign immunity. See id.\nThe United States, as amicus in favor of reversal, agrees with Clarke’s conclusion but disagrees as to the method. See Brief of Amicus Curiae the United States, in Support of Reversal at 26. According to the United States, the common law doctrine of official qualified immunity is more appropriate in this context. See id. The United States urges the Court to send the case back to the Connecticut Supreme Court to reconsider it under the doctrine of qualified immunity. See id.\nLewis foresees an alternate problem, asserting that broadly delineated tribal sovereign immunity intrudes upon States’ regulatory authority. See Brief for Petitioners, Brian Lewis et al. at 6, 20. According to Lewis, states utilize tort actions as a tool to advance their regulatory scheme. See id. at 27–28. Lewis cautions that should Clarke prevail, the result would weaken the regulatory power of the States. See id.\nClarke disagrees, asserting that the State of Connecticut’s regulatory powers are adequately preserved. See Brief for Respondent at 53–54. According to Clarke, Connecticut had an opportunity to negotiate for a waiver of sovereign immunity with the tribe, but chose not to do so. See id. Clarke thus argues that Connecticut’s power to regulate would not be diminished. See id.\nTRESPASSING ON THE DIGNITY OF SEMI-SOVEREIGN TRIBES\nAccording to Clarke, establishing narrow boundaries on tribal sovereign would constitute a trespass on the dignity of the tribes as “separate sovereigns pre-existing the Constitution.” Brief for Respondent at 43–44. Clarke asserts that inasmuch as law treats foreign dignitaries and officials with sovereign immunity, so it must afford the same to the tribes, which have long been recognized as “‘retain[ing] their historic sovereign authority’ unless Congress abrogates it.” Id. at 44.\nLewis, however, finds no such concern. See Brief for Petitioners at 23. According to Lewis, the framers of the Constitution devised sovereign immunity to ensure that States would not suffer the humiliation of appearing as parties in federal courts. See id. Lewis concludes that as the present lawsuit was filed against an individual, it would not force the tribe itself to appear in American courts. See id. Thus, according to Lewis, sovereign immunity concerns are not implicated here. See id.\nTHREATENING THE WEALTH OF THE TRIBES\nClarke also asserts that a narrow delineation of tribal sovereign immunity would threaten the financial integrity of a tribe. See Brief for Respondent at 50. According to Clarke, with sovereign immunity, tribes would be allowed to set their own rules of recovery, allowing them to avoid potentially ruinous liability. See id.\nIn response, Lewis asserts that the present lawsuit poses no danger to the financial security of the tribe. See Brief for Petitioners at 23–24. According to Lewis, the only reason the Mohegan tribe may suffer is because it agreed to cover the damages awarded against Clarke. See id. Lewis thus concludes that the tribe could avoid any potential liability by simply refusing to indemnify its employees. See id.\nINJUSTICE TO THE VICTIMS\nIn support of Lewis, the Connecticut Trial Lawyers Association and American Association for Justice (“Associations”) assert a verdict for Clarke could leave many injured parties without a remedy. See Brief for Connecticut Trial Lawyers Association et al., in Support of Petitioner at 5. Specifically, the Associations assert that barring Lewis’s suit would foreclose individuals injured by other tribal employees from having an opportunity to be heard in state and federal courts. See id. at 15. Lewis likewise repeats this concern, noting that it is inherently unfair to bar injured parties from American courts solely because of the employment status of their assailant. See Brief for Petitioners at 28–29.\nClarke challenges the Associations’ concerns as inconsistent with the facts of the case. See Brief for Respondent at 54–55. Clarke notes that the Lewis family would not be left without a remedy even if tribal sovereign immunity barred their suit in Connecticut courts. See id. According to Clarke, the Lewis family could have sued in the Gaming Disputes Tribal Court, to which other parties that were injured in the same accident appealed and received substantial compensation. See id. Thus, Clarke asserts that injured parties would not be left without a remedy even if the Court were to extend tribal sovereign immunity in federal courts to employees like Clarke. See id.\n- Matt Ford, Lewis and Clarke Get Their Day in Court, The Atlantic (Sep. 29, 2016).\n- Stella Shannon, Stella on SCOTUS: Lewis v. Clarke, The Politic (Oct. 11, 2016).\n- Andrew Westney, Justices May Open Tribal Immunity Loophole in Mohegan Suit, Law 360 (Sep. 30, 2016).']	['<urn:uuid:cb45de82-69b5-4921-95cb-7c28cf156d93>', '<urn:uuid:3a2431e6-c2c8-466b-9eae-5a31dc91b1da>']	factoid	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T17:51:21.175311	6	72	3222
84	As an environmental researcher studying forest preservation, I'm curious about the dual impact of cattle ranching in Central America - what's its role in current deforestation, and how does this affect global climate change efforts?	Cattle ranching is currently the primary driver of deforestation across Central America, especially within protected areas. In regions like Honduras' Río Plátano Biosphere Reserve, over 95% of clearing is for cattle, often used as an excuse for land grabbing. This deforestation has significant climate implications, as deforestation produces 10% of global carbon dioxide emissions. However, stopping deforestation and improving forestry practices could remove 7 billion metric tons of carbon dioxide annually, equivalent to eliminating more than all cars in the world today.	"['- The Megaflyover is focusing on the five largest forest blocks still standing in Mexico and Central America.\n- According to NASA’s Earth Observatory, the single biggest direct cause of tropical deforestation is conversion to cropland and pasture, “mostly for subsistence, which is growing crops or raising livestock to meet daily needs.”\n- The flyover researchers are sharing the findings with national protected areas agencies in the region as they go, and overtime, they will analyze how human influence has expanded over this region over the past 15 years.\nIf you were to fly over Central America on a small plane, from north to south, starting in Guatemala and ending in Panama, you’d be able to see isolated yet extensive pockets of green canopy covering the territory.\nThis is exactly what the aptly named “Central American Megaflyover” has been able to document during its 3,100-mile flight that took off on February 26th and is set to end by the middle of March. An additional 1,900 miles will be traveled overland.\nThe crew includes researchers with the Wildlife Conservation Society (WCS) in collaboration with the U.S. Fish & Wildlife Service, a government agency with a history of conservation outside of America’s borders. The U.S. Fish and Wildlife Service has been working for years with a variety of partners and communities in Guatemala’s Maya Forest in Guatemala to help conserve the forest.\nThe Megaflyover is focusing on the five largest forest blocks still standing in Mexico and Central America: the Maya Forest that straddles Mexico, Guatemala, and Belize, Honduras and Nicaragua’s Moskitia, Indio Maiz-Tortuguero National Park in Nicaragua and Costa Rica, La Amistad International Park in Costa Rica and Panama, and Panama’s Darién Gap. All of these forested areas share a wide variety of plant and animal species, and are home to numerous indigenous groups.\nThe goal of the expedition has been to witness and study the impact of humans, and in particular, their livestock. The findings will help inform conservation initiatives in the region.\nDr. Jeremy Radachowsky, Director for WCS’s Mesoamerica and Western Caribbean Program, says that the team expected to find the usual and already-documented impacts of cattle ranching in all countries. But so far, the researchers have also come across new developments, especially in Honduras’ Río Plátano Biosphere Reserve on the Caribbean coast.\n“Very surprising was the percent of deforestation in Río Plátano for cattle ranching,” says Radachowsky. “I would estimate that more than 95% of clearing is for cattle, and just tiny household plots for family gardens. Also surprising is the extremely low number of cattle in the cleared pastures. Cattle are clearly being used as an excuse for land grabbing.”\nThe reserve is part of the Mesoamerican Biological Corridor that stretches from Mexico to the southern tip of Central America, following the path of the Megaflyover itself. Río Plátano is home to a number of endangered species such as the Mantled howler monkey (Alouatta palliata), the Great Green Macaw (Ara ambiguus), and the Loggerhead sea turtle (Caretta caretta); in 2011 UNESCO placed the Río Plátano Biosphere Reserve on the List of World Heritage Sites in Danger.\nAccording to NASA’s Earth Observatory, the single biggest direct cause of tropical deforestation is conversion to cropland and pasture, “mostly for subsistence, which is growing crops or raising livestock to meet daily needs.”\nThe WCS findings so far agree: cattle ranching is the primary driver of deforestation across the entire region, especially within protected areas. Then follows deforestation caused by oil palm plantations, usually just outside or at the edge of the protected areas.\nRadachowsky explains how these factors typically work on the ground: Since the price of beef is almost $5 a pound in much of Central America, cattle production in protected areas is “typically not for consumption by local people or an important part of food security, but rather a business proposition by wealthy individuals.”\n“In many cases, cattle ranching in protected areas is really driven by land speculation or for money laundering, with hopes of eventually obtaining title to the already degraded lands,” explains Radachowsky. “Much of the cattle produced in Central America’s protected areas ends up as contraband, with cattle from Belize, Guatemala, Honduras, and El Salvador shipped to Mexico.”\nDeforestation caused by cattle ranching is especially rampant in Guatemala, Honduras, Nicaragua, and parts of Panama, he adds.\nThe Central American Megaflyover team members are taking detailed, high-resolution images they will compare with previous satellite imagery that was analyzed by WCS in 2002 to measure the extent of human influence across the earth’s land surface. The flyover researchers are sharing the findings with national protected areas agencies in the region as they go, and overtime, they will analyze how human influence has expanded over this region over the past 15 years.\n“Anecdotally and through our own experiences, we have observed the widespread impact of cattle ranching on Central America’s forests,” says Radachowsky. “Now we will be able to unequivocally and scientifically demonstrate these patterns to the world.”', 'The new few screens will show you how you can change the world.\nRainforests are powerhouse ecosystems that support a healthy Earth. Characterized by high levels of rainfall, an enclosed canopy, and high species diversity, rainforests enable life to thrive. They produce clean air and water, absorb our carbon dioxide, stabilize climate patterns, and contain half of all plant and animal species . Most importantly, rainforests are a crucial defense against climate change. You cannot chop or burn them down without running large climatic risks.\nYet, that is exactly humanity is doing. Every second, we lose a football field of rainforest. Deforestation produces 10% of global carbon dioxide emissions that cause climate change. However, stopping deforestation, restoring forests, and improving forestry practices could remove 7 billion metric tons of carbon dioxide annually, or as much as eliminating 1.5 billion cars—more than all of the cars in the world today! The question we must ask ourselves is “People do not burn Picassos or cathedrals, so why should they burn the Amazon Rainforest?”\nRainforests are being destroyed because they are perceived by their short-term value by short-sighted governments, companies, and landowners. We need to realize the true, long-term value of these forests and the impact their loss would have. Massive deforestation has many consequences – air and water pollution, soil erosion, less rainfall, the release of carbon dioxide into the atmosphere, the displacement and decimation of indigenous tribes, and the extinction of plants and animals. With all this in mind, what can be done to prevent this catastrophe?\nPrioritize. Solve. Activate.\nIt\'s time. The world needs you. Your ideas, your courage and your action. Change can begin with YOU.\nProtecting Earth’s Rainforests\nWhat’s Happening to the World’s Rainforests\nIn 1950, about 15 percent of the Earth\'s land surface was covered by rainforest. These areas took between 60 and 100 million years to evolve and are believed to be the oldest and most complex land-based ecosystems on Earth, containing over 80% of Earth’s land based species.\nYet in fewer than fifty years, more than half of the world\'s tropical rainforests have disappeared. Tropical rainforests now cover just around 5 percent of the world\'s land surface. Due to fragmentation and degradation by humans, much of this remaining area no longer retains its original biodiversity.\nIn 2019, deforestation in the Amazon climbed to the highest rate in a decade. An area of forest the size of Lebanon was burned in just a few months in the Brazilian Amazon. Roughly 20 percent of the Amazon Rainforest is already gone, and scientists say we are close to a tipping point. The Amazon is a closed loop system that waters itself. If enough forest is destroyed, the world’s largest rainforest could see a die-back scenario, where large swaths of once wet forest turn into a dry savanna and the whole system collapses. We need to put a stop to this needless destruction.\nWhy Are We Losing Rainforest?\nThe fundamental factors driving deforestation are economic — at both the global and local level.\nUnsustainable agriculture, illegal logging, mining, oil drilling, forest fires, dams, poaching, and urban development are all wiping out rainforests and the life within them. The production of agricultural commodities such as beef, soy, palm oil, and cocoa is responsible for the majority of global deforestation.\nPolitical will, incentives, and policy also play a massive role. The current lack of recognized land tenure for indigenous territory, economic opportunities, and enforcement of environmental regulations makes it difficult for rainforest communities to keep their forest standing.\nPeople who live in forest communities need a basic income–particularly for education and health care. Many have found that the surest and simplest means of making money is to cut down the trees and replace them with cash crops and pasture land.\nWithout alternatives, we can all understand why they would make this choice. Unfortunately, this practice degrades or destroys the environmental value of the forest, and due to the poor soil of rainforest land, it is not economically sustainable.\nIn addition, large agribusiness and resource companies will buy land or offer these communities large sums of money to extract resources from their land. Unlike local communities, these large companies have no connection to the land and nothing to gain from the forests besides short-term profit.\nA Growing Understanding\nFortunately, the world\'s growing understanding of the value of our rainforests is coming at the same time as the discovery of ways to build economies that rely on healthy forests for success. Sustainable agriculture methods like agroforestry allow cattle and crops to grow while simultaneously preserving the surrounding forest. Using techniques like this, products such as coffee and cacao are transforming from deforestation culprits into sustainable products. Services like ecotourism and traditional medicine provide an income to communities without harming the forest.\nThese examples show the forest as a sustainable resource rather than being a single-use mine for timber, oil, minerals, or unsustainable agriculture.\nRainforests also have an enormous amount of potential for medicine, and only 1% of rainforest plant species have been tested for medicinal benefits. Unfortunately, many of these indigenous communities with knowledge of medicinal plants have historically been exploited, and are reluctant to pass on their knowledge if they feel they might be taken advantage of again. Building a trusting relationship with these communities will be vital toward long-term conservation goals.\nWe also now know that investing in indigenous peoples is one of the most cost effective solutions to the climate and biodiversity crises. According to WRI, “Research shows that indigenous and community lands store about 25 percent of the world’s aboveground carbon, making these lands critically important in the global fight to curb climate change.” Indigenous peoples constitute less than five percent of the world’s population, but they safeguard 80 percent of the world’s biodiversity. We have to invest in them to protect our environment.\nA Chance for Regrowth\nRainforest ecosystems are resilient, and if we give them a chance they will thrive. Forests need to be managed effectively without endangering rare species of plants and animals and without risking global environmental damage. Companies that harvest timber should not be allowed to ""clear cut"" large areas of forest and should be required to plant new trees after they cut old trees down. Governments also need to create reserves where hunting and harmful resource extraction are not allowed.\nGovernments are partnering with indigenous peoples in reserve management and with NGOs that provide legal assistance to help them secure their land. Additionally, NGOs are working with indigenous communities to create sustainable economies that rely on thriving forests. Indigenous people know more about the forest than anyone and have an interest in safeguarding it as a productive ecosystem that provides them with food, shelter, and clean water.\nOne way we can contribute to conservation efforts in our own lives is by cutting out products made by companies that contribute to the destruction of the rainforest. Instead, we can support companies and programs that make a commitment to a zero-deforestation supply chain.\nA key strategy in conserving rainforests is aiding indigenous peoples and local communities in gaining formal title to their land. According to the World Resources Institute, “...deforestation is significantly lower within formally recognized indigenous-held lands. A recent WRI report found that in Bolivia, deforestation rates are 2.8 times lower within “tenure-secure” indigenous lands… than outside them.”\nIn places where it is difficult to gain ownership rights to land and where land is relatively open and abundant, there is little incentive to maintain or improve holdings. Once local people have a stake in the land they are farming, they will have an interest in using it efficiently instead of moving on to a new area of forest once soils are exhausted.\nEcotourism can also raise awareness and funds for conservation efforts. Areas in and around protected areas can charge a daily fee to visitors which goes toward supporting the forest.\nBuilding research facilities for training local scientists and guides can boost intellectual capital and introduce a new dynamic to an economy once based on resource extraction. Unlocking the value of forests provides a great opportunity for a country to capitalize on its natural assets. Through this, we can establish programs that promote sustainable use which are key to elevating the standard of living for people living around protected areas.\nWe need to universally implement these methods to protect our planet’s greatest resource.\nA Future for the Rainforest\nAs a major source of freshwater, biodiversity, carbon removal, and medicinal ingredients, it is of paramount importance that we protect this vital resource. With the combined efforts of conservation NGOs, sustainable businesses, indigenous peoples and local communities, and governments around the world, and global citizens, we can breathe new life into our rainforests. If we protect our environment we will reap the benefits for years to come.']"	['<urn:uuid:250eef42-0067-4987-9227-7c62df578279>', '<urn:uuid:6f534929-b4de-4bb8-a653-a42c85929f42>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	35	82	2301
85	I research ancient calendars and noticed both the Maya and Romans used interesting naming conventions for months. Could you compare how these civilizations named their calendar months?	In the Maya calendar, the months had specific names from the Yukatek list, such as Pop, Wo, Sip, and others, totaling 19 months including Wayeb. The Roman calendar initially had months named both numerically (like Quintilis for fifth month, Sextilis for sixth month) and after deities (March named after Mars, May after Maia, June after Juno). Later, some month names were changed to honor emperors - Quintilis became July (after Julius Caesar) and Sextilis became August (after Augustus).	"['The Maya calendar (and time keeping in pre-Columbian Mesoamerica) is a fascinating but poorly understood topic that has gathered much interest in recent years (particularly around 2012!). In this article, we will explain how the Maya calendar was made, how it works and how to read it.\nTime was extremely important to the Maya, they made elaborate and accurate calendars and used them in charting the movements of the sun, moon, stars and even planets.\nThese calendars served a variety of purposes both practical and sacred. They were used in astronomical calculations, recording important events, the reigns of rulers and their conquests and also in divination.\nNB: specialists say “Maya calendar” and not “Mayan calendar” (see: 10 red-flags for spotting an unreliable online resources).\n- Introduction: The Maya calendars\n- The 260-day count (Tzolk’in)\n- The 365-day count (Haab)\n- The 52-year count (Maya calendar round)\n- The Long Count\n- Maya Calendar facts\n- Further resources\nHow does the Maya calendar work?\nIn the late 19th century, Ernst Forstemann worked out how the Maya marked time. The Maya used what is now known as the calendar round which is made up of 3 interlocking cycles. A cycle of 20 names, a cycle of 13 numbers (which forms the 260-day sacred calendar) and a 365 days solar year. 52 years will pass until the three cycles line up again.\nThe following video -taken from the acclaimed documentary Breaking the Maya Code– gives a great overview:\nThe sacred calendar (Tzolk’in) 260 days.\nThe 260-day calendar, also known as the Sacred Calendar or Tzolk’in in Maya, has 20 day associated with the numbers 1-13 (20 x 13 = 260). It would run as follow: 1 Imix, 2 Ik, 3 Akbal, 4 Kan… to 13 Ben, then numeral 1 would return and get associated with the fourteenth day: 1 Ix, then 2 Men, 3 Kib and so on continuing in an endless cycle.\nEvery day was significant, similar to an astrology chart.\nNotice that the Maya did not have numbers like ours, they only had three digits; a dot standing for ‘one’, a bar standing for ‘five’ and a shell for ‘zero’ (see Maya numerical system)\nFor example: 4 would be 4 dots, 5 would be 1 bar, 10 would be 2 bars and 13 would be 2 bars and 3 dots.\nThe solar calendar (Haab) 365 days\nThis contained 19 ‘months’ – 18 months of 20 days and a closing month of 5 days (Wayeb).\nFor example: 0 Pop would be followed by 1 Pop, then 2 Pop, up to 19, then it would be 0 Wo, 1 Wo and so on.\nWhen these two calendars (Tzolkin and Haab) are working together, one day in this round such as “3 Kan 8 Pop” did not repeat until 52 years passed, which was called the Calendar Round.\nSo you basically have a number + day + number + month.\nMoving on from this was their absolute dating system, which we call the Long Count.\nLike our own calendar the Maya marked dates for more extensive time from a fixed starting point. In our calendar it is the birth date of Christ, for the Classic Maya the beginning of the present creation was 13th August, 3114 BC.\nEach great cycle lasted 5125 years and it repeated indefinitely. The first great cycle was to end on 21 December 2012. This led to the popular idea that the Maya prophesied the world was to end on that date. However, this is completely a modern invention, time was not lineal for the Maya, but cyclical and ever repeating.\nNow that you have a general idea of the Maya calendars, let’s see them in more details:\nThe Tzolk’in: the 260-day count\nThe 260-day count, which approximates the human gestation period, as well as the time it takes for a maize plant to come to fruition, is still used in some Maya communities today, mostly in the highlands of Guatemala.\nIt was, and still is, a sacred almanac that provided a chronological framework for Maya ceremonial life, and a basis for prophecies.\nBirth dates were recorded by this calendar, and the patron deity of a particular day became closely associated with the destiny of babies born on that day.\nTzolkin (Sacred Calendar)\nThe Tzolk’in is a succession of 260 days made up of the permutation of 13 numbers with 20 names (13 x 20 = 260), very much in the same ways our months consist of the association of 7 week-days (Monday to Sunday) and 28, 29, 30 or 31 numerals (for example Friday the 13th which is the 13th day of the month and the 5th day of the week).\nEach day is uniquely designated by the combination “coefficient/day-name”, and not until every single one of the numbers 1 through 13 had been attached to every one of the 20-day names was the cycle complete. It would take 260 solar days to complete.\nBelow: names and glyphs of the Tzolk’in 20 days\nThe first day of the Tzolk’in is “1 Imix”, the second is “2 Ik’”, the third is “3 Ak’bal”, the thirteenth is “13 Ben”, the fourteenth is “1 Ix”, the twenty-first is “8 Imix”, and so on.\nA particular combination will not recur until 260 days have elapsed.\nBelow: conceptual representation of the Tzolk’in cycle with numerals on the left (dot = 1, bar=5) and days glyphs and names on the right\nThis sacred calendar is still used by the highland Maya people today. Each community has their day keeper, generally a shaman, who carries out ceremonies on particular days of this calendar. People will go to the day keepers to ask questions about their future or help in curing an illness. Day keepers then ask the spirits for guidance.\nTo read an account of a Maya day ceremony – click here\nBoth the ancient and highland modern Maya believe that they have a spirit/animal companion, the ancient Maya called it Way (pronounced ‘Why’) and the Maya today call it their nahual/nawale. Your birthdate defines what animal/spirit companion you have and also tells you your character traits.\nBelow is the list of the 20 nahales and their respective calendar name, both in the Yucatecan Mayan language (in italic) and the highland Mayan language of Kaqchikel (in bold).\n- Crocodile – Imix – Imox\n- Wind – Ik – Iq\n- Night – Akbal – Aqabal\n- Sky – Kan – Kat\n- Snake – Chikchan – Kaan\n- Death – Kimi – Kame\n- Deer – Manik – Kiej\n- Venus – Lamat – Qaniel\n- Moon – Muluk – Toj\n- Dog – Ok – Tzi\n- Howler monkey, ancestor – Chuwen – Baatz\n- Tooth/Jaw – Eb – E\n- Maize – Ben – Aaj\n- Jaguar – Ix – Ix-balaam\n- Eagle – Tzikin – Men\n- Candle – Kib – Ajmaq\n- Earth – Kaban – N’oj\n- Flint, obsidian – Tijax\n- Storm – Kawak – Kawoq\n- Lord – Ajaw – Ajpu\nThe Haab: the 365-day count\nSometimes called the “Vague year” because leap year adjustment was never intercalated, the Haab is a 365-day period.\nIt is made up of 18 months of 20 days, and an extra month, called Wayeb, of only 5 days at the end of the year which gives a total of 365 days. Those 5 extra days were usually considered to be a special time.\nHaab (Sacred Calendar)\nThe months’ names are all taken from the Maya Yukatek list given to us by Bishop Diego de Landa in the 16th century: Pop, Wo, Sip, Sots’, Sek, Xul, Yexk’in, Mol, Ch’en, Yax, Sak, Keh, Mak, K’ank’in, Muwan, Pax, K’ayab, Kumk’u and Wayeb.\nThe 365-day count operates very much like our own calendar: the first month is “Pop”, and the first day of the year is “1 Pop” followed by “2 Pop”, and “3 Pop”, and so on until “19 Pop”. The following day is referred to as the “seating”, or “putting in place”, of the next month (Wo) and transcribed as “0 Wo” (yes, the Maya had figured out the concept of “zero”)\nThe unusual aspect of this calendar is that the “seating” of a given month is not considered as the first day of the month but rather as the last day of the previous one. This is akin to the tradition of calling December 31st “New Years Eve”.\nIn the Maya concept of time, the influence of a given month started on the last day of the previous one.\nBelow: names and glyphs of the Haab the nineteen months.\nThe Maya Calendar round\nIt is always written in the same order: (1) day coefficient + day name in the Tzolk’in, and (2) day number + month name in the Haab.\nFor example, January 9th 2012 was 8 Ben 1 Muwan in the Calendar Round.\nSince 260 and 365 have a least common denominator of 5, it will take 18,980 days (260×365/5), approximately 52 years, before a specific date in the Calendar Round recurs.\nThe 52-year count was in use throughout Mesoamerica.\nThe Mexica (Aztecs), for example, called these periods xiuhmolpilli, meaning “year bundle”. The start of a new xiuhmolpilli was cause for much celebration. The Maya name for these 52-year period is not known.\nThe association of the 20-day cycle in the Tzolk’in and the 20-day months in the Haab creates a noteworthy phenomenon: for any given year, the first day of all the months in the Haab will start with the same Tzolk’in day. For example, if the first day of the year (1 Pop) falls on a day Ben in the Tzolk’in, then the first day of every month in that year will be on a day Ben. These days are called “Year Bearers”.\nGiven of the short month of 5 days (Wayeb) at the end of the Haab, the first day of the following year has to be 5 days later in the Tzolk’in.\nSince there are only 20 days in the latter, there can be only 4 Year Bearers.\nDuring the Classic times, these were Ak’bal, Lamat, Ben, and Etz’nab. The same traditional Year-bearer pattern continues to hold in some traditional highland Maya communities. By the time of European contact, however, the Year-bearers used in the Yucatec calendar were K’an, Muluk, Ix, and Kawak.\nThe Maya Long Count\nMoving on from this was their absolute dating system, which we call the Long Count. Like our own calendar the Maya marked dates for more extensive time from a fixed starting point. In our calendar it is the birth date of Christ, for the Classic Maya the beginning of the present creation was 13th August, 3114 BC. Each great cycle lasted 5125 years and it repeated indefinitely.\nEvery inscription began with the date, usually the long count date and then followed by the calendar round (Tzolk’in date + Haab date).\nThe base unit in the Long Count was the day (K’in). Since the Maya used a vigesimal numeral system (i.e. based on 20, the same way our decimal system is based on 10), they then used a period of 20 days (Winal) which was in turn grouped in packs of 18 (not 20, probably to approximate the solar year) called Tun, and then the Katun which is 20 Tuns, and the Baktun which is 20 Katuns, etc\nThe Long Count then is the counting of days since the last creation and was divided into:\nK’in = 1 day\nWinal = 20 days (20 K’ins)\nTun = 360 days/K’ins (18 Winals)\nK’atun = 7,200 days (20 Tuns)\nBak’tun = 144,000 days (20 Ka’tuns)\nMaya Calendar Facts\nHow accurate is the Maya calendar?\nThe Maya did not use any system of leap days, so the Haab had an error rate of 1 day every 4 years when compared to the real solar year.\nHow many days did the Maya calendar have?\nThe Haab has 365 days (18 months of 20 days + 5). The Tzolk’in has 260 days (20 names and 13 numbers). The calendar round has 18,980 days (52 years).\nDid the ancient Maya predicted that the world would end on 21 December 2012?\nNo, they didn’t! A great cycle of the Maya Long Count was to end on 21 December 2012, but the next day the Maya believed that a new cycle would begin. There was to be no end of the world.\nIn fact, in the temple of inscriptions at Palenque, where we find the tomb of King Pakal, it was written that in AD 4772 the people would be celebrating the anniversary of the coronation of their King Pakal.\nMaya calendar activities (KS2)\nSee our post: Maya calendar activity and quizzes (KS2)\nFurther Resources on the Maya calendar\nLesson Plan (KS2) – Please note that if you are having problems downloading these files then please use your personal, rather than school’s email address. Some schools disable the ability to receive items outside of their domain.\nMichael D. Coe and Mark Van Stone (2005) Reading the Maya Glyphs. Thames and Hudson, London\nThis book includes a chapter on Time and the Calendar.\n2012 – End of The World\nAnthony Aveni (2009) The End of Time: The Maya Mystery of 2012, University Press of Colorado, Boulder\nDavid Stuart (2011) The Order of Days: the Maya World and the Truth about 2012. Harmony. New York\nFAMSI website has an excellent discussion and powerpoint of this phenomenon – http://www.famsi.org/research/vanstone/2012/index.html\nFAMSI website also has colouring books you can download for the Maya months and days – http://www.famsi.org/research/pitts/MayaMonths_English.pdf\nThere is also a selection of videos on the end of the world which can be found here', 'Some more miscellaneous and background information about Roman myths and legends that may interested you.\n|Seven Kings of Rome|\nBelow is the list of kings who had ruled Rome, before the time of the Republic (c. 509 BC).\nInformation and the history or legend about each king can be found in the Tales of Rome page.\nPlease note that the dates and duration of the reign are not precise, because I had relied on the traditional dates.\nAlso, during Romulus\' reign, Titus Tatius ruled together with him, for a number of years before he was assassinated. Titus Tatius was a Sabine king. I had not included his name in the list below (because it would disrupt my count of seven kings).\nHouse of Tarquin (Etruscan)\n|Seven Hills of Rome|\nBelow is the list of the famous Seven Hills of Rome.\nOf all the calendars around the ancient world in Europe, the Roman calendar survived to this time. Though, the calendar had undergone several evolutionary changes.\nAccording to Roman legend, the calendar used by the Romans began at the time of establishment of the Roman monarchy. Romulus, the founder of Rome, created the Roman calendar of 304 days with ten months, with the New Year starting with month of March. The months after June were Quintilis (fifth month) and Sextilis (sixth month). December was the tenth month of a year.\nNuma Pompilius, Romulus\' successor, added two more months to the end of the calendar: January and February. So Numa\'s calendar had 355 days. The first Etruscan king of Rome, Tarquinius Priscus, had wanted to move the New Year to January, but this reform was abolished when the Republic was established; the New Year was reverted back to March. The Roman calendar was based on the lunar calendar system of 355 days. An intercalary month (27 or 28 days) was added to the end of February, to prevent the calendar from being too far out of phase with seasons and the solstices.\nThis arrangement of the calendar remained unchanged through much of the period of the Roman Republic. The first of March was the day when two consuls were elected. The consuls were official magistrates that had the power of Roman king and could command army, but they only held office for the term of one year.\nIt wasn\'t until the 1st century BC, that the Senate had decided to move the New Year and the election of the consuls to the 1st of January. This was the time before Julius Caesar (died in 44 BC) came into power.\nIt was Julius Caesar who developed the new calendar system, which would be called the Julian Calendar. Caesar worked with Alexandrian astronomer, named Sosigenes, to calculate the solar calendar year of 365 1/4 days (or 365 days and 6 hours). The names of the months were kept the same. Getting rid of the intercalary month from the lunar calendar, the calendar would still be slightly out of phase with the seasons. To correct this – at the interval of every four year another day would be added to the end of February, so that it would have 29 days instead of 28. This year would be known as the Leap Year.\nDuring or after the reign of the first emperor, Augustus Caesar, the great-nephew of Julius Caesar, the month of Quintilis was changed to July after Julius Caesar, while the month of Sextilis was changed to August, after Augustus.\n|Early Roman Calendar (Romulus)||Early Roman Calendar (Numa)||Late Roman Republican Calendar (pre-Julian)||Julian Calendar|\n|March (named after Mars)||March||January||January|\n|May (named after Maia)||May||March||March|\n|June (named after Juno)||June||April||April|\n|Quintilis (5th month)||Quintilis||May||May|\n|Sextilis (6th month)||Sextilis||June||June|\n|September (7th month)||September||Quintilis||July (named after Julius Caesar)|\n|October (8th month)||October||Sextilis||August (named after Augustus)|\n|November (9th month)||November||September||September|\n|December (10th month)||December||October||October|\n|January (named after Janus)||November||November|\n|February (named after Februus)||December||December|\nHowever, the Julian Calendar was not perfect, because the true solar year had 365.242199 days, not 365.25 days. So calendar was still out of phase with the seasons (the Equinoxes and Solstices), because the Julian Calendar had a leap year on the year of a new century or centennial year. So the Julian calendar was out by 11 minutes and 14 seconds to the true calendar year.\nAs a little side note. The solar calendar is 365 days, 5 hours, 48 minutes and 46 seconds (all of which is equal to 365.242199 days).\nIt wasn\'t until 1582, that Pope Gregory XIII made amendment to the Julian calendar, to correct the error. Instead of having a leap year on each new century, the leap year happened at every four centennial years. So the centennial year will have no leap year unless it was divisible by 400. So the year 1600 was a leap year, while there were no leap year on the year 1700, 1800 and 1900. The year 2000 was a leap year (but I think I had slept through it).\nThe new amendment had caused the calendar be known as the Gregorian Calendar. This is calendar system that we are still using today. Though not everyone was happy with the new reform at the time, mostly because the Gregorian calendar made it more complicated to calculate the Easter festivals.\nWorse than that, since the old calendar had leap year in every centennial year, the Julian calendar was out of phase by 11 days with the equinox. To amend this, the 11 days was removed from the calendar when the Gregorian Calendar was adopted. But this caused minor peasants\' revolts. The peasants bitterly complained that the authorities stolen their 11 days, and they wanted those missing days back.\nThe Gregorian calendar was not universally accepted by some of the countries. The traditionally Catholic countries, such as Italy, Spain and the Catholic Germany mostly adopted the new calendar. The Protestant German states didn\'t adopted the calendar until 1699, while England (United Kingdom) didn\'t accept the new calendar until 1752. Greece made the change to the Gregorian calendar as late as 1923.\nBelow, I have included a table on the days that was dedicated to the Roman gods, with the modern equivalent.\nBelow is a list of some of the holidays and festivals that the Romans celebrated. This was gained from mainly the book, called Fasti, written by the Roman poet, Ovid. Only half of the Fasti survived (January to June). The rest of the dates were gained from other sources.\nIt should be noted that the Ides of Jupiter was celebrated on every month. In March, May, July and October, the Ides were held on the 15th day. On the other months, the Ides were held on the 13th.\nThe deities are listed in the brackets, beside the name of the festival.\nThere is one noticeable Roman holiday that I would like to say more about.\nI am referring to Saturnalia, an annual holiday dedicated to the god Saturn. Saturnalia lasted for several days, beginning on December 17th and ending on the 24th. It was days of merrymaking and exchanging of gifts. Clearly, like the Teutonic winter solstice, known as Yule, the Saturnalia is a pagan form of the Christmas (December 25).\nIn the area of west central Italy was a region known as Latium (modern Lazio). The people of Latium were known as the Latins, an ancient tribe that lived in a region on the south side of the River Tiber, with the Etruscans living on the other side of the river, known as Etruria (modern Tuscany). Their southeastern neighbour was the region of Campania, with some cities such as Cumae, was founded and colonised by the Greeks.\nWithin Latium was the city of Rome. The Latins spoke an Italic language, known as Latin, a language that the Romans had adopted.\nThe Italic language is a subfamily to the Indo-European language, confined to the regions in Italy. Other Italic languages spoken in ancient Italy were the Faliscan, Oscan, Umbrian and Venetic.\nEtruscan language is not an Italic language; in fact, the Etruscan language is a unique, but extinct language, which doesn\'t even belong to the Indo-European language family. Some linguistic experts believed that the Etruscan probably has its origin in the Anatolian language family, but so far this can\'t be proven.\nThe Etruscan alphabets had been derived from the Greek, who derived it from the Semitic (Phoenician) alphabets (see the Greek Alphabets). The Etruscan alphabets contained 21 letters.\nWhy am I talking about the Etruscan language, you may ask? Well, despite the Etruscan being a distinctive language from Latin, the Etruscan alphabets had played a major role to the Roman or Latin writing system.\nWhen Etruscans conquered much of central Italy, including Latium and Campania, a large group of Etruscans had settled in Rome. Under Etruscan kings in Rome, the Romans had learned engineering skills, such as architecture, town planning, road building and aqueduct systems. The Romans had even adopted some of the Etruscan culture and religious customs, as well as the Etruscan writing system.\nThe Roman alphabets, or more properly the Latin alphabets, became the most widely used alphabets in Europe, but the Latin alphabets were truly indebted to the Etruscan writing system.\nThe classical Latin has 23 letters, in which the Roman had adopted all 21 of Etruscan characters. Like the Etruscan alphabets, Latin has added on 4 vowels. Phonetics, the Latin alphabets were identical or similar to the English pronunciation.\nCenturies of conquests and settlements in the Empire provinces had also allowed the spread of its language and writing system. Though the alphabets had remained unchanged since then, the language spoken in the provinces began to diverge from the classical Latin which was spoken in Rome. This is because the people in the province introduced new vocabulary and accent into Latin. Late Latin (after 3rd century AD) became known as Vulgar Latin.\nWhat emerge from the Vulgar Latin, were the Romance languages. Though Latin was still used widely among the medieval clerics and scholars, the Romance languages were gradually replacing spoken Latin. There are five major Romance languages: French, Italian, Spanish, Portuguese and Romanian. There are also several minor languages found in Spain, France and Switzerland. I won\'t go to any more detail about the Romance languages.\nIn the medieval period, Latin had developed a further three letters in the medieval period (J, U and W).\nMany Latin words still persisted in the modern time, and are often used in other European languages, such as in English and German. Often, in the field of science, we attached a Latin name to an object, mainly for scientific purpose of categorising.\nIn the case of classical mythology, medieval and modern scholars would often use the Latin names in preference to the Greek names, such as Jupiter instead of Zeus, Mars instead of Ares, Hercules instead of Heracles, and the list go on and on.\nThis page belongs to Timeless Myths.\nSee Copyright Notices for permitted use.\nFor feedback, questions, or just to say ""hello"",\ncontact can made through the Contact page.\nNo mailing list or spamming, please.\nSeven Kings of Rome | Seven Hills of Rome | Roman Calendar | Roman Festivals | Roman Alphabets | Back\nHome | Classical Mythology | Pantheon | Heroic Age | Royal Houses | Geographia | Back\nWhat\'s New? | About | Bibliography | Fact & Figures | Genealogy | FAQs | Links | Copyright | Donation | Contact\nCopyright: Timeless Myths (Classical Mythology) © 1999, Jimmy Joe. All Rights Reserved.\nFirst Created (Facts and Figures: All Things Roman): 12/07/2001.\nLast Modified: 24/06/06.']"	['<urn:uuid:b159df2d-6cb6-4b1f-b879-a806556af60a>', '<urn:uuid:7d989275-99ed-4c05-8f8e-62d8eb0d4707>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T17:51:21.175311	27	78	4193
86	environmental psychology focal points what are main research areas	Environmental psychology has two main focal points. The first examines how the environment influences individual human behavior, cognition, affect, and physiology, including topics like restoration, way-finding, and place attachment. The second is conservation psychology, which studies relationships between people and the natural world and promotes environmentally friendly behaviors.	['When it comes to putting people at the core of building design, one can no longer rely on “hard” sciences alone. Human and social sciences — including environmental psychology — are key. To understand this fascinating discipline and how it can help create better spaces for their occupants, we called on Femke Beute, owner of LightGreen Health and Postdoctoral Researcher at Jönköping University.\nWhat is environmental psychology about? When was it born and why?\nFemke Beute (F.B.): Environmental psychology is about the dual interaction between the person and the environment. It is based on the recognition that the environment influences us but we also influence our environment. It is a relatively recent field within psychology that has received less attention than clinical and social psychology for instance, although it is growing in size and number of researchers working in this field. Indeed, psychology historically started with a focus limited to the individual, and was then broadened to the physical and social environment with the work of Egon Brunswick, Kurt Lewin and Roger Barker about one century ago.\nThere are two focal points within environmental psychology. The first, which I am involved in, looks at the influence of the environment on the individual as human behavior, cognition, affect, physiology, etc. Examples include restoration (mood improvement, stress reduction), or way-finding (how people find their way in an unknown building or city), or place attachment (what are the physical characteristics that make people get attached to a certain place). The second is conservation psychology and focuses on the relationships between people and the natural world and on how to support more environmentally friendly behaviors to conserve this natural world.\nIt is today not (yet) established and taught as a discipline like social psychology, and there are in fact only a few environmental psychology sub-departments in the world. Most people working in this field are scattered across many different disciplines, like occupational or clinical psychology, architecture, epidemiology, medical science. This raises some challenges but also gives it its strength.\nWhat are the links between environmental psychology and architecture?\nF.B.: I would say that they are two separate disciplines looking at the same thing, interested in the same topic. However, they use a different language and sometimes have different goals. Also, generally speaking if we look at practical implications, research is very slow. As an architect, you have an assignment, and you can’t delay the process too much by doing research. And as researchers, we want to be very careful about the statements we make. And then the challenge is to translate the findings of our studies into an architectural design. For example, the effects of a window on people’s well-being found in a laboratory may be different than when placed in the real world, potentially because of the shape and dimensions of the room, the ceiling height, the color of the walls, etc.\nBut I believe that they are growing towards each other. When I was a student in architecture, I was really interested in understanding how buildings influence people, and how I could use my designs to help people feel better. However, this type of research was not really understood within the architectural department where I was studying at that time. That’s why I started to look around and came upon the program “Human Technology Interaction” where they did look at those kinds of questions, and eventually switched programs at that point.\nToday, when we look at very similar architectural departments, more and more research on humans is going on. Architecture departments are starting to take the tools and research methods developed within environmental psychology. And I expect that this will become more and more. But we could still benefit from a communication in both directions. Researchers are indeed much into the numbers and controlling everything, while architects are more into design orientations, and translating a strong concept into materials. Environmental psychology could really learn from architects’ intuitions and design challenges. As I said, it is a very big step from implementing the micro scale manipulations that researchers do in the laboratory to the complex real world.\nWhat benefits could we get from a closer collaboration between environmental psychology and design?\nF.B.: The ultimate goal is to understand how our physical environment influences us, and to eventually implement that understanding so that buildings are good and healthy for the people who live in there, so that buildings are more than a shelter to keep us safe from the rain. This is not an easy task since it is always a combination of factors that makes a space. But this will become more and more important, as people are living more and more in cities and in built environments. And the current Covid-19 crisis has demonstrated how important it is to live in pleasant places and have access to the outdoors. Design needs the research knowledge and the inputs, and research needs to understand the design intuitions. We need both parts, as a common language, to translate the findings into the real world.\nDo you have examples of how environmental psychology is applied in practice to create healthier buildings?\nF.B.: There are a number of areas where you can see fruitful cross fertilization. Healing environments are a good illustration, where knowledge from restorative environments and what makes people relieve stress has started to be implemented. Having said that, pre/post occupancy studies could be used to support further implementation of environmental psychology in real buildings, through surveys in particular, so that we could learn by doing. We also have some developments within research methodologies that we could use to test things more out in the field, and help bridging the gap between design and research.\nFor instance, the use of ambulatory sensing — with smartphones and sensors ― provides more details on what people are doing and how they are using their environment. In the old days, the first technologies used within environmental psychology were physical tracers to see where people walked, how they were using the building. Today combined with modern age technology and sensors, that would help us to understand much more how the built environment influences us. We may even combine those with big data sets, combining all the physical characteristics of the environment and the human behavior in that environment ― what people feel, their activity level, their heart rate, etc. So the technology is there, the research method is there, but we need to implement it in a smart way, and people that are willing to do it at a larger scale and to pay for it.\nHow does your research and teaching impact your insights on daylight and views?\nF.B.: Of course when we learn about how important it is to get the right amount of daylight at the right time of day, to have a natural view, or to live in a natural environment, it certainly does impact you. I have moved to a place with more nature where my daughter can grow up in a more natural environment because I know it is very important for her. I also try to get enough daylight, sit close to windows as possible. And when I teach students on this topic, there are always people that come to me saying that this has changed some aspects in their lives. Simple interventions in your life can indeed have a large impact.\nBut if I talk to people in my environment not working in this area, there are a lot of things they are not aware of. There is still a big gain to get by educating, translating the research outcomes to knowledge. Take the example of hospitals. As the focus is on medical outcomes, and the benefits of nature and daylight are mostly related to mental and physical health, it is logical that this knowledge has been picked up first there, and applied as an intervention. But certain elements could also be applied to other types of environments such as homes and offices. Also, we could work with architects to design the most optimum building, but the actual effects would depend on how the users use it. For example, you can have a nice window opening, but if the occupant decides that there needs to be a very thick curtain in front of it the entire day, you lose the benefits.\nWhat are the main challenges and research gaps — in particular with regards to windows ― that environmental psychology should investigate in the future?\nF.B.: While there are still lots of aspects to be looked at, relatively speaking windows have received quite a bit of attention. Now we would also need to look at for instance where windows need to be placed in a room, and what other aspects of the room are also important — choice of materials, dimensions and shape of the room, etc. Views to the outside can be restorative, but the way the building is designed by itself can also be restorative. We need to take a more comprehensive look at nature, by including daylight in our perception of the natural environment, and not separating view types from daylight, because daylight is part of nature. We need to look more substantially at weather changes, weather dynamics in daylight and how that impacts occupants in buildings, and how we can capture that inside architectural design and use it to create restorative experiences. If we could see the shades of the leaves on the wall moving in the wind, or disappearing when there is a cloud in front of the sun, this could potentially make occupants feel better.\nNatural environments are restorative: when we go outside, we feel better and we can perform better. How in this increasingly urbanized world can we keep this connection with nature and daylight that is good for us, and how can make sure that even in cities people meet their minimum needs? This is the biggest design challenge.\nFemke Beute is an Environmental Psychologist, investigating the beneficial effects of our physical environment on humans. She specifically focuses on the effects of our natural and built environment and exposure to daylight on health and wellbeing. In addition, she is also keen to complement research conducted in the psychological laboratory with field research aimed at gaining a better understanding of the complex interplay between humans and their environment in everyday life by using ambulatory sensing and smartphone technology. She holds a Bachelor in Architecture and a Master in Human Technology Interaction. Her PhD project focused on the beneficial effects of exposure to nature and daylight on mental health. In 2018, she founded her own Research Consultancy Agency ‘LightGreen Health’ with the aim to deepen the scientific understanding of the beneficial effects of our physical environment on health and wellbeing and, importantly, to inform people outside of academia on these scientific outcomes as well as to support the application of potential restorative interventions in design and planning. She combines her consultancy work with an Academic position in lighting research at Jönköping University.\nEloïse Sok is Concept Creator in the SageGlass Europe & Middle-East Team. She holds a Double-Degree in the Engineering field from Ecole Centrale (France) and Tsinghua University (China). Her main interests include sustainable architecture, daylighting and occupant’s comfort. Her motto: “Passion is our best strength!”.']	['<urn:uuid:a1fe463b-9bbf-47c9-9835-af4caca1bca2>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	9	48	1882
87	I work in drug safety and need to understand how pharmacovigilance has evolved - what were the historical events that led to current drug monitoring systems, and what are the main components of modern pharmacovigilance activities?	Several tragedies, including the sulfanilamide elixir and thalidomide incidents, led to the establishment of modern drug monitoring systems in the 1970s. Today's pharmacovigilance activities include collecting and evaluating information about suspected adverse reactions, sharing safety information between relevant parties, monitoring spontaneous adverse drug reaction reports, conducting post-approval clinical studies, and tracking information from regulatory authorities worldwide. The system requires pharmaceutical companies to have qualified personnel responsible for pharmacovigilance who can ensure compliance with regulations, maintain pharmacovigilance systems, and oversee safety monitoring. These activities aim to improve public health safety, assess benefit-risk profiles of medicines, and promote understanding through education and training in pharmacovigilance.	"[""The absence of the centralized system of the pharmaceutical product’s authorization in Europe or USA has led to several tragedies: sulfanilamide elixir, thalidomide tragedy and etc. . This example shows that regulators should take into consideration not only the properties of the known API but also the safety of the excipient as well. Accordingly, Pharmaceutical Development should be carried out before manufacturing any dosage form, even those with well-established API, in order to ensure compatibility of all components of the medicine product.\nThe contemporary system of drug monitoring, introduced in 1970s, has increased the quality, safety, and efficacy control of medicinal products. Despite the strictness of the Marketing Authorizations (MA) Law and Good Manufacturing Practices (GMP), until the mid-1980s, it was possible to get MA for several drugs without the FDA’s standard evaluation procedure (for example, approval of the first antiretroviral drug-Azidothymidine) .\nNowadays, all MA are conducted in accordance with the regulations and guidelines established by Regulatory Authorities (RA), which on the other hand should be in accordance with updated guidelines of the International Conference on Harmonization of Technical Requirements for Registration of Pharmaceuticals for Human Use. The tragic results related to the Elixir sulfanilamide and other unregulated drugs made the specialization of healthcare regulatory affairs fast-growing and in high demand in order to ensure that certification of drugs meet regulatory standards for human use.\nAlthough the specialization of the healthcare products by Regulatory Affairs Associate (RAA) has only been around for about 40 years, it already had a tremendous impact on regulation of the pharmaceutical market. The RAA specialty is becoming more and more popular. According to the 2016 RAPS report, 88% of current regulatory professionals had been working in a different industry before transitioning into regulatory affairs; moreover, about 70% of working RAAs are satisfied with their current jobs .\nIt is possible to become a RAA if a person has a relevant degree in fields such as chemistry, medicinal chemistry, biochemistry, biotechnology, pharmacy, pharmacology, physics, biomedical science, or other similar fields. Continuing education and professional development are critical for the regulatory professional. Regulatory professionals must keep up to date with regulatory policies and procedures from one or more countries, as well as maintain an understanding of the scientific and technical background of healthcare products. Global aspects of regulatory affairs are taken up by organizations such as the Drug Information Association (DIA) .\nNowadays it is possible to gain a master’s degree in regulatory affairs at many well-respected U.S. universities . There are also several professional organizations which are helpful for career development in regulatory affairs:\n- Regulatory Affairs Professionals Society (RAPS)\n- NIH National Center for Complementary and Integrative Health, Office of Clinical and Regulatory Affairs (NCCIH/OCRA)\n- The Organization for Professionals in Regulatory Affairs (TOPRA)\n- Orange County Regulatory Affairs Discussion Group (OCRA)\n- US Food and Drug Administration, Office of Training Education and Development (OTED)\nLarge pharmaceutical companies recognized the necessity of a professional staff that would be involved in the process of pharmaceutical product authorization and be familiar with International standards and guidelines for healthcare products, as specific laws and queries varies in different countries. This staff would be able to provide valuable analysis of the lifecycle of a product using regulatory knowledge and critical thinking skills. Thus, the regulatory affairs department has become an essential unit of pharmaceutical companies.\nThe functions of RA department are very clear and detailed described in Chapter 20-Regulatory affairs of the book Drug Discovery and Development (Second Edition): “The RA department reviews all documentation from a regulatory perspective, ensuring that it is clear, consistent and complete, and that its conclusions are explicit. The department also drafts the core prescribing information that is the basis for global approval and will later provide the platform for marketing. The documentation includes clinical trials applications, as well as regulatory submissions for new products and for changes to approved products. The latter is a major task and accounts for about half of the work of the RA department” .\nAlong with this department, there are Regulatory Affairs managers (RAMs) working mostly independently from the manufacturer, located in the country or region where a manufacturer aims to authorize a future healthcare product. Their ability to analyze the lifecycle of a product and determining the types of Variations (if any) using local regulatory knowledge is very useful for pharmaceutical companies. In keeping with best practice standards, independent RAMs should keep track of the often-changing legislation in all the regions in which a company wishes to distribute its products.\nThe RAM acts as a bridge between pharmaceutical companies and regulatory authorities, ensuring that products are manufactured and distributed in compliance with appropriate legislation. It would be a mistake to regard the regional RAM as a mere courier. It is quite the contrary; one of the responsibilities of a RAM is to prepare the submission dossier, and with developing strategies to ensure regulation compliance, which often means long negotiations with the Manufacturer/MAH as well as National Regulatory Authority. The RAM should keep each interested department to be informed in each registration.\nA RAM can influence establishment of the standards for a company's operating procedures and can also help develop company policies concerning how the business operates. The RAM often needs to act as a diplomat, since the requirements from local Regulatory Authority do not always meet the manufacturer’s interests. The scope of RAM’s functions is to plan, direct, or coordinate production activities of a pharmaceutical company to ensure compliance with regulations and standard operating procedures.\nLike with respect to other types of regulations, Georgia is striving to comply with the European regulation standards, when it comes to pharmaceutical market. During the USSR, pharmaceutical regulations were centralized and applied through Russia, while other Soviet Republics only utilized local pharmacological committees and laboratories for making pharmacological product quality decisions. After the collapse of the Soviet Union, Georgia, as a new country, has realized the necessity of such regulations. The creation of the Drug Regulatory Agency became the core mission in the spontaneous and chaotic market of that time.\nThe Pharmaceutical Law of Georgia which regulates the scope of regulatory affairs in Georgia is as follows: there are two regimes of registration in Georgia-Recognition regime and National one. Under the Recognition regime it is possible to register (during 7 working days) pharmaceutical products, which are already authorized in USA, Canada, EU member states and other countries from official “List of eligible countries” . The deadline of National regime process is within 3 months after the required documents submission. A regional RAM is responsible for tracking the market for possible shortages for a specific type of product within the country, as technology has made information widely accessible and easily spread, which is very helpful to manage the situation.\nThe pharmaceutical legislation in many countries regulates mostly only pharmaceutical products, but also extends to the regulation of medical devices, dentistry materials, and sometimes food/dietary supplements. Food/dietary supplements regulation remains sloppy around the world; for instance, in the United States, dietary supplements do not need approval from the FDA before marketing. Only companies that manufacture or distribute dietary supplements containing “new dietary ingredients” are required to submit premarket safety notifications. Consequently, there are not any approved standards for those products and no assurance of their safety .\nThe European Commission model covers a definition of health supplements, provisions to establish risk assessments, labeling requirements, and a negative list of ingredients, provisions for nutrition and health claims, and standards for GMP for food supplements . In addition to registrations, a RAM sometimes is required to conduct Pharmacovigilance - a new direction in the field.\nWHO defines pharmacovigilance as: the science and activities relating to the detection, assessment, understanding and prevention of adverse effects or any other drug-related problem .\nThe legal scope for pharmacovigilance for healthcare products in the EU/European Economic Area (EEA) is set out in several Directives, which describe the obligations of MAH and the Regulatory Authorities. This requires them to set up a system for pharmacovigilance in order to:\n- collect, collate, and evaluate information about reported and suspected adverse reactions;\n- Share relevant information to allow all parties involved to meet their obligations and discharge their responsibilities.\n- Spontaneous adverse drug reaction (ADR) reporting reports.\n- Post-approval clinical studies and investigation of health and diseases in wider populations.\n- Information from pharmaceutical companies and information published in medical literature.\n- Information from regulatory authorities worldwide and from morbidity and mortality databases.\nFor new products, this is defined in a pharmacovigilance plan that is part of the risk planning information that accompanies an application for a marketing authorization . However, regardless of active regulations, side effects of marketed pharmaceutical products still persist, and information about pharmaceutical products, containing API Ranitidin is an obvious example .\nIn today’s world such incidents receive a fast response on international and local levels. For example (Figure 1), here is published information about carcinogenicity of the Zantac itself and its generics: The incident was published next day on the Georgian MOH’s official site (Figure 2) and gives information about postponement of registration of all registered products with Ranitidin in composition .\nZantac (Ranitidine) belongs to a group of drugs called histamine-2 blockers. Ranitidine works by reducing the amount of acid produced by stomach. The reason for a recall was based on the fact that traces of N-nitrosodimethylamine (NDMA) were found in ranitidine generics. “Initial testing suggested generic Zantac was “contaminated” with NDMA; however, it now appears that ranitidine is a fundamentally unstable molecule and NDMA forms during the degradation or breakdown of the molecule, meaning that no form is safe”. Such a fast reaction is a positive change and gives reassurance on timely actions in case of an unsafe or potentially dangerous product.\nActions taken for the identification and remediation of the possible Adverse Effect (AE) (side reactions) on the regulatory level is to be attributed to the Pharmacovigilance. Pharmacovigilance is used to:\n- To improve public health and safety in relation to used medicines.\n- To contribute to the assessment of benefit, harm, effectiveness and risk of medicines encouraging their safe, rational and more effective use.\n- To promote understanding, education and training in pharmacovigilance and its effective communication to health professionals and the public, in timely manner.\nHowever, on a local level, it is still hard to collect information about the side effects, would it be from a patient or a healthcare professional. From personal practice, I was notified by a doctor about a side effect of a drug. Although having checked, on my behalf, all the points in PREP (Patient, Reporter, Event and Adverse Event), doctor has refused to provide more detailed information upon request. Regardless of the lack of feedback, the precedent of the side effect was made known to the contact person of the MAH. Unfortunately, there is a lack of reciprocal communication between: the person involved in PV, the health professional and the patient.\nThere might be a growing need for the local level conferences that cover not only the efficacy of the pharmaceutical products, but also hold discussions about AE to ensure high quality of the public health.Alongside the PV, fight against the falsifications of pharmaceutical products holds a high priority. For that reason, on the 09th February 2019, the EC has revised a directive about counterfeit pharmaceutical products for this purpose; serialization is the key factor .\n“These new requirements will enhance patient safety by protecting the medicines supply chain from infiltration by counterfeit medicines and introducing new rules to more rigorously regulate the supply chain… (and) provides further security and protection for Irish and European patients now and into the future.” The directive necessitates the inclusion of two safety features on all prescription medicine packs in the European Union market: a unique 2D barcode and an Anti-Tampering Device (ATD). Pharmacies, drug stores and those authorized to supply medicines to the public are now required to authenticate products at point-of-sale by visually inspecting the ATD and performing a verification scan of the product using the barcode.\nLocal Regulatory Affairs Manager is important for regulatory framework for healthcare products authorization, as he/she is binding Governmental Regulatory Authorities and manufacturer/MAH of pharmaceutical products, being functioning like a mediator between them. The RAMs are reducing time for product reaching on the market, and accordingly, have considerable economic importance for any pharmaceutical company.\nPharmacovigilance is very important topic for ensuring pharmaceutical product’s safety, and bodies, involved in pharmacovigilance should collect and report about any AE to manufacturer/MAH, as well as to Governmental Regulatory Authorities. Providing Pharmacovigilance service in Georgia is still difficult because of low self-awareness in this regard among healthcare professionals and country’s population. The new safety feature may help them to be better aware of counterfeit medicinal products, too.\n- Florence AL. Is Thalidomide to Blame? (1960) Br Med J 2: 1954.\n- HIV/AIDS Historical Time Line 1981-1990\n- Scope of practice & compensation report for the regulatory profession (2016)\n- Drug Information Association.\n- Online Master's in Regulatory Science Programs\n- Hägglöf II and Holmgren A. Drug Discovery and Development (2013) Science direct, United Kingdom pp-285-301. https://doi.org/10.1016/b978-0-7020-4299-7.00020-2\n- Law of Georgia on “Drugs and Pharmaceutical Activity” (1997) Bulletin of Georgian Parliament:17-18.\n- Dietary Supplements Guidance Documents & Regulatory Information\n- Food supplements\n- Essential medicines and health products\n- Guidance on the format of the risk management plan (RMP) in the EU – in integrated format\n- Zantac Recall: Does Ranitidine Cause Cancer?\n- Suspension of registration of pharmaceutical products containing ranitidine\n- Commission Delegated Regulation (EU)\n*Corresponding author: Elene Chikobava, Department of Regulatory Affairs, “Quadrium Pharmhouse” Ltd, Tbilisi, Georgia, Tel: +955 595 555 224, E-mail: firstname.lastname@example.org\nCitation: Chikobava E and Beridze I. The challenges to be a regulatory affair associate: day-to-day questions on how to reach consensus between manufacturers of pharmaceutical products and government regulatory authorities (2020) Pharmacovigil and Pharmacoepi 3: 37-40.\nRegulatory affairs, Regulatory affairs manager,\nPharmacovigilance, National regulatory affairs officers, Adverse Effects."", 'The Qualified Person for Pharmacovigilance in the EU\nThere is sometimes a bit of misunderstanding about what the Qualified Person for Pharmacovigilance (QPPV or QP) is and does. In this post,I will attempt to clarify the situation.\nThe Qualified Person was initially defined in Directive 2001/83/EC.\n“As part of the pharmacovigilance system, the marketing authorisation holder shall have permanently and continuously at its disposal an appropriately qualified person responsible for pharmacovigilance in the EU (QPPV)” Article 104\nIt was later expanded and defined in Volume 9A which has since been further defined by the Good PV Modules modules 1 and 2.\nHere, then, is a basic summary of the QP requirements.\nThere must be a QP and a deputy in place when any Marketing Authorization Holder (MAH) has approval to market a product in the EU/EEA. In fact, it is necessary to specify the QP in the actual submission dossier of the MA to the EMA. Thus the QP must be identified at the time of submission and a pharmacovigilance (PV) system must be in place at submission even if it is not yet being used.\nIf a company does not yet have an MA or a submission and is just doing clinical trials there is no requirement for a QPPV. This is a point that is not always understood.\nThe QP and deputy must be permanently and continuously available and reside in the EU/EEA (note that this does not include Switzerland). The QP should be a senior person reporting at a high level in the management structure of the MAH with a clear reporting structure though this is not always the case.\nThe QP should have a formal written contract and there should be an SOP clearly defining the QP’s roles, functions and responsibilities. These responsibilities include the oversight, structure, performance and maintenance of the PV system in the MAH. This includes:\n- Establishing and maintaining the MAH’s PV system(s). The word “systems” here does not mean just “computer systems” but the entire PV/drug safety functioning of the MAH.\n- Ensuring that Quality Control (QC) and Quality Assurance (QA) mechanisms are in place to keep the MAH in compliance.\n- SOPs and Working Documents covering PV are in place, up-to-date, trained on and actually followed.\n- Metrics & KPIs on expedited and aggregate reporting and other key operational functions are tracked.\n- A quality management system (QMS) is in place which includes audits, inspections, Corrective Action, Preventive Action plans (CAPAs) as needed and that they are actually put in place and completed.\n- Ensuring that promises made to the health authorities in regard to safety are kept.\n- Ensuring that PV training is done in the drug safety/PV department as well as anywhere (everywhere) else in the company (or vendors, third parties etc.) where safety matters may arise\n- Ensuring that written agreements with other companies (including business partners, vendors, other third parties) are in place regarding safety and oversee their work.\n- Have authority and sign off on Risk Management Plans (RMPs). The QP usually signs off on PSURs and DSURs also.\n- Ensure that signal detection and trending mechanisms are in place.\n- Ensuring that all suspected adverse drug reactions ADRs received by the MAH are collected and collated and accessible at one or more points in the EU.\n- Ensuring that ICSRs, PSURs and Post-Authorization Safety Studies (PASS) cases and any other safety commitments are reported appropriately to the competent authorities (CAs).\n- Ensuring the continuing evaluation of the benefit/risk analyses of all products.\n- Maintaining an overview of the safety profiles and any emerging safety issues on company products.\n- Have access to and ensure that the Pharmacovigilance System Master File (PSMF) is in place, accurate and up to date.\n- In regard to the IT systems for PV, there must be a validated database/IT system.\n- Ensuring that the appropriate persons are in place and trained to capture AEs.\nThe MAH (the company) in turn has responsibilities that it must fulfill:\n- Set up the QP and deputy with one QPPV per PV system.\n- Support the QPPV and ensure that appropriate processes, resources, communication mechanisms and access to information are in place such that the QPPV receives all relevant information he/she needs.\n- Ensure full documentation of all QPPV procedures and activities.\n- Implement mechanisms for the QPPV to be kept informed of emerging safety and risk-benefit issues.\n- Ensure that the QPPV has the authority “influence the performance of the Quality System and the PV activities of the MAH”. That is he/she must be empowered.\n- Ensure that the QPPV has input into Risk Management Plans & the action taken in responses to CAs on safety concerns.\n- Ensure the presence of back-up procedures (e.g. personnel, AE database failure, failure of AE database & other safety related hardware or software).\n- Track the compliance of the whole PV system periodically (e.g. audits).\n- Notify the QPPV early in the process when the MAH is acquiring another company/product etc.\nThe person who is the QPPV must be:\n- Appropriately qualified in the theoretical and practical knowledge of PV. This is not further defined and thus leaves a lot of leeway.\n- Experienced in all areas of PV\n- Must work and reside in the EU/EEA (i.e. Norway, Iceland, Lichtenstein)\n- Registered with the EMA (as is the deputy)\n- Available 24/7 – either the QPPV or deputy\n- Be a single point of contact for PV issues\n- Available for PV inspections\n- Have access to a physician if the QPPV is not a physician\nThe QPPV may delegate tasks. If so, this should be done in writing and an oversight system for the out-sourced function be put in place. The QPPV (and MAH) still maintains responsibility for delegated tasks.\nThe EMA and MHRA (in a 2010 presentation by one of the Expert Inspectors) have made clear, however, that they will look very closely at delegated or out-sourced QPPVs. Broadly speaking, the CAs will not look kindly on a person (e.g. in a CRO) who functions as the QPPV for more than two or three companies at any time. It is felt that the job is so large, even in smaller or generic companies, that no one can handle all the responsibilities for more than 2 or 3 companies at a time. The MAH out-sourcing the QPPV must ensure that:\n- This person is appropriately qualified and has worked in PV\n- The relationship “will work in practice”\n- The QPPV can implement necessary changes to the PV system\n- The system will still function if the contract with the out-source company is terminated or it goes out of business or if there are mergers or acquisitions.\nFinally, some countries in the EU have established national QPs whose jurisdiction is only in that country. This is in addition to the EU/EEA level QPPV. Sometimes these national QPs are out-sourced and sometimes they are company employees. They may or may not directly report to the EU level QPPV. In any case, clear SOPs and responsibilities must be in place in written SOPs that define the system(s). The company/MAH must verify in each member state what local requirements are necessary such as a QP in order to market a product.\nThe QPPV is a major responsibility that all MAH’s in the EU must fulfill and fulfill well. This is not a routine job but one that must be done well. Although safety issues may arise less in smaller or generic companies, the potential for a safety issue (or safety crisis) always exists and it is at these times that the QP becomes an important player. Similarly, during government inspections the QPPV will also play a major role. Thus the QP should be sufficiently senior, have authority, responsibility and the appropriate “power”. Although not necessarily a physician, it is wise that the QP have some level of medical training and experience. It is an important job and should be so treated.']"	['<urn:uuid:7267456c-f25d-4987-be46-749bd382eee1>', '<urn:uuid:19982049-2453-4d53-b756-f8eed757c84d>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T17:51:21.175311	36	103	3662
88	What are the signs of high ammonia levels in a new fish tank, and how does the nitrogen cycle naturally process this toxic compound over time?	High ammonia levels in a new tank can cause fish to show signs of stress including gasping, shaking back and forth, clamped fins, lack of color, and refusing food. As for the nitrogen cycle process, ammonia from fish waste and breathing is first converted to nitrite by beneficial bacteria that grows in the gravel and filter over 1-3 weeks. Then a second type of bacteria develops over up to 5 weeks to convert nitrites into less toxic nitrates. The full nitrogen cycle typically takes 4-6 weeks to complete, at which point ammonia and nitrite levels should read zero while some nitrates are present.	['Normally, it takes 4-6 weeks for the growth of beneficial bacteria to complete the nitrogen cycle in a new aquarium. It is not unusual for seeded aquariums to fully cycle in half the time it would normally take, thus allowing you to stock more fish in the new tank sooner.\nHow long do you have to wait to put fish in a new tank?\nFilling Your New Aquarium After purchasing your new aquarium, make sure to set it up, add water, substrate and plants and allow it to settle for at least 24 – 48 hours before adding fish. Once you’ve got your aquarium set up, you’ll be ready to choose which fish will populate your tank.\nHow do I know when my tank is cycled?\nAfter testing your aquarium water for ammonia and nitrite and nitrate, if the reading shows 0 ammonia, 0 nitrite, and some nitrates then your fish tank is cycled. Cycling a new tank usually takes between four to six weeks. Cycling your fish tank can take a long time.\nCan I add fish after 24 hours?\nIf you introduce bottled bacteria and water from an established tank at the shop, the nitrogen cycle can be completed after 24 hours. It will take about a day for the chemicals to eliminate chlorine and bacteria to bring ammonia and nitrates to healthy levels for fish to live in.\nHow do I cycle my fish tank in a week?\nFish-In Cycling When setting up your aquarium, only add a few fish at the start. Feed your fish very lightly at first and then gradually increase the amount over the next four to six weeks. You can significantly speed up the aquarium cycling process by adding beneficial bacteria from the onset.\nCan you put fish in a new tank the same day?\nYou can add fish as soon as tank water is dechlorinated and to temperature, as long as you add the right bacteria, and the manufacturer states that it is possible to do so. But for the best possible (safest,) results start to Fishless Cycle on the day that the water is dechlorinated and to temperature.\nHow do I cycle my fish tank in 24 hours?\nSimple Methods to Accelerate Cycling If you have access, try using an old filter from a different tank. In this way, the filter will already contain all the healthy bacteria that your tank needs. Add filter media from an old tank. Increase water temperature. Use live bacteria. Increase oxygen levels.\nDo nitrates mean tank is cycled?\nWhen nitrates are being produced and ammonia and nitrite levels are zero, your tank is fully cycled and your biological filter is fully functioning (from 2-6 weeks). In low levels, nitrates are not highly toxic to fish. Routine partial water changes of about 10% should keep nitrate levels within a safe range.\nShould I water change during cycle?\nWhile not essential, we recommend water changes during cycling, although opinions differ. Since bacteria live on surfaces, removing water does not disrupt their development. Water changes can help control the amount of ammonia in the first stage of the aquarium’s life.\nHow long can a fish stay in a bag?\nSome say that fish can last 9 or 10 hours in a bag (or even a day or two in some cases). However, it’s best for you and your fish if you stick to leaving your fish in the bag for 5 to 7 hours. A lot of fish can stay alive without oxygen for 2 days in shallow water.\nDo you have to let water sit before adding fish?\nOrdinary tap water is fine for filling up the aquarium as long as you let it sit for several days before adding fish (the chlorine in the tap water will kill the fish).\nCan you put cold water fish straight into a new tank?\nIf your aquarium water is ready, we would advise stocking the tank very slowly to avoid filter overload – one or two fish to start with, then if there is no deterioration in water quality after 2 weeks, then you can add another one or two fish.\nWhat fish are good for cycling a tank?\nIn the first few weeks of having your aquarium, you should add plants into the environment and ‘good cycling fish’ such as most types of minnows, guppies, barbs and danios. They will be able to survive the high toxins for long enough to allow the beneficial waste-processing bacteria to grow.\nCan you cycle a tank without a filter?\nWith the use of starter bacteria products, cycling time can be shortened to a week plus. After filling the tank, we recommend letting the tank soak for a couple of days without running the filter. Do a 90% water change and dose another dose of starter bacteria culture. Wait a day and take ammonia/nitrate readings.\nHow often should you do water changes when cycling a tank?\nThere are different philosophies on how much and how often to change water, but 10% to 25% every 1 to 2 weeks is a good rule of thumb. Small frequent water changes are best. Don’t vacuum the gravel yet, as you may disrupt the good bacteria that are just starting to colonize your aquarium.\nWhat happens if you put fish in an Uncycled tank?\nAn uncycled tank can be dangerous and even lethal for your fish, as ammonia will quickly harm and kill any fish living in even small concentrations. The Cycle is what happens in your filter. Filters can be internal, external (canister), hang on back (HOB) or even an extra tank called a ‘sump’.\nWhat is new tank syndrome?\nSymptoms. New tank syndrome leads to ammonia toxicity in the fish, which can quickly become fatal. Fish will often die suddenly, without warning. The aquarium water is frequently cloudy and smelly due to the excessive ammonia and nitrite levels.\nHow long before ammonia turns to nitrite?\nAt about ten days into the cycle, the nitrifying bacteria that convert ammonia into nitrite, Nitrosomonas, should begin to appear and build. Just like ammonia, nitrite can be toxic and harmful to marine animals even at lower levels, and without nitrite present, the cycling process cannot complete itself.\nHow long does it take for nitrite to turn into nitrate?\nThis process normally takes anywhere from 2-6 weeks. At temperatures below 70F, it takes even longer to cycle a tank. In comparison to other types of bacteria, Nitrifying bacteria grow slowly.\nDoes ammonia turn into nitrate?\nNitrification is the process that converts ammonia to nitrite and then to nitrate and is another important step in the global nitrogen cycle. Most nitrification occurs aerobically and is carried out exclusively by prokaryotes.\nHow many fish can I add to a cycled tank?\nThe larger your aquarium, the more fish you can include in a fish-in cycle. Rule of thumb: One fish for every 10 gallons of water. So, if your tank is 1 gallon, only 1 fish. If your tank is 58 gallons, 5 fish.', 'The addition of an aquarium in your home should be a relaxing and enjoyable experience. Unfortunately, many people find the first four to six weeks frustrating.This tip sheet will help to explain what is happening in your aquarium after you put in your first fish, and what you can do to curb problems that will occur.\nEvery new aquarium must go through a process called the “nitrogen cycle.” Since the nitrogen cycle only begins when new fish arrive in an aquarium, we call this cycling process, “New Tank Syndrome.” Understanding New Tank Syndrome is important for learning how to keep your fish happy and healthy.\nIt takes approximately five to seven weeks for the cycle to run through. It is during this time that many fish enthusiasts feel frustrated. Why is my tank cloudy? Why can’t I put in all of the fish I want all at once? Why did some of my fish die?\nA key factor in enjoying this relaxing and rewarding hobby is realizing that your aquarium is much like a mini eco-system. In order for your fish and plants to thrive, specific care must be taken, right from the beginning, to ensure a healthy environment.\nBefore we discuss the steps that your aquarium will go through after adding your first fish, we need to talk about toxic ammonia!\nFish take in oxygen and release carbon dioxide into the water – that’s how they breathe. During this process, a by-product called ammonia is also released. Even more ammonia is released into the water as your fish eat and produce natural bodily wastes. Although these are natural functions, the ammonia is toxic to your fish. It’s understandable that this ammonia problem could be compounded if too many fish are added to the tank in the beginning. More fish = more breathing = more bodily wastes = ammonia build-up! On top of all that, some people tend to over feed their fish. Over feeding contributes greatly to high ammonia levels. Ask your Petland pet counsellor exactly how much food to feed your new fish.\nThere are many things which contribute to the production of ammonia; however, ammonia is needed, in proper amounts, to begin the cycling process. Since ammonia starts the nitrogen cycle, it stands to reason that fish must be added to start the cycle.\nAdding Your First Fish\nThe type and quantity of fish will depend on the size of aquarium you have and the species of fish you want to keep. The first fish that are added should be hardy.\nOnce you have added your first fish, they will immediately begin to produce ammonia (as we discussed above). For the first day or two, your fish and aquarium will look normal; however, don’t be surprised if after day two (sometimes up to day four or five) if your aquarium water goes cloudy. This is called “bacterial bloom.” This cloudiness is caused by initial good bacterial growth and is not harmful to your fish. It will clear up on its own. As you will see, you need this bacteria growth for a healthy aquarium. At this point, there is rarely a cause to perform a water change.\nDuring the first week that your aquarium is set up with fish, bacteria will begin to grow in the gravel. It will also grow in many other places, such as your filter’s carbon and sponge inserts. This is “good” bacteria, which will consume the toxic ammonia and convert it into a chemical called “nitrite.” It takes a while to grow this good bacteria; usually one to three weeks, at which point it will be abundant enough to handle the ammonia in your tank. Remember, more fish in your aquarium means that more ammonia is being produced, so it is best to add them slowly.\nWhen your tank is going through its cycle, partial water changes done with a gravel vacuum are necessary to help keep the high toxic levels down. Reduce the waste load in your aquarium by performing small water exchanges (25%) once a week for the first five weeks. A Petland pet counsellor will be happy to show you how to use your gravel vacuum. Water conditioners and supplements, such as Nutrafin® Cycle®, should be added after each water change. Once your tank is established (approximately five to seven weeks), a 25% change must be done every three to four weeks to keep your fish happy and healthy. An aquarium operating for six months or longer, will require fewer water changes.\nThere will be a significant drop in your aquarium’s ammonia level once the bacteria population is hefty; however, your concerns should not be lessened. Remember, that the good bacteria changes ammonia into NITRITE. At this point in the cycle, a second type of bacteria develops, which helps to consume nitrites.\nNitrites converts into a compound called “Nitrates.” Nitrates, unless found in high levels, are generally not toxic to your fish! Unfortunately, this second type of good bacteria is slow in multiplying. It may take up to five weeks for this bacteria to gain control over the nitrites in the water.\nHigh levels of nitrates act like fertilizer, producing a carpet of algae on the tank floor and walls. You can help prevent this problem by adding live plants to your aquarium. Live plants will help to consume the nitrates, thus reducing algae growth.\nStress from the ammonia and nitrites will undoubtedly have some ill effects on your fish if some preventative measures are not taken. Aquarium test kits should be used to monitor ammonia, nitrites, pH and other water quality conditioners at the beginning and through the life of your aquarium. Your Petland pet counsellor can show you how to test your water. Signs of stress in your fish may include gasping, shaking back and forth, clamped fins or lack of colour. They may also refuse food. Should your fish exhibit any of these signs or are not behaving normally, contact your Petland pet counsellor.']	['<urn:uuid:49cd36e3-2ed2-4538-a9cb-ce9a692cfb33>', '<urn:uuid:886dfb15-1ea3-49db-ac1d-ca00ee512df4>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	26	103	2172
89	want paint walls historic building ireland do i need special permission first	For a protected structure, even minor works like interior decorating such as plastering or painting may require planning permission if they would affect the character of the structure. You need to obtain a declaration from the local authority about what types of works would or would not materially affect the building's character.	['What is a Protected Structure\nA protected structure is a structure that a local authority considers to be of special interest from an architectural, historical, archaeological, artistic, cultural, scientific, social or technical point of view. Details of protected structures are entered by the authority in its Record of Protected Structures, which is found in Schedule 2 of the South Dublin County Development Plan 2016-2022. Each owner and occupier of a protected structure is legally obliged to ensure that the structure is maintained and safeguarded from endangerment.\nThe protection of a Protected Structure also extends to and includes all parts of the structure, including its interior, all land around it, and any other structures on that land. The obligation also applies to all fixtures and fittings forming part of the interior of a protected structure or of any structure on land around it. If immediately before 1 January 2000 a structure was listed for preservation or protection in a development plan, that structure automatically became a protected structure on that date. Otherwise, a local authority must follow certain procedures if it proposes to deem a structure to be a protected structure. These involve notifying the owners and occupiers of the structure, the Minister for Arts, Heritage, Gaeltacht and the Islands, and other bodies of the proposal. An owner or occupier is entitled to make comments on such a proposal to the authority.\nThe Record of Protected Structures (RPS) is listed under Schedule 2, South Dublin County Development Plan 2016-2022 written statement and the location can be identified using the Map/RPS Ref. number (allocated to each item) on our County Development Plan Maps. If in doubt whether a building or structure is protected please check the Record of Protected Structures or seek clarification from the Planning Department, South Dublin County Council.\nObligations of Owners Each owner and occupier must ensure that a protected structure or any element of a protected structure is not endangered through harm, decay or damage, whether over a short or long period, through neglect or through direct or indirect means. A local authority may: -\n- require an owner or an occupier of a protected structure to carry out works if it considers that the structure is or may become endangered;\n- require an owner or an occupier of a protected structure to carry out works if it considers that character of the structure ought to be restored;\n- acquire, by agreement or compulsorily, a protected structure if it considers that this is desirable or necessary in relation to the protection of the structure. Where a local authority requires works to be carried out to prevent a protected structure from becoming or continuing to be endangered, the owner or occupier concerned may be eligible for a grant under the scheme of grants for the conservation of protected structures.\nDevelopment Control and Planning requirements\nUnder the planning system, many minor works to structures do not normally require planning permission. These works are known as exempted development. However, for a protected structure, such works can be carried out without planning permission only if the works would not affect the character of the structure or any element of the structure that contributes to its special interest. Depending on the nature of the structure, planning permission could, for example, be required for interior decorating such as plastering or painting.\nA declaration from the local authority is necessary as to the type of works which would or would not materially affect the character of the structure.\nDeclarations (Section 57)\nAn owner or occupier of a protected structure may request the local authority to issue a declaration indicating the types of works that could be carried out without affecting the character of the structure or any element of the structure which contributes to its special interest. These works would not require planning permission. A local authority will, in general, issue such a declaration within three months of receiving a request. A Section 5 Declaration may also be used for a Protected Structure where the owner/occupier has specific works in mind.\nScheme of Grants for Protected Structures\nApplicants are invited on annual basis for consideration under the Conservation Grant Scheme for Protected Structures which operates each year to assist owners and occupiers to carry out conservation works to their protected structures. The scheme is part of a package of measures introduced by the Department of Environment, Heritage and Local Government to strengthen the protection of the architectural heritage. Details here']	['<urn:uuid:e69b957f-9a80-43ee-9732-c3351784b20b>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T17:51:21.175311	12	52	745
90	What are key factors to determine bathroom size requirements?	The size requirements for bathrooms should be based on who's using the bath, when it's being used, and how it's being used. For example, if it's a shared bath used frequently by family and guests, it's recommended to include a larger standalone shower rather than a bathtub. For universal design considerations, bathrooms should have wider walkways (minimum 42 inches), at least 60 inches of clear turning radius floor space, and 3 feet of space beside the toilet to accommodate wheelchair access.	['With their expense and critical importance to a cabin, kitchens and baths are spaces every homeowner wants to get right. A design misstep in these spaces can lead to daily annoyances at best and unsafe conditions at worst. Careful consideration of these spaces when you’re building new or renovating an existing cabin will pay off in ease of use, efficiency and comfort. Take a look at our tips below, but also consider working with a kitchen and bath designer. Because design pros configure layouts and specify materials and fixtures every day, they can offer solutions and ideas that might not occur to you.\nCabin Kitchens 101\nIf you want a beautiful, hardworking kitchen like this one, hire a professional designer to make that happen for you.\nCourtesy Rocky Mountain Log Homes\nKnow what your kitchen budget is? Good, because that number will help you make decisions throughout the design process. If you’re building a new cabin, Juanita Galliford from Bosch Home Appliances offers this tip: “After taking into account appliances, cabinetry, flooring, plumbing and more, I would estimate about 30% of the build budget going toward the kitchen.” Share your budget with your designer so he or she can help you make the most of it.\nOne of the first decisions you’ll make concerns the size of your kitchen. If you’re renovating, you may be confined to your existing kitchen’s space. Those planning a new cabin have a bit more leeway. While there are no set rules for the best kitchen size, most cabin owners lean toward an open or partially open kitchen. Fortunately, this arrangement can make a small kitchen feel larger. Think about just how open you want your kitchen to be to other living spaces. Do you want the kitchen visible to most of the main living areas, or would you prefer some visual separation? Are there kitchen functions you would like to close off (as with a closet-style pantry for food and supplies storage)? One way to estimate the right-sized kitchen is to list who will use the kitchen, how often and for what activities. Will you and overnight guests frequently prepare meals together? Do you have pets who will eat in the kitchen? Do you prefer to make meals alone, while still interacting with those not in the kitchen? Your designer should keep answers to these questions in mind when putting your new kitchen on paper.\nGuidelines published by the National Kitchen and Bath Association can help, too. (You can order a copy through the online store at nkba.org.) Remember these are only guidelines, so don’t worry if your specific circumstances can’t accommodate certain measurements. You must, however, pay attention to building codes applicable to your location. Some of the NKBA guidelines apply to the work triangle – the path between your kitchen’s cooking, food prep/clean-up and refrigeration areas. So, the points of your work triangle might include your cooktop, refrigerator and sink, for instance. The NKBA suggests that the total distance between the triangle points should be no more than 26 feet. The guidelines also recommend no less than 4 feet between points and no more than 9 feet. A well-placed work triangle can help you move efficiently in the space and avoid a situation where an island or wall blocks a well-traveled path.\nThe NKBA guidelines also suggest the amount of space that should be allowed around appliances and work centers. Review your design to make sure appliance doors can open fully without being blocked by cabinetry, entry doors or other appliance doors. As your kitchen design starts to jell, keep an eye on traffic patterns through the space. For example, does access to a deck mean that people would often walk through the work triangle? For safety’s sake, that’s a situation that should be avoided.\nWhile it might seem like more storage is always needed in a kitchen, the NKBA recommends certain shelf and drawer frontage totals. In terms of work space, the guidelines recommend one continuous counter run at least 36 inches long and a total of 158 inches of countertop. Whether you adhere to these guidelines will depend on your space constraints, who will use your kitchen, and your personal collection of dishes, gadgets and tools. A kitchen renovation may be the perfect time to pare down belongings, while keeping in mind what is necessary to feed the typical number of people who spend time in your cabin. Access to outdoor cooking and eating spaces is often critical for cabin kitchens. Some cabin owners devise ingenious pass-throughs to outdoor spaces, while others create mini-kitchens outside to make grilling and eating al fresco more convenient. Whatever option you choose, discuss it with your kitchen designer and visually walk through your plan to see how well it will accommodate meals prepared or eaten outside.\nCabins that are shared by several families at once can face unique kitchen challenges. Open shelves can make it easier for everyone to put things away. Extra storage should be considered for larger amounts of food. In this situation, will space be needed to store large coolers? Would double ovens help feed a crowd? Keep walkways as wide as possible to allow for more people to work together in the kitchen. Consider adding a small prep sink and possibly an ice machine or small refrigerator near an outdoor living space to help free up traffic in the kitchen.\nSmall Kitchen Tricks\n“Just because a kitchen is small, doesn’t mean it has to feel that way,” Galliford says. She suggests using smaller-sized appliances that look seamless, “especially when planning a cabin or secondary home with a compact floor plan. “I also like incorporating pieces that can multitask – like Murphy-style kitchen tables or hooks that provide additional storage space on the backside of cabinet doors,” she says. Flooding a smaller kitchen with natural light will also make it appear larger. “I like to bring as much natural light into the space as possible by using skylights or windows in place of the traditional backsplash.” Glossy surfaces like stainless steel, reflective cabinet finishes or shiny flooring can also help.\nCabin Bathroom Design\nAt the cabin, a walk-in shower can make more sense than a tub, especially if the bathroom is used by both the cabin owners and their guests.\nHeidi Long photo\nWhether you’re designing a new bathroom or planning to renovate an existing bath in your cabin, designer MaryJo Camp of DesignCamp in North Carolina encourages you to consider the following questions:\nWho’s using the bath,\nWhen is it being used, and\nHow is it being used?\nIs the bath a master bath for two of you to use every day? A guest bath used infrequently by another couple? A Jack-and-Jill arrangement shared by children? Consider a common cabin space with a full bath that’s used frequently by family members and overnight guests. For this room, Camp suggests including a larger, standalone shower and skipping the bathtub. “Most guests are not going to luxuriate in a tub – and most tubs aren’t that luxurious anyway,” she says. “Showers are easier to use for most people.”\nIn any bath where space is tight, Camp recommends using a wall-hung toilet. Because the water tank is recessed into the wall, the toilet can be squeezed into a smaller space, however, this type of toilet tends to be more expensive. For the vanity, weigh the need for double sinks versus providing enough counter space and usable storage. Camp prefers vanities that look like chests of drawers and have functioning drawers with cut-outs for the sink’s plumbing. Unlike cabinets where items can get lost toward the back, drawers make it easy to see what’s on hand, Camp explains. And they are deep enough for items like toilet-paper rolls.\nTo make the most of every inch, look for ways to recess medicine cabinets and other cabinets or shelves between wall studs. Those extra few inches can make a cabinet more accommodating for bulky things like folded towels. Does it make sense to compartmentalize the toilet and shower in a shared bath? Camp says it’s a question to discuss with your family and bath designer, but suggests you can get the same feeling of privacy from a half- or full-height wall near the toilet.\nBathrooms sometimes overflow with “stuff.” These tips can help you put things in their place:\nTry using wall hooks instead of towel bars to fit more towels on a wall and help them dry a bit faster, Camp suggests.\nNiches, ledges or corner shelves built in to the shower can provide landing spots for soaps and shampoo. Keep cleaning in mind when designing them.\nThink vertical: Use wall-hung shelves, pegs or baskets to stash necessary objects.\nWith open shelving, use baskets or boxes to control clutter, but be sure to label containers’ contents so everyone can find what they need.\nMake use of the back of the door for hooks or hanging organizers.\nIf you have a large open space under the sink, add pull-out shelves or baskets to keep items like cleaning supplies or blow-dryers handy but off the countertop.\nPlanning to retire to your cabin, or just want to make your kitchen and baths welcoming to people of all abilities? Research the fundamentals of universal design. In general, these guidelines cover: Safety, with suggestions like adding grab bars in bathrooms and roll-in showers to make bathing safer. Specify non-slip flooring or tiles. To increase ease of mobility, minimize stairs or level changes. Temperature regulation valves on showers protect against possible scalding. Faucets should be accessible from both inside and outside the shower. Space planning calls for wider walkways (about 42 inches minimum) and larger baths (with at least 60 inches of clear turning radius floor space and 3 feet of space to one side of the toilet) to allow for the use of a wheelchair. Expand door widths to 34 or 36 inches as you’re remodeling, if possible. Clear knee space under sinks in both bathrooms and kitchens allow someone to use them while seated. Lighting guidelines suggest adding layers of lighting to kitchen and baths, with general illumination overhead in addition to task lighting at work centers or mirrors. Consider installing permanent night-lighting fixtures.']	['<urn:uuid:0e36050a-93ec-4be8-9717-81f1841b4247>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T17:51:21.175311	9	81	1705
91	similarities between viking longhouse modern fire festivals	Viking longhouses and modern British fire festivals share communal gathering aspects. The longhouses hosted large feasts and offered hospitality to travelers, similar to how current fire festivals like Up Helly Aa bring communities together with processions of up to 1,000 participants. Both traditions emphasize collective celebration and maintain Norse cultural elements, with the festivals specifically incorporating Viking symbols like galley ships and traditional songs.	['Longhouses are one of the most iconic relics of the Viking Age and Norse culture in general. They’re instantly recognizable, absolutely fascinating, and still garner a lot of attention and many curious questions.\nViking longhouses were large wooden buildings that were home to a few families. Multiple generations lived under one roof, and there wasn’t much privacy. Longhouses contained separate rooms for domestic animals. The people tried to make the houses as comfortable as possible and often held feasts.\nThis article explores the fascinating longhouses of the Viking Age. It presents 11 bite-sized facts that one must know if they’re interested in Norse culture.\n1. Viking Longhouses Were Made of Wood\nViking longhouses were mainly made of wood, an abundant material that was relatively easy to work with.  The roofs were sometimes covered by tufts, and walls were often lined with clay or mud for additional protection from the cold. Additionally, straw or moss would be stuffed between the walls for added insulation. \nLonghouses were made from planks vertically stuffed into the ground. There were also internal posts that supported the roof and held the whole building together.\n2. They Required Constant Repairs\nViking longhouses had to be repaired very often. The materials they were made of were often not sturdy enough to withstand the harsh weather conditions without sustaining damage. This was particularly the case in damp areas, where the humidity damaged the wooden walls, which had to be mended.\n3. They Were Crowded\nThe privacy that modern homes allow was not commonplace in the Middle Ages. It was especially not commonplace in the longhouses, which were mostly populated by more than one family and multiple generations.\nBigger houses often accommodated dozens of people, so they got rather crowded and offered little privacy. Life was communal, and people lived and worked together, most often in a single room.\n4. Vikings Kept Their Domestic Animals Inside With Them\nNorse homesteads usually didn’t contain separate buildings for domestic animals such as pigs or cattle. Instead, there were rooms for those animals inside the houses. This might seem awfully unhygienic to modern people, but it wasn’t so unusual in medieval Europe.\n5. They Were Smoky\nNorse longhouses did not have chimneys. The smoke escaped through a hole in the roof. This meant the houses were often filled with smoke if the smoke couldn’t escape. Those holes would also probably cause problems in rainy or snowy weather.\nThere was usually one big open fireplace used for cooking and keeping warm. People would often huddle near the fire to keep warm during the long cold Scandinavian nights.\nApart from there being no chimneys, there were also no windows. This must have made the longhouses very difficult to air out properly, adding to the aroma of smoke lingering inside them.\n6. They Were Lighted With Oil Lamps\nThe lack of windows meant that the only natural light that reached the houses came through the doors. This was not nearly sufficient, and the people living inside those houses needed artificial light.\nFor that purpose, they used oil lamps, which were not difficult to procure. These relied on fish oil or whale oil, which was abundant, considering that there were many fishermen and whalers in those coastal Scandinavian regions.\n7. The Vikings Tried To Make Their Houses Comfortable\nDespite the harsh living conditions, the Norse who occupied longhouses tried to make the houses as comfortable as possible. Wooden benches and beds were built next to the walls, and they were covered with skins for warmth, while the pillows were stuffed with feathers for additional comfort.\nRicher people would adorn their walls with shields, tapestries, rugs, and other decorations. Anything that could be used to decorate a longhouse was used to do so. Plus, this showed that the owner was wealthy and that the house was prestigious.\n8. Viking Longhouses Were Havens of Hospitality\nA big, long house that can host dozens of people is the perfect place for a feast. The Norse knew this and used this possibility to the maximum. Feasts were common, and there were rules on how to behave as a guest and toward guests, contrary to the idea many people have of the Norse as uncultured and rowdy barbarians.\nHospitality was also extended to weary travelers. Looking for shelter and food on a Norse homestead was normal, and the owner was expected to accept the travelers in his home.\n9. They Weren’t Just for Vikings\nStrictly speaking, the word “Viking” refers to the fierce seafaring warriors who conducted raids across Europe. The word comes from the Anglo-Saxon word wīċing, meaning simply “pirate” or “raider.” \nThis means that not every Norse person was a Viking. Most were common people, farmers, craftsmen, and traders, rather than raiders. However, longhouses were common dwellings for all of those common Norsemen.\nIn fact, longhouses were mostly farmhouses, which means that the people inhabiting them were mostly farmers and not fierce warriors. However, the word “Viking” colloquially designates all Norsemen, whether or not they took part in raids, so the houses were stuck with the name “Viking longhouses”.\nLonghouses were also common in Anglo-Saxon England, and they were quite similar to the ones in Scandinavia. They were also loved by different Germanic peoples across Europe, probably due to their simple design and practicality.\n10. Their Sizes Varied, But All Were Long\nThe longhouses varied in size, but the name definitely isn’t misleading. On average, they were 16.4 to 23 ft. (5 to 7 meters) wide and 49.2 to 246.1 ft.(15 to 75 meters) long.  Even for today’s standards, that would be a decently-sized house. With a bit of redecorating, an average longhouse could probably be turned into a decent modern family home.\n11. The Largest Discovered Longhouse Is Over 200 Feet Long\nThe biggest longhouse ever found was excavated in 1983 in Lofort, Norway. It was most likely a chieftain’s house, and it was initially 220 ft. (67 meters) long. It was built around 500 AD and upgraded around 700, extending it to 272 ft. (83 meters).\nAt the site, tourists can visit the Lofotr Viking Museum, which is a reconstruction of the longhouse. There are several more longhouses in the area waiting to be excavated, so they might find an even longer longhouse!', 'UNITED KINGDOM — Winter in the UK is often a dull and dreary affair. The winds are cold and biting, the skies are grey and loaded with drizzle. Any snow, with its temporary sense of wonder and magic, tends to be short-lived. So what do we have to get us through the Winter Fire festivals!Britain, Scotland in particular, has a long history of winter fire festivals to mark the end of Yuletide and welcome the returning spring and days of more sun. Two well-known festivals are the Burning the Clavie and the world famous Up Helly Aa. Burning the Clavie takes place in the Moray region and harkens back to the old custom of tar barrelling. Up Helly Aa is described by Bryan Peterson of Shetland Arts as “36 hours of lawlessness, where by-laws are bypassed, marital vows are suspended and health and safety becomes very subjective.”\nPeterson adds: “For many Shetlanders, it’s bigger than Christmas and New Year put together.”\nOver recent years, the Up Helly Aa festival has seen an explosion of popularity. The event, whose name roughly translates as “End of Holy/holiday time for all,” marks the end of Yuletide on the Shetland Isles. Its date can alter as it is celebrated on the last Tuesday in January.\nThe Up Helly Aa tradition is an old one and originally involved the practice of tar barrelling, where barrels of tar were set alight and rolled around the streets in a procession with accompanying mischief making. This is likely to be a nod to the Lord of Misrule antics that often traditionally accompany festivals around such as this time. Since Christianity’s arrival, these traditions have been more associated with Epiphany.\nTar barrelling was phased out during the mid 19th century, and the more modern ceremony slowly evolved over that period. By the turn of the century, the galley ship and torchlit precession were firm fixtures of the Up Helly Aa festival. Today, the procession consists of roughly 1,000 torchbearers, known as guizers, making their way to a central spot where a huge Viking galley awaits.\nThe guizers form different squads, and the whole event is led by the Guizer Jarl, who has his own squad of between 50 and 70 men. This group has formed the Jarl Squad for Up Helly Aa since the 1920s. The squad dresses in Viking regalia and calls themselves Vikings for a day.\nA new galley is custom built every year for the festival. As the procession gathers, the guizers move around the boat in a spiral with each member throwing their torch into the boat. As the boat catches fire, wishes for the new year are taken to the gods. The current Up Helly Aa celebrations are held at 10 different locations on Shetland.\nThe festival is also a celebration of Shetland’s Viking roots.The Norn language, directly related to Old Norse, only died out in the mid 19th Century, and the current dialect (Shetland Scots) spoken there has many Norse-related words in its vernacular. Due to its location – it is roughly the centre of a triangle between the Northern coast of Scotland, Norway and the Faroe Isles – Shetland has a prominent Viking history and many of its traditions and dialect share a Scandinavian lineage.\nEven the costumes for the festival are given the utmost care and attention and are an important part of the ceremony. The Guizer Jarl wears a full suit of Viking armour which dates to the 1930s. Every year a new shield and suit design is commissioned for his men and preparations often begin up to two years in advance. Other squads are tasked with making their own suits.\nThe procession begins in the morning, when squads march into town and read The Bill which sets out the instructions for the day. This is usually light-hearted and informal direction. As the sun sets, which is early on Shetland, the squads make their way to the galley. As they approach the boat, they sing the Up Helly Aa song, followed by the Galley Song. Later they sing the Norseman’s Home as they come away from the boat. By that point, the galley is ablaze and is returning to Valhalla as described by Northern Tradition.\nThe guizers then retire to the local pubs, where lots of after-parties and dances take place, often into the next morning. The next day, thankfully, is a public holiday.\nThe popularity of these festivals continues to grow, drawing bigger and bigger crowds. Up Helly Aa is now streamed on You Tube. And other fire festivals are beginning to take root as Britain rediscovers its often neglected Viking links.\nIn the remote headland of Flamborough, East Yorkshire, a New Year’s Eve fire festival began two years ago as a celebration of the area’s Viking history. Like Up Helly Aa, the Flamborough Fire Festival also has a torchlit procession, during which a Viking longship is drawn through the streets.\nThis is a reenactment of an 8th Century Viking invasion at Flamborough and the settlements that existed there for hundreds of years since. The crowds are encouraged to throw coins into the longship for good luck. Throwing money into wells and fountains is a well-known European custom, and it manifests here in this newer event.\nThe Flamborough festival also has fire swallowers as part of its procession, which is said to be based on an old Northern tradition of using fire to clear the air to make way for the new energy of the year.\nSpeaking to the Scarborough Evening News, organisers Jane Emmerson and Pam Sayer said: “The Festival is a very special event to celebrate Flamborough’s Viking heritage, as we believe Flamborough is the home of the Vikings.”\nThe Vikings may be gone but more and more Britons are keeping their fire burning.']	['<urn:uuid:f9f95b57-a4a0-4fc6-b953-ed45a66a377b>', '<urn:uuid:e1858eaa-9a57-488a-93ec-60bd2d674038>']	factoid	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T17:51:21.175311	7	64	2012
92	viking clothing social class differences slaves elite garments materials distribution	Viking clothing clearly reflected social hierarchy through materials and distribution patterns. While elites had access to imported silks and fine furs, slaves typically wore their master's cast-off underclothes and work clothes as their outer garments. These hand-me-downs were reasonably fine but often patched and worn thin. However, heavily decorated garments were not typically handed down to slaves but rather donated to churches or passed to classes immediately below them. The quality of clothing diminished with lower ranks, though even slaves didn't wear the rough 'sack-cloth' as previously believed.	"['A question that is often asked of Regia members by members of the public is ""They wouldn\'t have had cloth that fine and soft would they?"". Well I have recently been doing a project on Anglo-Scandinavian cloth with particular reference to finds from York at the Archaeological Resource Centre there. This project has involved researching textiles not only from York but also from other Viking Age sites to enable me to draw parallels. I hope that the information I have accumulated will be of use to Regia members and others interested in this period.\nOne of the most important things to remember is that the recovery of textiles from digs is quite rare and is even rarer for large quantities of textiles to survive. This is due to the fact that textiles require anaerobic conditions, i.e. to be waterlogged and without oxygen, in order for them to survive. As some of you will know there are very few sites in Britain that have these conditions, but `ghosts\' of textiles sometimes survive in the corrosion products of metalwork, particularly on jewellery. The evidence preserved in these corrosion products is, however, generally very small, but it is still of use as the weave and thread size can usually be detected, though often with difficulty.\nNormally very fine textiles are found in graves and coarser textiles on settlement sites. This does not mean that people were only buried in fine cloth, but wore coarse clothes in life. What it suggests is that the finer cloth found in graves is cloth worn every day as clothing and that the coarser fabrics were used for everyday tasks and industrial purposes. In a town such as York or Dublin these coarser fabrics would also include sail-cloth and tarpaulins. Occasionally fine cloth is found on settlement sites e.g. Hedeby in Schleswig-Holstein where the cloth had a secondary use as caulking for boats after the cloth had served its useful life as clothing and it was not feasible to mend it any more.\nObviously the quality of clothing worn would diminish with lower ranks, but even the slaves were not wearing very rough \'sack- cloth\' as people have thought in the past and some still do today. Slaves would also probably wear their master\'s cast-offs, which would be reasonably fine, though they may have been very patched and worn thin in places. However this does not mean that slaves were going round wearing highly decorated, but patched, clothing. The slaves would receive the underclothes, i.e. under-tunics, undresses, work clothes etc., to be worn as overclothes. If the upper classes did \'hand down\' their more decorated garments then it would generally have been to the classes immediately below them or to their children. However this handing down of decorated garments does not seem to have happened. Of the most heavily embroidered garments, the habit was to donate them to the church, which were consequently embellished even more for the bishops etc.\nFrom sites in York and elsewhere in Europe, particularly Scandinavia, very fine cloth has been recovered, typically with counts of between 14x11 and 24x12 threads per cm. (first number = warp, i.e. threads hanging down, second number = weft, i.e. threads passing through). More often than not though, the greater number of textiles recovered from sites are of medium fineness, typically with thread counts of 10x7 to 15x9 threads per cm. These are, nevertheless, still reasonably fine fabrics and much finer than some of the cloth some public expect us to be wearing.\nEvidence from York and later Saxon London have shown that the majority of wool used was of a \'hairy medium\' type, meaning that the staple - the length of the individual fibres - was quite long allowing very fine yarn to be spun and, consequently, very fine fabrics to be produced, though they would not be exceptionally fine and it is quite possible that they would still have quite a rough, but smooth, feel to them. It does seem though that the people in the places settled by Scandinavians were using more hairy wools than those in the Anglo-Saxon settlements, so maybe here we are seeing an early north/south divide.\nThese hairy wools found on Scandinavian sites seem to equate with the modern mountain breeds of sheep; so maybe we are seeing here the ancestors of these sheep, which would appear to have been brought to Britain by Scandinavian settlers. We do not know what sort of breed was native to the Anglo-Saxon areas of England in this period unfortunately, but we can assume that it would be reasonably similar to the breeds living there today. All of these wools would produce quite fine cloth though, and if the cloth were fulled then the cloth would be even finer and smoother. Fulling mills from the later Viking period have been found in Britain, particularly in Fair Isle and the north of Scotland so it is highly possible that the Vikings and the Anglo-Saxons were producing fine fulled cloth. Obviously they were able to import even finer cloth, and silks, from the Mediterranean and even further afield, but it is often difficult to distinguish these imported cloths from native cloths in the archaeological record.\nThere is also a tradition in the north of Scotland and the Orkney and Shetland Isles that the cotton grass which is often in wetland in upland areas was spun and made into a type of fine cloth, but we cannot tell whether this is truth or just myth as fibres such as cotton and linen survive even more rarely than wool in archaeological record.\nI hope that this short article has been of interest and that the Bibliography below will produce further material for those of you who wish to pursue this topic further.\nClick here to return to the village.', 'The Vikings have been notoriously associated with long voyages and unrelenting invasions that changed the course of history from 800 AD. Since they were always involved in raids and skirmishes, it’s common knowledge that their attire was designed to withstand external elements.\nIn addition to being excellent warriors, they were skilled weavers and made protective clothing for battles and freezing temperatures in their homeland. In this article, we will explore different Viking attire and intricate details you’ll be surprised to know!\nTable of Contents\nArchaeological Evidence of Viking Clothing\nAccording to archaeologists, most Vikings were middle age farmers who wore simple and practical clothing. \nUlla Mannering, an archaeologist who researches North European textiles, explains that even those who engaged in ruthless battles and exciting trades abroad would seem plain to the modern man today.\nWhile Viking customs on various TV shows and movies seem extravagant, Viking warriors wore clothing far more coarse and fragmented than today’s refined weavings. Researchers possess a general sense of Viking style through the samples found in graves and bags.\nWe will elaborate on the clothing style in the next few lines.\nWhat Kind of Clothing Did They Wear?\nVikings wore what they could afford. For most of the Viking age, Viking raiders coveted the stolen armor and weapons from their enemies. There was a social hierarchy among the Norsemen who used clothing as a symbol of their status and wealth.\nSince the Viking age lasted over three centuries, their style and clothing eventually changed with the times.\nThrough Heimskringla, we get a clear idea of King Olaf Haraldsson’s warriors who were armed in “coats of ring-mail and in foreign helmets.” This shows that foreign equipment had a reputation for better quality than Norse battle-wear.\nWhat Did Men Wear?\nThe Scandinavians applied fine craftsmanship while weaving their coats and cloaks. Despite the stereotype that the Vikings only wore rugged, grotesque pieces, they indulged in extravagant, finely made furs.\nOf course, these imported furs were only accessed by the higher classes. Mannering explains that these garments were passed down from higher classes to lower class counterparts.\nSince Viking men were exposed to harsh weather and incessant battles, it was important for them to keep warm during strenuous moments.\nBase garments were thick and coarse in colder months. Men wore tunics embossed with symbols or patterns. Along with this, an outer garment – usually an overcoat and trousers – was added to keep them warm. Viking shoes were characterized by leather furnishings and were made from a process known as the “turn shoe” technique.\nWhat Did Women Wear?\nThe women wore thick strap-style dresses coupled with sturdy cloaks like men. These garments were made largely from wool or linen and protected against unbearable temperatures.\nThe Viking era existed during a time when low temperatures were common. For women, too, it was extremely important to keep warm. Just like men, they wore a base layer of linen underdress and a woolen strapped dress over it.\nWomen wore sturdy cloaks over this apparel which was usually made from fur or wool. Silk was available, but it had to be imported, so it was generally accessible to the elite members of the Viking society.\nWhat Did the Viking Warriors Wear?\nWe already know that the Vikings had a barbarous reputation, owing to the attacks on the Christian monasteries and their exaggerated description by numerous travelers. When it comes to battle wear, they adapted to the conditions of warfare in the region.\nSo when the Vikings raided a particular area, they were also notorious for stealing and looting ornaments, armor, weapons, and jewelry of the region.\nListed below are some Viking warrior garments worn during raids and battles.\nViking Lamellar Armor\nThe clothes worn during extensive battles were much more robust than normal garments. Lamellar armor was a colloquial term for metallic armor that was similar to chainmail in the general sense.\nMore than 30 lamellar were found in 1877 which proves that the Vikings wore them during battles.\nThis clothing was usually made by linking many iron or steel plates using leather. Lamellar armor was effective in offering some security to the warriors, but it was not as powerful as chainmail. Hence, the reason why many Danish kings imported chainmail from bordering lands.\nAlong with Lamellar armor, chain mail was widely used by Viking warriors too. They wore chainmail shirts made of iron rings linked to each other. The image should not be confused with the bulky steel suits worn by knights.\nChain mail was extensively used by the Vikings as a way to protect themselves from hits. Its evidence has been found in Scandinavia, where Vikings made it using a 4-1 pattern.\nLeather armor was one of the most accessible armor during the Viking age.\nIt was usually made up of leather patches and padded with thick wool clothing for extra protection. It was more common among warriors of low rank or status. The Viking Lamella armor was usually worn by elites or high-ranking warriors.\nThe Viking armor was incomplete without distinctive and robust helmets.\nViking helmets were specifically known as Nasal helms. They wore helmets to protect their heads and shield themselves from the enemy. Some metal helmets covered the head and entire face, while others were used to hide the face partially.\nIron helmets were used by Viking warriors consisting of a conical iron cap, a nosepiece, and eye guards. Since iron was expensive to procure, many preferred leather helmets as they were cheaper and easily accessible.\nThe alleged horned helmets showcased by popular culture are highly speculated by historians since the only Viking helmet found was without a horn.  Moreover, horned helmets would be impractical on a real battlefield.\nAccording to written sources, the Vikings loved accessorizing their battle armor.  Many warriors wore leather belts fastened to their trousers to carry their weapons around seamlessly.\nThe leather belt was primarily worn over the long tunics and it was used to carry weapons like axes, knives, and swords.\nLastly, heavy cloaks were used by Viking warriors when they had to traverse through freezing temperatures or unchartered territories. These cloaks often served as an additional layer to the battle armor worn underneath.\nViking weapons were an important part of the daily life of Scandinavians. Archaeologists have found evidence from lakes, graves, and battlefields to justify the prominent weapons used by them.\nWhile there were other weapons, the spear, shields, and axes were integral to a Viking warrior’s defense system.\nThe Vikings were known for their large and round shields. These shields were made from wooden boards measured up to a meter and riveted together. A hole in the center allowed the warrior to grip the shield properly. Other materials like fir, alder, and poplar wood were also used to make them.\nSometimes, the shields were covered in leather, and painted with images of mythical heroes. A characteristic feature of Viking battle armor, these shields were used to offer considerable protection from incoming blows.\nViking Spears were another common weapon used by Vikings. These spears had their unique design – metal heads with a sharp blade mounted on wooden shafts.\nThe shaft was usually 2 to 3 meters long, and they were made from ash trees. Each spear was designed for a specific purpose, whether throwing, chopping, or cutting.\nAs the most common hand weapon, axes were mostly used by the common Viking. These axe heads were usually made from wrought iron with a steel edge and were fairly cheaper than spearheads.\nThey were thrown or swung at the enemy to decapitate them instantly. The Dane Axe, which was a large axe with two hands, was used by warrior elites in prominent battles.\nHence, the Vikings were a group of people that differentiated themselves from others through their ways, clothing, and culture. Legendary as they were, Viking warriors and women were skilled and tenacious in every aspect of their lives.\nWith an impressive history and remarkable culture, they managed to prevail over many regions through their sheer will and determination for many decades.']"	['<urn:uuid:7f035a8c-789e-46c1-99a9-9c1ff258c2d2>', '<urn:uuid:d3b2c172-6fdf-42d4-9401-cb27551e1bf8>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	10	88	2313
93	In my research on ancient governance, I'd like to know how the Roman Empire's approach to ruling conquered territories differed from Alexander's final days of administration in Babylon?	Rome had a more structured administrative system, dividing territories into Senatorial provinces (governed by Senate-appointed officials, like Sicily and Macedonia) and Imperial provinces (governed by emperor-appointed officials, like Syria and Gaul), plus client kingdoms with semi-autonomous rule. In contrast, Alexander's final administrative activities in Babylon show a more centralized, personal approach - he was directly overseeing military preparations, issuing instructions to officers, and managing appointments to army posts, while spending his final days moving between various palaces and administrative centers in Babylon.	['DIGGING DEEP – 8 by Abidan Paul Shah\n- Alexander the Great and the spread of Greek culture.\n- Koine Greek\nOne constant factor during the writing of the New Testament was Rome. Roman history can be divided into three parts:\n- ROMAN KINGDOM – early stages of Rome. (753 BC – 509 BC) Not very significant for our discussion\n- ROMAN REPUBLIC – still an early stage of our discussion (509 BC – 44 BC)\n- ROMAN EMPIRE – (44 BC – 6th century AD)\nRome had great admiration for Ancient Greece. Unlike Persia and Greek, Rome knew how to make people Romans. Persian Empire tried to absorb the conquered people but they still did not create unity. Greece tried but not everybody could be a citizen of Greece. But Rome knew how to truly absorb the surrounding cultures – both people and gods.\nSome key aspects of the Roman Empire:\n- The Emperor\n- In 48 BC Julius Caesar declared himself the Dictator. After he was assassinated, his adopted son, Octavius through a series of battles and events became the first emperor of the Roman Empire. Some of those battles might be familiar to you (Mark Anthony and Cleopatra).\n- In 27 BC the Senate gave him the title of Caesar Augustus and they became just an advisory council to the Emperor. His rule brought the PAX ROMANA. Rome’s first institutionalized police force and fire fighting force. He built roads throughout Italy.\n- The Imperial cult was typically instituted after the death of the emperor. Domitian (AD 81-96) was the first to institute it in his own lifetime. He demanded to be addressed as “dominus et deus” = “Lord and god. This was a major problem to the Christians. (See I Peter 3:12-17; and Revelation 2:13 – “Satan’s throne” in Pergamum)\n- Main administrative units; 2 kinds: Senatorial and imperial\n- Senatorial provinces were ruled by governors who were appointed by the Senate. They were no threat to peace. For e.g. Sicily, Spain, Pamphylia, Africa, Macedonia, Asia, Crete, Cyprus, etc.\n- Imperial provinces were also ruled by governors who were appointed by the emperor. They were on the borders of the empire and had a threat of revolt. For e.g. Syria, Gaul, Galatia, and Britain.\n- Client Kingdoms\n- Not directly administered by Rome but still under Roman rule.\n- They were independent rulers who could appoint their own officials and implement their own policies.\n- Palestine was ruled by Herod from 40BC – 4 BC (Judea, Samaria, Galilee, Peraea, and Idumea)\n- After his death, his territory was divided into 3 parts: Judea, Idumea, and Samaria to Archelaus; North of Galilee to Philip; and Galilee and Peraea to Antipas.\n- Archelaus was removed after 10 years and Idumea and Samaria went under Roman procurators (Pilate, Felix, and Festus).\n- Later Herod Agrippa (Herod’s grandson) was a client king over North of Galilee, Galilee, Perea, and Judea. (Acts 12) His son Herod II (acts 25-26).\n- Colonies and Free Cities\n- Self governing settlements of Roman Citizens\n- For e.g. Philippi and Corinth (locals who were granted Roman citizenship)\n- Roman Citizenship\n- People could have dual citizenship like Paul (Tarsus and Rome).\n- It ensured loyalty.\n- Roman Law\n- Basis of Roman Authority was Legal. It all depended on RIGHT and JURISDICTION. Magistrates had complete power. Measure of all things was the LAW. (Greek – man; East – king; Jewish people – God).\n- It was not perfect – THERE WAS CORRUPTION. The further from Rome, the more the corruption.\n- The power over life and death (ius gladii) was always retained by Governors.\n- Roman Taxation\n- Poll tax was on every adult, including women and slaves.\n- An amount was set but the local governors could set their amount.\n- Pax Romana\n- Common Currency – Greek coins (lepta, drachmai,) and Roman coins (assaria, quadrans, denarius). Also temple currency was in shekels\n- Matthew 10:18; 11:8; 17:25; 18:23; 22:2; 25:40\n- John 18:31\n- Acts 16:19-40\n- Galatians 3:28\n- Acts 24:5', 'Alexander the Great’s death is an unsolved mystery. Was he a victim of natural causes, felled by some kind of fever, or did his marshals assassinate him, angered by his tyrannical ways? An autopsy would decide the question, but it is too late for that.\nThe trail is long cold. All who recalled the terrible fortnight of his dying had their own reputations to protect and they were not under oath when publishing their memoirs. The secret of Alexander’s end will not be discovered by poring over disputed narratives, but by assessing his interaction with others. Who were the men and women he knew, and who his friends and enemies? What did they think of him and he of them? Where lay their loyalties, and where the imperatives of self-interest?\nIn the year 323 BC, Alexander enjoyed an overdue vacation in the deluxe metropolis of Babylon in Mesopotamia. This was one of the great cities of the Persian empire and over the centuries had grown accustomed to looking after the needs of invaders. Its Hanging Gardens were one of the Seven Wonders of the ancient world. A few weeks there of uninterrupted leisure and pleasure were just what Alexander and his careworn soldiers needed.\nThe youthful Macedonian monarch had spent a good ten years fighting his way nonstop through the Persian empire to its Indian frontier, deposing the Great King and seizing power himself. After winning victories in the Punjab and along the Indus River, he marched back to civilization through a searing desert, losing thousands of his men for lack of water before reaching the safety and the comforts of Mesopotamia.\nAlexander was still a handsome man in his prime whose triumphant past augured a shining future. His next and imminent project was to establish commercially viable townships along the Arabian coast. A port had been specially built near Babylon to house a new fleet. Meanwhile the army prepared to march south by land. Victory was taken for granted, but after that, who knew what?\nFor now, in late May, as the unrelenting heat of summer approached, he needed a good rest. Babylon had all the necessary facilities. There was water everywhere; the river Euphrates on its way to the Persian Gulf passed through the center of the city and poured into the moats that lay alongside the lofty defensive walls of baked mud brick. And beyond the walls lay swamps and lagoons bursting with wildlife, irrigation channels, and reservoirs.\nTwo colossal palaces stood in the north of Babylon, with offices and workshops. One of them functioned, at least in part, as among the world’s earliest museums, housing treasured artifacts from earlier times, and was probably where kings and their families lived in grand but private seclusion. The other, which modern archaeologists have named the Southern Palace, was set aside mainly for administration and for ceremonial functions. Offices and workshops surrounded five courtyards, one of which opened onto a vast throne room whose walls were glazed in blue and yellow tiles and decorated with floral reliefs, lions, and fan-shaped designs suggesting the fronds of a palm tree.\nOn the river’s edge beside the palace, the Hanging Gardens astounded visitors. A set of ascending terraces, angled back one above the other, rested on great brick vaults. Each terrace contained a deep bed of earth and was planted with trees and shrubs. The effect was of a wooded hillside. A staircase led up to all the floors, and water drawn from the river by mechanical pumps irrigated each tier. The story was told that Babylon’s most successful king, Nebuchadnezzar II, constructed the Hanging Gardens for his wife, who missed the mountains of her childhood.\nIn principle, there was nothing so very unusual about them, for they were a condensed urban version of the large walled garden or park much favored by the wealthy and the powerful, who sought refreshing green relief from the parched landscapes of the east. The Greek word for such a garden was paradeisos, from which we derive our “paradise.”\nAs the design of the Hanging Gardens goes to show, the people of Babylon and other Mesopotamians were skillful managers of water. They built canals and irrigation systems, and just to the north of the Southern Palace they constructed what seems to have been a large reservoir.\nOn the eastern side of Babylon, an outer wall formed a first defense against attack and enclosed large areas of less populated ground. It led to a so-called summer palace, 2,000 meters north of the main city. Here ventilation shafts counteracted the heat of the day and, away from the crowded city center, afforded some relief to the ruling family. The palace may also have functioned as a military headquarters; there was certainly plenty of space for an army encampment nearby. Alexander preferred being with his men to living in the city, and spent time in the royal tent or aboard ships on the river. So whether there or in the palace, he oversaw the preparations for his Arabian expedition and relaxed.\nThe navy was approaching a state of high readiness and an intensive training program was under way. Different classes of warship raced against one another and the winners were awarded golden wreaths. Alexander decided to organize a banquet for the army on the evening of May 29th (according to the Greek calendar, Daesius 18). It was held to celebrate the end of one campaign, the invasion of India, and the imminent onset of a new one, the invasion of Arabia.\nBut in the interval there was time for a good time. Wine was sent round to every unit in the encampment, as were animals for sacrifice to the gods—that is, for roasting on an altar and then, as was the way in the ancient world, for eating. The guest of honor at the king’s table was his admiral of the fleet, a Greek called Nearchus, a loyal if not especially talented follower, who had been a boyhood friend.\nAlexander knew well his Euripides, the Athenian tragic poet of the late 5th century BCE, and recited verses from his play Andromeda. The plot concerned a beautiful young princess who was chained to a rock and awaited death from a sea monster. At the last minute the hero, Perseus, arrives on his flying horse, Pegasus, and rescues her. Only fragments of the drama have survived and we do not know what lines the king spoke, but one certainly fits his high opinion of himself: I gained glory, not without many trials.\nThe convention among civilized partygoers was that serious drinking only began once the meal was over. Wine was a little syrupy and could have a high alcohol content compared with vintages today. It was usually served diluted with water. A large two-handled bowl, or crater, containing wine (it could hold as many as six quarts of liquid), was brought into the dining room where guests reclined on shared couches. The host, or a master of ceremonies chosen by those present, decided how much water should be mixed with the wine and how many top-ups should be allowed. Guests had individual cups, and servants used ladles to fill them.\nThe Macedonians and their monarchs had a proud tradition of heavy alcohol consumption. It was not at all uncommon for a session to end with drinkers passing out. In a play performed in Athens earlier in the 4th century, Dionysus, the god of wine, sets out the stages of inebriation:\nFor sensible men I prepare only three craters: one for health (which they drink first), the second for love and pleasure, and the third for sleep. After the third mixing bowl is drained, sensible men go home. The fourth crater is nothing to do with me—it belongs to bad behavior; the fifth is for shouting; the sixth is for rudeness and insults; the seventh is for fights; the eighth is for breaking the furniture; the ninth is for depression; the tenth is for madness and unconsciousness.\nAlexander had experience of the higher numbers of the scale and drank a toast to each of the 20 men present in the room. Then he decided to leave the party early and get some sleep. This was unusual behavior for him; he may have been feeling a little off-color. As was his habit, he took a bath before sleeping, but then a Thessalian friend of his, Medius, invited him to join a late-night party. “You’ll enjoy yourself,” he promised. The king agreed and continued drinking. Eventually he left and turned in.\nOn the following day, he felt feverish and spent much of his time in bed. He played dice with Medius and dined with him. Alcohol was on the menu again. According to one version of events, Alexander challenged a fellow guest to down a crater of wine in one go. After he had done so, the man counterchallenged the king to repeat the trick. Alexander tried, but failed. He felt a stabbing pain in his back “as if he had been pierced by a spear,” gave a loud cry, and slumped back onto his cushion. He left the party, ate a little food, and took a bath. He now definitely had a fever and fell asleep on the spot in the bathhouse.\nBy the morning of the third day, Alexander was no better. He was carried out on a couch to conduct the usual daily sacrifice to persuade the gods to watch over him and his army. His indisposition was an annoying setback, but no more than that. He issued instructions to his officers for the imminent Arabian campaign and amused himself by listening to Nearchus reminisce about his adventures at sea.\nThen the king was carried on his bed to a waiting boat and taken downstream to the palaces in Babylon. Here he was installed in the paradeisos or, in other words, the Hanging Gardens, doubtless because of their calm, quiet, and coolness. He lay in a vaulted chamber beside a large bathing pool. He discussed vacant posts in the army with his commanders and spent time chatting with Medius.\nDays passed; Alexander’s condition gradually worsened. There seems to have been a variety of pools and bathhouses in the vicinity, and the king was transferred to at least one of them and finally to a lodge beside the reservoir. These constant removals suggest growing panic among the king’s staff.\nIt was increasingly obvious that he was gravely ill; his commanders and high officials were warned to stay within reach. Generals waited in the courtyard. Company and regimental officers were to gather outside the gates. On June 5th Alexander was ferried back to the Summer Palace. He stayed either there or in the royal tent in the nearby army encampment.\nThe fever did not abate. By the next evening it was obvious that the king was dying. He had lost the power of speech and he handed his signet ring to his senior general, Perdiccas. In this way he dramatized an at least temporary handover of power.\nA rumor spread that Alexander was already dead. Soldiers crowded round the palace entrance, shouting and threatening to riot. A second doorway was knocked through the bedroom wall so that they could walk more easily past their dying leader. They were let in, wearing neither cloak nor armor. Alexander’s historian Arrian writes:\nI imagine some suspected that his death was being covered up by the king’s intimates, the eight Bodyguards, but for most their insistent demand to see Alexander was an expression of their grief and longing for the king they were about to lose. They say that Alexander could no longer speak as the army filed past him, but he struggled to raise his head and gave each man a greeting with his eyes.\nSeven of his commanders undertook a ritual of incubation. They spent the night in the temple of a Babylonian deity, hoping for an omen-bearing vision or dream. They inquired whether the king should be moved there, but were told, discouragingly, that they should leave him where he was.\nOn June 11th, between three and six o’clock in the afternoon, Alexander died, a month or so short of his 33rd birthday. What was to happen next? everyone wondered uneasily. Nobody knew. If the stories are correct, the king himself had been no wiser. While still able to speak, he turned his disenchanted attention to the succession. When someone asked him: “To whom do you leave the kingdom?” he replied: “To the strongest.” He is said to have added: “I foresee great funeral games after my death.”\nPerdiccas asked when he wished divine honors paid to him. He replied: “When you yourselves are happy.” It is reported that these were Alexander’s last words.\nWhat killed the king was as uncertain as the future from which he was now excluded. Natural causes were assumed. However, after a while, circumstantial details of a plot to poison him emerged into the light of day. So the real question may have been who killed the king.\nWe have two explanations of Alexander’s death, both decorated with data, opaque with cross-my-heart-and-hope-to-die verisimilitude. One gives a verdict of murder, and the other of a complicated natural death. Which are we to believe?\nExcerpted from Alexander the Great by Anthony Everitt. Copyright © 2019 by Anthony Everitt. Excerpted by permission of Random House. All rights reserved. No part of this excerpt may be reproduced or reprinted without permission in writing from the publisher.']	['<urn:uuid:ddaf22dc-adcd-40a7-97db-12a40a75b775>', '<urn:uuid:595d627d-9cd4-4bf7-85ca-2df000509aba>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T17:51:21.175311	28	82	2930
94	How does a biography's structure relate to chronological storytelling, and what strategies do biography writers use when teaching others to craft effective biographical narratives?	Biographies are typically written in chronological order, with the order of key events being crucial for effective storytelling. Understanding the sequence of major events helps save time in writing the biography. As for teaching biographical writing, the process involves specific steps: starting with brainstorming and gathering important information, creating an information chart, finding a theme, developing a basic outline, and writing with great first sentences. The outline should include an introduction stating the theme, several chronological paragraphs in the body, and a conclusion. Writers are advised to use transitional phrases and words to link paragraphs and ensure flow.	"['A bio is a comprehensive summary of a person’s life. It consists of far more than the standard realities of an individual’s life. It likewise consists of the experiences that person had in the events that impacted his or her life. Biographies are a prominent method to find out about an individual’s life. Here are a few of the usual subjects that people cover. Every one of them are intriguing! Read on for more information! Uploaded in Bio, History, and Pop Culture\nAn integral part of a bio is the thesis statement. The thesis declaration is what notifies visitors regarding the biographee’s life. It must be supported by the biography itself. Biographies are usually written in sequential order. Understanding the order in which the key events took place will save you time creating the bio. There are several types of bios, so make certain to look into the subject thoroughly as well as make a decision just how to organize the bio. In this manner, you can be sure that the information you include is accurate.\nIn academic bios, chroniclers depend heavily on documented facts as well as kept in mind success. A bio of an artist or author, for instance, could go over how their life influenced a certain kind of art, while a biographer of an author would focus on his/her influence on a certain style of art. The order in which historical truths are presented in a biography is important because the lessons found out by a person can obtain shed in the min details.\nWhile biographies are expected to be real, they often fall short. This is especially true when they are written by a person who is not the writer’s first-person viewpoint. Biographers tend to write out of their very own point of view, sex, or time, which makes them biased. It is essential to bear in mind that a bio has to do with a living individual, not a piece of history. However, a biography can include blunders as well as can be a controversial item of writing.\nSome biographers write a bio concerning a team of individuals. Among the oldest instances of this type of bio is Captain Charles Johnson’s A Basic History of the Pirates (1724 ). This publication cataloged the lives of well-known pirates as well as developed the images of pirates that we have today. On top of that, there are smaller sized teams that can be the topic of a cumulative bio, like the individual scene in New york city City. The high quality of the work relies on the author’s capacity to deal with voids in the record.\nAt the turn of the 20th century, bios influenced by psychology as well as sociology grew in popularity. Darwinian theories explained human actions, yet sociological bios often tended to downplay individuality. A more detailed understanding of the biographical subject was likewise fostered by psychoanalysis. This encouraged biographers to stress youth and also very early life as essential aspects of the writer’s life. Bios of famous people became popular as a form of enjoyment.\nOne more type of bio is the docudrama genre. These movies tell a certain story from history, utilizing entertainments, archival video, as well as interviews with professionals to tell the story. Lots of renowned docudramas are based upon real occasions or are fictional. One example of a renowned film documentary is Grey Gardens, regarding Jacqueline Kennedy’s once-rich cousins. Another movie documentary is I Am Not Your Negro regarding the revolutionary writer James Baldwin.\nBiographies are written in numerous forms, including fictional as well as non-fiction designs. While a modern biography is frequently based upon a living individual, an imaginary bio generally takes place to make the story more enjoyable. 2 examples of a fictional biography are Z, an unique regarding Zelda Fitzgerald, as well as Hedy Lamarr’s tale in The Only Female in the Room. While fictionalized bios are not purely precise, they commonly utilize accurate product.\nIn spite of its appeal in recent years, bios remain to amass reader interest and focus. The bestselling musical Hamilton has provided a brand-new lease on life for historical biographies. Likewise, the brand-new motion picture Hamilton is based upon the prominent Alexander Hamilton biography by Ron Chernow. A biographer’s goal is to solidify an individual’s status as a great American. A historic biography can be an interesting read for anybody that enjoys background.\nMemoirs are an additional popular genre of bio. While an autobiography is created by the writer, it can focus on a particular aspect of the writer’s life. Michael Jollett’s Hollywood Park states his childhood in a cult as well as his rise to fame with the band Airborne Hazardous Occasion. Along with the compositions, Barack Obama’s very first governmental narrative, Becoming Head of state, takes a detailed look at his initial 4 years in office.\nA biographer should pick thoroughly what details to present. A bio can reveal problems of interest or perhaps conflicting accounts of a topic. Besides revealing these conflicts, biographers likewise have to examine their resources versus the perspective of their subject. The objective of a biography is to inform the whole story, so also the scandals and also bad choices need to be consisted of. A biography requires to record these details, as they might make visitors uncomfortable. This is why a bio has to have a good equilibrium between individual and expert life.\nA memoir, on the other hand, is a narrative written by the writer. The writer recreates his/her life in the narrative. It defines events, feelings, as well as memories as well as can give a life overview to the reader. Famous examples consist of Saint Augustine’s autobiography in 400 A.D. as well as the works of Maya Angelou, a civil liberties lobbyist and champion writer. When you read a bio, you’ll be influenced and motivated to take action in life!\nEarly biographers include Cornelius Nepos, that released the Excellentium Imperatorum Vitae in 44 BC. Plutarch’s Parallel Lives in the 80th century was an additional early instance. It pairs popular Greek as well as Roman figures as well as contains fifty biographies. An additional old bio is the De vita Caesarum, which was composed around AD 121. This work focuses on public figures and also is just one of the earliest examples of biographies. Get more information\nA biography can be discussed anyone. It can be a renowned historical figure, a star, an unsung hero, or a distinct team of individuals. It’s a terrific method to boost your analysis and also creating skills. Some biographies concentrate on a person’s life from youth to fatality, whereas others concentrate on a solitary occasion. Biographies can be written in various mediums, consisting of novels, films, and also even a journal.', 'Biography Writing : L. Ullah Biography Writing Power Point by:\nDepartment of English\n(Dhaka, Bangladesh) Slide 2: Let’s have a look at these people! William Shakespeare Jane Austen Charles Dickens Mark Twain Rabindranath Tagore William Wordsworth Slide 3: 27.08.2010 William Wordsworth (1770-1850), a British poet, credited with ushering the English Romantic Movement with the publication of Lyrical Ballads(1798) in collaboration with Samuel Taylor Coleridge.William Wordsworth was born on 7 April 1770 in Cockermouth, Cumberland, in the Lake District. His father was John Wordsworth, Sir James Lowther\'s attorney. The magnificent landscape deeply affected Wordsworth\'s imagination and gave him a love of nature. He lost his mother when he was eight and five years later his father. The domestic problems separated Wordsworth from his beloved and neurotic sister Dorothy, who was a very important person in his life. With the help of his two uncles, Wordsworth entered a local school and continued his studies at Cambridge University. Wordsworth made his debut as a writer in 1787, when he published a sonnet in The European Magazine . In that same year he entered St. John\'s College, Cambridge, from where he took his B.A. in 1791.\nDuring a summer vacation in 1790 Wordsworth went on a walking tour through revolutionary France and also traveled in Switzerland. On his second journey in France, Wordsworth had an affair with a French girl, Annette Vallon, a daughter of a barber-surgeon, by whom he had a illegitimate daughter Anne Caroline. The affair was basis of the poem ""Vaudracour and Julia"", but otherwise Wordsworth did his best to hide the affair from posteri\nIn 1795 he met Coleridge. Wordsworth\'s financial situation became better in 1795 when he received a legacy and was able to settle ----------- A Sample biographical sketch In particular: : In particular: A biography is a narrative account that covers the events of a particular person\'s experience, but different writers may choose to emphasize different events. So, what is a biography? : So, what is a biography? A biography is a written account of someone\'s life. There can be many versions of biography, and these versions may have very different angles, views, and conclusions.\nIn short, the story of a person\'s life, written by another. (Adjective: biographical) Steps of writing a biography: : Steps of writing a biography: STEP 1:\n“Brainstorming and gathering important information”\nYou need to think and take notes of the details about the person you are going to write about. If you don’t know much, go to a library and find as many printed materials about your subject as possible. Otherwise , you can also try some online resources provided they are reliable.\nFormulate an information chart: Slide 7: Biography of: _________________________________ Things you need to find out in writing a\nbiography - STEP 1(cont.):\nAn Information Chart: : An Information chart (Continued) : Steps of writing a biography : Steps of writing a biography Step 2:\n‘Finding out a theme’\nTry to find something specific about your subject\'s life to focus on as a theme.\n‘Doing a basic outline’\nWrite a basic outline for your biography, including an introduction in which you state what your theme is (your particular angle of studying the subject), several paragraphs in the body, each marking a chronological point in the subject\'s life, and a conclusion. Steps of writing a biography (continued) : Steps of writing a biography (continued) Step 4:\nUse the outline as a guide to write your biography. Try to utilize this outline as much as possible.\nGet into writing!\nStart off with a great first sentence( Or, the Thesis Sentence). It\'s a good idea to begin with a really interesting statement, a little known fact, or really interesting event.\nBe sure to use transitional phrases and words to link your paragraphs and make your composition paragraphs flow. Steps of writing a biography (continued) : Steps of writing a biography (continued) Fill the body of your biography with material that gives insight to your subject\'s personality. For instance, in a biography about Meriwether Lewis, you would ask what traits or events motivated him to embark on such a monumental exercise.\nThe final paragraph will summarize your main points and re-assert your main claim about your subject. It should point out your main points, re-name the person you\'re writing about, but it should not repeat specific examples. Let’s have a look at a Sample then! : Let’s have a look at a Sample then! An interesting First Sentence:\n""Abraham Lincoln once described himself as an ordinary man with an extraordinary story.“\nHere is a sample of a body paragraph:\n(Topic sentence) Abraham Lincoln struggled to keep the country together when some people wanted to see it split apart. The Civil War broke out after many American states wanted to start a new country. Abraham Lincoln showed leadership skills when he led the Union to victory and kept the country from splitting into. (Transition) His role in the Civil War kept the country together, but led to many threats to his own safety. A Sample (cont.): : A Sample (cont.): A Sample Summary Paragraph:\nYour summary (or conclusion) can look something like this:\nAlthough many people in the country did not like Abraham Lincoln at the time, he was a great leader for the United States. He kept his country together when it was in danger of falling apart. He also stood brave in the face of danger and led the way to equal rights for all people. Abraham Lincoln is one of the most outstanding leaders in American history. Your Turn! : Your Turn! Now it’s your turn!\nFollow the steps and write an excellent biography. Good Luck! : Good Luck! Thank you!']"	['<urn:uuid:4403fc5a-ce5a-4870-ab84-d65c545a3127>', '<urn:uuid:7d9e9659-25ec-4f9c-b5b5-1108d822073b>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T17:51:21.175311	24	98	2076
95	nuclear power infrastructure costs nonproliferation	Nuclear plants are extremely expensive to build, with costs rising from $2-4 billion to $9 billion per plant. Despite these high infrastructure costs, there's no evidence that U.S. nuclear arsenal reductions since the 1960s have promoted global nonproliferation - notably, the Non-Proliferation Treaty was ratified when U.S. nuclear capacity was at its peak.	"['Pros & cons of nuclear energy: is it safe?\nPeople often associate nuclear energy with nuclear bombs, or tragedies like Chernobyl or Fukushima. If this is your impression or image of nuclear power, you will be surprised to learn that nuclear energy is actually considered one of the most environmentally friendly forms of power generation. If you compare nuclear energy with other forms of energy generation, it is much more efficient than any other form and is actually considered one of the most environmentally friendly means of energy production available.\nThere are of course, many challenges associated with producing nuclear energy including initial cost, waste, security, safety, and more. Here are the main pros and cons associated with nuclear energy.\nWhat is Nuclear Energy?\nBefore going forward, let’s quickly define nuclear energy and how it’s created. Nuclear energy is created by splitting uranium atoms. When atoms are split they become 2 smaller, lighter atoms and because energy doesn’t just disappear, the “lost” mass is converted into heat. This heat is then used to produce electricity. Nuclear power plants create a controlled environment for this process.\nPros of nuclear energy\n- Nuclear energy protects air quality\nNuclear energy is the largest source of clean energy in the world. It produces about 800 billion kilowatt hours of electricity each year and produces 50% of the nation’s emissions-free electricity. This avoids over 470 million metric tons of carbon every year, which is like taking 100 million cars off the road.\n- Nuclear energy’s operating costs are low\nOnce a nuclear plant is built, the cost of producing electricity is much less than gas, coal or oil unless those fossil fuels are located very close to their power plant. Nuclear energy also has a comparatively low risk of price inflation. This is because less factors affect the price, as opposed to fossil fuels which are affected by a multitude of factors and their prices vary wildly.\n- Nuclear energy uses considerably less land\nNuclear power is incredibly efficient at producing electricity and because of this, it requires significantly less space than any other clean source.\nA standard 1,000 megawatt nuclear facility in the United States needs slightly more than 1 square mile to operate. Per the Nuclear Energy Institute (NEI) wind farms require about 360 times more land to produce the same amount of electricity and solar plants require 75 times more space.\n- Nuclear energy is the most reliable energy source in America\nNuclear power plants were able to run at full capacity more than 92% of the time and generated 20% of U.S. electricity last year. To put this in perspective nuclear operated at full capacity 336 out of 365 days last year (the 29 days off were for standard maintenance). In contrast, hydroelectric systems were running 38.2% of the time (138 days a year), wind turbines produced 34.5% of the time (127 days a year), solar arrays only generated electricity 25.1% of the time (92 days a year), and fossil fuels were active about 50% of the year. Nuclear is the clear winner on reliability.\n- Nuclear power puts less radiation into the environment than any other major source\nMost people don’t know that non-nuclear energy sources release radiation into the environment. The worst offender is actually coal which contains radioactive elements thorium and uranium. When coal is burned, its organic materials gasifies and its mineral components concentrate into waste. This waste is called fly ash. Since so much coal is burned all over the world, which produces so much fly ash, coal is actually the major source of radioactive releases into the environment.\n- Disposing nuclear waste is no longer a technological problem\nThe disposal of nuclear waste is a political problem in the U.S., not a technological one. Over 90% of the spent fuel in the U.S. could be recycled and used to extend nuclear power production for hundreds of years. Instead of doing this, it sits safely in impenetrable concrete and steel dry casks on the grounds of active reactors, slowly losing its radiation.\nIn New Mexico, the U.S. Waste Isolation Pilot Plant (WIPP) stores transuranic military waste and it could store commercial nuclear waste in its 2-kilometer thick bed of crystalline salt. The salt formation extends from southern New Mexico all the way to southwest Kansas. This could easily hold the entire world’s nuclear waste for 1000 years.\nTo put this in perspective, we could take all of the fuel used by the U.S. over the last 60 years and fit it in a football field about 10 yards deep.\nCons of nuclear energy\n- Nuclear plants are extremely expensive to build\nEven though they are relatively cheap to operate, nuclear plants are very, very expensive to build and are getting more expensive. The cost to build a nuclear power plant has risen from $2-$4 billion to $9 billion and construction often exceeds estimates. Nuclear plants must also set aside funds to build structures to protect and secure nuclear waste produced.\n- Risk of nuclear accidents\nEven though nuclear power plants have extremely strict measures in place to prevent accidents, they obviously do occur. Such accidents can cause radiation to leak which can lead to serious health complications or death and serious environmental damage.\nHowever, as technology advances, the negative effects are minimized and studies show that even if you account for catastrophes like Chernobyl fossil fuels are substantially more deadly.\n- Environmental Impact\nThough the actual production of electricity doesn’t produce emissions, the process of mining the uranium needed isn’t clean. The mining of uranium is known to release arsenic and radon to surrounding areas.\nNuclear power plants also cause thermal pollution. Most nuclear plants are located on a lake or ocean as they need water to cool their reactors. The body of water is used to condense steam back into water.\nThis process causes the water to heat up to around 100 degrees Fahrenheit. If this hot water is immediately released back into the body of water it can significantly change the chemistry and make it inhabitable to most aquatic life.\n- Radioactive waste\nThe waste produced by nuclear plants is radioactive and remains so for thousands of years. This waste needs to be safely stored and ideally recycled. There is also the potential for a leak which could cause damage to surrounding areas.\n- Nuclear is not currently renewable\nIf you account for all known uranium deposits and current consumption we have about 200 years of uranium left. If you account for recycling used fuel that number would increase by hundreds of years.\nThe future of nuclear energy\nThere are clear arguments that support both sides of the nuclear debate. But as technology advances nuclear energy could be a key factor in helping us achieve 100% clean energy in the future.\nAnother thing to keep in mind is if nuclear fusion can be achieved, it would supply nearly limitless clean energy.', ""Save the Nukes!\nAmerica's nuclear arsenal isn't dangerous, but getting rid of it might be.\nNow that the Cold War is over, it may be hard to see why the United States needs a large nuclear arsenal. It’s excessive; it’s provocative; it’s irresponsible; and it’s expensive, critics say—especially at a time when the country enjoys unparalleled conventional military superiority. Disarmament is no longer an exclusively left-wing cause. In 2007, a bipartisan group of foreign-policy establishment figures, including former secretary of state Henry Kissinger, called for a push toward “a world free of nuclear weapons.” President Barack Obama has proposed reducing the U.S. nuclear arsenal to 1,000 warheads, the smallest number since 1953.\nBad idea, argues Matthew Kroenig, a political scientist at Georgetown University. Writing in Foreign Policy, he says that the illusion that nuclear weapons no longer matter has been fostered by the muting of great-power rivalry since the collapse of the Soviet Union. But other nations don’t see things that way: “Russia, China, India, Pakistan and North Korea are modernizing or expanding their nuclear arsenals, and Iran is vigorously pursuing its own nuclear capability,” Kroenig says. And China, which now has only an estimated 50 warheads capable of reaching the United States, looks likely to revive the great-power struggle.\nYale political scientist Paul Bracken argues that the world is on the verge of a “second nuclear age” and that nuclear weapons are re-emerging as “a vital element of statecraft and power politics.”\nSome critics maintain that it would require only a small nuclear arsenal to deter these rising powers, but Kroenig’s research supports the opposite view. “In a statistical analysis of all nuclear-armed countries from 1945 to 2001 . . . the state with more warheads was only one-third as likely to be challenged militarily by other countries and more than 10 times more likely to prevail in a crisis.”\nHistory repeats this lesson. When the United States could demonstrate clear nuclear superiority over the Soviet Union, it enjoyed more favorable foreign-policy outcomes. Not only did Washington prevent Moscow from building submarine bases in Cuba, but in 1967 and again in 1973, Washington’s nuclear advantage kept Soviet forces from supporting Arab allies in their wars against Israel. However, in 1979, when the nuclear gap narrowed, the United States failed to roll back the Soviet invasion of Afghanistan.\nSome strategists argue that a large arsenal is provocative, increasing the likelihood of a preemptive attack from another nuclear power. The Obama administration has embraced this view, identifying “strategic stability” as a goal and proposing a new agreement with Russia further reducing the current treaty limit of 1,550 warheads in 2017. Kroenig sees tortured logic behind the notion that arsenals are provocative: “After all, the United States possesses a first-strike advantage against the world’s 184 nonnuclear states, and it doesn’t wring its hands about that.” Why fret about nuclear superiority over Russia and China? Beijing might even regard any U.S. arms reduction less as a reassuring gesture than as a prime opportunity to catch up.\nStill another argument for reducing the American nuclear arsenal is that in leading by example, Washington would promote global nonproliferation. Kroenig’s short answer: “Keep dreaming.” The United States has been cutting its nuclear forces since the 1960s, and there’s no evidence that these efforts have reduced worldwide proliferation. Indeed, Kroenig notes, the pathbreaking 1968 Treaty on the Non-Proliferation of Nuclear Weapons was ratified at a peak moment of U.S. nuclear capacity.\nShrinking the arsenal isn’t even cost effective, Kroenig contends. Most of the cost is in the infrastructure, not the weapons themselves, and nuclear programs make up only four percent of the U.S. defense budget. Demilitarizing bombs and weapon-building facilities would cost billions. It’s true that the United States could save approximately $3.9 billion annually by delaying the modernization of missiles and other delivery vehicles, but that’s “nothing short of trivial” compared to the $600 billion spent on defense each year. And the compelling rationale that fostered the U.S. nuclear buildup in the 1950s still applies today: Nuclear weapons are a much cheaper form of deterrence than conventional armies.\nThere is a simple principle at the bottom of Kroenig’s argument: “The more devastating that adversaries find the prospect of nuclear war, the less likely they will be to start trouble.” He thinks the Obama administration ought to abandon its talk of further reducing the nuclear weapons count and “follow through on its promise to fully modernize U.S. nuclear infrastructure.”""]"	['<urn:uuid:2d7ca715-9852-4971-9a32-a9c57cdb9a9a>', '<urn:uuid:a96c2aab-e014-4400-a31e-9e3da08fa556>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	5	53	1899
96	complications triple arthrodesis ankle	The complications of triple arthrodesis surgery include nonunion/malunion, degenerative joint disease, delayed wound healing, nerve injury, avascular necrosis, lateral instability, and stiff foot.	['- What Is\nWhat is a triple arthrodesis surgery?\nA triple arthrodesis is a surgical procedure that fuses three joints of the ankle. It is used to relieve pain from arthritic, deformed, or unstable joints.\nGenerally, surgeons try to avoid the fusion procedure by treating the ankle joints with alternative methods. However, when the pain is so severe, a triple arthrodesis remains the only option.\nMost of the time, a triple arthrodesis offers satisfactory results in the long term as well. In one study that followed patients for 44 years after their triple arthrodesis, it found that 95% of patients were satisfied with the outcome.\nWhy is a triple arthrodesis performed?\nA triple arthrodesis is performed for patients with the following conditions:\n- Rheumatoid/degenerative arthritis (severe arthritis of ankle)\n- Ankle deformity\n- Ankle instability\n- Severe flatfoot\n- Abnormal connections between bones\n- Excessively high arches\n- Joint instability due to neuromuscular disease such as poliomyelitis\nMany of these conditions may be treated using other procedures. Only when the nonsurgical treatments fail, the surgeon considers a triple arthrodesis. After examining the patient’s condition and disease progression, the surgeon decides whether to perform a triple arthrodesis.\nWhat is done before a triple arthrodesis?\nTwo to three X-rays of each foot will be taken from different angles to understand the ankle problem and how to go about the surgery.\nThe patient will be admitted the night before the surgery and asked not to eat or drink anything for the next 12 hours before the surgery.\nWhat is the technique used in a triple arthrodesis?\nA triple arthrodesis is performed by an orthopedic surgeon.\n- The patient might be given general anesthesia that makes them sleep throughout the procedure or spinal anesthesia that numbs their entire leg. Additionally, an injection in the back of the knee or ankle joint may also be given to ease the pain.\n- Next, the orthopedic surgeon makes one incision (cut) on each side of the foot.\n- Further, the surgeon removes the elastic structure over the joint surfaces (cartilage), followed by the roughening of the bony surfaces and filling of the bony defects with bone grafts.\n- Next, the surgeon attaches the bones with the help of hardware such as screws. This will eventually result in healthy fusion.\n- The surgeon then sutures the incisions and covers the surgical wound with a bandage.\nWhat is done after a triple arthrodesis?\n- The patient is observed for several hours after the surgery.\n- Analgesics are given, and the leg is elevated to reduce the postoperative edema and pain in the ankle.\n- The patient is discharged after one to three days. Instructions are given about which activities need to be restricted.\n- While at home, leg elevation should be continued for the next five days, and ice should be applied over the ankle around three to four times a day.\n- The patient should visit the hospital after one week to change the wound dressing.\n- A short leg cast is applied after the edema has disappeared to restrict the movements in the ankle joint and allow healthy fusion of the joints.\n- Sutures are removed after two weeks.\nWhat are the complications of a triple arthrodesis surgery?\n- Nonunion/malunion (bones may not fuse, or they may fuse abnormally)\n- Degenerative joint disease (ankle arthritis due to wearing of the surrounding structures)\n- Delayed wound healing\n- Nerve injury\n- Avascular necrosis (Damage to the adjacent blood vessels leading to the destruction of the ankle bone)\n- Lateral instability (inability to move the ankle sideways)\n- Stiff foot\nHow long does it take to recover from a triple arthrodesis?\n- The patient wears a plaster cast, wrapped around the ankle and foot, for the first 12 weeks following the surgery. Crutches can be used to move around and avoid weight bearing on the ankle.\n- Gradual weight bearing is allowed until full weight bearing is reached. This usually takes around three months. The cast is removed and replaced by a removable walker boot.\n- X-rays are obtained at approximately 12 weeks. If stable fusion is observed, the patient can switch from the walker boot to normal shoes.\n- Physical therapy is required to restore strength and mobility in the ankle joint.\n- It usually takes around ten 10 months to become pain-free and recover from the surgery.\n- The patient can experience maximum improvement one year after the surgery.\n- AHA News: This Tick Season, Beware the Tiny Bugs That Can Carry Lyme Disease – a Danger to the Heart\n- Ticks Can Take a Licking From Really Tough Weather\n- Drug Might Help Slow Nearsightedness in Kids\n- Does Your Kid Need a Summer Vacation From Smartphones?\n- Get in the Swim: Summer Pool Safety Tips\n- More Health News »\nHealth Solutions From Our Sponsors\nMedscape Medical Reference\nTop How Long to Recover From a Triple Arthrodesis Related Articles\nArthritis (Joint Inflammation)Arthritis is inflammation of one or more joints. When joints are inflamed they can develop stiffness, warmth, swelling, redness and pain. There are over 100 types of arthritis, including osteoarthritis, rheumatoid arthritis, ankylosing spondylitis, psoriatic arthritis, and gout.\nArthritis: 16 Bad Habits That Cause Joint PainBeing overweight, wearing uncomfortable shoes, or carrying a heavy purse can make joint pain and arthritis symptoms worse. Some bad habits increase inflammation and put you at risk of osteoarthritis and rheumatoid arthritis. Protect joints and muscles and prevent swelling and joint disorders by eliminating these joint problem bad habits.\nPhysical and Occupational Therapy for ArthritisPhysical therapy can help a patient with arthritis to work out stiffness without damaging their joints. Occupational therapy teaches the patient how to reduce joint strain during daily activities. Those receiving occupational or physical therapy will learn about their arthritis, be given a dietary plan if they are overweight, get foot care advice, and learn methods of relieving discomfort.\nJuvenile Rheumatoid Arthritis (JRA)Juvenile rheumatoid arthritis (JRA) affects one child in every thousand annually. There are six types of juvenile rheumatoid arthritis. Treatment of rheumatoid arthritis depends upon the type the child has and should focus on treating the symptoms.\nOA & Your JointsDealing with joint pain and arthritis? Learn why weight matters--and why NOT to stretch before exercise. See these solutions for joint pain and tips to protect your joints from damage.\nOsteoarthritis vs. Rheumatoid ArthritisOsteoarthritis (OA) and rheumatoid arthritis (RA) are chronic joint disorders. RA is also an autoimmune disease. OA and RA symptoms and signs include joint pain, warmth, and tenderness. Over-the-counter pain relievers treat both diseases. There are several prescription medications that treat RA.\nRheumatoid Arthritis (RA)Rheumatoid arthritis (RA) is an autoimmune disease that causes chronic inflammation of the joints, the tissue around the joints, as well as other organs in the body.\n16 Early Rheumatoid Arthritis (RA) Signs & SymptomsEarly rheumatoid arthritis (RA) symptoms and signs vary differently from person to person. The most common body parts that are initially affected by RA include the small joints of the hands, wrists, and feet, and the knees and hip joints. Joint inflammation causes stiffness. Warmth, redness, and pain may vary in degree.\nRA Friendly ExercisesRegular exercise boosts fitness and helps reverse joint stiffness for people with rheumatoid arthritis (RA). WebMD demonstrates helpful exercises to get you started.\nRA SlideshowWhat is rheumatoid arthritis (RA)? Learn about treatment, diagnosis, and the symptoms of juvenile rheumatoid arthritis. Discover rheumatoid arthritis (RA) causes and the best medication for RA and JRA.\nRA QuizHow is rheumatoid arthritis different from other forms of arthritis, such as osteoarthritis and gout? Take the Rheumatoid Arthritis (RA) Quiz to rest your RA IQ.\nRheumatoid Arthritis vs. ArthritisArthritis is a general term used to describe joint disease. Rheumatoid arthritis (RA) is a type of arthritis in which the body’s immune system mistakenly attacks the joints, causing chronic inflammation.\nRheumatoid Arthritis vs. FibromyalgiaThough rheumatoid arthritis (RA) and fibromyalgia have similar symptoms, RA is an autoimmune disease and fibromyalgia is a chronic pain syndrome. RA symptoms include joint redness, swelling, and pain that lasts more than 6 weeks. Fibromyalgia symptoms include widespread pain, tingling feet or hands, depression, and bowel irritability. Home remedies for both include stress reduction, exercise, and getting enough sleep.\nWhat Are the Four Stages of Rheumatoid Arthritis?Rheumatoid arthritis is a chronic inflammatory disease categorized into the following four stages and classifications. Learn the causes, symptoms, and complications of RA below.']	['<urn:uuid:70846e71-33a2-43bb-8a70-50841d919261>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T17:51:21.175311	4	23	1401
97	Which foreign actors shaped these renowned films?	In Cinema Paradiso, French actor Philippe Noiret played the crucial role of Alfredo the projectionist, taking the part after falling in love with the script despite having four other film commitments. Meanwhile, in Dreams, American director Martin Scorsese made a surprising but effective appearance as Vincent van Gogh in one of the film's most visually striking sequences.	"['Unfolding in a series of mythic vignettes, this late work by Akira Kurosawa brings eight of the beloved director’s own nighttime visions, informed by tales from Japanese folklore, to cinematic life. In a visually sumptuous journey through the master’s unconscious, tales of childlike wonder give way to apocalyptic visions: a young boy stumbles on a fox wedding in a forest; a soldier confronts the ghosts of the war dead; a power-plant meltdown smothers a seaside landscape in radioactive fumes. Interspersed with reflections on the redemptive power of art, including a richly textured tribute to Vincent van Gogh (played by Martin Scorsese), Akira Kurosawa’s Dreams is both a showcase for its maker’s imagination at its most unbridled and a deeply personal lament for a world at the mercy of human ignorance.\nThroughout his storied career, legendary director Akira Kurosawa has shared numerous, indelible tales with audiences around the world -- including epic samurai adventures, engaging crime stories, and deeply moving dramas about the human condition. But in one of his final movies, the filmmaker completely leaves these traditional modes of narrative behind in order to share something even more personal and elusive: his dreams. Taking viewers on a magical journey through visions both wondrous and frightening, the movie offers an intimate peek into Kurosawa\'s unbridled imagination, using cinematic expression to give form to joyful fantasy and haunting reverie.\nStructured as eight individual episodes, the runtime offers a series of vignettes inspired by the director\'s dreams and elements of Japanese folklore. Though these separate installments are not explicitly linked, they all feature the same traveling male protagonist, ""I,"" played by Akira Terao as an adult, Mitsunori Isaki as an adolescent, and Toshihiko Nakano as a little boy. As we watch ""I"" journey through each fantastical story, Kurosawa gradually weaves an eclectic group of tales, covering a clandestine fox wedding, an icy mountain excursion, a meeting with Vincent Van Gough, a nuclear apocalypse, and more, offering an affecting mixture of fantasy and nightmare.\nEvoking the more abstract rhythms of a dream, each vignette is really more of a tone piece or simple allegory than a full-fledged narrative, resulting in a heavy focus on mood, imagery, and theme rather than a fully developed plot. But as basic as the episodes can be, they still provide satisfying and at times quite stirring experiences, often examining recurring motifs related to nature, art, and war. Likewise, Kurosawa clearly injects elements of his own life and childhood into the proceedings, along with a healthy dose of Japanese mythology. And though some of these more culturally specific elements might be lost to international audiences, the underlying emotions and imagery behind each piece prove to be universal.\nMan\'s relationship with the natural world and the horrors of nuclear destruction are particularly prominent themes. To this end, one vignette features a terrifying power plant meltdown and another finds our protagonist traversing a post-apocalyptic landscape populated by mutated ""demons."" Likewise, even the more light-hearted segments offer a strong environmentalist message, including an early episode where a young ""I"" feels sorrow over the loss of his family\'s peach blossom orchard, and the closing vignette which focuses on a peaceful village that rejects modernity. But while these moral and existential quandaries are fairly effective on their own, it\'s really the film\'s exquisite imagery that leaves the most lasting impression, breathing lyrical beauty into Kurosawa\'s delicate parable.\nLeaving the waking world behind, the film\'s aesthetic fully embraces the colorful majesty and dreary horror of dream and nightmare, creating gorgeous visuals that transition from painterly beauty to haunting darkness. The final shot of the opening segment is especially striking, featuring a breathtaking shot of a young ""I"" setting out on a journey among a sea of flowers under the vibrant arch of a rainbow overhead. Similarly powerful imagery litters the runtime, filling the screen with cascading blossoms, frenzied snowflakes, ghostly armies and, in perhaps the movie\'s most visually arresting sequence, the swirling brush strokes of Vincent Van Gough (played surprisingly well by Martin Scorsese).\nIronically, however, as gorgeous as this Van Gough segment is, the episode also can\'t help but reveal some of the film\'s unfortunate flaws. Since certain special effects techniques have not aged particularly well, elements that may have once been groundbreaking are now rather rough. This leads to a few shots throughout isolated vignettes where outdated compositing, chroma key work, and filtering end up sticking out like a sore thumb. Thankfully, these instances are not the norm, and given the movie\'s age, they\'re perfectly acceptable (and expected). Likewise, even outside of the film\'s more overtly fantastical images, Kurosawa\'s masterful sense of cinematic style and staging always enhances the experience, often using wide framing to allow each dream to play out before the audience like some kind of phantom theater production secretly caught on film.\nEntrenched in the wistful melancholy of half remembered musings and fleeting visions, \'Akira Kurosawa\'s Dreams\' is a beautiful piece of magical realism. Joyous and somber all at once, the movie finds powerful emotion within its sumptuous images and simple parables, presenting audiences with a dazzling window into the hopes, fears, memories, and abstract reflections of one of cinema\'s greatest directors.\nThe Blu-ray: Vital Disc Stats\nCriterion presents \'Akira Kurosawa\'s Dreams\' on Blu-ray in their standard clear keepcase with spine number 842. The BD-50 Region A disc comes packaged with a pamphlet featuring an essay by critic Bilge Ebiri, and the director\'s script for a never-filmed ninth dream with an introduction by Nogami.\nThe movie is provided with a 1080p/AVC MPEG-4 transfer in the 1.85:1 aspect ratio. Filled with gorgeous visuals that transition between colorful fantasies and dreary nightmares, the film features a mostly spectacular video presentation.\nCulled from a new restored 4K digital transfer of the original 35mm camera negative, the source is in fantastic condition with a light to moderate layer of natural grain, giving the picture a pleasingly filmic appearance. Clarity is strong throughout, with a great sense of detail, bringing all of the film\'s imaginative imagery to life. Individual flakes of snow, blades of grass, falling raindrops, and fluttering petals are all easily visible on screen with impressive fine texture and depth, both in occasional close-ups and throughout Kurosawa\'s frequent use of wide shots. Colors are especially striking, with a bold and sumptuous palette of rich hues, taking viewers through vivid fields of flowers, misty forests, and even into the painted world of Vincent Van Gough. Contrast is well balanced with even whites and deep blacks that avoid any sense of blooming or crushing.\nWith all that, as beautiful as much of the transfer and cinematography are, not every vignette is as impressive looking as others. To this end, there are a few shots here and there that look comparatively rough with a softer, grainier, and flatter appearance. These instances usually involve effects heavy scenes and the lower grade look is likely a result of compositing and other post-production techniques. Outside of these inherent inconsistencies, I also detected some very minor banding during fades.\nAuthentic to the source and marked by some stirring images, \'Akira Kurosawa\'s Dreams\' comes to Blu-ray with an often spectacular transfer.\nThe film is presented with a Japanese DTS-HD MA 2.0 surround track and English subtitles. With thoughtful design work and solid range, the mix does a great job of complementing the disc\'s beautiful images.\nDialogue is clear, well prioritized, and correctly centralized throughout with no balance issues to report. Delicate atmospherics like rain and rustling wind are spread throughout the soundstage well with subtly immersive surround use. Likewise, key effects, like a jingling bell tied to a character\'s footsteps, are emphasized nicely. The track also livens up quite a bit with surprising range during more aggressive moments, including a fiery eruption and a dangerous avalanche, which both offer solid LFE. Finally, the film\'s score comes through with nice separation and fidelity, transitioning from gentle flute melodies to classical pieces and a joyfully bombastic funeral procession during the film\'s climax.\nWhile not as enveloping as full-fledged contemporary surround sound mixes, this is a strong track that expands upon the movie\'s sumptuous visuals nicely.\nCriterion has put together an exceptional collection of supplements, including a commentary and a comprehensive documentary. All of the special features are presented in 1080p with Dolby Digital audio and English subtitles for the foreign language portions (unless noted otherwise).\n\'Akira Kurosawa\'s Dreams\' offers a visually stunning journey into joyous fantasy and haunting nightmare. Though the storytelling is very simple, the images are striking and the emotions and themes tied to each vignette are affecting. From a technical perspective, this disc offers a stellar video transfer and a great audio mix. Likewise, Criterion has included a fantastic and comprehensive collection of supplements, including a behind-the-scenes documentary that is actually longer than the main feature. This is a strong release for a wonderful film. Highly recommended.', 'Giuseppe Tornatore\'s Cinema Paradiso won the 1990 Best Foreign Film Oscar after setting box-office records the previous year all over the world. Paradiso had a rough journey on its road to glory, however, with the then-32-year-old writer/director being forced to cut nearly 30 minutes from its original running time and facing critical excoriation and box office indifference upon its original release in Italy. It\'s a fitting metaphor for a film that has become a classic tale about fate, perseverance, and destiny.\nSet in Sicily beginning in the years just after WW II to the late 1950s, and framed by modern-day flashbacks of a renowned film director (French actor/director Jacques Perrin) returning to his Sicilian town for the first time in 30 years, Tornatore\'s hero (and alter-ego) is pint-sized Toto, who finds himself obsessed with the movies, and how they\'re shown, when gruff but tender-hearted projectionist Alfredo (Philippe Noiret) takes him under his wing. Few films in history have articulated so simply and so poetically what it\'s like to have a love affair with the movies.\nOn the 25th anniversary of its North American release, Cinema Paradiso will be honored with a gala screening of a restored print, sponsored by Luce Cinecittà and Dolce & Gabbana, at Hollywood\'s legendary Egyptian Theater, as part of AFI Fest, the evening of November 10, with Tornatore in attendance.\nGiuseppe Tornatore sat down with me during a stopover in Beverly Hills to look back on his classic film and the influence it carries to this day. Here\'s what transpired:\nI have to thank you for this movie. It\'s one of my all-time favorites. As a kid, I WAS Toto. All I have to hear is Ennio Morricone\'s theme from the film, and I start crying--and I\'m not a crier.\nGiuseppe Tornatore: (laughs) Thank you. Words like that are very gratifying.\nLet\'s start at the beginning: how was Cinema Paradiso born?\nIt\'s a very long story to tell in an interview, so I\'ll try to keep it simple. I got the initial idea in autumn of 1977. I was involved with the movie theaters in my village as a projectionist. That autumn, they closed one of the oldest theaters that dated back to the early 1930s. The owner decided to sell the building and they had to clear out all the furniture, and basically clean out and strip the building. He asked me to take anything I wanted. So I spent three or four days there, helping to clean it out...it was so dirty, so musty, the smell, the whole atmosphere was just so sad. It just came to me to take this atmosphere and put it into a story. For the next ten years, I made notes as ideas came to me. I interviewed many of the old projectionists in town for their stories, then I wrote the script. I always thought it was something I\'d make after I made a name for myself, maybe as my fifth or sixth movie. After I finished my first film, my producer said to me ""Don\'t you have a passion project? Something you\'re dying to make?"" And I told him the entire story of Cinema Paradiso, right there. He was so touched that I decided to make it as my second movie.\nI remember reading after I\'d seen it in 1989 that it was only your second feature. You were barely 30 years-old and it felt like a film made by a veteran director, not a kid.\nYes, I shot it in the beginning of 1988. I was 31 years old. When I got the Academy Award, I was 32. (laughs)\nIn terms of your own falling in love with film, was there one movie that did it for you, or a series of movies?\nI\'ve never been able to give a good answer for this question, because I know I will always leave something out. I was lucky enough to grow up in Italy during the sixties and seventies and see such a huge variety of films, some masterpieces, some good, some terrible, that they were all an education for me. From the age of about seven to 26, I would see at least one movie per day in a theater. That was a time when you could see a new film by a master like Fellini, a giallo by someone like Dario Argento, or a B-movie exploitation piece of shit, but I learned something from them all. That\'s what I tell young people who say they want to make movies: \'See everything!\' If you just see the sort of movies that you think you will like, your sensibility will be very narrow.\nIt sounds like you just respect the art of filmmaking across the board.\nYes, it\'s something I learned as a projectionist: I don\'t care if you\'re showing a masterpiece or a piece of crap, you treat that film you\'re showing with the utmost respect and make sure you\'re delivering the sharpest image and best sound possible to the audience. You respect the filmmaker\'s work, whether you like it or not.\nLet\'s talk about the two versions of the film, which are very different.\nIt\'s something that always comes up, but I\'ve never quite understood why. It\'s common for most movies to have multiple versions. For example, a director might premiere their film at Cannes in one version, then recut it before general release based on how it was initially received. When the movie was released in Italy, the timing wasn\'t good, and it bombed, at the box office and with the critics. When your movie isn\'t a success, nobody cares what you have to say. The producers said, ""Maybe it\'s because it\'s too long. Why not cut it down? It will be better if it\'s around two hours long."" So I decided to cut 26 minutes from the movie, putting it at two hours and four minutes. So I said, \'Now it\'s two hours, show me this big success.\' And here\'s what happened: exactly the same thing. Horrible reviews. Zero box office. It made less than $100,000. So this for me was a terrible experience. Then it won the Grand Prix du Jury at Cannes in 1989 and everything changed. Once it became successful, many of the people in the press who excoriated it came up with this alternate history that the long version was terrible and the film was saved by my cutting it down to two hours. Not true. It\'s like the line from The Man Who Shot Liberty Valance: ""When legend replaces truth, print the legend."" (laughs)\nDo you view the two versions as separate films or the same film?\nThat\'s a tough one to answer. I love both, obviously, but I prefer the evolution of the plot in the longer version. I love at the end of the story that the character of Alfredo has this surprising dark side, that he is not so bright as he is in the shorter version. I also like the dichotomy in Salvatore\'s life that he has huge professional success but no success in his personal life.\nThe way I interpreted Alfredo\'s character in the longer version was that he was an artist as much as Toto/Salvatore was, and that his creation was who Salvatore became. He never would have become that successful filmmaker without Alfredo\'s guidance.\nYes, not only that, but in the long version Alfredo is like a big character in a Greek tragedy: he is a mortal who is able to determine the fate of another mortal. So we lost that when I had to cut the movie, as well as this beautiful and tragic love story between Salvatore and Elena. When I cut the movie, I felt like an animal whose leg was caught in a trap, and chose to chew his leg off and live, instead of being a prisoner.\nNow you\'re making me think of that scene in A Pure Formality. Was that scene actually a hidden reference to what you went through with Cinema Paradiso?\n(laughs) Could be.\nTell us about working with the great Ennio Morricone.\nThat was one of the great miracles of my professional life. Ennio is not only a great musician, but has one of the easiest personalities. He works with you not like a temperamental artist, but like a carpenter. It\'s like I could say to him ""Ennio, I need a table with six legs."" He\'d say ""Sure,"" and I would have the most beautiful, perfect six-legged table on the planet. If I said to him, albeit with respect mind you, ""Ennio, I really don\'t like these pieces of music you wrote here, but I know you worked very long and hard on them,"" he\'d toss them aside and say ""Fine. Let\'s start over."" You know when you work with Ennio that you can trust him completely and he is utterly giving of himself and his talent. He\'s utterly accessible, a true collaborator.\nAnd his score is an indelible part of the movie.\nAbsolutely. It wouldn\'t be the same film without Ennio\'s music.\nTell us about the casting of legendary French actor Philippe Noiret as Alfredo.\nWe had three great French actors in major roles: Philippe, Jacques Perrin and Brigitte Fossey, although Brigitte was cut from the shorter version. None of the Italian actors we wanted were available, or they passed on the script. My producer asked me one fine day, ""If you were forced to choose a foreign actor for the part, who would it be?"" I said someone like Philippe Noiret, because I saw Alfredo and Toto being like a bear and a little mouse. At first he passed, because he had four other movies he was committed to. We asked him, as a favor, just to read the script, which he agreed to. Two days later, he called back, said he was in love with it, and if we could free him from one of these four contracts, he would be able to make the film. And he said ""I will play any character in this movie you want, even the child."" (laughs) So that\'s how we got Philippe and he was just a joy.\nYou\'ve got 22 films credited as director on IMDb, many of which are rightfully regarded as masterpieces along with Cinema Paradiso. I interviewed Francis Coppola several years back and asked him if he felt The Godfather was both his salvation and his cross to bear. He said yes, that it was, simply because he\'s done so many other films that he\'s proud of, but his epitaph is going to read ""Francis Ford Coppola, director of The Godfather."" Do you feel that way about Cinema Paradiso?\nYes, I\'m very happy to be so strongly identified with it. I understand that some of my other movies are overshadowed by it, but I love that people remember this film and remember me for it and for the film as being special. I can go into any country in the world for the promotion of my new movie, and I am always asked about Cinema Paradiso, because it\'s always with great love and affection. When I wrote the story, when I made the movie, I wasn\'t thinking of fame and fortune. The movie is not the product of a calculation. It was the result of a feeling and discover after so many years that so many people have been touched by it, telling me as you did that they were Toto. I like to say that the world is populated by a big crowd of little Totos, and that\'s great. I love it.']"	['<urn:uuid:8621a460-3009-472e-8b88-e76c6ef214e2>', '<urn:uuid:f7292831-8287-48bb-8698-b1109960d5fa>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T17:51:21.175311	7	57	3411
98	How do state lotteries generate revenue and impact gambling addiction?	State lotteries generate revenue for public programs through various games including scratch-offs and number-picking games, particularly during economic stress periods, though their popularity isn't directly tied to state fiscal health. However, this government-sanctioned gambling can contribute to addiction problems, particularly affecting vulnerable populations like bipolar disorder patients. For these individuals, gambling addiction can be diagnosed through specific criteria including frequent betting, borrowing money to gamble, neglecting bills, and feeling guilty about gambling, with five or more positive responses indicating an addiction requiring professional intervention.	"['A lottery is a form of gambling in which people pay money for the chance to win a prize. The prizes can be cash or goods. The lottery is often run by a state or government. In the United States, there are many different types of lottery games. The most popular are the ones that involve picking a series of numbers. Many of these games also offer a jackpot or other large prizes. The proceeds from these lotteries are usually used for public good.\nA basic requirement for any lottery is some way to record the identities of bettors and their amounts staked. This can be done by writing the bettors’ names on tickets that are subsequently collected and shuffled, or by purchasing numbered receipts that can later be matched to bettors’ records. In modern lotteries, computerized systems are frequently employed.\nIn addition to the identification and shaming of winners, lotteries need a means of regulating and controlling their operation and ensuring their integrity. This can include laws that prohibit players from betting more than a small amount at a time, and regulations that prevent them from obtaining multiple tickets or using multiple accounts. A number of states require lottery operators to be licensed. These requirements typically include a bond and a minimum level of net worth.\nSome states have also established public corporations to operate the lottery. These organizations are required to be audited by the state’s auditor general and are generally subject to a number of other requirements, including the obligation to provide unbiased services. In exchange for these obligations, the public corporation is normally granted a legal monopoly to operate the lottery.\nState governments adopt lotteries in the hopes that they will generate sufficient revenue to fund their public programs. This argument is often made in times of economic stress, when voters are worried about tax increases or cuts in public spending. However, studies show that the popularity of a lottery is not directly related to a state’s fiscal health.\nTo improve your chances of winning, choose random numbers that are not close together. This will reduce the odds of other players choosing the same numbers as you. You should also avoid playing numbers that have sentimental value, such as birthdays. In addition, try to buy more tickets so that you can cover more combinations.\nSome state lotteries allow you to purchase scratch-off tickets and instant-win games. These games are a quick and easy way to play, but they usually have lower odds of winning. The best way to increase your odds is to play a national lottery, which has a larger pool of numbers than a local or state game. A good tip is to use a method that has been developed by Stefan Lustig, who has won seven grand prizes. He recommends analyzing the odds of the previous drawing to determine which combination is most likely to appear. In addition, he suggests that you look for “singletons” – numbers that appear only once – on the ticket.', 'Bipolar disorder: Buying sprees and gambling\nBipolar disorder affects more than two million Americans, and countless more worldwide. In his book Bipolar II, Dr Ronald Fieve examines the causes, symptoms and treatments of this treatable illness. The following excerpt talks about two highly damaging symptoms: excessive spending and gambling.\nBipolar disorder: Reckless behavior\nAlong with sexual indiscretions and comorbid alcohol and substance abuse, many with Bipolar II seek help only after they have recklessly spent their life savings or made wild bets on the stock market -- and lost. It is not unusual for hypomanics to go on wild shopping sprees, spending money they do not have (by running up debt on credit cards) and buying items they never intend to use but want more than anything at the time of the purchase. For instance, I will never forget the patient who came to our first appointment carrying a duffel bag filled with more than 100 pairs of white tennis socks!\nBipolar disorder: Impulsive behavior leads to debt\nI asked Veronica, a 31-year-old retail fashion buyer with Bipolar II, to write down her thoughts and feelings when she was unable to restrain herself from overspending. She wrote the following:\n""When I start feeling that hypomanic \'buzz,\' I never worry about earning money. Instead, I become intent on spending as much as I want. It is not unusual for me to run up my credit card debt in excess of 15 to 20 thousand dollars over a period of 2 or 3 weeks during this mental state. I buy anything and everything I see without regard to whether I need it or will use it. For example, last spring when I was experiencing a mild high, I booked a 2-week Mediterranean cruise for myself and two girlfriends. I didn\'t even ask my friends if they could get off work at that time. I then realized that I had nothing to wear on the cruise and charged about $6,000 in resort wear, including two evening gowns for the late-night formal dinners. Looking back, I see how extravagant and emotionally based the spending was, especially when I found out my friends could not go and my credit cards were over their limits. But at the time, it seemed the right thing to do -- get what I wanted to soothe my hypomanic state, and try to impress my friends at the same time.""\nBipolar disorder: Out of touch with reality\nCompulsive spending is regarded as a problem with impulse control. At times, the hypomanic individual may be right on the brink of being out of touch with reality. An extreme example of this is a patient of mine who threw $50,000 out of his office window in midtown Manhattan and then called his bank to send over more money. Of course, the bank did not comply with his request, and my patient was later treated with a mood stabilizer to modulate his hypomanic mood state. Sure, this is a rare example, but it really did happen. And is there really any difference between tossing thousands of dollars ""to the wind"" and the compulsive gambler who loses all of his money in risky investments? Sadly, either might in consequence try to commit suicide -- and succeed.\nBipolar disorder: Thrill seekers\nWe believe that many hypomanics have the same components as what some psychologists label a ""Type-T,"" or thrill-seeking personality. This personality dimension refers to individual differences in stimulation seeking, excitement seeking, thrill seeking, arousal seeking, and risk taking. Whether he or she chooses the stock market, the racetrack, a poker game, or an all-nighter doing slots in Las Vegas, the hypomanic seeks excitement, craves the adrenaline rush, and thrives on the tension. When he loses, he starts again with a bigger and better plan to win. He finds that this form of instant gratification suits his hyperalert, fast-moving, impulsive temperament. And why not? I mean, with a real job, you have to clock in daily and wait 2 weeks for a bona fide paycheck. With high stakes at the racetrack, your winnings are instantaneous -- or far too often simply nonexistent.\nBipolar disorder: Looking for the ultimate high\nAs I explained in Moodswing, when hypomanics are making a fast deal, they feel in control. They love the sensation of power and the thrill of winning. Usually, the money is secondary -- something to have in case you need to eat. The real high comes from the thrill of the gambling itself -- and most try to beat the system until their last day. But while gambling is a high for the hypomanic, it can lead to far-reaching consequences at work and in the family. The problem of gambling with Bipolar II has escalated greatly with unlimited access to gambling facilities and day-trading accounts on the Internet. When the hypomania goes too high, the compulsive gambler may look for new ways to win money.\nBipolar disorder: Suicide risk\nBut why should someone get so depressed over money, one might ask; many people have repeatedly made and lost fortunes. But the psychiatrist wonders which came first in a financial disaster: Was it the chemical mood swing to a deep depression, or was it the actual loss of the fortune that prompted such a severe reaction? I maintain that the hypomanic high often prompts a reckless loss of money, and suicide is a consequence of what is now called double depression: the combination of the depression, which would have occurred once the mania subsided, and the depressive reaction to loss.\nDo you have a problem?\nMany people do not realize that they have a gambling problem until they lose everything. That\'s why an open discussion with a psychiatrist or other professional may help the person own up to the problem and seek intervention. When Bipolar II patients come to my office presenting with a possible gambling problem, I ask them the following questions. (If you answer ""yes"" to 5 or more of these 16 questions, you most likely have an addiction.)\n- Do you gamble for money frequently?\n- Do you regularly bet on sports games?\n- Have you ever used a bookie?\n- Do you regularly use slot machines or other ""devices""?\n- Do you bet on the lottery frequently?\n- Did anyone in your immediate family have a problem with gambling?\n- Do you feel guilty for gambling?\n- Have you borrowed money to gamble?\n- Have you neglected paying your mortgage or food bills to gamble?\n- Have you missed work because of gambling?\n- Have you ever overextended, gambling more money than you actually had?\n- Has gambling ever led you to consume alcohol or recreational drugs?\n- Do you receive criticism from a spouse or friend for gambling?\n- Have you stolen money to gamble?\n- Have you prostituted yourself to get money for gambling?\n- Have you had to borrow money to pay back gambling debts?\nBipolar disorder treatment\nTreatment goals for Bipolar II patients who are pathologic gamblers are consistent with treatment for major affective disorders, alcoholism, or substance abuse, in that the psychiatrist works on restoring to the patient a normal way of thinking and living. Behavioral therapy is used to improve the patient\'s social skills, and cognitive-behavioral therapy may help the patient prevent relapse. Intervention to reduce the risk of suicide is also necessary.']"	['<urn:uuid:50794e4c-b878-477a-aa13-26d42f17044e>', '<urn:uuid:439a16fa-5fd7-4c14-8153-8aa04702e03a>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T17:51:21.175311	10	84	1728
99	need to know about vehicle repair training and handling gas cylinders in workshop what are requirements	Students can receive automotive repair training through ASEEF-certified courses covering engines, brakes, transmission, and suspension at Denfeld campus. For gas cylinder handling in workshops, specific safety protocols must be followed: cylinders must be secured with chains or cylinder trucks to prevent tipping, valves must be kept closed when not in use, cylinders cannot be used as rollers or supports, and they must be kept away from welding operations or protected by fire-resistant shields. Transportation requires vertical securing and valve protection caps must be in place.	"[""Automotive and Engines\nA program of distinction, the only Automotive Service Excellence Education Foundation (ASEEF) and National Institute for Automotive Service Excellence (ASE) certified high school program in the region.\nEach course covers specific topics and provides an opportunity for students to earn industry-based ASEEF Certification in that area - completion of all required courses and four ASEEF Certifications provides a direct path toward a career or other post-secondary options in the mechanical field. The ASEEF is the student proficency certifications that allow students to transition to full ASE credentially through work and college.\nStudents are encouraged to participate in a successful and active SkillsUSA team with industry partnerships, internships and an opportunity to earn scholarships.The program teaches the safety, environmental, technical and hands-on skills necessary for job shadowing, summer internships, entry-level employment, and college articulation.\nPrograms are offered to students attending Denfeld and Duluth East - courses are located at Denfeld, transportation is provided to Duluth East students to and from the Denfeld campus.\nIntroductory Courses: Can be taken in either order. Content is transferable but not required to be successful. Provides the baseline for advanced courses that follow.\n- Automotive Basics: Engines and Brakes - How to repair properly automobile electrical and brake systems. Students will be challenged to diagnose problems in the electrical and brake systems using meters, analyzers, and computers.\n- Automotive Basics: Transmission and Suspension - Covers basic engine theory, construction, diagnosis, as well as computer, fuel and ignition systems and steering and suspension systems. Students will learn how to service these various systems using proper technical skills\nAdvanced Courses: Students must complete an introductory course and be in good standing. It's preferable that students complete both introductory courses prior to enrolling in advanced courses. Students wishing to enroll in an advanced course prior to completing both introductory courses must have instructor approval. Advanced courses are taught during the same block so content is often intertwined between the courses.\n- Automotive Diagnostics and Electrical - Students work at an advanced level in the automotive shop using test equipment and doing live on-car diagnostic and repair work on computer controlled electrical and brake systems. Students who meet specific criteria may participate in job shadowing and/or an internship at a local repair facility.\n- Automotive Engines and Performance - Covers advanced engine theory, construction, diagnosis, as well as computer, fuel and ignition systems, steering and suspension systems, and wheel alignment. Students will learn how to service various systems using the proper technical skills and are encouraged to participate in job shadowing and/or an internship at a local repair facility.\nAutomobile Technician, Brakes – ASEEF\nAutomobile Technician, Electrical/Electronic Systems - ASEEF\nAutomobile Technician, Engine Performance - ASEEF\nAutomobile Technician, Suspension and Steering - ASEEF\n4-Year High School Plan\nCourses students should take to follow this career path.\nEarn College Credit in High School Through This Coursework\nA structured way of learning a skilled occupation, combining on-the-job training with classroom instruction.\nDonate Your Old Vehicle! Help Students Learn a Career in Automotive Technology\nVehicles donated to this program help Denfeld and Duluth East High School students learn the skills they need to enter the high-tech field of automotive repair. Click here for details."", 'Welding, cutting and brazing present a host of safety and health risks, including fire, explosion, toxic atmospheres, ultraviolet radiation, and electric shock. If you work on sites where “hot work” is performed, you need to be familiar with the relevant standards, even if your employees perform no such work themselves. Let’s look at the basics.\nMoving, Transport, and Storage of Compressed Gas Cylinders: Cylinder valves must be kept closed when work is not being performed, when the cylinders are empty, or they are being moved.\nA chain, cylinder truck, or other steadying device should be used to keep cylinders from being knocked over while in use. Cylinders should be secured in a vertical position, when being transported in a vehicle. Regulators must be removed, and valve protection caps must be in place, before cylinders are moved. Employees should never lift cylinders by their valve protection caps.\nCylinders should be kept a safe distance from welding operations, or behind fire-resistant shields. They must not be placed in confined spaces, or where they might become part of an electrical circuit.\nCylinders, full or empty, must not be used as rollers or supports.\nUse of Fuel Gas: Before being connected to a regulator, a cylinder valve should be “cracked” (opened slightly and closed at once) by a person standing to the side of it. Do not “crack” the valve where the gas would reach welding work, sparks, flame, or another source of ignition.\nCylinder valves should always be opened slowly, and those on fuel gas cylinders opened not more than 1-1/2 turns. If a special wrench is required, it must be left in position, to allow the fuel gas flow to be shut off quickly. In the case of manifolded or coupled cylinders, at least one such wrench shall be kept close by. Nothing should be placed on top of a fuel gas cylinder that might damage the safety device, or impede the quick shutoff of the valve.\nFuel gas should be used through devices equipped with shutoff valves only if the pressure is reduced through a suitable regulator attached to the cylinder valve or manifold. Before a regulator is removed from a cylinder valve, the valve must always be closed, and the gas released from the regulator.\nIf gas is found to be leaking from the valve stem, the valve shall be closed, and the gland nut tightened. If that does not stop the leak, the cylinder must be taken out of use, tagged, and removed from the work area.\nIf fuel gas is leaking from the cylinder valve, and cannot be shut off, the cylinder should be properly tagged, and removed from the work area, except if a regulator attached to the cylinder valve will stop the leak through the valve seal.\nIf a leak should develop at a fuse plug or other safety device, the cylinder should be removed from the work area.\nFuel gas and oxygen manifolds: Fuel gas and oxygen manifolds must bear the name of the substance they contain in letters at least 1” high, painted on the manifold, or on a sign permanently affixed to it. The manifolds must be in safe, well-ventilated and accessible locations (never within enclosed spaces).\nOxygen and fuel gas hoses must not be used interchangeably, and a single hose having more than one gas passage must not be used. Hose couplings must be of a type that cannot be unlocked or disconnected without a rotary motion. Boxes used for the storage of gas hose should be ventilated, and hose must be inspected at the beginning of every shift, and removed from use if damaged or malfunctioning.\nTorches: Torches must be inspected at the beginning of each shift for leaking shutoff valves, hose couplings and tip connections. They may be lighted by friction lighters or other approved devices, but not by matches, or from hot work.\nRegulators and Gauges: Oxygen and fuel gas regulators and their gauges must be inspected regularly, and kept in good working order.\nOil and Grease Hazards: Cylinders, cylinder caps, valves, couplings, regulators, hose and apparatus must be kept free from oil and greasy substances, and must not be handled with oily hands or gloves.\nWelding Cables and Connectors: Use only cable without repair or splices within 10 feet from the cable end to which the electrode holder is connected. Cables with standard insulated connectors, or with splices having an insulating quality equal to that of the cable, are permitted. Cables in need of repair must not be used.\nSafe Operation of Equipment: All equipment should be operated in a manner consistent with the manufacturer’s instructions. OSHA requires that, when electrode holders are to be left unattended, the electrodes must be removed, and the holders placed or protected so that they cannot make electrical contact with employees or conducting objects.\nWhen the cutter or arc welder has occasion to leave his work, or to stop work for any appreciable length of time, or when the arc welding or cutting machine is to be moved, the equipment’s power supply must be shut off.\nWhenever practicable, arc welding and cutting operations are to be shielded by noncombustible or flameproof screens, as protection from the direct rays of the arc.\nNaturally, faulty or defective equipment shall be reported to the supervisor, and taken out of service until repaired, or determined to be in good operating condition.\nVentilation and Protection: Hot work operations involving toxic substances must not be performed without adequate ventilation. Employees exposed to the same atmosphere must be protected in the same manner as the welders and burners themselves.\nMechanical ventilation can be either a “general mechanical ventilation system” or a “local exhaust system.” General mechanical systems must produce the number of air changes necessary to maintain welding fumes and smoke within safe limits (as defined in subpart D of 29 CFR 1926).\nLocal exhaust ventilation consists of freely movable hoods, placed as close as practicable to the work. Such ventilation must remove fumes and smoke, so as to keep concentrations within safe limits.\nLocal exhaust ventilation or air line respirators are required, when hot work is to be performed in an enclosed space, and involves lead (other than as an impurity), metals coated with lead-bearing materials, cadmium, or mercury. Both local exhaust ventilation and air line respirators are required, when the work involves beryllium-containing base or filler metals.\nContaminated air from a workspace shall be exhausted into the open air, or otherwise clear of the intake air source. All air replacing that which has been withdrawn must be clean and respirable.\nOxygen must not be used for ventilation, comfort cooling, blowing dust from clothing, or for cleaning work areas.\nConfined spaces: Generally, the foregoing ventilation requirements must be observed whenever hot work is done in a confined space.\nBy way of exception, if ventilation cannot be provided without blocking the means of access, employees in the confined space shall be protected by air line respirators, with an employee outside the confined space being assigned to maintain communication with those working inside it, and to provide assistance in case of an emergency.\nWhen an employee must enter a confined space through a manhole or other small opening, means (e.g., safety belts and lifelines) must be used, and an attendant, with a pre-planned rescue procedure, stationed outside, to extricate the welder or cutter should the need arise.\nProtective equipment: Employees performing hot work, or working nearby, must wear suitable eye protective equipment. All who are exposed to radiation must be covered completely, to prevent harm from ultraviolet rays.\nFire prevention: Articles to undergo “hot work” must be moved to a designated location whenever practicable. No hot work shall be done where the application of flammable paints, or the presence of other flammable compounds or dust concentrations, creates a hazard.\nFor more complete details concerning safety and health requirements for “hot work,” refer to Subpart J of 29 CFR 1926, and the publications and other OSHA standards referred to in the same.']"	['<urn:uuid:87b16cf2-f93e-4e50-a52a-424b156fb548>', '<urn:uuid:d98f5476-a7b6-4885-a0eb-a6041295832f>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T17:51:21.175311	16	85	1865
100	What are the differences between how radiotherapy is used for breast cancer patients in clinical research studies versus actual treatment of DCIS patients?	In clinical research studies, radiotherapy is being investigated for cardiotoxicity effects by irradiating rat hearts with different doses over 25 consecutive days, with cardiovascular function monitored through echocardiography. In contrast, for actual DCIS patients, radiotherapy is delivered in daily sessions Monday-Friday over a five-week period, with each session lasting 10-15 minutes. The treatment for DCIS patients is carefully planned using CT scans to target the whole breast while minimizing damage to surrounding tissue, and may include an additional booster dose to the specific DCIS area.	['Who we are\nBoard of Recommendation\nHow to Become a Member\nMembers’ Activities Calendar\nWhat we do\nPolicy and Advocacy\nExchange the world\nIntroduction to IFMSA Exchanges\nList of Participating Countries\nResearch Projects Database\nMedical Students International\nYou are here:\nThe investigation of cardiotoxicity and risk of developing subsequent heart diseases following breast cancer radiotherapy: a clinical and experimental study\nPortugal (PorMSIC) - University of Lisbon, Lisbon\nAngiogenesis Lab, CCUL, Faculdade de Medicina da Universidade de Lisboa, Av. Prof. Egas Moniz,1649-028, Lisboa, Portugal\nType of Research Project\n- Clinical Project with Laboratory work\nWhat is the background of the project?\nRadiotherapy plays an important role in the treatment of breast cancer as well as many other malignant diseases; however,radiotherapy is not innocuous and it may lead to coincidental irradiation of the heart resulting in cardiotoxicity and subsequent heart diseases. In particular and the reason behind this project,is that there is still a lack of knowledge and means for early detection and prediction of cardiotoxicity following breast radiotherapy that can appear long before the onset of clinically significant cardiac events.\nWhat is the aim of the project?\nOur main goal is the development of innovative data crucial to better understand the cardiovascular toxicity induced by breast cancer radiotherapy.\nWhat techniques and methods are used?\nThis project is included in an experimental and clinical study. This project, MEDIRAD, is funded by HORIZON 2020(http://www.medirad-project.eu/) The experimental study will use rats as a test subject. The heart of rats will be irradiated with different doses of ionizing radiation by using a linear accelerator, during 25 consecutive days. Cardiovascular function will be assessed overtime by echocardiography. There is also a clinical study associated. A cohort study aims to evaluate early cardiovascular changes after breast cancer radiotherapy. Patients with left and right breast cancer, treated with postoperative radiotherapy alone (without chemotherapy) after breast conserving surgery are being included in this study and several exams are being done.\nWhat is the role of the student?\n- The student will mainly observe\n- The student will observe the practical experiments but will be highly involved in the analysis of the results\n- The tasks will be done under supervision\nWhat are the tasks expected to be accomplished by the student?\nThe student should actively collaborate with the other team members and give critical input every day in what regards to the experimental and clinical part of the present study presented in the techniques and methods. The student should analyze the echocardiography data obtained during the irradiation of the rodent hearts with different doses of ionizing radiation by the linear accelerator and discuss it with the tutor and lab partners. The analysis of the collected data will be made using SPSS (Statistical Package for the Social Sciences), a program for data analysis and statistics.\nWill there be any theoretical teaching provided (preliminary readings, lectures, courses, seminars etc)\nYes, the student will be integrated in the work team meetings and will discuss the project everyday with the supervisor.\nWhat is expected from the student at the end of the research exchange? What will be the general outcome of the student?\n- No specific outcome is expected\nWhat skills are required of the student? Is there any special knowledge or a certain level of studies needed?\nNo special skill is required, just a highly motivated student.\nAre there any legal limitations in the student’s involvement\nType of students accepted\nThis project accepts: - Medical students - Graduated students (less than 6 months) - Pre-Medical students from the American-British system\n- Curigliano; G. et al. Cardiotoxicity of anticancer treatments: Epidemiology; detection; and management. CA. Cancer J. Clin. 66; 309-325 (2016)\n© 2015 - IFMSA.org - Developed by web agency', 'Radiotherapy to treat DCIS\nRadiotherapy treats cancer by using high-energy rays to destroy the cancer cells, while doing as little harm as possible to normal cells. It reduces the risk of DCIS coming back and of developing an invasive cancer in that breast.\nRadiotherapy is given after surgery. The doctors will recommend whether you need radiotherapy based on:\nthe size of the area of DCIS\nwhether the margins of the normal tissue removed were clear enough\nwhether you had a breast lump or discharge from the nipple.\nIf you’ve been advised to have radiotherapy, your cancer specialist will explain why it’s recommended in your situation. It’s important to talk about any concerns you may have with your specialist and your breast care nurse.\nRadiotherapy is effective for all types of DCIS (high- and low-grade). However, as high-grade DCIS is more likely to become an invasive cancer, radiotherapy is more beneficial to people with a higher grade DCIS.\nWe have more information about radiotherapy treatment available.\nRadiotherapy is normally given in the hospital radiotherapy department as a series of short daily sessions. It uses equipment similar to a large x-ray machine. Each treatment session takes 10-15 minutes and they are usually given Monday-Friday, with a rest at the weekend. Before you start treatment, your doctor will explain everything to you and discuss any possible side effects.\nA course of radiotherapy for DCIS usually lasts five weeks. It’s given to you as an outpatient, so you come in for treatment and can then go home again afterwards.\nRadiotherapy doesn’t make you radioactive, and it’s perfectly safe for you to be with other people, including children, after treatment.\nTo make sure the radiotherapy is as effective as possible, it has to be planned carefully. This is to ensure the radiotherapy rays are aimed precisely at the treatment area and cause the least possible damage to the surrounding healthy tissues.\nTreatment is planned by a clinical oncologist. The planning is very important and may take a few visits. A CT scan of the area to be treated is often done as part of planning.\nMarks will usually be drawn on your skin to help the radiographer (who gives you your treatment) position you accurately and to show where the rays will be directed. These marks must stay visible throughout your treatment. Permanent marks (like tattoos) may be used. These are tiny, and will only be done with your permission. You may feel a little discomfort while they’re being done.\nRadiotherapy is normally given to the whole breast. You may also be given an extra dose to the exact area where the DCIS was. This is known as a booster dose.\nAt the beginning of each session of radiotherapy, the radiographer will position you carefully on the couch and make sure you’re comfortable. During your treatment, you’ll be alone in the room, but you can talk to the radiographer who will watch you from the next room. Radiotherapy is not painful, but you’ll have to lie still for a few minutes during the treatment.\nYou’ll need to be able to position your arm so that the radiotherapy machine can give the treatment effectively. Sometimes, your muscles and shoulder joint may feel stiff. If you can’t move your shoulder normally, it may be painful or difficult to give the treatment.\nA physiotherapist may teach you some exercises to make the position for treatment feel easier.\nThis video is of an oncologist explaining how radiotherapy is given and showing the machine.\nSide effects of radiotherapy\nBack to top\nYou may develop side effects over the course of your treatment. These usually disappear gradually over a few weeks or months after treatment finishes. Your doctor, nurse or radiographer will discuss this with you so you know what to expect. Let them know about any side effects you have during or after treatment, as there are often things that can be done to help.\nIt’s not unusual to feel worse before you start to feel better after radiotherapy. Some people can find this a very difficult time and they may feel low or even depressed for a while. The clinical oncologist can advise you about what to expect.\nYou may develop red, dry or itchy skin in the area being treated. If you have dark skin, it may get darker or have a blue or black tinge. Your doctor can prescribe creams to soothe your skin if it becomes sore and flaky. Usually, any skin reaction settles down 2-4 weeks after radiotherapy. You’ll be given advice on how to look after your skin during radiotherapy.\nHere are some tips that may help ease skin irritation:\nDon’t use perfumed soaps, talcum powder, deodorants, antiperspirants, creams or lotions unless they’ve been prescribed by your cancer specialist.\nHave showers rather than baths and turn away from the spray as it can be painful on your skin. If you do have a bath, avoid soaking the affected area for too long.\nPat the area dry gently with a soft towel - don’t rub or dry the area with a hairdryer.\nWear loose clothing as this may be more comfortable and less likely to irritate your skin.\nIt’s very important to avoid exposing the treated area to strong sunshine for at least a year after treatment finishes. Use sunscreen with a high sun protection factor (SPF) of at least 30 to protect your skin whenever it’s exposed.\nYou’re likely to feel tired during treatment and this may continue for a month or two after it finishes. Pace yourself and save your energy for the things you have to do, and that you enjoy. Get plenty of rest, but balance this with some physical activity such as short walks, which will give you more energy.\nWe have information on coping with tiredness (fatigue).\nAches and swelling\nYou may notice a dull ache or shooting pains in the breast, which lasts for a few seconds or minutes. Sometimes, your breast might become swollen during treatment, but this usually improves quickly after treatment is finished. Some women continue to have occasional aches and pains in the breast area after radiotherapy.\nLong-term side effects\nBack to top\nRadiotherapy to the breast can sometimes lead to long-term side effects, but most of these are rare.\nHow the breast looks and feels\nAfter radiotherapy, some women develop red ‘spidery’ marks (telangiectasia) on the skin because small blood vessels have been damaged. These affect how the breast looks but are harmless. Your skin may also have a slightly darker tone. It’s not unusual for the breast to feel firmer, and over months or years, it may shrink slightly in size.\nLet your surgeon or breast care nurse know if you’re unhappy with the shape of the treated breast or if it isn’t the same size as your other breast. There are often reconstructive techniques that can be done to improve this.\nOther long-term side effects\nRadiotherapy may cause a small amount of permanent scarring to your lung. If your lungs are healthy, this will not cause you any symptoms. The doctors take a lot of care when planning radiotherapy to minimise the amount of radiation that your lung receives.\nAlthough some women may worry about the effects of radiotherapy on the heart, it is now very carefully planned to avoid including the heart in the treatment area. It very rarely causes heart problems, and only women who had DCIS in their left breast are at risk.\nWe have more information about how lifestyle changes can help you look after your heart.\nRarely, radiotherapy weakens the ribs in the treated area, making them more likely to fracture.\nIf you’re worried about the risk of developing particular side effects from radiotherapy, talk it over with your cancer specialist.\nWe have more information about the possible long-term side effects of breast radiotherapy.']	['<urn:uuid:9b6aa767-58ab-41ae-a61f-6aca0acdbc22>', '<urn:uuid:fe98aab2-e391-48dd-a7d6-6383e39ed409>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T17:51:21.175311	23	85	1925
