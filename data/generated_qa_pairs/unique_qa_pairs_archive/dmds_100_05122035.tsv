qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	government rules importing motors destination taxes	Government regulations for importing vehicles vary by destination and include multiple tax considerations. Customs taxes, duties and tariffs are country/state specific and depend on factors such as the vehicle's purpose (personal or commercial use), manufacturing year, and value. Some countries offer tax exemptions - for example, New Zealand may waive Goods and Services Tax (GST) if importers agree not to sell the vehicle for two years. In the U.S., specific regulations include EPA emission standards compliance, DOT safety standards, and bumper standards. U.S. customs duty rates apply based on vehicle value, though returning U.S. residents may apply their $800 CBP exemption if the vehicle accompanies them and is for personal use. Additionally, certain vehicles may be subject to 'gas-guzzler' tax based on fuel economy ratings.	"[""What is it and why do I need one? Discover the many benefits of using a freight forwarder to transport your international shipments.Forwarding\n10 Things To Know\nThinking of sending your car, motorbike, truck, bus or tractor overseas? Shipping a vehicle can be daunting if you don't have the help of an industry expert. This guide aims to illuminate the process with eight helpful tips to know before you commence the process.\nThe method used to ship vehicles is commonly determined by the sea freight services available from origin to destination and overall associated costs.\n- Shipping Containers: Most vehicles are small enough to fit inside a standard 20ft or 40ft shipping container.\n- Roll-On, Roll-Off (RORO): The vehicle is physically driven on and off the vessel. The service is subject to availability which can be limited depending on the origin and destination.\n- Out of Gauge (OOG): If RORO services are unavailable and the vehicle is too large to fit inside a 20ft or 40ft shipping container, out of gauge shipping is required. Oversized cargo is typically placed on a flatrack container (a shipping container without sides or top) and secured for transport.\nPickup and delivery (i.e. door to door or port to port delivery) is an additional transport and budget consideration. Where the hire of a shipping container is required, shippers commonly opt to drop-off the vehicle at an agreed packing location for onward delivery to port. Similar considerations are required at destination.\nPickup and delivery considerations (i.e. door to door or port to port delivery) are an additional transport and budget consideration. Where the hire of a shipping container is required, shippers commonly opt to drop-off the vehicle at an agreed packing location for onward delivery to port. Similar considerations are required at destination.\nGovernment taxes, duties and tariffs are country/state specific, with consideration to the following (amongst other factors):\n- Vehicle's purpose (i.e. personal or commercial use, sale, etc.).\n- The year the vehicle was manufactured.\n- The value of the vehicle.\nSome tax exemptions are available depending on the country. For example, in New Zealand non-commercial vehicle importers may be asked to sign a deed agreeing not to sell the vehicle for at least two years to avoid Goods and Services Tax (GST).\nA variety of optional insurance packages are available. Costs are typically based on the price of the vehicle (either agreed or valued) and the level of insurance required (restricted or full). We recommend you take detailed photos of the vehicle prior to shipping in order to document any pre-existing scratches and dents which can be used (if required) to make an insurance claim.\nAll vehicle imports and exports must be cleared by local customs and other regulatory agencies (i.e. Ministry of Primary Industries) before delivery can be permitted. Applicable local forms and permits are required in order to facilitate clearance in addition to successful inspection (if carried out). Regulatory expenses are a further cost consideration.\nThe satisfactory completion and supply of transport documentation is required by carriers and regulatory agencies (amongst possible others) at various points of the shipping process. Submission errors or delays can result in fines and delivery suspension.\n6. Left Or Right-Hand Drive?\nDepending on the destination country, restrictions can apply to the import of left-hand drive cars and trucks where right-hand drive vehicles are permitted (or vice versa). In some instances (typically depending on the age and classification of the vehicle), it may need to be converted at the owner's expense before it can be driven on local roads. In most cases this is either too costly or impractical, especially if the vehicle has modern airbags and crumble zone protection installed.\n7. Preparing The Vehicle\nEnsure your vehicle is physically ready and safe for transport.\n- Remove all personal items from the vehicle.\n- Thoroughly wash and clean the vehicle. Pay particular care to remove any build-up of soil around the tyres and beneath the chassis which can be classified as a biosecurity risk in some countries.\n- Disconnect the battery.\n- Turn off or disable any vehicle alarms.\n- If the vehicle is stowed in a 20ft or 40ft shipping container for transport, ensure the fuel tank is empty. Champion Freight can arrange this at the time of packing if required. Alternatively, roll-on roll-off sea freight services require the fuel tank to be minimised, allowing just enough to drive the vehicle on and off the vessel.\n8. Shipping Terms\nIf you're shipping for the first time it won't be long before you hear industry jargon and acronyms that make it difficult to understand your bill and provisioned services. Please see our glossary of terms for an explanation of words commonly used by the industry.\nWhether it's a bill from a freight forwarder or a carrier, logistics industry invoices can be difficult to discern and understand what services have been charged. For example, you may incur a fee for BAF. What's BAF? For the record, BAF stands for Bunker Adjustment Factor which is a floating sea freight charge accounting for oil price fluctuations. The point is, there are other charges like this which, at first glance, seem ambiguous and make it hard to determine if they have been charged correctly. What's more, transport invoices are commonly charged prepay, which requires some trust on behalf of the shipper.\nFor this reason we recommend you consult a reputable freight forwarder who can explain the charges and guide you through the process. That includes discussion around customs related fees, which can seem inflated and unexpected if the container requires further inspection and processing (i.e. fumigation). Such charges are often beyond the control of the logistics provider, and are simply passed on to the shipper at a rate charged by the relative government agency. It's one of the key reasons why transport terms and conditions are notoriously long and comprehensive.\n10. Overall Costs\nCosts vary depending on a variety of factors, such as carrier expenses from origin to destination, taxes and tariffs, regulatory expenses (i.e. customs clearances), documentation, etc. As a result, it's impossible to offer even a ballpark estimate, but it's likely to be more expensive than you think.\nWe recommend transport overheads are weighed against the value of the vehicle, keeping in mind the loss on selling locally and buying at destination if you forgo shipping."", 'Importing motor vehicles and motor vehicle engines (hereafter referred to collectively as vehicles) into the U.S. can be a very complex and confusing process. Imported motor vehicles are subject to U.S. air pollution control (emission) standards, safety standards, and bumper standards. Most vehicles manufactured abroad that conform to these rules are manufactured and exported expressly for sale in the United States. Foreign made vehicles not manufactured for U.S. exports are unlikely to meet all relevant standards. Be skeptical of claims by a foreign dealer or other seller that a vehicle meets these standards or can readily be brought into compliance.\nThis page provides information for importing a motor vehicle into the United States, including requirements of:\n- U.S. EPA\n- U.S. Department of Transportation / National Highway Traffic Safety Administration\n- U.S. Border Protection\nPlease note that additional state rules may apply including air emission rules, titles and registration.\nBefore attempting to import any vehicle into the U.S., you need to consider the EPA requirements covering vehicle air pollution emissions.\nThe Federal emission testing requirements are based on complex laboratory testing procedures. The emissions are measured while the vehicle is being driven in the laboratory on a chassis dynamometer. The vehicle is driven through a specific driving cycle representing a typical urban drive of 10.5 miles, takes 14 to 36 hours, and includes fuel filling, starting, stopping, accelerating, decelerating, cruising, idling, and sitting while parked*. The emissions measured include hydrocarbons (HC), Carbon Monoxide (CO), Nitrous Oxides (NOx), Evaporative emissions, and particulate emissions. Commercially, testing for light-duty vehicles (e.g., passenger cars) typically costs about $1,000.\nAn EPA importation declaration form EPA Form 3520-1 must be submitted to the U.S. Customs Service for most vehicle importations by individuals (including motorcycles, disassembled vehicles, kit cars, light-duty vehicle/motorcycle engines). [This form is not required for motor vehicles that are imported by their original manufacturer and are new and are covered by an EPA certificate of conformity and bear an EPA emission control label.] One form per shipment may be used, with attachments including all information required to fully describe each vehicle or engine. Off-road vehicles/engines and heavy-duty engines must use EPA Form 3520-21.\nCertain vehicles are excluded from emissions requirements. These include (under specified conditions):\n- Older vehicles (e.g., light-duty gasoline-fueled car or truck built before January 1, 1968\n- Non-Chassis-Mounted Engine\n- Racing vehicles\n- Unregulated Fuel Vehicle\nEPA has published the Automotive Imports Facts Manual that will guide you through the necessary steps for importing a vehicle. Also, see More Resources below for additional information on EPA standards and regulations.\nU.S. Department of Transportation/National Highway Traffic Safety Administration\nMotor vehicles not more than 25 years old must conform to the Department of Transportation (DOT) motor vehicle safety standards that were in effect when these vehicles were manufactured. Passenger cars manufactured after September 1, 1973 must also meet bumper standards. The importer must file form DOT HS-7 at the time of entry, indicating whether the vehicle conforms to applicable safety and bumper standards. The original manufacturer is required to affix a label to the vehicle certifying that these standards have been met if the vehicle is intended for sale in the United States. Vehicles that do not bear a certification label attached by the original manufacturer must be entered as a nonconforming vehicle under a DOT bond for one and a half times the vehicle\'s dutiable value. This is in addition to the regular Customs entry bond.\nUnless specifically excepted (see List of Nonconforming Vehicles Eligible for Importation), the importer must sign a contract with a DOT Registered Importer (RI), who will modify the vehicle to conform with all applicable safety and bumper standards and who can certify the modifications. A copy of the RI\'s contract must be attached to the DOT HS-7 form and furnished to the Customs Service with the DOT bond at the port of entry.\nU.S. Border Protection\nVehicles imported to the U.S. must pass through a port of entry and be cleared by U.S. Border Protection (CBP). For Customs clearance you will need the shipper\'s or carrier\'s original bill of lading, the bill of sale, foreign registration, and any other documents covering the vehicle. You will also need a completed EPA Form 3520-1, or a manufacturer\'s label in the English language affixed to the car, stating that the vehicle meets all U.S. emission requirements.\nThe owner must make arrangements with U.S Customs for shipping a vehicle. Have your shipper or carrier notify you of the vehicle\'s arrival date so that you can coordinate with U.S. Customs. Shipments are cleared at the first port of entry unless you arrange for a freight forwarder abroad to have the vehicle sent in bond to a Customs port more convenient to you.\nYou may make arrangements to import your vehicle with an Independent Commercial Importer (ICI). In this case, the ICI will import your vehicle and perform any EPA-required modifications and be responsible for assuring that all EPA requirements have been met. ICIs can only import certain vehicles, however, and in general, their fees are very high.\nForeign-made vehicles imported into the U.S., whether new or used, either for personal use or for sale, are generally dutiable at the following rates:\nMotorcycles: either free or 2.4%\nDuty rates are based on price paid or payable. Most Canadian-made vehicles are duty-free.\nAs a returning U.S. resident, you may apply your $800 CBP exemption and those of accompanying family members toward the value of the vehicle if it:\n- Accompanies you on your return;\n- Is imported for personal use;\n- Was acquired during the journey from which you are returning.\nFor CBP purposes, a returning U.S. resident is one who is returning from travel, work, or study abroad.\nAfter the exemption has been applied, a flat duty rate of 3% is applied toward the next $1,000 of the vehicle\'s value. The remaining amount is dutiable at the regular duty rate.\n- U.S. citizens employed abroad or government employees returning on TDY or voluntary leave may import a foreign-made car free of duty provided they enter the U.S. for a short visit, claim nonresident status, and export the vehicle when they leave.\n- Military and civilian employees of the U.S. government returning at the end of an assignment to extended duty outside the CBP territory of the U.S. may include a conforming vehicle among their duty-free personal and household effects. The auto must have been purchased abroad and be in its owner\'s possession prior to departure. Generally, extended duty is 140 days or more. Navy personnel serving aboard a U.S. naval vessel or a supporting naval vessel from its departure from the U.S. to its return after an intended overseas deployment of 120 days or more are entitled to the extended-duty exemption. Conforming vehicles imported under the duty-free exemption are dutiable if sold within one year of importation. Duty must be paid at the most convenient CBP office before the sale is completed. Conforming vehicles so imported may remain in the U.S. indefinitely once a formal entry is made for EPA purposes.\n- Nonresidents may import a vehicle duty-free for personal use up to (1) one year if the vehicle is imported in conjunction with the owner\'s arrival. Vehicles imported under this provision that do not conform to U.S. safety and emission standards must be exported within one year and may not be sold in the U.S. There is no exemption or extension of the export requirements.\nCars Imported for Other Purposes\nNonresidents may import an automobile or motorcycle and its usual equipment free of duty for a temporary stay to take part in races or other specific purposes. However, prior written approval from the EPA is required and such approval is granted only to those racing vehicles that EPA deems not capable of safe or practical use on streets and highways. If the contests are for other than money purposes, the vehicle may be admitted for 90 days without formal entry or bond if the CBP officer is satisfied as to the importer\'s identify and good faith. The vehicle becomes subject to forfeiture if it is not exported or if a bond is not given within 90 days of its importation. Prior written approval must be obtained from DOT. A vehicle may be temporarily imported for testing, demonstration, or racing purposes. A vehicle may be permanently imported for show or display. Written approval from DOT is required and should be obtained before the vehicle is exported from the foreign country to the U.S. Information on how to import a vehicle under show or display is available at DOT\'s NHTSA Vehicle Importation Regulations website. A vehicle permanently imported for show and display must comply with all U.S. emission requirements as well, and in general must be imported through an EPA-authorized ICI for modification and testing. EPA will not allow the vehicle to be released to its owner until ICI work is complete.\nCleaning the Undercarriage\nTo safeguard against importation of dangerous pests, the U.S. Department of Agriculture requires that the undercarriage of imported cars be free of foreign soil. Have your car steam-sprayed or cleaned thoroughly before shipment.\nYour Car is Not a Shipping Container\nFor your own safety, security, and convenience, do not use your car as a container for personal belongings.\n- Your possessions are susceptible to theft while the vehicle is on the loading and unloading docks and in transit.\n- Many shippers and carriers will not accept your vehicle if it contains personal belongings.\n- The entire contents of your car must be declared to CBP on entry. Failure to do so can result in a fine or seizure of the car and its contents.\n- Your vehicle may be subject to seizure, and you may incur a personal penalty, if anyone uses it as a conveyance of illegal narcotics.\nProhibited Imports Importations from Afghanistan (Taliban), Cuba, Iran, Iraq, Libya, North Korea, Sudan, Serbia/Montenegro/Kosovo, or Yugoslavia that involve the governments of those countries, are generally prohibited pursuant to regulations issued by the Treasury Department\'s Office of Foreign Assets Control. Before attempting to make such an importation, information concerning the prohibitions and licensing policy should be obtained by contacting:\nDirector, Office of Foreign Assets Control\nU.S. Department of the Treasury, 2nd Floor Anx.\n1500 Pennsylvania Avenue, N.W.\nWashington, D.C. 20220\nYou can call either (202) 622-2500 or (202) 622-2480, or fax (202) 622-1657; or visit the U.S. Department of the Treasury\'s Office of Foreign Assets Control website.\nGas-Guzzlers. Certain imported automobiles may be subject to the ""gas-guzzler"" tax. The amount of the tax is based on a combined urban/highway fuel-economy (miles per gallon) rating assigned by the EPA for gas-guzzler tax purposes. This EPA rating may be different from fuel-economy ratings indicated by the manufacturer.\nIf the EPA has not assigned a gas-guzzler fuel-economy rating for the model automobile you import, a rating must be independently determined. No tax is imposed on automobiles that have a combined fuel-economy rating of at least 22.5 miles per gallon.\n- Imports of Vehicles, Engines and Equipment. Links to various documents and points of contact related to importing vehicles.\n- The Automotive Imports Facts Manual provides comprehensive information on importing your vehicle. The Facts Manual is recommended reading for all vehicle importers.\n- The Quick Overview of Vehicle Imports Requirements provides a way to quickly identify the EPA requirements for many imported vehicles.\n- EPA publishes a list of Canadian Vehicles Approved for Import. Canada has harmonized vehicle emission standards with the U.S. and many vehicles made for sale in Canada are eligible for import to the U.S.\n- Independent Commercial Importers (ICIs) are businesses authorized by EPA to import, modify and test vehicles to demonstrate conformity with U.S. emission requirements.\n- The EPA Kit Car Policy explains the EPA requirements for manufacture and import of replica vehicles.\n- Smog Check. Visit Smog Check if you intend to import a vehicle into California.\nDOT/National Highway Traffic Safety Administration (NHTSA):\n- National Highway Traffic Safety Administration (NHTSA). General instructions for vehicle importation, including Canadian and non-Canadian vehicles.\n- National Highway Traffic Safety Administration (NHTSA). An extensive list of frequently asked questions concerning vehicle importation.\n- National Highway Traffic Safety Administration (NHTSA) list of Approved Vehicles. This is a good place to start. If the vehicle you want to import is not on this list, it will be very difficult to import it. If it is on that list the next step is that it MUST be imported/ brought up to snuff through a ""Registered Importer"" and meet all US DOT and EPA requirements.\n- National Highway Traffic Safety Administration (NHTSA) list of Registered Importers. This list contains contact information for businesses that have been approved as a Registered Importer (RI) of non-conforming motor vehicles for 2006. The listing, which is sorted by state, then alphabetically by business name, is periodically updated as new RIs are approved or existing RIs withdraw from business.']"	['<urn:uuid:400a6f38-c893-45b4-a28d-dd8a14f01295>', '<urn:uuid:246bc10e-73ba-468e-8951-59439862ebdd>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	6	125	3212
2	I'm organizing outdoor band practices during summer, and I want to prevent dehydration. What's interesting is that students rarely complain about being thirsty - how can I identify if they're getting dehydrated?	Interestingly, thirst is rarely a symptom of true dehydration. In fact, the absence of thirst in dehydrated individuals is particularly problematic because this occurs when their body needs fluids most. Instead, watch for a sudden decrease in energy and performance levels. Take any complaints of headaches, dizziness, or body aches seriously. Dehydration occurs when the body loses large amounts of fluid through perspiration, especially when temperature is above 80 degrees and humidity is high. If fluids aren't replenished early, the body loses its ability to release extra heat through perspiration, which can lead to dangerously high body temperatures.	['Summer band camps and marching clinics have become a mandatory part of most high school band curriculums, and benefit students in these programs in many ways. Not only does the band get a head start on learning half-time and competition shows, incoming freshmen have the chance to acclimate to the rigors of the marching band before classes begin and every student benefits from the physical conditioning regimen many directors incorporate into these camps, which gets them in shape for the sometimes strenuous physical demands of long parades and complex marching shows. However, these outdoor rehearsals in the sweltering August heat pose the same risks and health threats as summer sports practices. Directors should be able to recognize the signs and symptoms of heat-related illnesses, learn how to prevent them, and arm themselves with information on how to help students who may fall prey to extreme heat and humidity.\nDehydration is perhaps the most common problem among those who participate in prolonged physical activity in the extreme heat of late summer. The body loses large amounts of fluid through perspiration, drastically more so when the temperature is above 80 degrees and humidity is high. If these fluids are not replenished at the first sign of dehydration, the body loses its ability to release extra heat through perspiration. Without this self-cooling mechanism, the body temperature can become dangerously high, and if there is not enough fluid in the body to maintain kidney function, dehydration can be fatal, even in young people.\nThe simplest way to prevent dehydration is to drink plenty of water (and maybe even a sports drink) before any physical activity and to replenish these fluids often during and after any workout. Surprisingly enough, thirst is rarely a symptom of true dehydration. In fact, the lack of thirst often experienced by dehydrated individuals is especially problematic because this is often when their body needs fluids the most. Directors should instead watch for a sudden decrease in energy and performance levels, and take any complaints of headaches, dizziness, or body aches seriously.\nDehydration may also be a symptom of other heat-related illnesses, some of which are especially severe. Heat cramps are forceful, painful muscle contractions, which are believed to be caused by dehydration and poor conditioning rather than a lack of salt or other mineral imbalances. Because these types of cramps usually strike the hamstrings or the muscles at the back of the calves, those who participate in summer sports or any other outdoor physical activities are particularly susceptible. To relieve heat cramps, move the affected student to a shaded area or other cool environment, give them plenty of water, and let them rest. Proper stretching and frequent water breaks may help to prevent heat cramps, or at least alleviate the potential severity of the cramps.\nThe main symptom of heat exhaustion is, as the name implies, exhaustion, but it is also a result of excessive heat and dehydration. Signs of heat exhaustion may include paleness, dizziness, nausea, vomiting, fainting, and a moderately increased temperature of 101-102 degrees. In this case, because the elevated body temperature is caused by the heat, it’s not truly a fever. Rest and water may be enough to cure mild heat exhaustion; the use of ice packs and fans may also help. In more severe cases, patients may need IV fluids, especially if vomiting prevents them from drinking enough water.\nHeat stroke, also known as sun stroke, is the most severe form of heat illness. In some cases, it can occur even in people who are not exercising, if the weather is hot enough. Victims of heat stroke have warm, dry, and flushed skin and- despite their exposure to extreme weather conditions- do not sweat. Other symptoms may include a sudden change in their level of consciousness, uncoordinated activity, body seizures, or even cardiac arrest, which occurs when the body is no longer able to prevent a further temperature rise through normal compensatory mechanisms such as sweating.\nPeople with heat stroke usually have a dangerously elevated body temperature of 106 degrees or higher. Some may even lose consciousness. This is a true medical emergency; if you suspect someone is suffering from heat stroke, dial 911 immediately. Don’t wait for the paramedics to arrive to take action; heat stroke victims need to have their body temperature reduced, and quickly because they could develop a lethal arrhythmia or go into multiple organ failure, and their brain cells may literally begin to cook. If possible, immerse the person’s body in cold water, or place ice packs under the arms or near other circulation pressure points. Even spraying the victim with cold water and placing them directly in front of a fan will help some. These people will likely be given IV fluids for re-hydration and may have to stay in the hospital for cardiac monitoring and observation.\nIn addition to providing students with plenty of water breaks, there are some other steps directors can take to minimize prolonged exposure to the elements. If possible, try to schedule rehearsals before 11:00 a.m. or after 6:00 p.m. to avoid being outdoors under direct sunlight during the heat of the day. If these rehearsals run longer than an hour or two, plan to spend at least part of that time indoors, whether it’s to learn new music, stretch out before or after a rehearsal, or simply to take a break from the activities. Directors may also want to consider easing up on the dress code a bit by encouraging students to wear light-weight, breathable clothing, such as active wear. While running shorts may not be allowed in the classroom, they are more than appropriate for a physical outdoor activity, such as marching in August. By taking a few simple precautions, directors can ensure that summer band camps will be a safe and productive experience for everyone involved.']	['<urn:uuid:f9c0a60a-5788-47d0-b673-900f451ba9d6>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	32	98	975
3	As a nephrologist monitoring fluid balance in patients, I'm wondering what calculations we use to determine the total amount of particles in blood - can you explain the formula for plasma osmolality?	Plasma osmolality (Posm) can be calculated using the formula: Posm = 2 x plasma [Na+] + [Glucose]/18 + [BUN]/2.8. The normal Posm range is 275-290 mosmol/kg. Na+, glucose and BUN are the major determinants of plasma osmolality.	['Physiology of water balance • Almost all dysnatremias are a result of water balance • TBW = 60% total body weight • 2/3 ECF compartment with water that is freely diffusable b/w ICF and ECF to maintain identical osmolality • So, serum osmolality measures all compartments, i.e. -- when total body water is elevated, osmolality falls • ADH – governs the excretion of water by the kidney, which prevents dilution of urine when present\nOsmolality v. Tonicity • Osmolality: total number of particles in an aqueous solution (mosmol/kg H2O) • Normal Posm = 275-290 mosmol/kg • Effective osmolality (tonicity): particles that can exert osmotic force across membranes, via movement of water into or out of cells • Na+, glucose and BUN are major determinants of plasma osmolality • Posm = 2 x plasma [Na+] + [Glucose]/18 + [BUN]/2.8 • Effective osmoles (Na+ , glucose) exert water shifts unlike urea, ethanol\nWhat factors regulate the ECF? • There are multiple sensors (afferent limb that monitors EABV – amount of arterial blood that fills the arterial circulation) • Baroreceptors • Low pressure baroreceptors in venous beds that monitor for volume overload – atria, pulmonary vascular bed • High pressure barocreceptors (carotids) • JG apparatus (located at the thick ascending limb of the Loop of Henle to regulate afferent arteriole) – renin release • Efferent limb – primarily renal • Regulates rate of sodium excretion, which is most significant in the tubules • RAAS system – Ang II • ADH – released by posterior pituitary with 1% change in blood osm • Catecholamine release – directly stimulates sodium reabsorption at PT and LOH\nADH • ADH is created in supraoptic and paraventricular nuclei • Released as granules travels to posterior pituitary gland where it is released as osmolality increases • Site of action is collecting tubule • Regulated by plasma osmolality (released at 280-290), volume, stress, nausea, pregnancy\nApproach to hyponatremia • History and physical often gives clue to etiology of hyponatremia -- so take a good history 1. Check serum osm to make sure – hypotonic hyponatremia – total body water issue 2. Determine volume status • Hypovolemic • ↓ [Na+] = ↓↓TBNa/↓TBW • Euvolemic • ↓ [Na+] = ↔ TBNa/↑TBW • Hypervolemic • ↓ [Na+] = ↑TBNa/↑↑TBW\nApproach to hyponatremia 3. Is ADH response appropriate? Is H2O excretion normal or impaired • Uosm < 100 mosmol/kg indicates that ADH is appropriately suppressed • Primary polydipsia • Reset osmostat (when Posm is below normal) • Low solute intake • Uosm > 100 mosmol/kg occurs in majority of hyponatremic patients and indicates impaired H2O excretion\nHypervolemic Hyponatremia • Increased water much greater than sodium • Usually states of decreased effective circulating volume • CHF, nephrotic syndrome, cirrhosis, renal failure • What is urine sodium in these cases?\nHypovolemic Hyponatremia • Total body sodium loss > total body water loss • Determine if sodium loss is renal or extrarenal • U[Na] < 10 indicated extrarenal loss (Vomiting, diarrhea, third spacing) • U[Na] >20 renal losses (diuretics, mineralocorticoid deficiency, salt-losing nephritis, osmotic diuresis, bicarbonaturia, ketonuria\nClinical manifestations • Depends on rapidity of fall which does not allow for cerebral adaptation • Symptoms typically occurs below 125 mmol/L (if from previously normal levels) • Mostly neurologic symptoms • Nausea, headache, lethargy, ataxia, psychosis, seizures, coma\nSymptomaticHyponatermia • Key points: • 3% saline only if seizures or other neurologic manifestations (can consider faster infusion) • Concomitant lasix helps to eliminate free water • Frequent labs absolutely necessary • What is the risk of too rapid correction?\nCase examples • An 82 y/o woman is admitted from a nursing home with increasing lethargy and confusion. She has a baseline dementia, but is normally animated and interactive with family and staff. She has had a poor appetite over the past year with significant weight loss, and currently eats very little. Two weeks ago HCTZ was added to her medications. Over the past few days, the nurses note some n/v, no diarrhea, fever or other complaints. On exam, she has some dry oral mucosa but she is not orthostatic. There is no evidence of CHF, ascites or edema. She is awake, but lethargic. Neuro exam is nonfocal. Labs: Na 121 (last 130 4 weeks ago), normal renal/liver function. Serum osm 200, urine osm 220, urine Na 30.\nDifferential for hyponatremia • Volume depletion from n/v • Thiazide diuretic – how does this cause hyponatremia? • Tea and toast • SIADH\nHow would you treat this patient? • Stop diuretic +/- IVF (given ?SIADH) • If you gave patient 1L NS and she had SIADH, what would you expect to happen?']	['<urn:uuid:0e08c056-d972-4f6f-ace5-94f43d8852b5>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	32	37	770
4	As a tuna farm operator planning to set up in the Mediterranean, I need to understand the different cage systems used - could you explain the main types of tuna farming cages and their typical specifications?	In the Mediterranean, two main types of cages are used: 'farming' cages for small tuna specimens kept for long periods (often more than 20 months), and 'fattening' cages where tuna are typically confined for 1-7 months. The industry mainly uses circular ringtype open-sea floating net cages, either locally built or purchased from large manufacturers. Cage sizes vary from 30-90 meters in diameter, with net depths commonly ranging from 15 to 20-30 meters. Spanish operators typically use larger cages (90m diameter), while Croatian operations prefer smaller ones with net depths around 13 meters.	['Tuna farming. The wild bluefin tuna are kept alive and carefully transferred to towing cages. The transfer action is a crucial activity as specimens may suffer severe stress that may lead to death.\nAt present there is no efficient method to establish the fish biomass moving into the towing or farm cages making it rather difficult to determine the size and age composition of the fish. During the transfer process the fish are gently forced to move from the purse seine net to the towing cage usually by sewing the nets together. Once the tuna are all moved into the towing cages, tugboats are used to transport the fish from the fishing area to the on-growing or farm site.\nTowing speed does not usually exceed 1–1.5 knots in order to avoid excessive tuna mortality. However, such a low speed implies long transportation trips that may last days, weeks or even months which are further complicated by the need to adequately feed the confined fish. Mortality rates during transportation are usually quite low (1–2 percent) although there have been rare cases where all the fish have died.\nIn the Mediterranean, the companies engaged in this form of mariculture start stocking their tuna cages in late spring (May/June). This input season lasts for a couple of months (May/June or June/July), however, in the case of Croatia and Malta the season may extend to late summer (September). Mediterranean tuna farms largely use circular ringtype open-sea floating net cages, either built locally or purchased from several large equipment manufacturers. The size of the cages varies from 30–90 m in diameter, with net depths commonly ranging from 15 to 20–30 m. The industry mainly uses cages with a 50 m diameter and net depths varying according to sea location. The larger cages (i.e. 90 m in diameter) are mainly used by the Spanish operators while those in Croatia prefer smaller ones in terms of net depth, i.e. 13 m.\nGenerally the weight of the stocked tuna is between 150–200 kilograms, however Croatian operations generally start their farming with smaller specimens weighing around 8–25 kilograms, while countries like Italy, Malta and Spain may even stock giant tunas weighing as much as 600 kilograms.\nIn the Mediterranean, there are mainly two types of cages used, those for “farming” and those for “fattening”. The “farming” cages are designed to contain generally small tuna specimens for long periods of time often more than 20 months. Most countries in the region do not retain the fish for such long periods and usually only confine the tuna for periods of 1–7 months. The “fattening” season which may extend to February and generally not beyond December/January is closely linked to the market demand/ opportunity. The fish may also be sold few days following capture as harvesting is often agreed beforehand between the producer and the fish trader.\nMany Mediterranean countries, including Portugal, are currently farming Atlantic bluefin tuna: Croatia, Cyprus, Greece, Italy, the Libyan Arab Jamahiriya, Malta, Morocco, Spain, Tunisia and Turkey. Farms obtain fish from local fishing fleets as well as from vessels bearing other flags (e.g. Malta and Cyprus often obtain their fish supply entirely from foreign vessels). Croatia, France, Italy and Turkey have the highest number of vessels used in tuna fishing (FAO, 2005).']	['<urn:uuid:c490a9f4-ce83-43f4-82d0-f9adda8a1760>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	36	92	545
5	medical conditions track family history checklist	The key medical conditions to track in family history include heart attack/hypertension, high blood pressure, mental illness (such as bipolar disorder and schizophrenia), cystic fibrosis, and sickle cell disease. Your doctor may also want information about conditions like blindness or hemophilia, so it's important to gather as much information as possible about these conditions in your immediate family (parents, siblings, grandparents, and children) and if possible, extended family members like aunts, uncles, nieces, nephews, and cousins.	['The winter holidays are the perfect time to catch up with your immediate and distant family to talk about current events, sports, and what has transpired in your lives over the past year. Perhaps it’s not a thought that’s crossed your mind, but it’s also an excellent time to have a slightly serious conversation–the one about medical history. As patients age, it’s more and more critical to have as clear of a picture as possible when it comes to family history, specifically when it comes to preventing diseases such as colorectal cancer. Read on to learn about the questions you should ask each other while you’re watching the Thanksgiving football game.\nWhy Is a Medical “Family Tree” Important?\nThere are myriad reasons to know your family’s medical history. At the very least, history can give physicians an adequate look into your proclivity to be diagnosed with certain diseases, such as colorectal cancer and breast cancer. Just because an immediate family member had a certain illness isn’t a guarantee you will as well, but it gives doctors important pieces of information, allowing them to run tests. Depending on the history and what the physician is looking for, it can also help doctors place early prevention treatment systems into place. Family history is also a determining factor whether you should get specific genetic testing. Genetic testing isn’t just important to your health–it’s also an inside look as to whether your children (or yet unborn children) are at risk of developing a particular disease.\nDiseases to Pinpoint\nThere is quite a laundry list of medical conditions, both physical and mental, that exist as a diagnosis. Don’t worry if you can’t track down every last bit of information. It can be very challenging if you’re part of a family that already has several deceased members, or who live far apart. This is another reason why family gatherings are a perfect time to ask questions.\nYou should concentrate on your immediate family first. This includes your parents, siblings, grandparents, and children. However, if you can collect information on aunts, uncles, nieces, nephews, and cousins, do so. Depending on the type of condition, this could be important information. Diseases to keep track of for your physician include:\n- Heart attack/hypertension\n- High blood pressure\n- Mental illness (bipolar disorder, schizophrenia)\n- Cystic fibrosis\n- Sickle cell disease\nYour doctor may have questions about other conditions, such as blindness or hemophilia, so obtain as much information as you can.\nAs stated, it’s a perfect idea to bring a checklist to your holiday get-together to try to gather some information. However, you may be met with a problem. It’s unlikely that your family will offer opposition as you ask them medical questions, but they may simply not remember. If you’re part of a family where the matriarch and patriarch have passed on, sometimes it can be difficult to learn this important information. In this case, there are a few things you can do to try to collect some info.\nGet as much information as you can, but also ask your family members for documents that may have in a safe or lying around the house. These would include documents such as birth and death certificates, but also things that are clipped, such as saved obituaries or old letters. Sometimes the best key to uncovering old information is to find it written down, as letters were a primary form of communication decades ago.\nIf you still find yourself coming up short, you can try to search public records either by using the library or a genealogy site. Those who are adopted may have a tough time finding information as well. You may want to consider a biological parent search if only to reveal details about your family history. If you need more information on how screenings change due to family history, make an appointment today with Dr. Sameer Islam, MD. Certain tests can help you find problems such as colorectal cancer before the disease progresses.']	['<urn:uuid:8d820b58-68fc-4c32-8032-8d759bac5068>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	6	76	666
6	archeologist seeking protection rules federal vs state preservation	At the federal level, Section 106 of the National Historic Preservation Act of 1966 protects cultural resources on all public and private development projects involving federal undertakings. At the state level, in Texas for example, the Antiquities Code requires coordination with the Texas Historical Commission for lands owned by state-chartered entities. For wildland fire management specifically, federal regulations require compliance with Section 106 of NHPA and consultation with State and Tribal Historic Preservation Officers for any fire management activities.	['The term cultural resources refers to properties such as archeological sites, buildings, and landscapes that, by their nature, potentially possess significant historical value. One of the earlier environmental laws enacted in the The United States was the Antiquities Act of 1906, which required the protection of cultural resources on federal land. Today, Section 106 of the National Historic Preservation Act of 1966 extends protections not only on federal land but also to all public and private development projects should they involve a federal undertaking (i.e., federal funds or the issuance of a federal permit). Some examples of federal undertakings that commonly trigger Section 106 compliance include:\n- USACE Section 404 Permits\n- Federal Housing Administration Loans and Housing and Urban\n- Development tax credits\n- US Department of Agriculture, Rural Development Grants\n- Federal Energy Regulatory Commission NEPA process\n- Federal Communications Commission NEPA process\nCultural resources are protected at the state level as well. In Texas, lands owned or controlled by an entity chartered under state law (i.e., cities, counties, water/power/utility districts, and authorities, etc.) must comply with the Antiquities Code of Texas. Compliance requires coordination with the Texas Historical Commission impacts, prior to the initiation of ground-disturbing activities, to address project impacts.\nBaer Engineering’s staff includes professional registered archeologists and historians with over 20 years of experience assisting clients with the cultural resources compliance process. Baer provides the following services:\n- Prior to the design phase of your project\n- identify documented cultural resources, and\n- provide guidance on potential permitting constraints for your project.\n- During the design phase\n- assist in navigating the overlapping jurisdictions of agency coordination to better define the compliance process as it pertains to your project.\n- If required by the Texas Historical Commission\n- conduct archeological and building surveys to identify and assess potential impacts to cultural resources, and\n- develop mitigation or treatment plans should they be warranted.\nFor our federal agency clients who are mandated by the National Historic Preservation Act and other federal regulations to provide for the long-term protection and preservation of historic properties, we offer a wide range of cultural resources services tailored to your individual management needs including:\n- Phase I Archeological Surveys\n- Phase II Archeological National Register Testing\n- Phase III Archeological Data Recovery\n- Historic Context Development\n- Historic Buildings Surveys\n- HABS/HAER Documentation\n- Native American Consultation\n- Archaeological Resources Protection Act (ARPA) Investigations\n- Development of Program Alternatives and Programmatic Agreement Documents\n- Interpretive Exhibits and Public Engagement\n- Materials Curation\n- Cultural Resources Evaluation\n- Comprehensive Research Design\n- NAGPRA Compliance\n- Archaeological Construction Monitoring\n- Historical Cemetery Investigations\n- Geoarcheological Analysis\n- Artifact Stabilization and Analysis\n- Collection and Database Management\nSMALL ARMS RANGE 2000-ACRE CULTURAL RESOURCES SURVEY / MOUNTAIN HOME AIR FORCE BASE, IDAHO\nThe U.S. Air Force required a survey of a small arms range at Mountain Home AFB in Elmore County, Idaho. The survey provided an inventory of, and potential effects assessment to, archaeological sites, in compliance with Section 110 of the National Historic Preservation Act, and with regulations set forth under 36 CFR Part 800 by the Advisory Council on Historic Preservation. During the survey, archeologists identified seven new archeological sites and evaluated them for inclusion in the National Register of Historic Places (NRHP) based on criteria established under 36 CFR, Section 60.4. They also identified 108 isolated finds, which by definition are not eligible for inclusion in the National Register because they lack demonstrable association with a context for significance.\nAVON PARK AIR FORCE RANGE CULTURAL RESOURCES SURVEY / FLORIDA\nThe U.S. Air Force required a Phase I archaeological inventory of 190 acres across Avon Park Air Force Range, Highlands and Polk Counties, Florida. The project also required Phase II testing for an archaeological site in the same location. Baer Engineering performed a Phase I cultural resources survey and archaeological inventory that consisted of a pedestrian survey and excavation of 854 shovel tests covering approximately 189.83 acres on six selected noncontiguous tracts within APAFR. Phase II testing and evaluation was conducted on a single previously recorded site, 8HG1107. In the course of the Phase II evaluation, eight 1m x 2m units were excavated, yielding 642 artifacts. The purpose of this investigation was to determine the site’s eligibility for inclusion in the National Register of Historic Places. Findings deemed the site eligible for inclusion in the Register. Geospatial data collected during execution included survey area boundaries, probability zone divisions, location of positive shovel tests, Phase II site datums, and georeferenced site maps. Data were processed in Florida and Texas and checked for accuracy. Deliverables included maps and tables derived from these data. Digital copies of these data were submitted as a CD to the APAFR GIS specialist and with the archived report. Polygons and data tables associated with new and updated archaeological sites were delivered to the FMSF office. All data were in UTM, Zone 17, NAD 83, with planar units in meters. Following the completion of this project and acceptance of the final report by the FDHR, all artifacts, field and laboratory notes, analysis sheets, records, photographs, and a photo-ready copy of the final report were transferred to the Avon Park CRM for in-house curation. The contractor cataloged and curated all materials following State of Florida guidelines. All artifacts were cleaned, dried, stabilized, and packaged in 4-mil resealable-closure polyethylene bags with acid-free labels denoting project, artifact, and provenience information placed inside, with the same information marked clearly and legibly on the exterior of each bag. The Baer team submitted these materials to the CRM for in-house final curation, packaged in cubic-foot acid-free boxes, clearly labeled by project, agency, dates, and FMSF numbers of enclosed sites, with a list of field specimen/sample (FS) numbers. Each box was accompanied by an inventory or packing list on acid-free/lignin-free paper. Original field notes, maps, photographs, and other documentation accompanied the collections of artifacts and other materials and were boxed in the same manner. Oversized maps or aerial photographs were rolled and stored in cardboard map tubes.\nViolet Crown Trail Archeological Services\nThe Violet Crown Trail represents the vision of the Hill Country Conservancy (HCC) to get people out on the land so they can experience the natural beauty of the Texas Hill Country. The 30-mile trail provides a unique Hill Country recreational experience through the urban wildlands of the Barton Creek Wilderness Park, the City of Sunset Valley and Hays County, while surrounding people in wildlife, clean streams, and breathtaking views.\nProject partners included a broad range of government agencies and nonprofit organizations that were involved in the project from initial concept development, research, planning, and construction.\nBaer Engineering obtained a Texas Antiquities permit and conducted archeological investigations of the proposed trails, which traversed through areas of existing green space and municipal parks in western Travis County. Work included a thorough examination of existing archeological and historical records in order to develop a field research plan. Field surveys were conducted along the proposed corridor of the trail to document and evaluate cultural properties, and resulted in the discovery and recording of several archeological sites and cultural features. Treatment recommendations for these properties were coordinated with the Texas Historical Commission, Texas Parks and Wildlife, and the Texas Department of Transportation, and the comments of these agencies were incorporated into an Environmental Information Document for public comment.\nMATOC 2014 Projects – Various Locations\nBaer Engineering provided archaelogical and historical cultural resource management services in support of\nAFCEC at multiple locations in the continental U.S., including Hill AFB, Eareckson Air Station, Moody AFB,\nPeterson AFB, McConnell AFB, Scott AFB, and Ellsworth AFB. Services included archaelogical historic\npreservation, HABS/HAER documentation, archival digitization of military cultural resource artifacts and\nrecords, and Native American consultation, among others. Contract Amount: $3,997,870\nCultural Resources Survey at Nellis AFB, NV\nBaer performed an intensive archaeological survey of three target areas in Range 71 North, resulting in the\nrecording of 25 prehistoric archaeological sites. Three sites were considered eligible for the National Register\nof Historical Places. Baer conducted a cultural resources inventory to document cultural features in the Range\n71 Desert Operation Area of the Nevada Test and Training Range. Baer assisted in addressing Section 106\nrequirements for federal management of military lands. Contract Amount: $65,750.88\nA historic cairn at one of the seven sites documented during the Mountain Home AFB survey.\nCistern feature at one of the sites documented during the Avon Park Air Force Range survey.\nFence-to-Fence Environmental Services at Holloman and Cannon AFBs, NM\nBaer Engineering was the lead firm for this contract. The objective of this project was to provide the services\nneeded to execute the Air Force Environmental Quality program at the two bases. The scope provided for\nenvironmental and cultural resources conservation, environmental compliance, pollution prevention, and\nenvironmental operations and maintenance support activities necessary to support Air Force and AFCEC\nenvironmental mission requirements. Contract Amount: $2,402,140\nMATOC 2013 Projects - Various Locations\nBaer Engineering provided archaelogical and historical cultural resource management services in support of\nAFCEC at multiple locations in the continental U.S., including Mountain Home AFB, Kirtland AFB, Eareckson Air\nStation, Avon Park AFB, and Barrow, AK. Services included post-fire cultural resources surveying, GIS support\nfor cultural resources data sets, and development of heritage resource inventories, among others.\nContract Amount: $309,333 (Fee)', 'Wildland Fire Planning and Cultural Resources\nWildfire is one of the most prevalent, rapid and dramatic forces that affect park cultural resources. Because of the nature of fire, it is also a high risk activity, with potentially serious consequences for firefighters, communities, and park visitors. Careful, methodical planning—far in advance of any fire event—is therefore required of all wildland fire management programs. Planning mitigates potential adverse effects of wildland fire and management of wildland fire on cultural resources and, as a result, requires strong involvement by cultural resource specialists. Deferring involvement until fire is on the ground increases the chance of unacceptable outcomes for cultural resources.\nNPS Management Policies (2006) directs that each park containing burnable vegetation will have an approved wildland fire management plan. Wildland fire encompasses planned ignitions (prescribed burns), unplanned ignitions (wildfires), and mechanical fuel reductions not involving fire. The wildland fire management plan is a strategic document that defines and documents a program to manage wildland fire within a specific park and is the cornerstone of a park’s wildland fire management program. Wildland fire management plans and associated compliance documents provide flexible programmatic multi-year coverage for wildland fire management activities and can accelerate compliance with Federal laws, policies, and Executive Order 13175 when planning or implementing individual fire events or projects.\nThis section of the Cultural Resource and Fire module of RM #28A: Archeology (The NPS Archeology Guide) provides guidance for wildland fire managers; cultural resource managers in parks, regions, and centers; Section 106 coordinators; and park superintendents for integrating cultural resource information into wildland fire planning for compliance with Federal laws, policies, and Executive Order 13175.\nEmployee and public safety is the first priority in every management activity. All planning and implementation activities must reflect this commitment. A job hazard analysis should be prepared for each incident activity.\nFor a downloadable checklist of activities to include cultural resources in wildland fire planning, go to the Wildland Fire Planning and Cultural Resources Checklist (.docx).\nWildland Fire Management Planning and Cultural Resource Law Compliance\nNPS DO #18: Wildland Fire Management and RM #18: Wildland Fire require that wildland fire management plans be compliant with the National Environmental Policy Act (NEPA) and National Historic Preservation Act (NHPA) Section 106, other Federal cultural resource laws, Executive Order 13175, and Department of the Interior (DOI) and NPS policies. Key elements of cultural resource legal compliance include compilation of information in order to consider the effects of fire planning and project implementation on cultural resources, and to allow individuals, associated groups, and federally-recognized Indian tribes to comment on the planning documents.\nConsultation with groups associated with the park, Indian tribes, State and Tribal Historic Preservation Officers, and the public is an important component of compliance with Federal cultural resource laws, policies, and Executive Order 13175. Consultation occurs when planning documents are developed (fire management plans, burn plans, etc.) and when the plans are implemented. Consultations at planning and implementation stages and appropriate documentation of the consultations are essential for compliance with Federal laws, policies and Executive Order 13175.\nIn addition to consultation for NEPA and NHPA compliance, Executive Order 13175 and DOI Policy on Consultation with Indian Tribes require that the NPS conduct government-to-government consultations with federally-recognized Indian tribes when undertaking any Federal action that may have a direct impact on tribes or tribal lands. Tribes may have concerns about cultural resources that may not necessarily be addressed during the Section 106 process.\nCultural Resource Implications\nCultural resource managers participate in the wildland fire management plan NEPA Environmental Assessment or Environmental Impact Statement planning process to ensure that fire program alternatives adequately consider and assess effects on cultural resources. Cultural resource managers should coordinate NEPA and NHPA public involvement for the purpose of developing Environmental Impact Statements and Environmental Assessments. Cultural resource managers can anticipate cultural resource assessment needs for compliance with NHPA Section 106 by initiation of cultural resource inventory under NHPA 110. An active park NHPA Section 110 program to identify and assess cultural resources aids in their protection during fires.\nThe cultural resource manager coordinates with the park or regional Section 106 coordinator. Appropriate tribal officials are contacted at the earliest possible point in the wildland fire planning process so that tribal input can be incorporated into wildland fire planning documents in compliance with Executive Order 13175 and DOI policy.\nFire Management and Cultural Resource Laws contains more information about Federal cultural resource laws, policies, and Executive Order 13175.\nWildland fire management plans and associated compliance documents are structured to provide flexible programmatic multi-year coverage for a range of wildland fire management activities while minimizing the amount of additional compliance efforts required for each individual fire event or project. Fire management planning activities have three distinct levels:\n- Strategic planning;\n- Annual planning; and\n- Project/event planning.\nEach will be described in the following sections. Cultural resource managers in parks, regions, and centers view their involvement in all three levels of planning as critical to their stewardship responsibilities.\nStrategic Wildland Fire Management Planning\nStrategic wildland fire planning considers short and long term benefits and damage from wildland fire to the landscape. Planned activities may include fuel reduction projects (prescribed burns and mechanical fuel reductions), and protocols for responding to unplanned fires (wildfires) that range from full suppression to monitoring from a distance. Land, resources, and incident objectives guide the incident manager in choosing the appropriate responses and tactics for each wildland fire. The overall objective for strategic planning is to reduce damage to natural and cultural resources and to increase beneficial effects of fire for those same resources.\nStrategic planning for wildland fire begins with the park’s general management plan (GMP) and/or foundation statement (FS). The GMP/FS establishes the overall direction for the role of wildland fire within the park. The park’s cultural resource management plans (or resource stewardship strategy) further describes how wildland fire is managed to fulfill park objectives, as well as to establish resource-related objectives for wildland fire management activities.\nThe wildland fire management plan implements the guidance and objectives described in higher-level park planning efforts (the GMP/FS and resource plans), and ensures that wildland fire management activities are integrated into other activities and furthers park stewardship goals. It provides specific operational guidance to the wildland fire management staff, including:\n- Demonstrating that wildland fire management in the park is derived from and consistent with park-wide goals and objectives put forth in other management plans;\n- Describing where, how and why wildland fire will be managed in the park;\n- Defining roles and responsibilities of park staff and cooperators in wildland fire management;\n- Describing how wildland fire will be safely employed to best meet park objectives;\n- Describing how detrimental effects of fire will be managed;\n- Describing the consultation schedule to ensure compliance with Federal cultural resource laws, policies, and Executive Order 13175.\nCultural resource information may include analysis of the positive and negative effects of the proposed actions on specified resources or classes of resources, as well as a preferred alternative. It may also contain descriptions of mitigation actions that become requirements for wildland fire managers when implementing the preferred alternative.\nCultural Resource Implications\nCultural resource managers must become familiar with the contents of their park’s wildland fire management plan and be proactively involved in its development and periodic updates, ensuring that relevant cultural resource information is included in planning documents. Close collaboration between the cultural resource program and the wildland fire program is the most important aspect of planning for cultural resource protection in the event of a wildland fire. The collaboration ensures that information about cultural resources at risk is incorporated into the wildland fire management plan.\nCultural resource managers work with the wildland fire program managers to plan fuels treatment projects that improve conditions of cultural resources. The goal is outlined in strategic plans and recognized as mutually beneficial for both cultural resources and wildland fire programs.\nFor more information about wildland fire management plans, see Chapter 4—Fire Management Plans of RM 18—Wildland Fire Management (.pdf).\nParks conduct an annual review of their wildland fire management program and assess the adequacy of the wildland fire management plan and related compliance documents. The annual planning cycle also may incorporate updates to the multi-year fuels project plan, including projects that may have been deferred, altered, added, or deleted since the last update. The multi-year fuels project plan give fire and cultural resource managers an opportunity to anticipate planning, funding, and survey needs well in advance of project implementation.\nThe annual review process is intended to be interdisciplinary in nature and incorporate affected disciplines across the park. Many parks have a standing wildland fire management committee that helps to review wildland fire management planning and implementation documents.\nCultural resource managers and wildland fire managers work closely to ensure that annual planning is in compliance with Federal cultural resource laws, Executive Order 13175, and NPS and DOI policies. Annual burn plans and the updated wildland fire management plan demonstrate that appropriate opportunities have been provided to comment and that concurrence has been sought from all concerned parties. The park Section 106 coordinator works to ensure that NHPA requirements are met and coordinates consultations for compliance with NHPA and NEPA.\nCultural resource managers maintain accurate and up-to-date levels of information about known resources. This ensures that cultural resource protection protocols are available and implemented for any known cultural resources or category of resources potentially affected by wildland fire. Wildland fire management program funding for emergency stabilization and recovery (ESR) efforts are limited to previously identified cultural resources. Therefore, the more comprehensive and organized the inventory of known cultural resources, the more effective the post-fire stabilization may be. Cultural resource managers ensure that private and sensitive information about the locations of cultural resources are protected but accessible to wildland fire program managers and incident managers.\nFor more information about developing relevant cultural resource information for wildland fire management planning, go to Cultural Resource Data for Fire Management.\nCultural resource managers should consider developing and updating the park cultural READ manuals to ensure that cultural resources will be considered when responding to fires and implementing fuel reduction projects.\nCultural Resource Implications\nCultural resource managers in parks, regions, and centers are involved in the annual review process, both to evaluate how well the wildland fire management program has performed in the past year, to keep abreast of plans for the coming wild fire season, and to update information and concerns about cultural resources potentially affected by the activities being planned. As soon as fuel reduction projects are identified, cultural resource managers should formulate cultural resource data compilation and collection needs into PMIS to begin the funding process.\nMany national parks have not identified all cultural resources within park boundaries. Developing good response protocols in the event of a wildland fire is essential for protecting cultural resources that have not been identified and documented prior to the fire. Protocols are included in wildland fire management documents and communicated to incident managers. More information about mitigating measures for cultural resources may be found in the Toolbox: Fire Treatment Measures for Archeological Resource Protection (.pdf).\nEach fuels reduction project and unplanned ignition has a distinct planning and decision process that guides its implementation. For each process described below, there are opportunities for involvement by cultural resource managers in parks, regions, and centers. Consultation about cultural resources for compliance with Federal cultural resource laws, DOI policies, and Executive Order 13175 is an important component of the project planning and implementation process. Consultation takes place when finalizing the planning documents, when implementing the planned activities, when responding to wildfires, and when planning and implementing post-fire recovery activities.\nPlanning for Wildfires (unplanned ignitions)\nThough this seems contradictory, wildland fire managers do in fact ‘plan for the unexpected.’ Initial response to an unplanned ignition is normally guided by requirements stated in the park’s wildland fire management plan and is influenced by the origin and location of the ignition and the weather and fuel conditions at the time of the response. Responses may be immediate and aggressive, or delayed and reserved, depending on the individual situation and requirements contained in the management plan. For ignitions whose wildland fire management is expected to extend beyond a single day, the interagency Wildland Fire Decision Support System (WFDSS) is engaged. Decision criteria and factors important to the park (identified during the park’s wildland fire management preplanning efforts) are often preloaded into WFDSS, and information about the unfolding event, including decisions on management strategies and tactics, are recorded.\nCultural Resource Implications\nIt is critical that issues and concerns regarding cultural resources be incorporated into the wildland fire management plan and WFDSS decision criteria. Including cultural resources information in the plan increases the probability that the incident manager will take cultural resources into account when implementing wildfire management activities.\nCultural resources at risk for adverse effects of wildland fire are identified and mitigation measures are described in the wildland fire management plan and/or EA/EIS. Cultural resources rehabilitation and restoration should be identified as a specific objective in wildland fire planning documents.\nFor more information about identifying cultural resources at risk from adverse effects of wildland fire, go to Cultural Resource Data for Fire Management.\nFuels Treatment Planning (planned ignitions and mechanical fuel reductions)\nFuels treatments are proactively implemented to meet a variety of wildland fire management and cultural resource management objectives, including reducing the probability of damaging fires through removal of biomass; increasing defensible space around structures and other resources subject to fire damage; restoring and maintaining fire-dependent ecosystems; and maintaining vistas or cultural landscapes.\nFuels management treatments are planned several years in advance in order to secure funding for implementation and allow time for detailed planning and preparation of the site for treatment. Planning includes survey of cultural resources and evaluation for eligibility for the National Register of Historic Places. Since the actual implementation of a given fuels project depends on a large number of variables including weather, funding, and staff availability, the exact fuels treatments planned at the beginning of a season may vary considerably from the those that are implemented as conditions change.\nEach fuels treatment—whether using fire, chemical or mechanical methods—requires a separate plan. Individual treatment plans include a detailed description of objectives, operational details (including the type of tactics and equipment to be used), description of constraints and mitigations, evaluation criteria, other information required for safe implementation, and documentation of cultural resource consultation.\nCultural Resource Implications\nCultural resource managers are closely involved in fuels treatment planning to insure that requirements, mitigation, and protection contained in the park’s wildland fire management plan and compliance documents are included and implemented. They stay current with fuels treatment project planning within individual fire seasons to keep abreast of potential changes and project substitutions. Cultural resource managers also ensure that post-project monitoring is identified in fuels planning documents to ensure that mitigation measures to protect cultural resources were effective.\nImplementation of Planning Documents\nCultural resource managers continue cooperation and coordination with wildland fire management program managers to ensure that cultural resources are considered during the implementation of wildland fire management plans, and that both plans and projects comply with Federal cultural resource laws, DOI policies, and Executive Order 13175.\nGuidance about cultural resource management and protection during wildfires may be found in Managing Cultural Resources during Wildfires. Guidance for ensuring that cultural resources will be considered when implementing fuel reduction projects may be found at Fuel Reduction and Cultural Resources.']	['<urn:uuid:fc6aebf4-8d83-40a3-a97a-f621e8efdbda>', '<urn:uuid:840f5f80-4606-4fd8-9e41-f89c0eb688fc>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	8	79	4132
7	what are the two main reasons why natural language processing systems work best in english	There are two main reasons: First, English is the most widely spoken language in the world, so most of the available training data is in English and English has comparatively straightforward grammar. Second, science is an international field where English is the working language, so researchers work and publish in English, leading to applications being developed in English first.	['International Mother Language Day: Why AI still prefers to speak English\nComputational Linguistics Professor Dietrich Klakow. Photo: Iris Maurer\nWhether voice controls, chatbots, dictation or translation programs – many people now use software that can process natural language on a daily basis. Experience has shown that most of these applications work best in English. Why this is so, whether this will change in the future, and what prospects there are for less common (native) languages, is explained by Saarbrücken computational linguistics professor Dietrich Klakow on the occasion of the International Mother Language Day on February 21.\nAs a native German speaker, one is still relatively well off, says Dietrich Klakow, professor of “Spoken Language Systems” at Saarland University. That’s because most IT speech applications work quite well in German. “But it’s true, many systems in the field of language processing still work best in English,” confirms the professor, who conducts research at the Saarland Informatics Campus.\nThere are two main reasons for this: First, most applications of computer-based language processing are built on machine learning, a subfield of artificial intelligence. “In machine learning, a programmer doesn’t tell the algorithm exactly what to do, but trains it with vast amounts of data from which the algorithm can learn on its own,” explains Dietrich Klakow. And this is precisely the first reason: English is the most widely spoken language in the world, so most of the available training data is also in English. “In addition, English is comparatively straightforward in terms of grammar, which is why computers cope well with it,” says Klakow.\nThe second reason, he says, are the researchers themselves: “Science is an international field of work, so the working language is usually English – also in computer science. So when researching or developing something new, one does so in a way that is easy for colleagues to follow. That’s why most researchers work and publish in English,” says Klakow. This, in turn, leads to many applications being developed in English first – the first machine-translated language pair was English-French. The first synthesized voice was software that read out English newspaper articles. “Most applications have a multi-year head start in English. And the major European languages are usually the first to follow suit,” the professor explains.\nBut what about smaller languages that have few speakers? “By far, most of the world’s languages are not supported at all. There are about 7,000 languages, of which only about 400 have more than a million speakers – and even those 400 are not all researched extensively enough to be used in natural language applications,” Klakow says. The “Google Translator,” which can provide a good first glimpse of the languages that have been researched in computational linguistics, supports a total of 133 languages at various levels as of February 2023.\nA much more serious problem than small languages that are not sufficiently researched in computational linguistics are very widespread languages that are hardly supported or not supported at all. Because here we are very quickly dealing with globally socially relevant issues of digital participation, says Dietrich Klakow. “Many African languages, for example, which easily have ten to 50 million native speakers, can hardly or only very poorly be processed by computers,” says the professor. Together with his doctoral students Jesujoba Oluwadara Alabi, David Ifeoluwa Adelani and Marius Mosbach, Dietrich Klakow has therefore developed a method to fine-tune existing language models to the 17 most widely spoken African languages in a memory-efficient way. Last October, he and his colleagues for this work received a Best Paper Award at the International Conference on Computational Linguistics, one of the leading conferences in computational linguistics.\nResearch to expand the machines’ language horizons continues. Asked how these language capabilities might develop in the future, Klakow says: „In terms of machine processing, more efficient machine learning models that require less training data, or better methods to be able to artificially generate training data, will certainly raise even more languages to a ‘product-ready’ level in the future. My guess is that in ten to 15 years, the 400 most common languages could all have reached this level.” However, he does not believes that all the world’s languages will ever function equally well: “There will never be enough training data to program a ‘Zulu ChatGPT’, for example. In this respect, English will probably always be ahead,” says the professor.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. 2022. Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4336–4349, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\nProf. Dr. Dietrich Klakow\nSpoken Language Systems\nUniversität des Saarlandes, Saarland Informatics Campus\nTel: +49 681 302 58 122\nBackground Saarland Informatics Campus:\n900 scientists (including 400 PhD students) and about 2500 students from more than 80 nations make the Saarland Informatics Campus (SIC) one of the leading locations for computer science in Germany and Europe. Four world-renowned research institutes, namely the German Research Center for Artificial Intelligence (DFKI), the Max Planck Institute for Informatics, the Max Planck Institute for Software Systems, the Center for Bioinformatics as well as Saarland University with three departments and 24 degree programs cover the entire spectrum of computer science.\nSaarland Informatics Campus\nPhone: +49 681 302-70741']	['<urn:uuid:237d7a09-6b5a-492c-a92b-0ddd7cfaf57b>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	15	59	883
8	How does the multieffect distillation process multiply energy?	The multieffect distillation process multiplies energy by reducing pressure in multiple distillation stages. The energy used to generate steam from one stage, or 'effect,' is used in subsequent effects at lower pressure to generate more steam.	['Upon rational reflection it should be concluded that the opportunities for utilizing wind-powered water desalination vegetation in vintage social worker alphabet guide to wellbeing poster growing nations are restricted. At current all that may be thought-about are pilot initiatives or demonstration schemes which cut back the\nvintage social worker alphabet guide to wellbeing poster\nthermal vapor compression (MED-TVC) and utilizing the horizontal tube falling movie evaporator. The equivalent mechanical power is representative of the case of cogeneration of electricity and recent water. And what does the longer term hold? According to Fojtasek, the concept is to develop totally renewable, municipal-scale projects that can convert municipal solid waste trash into low-cost electrical and thermal vitality that, in flip, could possibly be used to create low-price freshwater from at present unusable water sources. “ATSI has helped develop these technologies with completely different partners, and these combined-renewable-product tasks serve communities’ mandates of lowering landfill , producing low-value renewable power, and offering a low-value, stable vintage social worker alphabet guide to wellbeing poster source of ‘new’ water,” he says. In all, the quantity of power required to operate the system will thus be lowered by between forty and 60 percent versus typical thermal desalination techniques, in accordance with Fojtasek. This reduction which additionally equates to significant cost financial savings has been documented in a peer-reviewed paper published within the journal Desalination, he notes. The desalination course of, which is separate from the solar power-assortment system, works by feeding the collected warmth right into a distillation system. “The steam generated by the thermal energy drives a multieffect distillation course of that ‘multiplies’ the vitality by reducing pressure in multiple distillation phases,” Fojtasek explains. “That signifies that the power used to generate steam from one stage, or ‘effect,’ is used in subsequent\neffects at decrease strain to generate extra steam. But what if a price-environment friendly desalination system was already within the works, with the potential to assist Californians with their water provide regardless of drought circumstances? A successful 25,000 gal/day prototype built in the state’s Central Valley long identified for its fertile fields and successful crops has confirmed that a grid-impartial, solar-powered desalination plant can operate efficiently. With its success come plans to begin building of a full-size, 2 million gal/day facility next year within the rural group of Panoche. Design of the facility is presently underneath way. Our planet is covered by 1,390 million cubic kilometres of water, but virtually ninety eight% is undrinkable. The World Health Organisation estimated in 2017 that over 785 million people globally still lack entry to fundamental water services, and the urgent climate change deadline threatens even larger restrictions on the provision of the one useful resource mankind actually must survive. The way forward for utilities lies in technologies that supply both sustainability with effectivity. The ongoing development of photo voltaic-powered desalination opens up the potential for a just about endless water supply, and while the Middle East region could also be dropping influence in oil circles, the rest of the globe would do nicely to be taught from it for a waterplentiful future. It’s anticipated the project will produce low-price, environmental-pleasant water, at an estimated $zero.34/m3 , considerably lower than desalination crops that use reverse osmosis strategies. The price and scale of RO vegetation are so massive that only public water provide corporations with a large number of customers, and industries or resort motels, have considered this know-how as an option. Small RO vegetation have been in-built rural areas where there isn’t any other water provide choice. In some cases, such as the British Virgin Islands, the federal government offers the land and tax and customs exemptions, pays for the majority water received, and monitors the product quality. The government also distributes the water and in some circumstances provides help for the operation of the plants.']	['<urn:uuid:f2d23255-c5d4-4626-b592-462aea12bd39>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	8	36	638
9	java requirements audio plugin	You need to have a JAVA Runtime Environment (JRE) installed to use the plug-in since it uses the JAVA framework. The JRE can be downloaded from http://www.java.com/download/manual.jsp	"['For the reason that the plug-in development uses the JAVA framework you need to have an installed JAVA Runtime Environment.\nIf you don\'t have the JRE (JAVA Runtime Environment) installed then you\'ll get it here: http://www.java.com/download/manual.jsp\nJust unzip the dedicated ZIP-File and copy the plug-in folder into your VST plug-in directory.\nApple Mac OS Audio Units and VSTs:\n1.) Unzip the dedicated ZIP-File\n2.) Copy Transcoder.vst to ~/Library/Audio/Plug-Ins/VST\n3.) Copy Transcoder.component to ~/Library/Audio/Plug-Ins/Components\nImportant: To use the Audio Units you need to copy the *.vst as well.\nThe Principles of Ambiophonics*:\n1.) Ambiophonics uses RACE* (Recursive Ambiophonic Crosstalk Elimination) to enable binaural listening\n2.) The Speakers are positioned with a separation angle smaller than 20° (called Ambiodipol) instead of 60° used in stereo. No worries, in contrast to stereo, the sound stage goes far beyond the boundaries set by the speakers (up to 120°).\n- RACE is a free algorithm made available by the Ambiophonics Institute (http://www.ambiophonics.org)\n- Ambiophonics has been invented by the Ambiophonics Institute\nKey advantages in comparison to stereo by playing the same material from CD, LPs or your computer:\n1.) Elimination of Comb filtering effects and pinna direction finding errors\n2.) By having the speakers close together reflections by walls and the bass mode response of the room are much easier to control\n3.) The center presence is very stable and therefore always easy to locate\nThe sound stage gets a strong deepness, height and wideness which creates a real three dimensional listening experience.\nLocating a single sound source within the original stereo image is much easier with Ambiophonics. You\'re able to pinpoint each instrument in all three dimensions!\nTo achieve optimal cross talk cancellation Ambiophonics has a sweet spot (similar to stereo) along the median line where the sound stage has its best representation.\nWhich records sound best?\nIn contrast to the standard stereo setup where spatially rich recordings that contain binaural cues with inter-aural time differences (ITD) loose their deepness, Ambiophonic plays back a much richer sound stage.\nMany productions mix monaural microphones or direct sources between channels to create phantom images limited to inter-aural level differences (ILD), but even those recordings do get a wider and more precise sound stage with Ambiophonics.\nThe Ambiophonic Transcoder VST Plug-In\nFrequency Limited RACE:\nAs default RACE works on a limited frequency range of 0Hz-4.5kHz by keeping all other frequencies untouched. Deselection of the checkbox activates Recursive Ambiophonic Crosstalk Elimination for all frequencies between 0Hz and 20kHz.\nRecursive Ambiophonic Crosstalk Elimination Attenuation [dB]:\nIncreasing the attenuation value reduces the cross talk elimination which results in a smaller sound stage.\nA higher presence value reduces the crosstalk elimination of ""equiphase/similar amplitude"" signals to emphasize center sounds like vocals.\nDelay [micro seconds]:\nThe delay defines the time offset for the cross talk cancellation wavefront to make sure that the signal arrives the ear at the right point in time. The amount depends on speaker separation and listening distance. It is a good starting point to go for a speaker separation angle of 20° and a delay of around 70μs. Please keep in mind that the delay is a discrete value which changes in steps of ""1/sampling frequency"" (e.g 22.7μs for 44.1kHz or 10.4μs for 96kHz). The slider allows all values but the program chooses the nearest possible delay value.\nJust changing the balance of the audio signal.\nOutput Attenuation [dB]:\nThe Ambiophonics algorithm increases the amplitude under certain circumstances which makes it necessary to attenuate the output signal.\nDisclaimer and License:\nThis Software is provided ""as-is"", without any express or implied warranty.\nIn no event will the author be held liable for any damages arising from the use of this software.\nPermission is granted to anyone to use this software except for commercial use.\nPlease contact me (Stephan Hotto) if there is a wish for a commercial implementation.']"	['<urn:uuid:e482c19b-bb20-446e-8558-7534d5075eb0>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	4	27	639
10	How might the use of zip codes in business decisions lead to legal problems regarding discrimination, and what framework is used to evaluate such cases?	Under equal opportunity laws, using zip codes for business decisions could be problematic if those determinations have a disparate impact on protected classes and cannot be justified by legitimate business necessity. This is evaluated through 'disparate impact' analysis, which occurs when a company employs a facially neutral policy or practice that has a disproportionate effect on members of a protected class, absent a legitimate business need that cannot be achieved by less disparate means. Even if policies appear neutral on their face, they could violate equal opportunity laws if they disproportionately affect protected classes.	"[""The Federal Trade Commission's (FTC) recent report, Big Data: A Tool for Inclusion or Exclusion?, released on January 6, warns businesses engaged in big data analytics that the practice poses liability risks. While the FTC does not propose new law or policy, the report advises that existing laws-including the FTC Act, the Fair Credit Reporting Act (FCRA), and the Equal Credit Opportunity Act (ECOA)-already can reach certain uses of big data.\nThe FTC's warning is not intended as a prohibition. Indeed, the Commission recognizes, and even encourages, the use of big data to innovate and improve the ability of businesses to serve underserved communities. Nevertheless, the FTC plans to monitor the use of big data under privacy, credit, and fair lending laws and bring enforcement actions where appropriate. Accordingly, businesses using or considering the use of big data analytics should review the FTC's report carefully and consider the risks those practices may present, especially in the privacy and fair lending areas.\nThe FTC's report is the product of a public workshop held in September 2014, as well as a prior FTC seminar on alternative scoring products, and it aims to educate businesses on important legal compliance questions and broader big data research issues.1\nBefore addressing specific legal and policy considerations, the report discusses the “life cycle” of big data and emphasizes the associated potential benefits and risks. The FTC distinguishes descriptive analytics-where the objective is to uncover or summarize patterns in data sets-from predictive analytics-where the use of statistical models generates new data. The FTC notes that predictive analytics are frequently used to draw inferences about consumers' likely choices.2 The report highlights a number of benefits that may emerge as a result, including new methods to access credit as well as better health and educational outcomes. Echoing themes from the White House's 2014 Big Data report, the FTC's report also discusses potential risks, suggesting that analytics could be used to expose consumers' sensitive information, implement new forms of price discrimination, or weaken the effectiveness of consumer choice by drawing inferences about consumers who choose to opt out of data collection.3\nApplicable Consumer Protection and Equal Opportunity Laws\nThe FTC stresses that the report is not intended to identify gaps in the law, but rather to explain the Commission's view that various laws already apply to big data practices.4 Accordingly, the FTC notes that a number of existing consumer protection laws, including the FCRA, federal equal opportunity laws, and the FTC Act, may already apply to big data practices.\n- Fair Credit Reporting Act: The FCRA applies to companies that compile, sell, or use consumer reports to make eligibility determinations for certain benefits or transactions. The FTC's report notes that companies are increasingly purchasing and using predictive big data analytics products for eligibility determinations.5\nThe FTC cautions that while companies are generally free to use data derived from their own customer relationships for purposes of making decisions about their customers, this calculus changes when third-party data analytics providers are involved. If an unaffiliated third party evaluates a company's own data and provides that company with evaluations for covered eligibility determinations, the third party acts as a consumer reporting agency (CRA), and both companies can be considered users of consumer reports.6 The FTC emphasizes once more that businesses can be considered a CRA under the FCRA even when they compile non-traditional information, such as data from social media platforms, for eligibility purposes.7\nThe report also reevaluates the FTC staff's position from its 2011 report 40 Years of Experience with the FCRA and now explains that the FTC may consider reports in which identifying information about a specific consumer has been stripped away to be consumer reports for purposes of the FCRA under certain circumstances.8 Take, for example, a consumer who provides information about her social media habits on a credit application. If a company subsequently removes any identifying information, yet still uses the underlying data to generate an aggregated analysis and make a decision affecting that consumer, the analysis would likely be deemed a consumer report, according to the FTC report. The FTC encourages businesses to be mindful of any use of big data analytics to make an eligibility determination that could be covered under the FCRA.\n- Equal Opportunity Laws: The report devotes considerable attention to the applicability of federal equal opportunity laws, including ECOA, Title VII of the Civil Rights Act of 1964, the Americans with Disabilities Act, the Age Discrimination in Employment Act, the Fair Housing Act, and the Genetic Information Nondiscrimination Act, to big data use. The report notes that to prove a violation of these laws, plaintiffs must show either “disparate treatment” or “disparate impact,” based upon a protected characteristic such as race, gender, marital status, or receipt of public income.9 The report places a particular emphasis on compliance with ECOA, over which the FTC shares enforcement authority with the Department of Justice and the Consumer Financial Protection Bureau.10\nDisparate impact analysis is especially important in the context of big data analytics.11 A disparate impact occurs when a company employs a facially neutral policy or practice that has a disproportionate effect on members of a protected class, absent a legitimate business need that cannot be achieved by less disparate means. For example, a business might avoid expressly screening job applications based on gender and instead use analytics to screen applicants in a way that nevertheless has a disparate impact on women. Similarly, making credit decisions based on zip codes could prove problematic if those determinations have an impact on a protected class and cannot be justified by a legitimate business necessity. Thus, in certain circumstances, facially neutral policies could run afoul of equal opportunity laws.\nThe FTC's report cautions that advertising and marketing practices could implicate equal opportunity laws for creditors.12 Even if credit offers are open to all who apply, marketing that affects a creditor's subsequent lending patterns or the terms and conditions of the credit received by borrowers could be cited as evidence of discrimination.\n- Section 5 of the FTC Act: The FTC reiterates that businesses should take care to ensure that their use of big data analytics is done in a way that is not unfair or deceptive to consumers. The FTC recommends that companies engaging in big data analytics consider whether they (1) are honoring promises made to consumers and providing consumers with material information about their data practices, (2) have put reasonable security measures in place, and (3) are undertaking reasonable measures to know the purposes for which customers are using their data.13 At minimum, the FTC remains concerned that companies not sell or share analytics products if they know or have reason to know that the recipients could use those products for discriminatory or fraudulent purposes.\nThe report emphasizes that any FTC or regulatory inquiry under these laws is ultimately case- and fact-specific. The report provides a set of high-level questions focused on FCRA and ECOA requirements that companies using big data analytics should consider when navigating these laws.14\nBig Data Policy Considerations\nBeyond legal compliance, the report also discusses several policy considerations raised by big data. The FTC appears worried that errors and biases in data may be incorporated throughout the entire big data life cycle. The Commission cautions that adding more data may not correct inaccuracies or remove these biases.15 The FTC encourages businesses to consider the following questions when deploying big data analytics:\n- How representative is your data set? Citing the Boston Street Bump application that was designed to detect potholes via a smartphone app, the FTC explains that once the app team realized that lower-income individuals were less likely to carry smartphones, they also realized their application data was not representative of road conditions across Boston.16 According to the FTC, companies should be aware of how “digital divides” may produce under- or over-representative data sets before launching products or services that could produce skewed and unfair ramifications.17\n- Does your data model account for biases? The report notes that even before the widespread use of big data, computer models used to identify good job applicants could reproduce existing biases by incorporating pre-existing discriminatory actions into new decision-making. The FTC recommends that companies think carefully about how both their data sets and algorithms have been generated.18\n- How accurate are your predictions based on big data? The FTC cautions that while big data has improved the ability to detect correlations among data points, it does not always explain which correlations are meaningful.19 The report highlights efforts by lenders to improve access to credit by using non-traditional indicators such as rental or utility bill payment history, but notes that there could be legitimate reasons for consumers to withhold paying or otherwise dispute a bill, which could throw off these innovative credit models.20 The report stresses that when big data analytics affect consumers' ability to complete transactions, the potential effects from inaccuracies could be substantial.\n- Does your reliance on big data raise ethical or fairness concerns? The notion that big data analytics raises larger ethical issues emerged in the White House's 2014 Big Data report and continues to be of concern to the FTC. The report suggests that companies consider assessing what factors go into an analytics model and balance any predictive value against fairness considerations.21 The FTC, however, also highlights how big data can be used to expand opportunity.22\nAs the report states, “[T]he Commission will continue to monitor areas where big data practices could violate existing laws, including the FTC Act, the FCRA, and ECOA, and will bring enforcement actions where appropriate. The Commission will also continue to examine and raise awareness about big data practices that could have a detrimental impact on low-income and underserved populations, and promote the use of big data that has a positive impact on such populations.”23\nIn a concurring statement, Commissioner Ohlhausen acknowledges some of the concerns around big data analytics as legitimate, but encourages policymakers to evaluate these concerns within “the larger context of the market and economic forces companies face.”24 She notes that big data analytics may combine with a competitive market to resolve rather than exacerbate industry's misunderstandings of low-income populations. For instance, lenders could collect and analyze predictive data to market credit offerings to traditionally underserved communities. In this way, big data could allow lenders to reach populations that are difficult to score using traditional credit models.25 She recommends that the FTC test hypothetical harms with strong economic reasoning and empirical evidence as it addresses big data in the future.\nThe overarching message of the FTC report encourages businesses to apply big data analytics in a way that provides benefits and opportunities to consumers, while also identifying the legal pitfalls that could violate consumer protection or equal opportunity laws. Use of big data as a proxy for protected class status or in a way that results in a disparate impact based on protected characteristics without a countervailing justification based on a business necessity carries risks. Where businesses carefully consider their objectives and pressure test their practices, they can reduce exposure to discrimination claims and expand access to credit consistent with the FTC's guidance.""]"	['<urn:uuid:726ad1df-1b8c-4866-877e-17f122a38f1d>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	25	94	1858
11	american century meaning and end	The American Century refers to the idea of American global dominance and hegemony. It was introduced in 1941 by Henry Robinson Luce in Life magazine, equating a happy future with American dominance. However, this era has now officially ended, partly due to American mistakes like the Iraq war (which damaged U.S. credibility with allies) and the financial crisis (which tarnished America's free-market capitalism model and left the country in debt). Additionally, the rise of countries like China and India made it impossible for America to maintain perpetual dominance.	"['Early in 1941, Henry Robinson Luce, the founder of Life magazine, spoke in Tulsa, Oklahoma at a dinner hosted by an association of oilmen. Europe was already at war and Japan\'s attack on America at Pearl Harbor was nearly a year away. Luce, though, had a vision of America\'s global destiny in a world that seemed bent on destruction. ""Ours is the power, ours is the opportunity -- and ours will be the responsibility whether we like it or not,"" he declared. In February, these remarks became the basis of ""The American Century,"" a still-famous five-page editorial in the pages of Life, in which Luce, as one biographer noted, ""equated a happy future with American hegemony.""\nThe American Century, always an inflated notion, can now officially be declared over. Its demise is partly a result of American folly -- like the war in Iraq, which cost the U.S. its credibility with allies all over the world, and the financial crisis, which tarnished the American model of unfettered free-market capitalism and has left the country mired in debt. But even without such missteps, it was never in the cards for America to reign in perpetuity -- while the Chinas and Indias of the world stayed on their knees.\nThere are five roads to the future. The first is Chaos -- which could be a temporary state of affairs, as often occurs in global interregnums, or something more enduring. America could be the planet\'s last Big Daddy -- the last of the Empires to bestride the world, in a new era defined by the retreat of the State and the triumph of personal-empowerment technologies enabling global citizens to forge their own links to each other. Although chaos sounds scary, technology offers the prospect, at least the hope, of an epoch that bears proof of the libertarian idea that small is beautiful, that diversity and dispersion is a good thing, and that it is the control freaks of the world, whether in the form of an autocratic parent or a hegemonic global power, that accounts for the greatest miseries on earth.\nThe second road is a Multipolar World defined by a traditional order built around major nation-states. Nationalism once again exerts itself as a primal force in global affairs, with America and the West challenged by a rising Russia, Iran, India, Turkey and Brazil. This new multipolar arrangement would be global in scope and make for the empowerment of states long regarded by the West as barely capable of organizing themselves. New and improbable-sounding alliances would form -- between Russia and Turkey, and between Brazil and Iran, say. National \'spheres of influence\' might have to be recognized, making life unhappy for the small nations of the planet. Should Europe recover its lost will to power, it could be a major player in such a world -- otherwise it risks being dominated by the others. As in the 19th century, peace in a multipolar world would hinge on a balance of power maintained by the biggest players at the point of a gun.\nThe third road is a Chinese Century. This would be proof of the proposition that the world, after all, does need a dominant player, a global rule maker, to ensure stability. If debt-ridden, imperially over-extended America can\'t do the job, then China, destined to become the world\'s largest economy, might have to step in. The world\'s clocks would be set on Beijing time; the yuan would supplant the dollar as the globe\'s reserve currency. America would be humbled but not necessarily miserable. The pragmatic Chinese aim mainly to enrich themselves, which augurs for the preservation of a global trading system in which the U.S. can participate. A far worse fate (and a much less probable one) would be an Islamic Century and imposition of a global Shariah law. In the meantime, the envies and animosities that America attracted as the hegemon would gravitate to China, which would discover anew the eternal lesson that is it cold and very lonely at the mountaintop.\nThe fourth road is an era defined by global city-states. After the collapse of the Roman Empire, the chaos of the medieval European world yielded not, in the first instance, to the nation-state but to the smaller and more manageable unit of the city-state. The city, not the tribe, became the primary unit of allegiance -- and many cities, like Florence and Venice, became spectacularly inventive centers of culture. In the 21st century, this pattern could play out on a global scale, with the world\'s economy and culture dominated by its most dynamic metropolitan centers, North, South, East and West, from Sao Paolo to Mumbai and Hong Kong. American global cities like New York, Chicago and Los Angeles would be big players, along with European ones like London and Vienna. One of the new \'invented\' cities of the Middle East, like Dubai or Abu Dhabi, could join the pack. A hunkered-down Israel might reemerge as the city-state of Tel Aviv. In time, the new global cities, like those of old, could develop their own \'foreign policies\' and maintain, in effect, their own armies or militias.\nThe fifth road is a universal civilization leading to global government. The journey would be a creepy-crawl of an organic kind. The globalization of finance, already an economic reality, might lead naturally to a global financial sheriff. The globalization of ecological problems, like the warming of the planet, might lead to a global environmental regulator. A universal legal system might evolve to address matters like intellectual property rights. The new rulers of the world would be a global cosmopolitan elite -- the \'superclass\' that now exists in embryonic form, gathering in places like Davos. Global government would be an invitation for utopian thinkers to impose their ideas on the planet -- but the superclass is actually a diverse one of contrasting ideologies. There might be two main parties -- a party of the grand planners, but also a party of market-oriented libertarian types, reflecting the division that now exists among globalists.\nWhichever road ends up defining the future, Americans have to start making some rather large cognitive adjustments. We need a new vocabulary -- a new way of talking about our place in the world. And we must take care to avoid nostalgia for a time that has past -- for nostalgia, which takes its nourishment from illusion and resentment, is political and cultural poison.\nPaul Starobin is the author of the newly-published Five Roads to the Future: Power in the Next Global Age (Penguin) and a staff correspondent for the National Journal and a contributing editor to The Atlantic.']"	['<urn:uuid:0aa48993-3780-431c-9773-31d9d523b847>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	5	88	1106
12	why do some countries have low voter turnout causes of decreased participation	Declining voter turnout is a widespread problem in European democracies and is primarily a symptom of lack of interest and trust in the political system. The main issue is that voters need to believe that their vote can make a change, that the outcome of an election matters, and that the political process can be trusted. While the electoral system itself plays only a limited role in this issue, some measures like making voting more accessible through advance voting and appropriate voting days can have a positive effect.	['General elections in a modern democracy\nReport by Kåre Vollan, rapporteur\nThe workshop was chaired by Mr Mevlüt Çavuşoğlu (TU, EDG), Vice-President of the Parliamentary Assembly of the Council of Europe and moderated by Mr. Peter Wardle, Chief Executive, UK Electoral Commission. The rapporteur gave a short summary of his Issue paper before the following presentations:\nMr. Srdjan Darmanovic, member of the Venice Commission, Montenegro on ‘Legislative measures to improve the legitimacy of elections’; Ms. Lydie Err, Parliamentary Assembly of the Council of Europe (LU, SOC) and member of the Venice Commission on ‘The importance of gender balance as a part of strengthening representativity and inclusion’; Prof. Mark N. Franklin, Professor of Comparative Politics, European University Institute, Florence on ‘The impact of different types of electoral systems on election outcomes and government formation’; Judge Manuel Gonzalez Oropeza, Judge of the Supreme Court for Elections in Mexico on ‘The role of the judiciary in reinforcing citizens’ confidence in elections’; Ms. Anna Sólyom, Project manager, International relations, The Association of European Election Officials (ACEEEO), Budapest on ‘The role of electoral commissions in building public trust’ and Mr. Jonathan Stonestreet, Senior Election Adviser, OSCE Office for Democratic Institutions and Human Rights on ‘Election observation as a pre-requisite for government legitimacy’.\nThe following presents a summary of the most important issues covered in the presentations and the debate.\n2. The impact of systems of representation on the political outcome\nA large variety of systems of representation are being used by member states. The two main classes are the plurality/majority systems and the proportional systems. The systems have inbuilt qualities, and tradition and political considerations have been decisive for the choice of systems in various countries. Even though some systems do have features which would classify them not to be suitable in multi party national elections, the main systems like first-past-the-post (FPTP) in single member constituencies and list proportional systems (List PR) or STV are legitimate and common varieties of systems of representation.\nThe representation of parties under FPTP and List PR is very different. FPTP will normally over-represent the biggest party and the political system will often be reduced to a few-party system. This was illustrated in the presentation of Prof. Mark N. Franklin with striking examples from the UK. FPTP in single-member constituencies is normally chosen when personal accountability is given more weight than the system’s ability to reflect the overall election result in terms of distribution of seats among parties.\nProportional systems are being used where the political mirroring of the electorate in the parliament is regarded as the predominant quality. Dependent on factors such as thresholds, proportion systems will tend to have more parties represented in parliament and coalition governments are more common. Mr. Srdjan Darmanovic reviewed the advantages and disadvantages of plurality/majority systems and proportional systems in terms of their effect on the party system and on the stability of governments.\n3. The impact of systems of representation in other dimensions\nOther qualities are also important when assessing a system of representation, such as the ability to provide gender balance, promotion of dialogue and reconciliation after (and before) conflict, representation of ethnic, confessional or linguistic minorities, simplicity, geographical representation, etc.\nMs. Lydie Err made a strong case of showing that List PR systems would provide for the best gender balance, in particular when applied to large constituencies with large thresholds for representation. She referred to the Revised introductory memorandum1 of the Parliamentary Assembly on the ‘Impact of electoral systems on women’s representation in politics’ which states in its first conclusions:\n“….e. changing the electoral system to one more favourable to women’s representation in politics, including by introducing gender quotas, can lead to more gender balanced, and thus more legitimate, political and public decision-making;\nEmpirically, there is a better representation of women and minorities under List PR systems than under FPTP since every vote counts and parties will tend to include a representative image of the electorate on the candidate lists in order to appeal to all groups of the constituency.\n4. Representation of women and minorities\nAffirmative action, by its very nature, does limit the choice of voters and parties. As a measure to secure the representation of women and certain minorities, such measures are still seen to be legitimate. The measures may be implemented within most systems but there may be unwanted side effects connected to some of the possible methods.\nIn some countries (mostly outside of Europe) special races for women or minorities have been implemented. The disadvantage is that a separate race may represent a segregation which is not helpful to bridge social gaps in the long run. It is preferable that affirmative action is worked into the electoral system. Some of the methods for this are presented in the Issue Paper of this workshop which states that regardless of system used, the following should be kept in mind:\n· “Individuals should be able to freely choose if he or she wants to be considered part of a minority group or not.\nThere are fewer negative side effects when combining group representation with List PR than with majority/plurality systems since the rules can be applied on the composition of each party’s candidate list and therefore do not affect the relative strength of parties.\n5. The requirements to stand for elections\nA balance needs to be struck between the right to stand for election and the need to limit the number of candidates to those with a certain public support. On one hand, there should be equal and fair possibilities to stand for elections, on the other there is a legitimate need to limit the candidacies to those who have a possibility to win some support. The limitation needs to be fair and the possibilities to stand should not be restricted to only those who can actually win seats. The limitation should only touch those candidates who are not serious and who have little support.\nThe mechanisms for proving support are normally either to collect a certain number of signatures or to deposit a certain amount of money which may not be recovered unless the candidate wins a predefined share of the votes. The requirements for standing for election should not bar genuine candidates. All the measures need to be proportional, with a reasonable number of signatures, a reasonable deposit and a fair threshold for recovering the deposit.\nDeclining participation is a widespread problem in European democracies even though some countries have been able to maintain a reasonable turnout in recent elections. Declining turnout is a symptom of lack of interest and trust in the political system in general. The main remedy would for the actors in politics to make sure that their message is relevant and important to people, that the outcome of an election matters, that the political process can be trusted and that the voters can trust that their vote can make a change.\nThe electoral system plays only a limited role within the more fundamental issue of decreased citizen participation in public life, but some measures may have a positive effect. For example, the act of voting may be made more accessible to voters through advance voting, internet voting for special groups (The workshop speakers were more sceptical about general access to internet voting) and appropriate voting days.\nFurthermore, voters need to be able to trust the integrity of the voting and counting processes. This can be done by strengthening the transparency of the process, tasking the election administration to a truly independent election management body (EMB), having a good legal system with an efficient and transparent appeals system, providing easy access for observers to monitor the whole electoral process, etc.\nJudge Manuel Gonzalez Oropeza…[something to be added by K Vollan]\nRegarding electoral management bodies, Ms Sólyom felt that their organization, their independence (material and immaterial), the combination of their members as well as the publicity of their activities need to stand for interests of the community and not for those of the different political parties. Transparency of their activities is highly recommended in order to create confidence in elections.\n7. The role of political parties and media\nThe importance of political parties in providing clear, relevant and interesting messages and for media to cover the political scene, including the election campaigns, in a fair, equitable, interesting and critical manner was underlined. These subjects were, however, addressed in detail during other workshops.\nThe use of new technologies to communicate with voters was also discussed and many felt that when the broad public find interest in the political messages, all media including the new ones will be used. E-election is not only about the voting process but about using the whole range of possibilities to underpin the democratic process of competition for powers.\n8. Election observation\nMr. Stonestreet insisted that election observation is a crucial element of transparency in elections. The transparency that observation provides is a basis for building public confidence in the election administration and in the electoral process as a whole. Public confidence is perhaps the single most important feature of a credible, genuinely democratic election. Absence of public confidence in the elections can cast doubt not only on the legitimacy of those elected but ultimately even on the very concept that elections are meaningful. In that sense, election observation is a necessary condition, although not a sufficient condition, for a genuine election and for the legitimacy of government.\nOf equal, or even greater, importance in ensuring transparency is the role of domestic observation, undertaken by political parties and/or by non-partisan organizations. It is an activity of civil society, broadly construed. Election observation in this context means that citizens – the people themselves – can check that the law is being adhered to and that the process is delivering the collective result of each individual’s free choice. It is unfortunate that the efforts of civil society, particularly NGOs, are increasingly under threat in some parts of the OSCE and Council of Europe regions.\nThe main challenge related to domestic observation in the OSCE and Council of Europe community comes in States where legislation or practice presents barriers to effective observation. This takes a number of forms but includes burdensome accreditation requirements for observers, obstacles to the registration of NGOs, and pressure on, or intimidation of, observers.\nIn order to make observation efficient and credible it is important that elections are assessed against agreed standards and that the observation itself follows a sound methodology. In an effort to build consensus and professionalism in this field, the Declaration of Principles for International Election Observation has been launched under the auspices of the United Nations.\nIn established, old democracies there has often not been a role for observers from domestic NGOs and international organisations. Some countries have changed their laws recently to allow for such observation and it was underlined that this development should continue. Even old democracies can gain from being assessed against international standards and observation provides by itself an invaluable capacity-building measure across borders.\n9. Changes to Electoral Systems\nMr. Srdjan Darmanovic noted that electoral systems are under continuous reform and these reforms are highly political. He insisted that changes should not be implemented too close to the election day, partly to avoid any possibility of manipulation and also to make sure that all parties have a reasonable amount of time to prepare for elections without sudden changes to the rules of the game. There is a need for a certain continuity in order to avoid that the rules are changed frequently by new parliamentary majorities.\n1 AS/Ega (2009) 32 rev. Impact of electoral systems on women’s representation in politics. Prepared by the Committee on Equal Opportunities for Women and Men of the Parliamentary Assembly.']	['<urn:uuid:6ac9b30c-0295-49a2-8600-ee48805212c0>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	12	88	1953
13	Which cleans more VOCs, the Dracaena or rubber plant?	The Dracaena 'Janet Craig' demonstrates superior VOC removal efficiency, cleaning approximately 78% of benzene within 24 hours and being particularly effective at removing trichloroethylene (TCE). The rubber plant, while also effective, primarily specializes in eliminating formaldehyde and carbon monoxide. Both plants require specific care - the Dracaena needs regular leaf dusting and trimming, while the rubber plant needs full or bright filtered light, moderate watering, and monthly high-nitrogen fertilizer.	['We often think of pollution as an outdoor problem, but indoor air can also be polluted with harmful chemicals. According to the U.S. Environmental Protection Agency, people spend 90 percent of their time indoors and poor air quality can cause great harm to their health.\nIndoor air pollutants can trigger coughing, chest tightness, frequent headaches, sore throats, watery or itchy eyes, shortness of breath and asthma attacks.\nSome of the most dangerous indoor air pollutants include carbon monoxide, radon gas, formaldehyde, benzene, ammonia and trichloroethylene.\nThese harmful pollutants arise from cigarette smoke, paint, plastics, chemicals in carpets, cleaning solutions and numerous building and furniture materials.\nNature offers a simple and effective solution for indoor air purification. Many plants help remove pollutants from the air naturally.\nIn fact, a 1989 NASA study suggests having at least one houseplant with air purification power for every 100 square feet of home or office space.\nHowever, be careful in your selection of houseplants if you have pets in your house as some houseplants such as English ivy, red-edged dracaena, peace lily, weeping fig, rubber plant and florist’s chrysanthemum can be toxic to pets.\nHere are the 10 best plants you can grow indoors for air purification.\n1. English Ivy\nThe English ivy is one of the best houseplants for air purification. Its dense foliage can successfully absorb prevalent indoor pollutants and chemicals.\nIts air-filtering abilities can eliminate harmful chemicals, such as benzene, carbon monoxide, formaldehyde, trichloroethylene, xylene and toluene. It will even help clear the air of cigarette smoke.\nThis evergreen climbing plant is very easy to grow and needs little space, moderate temperature and medium sunlight. You can hang it or perch it on the floor, depending upon the available space.\nWater it generously during the early growing stage and simply keep the soil moist for maintenance. Apply a balanced liquid fertilizer on a monthly basis to keep it healthy.\n2. Peace Lily\nThe peace lily is one of the top indoor plants for cleaning air. This tropical plant breaks down and neutralizes harmful indoor gases, such as benzene, formaldehyde, trichloroethylene, xylene, ammonia and carbon monoxide.\nIt also absorbs alcohol and acetone from the air, which is emitted by electronics, adhesives and certain cleaners.\nThis indoor plant needs medium, indirect sunlight. Water it thoroughly 1 or 2 times a week but avoid standing water.\nUse an all-purpose liquid fertilizer once a month to help the plant grow generously. To maximize its air-cleansing ability, keep the foliage dust-free.\nThe occasional fragrant, white blooms of the peace lily during the summer will also enhance the interior of your home or office.\n3. Red-Edged Dracaena\nThe red-edged dracaena is another excellent air-cleaning plant. It has long, wide leaves that are often variegated with red lines.\nOne large potted red-edged dracaena can do the work of several smaller other plants. It helps remove indoor toxins, such as xylene, benzene, trichloroethylene and formaldehyde.\nKeep the soil damp but not soggy. The plant can grow up to 15 feet tall, so you’ll need to trim it occasionally. As the leaves accumulate dust, be sure to wipe them regularly with a damp cloth.\nThis vibrant plant is also great for decorating and filling up empty space in your home and office.\n4. Boston Fern\nThe Boston fern is also considered one of the most efficient air purifiers. It is highly effective in removing formaldehyde, which is found in glues and pressed wood products.\nIt also removes xylene and toluene from the air. It can even remove toxic metals, such as mercury and arsenic, from soil.\nIt prefers indirect sunlight, however a small amount of sunlight encourages growth. Water it regularly to keep the soil moist at all times but make sure the soil does not become soggy.\nGive it a balanced fertilizer every 2 weeks. Protect the plant from warm air from indoor heating as well as cold drafts.\nThis indoor plant prefers a fairly high humidity level, so you can even keep it in your bathroom. You can also keep it in a hanging basket as a decorative element.\n5. Snake Plant\nAlso known as mother-in-law’s tongue, the snake plant is another good one to grow in your house and office for air purification.\nIt can help get rid of formaldehyde, nitrogen oxide and benzene. In addition, it helps lower carbon dioxide levels.\nThis wonderful houseplant can thrive without much light or water. It generally prefers drier conditions and some sunlight. So, do not water it too much, especially during the winter.\nThe best place to keep this evergreen perennial plant is your bedroom, as it gives a slight oxygen boost while you sleep. You can even keep this plant in your bathroom or any other empty corner space in your house or office.\n6. Bamboo Palm\nAlso known as the reed palm, the bamboo palm helps filter out both benzene and trichloroethylene. When placed near furniture, it helps get rid of formaldehyde also.\nThis plant thrives in shady indoor spaces and is easy to care for. Keep the soil moist, but avoid overwatering and ensure proper draining. Make sure to protect it from cool drafts and regularly remove dead or wilted leaves.\nThis slow-growing, attractive plant makes a great addition to your indoor landscape. It can grow from 4 to 8 feet high and looks great in corners. In addition, it is a pet-friendly houseplant.\n7. Spider Plant\nThe spider plant, often called airplane plant, is a flowering perennial herb that helps reduce indoor air pollution. It is effective in reducing formaldehyde, xylene and toluene.\nThis plant is considered one of the most adaptable of houseplants and can thrive in a wide range of conditions. It flourishes in well-drained soil and bright, indirect light. Water the plant regularly but avoid soggy soil.\nThe spider plant makes a beautiful addition to any home interior. You can use it as hanging display or place it in a corner.\n8. Rubber Plant\nThe ornamental rubber plant is another powerful toxin eliminator and air purifier that you can easily grow indoors. It is especially efficient at removing formaldehyde from the air. It also eliminates carbon monoxide.\nThe rubber plant prefers full or bright, filtered light and thrives in wet, tropical conditions. Water it moderately and use a high-nitrogen fertilizer once a month.\nTrim the plant regularly to maintain its size. To keep the broad, deep green leaves shiny, wipe them gently with tepid water and a soft cloth from time to time.\nYou can use this small tree as a centerpiece on a table or on a shelf or windowsill. You can even place it on the floor next to a fireplace or near entryways.\nNote: Avoid contact with the milky latex of the rubber plant, as it can cause skin irritation.\n9. Weeping Fig\nThe weeping fig is one of the world’s most popular houseplants that helps eliminate harmful chemicals from indoor air. It is effective at getting rid of chemicals like formaldehyde, xylene and benzene.\nThis houseplant grows well in a bright room, with a little direct, morning sun. Water the plant regularly to keep the soil moist.\nIt also needs plenty of fertilizer throughout the growing season. The plant is sensitive to cold and should be protected from strong drafts.\nIt looks like a typical tree with a single trunk and a spreading canopy and makes beautiful corner plants. You can also use it in entryways and foyers.\n10. Florist’s Chrysanthemum\nThe florist’s chrysanthemum plant, also known as florist’s daisy and hardy garden mum, is another effective houseplant for purifying the air.\nIts strong air-cleaning qualities can help remove trichloroethylene, benzene, formaldehyde, ammonia and other chemicals.\nThis seasonal flowering plant needs bright, indirect light to grow properly. Water it quite often, perhaps up to 2 or 3 times a week to keep the soil damp.\nUse any good quality all-purpose fertilizer once a month. The flowers come in different colors, including red, orange, yellow, white and lavender.\nThis indoor plant can add splashes of color to brighten up a dull spot in your house or office. Avoid placing this it in very dry or low humidity areas.\n- Increase ventilation in your house to allow fresh air in. This will help remove indoor pollutants.\n- Install a HEPA (high-efficiency particulate air) filter in your house and office.\n- If using a gas stove, keep the kitchen window open while cooking or turn on the fan hood to reduce nitrogen dioxide buildup.\n- Avoid house cleaning products that have chlorine, ammonia and volatile organic compounds (VOCs).\n- Bathe your pet regularly. To reduce allergens in your house, you can brush your pet outside.\n- Use air conditioning to keep humidity down and reduce dust mite allergens.\n- Wash your bedding once a week in hot water and, if possible, dry it in direct sunlight.\n- Reduce dust by dusting often with a damp cloth or dust mop.\n- Use a vacuum with a HEPA filter.\n- Make a strict rule and say ‘no’ to smoking in your home.\n- Avoid the use of chemical air fresheners as they contain toxic chemicals. Instead, you can easily make your own natural air freshener at home with baking soda and essential oils. You can find the recipe in this post.', 'One silent but potentially deadly threat to overall wellness is indoor air quality. Since people sleep, eat and work mostly indoors, it’s important to limit the presence of harmful chemicals in these environments, effectively limiting the health issues they can cause\nOne group of these chemicals, known as volatile organic compounds, or VOCs, are abundant in most indoor areas largely because of their presence in consumer products, as well as in building materials and other things produced by mass industry.\nVOCs are unique in that they vaporize at room temperature. This is why the bathroom smells a certain way after it’s been cleaned, or it’s always easy to tell when a room has been freshly painted.\nThe Impacts of VOCs\nOverexposure can be felt as lightheadedness, dizziness, a headache, loss of vision or coordination, or even memory impairment. But long-term effects can be liver, kidney and central nervous system damage, and maybe even cancer.\nAnd this without mentioning their negative environmental effect. Limiting the presence of these chemicals a matter of not only environmental protection but also public health. These chemicals can pollute marine environments, and many are greenhouse gases, contributing to global temperature rise.\nThe best way to limit exposure is to remove the presence of these compounds from the home by using natural products, or by making sure areas where concentrations may be high are well-ventilated. But even when all measures have been taken, it’s nearly impossible to completely eliminate these airborne chemicals. Yet actively removing VOCs from the home is possible, and doing so requires a pollutant abatement strategy assisted by some unexpected friends.\nThe VOC-absorbing Plants\nSome plants happen to be really good at absorbing harmful toxins from the air. It’s been known for a long time that plants turn carbon dioxide into oxygen, therefore providing clean air for other life forms to breathe, but this ability to painlessly store harmful chemicals is something just now being understood.\nThis is not to say that putting these plants will remove all VOCs from the home. It’s still important to take all safety precautions, and it’s especially important to install a smart indoor air quality monitor to measure concentrations and prevent them from reaching levels that can cause health problems.\nBut these plants can still do wonders in helping improve indoor air quality. Most of them can be grown at any temperature and require little to no maintenance.\nHere are some of the most effective toxin-fighting plants:\nThe Areca Palm\nAlso known as the “butterfly palm,” this plant has been found to remove more xylene and toluene from the air than any other plant. It’s a relatively low-maintenance plant requiring little water and minimal sunlight, although it does enjoy a humid environment. It’s decorative, but it’s also effective at removing harmful toxins from the air.\nNamed for its resemblance to the famous bamboo three, the bamboo palm is known for being particularly effective at removing formaldehyde one of the most common VOCs out there, formaldehyde, from the air. It’s also been shown to be effective in reducing benzene concentrations. These plants also give off water vapor, too, giving the room humidity and making it easier to regulate temperature in an energy efficient manner.\nA great plant to grow at any temperature, the rubber plant has also been found to be effective in removing formaldehyde from the air, making it an easy-to-maintain pollutant removal option.\nDracaena “Janet Craig”\nThis particular plant has been rated for having very high removal efficiency. It’s known as one of the best plants for cleaning trichloroethylene (TCE) from the air. But it’s also been known to clean around 78 percent of the benzene in the air within 24 hours.\nThis is another good option when looking at plants to help purify and cleanse the air. Peace lily is known to remove organic toxins such as alcohols, acetone, benzene, TCE and formaldehyde. And because it also gives off water vapor, it helps keep air full of moisture and easy to breathe.\nThere are many other plants that can help to reduce VOC concentrations in the home and improve air quality. But these are some of the most effective and easy to maintain. Putting them in the home can help dramatically improve air quality and overall health.\nFor more information on VOCs, here are some additional resources:\nList of Common Volatile Organic Compounds. Aromatic hydrocarbons are everywhere. Know which VOCs are most common and where they come from to keep concentrations low.\nOrganic Chemicals: VOCs in Paint. Paint is one of the most common sources of toxic chemicals. Learn more about how to avoid exposure.\nVolatile Organic Compounds in Cigarette Smoke. As one of the biggest dangers to health, it should be no surprise that cigarette smoke has high concentrations of harmful chemicals.']	['<urn:uuid:d72c6e2b-3405-4d8a-affe-a6f3368a93ea>', '<urn:uuid:d00e608e-1811-42c8-b5dd-8f12f959fc34>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:35:02.547110	9	69	2344
14	what is hard determinism definition	Hard determinism is the view that every event that happens in the present is the necessary result of events that occurred in the past, meaning everything is governed by cause and effect.	['‘So, a lot of us figure that our thoughts and actions are free. But, most of us also believe that every effect has a cause, and that everything that happens now, in the present, is the necessary result of events that occurred in the past. This view is known as hard determinism. And [many people would argue that both can be true]; that many of your actions are free, and that the world is governed by cause and effect.\nBut, it turns out, you can’t rationally hold both views. Because, traditionally, libertarians have defined free actions according to what’s known as the Principle of Alternate Possibilities. That might sound like the plot device for a sci-fi show, but this principle says that an action is free only if the agent – that is, the person doing the thing – could have done otherwise.\nSo, truly free actions require options. Determinism, by contrast, doesn’t allow options. It holds that every event is caused by a previous event. Which means that an agent can never have done anything other than what they did, and therefore, they are never free.’\n– Green. H. (2016, August 15) Determinism vs Free Will: Crash Course Philosophy #24\nWhen learning foreign language vocabulary, repeated practice is essential for success; as words get established in the long-term memory, learners can move on and focus on new skills.\nTwo effects are at play in this process: the spacing effect, the finding that short practices spaced out over time is better for learning than cramming; and its related finding, known as the lag effect, which states that learners improve if they gradually increase the spacing between practices.\nThese ideas go back to 1885, when German psychologist Hermann Ebbinghaus pioneered the concept of the forgetting curve. He tested his ability to remember a string of words over different periods of time and found a consistent pattern to the decline of his ability to recall these words over time. Immediately after the learning experience, his recall was 100 percent, but memory dropped steeply the first few days. Further, he found that the memory loss was exponential, meaning it increased by the square of the previous number until finally flattening out at around 30 days post-learning.\nAccording to Ebbinghaus’ findings, the way to counter the forgetting curve (i.e. learners are more successful) when they plan short practice sessions and gradually increase the amount of time between each session.\n ‘Repeating list items leads to better recall when the repetitions are separated by several unique item than when they are presented successively; the spacing effect refers to improved recall for spaced versus successive repetition (lag > 0 vs. lag = 0); the lag effect refers to improved recall for long lags versus short lags.’\n– Kahana. M.J., Howard. M.W. (2005) Spacing and lag effects in free recall of pure lists Psychonomic Bulletin & Review, 12 (1), p. 159-164\n‘A lot of fallacious forms of argument cluster around the use of “authorities”. It is often necessary in argument to make use of some kind of authority – if only because we want to refer to facts and findings. But authorities can also be used as a way to bully opponents by suggesting that in failing to agree with some venerated source they must themselves be weak-minded, ignorant or wildly and dangerously at odds with common standards.’\n– “Can you spot a rhetorical fallacy?” The Guardian, 13 September 2013\nPeople attach greater value to things they have made themselves than if the very same product was produced by someone else. This cognitive bias is known as the IKEA effect.\nCreating something yourself boosts people’s feelings of pride and competence, and has the added effect of showing off their competence to others. This can lead to an overly subjective view of a project.\nChandler: [Rips his jacket on the large wooden tv unit.] Wow! That ripped! That ripped real nice!\nJoey: How many times I have to tell you! You, turn and slide! You know, turn and slide.\nChandler: Oh you don’t turn and slide, you throw it out! I’m tired of having to get a tetanus shot every time I get dressed!\nJoey: Well, we’re not throwing it out! I built this thing with my own hands! – Friends (1997) Season 4, Episode 2; “The One with the Cat” [No. 75]']	['<urn:uuid:e747fed3-866a-4d3b-8209-bda9e10cdfd2>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	5	32	724
15	Which drone can carry more weight, Boxer or M300?	The Avartek Boxer can carry a real payload of 5 kg with its 4 kW generator system at a maximum take-off weight of 25kg, while the M300 RTK has a lower maximum payload capacity of 2.7 kg and can mount up to 3 payloads simultaneously.	"['Avartek Boxer – Advanced Hybrid Power Technology\nAvartek Boxer Hybrid is a gasoline powered unmanned aerial system (UAS, UAV, RPAS or drone). The main feature of Avartek Boxer is the hybrid system that provides all the flight energy by transforming gasoline into electric thru its onboard power generator for much longer flight times while maintaining the heavy payloads of large LiPo powered drones.\nInstead of carrying heavy batteries the Avartek Boxer Hybrid is designed around a liquid cooled mini generator power plant that transforms gasoline into electrical power. Currently the Boxer generator is capable of delivering 4 kW of energy which translates to 5 kg of real payload and 2 hour flight times. This quadruples the flight mission lengths and provides easy on field operations due to the lack of battery management and charging problems.\nThe Boxer design is fully custom. The airframe was designed by Avartek in Solidworks and all parts are manufactured in Finland. Only motors, escs, the flight controller and other electronics are out sourced.\nThe whole copter is designed around the hybrid generator’s capabilities of producing 10x more energy than similar weight in LiPo batteries. 4 kg of gasoline provides 2 hours of flight time at MTOW 25kg, while a typical LiPo battery in this size of a drone of 11kg would only provide 30 minutes. 4 kg of gasoline (5 litres) equals 40 kg of LiPo batteries. On field operations greatly benefit from the very quick duty cycle of hybrid drone by offering easy refills instead of on battery management and recharging arrangements.\nAvartek Boxer’s Pixhawk flight controller provides an easy to use system with unparalleled options for customisation both from flight controller, sensor and add on hardware as well from the software perspective. Pixhawk 2 is 100% open system and compliant with latest technologies.\nTypical Payload Options\nAvartek offers ready made payload setups. Safe, quick mounting and fully suspensed – ready to mount the sensor, IMU and auxilaries, such as onboard PC and telemetry/cloud datalinks. In Boxer typically auxilaries are mounted inside the airframe while the IMU is securely attached to the sensor.\nQuicklocks provide quick mounting points to the hardware rail underneat the airframe. This provides quick access to the payload and allows for quick payload swaps in the field. The long payload rails and heavy lift capability makes the Boxer an ultimate multi sensor UAV. Boxer features a large deck that can accommodate payload auxiliary sensors such as hyper spectral camera irradiance sensors. Payload GPS antenna can be attached to the antenna tower. Cabling can be pulled thru the body with customisable deck covers.\nOptional retracting landing gear provides unrestricted field of view for most payloads and the front mounted FPV camera safety and video recordings of flown missions.\nFlight Controller: Cube Orange\nRemote Controller: Futaba 14ch\nESCs: T-Motor Flame 60A\nMotors: Avartek Boxer\nPower System: Avartek\nBattery: Lipo 12S 5Ah\nHybrid Power Management Unit: 4 kW\nFuel tank size: 5 litre\nFlight duration: @25kg TOW approx. 120 minutes\nFrame: Carbon fiber designed and manufactured by Avartek', ""- 15 km Max Transmission1\n- 55-min Max Flight Time2\n- 6 Directional Sensing & Positioning\n- Primary Flight Display\n- IP45 Rating\n- -20°C to 50°C Operating Temperature\n- Hot-swappable Battery\n- UAV Health Management System\nThe all-new OcuSync Enterprise enables transmission up to 15 km away and supports triple-channel3 1080p video. Real-time auto-switching between 2.4 GHz and 5.8 GHz4 enables more reliable flight near high-interference environments, while AES-256 encryption offers secure data transmission.\nThe refined airframe and propulsion system design gives you a more efficient and stable flight, even in harsh conditions.\nMax Flight Time\nMax Descend Speed5\nConfigure your M300 RTK to fit your mission needs. Mount up to 3 payloads simultaneously, with a maximum payload capacity of 2.7 kg.\nRecord mission actions such as aircraft movement, gimbal orientation, photo shooting, and zoom level to create sample mission files for future automated inspections.\nAutomate routine inspections and capture consistent results every time. Onboard AI recognizes the subject of interest and identifies it in subsequent automated missions to ensure consistent framing.\nCreate up to 65,535 waypoints and set multiple actions for one or more payloads, including 3rd party ones, at each waypoint. Flightpath planning is also optimized to maximize flexibility and efficiency for your missions.\nMark an object in camera or map view with a quick tap, and advanced sensor fusion algorithms will immediately calculate its coordinates, which are projected to all camera views as an AR icon. The location of the subject is automatically shared with another remote controller, or to online platforms such as DJI FlightHub9.\nIdentify and follow moving subjects like people, vehicles, and boats with the Smart Track function, where auto-zoom is applied for steady tracking and viewing. The subject’s dynamic location is continuously acquired and shared to another remote controller or to DJI FlightHub9.\nThe M300 RTK adopts a new Primary Flight Display (PFD) that integrates flight, navigation, and obstacle information to empower the pilot with exceptional situational awareness.\nEither operator can now obtain control of the aircraft or payload with a single tap. This creates new possibilities for mission strategies as well as higher flexibility during operations.\nTo enhance in-flight safety and aircraft stability, dual-vision and ToF sensors appear on all six sides of the aircraft, offering a maximum detection range of up to 40 m, with options to customize the aircraft’s sensing behavior via the DJI Pilot App. Even in complex operating environments, this 6 Directional Sensing and Positioning system helps keep the aircraft and the mission safe.\nThe new integrated Health Management System displays the current status of all systems, notification logs, and a preliminary troubleshooting guide. Also in the system are the aircraft’s flight logs, duration, and mileage throughout its entire lifecycle, and tips on aircraft care and maintenance.\nThe M300 RTK’s built-in advanced redundancy systems help keep your critical missions going even in unexpected scenarios.\n-20°C to 50°C\nAirSense ADS-B Receiver\nThe Zenmuse H20N integrates starlight sensors into its zoom and wide-angle cameras\nEstimate your M300 RTK’s flight time based on the payload configuration.\nIntegrate a variety of 3rd party payloads like gas detectors, loudspeakers, multispectral sensors, and more. Payload SDK supports DJI SkyPort, DJI SkyPort V2, and DJI X-Port. These greatly reduce the payload development lifecycle and maximize the potential of your payloads in more diverse scenarios.\nWith a large network of 3rd party mobile applications, you can unlock the capabilities of your drone platform to meet specialized mission needs. Utilizing Mobile SDK, the M300 RTK supports highly customizable mobile app development.\nWith Pilot 2's built-in MQTT based protocols in DJI Cloud API, you can directly connect the Matrice 300 RTK to Third-Party cloud platforms without having to develop an App. Access the drone's hardware, video live-stream, and photo data.\nQuickly assess a situation and plan accordingly while improving officer and bystander safety.\nConduct inspections of pipelines, well sites, and more while keeping workers away from risky areas.\nDigitize surveying workflows and improve efficiency in data collection and analysis.\nMatrice 300 RTK is available for order through official DJI Dealers. Contact us below for the team to reach out.\n1.Unobstructed, free of interference, when FCC compliant. Maximum flight range specification is a proxy for radio link strength and resilience. Always fly your drone within visual line of sight unless otherwise permitted.\n2. Actual flight time may vary because of the environment and payload configurations.\n3. Each RC supports two streams. Triple-stream channeling is only supported with dual RC.\n4. Due to local policies, some countries do not support 5.8 GHz transmission.\n5. Achieved in Forward Flight using S Mode.\n6. The service ceiling of 7000 m is achievable with high altitude propellers.\n7. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n8. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n9. Support for location sharing via DJI FlightHub is coming soon.\n10. D-RTK 2 Mobile Station for Matrice 200 Series V2 and Phantom 4 RTK can be upgraded to support Matrice 300 RTK.\n11. The CSM Radar will be available soon.""]"	['<urn:uuid:a83f4190-c299-4ac8-b1cb-f4b5899ebf4a>', '<urn:uuid:469779b3-9781-4cd5-be93-41108b4c32f7>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:35:02.547110	9	45	1355
16	differences between bioluminescence fluorescence jellyfish camouflage mechanisms deep ocean	Bioluminescence differs from fluorescence in that it uses enzymatic reactions for energy rather than light sources, offering greater sensitivity and wider dynamic range. In terms of camouflage, deep ocean jellyfish produce red pigments to become virtually invisible since red light is absent in deep waters. They also have red-pigmented digestive linings to hide any bioluminescent prey from predators.	"['Cnidaria? Ok, so maybe I should’ve called this post “Jellies” but I figured I’d be formal and give these majestic creatures the respect they deserve. Most of the common jelly fish you know and love belong to the phylum Cnidaria. Jelly fish are amazing organisms with a seemingly simple life of floating through the ocean being dragged through currents. My job is to show you how complex these “brainless” creatures truly are. So let’s do this!\nJelly fish or Cnidaria are comprised of three cell layers: the epidermis, mesoglea, and gastrodermis. The gastrodermis is a highly permeable membrane which takes in nutrients and oxygen while waste is eliminated. This layer is comprised of muscular nutritive cells and glandular cells. While they absorb nutrients taken in through the mouth, they also receive and release reproductive cells. Located between the epidermis and gastrodermis, the mesoglea is a jelly-like layer which comprises the largest volume of the jelly fish. While this gelatinous layer is mostly water and structural proteins, it does contain muscle and nerve cells which help form a type of internal “skeletal structure” for cnidaria. Along the outer membrane of the jelly fish, is the epidermis, a sensory, contractile layer which allows for movement and sensation. These sensory or nerve cells form the “nerve net” which is thought to be the most primitive nervous system in a multicellular organism.\nThis “nerve net” is comprised of multiple types of nerve cells which are responsible for balance in space (in other words what direction is up or down) called statocysts, light receptors known as ocelli, and many other nerve cells which can help distinguish the presence of nearby food sources, water composition, and animals in the near vicinity. While we think jellies simply float around, aimlessly spinning through the water in random directions, their simple yet complex nerve net allows them to swiftly navigate through the waters and feed on prey. Some jellies have complex eyes, complete with corneas, retinas, and lenses within a structure known as rhopalia however marine biologists still do not know how they interpret this complex visualized information given their primitive and simple nervous “system.”\nThe jelly fish tend to live in two major forms: polyps which are bottom dwelling and medusae, which are the jellies you know as the undulating bells with dangling tentacles. As a medusa, jellies use nematocysts which are basically microscopic tethered harpoons kept under high pressure distributed along each tentacle. When in contact with another organism, the nematocysts rupture like a jar-lid blowing open and the harpoon launches outward piercing the organism and releasing toxins which can sometimes be fatal to humans. Many species of cnidarians hitchhike on the backs animals as their nematocysts tether the jelly to the animal with which it came in contact.\nAnother defense mechanism used by jellies is that of camouflage and bioluminescence. While the reason for this is still unknown, one theory exists that jellies use bioluminescence as a means to attract larger animals so that their predators become prey themselves. Another theory suggests that when a predator comes into contact with a bioluminescent jelly, the flash of bioluminescence may frighten the predator.\nAnother defense mechanism is that of light perception and camouflage. Many jellies are clear to remain invisible in the dark ocean depths. However, those jellies that live in the deep ocean where light is scarce, tend to produce red pigments which make them virtually invisible to other organisms. Red light is absent in the deep ocean therefore many jellies will produce red pigment to ensure they do not refract any visible light. Also, they will have deeply red-pigmented digestive linings to hide any bioluminescent prey from potential jelly fish predators.\nAs you can see, jelly fish or cnidarians, are simple yet complex creatures with an evolutionary advantage over more complex multicellular organisms. Their graceful behavior yet toxic abilities make them interesting animals who have survived millions of years of evolution. Next time you are at the aquarium take note of their anatomy and why they look the way they do.\nAs always, enjoy!', ""Introduction to Bioluminescence\nLight emission has been used to detect experimental changes in biological assays for almost 100 years (1), and while the list of applications that use emitted light as a detection strategy is long, the type of light emitted varies. Luminescence is light emission as a result of a chemical reaction without the production of heat or any thermal changes. This type of light is clearly different from incandescence, which generates heat and is why incandescent light bulbs get hot during use. Luminescence can be separated into two types:\n- Bioluminescence – light emitted from a biological source\n- Chemiluminescence – light emitted from a non-biological source due to a chemical reaction\nBioluminescence vs. Fluorescence\nFluorescence is another type of emitted light commonly used in biological research and is the product of a fluorophore, a molecule that absorbs the energy from a light source and then emits light at a different wavelength. Bioluminescence, on the other hand, differs from fluorescence in that the excitation energy is supplied by an enzymatic reaction rather than from a source of light. While both bioluminescence and fluorescence are widely used in scientific applications, bioluminescent reporters display an ultrasensitive detection capacity and have a wider dynamic range compared to fluorescent reporters because of the enzymatic nature of the bioluminescent reporter.\nFluorescent reporters are susceptible to photobleaching, provide low quantum yields and have greater protein stability in cell-based assays compared to bioluminescent reporters, which make them less amenable for use as real-time reporters. Cellular components also have autofluorescent properties, which increases the non-specific background and decreases the sensitivity of fluorescent detection in cell-based assays. Conversely, cellular components have no inherent bioluminescence, allowing for greater sensitivity with bioluminescent assays.\nHowever, fluorescent reporters are more useful for visualizing targets in live cells than bioluminescent proteins, because fluorophores do not require cofactors or exogenous substrates for activity and are more stable than bioluminescent reporters.\nBioluminescence in Nature\nUses of bioluminescence in nature are quite diverse and are exhibited by both unicellular organisms like bacteria and dinoflagellates and higher-order organisms such as fish and insects. Bioluminescence is used by both terrestrial and aquatic organisms, although it is more commonly seen in marine animals, especially those living in the extreme depths of the ocean. Bioluminescence serves many natural purposes for these organisms.\nDefense: A strong bioluminescent signal can be used by prey to temporally blind an attacking predator, giving the prey a chance to escape. The light produced by the prey also puts the potential predator at risk of being detected by other predators.\nCamouflage: Animals that normally dwell in oceanic dark zones, including firefly squids, krill, dogfish sharks and hatchet- and lanternfish, come to the surface to feed at night. These animals use counter-illumination to blend into surrounding areas illuminated by moonlight. Similar camouflage is also used by certain species of shallow-water squid that give off light to blend in with the moonlight.\nFeeding: Predators often use bioluminescence to attract prey. The anglerfish, for example, lures prey using an elongated dorsal spine that supports a light-producing organ that houses bioluminescent bacteria. Also, some octopi, including Stauroteuthis syrtensis, have suckers that are lined with organs that produce blue-green light that are also used to attract prey.\nMating: Terrestrial animals such as fireflies and glow worms and marine animals like the octopod Japetella diaphana use bioluminescence to attract a mate.\nTwo agents are essential for the bioluminescent reaction to occur: luciferase, the enzyme that catalyzes the reaction, and the luciferase substrate. A wide variety of luciferase enzymes have been discovered, although the general mechanism of the light-producing reaction for all of the enzymes is the oxidative decarboxylation of the luciferase substrate in the presence of O2 to yield photon emission (light).\nLuciferases likely evolved at multiple stages during the course of history to yield enzymes that differ in expression pattern, substrate specificity, cofactor requirement and enzyme kinetics. The result of these evolutionary changes is that some organisms, such as bacteria and fungi, can emit light continuously, while other organisms emit bioluminescent flashes with varying duration and intensity. For example, the luminescence from a dinoflagellete is short, lasting 0.1 seconds, while that from jellyfish can last tens of seconds.\nThe variation in bioluminescent output by different luciferase enzymes has allowed scientists to classify them based on their output kinetics to accommodate different experimental designs. Luciferase enzymes with flash kinetics have maximum sensitivity because of high signal intensity, although the emitted light also rapidly decays. Conversely, enzymes with glow kinetics are less sensitive but stably emit light for at least 60 minutes.\nLuciferase enzymes are active in their native state and lose activity upon unfolding or denaturation, a property that has been utilized to study factors that influence proper protein folding. Also, luciferases such as Gaussia, Metridia and Cypridina are naturally secreted and therefore have been used to study the mechanisms and regulation of the secretory pathway.\n|Photinus pyralis||North American firefly luciferase||61||D-luciferin||Mg, ATP||No|\n|Luciola cruciata||Japanese firefly (Genji-botaru ) luciferase||64||D-luciferin||Mg, ATP||No|\n|Luciola italica||Italian firefly Luciferase||64||D-luciferin||Mg, ATP||No|\n|Luciola lateralis||Japanese firefly (Heike) luciferase||64||D-luciferin||Mg, ATP||No|\n|Luciola mingrelica||East European firefly luciferase||64||D-luciferin||Mg, ATP||No|\n|Photuris pennsylvanica||Pennsylvania firefly luciferase||64||D-luciferin||Mg, ATP||No|\n|Pyrophorus plagiophthalamus||Click beetle luciferase||64||D-luciferin||Mg, ATP||No|\n|Phrixothrix hirtus||Railroad worm luciferase||64||D-luciferin||Mg, ATP||No|\n|Renilla reniformis||Renilla luciferase||36||Coelenterazine||N/A||No|\n|Rluc8 (mutant of Renilla luciferase)||36||Coelenterazine||N/A||No|\n|Green Renilla luciferase||36||Coelenterazine||N/A||No|\n|Gaussia princeps||Gaussia luciferase||20||Coelenterazine||N/A||Yes|\n|Cypridina noctiluca||Cypridina luciferase||62||Vargulin/Cypridina luciferin||N/A||Yes|\n|Cypridina hilgendorfii||Cypridina (Vargula) luciferase||62||Vargulin/Cypridina luciferin||N/A||Yes|\n|Metridia longa||Metridia luciferase||23.8||Coelenterazine||N/A||Yes|\nBioluminescent Experimental Applications\nLuciferase has gained tremendous popularity in the study biological systems, because the bioluminescence is sensitive and the reagents are easy to use. Examples of areas where bioluminescence is routinely used include:\n- In vivo imaging\n- Cell proliferation assays\n- Protein folding/secretion analyses\n- Reporter gene assays\nIn vivo Imaging\nIn vivo imaging is a relatively new, non-invasive method that is gaining fast acceptance in biomedical research because of the ability to track molecular events in live animals such as mice and rats. In animal disease models, cells, pathogens, proteins or other molecules are labeled with bioluminescent luciferases and visualized after the localized or systemic addition of substrate. Low-level light is able to pass through tissue, and the level of penetration is dependent upon the wavelength of the emitted light. Therefore, although bioluminescence emitted from deep within an experimental animal can be detected with a sensitive detection system, luciferases that emit light at wavelengths >600 nm have the greatest sensitivity for in vivo imaging applications (2).\nCell Viability Assays\nBioluminescent assays have also been developed to monitor cell viability and proliferation. Bioluminescent cell viability assays quantitate the free ATP present in metabolically active cells using ATP-dependent luciferases. As a cell population proliferates, the amount of available ATP increases, resulting in a concomitant increase in the bioluminescent signal of the population. Commonly used non-bioluminescent methods to measure cell proliferation include:\n- Quantitation of DNA synthesis by measuring tritiated thymidine (3H-thymidine) or bromodeoxyuridine (BrdU) uptake\n- Identification of dead cells using propidium iodide, a membrane-impermeable DNA-intercalating dye\n- Quantitation of the reduction of the intracellular environment by tetrazolium salt reduction (MTT)\n- AlamarBlue reduction and quantitation of intracellular ATP concentration.\nReporter Gene Assays\nReporter gene assays are used to investigate the genetic regulatory elements that control the expression of genes of interest. A gene consists of multiple functional parts, including the coding region that specifies the protein to be made and regulatory elements that control the transcription of the coding region.\nThe reporter gene assay centers around fusing the putative regulatory elements to a reporter gene and monitoring the amount of the reporter protein expressed. Because reporter expression is under the control of the fused genetic elements, reporter expression is directly correlated with the activity of the regulatory elements. These elements could be promoters, enhancers or 5' or 3' untranslated regions (UTRs) that control either the transcription or translation events in the cell.\nThe characteristic of good a reporter protein is that it should be easy to assay or detect and is not present normally in the test system. Selection of the reporter system also depends on the experimental system, sensitivity needs and the available detection strategy, be it absorbance, fluorescence or bioluminescence. The proteins that have been traditionally used as reporters include β-galactosidase (lacZ), chloramphenyl acetyltransferase (CAT), β-glucuronidase (GUS) and fluorescent proteins (green, yellow or red fluorescent protein [GFP, YFP or RFP, respectively) and secretory alkaline phosphatase (SEAP).\nLuciferase Reporter Assays\nLuciferase-based reporter gene assays are widely used because of their ultrasensitive detection capacity and wide dynamic range. These assays involve placing a genetic regulatory element upstream of a luciferase gene and then transferring the resulting reporter construct into animal cells, plant cells or bacteria through transfection, transformation or injection. Expression of the luciferase reporter gene is then measured to quantify the activity of the regulatory element (cis-acting) or proteins (trans-acting) in the biological pathway affected by the target element.\nSingle- vs. Dual-Spectral Assays\nLuciferase enzymes can be used as single reporters to study one biological event in a given experiment, but because of their different spectral properties and/or substrates, multiple luciferase enzymes can be combined for multiplex luciferase experiments. With these types of experiments, called dual-spectral luciferase reporter assays, two luciferase enzymes are simultaneously expressed and detected in a single sample. Dual-spectral luciferase reporter assays are ideal for:\n- Analyzing more than one target per screen\n- Minimizing off-target effects\n- Identifying crosstalk between two or more pathways\n- Normalizing data to eliminate experimental artifacts\nFor data normalization experiments, one reporter generally functions as an experimental reporter and the second as an internal control to account for non-specific experimental variations due to operator error or as a result of non-specific effects of biological manipulations or cell treatments. Additional experimental variations include:\n- Differences in cell plating\n- Differences in cell viability or proliferation due to an experimental treatment\n- Edge effects caused by uneven environmental conditions in the cell culture incubator\n- Non-specific effects of the treatment compound on the function of the reporter itself\nDual-spectral luciferase reporter assays using luciferase enzymes with non-overlapping luminescence spectra can be measured simultaneously by filter-based detection in a single sample using a single reagent. Dual reporter assays like this that use either the same or different substrates are commercially available.\nWhen using luciferases with overlapping spectra, the luciferases can be measured sequentially in two steps using two reagents if they have different substrate specificities. In the two-step method, the luminescence from the first luciferase is measured, and the enzyme is then inactivated. A second reagent containing the substrate for the second luciferase is then added and the activity measured. Theoretically, more than two enzymes can be multiplexed in this way as long as they have different substrate specificities.\n|Luciferase Combinations||Multiplex Detection Strategies|\n|Renilla/Red Firefly||Coelenterazine/Luciferin||460 nm BP/615 nm LP|\n|Green Renilla/Red Firefly||Coelenterazine/Luciferin||525 nm BP/640 nm LP|\n|Gaussia/Renilla||Difficult but possible with coelenterazine analogs||N/A|\n|Gaussia/Green Renilla||Difficult but possible with coelenterazine analogs||N/A|\n|Gaussia/Red Firefly||Coelenterazine/Luciferin||470 nm BP/640 nm LP|\n|Cypridina/Red Firefly||Vargulin/Luciferin||470 nm BP/640 nm LP|\n|BP: band pass; LP: long pass|\nTime-Course Assays Using Dual-spectral Luciferase Assays\nDual-spectral luciferase assays are ideal for performing time-course assays using a secreted luciferase to measure the experimental promoter activity and an intracellular luciferase under the control of a housekeeping gene promoter to measure cell viability. As shown in the video animation below, experimental promoter activity results in luciferase secretion into the culture medium, which is collected at various time points throughout the experiment. The cells are then lysed at the completion of the time-course, and the lysate is measured for intracellular luciferase activity as the control for cell viability.\nNot all commercially available dual-spectral luciferase assays are compatible with time-course assays because of the requirement for a secreted luciferase reporter. For example, Renilla luciferase is not secreted, and thus the Pierce Renilla-Firefly Luciferase Dual Reporter Assay Kit will not work for this type of time-course assay. However, Cypridina and Gaussia luciferases are secreted; therefore, Pierce Cypridina-Firefly and Gaussia-Firefly dual-spectral luciferase kits are compatible with time-course assays.\n- Keilin D. (1966) The history of cell respiration and cytochrome. Cambridge,: Cambridge U.P. xx, 416.\n- Negrin R. S. and Contag C. H. (2006) In vivo imaging using bioluminescence: A tool for probing graft-versus-host disease. Nat Rev Immunol. 6, 484-90.\n- Smith K. C. (1989) The science of photobiology. New York: Plenum Press. viii, 426.\n- Campbell A. K. and Herring P. J. (1990) Imidazolopyrazine bioluminescence in copepods and other marine organisms. Marine Biology. 104, 219-25.\n- Robison B. H. and Young R. E. (1981) Bioluminescence in pelagic octopods. Pacific Science. 35, 39-44.\nFor Research Use Only. Not for use in diagnostic procedures.""]"	['<urn:uuid:59cc78dc-bccf-4370-b13f-1e5d0ae700d2>', '<urn:uuid:3acff138-0570-4dda-8c24-f224698c30b9>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:35:02.547110	9	58	2756
17	how did prison hunger strikes compare between uk sweden womens rights activists early 1900s	The documents only show evidence of hunger strikes among women's rights activists in the UK, with no mention of such tactics in Sweden. In the UK, suffragettes like Lilian Lenton and Leonora Cohen undertook hunger strikes while imprisoned, with Cohen's strikes nearly killing her and leading to her release under the Cat and Mouse Act of 1913. Another activist, Kitty Marion, was force-fed nearly 200 times. In contrast, Gulli Petrini's activism in Sweden focused on political and educational channels, with no mention of imprisonment or hunger strikes, suggesting different tactical approaches to the suffrage movement in these countries.	"['- BuyRebel Girls: Their Fight for the Vote by Jill Liddington\nVirago, 402 pp, £14.99, May 2006, ISBN 1 84408 168 0\nIn November 1913, ‘the Headingly two’, a dark-haired woman of about twenty-five and ‘a girlish figure in green cap and sports jacket’, stood trial for attempting to set fire to a football stand in Leeds. Among the evidence produced against them were some postcards, one declaring ‘No Vote, No Sport, No Peace – Fire, Destruction, Devastation’ and another addressed to Asquith: ‘We are burning for “Votes for Women”.’ It wasn’t a joke. In the last months of peacetime, suffragettes belonging to Emmeline Pankhurst’s militant organisation, the Women’s Social and Political Union, committed arson on a scale not seen since the rick-burnings of the Captain Swing riots in the 1830s. In the first seven months of 1914, a hundred buildings were set on fire, including the ancient White Kirk in East Lothian, which was totally destroyed, the refreshment room in Regent’s Park in London and houses in Liverpool and Manchester. There were other ‘outrages’, as the press called them: a railway carriage set ablaze; pillar boxes ‘fired’ with phosphorus packets which burned when exposed to air; telegraph and telephone wires cut; glass smashed in public buildings and shops. ‘Votes for Women’ was scorched in acid across golf greens in Bradford and Birmingham, and Lloyd George’s country house was attacked. In Doncaster, suffragettes tried to blow up an empty house, and a bomb went off under the Coronation Throne in Westminster Abbey; the explosion could be heard in the House of Commons.\nOne of the fire-raisers was Lilian Lenton, the daughter of a joiner from Leicester. She celebrated her 21st birthday by volunteering at her local WSPU office for window-breaking raids. After her first spell in prison, she graduated to arson, vowing to burn two empty buildings a week. She was arrested for setting fire to the orchid house and pavilion at Kew, went on hunger strike in prison, and was force-fed. (Kitty Marion, who set fire to the grandstand at Hurst Park racecourse, was force-fed nearly two hundred times in prison.)\nNot all of those in Jill Liddington’s account of the militant years were rebel ‘girls’ – except metaphorically. Leonora Cohen, for instance, was nearly forty when she hurled an iron bar (filed down from a domestic fire-grate) at a display case in the Jewel Room at the Tower of London. Unlike Lenton, Mrs Cohen was a respectable married woman, the wife of a staunchly Liberal watchmaker in Leeds. Until she became branch secretary in 1911, she had contented herself with bringing up her son, and selling newspapers and making marmalade for the WSPU. When Asquith suddenly announced a man-only suffrage bill in November of that year, reneging yet again on a commitment to women, she turned to direct action. Her subsequent hunger strikes, and refusal to accept fluids, in Armley jail in Leeds nearly killed her. After her husband bombarded the Home Office with furious letters condemning the barbarity of her treatment, she was released under the notorious Cat and Mouse Act, passed in the summer of 1913. Hunger-strikers could now be sent home to recover and clawed back into prison once they were well enough to continue their sentences. Eventually, the Cohens were persuaded to retreat to Harrogate, where Leonora set up a guesthouse for WSPU sympathisers and offered ‘reform diets’ of vegetarian meals and salads. She sheltered Lilian Lenton when Lenton violated her parole and helped her to escape a police cordon dressed in her son’s Norfolk suit and cap. Lenton was one of many suffragette fugitives who remained in hiding until the war brought them amnesty.\nLike their feminist forebears, suffrage historians have tended to take sides, for or against militancy. Jill Liddington’s influential first book, One Hand Tied behind Us, written with Jill Norris, and published in 1978, offered a corrective to ‘virtually all the books on the subject’ which told the suffrage story ‘in terms of the middle-class, London-based leaders’. Liddington and Norris uncovered a new group of women whom they called ‘radical suffragists’ and who lived and worked in the North of England, the cradle of the movement. Like the remarkable Selina Cooper, a Lancashire mill-girl from ‘Red’ Nelson, whose biography Liddington has also written, these women canvassed for the vote at the factory gate and on doorsteps, especially in the cotton towns of Lancashire. Nurtured by the Labour movement and active as trade unionists, the textile and factory workers were the only group of women with serious political clout. Some of them, like Cooper, became paid organisers for their local suffrage societies under the aegis of Millicent Fawcett’s non-militant National Union of Women’s Suffrage Societies; but all continued to work through existing organisations, refusing to isolate themselves from other political campaigns. They saw universal female suffrage as one element in a full programme of measures to improve women’s lives – equal pay, birth control and child allowances. One Hand Tied behind Us was ‘people’s history’ of the most effective and original kind. In retrieving the lives of those who had disappeared from the historical record, it transformed our idea of the suffrage movement and made the histories that had concentrated on Westminster look parochial.\nYou are not logged in\n- If you have already registered please login here\n- If you are using the site for the first time please register here\n- If you would like access to the entire online archive, buy a full-access subscription here\n- Institutions or university library users please login here\n- Learn more about our institutional subscriptions here', ""Gulli Petrini was one of the most active women of the libertarian liberal left-wing movement at the turn of the 20th century, particularly within the women’s suffrage movement.\nGulli Petrini was born in 1867. She grew up in Stockholm in a household replete with educational traditions. Her extended family and relatives played an important role in her future social activism. Her father, Jacob Rossander, was a professor at Karolinska Institutet and her mother, Emma Maria Godenius, who died when Gulli Petrini was only seven years old, was the daughter of the owner of Gustafsberg porcelain factory, which meant that the family had substantial financial and social resources. The family was already familiar with the concept of women’s rights. Two of Gulli Petrini’s paternal aunts, Alida and Jenny Rossander, were pioneers of higher education for girls. They set up the Rossander courses which, over time, became a formalised teacher training course called “Lärokurs för fruntimmer” (training course for women). Well-known feminists such as Anna Whitlock and Ellen Key attended this school. The six Rossander children were initially educated by their two paternal aunts. Gulli Petrini went on to attend the Åhlin school, and then the Wallin school where she gained her school-leaving certificate. This was a right extended to girls in 1870.\nTo both Gulli Petrini and her father it was obvious that she would continue her studies at university. She read mathematics and physics, both very successfully, and in 1900 became one of the earliest Swedish women to defend a thesis and gain her doctorate in physics. Through her family connections she found accommodation at Professor Holmgren’s family home, Villa Åsen in Uppsala, during her studies. This house was a known gathering place for libertarian and radical students and was also a fertile environment for lively and uninhibited young women like Gulli Petrini, who was fearless and interested in all sorts of social issues. She met people who became significant for her career path and who remained lifelong friends. These included radicals such as Hjalmar Öhrvall, Knut Wicksell, Anna Bugge Wicksell, and not least the dominant figure of her host family Ann-Margret Holmgren. The latter was a mother of eight, a writer, a feminist and a women's suffrage activist. Further politicially inclined lodgers included the likes of Lydia Wahlström and Gerda Hellberg, with whom she would come to collaborate in the struggle for women's right to vote.\nThe broadminded environment in the Holmgren family home sowed the seeds of the politically liberal stance which Gulli Petrini would espouse for the rest of her life. She herself said that from Hjalmar Öhrvall she had learnt to “Follow your own conviction, do not give way, call things out for what they are. You will not always be liked for it but you will become used to it.” Considering Gulli Petrini’s actions, beliefs, and political practise in various matters it becomes apparent that she never forgot those words of advice. She consistently dared to disrupt order and to tell the truth. She also applied this advice to her feminist projects and organisations which she helped to set up and run: Uppsala kvinnliga studentförening (Uppsala female students society) in 1897, Landsföreningen för kvinnans politiska rösträtt (LKPR, National Association for Women's Suffrage) in 1903, and Föreningen frisinnade kvinnor (FFK, the association of free-thinking liberal wome) in 1914. She also became an active member of the latter, initially as vice-chair and then as chair from 1920.\nShe also joined the radical association called Verdandi in Uppsala. There she met the man who became her husband, Henrik Petrini, who was a natural scientist, docent in mathematics and physics, a well-known debater, and a firm opponent of the contemporary school catechism. The couple married in a civil ceremony in 1902 and then moved to Växjö, where Henrik Petrini worked as a teacher at the boys’ school, whilst Gulli Petrini worked as a teacher at a girls’ school. Their first years in Växjö were difficult for the liberal couple as there seemed to be an air of conflict around them. What the ecclesiastical elite of the town found most upsetting was the Petrinis’ criticism of religion, their natural scientific viewpoints, and their belief in Darwin’s theory of evolution. The Petrinis criticised the catechism as taught in schools, the dogma of the church, and theocracy. They defended themselves against both the consistory and the bishop. Henrik Petrini was given the unflattering nickname “the bishops’ terror”.\nGulli Petrini was eventually fired from the girls’ school because she refused to deny that humans were related to apes. She was also not considered for the position as a senior teacher at the school despite her doctoral degree and having the highest qualifications. Nevertheless, Gulli Petrini did not confine herself to a laboratory, a different school, or a bourgeois family life because of these obstacles. Instead she embarked on a political career within Frisinnade landsföreningen (Free-minded National Association) and LKPR. She later said that it was this unfair treatment within the workplace that turned her into a politician. During this period she became the chair of the Växjö branch of the LKPR, as well as becoming a public educator and travelling speaker on behalf of suffrage, peace and democracy. She undertook long rabble-rousing travels across Sweden, sometimes under very simple conditions.\nDespite her setbacks in Växjö, in 1909 Gulli Petrini was elected onto the city council there and the following year she was the first Swedish woman to become a member of the “drätselkammare” (financial steering committee of the town). Within the city council she highlighted social issues such as poor relief, school attendance, the situation of unmarried mothers, and the legal position of children. She believed that social problems should be solved collectively at a political level. The little of her correspondence which has survived reveals that her main interest lay in the women’s suffrage movement during the movement’s peak years (1903-1921). However, her letters also reveal glimpses into other spheres and topics of interest, which she was asked to speak about by public educators and suffrage activists. Gulli Petrini was a popular public speaker. She was engaging, educated, humorous, and she dared to show anger as a speaker when dealing with social injustices. These attributes were diametrically opposed to the conventional feminine ideal, and appealed to many for precisely that reason. She was also an expert on various topics concerning suffrage, electoral procedures, especially the method of proportional representation, as well as an indispensable specialist in the movement’s relations with state and parliament and the various parties’ positions on the matters. In addition to scientific and teaching material that Gulli Petrini wrote together with her husband she also published a long series of writings on the parliamentary history of suffrage, municipal laws, ministerial positions, libertarianism, the consequences of war, and more – all of this from a woman’s perspective.\nIn 1914 the Petrinis moved back to Stockholm. Gulli Petrini became employed as a teacher at the Whitlock co-educational school. At the time the suffrage movement had entered a new phase as a result of the outbreak of the First World War. Things also heated up due to the establishment of FFK, and because both the suffrage and democracy movements became intensified from both the workers’ and women’s perspectives. Gulli Petrini was actively involved and her manner of expression, as well as her feminist demands on behalf of women, made a mark on the FFK’s reform programme. She was part of the FFK faction which wanted an independent women’s association. When the Liberal coalition party was dissolved in 1923 as a result of the debate on an alcohol ban, she along with many other libertarian women aligned herself to the Swedish Liberal party. She stood as a candidate for the 1921 parliamentary elections but was unsuccessful due to the split between the libertarian and the liberal party.\nFollowing the introduction of universal suffrage in 1921 Gulli Petrini came to focus on political efforts, in addition to her job as a teacher. She continued her campaigning on behalf of women’s rights in wider society. During the 1930s she participated in the discussion on the right of married women to have a career. The law in favour of this was passed in 1939 in Sweden. Her final years were spent travelling to various countries, often in order to participate as a delegate in large international congresses where she gave notable speeches on regulations, peace matters, citizenship, and marital laws. She remained a warrior in the global fight against oppression in all its forms, and for the individual’s right to freedom, as well as responsibility within political and social life, right to the very end.\nGulli Petrini died in 1941. Her grave is at the Norra cemetery in Solna.""]"	['<urn:uuid:4b4b8a02-f9b1-41bf-b0eb-afa8afe015cd>', '<urn:uuid:fcb3a967-7e02-4439-9239-22206c9fdd07>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T20:35:02.547110	14	98	2387
18	How do data visualization and community engagement approaches compare in promoting change?	Data visualization and community engagement represent complementary approaches to promoting change. Visualization, particularly through tools like matplotlib, helps tell data stories effectively - as shown in examples like population trends and histograms that make complex information accessible and compelling. Meanwhile, community engagement approaches focus on participatory techniques like citizens' assemblies and creative stakeholder decision-making to drive transitions, particularly in areas like renewable energy adoption. Both approaches aim to make complex information actionable, with visualization focusing on clear communication of data insights while community engagement emphasizes collective participation and decision-making.	"[""Weekly study3 hours\nSustainable Energy Access for Communities\nThis course isn't running right now. We can email you when it starts again, or check out these other courses you might like.\nExplore sustainable solutions to address the challenges of energy access\nThere are 789 million people around the world who live without access to electricity. In addition, 2.8 billion people globally have no access to clean and safe cooking fuels and technologies.\nOn this three-week course, you’ll explore the meaning of energy access and how to overcome its challenges to provide safe and clean energy for communities around the world.\nSince the launch of the United Nations 17 Sustainable Development Goals (SDGs), the international community has been working tirelessly to address the energy access challenge. But this cannot be achieved unless we find ways to inspire and support entire communities to transition to renewable energy.\nTo do this, you’ll identify the right technological solutions and the many sides to renewable energy, including technology, behaviour change, economics, and community dynamics.\nExamine the use of renewable energy sources\nThe course will help you address energy efficiency and how communities can save both money and energy with the right solutions.\nExamining different business models, you’ll identify several renewable energy and off-grid community solutions, such as solar PV, wind, hydro, bio-gas, and even bio-electricity.\nYou’ll also analyse a mini-grid case study and look into household energy consumption to understand where savings can be made.\nUnderstand the importance of community engagement\nWhen addressing energy access, it’s vital to consider the ways this can impact the wider community.\nYou’ll examine case studies and learn innovative techniques to promote community engagement and help you make a change in your community.\nFinally, you will take part in a peer review of your plans to help your community move closer to a renewable energy future.\nWeek 1: The meaning of energy access\n- Challenges for renewable energy\n- Defining access to energy\n- Moving to clean cooking\n- Alternative energy futures: an integrated approach\n- On or off grid? A debate\nWeek 2: Renewable energy solutions for communities\n- Energy efficiency: saving money, saving energy\n- Exploring renewable options for communities\n- Analysing household energy consumption\n- Business models for off grid community solutions\n- Mini grid case study and game\nWeek 3: Engaging the community\n- The importance of community engagement in the transition to renewable energy\n- Case studies in community engagement\n- A citizen’s assembly on energy in Lebanon\n- Creative ways to support stakeholder decision making\n- Planning a community engagement activity\nDate to be announced\nLearning on this course\nYou can take this self-guided course and learn at your own pace. On every step of the course you can meet other learners, share your ideas and join in with active discussions in the comments.\nWhat will you achieve?\nBy the end of the course, you‘ll be able to...\n- Describe the complex levels of access to energy and the relationship between access to energy and meeting the SDGs\n- Assess renewable energy solutions for communities\n- Identify a path to energy transition for you and your community\n- Experiment with energy efficiency\n- Discuss business models for renewable energy solutions\n- Explore approaches to engaging the community to move towards renewable energy\n- Produce a plan for community engagement with renewable energy\nWho is the course for?\nThis course is designed for anyone with an interest in renewable energy transitions. You could be an aspiring community activist or leader, a professional working in renewable energy, or a government professional with a role in renewable energy."", ""Intermediate Python for Data Science: Matplotlib\nCourse Excerpt: Intermediate Python for Data Science - Matplotlib\nBasic plots with matplotlib\nHi, my name is Filip, and I'm a data scientist at DataCamp. In this intermediate Python course, you will further enhance your Python skills for data science. You will learn how to visualize data and store data in new data structures. Along the way, you will master control structures, which you will need to customize the flow of your scripts and algorithms. We'll finish this chapter with a case study, where you'll blend together everything you've learned to solve a cool problem.\nThis first chapter is about data visualization, which is a very important part of data analysis. First of all, you will use it continuously to explore your dataset. The better you understand your data, the better you'll be able to extract insights. And once you've found those insights, again, you'll need visualization to be able to share your precious insights with other people. As an example, have a look at this beautfiul plot.\nIt's made by the Swedish professor Hans Rosling. His talks about global development have been viewed millions of times. And what makes them so intriguing, is that by making beautiful plots, he allows the data to tell their own story. Here we see a bubble chart, where each bubble represents a country. The bigger the bubble, the bigger the country's population, so the two biggest bubbles here are China and India.\nThere are 2 axes. The horizontal axis shows the GDP per capita, in US dollars. The vertical axis shows life expectancy. We clearly see that people live longer in countries with a higher GDP per capita. Still, there is a huge difference in life expectancy between countries on the same income level.\nNow why do I tell you all of this? Well, because by the end of this chapter, you'll be able to build this beautiful plot yourself.\nThere are many visualization packages in Python, but the mother of them all, is\nmatplotlib. You will need its subpackage\npyplot. By convention, this subpackage is imported as\nplt, like this.\nFor our first example, let's try to gain some insights in the evolution of the world population. I have a list with years here, year, and a list with corresponding populations, expressed in billions, pop. In the year 1970, for example, 3.7 billion people lived on planet Earth.\nTo plot this data as a line chart, we call plt.plot() and use our two lists as arguments. The first argument corresponds to the horizontal axis, and the second one to the vertical axis. You might think that a plot will pop up right now, but Python's pretty lazy. It will wait for the\nshow() function to actually display the plot. This is because you might want to add some extra ingredients to your plot before actually displaying it, such as titles and label customizations. I'll talk about that some more later on. Just remember this: the\nplot() function tells Python what to plot and how to plot it.\nshow() actually displays the plot.\nWhen we look at our plot, we see that the years are indeed shown on the horizontal axis, and the populations on the vertical axis. There are four data points, and Python draws a line between them. In 1950, the world population was around 2 point 5 billion. In 2010, it was around 7 billion. So the world population has almost tripled in sixty years, that's pretty scary. What if the population keeps on growing like that? Will the world become over populated? You'll find out in the exercises.\nLet me first introduce you to another type of plot: the scatter plot. To create it, we can start from the code from before. This time, though, you change the plot function to scatter. The resulting scatter plot simply plots all the individual data points; Python doesn't connect the dots with a line. For many applications, the scatter plot is often a better choice than the line plot, so remember this scatter function well. You could also say that this is a more -honest- way of plotting your data, because you can clearly see that the plot is based on just four data points.\nNow that we've got the basics of\nmatplotlib covered, it's your turn to build some awesome plots!\n# define a simple function import matplotlib.pyplot as plt year = [1950, 1970, 1990, 2010] pop = [2.519, 3.692, 5.263, 6.972] plt.plot(year, pop) plt.show() import matplotlib.pyplot as plt year = [1950, 1970, 1990, 2010] pop = [2.519, 3.692, 5.263, 6.972] plt.scatter(year, pop) plt.show()\nIn this video, I'll introduce the histogram. The histogram is a type of visualization that's very useful to explore your data. It can help you to get an idea about the distribution of your variables. To see how it works, imagine 12 values between 0 and 6. I've put them along a number line here. To build a histogram for these values, you can divide the line into equal chunks, called bins. Suppose you go for 3 bins, that each have a width of 2. Next, you count how many data points sit inside each bin. There's 4 data points in the first bin, 6 in the second bin and 2 in the third bin. Finally, you draw a bar for each bin. The height of the bar corresponds to the number of data points that fall in this bin. The result is a histogram, which gives us a nice overview on how the 12 values are distributed. Most values are in the middle, but there are more values below 2 than there are values above 4.\nOf course, also\nmatplotlib is able to build histograms. As before, you should start by importing the\npyplot package that's inside\nmatplotlib. Next, you can use the\nhist() function. Let's open up its documentation. There's a bunch of arguments you can specify, but the first two here are the most important ones.\nx should be a list of values you want to build a histogram for. You can use the second argument,\nbins, to tell Python in how many bins the data should be divided. Based on this number,\nhist() will automatically find appropriate boundaries for all bins, and calculate how may values are in each one. If you don't specify the\nbins argument, it will by 10 by default.\nSo to generate the histogram that you've seen before, let's start by building a list with the 12 values. Next, you simply call\nhist() and pass this list as an input, so it's matched to the argument\nx. I also specified the\nbins argument to be 3, so that the values are divided in three bins. If you finally call the show function, a nice histogram results. Histograms are really useful to give a bigger picture. As an example, have a look at this so-called population pyramid. The age distribution is shown, for both males and females, in the European union. Notice that the histograms are flipped 90 degrees; the bins are horizontal now. The bins are largest for the ages 40 to 44, where there are 20 million males and 20 million females. They are the so-called baby boomers.\nThese are figures of the year 2010. What do you think will have changed in 2050? Let's have a look. The distribution is flatter, and the baby boom generation has gotten older. With the blink of an eye, you can easily see how demographics will be changing over time. That's the true power of histograms at work here! Now head over to the exercises to experiment with histograms yourself!\nvalues = [0,0.6,1.4,1.6,2.2,2.5,2.6,3.2,3.5,3.9,4.2,6] import matplotlib.pyplot as plt plt.hist(values,bins=3) plt.show()\nCreating a plot is one thing. Making the correct plot, that makes the message very clear, is the real challenge. For each visualization, you have many options. First of all, there are the different plot types. And for each plot, you can do an infinite number of customizations. You can change colors, shapes, labels, axes, and so on. The choice depends on, one, the data, and two, the story you want to tell with this data. Since there are a so many possible customizations, the best way to learn this, is by example.\nLet's start with the code in this script to build a simple line plot. It's similar to the line plot we've created in the first video, but this time the year and pop lists contain more data, including projections until the year 2100, forecasted by the United Nations. If we run this script, we already get a pretty nice plot: it shows that the population explosion that's going on, will have slowed down by the end of the century.\nBut some things can be improved. First, it should be clearer which data we are displaying, especially to people who are seeing the graph for the first time. And second, the plot really needs to draw the attention to the population explosion. The first thing you always need to do is label your axes. Let's do this by adding the\n() functions. As inputs, we pass strings that should be placed alongside the axes. Make sure to call these functions before calling the\nshow() method, otherwise your customizations will not be displayed. If we run the script again, this time the axes are annotated. We're also going to add a title to our plot, with the title function. We pass the actual title, 'World Population Projections', as an argument. And there's the title!\ntitle, we can give the reader more information about the data on the plot: now they can at least tell what the plot is about. To put the population growth in perspective, I want to have the y-axis start from zero. You can do this with the\nfunction. The first input is a list, in this example with the numbers zero up to ten, with intervals of 2. If we run this, the plot will change: the curve shifts up. Now it's clear that already in 1950, there were already about 2.5 billion people on this planet.\nNext, to make it clear we're talking about billions, we can add a second argument to the\nyticks() function, which is a list with the display names of the ticks. This list should have the same length as the first list. The tick 0 gets the name 0, the tick 2 gets the name 2B, the tick 4 gets the name 4B and so on. By the way, B stands for Billions here. If we run this version of the script, the labels will change accordingly, great.\nFinally, let's add some more historical data to accentuate the population explosion in the last 60 years. On Wikipedia, I found the world population data for the years 1800, 1850 and 1900. I can write them in list form and append them to the\nyear lists with the plus sign. If I now run the script once more, three data points are added to the graph, giving a more complete picture. Now that's how you turn an average line plot into a visual that has a clear story to tell! Over to you now. Head over to the exercises, gradually customize the world development chart and become the next Hans Rosling!\nimport matplotlib.pyplot as plt import pandas as pd year = list(range(1950, 2101)) pop = [2.53,2.57,2.62,2.67,2.71,2.76,2.81,2.86,2.92,2.97,3.03,3.08,3.14,3.2,3.26,3.33,3.4,3.47,3.54,3.62,3.69,3.77,3.84,3.92,4.,4.07,4.15,4.22,4.3,4.37,4.45,4.53,4.61,4.69,4.78,4.86,4.95,5.05,5.14,5.23,5.32,5.41,5.49,5.58,5.66,5.74,5.82,5.9,5.98,6.05,6.13,6.2,6.28,6.36,6.44,6.51,6.59,6.67,6.75,6.83,6.92,7.,7.08,7.16,7.24,7.32,7.4,7.48,7.56,7.64,7.72,7.79,7.87,7.94,8.01,8.08,8.15,8.22,8.29,8.36,8.42,8.49,8.56,8.62,8.68,8.74,8.8,8.86,8.92,8.98,9.04,9.09,9.15,9.2,9.26,9.31,9.36,9.41,9.46,9.5,9.55,9.6,9.64,9.68,9.73,9.77,9.81,9.85,9.88,9.92,9.96,9.99,10.03,10.06,10.09,10.13,10.16,10.19,10.22,10.25,10.28,10.31,10.33,10.36,10.38,10.41,10.43,10.46,10.48,10.5,10.52,10.55,10.57,10.59,10.61,10.63,10.65,10.66,10.68,10.7,10.72,10.73,10.75,10.77,10.78,10.79,10.81,10.82,10.83,10.84,10.85] pop = [1,1.262,1.650] + pop year = [1800,1850,1900] + year plt.plot(year, pop) plt.xlabel('Year') plt.ylabel('Population') plt.title('World Population Projections') plt.yticks([0,2,4,6,8,10],['0','2B','4B','6B','8B','10B']) plt.show()""]"	['<urn:uuid:a39569cd-7a69-49d4-944e-6336170b3902>', '<urn:uuid:0937f2ca-4788-41e7-b5e1-a9cb0d5b9743>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	12	89	2519
19	As a construction site manager, I often need to explain basic terminology to clients - what's the difference between a renovation and a restoration project?	Renovation means to make new again using new materials and modern methods, while restoration means to repair to original condition using age-specific materials and/or methods.	['Congratulations! You finally nailed down a general contractor and now you’re ready to start renovating – cue in the daily emails, endless 30-second phone calls, and random phone pictures. The goal for your project is for it to be fun and productive. Part of being productive is being able to communicate and part of being able to communicate is being able to understand. Communication tools on site include cell phones, pictures, diagrams on wood scraps, pencil on drywall, and occasional body language. The renovation process is all about conveying your vision. It’s crucial to properly communicate with your GC in order get the job done and it helps to be able to speak in the language of construction.\nSo, here are some definitions of terms to help you with the process:\nAccent lighting: light that is focused on a specific area or object; commonly used to highlight a piece or art or artifact\nBacksplash: a panel behind a sink or stove to protect the wall from splashes\nBaseboard: wood moulding located on the base of the wall and above the floor\nCasing: wood moulding forming a frame around a door or window\nClearance space: the space needed to operate an appliance, cabinets, or a safe passage\nDemo: Short-term slang for: demolition. The process of removing and/or destroying objects for a designated purpose\nDrywall: also known as gypsum board, this is a plaster and/or fibrous board, used to form the interior walls of a house. The term “drywall” is typically used to describe the scope item of gypsum board installation and taping of gypsum board to create a smooth wall surface\nEnergy: the amount of power that a unit uses to operate functionally\nFoundation: a base of sturdy material, like stone or concrete, that supports a building from underneath\nGirder: a large steel or iron beam used to support a structure\nGrout: Cement compound used to fill gaps and seal joints, such as between tiles\nIsland:a free-standing cabinet unit with a counter on top, which allows access from all sides\nLow Voltage: Associated with and distinctively different from Electrical. Low voltage includes functional items in the building using a lesser amount of electrical voltage. Items commonly referred to as “low voltage” include doorbells, security systems, speaker systems, and internet/cable systems.\nMEP: Short-term slang for: Mechanical, Electrical, and Plumbing. These systems provide the functionality of utilities in a building. Mechanical (HVAC) includes the heating and cooling, Electrical includes power supply, and Plumbing includes water, sewer, and gas piping.\nMortar: A compound used to attach stones to another surface or to other stones, such as laying tile on a concrete floor.\nNew Construction: To build a new structure\nOff-the-grid: a system that runs on renewable energy sources\nPocket door: a sliding door that is moved into a compartment in the adjacent wall\nQuality: the standard of something as measured against something else. High quality is what all construction projects should entail.\nRenovation: to make new again using new materials and modern methods\nRestoration: to repair to original condition using age-specific materials and/or methods\nRouter: A tool with a circular blade used for cutting decorative finishes in moulding and door handle openings\nSawzall: short-term slag for: reciprocating saw. This is a handheld tool used for multiple purposes including making a lot of noise and a large mess when one is feeling excited.\nShim: A small piece of material, usually wood, used for leveling, aligning, or reinforcing building materials, Often used for leveling sub-floors to make them even and aligning door jams so the doors swing freely.\nSub-flooring: a floor laid as a base for a finished floor\nTongue & Groove: A term used to describe the installation method of attaching two building materials consisting of an inset piece and an insert piece. Commonly used for hardwood floor installations.\nRenovating property can be a challenging process and putting in enough time to communicate effectively is an important method for achieving your dream home. The terms listed above lend a helping hand in understanding the renovation process so that the next time you sit down with your GC, it will be a two-way conversation.\n- Italian: integrity; honest; upright\n- Latin: refresh; renew; restore']	['<urn:uuid:0890d375-8090-4a16-ae0a-7d1fbf376f82>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	25	25	698
20	classical music tuning systems comparison types	There are several major tuning systems used in music: the 12-Tone Equal Temperament (current standard), which divides the octave into 12 equal parts, and historical systems like the Pythagorean Tuning, which is based on perfect fifths using the ratio 3:2. The Pythagorean system is particularly important for 432-tuning, especially when using Scientific Pitch (C4=256Hz). Alternative approaches include Just Intonation, which uses ratios of small whole numbers, Five-Limit Tuning, which uses powers of 2, 3, and 5, and variations like the 'Twelve True Fifths Tuning' developed by Maria Renold, which combines natural fifths for white keys and a different series of fifths for black keys.	['I’ve been thinking more about musical tuning than CAs lately. For instance, four perfect fifths make (more or less) a major third. How crazy is that?\nThe diatonic scale most western music’s been based on for the past couple millennia comes from ancient Greece; they developed it by tuning their tetrachords using seven consecutive notes on a circle of fifths — F, C, G, D, A, E, B, for instance. Using a just perfect fifth, 701.96 cents, (Pythagorean tuning) the A (four fifths up from the F) is 2807.82 cents above F, or, dropping it down two octaves, 407.82 cents. This is not a particularly great approximation to a just major third, 386.31 cents, but it’s fairly close. Close enough that when music written in octaves or fifths or fourths gave way to use of thirds, musicians developed other ways of tuning diatonic scales for better thirds rather than dumping the entire system. What we’ve ended up with is equal temperament, with 700.00 cent fifths, four of which make a 400.00 cent major third.\nSo for many centuries we’ve been using a scale that was based on octaves and fifths to make music that uses thirds, because it happens to contain thirds that are close enough to just. Now, it’s a little odd to be using probabilistic terms to talk about simple arithmetic — two and two isn’t likely to be four, it is four — but I think you know what I mean when I ask, how likely is that? How lucky did we get?\nIt’s a hard question to quantify. But let’s generously say a major third is adequate if it’s within 25 cents of just. That’s a 50 cent range you want four fifths to fall into, so a single fifth has to be within a range a quarter of that size, 12.5 cents. If you didn’t know beforehand the size of a just perfect fifth, but knew it was somewhere between 650 and 750 cents, you might guess the odds of four fifths making a major third would at most be 12.5/100, or one in eight. Though worse, maybe, because the 12.5 cent range where it works might not be entirely contained within 650 to 750. In fact it might not overlap that range at all. (Though in actuality the range is from 690.33 to 702.83 cents.)\nOn the other hand, maybe the 25 cent range where two “fifths” make a major third does overlap, or the 16.7 cent range where three “fifths” will work does. So the odds of four or fewer fifths making an adequate major third might be a little better. Still seems small though.\nOh… but you also want to consider four or fewer downward fifths, or equivalently, four or fewer upward fourths. That improves the odds.\nSo let’s do a little simulation. Pick a number in the range, say, from 650.0 to 750.0 and see how many fifths, up or down, it takes to make an adequate third. Repeat, and get the distribution. Then ask about things like the average number of fifths needed.\nThere’s a difficulty here, though: Sometimes the answers get very large. Think about 720.00 cents. The notes that “fifth” generates are 720.0, 240.0, 960.0, 480.0, 0.0, 720.0… and it just repeats those five notes over and over; 720.00 generates a 5-equal scale. None of those notes is an adequate third, so you can run forever looking for it.\nOf course you have pretty much zero chance of picking 720.00 at random, but if you pick, say, 719.932771, you’ll have to add a lot of fifths before hitting an adequate third. (1099 of them, looks like.) You’ll get occasional large numbers, then, and they’ll have a big impact on the mean value. The answer you get will fluctuate a lot depending on which large numbers you end up with.\nThis is why medians were invented.\nSo I wrote a little Python script to do this. If you take the range of possible “fifths” as 650 to 750 cents, then there’s about a 22% chance four or fewer fifths, up or down, will produce an adequate third. The median number of fifths required to make an adequate third: 11.\nI think it’s safe to say if you needed 11 perfect fifths to make an adequate major third, the system upon which western music developed would have been entirely different. Different how, I have no idea, but different. Needing only four fifths was a “lucky” break… not win-the-lottery lucky, but definitely beating-the-odds lucky.', 'Last Updated onApprox. reading time: 6 minutesSeptember 27, 2013\nCONCERT PITCH vs TUNING SYSTEM\n(A COMMON MISUNDERSTANDING)\nA common misunderstanding when it comes to 432-Tuning is that many people do not understand the difference between “concert pitch” and “tuning system”. If you do not know the difference, then please, read this first!\nConcert Pitch 432Hz = using reference tone A4=432Hz to tune to, without Temperamental specifications.\n432-Tuning = using the reference tone A4=432Hz in combination with Pythagorean Temperament (or variation).\nWHY IS ONLY CHANGING THE CONCERT PITCH NOT ENOUGH?\nWell, when a piece was recorded using for example the 12-Tone Equal Temperament (12-TET, the present standard), pitching the whole piece down with -0.3166… cents is not sufficient. The “intervallic” relationship between the tones has not changed, when a change of Concert Pitch is implemented. Even though the A would then be in tune with 432-Tuning, the other 11 tones are still off pitch. The Temperament is still Equal Temperament.\nEXAMPLE: 12-TONE EQUAL TEMPERAMENT (the present standard)\nIn twelve-tone equal temperament, which divides the octave into 12 equal parts, the width of a semitone, i.e. the frequency ratio of the interval between two adjacent notes, is the twelfth root of two:\nThere are many other Musical Temperaments (most more complicated then 12-TET), using various mathematical formulas to create other ratios then 1.059463. The most know Temperaments are the: Pythagorean Tuning, Meantone Temperament, Well Temperament, Just Intonation and Five-Limit Tuning.\nIf your intentions are to use the 432-TUNING SYSTEM (not just 432Hz as Concert Pitch) you also need to implement The Pythagorean Temperament.\n432 TUNING – USING THE PYTHAGOREAN TEMPERAMENT\nThe Pythagorean temperament is been noted as the only correct temperament when we talk about 432-tuning. The Most Harmonic Numbers concept uses this method.\nPythagorean tuning is a system of musical tuning in which the frequency ratios of all intervals are based on the ratio 3:2, a ratio from the harmonic series. This ratio, also known as the “pure” or perfect fifth, is chosen because it is one of the most consonant intervals and easy to tune on by ear.\nThe Pythagorean Temperament is thus one of the oldest known “Just Intonation” (3-limit) Temperaments.\nMore information: Wikipedia\nIMPORTANT: When using the Scientific Pitch (C4=256Hz) and you wish A4 to be 432Hz as well, then there are NO alternatives for the Pythagorean Temperament (and variations on it)! Any other temperament used with C4=256Hz, will not result in A4=432Hz but by example with 12-TET in A4=430.5Hz (430.53824925657256Hz).\nTHE BEST ALTERNATIVE IMPLEMENTATION OF THE PYTHAGOREAN TEMPERAMENT:\n“TWELVE TRUE FIFTHS TUNING” (BY MARIA RENOLD)\nGraham H Jackson explains this tuning system on his web site as follows:\n“For the “twelve true-5ths tuning”: you first set C at 256 Hz. Then you tune the 7 “white keys” by the circle of 5ths, using however natural 5ths. Then you divide the octave at C exactly in half (which can be done handily with a special tuning fork), and tune the 5 “black keys” by natural 5ths to that F#. You end up with two series of natural 5ths–one of 7 notes and one of 5 notes, linked by an “unnatural” interval of an augmented 4th (which is actually the same augmented 4th found in the equal-tempered system).” [ Read More ] ►\nOTHER ALTERNATIVES FOR THE PYTHAGOREAN TEMPERAMENT?\nWithin the “432-Tuning community” there are those who favor other temperaments due to certain “limitations” of the Pythagorean Temperament, or simply because they prefer the usage of other perfect intervals (like a perfect Third) over the Perfect Fifth. If you could still call these methods of tuning “432-Tuning” is questionable (see blog “Concert Pitch” vs “Tuning System“).\nMost popular alternative temperaments (not based on Perfect Fifths) used in combination with A4=432Hz are:\nIn music, just intonation (sometimes abbreviated as JI) or pure intonation is any musical tuning in which the frequencies of notes are related by ratios of small whole numbers. Any interval tuned in this way is called a pure or just interval. The two notes in any just interval are members of the same harmonic series. Frequency ratios involving large integers such as 1024:927 are not generally said to be justly tuned. Just intonation is the tuning system of the later ancient Greek modes as codified by Ptolemy. More information: Wikipedia | XenHarmonic\nFIVE-LIMITED TUNING (diatonic scale)\nFive-limit tuning, or 5-limit tuning is a method to obtain a justly tuned musical scale. The frequencies of the notes of such a scale are obtained by multiplying the frequency of a given reference note (the base note) by powers of 2, 3, or 5, or a combination of them. More information: Wikipedia | XenHarmonic\nFIBONACCI + CONCERT PITCH A4=432Hz\nThe clearest demonstration of Fibonacci being represented in music is seen in scales. 13: the Octave is made of 12 chromatic tones plus 1 the octave. A (basic) scale is composed of 8 notes. The 5th and 3rd notes create basic foundation of chords, based off a whole tone that is 2 steps above root tone, which is the 1st note of scale. Ratios found in the first 7 numbers of the Fibonacci series (0, 1, 1, 2, 3, 5, 8) are related to key frequencies of musical notes.\n[ Read More ] ►\nCHANGING THE CONCERT PITCH IN POST PRODUCTION\nIf you would only like to change the concert pitch, then there are two ways to do so.\n1) PREFERED OPTION: CHANGING THE SPEED\nWhen you change the speed of a piece of music, both Tempo and Pitch will change. This method is the equivalent of slowing down a turntable. In order to get a -31.667 cents pitch change, you should reduce the speed with -1.818%.\n2) SECONDARY OPTION: CHANGING THE PITCH\nWhen you change the Concert Pitch from 440Hz to 432Hz, you need to pitch the audio-material down with -31.76665363342977 cents or -0.3176665363342977 semitones. If the software or hardware you use only allows you to use round numbers, you can set it to -31 or -32 cents. For electronic music -31 cents might work better (since that music is generally required to be more “punchy”), for acoustic music -32 cents seems to work out best in most cases.\nIMPORTANT NOTE: When only changing the pitch and not the tempo, there will be resolution loss, and the high-end sounds might sound “fake”. That’s why option 1 (changing the speed) is the preferred option!\nQ: CAN YOU RE-TUNE AND (RE-)TEMPER A COMPLETE SONG AT ONCE?\nA: A complete song (1 file, for example .wav or .mp3 file) with various instruments tuned to 440Hz + 12-TET, can NOT be re-tuned to 432-Tuning (432Hz + Pythagorean or related Temperament).\nYou can still pitch it -31.667…cents down, so the Concert Pitch changes from 440Hz to 432Hz, but the piece will not be “in tune” with the 432-TUNING SYSTEM, simply because the temperament has not been changed from 12-TET to Pythagorean (as explained at the introduction to 432-Tuning).\nIF you have recorded the instruments on separate tracks though, a lot can be done!\nSome instruments that can not change the Concert Pitch and/or Temperament – or might be too hard to intonate when used with another concert pitch and/or temperament then they were designed for – can be re-pitched and (re-)tempered in post production (after recording). This is sometimes the “only option”, specially when working with instruments with tone-holes / valves / pistons (particularly conical wood winds / brass) or those that are made out of one piece (Didgeridoo, Penny Whistle, et cetera), as well as melodic percussive instruments (like Vibraphone, Marimba, et cetera). Unfortunately there is no solution for these (acoustic) instruments when performing live.\nSome online sources proclaim re-pitching music from Concert Pitch A4=440Hz to 432Hz will positively effect the “overtones”. This is not true! [ Read More ] ►\nRECORDING IN THE RIGHT PITCH & TEMPERAMENT IS BEST!\nEvery form of “manipulation” will cause loss of quality to a recorded sound. The better the algorithm used with the software, the smaller the loss. Nonetheless, it is always preferable to tune the instruments you like to record to the right pitch and temperament, if possible.\nSome online sources proclaim 432-recorded music has more “Overtones” then 440-recorded music.\nThis is not true! [ Read More ] ►']	['<urn:uuid:d56c1190-f540-4f1f-b443-7f0674f01def>', '<urn:uuid:09b91563-0ff9-4d91-810d-2198bebf976c>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T20:35:02.547110	6	104	2126
21	How much money did Abbott leave for Maharashtra institutions?	Abbott left $140,000 for institutions in Maharashtra, out of a total of $375,000 set aside for charities in his Will.	"['That humanity has a common heart, is a truth generally recognized, but not always remembered in the actual relationships of man with man. A soul in distress, be it from a sense of sin, or from losses and disappointments, speaks a language which every other soul in distress perfectly understands. The feelings of the heart are the same, under the same conditions. When men try to express those common feelings in literary form, of necessity that form varies, but the substance remains the same.\nThe best definition of the word ""prayer,"" ever proposed, is found in the verse ""Prayer is the heart\'s sincere desire, uttered or unexpressed."" And it might be added that the ""uttered"" desire may be expressed in uncouth language, with superstitious conceptions, with irrational philosophy or with wild imaginings, but the heart\'s sincere desire may remain the same. This should not be forgotten in the actual relationships of man with man. The form of prayers may widely differ, but the eye of understanding must pierce these outer forms, and see the heart in its ""sincere desire.""\nThe Poet-saints of Maharashtra were all Vedantists in philosophy. Almost without exception they belonged to the bhakti school of thought. The Vedas, the Upanishads, the great epics of the Mahabharata, and the Ramayana, and the eighteen Purans, were to them sacred books, divinely inspired, and necessarily their metaphysical, and moral philosophy, and their stories of the deeds of gods and heroes, were accepted as undeniable truths. In their prayers, therefore, there are many things that must appear strange to those who have a different philosophy to those to whom the Puranic stories mean nothing but curious legends, and to those to whom even the moral implications contained in these prayers, seem at times at variance with the standard of morals, that the modern heart and conscience demand. Readers of these prayers may, therefore, do their authors an injustice by concentrating their attention on the form in which the prayers appear, instead of on the inner kernel of those prayers, the \'heart\'s sincere desire.""\nIn the writings of the Maratha poet-saints, those of a devotional character, such as might be called prayers, are not always prominent. And yet such was the devotional character of the authors themselves, that somewhere in their writings prayers are to be found. I have begun the collection of prayers, or rather ""Garland of Prayers,"" following India\'s poetic designation, with two prayers of Dnyanadev, which are the beginnings of two of the chapters in his Commentary on the Bhagavadgita. This Commentary is a remarkable book. It was written, as generally accepted, in the year 1290. It was the first attempt to give to those ignorant of Sanskrit, a knowledge of the contents of the Bhagavadgita, and Dnyanadev\'s high motive frequently appears throughout the book. Every chapter begins with an invocation, that might be considered a prayer, but the two that I have selected seemed most appropriate for this ""Garland of Prayers."" The Benediction with which he closes his book, is a beautiful prayer, making a worthy closing to any book designed to meet human needs. His Benediction, therefore, closes this ""Garland of Prayers.""\nI have given Mahipati the last place in the ""Garland"" for those after him are accounted among modern Marathi writers, and my translations are intended to be from the poetic writings of the older writers. I claim no special logical principle in my choice of specimens of prayers of the poet-saints from Dynanadev to Mahipati. As I turned over the pages of their books, my eyes caught some striking passage in a prayer, and when read more carefully, it was accepted for the ""Garland"" if it had the form of a prayer expressing some deep desire on the author\'s part, for himself or others. The three long prayers, the Vyankatesha Stotra by Devadas, the Kekavali by Moropant, and the Pandurang Stotra by Mahipati, are very popular in the Maratha country, and I could not omit them from the ""Garland of Prayers,"" The authors are arranged chronologically so far as the vexed question of dates permits.\nMany books on Comparative Religions have been written by scholars. Hinduism has been represented by many worthy Sanskrit students. They have drawn their information from Sanskrit students. They have drawn their information from Sanskrit books. But it is a fact that Sanskrit devotional books are so rare, that they have not attracted the attention of a single writer on Comparative Religions. In these books therefore, comparing Hinduism with other religions, it is the philosophy and ritual that is compared, and not the deeper depths of the soul\'s feelings, cravings, longings, hopes, confessions of sin, loves and hates, aspirations, sense of dependence on God, cries for help, such as may come from the depths of a Hindu\'s heart in sorrow and distress. Such things are not to be found in Sanskrit books so far as I know. They can only be known, therefore, through the Indian vernaculars. I am not aware that America, England, nor the Continent of Europe has produced a single scholar who can read with fluency the old Marathi literature, one of the store houses of these deeper feelings. All books, therefore, on Comparative Religions, dealing with Hinduism, are, I am convinced, deplorably defective.\nThe translations of the prayers in this ""Garland"", while very incomplete, so far as extent is concerned, will I trust, be found to cover so many aspects of a Hindu\'s heart\'s desire as to throw light on many of their deeper feelings, and so help to a better understanding of the Hindu heart. Misunderstanding of one another is one of the saddest things in human relationships. If therefore, this ""Garland of Prayers"" helps to a better understanding of Hindu hearts, this labour of love for India and her people will have its reward.\nMy Indian friends will, I know, easily pardon personal references. The manuscript of this book, I have completed this day, the 25th of December 1928. It is my 75th birthday. On Christmas Day in 1853 I came, I know now whence, into this world. My Hindu friends may perhaps picture for me some other birth, through which I have passed. I will not quarrel with them, but I know not whence I came. To speak the truth, that question of origin interests me very little, but the life of these 75 years has an interest that grows more intense as the natural limit of life approaches. I have many questions to ask myself. What have I done during these 75 years? What part have I had in making this a better and a happier world? What opportunities have I lost? What errors have disqualified me from a larger share in the world\'s betterment? As India has been the scene of the greater part of my life\'s activities, have I contributed to making India more truly known, loved and honoured by the rest of the world? Have the needs and the sufferings of so many appealed as they should to my own heart? The true answer to these questions belongs to the conscience in its deepest and most secret recesses, and are not for the public gaze. But as I approach the years of utmost uncertainty, and know full well that few live beyond the age of 75, I want to make a gift to India on this Christmas Day. I give this book, this ""Garland of Prayers"" to the land and the people I love. It is a small gift, but India has some beautiful stories of small gifts given, and accepted in the generous spirit that made them great. The handful of rice of poor Sudama and his wife, given to Krishna out of love, is the beautiful picture of ideal gifts and ideal receiving of gifts.\nIndia and the World:\nThe literary history of India of the nineteenth century is fascinating with scholars from America, England, France, Italy and other countries visiting our country and becoming eminent scholars of our languages, Sanskrit, Marathi, Bengali, Hindi, Tamil, Telugu, Malayalam, Kannada and the rest. Their advent was of historic interest, and Indian languages and culture benefited remarkably as a result.\nReverend Justin E. Abbott belongs to his magnificent galaxy of scholars. He was born in America in the town of Portsmuth in Hampshire in 1853, and he passed away on June 19, 1932. He was four years old when his parents visited Rahunni in Ahmednagar District of Maharashtra. Years later he came to India as a member of the American Marathi Mission. A scholar and preacher of the literature of the Saints of Maharashtra, Abbott lived in India with his wife for forty years. He was the editor of JNANODAYA, the English section of the American Missions\' Journal, for 25 years. He retired from the Mission in 1910 and settled at Summitt in New Jersey, studying the life and writings of the Marathi saints.\nAbbott returned to India ten years later in 1920 accompanied by his wife, on a pilgrimage to places revered by the Marathi saints. He spent a month in Pune at the Bharat Itihas Samshodhak Mandal engaged in research, studying the manuscripts of the saints. Later he visited with his wife Daulatabad and other places, particularly the hallowed placed where Eknath and his Guru Janardana Swamy lived. Mrs. Abbott was presently taken ill, and she expired at the Miraj hospital on June 26, 1921. Abbott returned to America thereafter and attended to the work of the American Oriental Society. He visited various Universities in Europe, looking for further information on the literature of the Marathi saints about whom he had already a rich treasure. Besides, he collected valuable information on Shivaji from the correspondence found in the voluminous ""Das Register"" belonging to the Dutch East India Company. He passed on the material that he gathered to the Bharat Itihas Samshodhak Mandal. His essay on the coins of Shivaji, published by the Asiatic Society, reveals his high regard for the history of Maharashtra.\nAbbott wrote a series of articles in JNANODAYA in 1920. Mrs. Abbott was equally fascinated by the literature of the Marathi Saints, which her husband decided to translated into English for the benefit of readers not knowing Marathi. This great work came to be known as ""Poet-Saints of Maharashtra"" published in a series of volumes. He was hesitant at first to undertake the work, but he was persuaded by his wife and Rev. Edwards of the Theological College of Western India, Pune. Abbott appointed the Marathi Pandit N. R. Godbole to assist him in this task of translation. The actual publication of the work rested with Rev. Edwards who saw it through. This great work on the Poet-Saints of Maharashtra, undertaken and completed by Abbott, placed the people of Maharashtra and India under a deep debt of gratitude to him.\n""The Poet-Saints of Maharashtra"" consisted of twelve volumes: 1. Bhanudas, 2. Eknath, 3. Bhikshugita, 4. Dasopant Digambar, 5. Bahinabai, 6. Stotramala, 7. Tukaram, 8. Ramdas, 9. Bhaktavijay Vol. I, 10. Bhaktavajay Vol. II, 11. Bhaktaleelamrit, 12. Jnaneshwar.\nThese volumes contain Abbott\'s English translation with notes, life-sketches, glossaries and the English meaning of certain Marathi words, besides the Marathi text, Abbott was acquainted with the literature of the Saints of Maharashtra but he did not realise the importance of the Marathi language until he was back in India, and studied the literature once again and came to the conclusion that this body of writing contained the literature of the highest order and supreme message. Abbott being should be lifted high exemplifying the essence of real Dharma. Nor did he ever lose his faith in Christ and Christianity.\nHe was of the opinion that Westerners never understood the Hindu way of Bhakti. He was sorry that he did not realise earlier the Hindu philosophy. He realised that the Hindus had faith in the supreme God though they might worship various idols.\nAbbott\'s love for Maharashtra is evident from his Will in which he left $ 3,75,000 for Charities, out of which $ 1,40,000 was set apart for institutions in Maharashtra. The Bharat Itihas Samshodhak Mandal benefited most. Other beneficiaries were:\n1. Servants of India Society, Pune.\n2. American Mission Hospital, Wai.\n3. American Oriental Society.\n4. Lepers Sanatorium, France.\n5. Gokhale Hall Library, Pune.\n6. Social Service League, Bombay.\n7. Anath Balikashram, Pune.\n8. Depressed Classes Mission, Mangalore.\n9. American Lepers Mission.\n10. Y.W.C.A. Summitt, USA.\n11. Y.M.C.A. Summitt, USA.\n12. Overlook Hospital, Summitt, USA.\n13. Sevasadan Society, Pune.\n14. Rev. Edwards for Poet saints series.\n15. American Marathi Mission Naghade Bombay.\nThe greatness of Abbott and his dedicated service are evident from the foregoing information obtained mostly from the late D. V. Potdar\'s Arvachina Charitra Kosa.\nYour email address will not be published *\nSend as free online greeting card\nEmail a Friend']"	['<urn:uuid:7e942774-1d5a-432a-9c1c-f368ab310406>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	9	20	2136
22	How do scientists test what babies understand about language?	Scientists use the intermodal preferential looking paradigm (IPLP), where they measure infants' language comprehension by showing them two side-by-side images and observing which one they look at when hearing linguistic stimuli. This method allows researchers to study infants' knowledge of language before they can speak, including areas like phonology, semantics, syntax, and morphology.	['In the next few issues, Perspectives on Psychological Science will publish several special sections celebrating the 25th anniversary of APS. Twenty-five is an interesting age for an organization: Many of our younger members were not yet born when APS began, and for many of our older members, 1988 was just another typical year in their long careers. Those of us in graduate school at the time may have wondered whether psychology really needed another professional organization and, if so, why it needed one with almost the same name as the behemoth American Psychological Association. (I’m sure many of you remember that APS began as the American Psychological Society before being renamed the Association for Psychological Science in 2006.) Well, 25 years later, our organization is thriving and it is close to the 25,000 member mark.\nThe articles in the current and upcoming special sections describe some of the huge changes in psychological science between 1988 and the present. There are now research and statistical tools that did not exist then, theoretical perspectives that have arisen or disappeared, and entire fields of inquiry that have been born, merged, split, renamed, and disbanded. The articles take two different forms. The longer ones delve deeply into the changes in various research areas that have been ongoing since (at least) 1988. We have two in the May issue: In the first, Eagly and Wood describe the years of tension between nature and nurture explanations for sex differences, and in the second, Ma, Golinkoff, Song, and Hirsh-Pasek show how the development of some very clever research tools improved our ability to study and understand children’s language acquisition. The shorter articles will begin appearing in the July issue. We asked the distinguished recipients of this year’s William James and James McKeen Cattell Fellow Awards, along with some other prominent researchers, to describe a major change in their area in under 1,000 words. We will thus be able to present views from across our discipline.\nBut before looking back only 25 years, we have a chance to look back 100 years. The first article in the special section is a book review of a collection of pieces by our hero William James, whose concerns 100 years ago have a surprisingly modern sound today.\nThis was first published in Perspectives on Psychological Science in May 2013.\nThe following are abstracts from articles in the May 2013 issue of Perspectives on Psychological Science celebrating the 25th anniversary of APS.\nBook Review: The Heart of William James\nBy Robert Richardson\nReview by James H. Austin\nWilliam James lived from 1842 to 1910. In the century since his death, no one has surpassed him in the prescience with which he anticipated themes now within the scope of the Association for Psychological Science. James’s words live on in this centenary edition of his work—fresh reminders of the countless ways his far-sighted perspectives illuminate our contemporary discourse. This collection of 17 essays (including some book chapters) was carefully selected from the definitive 19-volume set of The Works of William James, published between 1975 and 1988 by Harvard University Press. The essays are arranged in chronological order, beginning with the topic of emotion (1884) and closing with his prescription of a moral equivalent of war (1910). In the intervening years, Richardson follows James’s many-faceted mind through diverse topics, including his crucial contributions to habit (1892), will (1899), and pragmatism (1899).\nThe Nature–Nurture Debates: 25 Years of Challenges in Understanding the Psychology of Gender\nBy Alice H. Eagly and Wendy Wood\nNature–nurture debates continue to be highly contentious in the psychology of gender despite the common recognition that both types of causal explanations are important. In this article, we provide a historical analysis of the vicissitudes of nature and nurture explanations of sex differences and similarities during the quarter century since the founding of the Association for Psychological Science. We consider how the increasing use of meta-analysis helped to clarify sex difference findings if not the causal explanations for these effects.\nTo illustrate these developments, this article describes socialization and preferences for mates as two important areas of gender research. We also highlight developing research trends that address the interactive processes by which nature and nurture work together in producing sex differences and similarities. Such theorizing holds the promise of better science as well as a more coherent account of the psychology of women and men that should prove to be more influential with the broader public.\nTwenty-Five Years Using the Intermodal Preferential Looking Paradigm to Study Language Acquisition: What Have We Learned?\nBy Roberta Michnick Golinkoff, Weiyi Ma, Lulu Song, and Kathy Hirsh-Pasek\nThe intermodal preferential looking paradigm (IPLP) has proven to be a revolutionary method for the examination of infants’ emerging language knowledge. In the IPLP, infants’ language comprehension is measured by their differential visual fixation to two images presented side-by-side when only one of the images matches an accompanying linguistic stimulus. Researchers can examine burgeoning knowledge in the areas of phonology, semantics, syntax, and morphology in infants not yet speaking.\nThe IPLP enables the exploration of the underlying mechanisms involved in language learning and illuminates how infants identify the correspondences between language and referents in the world. It has also fostered the study of infants’ conceptions of the dynamic events that language will express. Exemplifying translational science, the IPLP is now being investigated for its clinical and diagnostic value.\nLeave a comment below and continue the conversation.']	['<urn:uuid:996e24e8-98aa-490c-a688-869ebd0f96f3>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	9	53	905
23	breitling galactic sleekt features water resistance	The Breitling Galactic Unitime SleekT has water resistance up to 100 meters, featuring a screw-locked crown with two gaskets. In comparison, other Breitling models like the Chronomat Airborne offer higher water resistance - 300 meters for the 41mm model and 500 meters for the 44mm model.	['Breitling’s newcomer dubbed Galactic Unitime SleekT is a timekeeper which represents a whole new chapter in the brand’s development and yet, at the same time it preserves the company’s distinctive DNA and spirit. It is a doubly innovative piece, both in its features and appearance. As for the looks, it premiers a whole new type of proprietary movement which is made without the expected and usual chronograph. Its appearance is different from other existing models in the brand’s offer since it uses a new type of bezel which is made of tungsten carbide. This allows it to look a bit different and more importantly to be more tough and resilient. New Galactic Unitime model with a world time feature is available in stainless steel and with two types of dials.\nBreitling Galactic Unitime SleekT marks an important milestone in Breitling’s development. So far, the name of the Grenchen based watch manufacturer has been associated with automatic chronographs and Bretiling has been famous for its capacity to produce a large amount of its own timepieces of this type. In order to be able to create its own watches of this kind, Breitling had previously developed its in-house B 01 movement. This first proprietary movement has been subsequently conjoined with additional modules that gave birth to other movements which all had the same base. The only difference was the addition of some additional features.\nWith this new model from the Galactic line of products, it becomes clear that the manufacturer from Grenchen wanted to diversify its offer and to start producing additional types of watches, apart from automatic chronographs that were its main focus. Unlike any of those watches, new Unitime SleekT comes supplied with B 35 caliber. It is rather confusing that Breitling previously had another caliber with the same designation which was entirely different in its nature. Unlike the old B35 caliber which was a flyback chronograph with ETA base movement, the new one is a manufacture movement.\nIn accordance with the company’s recognizable tradition, the novel B35 caliber is also mechanical and self-winding. However, it does not include a chronograph. As for its main features, one should emphasize its world time feature. The movement allows the wearer to switch between different time zones and to “jump” in one hour increments. It is possible to move time zones in both directions. As the wearer switches the time zone, the date display is automatically set into the right position, if necessary. Maybe it sounds fairly simple, but this sort of functions set was not easy to create. As a matter a fact, Breitling needed as much as four different patents to create such a thing. Two of the innovations are connected to the used differential system and its connection to the city disk of the movement. Two other patents that are featured in new B35 caliber are used for its winding and time-setting systems.\nWhen other important traits of the first manufacture Breitling movement that does not include a stop watch feature, we should note that it integrates 41 jewels and has a minimal power reserve of 70 hours. As it is customary for the brand’s models, it operates at the high-beat frequency of 28,800 vph. Another universal Breitling trait that can be seen in all of its calibers and this one as well is its COSC chronograph certification that guarantees high standards of functioning.\nAs it has been mentioned, the latest timepiece from the Galactic collection is not only different for its technical traits, but also for its appearance. This was mainly done with the introduction of a whole new type of bezel. The used fixed bezel features a lovely smooth appearance which was possible since it was made in tungsten-carbide which is responsible for its peculiar SmoothT part of the name. This is a high-tech composite material. In order to create it, it is needed to use tungsten powder which is then molded with the help of extreme temperatures and pressures. Once the process is completed, one gets a material with awesome properties. Tungsten carbide is very resilient. It is estimated that it is as much as four times harder than the previously used stainless steel. In addition, it looks absolutely stunning.\nApart from the smoothness of the bezel, the watch features some other appealing aesthetic traits. One that stands out among the others is the representation of the globe on the dial which was created by a filigree-worked pattern. Aside from the globe which stands as the background and the city disk at the periphery of the face, there is not much going on visually. There are oversized central hands and hour markers which are all covered with luminous coating for enhanced legibility. The date aperture for its calendar feature which automatically switches to the current date of the chosen time zone is placed at the traditional place, at the three o’clock position.\nWhen the dimensions are concerned, one has to note the watch is not that small, especially since it does not offer other functions, apart from its world time feature. The diameter of the watch is quite usual for the Breitling models and measures to 44 mm. The thickness of Galactic Unitime SleekT timepiece is 15.3 mm. The watch features a solid metal case back that unfortunately does not allow the wearer to observe the new type of movement we should be outlined as the major that will follow. The construction of the timekeeper also includes a screw-locked crown with two gaskets, as well as a cambered sapphire crystal which has anti-reflective coating on both of its sides. Supported water resistance of Breitling Galactic UNitime SleekT is to 100 meters.\nThe new world timer by the Grenchen company is avialble with two different dial colors. These are Antartica White and Trophy Black shades. The black one can be paired with a black leather strap with contrasting stitching or with Pilot stainless steel bracelet with five links. The other one is available only with the bracelet.', 'One of the great names in aviation timepieces is, of course, Breitling, now celebrating its 130th anniversary. As is the Baselworld custom, Jean-Paul Girardin, the brand’s Vice-President, introduced me to the latest additions to two of their iconic collections: the Chronomat and the Navitimer.\nIn the early 1980s, Ernest Schneider – to whom Willy Breitling had just sold his firm – learned that the Frecce Tricolori, the Italian airforce aerobatics team, wanted to create an official watch. Accordingly, he decided to create a brand-new chronograph specifically designed for pilots’ needs. Developed in close cooperation with the squadron, the Chronomat was designed in particular for its readability and precision functions and the resulting Frecce Tricolori chronograph was launched in 1983.\nIn 1984, the firm celebrated its 100th anniversary by launching its Chronomat collection and over the past three decades, this exceptional chronograph has enjoyed an aesthetic evolution whilst maintaining its iconic characteristics. In 2009, the Chronomat was the first model in the range to host the new Manufacture Breitling calibres and has become an horological classic.\nIn celebration of the 30th anniversary of its flagship Chronomat model, Breitling is now launching a special series of this legendary pilot’s chronograph, the Chronomat Airborne.\nDesigned in the spirit of the original Frecce Tricolori chronograph, this model has a satin-brushed rotating bezel equipped with four raised rider tabs serving to count off flight times, while facilitating rotation even with gloved hands. The stainless steel case with its polished finish is available in two sizes, 41 and 44 mm, with either an onyx black dial with silver-toned counters or Sierra silver dial with black counters. The caseback is engraved with the inscription Edition Spéciale 30e Anniversaire and has the outline of an Aermacchi, one of the ten planes flown by the Italian flight squadron.\nThe watch is equipped with a Manufacture Breitling Calibre 01 self-winding movement, chronometer-certified by the COSC (Swiss Official Chronometer Testing Institute) with a 70-hour power reserve, 1/4th of a second chronograph and 30-minute and 12-hour totalizers. The date aperture is set between 4 and 5 o’clock, there is a unidirectional ratcheted rotating bezel, screw-locked crown and pushbuttons, a cambered sapphire crystal glare-proofed on both sides and has an ultra-sturdy strap in black military-type fabric, secured by a folding clasp. The 41 mm model is water-resistant to 300 metres and the 44 mm model is water-resistant to 500 metres.\nThere are two new variations on the legendary Navitimer, the ‘wrist instrument’ purpose-built for pilots and other aviation professionals in 1952.\nThe new 46 mm Navitimer comes in either stainless steel or a Limited Edition of 200 pieces in 18 carat red gold equipped with an automatic, high-frequency (28,800 vibrations per hour), Manufacture Breitling Calibre 01 COSC movement with a 70-hour power reserve. The dial is black or Aurora blue , there are 1/4th of a second chronograph, 30-minute and 12-hour totalizers, a date aperture between 4 and 5 o’clock, a rotating bezel with circular slide rule, a cambered sapphire crystal, glare-proofed on both sides. There is a transparent sapphire crystal caseback and this Navitimer comes with either a leather or crocodile strap or a stainless steel bracelet. The watch is water-resistant to 30 metres.\n- Navitimer GMT by Breitling\nThe Navitimer GMT is an imposing 48 mm and comes with a Manufacture Breitling Calibre B04 self-winding, high-frequency (28,800 vibrations per hour) movement certified by the COSC, visible through the transparent caseback and equipped with a dual timezone system. User-friendly, the GMT system is ingenious: by simply pulling out the crown and turning it forwards or backwards in one-hour increments enables the instant setting of the travel time zone date and hour – while maintaining a home-time display in 24-hour mode and without losing any precision in terms of the minutes.\nThe large-format dial offers a very readable instrument panel either on a classic dial in black with silver counters or an all silver version.\nThere is a 70-hour power reserve, a 24-hour 2nd time zone display, 1/4th of a second chronograph, 30-minute and 12-hour totalizers, a date aperture between 4 and 6 o’clock, a cambered sapphire crystal, glare-proofed on both sides, a rotating bezel with circular slide rule and the watch is water-resistant to 30 metres.\nThe Navitimer GMT is completed by a steel bracelet or a leather or crocodile strap. A Limited Edition of 200 pieces in 18 carat red gold with a black dial is also available.\nFor Bentley aficionados, the Breitling for Bentley collection now has a its own Light Body GMT. In titanium, it has a Breitling Calibre B04 self-winding movement and the same chronographic features of the previous watches. However, added to this is a rotating inner bezel with the names of 24 cities thus enabling the user to have readings of all the time zones via the 24-hour hand. The watch is water-resistant to 100 metres and has that familiar broad bezel with a knurled motif inspired by Bentley radiator grilles.\nThe fish were happily swimming around in the gigantic fish tank that sits above the Breitling booth when I left and the sun was shining outside, making it a very pleasant year for their aficionados.']	['<urn:uuid:69f8c7cb-7968-4db0-8bac-522100af431e>', '<urn:uuid:351c6c69-7a54-43b9-8c10-b6e30936fcc5>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:35:02.547110	6	46	1859
24	What distinguishes hotspot volcanoes from other types?	Hotspot volcanoes form far from tectonic plate edges through oceanic lithosphere, making them less explosive than subduction zone volcanoes since they lack trapped water. The research shows these hotspots can drift, as evidenced by the Hawaiian-Emperor Seamounts chain. When hotspots occur under continental crust, they can form violent rhyolitic eruptions despite low water content, as seen in the Yellowstone Caldera.	"['Geologists have long assumed that the Hawaiian Islands owe their existence to a ""hotspot"" -- stationary plumes of magma that rise from the Earth\'s mantle to form Mauna Loa, Kilauea and Hawaii\'s other massive volcanoes. But a new study posted on the online version of the journal Science disputes that long-standing paradigm by concluding that the fixed hotspot in the Pacific was not stationary after all.\n""Our research suggests that the Hawaiian hotspot actually drifted southward between 47 and 81 million years ago during the Late Cretaceous to Early Tertiary,"" said David Scholl, consulting professor of geophysics at Stanford University and senior research scientist emeritus with the U.S. Geological Survey in Menlo Park, Calif. ""This finding will break across a very cherished idea about how things work in the innards of the Earth.""\nScholl is one of 11 co-authors of the study that appears in the July 24 edition of Science Express online (www.sciencexpress.org). The lead author is geophysicist John A. Tarduno of the University of Rochester, who earned a doctoral degree at Stanford in 1987. His principal collaborator is second author Robert A. Duncan of Oregon State University.\nUntil now, the prevailing theory among geophysicists has been that, for millions of years, the massive Pacific Plate has been slowly drifting over a fixed hotspot deep in the mantle, creating a trail of volcanoes whose peaks emerge from the ocean as Maui, Oahu and the other islands that make up the 49th state. The Hawaiian Islands are part of a long chain of volcanoes collectively known as the Hawaiian-Emperor Seamounts that stretch some 3,600 miles along the floor of the Pacific -- from the Big Island of Hawaii to Alaska\'s Aleutian Trench.\nThe segment known as the Hawaiian Ridge, which includes the Hawaiian and Midway Islands, forms a neat line of volcanoes that extends some 1,800 miles northwest across the Pacific. At that point, the Hawaiian Ridge meets the Emperor Seamounts -- an older volcanic chain that abruptly changes course, stretching more than 1,000 miles almost due north to the Aleutian Trench. According to most researchers, this sharp northward bend records a change in the direction of the Pacific Plate as it passed over the fixed hotspot about 47 million years ago.\nTo test this hypothesis and determine the true origin of the Emperor Seamounts, Tarduno, Scholl and other scientists embarked on a two-month excursion aboard the research vessel JOIDES Resolution to collect samples of solidified lava flows from four submerged volcanoes that form part of the Seamounts chain. The expedition was conducted under the auspices of the Ocean Drilling Program, an international research effort to study the world\'s seafloors.\nArgon and magnetite\nWhen the voyage ended, the researchers conducted geochemical analyses of the lava samples to determine where and when they formed. Age was determined by the radiometric dating of two chemical elements -- potassium and argon.\nWhen a rock forms, atoms of potassium start decaying into argon at a constant rate, regardless of changes in the rock\'s temperature, chemistry or pressure. By measuring the number of potassium-derived argon atoms in the samples, researchers were able to estimate that the submerged volcanoes formed some 45 million to 81 million years ago.\nTarduno and his team were able to determine where the volcanoes formed by analyzing a mineral called magnetite in the rock samples. As hot magma from an erupting volcano cools, magnetite residues align with the Earth\'s magnetic pole and become locked in place.\n""The magnetite behaves like miniature compass needles: The closer they are to the Earth\'s magnetic pole, the steeper their position,"" Scholl explained. Researchers were able to verify the latitudes at which the Seamounts formed by determining the angles at which the magnetite had frozen.\nUsing these data, the research team concluded that the ""fixed"" Hawaiian hotspot probably crept southward between 81 million and 47 million years ago at a rate of about 44 millimeters a year, ""changing our understanding of terrestrial dynamics,"" they wrote.\n""Given the central role the Hawaiian-Emperor bend has played as an example of [tectonic] plate motion change, these observations now question whether major plates can undergo large changes in direction rapidly, and whether plate boundary forces alone can play a dominant role in controlling plate motions,"" they noted. ""These data sets indicate a much more active role of mantle convection in controlling the distribution of volcanic islands.""\n""This study raises fundamental questions about how the mantle works and how plates work,"" Scholl added. ""What\'s really going on here? Why do these hotspots drift, and why do they suddenly stop? We know that the mantle moves, now we have to find out how deep the motion goes.""\nFuture drilling experiments in other ocean basins could provide answers, he noted.\nOther co-authors of the study are Rory D. Cottrell of the University of Rochester; Bernhard Steinberger of the Japan Marine Science and Technology Center; Thorvaldur Thordarson of the University of Hawaii; geophysics graduate student Bryan C. Kerr of Stanford; Clive R. Neal of the University of Notre Dame; Fred A. Frey of the Massachusetts Institute of Technology; Masayuki Torii of the Okayama University of Science; and Claire Carvallo of the University of Toronto. Funding was provided by the National Science Foundation.\nCite This Page:', 'In geology, a hotspot is an area in the Earth\'s mantle where a column of hot magma rises up to melt through the crust, resulting in volcanic activity. The term hotspot is also used when referring to the location on the Earth\'s surface where such volcanism has been taking place. In 1963, J. Tuzo Wilson came up with the idea that volcanic chains like the Hawaiian Islands result from the slow movement of a tectonic plate across a ""fixed"" hotspot deep beneath the surface of the planet.\nGeologists have identified some 40–50 hotspots around the globe. Of these, Hawaii, Réunion, Yellowstone, Galápagos, and Iceland overlie those that are currently most active.\nBy studying hotspots, geologists have been able to track the movement of the Earth\'s crustal plates. In addition, by examining lava flows from a series of volcanic eruptions, they have come to the conclusion that there is a reversal of Earth\'s magnetic poles at intervals of about 5,000 years.\nFor a long time, geologists have thought that a hotspot is caused by a narrow stream of hot mantle, called a mantle plume, rising up from near the Earth\'s core-mantle boundary. With the formation of a plume, magma builds up under a tectonic plate, eventually erupting with basaltic rock, forming what are called Large Igneous Provinces (LIPs). The resultant volcano then continues to pour out lava.\nHowever, recent evidence indicates that the upper mantle is hotter and more fluid than what was once believed. As a result, a number of geologists attribute hotspots to convection in the upper mantle, formulating an alternative to the plume model. It has been suggested that the term ""hotspot"" be replaced by the term ""melting anomaly.""\nEither model provides an explanation of how volcanoes can occur far from the edges of tectonic plates, where plates are subducted into the mantle.\nAlong with these models, there has been a revival of what has been called the ""antipodal pair impact hypothesis."" It is the idea that pairs of opposite hotspots may result from the impact of a large meteor.\nMost hotspot volcanoes are basaltic because they erupt through oceanic lithosphere (such as Hawaii, Tahiti). As a result, they are less explosive than subduction zone volcanoes, in which water is trapped under the overriding plate. Where hotspots occur under continental crust, basaltic magma is trapped in the less dense continental crust, which is heated and melts to form rhyolites. These rhyolites can be quite hot and form violent eruptions, despite their low water content. For example, the Yellowstone Caldera was formed by some of the most powerful volcanic explosions in geologic history. However, when rhyolitic magma is completely erupted, it may eventually turn into basaltic magma because it is no longer trapped in the less dense continental crust. An example of this activity is the Ilgachuz Range in British Columbia, which was created by an early complex series of trachyte and rhyolite eruptions, and late extrusion of a sequence of basaltic lava flows.\nAs the continents and seafloor drift across the mantle plume, ""hotspot"" volcanoes generally leave unmistakable evidence of their passage through seafloor or continental crust. In the case of the Hawaiian hotspot, the islands themselves are the remnant evidence of the movement of the seafloor over the hotspot in the Earth\'s mantle. The Yellowstone hotspot emerged in the Columbia Plateau of the US Pacific Northwest. The Deccan Traps of India are the result of the emergence of the hotspot currently under Réunion Island, off the coast of eastern Africa. Geologists use hotspots to help track the movement of the Earth\'s plates. Such hotspots are so active that they often record step-by-step changes in the direction of the Earth\'s magnetic poles. Thanks to lava flows from a series of eruptions in the Columbia Plateau, scientists now know that the reversal of magnetic poles takes about 5000 years, fading until there is no detectable magnetism, then reforming in near-opposite directions.\nHotspot volcanoes should not be confused with island arc volcanoes. Although each will appear as a string of volcanic islands, island arcs are formed by the subduction of converging tectonic plates. When one oceanic plate meets another, the denser plate is forced downward into a deep ocean trench. This plate releases water into the base of the overriding plate as it is subducted, and this water causes some rock to melt. This process results in a chain of volcanoes, such as the Aleutian Islands near Alaska and Sweden.\nBelow is a list of hotspots around the globe. The numbers in parentheses correspond to the numbered hotspots in the figure on the right.\nAll links retrieved January 14, 2018.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nThe history of this article since it was imported to New World Encyclopedia:']"	['<urn:uuid:ef6a7373-020a-46ae-9191-3b2b13fac242>', '<urn:uuid:ec04f0c5-eed2-4f83-af55-bccc25a0621d>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	7	60	1746
25	container materials impact wine quality difference	Different winemaking vessels have significant impacts on wine characteristics. Oak barrels impart woody notes, vanilla, and baking spice aromas, while allowing oxygen to soften tannins. Stainless steel is neutral and nonporous, preserving crisp, fruit-focused flavors without adding aromas. Concrete vessels offer a middle ground, allowing oxygenation without imparting flavors, and their egg shape can enhance mouthfeel through lees circulation. Clay amphorae create wines with tension and energy due to lower fermentation temperatures, and can add a chalky minerality to the palate.	['Winemakers often include technical details in their tasting notes about the fermentation and aging vessels used during the production process, like French oak barrels, concrete vats, or stainless-steel tanks. Fascinating, but what does that mean in the glass?\nThe winemaking vessel has a surprisingly large impact on a finished wine, mainly depending on the material used to make it. Because stainless steel, oak, concrete, and clay have different properties, each contributes distinctive characteristics to wine. Here’s a look at how they reveal themselves in a wine’s aromas, flavors, and textures.\nMuch like cooking a steak over a wood-fired grill gives the meat a smoky, caramelized flavor, aging wine in oak barrels imparts woody notes. This may translate to aromas of vanilla and baking spices, or flavors of caramel and toffee. Because wood is porous, barrels allow oxygen to enter the vessel, softening the wine’s tannins and enhancing aromas. Other factors, like the wood’s origin, the age of the barrel, and toast level, also make their mark.\nAmerican oak is bolder than European, with prominent vanilla and coconut notes, while French oak is more subtle and spicy. The greater the barrel’s toast level, the more pronounced the oak aromas and flavors in the wine. The amount of time a wine spends in the barrel also matters. Fermentation lasts just weeks compared to many months for aging, so a wine that is barrel-fermented but not barrel-aged will have less oak impact.\nStainless steel, unlike oak, is known for what it doesn’t contribute to wine. As a neutral, nonporous material, it adds no aromas or flavors, giving wine a crisp, fruit-focused character. Since stainless tanks don’t allow oxygen exchange, wine develops more slowly than it does in barrels.\nIt was these attributes that inspired Marimar Torres, proprietor at Marimar Estate Vineyards and Winery in California’s Russian River Valley, to create an all-stainless version of the winery’s oaked Chardonnay, starting in 2005.\n“I wanted to make a wine that would really highlight the fruit without the influence of the barrel,” says Torres. She named the wine Acero, the Spanish word for steel. “While the barrel-fermented and aged Chardonnay has more richness and complexity, Acero emphasizes brightness and purity of fruit.”\nFor winemakers seeking the best of both worlds, concrete vessels combine the attributes of oak barrels and stainless-steel tanks.\n“Unlike stainless, concrete is porous and allows for some oxygenation while not imparting any flavor, like oak does,” says Leslie Renaud, principal winemaker at Martin Ray Winery in the Russian River Valley, who makes Chardonnay in both concrete tanks and French oak barrels. “The oxygen exposure also allows for faster maturation than stainless.”\nWhile concrete wine vats have been around for hundreds of years, egg-shaped versions are a modern invention. Renaud favors them because their oval configuration naturally circulates the wine and keeps the spent yeast cells, or lees, in suspension. “This gives a perception of more mid-palate weight in the wine, so it’s less austere than it would be with a stainless-steel fermentation,” Renaud explains. “Concrete provides a more complex mouthfeel.”\nShaped by clay\nAmphorae, clay vessels whose origins date back thousands of years, share some characteristics with concrete tanks. Amphorae have a somewhat ovoid shape, pointed or tapered at the bottom, which allows the lees to circulate during fermentation.\nHowever, there are differences between concrete eggs and amphorae, says Andrew Beckham, winemaker and proprietor at Beckham Estate Vineyard in Oregon’s Willamette Valley. Beckham, who is also a ceramics teacher and artist, has been making his own amphorae since 2013, and now uses them for nearly all of his production.\n“We find that when we’re fermenting, and even when we’re aging the wines, a lees bed forms from the top of the container to the shoulder of the vessel,” Beckham says. “This is very unlike an egg, where the lees are down in the bottom of the wide portion of the vessel.”\nThis results in greater surface exposure with the lees, he adds, which builds a more supple, soft texture in the mouth. “It’s maybe like putting your tongue on a wet river stone or brick,” notes Beckham. “No aroma or flavor is imparted, but there’s a slight minerality and a dustiness on the palate. A chalkiness that’s coming from the clay.”\nHe also points out that, with amphorae, the fermentation process happens at a lower temperature and takes longer. “Because they haven’t gotten as hot,” says Beckham, “wines in amphorae have more tension and energy, and they’re brighter.”\n5 wines to try:\nThese wines show the impacts of different vessels in the glass:\nWith aromas of jasmine, green apple, and spice, this bright and beautiful wine was fermented and aged entirely in stainless steel tanks. The purity of fruit from the Don Miguel estate vineyard shines through, with lemon and mineral notes chiming in. The wine’s creamy texture comes courtesy of 100% malolactic fermentation.\nMade with fruit from the same vineyard as Acero, except the Chardonnay was fermented in 39% new French oak barrels, then aged in the barrel for nine months. The jasmine aroma is less pronounced, joined by soft vanilla and melon. Tangy and rich in the mouth, La Masia tastes like a liquid lemon meringue pie.\nFermented in egg-shaped concrete tanks, then left to rest in concrete for 10 months, this Chardonnay smells like fresh ripe peaches, with light floral accents. It has zingy acidity, with bright lemon zest and peach flavors. The texture is remarkably different from the Acero, much more mouth-filling, thanks to the natural lees suspension that occurs in the egg.\nFermented in open-top stainless-steel tanks. Then aged 15 months in neutral French oak barrels, so the oak impact on flavor and aromas is minimal. The wine has a slightly woody aroma with spicy accents, and the palate bursts with bright red cherries. Elegant with a lovely lightness, the wine has well-integrated tannins and lively acidity.\nMade from the same fruit as the estate Pinot Noir, but fermented and aged in amphora. Light garnet in color compared to the estate wine’s bright ruby, with pretty floral and red fruit aromas. Fruit flavor leans more toward red raspberry than cherry, with a bit more intensity and complexity. The wine finishes with a slight dustiness.']	['<urn:uuid:9c16dab4-f06e-44df-8fad-b4340f2eca36>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	6	81	1036
26	tichelaar makkum ironstone china production methods	Tichelaar Makkum and Ironstone china represent different pottery traditions and manufacturing methods. Tichelaar Makkum uses a traditional two-step baking process with tin glazing, applying finely ground oxides on top of the glaze for decoration. Ironstone china, developed by Charles Mason in 1813, is a heavy hard earthenware made stronger by adding iron slag to the mixture, allowing for the production of larger objects resistant to breakage. The methods serve different purposes - Tichelaar's process focuses on achieving decorative brilliance, while Ironstone's method prioritizes durability.	"['Click on a category below to show all the entries for that category.\nLearn about and understand the items, manufacturers, designers and periods as well as the specialist terms used in describing antiques and collectables. Either click one of the letters below to list the items beginning with that letter, or click on a category on the left side of the screen to list the items under that category.\nFounded in 1881 in the Finnish town of the same name, littala specialised on tableware and cookware.\nIittala took Finnish glass design upmarket in the 1930s, with Aino Aalto\'s glasses and Alvar Aalto’s iconic modernist Savoy vase designed, in 1936.\nBetween the 1930s and 1950s the company drew on the talents of leading Finnish designers with the aim of targeting the international design-conscious and giftware market.\nAt the 1951 and 1953 Milan Triennale exhibitions, two of IItala\'s designers, Tapio Wirkkala and Timo Sarpeneva were awarded the Grand Prix and this confirmed littala alongside Sweden’s Orrefors as leaders in the field of sophisticated glass design.\nHowever, littala continued to produce its more popular designs over several decades, and this makes valuation of these items more complicated.\nAs an example, Alvar Aalto’s Savoy vase has remained in constant production since 1937, and second-hand pieces are invariably cheaper than new ones.\nSimilarly, Sarpaneva’s Orkidea (Orchid) vase, designed in 1953, has been reproduced over several decades whereas his Lancetti, designed in 1952, was made for only five years.\nThe result is that the value of the Savoy vase is relatively low compared to the Orkidea vase.\nOne of the most popular and collected of the Japanese porcelains is Imari. Imari is in fact a European name for export porcelain produced in the town of Arita in the Hizen province of Japan. It was shipped through the nearby port of Imari from the second half of the 17th century and the first half of the 18th century. Pre-export period Imari is called Shoki-Imari.\nThere are two distinct styles of Arita or Imari porcelain.\nFirstly there is the rare and highly sought after Kakiemon porcelain. It is sparsely decorated predominantly in coral red on a very fine white glaze. Highlight colours include yellow, green and aubergine Kakiemon wares are of a consistently high standard and command very high prices\nIn contrast, the more commonly found Imari in the west is called brocaded Imari or Kinrande Imari, and is usually richly decorated with flowers, foliage and figures. These pieces have an overall floral decoration reminiscent of a rich silk textile, and typical colours are underglaze cobalt blue and iron red, which is highlighted with colours such as gold, green, aubergine and yellow. There is a great variation in quality, ranging from quite crude though decorative wares to very finely painted wares.\nItems exported to the West included garnitures of vases, plates, chargers, figures as well as utilitarian wares. Due to its popularity and success, Imari was widely imitated both in China and the West. English factories who produced Imari or ""Japan"" patterns as they were sometimes known included, Bow, Derby, Minton, Spode, Worcester and Mason\'s. European factories included Meissen, Chantilly and Delft.\nA technique where pigment is applied to a ceramic surface so that it stands out from the glazed surface in slight relief.\nImperial yellow, also called ""Chinese Yellow"" and ""Royal Yellow"" is an auspicious colour in Chinese culture.\nIt was the colour of Imperial China and the symbolic colour of of the five legendary emperors of ancient China. The colour was used to decorate royal palaces and used in the clothing of the emperors.\nImportant is a word used in the antique trade to indicate an object should be ranked above other similar objects, and is therefore more valuable.\nThe object could be considered important because it is by a famous designer or maker, has been shown at a major exhibition, is of exquisite workmanship, is rare or is a ""one-off"", was made for an important patron, and so on.\nEven further up the pecking order are objects that are described in catalogue descriptions as highly important or extraordinarily important.\nA record of a name, date or inscription, or a decoration scratched into a surface, usually of a glass or ceramic item with a blunt instrument to make a coarse indentation. Compare with engraving where the surface is cut with a sharp instrument such as a metal needle or rotating tool to achieve a fine indentation.\nDecorative patterns inserted into the main body of a piece of furniture, generally in wood of contrasting colour and grain, though brass, ivory, ebony, shell and sometimes horn have been used. Inlay may consist of a panel of well figured timber inset into a cabinet door front, geometric patterns, or complex and stylized designs of flowers, swags of foliage, fruits and other motifs. As a general rule, in pieces where the carcase is constructed in the solid, the inlay is relatively simple such as stringing, cross banding and herringbone banding. Where more elaborate and decorative work was required veneer was used. Inlay has been fashionable from at least the latter half of the 17th century, when a variety of elaborate forms were developed\nA term used to define a method of decoration most common in glassmaking and jewellery which involves engraving, carving or moulding an image into the background, leaving an impression, and leaving the top surface of the item flat, the opposite to relief carving, and also know as counter-relief.\nWhen used in relation to wooden items, intarsia is the Italian term for inlaying, where the background timber is cut away and and a selection of timbers of different colours and grains are inserted to form a picture or pattern.\nCredit for the invention of ironstone china is generally accorded to Charles Mason in 1813. Charles Mason was one of the two sons of Miles Mason who founded the Mason works in 1802, and ran the business with his brother George Mason.\nIronstone was a heavy hard earthenware which was slightly translucent, its strength supposedly coming from a very small quantity of iron slag added to the mixture. The additional strength enabled the company to make larger objects that were not susceptible to breakage.\nThe company manufactured dinner wares, toilet sets, tureens, jugs and so on, and the most popular patterns were blue and white, floral and Oriental Imari style colours.\nThe trade name ""Patent Ironstone China"" was registered by the company in 1813, but the patent was only valid for 14 years and was not renewed, enabling other potteries to use the word ""ironstone"" in describing their wares.\nMason wares are generally well marked with ""Mason\'s Patent Ironstone China"" .\nMost commonly used to describe the finish in glass and ceramics, an iridescent finish is one that subtly changes colour when moved and the light strikes it at a different angle. In glass, objects by Louis Comfort Tiffany in the United States, and Loetz in Bohemia often have irridescencent finish. Examples of ceramics with an irridescent finish are the Wedgwood Fairyland lustre range, the Doulton Flambe range, and the output of the Hungarian manufacturer Zsolnay.\nIsfahan is ancient capital of Persia, located about 450 km south of the Tehran, the present capital of Iran.\nThe city was the site of the royal carpet manufactory during the Safavid era, and noted for its rich silk rugs that often incorporated gold and silver thread. The Safavid dynasty lasted from 1502 to the early 1700s, when the country was invaded by Afghans, at which time the craft of weaving became stagnant.\nThe craft was revived in the 1920s, with the region again producing fine quality rugs, and one of the most popular designs incorporating a central medallion with floral borders.\nIvory is a hard white material that comes from the tusks of elephants, mammoth, walrus and boar, or from the teeth of hippopotamus and whales. The ivory from the African elephant is the most prized source of ivory. Although the mammoth is extinct, tusks are still being unearthed in Russia and offered for sale.\nIvory has been used since the earliest times as a material for sculpture of small items, both in Europe and the east, principally China and Japan.\nIn Asia ivory has been carved for netsuke, seals, okimono, card cases, fan supports, animals and other figures and even as carved tusks.\nIn the last 200 years in Europe ivory has been used to carve figures, for elaborate tankards, snuff boxes, cane handles, embroidery and sewing accessories, in jewellery and as inlay on furniture. Its more practical uses include being used for billiard balls, buttons, and a veneers on the top of piano keys.\nThe use and trade of elephant ivory have become controversial because they have contributed to Due to the decline in elephant populations because of the trade in ivory, the Asian elephant was placed on Appendix One of the Convention on International Trade in Endangered Species (CITES), in 1975, and in January 1990, the African elephant was similarly listed. Under Appendix One, international trade in Asian or African elephant ivory between member countries is forbidden. Unlike trade in elephant tusks, trade in mammoth tusks is legal.\nSince the invention of plastics, there have been many attempts to create an artificial ivory\nIznik (or Isnik) is a town in Turkeywhere pottery has been made from the 13th to the 19th century.\nThe pottery is typically painted with cobalt blue and green floral and geometric decorations on a white ground.\nSome Europpean potteries has produced designed inspired by and named after the Turkish originals.', 'Royal Tichelaar Makkum Decorative Earthenware Anno 1641\nThe family business that has been passed on from father to son for nearly four centuries, is now led by Jan Tichelaar.\nOn the unglazed edge on the bottom of the product shown in the picture alongside shows that Tichelaar used yellow baking clay. Afther the first baking-process, a masking layer of tin glaze is applied, a process that was already in use in the 16th century in the area.\nThey painted the earthenware with various finely ground oxides on the tin glaze. Then they were baked a second time in which the colors appear. The special character of the Tichelaar Makkumer earthen ware is produced by the self-made, mysterious tin-glaze which gives this pottery its gloss and brilliance.\nUnlike the Delft pottery where they switched to using white baking clay in the mid 18th century, Tichelaar has never deviated from this ancient technique. This is also the great difference between the pottery of “The Royal Delft” and the pottery of “Tichelaar”.\nBecause the painting is done on the glaze, the pictures “flows” somewhat into the glaze during the baking-process. In Delft they paint under the glaze layer, causing the pictures to be slightly more pronounced and sharper.\nThe logo embedded in the glaze on the bottom consists of a shield with the word Makkum and underneath\nit two T’s crossed the brothers Tichelaar. Along with the signature of the painter, this is the hallmark of the only factory of Makkum pottery from the 17th century: “Royal Tichelaar”.\nIn february of 2013 Tichelaar Makkum changed from producing traditional pottery to design.\nThe pottery of Tichelaar Makkum is located in a typical Dutch Frisian port town on the IJsselmeer coast named Makkum.\nMakkum Blue Earthenware\nThe blue painting with cobalt oxide gets, because of the fusion with the underlying glaze, a brilliance that has been impossible to make with later decoration techniques.\nThe decorative pottery is created with love and each piece is entirely handpainted. No print or stamp techniques thereby helps the painter.\nIn the map Makkum/Makkum Sale you’ll find the collection.\nMakkum Colored Earthenware\nFor colored Royal Tichelaar Makkum pottery besides the blue the colors yellow green and red are used.\nThe knowledge about the application of these Majolika colors came in the 17th century from China via Italy and Spain, Valencia, Faience technique to the Netherlands.\nYou will find this colorful collection in the map Makkum/Makkum Sale.\nMakkum White Earthenware\nWhat makes white Makkumer Tichelaars pottery so special is the masking white tin glaze.\nThe White Makkumer is made in the same traditional way as the rest of their pottery, only it is not painted.\nIn the map Makkum/Makkum Sale you will find some sample-products.\nMakkum Tiles in list\nTichelaar was in the 17th century the name of the tile and brick baker (in a tile furnace). The current descendants owe their name due to their profession from a distant past, a job that they are still doing today and with amazing skill.\nIn many historic buildings we can still find the hand-painted tiles behind stoves and fireplaces, manufactured in the same way and covered with tin glaze.\nIn the map Makkum Sale you will find some samples of decorated tiles in oak-frames.']"	['<urn:uuid:8d3aa5a9-4beb-49cc-8a55-c52df6c30939>', '<urn:uuid:084cd6a4-73da-4029-9f7e-e991e3dd5989>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	6	84	2137
27	mitre attack vulnerable components owasp ranking threat intelligence connection	The connection between MITRE ATT&CK, vulnerable components, and OWASP rankings involves threat intelligence and risk assessment. OWASP has elevated vulnerable components from #9 to #6 in its 2021 rankings due to increased exploitability concerns. Organizations can use MITRE ATT&CK's knowledge base to identify specific threat groups and their TTPs (Tactics, Techniques, and Procedures), while correlating this information with vulnerable components to drive focused remediation strategies. This integration allows security teams to better understand adversary behavior and prioritize defensive responses based on real-world cybercriminal activity.	['What that means for your threat landscape, teams, and tools\nThis month and next, Waratek Founder & CTO John Matthew Holt discusses the changes in the 2021 OWASP Top Ten list of web application risks. This week’s focus is on the move up the rankings from #9 to #6 of known vulnerabilities. The following is an edited version of John Matthew’s comments. Listen to the full commentary (6:51).\nQ: What is the most significant change in the 2021 OWASP Top 10 from the previous list?\nIn the 2017 list, there were two of the 10 categories related to injection. Why? Because if you’re measuring by incidents, those are different types of vulnerabilities that have different counts. Makes sense, but it doesn’t really help you on a strategic view. There’s a different ratio of risk there, and that wasn’t captured well in the 2017 rankings.\nThe change that has taken place in the OWASP rankings between ‘17 and ‘21 is to move to a mentality that’s thinking about exploitability. If you only solve all of the highly exploitable, high-impact vulnerabilities and even don’t address anything else, you immediately can sleep easier at night than the person who addresses, you know, the top three incidents’ vulnerabilities with no regard to exploitability.\nQ: You believe moving previous category #9 – Using Components with Known Vulnerabilities to the #6 – Vulnerable & Outdated Components position will have far reaching impacts. Why?\nVulnerable and outdated components is the category that captures, among other things, third-party components. What we have seen over the last 10-odd years is the ratio of business logic code to third-party components has been increasing in favor of third-party components. But, there’s a big security gremlin hiding in that detail.\nFrom an economies of scale perspective, malicious operators get bigger bang for their buck, no pun intended, by finding vulnerabilities in invulnerable and outdated software components that are part of application supply chains and with no business logic. So that’s important. And I think I think what we’re seeing with the rise of vulnerable, outdated components from number nine in 2017 to number six in 2021 is that thematic element coming through.\nQ: What’s stopping teams from fixing vulnerable components?\nWhilst there are a range of developer tools that target vulnerable and outdated components, things like software composition analysis (SCA) tools as an example, they are all developer oriented. But, you can’t just pluck one vulnerable, outdated component and drop a newer version of that component in an application, which has a 99 other components, with the expectation it will work seamlessly the next day.\nIt doesn’t work because that new version of the component, which fixes the vulnerability, has some other unintended changes which break compatibility. This is a real problem, and there’s no tooling to deal with that side of the problem. When SCA works, it works well; but when it doesn’t – and increasingly it doesn’t – it’s hard to make it work reliably at scale. Then, suddenly, people are tooless and to some degree, helpless because they’re trapped on vulnerable, outdated components.\nQ: How does changing the OWASP Top Ten List help address the issue of vulnerable components?\nThis is going to ripple down to developers to help them re-orient as they are learning about security. Both old developers, new developers learning about security will re-orient them to think about what’s important in security. I think it helps security practitioners as well to sharpen their focus on what they really need to care about. Also even for vendors, for tech companies like Waratek, they should be focusing their efforts on the highly exploitable, high-impact vulnerabilities so that they can give the biggest bang for the buck to their customers in terms of addressing and removing risk.\nListen to John Matthew’s full comments on the Vulnerable & Outdated Components category in the 2021 OWASP Top Ten (Time to Listen – 6:51)\nDecember 2nd: The problem with updating legacy components & the 2021 OWASP Top Ten', 'Threat actors are constantly evolving their tactics and techniques in the attack lifecycle and infiltrate company infrastructure. While most organizations are already performing vulnerability management based on CVEs by MITRE, few have considered the powerful correlations between threat intelligence, CVEs and the ATT&CK® framework. In this blog we highlight the benefits of bringing them together to drive focused remediation and improve cyber defense.\nWhat is the MITRE Att&CK framework?\nMITRE ATT&CK® is a globally accessible knowledge base of 14 adversary tactics and over 500 techniques based on real-world observations. The first model was proposed in 2013 and publicly released in 2015 which has been gathering momentum over the last couple of years. The MITRE ATT&CK framework provides an industry leading standard to help organizations develop, organize and use a risk-based strategy to inform defense strategies – that can be communicated in a standardized way across organizations and vendors to drive effective risk assessment based on observed incidents.\nIn the past, security teams have struggled to understand their entire attack surface and verifying attack methods, leading to them falling victim to cyber-attack from a false sense of security and overconfidence in their ability to defend against it. The ATT&CK project came along with the goal of eliminating this problem – the knowledge base was born to create a clear structure by providing a categorized list of all known attack methods and marrying it with threat intelligence on groups that use them.\nIn addition, the ATT&CK framework identifies critical information on the software hackers will look at to implement an attack and provides direction on the most effective defense measures to reduce this risk. The objective of the ATT&CK framework is to provide an evolving list of techniques used by adversaries so that security teams can trust and use with confidence to defend against them.\nUsing MITRE ATT&CK framework with threat intelligence\nOrganizations can use the ATT&CK framework to form conclusions based on verified data and the kill chain structure to improve prioritization and remediation strategies based on observations from real-world cybercriminal activity. Gartner’s analysis of the framework says the criteria for defining Tactics, Techniques, and Procedures (TTPs) in MITRE’s data provides an in-depth knowledge base of attack intelligence – making it easier and more straightforward to apply these into your own investigations.\nOrganizations can start by looking at a specific threat groups with an interest in stealing your data or assets based on who they’ve previously targeted. Once the threat groups of interest are identified from MITRE, you can leverage the insights to look at the specific TTPs being used. By understanding common TTPs from groups who you think will attack your organization, you can begin to form a prioritized list of detection and prevention controls that your security teams need to put in place and to reduce risk.\nFor more mature organizations, you can leverage threat intelligence to enrich what is already known about these groups by linking attack patterns and behaviors from specific threat actor campaigns, tools and feeds this information in an automated and usable way to help focus defensive activities. Identifying if specific tools and/ or software are being used, and whether these are reliant on any known vulnerabilities to gain access and establish a foothold.\nHaving access to this level of information allows businesses to better understand adversary behavior, campaigns and targets – including planned attacks on a specific company or sector, and advanced knowledge on the TTPs threat actors are using – to drive defensive responses and preventive action strategies ahead of time to deal with potential exploits.\nMapping MITRE ATT&CK to CVEs\nWhile the ATT&CK framework and threat intelligence seem a natural fit, can it be applied to CVEs? Historically threat management and vulnerability management have been seen as separate disciplines, but as the vast majority of attacks in the wild target a handful of CVEs, there’s a strong case for linking CVE exploits to what the attacker is trying to achieve. Let’s look at how traditional vulnerability management can be improved with threat intelligence and TTPs from the ATT&CK framework:\n“Find and fix” game – traditional vulnerability management takes a ‘find and fix’ approach by scanning infrastructure and assessing for vulnerabilities, and using the CVSS severity score to prioritize remediation. Despite being a severity indicator, CVSS score is static and limited – as it doesn’t take the external threat context into account and has no links to critical assets within your business, meaning you could be wasting time on fixing vulnerabilities that doesn’t pose a risk in the first place. So it’s only good for less mature organizations with smaller and static digital estates.\n“Vulnerability risk” game – level 2 is risk-based vulnerability management. This approach enables organizations to better understand the asset exposure with added threat intelligence to include information on whether a vulnerability is being exploited in the wild or how likely it will be exploited – essentially a vital prediction to help drive proactive remediation by surfacing the most dangerous and imminent risks first to aid vulnerability prioritization and shorten exposure time. Ideal for organizations with larger estates and security teams overwhelmed by the growing number of CVEs to remediate.\n“Threat vector” game – level 3 is about understanding how the attacker uses vulnerabilities to achieve their goals and linking these to TTPs from the MITRE ATT&CK framework. This approach starts with the attacker and uses threat intelligence to evaluate who may pose a risk to your organization, combining that with the MITRE ATT&CK framework to understand how they can compromise your organization i.e. TTPs, and then assess how a CVE can impact you across the attack path. This advanced approach means you can map and narrow down risks against your own list of hacker centric criteria, such as geographical spread, specific sectors and the types of organizations being targeted. Many security tools now come with signature sets already classified into categories that label alerts with the corresponding ATT&CK tactics and techniques they represent. This classification makes it easy to immediately start creating metrics and labeling the activity security teams can be alerted to with verified information from the ATT&CK techniques framework to direct effective remediation. This is the most effective form of threat intelligence—information sourced from actual attacks that have already occurred and categorized by MITRE, providing vital intelligence that can be fed into your vulnerability risk management process easily and automatically – empowering security teams to act quickly and decisively.\nThis hacker centric approach helps sift down millions of CVEs to imminent threats and drives a more proactive approach to vulnerability remediation which is crucial in the race against Ransomware. This was the case for CVE-2017-0144 (WannaCry vulnerability in 2017) and how TTPs were detected to show the threat of ransomware and mapping this to the vulnerability from MITRE to identify the three areas for patching (active scanning; file and directory discovery and remote system discovery). This might’ve been scored as a medium/ high criticality vulnerability by traditional vulnerability management methods using CVSS, however the additional threat intelligence information provides a quick win from a remediation perspective to prevent elements of this ransomware from taking hold inside an organization.\nTo advance your vulnerability management program it’s important to use both views from a risk-based angle and threat intelligence angle to understand what risks exist and how threat actors can compromise your organization to create a remediation ‘sweet spot’.\nWith improved understanding and data it’s possible to map CVEs to the MITRE ATT&CK framework and spot areas where the attack chains exist – allowing businesses to get ahead of exploits which could lead to ransomware and malware attacks.\nFinally it’s important to implement changes in a way that matches your organizations maturity level, size and risk appetite. Then thinking about how you can utilize advanced information from MITRE to analyze vulnerabilities from all angles (threat vectors and risk based views) to drive targeted remediation which isn’t always as effective with a traditional CVSS model']	['<urn:uuid:c4c776bf-30c0-42ad-831b-c77bccb5274f>', '<urn:uuid:7b02577f-f743-4c97-b513-b15fd84a24ab>']	factoid	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-12T20:35:02.547110	9	84	1991
28	breaking down graduate level immunology research areas seattle top institutions	Seattle combines leading institutions in immunology research, including the Fred Hutchinson Cancer Center, UW Departments of Microbiology and Immunology (ranked in top 10 by US News and World Report), UW Center for Innate Immunity and Immune Diseases (CIID), and Benaroya Research Institute which specializes in autoimmune disease research. Their work focuses on understanding innate and adaptive immunity, host-pathogen responses, and autoimmunity.	['To understand diverse microorganisms, their pathogenesis and resultant immune responses, research in MCB probes microbial physiology, the composition and function of microbiomes, innate and adaptive immunity, and translational strategies. This research and work helps us predict infection epidemics, design better therapies and reengineer host and microbe for improved health and industry.\nArea Directors help advise students about classes and rotations in their interest area. They also provide a listing of suggested courses for those interested in Microbiology, Infection & Immunity.\nFaculty Area Directors\n- Michael Lagunoff (email@example.com)\n- Anthony Rongvaux (firstname.lastname@example.org)\nStudent Area Directors\n- Amin Addetia (email@example.com)\n- Tayla Olsen (firstname.lastname@example.org)\nThe suggested curriculum outlined below is meant to guide you in choosing classes, they are not requirements. We highly encourage you to take the Foundational courses, while the Electives are more specialized and often cross between Areas of Interest. Remember to review the UW Time Schedule for the most accurate and up-to-date information regarding whether a course is currently being offered.\n2022-2023 Suggested Curriculum (document download)\nThis track is broadly divided into the related sub-tracks of immunology, virology, and bacteriology. The foundational courses include two courses focused on each sub-track, denoted as 1=Immunology, 2=Virology, and 3=Bacteriology. Interested students can focus on one sub-track or mix and match from these sub-tracks depending on their specific area of research. Area directors or more senior MCB students can discuss these sub-tracks with interested first-year students.\nFoundational Course 1A: IMMUN 532 – Intersection of innate and adaptive immunity in disease (“Advanced Immunology”)\nThis is the primary graduate-level survey of immunology. Many lectures are given by guest lecturers from the Dept. of Immunology who are renowned experts in these topics. Lectures are complemented by discussion and critique of relevant primary literature. Prerequisite: Undergraduate immunology course (e.g. IMMUN 441), or equivalent.\nOffered WIN, 4.0 credits, Offered every year\nFoundational Course 1B: IMMUN 537 – Immunological Methods\nThis course covers key methods required for immunological research. Prerequisite: Undergraduate Immunology course (e.g. IMMUN 441), or equivalent.\nOffered WIN, 4.0 credits, Offered every year\nFoundational Course 2A: MCB 532 – Human Pathogenic Viruses\nReplication, regulation, and pathogenesis of several groups of human viruses, including human immunodeficiency virus and papillomaviruses. Emphasis on the unique aspects of the viral-like cycles as they relate to effects on infected cells and organisms. Guest lecturers focus on viral immunology, measles. herpes simplex virus, and HHV-8.\nOffered AUT, 3.0 credits, Offered odd-numbered years, Will be offered in AUT 2023\nFoundational Course 2B: MICROM 540 – Virology\nThe molecular biology, transmission, and pathogenesis of human viruses will be explored. In addition to general principles of virology, lectures and paper discussions will focus on specific human pathogens including HIV, herpesviruses, ebolaviruses, alphaviruses, and adenoviruses, among others.\nOffered AUT, 3.0 credits, Offered even years\nFoundational Course 3A: CONJ 558 – Prokaryotic Biology\nBasic principles in prokaryotic cell structure, genomics, and metabolism. Introduction to prokaryotic physiology, bacterial pathogenesis, and microbial ecology.\nOffered WIN, 3.0 credits, Course not currently being offered\nFoundational Course 3B: CONJ 558 – Molecular Interactions of Bacteria with their hosts\nThe processes bacteria employ to shape interactions with their hosts will be explored in molecular detail through selected examples in the literature.\nOffered SPR, 3.0 credits, Offered odd-numbered years\nCONJ 539 – Modern Approaches to Vaccines\nCovers selected topics based on recent publications in viral and bacterial vaccine research. Emphasizes understanding the latest advanced and issues in vaccine discovery, mechanisms of action, and special topics in viral vaccines.\nOffered AUT/SPR, 1.5 credits, Offered every year\nCONJ 549 – Population Biology of Microorganisms\nPrinciples of ecology and evolution as they apply to microorganisms.\nOffered SPR, 1.5 credits, Offered even-numbered years\nCONJ 557 – Microbial Evolution\nSelected topics in microbial evolution including evidence for early life on Earth, molecular mechanisms of bacterial and viral evolution, speciation, adaptive niche differentiation, bioinformatics tools to detect selection, and evolution of the virulence and pandemic spread. Prerequisite: MICROM 412 or general biology background.\nOffered AUT/SPR, 1.5 credits, Offered every year, Course not currently being offered\nGLOBAL HEALTH 566 – Biochemistry and Genetics of Pathogens and Their Hosts\nProvides a strong foundation in biochemistry, molecular biology, and genetics for students interested in disease. Principles illustrated through examples focusing on pathogens, and infectious and non-infectious disease. Note: offered jointly with PABIO 551A.\nOffered AUT, 3.0 credits, Offered every year\nIMMUN 441 – Basic Immunology\nThis is an undergraduate class that presents a complete introduction to immunology. MCB students interested in this topic who have not taken a basic immunology course are encouraged to take or audit this course in preparation for more advanced immunology courses. Students must obtain approval from the MCB Co-Directors for this 400-level class to count toward their 18-graded credits.\nOffered AUT, 4.0 credits, Offered every year\nIMMUN 538 – Immune-based diseases and treatments\nThis course focuses on the role of the immune system in both causing and resolving disease. Topics include autoimmune disease, infection, and cancer immunology. Each class includes both a lecture component and a discussion of relevant primary literature.\nOffered SPR, 2.0 credits, Offered every year\nPABIO 552 – Cell Biology of Human Pathogens and Disease\nCell biology and immunology explored through diseases of public health importance. Examples of pathogen interaction with host cell biology and immune systems, unique aspects of the cell biology of pathogens, perturbations of these systems in non-infectious diseases, and design of therapeutics and vaccines to combat diseases of public health importance.\nOffered WIN, 3.0 credits, Offered every year\nMicrobes and the fight to contain them\nSeattle MCB combines leaders in microbiology, immunology, host-pathogen response and autoimmunity from premier institutions and departments including:\n- Fred Hutchinson Cancer Center\n- UW Departments of Microbiology and Immunology, which consistently rank in the top 10 programs for graduate studies according to the US News and World Report.\n- UW Center for Emerging and Reemerging Infectious Disease (CERID)\n- UW Center for Innate Immunity and Immune Diseases (CIID)\n- Benaroya Research Institute is a leader in autoimmune disease research.\n- Seattle Children’s Research Institute Center for Global Infectious Disease Research has intentional renowned scientists in the field of HIV/AIDS, tuberculosis, malaria, and emerging infectious diseases.\n- Institute for Systems Biology (ISB) explores environmental microbes and the host-pathogen interface.\n- Pacific Northwest Research Institute explores the genetics of human health and disease, including research on diverse infectious agents such as yeast, transposons, and transmissible cancers.']	['<urn:uuid:50708a70-75fc-41e6-9a54-99b964aa50f6>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	10	61	1058
29	pharmacist looking for details rimantadine administration timing contraindications interaction vaccines storage requirements	Rimantadine should be taken at evenly spaced times day and night to maintain constant blood levels. The oral liquid should be well-shaken and stored at room temperature for up to 14 days. Regarding vaccine interactions, live virus vaccinations should not be given for 2 days after stopping rimantadine, and rimantadine should not be administered within 2 weeks after receiving live vaccines, including FluMist. The drug must be stored in a closed container at room temperature, away from heat, moisture, and direct light, and kept from freezing. The final dose should not be taken near bedtime as it may cause insomnia. It's contraindicated in cases of hypersensitivity to rimantadine or amantadine.	"['rimantadine (Oral route)\nCommonly used brand name(s)\nIn the U.S.\nAvailable Dosage Forms:\nTherapeutic Class: Antiviral\nChemical Class: Adamantane\nUses For rimantadine\nRimantadine is used to prevent or treat certain influenza (flu) infections (type A) in adults (17 years of age and older). It is also used to prevent flu infection (type A) in children (1 to 16 years of age). It may be given alone or along with flu shots.\nRimantadine is an antiviral. It will not work for colds, other types of flu, or other virus infections.\nrimantadine is available only with your doctor\'s prescription.\nBefore Using rimantadine\nIn deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For rimantadine, the following should be considered:\nTell your doctor if you have ever had any unusual or allergic reaction to rimantadine or any other medicines. Also tell your health care professional if you have any other types of allergies, such as to foods, dyes, preservatives, or animals. For non-prescription products, read the label or package ingredients carefully.\nAppropriate studies performed to date have not demonstrated pediatric-specific problems that would limit the usefulness of rimantadine in children. However, safety and efficacy have not been established in children younger than 1 year of age.\nAppropriate studies performed to date have not demonstrated geriatric-specific problems that would limit the usefulness of rimantadine in the elderly. However, elderly patients are more likely to have unwanted side effects (e.g., anxiety, dizziness, nausea, vomiting, stomach pain), which may require caution and an adjustment in the dose for patients receiving rimantadine.\n|All Trimesters||C||Animal studies have shown an adverse effect and there are no adequate studies in pregnant women OR no animal studies have been conducted and there are no adequate studies in pregnant women.|\nThere are no adequate studies in women for determining infant risk when using this medication during breastfeeding. Weigh the potential benefits against the potential risks before taking this medication while breastfeeding.\nInteractions with Medicines\nAlthough certain medicines should not be used together at all, in other cases two different medicines may be used together even if an interaction might occur. In these cases, your doctor may want to change the dose, or other precautions may be necessary. Tell your healthcare professional if you are taking any other prescription or nonprescription (over-the-counter [OTC]) medicine.\nInteractions with Food/Tobacco/Alcohol\nCertain medicines should not be used at or around the time of eating food or eating certain types of food since interactions may occur. Using alcohol or tobacco with certain medicines may also cause interactions to occur. Discuss with your healthcare professional the use of your medicine with food, alcohol, or tobacco.\nOther Medical Problems\nThe presence of other medical problems may affect the use of rimantadine. Make sure you tell your doctor if you have any other medical problems, especially:\n- Epilepsy or seizures, history of—Use with caution. May make these conditions worse.\n- Kidney disease, severe or\n- Liver disease, severe—Use with caution. The effects of rimantadine may be increased because of slower removal from the body.\nProper Use of rimantadine\nTalk to your doctor about the possibility of getting a flu shot if you have not had one yet.\nrimantadine is best taken before exposure, or as soon as possible after exposure, to people who have the flu.\nTo help keep yourself from getting the flu, keep taking rimantadine for the full time of treatment.\nIf you already have the flu, continue taking rimantadine for the full time of treatment even if you begin to feel better after a few days. This will help to clear up your infection completely. If you stop taking rimantadine too soon, your symptoms may return. rimantadine should be taken for at least 5 to 7 days.\nrimantadine works best when there is a constant amount in the blood. To help keep the amount constant, do not miss any doses. Also, it is best to take the doses at evenly spaced times day and night.\nShake the oral liquid well before each use. Measure the medicine with a specially marked measuring spoon or other device to measure each dose accurately. The average household teaspoon may not hold the right amount of liquid.\nThe dose of rimantadine will be different for different patients. Follow your doctor\'s orders or the directions on the label. The following information includes only the average doses of rimantadine. If your dose is different, do not change it unless your doctor tells you to do so.\nThe amount of medicine that you take depends on the strength of the medicine. Also, the number of doses you take each day, the time allowed between doses, and the length of time you take the medicine depend on the medical problem for which you are using the medicine.\n- For oral dosage forms (suspension or tablets):\n- For treatment of flu:\n- Adults and teenagers 17 years of age and older—100 milligrams (mg) two times a day for 7 days.\n- Older adults—100 mg once a day.\n- Children up to 16 years of age—Use is not recommended.\n- For prevention of flu:\n- Adults and teenagers 17 years of age and older—100 milligrams (mg) two times a day.\n- Older adults—100 mg once a day.\n- Children 10 to 16 years of age—100 mg two times a day.\n- Children 1 to 9 years of age—Dose is based on body weight and must be determined by your doctor. The dose is 5 milligrams (mg) per kilogram (kg) of body weight once a day. Children in this age group should not receive more than 150 mg per day.\n- Children younger than 1 year of age—Use and dose must be determined by your doctor.\n- For treatment of flu:\nIf you miss a dose of rimantadine, take it as soon as possible. However, if it is almost time for your next dose, skip the missed dose and go back to your regular dosing schedule. Do not double doses.\nKeep out of the reach of children.\nDo not keep outdated medicine or medicine no longer needed.\nAsk your healthcare professional how you should dispose of any medicine you do not use.\nStore the medicine in a closed container at room temperature, away from heat, moisture, and direct light. Keep from freezing.\nStore the oral liquid at room temperature for 14 days.\nPrecautions While Using rimantadine\nIt is important that your doctor check the progress of you or your child at regular visits to allow changes in your dose and to check for any unwanted effects.\nrimantadine may cause some people to become dizzy or confused, or to have trouble concentrating. Make sure you know how you react to rimantadine before you drive, use machines, or do anything else that could be dangerous if you are dizzy or confused. If these reactions are especially bothersome, check with your doctor.\nIf your or your child\'s symptoms do not improve within a few days, or if they become worse, check with your doctor.\nWhile you are being treated with rimantadine, do not have any immunizations (vaccinations) without your doctor\'s approval. Live virus vaccinations should not be given for 2 days after stopping rimantadine, and rimantadine should be not be given 2 weeks after receiving live vaccines.\nDo not take other medicines unless they have been discussed with your doctor. This includes prescription or nonprescription (over-the-counter [OTC]) medicines and herbal or vitamin supplements.\nrimantadine Side Effects\nAlong with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention.\nCheck with your doctor immediately if any of the following side effects occur:Less common\n- Difficult or labored breathing\n- shortness of breath\n- tightness in the chest\nGet emergency help immediately if any of the following symptoms of overdose occur:Symptoms of overdose\n- chest pain or discomfort\n- dry mouth\n- fast, slow, or irregular heartbeat\n- irregular heartbeats\n- pounding or rapid pulse\n- seeing, hearing, or feeling things that are not there\n- trouble with sleeping\nSome side effects may occur that usually do not need medical attention. These side effects may go away during treatment as your body adjusts to the medicine. Also, your health care professional may be able to tell you about ways to prevent or reduce some of these side effects. Check with your health care professional if any of the following side effects continue or are bothersome or if you have any questions about them:Less common\n- Abdominal or stomach pain\n- acid or sour stomach\n- continuing ringing or buzzing or other unexplained noise in the ears\n- difficulty with concentrating\n- hearing loss\n- impaired concentration\n- lack or loss of strength\n- loss of appetite\n- shakiness and unsteady walk\n- sleepiness or unusual drowsiness\n- stomach discomfort, upset, or pain\n- unsteadiness, trembling, or other problems with muscle control or coordination\n- unusual tiredness or weakness\nOther side effects not listed may also occur in some patients. If you notice any other effects, check with your healthcare professional.\nCall your doctor for medical advice about side effects. You may report side effects to the FDA at 1-800-FDA-1088.\nSee also: Side effects (in more detail)\nThe information contained in the Truven Health Micromedex products as delivered by Drugs.com is intended as an educational aid only. It is not intended as medical advice for individual conditions or treatment. It is not a substitute for a medical exam, nor does it replace the need for services provided by medical professionals. Talk to your doctor, nurse or pharmacist before taking any prescription or over the counter drugs (including any herbal medicines or supplements) or following any treatment or regimen. Only your doctor, nurse, or pharmacist can provide you with advice on what is safe and effective for you.\nThe use of the Truven Health products is at your sole risk. These products are provided ""AS IS"" and ""as available"" for use, without warranties of any kind, either express or implied. Truven Health and Drugs.com make no representation or warranty as to the accuracy, reliability, timeliness, usefulness or completeness of any of the information contained in the products. Additionally, TRUVEN HEALTH MAKES NO REPRESENTATION OR WARRANTIES AS TO THE OPINIONS OR OTHER SERVICE OR DATA YOU MAY ACCESS, DOWNLOAD OR USE AS A RESULT OF USE OF THE THOMSON REUTERS HEALTHCARE PRODUCTS. ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE OR USE ARE HEREBY EXCLUDED. Truven Health does not assume any responsibility or risk for your use of the Truven Health products.\nCopyright 2016 Truven Health Analytics, Inc. All Rights Reserved.\nMore about rimantadine\n- Other brands: Flumadine', '- Prevention and treatment of influenza A infection in high-risk adults.\n- Prevention of influenza A infection in high-risk children.\nTreatment of influenza A infection in children.\nDiminishes replication of influenza A virus by inhibiting uncoating of the virus.\nWhen given prophylactically, prevents infection with influenza A virus. When administered within 48 hr of onset of infections, decreases the duration of fever and other associated symptoms.\nAbsorption: Well absorbed after oral administration.\nMetabolism and Excretion: Mostly metabolized by the liver; <25% excreted unchanged in urine.\nHalf-life: 25 hr (range 13–65 hr).\nTIME/ACTION PROFILE (blood levels)\n- Hypersensitivity to rimantadine or amantadine\n- Lactation: Lactation.\nUse Cautiously in:\n- History of seizures\n- Severe hepatic or renal impairment (dose ↓ recommended if CCr <30 mL/min)\n- Geri: May be at ↑ risk for GI effects\n- OB: Safety not established.\nAdverse Reactions/Side Effects\nCNS: SEIZURES, agitation, dizziness, fatigue, headache, impaired concentration, insomnia, mental depression\nGI: abdominal pain, anorexia, diarrhea, dry mouth, dyspepsia, nausea, vomiting\n* CAPITALS indicate life-threatening.\nUnderline indicate most frequent.\nPO (Adults): Prevention- 100 mg twice daily (duration range = 11 days-6 wk). Treatment- 100 mg twice daily for 7 days.\nPO (Geriatric Patients , Patients with Severe Hepatic or Severe Renal Impairment [CCr <30 mL/min]): Prevention- 100 mg daily.\nPO (Children 10–16 yr): Prevention- 100 mg twice daily (duration range = 5–6 wk).\nPO (Children 1–9 yr): Prevention- 5 mg/kg/day as a single dose (not to exceed 150 mg/day) (duration range = 5–6 wk).\nAvailability (generic available)\nTablets: 100 mg\n- Monitor respiratory status (rate, breath sounds, sputum) and temperature periodically. Supportive treatment is indicated if symptoms occur.\n- Risk for infection (Indications)\n- Do not confuse rimantadine with ranitidine.\nDo not administer within 48 hr before or 2 wk after administration of intranasal influenza virus vaccine ( FluMist ).\n- Consider available information on influenza drug susceptibility patterns and treatment effects before using rimantadine.\n- PO Do not administer last dose of medication near bedtime; this drug may produce insomnia in some patients.\n- Advise patient to take medication as directed at evenly spaced times for full course of therapy, even if feeling well. Take missed doses as soon as possible unless almost time for next dose; do not double up on missed doses.\n- May cause dizziness. Advise patient to avoid driving or other activities that require alertness until response to the drug is known.\n- Advise patient to consult health care professional about receiving a flu vaccination.\n- Inform patient that frequent mouth rinses, good oral hygiene, and sugarless gum or candy may decrease dry mouth.\n- Advise patient to consult health care professional before taking OTC medications or herbal products, especially aspirin or acetaminophen, or drinking alcoholic beverages.\n- Instruct patient and family to notify health care professional if influenza symptoms occur.\nAvoidance of or reduction in symptoms associated with influenza A infection.\nPediatrics Central™ is an all-in-one application that puts valuable medical information, via your mobile device or the web, in the hands of clinicians treating infants, children, and adolescents. Complete Product Information.']"	['<urn:uuid:f87d7203-995d-4490-96c6-5e792899d842>', '<urn:uuid:3bb71345-d99f-4aa8-83a7-adf2859b28ce>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:35:02.547110	12	110	2312
30	I'm researching alternative diamond production methods and want to understand the key difference between natural and CVD diamonds - how do their properties compare in terms of quality?	High-quality polycrystalline CVD diamond has similar properties to perfect natural stones. In fact, monocrystalline CVD diamond has demonstrated properties that even surpass those of the best natural diamonds. Natural diamonds show a wide range of properties and are categorized based on color, clarity, and defects.	['1. What is CVD?\nChemical vapour deposition or CVD is a generic name for a group of processes which used to produce high quality, high-performance, solid materials which is deposited from a gas onto a substrate and that chemical reactions are involved.\n2. What is CVD diamond?\nSynthetic diamonds are prepared or produced by CVD techniques and therefore it is called as CVD Diamonds. It is also known as Synthetic diamond, Lab Grown diamond, Man-Made diamond, Cultured Diamond.\n3. Why is it difficult to synthesize diamond?\nDiamond consists – like coal or graphite – of carbon. The main difference is the arrangement of the carbon atoms in the material (i.e. in the crystal lattice). Unfortunately graphite is the more stable carbon allotype (form) and therefore diamond is very rare in nature. To convert graphite into diamond high pressure and high temperatures (HPHT) must be applied. Under those conditions diamond becomes the more stable carbon allotype. For diamond CVD a carbon containing gas is decomposed and the carbon atoms are deposited on a surface. By proper conditions the growth of diamond crystals can be enhanced and the growth of graphite is suppressed.\n4. How does CVD work ?\nThe CVD process begins with tanks containing an initiator material (red) and one or more monomers (purple and blue), which are the building blocks of the desired polymer coating. These are vaporized, either by heating them or reducing the pressure, and are then introduced into a vacuum chamber containing the material to be coated. The initiator helps to speed up the process in which the monomers link up in chains to form polymers on the surface of the substrate material.\n5. What gases are used for CVD DIAMONDS ?\nMostly a mixture of hydrogen and methane gases are used for the making of CVD Diamonds.\n6. What are approx growth temperatures require for CVD Diamonds ?\nFor CVD Diamonds around 700-900°C . Also lower temperatures are possible but at strongly reduced growth rates.\n7. How fast does CVD DIAMOND Grows ?\nActually it is a very slow process. At growth rates between 0.1 and 10 micron per hour usually deposited on large areas (>100 cm2) diamond where, much higher growth rates (>100 micron per hour) have been demonstrated for small areas (<1 cm2).\n8. What substrate material can be used to make CVD DIAMONDS ?\nMaterials used in chemical vapor deposition include silicon, carbon fiber, carbon nanofibers, filaments, carbon nanotubes, silicon dioxide, silicon-germanium, tungsten, silicon carbide, silicon nitride, silicon oxynitride, titanium nitride, and diamond. The main requirements are: the material must be able to withstand high temperatures, it must not be attacked by the activated process gas and it must not dissolve carbon.\n9. What are typical grain sizes of polycrystalline diamond ?\nUsually the grain size is in the sub-micron range at the beginning of diamond growth. With increasing thickness the grains grow larger. Usually the grain size at the growing surface of a diamond film is about 10 % of the film thickness.\n10. What is nano-crystalline diamond ?\nBy enforced re-nucleation during diamond growth the diamond grains can be kept very small. This material is called nanocrystalline or ultra-nanocrystalline diamond (UNCD). This material does not have all the unique properties of diamond (e.g. it is not transparent and the thermal conductivity is low). But it exhibits a pretty smooth surface and there are interesting applications in tribology, micro-mechanics and bio-chemical sensing.\n11. What is the surface roughness ?\nThe surface of an as grown polycrystalline diamond film is pretty rough (like sandpaper). Hence polishing is a very important processing step for many applications. The surface roughness of polished diamond is usually a few nanometers.\n12. How does CVD diamond compare to DLC ?\nDLC is an acronym for diamond-like carbon. Other denotations are a-C:H or a-C. In this material the carbon atoms do not form a crystal lattice but a random (amorphous) network. It is deposited by bombarding a surface with energetic (hydro-)carbon particles. This can be done at room temperature. DLC films exhibit quite a lot of compressive stress which limits the maximum thickness. It is somehow “diamond-like” as it is chemically stable and hard, however, it is clearly no diamond.\n13. How does CVD diamond compare to natural diamond ?\nNatural diamonds show a widespread range of properties. A sophisticated system for the categorization of natural diamond stones with respect to color, clarity, defects and so on has been established. High-quality polycrystalline CVD diamond does have similar properties like perfect natural stones. For mono-crystalline CVD diamond properties have been demonstrated that even surpasses those of the best natural stones.\n14. Is CVD diamond expensive?\nThe answer is yes and no. For e.g. large area diamond disk i.e more than 1 mm thickness will be expensive as compare to small area diamond disk are rather cheap. So usually diamond is only used if necessary. This is mainly because the deposition process requires a lot of energy, low growth rate and the equipment expensive.\n15. Is CVD diamond a billion market ?\nThe answer is No as of now. Most of the CVD diamond products serving small niche markets. As natural diamond are expected to reduce and CVD Diamond for several applications may become important in the future.']	['<urn:uuid:2e7b8358-8bc5-4f1e-ab26-5070a33edcdc>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	28	45	878
31	what happens winter spring vineyard grapes risks protection methods	During winter, vineyard staff performs winter pruning, cutting back canes and deciding how many nodes per cane to leave. As temperatures reach fifty degrees multiple times, sap starts flowing and buds emerge, marking the transition to spring. However, this period is particularly delicate as spring frost can damage opening buds and young shoots. To protect against frost, vineyards employ both passive and active methods. Passive methods include site selection (mid-slope positions are warmer), varietal selection (choosing late bud burst varieties), and pruning techniques (delayed or double pruning). Active methods include wind machines that mix cool and warm air layers, helicopters, heaters, burning straw, and over-vine sprinklers. While most methods work against radiational frost, only properly used sprinklers can protect against advective freeze.	['Bud break is truly a magical time in the vineyard. It occurs when grapevines that have been dormant all winter long awaken, and the new growth that will become leaves, shoots, and, most important, this year’s grapes, emerge.\nThe specific day that bud break occurs is dependent on the temperatures in the vineyard. Multiple days reaching fifty degrees will get the sap flowing from the roots and truck it up into to the canes (growth from the prior year). On those canes will be little bumps, called nodes. Those nodes will push out and become buds, so the prior year’s growth sets up for the next year’s crop.\nOver the winter, the vineyard staff will do winter pruning and will cut back the canes. In the context of vine maturity, a cane is a mature shoot from last year’s growth that has developed a layer of bark, and no longer has leaves. The vineyard manager and winemaker do not want all the nodes on all the canes to become buds, as it would take too much energy out of the vine, and not produce the best-quality grapes. At the time of pruning, the decision is made on how many canes to leave on each vine, and how many nodes per cane. The numbers vary by vineyard manager, and is based on past experience, the specific grape and how prodigious it is, and the climate in the particular vineyard.\nNow that winter pruning has been completed and the temperature has reached fifty or higher degrees many times, the sap starts flowing into the remaining canes and can be seen at the cut ends as a drop of liquid. This is called “weeping” or “crying.” Following that,, the nodes on the canes will become swollen and become known as buds. Inside that bud are all the structures for that year’s growth. There are different phases to bud development before they burst open, and when it does, it is truly spectacular. You see tiny leaves, shoots, and the flower formations that will become grape clusters.\nIt is during this time that the young growth is quite vulnerable to cold temperatures and frosts. If any of you have been watching the local Virginia news, as well as photos and reports coming out of Chablis, Burgundy, and other winegrowing regions around the world, you know that frosts have been hitting vineyards hard. Last year, we posted a blog about frost and frost mitigation. Late frosts in Virginia have occurred as late as Mother’s Day in past years.\nBarring lost bud growth to frosts, those shoots, leaves, and flower formations will push their way out and open wide. Growth occurs rapidly at this point. Depending on the way the vineyard allows the vines to grow, the shoots may be tucked in between wires. This allows flow of air, which helps reduce molds and mildews on the clusters and warmth from the sun to help ripen the grapes over the next few months.\nLet us all cheer to this magical time in the vineyard, and the birth of the next crop!', 'During the growing season, all green parts of the grape vine are susceptible to frost. Spring is a particularly delicate period for the vines, since spring frost often damages opening buds and young shoots, thus affecting the crop load. Not all vine varieties are equally susceptible to frost as susceptibility depends on bud development stages of the different varietals. A closed bud during dormancy is protected from the cold by its thick walls, which enables it to resist temperatures as low as -15°C. Critical temperatures for grape vines differ based on the growth stage, while during the winter, dormant buds can resist temperatures below -10°C and even down to -20°C and more. However, new growths can be damaged at -1,1°C.\nThere are two different weather events that can harm vines –Radiational Frosts and Advective Freeze.\nAdvective Freeze : This takes place when a large cold air mass, accompanied with winds, cloudy conditions, and low humidity, blows into an area and replaces warmer air. In such days, temperatures often drop below 0 °C and stay in this range all day. Advective Freeze may cause more damage than radiational frost, because active protection measures are not effective against it.\nRadiational frost : This occurs in clear nights with little to no winds, and as the name implies is connected with heat radiation. When more heat radiates away from the earth surface than the surface receives, the temperatures drops, and so the coolest air is closer to the ground. Apart from a clear sky-temperature inversion, low dew-point temperatures, night air temperatures below 0°C, and daily air temperatures above 0°C are associated with radiational frost.\nIt can also happen that a combination of advective and radiative conditions occur. For example cold air mass enters into a region,which results in the advective freeze, then for several days clear and calm conditions follow that cause radiation frost.\nWinegrowers can protect vines against frost by employing several passive and active frost protection methods. However, these methods can prevent or limit frost damages due to radiational frost while there is very little that can be done to protect vines against an advective frost.\nPassive frost protection methods : These include biological and ecological protection methods, which are often less costly than active methods and can even eliminate the need for active methods. Therefore it is advisable to focus on passive frost protection methods first and foremost, as they are used to avoid frost danger\nSite & varietal selection :The most important element of frost protection is of course site selection and choice of varieties(late vs. early variety). Since cold air flows downhill, the mid-slope positions are warmer, if there are no trees, brush or other air dams that prevent cool air to flow out of the vineyard. Different grape varieties vary in the date of budburst, so it’s somewhat recommended to choose early bud burst varieties in locations with the lowest risk of frost.\nCover crop management : Although there are several benefits of using cover crops in the vineyard, when it comes to preventing frost, cover crop does not affect the situation drastically. However, cover crops prevent soil from absorbing and storing heat, and bare soil can be 2°C higher compared to the floor with 5cm high grass. Tall cover crops can also slow down the movement of the cold air out of the vineyard. What winegrowers can do is to mow ground cover short before the frost-prone period or if there is no erosion concerns even cultivate the cover crops prior to that period.\nPruning method : With vineyard management practices such as pruning, winegrowers can have an influence on the date of bud burst and thus reduce the risk of frost injury. This can be achieved with either a delayed pruning, since unpruned vines bud burst later than pruned vines, or with double pruning. When conducting double pruning,early pruning to long spurs with 5-8 buds suppresses the growth of basal buds (buds from the base of the main grapevine), and once frost risk passes or basal buds begin to break, a final pruning can be carried out.\nActive frost protection methods : These include activities in the vineyard which increase the temperature above the « danger » level by influencing the micro-climates in the vineyard. Obviously winegrowers focus on passive frost protection methods first and foremost, as they are used to avoid frost danger.\nWind machines : These work only with radiation frosts whenthere is an inversion and can provide between 1-3°F temperaturedifferences. Wind machines basically mix cooler air that is locatedaround the vines with the warmer air above the inversion layer, whichis usually at about (12-15 m) above the ground.\nWind machines are very expensive and are profitable on sites wherethere is a high probability of damaging spring frost – such as on 1 in every 5 years.\nHelicopters : Instead of purchasing wind machines,winegrowers sometimes prefer to make use of helicopters which also can mix inversion layers. This is a much more expensive method to protect vines from frost damages, but in areas with a low probability of spring frost more than a welcome option.\nHeaters : Heating the vineyard air is one of the oldest practices to protect vines against spring frost. Some winegrowers still burn cuttings from pruning to heat the air, while fossil-fueled heaters are not largely used as they are expensive due to the cost of fuel and labor, as well as being rather inefficient with lot’s of energy being lost to the sky, not to mention the environmental concerns. Heaters work best with temperature inversions, if there is little to no inversion or wind is blowing the heaters may not provide adequate protection. In order to properly protect vines, heaters should be uniformly distributed throughout the vineyard and turned on before the critical temperature is reached. To lower the cost of operation heaters can be used in combination with sprinklers or wind machines.\nBurning straw or hay : This is a similar technique which consists in burning bales of stray around the edges of the vineyard.The smoke that is given off provides a « protective blanket »which limits the loss of heat from the soil in absence of cloud cover.\nOver-vine sprinklers : Vines can be protected against frostalso with the help of water sprinklers. When water is sprayed andfreeze around green tissues it releases heat and thus protects vines.For this method, a large volume of water is required since constantliquid water is needed to form a freezing coating around vines budsand shoots to release heat and rise the temperature. This is the onlymethod that can provide frost protection during the advection frost as well, if used properly.\nConclusions : Passive methods often eliminate the need for active methods, plus they are much less costly. Active frost protection methods are chosen based on vineyard site specifics and winegrowers financial capabilities. Often winegrowers are combining different methods to protect vines and use for example sprinklers in combination with wind machines, heaters with wind machines or sprinklers with heaters, to efficiently protect vines and lower the costs.']	['<urn:uuid:373a79f5-61c8-4b48-857d-c5e00498391f>', '<urn:uuid:d050c61d-9260-47b5-8509-51d3351d2cbf>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:35:02.547110	9	122	1691
32	As a marine biologist studying survival mechanisms, I'd like to know how biological fitness relates to survival in general evolution theory, and what unique survival mechanism corals have developed when facing stress?	In evolutionary biology, fitness refers to an organism's ability to pass on genetic material to offspring and maintain population survival. This biological or 'Darwinian' fitness measures reproductive success and adaptation to environment. For corals specifically, they have developed a unique survival mechanism called colorful bleaching - when stressed, they produce a sunscreen layer that makes them glow in bright pink, purple and orange colors, which helps protect against harmful rays and attracts algae to return. This is effectively a self-regulating mechanism that can help corals recover if the stress event is mild enough.	['Fitness simply implies reproductive success to an evolutionary biologist, and it shows how well an organism is suited to its environment.\nSimilarly, How does fitness relate to natural selection quizlet?\nWhat is the relationship between fitness and natural selection? The organisms that are most suited to their surroundings will generate the most offspring. Offspring with more desirable features are more likely to survive and reproduce, increasing the frequency of specific traits.\nAlso, it is asked, What does fitness mean in evolution?\nsuccess in reproduction\nSecondly, What is fitness in ecology?\nThe term “fitness” is used by evolutionary biologists to define how good a genotype is at producing children in the following generation compared to other genotypes. So, if brown beetles continuously produce more progeny than green beetles due to their hue, you’d conclude that the brown beetles are more fit.\nAlso, What is the fitness of an organism?\nIn evolutionary biology, “fitness” refers to an organism’s capacity to pass on its genetic material to its children. The ability to survive long enough to reproduce and maintain a population or species alive is referred to as biological or “Darwinian” fitness.\nPeople also ask, How is natural selection related to a species fitness?\nFitness is only accounting; survival and differential reproduction are the outcome of natural selection, which is the driving force behind evolution. Organisms that are more adapted to their surroundings will reproduce more and hence increase the percentage of the population with their characteristics.\nRelated Questions and Answers\nDoes fitness and survival have the same meaning quizlet?\nDoes fitness (as used in biology) and survival (as used in biology) have the same meaning? Yes, if the population is the issue.\nWhat is relative fitness quizlet?\nRelative fitness refers to an individual’s contribution to the gene pool of the next generation in comparison to that of other people. Natural selection operates directly on the phenotype and indirectly on the gene that regulates it.\nWhat does greater fitness mean quizlet?\nIndividuals who exhibit a more extreme version of a characteristic are more fit than those who exhibit an average form or the opposite extreme form of the trait.\nWhat is the biological term for fitness?\nFitness is sometimes known as Darwinian fitness in biology since it refers to Charles Darwin’s evolutionary theory of natural selection. Darwinian fitness refers to an organism’s ability to successfully pass on its genes.\nWhat is evolutionary fitness quizlet?\nEvolutionary Fitness is a term used to describe a person’s The success of transferring genes from one generation to the next. Adaptation via evolution. Any genetically regulated characteristic that improves a person’s capacity to pass on his or her alleles. Single-Gene Traits and Natural Selection\nWhat is fitness biology quizlet?\nAn organism’s capacity to survive and reproduce in its environment is referred to as fitness.\nWhere does the word fitness come from?\nThough we have come to think of fitness solely in terms of physical fitness it is actually defined as “the quality of being suitable,” and is derived from the Old English suffix -ness, which means “the state of being,” and the root word fit, which means “proper or suitable,” and is of unknown origin (with theories\nWhat does fittest mean in science?\nThe phrase “survival of the fittest” was coined by British biologist Charles Darwin in the fifth edition of On the Origin of Species (published in 1869), which argued that creatures that are best adapted to their environment had the highest chance of surviving and reproducing.\nWhat does survival of the fittest mean quizlet?\nIt’s a case of “survival of the fittest.” Natural selection is the process through which individuals that are more fitted to their environment survive and reproduce the most effectively.\nWhat does the saying survival of the fittest mean?\n: the natural process through which organisms that are well-adapted to their surroundings have the highest chance of surviving and reproducing: the process of natural selection Our home is in the heart of the woods, with deadly snakes in the grass and snapping turtles in the ponds, and each day is marked by the survival of the fittest.—\nWhat is the definition of natural selection quizlet?\nthe process of natural selection The process through which organisms that are most suited to their surroundings survive and reproduce in order to pass on beneficial features to their progeny. offspring.\nWhat is the difference between fitness and relative fitness?\nAbsolute fitness refers to an organism’s fitness as measured by the number of children it would generate in its lifespan and the age at which those offspring would reach reproductive age. Absolute fitness is standardized relative fitness.\nWhy does the fitness of phenotype depend on frequency dependent selection?\nThe fitness of a trait in frequency-dependent selection is determined by how prevalent it is in the population. Depending on which side of their head their mouth is on, scale-eating fish attack their meal from the left or right.\nWhat is the fitness of a mule?\nWhat is a sterile mule’s relative fitness? Because fitness also involves reproductive contributions to the next generation, and a mule is incapable of producing children, the answer is zero.\nWhy do organisms with greater fitness generally?\nWhy do creatures with higher fitness have more offspring than organisms with lower fitness? The creature is more suited to its surroundings than the other.\nWhich phenotypes have higher fitness in disruptive selection?\nWhen two or more modal phenotypes have better fitness than the intermediate phenotypes between them, disruptive selection occurs in the population .\nWhat does evolutionary fitness measure quizlet?\nWhat is the definition of evolutionary fitness? Relative fertility success The founder effect is different from a population bottleneck in that it entails the isolation of a small colony of people from the rest of the population.\nWhich of the following is the best definition of evolutionary fitness quizlet?\nDue to heritable features, evolutionary fitness refers to the capacity to reproduce better than other members of your species. Because it may boost reproduction, an adaptation can improve a species’ evolutionary fitness.\nWhen we discuss fitness in a biological context what do we mean quizlet?\nThe capacity of a person to generate children that survive and reproduce in comparison to other individuals in the population is best characterized as biological fitness.\nHow is fitness measured biology quizlet?\nThe proportional number of alleles it provides to the gene pool of the following generation compared to the contribution of others is used to determine fitness. As a result, an individual’s fitness is determined by the number of fertile offspring produced.\nHow is fitness measured in biology?\nThe capacity of an organism to live and reproduce affects the quantity of its genetic contribution to the following generation, which is assessed by fitness. Fitness, on the other hand, is measured by the percentage of following generations that inherit an organism’s genes, not by the overall number of children.\nWhy is natural selection often referred to as the survival of the fittest?\nThe notion of survival of the fittest was popularized by Charles Darwin as a process underpinning natural selection, which drives the development of life. Those organisms with genes that are more adapted to their environment are chosen for survival and passed on to the next generation.\nWhat is natural selection or survival of the fittest?\nNatural selection is one of the most essential. Evolution, according to Charles Darwin (pictured), is based on the survival of the fittest principle. In a genetic sense, this implies that people in a group, or society, are more likely to survive if they are fit.\nWhat does survival of the fittest mean in reference to natural selection and evolution?\nThe expression “survival of the fittest” relates to natural selection, which is a mechanism that causes evolutionary development. Natural selection works by providing those who are more suited to a certain set of environmental circumstances an edge over those who aren’t.\nFitness is a term that refers to the ability of an organism to survive and reproduce. In natural selection, fitness is related to how well an individual can adapt to its environment.\nThis Video Should Help:\nWhen we are discussing natural selection the term fitness refers to how well an organism is able to survive and reproduce. Fitness can be measured in different ways such as relative fitness, reproductive success, or body size.\n- what is fitness in biology\n- how does evolutionary fitness change with abiotic and biotic factors\n- example of fitness in biology\n- natural selection definition\n- evolutionary fitness example', 'Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process.']	['<urn:uuid:05bfebc3-cdbe-476e-a735-68d72e9a7efe>', '<urn:uuid:369f1095-9589-4d83-9007-35418b336092>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	32	93	2354
33	What changes did Providence make to their elementary school assignment system during the pandemic, and what was the main reason behind this change?	For school year 2020-21, Providence Public School District altered their method for placing incoming kindergarten students at elementary schools. Instead of prioritizing families' school selections, they used an algorithm to assign students to the closest school with available seats for the student's educational type. This change was implemented to reduce time spent on buses and, by extension, reduce potential exposure to the COVID-19 virus. Under this new system, if there were still open seats in a school after all assignments were made, students might be reassigned based on their initial school choice.	['For school year (SY) 2020-21, Providence Public School District (PPSD) altered their method for placing incoming kindergarten students at elementary schools. The aim was to reduce time spent on buses and, by extension, potential exposure to the COVID-19 virus. As with any policy change, however, there can be unintended consequences. In this project, we looked at how kindergarten school access and enrollment patterns altered under this policy (i.e. between SY 2019-20 and SY 2020-21) so that PPSD can better balance COVID-19 precautions and equity concerns across demographic groups moving forward.\nFor school year (SY) 2020-21, Providence Public School District (PPSD) altered their method for placing incoming kindergarten students at elementary schools. The aim was to reduce time spent on buses and, by extension, potential exposure to the COVID-19 virus.\nKindergarten students at PPSD have historically been able to request assignments to schools regardless of proximity to their home. The registration deadline for school choice is typically in the spring prior to the upcoming school year. Under this model, students select up to four schools and are placed in the order of their school choice to the extent that is allowed by school capacity. If demand for a school exceeds capacity, students are generally assigned these seats by lottery. Students who are not assigned one of their four chosen schools or who miss the spring school choice registration deadline are assigned to the closest school with an available seat and a suitable educational program.\nFor SY 2020-21, however, PPSD announced that rather than prioritizing families’ school selections over school proximity, they would instead use an algorithm to assign students to the closest school with available seats for the student’s educational type (e.g. multilingual learner, general education, etc.).1 The order in which students were assigned was randomized. If, after all students were assigned schools, there were still open seats in a school2, students might be reassigned based on their initial school choice3. By assigning students to schools within one mile of their homes, PPSD hoped that commute times would be shortened and that fewer students would need bus transportation, thus lowering the risk of exposure to the coronavirus.\nAs with any policy change, however, there can be unintended consequences. PPSD was concerned that, given historical racial and socio-economic segregation patterns, emphasizing proximity and reducing school choice options might create more segregated schools or unintentionally put certain subpopulations of students at a disadvantage. In this project, we looked at how kindergarten school access and enrollment patterns altered under this policy (i.e. between SY 2019-20 and SY 2020-21) so that PPSD can better balance COVID-19 precautions and equity concerns across demographic groups moving forward.\nIn close partnership with PPSD, we first explored whether the policy was successful on its own terms. Did more students in fact attend schools within one mile of their homes? Next, we developed three key metrics to help determine whether prioritizing students’ proximity to their schools created or worsened disparities in: (a) kindergarten students’ choice to enroll in PPSD; (b) the quality of schools4 in which kindergarten students were enrolled; and (c) kindergarten student attendance.\nTo explore these metrics, PPSD’s Office of Research, Planning, and Analysis (RPA) provided us with extensive data, such as historical student demographics, school demographics, and students’ school preferences. We combined these data with publicly available sources such as data from the Census Bureau’s American Community Survey and looked at how trends differed between SY 2020-21 and previous school years.\nDid the policy change have the intended effect?\nYes, we found that 74% of kindergarten students attended a school within one mile of their home address in SY 2020-21 compared to 62% the year before. We did not observe disparities by geographic regions. Map 1 shows the areas with the largest increases in the proportion of kindergarten students who attended a nearby school.\nWere there disparities in school quality?\nOur findings suggest that the revised policy generally did not disadvantage under-resourced students. First, we determined that most students had access to higher performing schools, with almost all kindergartners living within one mile of a Star 2 or 3 school. Next, we checked to see if certain student populations were disproportionately attending Star 1 schools despite having access to higher performing schools. Here, we observed minimal discrepancy between the median Star ratings of schools students could access and the schools in which they were placed. We found no demographic bias when students (22%) were placed in a Star 1 school (which likely occurred when seats at better performing schools were filled).\nWere there disparities in student attendance across student demographics?\nUnfortunately, because this policy change took effect during the COVID-19 pandemic (which itself led to declining attendance), we were unable to tease out the degree to which changes in attendance were affected by the policy change versus the pandemic. On the whole, absenteeism rates increased across PPSD. We observed that increases in absenteeism rates were highest for students who resided in neighborhoods with ‘High’ and ‘Very High’ rates of poverty5 (a three to five-fold increase, regardless of racial group), and Hispanic and multiracial students who resided in neighborhoods with ‘Medium’ rates of poverty. This could be due to noise, but it bears looking into further. We also observed that Black and White students in ‘Medium’ poverty neighborhoods saw a decrease in the proportion of students with high absenteeism levels. Overall, the percent of students with excessive absences6 from September to December 2020 jumped from 8.3% to 26.5% between SY 2019-20 and SY 2020-21, with the percent of students who missed more than half of all school days increasing eightfold (from 0.8% to 6.3%).\nWere there increased disparities in kindergarten enrollment?\nAs with student attendance, we were unable to determine the impact of the policy change versus the pandemic on enrollment. However, we did find that the sharp decrease in enrollment, whatever the cause, was not evenly spread across the district or across student demographic groups. COVID-19 radically disrupted enrollment, which fell by 21% overall. Enrollment dropped the most in schools near the edges of Providence’s city limit (approximately 30% compared to 12%) and that had lower Star ratings7 (roughly seven times higher). Decreases in enrollment were also unevenly distributed among student populations, with Hispanic students (25.2%↓), girls (23.5%↓), and special education students (23.0%↓) experiencing steeper drops in enrollment than the overall population (20.6%↓). On the other hand, EL enrollment (18.3%↓) decreased less than the overall population. We note that given the overall interruption in enrollment in PPSD during the pandemic---a circumstance which has affected schools nationwide---we cannot know whether this disparity is connected to PPSD’s choice policy. Nevertheless, these numbers provide important context to what is happening in this unprecedented time.\nThese analyses confirmed that the new assignment algorithm reduced the number of miles traveled by students, without accidentally worsening any disparities. PPSD plans to continue using the algorithm for SY 2021-22.\nThis project would not have been possible without close collaboration from the Office of the Deputy Superintendent for Operations (DSO) to guide which analyses would be most impactful to the district, as well as from the Office of Research, Planning and Accountability (RPA) who provided historical data as well as their deep knowledge of how that data was created.\nThis project evolved mostly in weekly meetings between key stakeholders, including DSO, RPA, and the school assignment team. We thank them for their partnership in this work.\nExceptions were made for students in specialized programs (e.g. special education programs) to ensure they received the appropriate services. Siblings continued to receive preference to be placed in the same school as their sibling. Seats at limited enrollment programs (e.g. Dual Language programs) were assigned by lottery.↩\nThe pandemic took a heavy toll on overall enrollment: PPSD saw a 20.6% decrease in kindergarten enrollment in SY 2020-2021 compared to the previous year. As such, not all schools had filled seats.↩\nBecause of the timing of the COVID-19 pandemic, the SY 2020-21 school choice window was underway when PPSD decided to adjust its policy. PPSD still retained the school selections for these students.↩\nWe used Rhode Island’s Star Ratings system to define school quality. See:↩\nWe determined the poverty rate for each census block group (CBG) by dividing the CBGs into quartiles based on the percentage of households within the CBG that earned below the poverty threshold as determined by the American Community Survey for 2019. CBGs were then assigned ‘Low’, ‘Medium’, ‘High’, and ‘Very High’ poverty rates according to the quartile they fell under.↩\nExcessive absence is defined as missing between 20% and 50% of all school days.↩\nThe Star rating system is on a five-point scale, with one being the lowest rating. All elementary schools in Providence had a 1, 2, or 3 Star rating; no elementary schools had 4 or 5 rating.↩\nHow to cite this Project: The Policy Lab. (2021, September 27). How can districts improve equity through school choice policies?. The Policy Lab. https://thepolicylab.brown.edu/projects/how-can-districts-improve-equity-through-school-choice-policies']	['<urn:uuid:bc3c6aa2-22df-4da5-8a9c-8531937d3cfd>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	23	92	1489
34	ancient egyptian scribe originality writing expression	An Egyptian scribe named Khakheperresenb, who lived around 2000 B.C., expressed a desire for originality in writing by stating 'Would I had phrases that are not known, utterances that are strange, in new language that has not been used, free from repetition, not an utterance which has grown stale, which men of old have spoken.'	"['And most of us are far guiltier in stretching the chronological limits of what from the past we sift and coalesce into ideal. As Wordsworth was to say, in an article he wrote for Coleridge\'s The Friend (1809): ""There are two errors into which we may easily slip when thinking of past times."" One error lies in overlooking ""the large overbalance of worthlessness that has been swept away,"" and selecting only the very best as ""typical."" In our imaginative voyaging through the past, we are like those travelers through the jungle who are told where the grave mounds of giants from earlier days may be found. When we find the grave, with the remains of what may indeed prove to have been a giant, we then assume that he was typical (""There were giants in those days"") rather than that he had been given such a mound in the first place and then remembered simply because he happened to have been a giant. The second error is that we so quickly, in our habitual feelings, divide time merely into two parts, past and present, and then ""place these in the balance . . . not considering that the present is in our estimation not more than a period of thirty years, or half a century at most, and that the past is a mighty accumulation of many such periods."" It is precisely for these reasons that, as Ortega y Gasset was to say in our own century, every age will inevitably feel itself ""empty"" in comparison with the past.\nI always find it fascinating when someone generalizes as to what we all do mentally. Bate addresses other profound issues in this book as well, including taste, influence and recognized achievement in (Western) poetry.\nAs for writers, Bate makes it seem that the search for an original phrase or expression is not unique to modern times: Bate quotes (page 3) Khakheperresenb, an Egyptian scribe who lived sometime around 2000 B.C., who wrote, ""Would I had phrases that are not known, utterances that are strange, in new language that has not been used, free from repetition, not an utterance which has grown stale, which men of old have spoken.""\nHere is another passage I especially like. In it, Bate considers the achievements of the English Romantic poets and what directions may afford opportunities for writers today (pages 115 - 116):\nAnd yet, with all the strikes against them, the greater Romantics still succeeded (astonishingly, when we remember that in England we are dealing with only some twenty-five or thirty years, in a nation with about a twenty-fifth of the population of the English-speaking world now). To try to touch on what each of them did would demand not only another lecture but a series of lectures, and ideally a step-by-step biography of the drama of each writer\'s life. I use this moment to plead for a more sympathetic - a more psychologically and a more literarily informed - use of biography: a recognition of what the artist confronted in what were for him the most important things with which to struggle (his craft and his whole relation with tradition, with what has been done and with what he hopes can still be done). In comparison, so much to which we confine ourselves in literary biography is far less relevant - no more relevant than it would be for any number of other people who had devoted their years to doing nothing. (It is like assuming, as Coleridge said, that every ""deer-stealer"" had it in him to become a Shakespeare.) Strangely, biographies of statesmen or scientists (or artists in other fields) are less guilty of this reductionism to the ""deer-stealer"" approach, and will focus primarily on what the man really did, why and how he was great: the situation he inherited and his struggle with that inheritance. Why are we alone so shy of the essential? As with biography, so with the reconsideration of literary history itself that we now seem about to make: here too these concerns could profitably be nearer the center of our thinking.\nIf we are forced to try to answer our question in a few sentences, we have only to repeat the cliches about Romanticism - but with a special imaginative sympathy for the particular question we have been discussing here - and we can get a tolerable notion of what at least permitted, if it did not create, this remarkable end-product of the eighteenth century, which provided the creative capital off which the nineteenth century and much of the twentieth (though in the latter case uneasily) has continued to live. For example, one answer is surely to be found in the opening up of new subject matters where the challenge of the past was less oppressive: simple life (of which there were to be twentieth-century urban as well as romantically rural varieties), children, the poor and socially slighted; landscape and scenery; such inward experiences as revery, dream, and mysticism; the whole concept of the ""strange"" either to awaken attention through difference in mode or phrase, to explore something really new, or to provide setting and focus for familiar nostalgia; the past itself in periods or ways not previously exploited by the traditional genres; the geographically remote or unusual, or conversely its apparent opposite (for example, Wordsworth; or the young Emerson on the central challenge of the age: ""I ask not for the great, the remote . . . I embrace the common, I explore and sit at the feet of the familiar, the low""). Every attempt to ""define"" Romanticism in the light of a subject is doomed to failure except as it applies to a limited part. For the opening of new subject matters, as of approach, proceeds in almost every direction, like spokes pointing outward from the hub of a wheel but with no rim to encase them. The one thing they all have in common is an interest or hope in the hitherto unexploited. And despite the strong attraction of twentieth-century post-romantic formalism to ideals of retrenchment and self-limitation, that still remains with us as a premise with which we are disinclined to quarrel.']"	['<urn:uuid:e47b1aeb-c059-40e5-91ae-dbd1eb72e76c>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	6	55	1029
35	What are the three main zones in security systems engineering?	In secure engineering and architecture, there are three overlapping zones: Zone 1 contains the system of interest itself, Zone 2 encompasses assets within the immediate operating environment of the system of interest, and Zone 3 includes assets outside the operating environment that influence the system of interest.	"['Security & Privacy By Design\nWith the European Union General Data Protection Regulation (EU GDPR) effective in mid-2018 and the California Consumer Privacy Act (CCPA) on the near-horizon for 2020, companies have an obligation to demonstrate they implement both Security by Design (SbD) and Privacy by Design (PbD). Unfortunately, most businesses lack the knowledge and experience to undertake such documentation efforts. That means businesses are faced to either outsource the work to expensive consultants or they ignore the requirement and hope they do not get in trouble for being non-compliant with this compliance requirement. In either situation, it is not a good place to be. The good news is that ComplianceForge developed a viable cybersecurity and privacy program that is based on NIST 800-160 guidance for security by design and OASIS for privacy by design.\nProfessionally-Written, Editable NIST 800-160 & OASIS PMRM-Based Cybersecurity For Privacy by Design (C4P) Program\nThe Security & Privacy By Design (SPBD) product is designed to support your company’s existing policies and standards. Our solution is focused at the procedural and guideline levels.\nProduct Example - Security & Privacy By Design (SPBD)\nThe SPBD addresses program-level guidance on HOW to actually manage cybersecurity and privacy principles, so that secure processes are designed and implemented by default. Policies & standards are absolutely necessary to an organization, but they fail to describe HOW privacy and security principles are actually planned and managed. The SPBD provides this middle ground between high-level policies and the actual procedures of how developers, PMs, system integrators and system admins do their jobs to design, implement and maintain technology solutions.\n|Watch Our Product Walkthrough Video||View Product Example|\nCost Savings Estimate - Security & Privacy By Design (SPBD)\nWhen you look at the costs associated with either (1) hiring an external consultant to write cybersecurity documentation for you or (2) tasking your internal staff to write it, the cost comparisons paint a clear picture that buying from ComplianceForge is the logical option. Compared to hiring a consultant, you can save months of wait time and tens of thousands of dollars. Whereas, compared to writing your own documentation, you can potentially save hundreds of work hours and the associated cost of lost productivity. Purchasing the SPBD from ComplianceForge offers these fundamental advantages when compared to the other options for obtaining quality cybersecurity documentation:\n- For your internal staff to generate comparable documentation, it would take them an estimated 250 internal staff work hours, which equates to a cost of approximately $18,750 in staff-related expenses. This is about 4-8 months of development time where your staff would be diverted from other work.\n- If you hire a consultant to generate this documentation, it would take them an estimated 160 consultant work hours, which equates to a cost of approximately $48,000. This is about 3-4 months of development time for a contractor to provide you with the deliverable.\n- The SPBD is approximately 7% of the cost for a consultant or 17% of the cost of your internal staff to generate equivalent documentation.\n- We process most orders the same business day so you can potentially start working with the SPBD the same day you place your order.\nThe process of writing cybersecurity documentation can take an internal team many months and it involves pulling your most senior and experienced cybersecurity experts away from operational duties to assist in the process, which is generally not the most efficient use of their time. In addition to the immense cost of hiring a cybersecurity consultant at $300/hr+ to write this documentation for you, the time to schedule a consultant, provide guidance and get the deliverable product can take months. Even when you bring in a consultant, this also requires involvement from your internal team for quality control and answering questions, so the impact is not limited to just the consultant\'s time being consumed.\nCybersecurity & Privacy By Design - Program Level Privacy & Security Documentation\nThe SPBD can serve as a foundational element in your organization\'s privacy program. It can stand alone or be paired with other specialized products we offer.\nCybersecurity and privacy do not need to be hard. The Security & Privacy By Design (SPBD) document is meant to simplify how security and privacy can be operationalized in a “paint by numbers” approach. This product is comprised of editable Microsoft Word and Excel documentation so you can customize it for your specific needs.\nPlease keep in mind that security & privacy engineering principles are widely expected activities:\nWhat Is The Security & Privacy by Design (SPBD)?\nOur products are one-time purchases with no software to install - you are buying Microsoft Office-based documentation templates that you can edit for your specific needs. If you can use Microsoft Office or OpenOffice, you can use this product! The SPBD comes in both editable Microsoft Word and Excel formats. The SPBD is capable of scaling for any sized company.\n- The SPBD is an editable Microsoft Word document that providers program-level guidance to directly supports your company\'s policies and standards for ensuring secure engineering and privacy principles are operationalized.\n- This product addresses the “how?” questions for how your company ensures both security and privacy principles are operationalized.\n- It is a reality that most companies have either weak or non-existent guidance on how security or privacy principles are implemented.\n- The lack of operationalized security & privacy principles can lead to compliance deficiencies with many statutory, regulatory and contractual obligations.\n- NIST 800-160 is the ""gold standard"" on how to build security into the System Development Life Cycle (SDLC)\n- The concept of “secure engineering” is mandatory in numerous statutory, regulatory and contractual requirements.\n- The SPBD provides a “paint by numbers” approach to ensure your company has evidence of both due care and due diligence for operationalizing security and privacy principles.\n- The CIRP is based on numerous frameworks, but the core principles are based on NIST 800-160 and the Generally Accepted Privacy Principles (GAPP) which are the de facto standards on security and privacy design principles.\nWhat Problem Does The SPBD Solve?\n- Lack of In House Security Experience - Writing cybersecurity & privacy documentation is a skill that most cybersecurity professionals simply are not proficient at and avoid the task at all cost. Tasking your security analysts and engineers to write comprehensive procedure documentation means you are actively taking them away from protecting and defending your network, which is not a wise use of their time. The SPBD is an efficient method to obtain comprehensive guidance documentation to implement cybersecurity and privacy principles within your organization!\n- Compliance Requirements - EU GDPR requires companies that store, process or transmit the personal information of EU citizens to ensure that both cybersecurity and privacy principles are built into processes by default. Can you prove how cybersecurity & privacy principles are implemented?\n- Audit Failures - Security documentation does not age gracefully like a fine wine. Outdated documentation leads to gaps that expose organizations to audit failures and system compromises. The SPBD provide mapping to leading security and privacy frameworks to show you exactly what is required to both stay secure and compliant.\n- Vendor Requirements - It is very common for clients and partners to request evidence of a security program and this includes policies, standards and procedures. With EU GDPR, vendors and other partners will be expected to demonstrate evidence of compliance with the EU GDPR.\nHow Does The SPBD Solve It?\n- Clear Documentation - The SPBD provides a comprehensive approach to operationalizing both cybersecurity and privacy principles. This equates to a time saving of hundreds of hours and tens of thousands of dollars in staff and consultant expenses!\n- Time Savings - The SPBD can provide your organization with a templated solution that requires minimal resources to fine tune for your organization\'s specific cybersecurity and privacy needs.\n- Alignment With Leading Practices - The SPBD is written to support leading cybersecurity and privacy frameworks!\nReducing Risk Through Cybersecurity For Privacy by Design (C4P)\nThe Security & Privacy By Design (SPBD) document supports your company’s existing policies and standards. Our solution is focused at the procedural and guideline levels. The SPBD document is focused on understanding risk associated with cybersecurity and privacy so that risk can be:\n- Transferred; or\nImplementing both Security by Design (SbD) and Privacy by Design (PbD) principles is a systematic way to find and address weaknesses, flaws and risks to your company.\n- Repeatable, methodical processes that seek out both security and privacy risk reduces the chance of surprises.\n- Addressing security issues in an orderly manner gives your company a better assurance that gaps have been closed properly and as quickly as possible.\nWork Smarter! Leverage Common Touch Points Between Cybersecurity & Privacy\nSystems security engineering delivers systems deemed adequately secure by stakeholders. The fundamental relationships among assets, an asset-dependent interpretation of loss, and the corresponding loss consequences are central to any discussion of system security.\nThis is where aligning your company’s Security by Design (SbD) efforts with the Risk Management Framework (RMF) (e.g., NIST 800-37) can be very beneficial, since the RMF provides a well-established format to securely engineer and maintain systems throughout the entire life cycle of the asset. Utilizing common linkages, Privacy by Design (PbD) is incorporated into the RMF cycle.\nPaint By Numbers - Cybersecurity & Privacy Requirements\nWhat we\'ve done is take on the heavy lifting to integrate security and privacy controls into standard project management processes. This allows your teams to have a ""paint by numbers"" approach to demonstrating that both cybersecurity and privacy principles are baked into the process! We identified the stages where both cybersecurity and privacy requirements are expected as part of project development. This can enable your teams to work more effectively together and reduce the negative effect of teams working in silos.\nAll too often, when projects are commenced, involvement from key stakeholders is siloed, as compared to operating as a cohesive team. We want to help your company avoid the following security & privacy pitfalls where:\n- Project / application teams work in a vacuum, unaware of security or privacy concerns;\n- Privacy and security conduct their own assessments without any information sharing or collaboration; and\n- Security involvement is viewed as a final hurdle to overcome, just prior to “go live” for the project.\nThe SPBD Excel checklists provide a wealth of experience to bake in security and privacy principles by establishing methodical and repeatable processes.\n- Logically-organized phases\n- Task focus (How tasks support the lifecycle phases)\n- Task #\n- Activity Description\n- Reasonable Task Deliverables\n- Mapping to leading practices:\n- NIST 800-160\n- NIST 800-53\n- ISO 27002\n- OASIS PMRM\n- Level of Effort (expectation for basics or enhanced requirements)\n- Stakeholder RACI Matrix (Responsible, Accountable, Consulted, Informed)\nIn addition to logically organizing steps, we went the extra mile by calling out the deliverables expected and tied it to task #:\n- Proposed solution is documented that captures security-relevant criteria and tentative requirements.\n- Listing of applicable statutory, regulatory and contractual requirements are defined.\n- Business & technical constraints are identified and documented.\n- Data classification is identified.\n- System criticality is identified.\n- Data protection requirements are defined (e.g., controls) based on docuemented data classification and system criticality.\n- ""Best practices"" are defined to be used in the design & implementation of systems, applications and services (e.g., OWASP, NIST, DISA STIGs, etc.).\n- System hardening baselines (e.g., configuration management requirements) are defined and documented.\n- Security Concept of Operations (CONOPS) are defined and documented.\n- is defined and documented.\n- Standardized Operating Procedures (SOP) are documented.\n- Service Level Agreement(s) (SLAs) are defined and documented\n- Tentative life cycle is identified.\n- Roles and responsibilities for security requirements are assigned and documented.\n- Risk Assessment is conducted and a Risk Register (RR) is used to document findings.\n- Business Impact Analysis (BIA) is conducted and documented.\n- Privacy Impact Assessment (PIA) is conducted or modified.\n- Project stakeholder list is defined and documented (strategic personnel, business units and third parties).\n- Threat assessment is conducted and documented.\n- List of constraints (facts & assumptions) is defined.\n- Listing of expected systems and services that will be required to support the proposed solution is defined.\n- System Security Plan (SSP) is documented or modified.\n- Change Control Board (CCB) change request(s).\n- High Level Diagram (HLD) is documented.\n- Low Level Diagram (LLD) is documented.\n- Data Flow Diagram (DFD) is documented.\n- Plan of Action & Milestones (POA&M) is documented or modified.\n- End user training material is developed.\n- Security awareness training is provided.\n- Information Assurance (IA) testing (certification &accreditation) is commenced.\n- Key Performance Indicators (KPIs) are identified.\n- Authorization is granted (e.g., Authority To Operate (ATO) , Interim Authority To Operate (IATO) or Denied Authority To Operate (DATO)).\n- User Acceptance Testing (UAT) is conducted and documented.\nUnderstanding Privacy & Security Starts With Defining Requirements\nUnderstanding the requirements for both Security by Design (SbD) and Privacy by Design (PbD) principles involves a simple process of distilling expectations. This process is all part of documenting reasonable expectations to right-size the approach, since every organization is unique:\nSecurity by Design (SbD)\nPrivacy by Design (PbD)\nSecurity by Design (SbD) requirements come from numerous sources. In this context, the most important are:\nPrivacy by Design (PbD) requirements come from numerous sources. In this context, the most important are:\nData-Centric Security (DCS) = Defense-In-Depth Approach To Security\nThinking in terms of data, or information, it is your company’s most valuable asset. Therefore, being ""data-centric"" is how we approach our defense-in-depth concept. When you look at the diagram below, if you envision data protection as a set of concentric rings, at the center of the protection is your data.\nZone-Based Approach To Secure Engineering\nFrom a secure engineering and architecture perspective, it is worthwhile to take a zone-based approach to scoping an environment for secure systems engineering. This effort is meant to focus on particular systems of interest, while taking into account the systems elements and enabling systems that compose the system of interest. This supports the concept of Data-Centric Security (DCS), since the focus encompasses everything that either stores, processes or transmits the data in question, as well as the supporting infrastructure and services.\nFrom this perspective, assets can be logically grouped into three (3) overlapping zones:\nZone 1 – The asset is a system of interest;\nZone 2 – The asset exists within the immediate operating environment of a system of interest; or\nZone 3 – The asset exists outside of the operating environment but influences the system of interest.\nMethodical Approach To Privacy By Design (PbD)\nThe OASIS Privacy Management Reference Model and Methodology (PMRM) is a privacy framework that assists in operationalizing Privacy by Design. Thee PMRM identifies eight (8) privacy services that are needed to operate at a functional level. These services are meant to clarify the “architectural” relationships and can be logically grouped into three (3) categories: Core policy services, Privacy assurance services; and Presentation & lifecycle services.\nThe Security & Privacy By Design (SPBD) includes an editable checklist for PMRM controls. This is tied to the security controls, so it is easy to link both cybersecurity and privacy requirements. This allows for a more cohesive assessment and encourages information sharing. The end product is a more comprehensive assessment of risk to both privacy and security.']"	['<urn:uuid:bdbcf3b8-e46b-434a-a8b8-6058e7cc719c>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	10	47	2579
36	I noticed my regular light bulbs at home give off different colored light compared to the ones at my office - what's actually causing this difference in lighting?	The difference is due to the varying color temperatures of different types of light bulbs. Incandescent tungsten light bulbs used in homes typically measure between 2600-3200 Kelvin, while typical office fluorescent lights are around 4000 Kelvin.	"[""- Digital Camera Basics: Mastering Color Controls\n- The Different Colors of Light\n- What Is White Balance and How Does It Work?\n- Understanding Color Temperature\n- Choosing When and How To Use Your Camera's White Balance Settings\n- Changing Your White Balance Settings\n- Auto White Balance\n- White Balance Presets\n- Manual White Balance\n- Selecting Specific Color Temperatures\n- Reality Check: Do You Need White Balance or Simply Better Exposure Settings?\n- Using Your Camera's Other Color Controls\n- The Indefinable Essence of Color\n- Color Modes To Change Your Mood\n- Vivid Versus Natural Color\n- Saturating Your Colors\n- Black and White\n- Creating Antique-Like Heirloom Photos\n- Infrared Beauty: Colors Beyond the Visible\nUnderstanding Color Temperature\nWhile professional photographers will speak about the coolness (blueness) or warmth (reddishness) of light, they must be more precise when they are setting up a professional photo shoot. That's when they take out their color temperature meter or colorimeter and measure the temperature of light falling on their subject. Color temperature is measured in degrees Kelvin (K).\nIn the mid-nineteenth century, Lord Kelvin sought to establish a temperature scale that would have no negative values. Therefore, the Kelvin scale begins at absolute zero, below which molecular energy ceases. But what are important to photographers are Lord Kelvin's experiments with heating a carbon rod to such high temperatures that it changed colors, becoming red hot and then blue. It is this correlation of color to temperature that is the basis of the photographer's color temperature meter.\n5000 to 5500 K is generally considered to be daylight. However, as we mentioned earlier in the chapter, daylight fluctuates. So, 5000 to 5500 K is an idealized value of daylight at midday in June in the temperate zone of the northern hemisphere, rather than a true, constant value. Real daylight (which is a combination of sunlight and ambient skylight) can measure anywhere from about 2000 to 30,000 K depending on the time of day, the month of the year, latitude, altitude, and atmospheric conditions. For instance, sunrise (or sunset) can be 2000 K. One hour later it might go up to about 3500 K. Noontime summer skies in Washington, D.C., average about 5400 K. (Source: Kodak.com)\nSimilarly, unless your lights are rated and guaranteed for a specific color temperature, their values will vary. So, all we can talk about are average color temperatures in typical lighting situations. Incandescent tungsten light bulbs are generally about 2600 K to 3200 K (depending on the wattage), and typical inexpensive office fluorescents are about 4000 K. Candlelight, incidentally, is about 1800 K.\nTo make things even more complicated, light bulbs can age, and their color temperatures usually reduce as they get older. In addition, electrical current fluctuates and can influence how hot or cool a light bulb burns. So, a 3200 K tungsten bulb might dip periodically to 2800 or peak as high as 3400 K, and it can change second by second. That's why photographers use line stabilizers and power conditioners to keep the voltage constant, so color temperature won't vary during their shoot.\nIn DigitalBenchmarks Lab, we use special (and very expensive) full spread spectrum non-flicker 5000 K Kaiser fluorescent lights on line stabilizers and power conditioners for our simulated daylight setups. Even so, we measure their color temperature before, during, and after all test shoots. If the color temperature is slightly outside our normal parameters, we usually wait for the bulbs to warm up some more. If the variation still persists, we replace the bulbs.\nblog comments powered by Disqus""]"	['<urn:uuid:37e8cad3-8737-45f7-8a78-0d094121dccd>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	28	36	598
37	What should someone look out for when mailing frozen food to make sure it arrives safely?	When shipping frozen food through mail, several key steps must be followed. First, you need to know the required temperature and shelf life of your product. Then, use proper insulated containers - typically Styrofoam boxes inside cardboard boxes provide the best insulation. For frozen products, dry ice should be used as a cooling agent (though this requires hazardous material labeling), while ice packs work well for refrigerated items. The amount of cooling agent needed depends on the product size, required temperature, transit time, and climate. Finally, it's crucial to choose a shipping partner with guaranteed delivery times to ensure the product arrives safely.	['Guide to Trucking Perishable GoodsAugust 21, 2018\nWhether you are shipping produce, floral arrangements or pharmaceuticals, maintaining a cold chain from production to delivery is essential. If the temperature is not adequately controlled at all stages of production, your perishable goods could spoil or even make your customers sick. But for a small company, delivering perishable goods safely can be an intimidating task. You want to make sure your products arrive fresh and safe for consumption, but you may not know much about logistics management or shipping regulations.\nDepending on the size of your business and the goods you need to deliver, you may opt to ship your perishable goods in the mail or use a reefer truck for ground deliveries. No matter which method you choose, you must comply with federal regulations and maintain best practices for delivering perishable goods. Use this guide to perishable goods shipping to find the best delivery method for your business.\nHow Do I Ship Perishable Food and Goods?\nFor companies sending smaller deliveries directly to customers, shipping perishable goods in the mail may be the best option. Shipping cold or frozen products in the mail requires proper planning and care to make sure everything arrives safely to your customers. Follow these step-by-step instructions on how to ship perishable goods.\n1. Know Your Products\nBefore you begin shipping goods, make sure you know the temperature at which you must keep each product, so it stays fresh. You should know the shelf life of each product to determine how quickly it will need to be shipped. These factors will help you decide which type of packaging is best for your products.\n2. Choose a Container\nA variety of shipping containers are available for shipping perishable goods. These special packages provide insulation to keep out heat. You can purchase styrofoam boxes with thicker or thinner walls to keep out more or less heat, and they’re generally the most effective at maintaining temperatures. Typically, Styrofoam boxes are shipped inside of cardboard boxes for better durability during transport.\nYou can also purchase Styrofoam sheets, insulated liners or insulated pads to place inside of a cardboard box. Sheets and pads are often more cost-efficient than Styrofoam boxes, as you can purchase them in bulk and cut them to the right size for each package. You can also wrap insulated liners around your product for extra protection.\n3. Select a Cooling Agent\nOnce your product is packaged, you will need to add a cooling agent to keep your product cold during shipping. Ice packs are typically a good option for refrigerated goods, while dry ice is best for frozen products. Dry ice is considered a hazardous material, so you’ll need to clearly label any your packages containing dry ice and should check for additional requirements from your shipping provider.\nWhen determining how much dry ice or how many ice packs to use, consider the size of the product, what temperature you need to maintain, how long it will be in transit and the climate of the area it’s shipping to. Shipping trucks can reach very high heats in the summer, so be sure to take this into account when adding your cooling agent.\n4. Get It There Fast\nChoose a shipping partner that can provide guaranteed shipping times, so you know your product will be delivered on time. You should also balance speed with cost, as overnight or one-day shipping can be very expensive. If you want to provide a reasonable shipping fee for your customers, choose the longest shipping time that will still keep your product fresh and safe.\nChallenges of Shipping Perishable Goods in the Mail\nThe cost to ship frozen food can be expensive, as you will need to send it quickly and ensure that proper temperatures are maintained. Ice packs and dry ice also take up extra space, meaning you may need to use larger shipping containers, which can add additional cost.\nAim to use the smallest possible container for your product and choose the longest shipping time that will still keep your product safe. No matter what shipping method you select, make sure to have your shipping policies readily available on your company’s website so customers can understand the associated costs.\nPerishable Goods Delivery With Reefer Trucks\nWhen transporting large quantities of perishable goods, a reefer truck may be the best option. Reefer trucks have internal cooling systems that can maintain a specific temperature inside the trailer. These trucks can be set at a range of temperatures from freezing to room temperature.\nReefer trucks were invented in the 1930s as a solution to the problem of transporting cold and frozen goods safely. Manufacturers and producers wrestled with this question for decades, employing the first attempt at a cold shipment method on trains in the mid-19th century. Railroad companies filled train cars with large blocks of ice in an effort to keep products cold as they traveled across the country.\nAfter the invention of automobiles, drivers carried this method over to trucks and also used insulation to keep out heat. Fans in the vehicles also helped to move cold air around the container. This method was not sufficient, however, because drivers would still have to stop periodically to refresh the ice.\nIn the 1930s, inventor Fred Jones created a portable cooling unit that relied on the truck’s motor for power, and drivers could add it to the outside of shipping containers. These first refrigerated trucks allowed critical perishable items — such as food, blood and medicine — to be shipped to soldiers during WWII.\nRefrigeration technology for transporting goods has continued to develop, and reefer trucks are now available with a wide variety of capabilities. Modern refrigerated trucks are powered by diesel and can be adjusted to exact temperature specifications. For smaller shipping needs, refrigerated vans are also available.\nIn many reefer trucks, internal technology allows drivers to continuously monitor the temperature inside the truck to ensure products are stored properly during the entire shipping process. If temperature monitoring is not already built into a reefer truck, additional devices must be used for accurate temperature maintenance and recording.\nWhen to Use a Cold or Frozen Truck\nUsing a reefer truck can be an affordable, efficient way to transport perishable goods. Reefer trucks are valued for their ability to control both temperature and humidity, making them perfect for ensuring freshness of a variety of products.\nFrozen food shipping companies use reefer trucks to keep products frozen during transport, while fresh produce is shipped in refrigerated trucks so it does not spoil or become contaminated. While cold and frozen trucks are most often used in the food and beverage industry, they have many applications in other industries, as well.\nBelow are examples of perishable goods that many companies often transport using reefer trucks:\n- Frozen foods\n- Produce, dairy or meats\n- Cakes or baked goods\n- Vaccines and medications\n- Chemicals or engineered materials\n- Fine art or antiques that require humidity control\n- Perfumes and cosmetics\n- Tobacco products\n- Floral arrangements\nBenefits of Reefer Trucks for Perishable Goods Delivery\nReefer trucks offer many benefits for perishable goods delivery that are not available using traditional shipping methods. Below are a few reasons to use a cold or frozen truck for your perishable products:\n- Cost-Efficient: Reefer trucks allow you to ship more products at once while cutting back on packaging costs.\n- Great for Shorter Transports: You can complete local deliveries more quickly with a reefer truck than via mail, and your customers will be pleased with your fast delivery services.\n- Perfect for Large Shipments: Reefer trucks have the capacity for large shipments that would be costly to send via mail. If you have high-volume clients, they will appreciate receiving their deliveries quickly and at the same time rather than in several different packages.\n- Better Temperature Control: When using a reefer truck to deliver perishable goods, you can continuously monitor the temperature to ensure high-quality. When shipping products in the mail, you cannot control the temperature of the product once it leaves your business.\nPerishable Goods Delivery Regulations\nDepending on what type of perishable goods your company is transporting, there may be specific regulations that your logistics team must stay in compliance with to avoid liability. If you are carrying floral arrangements or other perishable products not meant for consumption, your primary concern will be maintaining quality rather than ensuring safety. If you are delivering food or pharmaceutical products, however, you must be aware of the applicable federal regulations.\nThe Food and Drug Administration (FDA) released the Food Safety Modernization Act (FSMA) in 2011 in an attempt to prevent food contamination that has become a growing problem in the U.S. As part of this regulation, the FDA specifies safe practices for transporting human and animal food. The rule on Sanitary Transportation of Human and Animal Food sets guidelines for maintenance and design of transportation vehicles, transportation operations, training procedures and record-keeping.\nAll vehicles transporting human or animal food must be made of materials that are able to be adequately cleaned and sanitized, such as stainless steel. Refrigerated shipping vehicles must be able to maintain the appropriate temperatures for food transport and have functioning temperature controls. During transportation, reefer truck temperatures must be maintained and monitored. Companies must keep accurate records of food temperatures at each stage of shipping, including loading, transport and unloading. Then, they must fully disclose these records when requested and retain them for up to 12 months.\nBelow are some other best practices to keep in mind when shipping food or beverages in reefer trucks:\n- Always pre-cool the truck before loading the product.\n- Always pre-cool the product before loading it into the vehicle. Reefer trucks are designed to maintain temperatures, not to lower the product’s temperature. This is particularly important with produce that has a high rate of respiration.\n- Verify that the reefer truck is at the correct set point required for the product you’re shipping.\n- Set the temperature mode to continuous, so the reefer truck actively maintains the set temperature. A continuous setting will have a set defrost cycle, so ice and frost do not build up on the truck’s cooling mechanisms. If the truck will be making a lot of stops and the door will frequently be open, you may need to adjust the defrost cycle, so the vehicle defrosts more regularly.\nWhen delivering pharmaceutical products, you must comply with the FDA’s Code of Federal Regulations. Pharmaceutical products must not be handled or stored in a way that could adversely affect their quality, including proper temperature and humidity control during processing and transport. FDA regulations also require accurate record-keeping throughout the transportation and distribution process.\nOutsourcing Your Transportation for Perishable Goods\nAs your company grows, you may need to develop a more advanced cold chain system for delivering your perishable goods. You may have been able to rely on shipping products in the mail before, but as your capacity grows, you will need a more efficient and cost-effective delivery system.\nManaging your own logistics system will mean hiring staff members and buying or renting reefer trucks. If you prefer to focus on creating your product rather than worrying about shipping, consider using a third-party logistics (3PL) partner to manage your perishable goods delivery. Outsourcing your perishable goods delivery system has many benefits that will save you time and money while allowing you to focus on what you do best — running your business.\nBelow are a few benefits of outsourcing your perishable goods transportation:\n- Less Stress: Your 3PL partner will be knowledgeable about regulations and policies that affect perishable goods delivery. They will handle all the shipping paperwork, so you can be confident your goods arrive at their destinations safely and within regulations.\n- More Flexibility: Working with a 3PL company means you’ll have access to different sizes of trucks and shipping vehicles to tackle any job. Rather than sending a truck that is only half-full, your shipping partner can supply the right-sized vehicle for the job. An excellent 3PL partner will be able to adapt to your changing needs on a day-to-day basis.\n- Save Money: When your 3PL partner handles your shipping paperwork and transportation management, you do not have to hire your own staff for the job, saving you time and money. The 3PL market has grown in recent years, making it an accessible and affordable option for small and large companies alike.\nOutsourcing your perishable goods transportation will allow you to focus on growing your business, rather than worrying about how your products will get to your customers. Save time, money and stress by outsourcing your perishable goods delivery.\nTrust Mitchell’sNY Logistic for Your Perishable Goods Delivery\nMitchell’sNY Logistic is a 3PL company serving New York City and the tri-state area since 1946. With decades of experience behind us, we know the tri-state area better than anyone else and have an unbeatable record of on-time performance. Our local expertise means we can supply fast, accurate perishable goods delivery in Manhattan and beyond, with consistency you can trust. We even offer two-hour deliveries anywhere in Manhattan, so you can fulfill orders faster and keep your customers happy.\nWhen you choose Mitchell’sNY Logistic for your refrigerated truck delivery service, our expert team will ensure your perishable goods always arrive at the proper temperature and ready for your customers to enjoy. We deliver to businesses, hospitals and residential addresses to meet all of your delivery needs. Whether you’re an individual seeking fresh grocery delivery or a hospital receiving pharmaceutical supplies, Mitchell’sNY Logistic can provide perishable goods delivery for you. Our reefer truck delivery offers temperature control up to 65°F, so you can ship anything from frozen fruit to fresh flowers.\nWhen your business works around the clock, Mitchell’sNY Logistic can offer early morning, late evening and weekend deliveries. We’ll help you meet deadlines and impress your customers with prompt delivery of refrigerated or frozen goods anywhere on the east coast. Plus, our reefer trucks have GPS tracking, so you will know exactly when your products have arrived.\nRest easy when you partner with Mitchell’sNY Logistic as your perishable goods delivery partner. We pride ourselves in handling each delivery with special care and a personal touch you won’t find at a larger company. Our team wants to provide the best perishable goods delivery service to you, so you can offer the best products to your customers. To hear more about our frozen and refrigerated truck services, contact us today for rates and services.']	['<urn:uuid:92a4300d-0bb3-4c16-97be-8e9a44887e1e>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	16	103	2429
38	yard peonies dying stems wilting grey fuzzy stuff on flowers what can do prevent next season	Your peonies are affected by Botrytis Blight, a fungal disease that attacks stems, buds and leaves. It causes young stalks to discolor at the base, wilt and fall over, and creates brown flower buds covered in gray, fuzzy fungal spores. To reduce disease pressure for next year, you should cut back diseased peonies to the ground in fall and dispose of infected material. For additional protection, spray copper sulfate when shoots are 6 inches tall next season.	"[""RIDEOUT: Rain effects plant disease\nPlant diseases occur in most gardens; many on a yearly basis, others sporadically.\nWhy disease occurs often is dependent on weather. In fact, there are three conditions that must exist for a disease to be present — a host, a pathogen and a favorable environment.\nObviously, you must have a susceptible host plant for disease symptoms to occur. The word susceptible used here does not necessarily mean a damaged or less thrifty plant, although that may increase susceptibility. Even seemingly healthy plants may be susceptible to diseases. Red Tip Photinia is one example of a plant that even when given perfect care will become infected with a disease — Entomosporium Leaf Spot. Research and plant breeding is used to develop new plant cultivars that have resistance to diseases, for example the Profusion series of Zinnias are resistant to Powdery Mildew.\nPlant pathogens (i.e., bacteria, fungi, virus) may be present year round but only cause disease when environmental conditions favor infection and disease development. The fungus that causes rust on tall fescue overwinters as spores on infected grasses. The disease does not infect the plant until a period of warm days and cool nights occur coupled with heavy dew or rain showers that keep the turf wet for several hours giving the spores a chance to germinate and infect the plant. A few days later you see the rust symptoms, orange to reddish brown pustules on the leaf surface.\nDogwood Anthracnose, caused by the fungus Discula, is favored by wet springs and we may expect to see more of it this year than in the past. Dogwood Anthracnose is common on flowering dogwood (Cornus florida) and appears as large, brown irregular shaped blotches on the leaves. These blotches are often located along the leaf mid-vein. Twigs are also infected and sunken brown spots, which develop on the tissue, will cause twig dieback. Infected leaves usually drop early; many times defoliation will be severe. Fungicide sprays of Cleary's 3336 or Mancozeb on the leaves and twigs from bud break through summer can help prevent this disease.\nPeony, a showy garden perennial, is another plant that suffers from diseases encouraged by excessive rain. Botrytis Blight is caused by the fungus Botrytis which attacks stems, buds and leaves. Young stalks discolor at the base, wilt and fall over. Botrytis is most common in cloudy, rainy weather. Wilt and shoot death continues with wet conditions. Other symptoms include large, irregular shaped leaves or brown flower buds covered in a mass of gray, fuzzy, fungal spores. To reduce disease pressure for next year, cut back diseased peonies to the ground in the fall and dispose of the infected material. Sprays of copper sulfate next season when the shoots are 6 inches tall will also help protect the plant.\nDiseases encouraged by rainy weather are not limited to ornamental plants. Vegetables, especially tomatoes, may suffer from diseases when plants stay wet. Septoria Leaf Spot, Early Blight, Bacterial Speck and Bacterial Spot are all influenced by rainy, wet weather. Septoria Leaf Spot overwinters on tomato plant debris left in the garden. Spores are spread from the debris to the plant when rain drops splash them onto the lower leaves. The disease then spreads during warm, wet weather causing circular spots with gray centers on leaves, lesions on stems and yellowing or dieback of infected leaves and petioles. Early Blight also overwinters on debris and is spread by splashing rain. Irregular dark brown areas, with target-like concentric rings, are the major symptoms of this fungus. These rings may appear on leaves, stem or the fruit.\nRemoving last year's tomatoes or rotating planting sites is the best way to avoid these diseases. A chlorothalonil-based fungicide will also provide control. Bacterial Speck and Bacterial Spot are introduced into gardens on infected plants. They persist in the garden on infected tomato plant debris and the bacteria are spread to healthy plants by wind-driven rain during thunderstorms or when gardeners are working in the garden when the foliage is wet. Both bacterial diseases develop during periods of warm, humid, wet weather. Symptoms appear as small, raised, water-soaked circular spots on the fruit often with a green halo. Garden rotation and removing plant debris will reduce sources of infection; copper-based fungicides will control these bacterial diseases.\nAlthough there is little we can do to stop the rain, knowing that rain encourages certain diseases may make you more aware of potential problems and more observant of your turfgrass, trees, perennials and vegetables. Researching your most prized plants and their associated disease problems will also help you know where to look for diseases and what, if anything, you can do to control them.\nDon't let disease pressures prevent you from gardening however. Most of the time, plants recover from disease as the rain slows down. For more information, contact me at the Henderson County Extension Office.\nContact Andy Rideout at the Henderson County Extension Office at email@example.com, at 270-826-8387 or stop by the Henderson County Cooperative Extension Service at 3341 Zion Rd, Henderson, Ky.""]"	['<urn:uuid:e3b86d90-c1e4-48be-9d46-7485b4c01f03>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	16	77	843
39	Which three countries haven't signed the NPT?	India, Israel and Pakistan are the only three States that have not signed the Non-Proliferation Treaty (NPT).	['Basic safeguards glossary\nThis glossary provides the technical terminology used in the Commission Regulation 302/05 and throughout the Nuclear Non-proliferation website. The IAEA Glossary website can be accessed through the link in the Related Link section of this website.\n- Additional Protocol\n- see Protocol Additional to Safeguards Agreements.\n- Basic Technical Characteristics (BTCs)\n- Design information on nuclear installations as provided for Euratom Safeguards. The BTCs include a description of the installation, the form, quantity, location and flow of nuclear material being used, the layout of the installation, containment features and procedures for nuclear material accountancy (skip to content) and control. The information is used, inter alia, to prepare the safeguards approach to the installation.\n- A portion of nuclear material handled as a unit for accounting purposes at a key measurement point and for which the composition and quantity are defined by a single set of specifications or measurements. The nuclear material may be in bulk form or contained in a number of separate items. Examples of batches are:\n- one fuel assembly;\n- one UF 6 cylinder;\n- a tray of pellets prepared for loading into one fuel rod;\n- several drums of UO 2 powder with the same specifications.\n- Book Inventory\n- A record of all the nuclear material held at a site and its location on the site. The book inventory consists of the results of the most recent physical inventory and of all the inventory changes that have occurred since that physical inventory was taken.\n- Commission Regulation (Euratom) No 302/05\n- Regulation 302/05 provides the current legal framework for the application of safeguards under the EURATOM Treaty.\n- Comprehensive Safeguards Agreements\n- Agreements made with the IAEA by non-nuclear weapon States (NNWS) to enable the application of safeguards on all source and special fissionable material in all peaceful nuclear activities, as required by the NPT. The model text for these agreements is published as IAEA document INFCIRC 153.\n- Containment involves the use of seals to provide assurance of the physical integrity of an area or item in order to prevent undetected access to or movement of nuclear material or safeguards-relevant equipment and data.\n- Depleted Uranium\n- Uranium containing less than the natural abundance of the isotope uranium-235. (Natural uranium contains about 0.7% U-235.)\n- Destructive Assay\n- The analysis of nuclear materials using methods, which involve the destruction of a sample, e.g. chemical analysis or mass spectrometry.\n- Effective Kilogram\n- A special unit used in the safeguarding of nuclear material, reflecting its strategic value. A quantity in effective kilograms is obtained by taking:\n- for plutonium, its weight in kilograms;\n- for uranium with an enrichment of 1% (0.01) and above, its weight in kilograms multiplied by the square of its enrichment;\n- for uranium with an enrichment below 1% (0.01) and above 0.5% (0.005), its weight in kilograms multiplied by 0.0001; and\n- for depleted uranium with an enrichment of 0.5% (0.005) or below, and for thorium, its weight in kilograms multiplied by 0.00005.\n- EURATOM Treaty\n- see Treaty establishing the European Atomic Energy Community.\n- European Safeguards Research and Development Association (ESARDA)\n- A network of European organisations involved in the implementation and development of nuclear safeguards.\n- Fertile Material\n- Nuclear material that can be converted to fissile material through the capture of a neutron. An example of a fertile material is uranium-238.\n- Fissile Material\n- Fissile materials are capable of undergoing nuclear fission (splitting of the nucleus of an atom) by slow neutrons. Examples of fissile materials are uranium 235 and plutonium 239.\n- High Enriched Uranium (HEU)\n- Uranium containing 20% or more of the isotope uranium-235. A quantity of HEU can be described in terms of either the total mass of all the uranium isotopes, kg U, or as the mass of the fissile isotope uranium-235, kg U 235. For example, 100kg U of 70% enriched HEU could also be described as 70kg U 235.\n- The International Atomic Energy Agency, based in Vienna, is an independent intergovernmental United Nations organisation that serves as the global focal point for nuclear co-operation.\nThe IAEA assists its Member States in planning for and using nuclear science and technology for peaceful purposes. It develops nuclear safety standards and, based on these standards, promotes the achievement and maintenance of high levels of safety in applications of nuclear science. The IAEA also verifies, through its inspection system, that States comply with their commitments under the Non-Proliferation Treaty and other non-proliferation agreements to use nuclear material and facilities for peaceful purposes only.\n- The Institute of Nuclear Materials Management (INMM)\n- A US-based organisation that works to support the development of all aspects of nuclear materials management.\n- Inventory Change Report (ICR)\n- A report that describes changes in inventory of nuclear material in a material balance area. ICRs are usually provided on a monthly basis.\n- Key Measurement Point (KMP)\n- A location where nuclear material appears in such a form that it may be measured to determine material flow or inventory. KMPs include, but are not limited to, the inputs and outputs (including measured discards) and storages in material balance areas.\n- Low Enriched Uranium\n- Uranium containing less than 20% of the isotope uranium-235 but more than that which occurs in natural uranium. (Natural uranium contains about 0.7% U-235.)\n- Material Balance Area (MBA)\n- An area inside or outside of a facility such that:\n- the quantity of nuclear material in each transfer into or out of each MBA can be determined; and\n- the physical inventory of nuclear material in each MBA can be determined when necessary, in accordance with specified procedures, in order that the material balance for safeguards purposes can be established.\n- Material Balance Report (MBR)\nA report of the nuclear material in a material balance area or facility, which shows aggregated transactions for the material balance period (e.g. year) in comparing the physical inventory with the book inventory.\n- Nuclear Materials Balance (also known as Material Unaccounted For (MUF) or Inventory Difference (ID))\n- The difference between the physical inventory and the book inventory. The difference can be either positive (apparent gain of material) or negative (apparent loss of material). MUF is caused primarily by measurement uncertainties. Whilst any actual loss of material would be included in the MUF, the existence of a negative MUF does not necessarily indicate a real loss of material. The MUF values for any accountancy period are assessed for their acceptability in relation to the activities, which occurred over that period. Note that the Euratom convention for MUF (physical inventory - book inventory) is the opposite of the IAEA convention.\n- Measured Discards\nNuclear material which has been measured, or estimated on the basis of measurements, and disposed of in such a way that it is not suitable for further nuclear use. An example of a measured discard is a discharge to the environment.\n- Natural Uranium\n- Uranium as it occurs in nature, consisting of about 99.3% uranium-238, 0.7% uranium-235 and very small quantities of uranium-234.\n- Non-destructive Assay (NDA)\n- The measurement of the nuclear material content of an item without producing significant physical or chemical changes in the item. NDA usually involves measurement of the radioactivity of the item for comparison with a calibration based on similar items whose nuclear material contents are very accurately known.\n- Non-Proliferation Treaty (NPT)\n- See Treaty on the Non-Proliferation of Nuclear Weapons.\n- Nuclear Facility (or Installation)\n- Under IAEA safeguards, a nuclear facility (or installation) is defined as:\n- a reactor, a critical facility, a conversion plant, a fabrication plant, a reprocessing plant, an isotope separation plant or a separate storage installation; or\n- any location where nuclear material in amounts greater than one effective kilogram is customarily used.\n- Nuclear Fuel Cycle\n- A system of nuclear facilities interconnected by flows of nuclear material. For example, the UK nuclear fuel cycle includes conversion plants, enrichment plants, fuel fabrication plants, reactors and reprocessing plants.\n- Nuclear Material\n- Any source material or special fissionable material.\n- Nuclear Materials Accountancy\n- A system to register material quantities and locations, track items and quantities through transfers and processes, record measurement data, and provide information for reporting and analysis. Accounts of nuclear materials are kept for security, safety, environmental and international safeguards reasons.\n- Physical Inventory (Taking)\n- The process of quantifying the amount of nuclear material present using techniques such as visual verification, sampling and analysis, weighing and other measurements (e.g. NDA). During the physical inventory taking exercise the material is transferred to locations within the facilities where measurements can be made.\n- Physical Inventory Listing (PIL)\n- A report listing all batches of nuclear material separately and specifying material identification and batch data for each batch.\n- Protocol Additional to Safeguards Agreements (Additional Protocol)\n- Agreements with the IAEA made by States that specify the additional authority necessary for the IAEA to fully implement its obligations under comprehensive safeguards agreements pursuant to the NPT. Additional protocols contain measures to improve the efficiency and strengthen the effectiveness of the IAEA safeguards system. The main features of the additional protocol are the requirements that States provide:\n- information beyond that required for nuclear materials accountancy, e.g. on nuclear fuel cycle-related R&D, specified manufacturing activities (e.g. centrifuge manufacture) and exports and imports of certain non-nuclear material and equipment; and\n- extended access to the IAEA to check this reporting.\n- Retained Waste\n- Stored waste containing nuclear material deemed unrecoverable by existing technology but retained in a form from which it is retrievable. The nuclear material content of retained waste is recorded on accounts separate from the main safeguarded inventory.\n- Nuclear safeguards are measures to verify that civil nuclear materials are properly accounted for and are not diverted to undeclared uses. The measures include nuclear materials accountancy, containment and surveillance.\n- Source Material\n- Uranium containing the mixture of isotopes occurring in nature; uranium depleted in the isotope 235; thorium; any of the foregoing in the form of metal, alloy, chemical compound, or concentrate; any other material containing one of more of the foregoing in such concentration as the IAEA Board of Governors shall from time to time determine; and such other material as the Board of Governors shall from time to time determine. The Euratom definition of source material is identical.\n- Special Fissionable Material (or Special Fissile Material)\n- Plutonium-239; uranium-233; uranium enriched in the isotopes 235 or 233; any material containing one or more of the foregoing; and such other fissionable material as the IAEA Board of Governors shall from time to time determine. The term special fissionable material does not include source material. The Euratom definition of special fissile material is identical, apart from the addition of ores/ore wastes to the exclusions.\n- The use of equipment, usually optical equipment such as video cameras, to confirm information on the movement of nuclear material or detect tampering with containment or safeguards relevant equipment, samples and data.\n- The amount of nuclear material processed through facilities, usually expressed over a time period (e.g. a semi-annual accountancy period).\n- Treaty establishing the European Atomic Energy Community (EURATOM Treaty)\n- The EURATOM Treaty, which entered into force on 1 January 1958. The UK became subject to the Treaty upon joining the European Community in 1973.\nThe safeguards section of the Treaty (Chapter VII) requires the European Commission to satisfy itself that:\n- ores, source materials and special fissile materials are not diverted from their intended uses as declared by the users; and\n- the provisions relating to supply and any particular safeguarding obligations assumed by the Community under an agreement concluded with a third State or an international organisation are complied with.\n- Treaty on the Non-Proliferation of Nuclear Weapons (NPT)\n- Under the NPT, the nuclear weapon States (NWS -China, France, Russia, UK and US) undertake not to transfer to any recipient whatsoever nuclear weapons or any other nuclear explosive devices or control over them, and not to support manufacture or acquisition of such weapons or devices by any non-nuclear weapons States (NNWS). [Article I]\nNNWS party to the NPT undertake not to receive any nuclear weapons or other nuclear explosive devices, nor to accept assistance in this respect. [Article II]\nThe NNWS party to the NPT also undertake to accept IAEA safeguards on all source and special fissionable material in all their peaceful nuclear activities. This undertaking is set out in agreements to be signed with the IAEA, which are known as comprehensive safeguards agreements. The NNWS undertake further to provide source and special fissionable material and relevant equipment to any other NNWS only if the material is covered by IAEA safeguards. [Article III]\nThe NPT does not affect the right of its parties to develop and use nuclear energy for peaceful purposes. All parties to the NPT undertake to facilitate and have the right to participate in the fullest possible exchange of equipment, materials and information on peaceful uses of nuclear energy. [Article IV]\nEach of the parties to the NPT undertake to pursue negotiations on effective measures relating to an early cessation of the nuclear arms race, to nuclear disarmament and to general and complete disarmament under international control. [Article VI]\nThe NPT was opened for signature on 1 July 1968, and entered into force on 5 th March 1970. At its 1995 Review and Extension Conference, it was agreed that the Treaty would continue in force indefinitely.\nOnly three States ( India, Israel and Pakistan) have not signed the NPT.\n- Voluntary Offer Safeguards Agreements\n- safeguards agreements made with the IAEA by the nuclear weapon States (NWS), i.e. China, France, Russia, UK and US. The NPT does not require the NWS to conclude safeguards agreements, but they have all voluntarily offered parts or the whole of their civilian nuclear fuel cycle for the application of IAEA safeguards, in order to allay concerns expressed by non-nuclear weapons States (NNWS) that their nuclear industry could otherwise be at a commercial disadvantage. The UK/Euratom/IAEA voluntary offer safeguards agreement came into force in 1978.']	['<urn:uuid:08f22489-8bcc-4b4d-89de-5a1f38df37ad>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	7	17	2361
40	daily walk length seniors memory problems	According to a study in the Journal of the American Medical Association in 2004, elderly people aged 71 to 93 who walked over 3 kilometres a day were twice more unlikely to develop dementia than those who walked less than a kilometre daily.	['Decades ago, people believed that senility or frequently forgetting a lot of things is a fact of life for old people. For them, there is nothing that one can do but gracefully accept that at a certain age, probably 70 or 80, one will start to show signs of cognitive function decline. In recent years, however, researchers have discovered that it is never too late to start to boost your brainpower.\nPhysical Exercise Is Also Good For The Mind\nA study published in the Journal of the American Medical Association in 2004 showed that people aged 71 to 93 who walked over 3 kilometres a day were twice more unlikely to develop dementia than those who walked less than a kilometre every day. Furthermore, women between 71 and 80 who clocked 90 minutes of walking a week did a lot better on cognitive examinations than those who had a sedentary lifestyle. Several studies have shown that increased exercise, even just walking, is crucial in keeping a healthy mind. Experts are not sure how much exercise and physical activities old people need to perform to have a boost on their cognitive functions. It is important to note, however, that regular exercise aids in keeping a healthy blood flow. These activities may also help in creating and maintaining new nerve connections in the brain.\nAnother benefit of exercise is that it aids in keeping glucose levels normal. Glucose is important for the proper functioning of the brain, so even a slight decline in the level of glucose may have a great impact on brain functions of an old person. Rresearch conducted by the New York University’s Center for Brain Health showed that people who have problematic glucose tolerance did not only perform badly in memory tests but also showed signs of brain shrinkage compared to those who had normal glucose tolerance or control. Exercise does not have to be boring or confined in a gym. Old people can find physical activities that they like, such as strolling in the park with colleagues, wading in the pool for a couple of hours, and playing bowling or badminton. See more.\nMind Aerobics Is Essential\nAnother crucial factor in keeping the brain healthy is through mental exercises. If you don’t bother about using your brain, then there is a big likelihood that you will lose it. Our brain functions are impaired or greatly diminished if not used regularly, much like our muscles atrophy when not utilised or exercised.\nNowadays, there are many ways in which you can improve your memory and brain functions. One of which is by enrolling in a brain booster class. There are many institutions in the US that are offering classes specifically designed for old people who want to keep their cognitive functions at their peak. Reading, doing crossword puzzles, playing board games, enrolling in school, or learning a new language is just some ways wherein you can help boost your mental functions. It is never too early or too late to start doing mental exercises.\nExperts believe that doing mental aerobics can help prevent or delay the development of Alzheimer’s or other forms of dementia. If your family has a history of dementia, there is a big chance that you will also develop this disease. However, mental exercises can significantly delay the development of such condition for weeks, months and even boost your brainpower.']	['<urn:uuid:3da78e32-ca2e-44e6-9a93-64910bacdd18>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	6	43	564
41	How do modern and historical shooting sports compare regarding equipment and rules?	Modern shooting sports like USPSA use contemporary firearms such as 2011s, CZ Shadows, and Tanfoglios, focusing on technical performance. In contrast, historical shooting sports like Cowboy Action Shooting require period-appropriate firearms like single action revolvers and lever action rifles, plus authentic costumes matching 19th-century characters. Both types enforce strict safety rules including eye and ear protection, muzzle control, and proper firearm handling, with everyone responsible for maintaining range safety.	"['Why You Should Target Growth In Competition Shooting Segment\nBy Tim Barker\nEvery other Monday night, a few dozen customers gather on the range at C.I. Shooting Sports in Normal, Ill., for a game they call Monday Night Marksman. The game was invented by the shop’s owner, Stephen Stewart, who describes it as a combination of two other popular shooting sports — bull’s-eye and Police Pistol Combat (also known as Precision Pistol Competition).\nStewart is a veteran of several shooting disciplines, including 3-Gun, USPSA and NRA Action Pistol. And he sees several reasons to host these sorts of events at his range, where they also have bull’s-eye and USPSA matches during the week.\nOn one level, they fit with the store’s belief customers should strive to be proficient with their firearms. “It’s better than standing there shooting at a piece of paper,” he said. “It also gives the customers something to look forward to.”\nBut there’s also a pragmatic business angle to these types of events. They generate enthusiasm and fuel customers’ interest in guns, ammo and accessories. It’s not uncommon for a novice shooter (or at least someone with no previous competition experience) to come in a try out one of the events and become hooked.\n“The next thing you know, they’re buying a 1911 Range Officer for Monday Night Marksman,” Stewart shared.\nThere’s a similar story at the Centennial Gun Club in Centennial, Colo., where they host monthly competitions in a wide range of handgun sports, including USPSA, IDPA, bowling pin and Steel Challenge.\nJoey Mizufuka, a sales manager and instructor, said the store started becoming more active in the competition shooting sector about four years ago. He’s been a USPSA shooter since 2007. The push into this arena has helped the shop differentiate itself in a crowded market.\n“That’s the direction we’ve been leaning toward more and more. We needed something. We went for it and it’s been very successful,” said Mizufuka, who also hosts a monthly introductory class for customers interested in competition.\nUSPSA, in particular, is popular in the area, with matches available every weekend of the month at various ranges. Their own match is on Sunday evenings after the shop closes. They max out at 50 shooters (each paying $20 to shoot) — and in four years, they’ve only failed to sell out once.\nWith that level of local interest, it just made financial sense to attempt to tap into the market.\n“Nobody carries this stuff locally,” Mizufuka relayed.\nAppealing to local competition shooters represents another way your store can\nstand out from a crowded field of options. Image Courtesy Of Davidson’s\nNosler Custom Competition Bullets\nKeys To Success\nTo be clear, for both shops, the competition shooter represents a relatively small business segment. These shooters — particularly the more advanced competitors — tend to have specific needs. And many of them are met by small online specialty shops that cater to the various shooting disciplines. One of the keys for a local store is knowing what to offer, said Stewart from Illinois.\n“They don’t buy a whole lot of the competition stuff from us,” he said. “But we still get enough of the business to make it worth our while.”\nOf course, these games start with the gun. And each of the games tends to be dominated by a small number of guns. Visit a local USPSA match, for example, and you’ll see a heavy emphasis on 2011s, CZ Shadows (with the newer Shadow 2 rising in popularity) and various Tanfoglios.\nEntry-level shooters, however, often tend to show up with whatever they happen to have in their gun safe. As they become more interested, most invariably find themselves eyeing guns designed with these games in mind.\nStewart carries a range of popular competition models, including the Springfield Armory Range Officer, STI’s line of DVC guns (both in 1911 and 2011) and CZ Shadow 2s. And with the fast-rising interest in USPSA’s new pistol-caliber carbine division, he stocks the SIG MPX and CMMG’s 9mm carbines.\nThe guns, however, aren’t the biggest moneymakers.\nThey also carry reloading equipment, including Dillon presses, powder, primers and bullets. And they offer a line of Blade-Tech holsters and ammo pouches used by many gamers.\n“The accessories are obviously what’s going to do the best for us,” Stewart said. “We’re always looking for the next cool product for competitors.”\nAt Centennial Gun Club, the offerings include a heavy emphasis on Tanfoglios (the Stock 2 and Stock 3) and CZs from the CZ Custom Shop. In December alone, they sold around 40 of the new CZ Shadow 2s.\n“GLOCKs are still popular with USPSA and IDPA. And we sell a ton of Walthers,” Mizufuka reported.\nThe store is one of only a couple in the nation to carry mag pouches and belts by specialty manufacturer Guga Ribas. And they stock a line of competition base plates by the Henning Group in nearby Longmont, Colo.\nTanfoglio Stock III Xtreme\nWalther PPQ Q4 TAC\nBoth shops illustrate one of the key considerations for any store venturing into the competition-shooting sector: They have people on staff who know the sports — people who can speak the language of the competition shooter — and know what those men and women need.\n“Having some knowledge is a big thing. It just worked out well I’m in the industry. I use the stuff. I don’t just go by reviews,” relayed Mizufuka, whose store also hosts special events from time to time, including a recent class by pro USPSA shooter JJ Racaza.\nIt’s about credibility, maintained Stewart, whose Illinois store is a sponsor of a state USPSA match that draws in hundreds of shooters from around the region.\n“We can talk about anything with these people,” he shared. “And we have a reputation for running a great USPSA match. Our stages are always exciting.”\nIt’s worth mentioning competition communities tend to be close-knit and heavily involved on social media. As a result, the only real advertising they do is through the shop’s Facebook pages (one for the store and one for the USPSA club operating out of the range). Word-of-mouth among the shooting community largely takes care of the rest.\nBut how important is it to have an on-site range? There’s obviously a monetary angle there, with matches pulling in as much as $1,000 a night — often after regular business hours. Still, there’s no reason a shop couldn’t reach out to the competition community by carrying the right mix of firearms and accessories, Stewart relayed. It’s just a range — and the competitions you can host — provide an advantage.\n“It gives people the opportunity to come in we might not see otherwise,” he concluded.', 'EYE AND EAR PROTECTION REQUIRED WHEN ON OR NEAR THE FIRING LINE. THIS APPLIES TO SPECTATORS AND SHOOTERS ALIKE.\nEVERYONE IS A RANGE OFFICER AND MUST PROMOTE SAFETY. IF AN UNSAFE CONDITION EXISTS, IT IS YOUR RESPONSIBILITY TO SEE THAT IT IS CORRECTED, OR IF IMMEDIATE ATTENTION IS REQUIRED, TO CAUSE A CEASE FIRE.\nTHIS IS AN OPEN ACTION RANGE. ACTIONS MUST BE OPEN AT ALL TIMES WHEN NOT ACTUALLY SHOOTING.\nFIREARMS MUST NEVER BE POINTED AT ANYONE. BE EXTREMELY CAREFUL OF MUZZLE DIRECTION. NEVER “SWEEP” ANYONE WITH THE MUZZLE OF YOUR GUN.\nLOADED GUNS WILL HAVE THE HAMMER DOWN ON AN EMPTY CHAMBER OR WITH SHOTGUNS, THE ACTION WILL BE OPEN. SHOTGUNS WILL NOT BE LOADED BEFORE THAT PART OF THE STAGE REQUIRING THEM. SHOTGUNS ARE ALWAYS LOADED ON THE CLOCK.\nWHEN THE RANGE IS HOT, DO NOT GO BEYOND THE FIRING LINE UNLESS THE APPROPRIATE FLAGS OR COMMANDS HAVE BEEN GIVEN TO INSURE IT IS SAFE TO DO SO.\nNO ALCOHOL ON THE RANGE UNTIL ALL GUNS HAVE BEEN PUT AWAY.\nWHAT IS SASS?\nSass is the Single Action Shooting Society is an international organization created to preserve and promote the fast growing sport of Cowboy Action Shooting™. SASS endorses Regional matches conducted by affiliated clubs and annually produces END of TRAIL, the World Championship of Cowboy Action Shooting & Wild West Jubilee™. Cowboy Action Shooting™ is a multifaceted amateur shooting sport in which contestants compete with firearms typical of those used in the taming of the Old West: single action revolvers; lever action rifles; and side by side double barreled, pre-1899 pump, or lever action shotguns. The shooting competition is staged in a unique, characterized, Old West style.\nHOW OFTEN DO YOU SHOOT?\nThe Green Mountain Regulators shoot monthly on the fourth Saturday of the month. We also hold an annual Regulator\'s Revenge Match and many themed matches throughout the year. There are a few other SASS Affiliated clubs in nearby in Central Texas (as well as all over the rest of the United States), and we encourage everyone to get the most out of the Cowboy Action experience by visiting other clubs as well. Someone is always shooting somewhere.\nThe Green Mountain Regulators Long Range shots the fourth Sunday of the month.\n**Non-members and Guests are required to sign a Release of Liability Waver.\nWHY USE ALIASES AND COSTUMING?\nOne of the unique aspects of SASS approved Cowboy Action Shooting™ is the requirement placed on costuming. Each participant is required to adopt a shooting alias appropriate to a character or profession of the late 19th century, a Hollywood western star, or an appropriate character from fiction. Their costume is then developed accordingly. Many event participants gain more enjoyment from the costuming aspect of our sport than from the shooting competition, itself. Regardless of a SASS member\'s individual area of interest, SASS events provide regular opportunities for fellowship and fun with like-minded folks and families.\nWHAT KIND OF FIREARMS CAN I USE?\nThe firearms used in the matches are 2 single action revolvers, a pistol caliber lever-action rifle, and either a Winchester 1897 pump, or double barrel shotgun (usually one of the coach gun styles). Most of the guns used are replicas of actual firearms from the 1800s time period. You will also need ""leather"" (holsters for your pistols, and cartridge belts for pistol and shotgun ammo) and a gun cart for moving your equipment around the range at matches. Lots of options available here as well. GMR has a couple of loaner gun carts and holsters available for use until you decide what you want.\nIf you\'re going to purchase equipment, we recommend coming to see a match first. You can learn a lot by talking to experienced shooters, who are usually very willing to help. There are lots of gun choices and it is recommended to shoot some of the different types so you can find what works for you before invesing in guns. These conversations and test firing some options can keep a new shooter from buying equipment that really doesn\'t suit his, or her, needs.\nWHAT IS SPIRIT OF THE GAME?\nAs the game of Cowboy Action Shooting™ has evolved, our members have developed and adopted an attitude towards their participation called ""The Spirit of the Game."" It is a code by which we live. Competing in ""The Spirit of the Game"" means you fully participate in what the competition asks. You try your best to dress the part, use the appropriate competition tools, and respect the traditions of the Old West. Some folks would call it nothing more than good sportsmanship. We call it ""Spirit of the Game.""\nWelcome to Green Mountain Regulators\nFounded September 20, 2006\nWe are a SASS affiliated club located north west of Austin, TX. We are located at 14230 S. FM 1174, Marble Falls, TX 78654. We are east of Marble Falls off of FM 1174 in Burnet County.\n(For Google Map, click here)\nGPS: 30.58412567320916, -98.11274886131286']"	['<urn:uuid:73c4e8a7-951d-47ce-9853-3d14cdcec989>', '<urn:uuid:256c2c58-7e15-4b39-a02d-86ad2bea86e3>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	12	69	1948
42	I'm studying cement manufacturing processes and I'm particularly interested in how the dry and wet methods differ in terms of raw material preparation and mixing - what are the key distinctions between these approaches?	The dry and wet processes for cement manufacturing differ primarily in how raw materials are prepared and mixed. In the dry process, raw materials are dried before crushing and the dry powder is stored in silos. The raw materials are proportioned with about 80% limestone components and 20% clay, then blended on rotating tables before being crushed to a fine powder. In contrast, the wet process involves mixing the raw materials in a wet state, creating a slurry that contains 35-40% water. This slurry is stored in special tanks rather than silos. Both processes eventually lead to the same next steps - the material passes through a preheating chamber and then enters the kiln where it's heated to 1400-1500°C for clinker formation.	['Cement is the most vital building material that binds almost every structure that humans build today from the simplest of a concrete slab to engineering marvels.\nIt is an ever-present constituent in concrete which is made by mixing it with aggregate comprising sand, gravel, and water. This paste hardens over time and gives the concrete structure its required shape and strength and durability.\nCement use finds mentioned in the thousands of years-long history from the ancient Vedic age to Roman civilization. But unlike the binding material used then, the cement we use now is completely different. In today’s time, Portland cement is the most basic type of cement in general use around the world.\nThere are several raw materials used in the production process of this material. Minerals of natural origin as well as industrial products are being used for cement production. There are some main constituents as well as additives that help give Cement its usual properties.\nLimestone is the most common ore containing calcium carbonate as raw material for cement manufacturing. Similarly, chalk which is a soft sedimentary rock as well as marl, which is a mixture of silica, clay substance, and iron oxide, is also used in cement. Deciding on the right proportion of lime is important because its excess causes expansion and disintegration of cement while deficiency decreases strength forcing cement to set undesirably fast.\nThe argillaceous constituent of raw mix, clay is formed by hydrous aluminum silicates. They play a major role in imparting strength to concrete. Excess silica prolongs the setting time of cement.\nIn a case where primary components needed in cement raw mix are absent in desirable proportions, corrective materials are used as additives. For example, Iron Oxide imparts color to cement. It helps in the fusion of raw materials as well. Magnesium Oxide imparts strength to the cement when mixed in a small quantity. Sulfates and Chlorides are also used in cement production with alkalies as important additives. Cement is also blended with some other materials like fly ash, ground blast furnace slag, and silica fume.\nCement manufacturing is a complex process and involves a series of steps. Each one plays an important role in determining the overall quality of the finished product.\nThere are six main stages of the cement manufacturing process.\n- Raw material extraction: Raw ingredients needed for cement production are limestone, sand and clay, fly ash, shale, and of course bauxite. Ore rocks are quarried and crushed into bits of smaller pieces. Thereafter, the ingredients are prepared for heat-processing.\n- Grinding and Blending: Crushed raw ingredients mixed with additives inside a kiln and powdered to a fine homogenous mixture. The composition of the raw mix is proportioned depending on desired properties. Generally, 80% of it is limestone components and 20% clay. First, the mixture is dried to reduce moisture content and then blended in rotating tables before the roller crushes it to a fine powder.\n- Pre-Heating: The raw cement mix is turned into oxides inside a pre-heating chamber. Here, a series of cyclones utilize the hot gases produced from the kiln to heat the mixture before it is put into the kiln. The process reduces energy consumption and makes cement production more environment-friendly.\n- Kiln Process: This is the principal stage of the cement production process. Here, the raw mix transforms into clinker after a series of chemical reactions between calcium and silicon dioxide compounds.\n- Cooling and grinding: Once it exits the kiln, the clinker is rapidly cooled down with airflow. During this stage, several additives are combined with the clinker before it is ground to produce the final product, cement. In the final last stage of cement production, the cooled clinker product is transferred to rotating drums and ground into fine powder.\n- Packing: Cement now finally produced is first conveyed from grinding mills to large storage tanks called silos where it is packed in certain size bags.', 'Ordinary Portland Cement -Constituents, Properties, Types and Uses\nFasi Ur Rahman\nReading time: 1 minute\nOrdinary Portland cement is one of the most widely used type of Cement. Types, properties, constituents, manufacture, uses and advantages of Ordinary Portland Cement is discussed.\nIn 1824 Joseph Aspdin gave the name as Portland cement as it has similarity in colour and quality found in Portland stone, which is a white grey limestone in island of Portland, Dorset.\nThe principal raw materials used in the manufacture of Ordinary Portland Cement are:\nArgillaceous or silicates of alumina in the form of clays and shales.\nCalcareous or calcium carbonate, in the form of limestone, chalk and marl which is a mixture of clay and calcium carbonate.\nThe ingredients are mixed in the proportion of about two parts of calcareous materials to one part of argillaceous materials and then crushed and ground in ball mills in a dry state or mixed in wet state.\nThe dry powder or the wet slurry is then burnt in a rotary kiln at a temperature between 1400 degree C to 1500 degree C. the clinker obtained from the kiln is first cooled and then passed on to ball mills where gypsum is added and it is ground to the requisite fineness according to the class of product.\nThe chief chemical constituents of Portland cement are as follows:\n60 to 67%\n17 to 25%\n3 to 8%\nIron oxide (Fe2O3)\n0.5 to 6%\n0.1 to 4%\nSulphur trioxide (SO3)\n1 to 3%\nSoda and/or Potash (Na2O+K2O)\n0.5 to 1.3%\nThe above constituents forming the raw materials undergo chemical reactions during burning and fusion, and combine to form the following compounds called BOGUE COMPOUNDS.\nTricalcium silicate (3CaO.SiO2)\nDicalcium silicate (2CaO.SiO2)\nTricalcium aluminate (3CaO.Al2O3)\nTetracalcium aluminoferrite (4CaO.Al2O3.Fe2O3)\nThe proportions of the above four compounds vary in the various Portland cements. Tricalcium silicate and dicalcium silicates contribute most to the eventual strength. Initial setting of Portland cement is due to tricalcium aluminate. Tricalcium silicate hydrates quickly and contributes more to the early strength.\nThe contribution of dicalcium silicate takes place after 7 days and may continue for up to 1 year. Tricalcium aluminate hydrates quickly, generates much heat and makes only a small contribution to the strength within the first 24 hours. Tetracalcium alumino-ferrite is comparatively inactive.\nAll the four compounds generate heat when mixed with water, the aluminate generating the maximum heat and the dicalcium silicate generating the minimum. Due to this, tricalcium aluminate is responsible for the most of the undesirable properties of concrete.\nCement having less C3A will have higher ultimate strength, less generation of heat and less cracking. Table below gives the composition and percentage of found compounds for normal and rapid hardening and low heat Portland cement.\nComposition and compound content of Portland Cement:\n(a) Composition: Percent\n(b) Compound: Percent\nProperties of Ordinary Portland Cement\nTable 2 : Properties of OPC cement\nInitial Setting time\nFinal Setting time\nManufacture of OPC cement\nMajorly there are 5 steps involved in the manufacture of OPC cement,\n1. Crushing and grinding of raw material\nIn the first step of the manufacture of cement, the raw materials are crushed and grinded into small suitable size particles. There are 3 type of manufacture process of cement\nSemi wet Process\nCrushing and grinding process varies depending upon the type of manufacturing process. For dry process the raw materials are dried up before crushing.\nFig 1: Flow chart of Manufacture of OPC cement.\n2. Mixing or Blending\nIn this step, the grinded raw material (lime stone) is blended or mixed with clay in the desired proportion ( limestone :75%, clay : 25%) and mixed well by the help of compressed air to get a homogeneous mix. In the dry process these mixes are stored in silos; slurry tanks are used in the wet process. The resulting material is known as slurry having 35-40% water.\nThis is the main important step in the manufacture of OPC cement, the product obtained from the mixing is passed into the Kiln by the help of conveyor belts.\nFirstly the mix is preheated to 550C ,where all the moisture content is evaporated and clay is broken into silica, aluminium oxide, iron oxide.\nIn the next zone the temperature is rised to 1500 degree Celsius where the oxides form respective silicate, aluminates & ferrite.\nIn the final step the product is cooled down to 200C , where the end product obtained in the kiln is known as cement Clinkers, which is in the form of greenish black or grey colored balls.\nIn this step the cement clinkers and Required amount of gypsum is mixed and grinded into very fine particles which are stored in the silos and later packed in cement bags and distributed.\nThe Expiry date of OPC cement is normally 3 months.\nTypes of Ordinary Portland Cement\nThe differentiation of OPC cement is based on the different codes of different countries.\n1. AS per ASTM 150 (American Standards)\nType I Portland cement is known as common or general-purpose cement. It is generally assumed unless another type is specified.\nType II provides moderate sulfate resistance, and gives off less heat during hydration.\nType III has relatively high early strength. This cement is similar to type I, but ground finer.\nType IV Portland cement is generally known for its low heat of hydration.\nType V is used where sulfate resistance is important. This cement has a very low (C3A) composition which accounts for its high sulfate resistance.\n2. As per EN 197 norm ( European norm)\nCEM I Comprising Portland cement and up to 5% of minor additional constituents.\nCEM II Portland cement and up to 35% of other single constituents\nCEM III Portland cement and higher percentages of blastfurnace slag\nCEM IV Portland cement and up to 55% of pozzolanic constituents\nCEM V Portland cement, blastfurnace slag or fly ash and pozzolana\n3. As per CSA A3000-08 ( Canadian standards)\nGU, GUL > General use cement\nMS > Moderate sulphate resistant cement\nMH, MHL > Moderate heat cement\nHE, HEL > High early strength cement\nLH, LHL > Low heat cement\nHS > High sulphate resistant; generally develops strength less rapidly than the other types.\nUses of Ordinary Portland Cement\nIt is used for general construction purposes where special properties are not required such as reinforced concrete buildings, bridges, pavements, and where soil conditions are normal.\nUsed for most of concrete masonry units\nAdvantages of Ordinary Portland Cement\nIt has great resistance to cracking and shrinkage but has less resistance to chemical attacks.\nInitial setting time of OPC is faster than PPC so it is recommended in projects where props are to be removed early.\nCuring period of OPC is less than PPC and curing cost reduces. Hence recommended where curing cost prohibitive.Disadvantages\nDisadvantages of Ordinary Portland Cement\nIt cannot be used for mass concreting as it has high heat of hydration as compared to PPC.\nThe durability of concrete made using OPC is less than that of the concrete made using PPC.\nIt produces comparatively less cohesive concrete than PPC, hence concrete pumping becomes a little difficult.\nOPC has lower fineness, hence has higher permeability and as a result it has lower durability.']	['<urn:uuid:b43e1b85-1bad-4c83-aa78-a69134932801>', '<urn:uuid:7703d822-9597-4258-a29b-27c14a8b7a4c>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T20:35:02.547110	34	122	1856
43	mysterious magnetised filaments galactic center origin	The mysterious magnetised filaments are thread-like structures that have been found exclusively in the Galactic centre. While their origin remained unexplained for 35 years after their discovery, the recent findings suggest they may have been created by a powerful eruption near the galaxy's central black hole a few million years ago. Almost all of the more than one hundred filaments are confined within the radio bubbles observed in the Galactic centre.	"['From: University of Oxford\nPosted: Wednesday, September 11, 2019\nPublished in Nature today, scientists have shown how this hourglass-like feature, which dwarfs all other radio structures in the Galactic Centre, is likely the result of a phenomenally energetic burst that erupted near the Milky Way\'s supermassive black hole a few million years ago.\nUsing the South African Radio Astronomy Observatory (SARAO) MeerKAT telescope, Ian Heywood of the University of Oxford\'s Department of Physics and his colleagues mapped out broad regions in the centre of the galaxy, conducting observations at wavelengths near 23 centimetres. Radio emission of this kind is generated in a process known as synchrotron radiation, in which free-floating electrons are accelerated as they interact with powerful magnetic fields. This produces a characteristic radio signal that can be used to trace energetic regions in space. The radio light seen by MeerKAT penetrates the dense clouds of dust that block visible light from the centre of the galaxy.\nLead author Dr. Heywood, who processed the large amount of observational data which led to this result, said: \'The centre of our galaxy is relatively calm when compared to other galaxies with very active central black holes. Even so, the Milky Way\'s central black hole can become uncharacteristically active, flaring up as it periodically devours massive clumps of dust and gas. It\'s possible that one such feeding frenzy triggered powerful outbursts that inflated this previously unseen feature.\'\nBy examining the nearly identical extent and morphology of the twin bubbles, the scientists believe they have found convincing evidence that these features were formed from a violent eruption that over a short period of time punched through the interstellar medium in opposite directions.\nMeerKAT has unprecedented sensitivity and imaging capabilities which, coupled with its geographic vantage point for observing the Galactic centre, has resulted in the clearest ever image of the radio waves emanating from the centre of the Milky Way, a part of the sky that is notoriously difficult to image at such wavelengths.\nThese new observational capabilities are unlocking a ""fossil record"" which allows scientists to piece together the history of the Galactic centre and the supermassive black hole that lurks there. Although the structure is likely to be a few million years old it is still possible to observe it, and from there scientists can infer from where it came.\nHead of Astrophysics at the University of Oxford, Professor Steve Balbus, said: \'It is extremely exciting to be able to peer at the centre of the Galaxy with such high definition and precision. This is the nearest supermassive black hole to us in the Universe, and MeerKAT has provided us with front row centre seats. We are going to learn an enormous amount about how black holes feed themselves and how they influence their environments. In the relatively brief time that MeerKAT has been up and running, the science that the team at Oxford has already produced has been transformational. Oxford can be rightly proud of their leadership involvement with the MeerKAT Project, and I personally can\'t wait to see what more there is to come - this is only the beginning of a new era in radio astronomy.\'\nCo-author, William Cotton, from the National Radio Astronomy Observatory in Charlottesville, Virginia, said: \'The shape and symmetry of what we have observed strongly suggests that a staggeringly powerful event happened a few million years ago very near our galaxy\'s central black hole. This eruption was possibly triggered by vast amounts of interstellar gas falling in on the black hole, or a massive burst of star formation which sent shockwaves careening through the galactic centre. In effect, this inflated energetic bubbles in the hot, ionized gas near the galactic centre, energizing it and generating radio waves which we could eventually detect here on Earth.\'\nThe event, reported today in Nature, could also be the origin of the population of electrons that are required to power the radio emission from mysterious magnetised filaments. These thread-like structures have been seen nowhere but in the Galactic centre, and there has been no definitive explanation for their origin since they were first discovered 35 years ago. ""Almost all of the more than one hundred filaments are confined by the radio bubbles,"" said co-author Prof. Farhad Yusef-Zadeh of Northwestern University in Evanston, Illinois.\nCo-author Fernando Camilo of SARAO in Cape Town, said: \'These enormous bubbles have until now been hidden by the glare of extremely bright radio emission from the centre of the galaxy. Teasing out the bubbles from the background \'noise\' was a technical tour de force, only made possible by MeerKAT\'s unique characteristics and propitious location in the Southern hemisphere. With this unexpected discovery we\'re witnessing in the Milky Way a novel manifestation of galaxy-scale outflows of matter and energy, ultimately governed by the central black hole.\'\nToday\'s result cements the great ""discovery machine"" potential of the new generation of radio telescopes as we move towards the construction of the Square Kilometre Array.\n// end //']"	['<urn:uuid:2941945b-cb4e-474c-bf73-649c56825f1a>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	6	71	830
44	I'm new to French press coffee - what affects the taste and strength of my brew?	The taste and strength of French press coffee is influenced by several key factors. The extraction process determines flavor, with early stages producing sharp, acidic tastes that become more balanced over time. Key variables include: brew ratio (recommended 1:14-15 coffee to water), grind size (coarse like breadcrumbs), water temperature (195-205°F), and steep time (typically 4 minutes). The wire mesh filter allows oils and fine particles through, which creates a heavier body but requires higher overall strength for good flavor clarity. For best results, use fresh whole beans, clean your press thoroughly, and avoid letting coffee sit on grounds too long to prevent bitterness.	"[""How to brew with your french press, using an explanation of extraction, strength, and the variables you can use to make better coffee.\nThere aren’t many purer representations of the wonders of full-immersion coffee brewing than the french press. Resulting in big body and rich flavors, the brewer’s simple design obscures the more complex process happening inside—the interaction of ground coffee, water, and time all work together and, with your influence and know-how, produce a drink suitable for even the most discerning coffee drinker. This article is made to take you beyond simply having a recipe that you use every day, and instead will propel you into understanding how you can influence extraction and strength in your mug and brew up the perfect cuppa joe with any bag of beans you like.\nIf you haven’t already, check out our Basic Brewing Principles guide where we detail the fundamentals of good coffee, good water, and good recipes. Or, if you’re interested in these concepts in a general sense instead of specifically for your french press, take a gander at our Intermediate Brewing Principles. If you're on the market for a french press, make sure to check out our Comparison of French presses.\nBrewing is Extraction\nWhether by immersion or percolation, brewing coffee is actually the process of extraction. A series of physical and chemical interactions are set in motion when you mix coffee and water, starting with washing insoluble materials from the surface of your bed of coffee grounds and quickly moving into dissolving soluble flavor compounds from inside of those coffee particles. All of this material, both soluble and insoluble, contributes to your resulting cup of coffee—those coffee oils and superfine particles that were washed from the surface of your coffee grounds will help define the body and texture of your beverage, while the combination and ratio of dissolved flavor compounds will determine the flavor of your fresh brew.\nThe concentration and mixture of different compounds that end up in your brewed coffee will change constantly throughout the extraction process—this will always be the case because we know that different chemical compounds in your roasted coffee (like organic acids, simple sugars, and more complex carbohydrates) all dissolve and extract at different but predictable rates. This means that in the earliest stages of brewing in your french press you can expect the beverage to be sharp, dilute, and acidic in flavor, growing more balanced over time until eventually extracting astringent compounds and taking on some characteristics that we associate with “over-extraction”. This predictability in the extraction rates of different flavors gives us a way to have some influence over what our brewed coffee tastes like, as well as giving us a baseline to be able to assess our extraction as either under-extracted (we removed too little of the soluble material), over-extracted (we removed too much of the soluble material), or ideally extracted (we removed just the right amount of the soluble material).\nHow to Judge Your Extraction\nThere are lots of ways that you could try to assess how your brew went, and if you look for very long at the science of brewing you’ll come across all kinds of measurements, numbers, and formulas made to inform that assessment. Though quantitative data can be a helpful way to communicate precise information from one person (with the necessary tools to measure and understand the measurement) to another person (with the same tools and understanding), any coffee professional should be able to tell you that you don’t taste numbers, measurements, or extraction percentages, you taste coffee! Since we know that the flavors in our mug are influenced by the relative extraction we achieve, how do we know if our brew is over-extracted, under-extracted, or just right? By taste! There are some flavors that tend to consistently come through at different levels of extraction that you can look out for as you’re learning to dial in your recipes.\nUnder-extracted coffee will likely taste sour, salty, or overbearingly acidic. The early stage of extraction contains most all of the acids that will be extracted, as well as some bitter compounds (like caffeine) that can be overwhelming when left unbalanced by the flavors that will extract later in the brew.\nOver-extracted coffee will likely taste bitter and feel unpleasantly astringent. Many of the flavors extracted late in a brew are bitter in taste, pushing the flavor balance in that direction quickly.\nIdeally extracted coffee should be balanced and flavorful, achieving both a satisfying texture and an acidity that complements the flavors of your brew. This can mean sweet and brimming with fruit flavors, deep and rich with chocolatey goodness, or complex and alluring with savory complexion.\nIf you’re not a confident taster right now that’s totally okay! Tasting is a skill you can learn, and with a little trial and error you’ll be able to start judging your extraction accurately enough to make educated decisions about how to adjust your recipe for a better brew.\nThe Importance of Strength\nAn ideally extracted dose of coffee is the first goal in brewing, but ensuring that you brew coffee with good strength is a very close second. Our ability to taste coffee well relies on the concentration of its flavors being just right—too much flavor in solution and our taste buds become overwhelmed, while too little flavor will leave a beverage to come across as dilute and disappointing.\nThe balance of strength and extraction in your press pot can get especially interesting because the wire mesh filter used in most french presses allows a much greater volume of insoluble material (oils and fines) to end up in your brew. These insoluble materials contribute little flavor but add lots of weight to the body of your brew, which in turn makes it more difficult to taste flavors clearly. Ultimately, this means that in order to achieve a similar level of flavor clarity as in a paper-filtered brew you’ll need to brew coffee the results in a higher overall strength, making it possible to discern the flavors in your brewed coffee through the veil of the heavy body that your brewer excels at producing.\nWith a dose, grind setting, and brew time all within a typical range for immersion brewing, it’s pretty straightforward producing coffee at a palatable strength with your french press. But, if you’re looking for more control over your new brewer, we’ll cover below the variables that you can adjust in order to fine-tune your recipe to brew up your ideal cup of coffee time and time again.\nMastering Your French Press\nLearning to succeed with your french press is all about understanding how to tweak just a few variables in order to achieve the best extraction and strength for any bag of beans that you’ve got. So much of the beauty of immersion brewing is in its simplicity, meaning that more so than in any pour-over brewer you can focus strictly on these four inputs without worrying about other extraneous (and more difficult to manage) variables like pour stages, flow rate, channeling in the brew bed, and loads more.\nBrew ratio is the amount of ground coffee and brew water you use to make a single extraction. Immersion brewers like the french press tend to perform better with a higher ratio of coffee to water than most pour over or auto brewers do, so I’d recommend starting at 1g of ground coffee per every 14 or 15g of water used. Extraction and strength will be affected greatly by this ratio—the more coffee you use the higher your strength will be and the lower your total extraction will be, and vice versa. Expect to make adjustments here to impact your total strength first and foremost, but typically finding a consistent ratio will help you learn to adjust the other variables with more precision quickly.\nThe grind setting that you choose will also greatly impact your total extraction by affecting the rate at which coffee flavor will dissolve and extract out of your grounds. The commonly accepted coarseness for french press has been “very coarse” for a long time now, but more and more lately loads of pros are recommending a setting closer to a standard drip grind. Regardless of which grind you decide to go with, be sure that it pairs well with your chosen contact time.\nMany immersion brewers, like the french press, give you the ability to directly and consistently control your total contact time in a way that you can’t with pour over brewers. While you’ll always need at least a certain amount of time to reach an appropriate extraction level, it seems as though very long steep times are becoming more and more common (as evidenced by a couple of the links above). Whatever your process, remembering that more time always means more extraction will help you dial in how long you let your french press steep before either breaking the crust, depressing the plunger, or decanting the coffee.\nThe Temperature of your brew water is the final control variable, and it will relate directly to your rate of extraction (hotter water causing faster extraction and vice versa). The general standard for brewing says to use water 195°F–205°F (90.5°C–96°C), but there are lots of folks who prefer hotter or colder water for their own brews. Remember that your “temperature” isn’t necessarily the temperature in your kettle, but rather the temperature of the water throughout the brewing process in the brewer itself. Preheating your french press to aid in heat retention will be important no matter the design of your brewer, but it might behoove you to use a slightly higher temperature if your brewer is single-walled glass or metal as opposed to a double-walled or otherwise well insulated brewer.\nAll of the reading in the coffee world won’t make a difference without investing a little time into brewing and tasting, so get to it! Hopefully now with a little understanding of extraction and strength, guidelines for how to taste and assess your brew, and an overview of the variables you can adjust in your french press, you’re ready to dive in and make a little magic in the pursuit of the perfect mug of coffee.\nHappy brewing, y’all."", ""How To Make French Press Coffee\nI drink a lot of coffee. In fact, I'm pretty sure that without coffee life wouldn't be worth living. Over the years I have tinkered with all sorts of different coffee makers, from automatic drip brewers, pour-over, Moka pots, espresso machines and with the French Press. As great as all of the other brewing methods are the French Press has always had a special place in my heart. It was one of the first coffee makers I used when I transitioned away from push-button brewers, and it is here where my love of coffee began.\nSo if you have got on the French Press pot bandwagon and have just purchased a new press pot, I bet you’re wondering how to make French Press coffee. On the other hand, if you’re simply looking for some tips to make your daily brew that little bit better then this article is for you.\nFrench Press Brewing At A Glance\nWhat You Need\n- 8- cup French Press pot\n- 56g (8 tablespoons) of fresh whole coffee\n- Hot water just off the boil (about 205°F)\n- Coffee grinder (burr grinder recommended)\n- Wooden spoon\n- Coffee scale\n- Coffee mug\n- Timer (recommended)\nPress Pot End Result\nTotal brew time: 4:00\nYield: 8 cups (1L)\nCup Characteristics: Heavy and robust\nFrench Press How To Use\nBrewing with a French Press is super simple, to be honest, its probably one of the easiest, most forgiving, and least expensive ways to brew great tasting coffee at home. Follow my below step-by-step guide on how to use a French Press.\nStep 1: Boil Your Water\nTurn on your kettle and heat your water to 205 degrees by bringing it up to a boil and letting it stand for 30 seconds. Even though I'm using a gooseneck, no fancy kettle is needed for the French Press a regular kettle is just fine.\nStep 2: Weigh Out And Grind Your Whole Coffee\nDifferent French Press Coffee Ratio\nWeigh out 56 grams of whole bean coffee (about 8-10 tablespoons) and set your coffee grinder to a coarse setting. The ground coffee needs to be as coarse as breadcrumbs so try and aim for that consistency when grinding. Take a look at my coffee grind guide here.\nStep 3: Pre-Heat Your French Press\nBy now your Kettle should have boiled. With your hot water give your French Press a rinse, this helps to maintain the temperature of the French Press while brewing. Pour away rinse water once done.\nStep 4: Add Your Coffee And Some Water\nHave your timer and hot water nearby. Add your coffee grounds to the French Press and start to pour in your hot water. As soon as you start to pour start your timer. Stop pouring when you reach roughly half way up the French Press.\nStep 5: Gently Stir\nWhen your timer reads 1-minute take a wooden spoon (or as I have used the AeroPress spatula) and start to break through the top layer (the crust). Give your French Press coffee a good stir. The goal here is to get all of the coffee submerged with water.\nStep 6: Add More Water\nWith the timer still ticking away fill the French Press with water to the top. Place the plunger on the top but do not push down just yet.\nStep 7: Press Down The Plunger\nWhen your timer read 4-minutes, it’s time to push down the plunger. Using both hands, one to hold the French Press and the other on the plunger, slowly begin to push down. Don’t push down too fast, try and apply firm but moderate pressure as you push.\nStep 8: Serve And Enjoy\nYou're done! If you are drinking the coffee straight away pour into your coffee mugs, otherwise pour into a decanter immediately to avoid over extraction (If the coffee sits on the grounds too long, it will continue to extract, and you will find that your coffee becomes bitter.\nFrench Press Brewing Tips\nAlways make sure that your French Press is clean before you brew coffee in it. Most of the mesh filter will unscrew so you can get rid of any old coffee grounds. If you leave these stale grounds inside the filter there is a good chance that your coffee will taste bitter.\nAs with all brewing methods I strongly recommend that you start off by using whole coffee beans and grind just before you brew. If you grind your coffee too early (storing your coffee already ground), you will find that the coffee will lose almost all of the compounds that give it such delightful flavors and aromas.\n- Coffee Tastes Weak: Most likely your coffee grind is simply too coarse nest time try grinding the coffee a little bit finer (remember to aim for a consistency like breadcrumbs). Also make sure that you are steeping the coffee for 3-4 minutes.\n- Coffee Tastes Bitter: If your coffee is tasting bitter the chances are that you have ground your coffee too fine. Next time you brew opt for a coarser grind. Also its worth mentioning that if you are using a dark roast, make sure the coffee is fresh and also try to lower the brewing temperature to around 195 F.\n- Coffee Taste Too Strong: Try to reduce the steep time to 3 minutes. Once brewed don’t let the coffee sit inside of the carafe, pour into a decanter if its not going to be drunk straight away.\n- Gritty/Thick Sediment At The Bottom Of Your Mug: Either your coffee grind is too fine and the grounds are passing through the mesh plunger filter or there is a problem with your filter not forming a tight seal inside of the glass carafe.""]"	['<urn:uuid:e13cb441-0ec7-427b-a6f3-fb61ddcc6178>', '<urn:uuid:326452fe-6e9b-42aa-a846-60c603b33722>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:35:02.547110	16	103	2670
45	as radiology department head need details radio imaging technology diploma program duration entry requirements career paths	The Radio-Imaging Technology diploma is a two-year full-time program with experiential rotations in various aspects of professional practice. The entry requirement is 10+2 in any stream, preferably with Physics, Chemistry and Biology. Graduates can pursue several career paths: working as a Radiographer in full-time positions, as healthcare professionals in the corporate world, as Cath-Lab Technicians, or as part of healthcare teams in accident and emergency departments, intensive care units, and operational theatres.	['Radio-Diagnostic Imaging is a critical component of patient’s treatment. Almost all departments rely on the radiological examinations for the diagnosis of pathologies and conditions. All the patients coming to radiology department have the right to receive optimum quality image with minimum radiation exposure consistent with good patient care.\nThe purpose of this program is to standardize the Radio-Imaging technology at diploma levels throughout the country so that it will benefit in achieving uniformity work as a practitioner as well as resultantly relating component radiographers working with appropriate expertise.\nAt the present, in our state very few universities are generating health professionals specialized in Radio-Imaging domains.\nThis diploma program introduced by Faculty of Allied Health Sciences (SGT University) prepares healthcare professionals having extensive and practical knowledge in the fields of Radiology and Nuclear medicine on Local, National, and International fronts.\nThe primary goal of the Radio-Imaging Technology diploma program is to train radiographers with the knowledge, skills and competency to provide optimum quality professional services in a wide variety of settings including academic, governmental, corporate, and military and community based organizations.\n- The primary goal of the Radio-Imaging Technology diploma program is to train radiographers with the knowledge, skills and competency to provide optimum quality professional services in a wide variety of settings including academic, governmental, corporate, and military and community based organizations.\nDuration of Study:\nDiploma- Two years program(full-time experiential rotations in various aspects of professional practice in the field)\nDiploma – 10+2 in any stream, preferably Physics, Chemistry and Biology.\n- Professionally work as a Radiographer (full time job).\n- Can work as a health care professional in corporate world.\n- Professionally work in Cath-Lab as a Cath-Lab Technician.\n- As a part of health care team in accident and emergency.\n- As a part of health team in intensive care unit and operational theatre.\nDepartment of Radiology with support from Anatomy, Physiology and Community Medicine\nThe interdisciplinary curriculum is based on both a clinical and public health model for practice. The major focus is on population aggregates rather than individuals and indirect rather than direct care to clients. Fellows are prepared to participate in a multi-disciplinary approach to planning, implementing, managing, and evaluating programs and services for worker health and safety.\nIt is assumed that there will be approximately 1630 teaching hours in three year period of course. Out of these, Theory teaching (Modules) will be of 1180 hours and Practical will be of 450 hours. The teaching will be carried out on various modules for Occupational and technical medicine.\nThe fellows will be exposed to practical demonstration in various Industries for above purposes\n|Heads||Semester wise||Total hour/ Year|\n|Home assignment: Any 3 assignments per semester as given in modules (only one from each module).||10 hours for each assignment\n(30 hours/semester )\n|Participative learning||150 hours /semester||\nThe theory training will be primarily from cognitive domain while practical training will be from cognitive, psychomotor and affective domain.\nCourse distribution: The Graduation program will be of three years duration. It will be divided into three equal terms of one year each.\n|TOPIC (Subject wise)||HOURS||CREDIT HOURS|\n|The topics covered in first year will include|\n|Modular teaching modules||560 hours||\n|TOTAL TEACHING HOURS||1070 hours||\n|The topics covered in second year will include|\n|Modular teaching modules||480 hours||\n|TOTAL TEACHING HOURS||990 hours||\nSubsidiary subjects are added for overall development of students and to enhance their knowledge.\nProfessional /Clinical Training\n- Students are expected to complete at least three months training with corporate hospitals or diagnostic centers.\n- Students will have to observe the protocols on different modalities.\n- Students assume primary responsibility for preparing the patients for diagnostic examinations.\n- Students will have to carry out diagnostic imaging under the supervision of healthcare staff.\nElective Training / Optional posting\nWould be arranged by the students duly supported by the faculty for observation / hands on training in the Industry / Hospital or similar external Organization may be as Summer Placement / Winter Placement\nSummer / winter vacation Training / Optional posting:\nThe students will be given assignments/project for their summer and winter vacations for which they will be required to undergo training at the hospital/laboratories/ diagnostic centres of their choice .The students will be asked to present their project/work report on coming back after their vacations. This will be reflected in their evaluation for internal assessments. Due support to the students will be provided for this training by the faculty.']	['<urn:uuid:b375af94-2d23-4cda-8ecc-7347a6187c1a>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	16	72	734
46	I'm curious about how dehydrogenase enzymes work in our bodies - what exactly do they do to molecules during chemical reactions?	Dehydrogenases are enzymes that oxidize a substrate by reducing an electron acceptor, typically NAD+/NADP+ or flavin coenzymes like FAD or FMN. During this process, they remove hydrogen from a substrate, transferring the hydrogens and their electrons to a carrier or another product.	"['favorite; … One such reaction is: AH 2 + O 2 ↔ A + H 2 O 2.. A In order to reduce this molecule, a hydrogen and two electrons must be added to the 6-carbon ring of nicotinamide; one electron is added to the carbon opposite the positively charged nitrogen, causing a rearrangement of bonds within the ring to give nitrogen more electrons; it will lose its positive charge as a result. The ratio of NADP+ to NADPH in the cell is kept rather low, so that NADPH is readily available as a reducing agent; it is more commonly used as a reducing agent than NADP+ is used as an oxidizing agent.. FAD, or flavin adenine dinucleotide, is a prosthetic group (a non-polypeptide unit bound to a protein that is required for function) that consists of an adenine nucleotide and a flavin mononucleotide. The distinction between the subclasses of oxidoreductases that catalyze oxidation reactions lies in their electron acceptors.. Hydrogenation of Unsaturated Fats and Trans Fat Last updated; Save as PDF Page ID 440; Hydrogenation Reaction; Trans Fat; Contributors; In the late 1970’s the lipid hypothesis came in to existences stating that eating saturated fats leads to elevated LDL (Low Density … S  FAD is unique because it is reduced by two electrons and two protons, as opposed to both NAD+ and NADP, which only take one proton. These example sentences are selected automatically from various online news sources to reflect current usage of the word \'dehydrogenation.\' A dehydrogenase (also called DH or DHase in the literature) is an enzyme belonging to the group of oxidoreductases that oxidizes a substrate by reducing an electron acceptor, usually NAD+/NADP+ or a flavin coenzyme such as FAD or FMN. I An oxidase will remove electrons from a substrate as well, but only uses oxygen as its electron acceptor. O Aldehyde dehydrogenases (ALDH) are NAD+ dependent enzymes that function to remove toxic aldehydes from the body, functioning mostly in the mitochondria of cells. The product is water, instead of hydrogen peroxide as seen above.  Dehydrogenases typically fully reduce FAD to FADH2. , The result of a dehydrogenase catalyzed reaction is not always the acquisition of a positive charge. The two hydrogens have been transferred to the carrier or the other product, with their electrons. en chimie, fait de déshydrogéner, d\'enlever un ou plusieurs atomes d\'hydrogène à un corps, à une molécule ""DESHYDROGENATION"" dans l\'encyclopédie . Free learning resources for students covering all major areas of biology.  Note how when the hydride is transferred from A to B, the A has taken on a positive charge; this is because the enzyme has taken two electrons from the substrate in order to reduce the acceptor to BH. , NADP+ differs from NAD+ only in the addition of a phosphate group to the adenosine 5-membered carbon ring. Dehydrogenation of organic compounds converts single carbon–carbon bonds into double bonds. \'All Intensive Purposes\' or \'All Intents and Purposes\'? The removal of one or more hydrogen ions or protons from a molecule. F  An example of an oxidase that functions like this is complex IV in the Electron Transport Chain (ETC). Dehydrogenases are a subclass of the class of enzymes called “oxidoreductases.” Oxidoreductases, in general, catalyze oxidation and reduction reactions. mild or merciful in disposition or character; lenient; compassionate. Déshydrogénation. The analysis of chemical reaction in dehydrogenation process is performed first in the laboratories. © 2020 Encyclopædia Universalis France.Tous droits de propriété industrielle et intellectuelle réservés. tion. Views expressed in the examples do not represent the opinion of Merriam-Webster or its editors. , Reduction of NAD+: NAD+ + 2H+ + 2e− ↔ NADH + H+, NAD+ is mostly used in catabolic pathways, such as glycolysis, that break down energy molecules to produce ATP. Build a city of skyscrapers—one synonym at a time. : +33 3 83 96 21 76 - Fax : +33 3 83 97 24 56 The ratio of NAD+ to NADH is kept very high in the cell, keeping it readily available to act as an oxidizing agent. Les noms propres ne sont pas pris en compte. X This tutorial gives an overview of the nervous syste.. The other electron is ""stolen"" from an additional hydrogen, leaving the hydrogen ion in solution. © 2001-2020 BiologyOnline. Dehydrogenation is a chemical process in which a hydrogen atom is removed from a molecule of an organic compound.  - Renew or change your cookie consent, Mobile Tools: A Disruptive Technology for Oil & Gas Operations, Pipelines: Lifelines of the Oil & Gas Industry, How Big Data is Transforming the Oil & Gas Industry, Sweet vs. The content on this website is for information only. Asset Integrity Management Key for Smart Operations, Oil & Gas Retailing Secondary Distribution Processes. Soon after, primitive life forms that could assimilate oxygen thrived.. Dehydrogenase and oxidase are easily distinguishable if one considers the electron acceptor. Decarboxylation in the largest biology dictionary online. This lesson looks at population attributes, regulation, and growth. Inscrivez-vous à notre newsletter hebdomadaire et recevez en cadeau un ebook au choix ! © 2012 - CNRTL 44, avenue de la Libération BP 30687 54063 Nancy Cedex - France Tél. The production of FADH is rare. Q HENRI_JEAN-MARIE_DOUHenri Jean-MarieDOUingénieur I.P.S.O.I., docteur ès sciences, directeur de recherche au C.N.R.S., au Centre de recherche rétrospective de Marseille, président de la Société française de bibliométrie appliquée, JEAN-EUGENE_GERMAINJean-EugèneGERMAINprofesseur de chimie à l\'université de Lyon-I-Claude-Bernard. Please tell us where you read or heard it (including the quote, if possible). Sometimes the substrate loses a proton. All Rights Reserved, The Conscious & Unconscious Nervous System, Chromosomes X and Y and Sex Determination. Learn a new word every day. Removal of a pair of hydrogen atoms from a compound by the action of enzymes (dehydrogenases) or other catalysts. They also catalyze the reverse reaction, for instance alcohol dehydrogenase not only oxidizes ethanol to acetaldehyde in animals but also produces ethanol from acetaldehyde in yeast. Dehydrogenation definition is - the removal of hydrogen from a chemical compound.  These enzymes are only one example of the many different types of dehydrogenases in the human body; their wide array of functions, and the impact that their deactivation or mutations has upon crucial cell processes underscores the importance of all dehydrogenases in maintaining body homeostasis. The analysis of chemical reaction in dehydrogenation process is performed first in the laboratories. Z, Copyright © 2020 Petropedia Inc. - In an E1 reaction, the reaction rate is proportional to the concentration of the substance to be transformed; in an E2 reaction, the reaction rate is … , Oxidation-reduction reactions are essential to growth and survival of organisms, as the oxidation of organic molecules produces energy. …the reaction is known as dehydrogenation. These enzymes fall into six categories: oxygenases, reductases, peroxidases, oxidases, hydroxylases, and dehydrogenases. It is the opposite of hydrogenation process. This can be done by a dehydrogenase enzyme. For this reason, dehydrogenases have pivotal roles in metabolism..']"	['<urn:uuid:699d7232-c0bd-41c6-88dc-20adb40aec07>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	21	42	1151
47	How do the preparation and measuring steps differ when installing vinyl on walls versus vinyl plank flooring, and what specific layout tools are required for each type of installation?	For wall vinyl installation, preparation requires cleaning the surface with pharmacy alcohol and using masking tape to mark straight guides. For vinyl plank flooring, more extensive measuring tools are needed including a tape measure for determining dimensions, chalk line for creating straight reference lines, carpenter's square or T-square for accurate 90-degree angles, and spacers for maintaining consistent expansion gaps between planks and walls. Both installations require thorough cleaning of the surface, but flooring installation demands more precise layout planning and measurement tools.	['How to place a vinyl on a wall or glass without bubbles or wrinkles19-11-2020\nThere are many types of vinyls and many types of surfaces on the market. Each combination has its own placement technique. Keep in mind that a vinyl is like a giant sticker and that it requires a little skill so that you do not get bubbles, so that it does not wrinkle and so that it sticks well. Here we will explain the most common techniques and tricks so that you lose your fear of vinyl.\nBefore you start laying the vinyl\nFirst of all, make sure you have the essentials to place the vinyl on hand: a spray bottle with water and a drop of soap, masking tape, a VISA-type plastic spatula or card, a clean cloth, pharmacy alcohol and the vinyl itself.\nWe can start now\nThe first example that we propose is that of a printing vinyl placed on a surface that can get wet, such as glass. Do you know what a printing vinyl is? Here you will find all the information about printing vinyls.\nFirst clean the area where the vinyl is to be placed with pharmacy alcohol and a clean cloth to remove the remains of dust, grease, glues, etc. Attention! Avoid using cleaning products with ammonia. The masking tape will help us to mark a straight guide where we will place the vinyl. With the sprayer (mixing soap and water) we will wet the surface. Then place the vinyl upside down, that is, with the vinyl side glued to the surface and the silicone paper on the outside. Once in place, peel off the silicone paper from the top tip and slowly pull diagonally to completely remove the silicone paper. Re-spray the adhesive part of the vinyl with water. You turn the vinyl over, place it on the surface in the place where you marked with the masking tape and with the spatula remove the water from the inside out. When you finish, dry the entire area avoiding scratching or leaving scratches.\nIf you found the explanation a bit confusing, here you will find a much more graphic explanatory video.\nMore difficult still: laying a vinyl without water\nBut not all surfaces can get wet. How do we put a vinyl on a wall or on a wood? We will do it dry, that is, without water. The process is very similar but more delicate since the water allows us to stick and detach the vinyl without deforming it. Imagine that we want to place a cut vinyl of a die-cut color (here you will discover everything about cut vinyl) on a wooden table. First we clean the surface to remove dust and impurities. We raise the vinyl on the surface and with one end glued, we slowly place the vinyl with the spatula.\nThe damn bubbles\nOne of the most common headaches in vinyl placement is the appearance of bubbles. If we apply the vinyl with water it is easy to remove them since the water repositions the vinyl easily. With dry vinyls things get complicated. If bubbles appear, you can remove them by applying a little heat with a blow dryer or lighter or by pricking them with a needle. Then smooth the vinyl with the spatula being careful not to wrinkle or pinch it.\nThe infinite world of vinyl\nIn this blog we have explained the basic tricks to place the most common vinyls on the market. But each vinyl requires a different technique. There are micro-perforated vinyls, translucent vinyls, reflective vinyls, glazed vinyls, vinyls for vehicles, vinyls for whiteboard or Velleda blackboard... and each one has its own tips and recommendations.\nDo not despair if it does not come out the first time since the placement of vinyl requires a bit of practice. If they are small, you can install them yourself, but on large vinyl you will surely need help. At Gràfic Centre we have a team of expert sign makers who will ensure impeccable placement so that your windows, shop windows, tables or walls look like never before.', 'When it comes to installing vinyl plank flooring, having the right tools on hand is essential for a successful and efficient project. In this article, we will discuss the necessary tools that will make the installation process a breeze.\nFrom cutting and measuring tools to adhesives and other essential accessories, we will provide you with a comprehensive list that will ensure your vinyl plank flooring installation goes smoothly and looks great when completed.\nBefore installing vinyl plank flooring, it’s essential to gather the necessary tools for a successful project. In this section, we will discuss the tools needed for measuring and layout.\nMeasuring and Layout Tools\nAccurate measurements and proper layout are crucial for a professional-looking floor installation. Here are the primary tools required for this process:\n- Tape measure: A tape measure will help you determine the dimensions of your installation area and calculate the amount of flooring material required.\n- Chalk line: A chalk line is a valuable tool for creating straight reference lines across the installation area.\n- Carpenter’s square or T-square: These precision tools are used to create accurate 90-degree angles in your layout.\n- Spacers: Spacers are small, removable objects used to maintain consistent expansion gaps between the planks and the wall.\nHere is a summary of the tools needed for measuring and layout:\n|Tape measure||Measuring dimensions and calculating flooring material|\n|Chalk line||Creating straight reference lines|\n|Carpenter’s square or T-square||Creating accurate 90-degree angles|\n|Spacers||Maintaining expansion gaps between planks and walls|\nProper preparation, including accurate measuring and layout, is essential for a successful vinyl plank flooring installation. Ensure you have these tools on hand before beginning your project.\nWhen installing vinyl plank flooring, you will need various cutting tools to ensure precise and efficient cuts. In this section, we will discuss three primary tools: Utility Knife, Vinyl Tile Cutter, and Jigsaw or Circular Saw.\nA utility knife is an essential tool for cutting vinyl planks, as it allows for clean, straight cuts. It is suitable for narrow planks or small adjustments where other cutting tools might be too cumbersome. When using a utility knife, ensure you have a sharp blade and a straight edge to guide your cuts.\nVinyl Tile Cutter\nA vinyl tile cutter is a specialized cutting tool designed explicitly for cutting vinyl planks. This tool can provide swift and precise cuts, significantly speeding up the installation process. A vinyl tile cutter typically consists of a cutting mechanism and a measuring guide, which helps ensure accurate cuts. Some of the benefits of using vinyl tile cutters are:\n- Quick and accurate cuts\n- Minimal dust and debris\n- Easy to operate with minimal effort\nJigsaw or Circular Saw\nFor more complex cuts, such as around door frames or vents, a jigsaw or circular saw is recommended. These power tools can handle intricate and curved cuts with ease. When using a jigsaw or circular saw for cutting vinyl planks, it is crucial to use the appropriate blade designed for cutting vinyl materials. Some helpful tips for using these saws are:\n|Use a fine-toothed blade||A fine-toothed blade reduces the risk of chipping and gives smoother cuts.|\n|Cutting speed||Maintain a slow and steady cutting speed to prevent overheating of the blade.|\n|Support the vinyl plank during cutting||Ensure the vinyl plank is adequately supported to reduce vibration and movement during cutting.|\nEach cutting tool has its benefits, and having a combination of these tools at hand will help you install vinyl plank flooring efficiently and effectively.\nTapping Block and Rubber Mallet\nWhen installing vinyl plank flooring, you’ll need a tapping block and a rubber mallet to help ensure a tight, secure fit between the planks. The tapping block provides even pressure distribution, protecting the flooring from damage, while the rubber mallet enables you to tap the planks gently into place without causing dents or scratches.\nA pull bar is a vital tool for fitting the last rows of planks near the wall. This tool helps to close and tighten the gaps between planks, allowing for a neat and professional finish. The pull bar works by hooking onto the edge of the plank and applying pressure when tapped with a mallet.\nSpacers and Wedges\nSpacers and wedges ensure that the vinyl plank flooring expands and contracts evenly, maintaining the appropriate distance from the walls. Using these tools helps to prevent buckling, warping, or gap issues. Place spacers around the room’s perimeter, maintaining a consistent gap, while wedges are used to tightly align the planks.\n- Spacers: Maintain a consistent gap around the room’s perimeter\n- Wedges: Ensure tightly aligned planks\nA flooring roller is essential for applying consistent pressure across the entire surface of the vinyl plank flooring during installation. This tool ensures proper adhesion between the planks and subfloor, removing any trapped air or imperfections. A flooring roller typically weighs between 50 to 100 pounds and can be rented or purchased for the job.\nWhen installing vinyl plank flooring, it is essential to use the appropriate safety equipment to protect yourself from potential hazards. Some of the key safety items you should have on hand when working with vinyl planks include:\n- Protective eyewear: Safety glasses or goggles help shield your eyes from dust and small debris.\n- Ear protection: Earplugs or earmuffs can protect your hearing when using noisy power tools, such as a chop saw or jigsaw.\n- Gloves: Durable work gloves provide a better grip while handling vinyl planks and protect your hands from cuts.\n- Knee pads: Wearing knee pads can significantly reduce strain and discomfort when working on your knees for an extended period.\nUsing these safety items can help prevent accidents and injuries during the installation process. Additionally, it is a good idea to follow some general safety guidelines:\n|Proper ventilation||Working in a well-ventilated area helps in dispersing dust and fumes generated during cutting and installation of vinyl planks.|\n|Work area cleanliness||Keep the work area tidy by removing trip hazards and disposing of cut-off materials promptly.|\n|Tool maintenance||Regularly inspect your tools and ensure they are in proper working order to avoid malfunctions that could lead to injury.|\nBy incorporating these safety equipment items and guidelines, you can minimize risks and complete your vinyl plank flooring installation with confidence.']	['<urn:uuid:3ab6ee1a-1cc5-4dd1-b5ce-3f2b277579ac>', '<urn:uuid:165d6618-12b5-4267-b856-8626e89872d6>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T20:35:02.547110	29	82	1721
48	tinnitus multi infarct dementia treatment ginkgo effectiveness	Ginkgo biloba shows different treatment implications for tinnitus and multi-infarct dementia. For tinnitus, while many people believe ginkgo biloba is helpful, large-scale studies have been unable to prove its effectiveness as a treatment. For multi-infarct dementia, ginkgo biloba should be completely avoided due to increased risk of bleeding, especially in cases involving cerebral amyloid angiopathy.	"[""None of these treatment options are supported by science. Many people are convinced that the herb gingko biloba is helpful, however large-scale studies have been unable to prove this. There are many nutritional supplements claiming to be tinnitus remedies. These are usually a combination of herbs and vitamins, often including zinc, ginkgo, and vitamin B-12.\nLidocaine, a medication used for the treatment of certain types of abnormal heart rhythms, has been shown to relieve tinnitus for some people, but it must be given intravenously or into the middle ear to be effective. However, the benefits of lidocaine are almost always outweighed by the risks of the drug and it is therefore not recommended and not used for tinnitus.\nMost tinnitus is subjective, meaning that only you can hear the noise. But sometimes it's objective, meaning that someone else can hear it, too. For example, if you have a heart murmur, you may hear a whooshing sound with every heartbeat; your clinician can also hear that sound through a stethoscope. Some people hear their heartbeat inside the ear — a phenomenon called pulsatile tinnitus. It's more likely to happen in older people, because blood flow tends to be more turbulent in arteries whose walls have stiffened with age. Pulsatile tinnitus may be more noticeable at night, when you're lying in bed and there are fewer external sounds to mask the tinnitus. If you notice any new pulsatile tinnitus, you should consult a clinician, because in rare cases it is a sign of a tumor or blood vessel damage.\nHer most recent study, published in January 2018 in the journal Science Translational Medicine, showed success rates similar to Kilgard’s on 20 adult tinnitus patients. (5) People who underwent the therapy 30 minutes a day for one month reported about a 50 percent drop in the loudness of their tinnitus. More than half of the study participants also reported that their tinnitus bothered them less after the therapy, she says.\nThe researchers point out that up to one in five adults will develop tinnitus, a distressing disorder in which people hear buzzing, ringing and other sounds from no external source. Tinnitus can occur in one or both ears, and is usually continuous but can fluctuate. A randomised controlled trial is the best way of assessing the effectiveness of an intervention.\nWith respect to incidence (the table above is about prevalence), Martinez et al (2015) reported that there were 5.4 new cases of tinnitus per 10,000 person-years in England. We don't find this statistic much use as tinnitus is highly prevalent in otherwise normal persons. It seems to us that their study is more about how many persons with tinnitus were detected by the health care system -- and that it is more a study of England's health care system than of tinnitus.\nIndividuals with tinnitus describe perceiving a wide variety of sounds including ringing, clicking, hissing, humming, chirping, buzzing, whistling, whooshing, roaring, and/or whirling. These sounds may be present at all times, or they may come and go. The volume, pitch or quality of tinnitus sounds can fluctuate as well. Some people report that their tinnitus is most obvious when outside sounds are low (i.e. during the night). Other individuals describe their tinnitus as loud even in the presence of external sounds or noise, and some describe it as exacerbated by sounds. Tinnitus can affect one ear or both ears. It can also sound like it is inside the head and not in the ears at all.\nTherefore, the Department of Defense and Congress have taken an interest in furthering tinnitus research, adding it to a list of researchable conditions that impact the military. Both American Tinnitus Association and the Department of Defense fund tinnitus research. New research developments are reported in journals such as Tinnitus Today and the International Tinnitus Journal.\nTinnitus can be caused by a variety of less common underlying conditions or injuries. If you have tinnitus, start your treatment path by seeking out a hearing healthcare professional who specializes in tinnitus diagnostics so they can help identify the underlying cause. If the common causes for tinnitus are ruled out, the practitioner will refer you to another specialist for further evaluation. Listed below are related medical specialties who might be able to help you determine the cause of your tinnitus.\nHerbal home remedies (ginkgo biloba, melatonin), and the vitamin zinc are not recommended by the American Academy of Otolaryngology. Lipo-flavonoid is a supplement being marketed as a way to relieve tinnitus, but there is no current evidence it is effective for most cases of the condition; however, it may be helpful for symptoms of Meniere's disease. Check with your doctor or other health care professional before taking any herbal or over-the-counter (OTC) natural remedies.\nWhile there may be a wide range of causes, an important underlying factor for the development of tinnitus is brain plasticity.5,7 This property allows the brain to change and adapt, and it is essential to how we learn. Unfortunately, in some cases, such as with hearing loss, the auditory part of the brain may be altered as brain plasticity tries to compensate for the abnormal auditory inputs. This response leads to changes in brain activity in the auditory system (e.g., the auditory cortex) that can create a phantom percept: tinnitus. As such, while tinnitus may begin a problem at the auditory periphery, it persists because of changes throughout the auditory system. Treating tinnitus may require addressing both the initiator (e.g., hearing loss) and the driver (changes in the auditory brain).\nDiseases, illnesses and injuries. There are several medical conditions that can cause tinnitus. These include Meniere’s disease, temporomandibular joint disorders (TMJ), head or neck injuries, brain tumors, etc. Most people don’t know if they have Meniere’s disease until properly diagnosed. This RARE disease brings on dizziness, tinnitus and ear pressure that can last for a short period of time and then disappears. TMJ causes pain in your jaw muscles. With TMJ, you’ll often hear a clicking noise when chewing. TMJ has shown to influence your chances of developing tinnitus, so be sure to treat the condition in order to reduce your chances of getting tinnitus. Head and neck injuries have also been shown to cause tinnitus, so always wear your helmet when you’re out biking and drive safely when you’re in your car.Believe it or not, but tinnitus can be caused by something as simple as an ear infection. Don’t take ear infections lightly they can be devastating at any age. Brain tumors, while equally as rare as Meniere’s disease, can also generate tinnitus symptoms. While you can alleviate your tinnitus immediately with an over the counter tinnitus treatment, you should also seek the help of a tinnitus specialist in your area to determine what the underlying cause of your tinnitus is.\nWhile tinnitus is as varied as its causes, it can be grouped into two categories: tonal and non-tonal. Tonal tinnitus is more common and describes the perception of a near-continuous sound or overlapping sounds with a well-defined frequency (e.g., whistling, ringing, buzzing). Non-tonal forms of tinnitus include humming, clicking, crackling, and rumbling.\nDemographic variables (age, sex, type of tinnitus) and baseline THI scores of placebo (n = 16) and treatment (n = 11) groups did not significantly differ from one another at the start of the study. At 3 months, participants in the treatment group reported significantly lower scores on the THI when compared to the placebo group (p < .05). The treatment group also showed an 11-point drop in THI scores when comparing baseline and 3 months (p < .05; please see Figure 2). THI scores for the placebo group comparing both time points were non-significant. Past studies have indicated that the minimum change in the THI score to be considered clinically significant is a drop of 6 to 7 points.9 As such, the results of our clinical study suggest that tinnitus and its related symptoms can produce a clinically significant reduction in tinnitus within the first 3 months using the personalized music-based therapy.\nChanges in the bones of the middle ear. A person’s ear is made up of several different bones: the malleus, Incus and Stapes. In some individuals, these bones may actually change shape or harden over the years. This process is known as otosclerosis and often runs in the family. This can cause ringing in the ears to begin or, if it has already started, to get worse over time.\nTinnitus Retraining Therapy. Tinnitus Retraining Therapy (TRT) combines a wearable device that is individually programmed to mask the specific tonal frequency of that person’s tinnitus, with psychological therapy that teaches a patient to ignore the sounds his tinnitus is creating. I consider it the best of all of the above noise suppression techniques, as it is individually tailored for each person and involves support from a trained psychological therapist. It is also the most expensive and time consuming, but in my medical opinion, the most beneficial of all the noise suppression techniques listed above.\nThe physician may also request an OAE test (which is very sensitive to noise induced hearing damage), an ECochG (looking for Meniere's disease and hydrops, an MRI/MRA test (scan of the brain), a VEMP (looking for damage to other parts of the ear) and several blood tests (ANA, B12, FTA, ESR, SMA-24, HBA-IC, fasting glucose, TSH, anti-microsomal antibodies).\nSubjective tinnitus is the most frequent type of tinnitus. It can have many possible causes, but most commonly it results from hearing loss. When the tinnitus is caused by disorders of the inner ear or auditory nerve it is called otic (from the Greek word for ear). These otological or neurological conditions include those triggered by infections or drugs. A frequent cause is noise exposure that damages hair cells in the inner ear.\nTinnitus is the name for hearing a sound that is not physically present in the environment. Some researchers have also described tinnitus as a “phantom auditory perception.” People with tinnitus most often describe it as ringing, buzzing, cricket sounds, humming, and whooshing, although many other descriptions have been used. To hear some sound samples access the American Tinnitus Association website, where they have put together files of different manifestations of tinnitus to listen to for education purposes.\nAn ultrasound is another test that may be used to aid in the diagnosis of tinnitus. An ultrasound uses reflected high-frequency sound waves and their echoes to create images of structures within the body. An ultrasound can reveal how blood flows within vessels, but is only useful for accessible vessels. It is not helpful for blood vessels within the skull.\nSome instances of tinnitus are caused by infections or blockages in the ear, and the tinnitus can disappear once the underlying cause is treated. Frequently, however, tinnitus continues after the underlying condition is treated. In such a case, other therapies -- both conventional and alternative -- may bring significant relief by either decreasing or covering up the unwanted sound."", 'Multi-infarct dementia is a heterogeneous disorder caused by the sequel of cerebrovascular disease that manifests in cognitive impairment affecting memory, thinking, language, behavior, and judgment.\nMulti-infarct dementia was first mentioned by Thomas Willis in 1672. It was later further described in the late 19th century by Binswanger and Alzheimer as a separate entity from dementia paralytica caused by neurospyhillus (1). This concept has evolved tremendously since the advent of neuroimaging modalities.\nSynonym(s): Vascular dementia (VaD); Vascular cognitive impairment (VCI); Vascular cognitive disorder (VCD); Arteriosclerotic dementia; Post-stroke dementia; Senile dementia due to hardening of the arteries; Binswanger’s disease; mixed dementia (2).\n- It is the 2nd most common cause of dementia after Alzheimer’s dementia in the elderly.\n- After careful consideration of the difficulties in diagnosing multi-infarct dementia and the many geographical and methodological differences, there is a lack of agreement in terms of its prevalence and epidemiology.\nAbout 6–12 cases per 1000 person years >70 (3)\n- About 1.2–4.2% in those >65 (3)\n- 14–32% prevalence of dementia after a stroke\n- Previous stroke\n- Atrial fibrillation\n- Peripheral vascular disease (PVD)\n- Metabolic syndrome\n- Coronary atherosclerotic heart disease\n- Cerebral autosomal dominant arteriopathy (CADASIL) is caused by a mutation in the NOTCH3 gene on chromosome 19 that results in leukoencephalopathy and subcortical infarcts. This is clinically manifested in recurrent strokes and associated cognitive decline (4).\n- Apolipoprotein E gene type: Those with ApoE4 subtypes are at higher risk of developing both multi-infarct and Alzheimer’s dementia.\n- Amyloid precursor protein (APP) gene: Leads to a form of vascular dementia called heritable cerebral hemorrhage with amyloidosis (5).\n- Optimization and aggressive treatment of vascular risk factors such as hypertension, diabetes, and hyperlipidemia\n- Hypertension is the single most modifiable risk factor and must be optimized.\n- Lifestyle modification: Weight loss, physical activity, smoking cessation\n- Medication management for vascular risk reduction: Aspirin usage, statin therapy for hyperlipidemia, antihypertensive therapy\nUpon autopsy of those with dementia, 1/3 have significant vascular pathology present but this is not necessarily correlated clinically with multi-infarct dementia (6). There are no set pathological criteria for the diagnosis of multi-infarct dementia such as those that exist for Alzheimer’s dementia.\n- Large vessel disease: Cognitive impairment that follows a stroke\n- Small vessel disease: Includes white matter changes (leukoaraisosis), subcortical infarcts, and incomplete infarction. This is usually the most common cause of multi-infarct dementia.\n- Subcortical ischemic vascular disease: Due to small vessel involvement within cerebral white matter, brainstem, and basal ganglia. Lacunar infarcts and deep white matter changes are typically included in this category (7).\n- Non-infarct ischemic changes and atrophy (8)\n- Transient ischemic attack (TIA)/stroke\n- Vascular, demographic, genetic factors\n- Vascular disease (i.e., hypertension, PVD, atrial fibrillation, hyperlipidemia, diabetes, etc.)\nCommonly Associated Conditions\n- Cerebral amyloid angiopathy (CAA): Accumulation of amyloid in cerebral vasculature resulting in infarctions and hemorrhages (9).\nDifferentiation between Alzheimer’s dementia and multi-infarct dementia can be difficult and there can be significant overlap in the clinical presentation of these two dementias. The diagnosis of multi-infarct dementia is a clinical diagnosis.\n- Gradual, stepwise progression is typical.\n- Ask about onset and progression of cognitive impairment and the specific cognitive domains involved.\n- Ask about vascular risk factors and previous attempts to control these risk factors.\n- Ask about medication compliance.\n- Ask about urinary incontinence and gait disturbances.\n- Look for early symptoms including difficulty performing cognitive tasks, memory, mood, and assessment of instrumental activities of daily living (IADLs) (8).\n- Past history may include TIAs, cerebrovascular accidents, coronary atherosclerotic heart disease, atrial fibrillation, hyperlipidemia, and/or peripheral vascular disease.\n- Screen for hypertension\n- Focal neurological deficits may be present\n- Gait assessment is important especially looking at gait initiation, gait speed, and balance.\n- Check for carotid bruits as well as abdominal bruits and assess for presence of peripheral vascular disease.\n- Check body mass index and waist circumference.\n- Do a thorough cardiac evaluation that includes looking for arrhythmias (i.e., atrial fibrillation).\nDiagnostic Tests & Interpretation\n- Cognitive screening, such as Mini-Cog, Mini-Mental Status Exam (MMSE), Saint Louis University Mental Status (SLUMS), and Montreal Cognitive Assessment (MOCA), provides more definitive information in terms of cognitive deficits, especially executive function, which may be lost earlier in multi-infarct dementia (8).\n- Neuropsychological testing may also be beneficial especially in evaluating multiple cognitive domains and their specific involvements and deficits.\nAs appropriate, consider: Complete blood count, comprehensive metabolic Profile, lipid panel, thyroid function, hemoglobin A1C, vitamin B12.\n- Imaging is used in conjunction with history and physical examination to support a clinical diagnosis of multi-infarct dementia.\n- Cognitive deficits observed clinically do not always have to correlate with findings found on neuroimaging studies.\n- There are no pathognomonic neuroimaging features for multi-infarct dementia (6).\n- Magnetic resonance imaging (MRI) is best in terms of evaluation of subtle subcortical deficits.\n- Alzheimer’s dementia\n- Drug intoxication\n- Central nervous system tumors\n- Vitamin B12 deficiency\nPrevention is the real key to treatment.\n- Control of risk factors including hypertension, hyperlipidemia, and diabetes.\n- Avoidance of tobacco and stopping smoking.\n- Healthy, low-cholestrol diet\n- Acetylcholinesterase inhibitors may be used but are of limited benefit in multi-infarct dementia (6,10).\n- The clinical evidence for use of memantine is not as strong as for acetylcholinesterase inhibitors and therefore the clinical benefit is likely modest (6).\n- Controlling blood pressure with any antihypertensive medications, treatment of dyslipidemia (e.g., statins), and treatment of diabetes are very important.\n- Limit alcohol intake to ≤1 drink per day in women and 2 per day in men.\n- Heavy sustained alcohol use contributes to hypertension.\n- Aspirin and/or clopidogrel may be useful in some cases.\nComplementary and Alternative Medicine\nGinkgo biloba should be avoided due to increased risk of bleeding especially in cerebral amyloid angiopathy.\nCarotid endarterectomy or stenting if evidence of significant internal carotid artery stenosis (i.e., >70-80%).\n- Remain sensative to functional assessment and avoidance of pressure ulcers after cerebrovascular accients.\n- Urinary incontinence treatment may be needed after CVA.\n- Avoid Foley catheter usage unless absolutely necessary due to increased risk of infection.\n- Nonpharmacological approaches to behavior management should be attempted prior to medication usage.\n- Providing optimal sensory input to patients with cognitive impairment is important during hospitalizations to avoid delirium and confusion. Patients should be given frequent cues to keep them oriented to place and time. They should be informed of any changes in the daily schedule of activities and evaluations. Family and caregivers should be encouraged to be with patients with dementia as much as possible to further help them from becoming confused during hospitalization. Recreational, physical, occupational, and music therapy can be beneficial during hospitalization in avoiding delirium and preventing functional decline.\n- Particular emphasis has to be placed on screening for and optimizing the mood of the patient. Depression is very common in older patients especially those that have had strokes and have become hospitalized. Depression in itself can present as “pseudodementia” with worsening confusion during hospitalization and is a treatable condition.\nMulti-infarct dementia is a condition that should be followed with multiple visits in the office setting with goals of optimizing cardiovascular risk profiles for patients. Future planning and advanced directives should be addressed early. Family and caregiver evaluation and burden should also be evaluated.\nRegular follow-up with a primary care provider or geriatrician for risk factor modification and education on importance of regular physical and mental exercises as tolerated.\nAppropriate evaluation and diagnosis of this condition, need for future planning, optimizing vascular risk factors, lifestyle modification counseling, therapeutic interventions\n- American Heart Association diet and DASH diet recommended for optimal blood pressure and cardiovascular risk factor control.\n- Low-fat, decreased concentrated sweets and carbohydrates especially in those with metabolic syndrome.\n- Lifestyle modification is important in vascular risk reduction (smoking cessation, exercise counseling, dietary counseling, weight loss counseling).\n- Optimizing vascular risk factors via medications (i.e., hypertension, diabetes, atrial fibrillation, PVD, heart disease)\n- Avoiding smoking including secondhand smoke.\n- Home blood pressure monitoring and glucometer testing of blood sugars if hypertension, impaired glucose tolerance and/or diabetes is present.\n- Instruct patients to call 911 for any TIA type symptoms.\n- Lost cognitive abilities that persist after initial recovery of deficits from stroke do not usually return. Some individuals can have intermittent periods of self-reported improvement in cognitive function.\n- Risk factors for progression of cognitive and functional impairment poststroke include age, prestroke cognitive abilities, depression, polypharmacy, and decreased cerebral perfusion during acute stroke (6).\n- Physical disability from stroke\n- Severe cognitive impairment\n1. Román G et al. Vascular dementia: a historical background. Int Psychogeriatr. 2003;15 Suppl 1:11–3.\n2. Jellinger KA et al. The pathology of “vascular dementia”: a critical update. J. Alzheimers Dis. 2008;14:107–23.\n3. Hébert R, Brayne C et al. Epidemiology of vascular dementia. Neuroepidemiology. 1995;14:240–57.\n4. Pinkston JB, Alekseeva N, González Toledo E et al. Stroke and dementia. Neurol. Res. 2009;31:824–31.\n5. Russell MB et al. Genetics of dementia. Acta Neurol. Scand., Suppl. 2010;:58–61.\n6. Rojas-Fernandez CH, Moorhouse P et al. Current concepts in vascular cognitive impairment and pharmacotherapeutic implications. Ann Pharmacother. 2009;43:1310–23.\n7. Chui HC et al. Subcortical ischemic vascular dementia. Neurol Clin. 2007;25:717–40, vi\n8. Moorhouse P, Rockwood K et al. Vascular cognitive impairment: current concepts and clinical developments. Lancet Neurol. 2008;7:246–55.\n9. Rockwood K, Ebly E, Hachinski V, Hogan D et al. Presence and treatment of vascular risk factors in patients with vascular cognitive impairment. Arch. Neurol. 1997;54:33–9.\n10. Kavirajan H, Schneider LS et al. Efficacy and adverse effects of cholinesterase inhibitors and memantine in vascular dementia: a meta-analysis of randomised controlled trials. Lancet Neurol. 2007;6:782–92.\nSee Also (Topic, Algorithm, Electronic Media Element)\nAlzheimer’s Dementia, Depression, and Mild Cognitive Impairment\n290.40 Vascular dementia, uncomplicated\n- 56267009 multi-infarct dementia (disorder)\n- 429998004 vascular dementia (disorder)\n- Executive dysfunction and gait abnormalities are often seen early and are more pronounced in multi-infarct dementia as opposed to Alzheimer’s dementia (48).\n- Memory is relatively preserved in multi-infarct dementia when compared to Alzheimer’s dementia in the early stages of this disease (4).\n- Stepwise progression as opposed to progressive decline in Alzheimer’s dementia is typical.']"	['<urn:uuid:0eb8dc34-6700-4a1e-933b-1ee50c8a43f7>', '<urn:uuid:efe89ffa-9b2f-4711-969c-3c22c5079ef2>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	7	55	3520
49	doctor advise nsaids opioids chronic inflammatory pain	For chronic inflammatory pain, NSAIDs and opioids have different considerations. NSAIDs work by blocking chemicals that cause inflammation and are good at treating pain from slow tissue damage, like arthritis pain. They can be used for longer periods under medical supervision, but require monitoring for side effects. Opioids, while very effective for pain relief, must be used with great caution due to the risk of mental and physical addiction. When opioids are used long-term, doctors must closely monitor patients. In some cases, combination medications (like Vicodin and Percocet which combine opioids with acetaminophen) may provide better pain relief than either medication alone, allowing for lower doses of each medication.	"[""Editorial Staff and Contributors\nThe information provided here is meant to give you a general idea about each of the medications listed below. Only the most general side effects are included, so ask your doctor if you need to take any special precautions. Use each of these medications as recommended by your doctor, or according to the instructions provided. If you have further questions about usage or side effects, contact your doctor.\nMedications may help to either prevent or reduce side effects of treatment or to manage certain side effects after they occur. You can develop side effects from the treatment and/or from the cancer itself. Tell your doctor when you notice a new symptom, and ask if any of these medications are appropriate for you.\nNonsteroidal Anti-inflammatory Drugs (NSAIDs)\nCommon names include:\nAntiemetics are given to help treat nausea and vomiting that may be caused by\nradiation therapy, or surgery to treat cancer. Prochlorperazine can be taken by mouth, injection, or a suppository. Ondansetron and granisetron can be taken orally or as injections. Metoclopramide is usually given by injection.\nSome side effects include:\nCorticosteroids help to minimize inflammation and to relieve pain due to inflammation. You may experience pain and inflammation for a variety of reasons, such as:\nCommon side effects include:\nOpioids act on the central nervous system to relieve pain. These drugs can be very effective however, they must be used with great caution because they can be mentally and/or physically addicting. If you are going to take one of these drugs for a long period of time, your doctor will closely monitor you.\nVicodin and percocet is a combination medication. An opioid analgesic and acetaminophen used together may provide better pain relief than either medication used alone. In some cases, lower doses of each medication are necessary to achieve pain relief.\nThe most common side effects of opioids include:\nDuring cancer treatment, blood cells can be destroyed along with cancer cells. Filgrastim helps your bone marrow make new white blood cells. White blood cells help your body fight infection. Therefore, filgrastim helps to reduce your risk of infection.\nEpoetin helps your bone marrow to make new red blood cells. Low red blood cell levels can lead to anemia. Therefore, epoetin helps reduce your risk of anemia. Epoetin is effective, but it has a 2-week delay between the injection and when your red blood cell count starts to come back. It is not used as a quick fix for a low red blood cell count. A blood transfusion is usually performed if you need to recover your red blood cell count more quickly.\nBoth filgrastim and epoetin are given by injection in your doctor's office.\nSome specific bacterial infections are associated with lymphomas that affect the stomach, lungs, or intestines. Antibiotics are used to fight the infection. In some cases, it may also help with treating the associated lymphoma. Antibiotics are usually taken by mouth. If the infection is serious, they can be given by IV. For some infections, a combination of antibiotics may work best. Talk to your doctor if you are or think you are pregnant, or breastfeeding. Some antibiotics may need to be avoided during pregnancy. It is important to take all of the antibiotics as prescribed, even when you are feeling well.\nSerious side effects associated with clarithromycin include:\nNSAIDs are used to relieve pain and inflammation. You may experience pain and inflammation for a variety of reasons, such as:\nIf you are taking medications, follow these general guidelines:\nBallantyne J, Mao J. Opioid therapy for chronic pain.\nN Engl J Med.\nClarithromycin. EBSCO DynaMed Plus website. Available at: http://www.dynamed.com/topics/dmp~AN~T233406/Clarithromycin. Updated September 27, 2016. Accessed October 6, 2016.\nFDA's MedWatch safety alerts. US Food and Drug Administration website. Available at:\nhttp://www.fda.gov/ForConsumers/ConsumerUpdates/ucm142815.htm. Updated October 14, 2014. Accessed March 28, 2016.\nGourlay DL, Heit HA, Almahrezi A. Universal precautions in pain medicine: a rational approach to the treatment of chronic pain.\nHelicobacter pylori infection. EBSCO DynaMed Plus website. Available at: http://www.dynamed.com/topics/dmp~AN~T114484/Helicobacter-pylori-infection. Updated October 4, 2016. Accessed October 6, 2016.\nLarson AM, Polson J, Fontana RJ, et al. Acetaminophen-induced acute liver failure: results of a United States multicenter, prospective study.\nNon-Hodgkin lymphoma (NHL). EBSCO DynaMed Plus website. Available at: http://www.dynamed.com/topics/dmp~AN~T116014/Non-Hodgkin-lymphoma-NHL. Updated May 5, 2016. Accessed October 6, 2016.\nLast reviewed March 2015 by Mohei Abouzied, MD\nEBSCO Information Services is fully accredited by URAC. URAC is an independent, nonprofit health care accrediting organization dedicated to promoting health care quality through accreditation, certification and commendation.\nPlease be aware that this information is provided to supplement the care provided by your physician. It is neither intended nor implied to be a substitute for professional medical advice. CALL YOUR HEALTHCARE PROVIDER IMMEDIATELY IF YOU THINK YOU MAY HAVE A MEDICAL EMERGENCY. Always seek the advice of your physician or other qualified health provider prior to starting any new treatment or with any questions you may have regarding a medical condition.\nCopyright © EBSCO Information Services. All rights reserved."", ""What are non-steroidal anti-inflammatory drugs (NSAIDs)?\nWhen your back hurts, head aches, arthritis acts up or you’re feeling feverish, chances are you’ll be reaching for an NSAID (non-steroidal anti-inflammatory drug) for relief.\nYou take an NSAID every time you consume an aspirin, or an Advil®, or an Aleve®. These drugs are common pain and fever relievers. Every day millions of people choose an NSAID to help them relieve headache, body aches, swelling, stiffness and fever.\nYou know the most common NSAIDs:\n- Aspirin (available as a single ingredient known by various brand names such as Bayer® or St. Joseph® or combined with other ingredients known by brand names such as Anacin®, Ascriptin®, Bufferin®, or Excedrin®).\n- Ibuprofen (known by brand names such as Motrin® and Advil®).\n- Naproxen sodium (known by the brand name Aleve®).\nYou can get non-prescription strength, over-the-counter NSAIDs in drug stores and supermarkets, where you can also buy less expensive generic (not brand name) aspirin, ibuprofen and naproxen sodium.\nAcetaminophen (Tylenol®) is not an NSAID. It’s a pain reliever and fever reducer but doesn’t have anti-inflammatory properties of NSAIDs. However, acetaminophen is sometimes combined with aspirin in over-the-counter products, such as some varieties of Excedrin®.\nWhat do you use NSAIDs for?\nNSAIDs are used to treat:\n- Pain of rheumatoid arthritis (RA), osteoarthritis and tendonitis.\n- Muscle aches.\n- Dental pain.\n- Pain caused by gout.\n- Menstrual cramps.\nThey can also be used to reduce fever or relieve minor aches caused by the common cold.\nHow do NSAIDs work?\nNSAIDs block the production of certain body chemicals that cause inflammation. NSAIDs are good at treating pain caused by slow tissue damage, such as arthritis pain. NSAIDs also work well fighting back pain, menstrual cramps and headaches.\nNSAIDs work like corticosteroids (also called steroids), without many of the side effects of steroids. Steroids are man-made drugs that are similar to cortisone, a naturally-occurring hormone. Like cortisone, NSAIDs reduce pain and inflammation that often come with joint and muscle diseases and injuries.\nHow long should I use an over-the-counter NSAID?\nDon’t use an over-the-counter NSAID continuously for more than three days for fever, and 10 days for pain, unless your doctor says it’s okay. Over-the-counter NSAIDs work well in relieving pain, but they’re meant for short-term use.\nIf your doctor clears you to take NSAIDs for a long period of time, you and your doctor should watch for harmful side effects. If you notice bad side effects your treatment may need to be changed.\nHow long do NSAIDs take to work?\nThat depends on the NSAID and the condition being treated. Some NSAIDs may work within a few hours, while others may take a week or two.\nGenerally, for acute (sharp sudden pain) muscle injuries, we recommend NSAIDs that work quickly. However, these may need to be taken as often as every four to six hours because of their short action time.\nFor osteoarthritis and rheumatoid arthritis that need long-term treatment, doctors usually recommend NSAIDs that are taken only once or twice a day. However, it generally takes longer for these drugs to have a therapeutic (healing) effect.\nHow are NSAIDs prescribed?\nNSAIDs are prescribed in different doses, depending on the condition. These drugs may need to be taken from one to four times a day. Don’t increase the dose without asking your doctor first.\nYou may be prescribed higher doses of NSAIDs if you have rheumatoid arthritis (RA), for example. RA often causes a significant degree of heat, swelling and redness and stiffness in the joints. Lower doses may be prescribed for osteoarthritis and acute muscle injuries since there is generally less swelling and frequently no warmth or redness in the joints.\nNo single NSAID is guaranteed to work. You and your doctor may need to try out several types of NSAIDs in order to find the right one for you.\nWhen are stronger NSAIDs prescribed?\nPrescription-strength NSAIDs are often recommended for rheumatologic diseases, including rheumatoid arthritis and moderate-to-severe osteoarthritis. These NSAIDs are also prescribed for moderately painful musculoskeletal conditions such as back pain.\nWhat are some prescription NSAIDs?\nHere are a few examples of prescription NSAIDs. Some NSAIDs are only available as generic formulations (no brand names).\nGeneric names/common brand names\n- Celecoxib (Celebrex®)\n- Diclofenac (Voltaren® [available by brand name in topical form])\n- Fenoprofen (Nalfon®)\n- Indomethacin (Indocin® [available by brand name in liquid form])\n- Ketorolac tromethamine (Toradol®)\nGeneric names (no brands)\n- Meclofenamate sodium.\nHow does my doctor choose an NSAID that’s right for me?\nIn planning your treatment, your doctor looks at the effectiveness and the risks of these drugs. Your medical history, physical exam, X-rays, blood tests and presence of other medical conditions all play a part in deciding which NSAIDs will work for you.\nAfter you start your NSAID program meet with your doctor regularly to check for any harmful side effects and, if necessary, make any changes. Blood tests or other tests (including a kidney function test) may need to be done for this part of your treatment.\nAre there specific warnings associated with NSAID use?\nThe Food and Drug Administration requires that the labeling of NSAIDs contain these specific warnings:\nThese warnings are for non-aspirin NSAIDs:\n- Non-aspirin NSAIDs can increase the chance of heart attack or stroke. This risk may be greater if you have heart disease or risk factors (for example, smoking, high blood pressure, high cholesterol, diabetes) for heart disease. However, the risk may also be increased in people who do not have heart disease or those risk factors. This risk can occur early in treatment and may increase with longer use.\n- Heart problems caused by non-aspirin NSAIDs can happen within the first weeks of use and may happen more frequently with higher doses or with long-term use.\n- Non-aspirin NSAIDs should not be used right before or after heart bypass surgery.\nThis warning is for all NSAIDs including aspirin:\nNSAIDs may increase the chance of serious stomach and bowel side effects like ulcers and bleeding. These side effects can occur without warning signs. This risk may be greater in people who:\n- Are older.\n- Have previous history of stomach ulcers or bleeding problems.\n- Are on blood thinners.\n- Are on multiple prescription or over-the-counter NSAIDs.\n- Drink three or more alcoholic beverages per day.\nWhat are common side effects of NSAIDs?\nYou may have side effects if you take large doses of NSAIDs, or if you take them for a long time. Some side effects are mild and go away, while others are more serious and need medical attention. Unless your doctor tells you to do so, don't take an over-the-counter NSAID with a prescription NSAID, multiple over-the-counter NSAIDs or more than the recommended dose of an NSAID. Doing so could increase your risk of side effects.\nThe side effects listed below are the most common, but there may be others. Ask your doctor if you have questions about your specific medication.\nThe most frequently reported side effects of NSAIDs are gastrointestinal (stomach and gut) symptoms, such as:\nThese gastrointestinal symptoms can generally be prevented by taking the drug with food, milk or antacids (such as Maalox® or Mylanta®).\nCall your doctor if these symptoms continue for more than a few days even if you’re taking the NSAID with food, milk or antacid. The NSAID may need to be stopped and changed.\nOther side effects of NSAIDs include:\n- Feeling lightheaded.\n- Problems with balance.\n- Difficulty concentrating.\n- Mild headaches.\nIf these symptoms go on for more than a few days, stop taking the NSAID and call your doctor.\nWhat side effects should I tell my doctor about right away?\nIf you have any of these side effects, it is important to call your doctor right away:\n- Black stools – bloody or black, tarry stools.\n- Bloody or cloudy urine.\n- Severe stomach pain.\n- Blood or material that looks like coffee grounds in vomit (bleeding may occur without warning symptoms like pain).\n- Inability to pass urine, or change in how much urine is passed.\n- Unusual weight gain.\nHead (vision, hearing, etc.):\n- Blurred vision.\n- Ringing in the ears.\n- Photosensitivity (greater sensitivity to light).\n- Very bad headache.\n- Change in strength on one side is greater than the other, trouble speaking or thinking, change in balance.\nPossible allergic reactions and other problems\n- Fluid retention (recognized by swelling of the mouth, face, lips or tongue, around the ankles, feet, lower legs, hands and possibly around the eyes).\n- Severe rash or hives or red, peeling skin.\n- Unexplained bruising and bleeding.\n- Wheezing, trouble breathing or unusual cough.\n- Chest pain, rapid heartbeat, palpitations.\n- Acute fatigue, flu-like symptoms.\n- Very bad back pain.\n- Feeling very tired and weak.\nCan I take NSAIDs if I'm being treated for high blood pressure?\nNSAIDs can cause high blood pressure (hypertension) in some people. You may have to stop taking NSAIDs if you notice your blood pressure increases even if you’re taking your blood pressure medications and following your diet. Ask your doctor about this before you start taking NSAIDs.\nIn what cases should I check with my doctor before taking NSAIDs?\nIf you have any of the following conditions or circumstances please check with your doctor before you take NSAIDs:\n- Pregnancy (NSAIDs should be avoided in the third trimester. Consult with your provider about use in the first or second trimester).\n- Children and teenagers with viral infections (with or without fever) should not receive aspirin or aspirin-containing products due to the risk of Reye's syndrome (a rare but deadly illness that can affect the brain and liver).\n- Those who have an upcoming surgical procedure, including dental surgery.\n- People who have three or more alcoholic beverages per day.\n- Asthma that gets worse when taking aspirin.\n- If you are 65 years of age or older.\n- Diabetes that is difficult to control.\n- Known kidney disease.\n- Known liver disease.\n- Gastroesophageal reflux disease, also known as GERD.\n- Crohn’s disease or ulcerative colitis.\n- Active peptic ulcer disease (stomach ulcers or previous history of stomach ulcer bleeding).\nHeart and bleeding conditions\n- Bleeding problems (people who have a history of prolonged bleeding time or who bruise easily).\n- High blood pressure that is difficult to control.\n- Active congestive heart failure.\n- History of stroke or heart attack.\nAllergic and drug interactions\n- Known allergies to medications, especially aspirin, other NSAIDs and sulfa drugs.\n- Nasal polyps (linked to a greater chance of NSAID allergy).\n- Please check with your pharmacist or healthcare provider before starting an NSAID to determine if your current medications, both prescription and OTC, and also your dietary/herbal supplements, are compatible with the NSAID. Do this especially if you are on warfarin (Coumadin®), clopidogrel (Plavix®), corticosteroids (for example, prednisone), phenytoin (Dilantin®), cyclosporine (Neoral®, Sandimmune®), probenecid and lithium (Lithobid®).\n- If you take diuretics (also known as water pills) to control your blood pressure, you may be at greater risk of kidney problems if you take an NSAID.\n- Phenylketonuria (PKU). Some nonprescription NSAIDs are sweetened with aspartame, a source of phenylalanine.\nCan NSAIDs cause allergic reactions?\nRarely, an NSAID can cause a generalized allergic reaction known as anaphylactic shock. If this happens, it usually occurs soon after the person starts taking the NSAID. The symptoms of this reaction include:\n- Swollen eyes, lips or tongue.\n- Difficulty swallowing.\n- Shortness of breath.\n- Rapid heart rate.\n- Chest pain or tightness.\nIf any of these symptoms occur, call 9-1-1 or have someone drive you to the nearest emergency room immediately.\nRemember, before any medication is prescribed, tell your doctor:\n- If you are allergic to any medications, foods or other substances.\n- If you currently take any other medications (including over-the-counter medications) and/or herbal or dietary supplements.\n- If you are pregnant, planning to become pregnant, or are breast-feeding.\n- If you have problems taking any medications.\n- If you have anemia, kidney or liver disease, stomach or peptic ulcers, heart disease, high blood pressure, bleeding or clotting problems, asthma or growth in the nose (nasal polyps).""]"	['<urn:uuid:2160a2bd-1e3d-4bab-90cc-e30e6c162c9b>', '<urn:uuid:d789fb2e-6f95-48af-82c8-7d4a389b3fa4>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:35:02.547110	7	109	2862
50	best food source highest amount potassium per serving compare nutrients	White beans are the best source of potassium among all foods mentioned, containing 1,189 mg per cup, which represents a quarter of the daily recommended value (4,700 mg). Additionally, the same 1-cup serving of white beans provides 20 grams of protein and 13 grams of fiber, making it a nutrient-dense food choice.	['“Because every nutrition is important.”\nGood nutrition is an important part of leading a healthy lifestyle. Combined with physical activity, your diet can help you to reach and maintain a healthy weight, reduce your risk of chronic diseases (like heart disease and cancer), and promote your overall health. When you think about all of the nutrients your body needs, your mind might jump to protein, fiber, calcium, vitamin D, or even omega-3s. But what about potassium? The essential electrolyte apparently gets swept to the avocations.\nWhat is the importance of potassium in our body?\nPotassium helps your nerves and muscles communicate with one another, moves other nutrients into your cells, and keeps your sodium levels in check? Not getting enough of the stuff can cause high blood pressure (thanks to its close relationship with salt) and increase your risk of kidney stones, according to the National Institutes of Health.\nWe are here with the list of foods which provides a good amount of potassium to our body. But if you default to bananas, not so fast. While each medium banana has 422 mg of the mineral maybe about 9 percent of your 4,700 mg recommended daily value (DV) but you can easily find more in other fruits and vegetables.\nWe are here with the list of 16 foods that has more potassium than a banana:\nRegular plain yogurt (not the Greek stuff) has an impressive 573 mg (12% DV) of potassium per cup. But, it should not be taken in the breakfast as yogurt and curds are not good to take in empty stomach. Plus, it packs nearly half your daily calcium needs. Look for one that contains live active cultures, so you’ll get a nice dose of gut-friendly probiotics, too.\n2. Butternut Squash\nOne cup of this slightly sweet fall favorite packs 582 mg (12% DV) of potassium. You’ll also get a hefty dose of vitamin A, along with some vitamin C, magnesium, folate, and calcium.\n3. Swiss Chard\nOne cup of cooked chard has a whopping 961 mg (20% DV) of potassium. These hearty greens also pack calcium, iron, and vitamins A, C, and K.\n4. White Beans\nWhite beans might be the best source of potassium in the grocery store: A single cup has a whopping 1,189 mg. That’s a full quarter of what you need every day. That same 1-cup serving also packs an impressive 20 grams of protein and 13 grams of fiber.\n5. Tomato Sauce\nThis plain old pasta topper is a secret source of potassium, with 728 mg (15% DV) in each cup. Tomatoes are also rich in lycopene, a disease-fighting plant pigment that gives certain fruits and vegetables their signature red hue. Look for a low-sugar tomato sauce sold in BPA-free packaging.\n6. Canned Salmon\nCanned salmon is a lazy cook’s dream. Pop open one 5-ounce can and you’ll get 487 mg (10% DV) of potassium. What’s more, salmon is rich in omega-3 fatty acids, which are essential fats for your eye, heart, and brain health that your body can’t make on its own. Salmon is also high in B vitamins, which aid in the production of red blood cells and convert the food you eat into energy. On top of that, salmon is a great source of lean protein—perfect for those trying to lose weight or build muscle.\nNosh on two refreshing watermelon wedges, and you’ll get 641 mg (14% DV) of potassium. Watermelon is also a great source of lycopene, as well as vitamins A, C, and B6. Plus, more than 90 percent of the fruit is water, so you’ll feel full after snacking for very little calories. And if you rather sip the stuff? Cold-pressed watermelon juice is a great alternative.\n8. Black Beans\nChances are you’re already buying canned black beans for a boost in fiber and protein—two nutrients that keep you feeling full longer. However, they’re also a great source of potassium. Eat one cup and you’ll get 739 mg (16% DV) of the mineral. Black beans also offer some calcium, magnesium, and folate.\n9. Frozen Spinach\nAdd 1 cup of frozen spinach to your next stir-fry or pasta dish and you’ll get a respectable 540 mg (11% DV) of potassium. Spinach is also rich in magnesium, vitamin A, and calcium. Bonus: It’s crazy inexpensive—usually much cheaper than fresh veggies.\n10. Sweet Potato\nA medium baked sweet potato has 542 mg (12% DV) of potassium. These tubers are also rich in vitamin A for your eyes, vitamin C for your skin, and gut-filling fiber. They also just happen to be ridiculously tasty.\n11. White Potato\nSurprise, surprise: A single medium baked potato has 941 mg (20% DV) of potassium. You’ve probably been conditioned to fear these spuds, but when prepared the right way (baked or boiled instead of deep fried), they’re low in calories, fat, and sodium. Plus, white potatoes offer a healthy dose of vitamin C and magnesium, too. Let your spud cool before you eat it and you’ll get a dose of gut-friendly resistant starch.\nA cup of cooked, sliced beets delivers 518 mg (11% DV) of potassium, while a 1-ounce serving of beet chips has an impressive 90 mg. One snack to try: Rhythm Superfoods Naked Beet Chips. The sweet root vegetables are super versatile, though, and can be used in everything from salads to juices to soups.\nAvocados provide a whopping 507 mg of potassium per 3.5 ounces. Moreover, they are a great source of healthy fats and fiber. Avocados lend a nice creaminess to recipes. You can enjoy it over toast, create a delicious pasta sauce, or whip it into a flavorful salad dressing.\n14. Coconut Water\nStore-bought coconut water packs a powerful punch of potassium, delivering about 350 mg per 8 fluid ounces. It makes a great alternative to sugary sports drinks and a delicious base for post-workout smoothies. Just be sure to buy the unsweetened versions to avoid added sugar.\n15. Dried Apricots\nDried apricots supply 430 mg of potassium per 6-piece serving, giving you a big nutritional bang for your buck. Remember to choose unsweetened versions at the grocery store to avoid loading up on extra sugar. We like to chop dried apricots and incorporate them into homemade granola bars and trail mixes.\nWhole soybeans are one of the world’s greatest sources of plant-based protein, but that’s not the only trick up their sleeve: 1 cup also supplies 676 mg (14% DV) of potassium. Eat them as a snack, toss ’em in a salad, or serve them up as a side dish.\nThere’s a reason athletes love to have beetroot juice recently, researchers concluded that drinking the stuff 90 minutes before your workout could boost performance (Just don’t freak out if they turn your pee pink or red afterward. It’s totally normal, we swear). Your food choices each day affect your health — how you feel today, tomorrow, and in the future.\nCover Image: Source']	['<urn:uuid:6c1cfd27-aa8f-44ff-b2f7-308aebb528a6>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	10	52	1156
51	fm synthesis how does carrier frequency affect harmonics and what is relationship with harmonicity ratio	The carrier frequency affects the fundamental pitch, while the harmonicity ratio controls the relationship of sidebands to the carrier. With integer harmonicity ratios, the sidebands are harmonic. When the carrier frequency changes, the harmonicity ratio does not need to change to maintain the same timbre, as it's calculated as a ratio to the fundamental. For example, 500 Hz modulated by 124.8 Hz will have the same timbral character as 1000 Hz modulated by 249.6 Hz. The modulation index controls the intensity of the partials, with higher values introducing more harmonics.	"['Few soft-synths have stood\nthe test of time like Propellerhead Thor. Introduced 10 years ago, Thor remains\nthe centerpiece of Reason’s synthesizer suite and is also available as an iOS\napp. As a result, the synth has achieved a serious following, thanks to its\ncollection of vintage-inspired oscillators and filters, modulation matrix, and\nability to route signals in ways we associate with modular gear. This month\nwe’ll explore ways to make greater use of the powerful engine lurking under its\nFig. 1. Thor’s Mod\nEnvelope stages can sync to tempo and loop, enabling it to double as an LFO.\nFig. 2. Global Envelope, in conjunction with its global filter, lets you re-create ’70s-era paraphonic synths with ease.\nStarting with tips on modulation tools may seem a bit unorthodox, but in the case of Thor it makes sense because the synth’s distinctive envelopes, LFOs, and step-sequencers are the keys to unlocking its design resources. With a modulation matrix that accesses nearly every synthesis parameter, understanding the possibilities is essential.\nEnvelopes. While the amplifier and filter envelopes are straightforward ADSR affairs, the Mod Envelope offers unique features that make it useful for a variety of alternate applications (see Figure 1). For example, its delay, attack, decay, and release parameters can all be quantized to note values and synced to tempo–fantastic for dance music. If you want to create a filter decay that’s exactly one measure, just turn on tempo sync, set the decay to 4/4 (four quarter notes) and you’re in business.\nAnother handy feature is the ability to loop the delay, attack, and decay parameters, creating a customizable LFO. Want a classic sawtooth? Set the attack to zero and the decay to a longer amount. Ramp up? Invert those values. Triangle? Set them both to equal amounts.\nThere’s also a fourth global envelope that works in a paraphonic manner (see Figure 2). That is, it affects whatever it’s assigned to in monophonic fashion with “single-trigger” operation, much like an old-school string synth such as the Moog Opus 3. This means that the envelope doesn’t retrigger until all keys are lifted, at which point it resets. This envelope also includes an additional hold segment between the attack and decay—great for creating the punchy sound of vintage Moogs.\nLFOs. Thor’s dual LFOs offer a lot of exotic functionality despite their deceptively simple layouts. For starters, there’s a much wider range of waveshapes than you’ll find on most synths: Eighteen in total. Fortunately, every waveform is displayed graphically. It’s also worth noting that the last nine shapes are stepped and can be used for faux arpeggiation effects when applied to the oscillators, or pseudo step-sequencer patterns when applied to filters or other timbral destinations.\nFig. 3. In addition to tempo-sync, the primary LFO can also deliver audio-range modulation with scalable keyboard tracking.\nAlthough the LFO’s rate parameters can be synced to tempo, in standard mode they extend into the lower audio range, which is fantastic for nasty FM effects. Additionally, LFO 1 includes a key-follow parameter that lets the rate track the keyboard, with higher notes increasing in speed accordingly (see Figure 3).\nWhereas LFO 1 is polyphonic, with a discrete instance for each voice, LFO 2 is paraphonic—like the global envelope—and affects all voices simultaneously. This is great for ’80s funk chord stabs (a la Prince) when used in triangle or sine mode and applied to pitch. Of special note to Deadmau5 fans: Setting LFO 2’s waveform to the downward saw-like shape, with a synced eighth-note rate, and applying it to filter cutoff, results in his trademark “pulsed chord” effect.\nStep Sequencer. Thor’s integrated step sequencer is deep enough to warrant its own tutorial, but in the interest of our emphasis on sound design features, we’ll focus on setting it up exclusively as a parameter modulation tool. For that, just toggle off all of the red switches under its 16 steps. This deactivates the note triggers, leaving its dual curves available for sequencing the values of destination parameters assigned to it. With forward, backward, two types of “pendulum” (back-and-forth), and random modes, Thor’s step sequencer is a powerful modulation feature for complex rhythmic/tonal patterns.\nNow that we have a handle on Thor’s modulation amenities, it’s time to explore their applications. Thor’s audio path includes six oscillator types that can be configured in countless ways, offering a vast range of synthesis techniques.\nEach of the oscillator modes is based on a different synthesis type, including analog, FM, wavetable, phase modulation, multi-oscillator, and noise. Connoisseurs may immediately recognize the original sources for all of these models, but the bottom line is that each option is sonically distinctive.\nAnalog. While this oscillator is the most familiar, it’s worth noting that the pulse width is continuously variable, so classic pulse-width modulation—via LFO or envelope—is available. Square waves are produced at the 50% setting and the extremes are either 0% or 100%, which result in no sound. This may be a bit confusing for newcomers, especially with extreme modulation amounts, which may cause the oscillator to unexpectedly cut out.\nPro Tip: Apply one of the step sequencer curves to the pulse width for unusual rhythmic timbres, especially where the modulation can reach the extremes of 0% or 100%, creating a gated effect.\nPro Tip 2: In sine wave mode, you can use the oscillator to reinforce the fundamental and add body to a patch, or add low-end boost when tuned an octave lower.\nWavetable. Fans of the groundbreaking PPG Wave 2.x synth from the early ’80s may have noticed that Thor includes 11 of its original wavetables, which are great for re-creating vintage sounds from Depeche Mode and The Fixx (“PPG 2 Bell” is a standout here). Propellerhead’s original wavetables are equally useful for metallic textures that evoke that retrowave sound. Whereas the LFO and step-sequencer are modern approaches for animating the wavetables, purists should opt for the mod envelope with a long decay and release.\nFig. 4. Tucked away on the\nleft side of the interface are Thor’s hard sync buttons, which allow for sync\neffects, even on FM and Wavetable oscillators\nAnother interesting trick is to place a wavetable oscillator in the second or third oscillator slot and then sync it to oscillator 1. This vastly increases its timbral range, since changing the tuning—especially by large amounts, such as octaves—delivers dramatic harmonic shifts.\nPro Tip: The hard-sync option also works on the FM and Phase Modulation types, which is an extremely unusual feature for any synth (see Figure 4). Additionally, when re-tuning the synced oscillator to higher octaves, low frequencies are generally attenuated. Consequently, blending in the analog oscillator’s sine waveform (described above) helps retain the fundamental, keeping the result full-sounding.\nPhase Modulation. This form of synthesis was originally called “Phase Distortion” when Casio introduced its CZ digital synths in the mid-’80s. In use, Thor’s PM oscillator morphs a sine (or sine-like) wave into a complex shape that resembles common analog waveforms (see Figure 5). To get the hang of this oscillator type, start by leaving the second modulation wave off, set the PM amount knob to maximum, then scroll through the eight waveform options on the first modulator to hear their timbres. Next, turn the PM knob back to zero to hear the transition between sine and the selected shape. As with the CZ, selecting one of the five waveforms for the second modulator shifts the timbre further, by alternating it with the first selection.\nFig. 5. Based on the Casio CZ Series, the phase modulation oscillator can be extremely dynamic when modulated via velocity.\nIt may sound complicated, but in practice, configuring the PM oscillator is straightforward. The PM knob behaves similarly to the cutoff of a lowpass filter, but with a different nuance when modulated via Thor’s tools.\nPro Tip: While envelope and LFO modulation options are the obvious choice for animating oscillator textures, for real-time performance, try using velocity on the PM amount (or wavetable, pulse width, etc.). This can be a dramatic way to add dynamics to your patches.\nFig. 6. Even though it’s\nlimited to a single carrier/modulator pair, the FM oscillator is capable of\nevoking many of the DX7’s signature sounds. Shown here is the configuration for\nthe “future house” bass sound that’s currently in fashion.\nFrequency Modulation. If you’re a DX7 fan, this oscillator is a great way to add a slice of its sound to your patches. Although Thor’s FM tuning options are integer-only (no clangorous bell tones here, sadly), its palette is broad enough to generate many familiar DX tonalities quickly. The secret is to use one of Thor’s envelopes as the modulator envelope—a central component of FM synthesis. Once that’s set up, it’s just a matter of selecting a carrier/modulator ratio that captures the sound you’re after (see Figure 6).\nHere’s a handy cheat sheet for the ratios of classic FM sounds (with carrier in the first position):\n1:1 Great for electric bass or plucky synths in the mid-upper range.\n1:2 The essential “future house” bass sound.\n1:3 Basis for the DX7 “Jazz Guitar” patch.\n1:4 Square-wave-like, often used for simulating woodwinds.\n1:5 – 1:10 Bells, mallet percussion, chimes. 1:10 and greater: Glistening tones, with some of the same qualities as the classic DX electric piano.\nMulti Oscillator. Roland introduced the “supersaw” in the JP-8000 synth back in 1996, and it quickly dominated both trance and EDM for two decades. Thor’s Multi Oscillator re-creates that supersaw, while offering additional waveforms and detuning algorithms. For those looking to avoid the obvious EDM applications, this oscillator is wonderful for thickening the sound of other oscillator types. Set it to either the muted saw or square waveform, increase the detuning, then mix it behind your timbral oscillators. It adds a richness that’s subtler than simple chorusing.\nThis oscillator can also create massive THX-style rises for dance music and film soundtracks. While most of the detuning algorithms are dissonant at their extreme settings, “Oct UpDn” and “Fifth Up” transform in a very musical manner.\nFor a dramatic build, try this approach: Assign the modulation wheel to control the detuning amount via one of the previously mentioned algorithms; or better still, use each mode on a separate oscillator. This way, you can precisely control the duration of the rise in real time or with automation.\nFig. 7. Thor’s Noise\nOscillator offers a surprising range of textures and is a fantastic resource\nfor audio-range modulation when applied to the other oscillators’ pitches\nNoise Oscillator. While noise is a standard tool in any synth’s arsenal, Thor’s array of noise generators are the most versatile I’ve ever encountered. The white and colored noise offer the most familiar results, while the “static”, “S/H” and “band” modes can be used for clever sonic tricks in conjunction with the Noise Mod parameter (see Fig. 7). Here’s a cheat sheet for those effects:\nStatic: Noise is generated via an increasing number of random static clicks. Turning the knob from low to high increases the click rate until the sound transforms into white noise. If you want to add “vinyl noise” to a lo-fi sound, try this mode.\nS/H: In the early days of arcade video games, explosion sound effects were created with an audiorate sample-and-hold. Here, the knob functions in a similar manner. To create chip-tune effects, apply an envelope to this parameter and adjust the modulation amount and envelope decay until you hit the target.\nBand: I’m a fan of “sonar pings,” which are normally created by passing white noise through a resonant low-pass filter. Here, one knob handles the heavy lifting for dialing in that tone, offering a wide range of pitched noise textures.\nPro Tip: This is huge, so pay attention. Nearly every audio source in Thor can also be used as a modulation source. That means you can take the output of an oscillator and use it to modulate another parameter. To experiment with this feature, begin with a noise oscillator and route it to the pitch of a sawtooth oscillator, listening to the results as you tinker. For example, with the noise oscillator’s knob set to its lowest values, the static mode adds tiny glitches to the sound, the S/H mode delivers random effects quite unlike the LFO options, and the band mode adds a rapid, chaotic drift to the pitch, like a swarm of bees.\nThor’s filtering options are equally extensive, though the types may be a tad more familiar. Even so, there’s one detail worth mentioning before you begin experimenting: The default settings for all filters include envelope modulation with depth controlled by velocity. This is a great starting point for beginners because it is dynamic and the dedicated knobs are a huge plus. That said, seasoned programmers often find themselves turning it off every time they start a patch. Try creating an “init” preset that’s identical to the default, but with those envelope parameters set to zero.\nPro Tip: All filters include a drive fader, which adds warmth, volume, and distortion when maxed out. Because this can also be modulated, you can create dynamic overdrive by routing a touch of velocity to control this parameter. A little goes a long way, because volume is also affected. Experiment until you find a value that matches your playing style.\nThe filter types are Ladder (an emulation of the Moog lowpass), State Variable (multimode with an Oberheim flair), Comb and Formant. While the first two modes should be familiar to EM readers by now, the second two are worthy of a closer look.\nFig. 8. While many\ndesigners use comb filters for flanging effects, here it’s set up for\nre-creating Karplus-Strong synthesis.\nComb Filter. For many synthesists, comb filtering can be a bit confusing. At first listen, it sounds like an intense flanger, which makes sense, since both are based on a network of one or more delays with extremely short times. Sweeping the frequency knob is a great way to generate flange effects, to be sure (see Figure 8). However, because of its delay-based structure, comb filtering is also a fantastic way to get into Karplus-Strong synthesis, which is how many physical modeling synths create plucked string effects.\nHere’s how to get started with this type of synthesis: Reset Thor to its defaults and turn off envelope modulation to the filter. Select the noise oscillator and reduce its mixer volume (the 1+2 fader) to zero. Then, modulate the volume of 1+2 with a very fast mod envelope with instant attack and short decay. The result should be a short click, also known as an “impulse.” From there, change the filter type to comb and increase the keyboard tracking and resonance to maximum. If you’ve set it up correctly, you’ll immediately hear a pitched string tone like a clavinet. Welcome to Karplus-Strong!', 'carrier and harmonicity in fm synthesis\nHow do the carrier and harmonicity relate mathematically to one another in fm synthesis?\nFor example if I change the frequency of the carrier, then how much should I change the harmonicity?\nalso do check out [simpleFM~], which I believe comes with the default install of max.\nI guess you were looking at this:\nHarmonicity controls the relationship the sidebands have to the carrier. With an integer harmonicity, they are always harmonic. You don’t need to change this ratio when the carrier changes. But, you might want to change if you want to create a different timbre.\nin real life, the harmonicity of a body, string or drum is not only depending on length, thickness and how it is stimulated, but also on the material.\nthe relation between length and thickness of a string is one of the factors which influence the harmonicity factor. this is why a harp sound better than a toy guitar and a concert piano better then an upright piano.\nno idea what you now would do for a frequency modulated sinewave in a synthesizer, or what you would need to do.\ni would go so far saying that if harmonicity is a matter in a digital synth, then it is caused by the intermodulation distortion of the speakers, and not happening in the computer code at all. ;)\nin an FM synth, harmonicity (or better: inharmonicity) is to my knowledge somethign you dont need to remove, but you will typically add some by detuning the modulator a bit against the carrier.\nand this factor should be linear … 500 Hz modulated by 124.8 Hz should fit perfectly to 1000 Hz modulated by 249.6 Hz.\n[Sorry for any duplication of information]\nThe point of the harmonicity ratio and modulation index is that they will provide a constant timbre across the spectrum, so you don’t need to change them at all with the fundamental. (they’re calculated as a ratio to the fundamental, so their frequencies are automatically scaled with the fundamental)\nLoosely speaking, the harmonicity ratio (as mentioned earlier) controls the harmonicity/inharmonicity of the spectra. A harmonicity of 1 will (potentially) have all harmonics. A harmonicity of 2 will have only the odd harmonics. The higher the harmonicity, the broader the spacing of the harmonics. (I’m assuming sine waves for both oscillators) Near integer values will often sound better than pure integer values (1.995 vs 2.0) because of the subtle modulations, beatings, etc. they introduce.\nThe modulation index controls (roughly) the intensity of the partials. The exact volume of each partial (for two operator FM) is non-linear and described by a Bessel function. You can find a chart of that with a quick search. With a mod index of zero, you get the carrier frequency, since there’s no modulation signal present. As you increase this value, you will see more harmonics.\nAll that said: FM is really complex once you start introducing more operators, and it becomes quite hard to predict. Chowning himself said that he doesn’t anticipate that we will come anywhere close to exhausting the timbral possibilities of six operators (e.g. the Yamaha DX7!).\nThis simple patch demonstrates the concise, eloquent and comprehensive explanations already given:\n----------begin_max5_patcher---------- 1976.3oc6asziaaCD97Ff7ef0WZahiAeHpGEEEnH.AsG1aE8RSPfrDsMSkkb oj23MAc+sW9PZW6csjn13Uqrc8ASIJIxY93vgy7Qou9xWbwnoYaX4i.+D3u. WbwWk0bgtNUMWTUwEiVFtIJILWeiihxVtjkVLZb4EKXaJzW3mk+x4wrogow4 fvkqR3EqiY.dNXFeCK9Wz+t84VEVDsfmN+iBVTgQDHT+Ivw.OO0+DDUUfQSf fOT8PyDymptWn7RkUwi08d1zO8FWxssd55kYqKRXEZg91adVVZQN+KLUkH7j cpOMbot9Q+ISDGlFtcawSqZJjtx+8kuPUJKF+zAbQKBSmyxaG2BBPea3F9nD 23ofhEL.aiBxXfLyoB97EEi0GdGTlMSWwxr30IgEYBkQYLOORvJXf2r+a9tg jX1JlprrGlsNMNTIRgIiAWEJtVNZnuPAe4TgrkhDY44FggMmmWvDS.+gpEyR VWvksRYCkvlUnjj2tH6yoxF46ykOP9przbYakouEoMPRBSZFnjJ4HtfEVn5t 2ORdTB3yYhj32OprmyA+vTgD1.SYIIRKozXvJlHZcdtrS+QfDud2kStEGS3o rnr0oZvDgZv9xyyUaRQIpBTfitv0qCVXvSHKL0vlMFX4QgIrXvzq0W5xrXvu mFy1LFrLjKMd3oU1MWwDWCjBgrO1HGjkMfb7JeAekbfr3yLVJHJTH3LAXlf8 OqYoQWqGb2o6uybU16KBEKyR4Q40LZCaXz1UB+z6FsodlBr8i1zfg7ncJ6yR g7gC1uRpxvIM4mEAoZGsXu.UAE9PGsQYIYBycCmPn3.GzX0QDOLkpNhhQT2. Ift8SsM14UC1c6DTScEWuhY5mQyRxBkR3GNbfK9fCtRPjwuhcCXtHb0hl.YW GsAG0W6yAAwODj2Ftb6LbkymmFlbPwqCuwXtbAGK.KockBdbLA.362LVQOFm VZIRD3A6.R3bLhDe2ajKSCabtiqwbvDGsSKf.oySc3oCF+LqWNkItoQu0Hec fJDieZp6CQCoqf5WEiTG9fay0x3G5SVFX.ammcH5jVJ9MhnAt508vNZnz35o VyKhyoom4kEYyrAjPFiulmCRncFjFHviz.W1RMGsjeIR3UWzRaiDdsMaaUnP pSxjo9HKMbZBaG+00DTjblnLl34CDLatLl+arYQLSHPHhA+n0AY9GVH6NuW6 5m+.404uySjonHrP+QF+KDcgJMj8q9AGV0WozOP2WIypVkouJirsssoljhpU NOPXVsYldWhl2lDXyyEMdkHlDXPFmSXn84z4zsL3QSnATju2dmd8qBtxNqmR huJ24r7HdhFyZFoPl4fPcQ.ry.E9XEn9sRRC3EWCDJKda7tiL4oE31YbxYfi SO1njpXIf.MtwvODY9lYIv4Pm16vAdesMgfRLLRR2C1tMJ4MPPoCen5u5F.r YivRhSL4CSbaFn7OgAJafox0Ec8aFlBNYgonqiRX1AUXssjSyS8nvyXnpzIU YHDsgTnidjp1HJd2kfpjJZhmFcdXUY8PC5ZjDzgdDW4qjJqPF+Y1plMbJ491 EYVjyX+ff6fEwYpsOpJ3AEfgp01hf+VnWn+npiVFwjYru6L0g9el5169BfMz O0BScX+ySl5p.ox3zaloNr6oKScdtlkzQVvSG14zmmt1QLhuggNCeUsAYjSe Hi8k3vHaVaiVF9Hs5+8CYn1VR+.4fsUJYq1gU6njECO0njsT+siRVzyHkr54 fviEBYcLvJwE9H4i8nkkwRfR8tOYGirX2xQU5iDpHmGLWS7L65OL3QRb8wJL cYoEUVJfqdA6rHTGL5wQZM8rfa+pnaLw0zcTx8XEkd68eMKazuTIoplDGB76 JJ4cjt+GsQ8rSE6E90w77Na+AzwCRCTKcGfT6+w1GUWD0mt7v9ZaBFk3SsX2 OPvy1c+fXxKC647jYBhPmoaYRI1VF4ZK6XBBeFuM.UHkIv0V1F.D4bFoJSx1 D5ZaHkyY9Fl3X97Cdz6WBx+YIxCi3o+PTt+mbnVIUW3dfVd1ZQT0fX0WGAX6 MCgkKyjTG6+V2k5siX66ZAONlktiFFyyUzOnAEX8iiVKXJZQaWvtu3uGAaIO dUFOsHu5Suhp9XbPAlW1Fnzqa0oa0YGZkgXixP5rxPjoRn9xhH5kkcMlw6dF xubOujwSSKO8oSQU7UYghFzYE0QqnlDxIXs++cNw7NSIUYp93mNM79SEpYBi RCQ85DFpUBlZCL66YxPqlIS6cAS859Zgf4z6Blc1XDxyyPIpUDq+ELpcNX8G n1XpcmomcWzAAqmM9sJhDZ6B1tYjNdq+POoKPXmWE3PU9oPqWfaXh+VEBh1J CMHwezP0UlcoJ32+txnVgX8+53DGqMEGjB1yfMlc4Oz+wJZmb0+qVZWBW3mg 72sZYPDZnJXv9WvrJper+.kJl1y3c+LwfcbTENN2c1yMQLn9Obb6RRFOnErd 1ymUVk39edLBY6zkCR7y2moLCGXDHwv7u+sm8Dx5G1ZWpCxPtQ14U.OTS4wN KNDYvJ+V4iA4LTk+.qMePOAy3qli6Pzqit0Y1owkaiT3pUWwD4kBsQYGsL7S FIxcr4bdp4byVCNRvthW8HlWI0QghnE7BVTwZgYGs135LRdEcGJ+6+DEU.FL -----------end_max5_patcher-----------\nChowning’s original article explains this pretty clearly. MSP Tutorial 11 tries to condense the vital info in simple prose, and demonstrates a basic implementation.\nN.B. In the current MSP tutorial the following paragraph has been deleted, which was a sidebar in earlier versions.\nTechnical detail: In John Chowning’s article ""Synthesis of Complex Audio\nSpectra by Means of Frequency Modulation"" and in Curtis Roads’\nComputer Music Tutorial, they write about the ratio Fc/Fm. However, in\nF.R. Moore’s Elements of Computer Music he defines the term\nharmonicity ratio as Fm/Fc. The idea in all cases is the same, to express the\nrelationship between the carrier and modulator frequencies as a ratio. In\nthis tutorial we use Moore’s definition because that way whenever the\nharmonicity ratio is an integer the result will be a harmonic tone with Fc\nas the fundamental.\nThe problem with the idea of harmonicity ratio (HR) is the implication that harmonic partials are produced only when the ratio is an integer. Look at preset #7 in the MSP Tutorial 11, which has a carrier of 392Hz and a pitch frequency that is the same. If you change the HR to 0.6666 and the carrier Freq to 1176Hz, you will hear a very similar sound. They both have odd # harmonics and are at the same pitch frequency — 392Hz, midi note# 67. In fact there are numerous non-integer ratios that produce harmonic partials.\nThe question of how to best express the ratio of the carrier frequency (Fc), the modulating frequency (Fm) and resulting pitch frequency was carefully considered many years ago and the conclusion was to have a freq (pitch frequency) that can be scaled by independent numbers for Fc and for Fm. So I recommend adding a third flonum between the two and change the labels from ""carrier Freq"" and ""Harmonicity""\nto ""Freq (pitch)"" and ""c"" and ""m"" with the understanding that the carrier frequency will be c * Freq and the modulating frequency will be m * Freq.\nThere is a modification of the 11FM synthesis patch that is downloadable from\n(see attachment or)\nthat shows this change. The original Harmonicity value is labeled.\nAs long as c & m are integers and form an irreducible fraction, the pitch will be the pitch frequency. For example, 6 and 4 will produce a pitch an octave higher and should be 3 and 2. Note that c:m of 3:2, 2:3 3:4, 4:3, 5:1, 5:2, etc. all produce harmonic partials at the same pitch frequency (392Hz) but with differing spectral shapes for a constant modulation index function shape. With a low Modulation Index, e.g. 1, a resonance can be produced by making c=5 and m=1. The spectral envelope has a peak at the 5th harmonic and the harmonics are at intervals of the pitch frequency — adjacent harmonics, but none at the pitch frequency (fundamental)! At low pitch frequencies the sound is double reed-like. With an increase of Index the bandwidth of the spectrum increases and the fundamental will appear. Regenerate the 5:1 example as a long tone (15000ms) having a freq at 80Hz and an Index function that ramps up from 0 to about 10. At first you hear 400Hz and and then the pitch freq of 80Hz. Now set m = 1.01 and you will hear beats as the lowest harmonics spread into the negative freq domain, reflect around 0Hz and add to the harmonics in the positive domain with a small offset.\nWith an eye on the Harmonicity ratio number object, note it is not at all apparent that these timbres are possible.\nYou can download a sound synchronous slide set that shows how the Bessel functions and Index determine partial amplitudes and bandwidth.\ncool patch n00b_meister!\nI love this forum. Don’t ever change.\nThanks, just a hack of simpleFM~.\nMaybe Christopher or John could improve it; Bessel functions frighten me.\nn00bs and Legends. All together.\nA small addendum for cases when the Fc is rather small and the modulation index is relatively high (in other words, when negative frequencies appear in the spectrum). In those cases, even if one would guess (based on the harmonicity ratio) that the result is a harmonic sound, this might not hold due to the mirrored frequencies.\nThe point is that the mirrored freqs will coincide with (some of) the original (positive) freqs only if 2Fc/Fm is integer. In this case, the mirrored freqs would only modify the spectral shape of the sound, but they won’t introduce new partials. If 2Fc/Fm is not an integer, the mirrored freqs will appear as new spectral components, in which case one needs to dig a bit deeper to decide whether the result would be harmonic or not.\nsomehow it is very suspect that brendans patch is centered around a [!- 110], isn´t it?\ntake care not to press the wrong button in this patch, or there will be no longer any harmoni in your -city.\nForums > MaxMSP']"	['<urn:uuid:ad50f4c1-db4c-49da-a58e-3d01b1477893>', '<urn:uuid:8d2a1f3b-ee2a-4292-9f3f-44866ba7ae38>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:35:02.547110	15	90	3867
52	Can exercise and gut health improve mood?	Yes, both exercise and gut health can improve mood. Exercise enhances endorphins and norepinephrine production, directly targeting brain chemicals while improving overall health. Meanwhile, a healthy gut with diverse microbiota is essential for normal brain function and mood regulation, as 95% of serotonin is produced in the gastrointestinal tract. Together, they can help alleviate depression symptoms.	"['Can Diet Cause Depression?\nIs the Brain Connected to the Gut?\nAccording to a World Health Organization report, depression ranks as the number one cause of disability in America and the third highest, following heart disease and strokes, in Europe. No one knows the exact causes of depression. But recent research on diet and mood has revealed that food may be a major factor.\nIs the Brain Connected to the Gut?\nScientific research demonstrates that about 95 percent of “serotonin, a chemical neurotransmitter responsible for your mood, is produced in your gastro-intestinal tract.” The gastro-intestinal tract, commonly referred to as your gut, is also responsible for about 80 percent of your body’s immunity. So it seems that a poor diet will damage the gut, resulting in reduced serotonin levels and increasing your chances of developing anxiety and depression.\nLast September, ABC News reported on the connection between anxiety and the gastro-intestinal tract saying, many “are beginning to recognize the power of healthy gut bacteria. The average adult carries up to five pounds of bacteria—trillions of microbes—in their digestive tract alone.” And if there is a connection between your gut and your brain, as more and more scientists believe there is, then all that “bacteria may play a role in disorders such as anxiety, schizophrenia and autism.” Some patients have even demonstrated a link between strep bacterium and OCD.\nDr. James Greenblatt, a psychiatrist based around Boston, says that testing for chemical byproducts inside the body can reveal microbial imbalances that may be causing psychiatric conditions by building up too much dopamine in the body. Greenblatt says he doesn’t “know why this test isn’t done on every psychiatric patient” because it is a “more common scenario than we know.”\nThe Food behind the Issues\nSome of the foods that may contribute to anxiety and depression include your typical junk foods that are processed and high in artificial ingredients, refined sugars and fats. This type of food is often comforting but delivers a swift crash after ingestion.\nExcess intake of saturated fats and animal fatty acids can cause a fluid shortage in new cell membranes. This affects the brain, which is the richest source of fatty acids in the entire body. A lack of proper membrane fluid can affect moods, behavior and mental function.\nThough it is viewed as a mood improver, alcohol can also contribute to depression. This is because it creates vitamin deficiencies in the body. Caffeine is a stimulant that also gives a short high and can help with energy levels. But reliance on caffeine can easily backfire to result in sleeping problems, even insomnia.\nCarbohydrate rich diets are another example. Carbs can help boost serotonin levels which improves mood. Fascinatingly, many depressed people crave carbs, and this may be a physical response. However, a diet which is substantially comprised of carbohydrates can cause weight gain as a side affect. Often times, weight gain results in a negative body image which only creates a new source of depression.\nCan Better Food Solve the Problem?\nOf course, it is impossible to shift all the blame onto diets alone. In a chicken and egg scenario, depression often leads patients towards a poorer diet which, in turn, worsens brain chemistry. But the cycle can also begin with bad diet habits that result in poor mental function causing depression and perhaps reinforcing unhealthy eating.\nBut just as poor diets can be a major factor, healthy diets are being touted as an important piece of healing from depression. “Diet therapy” is a term which involves pumping the body and the brain with rich nutrients and vitamins in order to fight depression with proper diet and supplementation.\nSome important dietary supplements for mental functionality include:\n•Tryptophan—a natural relaxant to reduce anxiety and depression, it can increase serotonin levels.\n•Omega 3’s—essential to healthy mental functioning. Some studies show that omega 3 fatty acids powerfully impacted schizophrenic patients by working to correct membrane abnormalities.\n•Vitamin B1—needed for energy production and nerve cell health.\n•Vitamin B6—helps properly balance hormones.\n•Vitamin C—boosts immunity and can help cleanse the liver and aid digestion.\n•Magnesium—important to control the hypothalamic activity in the pituitary adrenocortical axis. This improves the stress response system and reduces anxiety.\nSo which types of foods should you eat and which should you avoid? One infographic by Hartmann Direct answers this question. It explains the importance of avoiding processed meat, especially with lots of salt.\nExercise Can Also Alleviate Depression\nJust like diet is an important aspect of general health as well as fighting depression, exercise is another factor that improves all around wellness and can be especially important in dealing with depression. Harvard Health explains that exercise may relieve depression because it “enhances the action of endorphins, chemicals that circulate throughout the body. Endorphins improve natural immunity and reduce the perception of pain. They may also serve to improve mood.” And yet “another theory is that exercise stimulates the neurotransmitter norepinephrine, which may directly improve mood.”\nExercise works by directly targeting brain chemicals to help your mood. But it also helps your total body recover by lowering blood pressure, fending off heart disease and cancer, increasing self-esteem and making your feel happy. When depressed, many people want to sleep and avoid physical activity. But exercise is needed to help the body and the brain.\nBoth diet and exercise can play an important role in relieving depression. Some cases may still require medication but can still be helped by general healthy habits and key foods supplements.\n(depression / shutterstock)', ""10 min read\nThe Gut-Brain Axis: How Your Microbiome and Mental Health are Linked\nThe human body has more bacteria than cells. And collectively, the different species and types of bacteria are referred to as the microbiome or gut microbiota. Those trillions of bacteria living in your gastrointestinal system impact multiple systems, organs, and processes in our body and impact our overall health and well-being. Similar to how different types of medications work differently, bacteria do too, and a balance between the different types, and ‘good’ and ‘bad’ is needed.\nWhat is the Gut-Brain Axis?\nThe gut-brain axis describes how the gut and the brain interact and talk to and influence each other. That is why your gut is sometimes called your ‘second brain’ and why we have 'a gut feeling’ or ‘have butterflies’ in anticipation of an event. The connection between the brain, gut, and microbiome has a significant impact on health and is currently an exciting and up-and-coming area of active research.\nGut-Brain Axis and the Microbiome\nMounting evidence indicates that a healthy gastrointestinal tract that has a diversity of microbiota is essential for normal brain function and well-being.1 Scientists are investigating the communication pathway between the brain and the gut to better understand how neurological and neuropsychiatric disorders such as stress, autism, depression as well as Parkinson’s, and Alzheimer’s disease are connected and possibly caused by an imbalance of the gut microbiota.2 Side effects of a fast-paced lifestyle that includes eating highly processed foods, experiencing stress, taking antibiotics, and being exposed to pesticides can all contribute to an imbalance of the gut microbiota.\nPathways of the Gut-Brain Axis\nThere are several pathways that connect the gut and the brain. Because these two systems are in regular communication treatments and therapies that help one may also help the other.\nThe Vagus Nerve Pathway\nThe Vagus nerve is the primary link between the brain and the gut and has an important job communicating and overseeing many body functions including managing mood, immune response, heart rate, and digestion.3 The intestinal microbiota and the chemicals produced regulate nerve signals. The vagus nerve controls gastrointestinal motility—the contractions that move food through the digestive tract through the process of digestion.\nWith the plethora of bacteria living in the gut, they can impact how the body digests food and create chemicals that aid with hunger and fullness signals that are sent to the brain—so the gut bacteria may influence both weight and appetite.\nThe Endocrine Pathway\nThe endocrine system is made of glands that make and regulate hormones and is another communication highway connecting the brain and the gut.4\nEndocrine system hormones regulate metabolism, growth, reproduction, sleep, and mood. Stress produces cortisol impacting the endocrine system and can lead to disruptions in mood and digestion. Cutting-edge research is seeking to better understand how the microbiome can keep cortisol levels from increasing to reduce anxiety and help alleviate depressive symptoms.\nThe Immune Pathway\nMost of the immune system (70%) resides in the gut and the bacteria work diligently responding to toxins, foreign invaders, and pathogens. Stress can also impact the immune system in a variety of ways that include thinning the lining of the gut potentially allowing ‘bad bacteria’ or toxins through and activating the immune system. These pathogens can also lead to inflammation and gastrointestinal disorders.\nInflammation, Depression, and the Microbiome\nIt is believed that an imbalance of the gut microbiota triggers inflammatory processes and also impacts mood.5 It is understood that mental health issues disorders are complex and those with depressive disorders suffer from multiple conditions simultaneously—stress hormones are imbalanced and inflammation that impacts the brain, immune and digestive systems.\nMicrobiome and Mental Health Takeaway\nAs we learn more about the role of the gut microbiome and how it impacts health—it is increasingly clear that maintaining a balanced and healthy gut microbiota is essential to overall health and well-being. The gut-brain axis impacts multiple systems that are integral in controlling mood, hormones, and digestive health as well as building a strong immune system.\n- Suganya, K., & Koo, B. S. (2020). Gut-Brain Axis: Role of Gut Microbiota on Neurological Disorders and How Probiotics/Prebiotics Beneficially Modulate Microbial and Immune Pathways to Improve Brain Functions. International journal of molecular sciences, 21(20), 7551.\n- Kim, N., Yun, M., Oh, Y. J., & Choi, H. J. (2018). Mind-altering with the gut: Modulation of the gut-brain axis with probiotics. Journal of microbiology (Seoul, Korea), 56(3), 172–182. https://doi.org/10.1007/s12275-018-8032-4\n- Breit, S., Kupferberg, A., Rogler, G., & Hasler, G. (2018). Vagus Nerve as Modulator of the Brain-Gut Axis in Psychiatric and Inflammatory Disorders. Frontiers in psychiatry, 9, 44. https://doi.org/10.3389/fpsyt.2018.00044\n- Neuman, H., Debelius, J. W., Knight, R., & Koren, O. (2015). Microbial endocrinology: the interplay between the microbiota and the endocrine system. FEMS microbiology reviews, 39(4), 509–521. https://doi.org/10.1093/femsre/fuu010\n- Hayley, S., Audet, M. C., & Anisman, H. (2016). Inflammation and the microbiome: implications for depressive disorders. Current opinion in pharmacology, 29, 42–46. https://doi.org/10.1016/j.coph.2016.06.001\n- The gut-brain axis describes how the gut and the brain interact and talk to and influence each other. That is why your gut is sometimes called your ‘second brain’ and why we have 'a gut feeling’ or ‘have butterflies’ in anticipation of an event.\n- Mounting evidence indicates that a healthy gastrointestinal tract that has a diversity of microbiota is essential for normal brain function and well-being.\n10 min read\nHealth Facts on Sugary Soda: The Truth About Artificial Dyes\nMany foods & drinks contain a concoction of artificial dyes. Today, there are 9 approved by the FDA, but are they safe?""]"	['<urn:uuid:97bc0faa-a6bf-4816-88bb-ac2338b9a98d>', '<urn:uuid:90dfa741-eac9-4c07-b707-952bf8023dad>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:35:02.547110	7	56	1847
53	I'm trying to understand what happened - how did China respond when western countries imposed sanctions on them for the Uyghur situation?	China swiftly retaliated by announcing their own sanctions on the same day. They targeted ten European individuals (including EU lawmakers, Dutch, Belgian, and Lithuanian lawmakers, and scholars) and four institutions (including EU committees and research institutes). China's sanctions included travel bans on these individuals and their relatives, and prohibited the sanctioned persons and their associated companies from conducting business with China. The Chinese government claimed these individuals and institutions had 'severely harmed China's sovereignty and interests and maliciously spread lies and disinformation.'	['On March 22, 2021, the EU, UK, US and Canada announced a range of coordinated sanctions to crack down on alleged serious human rights abuses in the Xinjiang Uyghur Autonomous Region (XUAR). The coordinated announcements comprised measures of various types, including asset freezes and travel bans against individuals and entities alleged to be involved in serious human rights violations against Uyghurs and other minority groups in the XUAR. The measures elicited the swift imposition of retaliatory sanctions by China against a group of EU individuals and institutions.\n- Zhu Hailun (Former Secretary of the Political and Legal Affairs Committee of the XUAR, former Deputy Secretary of the Party Committee of the XUAR, and former Deputy Head of the 13th People’s Congress of the XUAR, a regional legislative body);\n- Wang Junzheng (Party Secretary of the Xinjiang Production and Construction Corps (XPCC), Deputy Secretary of the Party Committee of China’s XUAR, Political commissar of the XPCC, former Secretary of the Political and Legal Affairs Committee of the XUAR);\n- Wang Mingshan (Member of the Standing Committee of the Party Committee of the XUAR, Secretary of the Political and Legal Affairs Committee of the XUAR, former Director and Deputy Party Secretary of the Xinjiang Public Security Bureau (XPSB));\n- Chen Mingguo (Director of the XPSB since January 2021 and Vice-Chairman of the XUAR People’s Government); and\n- XPCC Public Security Bureau (in charge of implementing all policies of the XPCC relating to security matters, including the management of detention centres. The XPCC is a state-owned economic and paramilitary organization in China’s XUAR, which exercises administrative authority and controls economic activities in Xinjiang).\nThe precise reasons for the listings are available in the Annex to the EU Global Human Rights Sanctions Regime (Regulation and the Decision). In particular, those listed are considered to be responsible for serious human rights violations, including large-scale surveillance, arbitrary detentions and indoctrination, systematic violations of freedom of religion or belief, degrading treatment as well as forced labor inflicted upon Uyghurs and people from other Muslim ethnic minorities.\nThe listed individuals and entities are subject to an asset freeze in the EU and EU persons and entities are prohibited from providing them with funds and economic resources. In addition, listed individuals are subject to a travel ban to the EU.\nThe new listings represent the first restrictive measures taken by the EU against China since the political arms embargo adopted by the European Council following the Tiananmen Square protests in 1989. The new measures were only possible because they were adopted under the EU Global Human Rights Sanctions Regime, which allows the EU to impose sanctions against persons and entities involved in serious human rights violations and abuses worldwide, regardless of where they occurred. Unlike most other EU sanctions frameworks, it is not directly targeting a specific country. In fact, it would not have been possible to adopt a sanctions framework which would be directly targeting China since the adoption of EU sanctions requires unanimity among all Member States and such consensus could not have been obtained for practical political reasons.\nUK Foreign Secretary, Dominic Raab, announced the designation of four individuals and one entity under The Global Human Rights Sanctions Regulations 2020. The UK notice of designation states that the measures target Chinese government officials and a Xinjiang security body responsible for the administration of China’s so-called “re-education” policy in the XUAR. Each of the designated persons allegedly is responsible for serious violations of human rights against Uyghurs and other minority groups that have taken place in so-called “training centres” in the XUAR.\nThe UK measures impose asset freezes and travel bans aimed at the same individuals and entity as were targeted by the EU. Additionally, the measures prohibit making funds or economic resources available to, or for the benefit of, the designated persons, directly or indirectly. The measures apply to individuals and entities within the UK and non-UK entities in respect of any business done – in whole or in part – within the UK as well as to the activities anywhere in the world of British nationals and legal entities incorporated in the UK (including their foreign branches).\nThe prohibition on dealing with the funds or economic resources of the five designated persons also extends to any legal entities that they own or control, directly or indirectly, even if the particular entity is not itself listed by the UK as a designated person.\nThe measures represent the first time in over 30 years that the UK has sought to sanction China for human rights abuses. In announcing the measures, Dominic Raab, stated that “[a]cting together sends the clearest possible signal that the international community is united in its condemnation of China’s human rights violations in Xinjiang and the need for Beijing to end its discriminatory and oppressive practices in the region.”\nThe US Department of the Treasury announced the imposition of sanctions on Wang Junzheng and Chen Mingguo. Both individuals were designated as Specially Designated Nationals (SDNs) pursuant to Executive Order 13818 (Blocking the Property of Persons Involved in Serious Human Rights Abuse or Corruption), which implements the Global Magnitsky Human Rights Accountability Act. The property and interest in property of SDNs must be blocked (i.e., frozen) when within the United States or within the possession or control of a US person, and US persons are generally prohibited from dealing with SDNs. In announcing the sanctions, the Director of the Office of Foreign Assets Control, Andrea Gacki, stated, “Chinese authorities will continue to face consequences as long as atrocities occur in Xinjiang. Treasury is committed to promoting accountability for the Chinese government’s human rights abuses, including arbitrary detention and torture, against Uyghurs and other ethnic minorities.” The other three Chinese persons targeted by the EU, UK, and Canada already were subject to US sanctions.\nCanadian authorities targeted the same four Chinese persons as the EU and UK pursuant to Canada’s Special Economic Measures (People’s Republic of China) Regulations. Persons in Canada, and Canadians outside of Canada, are prohibited from engaging in any activity related to any property of the listed persons or providing financial or related services to them.\nShortly after the EU imposed its sanctions, China retaliated on the same day by announcing sanctions on ten Europeans and four institutions on the grounds that they “severely harmed China’s sovereignty and interests and maliciously spread lies and disinformation.”\nThe sanctioned individuals are:\n- EU lawmakers Reinhard Butikofer, Michael Gahler, Raphaël Glucksmann, Ilhan Kyuchyuk and Miriam Lexmann;\n- Dutch lawmaker Sjoerd Wiemer Sjoerdsma;\n- Belgian lawmaker Samuel Cogolati;\n- Lithuania lawmaker Dovile Sakaliene;\n- German scholar Adrian Zenz; and\n- Swedish scholar Björn Jerdén.\nThe sanctioned institutions are:\n- the Political and Security Committee of the Council of the European Union;\n- Subcommittee on Human Rights of the European Parliament;\n- the Mercator Institute for China Studies in Germany; and\n- the Alliance of Democracies Foundation in Denmark.\nThe sanctions measures imposed by china include travel bans on “concerned persons and their relatives” and bans on sanctioned persons – or associated companies and institutions – conducting business with China. As certain key terms such as “concerned” and “associated with” are undefined, there appears to be an element of ambiguity with respect to the scope of coverage of the bans.\nThis development represents the broadest example yet of democratic governments uniting to advance the protection of human rights in China through their coordinated use of sanctions. By acting in concert, the EU, UK, US and Canada have demonstrated unity in condemning Beijing’s repression of Uyghur Muslims and other ethnic minorities in the XUAR.\nTaken in isolation, the measures announced by each country may have carried limited weight – aimed as they were at a limited number of individuals and entities. Their impact is significantly amplified, however, by the collective nature of the actions taken.\nIn response to this development, companies are encouraged to perform a health check of their operations in, or relating to, China to ensure that they are in compliance with the newly announced sanctions measures. It is particularly important for companies to map the ownership and control structures of their business partners through the performance of robust due diligence, as the EU, UK and US sanctions regimes can extend to entities owned or controlled by listed persons.\nThese developments have opened the way into unchartered territory, which we will be following closely.']	['<urn:uuid:85637968-df3d-4a7b-a637-2bb4faa4b036>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	22	82	1397
54	need understand how blockchain helps reduce costs banks cultural heritage projects explain main advantages	Blockchain offers several cost-saving advantages in both banking and cultural heritage projects. For banks, it eliminates intermediary costs in payment processing, reduces operational expenses through automation, minimizes human errors in transaction processing, and streamlines trade finance by centralizing document storage. The technology also cuts costs in KYC compliance by providing a secure shared database for customer information. For cultural heritage preservation, blockchain provides a cost-effective way to store and protect cultural information without requiring expensive centralized databases or websites that are vulnerable to attacks. The technology's decentralized nature means no single entity bears the full cost of maintaining the system, while its cryptographic security ensures data integrity without expensive traditional security measures.	"['The number on the lower edge is the public bitcoin address, which is connected to an unique, publicly accessible, but immutable entry in the Bitcoin blockchain. Design: Azra Aksamija\nThe following text is paraphrased from a recent medium article on the subject:\nThe cryptographic heritage project, led by Dietmar Offenhuber, explores best practices how to use Bitcoin and blockchain technology to store evidence of cultural heritage under threat in ethnic and nationalistic conflicts.\nTo give an example — in nationalistic conflicts, cultural heritage such as libraries, religious buildings, or historic monuments are the first things targeted for destruction. This is because they bear testimony of a multicultural past, as Aksamija has demonstrated in her analysis of the Balkan wars of the 90s. These efforts included denial that a particular community has been living in an area before ethnic cleansing — to the extent that buildings are erased not only from the city, but also from old postcards and archival material. Needless to say, this also has implications for ownership of land and houses of the displaced.\nUNESCO and other entities tried to address this by capturing immaterial cultural heritage through databases and websites. The problem is that these websites are just as vulnerable as the practices and buildings they are supposed to document.\nHere is where Bitcoin comes into the picture. All Bitcoin transactions are documented in a decentralized database called the blockchain, which exits in thousands of copies across the world. The blockchain can be publicly read, but its contents cannot not deleted or manipulated – as ensured by strong cryptography. The blockchain can be appended by everyone through making a transaction.\nEvery transaction can hold a small amount of additional information, 80 bytes to be precise. Together with the cryptographic proof that the transaction has been authorized by a specific address at a specific time, this additional message constitutes evidence.\nThis can be taken advantage of in many ways: Residents could publicly declare “this is my house” and provide evidence that nobody can hide or delete after displacement and ethnic cleansing. They could also use the blockchain to prove that a particular document (e.g. a property title) existed in a specific form at a specific time, and to produce evidence whether this document has been manipulated in the meantime.\nFurthermore, the author of the transaction can also prove his identity, by cryptographically signing a messages associated with the transaction. In this regard, Bitcoin offers accessible and versatile cryptographic tools beyond financial transactions.\nThe process of encoding cultural information into monetary transactions may seem foreign, but has a clear equivalent in the world of physical money, for example when a contract is made official by the symbolic transaction of one dollar.\nThe idea of cryptographic heritage is at the moment tested in Azra Aksamija’s Memory Matrix project, which recreates destroyed heritage on MIT campus in a participatory project. Individual plexiglass jewelry, arranged as pixels to form the destroyed arch of Palmyra, are inscribed with the cultural memory of individual participants, and are also encoded with a message in the blockchain that only the original author has control over.The owner of a pixel can prove ownership of the jewelry through the associated private key, can use the key to sign and authenticate messages. Also the public can use the public key to encrypt messages that only the owner can decode. Victims of ethnic cleansing often keep the keys to the front door of their former home from which they were expelled as a memento. See the recent documentary project by photographer Bradley Secker. With cryptographic heritage, the key is no longer a symbolic item, it can store value, prove ownership, transmit messages, and leave a mark and testimony in the world.\nAzra Aksamija’s Memory Matrix project installed at the MIT Medialab building.', ""By-Dr Naliniprava Tripathy, Sushant Sant & Maharshi Chhaya\nMost of the financial services sector have made substantial investments in range of services and applications due to the glitches of network downtime and security breaches. The Banking Sector has always been the first mover and has adopted new technology to shift from conventional banking practices to convenient banking services. One such technology that has grabbed the attention from all the corners is Blockchain technology. Blockchain, primarily a distributed ledger technology (DLT), has emerged as one of the most groundbreaking application and has a tremendous potential to metamorphose the workings of financial sector in recent years.\nBlockchain technology ledgers have innovative benefits over traditional approaches which facilitate bilateral settlement by eliminating intermediaries failures, delays, collateral costs, minimize credit risks, faster implementation of transactions, enhanced transparency in operations, amongst the others. A blockchain in simple terms is a series of connected blocks containing a record of data managed by a bunch of computers not owned by any single entity secured by using a cryptograpy principle. Blockchain technology includes three main properties, such as Decentralization, Transparency, and Immutability. In a decentralized system, the data and information are stowed by multiple entities and each one in the networks owns the data.\nWatch: How to file ITR-1 in less than 15 minutes\nIf any new transaction is undertaken, the same will automatically be stored in the block and added to the chain. Each block is endorsed by an individual entity securing using electronic cryptography to safeguard the reliability of the database. The level of transparency is high. A person’s identity is protected by cryptography represented by their public address. Once the block has been added to the entire chain, any further changes in the block cannot be made unless it is approved by all the members and therefore, tampering with the existing data in this technology is almost impossible. Immutable records are visible to all participants, which improves data accuracy, security, and help to reduce the risk of fraud. One of the significant advantages of blockchain technology is that it is safe, secure, decentralized, transparent as well as relatively cheaper.\nImpact of Blockchain on Banking and Insurance sector\nBanking space has been facing many issues in terms of recoverability of loans granted by the banks. Usage of Blockchain technology can be a breakthrough where all the transactions right from the disbursal till its end use will be recorded in a block. Generating the blocks of every transaction will help the banks in tracing the diversion of the loaned funds, if any.\nThe domestic payments usually take minutes to hours, but several days are required to complete the transaction for cross-border payments. Further, inadequate infrastructure creates security concerns while making the international transfer and therefore, these payments are open to cyber-attacks that can interrupt transmission. Therefore, blockchain technology facilitates payment systems, decreasing the operational costs, human blunder, and falsification. It also facilitates banks to get rid of all intermediaries in the payment processing system to lower the costs to process payments between banks and clients.\nBlockchain technology can be of great use in administering trade finance transactions for the banks. Under the blockchain technology, all the documents such as LCs, Bill of lading/ Shipping bill, Tax invoices can be recorded in a centralized repository where all the parties involved can access the real-time data. This will improve the efficiency tremendously and will allow the parties required to track the transactions with ease. Using smart contracts to automate workflows and clearing calculations reducing processing time and benefit the banks by reducing errors resulting from human mistakes. Barclays and an Israel-based start-up company have successfully executed a trade transaction using Blockchain, which reduced the processing time from 7-10 days to less than 4 hours. The Bank of America, Merrill Lynch, HSBC and the Infocomm Development Authority of Singapore has applied blockchain in processing trade transaction using a paper-less letter of credit.\nAll large amount of record keeping transaction and operations can be recorded using blockchain distributed ledgers that are unalterable and impede fraud. Further, the decentralized nature of transactions reassures the banks better security over the records.\nSmart Contract is a self-executing contract with the terms and conditions between the parties to the contracts are codes on a platform. Therefore, Smart Contracts extend the blockchain’s usage from simply record keeping to automatically implementing terms of multi-party agreements. With a shared database running a blockchain, the Smart Contracts are executed based on matching of terms and are validated by other parties instantaneously and without the need of an intermediary.\nIt becomes essential for the banks and other financial institutions to build a database containing all the information of the customers including their identity proof like PAN card, passport, Aadhar card, driving license, etc. mainly to avoid the money laundering, other forms of frauds and complying with the regulatory KYC norms. Blockchain technology can help the banks to overcome the problem of establishing identity by offering cryptographic protection that ensuring the involvement of all parties to the transactions.\nThe insurance sector also faces many difficulties, as there are inefficiencies involved, frauds in claim settlement and other issues, which eventually push the costs up. Application of blockchain technology can be of great help in this industry as all the relevant information will be available to the parties concerned, i.e., the claimant, insurance companies. Blockchain technology can help the insurance companies (including re-insurers) and regulators to access the required data and it will, in turn, help them in settling the insurance claims at a much faster rate. In case of property and casualty insurance, it becomes essential to have legal documentation of the property damaged. Owing to the proposed integration of the records, companies can now trace the history of that property with ease and can detect whether the claims made by the claimant are legitimate or not. This will bring in more efficiency, and eventually, companies will be able to reduce the operating costs significantly and can earn higher margins.\nThe Way Ahead\nBlockchain reduces delays, conflicts, and confusion in many aspects of financial services. Further, blockchain represents a new form of trust. Already, the Australian Securities Exchange using a blockchain-based system to handle its post-trade clearing and settlements systems. The banks like JP Morgan Chase, Bank of America, and Goldman Sachs actively involved in blockchain technology for their operational activity. Blockchain has potential to transmute the financial services sector by plummeting potential costs and labor savings.\nNearly about 24% of financial executives from all around the world are conversant with blockchain technology accordingly to PwC report. About 77% of Fintech is expected to adopt blockchain in their production system by 2020, according to Global Fintech Report 2017. Concisely, blockchain is a boon for financial services sector as it seeks to bridge the existing gap that prevails between the regulators, financial institutions and the consumers, while it also comes with a caveat of an initial upfront investment that will be required in order to bring all these agencies on a common platform and operates in tandem.\n(This article has been written by Dr Naliniprava Tripathy, Professor (Finance) IIM Shillong and Sushant Sant & Maharshi Chhaya, PGP Final Year, IIM Shillong. The views expressed are authors' own)""]"	['<urn:uuid:592a89a1-d09b-41ed-b2d6-d2c40dd2b119>', '<urn:uuid:35c9061c-6a31-4e30-ba7d-99b778ec5816>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-12T20:35:02.547110	14	112	1836
55	what harmful substances can someone find in everyday items at home that are dangerous to breathe	Common household products that can be dangerous when inhaled include aerosol sprays (like deodorant, hair products, fabric protection, cooking oil, and paint), gases (such as nitrous oxide found in whipped cream dispensers, butane from lighters, and propane), volatile solvents (including paint thinners, gasoline, lighter fluid, dry-cleaning fluids, marker fluid, glue, correction fluids, and keyboard cleaners), and nitrites (often marketed as nail polish removers, video head cleaners, room odorizers, and leather cleaners).	['Inhalant Abuse: Short- and Long-Term Effects of Inhalants\nOften found in common household products, inhalants are volatile, vapor-emitting or aerosolized substances that are inhaled to get high.1,2 When used for their intended purposes such as cleaning, the products are relatively harmless, but they can be dangerous and sometimes deadly when inhaled.2\nIncluding aerosol sprays, solvents, gases, nitrites, and more, inhalants can generate a host of short- and long-term effects.3 Additionally, long-term misuse of some inhalants can lead to a host of issues such as liver injury, hearing loss, asphyxia-related brain damage, and bone marrow disease.2 Plus, in some instances, first-time use of inhalants can be fatal.4\nTake a deep dive into inhalants and their various short- and long-term effects. Plus, learn more about inhalant addiction, dependence, withdrawal, and treatment.\nWhat are Inhalants?\nInhalants comprise a category of several different substances that can produce mind-altering effects when inhaled.5 Use of inhalants is often referred to as chroming, bagging, and huffing. And while inhalant effects last only a few minutes to an hour, many people use these substances repeatedly over time, often over the course of more than a year.6\nInhalants are typically organized into four main categories:3\n- Aerosols are sprays that are typically found in pressurized cans. Examples include spray forms of deodorant, hair products, fabric protection, cooking oil, and paint.\n- Gases are used for medical anesthetic products as well as in commercial products. Nitrous oxide (aka laughing gas), the most commonly misused gas, can be found in whipped cream dispensers. Other inhaled gases come from butane lighters, propane tanks, and anesthetics such as chloroform and ether.\n- Volatile solvents are liquids that become gases at room temperature. They can be found in industrial and household products (such as paint thinners, gasoline, lighter fluid, and dry-cleaning fluids) and in art and office supplies (such as felt-tip marker fluid, glue, correction fluids, and keyboard cleaners).\n- Nitrites are unique among inhalants in that they also act to dilate blood vessels and relax muscles. Nitrites that are misused, which are often called “poppers,” are frequently used for sexual enhancement rather than mood alteration. They are often sold online or in adult novelty stores and are marketed as nail polish removers, video head cleaners, room odorizers, leather cleaners, and liquid aromas.\nHundreds of common products can be used as inhalants, which means inhalants are legal, relatively inexpensive, and easy to obtain.4 As such, children who often lack the means to secure less available or more expensive drugs typically top the list of inhalant users.7\nImmediate and Short-Term Effects of Inhalants\nInhalants enter the bloodstream quickly through the lungs and reach the central nervous system (CNS) and other organs.1 Most inhalants generate initial rapid short-term effects that are similar to alcohol intoxication, which features feelings of excitement followed by drowsiness, lightheadedness, decreased inhibitions, and agitation.3\nThe immediate side effects of inhalants usually only last several minutes, leading some people to continue using over several hours to maintain a high. However, successive inhalations can lead to loss of consciousness and even death.3 In fact, a single initial use of inhalants can be fatal.4\nCommon adverse short-term effects of inhalant use include:9\n- Poor muscle control (i.e., ataxia).\nLong-Term Effects of Inhalants\nWhile some inhalant-related effects may be reversible once you stop using, other effects from prolonged use can be permanent.1\nPotentially severe long-term side effects include:2,7\n- Cardiac toxicity (e.g., irreversible heart inflammation, congestive heart failure).\n- Respiratory damage (e.g., Goodpasture’s syndrome, emphysema).\n- Liver and kidney damage.\n- Bone marrow suppression/damage.\n- Progressive neurological injury resulting in hearing loss, limb spasms, and loss of coordination.\n- Delayed behavioral development.\n- Anoxic brain damage (secondary to asphyxia or respiratory arrest).\nInhalant use can also lead to death via the following methods:3\n- Suffocation (lack of oxygen due to inhaling fumes within a plastic bag placed over the head).\n- Asphyxiation (high concentrations of inhaled fumes, which displace available oxygen in the lungs).\n- Choking (in a setting of decreased consciousness and an unprotected/relaxed airway, potentially resulting in inhalation of vomit after inhalant use).\n- Fatal injury (accidents suffered while intoxicated).\nInhalant Addiction and Dependence\nAccording to data from the National Institute on Drug Abuse, addiction is a compulsive drug use despite negative consequences. It’s characterized by a failure to meet work, family, and social obligations and an inability to stop using the drug. Addiction may entail not only physiological changes (such as tolerance and dependence) but several harmful behavioral changes adversely impacting every aspect of an individual’s life. Addiction development is accompanied by functional changes within the brain that can impact an individual’s drive, motivation, thought processes, and behaviors so much that drug use becomes prioritized over all else.10\nDependence is the physiological adaptation of the body to a substance, wherein the body becomes so used to the drug being present in the system that when the individual cuts back on their use or quits, withdrawal symptoms emerge. With significant levels of physiological dependence, a person may continue to compulsively drink or use drugs to avoid unwanted withdrawal symptoms.10\nWhile relatively uncommon, use of inhalants can lead to addiction dependence, and withdrawal.2,11\nTreating Inhalant Addiction and Withdrawal\nAlthough there is no specific withdrawal syndrome to cover the entire class of inhalants, withdrawal symptoms may occur. Sometimes resembling sedative withdrawal, such symptoms may appear after using inhalants for as few as 3 months, and they usually last for roughly 2 to 5 days.11\nWhile there are no medications or reversal agents available to specifically treat inhalant symptoms associated with withdrawal, supportive care (including lack of access to inhalants, proper sleep, and a healthy diet) is often sufficient for recovery.7,11\nSo how do you treat inhalant addiction?\nTreatment for an inhalant use disorder can occur in a variety of settings including outpatient care (where patients attend counseling but live in their own residences) and inpatient treatment (which provides 24/7 care and supervision in a hospital or residential setting).10,11,12\nRegardless of the type of program, strict abstinence from inhalant use is vital for recovery.7 Thus, removing access to any substances that can be misused from the home and personal space of a recovering person can assist in abstinence.\nAdditional assistance can be found in continued care or aftercare programs, which follow initial inpatient or outpatient treatment programs.10 Aftercare can be found in sober living homes, community-based programs such as Smart Recovery, and through a treatment program.\nIf you or a loved one is struggling with inhalant use, American Addiction Centers has several treatment facilities across the United States that can help. The road to recovery starts with a phone call. Contact our admissions navigators 24/7 to take your first step.']	['<urn:uuid:122db88d-6c90-4284-a093-4068287523d8>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	16	71	1113
56	As a biotechnology researcher interested in hemophilia treatment approaches, I'd like to know what are the key differences between soybean-based factor IX production and the gene therapy treatment being tested for hemophilia B?	Soybean-based factor IX production and gene therapy represent two different approaches to treating hemophilia B. The soybean method involves developing transgenic soybeans that produce clotting factor IX in their cells, which must then be extracted and purified. This approach is still in development, with researchers working on improving purification systems. In contrast, the gene therapy approach (FLT180a) involves a single injection that delivers a functional copy of the factor IX gene directly to patients using an adenovirus-associated virus. This gene therapy has shown success in initial trials, with 9 out of 10 participants maintaining increased factor IX levels ranging from 23% to 260% of normal for about two years, eliminating their need for regular factor IX injections.	['AbiuroThe millennia-old practice of using plants to treat disease is blazing new trails in the field of biotechnology. Dozens of experiments by companies and academic institutions all over the world are using techniques to insert genes in the genomes of plants, enabling them to start producing enzymes that have pharmacological value. This will enable transgenic versions of soybean, corn and potato crops, or even ornamental plants, to be used in the future for large-scale production of medical drugs. One example of these experiments in Brazil is currently underway at the Genetic Resources department of the Brazilian Agricultural Research Corporation (Embrapa) in Brasília, Brazil’s federal capital. Researchers there are developing a variety of soybean that produces a viricide or microbicide that can prevent contamination by the AIDS virus. With the help of genetic engineering, these leguminous plants are producing seeds that contain cyanovirin-N, an enzyme whose effectiveness against HIV has already been proven in laboratory tests during pre-clinical trials.\nThis type of experiment gained momentum in May 2012 when the United States Food and Drug Administration (FDA) issued its first-ever approval for a pharmaceutical drug produced by genetically engineered plant cells, allowing its commercial use to treat human patients. The active principle of the drug in question is taliglucerase alpha, a protein produced by transgenic carrot cells for the treatment of Gaucher’s disease, a rare genetic condition caused by a lack of glucocerebrosidase, an enzyme our bodies need to process glucocerebrosides, a type of cellular fat. Gaucher’s patients are anemic and have an enlarged spleen and liver. The medication was developed and produced by Protalix, an Israeli company, and is distributed in partnership with US-based Pfizer. It was also approved for sale in Israel and, in March 2013, by the National Health Monitoring Agency (Anvisa) in Brazil, where it will be sold under the name Uplyso. Up until now, Gaucher’s Disease was treated with a different drug, whose active protein is produced by hamster cells modified through a biotechnological process that is more vulnerable to contamination.\nThe protein synthesized from carrots is similar to the one produced by the human body. For cyanovirin, it’s a different story. The enzyme was isolated in the 1990s in the United States from a cyanobacterium that goes by the scientific name Nostoc ellipsosporum, in research studies by the National Cancer Institute and the National Health Institutes. Cyanobacteria are the blue bacteria commonly and mistakenly called “blue-green algae.” Researchers at NIH and the University of London in England have designed a cyanovirin gel for people to apply before having sexual intercourse. The active principle inhibits replication of the HIV virus by bonding with oligosaccharides (sugars) on the virus’s surface. “Cyanovirin-N is at the pre-clinical development stage, so it hasn’t been tested on humans yet,” says researcher Barry O’Keefe, deputy chief of molecular biology at the NCI’s Molecular Targets Laboratory. O’Keefe spearheaded a study published in 2003, which showed that the protein was also active against some strains of the influenza virus (influenza A and B). He is now taking part in studies to develop cyanovirin. “We still lack a commercially viable, low-cost way to achieve large-scale production of cyanovirin-N, and plants are a good means to that end,” says O’Keefe.\nObtaining large quantities of the protein was the American researchers’ first stumbling block when they had finished the laboratory tests that indicated its effect on some types of virus. The NIH attempted a production method using recombinant DNA, in which the gene that encodes the protein is inserted in the genome of Escherichia coli (which is more easily grown in the laboratory) for subsequent extraction of the desired substance. But productivity was low and it proved to be economically unviable. The solution found by the NIH team, led by O’Keefe, was to seek out Professor Elíbio Rech at Embrapa, the coordinator of the Brazilian group that had filed an international patent for a gene insertion technique for soybeans, and who had experience in developing transgenic crops. “The Americans contacted us in 2007 and we worked out the partnership. They gave us the genetic sequence that encodes the gene, and we inserted it in the genome of Embrapa’s soybean variety 10-16. And it worked. We already have the seeds from the plants we engineered, and they are producing cyanovirin,” says Rech. The researchers isolated the active principle from the soybeans. The viral tests to confirm the effects of the cyanovirin produced by Embrapa were conducted by Professor Amilcar Tanuri at the Federal University of Rio de Janeiro (UFRJ) and by O’Keefe’s laboratory in the US. The result was positive.\nThe latest challenge has been to improve the protein extraction process so as to purify larger quantities of cyanovirin from the soybean seeds. “Our results indicated the presence of 10 grams of the protein per kilogram of fresh seeds. We know that we can’t extract 100% of the drug from the beans because the purification process normally involves a certain level of waste. Up until now, we are up to 20%, or 2g, and our goal is to reach 50%,” says Rech. Purifying the protein is a painstaking, multi-step process. At Embrapa, the researchers are using a resin-based purification method. As the soybean oil undergoes a process similar to filtering, in which the resins serve as filters, the proteins in the soybeans dissolve — including cyanovirin.\n“Our intent is to produce a sufficient amount of the protein to test the active principle on macaques in the United States, and later on human beings,” Rech explains. The ultimate goal of the work developed by the NIH, the University of London and the Council for Scientific and Industrial Research (CSIR Biosciences) in South Africa, the groups that are taking part in the research, is to take the gel to Africa where AIDS transmission is still high. Cyanovirin production through tobacco plants is also being tested at the University of London and in the United States. “In tobacco plants, the drug is not just found in the seeds, but is expressed throughout the entire plant. In Africa, under CSIR researcher Rachel Chikwamba, the experiments are also following this same path of cyanovirin production through soybeans and tobacco, but without obtaining success thus far,” says Rech.\nAn additional accomplishment by Embrapa in Brasília was the development of a transgenic line of soybeans whose cells produce clotting factor IX, a component of human blood whose absence is one of the causes of hemophilia, a genetic disease that compromises healing and blood clotting, making it difficult to stop hemorrhages. This clotting factor is currently produced either from the blood donated in hospitals or through mouse cell cultures, by inserting the gene that encodes the factor IX protein into the rodent’s genome. “There is also a bottleneck in the development of more efficient and productive purification systems,” says Rech. “We finished this soybean with factor IX in 2012 after five years, we tested the molecule found in the seeds, and now we are transferring the material to the Hemotherapy Center of Ribeirão Preto [of the University of São Paulo (USP)], a partner in the project, to proceed with the molecule purification stage.”\n“We received 360 g of freeze-dried transgenic soybeans, and the tests that confirm the presence of factor IX have already been completed. I’ve taken a position as a professor at the Genetics Department of USP’s School of Medicine in Ribeirão Preto, so these studies are now being coordinated by Professors Dimas Tadeu Covas and Lewis Joel Greene at the Hemotherapy Center of Ribeirão Preto,” says biologist Aparecida Maria Fontes, former researcher at the Hemotherapy Center and partner in the soybean research study. “Producing factor IX by using plants is very important because not only does it save blood bank material, which is scarce, but it also creates an alternative using a different production vehicle. To date, the only factor IX molecules made using biotechnology have been produced in hamster cells,” says Fontes.\nIn all the research studies, and even for future soybean crops genetically modified to produce medicines, a wide range of biosafety initiatives are taken into account. “The plants are grown in contained environments, in greenhouses fully covered by screens. This is done to prevent situations that are actually very unlikely, such as a bird picking up a seed and taking it somewhere else, where it can grow and someone could eat the seeds. It is not poison, but we should treat these plants as a source of medicine, unlike the soybean plants used in food products. Future plantations should also be fenced in, so that no unauthorized personnel will have access,” says Rech.\nThe advantages of producing drugs in plants include the lower costs introduced by large-scale production, and higher levels of safety when compared with human, fungal, bacterial or animal cells. “It is also easier to manipulate agricultural products. The advantage of soybeans or any other plants is that we can harvest and store them,” says Rech. In an article published in the News in Focus section of the journal Nature in 2012 (May 10), commenting on approval for commercial use of the Gaucher’s disease drug produced from carrots, author Amy Maxmen says that Elelyso — or Uplyso —, the medication approved by the FDA, can be sold for 75% of the price of Cerezyme, the traditionally used drug produced from hamster cells. The traditional treatment can cost up to US$300,000 a year per patient. Maxmen reports that the global market for drugs from biotechnological products achieved the mark of US$149 billion in 2010. “The future of plant-based production methods is very promising for biopharmaceuticals. This is a very exciting time for those who do this type of research,” O’Keefe told Pesquisa FAPESP. “Elíbio Rech and his colleagues at Embrapa are part of a growing industry that very important for the future.”\nO’KEEFE, B.R. et al. Potent Anti-Influenza Activity of Cyanovirin-N and Interactions with Viral Hemagglutinin. Antimicrobial Agents and Chemotherapy. v. 47, n. 8, p. 2.518-25. Aug. 2003.\nRECH, E.L. et al. High-efficiency transformation by biolistics of soybean, common bean and cotton transgenic plants. Nature Protocols. v.3, n. 3, p. 410-18. Feb. 2008.', 'Haemophilia B: Gene therapy trial shows early success in people with blood-clotting disorderJuly 21, 2022\nIn a small study of 10 people with haemophilia B, nine were still forgoing the condition’s standard treatment about two years after receiving a single gene therapy injection\n21 July 2022\nA form of gene therapy has shown initial success in treating a rare form of haemophilia, but whether the treatment will be effective in the long term is unknown.\nHaemophilia B is a rare genetic condition that prevents blood from clotting effectively, sometimes causing life-threatening bleeds. It occurs due to a genetic mutation in the X chromosome that leaves people unable to produce sufficient levels of the protein factor IX, which is crucial for blood clotting.\nIt is currently treated by regular injections, usually weekly, of laboratory-produced factor IX. Despite this treatment, some people continue to have debilitating joint damage.\nTo test the potential of gene therapy to correct this genetic fault, researchers at University College London, the Royal Free Hospital in London and the biotechnology company Freeline Therapeutics infused 10 adult men experiencing haemophilia B with a single dose of a treatment called FLT180a, administered via an adenovirus-associated virus.\nAny form of haemophilia is far more common in males than females as the latter carry two X chromosomes and therefore only experience the condition if both their chromosomes are affected or if one chromosome is missing or non-functioning.\nTwenty-six weeks after the gene therapy was administered, all the participants had increased factor IX levels, which varied according to the gene therapy dose the individual received.\nAbout two years later, this response was sustained in nine of the 10 participants. At the start of the study, all the men had factor IX levels of 2 per cent or less of what is considered normal. At their last reported check-up, this ranged from 23 per cent to 260 per cent in the nine participants, depending on their gene therapy dose.\nMoreover, the nine participants no longer needed to undergo their previous treatment regimen of factor IX injections.\n“Removing the need for haemophilia patients to regularly inject themselves with the missing protein is an important step in improving their quality of life,” Pratima Chowdary at University College London said in a statement.\nThis form of gene therapy uses a packaging from the proteins found in the outer coat of the adenovirus-associated virus to deliver a functional copy of a gene directly to an individual’s tissues, compensating for the gene that isn’t working properly, according to the researchers. Newly synthesised proteins are then released into the blood.\nThe participants will be followed for 15 years in total. “The long-term follow-up study will monitor the patients for durability of expression and surveillance for late effects,” said Chowdary.\nBased on the evidence so far into this type of gene therapy, John McVey at the University of Surrey in the UK thinks the participants’ factor IX levels will drop in the long term. In addition, people cannot receive this specific gene therapy with the same type of viral vector more than once.\n“The body might recognise it [the viral vector] as foreign and increase antibody levels, which immediately bind to the virus and neutralise it,” says McVey.\nThe therapy also requires participants to regularly take immunosuppressants to ensure their body doesn’t reject FLT180a. Long-term use of immunosuppressants has been linked with an increased infection risk. McVey says it is unclear how long the participants will need to be on immunosuppressants.\nOf the reported adverse events in the study, about 10 per cent were related to the gene therapy and 24 per cent to immunosuppression. A serious blood clot occurred in one of the participants who received a high dose.\nOverall, McVey says the findings are encouraging: “There’s a lot of work to be done but it’s very promising.”\n“There’s a big difference between one injection that cures you for a particular length of time compared to replacement therapy – which is what we use now – that requires injections at least once a week for the whole of their lives,” he says.\nJournal reference: New England Journal of Medicine, DOI: 10.1056/NEJMoa2119913\nMore on these topics:']	['<urn:uuid:686f021b-0622-42e8-89ab-48d24b1b54b6>', '<urn:uuid:73c852e2-a2b0-4ace-8f79-20cd3f865049>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	33	117	2378
57	aid programs american indians south dakota	In South Dakota, there are several programs providing aid to American Indians, particularly on the Cheyenne River Reservation. The Cheyenne River Youth Project offers youth programming and family services. The Community Food Banks of South Dakota in Rapid City provides food to thousands in need. Additionally, the Eagle Butte Food Pantry on the Cheyenne River Indian Reservation provides wholesome food to Indian children and their families.	['Strategic and Community Partners\nAmerican Indian Youth Running Strong, Nationwide\nA sister organization of Christian Relief Services, working to alleviate poverty and increase the standard of living for American Indians across the country.\nBilly Hooton Memorial United Methodist Church, Oklahoma\nChristian Relief Services has long partnered with the American Indian congregation Billy Hooton Memorial United Methodist Church in Oklahoma City to feed the hungry.Chadron Native American Center, Nebraska\nLocated in rural Chadron, Nebraska the Chadron Native American Center works to alleviate hunger among Native Americans in the remote area of Chadron.\nCheyenne River Youth Project, South Dakota\nThis important program, located on the Cheyenne River Reservation provides innovative youth programming and family services, also serves as a grassroots initiative tailored to meet the needs of its community.\nCommunity Food Banks of South Dakota\nIn Rapid City, the Community Food Banks of South Dakota provides food to thousands of people who need it most.\nEagle Butte Food Pantry, South Dakota\nChristian Relief Services’ Eagle Butte Food Pantry on the Cheyenne River Indian Reservation provides wholesome food to Indian children and their families on the reservation.\nMany Waters Ministry, New Mexico\nA ministry for the American Indians and is located on the border of the Navajo Indian Reservation in the San Juan River Valley in New Mexico. Christian Relief Services works with Many Waters to help provide food to American Indians is this area.\nNorman First American United Methodist, Oklahoma\nChristian Relief Services contributes to Norman’s monthly food program.\nSlim Buttes Agricultural Development Program, Nebraska\nChristian Relief Services supports Slim Buttes Agriculture Development gardening program.\nParshall Resource Center, North Dakota\nA drug and alcohol halfway house open to the public. The center provides residential short-term treatment and residential long-term care and has special programs for persons with co-occurring mental and substance abuse disorders, pregnant and postpartum women, men, and criminal justice groups.\nAPPALACHIAN AND URBAN AMERICAN MINISTRIES\nAmericans Helping Americans®, Nationwide\nAmericans Helping Americans® works to alleviate extreme poverty in Appalachia through fostering community development and self-help programs, as well as providing grants for emergency heat, food, school supplies, and other essential items. In more urban areas, Americans Helping Americans® works with low income housing communities to reduce resident isolation, build neighbor relations, address the needs of residents, provide positive self-improvement opportunities and promote resident pride in their community.\nAmericans Helping Americans in Appalachia, West Virginia\nA project of Americans Helping Americans®, Christian Relief Services provides monthly shipments of holiday supplies, hygiene items, food, coats, clothing, and school supplies.\nAppalachian Region Mission, Kentucky\nLocated in Harlan County, Appalachian Region Mission partners with Christian Relief Services to provide food, household items, and many more items to help people in need.\nThe Family and Children’s Trust Fund of Virginia\nSince 1986, the Family and Children’s Trust Fund (FACT) has worked to prevent and treat family violence in Virginia. Family Violence includes child abuse and neglect, domestic violence, sexual assault, elder abuse and neglect, dating violence, and suicide.\nGood Shepherd Housing, Virginia\nGood Shepherd Housing works to provide housing alternatives for people struggling to find properties. It also works to provide emergency housing for people facing eviction, and financial education for low-income families and individuals.\nHeaven’s Grocery Store, District of Columbia\nChristian Relief Services provides monthly donations of hygiene items, new toys, food, new coats, and school supplies to this non-profit grocery store located in Washington, DC.\nJesus House Ministries, Virginia\nChristian Relief Services provides shipments of school supplies to Jesus House in Arlington, Virginia.\nNever Say Never Ministries, District of Columbia\nIn connection with its urban ministry, Christian Relief Services partners with Never Say Never Ministries in Washington, DC to provide hygiene items, new coats, and school supplies.\nPrayer Mission Outreach, District of Columbia\nChristian Relief Services provides school supplies, hygiene items, and new toys.\nWestern Fairfax Christian Ministries, Virginia\nA human resources ministries in Fairfax County, Western Fairfax Christian Ministries works to combat hunger and homeless in the county.\nINTERNATIONAL PARTNERS’ MINISTRIES\nAlpha Foundation, Sierra Leone\nLocated in Sierra Leone’s capital city of Freetown, the Alpha Foundation is at the forefront of rebuilding Sierra Leone, which is still recovering from a brutal civil war in the 1990s. Christian Relief Services partners with Alpha on many education, health and medicine, relief efforts, and community development projects.\nBread and Water for Africa, International\nChristian Relief Services’ sister organization, located in Virginia, Bread and Water for Africa® works with grassroots organizations across Africa to help foster long-term, positive change.\nFaith Healing Development Organization (FHDO), Sierra Leone\nLocated in Freetown, Sierra Leone, but with projects throughout much of the country, FHDO works daily to provide health care, education and agricultural development opportunities to promote self-sufficiency among local communities. Christian Relief Services is proud to partner with them in their good work.\nFriend Ship Unlimited, Honduras\nChristian Relief Services partners with Friend Ships to help the poor people of Honduras to hope for a better future. In partnership with Friend Ships, shipments of tools are sent.\nInternational House of Mercy, Democratic Republic of Congo\nChristian Relief Services partners with the International House of Mercy to help people coping with the violence and turmoil that are too often a daily fact of life for the people of the Congo. Christian Relief Services sends regular shipments of medical supplies, clothes, tools, and hygiene items to the International House of Mercy.\nTrinity Ministries, Mexico\nChristian Relief Services partners with Trinity Ministries to help alleviate the suffering of the poor and needy of Piedras Negras through shipments of new clothing, hygiene items, and toys.']	['<urn:uuid:25c2a299-4294-4140-9b34-4e016f552e3e>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	6	66	922
58	What role do family and social support play in helping someone with anxiety disorders versus childhood schizophrenia?	For both conditions, family and social support play crucial but somewhat different roles. In anxiety disorders, personal support from family and friends is described as valuable at all stages of the condition, working alongside professional treatments like medication and counseling. For childhood schizophrenia, families take on a more intensive role - they need to actively participate in care, learn about the condition, try to understand how the child feels, and ensure ongoing treatment compliance. Additionally, for children with schizophrenia, families need to recognize and respond constructively to symptoms that may initially appear as anxiety or rebellious behavior, making their role more complex and demanding than in anxiety disorders.	['Anxiety is a normal response to stress, worry or threat – but when it is very severe, long-lasting, or out of proportion to the circumstances, it becomes known as an anxiety disorder.\nThere are several different types of anxiety disorder: for example, generalised anxiety disorder, social anxiety disorder, panic disorder and obsessive–compulsive disorder. By affecting a person’s mood, thoughts and behaviour, an anxiety disorder can make it difficult to cope with daily life at home, at work or school, and when socialising.\nThe cause of anxiety disorders it not known. However, certain alterations in brain functions have been shown to be implicated in various anxiety disorders. In addition, social conditions and stresses may contribute to the risk of developing an anxiety disorder.\nAlthough they vary between individuals, typical symptoms of anxiety disorders include fear, inner tension, irritability and poor concentration, and physical signs such as dry mouth, dizziness, tense muscles, sweating and palpitations – all of which interfere with daily life. A person may suffer from more than one anxiety disorder at the same time, and sometimes alongside other mood disorders such as depression (so-called ‘co-morbidity’). The co-occurrence of anxiety and depression is very common.\nGeneralised anxiety disorder (GAD) involves a non-specific anxiety that something undesirable could happen. Excessive and uncontrollable worry, anxiety and tension are typical symptoms, together with physical symptoms such as dry mouth, clammy hands, sweating or dizziness.\n¨ Social anxiety disorder (SAD) causes people to fear or avoid social situations. The person fears that he or she will act in a way that will be humiliating or embarrassing.\n¨ Panic disorder is typified by sudden panic attacks associated with marked fear or nervousness. Panic disorder can also include physical symptoms such as sweating, pain, headache, nausea, a pounding heart or a dry mouth.\n¨ Obsessive–compulsive disorder (OCD) causes repetitive, obtrusive and unwanted thoughts, which result in unreasonable fears (obsessions) associated with cleanliness, body secretions or health, for example. In response to these fears, patients may also carry out special rituals (compulsions), including persistent washing, cleaning, bathing, constant checking and rechecking, or maintaining a rigid diet.\nIt has been estimated that anxiety disorders affect between 5 and 7% of the general population, and that up to 29% of people will suffer from an anxiety disorder at least once during their lifetime.1 In 2004, over 28 million people worldwide had obsessive–compulsive disorder, and over 30 million experienced panic disorder.2\nAnxiety disorders affect both men and women across the world, but the burden of the disease is greater in women than in men.3\nSeeking diagnosis and care\nMany treatments are available to effectively manage anxiety disorders, and personal support from family and friends is valuable at all stages.\nDuring a medical appointment, a doctor will diagnose anxiety disorders by asking questions about symptoms, daily life and family history. There may also be a physical examination to exclude other conditions. If an anxiety disorder is confirmed, treatment options will then be considered, including medications, counselling, social support, exercise, relaxation and self-help techniques.\nIn all cases, it is important that professional advice is sought.\n1. Baldwin, D.S. and Hirschfeld, R. M. A. (2005). Fast Facts: Depression, 2nd edn. Health Press, Oxford, UK.\n2. World Health Organization (2004). Prevalence for Selected Causes in WHO Regions, 2004. http://www.who.int/healthinfo/global_burden_disease/PREV6%202004.xls. Accessed 16/09/11.\n3. World Health Organization (2004). The Global Burden of Disease. 2004 Update. www.who.int/healthinfo/global_burden_disease/2004_report_update/en/index.html. Accessed 16/09/11.', 'Childhood schizophrenia is a rare form of schizophrenia, a mental health illness.\nApart from the age of onset, childhood schizophrenia is similar to adult schizophrenia. However, the symptoms can affect children and adults differently. In the long term, the symptoms may be more severe in people who develop them early.\nIn this article, we focus on how schizophrenia affects children.\nIn the past, some autistic children may have incorrectly received a diagnosis of schizophrenia. Distinguishing schizophrenia from autism and other conditions remains a challenge.\nSchizophrenia is rare among children, and some of the symptoms and risk factors may overlap with those of autism. In addition, some family and genetic studies have identified similarities between autism and childhood schizophrenia.\nAs a result, in some rare cases, it can take time to obtain a correct diagnosis of schizophrenia in children. Doctors are likely to be able to diagnose autism much more quickly.\nThe symptoms of schizophrenia in children are similar to those in adults, but they can have different implications.\nThe symptoms include:\n- auditory hallucinations, in which the child hears voices\n- developmental delays\n- language difficulties\n- difficulty coping with school work and social relationships\n- trouble expressing or recognizing emotions, known as “flat affect”\nFlat affect may be noticeable during social interactions, emotional films, and cartoons. It can also affect the ability to identify another person’s emotions by looking at their face.\nIn more than half of the children who go on to develop childhood schizophrenia, unusual features are present from the early months of life.\nThe American Academy of Child & Adolescent Psychiatry (AACAP) note that changes may slowly occur over time. Children who previously made friends easily or did well at school may start to find these things challenging.\nThe AACAP add that parents and caregivers may notice that their child:\n- has unusual behavior or speech\n- has unusual or bizarre thoughts and ideas\n- confuses television and dreams with reality\n- seems confused in their thinking\n- experiences severe mood changes\n- shows changes in their personality\n- believes that someone is after them or talking about them (paranoia)\n- appears anxious and fearful\n- has difficulty relating to peers and maintaining friendships\n- becomes withdrawn and increasingly isolated\n- neglects their personal grooming\nThe child may not always be aware that their experiences are different than those of other people.\nResearch suggests that schizophrenia symptoms may be more severe in children than in adults.\nIn the video below, Prof. Rochelle Caplan, an expert on childhood schizophrenia, talks about how the symptoms appear and the effect that they can have. The Child Mind Institute, a nonprofit organization, produced the video.\nProf. Caplan describes how symptoms appear gradually in most cases. She explains how the experience can be “very scary” for the child at first. To parents or caregivers, this may present similarly to anxiety.\nThe child may feel afraid, for example, because the hallucinations or delusions can feel threatening.\nThe child might also have trouble paying attention, and they may become irritable or have difficulty sleeping. Prof. Caplan notes that some of these changes can resemble rebellious behavior.\nUnderstanding what the child is experiencing can help parents and caregivers react in a constructive way that can help the child.\nAccording to the authors of one case study, early onset schizophrenia is when a child aged 13–18 years experiences symptoms of schizophrenia.\nVery early onset schizophrenia is when symptoms appear before the age of 13 years.\nThe researchers describe a child who experienced unusual perceptions from the age of 3 months.\nThere are no separate criteria to distinguish between childhood and adult schizophrenia.\nIt can be challenging for doctors to diagnose early onset schizophrenia.\nOne reason for this difficulty is that the condition is rare. In addition, other conditions may result in similar behaviors and symptoms.\n- bipolar disorder\n- personality disorders\n- post-traumatic stress disorder\n- some types of obsessive-compulsive disorder\nAutistic children may have characteristics that resemble those of schizophrenia, such as:\n- social withdrawal\n- unusual communication styles\n- avoiding eye contact\nAs with adult schizophrenia, there is no single diagnostic test for the disorder in children, and diagnosis relies on the elimination of other conditions and disorders that could explain the symptoms.\nDoctors will use the same criteria for childhood schizophrenia as for adult schizophrenia.\nTreatment is available for children with schizophrenia.\nDrugs called antipsychotics help manage hallucinations, delusions, and disorganized thinking. One example is clozapine (Clozaril), but a healthcare professional may be able to recommend other options.\nDepending on the drug, adverse effects can include:\n- rapid heartbeat\n- a low white blood cell count\n- movement side effects\n- weight gain\n- high fat levels in the blood and other metabolic symptoms\nHowever, it is important to continue taking the drugs unless a doctor changes the prescription. If a person stops taking them, the symptoms will return.\nAntipsychotic treatments do not cure schizophrenia. The person will need to take medication throughout their life to manage the symptoms and prevent psychosis.\nExperts encourage families to take an active role in caring for a loved one with schizophrenia and helping them face the ongoing challenges.\nSchizophrenia is a lifelong condition. It is not possible to cure or prevent it, but treatment can help manage it.\nIf a child has a diagnosis of schizophrenia, their family and caregivers can help by learning as much as they can about the condition, trying to understand how the child feels, and ensuring that they receive ongoing treatment.\nDepending on the type and severity of symptoms, treatment can help many people with the condition go on to work and enjoy fulfilling relationships.']	['<urn:uuid:6f94984e-151a-4808-a33e-f7d8caf3ebdf>', '<urn:uuid:f4c305ec-43eb-4a91-a286-bdd8071fff1b>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:35:02.547110	17	108	1507
59	military defense improvements counter hybrid warfare technological advancements strategic implementation	To counter hybrid warfare challenges, military forces are implementing technological improvements focused on tactical superiority. These include advanced sensors providing real-time situational awareness and reliable networking for rapid threat response. New radar systems, particularly multi-mission capabilities, are crucial for detecting various threats from small drones to artillery. These systems are being enhanced with digital, phased array technology and advanced signal-processing algorithms. This is exemplified in India's modernization efforts, where they're upgrading their Su-30MKI fighters with AESA radar systems and enhanced flight control systems to support hypersonic missile capabilities, demonstrating how military forces are adapting to modern warfare challenges through technological advancement.	"['‘Hybrid Warfare’ insurgency rely on the integration of advanced capabilities (precision effects, command, control and coordination) while negating the military forces’ superiority in Intelligence, Surveillance and Reconnaissance. By denying the military the ISR producing target locations for standoff precision attacks, insurgents force a superior military power to strip out of most of its advantages, to fight the insurgents in high attrition, close-in battles, which would take a toll in time, casualties, financial cost and morale.\nFacing such hybrid warfare, modern military forces are relying on small and agile elements of military or special forces to carry our the fight. These teams are often providing the coordination and target acquisition elements controlling and guiding powerful, precision effects delivered from stand-off distance by aerial or fire support. When regular combat units that are required to fight in such hybrid warfare they must be equipped with the tactical means improving their capabilities to operate in terms of uncertainty and quickly respond to surprise attacks.\nNew technologies are key to achieving tactical superiority, by improving force protection, regaining the manoeuvring capability necessary to generate the ‘friction’ that would draw insurgents to respond and fight, thus giving away their main advantage – stealth and surprise. Taking advantage of such ‘friction’ tactics require adequate sensors and intelligence, providing the combat forces at the lowest combat echelons with real-time situational awareness, through the use of advanced sensors, and reliable networking enabling rapid and accurate response to defeat the threats, using smart and precise weapon systems, deployed with the forward most elements and controlled by those units.\nAirborne and forward deployed sensors were considered a must for such tactics, but they require complex coordination and assured air supremacy which cannot be guaranteed everywhere. While EO sensors offer an excellent capability detecting visible targets, they are quite limited in persistently covering large areas to provide early warning on incoming attacks. Radars are providing these services much more effectively, but in the past, analog-only radars could carry out those tasks one by one, as often they were tailored for specific tasks.\nAir surveillance radars, even those designed to detect and track fast jets or helicopters at low altitude are not well equipped to detect small, slow flying targets such as drones, ultralight aircraft (ULA) or gliders. In contrast, ground surveillance radars, designed to detect personnel or vehicles on the ground are not capable of alerting on enemy direct or indirect fire coming at them. These services are provided by yet another type of radar – Counter-Rocket, Artillery and Mortars () radars, that provide effective early warning from such attacks, but are often quite bulky and heavy, require complex networking to distribute early warning on imminent attacks, and are often useless protecting tactical units attacked from close range by mortars or direct fire.\nWith digital, phased arrays becoming smaller and lighter, ‘multi-mission’ radars are reaching the field – Elta’s Multi Mission Radar (MMR), Giraffe AMB fromand ’s G/ATOR are all examples of tactical radars that are powerful and capable of supporting combat forces from a stand-off distance. To be affordable these systems utilise common, commercial off the shelf (COTS) hardware enabling the military to deploy such systems at corps and division level, and in support of contingency operations.\nTheir multi-mission capability is derived by advanced signal-processing and algorithms enabling automated performance. As relatively large systems, these assets are often deployed on trucks, positioned at stationary sites to support the division area of responsibility.\nWhile those assets are providing essential support for the division level, there is still a need for similar support provided integrally to the tactical level, particularly when operating in complex terrain where line of sight and other obstacle impair the coverage by stand-off sensors. These capabilities are now becoming available, with the appearance of radars for the manoeuvre forces.\nSuch tactical radars operated as an organic asset with the combat element are required to deliver threat warning in real time, enabling the troops to take cover, evade the attack or respond with effective counterstrike. To support these functions the system should be able to detect direct fire and ballistic threats, calculate the location of the source of fire and projected impact point, determining the relevance to the protected units. These radars are designed to operate on the move, and perform a multitude of missions automatically, exclusively by software control.\n’s defense electronics manufacturer RADA has developed a full line of such ‘manoeuvre radars’ comprising of two basic families: the Compact Hemispheric Radars (CHR) and Multi-mission Hemispheric Radars ( ) Families. As a manoeuvre tactical radar, the CHR can be used for vehicle active protection, while some of the MHR variants provide ground based air defense, supporting VSHORAD missiles, and providing early warning for mobile forces. Operating in static deployments MHR provides short range C-RAM alerts, conduct perimeter security applications or be used as a sectorial gap filler. Both families are based on identical, interchangeable subsystems, thus simplifying support and reducing cost.\nEmploying modern Active Electronically Scanned Array – AESA antennae technology, these radars provide extremely fast volume coverage performing target search, classification and tracking. Innovative angular measurements techniques are used to overcome the small antenna size, along with Pulse-Doppler processing, and digital, adaptive beam forming, enabling a single radar to monitor a wide range of threat velocities.\nBy electronically stirring multiple beams the radar performs track while scan over a full hemispheric coverage, including very high elevations angles, required for ballistic trajectory calculations of typical RAM targets. It also provides real-time range and angle measurement required to support APS.\nThese software controlled radars are offering beam forming to control the spatial coverage, order of beams and their waveform, to tailor the radar for multiple missions either as a dedicated system or an ‘all in one’ sensor, interleaving several missions over certain periods of time. Switching between missions can be programmable, predefined, upon real-time events or manual.', 'Explained: Behind India\'s move to build Super Su-30MKIs fighters\nThe upgrade program, conducted by Hindustan Aeronautics Ltd (HAL), will focus on advanced technology integration, including AESA radar, electronic warfare systems, and hypersonic missile capabilities. Defence Analyst Girish Linganna explains\nIndia is taking steps to upgrade and modernize its Su-30MKI fighter aircraft, driven by concerns about the strengthening air capabilities of neighbouring nations such as Pakistan and China. These upgrades will incorporate various new technologies developed within the country\'s domestic defence sector. As part of its continuous endeavours to boost the capabilities of its main fighter aircraft, India is working towards transforming it into a version known as the \'Super Sukhoi\'.\nThe upgrade program for India\'s Su-30MKI fighter jets will be conducted by Hindustan Aeronautics Ltd (HAL) at an anticipated cost of approximately 56,250 crore Indian Rupees (INR), with all work taking place domestically. Nevertheless, collaboration with Russia\'s United Aircraft Corporation (UAC) remains essential.\nThe Indian Air Force holds the distinction of being the largest operator of Su-30MKI aircraft outside of Russia, with 272 locally produced under license by Hindustan Aeronautics Limited (HAL). The effort to enhance the capabilities of India\'s primary fighter aircraft is slated to begin in 2026, with the objective of aligning the Su-30MKI with the advancing demands of modern warfare technology and capabilities.\nNew Delhi may encounter notable hurdles if it fails to elevate its Su-30MKI fighter jets to the ""Super Sukhoi"" standard, given that its counterparts like Pakistan and China currently possess more advanced fighter aircraft equipped with Active Electronically Scanned Array (AESA) radar technology.\nIndia\'s apprehensions regarding the effectiveness of its Su-30MKI aircraft became evident when the Pakistan Air Force introduced the JF-17 Block 3 fighter aircraft, reportedly influenced by China\'s fifth-generation J-20 \'Mighty Dragon\' aircraft.\nAlongside the JF-17 Block 3, Pakistan has also procured China\'s J-10C fighter jets, notable for their larger AESA radars featuring both transmit and receive modules. During times of increased tension caused by ongoing border disputes between the two neighbouring countries, Beijing has reportedly stationed its advanced J-20 fighter aircraft at airbases in northern India as well.\nIndia and China have also been involved in armed clashes on multiple occasions. Local media reports suggest that approximately 80 to 100 Su-30MKI fighter jets will undergo upgrades in the program\'s initial phase, with plans to eventually extend the modernization to the entire Indian Air Force fleet of these aircraft.\nNevertheless, the upgrade initiatives will exclude any modifications to the airframe and engines of India\'s Su-30MKI aircraft.\nCurrently, India\'s Su-30MKI fighter jets are equipped with AL-31 F Engines manufactured by a Russian company called NPO Saturn. As per sources, the upgrade initiatives will encompass outfitting the fighter aircraft with Beyond Visual Range (BVR) capabilities, modern electronic warfare systems, and AESA radar.\nThe AESA radar set to be installed on the Su-30MKI as part of the upgrade program is a domestically-developed version named \'Uttam\', originally being developed for the Tejas Mk1A aircraft.\nIndian defence media reports indicate that the AESA radar destined for the Su-30MKI will feature larger Transmit and Receive (TR) modules, aiming to enhance its tracking and targeting capabilities.\nApart from integrating AESA radar, the Su-30MKI upgrade program in India will also involve enhancing the fighter\'s flight control systems to support the carriage of hypersonic guided missiles and various other guided munitions.\nWithin the framework of this program, the Su-30MKI aircraft will also be furnished with a new Mission Computer, an enlarged touchscreen cockpit display, and an updated weapon systems package.\nThe program\'s objective is to annually upgrade around 25 Su-30MKI aircraft, with the goal of completing upgrades for all the aircraft by the year 2034.\nWhat is AESA Radar?\nAESA radar stands for \'Active Electronically Scanned Array\' radar. Radar is crucial for detecting and tracking enemy aircraft, ships, and missiles. It provides early warning and situational awareness, helping military pilots make informed decisions during combat.\nImagine traditional radar like a flashlight that you have to manually move around to see things in the dark. AESA radar is like having a bunch of tiny flashlights that can each point in different directions simultaneously.\nA traditional radar sends out one radar signal at a time and then waits for the signal to bounce back (like a single flashlight beam). This can be slower and less flexible. The AESA Radar, on the other hand, uses lots of small radar modules that can individually send out and receive signals. These modules can work together to scan multiple areas at once, providing faster and more precise information (like having many flashlights that can point wherever you want). So, AESA radar is faster, more efficient, and can track multiple targets at the same time, making it much more useful in modern military aircraft and other applications where quick and accurate information is crucial.']"	['<urn:uuid:83e9b887-60f1-4706-bd7d-60868f0c4e07>', '<urn:uuid:f8c9778d-9272-4f9a-9b07-5cb9da7891f0>']	open-ended	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-12T20:35:02.547110	10	101	1774
60	I'm learning to make traditional desserts and I'm curious - what kinds of veggies did people actually turn into candy in the past?	In Renaissance dining, various vegetables were transformed into candy including fennel bulbs, cucumber, lettuce, gourds, borage stalks, borage flowers, chicory, and artichokes.	"['Candy-Making-art - 7/4/12\n""The art of Candymaking: a brief introduction"" by Lady Caterine de Vantier.\nThis article was submitted to me by the author for inclusion in this set of files, called Stefan\'s Florilegium.\nThese files are available on the Internet at: http://www.florilegium.org\nCopyright to the contents of this file remains with the author or translator.\nWhile the author will likely give permission for this work to be reprinted in SCA type publications, please check with the author first or check for any permissions granted at the end of this file.\nMark S. Harris...AKA:..Stefan li Rous\nstefan at florilegium.org\nSubmitted by shannon on March 2, 2012 - 10:00pm\nThe art of Candymaking: a brief introduction\nby Lady Caterine de Vantier\nWhy make candy?\nFor me, as someone specializing in 16th century Italian cooking, it\'s important because it\'s such a crucial part of any menu. The meal would just not be complete without candy. Messisbugo includes candy as a key element in his how-to guide to throwing parties. Scappi, who has an entire book of menus he has cooked, will often round out a meal with the phrase ""condite, & confettioni a beneplacito"". More often, however, he\'ll present a list - often 15-20 items long - of the candies that were served, and what the volume of these was. Lancelotti\'s lists look shorter - but that\'s because he includes three or four items per line.\nBut candies weren\'t exclusive to Italian dining, nor to the 16th century. Due to the expensive ingredients involved, and the time-consuming process of making them, candy was always a luxury item - and therefore an appropriate addition to any high-end menu. Candy shows up in every culture and every region; it is an integral part of fine dining in the Renaissance.\nWe probably can\'t recreate the volume of candies presented at the end of an Italian feast - but we can certainly make and present some candy, which brings us that little step closer to more authentically recreating the dining practices of the elite.\nCandy: Pros & Cons\nIt\'s easy to learn to do\nIt\'s a fantastic way to make your feast more authentic\nIt\'s time consuming\nIt can be hard to get hold of food grade gum arabic in the quantities needed for nuts and seeds\nWhat was candied?\nChances are, if it\'s a fruit, vegetable, nut, seed or spice, someone somewhere tried serving it up as candy. Everything I\'ve listed below appears on at least one Italian 16th century menu.\nNuts (pine nuts, pistachios, almonds, walnuts and hazelnuts could be preserved in different ways)\nSeeds and spices (caraway, fennel, coriander, cardamom, anise, dill, cinnamon, melon)\nPeel (lemon, orange, lime, and citron)\nWhole small green oranges and lemons\nFruit (oranges, lemons, cherries, quinces, melons, pears, apples, peaches, plums)\nVegetables (fennel bulbs, cucumber, lettuce, gourds, borage stalks, borage flowers, chicory, artichokes)\nThese were used to freshen the breath at the end of a meal, never mind that all that sugar might make your teeth fall out. Candied seeds and nuts made using the process below are often referred to as ""confits"". By the Italians they are termed as ""confetti"" or ""confettioni"", are often described as ""white"" and sometimes as ""large"", and are almost always ""in good number/volume"".\nThey are really easy to make, they just require patience:(http://ildhafn.lochac.sca.org/node/475#sdfootnote1sym"">1) it will take you three to four hours to turn 50g of seeds into candy. Fortunately, this makes a much larger volume of candy than what you started with (three or four times as much). The process is to first cover the seeds with multiple layers of gum arabic (each layer is called a ""charge""), and then to repeat the process with sugar syrup.\nThis type of\ncandy making is very good as a form of meditation. Otherwise, I suggest you\nhave a stash of dvds or audio books that can be set up so you can watch/listen\nwhile you make it.\nWorking with heat and sugar:\nSafety and caution are really important with making candied seeds. You are dealing with putting your hand in a pile of stuff that includes boiling sugar syrup. Sugar is more than capable of burning through to the bone, very quickly. Treat the sugar with respect. This is NOT safe for children, unless they are at least nine or 10, really sensible, and have really close adult supervision.\nUse a wok or frying pan with something underneath to hold it off the element.\nMix two teaspoons gum arabic to three tablespoons of water (that\'s enough for about 25g of seeds); 3-8 charges necessary for good results\nPreferably use icing sugar, probably in a proportion of 2 1/4 cups icing sugar to 1/2 cup water; 30-40 charges done over multiple days\nBe prepared to play with sugar/water quantities\nLess is more: use the least amount of gum arabic/sugar syrup per charge that you can get away with.\nAll you need is a wok and a wok stand, or a (preferably non-stick) frying pan and a smaller one (or a pot, or a baking tin) that you can turn upside down and use as a stand under it; and a pot to make the sugar syrup in. This is what I had been contemplating trying when I read Dame Alys Katherine\'s article on how to use this equipment to produce candy, it was very serendipitous to then come across her article.\nThe gum arabic:\nThe amount you need to mix with water will vary depending on the particular gum arabic that you are working with. Dame Alys Katherine works with one teaspoon of gum arabic to three tablespoons of water. I\'ve tried this and it works, but two teaspoons of gum arabic to three tablespoons of water works much better for the gum arabic that I\'m using. Mix it in really well and let it sit overnight before using.\nDame Alys Katherine suggests that getting the right syrup for smooth confits depends on getting the temperature of the syrup right. I\'ve found that actually, more importantly, it needs to have the right proportion of liquid to it. Making the ""right"" syrup also appears to depend on where you\'re living, as the outcome appears to be affected by humidity. Plat used three pounds of sugar dissolved in a pint of water (a ratio of around two to one), Dame Alys Katherine uses two and a quarter cups of sugar to half a cup of water. This ratio seems to be right if you are using icing sugar. For normal white sugar, I use two cups of sugar to half a cup of water, a ratio of four to one. If your mixture has too much liquid, the seeds (and to an extent the nuts as well) will tend to clump together and be difficult to separate.\nYou can make the syrup with normal white sugar crystals. This makes a perfectly decent syrup, but tends to solidify quickly as it cools, crystallising unevenly. Plat specifies the use of powdered sugar, and this is in fact superior in terms of usability. Icing sugar stays liquid for longer and provides a slightly smoother finish (noticeable on almonds). You can also continue using it to successfully make smooth confits even once it is pretty much cold, but hasn\'t yet solidified. However, it also takes longer to set on each layer and seems to create thinner layers - so you\'ll probably need more of them. It\'s difficult to reuse a syrup once it\'s cooled and solidified, as you tend to end up with lumps through it - I\'ve found that you\'re better off making small amounts at the time, and start afresh.\nAdd a small amount of the gum mixture or sugar syrup at the time. You\'re better off using the smallest amount you think you can get away with. I start with about a 1/2 tablespoon of mixture to 50g of seeds, and gradually build up. Keep the heat as low as you can manage it. You may need to turn the element on and off to manage this, or occasionally to remove the pan from the heat completely. You will need at least three charges of gum arabic to make the sugar syrup stick; I usually aim at 5-6 to be on the safe side. This takes about an hour. Leave the seeds to sit at least overnight once you\'ve finished putting on the charges. If they start to clump together once you\'re done, heat them gently in the pan and work them well with your fingers to separate them before moving on to the sugar syrup. You will need to do about 30-40 charges of sugar syrup to get a good result. I usually do this in doses of ten charges per night, leaving the seeds to sit overnight (or for several days) to dry. It\'s really important to let them dry thoroughly, this makes whiter confits.\nA note for those prone to OOS *:\nYou will need to hold the pan while you are working. I find that if I am holding the pan incorrectly while I work, my OOS flares up. Rather than hold the pan by the handle, I find it better to hold the rim of the pan - this is also a good way to ensure that you\'re not heating the pan too much, since you won\'t be able to do this if it\'s too hot. Also make sure to alternate hands, and I wouldn\'t work for more than three hours a day.\n* ""Occupational Overuse Syndrome"" - our new replacement term for RSI (""Repetitive Strain Injury""). This may be an Australian term.\nThe process for candying nuts is identical to the process for candied seeds. There are a few key points though that are worth noting.\nYou do not necessarily need more gum arabic solution for your nuts, just because they\'re bigger. If you\'re working only with a cup full of nuts at the time, in fact, the same quantity of gum arabic as you would use for 50g of seeds is probably sufficient (four teaspoons gum arabic to six tablespoons of water).\nA cup of nuts is about what\'s comfortable to work with in the wok at any one time. Since they\'re larger, they grow faster - so you will probably end up dividing one cup of nuts into two or three groups as they grow.\nThe nuts are more noticeable if they start clumping together. Really take your time to work through them thoroughly to ensure that this isn\'t happening; if it\'s happening a lot, you probably need to halve the amount you\'re working with in the pan, and use less syrup. You can usually gently separate them with a thumb and finger if they have glued together, however, sometimes this will mean that the sugar coating breaks off from one of them.\nI find that once the first ten charges are on the nuts, I get a better result if I only apply six charges at a time.\nA Note on Storage:\nStore your seeds and nuts in plastic ziplock/resealable bags, or in plastic containers that seal well. They go off when you store them in a glass jar. I believe you can also freeze them but I haven\'t tried this yet.\nOther types of candy\nTo make the best candied peel, the trick is to get as much of the pith off the peel as possible. This makes it a bit more tedious to prepare, but the result is worthwhile. I find soaking the peel for a few days is helpful in softening it, changing the water twice daily - several of the period recipes specify that you should do this, for up to nine days. This is also handy if you\'re eating an orange a day at lunch to build up a sufficient quantity of peel to be worth candying. When I make it without soaking, I end up cooking it for three to four times as long to get it to the same point. Nostradamus says that the peel is cooked when it can be pierced easily by a pin. He\'s right. It\'s also important to cook the syrup and pour it over the peel, then remove the syrup again at least once to recook it, as it will have absorbed some of the moisture remaining in the peel. Peel seems to have been served for the most part still in its sugar syrup, however, for ease of eating at a feast I have found it helpful to drain the syrup off and dust the pieces lightly in caster sugar.\nIt\'s fairly well accepted that the term ""sugar plum"" was not used in the Renaissance, but is a later invention that may not even necessarily mean candied plums. However, plums certainly could be presented as a candy in the Renaissance. There are multiple methods for candying plums. I have tried several of these and they come out with an identical result. The trick to making the best candied plums is, once they have been thoroughly preserved in a sugar syrup, to drop them briefly into boiling water then drain them. This turns the plum into something resembling a fruit jelly, and turns them from sweet and palatable into something heavenly.\nCoating things with caster sugar to serve:\nThere is no point in doing this until the absolute last minute. You will want to drain your candies well first. Allow plenty of caster sugar to roll them in. Even once drained, the candies will start liquefying the sugar you have dusted them with. If you try and do this well ahead of time, all you have done is add extra sugar to them and made them that little bit stickier - you will need to repeat the process again before serving. Some candies were presented in this way - Scappi describes them in his menus as ""asciutte"", or ""dried/wiped clean"" - however, it was fairly common for candied fruits and peels in particular to be served with their syrup over them.\nNote that throughout I have taken a teaspoon to be 5 grams/mls, and a tablespoon to be 15 grams/mls.\nStefan\'s Florilegium has the most useful information I\'ve been able to find online\nDame Alys Katherine, Historic Comfits Using Modern Equipment, http://www.florilegium.org/files/FOOD-SWEETS/Smooth-Cmfits-art.html\nDame Alys Katherine, The Candying Process, http://www.florilegium.org/files/FOOD-SWEETS/Candying-art.html\nIvan Day, Sugar-Plums and Comfits on Historic Food - http://www.historicfood.com/Comfits.htm\nLaura Mason, Sugar-plums and Sherbet The Prehistory of Sweets, (Prospect Books, Devon, 2004)\nhttp://ildhafn.lochac.sca.org/node/475#sdfootnote1anc"">1 I also find sheer determination to be a good substitute.\nCopyright 2012 by Shannon Wanty, <dragonfish.dance at gmail.com>. This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License.\nIf this article is reprinted in a publication, I would appreciate a notice in the publication that you found this article in the Florilegium. I would also appreciate an email to myself, so that I can track which articles are being reprinted. Thanks. -Stefan.']"	['<urn:uuid:5dc9748f-af93-4278-ba45-135b7e757c10>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	23	22	2480
61	How do music foundations support education and what scholarships do they provide?	The Bob Moog Foundation operates Dr. Bob's SoundSchool, a 10-week curriculum teaching 2nd grade children about sound science through music, currently in 32 schools in Western North Carolina. Meanwhile, the Narada Michael Walden Foundation provides music education grants launched in 2016 and has partnered with the Academy of Art University to award scholarships for their pre-College Summer Art Program, with eight high school students receiving summer scholarships and three earning 4-year Bachelor of Fine Arts Degree scholarships.	"['Dance music promoter donates $10,000 to Foundation, develops ticketing donation portal for its fans. http://bobmoogfoundation.eventbrite.com\nAmerican electronic music promoter Disco Donnie Presents (DDP) and the Bob Moog Foundation have announced a strategic partnership to further the Foundation’s mission to educate and inspire children through the intersection of music and science.\nThe Asheville-based Foundation, which carries on the legacy of electronic music icon Robert Moog, received a $10,000 donation to assist with its innovative science and music education outreach program, Dr. Bob’s SoundSchool (DBSS), as well as a donation portal on all Disco Donnie Presents tickets pages to help raise additional support. The $10,000 donation was provided from the proceeds of Houston based Halloween electronic music festival, Something Wicked, a partnership between Disco Donnie Presents and Houston promoter Nightculture Events. Additionally, Disco Donnie Presents will match fan donations received through the donation portal.\n“We’re thrilled to have found a partner in The Bob Moog Foundation — a foundation that shares the same values in the importance of education and creativity in the areas of music, math, and science. The strategic partnership paves the way for online donations from our customers, but also the continued support of our company,” Disco Donnie Presents CEO Donnie Estopinal said.\nThe Foundation sees the partnership as a perfect fit for both organizations.\n“Donnie’s prominence within the EDM community and his interests in the area of innovative education that integrates music and science make the partnership a natural fit. We look forward to cultivating our strategic partnership with DDP to work together to ignite creativity in children’s lives through Dr. Bob’s SoundSchool,” said Michelle Moog-Koussa, Executive Director of the Bob Moog Foundation. She continues, “We are especially grateful that DDP has agreed to match all donations made through the ticket donation portals. This extra show of support will provide added incentive for concert-goers to pay tribute to their musical roots while inspiring the future through supporting the Bob Moog Foundation”.\nThe Foundation is in at a pivotal juncture of growth with Dr. Bob’s SoundSchool. The acclaimed program is centered in western North Carolina, but the foundation hopes to grow it regionally and nationally in the next three to five years. The growth of the program costs $20 per student or $500 per classroom.\nThe idea for a strategic partnership first came about when a mutual friend introduced Moog-Koussa and Estopinal at the EDM Biz conference in Las Vegas earlier this year.\nEstopinal’s Disco Donnie Presents, both on its own and through partnerships, produces over 1,000 live events, festivals and club shows around the US annually. Ticket buyers for every show will now have the ability to donate to the Foundation. The Foundation and DDP hope the national focus and number of events will help spread the word and raise financial support.\n“One of the reasons I’m excited about the strategic partnership is that fan ticketing donations are a good way for the electronic music culture to start giving back, said Estopinal. Historically, electronic music fans haven’t really been known for philanthropy, and I’m hoping that’s a perception we can all change together. I’m pleased to offer to match all donations that are made through the ticketing portal to enhance everybody’s contributions to this important cause.”\nWith school music and arts program cuts all across the country and U.S. students lagging behind students in other countries in the areas of science and math, the Bob Moog Foundation seeks to make an immediate impact in the lives of children through Dr. Bob’s SoundSchool.\nThe Foundation operates with two main objectives, education and preservation. The goals come to life through Dr. Bob’s SoundSchool and Archive Preservation Initiative. Eventually, a Moogseum is also in the works.\nContact: Evan Bailey\nCommunications Director, Disco Donnie Presents\nDr. Bob’s SoundSchool\nDr. Bob’s SoundSchool (DBSS) is a 10 week curriculum which engages 2nd grade children in the science of sound through the magic of music. The curriculum explores such basic physics of vibrations, waveforms, amplitude, and pitch. Both acoustic and electronic musical instruments are used to demonstrate concepts, as well as custom designed educational materials developed by our DBSS team of educators and technologists. The curriculum is currently in 32 schools in Western North Carolina. The Foundation’s goal is to grow the program regionally and nationally in the years ahead.\nArchive Preservation Initiative\nThe Bob Moog Foundation Archive is a unique and vast collection of schematics, notes, breadboard prototypes, photos, articles, reel-to-reel tapes, instruments and more. The Foundation works to protect, interpret and exhibit these materials so that future generations can be inspired by Bob Moog’s legacy, and by the work of those with whom he collaborated. The manifestation of this preservation will be to share the archives through traveling exhibits and eventually through our own Moogseum.\nThe Moogseum is planned to be both a website and facility in Asheville where the outreach and preservations goals converge in a hands-on interactive environment.\nFor more information about the Bob Moog Foundation and ways you can help, please visit: www.moogfoundation.org. Donations of any size can also be mailed to: Bob Moog Foundation, P.O. Box 8136, Asheville, NC 28814.\nAbout Disco Donnie Presents\nDisco Donnie Presents (DDP) is an award-winning and recognized leader in electronic dance music event production, generating over 1,000 live events, arena shows and outdoor festivals annually. DDP, both on its own and through partnership, develops innovative, creative and strategic business opportunities in live entertainment. For more information, please visit www.discodonniepresents.com\nAbout NightCulture, Inc.\nNightCulture, Inc. (NGHT.OB), “Concerts that Change Your Life”, is a premier producer of live concerts and events in the following U.S. markets: Houston, Austin, San Antonio and Oklahoma City.', ""Narada Michael Walden Foundation\nMaking a Difference Through MUSIC\nTo enable youth to experience creativity, love and delight through music, and be inspired to make a productive contribution to our community, our nation and our world.\nThe Narada Michael Walden Foundation is a 501(c)(3) non-profit organization dedicated to making a difference in the lives of young people by supporting music appreciation and education for Bay Area youth. Through grants, scholarships, educational programs, performances, mentorships and collaborations with community organizations the Foundation provides opportunities for emerging young artists.\nEstablished in 2004, by Grammy award-winning producer, songwriter, and musician Narada Michael Walden, the NMWF is focused on music appreciation and education.\nThe Foundation delivers its services through a network of non-profit organizations with proven track records of success in youth education and community empowerment. We partner with organizations that provide instruments, music education, and mentoring to students. The Foundation also facilitates the award of college scholarships for aspiring artists and enables the stars of tomorrow by offering talented, emerging young artists access to recording facilities, coaching and production support.\nThe NMWF is a collaborative effort supported by recognized musicians joined together to support youth. The Narada Michael Walden Foundation respects the potential in every person.\nWhat We've Achieved\nThe Music Grant Program: Launched in 2016, the Grant Program directly supports students who are pursuing their music dreams/education and are in need of assistance. This exciting new program continues the Foundation's tradition of working with and providing access to music for talented young people.\nThe Holiday Jam: A joyous 19-year tradition that provides opportunities for young musicians to perform with guests who have included Dionne Warwick, Carlos Santana, Sammy Hagar, Martha Reeves, Tom Johnston, Clarence Clemmons, David Grisman and many others. Young people play key roles in all areas of performance and production, including sound engineering and video recording.\nMentoring Emerging Young Artists: More than 500 students have performed in our events and/or worked with Narada and his talented production team. One example is The Singer’s Experience a program where students develop their talents by studying important artists past and present. A highlight of this program was the first Spring Fling with Strings concert where 15 young singers learned and then performed the music of the legendary Billie Holiday.\nSan Francisco Conservatory of Music: In 2009, the NMWF donated a $10,000 percussion scholarship to the SF Conservatory and featured their50-piece orchestra in the Foundation’s 2009 Let the Sunshine In Concert at Davies Symphony Hall.\nPerforming Stars of Marin: For more than 18 years Narada and the NMWF have provided mentoring, instruments and performance opportunities to the children of Performing Stars, an organization that provides training in dance, music, and social skills to under served children. In 2012, the Foundation presented snare drums and provided drum lessons to these young musicians who then performed at the 16th Annual Holiday Jam.\nAcademy of Art University: In partnership with AAU, a private art school founded in San Francisco in 1929, we facilitate the awarding of scholarships to the University’s pre-College Summer Art Program. Since 2012, eight high school students have received summer scholarships, with three subsequently earning 4-year scholarships to the Bachelor of Fine Arts Degree Program.\nLet the Sunshine In Concert In 2009: We partnered with more than 100 artists to produce a benefit at Davies Symphony Hall in San Francisco for Christopher Rodriguez and the San Francisco Conservatory of Music. Christopher, a talented 12-year-old piano student attending Oakland School of the Arts, was the victim of gun violence.\nMarin School of the Arts: Since 2011, the NMWF has mentored young MSA musicians at this arts academy within Novato High School. We have sponsored drum clinics, individual instruction, and performance opportunities and have partnered with MSA in fund raising benefits.\nHarbor Point Charitable Foundation: In 2012 and 2013, we partnered with Harbor Point and the Marin School of the Arts to present the Divine Wine Extravaganza with proceeds benefiting organizations increasing the quality of life for Bay Area children.\nNarada Michael Walden Foundation is a 501 (c)(3) nonprofit organization\nTax I.D. 33-0611377""]"	['<urn:uuid:5953b49e-2074-4016-aa4e-d49450ae0fca>', '<urn:uuid:a875006f-d08c-485d-a3af-1902aabd2f10>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:35:02.547110	12	77	1615
62	What safety precautions apply when soldering and handling electronics?	When working with electronics, you must work in well-ventilated areas to avoid toxic solder fumes, handle soldering irons carefully to prevent burns, and keep flammable materials away. For RCA connections, use rosin-core solder with a fine-tip soldering iron, and when working with any electronics, ensure proper ventilation for heat dissipation and avoid overloading circuits to prevent fire hazards.	"['One of the most versatile connectors for audio and video use is the familiar RCA plug. It was originally developed by the RCA Victor company as a way to connect its record players and radios, and it is still used today for connecting audio components. Some manufacturers and hobbyists also use RCA plugs to connect speakers because of its convenience and ready availability. RCA connectors can be used with conventional, two-strand speaker wire or shielded coaxial cables.\nMeasure lengths of two-strand speaker wire that will reach from your amplifier to the speakers. Separate the two strands at one end of the first wire, pulling them apart for approximately 2 to 2 1/2 inches. Repeat at the other end, and then do the same with the second wire.\nStrip the insulation from the last 1/4- to 3/8-inch of wire and twist the strands together tightly. Unscrew the shield from the first RCA connector and slide it down over the wire. If you have both an outer shield and an inner insulator, place them on the wire in that order.\nExamine the terminals inside the RCA connector. One will be in the center, and it\'s usually a brass, copper or aluminum terminal. That is the positive terminal. The second is a longer terminal, usually with a hole for the wire and a pair of small lugs that can be crimped down to hold the wire in place. That\'s the negative terminal.\nIdentify the two sides of your speaker wire. One of the two strands is always identified in some way. The wire itself might be a different color, it might have a stripe or lettering on it, or the insulation might be patterned with a series of ridges. The marked side corresponds to the negative terminal on your RCA connector.\nSolder the unmarked side of your wire to the center terminal of the RCA connector, using rosin-core solder and a pencil-style soldering iron with a fine tip. Attach the marked side of your wire to the negative terminal by looping its end through the hole, then crimping the lugs to hold it in place. Finish the connection by soldering the wire to the terminal.\nSlide the outer shell up over the wire and screw it back into place over the connector. If your connector has an inner insulating sleeve, slide that back into place first.\nMeasure and cut a length of coaxial cable for each of your speakers. Use a small, sharp knife or a coaxial cable stripper and remove approximately 1/2- to 5/8-inch of the external insulation, revealing the braided insulating cable. Fold the braid back over the remaining insulation, baring the white inner insulation surrounding the core wire.\nTrim away 1/4-inch to 3/8-inch of the inner insulation with a sharp knife, taking care not to nick the cable\'s core wire. Strip and trim the remaining cable ends the same way.\nUnscrew your first RCA connector and slide the outer shell down over the end of the wire. If an inner insulating sleeve is present, slide that over the cable as well.\nExamine the terminals in your connector. One is relatively short and located in the center of the connector; that\'s the positive terminal. The other is longer and at the side of the connector. That\'s your negative terminal.\nSolder the core wire of your coaxial cable to the positive terminal in the center of the connector. Roll the braided wire forward and twist it together into a tight strand. Solder the strand of braided shielding to the negative terminal.\nSlide the outer shell up over your cable and thread it back onto the connector. Repeat for the remaining three speaker cable ends.\n- If you\'re likely to rearrange the room, allow an extra two to three feet of speaker wire to give yourself some flexibility.\n- For general-purpose use, 16-gauge speaker wire is adequate. For longer runs or high-power amplifiers and speakers, 12-gauge or 10-gauge wire offers better signal transfer. For best results, match your connectors to the grade of cable you\'ve chosen. Inexpensive lightweight connectors are acceptable on conventional speaker wire, while heavy-duty, gold-plated brass is a better match for heavy gauges or high-end coaxial cable.\n- Although solder terminals are the most common way to connect wire to the RCA plug, there are other methods. Some connectors crimp onto the wires or use set screws. Connectors intended for coaxial cables are sometimes designed to slide between the layers of wire and insulation. If you aren\'t prepared to solder your own connectors, staff at your local Radio Shack can advise you which plugs use these alternative connection methods.\n- A soldering accessory such as Radio Shack\'s ""Helping Hands"" (catalog number 64-079) can simplify the process, using alligator clips to hold your wire and connector in place. This frees both of your hands for soldering.\n- For better durability and a more finished appearance, you can slide a short piece of heat-shrink tubing over the section where the plug\'s outer shell meets the wire. Shrink the tubing into place with a heat gun.\n- Take care not to nick the core wire of a coaxial cable while you\'re removing the white inner insulation. If you cut most of the way through, then twist off the piece of severed insulation with your fingers, you\'ll minimize the risk.\n- Soldering irons and melted solder can cause serious burns. Handle your soldering equipment with care and have a soldering iron holder or designated heatproof surface near your workspace so you can safely set it down.\n- Solder fumes can be toxic if inhaled. Work in a well-ventilated area and avoid breathing the fumes.\n- Two-strand speaker wire\n- Wire cutter/stripper tool\n- RCA plugs\n- Rosin-core solder\n- Pencil-type solder iron, with a fine tip\n- Flexible coaxial cable\n- Sharp knife or coaxial cable stripper\n- Comstock/Comstock/Getty Images', 'I’ll never forget a bad experience I had once as an electronics apprentice. When it comes to working with electronic equipment, Arduino included, knowing how to deal with fire hazards is crucial. If ignored, your Arduino can catch fire and cause you damage even if it’s the smallest component, such as a diode.\nBut, can Arduino catch fire? Here’s what you need to know: Yes! Arduino can catch fire under certain conditions. If this is not taken into considerations when working on your projects, it can cause serious damages or even hurt you.\nThis article will focus on the ways to keep your Arduino projects and yourself safe from the risk of fire and the factors which can prove to be a fire hazard when working with electronic equipment. So, make sure you give this article a good read till the last word.\nWhat Is a Fire Hazard?\nFire hazards are materials, situations, or practices that can lead to an uncontrollable fire. Strictly talking about electronics, a small spark can ignite a massive fire or even an explosion within a few seconds.\nThe incorrect methods of handling electronic equipment, together with the wrong type of components used in projects, are some of the factors that contribute towards an electrical fire.\nThere are many other types of fires, and each one has a different method of extinguishing it. Some fires need water to extinguish, while others are strictly put down by using only specific chemicals such as carbon dioxide.\nWhat Are the Causes of Fire in Arduino Projects?\nNow that you’re aware of what a fire hazard is, let’s take a look at the possible causes of fire in Arduino projects.\nWhat could go wrong when working on a simple Arduino project? You might be wondering.\nCan an Arduino board cause a fire? Let’s find out!\nHere 9 Reasons Your Arduino Can Catch Fire\nReason #1: Poor Ventilation\nThe most common cause of fires in Arduino projects as well as other electronic projects is poor ventilation.\nWhen an electric current passes through a piece of conductor, it produces heat energy because electrons are flowing through the conductor, and they bump into atoms of the conductor, giving off heat energy.\nWe use to make many connections on our Arduino boards when working on a project, meaning that multiple currents are flowing through it; hence generating a considerable amount of heat energy.\nSo, the heat that electronics projects produce needs to be guided out of the system through a proper ventilation channel.\nYou shouldn’t, for example, build an Arduino project and keep it without proper ventilation (or close it in a tiny room) where there is zero ventilation because sooner or later, you’d have to call the fire brigade to help you out!\nSo, always make sure to place your Arduino projects in a well-ventilated area when powered on.\nReason #2: Loose Connections\nLoosely connected wires or other components are another cause of the fire because they obstruct the current path and increase the resistance of that connection, causing heat to dissipate.\nIf you’ve ever seen a loosely connected plug hanging on the wall socket, you would’ve noticed that it has turned black or it’s hot at the touch, because of the extra heat produced.\nLoose wires in Arduino projects can also burn out and cause a fire to start.\nReason #3: Short-Circuiting\nA short circuit occurs when two wires or two conductors come into contact with each other when they’re not supposed to.\nThese “wrong contacts” lead to a high flow of current, and often you can see a spark coming out of it.\nIf the short circuit continues, the spark may develop into a fire and create a lot of unwanted mess.\nReason #4: Defective Components\nUsing inferior quality components in Arduino projects or using a fake Arduino board is also classified as a fire hazard because the defective parts cannot handle a lot of heat. Hence, they often burn out and may cause a fire to flare up.\nIf you want to check whether your Arduino board is original, check out this article.\nReason #5: Overloading\nAlways make sure not to overload your Arduino board because it might lead to a fire incident.\nOverloading means connecting the Arduino board to too many components, where each of them is driving a significant amount of current from the microcontroller board.\nWhen you overload your Arduino board for a long time, the connections heat up, and eventually, the temperature of the board rises above the safe limits and leads to a fire.\nHence, you should always avoid overloading your Arduino board when using it in projects.\nReason #6: Presence of Flammable Materials\nMaterials that can catch fire easily, such as clothes and flammable liquids, must always be kept at a safe distance from electronic circuitry.\nAvoid keeping these items near your Arduino project because these materials will help the fire spread very fast.\nEven if there’s just a tiny spark that won’t turn into a fire, these materials can make the sparkle grows into a fire.\nTo see which household substances are highly flammable, click here.\nReason #7: Faulty Outlets\nFaulty outlets and outdated electrical wiring in the house is a significant reason behind household fires.\nSo, if you’re using a DC adaptor plugged onto a wall socket to power your Arduino board, make sure that the wall outlet is not faulty to prevent a fire in your house.\nOne can easily recognize faulty outlets checking simple signs, such as burn marks near the socket.\nIf you need to read more about how to identify faulty sockets around the house, click here.\nReason #8: Lithium Batteries\nUsing lithium batteries to power up your Arduino board is a big NO! Because lithium batteries are highly flammable and may explode when exposed to heat.\nArduino projects, as explained before, dissipate heat especially if running for a long time, and this can flare up a fire in the presence of highly flammable lithium batteries.\nSo, it’s best not to use those batteries since you have a lot of other alternatives to power up your Arduino projects which offer a low risk of fire.\nReason #9: Polarized Capacitors\nUsing polarized capacitors in Arduino projects can sometimes prove to be catastrophic, mainly when these capacitors are supplied with either reversed-polarity voltage or exposed to high temperatures.\nSometimes, a high AC can also lead to a fire. Hence, always make sure to take necessary precautions when using polarized capacitors in Arduino projects.\nIf you want to know everything about capacitors, check out this awesome article that even shows a lab test with an explosion. Who does not love to blow up things in the lab?!\nMake sure to place the project in a cool, well-ventilated place when powered on, and the input voltage and current are at the right level.\nHow Can You Reduce the Risk of Fire in Arduino Projects?\nNow that you know what the possible causes of fire in Arduino projects are, you can quickly reduce the risk of fire by following the following, simple steps.\nStep 1. Monitor the Temperature\nAlways place your projects in a well-ventilated area or attach a cooling fan along with your project. Taking these precautions will make sure that the temperature of your Arduino project does not reach the dangerous level where a fire can quickly start.\nYou can also program your Arduino project in such a way that if the temperature rises above a certain level, the project turns itself off for a while and then starts again when the temperature drops down.\nStep 2. Double-Check the Connections\nAdopt the practice of checking your Arduino project’s connections before powering it on.\nMake sure you’re supplying the right amount of voltage with the correct polarity, and to fit all the connections tightly.\nIn case you’re using an adaptor to power up your Arduino board, make sure to connect it firmly to a power outlet that isn’t faulty, thus reducing the risk of fire.\nStep 3. Calculate the Currents\nSupplying current higher than the recommended value can cause unnecessary heat to build up and increase the temperature of your Arduino project.\nSo, the best practice would be to calculate the currents before assembling the project so that you don’t have to deal with the hassle of putting out an electrical fire.\nStep 4. Keep it Away from Flammable Materials\nFlammable materials are a primary cause of spreading fire. As mentioned above, these substances can catch fire within a few seconds; hence, keep these materials away from your Arduino project when they’re powered on.\nStep 5. Build Neat Circuits\nCircuit boards that have a lot of components squeezed into a tiny area are more likely to develop a short circuit in comparison to those neatly constructed, with components placed apart and separate connecting wires which are not tangled up.\nAnother advantage of building a neat circuit is that you’ll know which wire is connecting which components and when making amendments, you wouldn’t have to spend a lot of time figuring out which connection you have to break or change.\nAs you can see, neat circuits are not only a treat for the eyes, but they also help reduce the risk of fire.\nNow, after reading this article, you’re ready to fight/prevent the risk of fire associated with Arduino boards or any of your electronic projects.\nAll you need to do is always to keep the causes of fire, described above, in mind while working on your upcoming Arduino project!\nHave you ever had a bad fire hazard experience while working on your electronics or Arduino projects before? What caused the error?\nLet us know on the comments below! You can help other people to keep themselves safe while enjoying their Arduino projects!\nWant to know a bit more about this topic? Check out these articles:\n- 5 common causes of electrical fires (firerescue1.com).\n- Fire hazards posed by personal electronic devices (imca-int.com).\n- Electrical Fire Hazards (systemsafetyengineering.com).\n- An inside look at capacitors (craneengineering.com).\n- Safety of using microcontrollers such as Arduino (electronics.stackexchange.com).']"	['<urn:uuid:4962ebd9-a5e6-4230-a7d0-5a16e736d1e3>', '<urn:uuid:147dcb55-31e5-443a-aea9-6b4955975964>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:35:02.547110	9	58	2657
63	gps farming vs driverless tractors benefits savings	Both GPS farming systems and driverless tractors offer cost savings, but in different ways. GPS mapping helps farmers understand soil conditions and optimize fertilizer use, while driverless tractors can reduce labor costs by 30-40% through automated harvesting, weeding, and pruning features. Driverless tractors also protect farmers from harsh weather conditions and health hazards during operations like insecticide spraying.	"[""What do data analytics and modelling, fertiliser applications, harvesting, irrigation, seeding, planting and spraying equipment, GPS systems, robotics and smartphone apps have in common? They’re all precision agriculture systems that can revolutionise various aspects of a farm's operations. And as technology evolves to keep pace with the changing needs of a business that’s worth as much as 12% of the country’s GDP, they’re poised to help transform Australia’s agricultural industry.\nWith the Federal Government wanting Australian agriculture to become a $100 billion industry by 2030, a key focus for the industry is the unlocking of new technology, supported by a favourable tax and policy environment for innovation. It’s no surprise, then, that interest is firmly fixed on how producers can harness the benefits of precision agriculture to achieve everything from increased production and reduced labour costs to water savings, lower wastage and greater task automation.\nThe breadth of available applications is huge. Whether it’s computer and mobile-based apps designed to provide real-time information relating to crop disease, or GPS mapping to better understand soil conditions, new technologies are emerging to help farmers collect data, manage risk, increase yields, save time and cut costs.\nAs Anastasia Volkova, CEO and Founder of data analytics platform FluroSat explains, “there have been major advances in spatial and navigation technology, meaning that agriculture can increasingly be underpinned by technologies like self-driving tractors and robotics. Secondly, the ability to use proximal or remote sensing to collect information that can both inform decision-making and measure its effects is also key. And the final piece of the puzzle is machine-learning advancements, and the ability to collect data to create a highly detailed picture – which again, can have hugely beneficial implications for agriculture.”\nTechnologies that address drought-related challenges are another burgeoning industry. Following the government’s recent plans to set up a $3.9 billion fund to support water infrastructure and drought-related projects, the Global Forum for Innovations in Agriculture (GFIA) has identified a suite of new technologies that can help combat Australia’s current drought. They include a water conditioning system that enables growers to utilise supplies previously considered unsuitable, a device that continually informs farmers of flow rates and total water used, and a microclimate solution combining sensors, analytics and apps that analyses atmospheric conditions, leaf wetness, soil moisture and temperature to provide localised growing condition data.\nAnastasia believes there’s a strong case for embracing new technologies, especially in the face of extreme conditions. “Robustness is the key characteristic needed for agricultural systems, especially in the face of adverse weather like drought. Technologies like variable rate, advance sensing and machine learning technologies can underpin a successful solution that can mitigate the worst effects and support the industry through these types of challenges.”\nAgTech Cluster Leader at CSIRO Data61 Dr Peyman Moghadam agrees that new technologies are receiving a huge amount of interest currently, especially relating to drones, aerial and satellite imaging, robotic technologies, automated harvesting and precision spraying. Yet a sense of innovation fatigue is also affecting the industry, he says. “Growers are right now more interested in adapting technology to fit their existing equipment than investing in brand new assets. So what’s happening currently is a trend of retro-fitting existing assets to make them smarter. We’re not taking drivers out of the tractors just yet. Instead we’re offering smart features, retro-fitted to make lives easier, using technology like auto steering and automated harvesting, weeding and pruning, which can help reduce labour costs by as much as 30-40%.”\nWhether through robotics, virtual reality, AI or new automation technologies, the digitalisation of agriculture means that farmers and growers will over time be better able to increase production and yields, with fewer inputs and labour costs. As Anastasia explains, “Costs can be reduced by knowing for instance the response to nutrients in different areas of a farm, so that fertilizer use can be fully optimized. Whether it’s used for weed detection in the field, or for grading the protein content of a grain that’s about to be harvested, precision agriculture solutions can offer impressive results for farmers who are able to embrace these new technologies.”\nThe GFIA will present the latest innovations and technologies for smart food production in the precision agriculture sector at Australia’s largest agricultural innovation event, GFIA in Focus, in Brisbane on November 27-28. Anastasia and Peyman will join a host of guest speakers discussing everything from smart irrigation and big data to robotics and Virtual Reality.\nFor more information:"", 'Mahindra’s Driverless Tractor is the First in India\nMahindra & Mahindra Ltd., a part of the USD 19 billion Mahindra Group today displayed its first ever Driverless Tractor. Developed at the Mahindra Research Valley, the Group’s hub of innovation and technology located in Chennai, the driverless tractor is all set to redefine the mechanization process for the global farmer.\nWith this launch, Mahindra becomes the pioneer in the Indian tractor industry with the unique proposition of a driverless tractor. This innovation will change the future of farming by increasing productivity, leading to increased food production to feed the growing needs of the world.\nSpeaking on this occasion, Dr. Pawan Goenka, Managing Director, Mahindra & Mahindra Ltd. said,\nOur tractor R & D has always been at the forefront of pioneering cutting edge solutions. Today’s display of the driverless tractor is another proud moment for us, as it opens up new possibilities in farming. We are happy to dedicate this innovative mechanization for the global farming community, in line with our Farming 3.0 proposition\nRajesh Jejurikar, President, Farm Equipment Sector, Mahindra & Mahindra Ltd. said,\nToday the need for farm mechanisation is higher than ever before, due to labour shortage and the need to improve productivity and farm produce yield. Coupled with our ‘DiGiSENSE’ technology that we launched last year, the driverless tractor offers a distinct advantage to the Indian farmer by bringing an unprecedented level of intelligence to the tractor\nThis technology will be deployed across Mahindra tractor platforms in due course of time. It will also be deployed across international markets such as USA and Japan, giving Mahindra tractors a distinct competitive edge in global markets with technological advancement being at the forefront. Mahindra plans to offer the driverless tractor technology across its range of tractors from 20 HP to 100 HP over a period of time.\nMahindra Driverless Tractor Features\nThe pioneering driverless tractor is equipped with state-of-the-art technology and boasts of several unique features:\nAutosteer – GPS based technology that enables a tractor to travel along a straight line.\nAuto-headland turn – Enables the tractor to orient itself along adjacent rows for continuous operation without any steering input from the farmer.\nAuto-implement lift – Feature in the tractor that automatically lifts the work tool from the ground at the end of a row and lowers the tool after the tractor has oriented itself for operation at the next row.\nSkip passing – This technology feature enables the tractor to steer to the next row for continuous operation without any intervention of the driver.\nMahindra Driverless Tractor Safety Features\nIn addition, the driverless tractor is also equipped with some unique safety features as below:\nGeofence lock – Prevents tractor from going outside the boundaries of the farm\nControl via Tablet User Interface – Enables the farmer to program various inputs needed to farm efficiently. Also offers controls to prevent the tractor veering off from its intended path or desired operation. He can also control the tractor remotely via a tablet.\nRemote Engine Start Stop – Ability to stop the engine and hence, bring the tractor to a complete STOP if needed in cases of emergency\nWith the deployment of this technology on Mahindra tractors, the farmers can work their fields for long hours without exposing themselves to harsh weather or difficult operating conditions. They can also protect themselves from potential health hazards resulting from operations like insecticide spraying which now can be done without human intervention. It will also ensure better quality and consistency in farming operations, leading to higher productivity and farm produce yields.\nYou can add more to this story by taking part in the discussion forum.']"	['<urn:uuid:6e5a0886-1bef-4fc7-b7f5-52886f0e30cc>', '<urn:uuid:4067dd64-4863-4e24-8435-e97ec83f7083>']	factoid	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:35:02.547110	7	58	1353
64	What factors influence whether assault victims report crimes to the police, and how does trauma affect women's behavior in prison environments?	Several factors influence assault reporting to police. Victims are more likely to report if physical injury is severe (77% report when hospitalized), if weapons were used (55% vs 28% without weapons), and if they consider it a crime (45% vs 11% when not considered a crime). Location matters too - assaults at home or work are reported more (36-37%) than those in public places (25%). Interestingly, victims are less likely to report if the offender was their partner (17%) compared to unknown offenders (48%). As for trauma's effects in prison, it significantly impacts women's behavior. Around 90% of women in prison have trauma histories, which can lead to difficulty trusting others, hypervigilance, and emotional numbing. In prison environments, trauma sufferers may exhibit behaviors like self-harm, defiance, and extreme emotional reactions - behaviors often misinterpreted as 'attention seeking' or 'non-compliance' rather than trauma responses.	"[""|Page tools: Print Page RSS Search this Product|\nLIKELIHOOD OF VICTIMS REPORTING CRIME TO POLICE\nMarital status appears to impact on the likelihood of a victim reporting an assault to police, with married people having a higher proportion (37%) than unmarried people (27%) (table 11.5). The difference may actually be driven by age rather than marital status as younger people are more likely to be unmarried.\nA person's labour force status (i.e. employed, unemployed or not in the labour force) does not appear to be a significant contributor to a victims propensity to report assault to police.\nVictims in capital cities (29%) are marginally less likely to report assault to police than victims in non-metropolitan areas (34%).\nCHARACTERISTICS OF ASSAULT\nDifferences were found in the reporting of assault to police by location of the assault (graph 11.6). Victims were more likely to report the most recent assault to police if the assault occurred at home (37%) or place of work/study (36%), than if the assault occurred in a public place or some other location (25% each).\nDespite the propensity of victims to report an assault that occurred in their own home, victims were less likely to report an assault if the offender was their partner (17%) than if the offender was not personally known to them (48%) or was a neighbour (54%) (graph 11.7). Victims of offenders who were friends were also less likely to report an assault to police (23%). There was no significant difference in the proportion of victims reporting an offence where the offender was the victim's ex-partner, acquaintance or work/study colleague.\nPhysical injury is a major contributor to a victims propensity to report the most recent assault to police, rising with the severity of injury (graph 11.8). Over three quarters (77%) of victims would report an assault to police when admitted to hospital. Those injured, but not admitted to hospital comprised 42%. Only 26% of victims that were not physically injured reported the most recent incident to police.\nThe number of offenders involved in an assault did not significantly influence the likelihood of a victim reporting to police (graph 11.9).\nThe use of a weapon influenced a victims decision to report the most recent assault to police (graph 11.10). Over half of victims (55%) of assault involving a weapon reported the most recent assault to police compared to victims where there was no weapon used (28%).\nVictims that considered the most recent incident to be a crime were more likely to report the most recent assault to police (45%) than those that did not consider the incident a crime (11%) (graph 11.11).\nVictims were more likely to report the most recent assault incident to police if it was serious. The extent of injury resulting from the incident, the use of a weapon in the incident and whether the victim considered the incident to be a crime all had a significant impact on the likelihood of a victim reporting the incident to police.\nAge also is a factor with older people more likely to report to police. The location of the incident and whether the offender was known to the victim also had some impact, with victims more likely to report if the incident occurred at home than if it occurred in a public place or some other location, and less likely to report the incident if the offender was their partner.\nREASONS FOR NOT REPORTING\nThe most common reason provided by victims who did not report an assault to police was that the matter was too trivial/unimportant (29%) (graph 11.12). This is consistent with findings that show that victims were more likely to report incidents of a serious nature.\nLess than a quarter of victims (24%) indicated that the incident was a personal matter or the victim would take care of it themselves as the reason for not reporting to police. Other reasons for not reporting included: there was nothing the police could do (9%); and police would be unwilling to do anything (10%).\nThese documents will be presented in a new window."", 'Trauma hiding in plain view: the case for trauma informed practice in women’s prisons\nPrincipal Adviser Female Offenders, Programmes and Interventions, Service Development,\nDepartment of Corrections\nHannah is the Principal Advisor, Female Offenders at the Department of Corrections where she is leading a strategy to improve outcomes for women in the corrections system. She completed a law degree and post graduate studies to become a barrister in England before commencing work for the Department in 2013. She started as a corrections officer at Auckland Region Women’s Corrections Facility and since then has worked in both strategic and operational policy roles at National Office.\n“It just blows you away the lives they’ve led. It’s a testament to their strength that they’re still in the condition they’re in. They’re survivors and I guess they’ve been like that forever” – Corrections officer in a women’s prison in New Zealand\nCriminal justice systems across the world have started to understand that men and women need to be managed on the basis of their different needs – what works for men, and what works for women (Barett, Allenby and Taylor, 2010, Government of South Australia 2014, Prison Service Order 4800 HM Prison Service 2008, Bloom, Owen and Covington 2003). Female offenders are different to male offenders and while this statement may be obvious, many jurisdictions have taken the same approach to managing women as they have with men, with not surprisingly, mixed outcomes. They need different interventions and respond to management and supervision in different ways (Wilton, 2012; Bevan & Wehipeihana, 2015; Wright, Van Voorhis, Salisbury & Bauman 2012). Working with female offenders to achieve change takes a particular skill set and a particular level of resilience.\nThis is a significant shift from the historic practice of women being an afterthought in system design. Policies, practices, processes and infrastructure have often been designed with men in mind.\nAcross the world, including in New Zealand, women typically make up 6% of the prison population and 20% of the community offender population. This means they aren’t just a slight minority, they are the significant minority. On the whole, women are in prison for less serious offences than men, are on shorter sentences and have a lower risk of re-offending. However, these trends are tracking in the wrong direction. The number of women managed by Corrections today is 69% higher than it was at the start of this decade. Recidivism is increasing, with more women starting a second sentence and unsurprisingly, this is mirrored in their risk of re-offending levels which have also shifted upwards.\nA fresh focus on female offenders in New Zealand\nCorrections has been working for many years to meet women’s needs and reduce their criminogenic risks. However, given recent trends, a fresh focus is being placed on female offenders.\nTo reduce women’s re-offending, Corrections is seeking to ensure that women have equitable access to services and interventions to meet their offending needs, and that women’s management is trauma informed, relational and empowering (Bevan & Wehipeihana, 2015).\nKey to our approach is effective responses to women’s mental health issues, substance dependence and experiences of trauma. All of these are likely to be intertwined for women; substance dependence and mental health challenges are often linked to historic trauma. These issues play a significant role in the lives of women who offend, and impact on how they are managed within the system, especially in prison. We know that these issues are common across our entire offending population (Indig, Gear & Wilhem, 2016). The picture for women is bleaker in every category.\nThis prevalence of trauma may explain some of the behaviours that are potentially misinterpreted by those working with the women: behaviours viewed as “manipulation”, “non-compliance” and “neediness.” The remainder of this paper provides a brief look at how trauma can affect individuals, how prison environments and culture can trigger re-traumatisation, and what effect trauma informed practice can have on women’s success in custodial environments.\nTrauma: what is it and what does it do to you?\nRecent research by Corrections suggests that around two thirds of women in prison have experienced family violence or sexual assault prior to their entry into prison (extracted data from Indig, Gear & Wilhem, 2016). Looking at all types of traumatic events, some international studies estimate that rates of trauma histories among women in prison are as high as 90% (Bloom, Owen & Covington, 2005; Wright et al 2012).\nTrauma has a range of definitions, multiple causes, and varied responses depending on the individual. Trauma results from an event, series of events, or set of circumstances that is experienced by an individual as physically or emotionally harmful or threatening and that has lasting adverse effects on the individual’s functioning and physical, social, emotional, or spiritual well-being (SAMSHA, 2012). It can be caused in a range of ways, from physical or sexual assault to death of a loved one or loss of a job or a relationship. Historical trauma as a result of intergenerational poverty, racism or disenfranchisement is also an area with a growing body of research (Brave Hears, 2005). This is particularly relevant to New Zealand’s prison population where Māori are significantly overrepresented, even more so in the women’s prison estate.\nThe impact of trauma can be “subtle, insidious, or outright destructive” (SAMSHA, 2014), with many sufferers stuck in a constant state of extreme stress and self-protection. In real terms, trauma can have the following impacts:\n- Difficulty trusting, making it hard to establish close relationships\n- Negatively affected cognitive abilities\n- Undermined sense of safety causing counterproductive behaviour in an effort to regain control over their environment. Such behaviour could include self harm, defiance and aggression\n- Hypervigilance and fearfulness\n- Physical pain or illness symptomology\n- Emotional numbing, feeling nothing most of the time\n- Freezing when there is a present or perceived danger.\nIn a prison environment the impacts of trauma can be exemplified because many of the day-to-day occurrences in a prison can be perceived as threatening for trauma sufferers. Musters, loud noises, banging doors, shouting, confined spaces, control and restraint techniques, lack of privacy and body searches can all trigger responses for trauma sufferers, or profoundly retraumatise them (Benedict n.d.).\nThis means that many of the behaviours that staff in women’s prisons witness on a daily basis may be better understood and explained as the result of trauma, and not as women being “attention seeking”, or “non-compliant”. Building on the list above and making it directly relevant to the prison environment, these behaviours also include self harm, defiance, extreme emotional reactions and refusal or difficulty engaging positively with staff, other prisoners or in rehabilitative programmes because their trauma is a responsivity barrier (Benedict, n.d., Miller & Najavits, 2012.) The tendency of women in prison to form intense relationships and pseudo families with other prisoners has also been cited as common among women suffering from the on-going symptoms of trauma (Benedict, n.d.).\nIt is unsurprising that the connection between the trauma and the behaviour seen by prison staff goes unnoticed. This is not confined to prison services, and many different services have the potential to re-traumatise. The Manitoba Trauma Information and Education Centre states first and foremost that:\n“Service organisations are confronted by the signs and symptoms of trauma every day and yet often fail to see it and make the necessary connections. Trauma hides in plain view. Every organisation and system has both the potential to re-traumatise and interfere with recovery and the potential to support healing.”\nTrauma informed practice: what is it and how can it work in a prison?\nTrauma informed practice is about taking the time to understand the reasons for women’s behaviour, and responding accordingly. Those responses seek to avoid causing further trauma.\nPrisons are full of unavoidable triggers for trauma sufferers and institutional security will continue to be a primary function of prison staff. However, this does not mean that trauma informed practices cannot be introduced in a prison environment, and early evidence indicates wide ranging benefits for women in prison and prison staff.\nA number of women’s prisons across the world have begun making their environments more trauma informed, and have started offering trauma specific services as well. There are examples across the USA where these new practices and services have been implemented, and England and Wales have taken some initial steps.\nBy introducing these measures the women’s prisons have seen benefits for staff safety, prisoner safety and prisoner’s engagement in industry, treatment and learning. These benefits have included:\n- Improved attendance and participation in programmes and other interventions\n- Improved job satisfaction and staff morale for prison staff\n- Decreased prisoner on staff assaults, and prisoner on prisoner assaults\n- Decreased conflict between prisoners\n- Decreased suicide and self harm attempts\n- Decreased use of segregation\n- Decreased use of restraint\n- Decreased disciplinary charges.\n(Benedict n.d., Miller & Najavits, 2012; Paterson, Uchigakiuchi & Bissen, 2013)\n“Since the staff learned about trauma, they act differently. Some staff used to be hostile, but now they explain the rules calmly, they don’t yell. They take more pride in their jobs. It has changed their negative, judging attitudes to acceptance and understanding.”\nRoberta, a prisoner at Women’s Community Correctional Centre, Hawai’i\nTo achieve these benefits, the prisons that have begun implementing trauma informed practice introduced a range of measures that included:\n- Significant training for all prison staff\n- Inclusion of women in strategic decisions\n- Increasing community outreach and engagement in prison life\n- Language changes\n- Increased access for women to their children and changes in the activities women undertook with their children during visits\n- Environmental change including increased grassy areas which replaced concrete yards and replacing “institutional colours” with bright colours and artwork.\n(Benedict n.d., Miller & Najavits, 2012; Paterson, Uchigakiuchi & Bissen, 2013)\nA step in the right direction for New Zealand\nIn New Zealand prisons we have already taken some significant steps to manage women according to their gender specific needs. Some trauma specific services are offered to women in prison, including counselling through ACC. In recognition of the high numbers of women in prison who are suffering from the ongoing effects of trauma, funding has recently been granted by the Justice Sector Fund to provide full time counsellors and social workers in women’s prisons, employed by Corrections. The counsellors will concentrate their time on delivering trauma specific services to those women in need. The social workers will dedicate their time to supporting and enhancing women’s wellbeing in prison, particularly women who are mothers.\n“In the past we would see negative behaviour as simply a management issue; now we are looking more closely at the root causes of negative behaviour. This shift allowed us to break new ground and look at the impact of trauma on the behaviour of women”\nLynn Bissonnette, Superintendent, Massachusetts Corrections Institution at Farmingham\nWhile the provision of trauma specific services is important for the women, the work done in these sessions will be most successful if it is supported by an all of prison commitment to trauma informed practice. Initial conversations have taken place, notably in the Southern Region, and as part of Service Development’s fresh focus on women, we have committed to the further investigation and eventual implementation of trauma informed practice in our women’s prisons. This work will be challenging and will require a culture shift in the way we work, but this commitment is a step in the right direction.\nBenedict, A., (no date), Using Trauma Informed Practices to Enhance Safety and Security in Women’s Correctional Facilities, National Resource Center on Justice Involved Women, CORE Associates\nBevan, M., and Wehipeihana, N., (2015) Women’s Experiences of Reoffending and Rehabilitation, New Zealand Department of Corrections\nBloom, B., Owen, B., & Covington, S., (2005) A Summary of Research, Practice and Guiding Principles for Women Offenders, The Gender-Responsive Strategies Project: Approach and Findings, Washington, DC: National Institute of Corrections\nBloom, B., Owen, B., and Covington, S., (2003) Research, Practice and Guiding Principles for Women Offenders, US Department of Justice, National Institute of Corrections\nBrave Hears, M.Y.H., (April 22 2005) From intergenerational trauma to intergenerational healing, Keynote address at the Fifth Annual White Bison Wellbriety Conference, Dever, CO\nGovernment of South Australia, (2014) Strong Foundations and Clear Pathways, Women Offender Framework and Action Plan, June 2014 – June 2019\nIndig, D., Gear, C., and Wilhelm, K., Comorbid substance use disorders and mental health disorders among New Zealand prisoners, New Zealand Department of Corrections, 2016\nMiller, N., and Najavits, L., (2012) Creating trauma informed correctional care: a balance of goals and environment, European Journal of Psychotraumatology\nPatterson, M., Uchigakiuchi, P., and Bissen, T., (2013) Can Prison be a Place of Healing? The Trauma Informed Care Initiative at the Women’s Community Correctional Centre, Hülili: Multidisciplinary Research on Hawaiian Well-Being, Vol 9\nSubstance Abuse and Mental Health Services Administration (SAMSHA) (US), (2014) Trauma Informed Care in Behavioural Health Services, Treatment Improvement Protocol Series no 57, Rockville (MD), accessed at www.ncbi.nlm.nih.gov/books/NBK207191\nSubstance Abuse and Mental Health Services Administration (SAMSHA), (2012) SAMHSA’s working definition of trauma and principles and guidance for a trauma-informed approach. Rockville, MD: Substance Abuse and Mental Health Services Administration\nWilton, Z. (2012) What Works for Women in Offending Behaviour Treatment Programmes, A Rapid Evidence Literature Review, New Zealand Department of Corrections\nWright, E.M., Van Voorhis, P., Salisbury, E.J. and Bauman, A., (2012) Gender-Responsive Lessons Learned and Policy Implications for Women in Prison: A review, Criminal Justice and Behaviour, 30:1612']"	['<urn:uuid:7b2ba8cc-6b69-4e92-8d23-3a3b6aadcbe9>', '<urn:uuid:419ecedd-6c39-4d8d-9af2-6d1a6cd9cc36>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:35:02.547110	21	143	2922
65	what happens to stress hormone levels during long ongoing problems	During long-term chronic stress, cortisol levels initially remain consistently higher throughout the day. However, eventually the body becomes unable to maintain this high cortisol production as nutrients and precursors become depleted. This leads to declining levels of cortisol and other hormones like aldosterone, testosterone, and epinephrine. The normal daily pattern of cortisol production becomes disrupted, causing late-evening spikes and symptoms such as fatigue, insomnia, lack of enthusiasm, and reduced vitality.	['Stress is how the body reacts to a real or imagined stressor — a stimulus that causes stress. Acute stressors affect a bodily organ in the short term; chronic stressors over the longer term. Chronic stress is the state of prolonged tension from internal or external stressors which may cause various physical manifestations such as asthma, back pain, arrhythmias, fatigue, headaches, irritable bowel syndrome, ulcers, and suppression of the immune system. Chronic stress takes a more significant toll on the body than acute stress. It can raise blood pressure, increase the risk of heart attack and stroke, and induce symptoms of anxiety and depression.\nThe Three Stages of Stress – From Acute to Chronic\n- Alarm: In this first stage, when the threat or stressor is first identified or realized, the body’s stress response is in a state of alarm. During this stage, adrenaline is produced in order to bring about the flight-or-fight response, causing sweating, raised heart rate, etc. The body’s resistance to the stressor drops temporarily below the normal range and some level of shock may be experienced. There is also some activation of the HPA Axis, producing cortisol, as discussed in our last post.\n- Resistance: If the stressor persists, the body must find some means of coping with the stress. Although it begins to try to adapt to the strains or demands of the environment, the body cannot keep this up indefinitely, so its resources are gradually depleted. As it attempts to cope with the condition that is causing the stress, the mind may try to focus on the problem, which can actually exaggerate the awareness of the problem and make it seem difficult to overcome.\n- Exhaustion: third stage. At this point, all of the body’s resources are eventually depleted and the body is unable to maintain normal function. The initial symptoms may reappear (sweating, raised heart rate, etc.). Long-term damage may result, as the body’s immune system becomes exhausted, and bodily functions become impaired. The result can manifest itself in obvious illnesses such as ulcers, depression, diabetes, digestive system problems or cardiovascular problems. It can also manifest as a chronic pain syndrome, guarding/avoidance behavior, and/or sleep disturbance. Hopelessness can set in.\nChronic Stress and Cortisol\nWhen the body’s HPA-axis cannot overcome a challenge and/or is chronically exposed to a threat, this system becomes overtaxed and can be harmful to the body and brain. An increased level of cortisol is one of the most dangerous outcomes of chronic stress.\nCortisol is an important hormone in the body, secreted by the adrenal glands and involved in some of the following functions: proper glucose metabolism, regulation of blood pressure, insulin release for blood sugar maintenance, immune function and inflammatory response. Normally, cortisol is present in the body at higher levels in the morning and is at its lowest level at night. Although stress is not the only reason that cortisol is secreted into the bloodstream, it has been termed “the stress hormone” because it’s also secreted in higher levels during the body’s ‘fight or flight’ response to stress, and is responsible for several stress-related changes in the body. Small increases of cortisol have some positive effects: a quick burst of energy for survival reasons, heightened memory functions, a burst of increased immunity, lower sensitivity to pain, and helping to maintain homeostasis in the body.\nPeople are biologically ‘wired’ to react differently to stress.\nWhile cortisol is an important part of the body’s response to stress, it is important that the body’s relaxation response be activated so the body’s functions can return to normal following a stressful event. Unfortunately, in our current high-stress culture, the body’s stress response is activated so often that the body doesn’t always have a chance to return to normal, resulting in a state of chronic stress, thus producing high chronic cortisol levels.\nHigher and more prolonged levels of cortisol in the bloodstream like those in chronic stress have been shown to have negative effects, such as:\n- Impaired cognitive performance (loss or poor concentration, inability to complete tasks or heightened confusion in mildly stressful situations\n- Suppressed thyroid function\n- Blood sugar imbalances such as hyperglycemia\n- Decreased bone density\n- Decrease in muscle tissue\n- Higher blood pressure\n- Lowered immunity and inflammatory responses in the body, slowed wound healing, and other health consequences\n- Increased abdominal fat, which is associated with a greater amount of health problems than fat deposited in other areas of the body.\nWhen people feel stressed, stress hormones can be over-secreted, dramatically affecting the brain. Cortisol also plays a large part in post-traumatic stress disorder (PTSD) and memory. In a 2002 article in Biological Psychiatry regarding cortisol, PTSD and memory1, cortisol was noted to work with epinephrine (adrenaline) to create memories of short-term emotional events. This effect may serve as a means to help a person remember what situations to avoid in the future. However, long-term exposure to cortisol damages cells in the hippocampus and can create impaired learning ability. It has been shown that cortisol inhibits memory retrieval of already stored information.\nCortisol secretion varies among individuals. People are biologically ‘wired’ to react differently to stress. One person may secrete higher levels of cortisol than another in the same situation. Studies have shown that people who secrete higher levels of cortisol in response to stress also tend to eat more food, and food that is higher in carbohydrates, than people who secrete less cortisol.\n1“Depression. What happens in the brain?” Biological Psychiatry, 2002', 'Cortisol is a steroid hormone that is responsible for a wide range of processes within the body including immune responses, the regulation of metabolism, and acting as an anti-inflammatory. It also plays an important role in the way in which the body responds to stress. It is created in the cortex of the adrenal glands and is transported and released through the body by our blood.\nThe balance of cortisol levels that are present within the body at any one time is extremely important for our overall health. Having too much and too little cortisol in your system can cause a number of issues that range from minor to serious. The effects of having too much cortisol in the body can include rapid weight gain, high blood pressure, muscle weakness, and severe mood swings that manifest in anxiety and depression. On the other hand, individuals who possess an extremely low amount of cortisol are susceptible to experiencing problems such as dizziness, fatigue, weight loss and a peculiar darkening in certain skin areas.\nFortunately, the body possesses a clever system that enables us to regulate the levels of cortisol that are present in our systems. The optimal level, interestingly, varies quite drastically at different times throughout the day.\nCortisol levels are generally high in the morning as we wake from a prolonged period of sleep, with an increase of up to fifty percent in the twenty to thirty minutes after waking. This is known as the ‘cortisol awakening response’. Then, as the day progresses, our cortisol levels naturally begin to drop in a fairly constant and regular fashion that is termed a diurnal rhythm, ending up as low in the late evening. This allows the body to keep a regular sleeping pattern, with the cortisol level dropping for periods of sleep, then replenishing during the following morning.\nThe body can also detect and change the timing and cycle of cortisol production and release for certain individuals. A great example is those individuals who work on night shifts. In these cases, the pattern and timing of the release of cortisol is reversed to allow for higher levels throughout the late evening and early morning hours. For those of you who travel long distances, a similar rearrangement occurs when we experience jetlag.\nCortisol levels are not just dependent on the time of day. Stress also plays an important role. The exact response depends on the type of stress, whether its short-term acute stress or long-term chronic stress. For short-term stressors like an argument or a fall, we will see a brief spike in cortisol. For longer term stressors like work stress or illness, we see a consistently higher level of cortisol at all times of the day.\nHow does all this relate to Adrenal Fatigue? Well, those long-term stressors can eventually deplete the nutrients and precursors that we need to produce cortisol and other hormones. In other words, chronic stress will raise your cortisol levels for a while, but eventually your body is unable to continue producing cortisol in such high amounts. At this point we start to see declines in not only cortisol, but also key hormones and neurotransmitters like aldosterone, testosterone, epinephrine and more. Additionally the diurnal rhythm of cortisol production is often disrupted, resulting in late-evening spikes that cause insomnia.\nThis dysregulation of hormone production is the ‘burnout’ that follows long periods of stress. The body is no longer able to regulate cortisol levels effectively, which leads to symptoms like fatigue, a lack of enthusiasm, insomnia, and a general lack of vitality.\nHow can we reverse the effects of long-term stress and regain our energy levels? A combination of good nutrition, supplementation, and effective stress management techniques can quickly result in some significant changes. In the longer term, eliminating the causes of stress is crucial for a full recovery.']	['<urn:uuid:d7c94324-42c7-4519-9ffb-30768ad1d960>', '<urn:uuid:16bc5d31-8d70-4046-945c-c85fccfe31ae>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T20:35:02.547110	10	70	1556
66	lettuce production inside buildings vs corn soybeans environmental impact	Indoor lettuce production is much less efficient than growing crops in natural sunlight. Growing crops indoors requires impossibly high amounts of energy for artificial lighting, especially for nutrient-dense crops like grains. It would be wasteful to convert sunlight into electricity (through wind turbines or solar panels) just to power artificial lights for plants that could grow better in direct sunlight. Instead of focusing on indoor farming, a more effective way to protect the environment would be to stop degrading millions of acres of land used for growing corn and soybeans for biofuels and cattle feed, and restore these areas as grasslands.	['Why, after more than a decade, does the idea of “vertical farming” keep gathering momentum? Why hasn’t it collapsed under its own weight of illogic? And why is media coverage of vertical farming almost universally positive, often enthusiastically so?\nI suppose we shouldn’t be surprised when a fantasy persists and thrives despite being unrealistic; after all, that’s what fantasies do. And the vertical-farming concept, unlike, say, creationism, aims at worthy goals. But when a pipedream comes to be regarded, wholly uncritically, as a means of fixing our broken food system, it becomes a dangerous distraction.\nOut here in Kansas, for example, farmers and agribusinesses often back up their resistance to much-needed systemic change by claiming that America’s urban-suburban majority has no understanding of what it takes to produce food. And when they learn that city people are wanting to stack fields of crops one above the other, you can be sure that their convictions are reinforced.\nVertical farming, as originally conceived by Dickson Despommier, a professor of public health at Columbia University, would involve using the floorspace of tall urban buildings for growing food plants through largely hydroponic methods. This is envisioned as a way to integrate food production with dense human populations, increase production per unit of land area, protect crops against pests without the use of chemicals, and take vulnerable agricultural soils out of production by relocating crops to cities. It can, in fact, achieve none of these goals.\nThe most obvious problem is that of scale. Despommier’s initial, commendable objective was to help prevent soil degradation. But to benefit the continent’s agricultural soils on a meaningful scale would require substituting floorspace in buildings for a substantial share of cultivated land. Otherwise, vertical farming would simply be adding a little bit to production without taking the burden off the countryside.\nBut what if we committed to making at least a modest start anyway, by converting U.S. vegetable production from a horizontal to a vertical enterprise? Vegetables (not counting potatoes) occupy only 1.6% of our total cultivated land, so that should be no problem, right? Wrong. At equivalent yield per acre, we would need the floorspace of 105,000 Empire State Buildings. And that would still leave more than 98 percent of our crop production still out in the fields.\nIt is indeed vegetables, mostly leafy ones, that are envisioned in many vertical farming plans. That is appropriate, because such plants do not suffer as much from sunlight deprivation as do other crops. And the bulk of the plants in a true vertical farm will be deeply sunlight-deprived. If it reaches plants at all, direct sunlight would be reduced in intensity by the glass itself; the light would strike only a few plants at a time—those near the glass walls—usually at a low angle and only for a portion of the day; and those peripheral plants would shade plants that are deeper into the room. (Windows can admit enough light for to see your way around the center of a room on a sunny day; however, vision and photosynthesis are very different processes. Crop plants, if they’re expected to produce a harvest of food, require many times as much light as you require indoors.)\nBreathless reports of salad greens being grown indoors under artificial light are becoming a staple of the food media. But my colleague David Van Tassel and I have done simple calculations to show that grain- or fruit-producing crops grown on floors one above the other would require impossibly extravagant quantities of energy for artificial lighting. That’s because plants that provide nutrient-dense grains or fruits have much higher light requirements per weight of harvested product than do plants like lettuce from which we eat only leaves or stems. And the higher the yield desired, the more supplemental light and nutrients required.\nHighly efficient LED lighting would improve the overall system only marginally. Meanwhile, Despommier has suggested that vertical farms’ electrical demand be met with renewable sources. But to divert any portion of that tiny amount of sunlight that we manage to harvest and convert into electricity—with wind turbines, photovoltaic arrays, etc. (all arranged horizontally, by the way)— and then, in essence, convert the electricity back into light for illuminating plants that would have grown much better out in the free sunlight in the first place, is something only a society wallowing in a huge energy surplus would even consider. Green plants themselves are solar collectors and should be treated as such.\nAs the vertical-farm idea’s increasingly obvious flaws are pointed out, proponents respond with a lot of tweaking. And as plans evolve, they are looking both less vertical and less like farms (as when they involve growing food on Ferris wheels). Some believers who’ve caught onto the lighting problem are now talking about giant pyramids or terraced lean-tos that would expose all plants to light from above. That makes life a lot better for the plants, but such Hanging-Gardens-of-Babylon designs, like roof gardens or “green wall” arrangements that would display plants on the south faces of buildings, do not augment land area in the way intended by the original vertical-farm design. Whereas a bona fide vertical farm, if it could work, would in theory multiply the cropped area many times over, a pyramidal or diagonal structure would provide no more effectively sunlit cropped surface than a vacant lot of similar size, and at a vastly greater cost.\nLighting is only the most, um, glaring problem with vertical farming. Growing crops in buildings (even abandoned ones) would require far more construction materials, water, artificial nutrients, energy for heating, cooling, pumping, and lifting, and other resources per acre than are consumed even by today’s conventional farms—exceeding the waste of those profligate operations not by just a few percentage points but by several multiples. Vertical enthusiasts also claim that crops grown in buildings chemical-free will somehow be protected from diseases and pests, but as anyone who has worked in a greenhouse can tell you, epidemics and infestations can explode into total losses overnight on plant grown in confinement.\nAnd raising crops in such restricted spaces would, necessarily, mean substituting a lot of human labor for much of the mechanical power now used in farming. That’s fine environmentally, but who will own these enormous high-tech facilities, how much of the hard work—hauling, transplanting, tending, harvesting, more hauling—will be done by idealistic entrepreneurs, and how much will end up being carried out by the same underpaid, overexploited people who do all that grueling stoop work that currently provides us with most of our vegetables and fruits today?\nThe goals of the vertical-farming concept are generally laudable (Despommier’s Wikipedia-page photo features a slide showing the word “hunger” canceled with a red circle-and-slash) but it has virtually no potential for saving soil or strengthening food security. It’s just another proposal (if probably the most high-input one) for urban agriculture, a practice intended to reduce the distances that food is transported while supporting local economies. There’s no doubt that local fruit and vegetable production is good for consumers. But even if we planted every urban flat roof while deforesting and farming all of America’s front and back yards and open urban spaces, we could supply only a tiny portion of the nation’s food supply. Add in those 105,000 Empire State Buildings full of vegetables, and we’d still have well over 95 percent of food being produced outside of cities.\nAnd ecological impact cannot be estimated simply by counting food miles; food’s ecological footprint lies mostly in production, not transportation. Were vertical-farm planners to add up the enormous quantities of energy and materials required for construction, maintenance, operation, and eventual dismantling, they would be forced to conclude that the structures they’ve envisioned can succeed only in supplying the more affluent city-dwellers with leafy salads.\nIf we want to protect North America’s soils, the most effective immediate action would be to stop degrading scores of millions of acres every year to raise corn and soybeans for making biofuels and feeding cattle. Those landscapes should be restored as grasslands (and, eventually, mixed stands of perennial grain crops.)\nThe current corn-and-soybean system serves only to keep animals alive and unwell in confinement, with an extraordinary waste of resources. But if we want to resolve the myriad problems currently created by factory-farming animals in sheds and feedlots, we’ll never do it by factory-farming plants in skyscrapers.\nStan Cox’s book Any Way You Slice It: The Past, Present, and Future of Rationing, will be published in May by The New Press. Write to him at firstname.lastname@example.org.']	['<urn:uuid:0a546f98-eab1-4651-981c-e56ff2eaf4a5>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	9	101	1425
67	What's the difference between normal and breathing roots?	Normal roots primarily function to absorb water and mineral salts, anchoring the plant in soil. In contrast, breathing roots (pneumatophores) grow vertically up from water or marshy soil like conical spikes around tree trunks, containing numerous pores (breathing pores) for taking in air for respiration, as seen in species like Rhizophora. These breathing roots are specially adapted for plants growing in oxygen-poor environments.	"[""They also mention a strong characteristic of roots, which is the presence of a root cap. The roots appear like conical spikes coming out of the water. 398 Rhodora [Vol. Referring to the species Phenakospermum, he writes: ''It is one of the dogmas of elementary botany that the rhizome is morphologically equivalent to a stem modified as a horizontal, storage and propagating organ. Their primary function is to provide support to the plant. Conical fleshy roots occur in Sweet potato Dahlia Asparagus Carrot Answer: 4 Q4. In choosing this label, I took into account the fact that angiosperms have roots, stem and leaves, like Pteridophyte, and so rhizophore seemed an appropriate choice. The root surface has hundreds of lenticel openings, like the pneumatophores in Avicennia and Laguncularia, and knee roots of other species. Napiform roots are recorded form Radish Carrot Beet … Abundant pores, called lenticels, on root surfaces connect to spongy internal tissues that can hold large volumes of air. Money plant, black pepper (kali mirch), betel (pan). 16. International Journal of Plant Sciences, 153(3, I):394-399. The mangrove has three basic organs, like any other plant, the stem, leaf and the root. [ Links ], TOMLINSON PB. The black mangrove ( Avicennia germinans ) and the white mangrove ( Laguncularia racemosa ) are able to take up seawater through their roots, but they excrete excess salt through pores, or salt glands, located on the surface of leaves. Biotropica 9: 145-155. The adult root system. 15), appearing inside the endodermis. Die Nadel der Schwarzkiefer - Massenprodukt und Kunstwert der Natur. e.g. Figure 1 (from Werner and Stelzer, 1989) depicts Na+ and K+ concentrations measured at various layers of root tissue, starting at the growth medium and going inwards to … However, the absence of leaves and nodes, which are characteristics of roots, supports the idea of a rhizophore as an intermediate organ between root and stem (Goebel 1905). 22) with exarch protoxylem (Fig. These roots are also called assimilatory roots due to their partial capacity of photosynthesis. Studies ofthe growth of Red Mangrove (Rhizophora mangle L.) 2. Leaves measure up to 8 cm in length and 5 cm in width. Maize and sugarcane have prop roots. When sea-level rises, plants are exposed to increased salinity, as well as tidal flooding. These are non-green and non-photosynthetic The results obtained are in good agreement with their position in the lower, flooded zone of natural mangrove communities. Mangrove vegetation. [ Links ], HALLÉ F, OLDEMAN RAA AND TOMLINSON PB. [ Links ], CHAPMAN V J. Cuscuta, Orobanche, Viscum, Lorathus. Environment have badly affected the physico-chemical soil, especially soil redox potential, pH and availability of O 2 in the soil 6, the accumulation of CO 2, induces the anaerobic decomposition of organic matter and reduce iron and manganese 7. Rhizophores in Rhizophora mangle L: an alternative interpretation of so-called ''aerial roots'', Instituto de Biociências, Universidade de São Paulo, Caixa Postal 11461, 05422-970 São Paulo, SP, Brasil. This buoyant germinated seed is the first stage of the root system. Due to its anatomical structure, the rhizophore is not a root in the exact sense''. Such structures are derived from stems; thus, the term ‘rhizophore’ is more appropriate than stilt, prop or aerial root (De Menezes, 2006), and we will use it hereafter. Their studies greatly improved our understanding of mangrove roots because they stressed the changeable nature of living roots rather than treating the topic 397. 1992. They are present at the basal part of the stem and spread in different directions in the soil. Such plants e.g. In a mangrove forest in southern Thailand, the root biomass was estimated for the Sonneratia zone, the Sonneratia-Bruguiera ecotone, the Bruguiera zone and the Rhizophora zone. In mature Rhizophora, the trunk of the tree is completely suspended above the water by the arcing stilt roots. also allows them to quickly trap sediment and build soil to match up with the global sea-level rise,” said Prabakaran. Red mangroves (Rhizophora… The adult root system. Acta Sci Nat Univ Sunyatseni. In other words, unlike roots (except for the radicle), they are not endogenous in origin. They grow horizontally in response to gravity and hence referred as diageotropic similarly, they show branching pattern and hence they are also referred as plagiotropic. 13), a phellogen can be seen, which produces a protective periderm. Above the root layer there is a layer of mangrove muck homogenized by the bioturbation that results from the activities of burrowing crabs and other detritivores. Pitot (1958, p. 1118) also states that: ''the transformation of an aerial organ into an underground organ is observed; from endarch bundle into alternate exarch bundle, with a rhizophore root structure. Presença de rizóforo em espécies de Vernonia da Serra do Cipó. Rhizophoraceae. As such, front roots must have a relatively high competition rate among epibiont genera. Worth Publishers, Inc. New York. They take part in transport of absorbed water and minerals to shoot system. Root adaptations make it possible for mangroves to live in the soft sediments along the shoreline Root adaptations increase stability of mangrove trees in the soft sediments along shorelines. Modifications of Tap Root for Storage of Food: Taproots of carrot, turnip and adventitious roots of sweet potato, get swollen and store food. Above all, attention isdrawn to the fact that the root has a subapicalmeristem similar to all mono- and dicotyledonous roots, while the rhizophore does not present thesame type of subapical region. e.g. Photo by Nehru Prabakaran. According to Goebel (1905), this organ is neither root nor stem, but a sui generis organ, half way between stem and root. Huge collection, amazing choice, 100+ million high quality, affordable RF and RM images. E.g. Rhizophora mangle L. Bot Jahrb 4: 519-548. Even when secondary growth has been fully established (Fig. (1996), to be the most speciose. We shall study the modification of roots for tap root system and adventitious system. O sistema subterrâneo em Dioscorea kunthiana Uline and R. Knuth (Dioscoreaceae). Studies ofthe growth of Red Mangrove (Rhizophora mangle L.) 1. This results in lower pressure within the root system. Stout, large arching prop roots are characteristic of the species, which support the main trunk and contain numerous lenticels (air pores) on their surfaces. Grasses, sedges, momordica. Later, while studying members of Dioscoreaceae (Rocha and Menezes 1997) and Smilacaceae (Andreata and Menezes 1999), a comparative analysis of these two families led the discovery that the tuberized structures are indeed rhizophores, as in Vernonia. Bot Acta 103: 296-304. They are found in diverse habitats like aquatic, terrestrial and epiphytes. epiphytes orchids. Pneumatophore s are specialized root structures that grow out from the water surface and facilitate the aeration necessary for root respiration in hydrophytic trees such as many mangrove species (e.g., Avicennia germinans and Laguncularia raecemosa), bald cypresses, and cotton (tupelo) gum (Nyssa aquatica). Bull Inst Fr Afr Noire 20: 1103-1138. 7). On the other hand, these branches do not form leaves and in this respect they are similar to roots. Shallow widespreading roots, surrounds the trunks of black mangroves, adding to the structural stability of the tree. One of the most striking features of this species is the presence of structures that expand its supporting base. Napiform roots are recorded form Radish Carrot Beet … Photo courtesy South Florida Water Management District. underground root mass of Rhizophora becomes so dense that an anaerobic peat often develops. They provide support to the plant as the ropes provide to the tent. Half of the fleshy part is derived from the hypocotyl, Spherical at the base and sharply tapering towards the apex (top shaped or globular). 2, a sympodial branching with leaves and the basal system of rhizophores, also with sympodial branching. [ Links ], GILL AM AND TOMLINSON PB. 1988. Phenology of the shoot. On tidal submergence, the lenticels close, oxygen is used by the plant and carbon dioxide is absorbed into the seawater. A 0.2 m (width) × 15.5 m (length) × 1.0 m (depth) trench was dug between two trees of Rhizophora apiculata and the roots in the trench harvested, rinsed, sorted by diameter and weighed. Roots in some plants change their shape and structure and become modified to perform functions other than absorption and conduction of water and minerals. Chen, L. and Wang, W., 2017. One is tempted to compare the rhizome in the Scitamineae with Stigmarian axis of Lepidodendron and the rhizophores of Selaginella similar organs, the morphology of which is unexplained''. In addition, Rhizophora mucronata is commonly used in the tropical mangrove replantation programmes due to its adaptability and rapid growth rate. These are dispersed … Habit and general morphology. Rhizophora growing in swampy areas (mangroves), many roots come out of the ground (negatively geotropic) and grow vertically upwards. 25) in certain regions of the cell.\nEugenia Hedge Disease, Who Sings Oh, What A Night, Dlt Trading Spyderco Exclusive, Safety Engineer Resume, Korean Laundry Symbols, Auckland Weather In March 2020, Pink Lady Apple Recipes, Microsoft Azure Virtual Training Day: Fundamentals Part 1, Zion Lutheran Church Records,"", '1. The tap root system bearing secondary, tertiary and other lateral roots ramify more and more in the soil and anchor the plant very firmly.\n2. Root hairs absorb water and mineral salts from the soil and send it to the upper part of the plant.\nKINDS OF ROOTS:\nA. Tap root system:\nA root system which develops from the radicle is called tap root system, serves the normal functions such as anchorage and absorption.\nB. Adventitious root system:\nThe root developing from any portion of the plant other than the radicle is known as adventitious root. It may develop from the base of the stem or from any node or inter node of the branch, or even from the leaf. They serve the normal functions such as anchorage, absorption and conduction of solutes and water. Adventitious roots carrying on normal functions arc of the following types.\n(i) Fibrous roots:\nCluster of fiber like roots developed from the base of the plumule or from its lower nodes arc known as fibrous roots. Such types of roots are seen in rice, maize, etc.\n(ii) Foliar roots:\nThese roots develop directly from the leaf, from the petiole, or the vein. Ex. Petiole of rubber plant and leaf margin of Bryophyllum.\n(iii) Adventitious roots:\nThese roots develop from the nodes and sometimes from the internodes of the stem, e.g. Oxalis. Stem cuttings of rose, sugarcane, China rose, marigold, etc., give rise to such type of roots.\nModification of roots:\nRoots may be modified to different shapes in order to perform special functions besides their normal functions. Root modification is found both in tap root and adventitious root.\nA. Tap root modification:\nTap root system is modified into different forms to carry out special function such as storage of food such a root is known as modified tap root. It is differentiated into following types:\nIt is a modified tap root. The primary root is swollen in the middle while both the ends gradually taper forming a spindle shaped structure, e.g., Radish (Raphanus sativus).\nThe hypocotyls region is considerably swollen, becoming almost spherical and then abruptly tapering towards the lower end, e.g., Turnip (Brassica campestris var. rapa) and beet (Beta vulgaris).\nThe roots swell, becoming broad at the base and gradually taper towards apex forming a cone like structure, e.g., Carrot (Daucus carota).\n(iv) Tuberous or tubercular:\nIn this case, the root is thick and fleshy but does not form any definite shape, e.g., Mirabilis jalapa.\nThese roots grow vertically up and come out of the water or marshy soil like conical spikes. They occur in large numbers around the tree trunk. Such roots are provided with numerous pores (breathing pores) through which air is taken for respiration, e.g., Rhizophora.\nB. Adventitious root modification\nRoots produced from any portion of the plant other than the radicle is called adventitious roots. It may be changed to different forms for special functions such as storage, mechanical support and vital functions.\n(a) For storage of food:\n(i) Tuberous root: In this case, adventitious roots grow from the nodes of the running stem. These roots swell and get modified to irregular forms, known as tuberous. These are produced singly, e.g., Sweet potato (Ipomoea batatas).\n(ii) Fasciculate root:\nIn Dahlia, Asparagus the tuberous roots are developed from base of the stem! in clusters.\n(iii) Nodulose root:\nThe underground stem produces adventious roots. Some of the slender roots become suddenly swollen near the apex forming nodule like structures, e.g.] Mango-ginger (Curcuma amada).\n(iv) Moniliform or Beaded root:\nSwellings are found in the root at frequent intervals. It gives rise to a beaded or moniliform appearance, e.g., Basella, Momordica, Vitis trifolia, Dioscorea alata and many grasses.\n(v) Annulated roots:\nIn this type, the root has a series of ring-like swellings appearing as if formed by number of discs placed one above another, e.g., Cephalis ipecacuanha.\n(b) For mechanical support:\n(i) Prop roots: The plant has a tap root system. The horizontal branches of the stem give rise to aerial roots that hang vertically downwards. On reaching the ground, these grow and act like pillars providing mechanical support to the plant, e.g., Ficus bengalensis.\n(ii) Stilt roots: The plants like screwpine have its own root system, but the anchorage is not very strong. Therefore, it develops adventious roots from near the base of the stem which grow obliquely downwards, act like stilt and provide mechanical support (Pandanus). The maize plant also produces adventitious roots from the lower nodes which also act in a similar way.\n(iii) Climbing roots:\nThese are found in some climbers. Aerial roots are developed from the nodes. These roots twine around and clasp the support and help in climbing, e.g., Betel (Piper betel), Piper nigrum and Pothos, etc.\n(iv) Clinging roots:\nThese roots are seen in orchids, which are developed from the stem base and enter the crevices of the support to fix the epiphyte. It provides mechanical support to the plant.\n(v) Root buttresses:\nLarge trees have great plank-like roots that radiate from the base of the stem. These consist of partly root and partly stem, e.g., Bombax .\n(c) For vital functions:\n(i) Sucking roots or haustoria:\nCuscuta, a leafless, parasite, non-Greek plant always grows in association with other plants. At the point of attachment with the host plant it produces some knob like roots known as haustoria which penetrate into the host plant and draw nourishment.\n(ii) Epiphytic roots:\nOrchids are the plants which grow perched on branches of trees. They firmly attach themselves to the trunk of suitable trees by special roots known climbing roots. The epiphytes also have aerial absorbing roots. They remain fleshy and grow along with the clinging roots. They have a spongy vela men tissue which absorbs atmospheric moisture. They also have chlorophyll and perform photosynthesis.\n(iii) Floating roots:\nThese roots are found in aquatic plants like Jussiaea. These are adventitious roots developing at the nodes of the floating branches. The roots are spongy and colorless. They remain above the water level. Being soft and spongy they store air which help the plant in floating. They also facilitate respiration.\n(iv) Assimilatory roots:\nNormally roots are non-green in color. But the adventitious roots developed from the branches of Tinospora cord folia being green in color carry out photosynthesis and are called assimilatory roots. These roots are slender, long and in hanging state.']"	['<urn:uuid:e9e34b87-1584-4ee1-a4e6-7d487009020b>', '<urn:uuid:45fb748f-a359-4541-8170-44325b26ed41>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T20:35:02.547110	8	63	2562
68	Which causes more coverage issues: misrepresenting home vacancy or incorrect candy cane coral water parameters?	Misrepresenting home vacancy is more serious for coverage issues, as it can completely void insurance coverage and result in denied claims, potentially leaving you without protection for major incidents like fires. While incorrect water parameters can harm candy cane corals, this is primarily a maintenance issue that can be corrected by adjusting calcium (350-420ppm), pH (8.1-8.4), and temperature (75°C-82°C) levels.	['Cheapest Home Insurance May Not Be Clever At All!\nEveryone is looking for cheap house insurance quotes — and many will go through great depths in order to find it, whether legal or not. For those you may ponder about lying to an insurance company in order to try to cover an incident or simply buy cheap insurance, misrepresentation may simply leave you without coverage and insurance fraud may land you without the benefit of your freedom; the least penalty would be a cancelled policy on record with the inability to obtain insurance again from a preferred insurance carrier:\n- Who Resides In Dwelling (Owner, Tenant, or is it vacant)? The true answer determines the type of policy required to cover the home; the wrong type of policy would surely void coverage by definition. Imagine the house catching fire followed by a denied claim by the insurance carrier perhaps accompanied with a check for your premium refund. Now, this instance could occur due to misrepresentation or by a simple change in occupancy without reporting the change to the insurance carrier. It is important to understand the insurance contract which you are paying for; too many clients are constantly shopping around for the cheapest premium with ABSOLUTELY NO REGARD for the coverage and terms provided. Many of these clients would be better off uninsured.\n- Do You Have a Dog (if so, what breed)? Each carrier has a list of dog breeds which are their prohibited breeds; all lists include the verbiage ‘any mix of the above’ and ‘any dog with prior aggressive history’. Claims due to dog aggression comprise approximately 30% of liability claims on property insurance policies; it is much more likely to present the carrier with a dog bite claim than a house fire. Insurance is all about risk; misrepresentation may result in a denied claim and a canceled policy. What use is saving a few dollars on your home insurance policy when you could’ve stayed with your prior carrier who permitted your breed of dog? Insurance is serious business; the application process and insurance contract will be closely adhered to when a claim is presented – this is what is expected and ultimately will happen!\n- Do You Have a Trampoline? Insurance companies are not trampoline haters, there is a reason for each question asked! The presence of a trampoline greatly increases the risk for bodily injury on the insured property. If someone is hurt on your trampoline; whether falling off or landing on his/her own weight within the trampoline, this could easily land you a lawsuit potentially causing you out of pocket medical expenses and legal fees if you haven’t disclosed the trampoline to your carrier. Although most insurance companies will not accept homes with trampolines, there are a few carriers who will accept an in ground trampoline surrounded by a protection netting preventing people from falling out and further injuring themselves.\n- Have You Had Any Prior Claims? Prior to purchasing homeowners insurance, all insurance carriers will pull the CLUE report and utilize any type of available investigative tools which will help underwrite the risk. A minute percentile of claims may go unreported but this would be no gift if you were to withhold the truth from a new insurance carrier; lying could result in your home insurance claim being denied if a loss were to occur or if the carrier were to find out at a later date, your insurance would be cancelled for misrepresentation.\nInsurance carriers are not your enemy; insurance policy contracts are standardized to offer uniform protection to the consumer and regulated by the Department of Insurance in each State. The system is designed to share your risk amongst a larger segment of the community but not to allow any one person to take advantage of the multitude.\nTrying to mislead the underwriting process as a way to get cheap house insurance. Since the purpose of insurance is to cover you in the event of a loss, the transparent honesty is the only way to go. The best option is to review all available discounts, perhaps remove any optional coverage, purchase lower limits of required package coverage and observe lower premium rates for higher deductible options. For the future, consider keeping your insurance record claims free as all carriers have special discounted rates for claims free customers as well as the obvious of not being charged for any incident surcharges. As of late, most insurance carriers do review property claims for the past 60 months although some carriers claim only to surcharge incidents which have occurred during the past 36 months.', 'Caulastrea furcata, often known as candy cane, trumpet, or bullseye coral, is a bright and colorful LPS (large polyp stony) coral. Characterized by striped polyps inspiring its common name, it can be found in vivid yellow, green, red, or blueish brown. Each branching polyps contains a neon green mouth. Some common varieties available to purchase are Kryptonite, Orange, Green, and Alien Eye Candy Cane Coral.\nCandy Canes originate from the Indo-Pacific Ocean from Fiji to Australia and the surrounding waters. In the wild, they can usually be found in shallow waters, tide pools and even lagoons. Though they prefer moderate water flow and lighting, they can flourish in a variety of conditions making them perfect for a beginner hobbyist.\nIn this article, we will talk you through everything you need to know to care for your Candy Canes and keep your saltwater aquarium looking vibrant and healthy.\nCandy Cane Coral Care\nCandy Cane Coral is extremely easy to keep as it is a hardy coral, meaning it’s resistant to variations in lighting, flow, and other minor changes in its environment. This makes it forgiving to mistakes commonly made by novices and a great choice whether you are experienced or just starting out.\nAre Candy Cane Coral Aggressive?\nCandy Cane Coral is considered not very aggressive since it has relatively short (2 inch) sweeping tentacles, unlike many other LPS corals. However, it could still sting corals which are close by, so it is important it has plenty of room to grow. As it is one of the fastest growing corals you should aim to give it around 6 inches of space at a minimum.\nCandy Cane Coral Placement\nThe best placement for Candy Cane Coral is in an area of moderate lighting and flow, although it can also thrive in low flow or light intensity. For this reason, it is ideally placed at the bottom of your tank and no higher than the middle. You can place it directly on the sand bed as sand is its preferred substrate.\nIf the flow is too high, it can damage the fleshiness of the polyps; too low and the coral will need more help with feeding, although this isn’t an issue if you plan to feed it regularly yourself. To ensure it takes well to your desired placement it’s recommended to start with a darker and lower flow part of your tank and gradually move it over the course of a few weeks.\nCandy Cane Coral Lighting Requirement\nCandy Cane Coral does not have a high lighting requirement, responding best to low-to-moderate intensity. Anything from 30 to 150 PAR is acceptable, though between 50-70 is recommended. Avoid more than 150 though as light intensity that’s too high could bleach or irritate the coral.\nPlacing your Candy Cane Coral towards the edges of your tank can help to soften the intensity it’s exposed to. Alternatively, a shaded area can be a good spot to provide the right conditions too.\nCandy Cane Coral Temperature\nThe temperature preferred by Candy Cane Coral is between 75°C-82°C/24°C-28°C. Higher temperatures cause thermal stress, resulting in much of the zooxanthellae algae being expelled and depriving the coral of the nutrients they provide.\nCandy Cane Coral pH\nA pH range of 8.1-8.4 is ideal for Candy Cane Coral. The calcium levels should also be kept between 350-420ppm. As it is an LPS coral, these parameters are essential to prevent erosion of the coral’s hard skeleton and to keep it strong and healthy.\nCandy Cane Coral Growth Rate\nCandy Cane Coral has a fast growth rate, especially compared to other corals. This is particularly true when it is fed on a regular basis. The exact growth speed will depend on the available nutrients, light and space, including nearby competition and rocks which could limit the area it spreads.\nCandy Canes grow by dividing a polyp into two identical polyps. This is a big reason why their growth is so rapid if their environment allows it, as their polyps can quickly multiply, and the process can occur with multiple polyps simultaneously.\nCandy Cane Coral Growth Height\nThe growth height of Candy Cane Coral can depend on a number of factors from the amount of food to the light intensity it receives, just like its growth rate. In the average aquarium you can probably expect it to grow up to several inches tall. However, due to their variability it’s difficult to predict exactly how high Candy Canes will grow.\nWhat Do Candy Cane Coral Feed On?\nIn general, Candy Cane Coral is happy to feed on a variety. Bitesize meaty food is most suitable, such as prawns, mysis shrimp and small pieces of krill. It will also accept LPS pellets and other coral foods, or even pellets and flakes intended for fish.\nWhen feeding your Candy Canes, you’ll get the best results if you use some form of feeding apparatus, or even a regular turkey baster. Try to place the food directly into the mouths to make it easiest for the sweeping tentacles to reach. This also helps to prevent your fish from stealing it – they are sometimes known to harass the coral by biting it to access the food inside if given the opportunity.\nIt’s useful to remember that Candy Canes are nocturnal, so the tentacles will naturally come out at night, or when the lights are off. While they extend during the day if they detect food nearby, strategic timing can speed up the process. You should aim to spot feed them 2-3 times a week.\nIn some cases, you may not need to directly feed your Candy Cane Coral at all. Providing that the flow isn’t too low, and you feed the rest of your tank heavily enough, it may get enough nutrients on its own. It will also benefit from its symbiotic relationship with the zooxanthellae algae living in its surface tissues photosynthesizing and giving another source of nutrients. However, spot feeding Candy Canes can help them to grow faster and appear brighter in color, so it’s up to you which approach will achieve your desired effect.\nHow to Split Candy Cane Coral\nIn order to split Candy Cane Coral, look for an outer branch for easy access and ensure you cut as far away from the polyps as you can to avoid damaging them. Glue the piece to a frag plug or rock to allow it to heal.\nBone cutters or a band saw work well as a cutting tool – the latter has the advantage of creating a flat base which is helpful when gluing. After a couple of weeks of healing the frags should be ready to be traded or even sold.\nCandy Cane Coral Dying\nIf your Candy Cane Coral is dying you will notice a change in appearance – in particular, the polyps not opening or puffing up is a tell-tale sign that something is wrong. In some cases, the flesh may recede, causing the skeleton to show. You may also see changes in color.\nDon’t panic if you notice these symptoms however, as with careful monitoring you should be able to get your Candy Canes back to being happy and healthy. The most common cause of these changes is a chemical composition in the water that is harmful to Caulastrea. To diagnose the problem, start by measuring the levels of calcium, magnesium, nitrates, and water hardness. The ideal ranges for these variables are as follows:\n- Calcium: 350-420ppm\n- Magnesium: ~1350ppm\n- Nitrates: <10ppm\n- Water hardness: 8°-12° dH\nIf your water falls within these recommendations, there could be an issue with the temperature or pH. As previously mentioned, Candy Canes require a temperature of between 75°C-82°C/24°C-28°C and alkalinity ranging from 8.1-8.4 pH. Check the light intensity too as anything higher than 150 PAR can cause the polyps to retreat. If the water flow is too high, it can also damage the coral.\nIt’s also possible that the coral is simply adjusting to recent changes in conditions. If you have moved it to a different spot in the tank, replaced your lighting or made other alterations, it can affect the appearance. If this is the case, your Caulastrea should recover quickly on its own.\nSometimes the cause is other livestock in your tank. Try observing your Candy Canes to spot fish or other coral bullying them, especially around feeding time. If this is the case you may be underfeeding your fish, or there may not be sufficient space between your corals. Keep in mind though that at times this behavior from fish is unavoidable and you might need to manage the problem instead of attempting to prevent it entirely.\nFinally, if none of the above methods prove successful, you may need to resort to fragging a healthy segment and starting over.\nThat concludes our guide to Candy Cane Coral! If you found it helpful, be sure to check out our other articles to help make your aquarium the best it can be.']	['<urn:uuid:04b2f017-5fa7-4483-a4e2-7e5edcf9a1c6>', '<urn:uuid:8f5fe40f-154b-4e39-840e-1e66b49280c3>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	15	60	2269
69	first classical music publication north america	The Bay Psalm Book of 1640 was the first music publication in English-speaking North America. It was also the first publication of any kind in North America. Before this, settlers of the Massachusetts Bay Colony had brought psalm books like the Ainsworth Psalter from Europe.	"['American classical music\nAmerican classical music is music written in the United States but in the European classical music tradition. In many cases, beginning in the 18th century, it has been influenced by American folk music styles; and from the 20th century to the present day it has often been influenced by folk, jazz, blues, Native American, and pop styles.\nThe earliest American classical music consists of part-songs used in religious services during Colonial times. The first music of this type in America were the psalm books, such as the Ainsworth Psalter, brought over from Europe by the settlers of the Massachusetts Bay Colony. The first music publication in English-speaking North America — indeed the first publication of any kind — was the Bay Psalm Book of 1640.\nMany American composers of this period worked (like Benjamin West and the young Samuel Morse in painting) exclusively with European models, while others, such as William Billings, Supply Belcher, Daniel Read, Oliver Holden, and Justin Morgan, also known as the First New England School, developed a native style almost entirely independent of the most prestigious European models, though it drew on the practice of West Gallery music composers such as William Tans\'ur and Aaron Williams. Many of these composers were amateurs, and many were singers: they developed new forms of sacred music, such as the fuguing tune, suitable for performance by amateurs, and often using harmonic methods which would have been considered bizarre by contemporary European standards. Some of the most unusual innovators were composers such as Anthony Philip Heinrich, who received some formal instrumental training but were entirely self-taught in composition. Heinrich traveled extensively throughout the interior of the young United States in the early 19th century, recording his experiences with colorful orchestral and chamber music which had almost nothing in common with the music being composed in Europe. Heinrich was the first American composer to write for symphony orchestra, as well as the first to conduct a Beethoven symphony in the United States (in Lexington, Kentucky in 1817).\nBecause the United States is made up of many states, some of which were parts of other empires, the classical music of the nation has derived from those empires respectively. The earliest classical music in what is now California, and other former Spanish colonies, was the renaissance polyphony of Spain. This sacred classical music was provided to support the liturgy of the Catholic Church.\nSecond New England School\nDuring the mid to late 19th century, a vigorous tradition of home-grown classical music developed, especially in New England. The composers of the Second New England School included such figures as George Whitefield Chadwick, Amy Beach, Edward MacDowell, and Horatio Parker, who was the teacher of Charles Ives. Many of these composers went to Europe — especially Germany — to study, but returned to the United States to compose, perform, and acquire students. Some of their stylistic descendants include 20th-century composers such as Howard Hanson, Walter Piston, and Roger Sessions.\nIn the early 20th century, George Gershwin was greatly influenced by African American music. He created a convincing synthesis of music from several traditions. Similarly inclined was Leonard Bernstein, who at times mixed non-tonal music with Jazz in his classical compositions. Leroy Anderson, Ferde Grofe and Morton Gould also composed pieces in the ""symphonic jazz"" vein.\nMany of the major classical composers of the 20th century were influenced by folk traditions, none more quintessentially, perhaps, than Charles Ives or Aaron Copland. Other composers adopted features of folk music, from the Appalachians, the plains and elsewhere, including Roy Harris, Elmer Bernstein, David Diamond, Elie Siegmeister, and others. Yet other early to mid-20th-century composers continued in the more experimental traditions, including such figures as Charles Ives, George Antheil, John Cage and Henry Cowell.\nIt was during the 20th Century that film music was first created. Over the evolution of the cinema the music took on greater and greater sophistication. Significant composers of film music include Erich Wolfgang Korngold, Bernard Herrmann, Jerry Goldsmith and John Williams.\nThe 20th Century also saw important works published by such significant immigrant composers as Igor Stravinsky and Arnold Schoenberg, who came to America for a variety of reasons, including political persecution, aesthetic freedom and economic opportunity.\nIn the 1970s and 1980s, after a period during which American composers like John Cage adopted atonal structures, Philip Glass revived tonality and traditional genres, such as opera in works like Einstein on the Beach. Glass helped create a mass market for ""classical"" music after audiences outside of the avant-garde had simply generally refused Modernist, atonal music.\nA pessimist model, shared by Aldous Huxley and Theodor Adorno, of the classical tradition in Europe was that it peaked with Beethoven. Aldous Huxley believed that subsequent classical music was vulgarized with the re-entry of the unsublimated erotic and Adorno believed that commodification entered with Wagner. ""American classical music"" flourished much after Beethoven. Some might say, beyond the pessimism of European sentimentalities.\n- J. H. Dorenkamp. ""The ""Bay Psalm"" Book and the Ainsworth Psalter"". Early American Literature. Vol. 7, No. 1 (Spring, 1972), pp. 3-16. Published by: University of North Carolina Press\n- ""America\'s First Book"". Library of Congress. Retrieved July 14, 2013.']"	['<urn:uuid:6e028af6-241d-4808-bf66-d43cbe585908>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	6	45	865
70	hey im writing poems need help creating something original what is way to make random poem by cutting paper	The cut-up technique, also known as découpé, involves taking a finished text and cutting it into pieces with a few or single words on each piece. These pieces are then rearranged to create a new text. This method was notably described by Tristan Tzara in his instructions 'TO MAKE A DADAIST POEM.' The technique was popularized in the late 1950s and early 1960s by William S. Burroughs and has been used in various creative contexts since then.	"['The cut-up technique (or découpé in French) is an aleatory literary technique in which a written text is cut up and rearranged to create a new text. The concept can be traced to at least the Dadaists of the 1920s, but was popularized in the late 1950s and early 1960s by writer William S. Burroughs, and has since been used in a wide variety of contexts.\nThe cut-up and the closely associated fold-in are the two main techniques:\n- Cut-up is performed by taking a finished and fully linear text and cutting it in pieces with a few or single words on each piece. The resulting pieces are then rearranged into a new text, such as in poems by Tristan Tzara as described in his short text, TO MAKE A DADAIST POEM.\n- Fold-in is the technique of taking two sheets of linear text (with the same linespacing), folding each sheet in half vertically and combining with the other, then reading across the resulting page, such as in The Third Mind. It is Burroughs and Gysin\'s joint development.\nAlso in the 1950s, painter and writer Brion Gysin more fully developed the cut-up method after accidentally re-discovering it. He had placed layers of newspapers as a mat to protect a tabletop from being scratched while he cut papers with a razor blade. Upon cutting through the newspapers, Gysin noticed that the sliced layers offered interesting juxtapositions of text and image. He began deliberately cutting newspaper articles into sections, which he randomly rearranged. The book Minutes to Go resulted from his initial cut-up experiment: unedited and unchanged cut-ups which emerged as coherent and meaningful prose. South African poet Sinclair Beiles also used this technique and co-authored Minutes To Go.\nGysin introduced Burroughs to the technique at the Beat Hotel. The pair later applied the technique to printed media and audio recordings in an effort to decode the material\'s implicit content, hypothesizing that such a technique could be used to discover the true meaning of a given text. Burroughs also suggested cut-ups may be effective as a form of divination saying, ""When you cut into the present the future leaks out."" Burroughs also further developed the ""fold-in"" technique. In 1977, Burroughs and Gysin published The Third Mind, a collection of cut-up writings and essays on the form. Jeff Nuttall\'s publication My Own Mag was another important outlet for the then-radical technique.\nIn an interview, Alan Burns noted that for Europe After The Rain (1965) and subsequent novels he used a version of cut-ups: ""I did not actually use scissors, but I folded pages, read across columns, and so on, discovering for myself many of the techniques Burroughs and Gysin describe"".\nHistory in literature\nIn 1969, poets Howard W. Bergerson and J. A. Lindon developed a cut-up technique known as vocabularyclept poetry, in which a poem is formed by taking all the words of an existing poem and rearranging them, often preserving the metre and stanza lengths.\nA precedent of the technique occurred during a Dadaist rally in the 1920s in which Tristan Tzara offered to create a poem on the spot by pulling words at random from a hat. Collage, which was popularized roughly contemporaneously with the Surrealist movement, sometimes incorporated texts such as newspapers or brochures. Prior to this event, the technique had been published in an issue of 391 in the poem by Tzara, dada manifesto on feeble love and bitter love under the sub-title, TO MAKE A DADAIST POEM.\nA drama scripted for five voices by performance poet Hedwig Gorski in 1977 originated the idea of creating poetry only for performance instead of for print publication. The ""neo-verse drama"" titled Booby, Mama! written for ""guerilla theater"" performances in public places used a combination of newspaper cut-ups that were edited and choreographed for a troupe of non-professional street actors.\nKathy Acker, a literary and intermedia artist, sampled external sources and reconfigured them into the creation of shifting versions of her own constructed identity. In her late 1970s novel Blood and Guts in High School, Acker explored literary cut-up and appropriation as an integral part of her method.\nHistory in film\nAntony Balch and Burroughs created a collaboration film, The Cut-Ups that opened in London in 1967. This was part of an abandoned project called Guerrilla Conditions meant as a documentary on Burroughs and filmed throughout 1961–1965. Inspired by Burroughs\' and Gysin\'s technique of cutting up text and rearranging it in random order, Balch had an editor cut his footage for the documentary into little pieces and impose no control over its reassembly. The film opened at Oxford Street\'s Cinephone cinema and had a disturbing reaction. Many audience members claimed the film made them ill, others demanded their money back, while some just stumbled out of the cinema ranting ""it\'s disgusting"". Other cut-up films include Ghost at n°9 (Paris) (1963–72), a posthumously released short film compiled from reels found at Balch\'s office after his death, and William Buys a Parrott (1982), Bill and Tony (1972), Towers Open Fire (1963) and The Junky\'s Christmas (1966).\nInfluence in music\nFrom the early 1970s, David Bowie used cut-ups to create some of his lyrics. Thom Yorke applied a similar method in Radiohead\'s Kid A (2000) album, writing single lines, putting them into a hat, and drawing them out at random while the band rehearsed the songs. Perhaps indicative of Thom Yorke\'s influences, instructions for ""How to make a Dada poem"" appeared on Radiohead\'s website at this time.\nStephen Mallinder of Cabaret Voltaire reported to Inpress magazine\'s Andrez Bergen that ""I do think the manipulation of sound in our early days – the physical act of cutting up tapes, creating tape loops and all that – has a strong reference to Burroughs and Gysin.""\nPaul McCartney was a Burroughs acquaintance in London when the White Album was recorded -- See ""Revolution No. 9.""\n- Dissociated press\n- Surrealist techniques\n- Assemblage (composition)\n- Vocabularyclept poetry\n- Found poetry\n- ""manifestos: dada manifesto on feeble love and bitter love by tristan tzara, 12th december 1920"". 391. 12 December 1920. Retrieved 27 June 2011.\n- Laura Caruso. William Burroughs, the infinite groundbreaker. Buenos Aires Herald. buenosairesherald.com.\n- Break Through in Grey Room\n- David W. Madden. ""A Conversation with Alan Burns"". Retrieved 5 June 2013.\n- Rogers, Ben (February 1969). ""Some Neglected Ways of Words"". Word Ways: The Journal of Recreational Linguistics. Greenwood Periodicals. 2 (1): 14–19.\n- Lindon, J. A. (May 1969). ""The Vocabularyclept Poem, № 1"". Word Ways: The Journal of Recreational Linguistics. Greenwood Periodicals. 2 (2): 85–89.\n- Bishop, Yvonne M.; Fienberg, Stephen E.; Holland, Paul W. (2007). Discrete Multivariate Analysis: Theory and Applications. Springer. pp. 340–342. ISBN 978-0-387-72805-6.\n- ""Tzara Combinations"". www.in-vacua.com. Retrieved 9 April 2018.\n- Intoxication: Heathcliff on Powell Street, Slough Press, 2009, ISBN 978-0615271033\n- Booby Mama! Cut-Up Spoken Word, 1977 https://www.amazon.com/Booby-Mama-Surreal-Cut-Up-Spoken/dp/1507829159\n- ""A Complete Disorientation of the Senses: William Burroughs\' and Antony Balch\'s \'Cut Ups\'"". dangerousminds.net. 16 December 2010. Retrieved 9 April 2018.\n- ""An Appraisal of the Films of William Burroughs, Brion Gysin, and Anthony Balch in terms of Recent Avant Garde Theory – Bright Lights Film Journal"". brightlightsfilm.com. 1 February 2003. Retrieved 9 April 2018.\n- ""UbuWeb Film & Video – William S. Burroughs"". www.ubu.com. Retrieved 9 April 2018.\n- ""Radiohead – Everything in Its Right Place"". radiohead1.tripod.com. Retrieved 23 October 2015.\n- ""Vintage Cab Sav,"" Andrez Bergen. Inpress, 1996.\n- The Ultimate Cut-Up Generator An online version that cuts-up the Internet, a specific URL, or your own text.\n- UbuWeb: William S. Burroughs featuring a cut-up, K-9 Was in Combat with the Alien Mind-Screens (1965), made with Ian Sommerville\n- The Tristan Tzara Arcade is a collection of Cut-up pieces composed from text found in the public domain. These pieces can be further arranged by the reader using an automated (jQuery script) reTypesetting function (which illustrates how possible variant compositions can be achieved using the Cut-up technique).\n- This Unruly: a repository of video cut-ups featuring video cut-up examples with an accompanying literature review about the practice of video re-mixing, re-purposing, video collage and appropriation techniques.\n- ReorderTV: a critical mixtape of video cut-ups of historically-ordered, annotated and curated collection designed to play sequentially from current video remixes to early experimental film examples.']"	['<urn:uuid:9959d09e-924c-45dc-8d59-ccb0535d72cd>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	19	77	1376
71	What is the main purpose of the Gleim Airline Transport Pilot FAA Knowledge Test book according to its description?	The main purpose is to provide the easiest, fastest, and least-expensive means to prepare for and pass the FAA knowledge test.	"['2 edition of This is the FAA. found in the catalog.\nThis is the FAA.\nby U.S. Dept. of Transportation, Federal Aviation Administration in [Washington, DC] (800 Independence Ave., SW, Washington 20591)\nWritten in English\n|Contributions||United States. Federal Aviation Administration.|\n|The Physical Object|\n|Pagination||24 p. :|\n|Number of Pages||24|\nI remember taking my private pilot knowledge exam in and recognizing nearly every question from test prep books that I used beforehand. But for the past year, everyone taking a PAR or IRA test has had different questions, with no more exact matches. The FAA has . The FAA Computer Testing Supplement for Inspection Authorization (FAA-CTD) is a loose-leaf notebook with excerpts from the pertinent regulations (14 CFRs), Airworthiness Directives, charts and tables, all the sample Type Certificate Data Sheets used in the IA test, and the FAA forms with which IA candidates must be esthetic-tokyo.comions: 3"" letter-size binder.\nThe purpose of the Gleim Airline Transport Pilot FAA Knowledge Test book is to provide you with the easiest, fastest, and least-expensive means to prepare for and pass the FAA knowledge test.. This is the printed version of the book. The eBook can be purchased esthetic-tokyo.com: Gleim. The FAA books are the material you read, and the ASA test guides provide example questions that are pretty dang spot-on in regards to what you\'ll be asked on all your tests; written, oral, and practical. View entire discussion (6 comments) More posts from the aviationmaintenance community. Posted by 2 days ago. FOD.\nOct 04, · Airman Certification Standards: Private Pilot - Airplane: FAA-S-ACS-6B.1 (Airman Certification Standards series) [Federal Aviation Administration (FAA)] on esthetic-tokyo.com *FREE* shipping on qualifying offers. FAA-S-ACS-8B, Effective June 11, Includes Change 1, Effective June 6, This book is part of ASA’s Airman Certification Standards Series.5/5(17). This means while an FAA written test prep book from, say, , is severely out of date, if the version of our test prep you are using shows the current year, then you are just fine There\'s absolutely no benefit to you in spending hours obsessing over the technical details of the FAA written test banks - .\nHandwriting and Character\nEEG in clinical practice\nYearbook of construction statistics.\nWriting the technical report\nGreat Jobs for Theater Majors\nA vvarning-peece to the Commons, Commissioners, Army ge, nerall [sic], which now sit as judges on His Majesties sacred person, in the behalfe of the whole kingdome\nComplete poems 1920-1976\nRecommended Country Inns\nChrist, My Passion\nAspects of Islamic Civilization\nWe say No\n39 rows · Nov 17, · FAA Home Regulations & Policies Handbooks & Manuals Aviation Aviation Handbooks & Manuals. Share on Facebook; Tweet on Twitter; Any reproduction or modification of this material from original FAA source material is solely the responsibility of the publisher.\nU.S. Department of Transportation Federal Aviation Administration Independence Avenue, SW Washington, DC () tell-FAA (() ). FAA-H, Rotorcraft Flying Handbook (PDF, MB) Note: This handbook is to be used for gyroplane information only. Disregard any Helicopter information presented.\nFAA Home Regulations & Policies Handbooks & Manuals Aviation Pilot’s Handbook of Aeronautical Knowledge. Share on Facebook; Tweet on Twitter; Pilot’s Handbook of Aeronautical Knowledge, FAA-HB (full version — low resolution) (PDF, MB) Front Matter (PDF, MB) Table of Contents (PDF, KB) Chapter 1: Introduction to Flying.\nAug 07, · The Federal Aviation Administration (FAA) is the government agency that oversees and regulates procedural standards for the aviation industry. Aviation Supplies & Academics, Inc. (ASA) is the leading publisher of aviation books and the largest producer of pilot supplies in the USA today, providing trusted materials for more than 80 years/5().\nGet this from a library. This is the FAA. [United States. Federal Aviation Administration.;] -- The men and women of the Federal Aviation Administration (FAA) are proud to be part of this aerial pageant.\nThey administer the world\'s busiest civil aviation system. In a single day, air traffic. Are you This is the FAA. book frequent flyer and air traveler of the friendly skies.\nYou may be interested in some of these publications produced by the U.S. Federal Aviation Administration (FAA). In this collection, you will find an assortment of official materials, such as the Aeronautical Information Manual: Official Guide to Basic Flight Information and ATC Procedures print subscription -a ""tell-all for.\nFeb 03, · Learn to fly a plane according to Federal Aviation Administration (FAA) regulations; The most complete guide to the rules of aviation accessible anywhere Contains all of the information needed to operate safely in US airspace and is fully updated; If you are an aviation enthusiast or an aviator, you need to have the newest edition of the FAR/AIM/5().\nNote: Citations are based on reference standards. However, formatting rules can vary widely between applications and fields of interest or study. The specific requirements or preferences of your reviewing publisher, classroom teacher, institution or organization should be applied.\nOnline shopping from a great selection at Books Store. Remote Pilot Test Prep Study & Prepare: Pass your test and know what is essential to safely operate an unmanned aircraft from the most trusted source in aviation training (Test Prep Series).\nFAA Commercial Space Transportation Associate Administrator Wayne Monteith welcomes us back to the second day of the 23rd Annual Commercial Space Transportation Conference.\nTune in at a.m. ET to hear FAA Administrator Steve Dickson outline the agency’s top priorities for Commercial Space/5(K). The FAA recommends 30+ regulations, advisory circulars, handbooks, and other references in their Practical Test Standard for powered parachutes. You can purchase all of that material and organize it if you like.\nOr you can get the one book that does that for you. Roy\'s Powered Parachute Book is a big book that covers a lot of ground. Mar 18, · What would become the FAA was born in with passage of the Air Commerce Act.\nThe law established the framework of the modern FAA by directing the Cabinet-level Department of Commerce with promoting commercial aviation, issuing and enforcing air traffic rules, licensing pilots, certifying aircraft, establishing airways, and operating and maintaining systems to help pilots navigate.\nStore. Shop by Department. Shop Apparel» Shop Patches & Decals» Shop Books» Ladies MicroFleece Jacket with FAA seal - Navy Heather $ Power Plant Test Guide $ General Test Guide $ Helicopter Flying Handbook $ Aviation Mechanic Handbook.\nFeb 23, · FAA B The FAA\'s Airplane Flying Handbook has been required reading for all pilots for more than 30 years. This book introduces the basic pilot skills and knowledge essential for piloting airplanes. It benefits student pilots just beginning their aviation endeavors, as well as those pilots wishing to improve their flying proficiency and aeronautical knowledge, pilots preparing for /5(52).\nJul 12, · Private Pilot FAA Written Exam [Irvin N. Gleim] on esthetic-tokyo.com *FREE* shipping on qualifying offers. The primary purpose of Private Pilot and Recreational Pilot FAA Written Exam is to provide you with the easiest, fastest/5(8). Free Aviation Training Books.\nDon\'t pay for your aviation training books. Download PDF versions of all the FAA Aviation Books Here!. The Federal Aviation Administration (FAA) is a governmental body of the United States with powers to regulate all aspects of civil aviation in that nation as well as over its surrounding international waters.\nIts powers include the construction and operation of airports, air traffic management, the certification of personnel and aircraft, and the protection of U.S.\nassets during the launch or Annual budget: US$ billion (FY). Jan 10, · Another reason Sporty’s is critical to this process, and why you shouldn’t just use the book, is it consistently updates its questions based on what’s coming through on the FAA written exam.\nTwo days before my exam, about 20 new questions came through so using Sporty’s allowed me to study the most current information. Get this from a library. This is the FAA: mission, organization, and activity.\n[United States. Federal Aviation Administration.]. The purpose of the Gleim Airline Transport Pilot FAA Knowledge Test book is to provide you with the easiest, fastest, and least-expensive means to prepare for and pass the FAA knowledge test.\nThis is the online access version of the book. Online access is provided for 12 months. The book can be downloaded and saved for access anytime.Nov 24, · The FAA is to the USA as ICAO is to the entire world, with a few exceptions. Both are concerned with safety and procedures, but the FAA is also tasked with the promotion of aviation interests, such as the development of new technologies and operat.ASA reprints a variety of FAA handbooks and Advisory Circulars.\nPrinted by ASA, they\'re always the most current version produced by the Federal Aviation Administration. In this way, ASA provides FAA publications at a substantial savings and greater convenience for everyone.']"	['<urn:uuid:160aa881-a7e8-46f9-9b85-e9c0578f7d0a>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	19	21	1454
72	How does the nutritional profile of Euglena-based products compare to dairy production's global sustainability challenges?	Euglena provides 59 kinds of nutrients including vitamins, minerals, amino acids, DHA, and unsaturated fatty acids, with high digestibility due to lacking a cell wall. Meanwhile, dairy production faces major sustainability challenges, as studies show that Europe's meat and dairy production must be reduced by half by 2050 due to environmental concerns. Dairy farming significantly contributes to greenhouse gas emissions and requires extensive land and water resources, leading to environmental degradation across multiple countries.	"['Improved nutritional balance! Euglena blended meat substitutes with less environmental impact\nOnline sales from Meat Day (February 9th)\n""NEXT Euglena Yakiniku EX"" jointly developed by Next Meets and Euglena\nNext Meets Co., Ltd.\nEuglena Co., Ltd.\nNext Meets Co., Ltd. (Headquarters: Shinjuku-ku, Tokyo, Representative Director: Hideyuki Sasaki, hereinafter ""Next Meets"") and Euglena Co., Ltd. (Headquarters: Minato-ku, Tokyo, President: Mitsuru Izumo, hereinafter ""Euglena"") , Ishigakijima Euglena (Japanese name: Euglena"") and Yaeyama Macrorera extract (hereinafter ""Chlorella extract"") alternative meat * 1, ""NEXT Euglena Yakiniku EX"" was jointly developed, February 9, 2021 We are pleased to announce that it will be released by Next Meets from (Tuesday).\n""NEXT Euglena Yakiniku EX""\nNext Meets and Euglena Co., Ltd. signed a food joint development contract in December 2020 in order to realize a sustainable lifestyle together. This time, we will release Euglena Yakiniku EX"" as the first jointly developed product.\n""NEXT Euglena Yakiniku EX"" is a product that uses substitute meat containing 500 mg of Euglena Since it does not use animal raw materials, it is kind to the global environment * 2, and you can ingest nutrients such as proteins, vitamins, and minerals in a well-balanced manner. Also, because it is a vegetable protein, it is a healthy grilled meat with zero cholesterol. In addition, the texture of the meat is reproduced without using chemical seasonings, so you can enjoy it with confidence.\n""NEXT Euglena Yakiniku EX"" will be on sale at the official online store of Next Meets.\nNext Meets and Euglena will continue to aim to realize a sustainable society by developing food products that take into consideration ""health"" and ""global environment"", which are important themes for society.\n* 1 Meat substitutes are plant-based foods made from soybeans, etc., made by reproducing the taste and texture of meat. The market size is expanding as the number of people who do not eat meat, such as vegetarians and vegans, and those who eat less meat are increasing. It is also attracting attention because it has a lower environmental impact than animal meat, which emits a large amount of greenhouse gases during the production process.\n* 2 From the viewpoint of the land area required for breeding, feed, and water supply, livestock animals require more resources for protein source production than vegetable foods such as grains and vegetables, and have an environmental burden. Is said to be large. Among them, it has been reported that cattle emit methane gas, which leads to global warming, from belching and feces that are ruminant during digestion.\nProduct name: NEXT Euglena Yakiniku EX\nRelease date: February 9, 2021 (Tuesday)\nPrice: 480 yen (tax included) * The EC site sells a set of four.\nBest-by date: 1 year (frozen)\nCapacity: Package containing 80g\nIngredients: Processed soybeans (domestic production), seasonings (soy sauce, sugar, fermented rice seasonings, yeast extract, apples, brewed vinegar, garlic, salt, sesame oil, doubanjiang, pepper), edible rapeseed oil, (partly wheat) , Soybeans, apples, sesame), Euglena, Chlorella extract\nSales site: https://shop.nextmeats.jp/\n* It will be delivered by cool mail as frozen food.\n""NEXT Euglena Yakiniku EX""\nSponsoring a part of sales ""Euglena GENKI Program""\nNext Meets supports the ""Euglena GENKI Program"" and co-sponsors a portion of the sales of Euglena\nEuglena Euglena cookies, which are rich in nutrients, to children in Bangladesh free of charge. Since its inception in April 2014, we have expanded the scale of our activities with the support of many customers. As of the end of September 2020, Euglena containing cookies were distributed to approximately 10,000 children at 66 schools, and the total number of distributions exceeded 10 million meals.\n● About Ishigaki Island Euglena\nEuglena is a member of the algae. Growing up in the abundant nature of Ishigakijima, Euglena Ishigakijima has 59 kinds of nutrients (vitamins, minerals, amino acids, DHA, unsaturated fatty acids such as oleic acid, peculiar ingredient paramilon, etc.) that humans need. Since it does not have a cell wall, it is a superfood that has a high rate of digestion and absorption of nutrients.\n<About Next Meets Co., Ltd.>\nA Japanese startup that conducts business activities in the field of alternatives with the philosophy of ""not ending the earth"". Research began in 2017, legalized in 2020 when the product was completed, and listed on OTCBB under the SPAC scheme in the Japanese market in January 2021.\nLaunched the world\'s first alternative to grilled rice, ""NEXT Grilled Rice,"" a rice bowl made from plant-based rice, ""NEXT Rice Bowl,"" and an alternative rice hamburger, ""NEXT Burger."" Currently, ""NEXT Hamburg 1.0"" is crowdfunding at Makuake. https://nextmeats.co.jp/\nIt became a hot topic on TV programs, and the number of enthusiastic fans, especially young people, increased rapidly. In December of last year, Toyota Tsusho, a general trading company, announced a basic partnership agreement. Our mission is to start research and development of a wide range of alternative proteins and to replace all of them by 2050.\n[Next Meets Official Store]\n<About Euglena Co., Ltd.>\nIn 2005, the world\'s first edible microalgae Euglena (Japanese name: Euglena)\nSucceeded in establishing outdoor mass culture technology. Utilizing microalgae Euglena, chlorella, etc.\nIn addition to the development and sale of used foods and cosmetics, research for the production of biofuels\nWe are also researching. Also, since 2014, the child of Bangladesh\nEuglena GENKI Program"", which delivers Euglena cookies with abundant nutrients to children, will be expanded to all group products including cosmetics. Listed on TSE Mothers in December 2012. TSE First Section market change in December 2014. We define ""Sustainability First"" as the Euglena Philosophy and develop our business. https://euglena.jp\n-Contact for inquiries from the press-\nEuglena Co., Ltd. Corporate Communication Division', ""Environment soccer us politics business tech the upside rooftop farming: why vertical gardening is blooming in animals farmed europe's meat and dairy production must halve by 2050, expert. W w w i n t e r r e g d a i r y m a n e u environmental effects of dairy farming - focusing the results of the eu-dairyman project elsaesser. Environment tools people in dairy farm land water carbon information on energy and climate policies, including likely effects on dairy shed energy costs. Dairy farming & the environment it therefore ought to be no surprise why fish & game holds strong views about the adverse environmental effects of dairy farming. Dairy's impact on the environment loss of wild areas to agriculture is the leading cause of the current mass extinction of wildlife.\nEnvironmental impacts of dairy farming and many of these suggest that outcomes to date have understated the negative impact a recent report from environment waikato (2008) on. About 10 billion land animals in the united states are raised for dairy, meat, and eggs each year factory farming accounts for 37% of methane (ch4) emissions, which has more than 20 times the global warming potential of co2. In new zealand dirty dairying refers to damage to the ecological health of new zealand's freshwater environment by the intensification of dairy farming, and also to the high profile campaign begun in 2002 by the fish and game council to highlight and combat this. We raised dairy cows and if you are familiar with dairy farming, you know that it gets to be quite a task it is a year round job that requires a large investment of your time it is a year round job that requires a large investment of your time.\nThe hidden effects of dairy farming on public and environmental health in the netherlands, india, ethiopia, and uganda, considering the use of antibiotics and other agro-chemicals maria j groot 1 and katrien e van't hooft 2,. Ultimately, the relative impacts of non dairy milk - hemp, soya, oats, rice, almonds etc - are probably lower than the environmental footprints of dairy milk, but there aren't any conclusive. 1 environmental impacts of dairy farming in lembang, west java estimation of greenhouse gas emissions and effects of mitigation strategies working paper no 221.\nThe innovation center for us dairy is helping farms and businesses pursue environmentally friendly dairy practices learn more about these dairy farming initiatives. Environmental risks facing farmers potential adverse health effects on farmers, farm workers and others living close to farms include cancer, respiratory disease. A look into how the dairy industry impacts the planet a medium size dairy factory farm facility houses i also think a post about the environmental of plant-based dairy products vs dairy. In addition, on-farm practices should ensure that milk is produced by healthy cattle under sustainable economic, social and environmental conditions to that aim, this document provides a set of principles and practices for sustainable dairy farming for the mainstream market in all regions of the. Climate change and the dairy industry extensive research is being undertaken by a number of organisations into climate change and its effects upon the dairy industry victorian climate change adaptation program (vccap) aims to increase the knowledge and capabilities of government, the agriculture sector and farming businesses to adapt to.\nCalls to limit dairy conversions are being made in the wake of a new report confirming fears dairying is polluting waterways the parliamentary commissioner for the environment today released a. How growth in dairy is affecting the environment says is within a five-mile radius of 10 big dairy farms and about 60,000 cows center of a global debate about dairy's impact on the. Environmental risks of proposed dairy farm said about dairy farms and their environment: bodies that have identified the adverse environmental effects of agriculture, and especially. Factory farming is a method of farming where livestock is raised in confinement in large numbers (and at high density) in a factory-like environment the aim of factory farming is to produce as much meat, eggs or dairy at the lowest.\nDairy farming systems are important sources for the emission of a number of materials that include various forms of nitrogen (no 3 −, n 2 o and nh 3) with potential environmental impact the present paper is a systems synthesis study and assesses the likely impact of changes in management on n flows and losses. The environment agency told the bureau in a statement: agriculture uses 70% of the land in england and farmers have a major impact on the environment most farmers act responsibly and we work with the industry to respond to incidents, tackle the root causes of pollution and promote good practice. Dairy sustainability: ten environmental considerations high impact responses to achieve low demand as a result the potential negative impacts of dairy farming. Milk production impacts the environment in various ways, and the scale of these impacts depends on the practices of the dairy farmers and feed growers dairy cows and their manure produce greenhouse gas emissions which contribute to climate change.\nDairying is a relatively intensive livestock production system and contributes to a range of environmental impacts in the southern hemisphere, dairy farming systems are based mainly on outdoor grazing of permanent pastures the objectives of this study were to (i) assess environmental profiles and. Dirty dairy new zealand's dairy industry has a significant detrimental effect on our environment the 2015 environment aotearoa report recorded a 28 per cent surge in the land area used for dairy farming over the last 10 years.""]"	['<urn:uuid:8b74ff95-b4de-4cb8-9fcc-e73f4dfb61c6>', '<urn:uuid:3f376b45-573c-4c49-8249-a71902575707>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	15	74	1867
73	What metrics evaluate habitat quality for conservation, and how is carbon storage considered?	Habitat quality is evaluated through multiple metrics including habitat models that predict species distribution based on environmental information, connectivity to surrounding populations, and site quality variation throughout the landscape. These assessments require combining connectivity measures with habitat models in reserve selection. Regarding carbon storage, it is quantified for conservation areas - for instance, Lakeview WMA stores over 155 metric tons of carbon, equivalent to CO2 emissions from 64 million gallons of gasoline or 629 million pounds of coal burned.	"['This article was featured in the 2022 Autumn Newsletter by Zack Simek and Rob Williams-APIPP/SLELO.\nIs the health of our Priority Conservation Areas (PCAs) getting better, worsening, or maintained as a result of our invasive species protection & management strategies? Answering this important question first requires a benchmark. To achieve this, the SLELO PRISM has been developing a scorecard for each PCA based on current analysis.\nThe first step is to describe the PCA and become oriented with the natural characteristics of the PCA. We describe the acreage, habitat types, terrestrial and aquatic, we map it and if available we mark the boundary and describe the Highly Probable Areas or HPA’s where invasives are most likely to occur.\nWe then use The Nature Conservancy’s resilient and connected network (or RCN) database that identifies where plants and animals have the best chance to adapt to a changing climate. This compares a site’s resilience, connectedness, and landscape diversity.\nWe also look at the sources and quantities of carbon stored at the PCA. For example, in one of our PCAs (Lakeview WMA), we estimate over 155 metric tons of stored carbon which is equivalent to CO2 emissions from 64 million gallons of gasoline consumed or 629 million pounds of coal burned.\nNext using multiple sources including work by New York’s Natural Heritage Program, we look at native species communities and their state ranking which reflects their rarity. We then insert our invasive species abundance and management knowledge and we do this for both aquatic and terrestrial invasive species. By comparing resiliency, connectedness, and diversity with invasive species extent or density the total score is the average (RCD+IS).\n- Lakeview Wildlife Management Area PCA\n- RCD Score (1.18 + 1.08 + 1.35) = 1.20 = A (95)\n- IS Density Score (s) – A (95)\n- Yellow Iris – A (95)\n- Swallow-wort – A (95)\nTotal Score= (95 + 95)/2 =95=A\nEcological Restoration Factor\nAn ecological restoration factor may be included based on the success or failure of native plant recovery and can be accounted for either by natural succession or intentional restoration. If the observer notices the recovery to natives, the individual can add a (+) to the score. If the observer notices the return to non-natives, the individual can add a (-) to the score. This now becomes a Discretionary Restoration Adjustment or DRA.\n95=A + DRA=A+\nIf target invasives are being reduced then forest health, carbon and biodiversity are presumably sustained. If a new IS such as an insect enters the system or if the abundance of an invasive plant increases, then the site might be “regressing”. If IS are reducing, there are no new introductions and native species continue to thrive, then the PCA is likely to be improving. So, this becomes our (Health Profile or Benchmark Score). At Lakeview PCA we are now seeing after 11 years of treatment – a 33% reduction in the extent of sites managed for swallow-wort, which indicates we are progressing in a favorable direction.', ""Site map :\nBiodiversity Conservation Informatics Group\nphone +358 9 191 57734\n5. Evaluation of conservation outcome: habitat suitability, connectivity and persistence\nResearchers: Atte Moilanen, Jussi Laitila and Federico Montesino Pouzols\nSelection of reserve networks requires data on the distributions of species or communities of conservation importance. It is nowadays widely accepted that reserve selection based on just presence or presence/absence records of certain species may result in reserve networks that are small and fragmented, and which will not ensure the long-term persistence of biodiversity (Cabeza & Moilanen, 2001). Reasons for this can be found in the type of data used: a snap-shot of the occurrence of biodiversity, without consideration of site quality variation.\nAs landscapes and species communities are dynamic in space and time, site quality varies as well. Reserve selection methods need to account for landscape dynamics and connectivity, to avoid selecting small (cheap) fragments as a reserve, which in practice are sinks from large (but expensive) habitat patches nearby, which remain unprotected and might therefore be lost (Cabeza, 2003; Cabeza & Moilanen, 2003; Van Teeffelen et al. 2005). Zonation (see topic 3 is a framework that can easily account for variable site quality. Variation in site quality can be taken into account by predicting species distribution based on environmental information, with so called habitat models or species distribution models (see Elith et al. 2006 for review). Predictions from these models give insight into site quality differences throughout the landscape. However, these models typically consider only local variables, whereas connectivity to surrounding habitat and populations are of major importance to population dynamics and therefore population persistence.\nCabeza (2003) was among the first to combine connectivity measures and habitat models in reserve selection. We have investigated a number of different (species-specific) ways of including connectivity effects into spatial prioritization (Cabeza 2003; Moilanen 2005a, 2005b; Moilanen et al. 2005; Moilanen and Wintle 2006, 2007; Moilanen et al. 2008). While connectivity can be deal with semi-rigorously, the translation from habitat amount, habitat quality and connectivity persistence remains a difficult question, which requires application of techniques such as metapopulation models or PVA simulation models.\nThere is substantial knowledge in the MRG about the influences of spatial pattern to connectivity and persistence, and our collaborators including Dr Jane Elith (Melbourne), Dr John Leathwick (NIWA, NZ) and Dr Simon Ferrier (CSIRO) are substantially experienced in the development of habitat models. Habitat quality, connectivity and persistence are fundamental quantities of population biology and conservation, and they will continue to be basic building blocks of the analyses we develop.\nCabeza, M. 2003. Habitat loss and connectivity of reserve networks in probability approaches to reserve design. Ecology Letters, 6:665-672.\nCabeza, M., Araujo, M.B., Wilson, R.J., Thomas, C.D., Cowley, M.J.R. and A. Moilanen. 2004b. Combining probabilities of occurrence with spatial reserve design. Journal of Applied Ecology, 41:252-262.\nCabeza, M., and A. Moilanen. 2001. Design of reserve networks and the persistence of biodiversity. Trends in Ecology and Evolution, 16:242-248.\nCabeza, M., and A. Moilanen. 2003. Site-selection algorithms and habitat loss. Conservation Biology, 17:1402-1413.\nCabeza M., Moilanen, A. and H.-P. Possingham. 2004a. Metapopulaton dynamics and reserve network design. Pages 541-564 in I. Hanski and O. Gaggiotti, eds. Metapopulation ecology, genetics, and evolution. Academic press.\nElith, J., Graham, C. H., Anderson, R. P., Dudík, M., Ferrier, S., Guisan, A., Hijmans, R. J., Huettmann, F., Leathwick, J. R., Lehmann, A., Li, J., Lohmann, L. G., Loiselle, B. A., Manion, G., Moritz, C., Nakamura, M., Nakazawa, Y., Overton, J. M., Peterson, A. T., Phillips, S. J., Richardson, K. S., Scachetti-Pereira, R., Schapire, R. E., Soberón, J., Williams, S., Wisz, M. S. and N.E. Zimmermann. 2006. Novel methods improve prediction of species' distributions from occurence data. Ecography, 29, 129-151.\nHodgson, J., Moilanen, A., Wintle, B.A., and C. D. Thomas. 2011. Habitat area, quality and connectivity: striking the balance for efficient conservation. J. Applied Ecology, 48:148-152.\nHodgson, J., Thomas, C.D., Wintle, B.A. and A. Moilanen. 2009. Climate change, connectivity and conservation decision making - back to basics. Journal of Applied Ecology, 46: 964-969.\nLehtomäki, J., Tomppo, E., Kuokkanen, P. Hanski, I., and A. Moilanen. 2009. Applying spatial conservation prioritization software and high-resolution GIS data to a national-scale study in forest conservation. Forest Ecology and Management, 258: 2439-2449.\nMoilanen, A. 2005a. Methods for reserve selection: interior point search. Biological Conservation, 124: 485-492.\nMoilanen, A. 2005b. Reserve selection using nonlinear species distribution models. American Naturalist, 165: 695-706.\nMoilanen, A., Franco, A.M.A., Early, R., Fox, R., Wintle, B., and C.D. Thomas. 2005. Prioritising multiple use landscapes for conservation: methods for large multi species planning problems. Proc. R. Soc. Lond. B Biol. Sci., 272: 1885-1891.\nMoilanen, A., Leathwick, J.R., and J. Elith. 2008a. A method for freshwater conservation prioritization. Freshwater Biology, 53: 577-592.\nMoilanen, A., and M. Nieminen. 2002. Simple connectivity measures in spatial ecology. Ecology 84:1131-1145.\nMoilanen, A. and B. A. Wintle. 2007. The boundary quality penalty a quantitative method for approximating species responses to fragmentation in reserve selection. Conservation Biology, 21: 355-364.\nVan Teeffelen, A., Cabeza, M. and A. Moilanen. 2006. Connectivity, probabilities and persistence: comparing reserve selection strategies. Biodiversity and Conservation, 15, 899-919.""]"	['<urn:uuid:22333b17-520b-4b64-8195-2e1c2aff3693>', '<urn:uuid:e32f8149-2a48-4d90-969b-fb2a7df95e93>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	13	79	1349
74	ancient belief spirit communication druid priests october thirty first night festival	Druids or ancient priests claimed that on this special Halloween night, they could communicate with the spirits of the departed. Furthermore, the ghosts of the ancestors could assist the druids in foretelling what was going to happen in the year that was about to begin.	"[""We can trace Halloween back to Celtic festivals which\ncelebrated the new year on the 1st of November (not 1st of January). One thread that runs through all Halloween legends is the belief that the night of October\n31st was the time when ghosts of the dead\nreturned to earth. The ancients name for this festival Samhain** and its role was to mark the boundary between the old year and new.\nWith the spread of Christianity, the Church emphasised All Saints day on the 1st of November. This date was also known as All Hallows and gradually, the name for the night before All Hallows changed to hallow's eve, then hallow e'en\nand finally Halloween.\n**Samhain - In\nmedieval English 'mh' was pronounced 'w', hence: Sow-en.\nCelebration of Halloween\nCompared with our Pagan ancestors, today we trivialise Halloween. Today in the UK and USA we don't even get a holiday on the 31st of October or the 1st on November, whereas, to the ancient Celts this\nwas one of the 4 most important dates in the year.\nDruids or ancient priests claimed that on this special Halloween night, they could communicate with the spirits of the departed. Furthermore, far from being threatening, the ghosts of the ancestors could assist the druids\nin foretelling what was going to happen in the year that was about to begin.\nTo appease the spirits of the dead and to encourage\nthem to appear, the Druids arranged for a communal festival on the night before Samhain. The precise details of the Halloween celebration varied from region to region, and from era to era, but a constant theme is\nof bonfires and animal sacrifices. Many Halloween festivals lasted for 3 days.\nWhen Christian influences curtailed pagan beliefs, Samhain was replaced with All Saints day on the 1st of November, and All Souls\nday on the 2nd of November. This is a classic example of an ancient, essentially pagan festival being taken over and sanitised by Christian beliefs. Yet, aspects of the\nancient festival survive, for example worship of the spirits of the dead of Halloween.\nWitches and Halloween\nWitches, shaman, sages, witch-doctors, wise women or wicca* appear in many societies. If you examine the role of witches at Halloween, then it is as hard to separate fact from legend, as it is to\nseparate a witch from her broom.\n*The plural of wicca is not wiccas, but wiccan.\nWitches feature in the Bible: Exodus 22:18 'Thou shalt not suffer a witch to live'. However, it was during the middle ages\nbetween about 1400 and 1700, that witchcraft was most enthusiastically persecuted first in Europe and then later in America. Perhaps the most famous witches' denouement was at the Salem Witchcraft trial of 1692.\nInterestingly, these days Salem Massachusetts hosts some of the biggest Halloween celebrations in America.\nWhat separates the Halloween witch from her sisters is her broomstick. In pagan festivals the\nbroomstick is associated with cleansing and initiation rituals. It is easy to see how flying could enter the Halloween picture, because this is a time when the spirits were coming down to earth, and also the\nliving were high from eating magic mushrooms and other hallucinogenic potions. And of course her caldron was handy for witches to brew up spells to control those who offended her; take Shakespeare's Macbeth and the three\nwitches famous refrain: 'Double, double toil and trouble; Fire burn, and caldron bubble'.\nSee more on witches at Halloween\nTrick or Treat\nThere is no doubt that 'Trick or Treat' has very old roots dating back to the pagan customs surrounding Samhain. Related customs include wassailing and souling, indeed, in its original form the idea\nwas that the recipients of barnbrack cakes would pray for the souls of the relatives who gave them the 'treat'.\nAs with many customs Trick and Treat waxed and waned. During my childhood in the UK of the 1950s and\n1960s, I don't remember any organized 'Trick and Treating'. What seems to have happened is that it has been re-introduced into the UK from America in the 1990s. These days parents are so paranoid about their\nchildren being out after dark and alone, that they now supervise their children's Trick or Treat from the shadows of the nearest lamppost.\nCatholic areas in general, and Ireland in particular, favour Trick and\nTreat, whereas Protestant heartlands celebrate the gunpowder plot on November the 5th. My point is that this season, Halloween / November the 5th is great time for celebrations based around bonfires and\nfireworks. And 'Trick or Treat' is a great ruse to stock up on food ready for the cold night's festivities. See more on trick or treat.\nCost of Halloween Escalates\nIt's almost scary, the money people spend on Halloween say Will and Guy.\nThis year's spending on the holiday will average $70 a person, up from last\nyear's $61.50 and comparable to the $66.54 average spent in 2008.\nTotal consumer spending on Halloween is expected to reach $6.2.billion.\nThe largest item will go fancy-dress costumes on average of $24.97.\nAmericans are expected to spend an average of $21.15 on candy, $19.23 on\nWe are told that that's a fraction of the $17 billion that Valentine's Day\ninspires; flowers and diamonds cost more than pumpkins and candy corn, but\nit's still a considerable amount of cash. Also, it doesn't even include all\nthe alcohol consumed at those crazy costume parties!\nThe Cost of Halloween in the\nBritain's recession continues to bite in 2010 as the coalition government\nmake swingeing cuts in all walks of life. However, report Will and Guy, the\ncommercialised side of Halloween appears doing very well in the UK.\nPeople who do not warm to Halloween's opportunity for ghoulish high jinks\nmay bemoan its ballooning US-style commercialisation, but Planet Retail have\ntold us that UK spending on Halloween goods rose from just £12m in 2001 to\n£235m last year - a 20-fold increase.\nWe asked ourselves why has Halloween, originally a pagan festival, grown\nso much in popularity and retail clout? Perhaps the key reason is the fact\nthe supermarkets have massively increased their range and promotion of\nGo back 20 years and you could have picked up a toffee apple and pumpkin\nwith your weekly shop before Halloween, but that was about it. Today, the\nsupermarkets have whole aisles of Halloween goods; everything from fancy\ndress to decorations, toys, games and novelty bakery products.\nAncient Maxmen (natives of the Isle of Man) celibrated Hop-tu-Naa as the\nCeltic New Year; when summer gave way to winter on the 31st of October.\nHop-tu-Naa is a time when people give thanks for the harvest and a time\nto prepare for the long, cold winter.\nmore on Hop-tu-Naa\nGermany and Austria\nCatholics In Germany and Austria celebrate Seleenwoche between 30th October and 8th November. Rituals include leaving out bread and drink for spirits of ancestors returning for the night of Halloween.\nstrong European superstition is to put away all knives in case they hurt returning spirits, who are unsteady on their feet, or with their flying. Incidentally the Chinese have a similar superstition about\ntheir new year.\nIt is considered particularly unlucky if a black cat crosses your path at Halloween!\nSpain and Italy\nIn Catholic countries such as Spain and Italy it is All Saint's day that is the focus of the celebration, indeed, the 1st of November is a public holiday.\nMuch of America has a plentiful supply of pumpkins suitable fore making Jack-o-Lanterns. Americans have also perfected the art of 'Trick or Treat'.\nIn Mexico on Los Angelitos (All Saints Day), the whole family visits the graveyard of the deceased relatives, where they have picnic lunches on the graves. They believe in making Halloween a day of\nremembrance, happiness and celebration.\nIn China Halloween is known as Teng Chieh, or 'The Feast of the Hungry Ghosts'. While the rituals are reminiscent of Halloween, their timing is for the full moon at end of the Chinese New Celebrations\nin February, rather than at the end of October.\nThe Chinese assemble pictures of departed relatives and place food and drink on the table for their spirits. Some burn money, their way for paying the\nprice of travel from the netherworld. Others light bonfires and lanterns is to guide the spirits path back to their former homes.\nIn Hong Kong they have similar beliefs and call their Hungry\nGhosts' Festival festival - Yue Lan. However the Hong Kong celebrations are held at the end of August not the end of October.\nThe Four Fire Festivals\n1st Nov Samhain - Nos Galen-Gaeaf in Welsh\nIn ancient times the\ncelebration of any festival started the night before, hence Halloween is the evening of 31st October.\n2nd Feb Candlemas - Oimelc in Irish. Celebration of\nSpring / 40 days since the nativity.\n1st May Beltane - May Day\n1st Aug LammasLughnasadh in Gaelic, celebrates the start of the harvest\nSee more funny Halloween jokes, also clean pumpkin jokes\n• Halloween 2012 • Coven of witches""]"	['<urn:uuid:d6639007-f1b0-4b76-a691-33b6ceb346d0>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	11	45	1506
75	What are the training procedures and certification requirements for commercial pilots?	Training procedures for pilots are highly intensive and involve using simulators that emulate real flight conditions before flying actual aircraft. Pilots must accumulate a specific number of flight hours for certification. As of August 2014, stricter ATP certification requirements include a mandatory ATP-CTP course with 30 hours of ground training and 10 hours of flight simulation at an FAA-approved center. After completing the ATP-CTP course, candidates must pass a written exam and prepare for an FAA examiner checkride. Additionally, pilots must maintain constant updates with new developments, and their competence is regularly evaluated to prevent accidents from fatigue or avoidable causes.	['Safety while flying elicits a lot of attention from all those who venture to fly. As much as statistics show that travelling by cars is more dangerous than aircrafts, flying is more feared due the loss of control that people feel as a result of being suspended in the air. Many regulatory bodies look at aviation safety seriously as much as passengers. These bodies are tasked with the enforcement of rules and standards that address safety of the passengers while on board a moving aircraft from liftoff up to landing.\nThe aviation industry is one of the most highly regulated. All players from the manufacturers of the giant jumbos to the suppliers of the smallest parts are required to adhere to a wide range of standards. Each part that is made is tested so as to verify whether it is compliant with predetermined tolerance limits failure to which it must be done away with. There are approved third parties who are empowered to certify companies that engaged in the aircraft industry (Krause, 2003).\nAll companies that are involved in air travel are closely monitored to ensure they are competent. This is not only expected of manufacturers of parts but also those companies that assemble aircrafts. After the assembly every plane is tested so as to ascertain whether it complies with regulatory requirements. The tests include flying so as to establish whether it is operating properly and safely (Krause, 2003).\nDuring a test flight only a small portion of the crew is taken on board. New models are examined through a battery of tests that are designed to simulate real flight conditions. This process can take over a year to be completed. Production runs of a particular model are also subjected to lesser demanding tests but test flights are always conducted on every produced unit. Regulations are also enforced when a plane is in service. Actually, the rules become stricter through a schedule of maintenance and inspections before every flight (Stolzer & Goglia, 2011).\nIf an aircraft deviates from a set-out schedule it may be grounded. Every busy plane is usually inspected for more than one occasion every day. Those that fly occasionally are inspected after a few days or weekly. If a plane fails an inspection it is immediately grounded until all required repairs are completed. Minor maintenance procedures are done at least one time a month or after a few days. On the other hand major maintenance is done as required or at least one time after every two months. The regulations that govern how safe aircrafts are also applied to the crew.\nAs far as the crew is concerned, the major bone of contention is the number of crew members that are included in each flight as well as how experienced they are and for how long that they have flown. This is extended to how much sleep pilots have before entering into a cockpit every time. As much as most people worry about how safe they are aboard an aircraft is one of the safest activities that anyone can engage in (Krause, 2003). Rules and procedures are enforced so as to reduce the number of airborne accidents to the minimum by ensuring that all aircrafts are in proper working order at all times.\nEvery passenger who boards a plane always expects that it is well equipped with devices for utilization during an emergency situation. In many modern aircrafts such devices extend beyond procedures for ensuring that passengers are safe to warning systems that are designed to avoid accidents. These systems have to be correctly installed and constantly maintained (Stolzer & Goglia, 2011). Devices that are considered as basic for ensuring that each and every flight is safe are standards that must be adhered by each plane whether it is the smaller planes to the huge commercial jets. Large jets have a deployable slide that is used to evacuate passengers from the sides very quickly in case of an emergency. If such devices are not there, evacuation may not be feasible which portends that a jet would hit the ground while passengers are aboard which would lead to catastrophic results (Rodrigues & Wells, 2012).\nParachutes are a primary requirement in the smaller aircrafts. This device is one of the foremost precautions that is available in every plane and should be maintained adequately to ensure that it is in proper working condition and it can be deployed without damaging the parachute. Parachutes are rarely used during an accident but they still remain an option that is considered seriously (Huang, 2009).\nThere are computerized equipments that are installed in aircrafts and calibrated to avoid occurrence of accidents altogether. These are recent developments and pilots consider them as important ways of controlling planes and helping them to understand problems as they develop during a flight, thereby giving them sufficient time to appropriately react.\nThe aircraft industry has seen the enforcement of so many standards that are geared towards ensuring that all flights are safe. Companies have to hire highly experienced personnel to fly and manage their planes. Such personnel should have an established record of safety as well as in implementation of best practices. One of the most basic ways of ensuring that a plane flies and lands without interruption is that every cockpit must have two pilots just in case one of them becomes deactivated during a flight.\nStatistically the number of aircraft accidents is low as a result of developments that have enhanced the performance of aircraft through superior engine design as well as adequate training of pilots which are deemed to be the focal points of ensuring that each plane lifts off and lands without any incidences. International organizations that govern manufacture and management of aircrafts always recommend that all national authorities should implement safety regulations.\nSince the advent of flying planes have crashed leading to very serious results. This is a result of the flight nature which is unforgiving as a small medium is suspended in air to support a large mass whose failure does not give an option but quick descent with catastrophic eventualities (Stolzer & Goglia, 2011). Due to this planes are designed with an emphasis of reducing the chances of mechanical malfunctions and pilots are intensively trained.\nIn the earlier developments in aircraft technology there were those who have been of the view that a plane should be designed with consideration of how it would endure an accident. However, such a design would lead to the manufacture of the very heavy model that would be more susceptible to crash as a result of the added weight. As a result of this it is not uncommon to find the nonstructural parts of planes, especially the small ones are very light and when such planes crash devastating results follow.\nFlying in private jets is more dangerous in comparison to the scheduled flights. This is partly attributable to less maintenance and training. In most cases privately owned aircrafts do not have system redundancy to be utilized if a system failure occurs. Therefore it can be presupposed that it is safer to fly using commercial airlines (Rodrigues & Wells, 2012).\nAir traffic can be considered to be intense as there are usually ten thousand planes in flight at the same time. Despite this accidents are rarely sighted given the high number of operations. Statistically flying by scheduled airlines in the developed world is at least ten times safer than driving. However, this is not the case in many African nations and Russia. Plane accidents make headlines throughout the world when they occur (Rodrigues & Wells, 2012). The perceived impact of such accidents has magnifying effect. The main reasons that lead to the airborne accidents are flaws in the design, pilot’s errors and improper maintenance procedures.\nThere are inspectors whose sole mandate is to ensure that regulations regarding performance and quality of aircraft including personnel are adhered to. These professionals are specialized in the inspection of either single or multiple engine aircrafts or commercial carriers. Other inspectors perform inspections on the operation of airlines where they evaluate commercial carriers as well as air operations. In addition inspectors also certify flight instructors, pilots as well as ensure verification of flight training facilities and programs.\nThe aeronautical profession is very dynamic and highly technical. There are numerous technologies that emerge with a view to improve travelling through airborne means. Every flight implies that many lives are at stake at any one time which places a very high demand on the crew and pilots to be highly trained and constantly updated with new developments (Rodrigues & Wells, 2012).\nThe training of pilots must be very intensive so as to ensure that the passengers aboard their aircraft are safe. Prior to being allowed to fly an actual aircraft an aviator is required to hone his skills using simulators that emulate real flight conditions. This is intended to prepare the pilots to deal with many types of scenarios that may present themselves while in flight mode. A pilot is certified after amassing a certain number of flight hours.\nPilot trainers are also constantly put to speed with every new development so that they pass on the knowledge to their students. Aircraft mechanics and engineers are also required to be equally competent as pilots and are held responsible in case a plane develops mechanical problems while in flight (Huang, 2009). This is intended to ensure that they carefully do their work so as to prevent aircraft accidents that usually cause multiple deaths.\nPlanes do not fly a lot during winter. However, the importance of proper winterizing a plane should not be understated as it is a matter of safe flying as well as enhancing the useful life of an aircraft. Manuals usually spell out how a plane should be winterized. One basic thing that should be done is to cover the canopy, windows, prop blades and the horizontal tail as these are the most crucial external parts. Covering reduces damage that is caused by snow, ice, rain and cold which are experienced during winter.\nIn order to enhance the welfare of all passengers it is important for a pilot to be very observant about pressure and oil temperature while flying in cold conditions as such features oscillate significantly when in flight as well as while not in flight. Repairs should be immediately done in case an oil cooler bypass occurs (Stolzer & Goglia, 2011).\nOne of the most basic things to do so as to ensure that flight is completed successfully is checking that the fuel tank is full and that the valves are properly closed. Emptiness in the fuel tank of a plane leads to condensation during winter which may cause corrosion that leads to complications during flight. When flying during cold weather, a pilot should establish whether his plane requires the use of isopropyl alcohol that is added to the fuel while operating in low temperatures.\nMany regulations in aircraft management are usually precipitated by catastrophic events. Such rules have gone long way in preventing other air disasters. The ability of a pilot is one of the most crucial factors during the occurrence of accidents. Pilots are properly evaluated so as to ensure that accidents do not occur as a result of easily avoidable things like fatigue. Aviation safety will continue to be an important matter for all national and international air travel regulators. This will ensure that travelling by air continues to be the safest mode of transport in comparison with others like motor vehicle transportation (Huang, 2009).', 'If you are an ATP (Airline Transport Pilot Certificate) or CPL holder of a Foreign (ICAO) license – you are able to receive your FAA Certificate by clicking here to start the process: Foreign ATP and Commercial Pilots.\nIn our Academy, you can also go through the training program and pass an exam to obtain an ATP license (airline pilot) required to perform the functions of the pilot-in-command of an aircraft. Most of US part 121/part 135 carriers require the ATP license for employment. Similar requirements are found in legislation of most airlines in other countries. If your dream is to fly an Airbus 320 or Boeing 737 you will need this certificate to become a member of the flight crew.\nAn Airline Transport Pilots License certificate gives you the ability to carry passengers in authorized commercial operations. Pilots with an ATP certificate will meet the hiring minimums of most regional airline pilot jobs. An ATP certification will also open job opportunities with various corporate and private air carriers.\nDue to the legislation changes in The United States, effective August 2014, the requirements for ATP certification are stricter and more complex. The prerequisite for allowing you to take the written exam is the mandatory ATP-CTP course. This course, which must be taken at an FAA-approved flight center, includes 30 hours of ground and 10 hours of flight simulation. ATP-CTP course is now available at SkyEagle Aviation Academy.\nAs a part of the program, we are working in conjunction with an authorized training center to offer this type of training. After the completion of The ATP-CTP course, the candidate will be able to take the written exam in our testing center and prepare for a checkride with an FAA-examiner. Since January 2019, we have been offering this class, which starts on a weekly basis.\nAfter passing the written test, the candidate may choose the training method for obtaining the ATP Certificate.\nTo be eligible for an airline transport pilot certificate, a person must meet the following requirements:\nAfter July 31, 2014, for an airline transport pilot certificate with an airplane category multiengine class rating or an airline transport pilot certificate obtained concurrently with an airplane type rating, receive a graduation certificate from an authorized training provider certifying completion of the airline transport pilot certification training program before applying for the knowledge test;\nSource: FAR 61.153\nAirline Transport Pilot Aircraft-Single Engine Land License and Airline Transport Pilot Aircraft-Multi Engine Land License are two different types of licenses. It is necessary to obtain the Multi Engine Rating which is required for every pilot who plans to fly higher performing aircraft with two or more engines.\nThe FAA regulations require an airman to hold a type rating when flying for a particular air carrier with aircraft having a weight of over 12,500 pounds. This option is suitable for most pilots, who have more than 1,500 flight hours and do not know the extent of the exact make and model they will be flying on.\nA type rating is not required for most carriers who fly aircraft that weights less than 12,500 pounds.\nFor this reason, we offer candidates the option to conduct their training in one of our two light twin aircraft: The Beechcraft Duchess BE-76 or Tecnam P2006T. The short course consists of 7-10 hours of flight training, as well as 10 hours of ground school. A candidate must become familiar with all aspects of the aircraft prior to the checkride.\nThe examination is carried out on the exact aircraft the candidate has trained on. Upon successful completion of the check ride, the airman will receive the FAA ATP Multiengine Certificate.\nIf the candidate is positive as to which aircraft he will fly during his career, we will be able to provide the combination of ATP and Type Rating Training (A320).\nAfter successful completion of this training program, the candidate will receive both The Airline Transport Pilot Certificate and Type Rating for the selected aircraft.']	['<urn:uuid:a11a4a5c-c4dd-4710-a236-e71f5f39cb79>', '<urn:uuid:6426f9e2-0fc0-49ed-a578-b4f60cc18031>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	11	101	2588
76	old vs new entertainment environmental cost comparison	Traditional entertainment forms like classic circuses heavily relied on animal acts, particularly ones involving cattle and other livestock. Today we know that livestock contributes to 14.5% of total greenhouse gas emissions globally, with beef cattle being especially problematic. Modern entertainment alternatives, like new-style circuses such as Cirque Berserk, have evolved to be more environmentally conscious by eliminating animal acts and instead featuring human performers doing acrobatics, silk rope artistry, and stunt work, enhanced by theatrical elements like special effect lighting and innovative set design.	['Edinburgh Fringe 2018\nThis is a touring production from London’s West End featuring 30 performers who impressively cover a wide variety of circus skills in a high-energy presentation with many visual effects.\nCirque Berserk! describes itself as “Real Circus made for theatre”. Instead of a circus tent there is a stage with a dramatic setting. Instead of elephants and tigers there are acrobats, new-style clowns, silk rope artists, and stuntmen. Using special effect lighting, driving music, innovative set design, and haze, the audience is transported to a surreal world where the viewer becomes fully engaged in the experience.\nCircus has a history of 250 years. The modern circus started in London in 1768, first with trick horseback riding to music, subsequently adding clowns, bands, and acrobats. Through the Victorian era, circuses began to travel widely, in large tents and into amphitheatres. With increased popularity, productions added more “rings” for performance areas and more variety artists. The famous American entrepreneur PT Barnum, lately front and centre in the feature film “The Greatest Showman”, brought in a new dimension of unusual characters who were often put on display to attract the curious. Animal acts expanded to tigers jumping through fiery hoops, elephants rotating on stools, and bears dancing.\nAs the public concern for animal welfare increased, variety performers became more important and featured in circuses. In fact, many countries now ban live animals in circus shows. Quebec street performers designed a new-style circus that focused on storytelling with characters in shimmery costumes plying their trade in acrobatics, juggling, clown, and more. That became Canada’s Cirque du Soleil, now a gigantic international enterprise.\nCirque Beserk! stages its production in that new tradition. The show is high drama, from the pulsating drum beats to the purple-washed stage and lighting effects. The set consists of large platforms hung on angles in the background, with the ever-present giant wire globe in the middle. The costuming throughout the show is well-designed to fit each scene, from medieval frocks to stunning body suits to flamenco skirts. Acrobats and tumblers show their agility at the opening, as they easily transition between flying through the air to creating human pyramids. One moment the cast is doing cartwheels across the stage and suddenly two women in bright red and gold costumes are executing perfectly synchronized movements on ropes with the precision of Olympic ice dancers. Other skills showcased by the 30-member case include the South American Bola ropes, fire sticks, aerial silks, stilt walking, chair balancing, and even knife-throwing, just what you would expect of a top quality circus. Comic relief comes from a pair of “janitor” clowns, who employ slapstick routines to give the audience a break from the high-velocity acrobatics. And, of course, there are the death defying moments – you’ll see have to see those for yourself\nThe concept of cirque is not original but the execution is innovative. The music is a constant throbbing driver. The cast members work in large ensembles and are well-coordinated. Everything is performed with impeccable timing. The variety of scenes, reinforced by the change of costumes, music, and lighting, broaden the production.\nThis is a great show for all ages. Families will enjoy it, as the kids will be wide-eyed at the stunts, but it is also a spectacle to be appreciated by adults.', 'Global trade gives consumers access to endless tastes and flavors from around the world and a boundless supply of meats and poultry at their local grocery store. But do consumers understand the real price of global consumption? And if they did, would they change their buying patterns? The manufacturing processes involved in food production are often complex with any given product seeing many stages before it makes its way to our grocery store shelves.\nResearchers from the University of Technology Sydney and Duke University claim in their study published in Nature Climate Change that shoppers are largely unaware of the greenhouse gas emissions associated with the food they purchase.\nIt’s easy for us to satisfy our hunger through cooking technologies like microwaves, ovens, and toasters or even though delivery. While these technologies heighten short-term convenience for us, they can have long-term negative effects on the environment.\nThe study finds that if food retailers label their products indicating the amount of carbon emissions each food item generates, consumers would be willing to purchase meals and ingredients that are more environmentally friendly.\nAccording to the study, the proportion of greenhouse gas emissions that can be attributed to food production and consuming habits is between 19 to 29 percent globally. This estimate includes all steps in the entire supply chain of a product such as fertilizing, farming, processing, exporting goods through transportation, household management, and waste.\nFurthermore, the Climate Change, Agriculture and Food Security institution (CCAFS) states that livestock contributes to 14.5 percent of total greenhouse gas emissions globally. This makes the animal agriculture industry the largest contributor to emissions out of all food supply chains.\nBeef and dairy cattle create the most damaging greenhouse gasses, resulting in 65 percent of the total 14.5 percent of emissions generated from livestock, according to the UN’s Food and Agriculture Organization. This is due to cattle being fed grain which is produced using fossil fuels. Another issue is the methane that cows produce in their digestive system which is released from their manure and further contributes to the accumulation of greenhouse gasses.\nFurthermore, another food report called “Options for keeping the food system within environmental limits” also published by Nature claims that animal food processing makes up 72 to 78 percent of total agricultural emissions.\n“Beef is more than 100 times as emissions-intensive as legumes,” said Marco Springmann from the University of Oxford, a co-author of the report.\n“In today’s agricultural system, we grow plants to feed animals, which require all of those resources and inputs: land, water, fossil fuels, pesticides, herbicides and fertilizer to grow,” said Sharon Palmer, a registered dietitian nutritionist and plant-based food and sustainability expert. “And then we feed plants to animals and care for them over their lifetime, while they produce methane and manure.”\nThe University of Technology Sydney, and Duke University study further tested the knowledge of more than 1,000 consumers on 19 different food items and 18 different electrical food appliances. They asked consumers what level of carbon emissions each product was responsible for producing, and found that participants’ highly underestimated how much the two categories contributed to global warming.\n“If you ask people to guess the difference between items such as beef and vegetable soup on the environment, they assume there is not much difference, but beef soup creates more than 10 times the amount of greenhouse gases than vegetable soup,” said Dr. Adrian Camilleri, lead author on the report.\nCamilleri explains that consumers are often unaware of how the production of certain food items is causing negative effects on global warming, creating confusion with consumers as to what steps they should take in order to help create a more sustainable environment.\n“This is a bit of a blind spot because if someone wants to reduce their greenhouse gas emissions, they might think to turn off the heater, drive less or fly less,” said Camilleri. “Very few people think to eat less beef.”\nThe university study further indicates that food retailers or manufacturers could create a mandate to implement logos on food items modeled after the EnerGuide labeling used on mechanical equipment.\nThe purpose of this would be to measure the carbon footprint the food item generates, and expose that carbon footprint to consumers, giving shoppers better insight and opportunity to eat food products with less detrimental effects on the environment.\nIn testing an example of these labels on 120 participants, the researchers found the results were positive. According to the study, more consumers bought vegetable soup as opposed to beef when the carbon footprint labeling was included on the packaging.\nIn order to limit climate change in 2019, studies are showing that consumers can help by substituting some of their meat protein with a diet that contains plant and nut-based alternatives as well as buying food produced locally.\n“Research consistently shows that drastically reducing animal food intake and mostly eating plant foods is one of the most powerful things you can do to reduce your impact on the planet over your lifetime, in terms of energy required, land used, greenhouse gas emissions, water use and pollutants produced,” Springmann states.\nAlthough completely cutting out meats is not an ideal option for everyone, food experts are saying that a flexitarian diet makes for a healthy alternative that has less of a drastic impact on your body. Opting for animal protein that results in fewer carbon emissions, like eggs, fish, and poultry, followed by an abundant amount of fruits, vegetables and plant-based protein, could be a feasible alternative. According to Springmann, this option “is the least stringent that is both healthy and would reduce greenhouse gas emissions enough for us to stay within environmental limits.”\nSpringmann’s environmental food study further states, daily consumption of processed food and red meats could result to negative environmental effects increasing by 50 to 90 percent in the food industry by the year 2050. With plant-based protein being a prominent food trend, it could be possible for consumers to help prevent climate change by choosing products which create fewer carbon emissions.']	['<urn:uuid:11aff82b-4df3-479a-95da-67a4b9e8a8ab>', '<urn:uuid:d8718ba1-dba2-45c7-8348-0c8ad4dfa024>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:35:02.547110	7	84	1558
77	prayer church izra location	The church was located in Izra/Zorava, and contained inscriptions related to prayer and divine worship. One of the inscriptions found there praised God's greatness, His ability to work miracles, and emphasized the unique holiness of the Holy Trinity.	"[""Saint NameMary, Mother of Christ : S00033\nType of EvidenceInscriptions - Formal inscriptions (stone, mosaic, etc.)\nEvidence not before500\nEvidence not after600\nActivity not before500\nActivity not after600\nPlace of Evidence - RegionArabia\nPlace of Evidence - City, village, etcBosra\nPlace of evidence - City name in other Language(s)Bosra\nSakkaia / Maximianopolis\nSakkaia / Maximianopolis\nCult activities - PlacesCult building - independent (church)\nCult activities - Non Liturgical Practices and CustomsPrayer/supplication/invocation\nSourceThere is no detailed description of the stone on which the inscription was carved, and which is now lost. Maurice Sartre supposes that it must have been a large lintel with a circle containing a cross or a crown in the middle of the inscribed face, as the copy by William Waddington who saw the object indicates the presence of such a decorative motif.\nFound in a church in Izra/Zorava. First seen and copied by Ulrich Jasper Seetzen on 30 May 1805. Later independently recorded by William Waddington in the 1860s, who was probably the last person who saw the inscription. The most recent edition, by Maurice Sartre, is based on the earlier publications.\nSeetzen found another inscription in the same church (see: IGLS 15/1, no. 189). That text praises the greatness of God, His ability to work miracles, and stresses the unique holiness of the Holy Trinity. Based on this, Trombley suggested that the church was dedicated to the Holy Trinity. He also argued that the shrine was probably built soon after the replacement of the cult of an obscure pagan God, Theandrites (see the comments in E01754), by the Christian religion, which he presumed to have happened in the early 6th century. Sartre rightly rejects these hypotheseis as too far fetched and lacking basis in the evidence.\nFor another inscription from Izra/Zorava, probably from a church dedicated to Mary, see: E02114.\nDiscussionThe inscription begins with a popular dedicatory formula which appears in the anaphoras of the Liturgies ascribed to John Chrysostom and Basil of Caesarea (τὰ σὰ ἐκ τῶν σῶν σοι προσφέρομεν / 'thine own from thine own we offer unto thee'); the use of the formula in Late Antiquity is evidenced by a number of inscriptions from Anatolia and the Near East.\nLines 3-4 express the belief that the intercession of Mary (as the influential Mother of Christ) is at least welcomed, or even necessary, for a successful offering, as people can only bring meager goods, compared with the magnificence of God.\nThe inscription ends with a reference to the biblical story of the poor widow whose two copper coins were considered by Jesus to be a much better offering than the large sums brought to the Temple by rich people (see Mark 12,41-44; Luke 21,1-4).\nDating: there is no reliable way to date the inscription. A date in the 6th c. is possible, as other dated texts from Izra/Zorava come from this period.\nSartre-Fauriat, A., Sartre, M., Inscriptions grecques et latines de la Syrie, vol. 15/1: Le plateau du Trachôn et ses bordures (BAH 204, Beyrouth: Institut Français du Proche-Orient, 2014), no. 188.\nFelle, A.E., Biblia epigraphica. La sacra scrittura nella documentazione epigrafica dell'«Orbis christianus antiquus» (III-VIII secolo) (Bari: Edipuglia, 2006), no. 106.\nWaddington, W.H., Inscriptions grecques et latines de la Syrie (Paris: Firmin Didot Frères, Libraires-Éditeurs, 1870), no. 2500.\nKruse, F., Fleischer, H.L., Commentare zu Ulrich Jasper Seetzen's Reisen (Berlin, 1859), 78-79.\nSeetzen, U.J., Reisen durch Syrien, Palästina, Phönicien, die Transjordan Länder, Arabia Petrae und unter-Aegypten, vol. 1 (Berlin, 1854), 115.\nJalabert, L., DACL, vol. 3, 1738, no. 132.\nTrombley, F.R., Hellenic Religion and Christianization c. 370-529, vol. 2, (Leiden - New York - Cologne: Brill, 1994), 362 (English translation).""]"	['<urn:uuid:4a6a4227-70fd-44d2-b891-8e76a1f52a2e>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	4	38	606
78	Do fire management and biological control require expert involvement?	Yes, both methods require expert involvement but in different ways. Fire management needs conservation experts and experienced resource managers to develop comprehensive fire management plans, schedule burns, and ensure safety through careful monitoring and permits. Biological control requires scientific expertise for international cooperation, laboratory work studying life-history traits of control agents, and collaboration with various institutions. Both approaches also involve working closely with stakeholders like farmers, land managers, and conservationists.	['Burning to Restore Life\nThe crop fields and natural areas in Wisconsin’s Central Sands area make up a diverse and complex landscape that intermingles one of the nation’s most productive potato and specialty crop production areas with remnants of prairies, oak savannahs, pine breaks, wetlands, streams and lakes that existed after the glaciers receded thousands of years ago. This diversity provides ecological services—such as soil health, drainage, pollination and natural regulation of unwanted or invasive species—that go largely unrecognized and yet are essential to a healthy and vibrant ecosystem and the communities that depend on it. Much of the land in the Central Sands is privately owned by the farmers, whose families settled it generations ago. We are indeed fortunate that these far-sighted growers recognize the importance of the diversity contained on their farmsteads and are taking steps to preserve it by restoring remnants of natural habitats to their original condition. More than ten years ago, several potato growers in the Sands set out to develop more sustainable approaches to farming that preserved natural resources and ecological diversity. The program, called Healthy Grown™, involves the restoration on habitat remnants, and the use of fire is an important tool in that process.\nHistorically, the natural burning of prairies, wooded areas, and wetlands was important to maintain the diversity of species requiring more open landscapes. Without fire, plant species diversity is diminished and regional landscapes change, but with proper timing, prescribed burning controls many undesirable woody plants and herbaceous weeds while invigorating native, fire-dependent species. Prescribed burning can prepare a site for planting and/or seeding, inhibit exotic/invasive species, improve habitat for grassland species and reduce the potential for property-damaging wildfires. Fire removes invading woody plants that store most of their active growing tissue above ground while deep-rooted prairie plants can regenerate and thrive using growing tissue located below the ground. Fire also returns nutrients to the soil, and the bare soils warm up earlier in the spring to promote rapid growth of native plants.\nWhile the need for fire still exists, experts need to be the ones who schedule, manage, and control burning in our crowded world. Growers in Central Sands have been working with conservation experts and experienced resource managers on developing comprehensive fire management plans for privately owned, diverse landscapes on their farms. This effort has been led by the International Crane Foundation, where staff ecologist Jeb Barzen has been advocating for the growers and their on-farm restoration efforts. Barzen promotes the use of fire in the context of managing the farm as a whole and understanding that farms produce more than just food but also provide a healthy ecological system and diverse services, which are beneficial to communities and neighbors alike. Fire helps maintain areas to promote these ecological services and is a necessary part of restoring native ecosystems.\nA fire plan must be carefully managed and include objectives, maps, site background information, safety considerations, communication plans, weather predictions, a plan to manage smoke and appropriate permits. During the burn, the landowner, burn team, and conservation field ecologists monitor the site and ensure that the burn always remains under control. The fun part comes after the burn, when the site is monitored and the beauty of new species emerging becomes evident. Burns need to be repeated every few years to allow native species to flourish, but the results are positive each year as more species re-emerge. “Following a prescribed burn, one of my greatest joys is to see a puccoon or meadowlark return to an area where it was once thought lost for decades” said Barzen. Fire is frequently associated with destruction of property and natural areas, but when used in the right places, at the right times, it can bring benefits to all. Remember this as you travel through the Central Sands this fall and see smoke rising above the landscape. That smoke is often a local farmer with an environmental ethic, working to restore diversity!', 'Can we improve invasive species management by using biological control?\nDrone image of a wetland invaded with giant salvinia. We use different tools to measure the impact of biological control and restoration of ecological services.\nWe study invasive species impact on managed and natural ecosystems\n...and consider biological control the cornerstone of invasive species management.\nBiological control could happen in a matter of months.\nDrastic reduction of cover and biomass of giant salvinia due to action of the salvinia weevil, Cameron Parish, Louisiana. Image before was taken in June, and after was taken in September.\nWHAT WE DO\nINVASIVE SPECIES IMPACTS\nOur laboratory is involved in research on invasive species which are in different stages of the invasion process. We have worked on detection techniques adapted to the life history of a range of species including emerald ash borer, crape myrtle bark scale, and roseau cane scale. We document the distribution and dispersal mechanisms of invasive species using field surveys, remote sensing, and citizen science.\nSampling methodologies are implemented to assess the impact of invasive species and they vary a lot. For example, we use drones to measure the coverage of aquatic weeds, and record the changes in dissolved oxygen, leaf chlorophyll content and photosynthesis, plant metabolites, among others. Due to the large diversity of species, we had worked in different habitats such as coastal wetlands, freshwater marshes, roadside disturbed habitats, forests, and urban settings.\nWorking with invasive species involves direct cooperation with our stakeholders including land managers of state parks, homeowners, farmers, aquatic weed managers, foresters, scientists, plant health regulators, conservationists, among others.\nIMPROVING BIOLOGICAL CONTROL\nBiological control is the use of natural enemies to reduce pest populations. Our laboratory is actively involved in classical biological control which is the use of host-specific natural enemies from the native range of the pest. International cooperation is at the core of our efforts. We enjoy facilitating the explorations of natural enemies of pests in USA, as welll as traveling to the native range of the organisms to find natural enemies! Because some invasive species occur over large regions, we have developed collaborative projects with several institutions in Southeastern U.S.A.\nA new challenge on classical biological control is the climatic mismatch between agents and pests. Specifically, we work with subtropical weeds that are invading temperate regions such as water hyacinth, alligatorweed, giant salvinia, and parrot feather. We seek to understand the underlying problems of poor population growth of agents in the laboratory and in the field. We explore variation in life-history traits of existing populations by measuring thermal limits of agents, and search for new populations with desirable traits in the native range. Other approaches include habitat manipution to enhance overwintering survival, increase releases efforts early in the growing season, and integration with other control tactics based on site conditions.\nUnderstanding fortuitous biological control has been fascinating. We discovered three non-native parasitoids of the roseau cane scale, native weevils attacking parrot feather and a non-native delphacid associated with a dieback of elephant ear. In next few years, we will be studying the relevance of these findings from an applied perspective.\nWahl,C., Diaz, R. and J. Ortiz. 2020. Aquatic Invasions In Press.']	['<urn:uuid:8924a82f-a7f0-4b59-8d3f-bc393b754195>', '<urn:uuid:15f51fd7-e20b-4909-8676-2eedbb966a4d>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:35:02.547110	9	70	1187
79	gunshot wound vs combat injury treatment differences	While both gunshot wounds and combat injuries require immediate attention to bleeding control and airway management, their treatment protocols have some differences. For civilian gunshot wounds, the focus is on assessing ABCDE (Airway, Breathing, Circulation, Disability/Deformity, Exposure) and getting to a hospital within 10 minutes of injury. Combat casualty care emphasizes controlling extremity hemorrhage through tourniquets and hemostatic agents, with studies showing tourniquets saved numerous lives in battlefield conditions without causing limb loss. Both scenarios require attention to bleeding control, but combat medicine has more emphasis on tourniquet use and self-care/buddy-care protocols.	"['Gunshot wounds are extremely traumatic and painful for the injured person. While there are ways for you to stabilize the injured person, they still need to get to a hospital as quickly as possible. If you have a first-aid kit and know how to treat a gunshot wound you can do so immediately when you find the injured person. You may not be able to assess how much damage has been done, but controlling the bleeding may be the difference between life and death for the victim.\nIn the meantime, there are steps you can perform to ensure that you give the victim as much care and attention as you possibly can. It is vital that you act no later than 10 minutes after the gunshot itself. There are different gunshot wounds depending on where the person is hit, e.g. the head, abdomen or limbs (legs or arms). For these different areas, different procedures are needed and you must not mix one treatment for a different wound, or there is a risk to worsen the victim’s condition.\nHow the damage occurs\nBefore we get to explaining how to treat a gunshot wound we need to look a bit closer to how the damage occurs in the first place. First of all let’s take a look what a bullet looks like. It’s a small casing, which contains powder charge. As a person shots the bullet, it will fly at an incredible speed – about 1500 meters per second – the speed varies and depends on the actual bullet, its weight and the gun it’s been shot from.\nAs the bullet hits a target, the powder charge explodes and damages everything nearby. Usually the upper body (head and abdomen) are most vulnerable, and if hit by a bullet, the person may suffer rapid blood loss and serious damage to internal and vital organs.\nLet’s look even closer to how much damage a gunshot bullet can cause. As the bullet penetrates the body, it will start tearing tissue, muscles and other body structures along its path. Some non-elastic tissues like the brain may suffer more, since they can’t recoil back, like other tissues (skin for example) can. Usually, a ‘cavity’ appears at the injured place, which can’t be repaired and the wound is opened to infection and high risk of blood loss. In some cases a bullet may penetrate the body on one side and leave it on the other, which may even further progress the loss of blood.\nThe result of a gunshot wound may be various but most often the injuries are the following:\n- Heavy bleeding/blood loss\n- Injury to vital organs\n- Broken bones\nUsually surgery is necessary to remove the bullet’s pieces and if there are broken bones, the bones’ pieces must also be removed, since they can severely damage nearby tissues and/or organs. Also, the wound must be washed, disinfected and sewn.\nTreating a gunshot wound\nBefore you begin treating the wound you need to assess the severity of the damage. You have to look for the bullet and even may have to remove it, so you can safely clean the wound. If the bullet remains in the wound an infection may progress and make the victim’s condition far worse.\nIn other cases, there may be small shards from the bullet that have remained around the wound, but it may cause more damage by trying to remove them, rather than if you just leave them there. If you’re in the surgery and medical personnel can do the operation, then they can also remove all pieces from the wound.\nIf you’re somewhere in the wild, and far from a hospital or the emergency, you should at least try to remove the larger pieces of the bullet, so you can clean and disinfect the wound. You have to bandage the wound and make sure you have stopped the bleeding, before you can find a hospital. You must change the bandage regularly and keep it clean all the time, so you can prevent infection. This may be your main concern. If you managed to control the bleeding, then the next threat to you is infection. In order to perform all these, you will have to have a well-equipped first-aid kit (we have a great set of reviews here).\nIf you have antibiotics, by all means, take some. They will prevent the progress of any infection, which may have started as you got wounded. Also, don’t hesitate to take any painkillers (if you have any). They will subdue the pain and will keep you sane and focused to find your way to civilization so you can get adequate help. If you don’t know how to determine if a wound or cut is infected, take a look at our article on how to tell if infection occurred.\nAnd here is another important thing, which may help you. Depending on where you got shot, you have to keep the wound higher in elevation from the heart. For example, if you got shot in the leg, the best thing to do is to prop the leg slightly elevated, above the heart level. This will prevent from blood surging into your leg and causing too much swelling.\nIf someone else (your camping/hiking companion, friend or relative) got shot you will have to treat them yourself, before you can get to a hospital. Your first 5 actions are the so-called ‘A, B, C, D, E’. Here is what they mean:\n- Airway (A) – This is one of the most important things you must check first, meaning, can the person still breathe. If they are conscious and can speak, and don’t seem to struggle breathing, then they are fine, and the airway passage is clean. In the cases when the person is unconscious you have to assess if they can breathe or if the airway passage is obstructed somehow.\nCheck their throat by opening their mouth and see if there’s something blocking the airway passage. If the tongue is in the way, try to remove if from the way. It is often possible that the shock caused the person to literally swallow their tongue, which causes them to suffocate. If the tongue isn’t in the way, then maybe there’s blood accumulated in the throat (if for example the person was shot in the lungs, throat or other vital organs damaging the lungs).\nTry to remove the accumulated blood, by either turning the person around so the blood seeps out of the mouth, or you can soak it up with a piece of cloth.\n- Breathing (B) – If you successfully performed the above, then you probably have already determined if the person can breathe or not. If for example, there haven’t been any airway passage obstruction and yet the person in unconscious, there is a chance that maybe they don’t breathe. You may have to perform mouth-to-mouth to make the person breathe again.\nAlways check if the chest is rising or falling (a good indication for breathing). Also, don’t ignore factors like weird, rapid or in any way unnatural breathing the person has, even if they are conscious. There is a chance that something is going on and you must be alert of any changes in the victim’s behavior.\n- Circulation (C) – This is related to the blood circulation and to what extend it may cause serious blood loss. You must apply pressure on the wound so you can minimize as much as you can any further blood loss, which may occur. Also, check regularly the victim’s pulse, either at the wrist, or the throat. If the person is unconscious, and they don’t have a pulse you may have to perform CPR (rescue breathing).\n- Disability/Deformity (D) – This check is important before moving on to helping the victim. What this means is that if the person has an injury to the spinal cord (disability), and you try to move them, position them somehow different from how you found them, could potentially make their situation worse, to the extent that they remain permanently disabled.\nBy deformity we mean that the person may suffer an injury, which somehow deforms their body in an unnatural way (strangely broken limbs, joints, etc). These are closely related to the nervous system and any movement may cause even more damage. This is why it is so vital that you first assess these before you proceed to any action.\nUsually the Red Cross advises that, if a person suffers from a spinal cord injury, they should not be moved until help comes, or if you have to move them to help them breathe for example, you have to be extra careful and move them very slowly. But this is only in extreme situations where a person doesn’t breathe. Helping them breathe is of top priority.\n- Exposure (E) – It is also important that you find all the bullet wounds that you can, including any possible exit wound. You may have to look even the armpits and other such difficult to access areas. You are not advised though to completely undress the injured person, as this may cause shock for them.\nHow to properly treat different parts of the victim’s body\nAfter you’ve assessed the victim’s condition, you have to prioritize your actions. If the wound is severe and bleeds a lot, you must immediately find a way to stop the bleeding. The first thing to do is to apply pressure directly over the wound. You could use pads, clothing or even your fingers to apply pressure and try to control the heavy bleeding. Find clean pieces of cloth and bandage the wound. Use as much clothing as you can find, but make sure they are clean, or you may expose the victim to infection.\nIf any limb has been shot (arm, leg or thigh) you could apply pressure to specific areas near the wounds. For example, if the person is shot in the arm, you could press the area of the armpit, since there is an artery there and it leads directly to the heart. If you press that artery, less blood will be pumped by the heart to the arm and as a result the blood loss will be reduced. If you have some paracord around, you can use it to create a rudimentary tourniquet. If you don’t know how, take a look at our tutorial here.\nThe same applies to injuries on the legs or thighs. There is a similar artery near the bikini area, so you can press that area. It is possible that you can even feel the pulse there. Press the artery and the blood loss should diminish somewhat.\nIn cases when the gunshot wound is in the abdomen (torso) area, and vital organs may be damaged, it is usually difficult to do anything for the person, without special medical attention. Yet, some commercial products like QuikClot and Celox could help in a way. The principle behind these products is that they increase the blood clotting extremely quickly and could be helpful in severe cases.\nFor example, QuikClot contains kaolin, which is a potent chemical, accelerating the blood clotting. Celox works on the same principle and could be applied as a gauze, and claim that it’s possible to stop the bleeding within no more than 5 minutes of applying pressure to the open wound.\nThese products are mostly manufactured for the military, but they are open to purchasing to the general public. For example, you could purchase several Celox gauze and carry them always in your backpack as a life-saving measure in case an incident happens.\nYou are often advised to get to know the instructions and read the manual for using, but it is often much simpler, and you have to simply apply the pad and hold it slightly pressed to the wound so that it can do its work. As we said, 5 minutes are enough to have significantly affected the blood loss.\nAlso, you have to be prepared that the victim may suffer shock. This is a traumatic event and it can affect the victim’s behavior. Not only that but the injury itself, pain and blood loss are also affecting factors and can lead to a state of shock. In order to help the victim as much as possible, try to keep them warm (if they feel cold to the skin) or try to fan them if they are hot and seem to have a fever (possibly an infection is going on).\nIf you have antibiotics, you should administer them to prevent the spread of the infection. And again, don’t attempt to elevate the legs, if the wound is on the abdomen/torso area. There is a risk that the person may begin to suffocate due to the blood accumulation in the torso area.\nYou need to be psychologically prepared that a gunshot wound in the head is in most cases fatal. There is little you can do. The only thing that is in your hands is to get the person to a hospital as soon as humanely possible, and also, try to keep the head upright. If the head is positioned horizontally it may more rapidly fill with blood and thus the brain may swell and even if the person was still alive, an infection and brain swelling can kill them much more quickly.\nAreas like the face and neck tend to bleed profusely. The victim can lose significant amount of blood within less than a minute or two and if you don’t apply pressure immediately, the end can be fatal. The only thing to consider when applying pressure to the neck is that you don’t press the carotid arteries. They are the passage for blood to the brain and if you press these arteries, the brain will end up being oxygen deficient and you may cause even more damage to the person (leading to coma or brain death).\nAnother thing to consider is wounds to the chest and back. You must be prepared that these area also hard to treat, since the rib cage obstructs the access to the actual wound. Sometimes, lungs may be punctured by the gunshot and as the person breathes in and out, they may draw blood in the lungs (and can literally drown in their own blood). This is called ‘sucking chest wound’. To prevent that from happening, you should try to tightly seal the lung wound with a plastic or an airtight bandage.\nAs you apply it, press the bandage on three sides around the wound, but leave the fourth side open, so that you can allow the chest to have its negative pressure. The idea is that as the person inhales, the air will escape from the fourth opening/valve, and no blood will enter the wound.\nFinal tips on treating a wound\nAs you manage to get the victim to the emergency, you must tell them exactly what you did so far to stabilize the person. Also, you must remember that pressure is the major key factor to keeping a person alive as long as possible.\nWhen it comes to spinal cord injuries, as we said, move slowly, and also, try to keep the neck, head and back in one line (aligned). This will ensure that there is no further damage to any nerves.\nKeep in mind that, even if you can control the outer wound, there may be internal bleeding. You may not be able to do much about that, and it may be the thing which worsens the victim’s condition. So, even if you managed to successfully treat the exit/enter wounds, the internal damage may be far more severe.\nAnd finally, as we said, it is possible that a gunshot wound be fatal for the victim. Do not risk your own life. As long as you are safe, you can proceed to helping the victim. If you are the injured person and you’re conscious you can apply the above advice.', 'Tactical EMS: An overview\nLearn about the basics of this rapidly evolving EMS subspecialty\nJim Morrissey is a Tactical Paramedic for the San Francisco FBI SWAT team and the founder of the Tactical Medical Association of California (TMAC). Jim is also the Terrorism Preparedness Coordinator for the Alameda County EMS Agency. Jim has a master’s degree in Homeland Security from the Naval Postgraduate School in Monterey, CA. He can be reached at firstname.lastname@example.org\nWhat is a ""SWAT medic"" and what does it take to become one? Tactical medicine is a specialized and highly discriminating endeavor that requires intensive training, discipline and a unique relationship with law enforcement. Tactical medics have the primary responsibility of providing medical care to the SWAT team, but their duties extend far beyond that task.\nThis article examines the evolution, and current tactical medical principles and procedures employed by military and law enforcement agencies (LEA), specifically SWAT teams.\nTactical medicine concepts have been around since the dawn of medicine….and warfare. Even in early battles the “King’s Doctor” was to be close at hand to deal with injuries, but only to the royal few. Outcomes were not very good as the initial insult might have killed the victim; if that didn’t, then infection usually did.\nHistorical accounts and personal diaries of military medics through the centuries are fascinating and show a steady progression of tools and techniques and an ever improving understanding of injury management. The Vietnam War showed that rapid transport to a higher echelon of care had a significant impact on survival.\nThe last 10 years of conflict overseas has shown that controlling extremity hemorrhage and aggressive airway management accounted for a significant reduction in the casualty fatality rate (CFR). The CFR is the percentage of those who are wounded in battle die.1\nCasualty Fatality Rate\nWWII 19.1 %\nVietnam War 15.8\nIraq/ Afghanistan conflicts 9.4\nPrior to 2004, there were a significant proportion of deaths in American soldiers during the Global War of Terror (GWOT) associated with each of the following injuries:\n- Hemorrhage from extremity wounds\n- Junctional hemorrhage (where an arm or leg joins the torso, such as in the groin area after a high traumatic amputation)\n- Non-compressible hemorrhage (such as a gunshot wound to the abdomen)\n- Tension pneumothorax\n- Airway problems\nIt was noted that extremity hemorrhage was the most frequent cause of preventable battlefield deaths. The U.S. military re-introduced and emphasized tourniquet use and hemostatic agents with measurable success.\nTourniquets were proven to save lives on the battlefield including 31 lives saved in 6 months by tourniquets after the retraining. Kragh et al. estimated that 2000 lives were saved with tourniquets during the Iraq conflict. As importantly, there were no arms or legs lost because of tourniquet use.2\nCauses of preventable battlefield death are not that different from the potential injuries of law enforcement/ SWAT operators during high-risk law enforcement operations.\nToday\'s tactical EMS\nTactical medicine has become a discipline and specialty within law enforcement circles. High profile events such as the Columbine, Virginia Tech, Aurora Movie Theater and countless other “active shooter” incidents have shifted the way law enforcement operates.\nThere have been two major shifts in doctrine related to SWAT and law enforcement operations over the last 10 years. One change was focused on aggressively going after active shooters with whatever assets happen to be on hand, instead of waiting for a SWAT team.\nThe second has been to recognize the need for emergency medical contingency planning. This includes training officers/agents in the basics of “self-care” and “buddy care” with the focus on bleeding control.\nAnother aspect of this doctrine shift is the inclusion of organic assets (the tactical medic), and involving, or at least notifying local EMS, hospitals about law enforcement operations that have a high risk for injuries. SWAT teams have increasingly a dedicated tactical medical component as part of their setup.\nSome law enforcement agencies (LEA) have decided to send officers to EMT school, or specific tactical medical classes. That may be a workable solution; however, it is unlikely those individuals have the experience and patient assessment skill needed to be the best medical practitioner they can be.\nIt may make more sense to train an experienced medic to work in a warm zone environment and keep the scope of that person\'s job as the medic.\nTactical EMS models\nThere are countless workable models for the incorporation of a medical contingency plan for law enforcement operations. Some of the more common models are listed below:\n- Officer-Agent/medic- sworn LEO having either dual roles as an ""operator"" and medic, or strictly as a medic; but has law enforcement powers and can certainly protect themselves from potential threats.\n- Agency contract- LEA has a contract or memorandum of understanding (MOU) with a local EMS provider to provide ""up-close"" medical care. Some agencies put the medics through a Reserve Officer school, so that they can be armed as LEOs.\n- Individual contract- an individual or a team made up of individuals who have either a contract or MOU between them and the LEA providing coverage for SWAT missions and training.\n- ALS Stand by- No Tactical Medics, but ALS unit will stage close by and respond if scene is secured by law enforcement.\nLearning about Tactical EMS\nThere are many in the EMS field that would like to explore the option of getting into the field of tactical medicine. In addition to the medical training one already has such as a physician, nurse, paramedic or EMT, it is highly recommended to procure specific tactical medical education.\nPrograms such as NAEMT Tactical Combat Casualty Care (TCCC) and other like it are one to two days in length and have been well received by the EMS community. The International School of Tactical Medicine (ISTM) offers a 2 week intensive program aimed at medical practitioners who need basic training on law enforcement operations, and how to work within a law enforcement team as the medic.\nThe emphasis in this program is to insure that the tactical situation is resolved and EMS providers are not put in harm’s way. There are many skills sessions and tactical/ medical scenarios to test the knowledge learned under stressful conditions.\nThe tactical medic\nThe Tactical Medic is the logical liaison to the on-scene EMS assets that support law enforcement operations. Typically the Tactical Medic will have a face-to-face meeting with EMS supporting units if they are available.\nIt is imperative that EMS providers are not put in harm’s way and are not allowed to enter a scene where there is a shooter, or other threats are still possible. In an active shooter type situation, once the threat is eliminated, the scene is no longer a hot zone. A quick sweep from a SWAT team can confirm this assumption.\nIf there are significant casualties, the next priority should be to escort the EMS providers into the scene quickly and safely into the newly created “warm zone”. They would work closely with the Tactical Medic throughout the event.\nScenario of a tactical mission\nOn a typical hypothetical SWAT operation there are several phases and steps that take place well before the “hit”. Most often, the mission is a planned high risk search or arrest warrant. After getting a ""warning order"", the SWAT team operators and all of the support elements (medical, communications, negotiators, etc.) typically convene at a Forward Staging Area (FSA).\nA briefing will occur, where mission goals, subjects, and target location layouts are reviewed. The tactical medic then calls the local EMS transport provider to have an ALS ambulance stage close to the location.\nMost times if EMS is notified beforehand, they are welcome into the briefing. The local EMS crews are given instructions and a communications plan. In most cases they follow the vehicle convoy in and are in close proximity to the target location.\nLocal hospitals and trauma centers are notified that the mission is taking place and to be on alert in the event of casualties.\nPersonally, I’ve been on close to 200 SWAT missions and thankfully there have been no gun-fights with injuries.\nOn most of SWAT missions, we have a day or two to plan. Most large city police departments have a higher percentage of spontaneous SWAT missions such as a bank robbery “gone bad” or a barricaded subject.\nTactical medicine is an exciting and evolving field of emergency medicine. Consider taking a tactical medicine class and see if you are up to the task.\n1 NAEMT Tactical Combat Casualty Care (TCCC) Curriculum, http://www.naemt.org/education/TCCC/tccc.aspx\n2 Kragh J et al. Practical use of Emergency Tourniquets to stop Bleeding in Major Limb Trauma Journal of Trauma, 2008:64; 30-50 http://www.smcaf.org/InPressKragh.pdf']"	['<urn:uuid:1dab3eb9-3c3f-4163-bd88-2aaec4a49539>', '<urn:uuid:0908b512-9801-41af-bf44-7b2cbbb746a8>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:35:02.547110	7	92	4116
80	I'm curious - which causes more problems: new companies or substitute products?	Both new entrants and substitute products pose significant threats but in different ways. New entrants can destabilize markets through aggressive tactics like lower pricing and better supplier terms, especially when there are low barriers to entry, minimal startup costs, and reduced customer loyalty. Substitute products are particularly dangerous because they are typically cheaper alternatives that can serve the same purpose, even if produced by different companies. The threat from substitutes becomes especially severe in markets with poor branding and weak customer loyalty. Both threats can be countered through building strong brand loyalty, but substitutes require additional focus on proper advertising and keeping customers informed about product qualities.	['The success of a business is guaranteed when all the stakeholders offer it a healthy environment. Players like the government, investors, customers, competitors, and employees are indispensable aspects that determine the success of a business. There is a need for a healthy relationship amongst stakeholders to ensure the smooth running of organisations. Michael Porter identified five key aspects that are important in explaining the structure of organisations. He referred to these aspects as forces that shape the activities of businesses and regulate their behaviour. These forces are the bargaining powers of suppliers, bargaining powers of buyers, threats of new entrants, the threat of substitutes and rivalry amongst competitors. This discussion examines the application of Porter’s five forces in a grocery delivery business.\nBargaining Power of Suppliers\nAn investment is a collection of financial and human resources that are combined to convert raw materials into finished goods or offer services to clients. Investors must pay for the time, energy and other inputs to ensure a business is operational. The profits that businesses generally are proportional to the expenses incurred in acquiring labour or raw materials used in the production process. The bargaining power of suppliers should be lower than that of a business to ensure fewer expenses are used to convert raw materials or labour into finished goods and services respectively. For instance, a grocery delivery business obtains goods from farmers and other producers. The prices of these goods should be controlled by the business if it wants to generate profits. However, if the suppliers control the pricing, the business will be forced to incur huge expenses in acquiring its supplies. A balanced power of the supplier means that both parties (supplier and grocery delivery business) get profits because of the standardised prices of goods.\nBargaining Power of Buyers\nThere cannot be any business without customers because they are the engine that drives investments. Buyers offer market and pay for goods or services produced by a business. Businesses should ensure they price their goods and services within the powers of buyers. Very high prices discourage buyers from buying goods because they cannot afford them. On the other hand, moderate prices attract customers and retain them because they know they can afford to pay for the goods or services offered by a business. However, too low prices may compromise the quality of goods or services produced by a business and force customers to keep away from substandard products. A grocery supplier cannot offer very cheap prices for goods because customers will doubt their qualities. Investors should conduct surveys and identify the correct prices of goods or services so that their products do not cost too expensive or cheap.\nThe threat of New Entrants\nSuccessful businesses attract investors in an industry because it is perceived to be profitable. The threat of new entrants refers to the market instability occasioned by an increase of investors in the industry. Some new entrants have enormous capital and experience in other fields and may carry these strengths and use them to destabilise markets. Low start-up costs, lack of regulations and reduced customer loyalty give more powers to new entrants to succeed and offer stiff competition to veteran investors. In addition, minimal economies of scale and easy access to clients and inputs put existing businesses at risks of collapsing or reduced profitability when new entrants strike. The grocery industry is volatile, and new investors can easily enter and control it. Companies can reduce the threat of new entrants by enhancing their brand images and winning customer loyalty. In addition, they can utilise patent rights to protect their products and services and block new entrants from imitating them. Lastly, they can create alliances with companies that produce complementary goods to eliminate room for cheap competition bought by new entrants.\nThreat of Substitutes\nSubstitutes are products or services that can perform similar roles in the absence of each other, even though they are produced by different companies and do not have similarities. Substitutes are serious threats to businesses because most of them are cheaper and available in the market. Modern technology has created room for innovative investors to explore different markets and introduce new products. For instance, grocery delivery faces stiff competition from genetically modified products that are cheaper and available in all markets. The threat of substitutes becomes lethal due to the use of poor branding and marketing techniques in a market that does not have strong customer loyalty. In addition, companies that produce goods that do not have real benefits to customers may fall victim to this threat. Proper advertising, labelling, branding and keeping customers informed about the latest changes in the qualities of a product are the best ways of fighting this threat.\nCompetition amongst Rivals\nCompetition from rivals is the main threat that existing businesses face in their daily activities. Threats from competitors become stiff when there is a shrinking or slow market that does not promote quick stock turnover. In addition, this threat occurs when there is no differentiation, and almost all businesses produce similar goods. Sometimes the high costs of production and the short lifespan of commodities force businesses to use complicated tactics to remain relevant in the market. In addition, some investors have a passion for leading others and ant to stay at the top regardless of the profits they generate. Stiff competitions complicate exit plans and make them expensive. A grocery delivery business may fight this threat by focusing on a unique supply sector. For instance, an investor may decide to supply naturally produced fruits and vegetables that are more attractive than modified ones. The supplier may decide to build a strong relationship with customers and win their loyalty to ensure competitors do not sway them.', 'Harvard economics professor Michael Porter developed the five forces model to help business executives understand and deal with competition. Porter argues that factors affecting competition are largely similar regardless of the industry. His five forces that shape competition include competition among existing competitors, bargaining power of customers, bargaining power of suppliers, threat of substitute products and threat of new entrants.\nCompetition among Existing Competitors\nEstablished companies have competitive advantages such as loyal customers, premier locations, good supply chains and a better understanding of the market. New companies may find it difficult dealing with already-established competitors, since they need to convince their customers to switch providers. To overcome this hurdle, some firms get information about rivals from customers, suppliers and employees, and study the specific products and services they sell. They then use this to develop strategies to overcome the competition.\nBargaining Power of Customers\nCustomers’ bargaining power greatly influences a company’s competitive strategy. Buyers have the power to demand lower prices in a system where their number is relatively small compared to that of sellers. In such cases, when a customer becomes dissatisfied with the quality of service or pricing in a certain store, he can easily switch to rivals. On the other hand, if your business is the lone provider of a product or service, the customer has little bargaining power and you can be more aggressive in your pricing strategy.\nBargaining Power of Suppliers\nSuppliers provide the inputs or raw materials for business. Their bargaining power has a direct impact on the company’s profit margins, as the price at which they sell inputs to business will determine the selling prices of the finished products. Suppliers have a high bargaining power in cases where they are few, there are no substitutes for supplies or when there is no unity between buying companies. If there are two suppliers for a specific good your business needs, for example, and one goes out of business, the other supplier has the power to raise prices, which will impact your ability to keep your cost structure intact.\nThreat of Substitute Products\nSubstitute products give consumers the opportunity to choose alternatives based on price or quality. This increases competition, since a consumer can easily forgo buying a particular product in place of another. For instance, a consumer may buy almond butter in place of peanut butter because of the former’s apparent health benefits. For a business to maintain superiority over rivals, it needs to invest more in developing brand loyalty based on consumers’ preferences to stand out from other products in its category.\nThreat of New Entrants\nIf an industry has low barriers to entry, a business faces the risk of competition from new entrants. New entrants can introduce tactics like lower pricing and aggressive marketing strategies in an effort to attract customers. New companies also can offer suppliers better prices and terms so that they increase competition with existing players. High entry barriers, such as customer loyalty, limited distribution channels and shortage of key resources, may limit this competitive danger.\n- Jupiterimages/Creatas/Getty Images']	['<urn:uuid:75b435b3-d941-46cb-8120-cfc5cef923df>', '<urn:uuid:6d04fa3c-354c-4b40-b996-81f9f5bf4e3e>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T20:35:02.547110	12	107	1463
81	What is the purpose of anti-money laundering regulations in banking, and how do they help prevent illegal activities?	Anti-Money Laundering (AML) regulations prevent the process of dealing with criminal proceeds and projecting them as untainted property through banking channels, as well as preventing financing of terrorism-related activities. These regulations require banks to know their customers (KYC regulations) and monitor their transactions. Banks must submit specified information about cash and suspicious transactions to the Financial Intelligence Unit, India (FIU-IND), which analyzes and forwards intelligence information to law enforcement agencies. This helps in identifying suspicious transactions and fighting against money laundering and terrorist financing.	"[""Current bank account is opened by businessmen who have a higher number of regular transactions with the bank. It includes deposits, withdrawals, and contra - transactions. It is also known as Demand Deposit Account.\nWhat is the type of information I need to submit to the Bank at the time of account opening?\nYou need to submit the information to establish your identity and your address and a copy of your latest photograph. You can choose from the variety of the documents acceptable by the Bank for this purpose. The documents accepted by the Bank are as under:\nCurrent Account Rules- FAQ\nProof of identity: (any one of the following)\n2) PAN Card\n3) Voters Identity Card\n3) Driving License\n4) Any other proof acceptable to the bank, as per RBI guidelines.\nProof of current address: (any one of the following)\n1)Telephone bill (Landline)\n2)Passport (supported by PAN allotment letter, Mobile Bill, Credit Card Bill)\n3)Ration card (supported by PAN allotment letter, Mobile Bill, Credit Card Bill)\n4)Latest Bank account statement (2 months transactions) with KYC confirmation remarks and officers signature/designation.\n5)Registered leave & license agreement with proof of permanent address or notarized leave & license agreement with police N.O.C. with proof of permanent address.\n6)Business proof such as Shop & Establishment, VAT registration, IT return etc.\n7)Any other proof acceptable to the bank./ as per RBI guidelines.\nDocuments required (Partnership)\n1)Photographs of all the partners\n2) Address proof of all the partners\n3) Identity Proof (PAN Card, Passport, Senior Citizen Card) – all partners\n4) Introduction is must (usually from a current a/c holder)\n5) Partnership deed – duly registered.\n6) Stamp of the firm\n7) Registration certificate.\n8) Telephone bill in the name of firms/partners.\nDocuments Required( Trust-Clubs Association)\n1) A/c opening form\nPhotos of the office bearers\n3)Identity Proof of office bearers(pan Card, Passport, Senior Citizen Card)\n4)Address proof of all the office bearers\n5)Stamp of the Clubs / Associations\n6)Original certificate of registration\n7)Certified true copy of memorandum of association\n8)Certified true copy of the rules, regulations, bye-laws\n9)Resolution of managing committee certified true by the chairman/secretary authorized\n10)The type of a/c to be opened should be specified in the resolution11) Resolution should be in terms of rules, regulations, bye-laws of the body and not contrary to them.\nI approached the Bank in the status of a legal person such as a company trust etc. and not as a natural person. Do I still need to submit information mentioned above\nFor legal persons, the Bank is required to obtain the proof of the existence of entity (such as copy of the certificate of incorporation, memorandum/articles of association, trust deed etc. as may applicable) along with a copy of the resolution from the entity's managing body (such as the Board of Directors or the Board of Trustees as may be applicable to it) to open the account. Further, the person interacting with the Bank for operating the account on behalf of the entity needs to obtain a mandate to that effect from the entity and establish his identity (photo identity and photograph of the person is required). Please contact our branch official to assist you in terms of the documents that could be submitted for a legal person.\nI am doing a large cash withdrawal (above Rs. 10 lacs) and the cashier is insisting that I should provide him information on the purpose of withdrawal. After all, I am withdrawing money from my own account and therefore, why should I share this information with the Bank?\nThe extant regulations of RBI require the Banks to closely monitor the large cash transactions and the teller is just discharging his regulatory obligation. Such an information assists the Bank in identifying any unusual transactions that are being routed through it.\nYou are repeatedly mentioning that the extant regulations of RBI and the rules notified under Prevention of Money Laundering Act, 2002. What do they really talk about and try to achieve?\nThe regulations and the rules referred to above deal with what is called as an initiative of Anti Money Laundering (AML) and Countering the Terrorist Financing (CTF). These rules and regulations try to prevent the process of money laundering (i.e., the process of dealing with criminal proceeds and projecting it as untainted property through banking channels) and financing of terrorism related activities. The regulations, in the broadest sense, require the banks to know their customers (popularly known as the Know Your Customer or KYC regulations) and to monitor their transactions. Further, specified information under the PMLA about the cash and suspicious transactions need to be submitted to the Financial Intelligence Unit, India (FIU- IND, a Government of India department, which analyses and forwards the intelligence information to the law enforcement agencies). Compliance with AML regulations assists the banks and the society at large in keeping a close watch on the transactions to identify suspicious transactions, if any, and thereby provide intelligence information (through reports to FIU-IND) to the law enforcement authorities to deal with such transactions as per the applicable laws and regulations. Co-operating with the Bank by providing required information, assists the Bank and the society in a long way in their fight against money laundering and terrorist financing.""]"	['<urn:uuid:466bae0a-026c-4988-92d0-868df9e95331>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	18	84	871
82	Are bullion coins valued same way as problem coins?	No, bullion coins are primarily valued based on their melt value and metallic content, with the U.S. Mint guaranteeing the metallic content of their uncirculated coins. In contrast, problem coins certified by NCS are valued based on their condition and degree of damage, with lightly cleaned coins typically worth about half of their non-cleaned counterparts, regardless of metal content.	"['The Federal Trade Commission has this advice for anyone planning to buy collectible coins: Research, research, research.\n• Ask for the coin\'s melt value, the intrinsic bullion value of the coin if it were melted and sold. Read trade magazines like Coin Dealer Newsletter or Certified Coin Dealer Newsletter to determine the wholesale value of the coins.\n• Examine the coins in person. Photos and conversations with the seller won\'t cut it.\n• Ask for the coin\'s grade, and whether it was certified by one of the major grading services: Professional Coin Grading Service (PCGS) and Numismatic Guaranty Corp. (NGC). Verify the serial number of the certification with the service that supposedly issued it.\n• Get a second opinion about the grade of the coin.\n• Get written copies of the coin dealer\'s return and buyback policies, and note their restrictions.\n• Consider the tax implications. Certain gold products, classified by the government as collectibles, may care higher income tax implications.\n• Beware of counterfeits, false grading claims and inflated valuations, and take note of consumer complaints about particular companies on Internet bulletin boards.\nThe FTC says buyers of bullion or bullion coins -- whose value depends on their metallic content --also need to get smart. ""Being uninformed can have serious consequences,"" the agency says.\nThe most common bullion coins are the Amercian Gold Eagle, the Canadian Maple Leaf, the Australian Gold Nugget and the African Krugerrand.\nThe U.S. Mint has two types of bullion coins. Proof coins are minted for collectors and are usually sold by the U.S. Mint in protective displays. Uncirculated coins are sold to authorized buyers based on the current spot price, plus a premium charged by the Mint. Their metallic content is guaranteed.\nForeign coins may not meet the same standards. The prices of these coins depends most heavily on their melt value rather than their grade.\nThe FTC advises:\n• Ask for the coin\'s melt value.\n• Shop around. Most banks offer gold bullion, often at lower markups than dealers.\n• Get an independent appraisal rather than accepting the seller\'s.\n• Factor in associated costs, such as insurance, safe deposit box or other storage fees.\n• Walk away from sales pitches that minimize risk and those claiming that written risk disclosures are unnecessary formalities.\n• Refuse to ""act now,"" regardless of the consequences.\n• Know the seller. Look beyond simple ratings by the Better Business Bureau.\n• Ask for a guaranty or certificate of authenticity for the bullion\'s precious metal content. But beware: such certificates can be faked. Research the company behind the guaranty before you buy.\n• Be wary of false claims. Some sellers overprice their coins, lie about the melt value or try to sell bullion coins as rare collectibles.\n• Ignore sales pitches that make claims that particular precious metals are likely to increase in value. Be especially cautious of leveraged investment scams premised on these pitches. Here\'s how they work: A seller offers bullion at a fraction of its cost, with a lender paying the balance. The lender holds onto the bullion as collateral.\nIf the metal increases in value, you sell some or all of it to pay off your loan. But in the meantime, the seller may levy costly fees and commissions that wipe out the initial investment.\nAnd if the value of the metal drops below a set margin, you may have to put in more money or sell the metal to pay off the loan. ""Either way, it\'s very likely you\'ll lose some or all of your investment,"" the FTC says.', 'I’ve discussed many times the process in which how nice coins are assigned price levels. But how are problem coins valued? This is an interesting question and one which is becoming a bit easier to answer since NCS coins have become a well-accepted part of numismatics. (Before I begin, I should state here that NCS or Numismatic Certification Service is a division of NGC that certifies and encapsulates “problem coins” which NGC does not see fit to put in their regular holders. This includes coins that are harshly cleaned, polished, heavily scratched, rim filed, etc. NCS only uses adjectival grades—i.e., they would call a coin “AU details” as opposed to “AU55 details.”)\nThe reason why non-problem coins are easier to value than problem coins is, well, because they don’t have problems. There is a greater degree of consistency of appearance between an 1830 half eagle in PCGS AU55 (or NGC AU55) than there is with this same issue when it has the details of an AU55 but it has been cleaned.\nLet me explain what I mean by this. If you were to call me up and offer me an 1830 half eagle in PCGS AU55, I would have a decent idea of what to expect. I’m figuring that it has light wear, a decent amount of remaining luster, maybe a few scattered marks in the fields and probably a pretty good overall appearance. But if you call me an offer me an 1830 half eagle in an NCS holder that states the coin has “AU details” but has been “cleaned,” I’m not sure what to expect. Has it been lightly cleaned or harshly cleaned? Does it have an acceptable appearance or does it look overly shiny from having been polished or perhaps whizzed?\nFrom my experience with viewing NCS coins, there is a very wide range of coins in these holders.\nI’ve seen coins that NCS has called “cleaned” that look pretty acceptable to me; not very different, in fact, from coins encapsulated by both NGC and PCGS. I’ve also seen coins placed in NCS holders that had planchet flaws or mint-made surface that, in my opinion, could just as easily be in “normal” NGC or PCGS holders.\nBut back to cleaned coins and how to value them. As a general rule of thumb, I think that if a coin has been lightly cleaned it is worth around half of what a non-cleaned example would be worth. The NGC or PCGS AU55 1830 half eagle that I mentioned above is a $60,000 coin if it has a decent, original appearance. In an NCS “AU details—cleaned” holder it’s more likely worth $30,000 or so. And if it’s a very harshly cleaned AU coin with some damage as well it is more likely worth in the area of $15,000-20,000.\nThis brings me to a philosophical question. Would I, as a collector, want to own a very rare but very ugly coin that costs $20,000 or $30,000? I do not personally like problem coins even though I can understand their value and why certain collectors would want, say, an “affordable” example of a desirable coin like an 1808 quarter eagle. But I would rather spend my $20,000 or $30,000 on a coin that was less rare and more aesthetically pleasing. If my budget for a very rare coin like an 1808 quarter eagle was only $20,000-30,000 I would re-examine the “need” for this issue to be in my collection and would spend the money on something nicer.\nRemember what I said above about the varying degrees of “cleanedness” or damage seen on NCS coins? I might actually not mind owning an NCS encapsulated 1808 quarter eagle with VF or EF details that had been lightly cleaned and which had a good appearance. The thing to consider, though, is that many other collectors feel the same way and such a coin might actually sell for a value level not much less than a VF or EF that was in a regular NGC or PCGS holder. The real question is would I want to own an 1808 quarter eagle that looked like it was run over by the proverbial train. And if I did, what would I pay for it?']"	['<urn:uuid:4c2c6731-9abe-4d85-946b-d56366da231d>', '<urn:uuid:55d1e0a8-e12b-4381-b6c4-193fadb18407>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:35:02.547110	9	59	1301
83	safe food pathogens regulation split fda usda	The regulatory authority is divided between agencies: USDA's Food Safety and Inspection Service (FSIS) is responsible for meat, poultry and egg products safety, while FDA handles canned and bottled goods, and processed items like frozen pizzas, sandwiches and flavoring. Products containing meat that has already been inspected, such as pizzas with pepperoni, don't pose increased health risks but require daily inspection, adding costs. It would be more efficient to give USDA authority over all animals and animal products (including fish, seafood, bison, dairy) and let FDA handle processed goods.	['Editor’s note: If you had a magic wand, how would you conjure up sustainable ways to make the food supply safe? We asked several people to consider the possibilities. Here is another response, from Dr. Richard Raymond, former Undersecretary for Food Safety at the U.S. Department of Agriculture.\nIf I had a Food Safety Magic Wand … that would be a daunting development, so to make it less daunting and to stay more focused, I am going to assume my Magic Wand is to primarily make our meat and poultry products safer. I will let the former FDA food safety leaders pen their own cures for what ails the food safety arena that FDA has responsibility for.\nBut before I focus primarily on the U.S Department of Agriculture’s (USDA) responsibility for meat, poultry and egg products safety, I want to repeat, briefly, what I said in an article for Food Safety News that was posted Jan. 3, 2011, and can be read in its entirety here.\nI am going to use my Magic Wand to improve food safety by using its powers to give the USDA and its Food Safety and Inspection Service (FSIS) regulatory authority over all animals and animal products, including all fish and seafood, bison, dairy products and eggs, and to give FDA regulatory authority over all canned and bottled goods, and processed entities like frozen pizzas, sandwiches and flavoring.\nThe meat in these proposed FDA products has already been inspected and passed. Adding pepperoni to a pizza that contains spinach, green onions, tomatoes, cheese, pepper and other products that have been linked to foodborne illnesses does not change the public health risk, nor does adding beef to the vegetables in vegetable beef soup increase risk. But these additions of meat do add cost by now requiring daily inspection of the product.\nThe savings produced by moving these very low risk products to the FDA could be used to increase the inspection of animals and animal products, such as eggs and oysters, that are now FDA responsibility and have been linked to foodborne illnesses by daily FSIS inspection.\nThe very next thing I would use my wand for is to declare whole carcass, low dose, non-penetrating irradiation to be a processing aid, not a food additive. That done, the industry can now embrace the concept, explore its applications, and over the next few years begin to significantly decrease the pathogen load on carcasses prior to the application of other processing aids down the line, greatly reducing our risks of falling ill from consuming meat.\nAnd, since industry will now have this very important tool to help it produce a safer product, I am now going to declare all non-O157:H7 Shigatoxin producing E coli strains to be adulterants.\nAt the same time, I am going to take personal risk using the Wand as my protective shield and establish a tolerance level for E coli. Before you declare me clinically insane, think about the effectiveness of the current “zero tolerance” policy. It is, at this time at least, unobtainable and non-enforceable. Develop a realistic tolerance level that is reachable with today’s interventions, and then enforce it. Allow FSIS to bring action against those few plants that provide the greatest risk to the public with their product.\nAnd speaking of plants that impose a risk, I am also waving my wand at FSIS and mandating that they do trace back to the source of contamination to the very best of their ability. For example, in the FSIS testing program for 2010, 64 ground beef samples were positive for E coli O157:H7. Of those positives, 29 were found at plants that used only outside source material and did no slaughter of their own. And, these 29 plants used only one source in the tested lot. And that same product from that source was most likely sold to other processors as well. Yet FSIS maintains that so much ground beef is a blended product from multiple sources that trace back is next to impossible. Magic Wand, fix this. Find the source and take action.\nWe have other products available to assist us in making our products safer, but sometimes the bureaucracy of having three agencies (Animal and Plant Health Inspection Service, FDA and FSIS) with overlapping oversight of these products produces little movement in the approval process, as we saw with the E. coli vaccine. So I will use the powers of the Wand to declare that Phage treatments are a processing aid, not a food additive, and can therefore be used on primals and trim without labeling restrictions.\nBefore my Magic Wand’s powers weaken, I want to accelerate the research on the efficacy of the E coli vaccines. One way I will do this is to provide tax incentives for those companies willing to spend their own resources in this effort. Heck, I think I will provide tax incentives for any research into more effective ways to produce a safer product. And then I will wave the wand again, and make FSIS a more cooperative partner with plants looking for new ways to improve safety, and become less of a hindrance to them.\nNow that my wand has effectively provided me with the information to declare the E. coli vaccine to be a very effective pre-harvest tool in reducing E coli in beef, I am going to declare the pathogen to be an environmental hazard worthy of the government’s attention to reduce its presence.\nProduce is accountable for 34 percent of all E. coli O157:H7 foodborne illnesses, ground beef being responsible for 33 percent, according to Robert Tauxe of the Centers for Disease Control and Prevention. One-third of E. coli illnesses are not even related to food, but come from our drinking water, recreational water, petting zoos and person-to-person contact. We need to get it out of the environment to save lives. T hat means reducing it in cattle. The vaccine will help us get there.\nBut why should the rancher, feeder or packer bear this expense? Ultimately it will be the purchaser of beef that pays the price. So, Magic Wand, make the government develop and pay for a mandatory vaccination program for beef that will save children’s lives, just like they do for most childhood vaccines available now. The federal government took positive steps to reduce or eliminate Bovine tuberculosis and brucellosis, it should do it for E. coli O157:H7.\nIt would truly have to be magical, but, when weather permits, washing cattle in the feed lots or on the farms, before loading into trucks and hauling to the holding pens, would effectively reduce the pathogen loads in and around the slaughter facilities.\nAnd the Magic Wand is going to try to find out what truly causes “event days” in beef slaughter facilities so preventive measures can be introduced and implemented by all facilities. And this just might include changes in facility design to reduce pathogens being introduced by measures other than just the cattle.\nI think the Magic Wand, if it has any energy left after taking on the payment method for the E. coli vaccination program, will probably do the same thing for Salmonella vaccines in our poultry flocks.\nAnd then the Wand is going to help me convene a high-level conference on antibiotic use in food animals, and the participants will come willing to listen and to learn, and to stop the non-productive claims that are so often repeated. There is a middle ground here that can be reached but, if it is not, Co\nngress will come along and\ncreate problems like it did with catfish inspection.\nSpeaking of Congress, the Magic Wand will give me the power to immediately rescind the actions of Congress that prohibit FSIS from moving forward with risk-based inspection. This was a budget neutral process that would have increased inspection activities in plants with poor safety records and/or producing high risk products, and would have reduced inspection activities in plants with stellar safety records and plants producing extremely safe products.\nThe Wand holder feels this will be easier to accomplish now that Congress has mandated in the Food Safety Modernization Act that the FDA use risk as a key element in determining inspection levels and frequency.\nWhile on the subject of high risk products, frozen ground beef patties are a special problem. When cooked in restaurants with validated and documented kill steps, frozen patties are not a problem. But when cooked at tail gate parties, pool parties, camp grounds, etc, we have a problem. I might lend the Magic Wand to someone else and let them decide whether to ban the sale of frozen GB patties at retail, or to require that the meat be irradiated with penetrating beams.\nAnd lastly, while the Wand still has some energy left in it and powers to create change, I am going to try and make that final kill step in all homes and restaurants and institutions a reality. My plan will use the same energy and resources that the government used to educate us all about the switch to cable television and the “dangers” of Y2K to help the American public understand that raw meat and poultry should not be considered sterile.\nAs a result of this education effort, the majority of the American public will now know how to safely handle raw meats and poultry and, most importantly, how to measure the internal temperature for doneness.\nActually, this should probably have been the first action taken using the power of the Magic Wand. It is by far the most important and will have most immediate effect on the safety of meat and poultry products.']	['<urn:uuid:1d8a53ff-1bc3-4f39-b9bb-d43887bf17cc>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	7	89	1613
84	phenol medicine uses side effects	Phenol is used as a medicine to relieve pain and irritation from sore throat, sore mouth, or canker sores. However, it can cause serious side effects. When used as directed, side effects may include difficulty breathing and worsening of pain, redness, swelling, or irritation in the mouth area. If phenol is misused or overexposed, it can cause severe burns to skin, eyes, and digestive tract, trigger seizures and coma, interfere with breathing patterns, and cause dangerous heart rhythm changes.	"['Description and Brand Names of Painalay – Phenol (Oromucosal Route)\nUS Brand Name\n- Assure Sore Throat\n- Cheracol Sore Throat\n- Isodettes Sore Throat\n- Osco Sore Throat\nPhenol is used to relieve pain and irritation caused by sore throat, sore mouth, or canker sores.\nThis medicine is available without a prescription; however, your doctor may have special instructions on the proper use and dose for your medical problem.\nThis product is available in the following dosage forms:\nBefore Using Painalay – Phenol (Oromucosal Route)\nIn deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For this medicine, the following should be considered:\nTell your doctor if you have ever had any unusual or allergic reaction to this medicine or any other medicines. Also tell your health care professional if you have any other types of allergies, such as to foods, dyes, preservatives, or animals. For non-prescription products, read the label or package ingredients carefully.\nNo information is available on the relationship of age to the effects of phenol in the pediatric population. Safety and efficacy have not been established in children below 3 years of age.\nNo information is available on the relationship of age to the effects of phenol in geriatric patients.\nAlthough certain medicines should not be used together at all, in other cases two different medicines may be used together even if an interaction might occur. In these cases, your doctor may want to change the dose, or other precautions may be necessary. Tell your healthcare professional if you are taking any other prescription or nonprescription (over-the-counter [OTC]) medicine.\nCertain medicines should not be used at or around the time of eating food or eating certain types of food since interactions may occur. Using alcohol or tobacco with certain medicines may also cause interactions to occur. Discuss with your healthcare professional the use of your medicine with food, alcohol, or tobacco.\nOther Medical Problems\nThe presence of other medical problems may affect the use of this medicine. Make sure you tell your doctor if you have any other medical problems, especially:\n- Infection in or around your mouth or\n- Large sores in or around your mouth – The chance of side effects may be increased.\nProper Use of Painalay – Phenol (Oromucosal Route)\nUse this medicine exactly as directed by your doctor. Do not use more of this medicine, do not use it more often, and do not use it for a longer time than directed. To do so may increase the chance of absorption into the body and the risk of side effects.\nThis medicine should be used only for problems being treated by your doctor or conditions listed in the package directions. Check with your doctor before using it for other problems, especially if you think that an infection may be present.\nBe careful not to get any of this medicine in your eyes because it can cause severe eye irritation. If any of the medicine does get in your eyes, wash it with water and check with your doctor right away. Also, be very careful not to inhale (breathe in) the medicine.\n- Apply or spray to the affected area.\n- Remain in place for at least 15 seconds, then spit the medicine out of your mouth.\nDo not use this medicine for more than 2 days without checking first with your doctor.\nThe dose of this medicine will be different for different patients. Follow your doctor’s orders or the directions on the label. The following information includes only the average doses of this medicine. If your dose is different, do not change it unless your doctor tells you to do so.\nThe amount of medicine that you take depends on the strength of the medicine. Also, the number of doses you take each day, the time allowed between doses, and the length of time you take the medicine depend on the medical problem for which you are using the medicine.\nFor oromucosal dosage form (spray):\nFor sore throat or mouth pain:\n- Adults and children 12 years of age and older – Spray 5 times to the affected area every 2 hours.\n- Children 3 to 12 years of age – Spray 3 times to the affected area every 2 hours.\n- Children younger than 3 years of age – Use and dose must be determined by your doctor.\n- For sore throat or mouth pain:\nIf you miss a dose of this medicine, take it as soon as possible. However, if it is almost time for your next dose, skip the missed dose and go back to your regular dosing schedule. Do not double doses.\nStore the medicine in a closed container at room temperature, away from heat, moisture, and direct light. Keep from freezing.\nKeep out of the reach of children.\nDo not keep outdated medicine or medicine no longer needed.\nAsk your healthcare professional how you should dispose of any medicine you do not use.\nIf your condition does not improve within 7 days, or if it becomes worse, check with your doctor.\nAfter spraying this medicine to the mouth or throat of your child, watch the child carefully to make sure that he or she does not get any of the medicine into his or her eyes or he or she does not inhale the spray.\nStop using this medicine and check with your doctor right away if you have difficulty with breathing; fever; skin rash; or worsening of pain, redness, swelling, or irritation in or around the mouth.\nCall your doctor right away if you start to have a severe sore throat or sore throat that occurs with a high fever, headache, nausea, or vomiting. These maybe signs of an infection.\nSide Effects of Painalay – Phenol (Oromucosal Route)\nAlong with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention.\nCheck with your doctor immediately if any of the following side effects occur:\nIncidence not known\n- Difficulty with breathing\n- worsening of pain, redness, swelling, or irritation in or around the mouth\nOther side effects not listed may also occur in some patients. If you notice any other effects, check with your healthcare professional.\nCall your doctor for medical advice about side effects. You may report side effects to the FDA at 1-800-FDA-1088.', ""What is phenol?\nPhenol may be found as a translucent, clear, or light-pink crystalline mass; a white powder, or a clear liquid. It has a sweet, sharp odor. Phenol is used in many commercially available products including plastics, resins, fertilizers, paints, photographic developers, and some medicines.\nWhat immediate health effects can be caused by phenol exposure?\nPoisoning can occur when phenol gets on the skin or in the eyes, when it is inhaled, or when it is swallowed. Skin and eyes can be mildly or severely burned, depending on how much and how long the phenol was in contact with them. Breathing phenol vapors can burn the lining of the nose, throat, and lungs, just as it burns the skin. Severe injury to the lungs can cause them to fill with fluid, making breathing difficult. Swallowing phenol burns the lining of the digestive tract and can result in internal bleeding. Generally, the more serious the exposure, the more severe the symptoms. Phenol is absorbed easily into the body through the skin, lungs, and stomach. The brain is very sensitive to phenol. Phenol can cause seizures and coma and may interfere with the brain's control of regular breathing patterns. It can cause dangerous rhythm changes in the heart.\nCan phenol poisoning be treated?\nThere is no antidote for phenol, but its effects can be treated, and most exposed persons do get well. Persons who have had a serious exposure may need to be hospitalized. If phenol got in your eyes, the doctor may put a special dye in your eyes and examine them with a magnifying device. If you swallowed phenol, you may have been given a solution containing charcoal, which will soak up phenol in your stomach.\nAre any future health effects likely to occur?\nA single small exposure from which a person recovers quickly is not likely to cause delayed or long-term effects. After a severe exposure, you may not notice any symptoms for up to 24 hours.\nWhat tests can be done if a person has been exposed to phenol?\nDepending on the severity of symptoms, the doctor may order blood tests, urine tests, chest X-ray, and a heart monitoring test. These tests may show whether damage has been done to the heart, kidneys, lungs, or nervous system. Abnormally high amounts of phenol may be found in the urine if exposure was severe. Testing is not needed in every case.\nWhere can more information about phenol be found?\nMore information about phenol can be obtained from your regional poison control center; your state, county, or local health department; the Agency for Toxic Substances and Disease Registry (ATSDR); your doctor; or a clinic in your area that specializes in occupational and environmental health. If the exposure happened at work, you may wish to discuss it with your employer, the Occupational Safety and Health Administration (OSHA), or the National Institute for Occupational Safety and Health (NIOSH).\nVisit our Regulatory Agencies page to find contact information for the aforementioned government agencies.""]"	['<urn:uuid:672d819e-582e-459a-9f9e-765b34481293>', '<urn:uuid:a4f34945-ff45-4874-9f1a-ad9cdffa70e8>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:35:02.547110	5	79	1589
85	How do the excavation challenges faced by modern archaeologists at both the Arctic Circle's Jurassic sites and ancient Carthage compare in terms of preservation and recovery of valuable specimens?	The excavation challenges at both sites are significant but differ in nature. In the Arctic Circle, excavators face physical challenges like permafrost, mud with caramel-like consistency, and the need for heavy equipment like chainsaws to access specimens, particularly when excavating plesiosaur remains. The preservation of specimens is affected by natural forces like rain and wind. At Carthage, the challenges were more related to historical human intervention and bias, particularly during the 1830s-1900s excavations, where archaeologists focused primarily on Roman history and anti-Semitic bias influenced which artifacts were preserved or discarded. Additionally, many of Carthage's original structures were dismantled over time for building projects, further complicating archaeological work.	"['NG Emerging Explorer Jørn Hurum is currently on Spitsbergen Island in the Arctic Circle excavating the remains of ancient marine reptiles worthy of the most fantastic Norse legends. Follow the expedition here on Explorers Journal through updates from him and his team, and catch up on his previous expedition for more.\nBy Erik Tunstad\nTo us, Svalbard has a year zero. We call it Dorsoplanites, an easily recognizable layer in the rocks dating from the upper Jurassic. Relative to this layer you can date and place everything else. A find is located ten meters under Dorsoplanites for example, or another is five meters above.\nDorsoplanites’ distinguishing characteristics are probably the result of some storm, or rather a series of storms, much much stronger than anything we know today, that occurred 145 million years ago. They killed both the sea-floor-dwelling animals which in this case appear to be mostly buchia-molluscs, and the open-ocean-swimming life forms, mainly squid and ammonites–spiral shaped, fossilized motor snakes. Stig found an exceptionally nice one yesterday.\nThe Jurassic was generally warmer than today, Pat explains, and the storms probably a lot more common, and much more violent.\nAs we look at the Jurassic, below Dorsoplanites, as a rule of thumb we can say that a centimeter of deposits correspond to 5000 years, Jørn says. It must have been quite a calm period, where lots of the same happened, millennia after millennia.\n“With that rate, two meters represent one million years,” he adds. “When we know that the entire span of existence for a species is someplace between one and two million years, it means that this is an amazing opportunity to study how these animals changed through time.”\nAbove the Dorsoplanites layer, into the Cretaceous rocks, the rate varies more. At times it seems like nothing’s deposited at all, in other layers a lot has been deposited over a short period of time. In other words, it was a very unstable environment.\nFive meters above Dorsoplanites, a little into the Cretaceous that is, lies the long necked plesiosaur that Julie and Oyvind have been working on the last couple of days. The dig site is more or less pure mud, and the specimen has already got the nickname Britney – presumably because of the song “Dirty Girl.”\nShe’s laying butt out, with her neck disappearing into the mountain. There could be a skull here somewhere – a real treasure. The Spitsbergen Jurassic Research Group has found many plesiosaurs, but so far, no skulls.\n“Why?” Jørn thinks aloud. “The plesiosaurs had big and heavy bodies, a long thin neck and a relatively small head at the end of this. When they died, they would have hit bottom with the heavy body first – and with great force. The head would be lying higher up than the rest, more exposed to scavengers and erosion. Could that be the reason why we never find them?”\nBut Britney has potential. At least the head hasn’t been destroyed by the last years of rain and wind. What we have of the rest of the specimen is now covered in plaster. But the neck can be up to three meters long, and it disappears under a massive block of permafrost. All that dirt has to go, before we get the answer.\nJulie, Oyvind, Stig and yours truly drag the necessary equipment up. Oyvind carries the chainsaw.\nChainsaw in rock and permafrost: the stone bursts into a million pieces all over the place, the frost and the mud give it the consistency of caramel – or frozen nougat, but even harder.\nIt will be a long haul. Stone splinters fly, the motor roars and smokes from the cutting edge. Oyvind uses all his muscle power to push the blade down, cutting as deeply as possible. A few minutes pass, then the machine has to be stopped and the chain has to be tightened. The mechanism is covered in sludge. The muck is scraped off, and it’s ready for another round. The motor starts reluctantly and the chain goes round slowly, but picks up speed after a while – and Oyvind is ready for yet another exertion.\nThe clock is getting close to eleven p.m., and the machine stops yet again. Out come the screwdrivers—but they’re no use—there’s no more chain to tighten. It is all used up.\nIt’s a bit of a pain, because there are a few hundred kilos of permafrost left to remove before we have dug out enough to continue detailed excavating tomorrow. We are starting to get tired now. We are wading in caramel sludge and it is clotting under our boot soles, making our feet weigh 20 kilos a piece. It is cold, our muscles buzz and our bodies ache.\nThe next weapons are chisels; meter-long metal chisels. And pickaxes. And raw muscle power.\nAn hour later the rest of the team members come climbing up from their own quarries.\n“This is our last and only chance, ever, to find the head of a plesiosaur,” Jørn says. Tomorrow could be really exciting. This day is over. (Even though the girls just have to have a sledging competition on the way to the mess tent.)\nDinner is served after midnight. It’s a tired gang sitting around the fire place. I put on some relaxing music–Iron and Wine. Not even Jørn is nagging for heavy rock.\nAbout half past one, most of us creep into our sleeping bags. But I can hear deep male voices from the mess tent as I drift into unconsciousness. Stig and Oyvind, I presume…\nRead More From the Expedition', ""Carthage was a Phoenician city-state on the coast of North Africa (the site of modern-day Tunisia) which, prior the conflict with Rome known as the Punic Wars (264-146 BCE), was the largest, most affluent, and powerful political entity in the Mediterranean. The city was originally known as Kart-hadasht (new city) to distinguish it from the older Phoenician city of Utica nearby. The Greeks called the city Karchedon and the Romans turned this name into Carthago.\nIt was founded c. 814 BCE by the legendary Phoenician queen Dido, increased in size after an influx of refugees from the city of Tyre following Alexander the Great’s conquests of 332 BCE, and afterwards expanded until it was the seat of the Carthaginian Empire with colonies (such as Sabratha) along the North African coast, in Sicily, Spain, and elsewhere; these would all be lost following the Punic Wars which elevated Rome to Carthage’s former position as the greatest Mediterranean power.\nThe history of the ancient city is usually divided into five periods:\n- Ancient Carthage (Punic Republic) – c. 814-146 BCE\n- Roman Carthage – 146 BCE - 439 CE\n- Vandal Carthage – 439-534 CE\n- Byzantine Carthage (Exarchate of Africa) – 534-698 CE\n- Muslim Arab Carthage (Islamic Carthage) – 698-1270 CE\nOwing to limitations of space, this article will primarily deal with Ancient Carthage/the Punic Republic.\nIn 698 CE, the city was conquered during the Muslim Arab invasion of North Africa and destroyed. It would be rebuilt, though on a modest scale compared with the city at its height, until it was completely destroyed under the reign of Muhammad I al-Mustansir (r. 1228-1277 CE) after defeating the European Christian invasion of the Eighth Crusade of 1270 CE. The site would continue to be inhabited, though the ancient ruins were neglected until the 1830s CE when modern excavations began.\nFoundation & Expansion\nAccording to legend, Carthage was founded by the Phoenician Queen Elissa (better known as Dido) c. 814 BCE; although Dido's historicity has been challenged, the founding does date to about this time. Dido was allegedly fleeing the tyranny of her brother Pygmalion of Lebanon, landed on the coast of North Africa, and established the city on the high hill later known as the Byrsa. The legend claims that the Berber chieftain who controlled the region told her she could have as much land as an ox hide would cover; Dido cut a single ox hide into thin strips and lay them end-to-end around the hill, successfully claiming it for her people.\nDido’s reign is described by the Roman poet Virgil (l. 70-19 BCE), and others, as impressive, noting how the city grew from the small community on the hill to a grand metropolis. This account, and others like it, are legendary but Carthage, which seems to initially have been a minor port on the coast where Phoenician traders stopped to resupply or repair their ships, was clearly a major center of trade by the 4th century BCE.\nThe city developed significantly following Alexander's destruction of the great industrial and trade center of Tyre (considered Carthage’s mother-city) in 332 BCE when Phoenician refugees fled from there to Carthage. These Tyrians arrived with whatever wealth they had and, since many whom Alexander spared were those rich enough to buy their lives, they landed in the city with considerable means which established Carthage as the new center of Phoenician trade.\nThe Carthaginians then established a working relationship with the tribes known as the Masaesyli and the Massylii of the North African Berber (Imazighen) Kingdom of Numidia who would fill the ranks of their military, primarily as formidable cavalry troops. From a small town on the coast, the city grew in size and grandeur with enormous estates covering miles of acreage. Carthage quickly became the richest and most powerful city in the Mediterranean.\nCarthaginian government, formerly a monarchy, was a republic based on meritocracy (rule of the elite) by the 4th century BCE. The top position was held by two elected magistrates known as suffetes (“judges”) who governed in conjunction with a senate of between 200-300 members who held the position for life. Laws were passed by an assembly of citizens who would vote on measures proposed by the suffetes and senate. The aristocrats lived in palaces, the less affluent in modest but attractive homes, and the lower classes in apartments or huts outside the city.\nTribute and tariffs regularly increased the city’s wealth on top of the lucrative business in maritime trade. The city’s harbors were immense, with 220 docks, and gleaming columns which rose around it in a half-circle, in front of towering arches and buildings ornamented with Greek sculpture. There were two harbors, one for trade and the other for warships, which operated constantly in resupplying, repairing, and outfitting vessels. The Carthaginian trading ships sailed daily to ports all around the Mediterranean Sea while their navy, supreme in the region, kept them safe and, also, opened new territories for trade and resources through conquest as the Carthaginians built their empire.\nThe city had four residential sections, which grew up around the citadel of the Byrsa in the center, and was surrounded by walls which stretched 23 miles (37 kilometers) in length from the harbors inland. The city had all the accommodations and refinements of any great ancient city – a theater for entertainment, temples for religious observances, a necropolis, an agora (marketplace) – but on a much grander scale. Its patron deity was the goddess of love and fertility, Tanit who was worshipped alongside her consort Baal-Hamon. It is possible that children were sacrificed to Tanit in the sacred precinct known as the Tophet, but this claim has been challenged, and it is equally likely that the Tophet of Carthage was simply a necropolis reserved for infants and the young.\nAffluence & Invasion\nThe city’s wealth was due not only to its advantageous position on the North African coast, from which it could control sea traffic between itself and its colony on Sicily, but also to the people’s skill in agriculture. The writer Mago of Carthage (dates unknown) wrote a work of 28 volumes devoted to agriculture and veterinarian science which was considered the most comprehensive on the subject of its time and reflects the Carthaginian’s intense interest in farming and animal husbandry. Mago’s works were considered so important that they were among the few that would be spared by the Romans after Carthage’s final defeat in 146 BCE. Roman references to the books are now all that remain of them.\nThe Carthaginians planted fruit trees, grapes, olive trees, and vegetables in a ring of gardens irrigated by small canals and then expanded their cultivation outward beyond the city walls to fields of grains. The fertility of the land, and their expertise in cultivation, increased the city’s wealth through trade with the interior as well as maritime trade elsewhere as Carthage continued to flourish.\nIt was this expansion that first brought Carthage into conflict with others. In 310-307 BCE, North Africa was invaded by Agathocles of Syracuse (r. 317-289 BCE) who sought to subdue Carthage and use her wealth to fund his wars. Agathocles was able to feed his army easily off the land because the crops grew in such abundance. He was only defeated because the Libyans and Berbers, who worked the land, sided with the Carthaginians who had treated them well. Agathocles was driven from North Africa and Carthage continued to prosper until it became involved in a conflict with Rome, then just a small city-state on the Tiber River in Italy, in 264 BCE.\nThe Punic Wars\nControl of Sicily was divided between Rome and Carthage who supported opposing factions on the island which quickly brought both parties into conflict directly with each other. These conflicts would be known as the Punic Wars from the Phoenician word for the citizens of Carthage (given in Greek as Phoinix and in Latin as Punicus). When Rome was weaker than Carthage, they posed no threat. The Carthaginian navy had long been able to enforce the treaty which kept the Roman Republic from trading in the western Mediterranean. When the First Punic War (264-241 BCE) began, however, Rome proved far more resourceful than Carthage could have imagined.\nThough they had no navy and knew nothing of fighting on the sea, Rome quickly built 330 ships which they equipped with clever ramps and gangways (the corvus) which could be lowered onto an enemy ship and secured; thus turning a sea battle into a land battle. After an initial struggle with military tactics, Rome won a series of victories and finally defeated Carthage in 241 BCE. Carthage was forced to cede Sicily to Rome and pay a heavy war indemnity.\nFollowing this war, Carthage became embroiled in what is known as The Mercenary War (241-237 BCE) which started when the Carthaginian army of mercenaries demanded the payment Carthage owed them. This war was finally won by Carthage through the efforts of the general Hamilcar Barca (l. c. 285 - c. 228 BCE), father of the famous Hannibal Barca (l. 247-183 BCE) of the Second Punic War.\nCarthage suffered greatly from the First Punic and Mercenary War and, when Rome occupied the Carthaginian colonies of Sardinia and Corsica, there was nothing the Carthaginians could do about it. They tried to make the best of their situation by expanding holdings in Spain but again went to war with Rome when Hannibal attacked the city of Saguntum, an ally of Rome in Spain, in 218 BCE.\nThe Second Punic War (218-202 BCE) was fought largely in northern Italy as Hannibal invaded Italy from Spain by marching his forces over the Alps. Hannibal won every engagement against the Romans in Italy. In 216 BCE he won his greatest victory at the Battle of Cannae but, lacking sufficient troops and supplies, could not build on his successes. He was finally drawn from Italy and defeated by the Roman general Scipio Africanus (l. 236-183 BCE) at the Battle of Zama, in North Africa, in 202 BCE and Carthage again sued for peace.\nPlaced, again, under a heavy war indemnity by Rome, Carthage struggled to pay their debt while also trying to fend off incursions from neighboring Numidia under the king Masinissa (r. c. 202-148 BCE). Masinissa had been Rome's ally in the Second Punic War and was encouraged by Rome to raid Carthaginian territory at will. Carthage went to war against Numidia and, in so doing, broke the peace treaty with Rome which forbid Carthage from mobilizing an army.\nCarthage felt it had no choice but to defend itself against Masinissa's invasions but was censured by Rome and ordered to pay a new war debt to Numidia. Having only recently paid off their debt to Rome, they now owed a new crippling war debt. Rome was not concerned with whatever conflict Carthage and Numidia were involved in but did not care for the sudden revitalization of the Carthaginian military.\nCarthage believed that their treaty with Rome was ended when their war debt was paid; Rome disagreed. The Romans felt that Carthage was still obliged to bend to Roman will; so much so that the Roman Senator Cato the Elder ended all of his speeches, no matter what the subject, with the phrase, “Further, I think that Carthage must be destroyed.” In 149 BCE, Rome decided upon just that course of action.\nA Roman embassy to Carthage presented a list of demands which included the stipulation that Carthage be dismantled and then rebuilt further inland, thus negating the long-recognized advantage it had in trade from its position on the coast. The Carthaginians, understandably, refused to do so and the Third Punic War (149-146 BCE) began.\nThe Roman general Scipio Aemilianus (l. 185-129 BCE) besieged Carthage for three years until it fell. After sacking the city, the Romans burned it to the ground, leaving not one stone on top of another. A modern myth has grown up that the Roman forces then sowed the ruins with salt so nothing would ever grow there again but this claim has no basis in fact. It is said that Scipio Aemilianus wept when he ordered the destruction of the city and behaved virtuously toward the survivors of the siege.\nUtica now became the capital of Rome’s African provinces and Carthage lay in ruin until 122 BCE when Gaius Sempronius Gracchus (l. 154-121 BCE) the Roman tribune, founded a small colony there. Gaius’ political problems, and the memory of the Punic wars still being too fresh, however, caused the colony to fail. Julius Caesar proposed and planned the rebuilding of Carthage and, five years after his death, Carthage rose again. Power now shifted from Utica back to Carthage – which became Rome’s breadbasket owing to the same agricultural success which had enriched it before - and it remained an important Roman colony until it fell to the Vandals under their king Gaiseric (r. 428-478 CE) in 439 CE.\nCarthage had risen in prominence as Christianity grew and Augustine of Hippo (St. Augustine, l. 354-430 CE) contributed to its prestige by living and teaching there. The city was considered so illustrious, in fact, that the Council of Carthage of 397 CE was held there; the series of synods which would confirm the biblical canon for the Western Church, legitimizing the narratives which would come to be known as the Bible. The Vandal invasion of North Africa did nothing to halt Christianity’s development there, but tensions would rise between the Arian Christians (the Vandals primarily) and Trinitarian Christians just as they did elsewhere.\nThe Vandals under Gaiseric took full advantage of the location of their new city and plundered passing ships at will while also raiding coastal cities. Roman attempts to dislodge them failed and so a treaty was signed in 442 CE between Gaiseric and Valentinian III (r. 425-455 CE) acknowledging the Vandal Kingdom of North Africa as a legitimate political entity and establishing peaceful relations. When Valentinian III was assassinated in 455 CE, however, Gaiseric disregarded the treaty, believing it was an agreement only between himself and the emperor, and sailed for Rome. He looted the city but, in accordance with the request of Pope Leo I (served 440-461 CE), did not damage it nor harm the populace. The Vandals would continue to hold Carthage, and profit from its location, until after Gaiseric’s death.\nThe later Vandal king Gelimer (r. 530-534 CE), an Arian Christian, reinstituted the persecution of Trinitarian Christians which enraged the Eastern Roman emperor Justinian I (r. 527-565 CE), a trinitarian, who sent his great general Belisarius (l. 505-565 CE) to North Africa. Belisarius won the short-lived Vandalic War (533-534 CE), brought Gelimer back to Constantinople in chains, and restored Carthage to the Byzantine Empire (330-1453 CE) under which it continued to flourish.\nUnder the Byzantines, Carthage prospered through trade and as a major source of grain for the Eastern Roman Empire (the Western Roman Empire having fallen c. 476 CE). Around 585 CE, Carthage became the seat of the Exarchate of Africa under the Byzantine emperor Maurice (r. 582-602 CE), a separate administrative region established for more effective rule of the western areas of the empire.\nIn 698 CE, the Muslims defeated the Byzantine forces at the Battle of Carthage, destroyed the city completely, and drove the Byzantines from Africa. They then fortified and developed the neighboring city of Tunis and established it as the new center for trade and governorship of the region. Under the Arab Muslims, Tunis fared better than Carthage, but the city continued to thrive until the Eighth Crusade of 1270 CE when it was taken by the European Crusaders who fortified the citadel of the Byrsa. Once they were defeated, Muhammad I al-Mustansir had the city’s defenses torn down and many of the buildings razed to prevent any further such occupation.\nThe site of the ancient city continued to be inhabited and was included in the region taken by the Ottoman Empire (1299-1922 CE) who had no interest in excavating the ruins. The stones of the fallen houses, temples, and walls were carried off for personal or administrative building projects or left where they had been found. Modern archaeological excavation began in the 1830s CE through the efforts of the Danish consulate and continued under the French between c. 1860-1900 CE.\nFurther work at the site was undertaken throughout the first part of the 20th century CE but, as at Sabratha and other sites, the archaeologists were more interested in the Roman history of Carthage. The political and cultural zeitgeist of the time defined the Carthaginians, who were Semites, as a people of little value, and anti-Semitism significantly influenced not only the interpretation of physical evidence but the choice of what was kept for placement in museums or discarded.\nThe history of the period of Ancient Carthage, therefore, suffered as much from these modern-day excavations as from the city’s destruction by Rome or later conflicts. It was not until after World War II that systematic, unbiased, work at Carthage would begin; a paradigm consistent with the excavation and interpretation of many other ancient sites.\nCarthage still lies in ruin in modern-day Tunisia and remains an important tourist attraction and archaeological site. The outline of the great harbor can still be seen as well as the ruins of the homes, public baths, temples, and palaces from the time when the city of Carthage ruled the Mediterranean as the most opulent jewel of the North African coast.""]"	['<urn:uuid:34c1dfee-0182-4e06-b283-f89c96b04cd5>', '<urn:uuid:98ea1cde-1a21-4563-a4c6-88da15ad2cb2>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:35:02.547110	29	107	3851
86	rice storage losses developing countries percent	Studies conducted in developing countries have shown that nearly 10% to 15% of paddy is lost during postproduction operations due to improper post-harvest techniques.	['Gunathilake DMCC1 and Senadeera W2*\n1Department of Agro-Technology, IARS, University of Colombo, Sri Lanka\n2Department of Engineering, University of Southern Queensland, Australia\n*Corresponding author: Senadeera W, Department of Engineering, University of Southern Queensland, Australia\nSubmission: May 25, 2022; Published: August 18, 2022\nISSN 2639-0590Volum4 Issue1\nStudies conducted in developing countries have shown that nearly 10% to 15% of paddy is lost during postproduction operations due to improper post-harvest techniques. These studies also found; major component of these losses occurred during storage due to practices. Hermetically sealed storage can be used as an alternative method for commercial scale paddy storage with a view to cut down post-harvest losses. Very less research studies have been conducted in this important area. Therefore, this study was conducted to evaluate the effect and impacts of commercial scale modified atmospheric storage conditions for altering of physical properties and quality parameters of paddy under tropical climatic conditions prevailing in Sri-Lanka. In the study storing paddy under commercial level modified atmospheric storage conditions for 9 months and compared quality changes of paddy such as physical properties, milling characteristics and other quality parameters compared against, paddy stored in conventional warehouse. Moreover, oxygen level, temperature inside the modified atmospheric storage and warehouse were also measured. It was observed modified atmospheric storage took 6 weeks to reduce oxygen level from 20.5% to 3.2%. Temperature fluctuation inside the modified atmospheric storage was very low in comparison with warehouse storage. Moisture content of initial warehouse samples and modified atmospheric storage were not significantly different. Modified atmospheric storage paddy samples reported similar initial values in terms of quality parameters such as impurities percentage, immature seed (Bol) percentage and damaged seed percentage even after 9 months of storage. Whereas those values were significant changed in warehouse samples. Weight/mass loss of grain during storage period was significantly low in cocoon (modified) samples. Bulk density and paddy kernel hardness values were significantly reduced in both storage methods against its initial values. Rice kernel whiteness was preserved by modified atmospheric storage in comparison to conventional storage. Germination percentage of paddy was significantly reduced from its initial value in both storage methods. Paddy kernel resistance against milling stress increased during modified atmospheric storage. It can be concluded that modified atmospheric storage has more advantages in comparison to conventional warehouse storage for large quantity commercial grain storage. However, modified atmospheric storage condition must be maintained throughout storage period to gain those advantages\nKeywords: Commercial paddy storage; Modified atmospheric storage; Paddy quality\nLoss assessment studies conducted in developing countries have shown that nearly 10 to 13% of paddy is lost during postproduction operations. It is also found that major component of paddy harvest is losses during storage practices. On the other hand, paddy price drops even less than cost of production at the harvesting season due to high supply. It needs to promote farmers to store paddy till the off-season for overcome this problem. Modified atmospheric storage is one of the suitable storage methods for grain storage because it can be overcome most of storage quality problems. However, paddy should be dried up to 12% moisture content before modified atmospheric storage . Many research studies carried out in different countries revealed that growth of fungus, insects and pests can be inhibited under the hermetic storage conditions and metabolic rate of grain will be decreased due to lack of oxygen . A very few research studies have been performed in Sri Lanka for evaluation of physical and quality changes of paddy under commercial level modified atmospheric storage of paddy under tropical conditions. Therefore this research study is focused on to evaluate and compare conditions (such as temperature, oxygen percentage), change of initial physical properties, grain qualities and milling qualities (such as bulk density, thousand kernel weight/mass, kernel hardness, impurity percentage, immature grain percentage (Bol), moisture content, kernel color, husk content percentage, degree of polish percentage, total yield, broken grain percentage, head rice yield percentage and germination percentage between commercial scale modified atmospheric storage and conventional warehouse storage for 09 month storage period.\nFreshly harvested paddy variety BG 352 was used for the experimentation. Paddy seed was dried up to 12% moisture content before the experiment. Modified atmospheric conditions to the paddy was given using the Grain Pro cocoon bag manufactured by Grain Pro Inc. USA. As shown in the Figure 1., 1150kg of paddy seeds bagged in 22 poly-sack bags and they were storaged in the Grain Pro cocoon bag, sealed and placed in the warehouse. Same amount of paddy seeds bagged in same poly-sack bags and placed in a warehouse under ambient condition. The 300g paddy samples from the two treatments were used in initially and after the storage trial to measure following mentioned physical properties for quality parameters and milling characteristics. Inside oxygen level of the hermetically sealed cocoons was measured using an oxygen meter manufactured by Grain Pro Inc, USA once in two weeks till the end of the experiment. Inside temperature of hermetically sealed cocoon and outside warehouse temperature were measured using thermocouples placed inside sealed cocoon and outside warehouse. Grain quality parameters of paddy such as impurity percentage, immature grain percentage (Bol), moisture content damaged seed percentage and milling characteristics such as husk content percentage, degree of polish percentage, total yield, broken grain percentage and head rice yield percentage were measured before and after storage trail. Milling characteristics were measured using the methodology outline by Bal et al. . Other physio-chemical properties of paddy such as germination percentage, bulk density, thousand seed mass, kernel hardness and seed color were measured initially and after 8 months’ storage by following methods.\nFigure 1: Filling sealing and placing of cocoon bag.\nMeasuring of grain weight loss during storage\nTotal weight loss percentage under different storage methods was calculated by dividing final weight losses from initial weight (equation 01). The final total weight of paddy bags under 2 different storage method was recorded for calculating weight loss percentage.\nMeasuring of germination percentage\nStandard germination test, 50 paddy seeds placed in germination paper wrapped with adequate water and covered with polyethylene sheet and kept room temperature for 7 days was performed.\nMeasuring of moisture percentage\nMoisture content of paddy subjected to different storage treatments was measured using the method suggested by Association of Official Analytical Chemists (AOAC 2000) that drying of paddy sample for 24 hours at 120 ℃. Accordingly, moisture content of seeds was calculated using the equation 2. It was replicated 3 times.\nBulk Density is defined as the mass of the sample per unit bulk (seed + air space) volume. The volume of laboratory, bulk density apparatus was filled by soybean. Mass of filled soybean was measured and bulk density was determined using equation 3. It was replicated three times.\nMeasuring of 1000 seed mass\n10 seeds were selected from the sample and their mass were measured. It was replicated ten times and average mass of 10 seeds was taken. Thousand (1000) seed mass value obtained by multiplying the average mass value of 10 seed by 100. Precision laboratory top loading balance was used to measure the mass of the seed.\nMeasuring of seed hardness\nA compression test was carried out to measure seed hardness (yield stress). The Instron, TA, XT2 texture analyzer has been adapted to perform a compression test. Force at rupture was considered as the seed hardness. Three replicates were considered for each measurement and the measured compression force was averaged.\nMeasuring of seed color\nPaddy seed color was measured by using Mini-scan XE plus Hunter Lab Colorimeter. Hunter scale ‘L’ ‘a’ ‘b’ values were measured. ‘L’ stood for lightness (black=0, white=100), ‘a’ represented for greenness and redness (+a, redness; -a; greenness), ‘b’ indicated blueness and yellowness (-b, blueness; +b, yellowness).\nMinitab statistical software was used for analyzing the data. Each treatment was replicated two times. Analysis of Variance (ANOVA) on Complete Randomized Design (CRD) by General Liner Model (GLM) procedure was performed and treatment means were separated by the Duncan’s Multiple Range Test (DMTR) at α=0.05 level of significance.\nChange of oxygen level and temperature inside the hermetically sealed cocoon\nOxygen level reduced inside cocoon from environment O2 level i.e., 20.5% to 3.0% within 6 weeks of storage. It was reported minimum oxygen level 1.8% after 6 months sealed storage. However, oxygen level fluctuated in the range of 1.8% to 3.5% inside of sealed cocoon after 6 weeks. Figure 2 shows the temperature fluctuation inside the hermetically sealed cocoon and warehouse. The temperature fluctuation was low inside the cocoon in comparison to warehouse.\nFigure 2:Change of temperature inside cocoon and outside environment (warehouse).\nComparison of quality parameters, milling qualities and physical properties of paddy between hermetic cocoon and warehouse storage\nPaddy kernel moisture content was not observed significant difference between hermetically sealed sample and warehouse. Average moisture content was 12.66 and 13.3% in hermetically sealed and warehouse samples respectively. Comparison of initial paddy quality values such as impurities %, immature seed (Bol)% and damaged seed % with warehouse sample and cocoon sample after trial period is shown in Figure 3. Warehouse samples showed significant changes from initial values. However, no significant differences were observed between initial samples and hermetically sealed storage samples with respect to quality values. Due to high respiration rate of seed in warehouse sample immature seed (Bol) % was increased significantly comparison to the sealed cocoon sample. Insect pest and fungal attacks are major causes that alter quality values of warehouse paddy seed sample. But Insect pest and fungal attacks were not observed in hermetically sealed cocoon samples. Growth of fungus, insect and pests were inhibiting under the hermetic storage conditions and metabolic rate of grain is also be decreased due to lack of oxygen [1,3]. And impurity percentage increase due to secretions of insect and totally damaged seed by insect. Milling characteristics such as husk content percentage, bran content percentage and total milling yield did not change significantly with storage practices. However, it is clear from Table 1 significant variation can be observed between cocoon sample and warehouse stored samples with respect to broken grain percentage and head rice yield percentage. It was not observed significant difference between initial values of broken grain and head rice yield percentage with cocoon stored samples. Cocoon stored samples reported similar values as its initial value. But values were significantly different in warehouse sample. Hence it can be concluded that the paddy stored under hermetically sealed conditions (modified atmospheric) withstand /resistant to milling stress. Kernel weight/mass significantly reduced in conventional warehouse storage compared to hermetically sealed cocoon.Hermetically sealed cocoon preserved kennel weight/mass due to slow metabolic rate of seeds [1,3]. Results showed that the initial bulk density and paddy kernel hardness values were significantly reduced from its initial values in both storage methods. Rice kernel whiteness was significantly reduced from its initial value in both storage methods; however, kernel whiteness was significantly lower in warehouse paddy in comparison to cocoon paddy for 9 months of storage. Germination percentage of paddy was not significantly affected by both storage methods, but it is significantly reduced from its initial value . MC moisture content, IM impurities%, DS damaged seed%, Bol immature seed%, HC husk content, Br bran, TY total yield, BG broken grain, HRY head rice yield, TKW thousand kernel weight, BD bulk density, Hd kernel hardness, KW kernel whiteness and GM germination .\nFigure 3:Change of quality parameters of paddy with different storage conditions.\nTable 1: Columns having same letter are not significantly different at =0.05 by DMRT.\nIt was observed that paddy stored in cocoon took 6 weeks to reduce constant oxygen level to 3.2% The inside temperature of the hermetic cocoon had less fluctuated in comparison to outside environment temperature fluctuation. It also was observed that hermetically sealed cocoon paddy sample reported similar values as its initial values in terms of quality parameters such as impurities %, immature seed (Bol) % and damaged seed % after 9 months of storages. Results also revealed that weight loss percentage was minimum in sealed cocoon in comparison to conventional warehouse storage. Paddy kernel had more resistance to milling stress when stored under hermetically sealed cocoon. Hermetically sealed storage preserves kennel mass and whiteness over time. Hardness (textural characteristics of paddy) and bulk density were reduced even in hermetically sealed storage. Finally, it can be concluded that hermetically sealed cocoon storage method has more advantages compared to conventional warehouse storage. However, hermetically sealed condition must be maintained throughout the storage period to gain those advantages.\n© 2022 Senadeera W. This is an open access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use, distribution, and build upon your work non-commercially.']	['<urn:uuid:f1838dac-9ea0-447f-94ca-81f1046b6339>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	6	24	2108
87	What's special about shoulder bones and muscles, and how should we train them?	The shoulder has 3 bones (scapula, humerus, clavicula) connected by 4 joints, with the scapula uniquely held only by fixator muscles. For training, global mover muscles should be trained with less than 8 repetitions and higher weight, while local stabilizer muscles need 15-30 repetitions with lower resistance.	"[""Anatomy and Physiology: Acting in Concert\nActing in Concert\nWith almost 700 muscles, both superficial and deep, it makes sense that some of them might work together. If you remember the structure and function of the sarcomere (see The Structure of the Muscles and Muscle Cells), you may remember that muscles only work in one direction: contraction. After a contraction the muscle is at the mercy of other muscles, since it cannot elongate on its own. When the muscle is relaxed, however, the sarcomeres, and thus the muscle as a whole, can be stretched.\nAn easy mistake to make is to assume that one muscle is always the agonist, and the other is always the antagonist. In truth, the agonist is always the muscle that is contracting at that moment (let's say the biceps brachii), and the antagonist is the muscle that is being stretched (let's say the triceps brachii). As soon as the triceps start to contract, it becomes the agonist, and the biceps become the antagonist.\nThat stretching, however, requires the contraction of other muscles. For that reason, we evolved muscles on opposite sides of the bones. Remember flexion and extension, or abduction and adduction, from The Joints? The pair of muscles that make these movements are called antagonistic pairs. The muscle doing the initial movement (of the pair of movements, flexion, for example) is called the agonist, or prime mover. The muscle doing the opposite movement, in this case extension, is called the antagonist.\nWith about 700 muscles, it makes sense that some muscles may not work alone, especially with both superficial and deep muscles. These helper muscles, called synergists, work to stabilize a movement, and also to increase the efficiency of the prime mover. Another type of muscle is called a fixator, and its job is to stabilize the origin of the prime mover. This is especially important in terms of the scapula.\nWhen you compare the articulations of the scapula to those of the clavicles or the pelvic bones, you will notice something interesting. The clavicle articulates with the axial skeleton at the sternum, and the pelvic bones articulate at the sacrum, but the scapula doesn't articulate with the axial skeleton at all. The only thing holding the scapula in place is the action of the fixators; without them, the scapula would be pulled away from the body whenever we did something as mundane as flexing our forearm.\nA Muscle by Any Other Name …\nWith so many muscles, anatomists decided to name muscles using a few simple principles. Understanding these principles makes muscle names a snap. We have covered most of the essential principals already when we learned directional terms, bone names, and specific muscle movements (The Joints).\nTo give you an example of how to use the names, consider the tibialis anterior. From the name you know two things: It's not only near the tibia (tibialis), but it's in front of it (anterior). Another example is the flexor carpi ulnaris: It connects the carpals (carpi) and the ulna (ulnaris), and the movement it does is flexion (flexor). Not all the muscles are so easy, but you'll be surprised at how many muscles follow these simple naming rules. There are only a few loose ends, such as muscle shape and the direction of the muscle fiber, that we will need to get out of the way before you become an expert at muscle names. The seven characteristics used to name muscles can be seen in the following table.\n|Action||Muscle movement (The Joints)||Supinator|\n|Fiber direction||Angle from midline||Rectus abdominus|\n|Location||Bone, directional terms||Tibialis anterior|\n|Number of origins||If more than one||Biceps femoris|\n|Origin and insertion||Location of tendons||Sternocleido-mastoid|\n|Shape||General muscle shape||Deltoid|\n|Size||Big, small, long, short||Adductor brevis|\nAll Shapes and Sizes\nFlex Your Muscles\nThe name of the muscle includes more than just biceps. Were you aware that you have two biceps? In your arm you have the biceps brachii (brachii means arm), and in your leg, close to your femur, you have the biceps femoris.\nThe gluteal muscles are used in extending your leg, which is very useful in walking. Other animals have such muscles, but none as big as yours! We are bipeds, which means the muscles of the legs are used extensively as we walk. What is the implication of this fact? Humans have big butts!\nMuscle shape is often a factor in muscle names. Many of these are based on geometric shapes: The deltoid is shaped like a triangle, the rhomboideus major is rhomboid (or diamond) shaped, and the trapezius is trapezoid shaped. A nongeometric shape name is the serratus anterior, which is saw-toothed in shaped (think serrated knife).\nThe number of origins is also used. Most muscles have only one origin, so the exceptions usually have that fact as part of their name. Biceps, means two origins, triceps means three, and quadriceps means four origins.\nIn terms of muscle size, the words maximus (big), medius (medium), and minimus (small) are often used. Think of extending your leg (you can feel this when you climb stairs), and you will be thinking of the large gluteus maximus, the mid-size gluteus medius, and the smaller gluteus minimus. These muscles, as you will often find, are organized from superficial (maximus) to deep (minimus). The words major and minor are also used to mean big and small, as in the zygomaticus major and the zygomaticus minor. Remember, think in terms of groups when you hear certain muscle names: If there's a major, look for a minor; if there's a maximus, look for a minimus!\nThe length of a muscle is sometimes part of the muscle name. The word longus, as you would imagine, means long, as in the adductor longus. Short muscles use the word brevis, meaning short (as in brevity), as in the peroneus brevis. Well, that's the long and short of it!\nExcerpted from The Complete Idiot's Guide to Anatomy and Physiology © 2004 by Michael J. Vieira Lazaroff. All rights reserved including the right of reproduction in whole or in part in any form. Used by arrangement with Alpha Books, a member of Penguin Group (USA) Inc."", 'How many repetitions you should do for your shoulder exercises depends on whether you train muscles or movements. And then, which muscles and/or which movements?\nBut before I explain the difference, I think it’s important to understand the anatomy of the shoulder. Because the shoulder is much more than what you see when you look down to your left and right.\n1. Shoulder Girdle Anatomy\nThe shoulder girdle is made up of 3 bones, the scapula, the humerus, and the clavicula which connect through 4 joints. Of the 4 joints, the glenohumeral joint (GH Joint) is what is generally referred to as the shoulder. The other parts are the scapulothoracic joint (ST Joint) between your scapula and the thoracic spine, the acromioclavicular joint (AC Joint) between the roof of the scapula and the collarbone, and the sternoclavicular joint (SC Joint) between the collar bone and the breastbone.\nYou can never see consider? any of the joints of the shoulder girdle separately. At all times, they’re closely interrelated in their function.\nThis is essential because more joints allow for more movement freedom. Which in turn requires a large amount of active stability. This is stability provided by muscles that you control (actively) contrary to passive stability which comes from bones, joint capsules, labrum, ligaments, and connective tissue.\nNow, let’s have a look at the muscles that control shoulder movement and their specializations. Because this will determine how many repetitions you should do for your shoulder exercises.\n2. Which Muscles Belong to the Shoulder Girdle?\nThere are two main types of muscles making up the shoulder girdle, these are the global movers and the local stabilizers.\nThe global movers are good at moving your arm and scapula whereas the local stabilizers stabilize and thus centralize the humerus inside the glenoidal surface. The better the humerus maintains its position at the center of the glenoid, the more stable it is.\nThe global movers of the shoulder joint (with their main movement) are the:\n- Deltoids (all parts, anteflexion, abduction, and horizontal abduction)\n- Pectoralis major (horizontal adduction)\n- Pectoralis minor (elevation of the scapula)\n- Coracobrachialis (adduction)\n- Biceps brachii (anteflexion)\n- Triceps Brachii (retroflexion)\n- Latissimus Dorsi (adduction)\n- Teres Major (adduction)\n- Trapezius (all parts, elevation, retraction, and depression of the scapula)\n- Rhomboids (all parts, elevation, and retraction of the scapula)\n- Levator Scapulae (elevation of the scapula)\n- Serratus Anterior (which I consider both a stabilizer and a mover, makes a protraction of the scapula)\nThe local stabilizers are the:\n- Subscapularis (internal rotation)\n- Supraspinatus (abduction)\n- Infraspinatus (external rotation)\n- Teres minor (external rotation)\n- And to some extent the serratus anterior (stabilization of the scapula on the rib cage)\nEach of these muscles is equipped with the perfect muscle fibers to execute its jobs.\nThe global movers have a larger part of fast twitch muscle fibers and can thus generate more force and speed. Whereas the local stabilizers have a larger part of slow twitch muscle fibers which have a greater capacity to maintain tension over time.\nSo, here’s everything I discussed so far in 2 sentences:\n- Global movers are muscles that move the joints of the shoulder girdle in all directions and consist mainly of fast-twitched muscle fibers that are good at generating force and speed.\n- Local stabilizers are muscles that consist mainly of endurance slow twitch muscle fibers, that stabilize the joints of the shoulder girdle in its optimal position so the global movers can generate force efficiently.\n3. How Many Repetitions Should You Do for Your Shoulder Exercises?\nFrom an anatomical perspective, it’s now easy to answer how many repetitions you should do for your shoulder exercises.\nGlobal movers are best trained with a lower number of repetitions and a higher amount of weight. Anything below 8 repetitions will do. The lower the number of repetitions the more you improve your maximum strength and the more you train the nervous system.\nLocal stabilizers are best trained with a high number of repetitions and lower resistance. I recommend anywhere between 15-30 repetitions with a weight with which you could do at least 3 more repetitions than your desired amount. This is a “3-repetition reserve”.\nNevertheless, rock climbing isn’t necessarily in line with your anatomy and might therefore require a different number of repetitions for your shoulder exercises.\nSo, another approach to knowing how many repetitions you should do for your shoulder exercises is by looking at rock climbing, analyzing the movements, and understanding what’s asked of your shoulder muscles.\nRock climbing exists of combined movements. So, it’s never only adduction, an internal rotation, and a horizontal abduction separately. It’ll be all these movements at the same time resulting in a pulling movement. Then for a gastone move, on the other hand, you need external rotation, horizontal abduction, abduction, and so on.\nWhen you don’t train muscles but climbing movements the number of repetitions depends on what you want to accomplish:\n- Endurance: 15-20 repetitions or more\n- Hypertrophy 8-20 repetitions to failure\n- Strength: 1-6 repetitions\n- Speed: 1-10 repetitions with a focus on speed and acceleration and thus a lighter load (30-40% of your 1RM)\n4. Important Take-Away’s\nBoth ways of approaching your shoulder exercises and the repetitions you use are valid forms of training. I think it’s best to combine both for optimal results. Training climbing movements help you to get stronger for specific movements whereas training muscles prevents you from developing imbalances and maintaining a healthy posture.']"	['<urn:uuid:61e73098-7930-455c-9899-7bcd195f3544>', '<urn:uuid:378539c7-1b1c-420a-93eb-4d0bd10106ab>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:35:02.547110	13	47	1938
88	save file formats topaz photo editor	Topaz Studio allows saving edited images as PNG, JPG, or TIFF files.	['برنامج Topaz Studio عبارة عن مجموعة أدوات إبداعية سريعة ومرنة وقوية ، وهو محرر للصور رائع ومميز جدا للمصورين الفوتوغرافيين .\nيوفر لك استوديو توباز نتائج سريعة مذهلة مع تقنية معالجة الصور الحصرية في توباز .\nبرنامج Topaz Studio منصة بسيطة لاستخدام التحرير مع تقنية تحسين الصور المشهود لها من Topaz Labs.\nوهو يعمل كمحرر مستقل و يعمل أيضاً كمكوّن إضافي في برنامج Lightroom و Photoshop ، مما يعني أنه يعمل بالطريقة التي تريدها بالضبط.\nوفى هذه التدوينة اقدم لكم آخر إصدار من هذا البرنامج الرائع\nبرنامج توباز ستوديو للمصورين | Topaz Studio 2.0.0\nTopaz Studio is a fast, flexible, powerful creative toolbox and it’s the next level image editor for photographers. Topaz Studio gives you lightning fast results with Topaz’s exclusive image processing technology, in an intuitive, affordable, easy to use application. Topaz Studio a simple to use editing platform with Topaz Labs’ acclaimed photo enhancement technology. It works as both a standalone editor, a plugin within Lightroom and Photoshop, and a host application for other Topaz plugins which means it works exactly how you want it to. Now there’s no more cumbersome workflows that require you to open multiple plugins to achieve your look!\n“This is all your Topaz products wrapped into one program.”\nTopaz Studio is an easy to use editing solution built with the photographer in mind. This all new editing platform features professionally created 1-click effects, 24 flexible drag and drop Adjustments, and powerful image processing.\nWorking in Topaz Studio\nWorking in Topaz Studio is easy! Topaz Studio is set up with the photographer in mind. Whether you’re just getting started with photography or a tried and true professional, Topaz Studio has exactly what you need. With an ever growing library of expertly crafted pre-made effects to professionally crafted assets to artistic and technical tools alike, Topaz Studio is the perfect photographer toolbox, no matter what level of expertise!\nAll About the Workspace\nTopaz Studio features an easy to use Toolbar, a fast and fun Effect Panel with search buttons and features, the workspace, an Image Browser, the Scopes panel, and the Adjustment Panel, complete with Adjustment Buttons.\nTopaz Studio Main Toolbar\nThe Main Toolbar gives you options to open and save images, an option for comparing the before image and after image, zooming, and split views. You can also click the Open Button to open additional images and save images with the Save Button and Save As Button.\nSaving Your Image\nSave or Save As Images. Save edited images as PNG, JPG, or TIFF files with easy to locate buttons in the Main Toolbar, by using shortcut keys (Command S or Command Shift S), or by going to Menu > File > Save or Save As.\nTopaz Studio Effect Panel\nWhen you first load an image, you’ll see Featured Effects in the Effect Panel. Here you can search for effects by typing an author name, an effect name, tags, or categories.\nClick the Grid Button to see more effects at a time. Once in Grid View, click the List Button to see a single column list of effects.\nTopaz Studio Effects Buttons\nThe Effect Buttons give you access to a list of Categories and Tags to search for effects quickly and easily. Click the More Button to see a full list of all available categories and tags.\nTopaz Studio Canvas\nThe Canvas area is where your active image will be displayed. Easily change the view options of the Canvas with the Main Toolbar, found in section 1 above.\nTopaz Studio Image Browser\nUse the Image Browser to view and adjust various edits of an image side by side or work on several images at once.\nTopaz Studio Scopes Panel\nAt the top right of the application is the Scopes Panel that features 4 separate views. The first allows you to view the Histogram of the image. The second displays the Hue, Saturation, and Luminance of the image, while the third button displays the amount of image Details. Finally, the fourth button helps you navigate your image while zoomed in.\nTopaz Studio Adjustment Panel\nThe Adjustment Panel is where you can modify effects or create your own. It gives you control over the large range of tools and adjustments located in the Adjustment Buttons. The Adjustment Buttons features some of the most commonly used adjustments, and you can see all available adjustments by clicking the “More” button at the bottom of the list.\nSaving Your Custom Effect\nSave and Share Your Effects. Save your new effect by clicking the Save button and share your creation with others by selecting ‘Yes’ in the Public field. Your effect will be shared to the entire Topaz Community instantly!\nTopaz Studio Adjustment Buttons\nThe Adjustment Buttons gives you access to a list of commonly used Adjustments like Crop, Lens Correction, Basic Adjustment, and more. Click the More Button to see a full list of all available Adjustments.\nWe’ve also included a tools section. You’ll find lens correction, global masking (with 5 styles and 4 different tools to customize the overall effect), and crop and rotate. Everything is located in a simple to use toolbar with easy-to-use sliders and functionality.\nChange Up the Workspace\nCustomize your workspace windows with just the click of a button. The Effect Panel, Scopes Panel, Adjustment Panel, and Image Browser can each be collapsed by clicking the arrow on the bar on the inside edge of the panel. The Effect Buttons and Adjustment Buttons can also be collapsed and expanded by clicking at the top of the toolbar.\nWhat are Adjustments?\nAdjustments are the core of Topaz Studio, they are built with very focused functionality in mind. Adjustments may be used independently or combined to create extremely unique looks and custom effects. Currently, there are 24 Adjustments available covering everything from simple Film Grain, to advanced color control with Color Theme. Adjustments are grouped according to complexity, with Common Adjustments appearing at the top, Advanced Adjustments in the next section, Artistic Effect Adjustments below that, and Compositing Adjustments appearing last.\nThe Topaz Studio Integrated Masking is unlike anything you have seen before. We have included 5 masking types: Brush, Spot, Gradient, Color, and Luminosity, as well as a mask adjustments panel and an option to invert or completely reset your mask. Our brush masking includes intuitive, content-aware technology that was developed in house. I’ve gotten the chance to play with it quite a bit – it’s awesome and a complete time saver to say the least. Not only are the new masking features amazing, but you can now combine masking types. After making adjustment with one type of mask, click apply or switch to another mask type and build upon that mask. It’s never been so easy!\n– Windows 7/8/10 x 64bit + OpenGL 3.3\n– RAM: 4GB (Minimum)\n– GPU VRAM: 1GB (Minimum)\n– Does not support Windows 32bit\nمساحة البرنامج 630 ميجا تقريباً\nتحميل موفق للجميع\nكلمات دلالية :\nTopaz Studio , برنامج Topaz Studio, تحميل برنامج Topaz Studio, تنزيل برنامج Topaz Studio, تعديل الصور , برنامج تعديل الصور , برنامج صور , برنامج تصوير , برنامج تصميم الصور , برنامج تظبيط الصور , تظبيط الصور , تحرير الصور , برنامج لتعديل الصور , برنامج تجميل الصور , افضل برنامج لتعديل الصور , برنامج تحسين الصور , برنامج محرر الصور , تجميل الصور , برامج تصوير فوتوغرافي , برامج تحرير الصور , برنامج للتصوير , محرر الصور , برنامج تغيير الصور , برنامج التعديل على الصور , برنامج تصميم\nمع تحيات موقع المتميز كم']	['<urn:uuid:3dab7b75-4395-4e69-b1fd-aa4e8949da3d>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:35:02.547110	6	12	1251
89	gamete chromosome mixing inheritance mechanisms	Genetic variation through gametes occurs through multiple mechanisms that affect chromosome inheritance. During meiosis, recombination and independent assortment create gametes containing different sets of DNA, producing unique gene combinations in zygotes. Crossing over occurs during prophase I, leading to genetic recombination. In contrast, introgression involves a different kind of genetic mixing, where genes from one species enter another species' gene pool through repeated backcrossing of hybrids with parent species. These processes contribute differently to genetic variation: meiotic processes create variation within a species, while introgression introduces variation between species.	"['Introgression, also known as introgressive hybridization, in genetics (particularly plant genetics) is the movement of a gene (gene flow) from one species into the gene pool of another by the repeated backcrossing of an interspecific hybrid with one of its parent species. Purposeful introgression is a long-term process; it may take many hybrid generations before the backcrossing occurs.\nIntrogression is an important source of genetic variation in natural populations and may contribute to adaptation and even adaptive radiation. It can occur across hybrid zones due to chance, selection or hybrid zone movement. There is evidence that introgression is a ubiquitous phenomenon in plants, animals, and even humans, in which it may have introduced the microcephalin D allele.\nIntrogression differs from simple hybridization. Introgression results in a complex mixture of parental genes, while simple hybridization results in a more uniform mixture, which in the first generation will be an even mix of two parental species. Natural introgression does not have human direct interference while the exotic introgression is induced intentionally (as for instance genetically modified organisms[clarification needed]) or not (human activities affecting local races of crops or human disturbances such as by introducing weeds).\nOne important example of introgression has been observed in butterfly mimicry. Genus Heliconius has been studied. This genus comprehends 43 species and many races with different color patterns. Congeners exhibiting overlapping distributions show similar color patterns. The distribution of the subspecies H. melpomene amaryllis and H. melpomene timareta ssp. nov. overlap. Using the ABBA/BABA test, some researchers have observed that there is ≈2-5% introgression between the pair of subspecies. It is important to know that this is not random introgression. They saw important introgression in chromosomes 15 and 18, where important mimicry loci are located (loci B/D and N/Yb). They compared both subspecies with H.melpomene agalope, which is a subspecies near H.melpomene amaryllis in entire genome trees. The result of this experiment was that there is no relation between those two species and H.melpomene agalope in the loci B/D and N/Yb. Moreover, they performed the same experiment with two other species with overlapping distributions, H.timareta florencia and H.melpomene agalope. They demonstrated introgression between the two taxa, especially in the loci B/D and N/Yb. Finally, they concluded their experiments with sliding-window phylogenetic analyses, estimating different phylogenetic trees depending on the different regions of the loci. When a locus is important in the color pattern expression, there is a close phylogenetic relationship between the species. When the locus is not important in the color pattern expression, the two species are phylogenetically distant because there is no introgression at such loci.\nIntrogression could be an important conservation problem for wild species through hybridisation, for instance, between wild and domestic cats  or among wild canids and domestic dogs.Another important example has been studied by Arnold & Bennett 1993: iris species from southern Louisiana.\nAn introgression line (abbreviation: IL) in plant molecular biology is a line of a crop species that contains genetic material derived from a similar species, for example a ""wild"" relative. An example of a collection of ILs (called IL-Library) is the use of chromosome fragments from Solanum pennellii (a wild variety of tomato) introgressed in Solanum lycopersicum (the cultivated tomato). The lines of an IL-library usually cover the complete genome of the donor. Introgression lines allow the study of quantitative trait loci, but also the creation of new varieties by introducing exotic traits.\n- Gene flow\n- Genetic engineering\n- Genetically modified organism\n- Transgenic plant\n- Chimera (genetics)\n- Gene pool\n- Genetic pollution\n- Genetic erosion\n- Grant, P.R., Grant, B.R. & Petren, K. (2005). ""Hybridization in the Recent Past"". The American Naturalist 166: 56–67. (available online at The American Naturalist)\n- Richard Buggs (2007). ""Empirical study of hybrid zone movement"". Heredity 99 (3): 301–312. doi:10.1038/sj.hdy.6800997.\n- Dowling T. E., Secor C. L. (1997). ""The role of hybridization and introgression in the diversification of animals"". Annual Review Ecology and Systematics 28: 593–619. doi:10.1146/annurev.ecolsys.28.1.593.\n- Bullini L (1994). ""Origin and evolution of animal hybrid species"". Trends in Ecology and Evolution 9 (11): 422–426. doi:10.1016/0169-5347(94)90124-4. PMID 21236911.\n- Holliday T. W. (2003). ""Species concepts, reticulations, and human evolution"". Current Anthropology 44 (5): 653–673. doi:10.1086/377663.\n- Evans, Pd; Mekel-Bobrov, N; Vallender, Ej; Hudson, Rr; Lahn, Bt (Nov 2006). ""Evidence that the adaptive allele of the brain size gene microcephalin introgressed into Homo sapiens from an archaic Homo lineage"". Proceedings of the National Academy of Sciences of the United States of America 103 (48): 18178–83. Bibcode:2006PNAS..10318178E. doi:10.1073/pnas.0606966103. ISSN 0027-8424. PMC 1635020. PMID 17090677.\n- Review of scientific papers on gene introgression between wild and domestic cats\n- Review and link to scientific papers regarding introgression of dog genes into wild canid populations\n- Arnold, M. L. & Bennett, B. D. (1993). ""Natural Hybridization in Louisiana irises: genetic variation and ecological determinants"". In: Harrison, R. G. (ed.) Hybrid Zones and Evolutionary Process, pp. 115-139. Oxford University Press, New York. ISBN 978-0-19-506917-4\n- Arnold, M. L. (2007). Evolution through Genetic Exchange. New York: Oxford University Press. ISBN 0-19-922903-1.\n- Anderson, E. 1949. Introgressive Hybridization. Wiley, New York.\n- Eyal Friedman et al., ""Zooming In on a Quantitative Trait for Tomato Yield Using Interspecific Introgressions"", Science vol.305 pag.1786-1798 (2004)\n- Rieseberg, L. H. & Wendel, J. F. (1993). ""Introgression and its consequences in plants"". In: Harrison, R. G. (ed.) Hybrid Zones and Evolutionary Process, pp. 70–109. Oxford University Press, New York. ISBN 978-0-19-506917-4\n- Martinsen G. D., Whitham R. J. Turek, Keim P. (2001). ""Hybrid populations selectively filter gene introgression between species"". Evolution 55 (7): 1325–1335. doi:10.1554/0014-3820(2001)055[1325:hpsfgi]2.0.co;2. PMID 11525457.\n- Whitney, K.D., Ahern J.R.,Campbell L.G, Albert L.P., King M.S. (2010). ""Patterns of hybridization in plants"" (PDF). Evolution and Sytematics 12: 175–182. doi:10.1016/j.ppees.2010.02.002. (""Forbidden"" - No Access 2015-04-06) Alternate Link: Patterns of Hybridization in Plants\n- The Heliconius genome Consortium. Butterfly genome reveals promiscuous exchange of mimicry adaptations among species. Nature 487,94-98 (2012).', 'How does gametes affect genetic variation?\nGenetic variation is increased by meiosis\nBecause of recombination and independent assortment in meiosis, each gamete contains a different set of DNA. This produces a unique combination of genes in the resulting zygote. Recombination or crossing over occurs during prophase I.\nWhat effect do mutations have on variation?\nMutations can be inherited and therefore passed on from one individual to another. If a mutation causes a new phenotype that makes an organisms better suited to a particular environment, it can lead to rapid change in the characteristics of the individuals in that species.\nHow does mutation occur in genetic variation?\nMutations are the original source of genetic variation. A mutation is a permanent alteration to a DNA sequence. De novo (new) mutations occur when there is an error during DNA replication that is not corrected by DNA repair enzymes.\nWhat will happen if there is a mutation to the gametes?\nIf a mutation occurs in a germ-line cell (one that will give rise to gametes, i.e., egg or sperm cells), then this mutation can be passed to an organism’s offspring. This means that every cell in the developing embryo will carry the mutation.\nWhy do gametes vary?\nThe reason why this sharing of traits occurs is because of meiosis, which is the process in which gametes are produced. … Crossing over results in two new and different autosomes, and each will go their seperate way into a seperate gamete, (gametes are haploid cells, so they will only have one copy of each chromosome).\nWhat are 3 reasons to explain genetic variations in meiosis?\nGenetic diversity arises due to the shuffling of chromosomes during meiosis.\n- Process of Meiosis. A man produces sperm and a woman produces eggs because their reproductive cells undergo meiosis. …\n- Crossing Over. …\n- Random Segregation. …\n- Independent Assortment.\nWhat is mutation variation?\nGenetic variations can arise from gene variants (also called mutations) or from a normal process in which genetic material is rearranged as a cell is getting ready to divide (known as genetic recombination). Genetic variations that alter gene activity or protein function can introduce different traits in an organism.\nIs mutation the same as variation?\nGenetic variation refers to diversity in gene frequencies. Genetic variation can refer to differences between individuals or to differences between populations. Mutation is the ultimate source of genetic variation, but mechanisms such as sexual reproduction and genetic drift contribute to it as well.\nWhat causes variation?\nGenetic variation exists because of changes to chromosomes or genes (DNA). Mutations are random changes in the number of chromosomes (e.g. Down syndrome) or the structure of a gene (e.g. cystic fibrosis). Independent assortment during meiosis increases variation because it results in genetically different gametes .\nWhat is the process of mutation?\nA mutation is a change that occurs in our DNA sequence, either due to mistakes when the DNA is copied or as the result of environmental factors such as UV light and cigarette smoke. Over a lifetime our DNA? can undergo changes or ‘mutations?‘ in the sequence of bases?, A, C, G and T.']"	['<urn:uuid:be3432d6-7815-4dee-993d-a66c47c452c0>', '<urn:uuid:dd831cf8-fa9e-4325-b5a9-8d1722ac7c1b>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:35:02.547110	5	89	1501
90	When did banks start having compliance departments?	The compliance control function emerged in the financial sector in the late 1980s in Anglo-Saxon countries.	['CHALLENGES OF CONPLIANCE FOR MICROFINANCE INSTITUTIONS\nI. The compliance\nCompliance is a recent concept that has introduced new obligations for businesses in general and particularly for financial institutions. It is defined as the obligation to ensure that the financial institution continuously complies with:\n- Legislative and regulatory provisions specific to financial activities;\n- Professional and ethical standards and practices;\n- Codes of conduct, including ethical codes and internal procedures.\nNon-compliance risk is defined as the risk of legal, regulatory, or disciplinary sanctions, significant financial losses, or reputational damage resulting from a failure to comply with specific professional provisions, whether legislative, regulatory, professional, ethical, or executive instructions.\nWithin the Financial Institution, the Compliance function covers:\n- Financial Security: Combating fraud, money laundering, terrorism financing, market abuse, and embargoes;\n- Customer Protection: Continuous protection of customers, preserving their interests and those of the markets or the Financial Institution;\n- Ethics: Adherence to the establishment’s ethical rules and handling reports from all employees.\nThe notion of compliance and its objectives are clearly illustrated in the original definition by the Basel regulator: “The purpose of the compliance function is to assist the bank in managing its compliance risk, which can be defined as the risk of legal or regulatory sanctions, financial loss, or loss to reputation a bank may suffer as a result of its failure to comply with all applicable laws, regulations, codes of conduct, and standards of good practice.”\nII. Importance of Compliance for Microfinance Institutions\nThe compliance control function emerged in the financial sector in the late 1980s in Anglo-Saxon countries. Nowadays, it has become a strong international requirement, with international organizations paying great attention to it, and regulators attaching increasing importance to it. The entire society demands more transparency and ethics.\nIn Tunisia, the compliance control function, commonly referred to as the “compliance function,” was established in 2006 under Article 34 quarter of Law No. 2006-19 dated May 2, 2006. Several texts and circulars subsequently reinforced the positioning of this function:\n- Law No. 2016-48 dated July 11, 2016, relating to banks and financial institutions, stipulating that banks and financial institutions must establish internal audit, risk management, and compliance control functions independent of operational and support bodies.\n- Circular BCT 2017-08 dated September 19, 2017, which sets internal control rules for managing the risk of money laundering and terrorism financing. The designated representative of the Financial Analysis Commission (CTAF) and their deputy must be part of the compliance control body.\n- Circular BCT 2021-05 dated August 19, 2021, titled “Governance Framework for Banks and Financial Institutions.” It highlights the advisory role of the compliance control function to the administrative and management bodies regarding compliance with applicable laws and regulations and requires various compliance-related actions.\nIII. Microfinance Institutions facing Non Compliance Risk\nMicrofinance institutions (MFIs) are financial entities specialized in assisting individuals who do not have access to traditional banks. Like a bank, an MFI provides loans, but the loan amounts are smaller than those granted by traditional banks. MFI clients are considered risky by traditional banks due to the lack of tangible assets to secure the loans.\nMFIs face specific risks that increase their compliance responsibilities, including institutional risks related to their social or commercial mission and dependence on international organizations, operational risks such as credit risk, fraud, or security, financial management risks, and external risks related to regulations, competition, demographics, physical environment, and macroeconomic conditions.\nCompliance with regulations is critical for MFIs due to their ethical commitment and humanitarian responsibility. Ensuring compliance requires establishing a robust non-compliance risk management system and regulatory monitoring within the MFI. This includes adopting various policies, codes of conduct, and risk mapping, as well as providing training and guidance to staff regarding regulatory compliance.\nSome analysts believe that the compliance function within MFIs should go beyond defining rules and monitoring risks. It should contribute to developing a strong ethical culture, exemplifying best business practices, and ensuring adherence to rules for sustainable performance. The compliance function should act as a partner to the business, translating regulations into specific operational actions, monitoring real residual risks, and facilitating change initiatives. Moreover, it should be proactive in anticipating changes in the profession and defending the interests of the MFI through lobbying efforts']	['<urn:uuid:5620cfd5-6149-411e-ac6b-3eb2410ad53f>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	7	16	704
91	What's the relationship between heat resistance and workplace hazards in coating methods?	Regarding heat resistance, powder coating is limited as it's baked at only 400 degrees while exhaust temperatures can reach 1200 degrees, leading to peeling and decolorization. Ceramic coating is superior as it can withstand up to 1300 degrees. In terms of workplace hazards, the high temperatures used in curing ovens can cause serious burns if mishandled. The process requires careful handling of hot equipment and proper safety protocols. Additionally, the airborne powder particles and chemical agents used in the process pose respiratory risks and potential fire hazards, necessitating proper storage, ventilation systems, and protective equipment for operators.	"['Exhaust pipe modifications can be a mixed bag. When it comes to coating, it’s important to use the right material and to be aware of the performance and aesthetic changes that will follow.\nCan a motorcycle exhaust be powder coated? Powder coating is not the best option. Temperatures in the exhaust can reach 1200 degrees, but powder paint gets baked at 400 degrees. This can lead to peeling away and decolorization. A better alternative is ceramic coating since it can withstand higher temperatures of up to 1300 degrees.\nBelow we will briefly discuss the types of coatings for motorcycle exhaust, including their pros and cons.\nTypes of Coating For Your Motorcycle Exhaust\nCoating is defined as the layer of outer material that covers a construction element (motorcycle exhaust in this case) to protect it against the action of certain external agents. When it comes to the exhaust, coating is important for reducing the impact of heat damage.\nPowder coating and ceramic coating are the two main options. They are sometimes treated similarly in the world of motorcycles, but they are quite different. So make sure you’re precise in your request.\nPowder Coating For Motorcycle Explained\nPowder coating is simply a dry powder coating for metals. It is currently the painting method most used in the United States for the restoration, modification and customization of motorcycle, automobile and nautical parts. It’s widely used due to the quality it offers both in durability, resistance and finishes.\nOnce the dry powder paint has been applied using electrostatic equipment, the pieces go to the oven where the paint hardens and cures at the appropriate temperature and time.\nPowder coating is practically a mandatory application on parts such as chassis, wheels and spokes, screws, suspension bottles, springs, handlebars, levers, engine covers, foot pegs, fenders, turrets, backrests, seat posts, brake discs, pulleys, chainrings, trestles, and especially exhausts, and any other part that with conventional liquid paint would be impossible to paint or would not have the correct hardness, resistance and finish.\nIt is also recommended for newly manufactured parts or parts that need to be restored, as it offers a higher quality appearance in its finishes.\nGeneral Characteristics And Advantages of Powder Coating\n- It is much more resistant, adherent and flexible than conventional liquid paint\n- Excellent adhesion to all types of metals (aluminum, stainless steel, copper, cast iron, any metal alloy, iron, etc.) regardless of whether they are new, used, rusted or with some type of coating.\n- Offers excellent resistance to abrasion, corrosion and chemicals; as well as extreme weather conditions.\n- High mechanical resistance against impacts, torsions and deformations.\n- Once cured, it does not soften in heat.\n- It is considered ecological since it doesn’t contain solvents or toxic or polluting components.\n- The layer that creates the powder paint is thicker and more homogeneous, which facilitates the rounding of the edges and helps to hide imperfections and defects, so that the piece, once painted, offers a better finished and quality appearance.\nDisadvantages of Powder Coating For Motorcycle Exhaust\nPowder coating has various advantages on its several applications, but it has glaring issues, and that’s why it is not recommended for coating your motorcycle exhaust. They include:\nCost: professionals best do powder coating, and it can become expensive for some people to complete. DIY isn’t easily attainable because there are specialized types of equipment and heat requirements for it to be a success.\nHeating Process: As said before, the heating required for powder coating is about 700 degrees, and tools that get this done are mostly available to professional coating shops.\nNot Heat Resistant: Now, don’t get this wrong, it won’t easily melt. However, the amount of heat produced by our motorcycles can be very high, and powdered coating treatment of your exhaust can start to cake and fall off sooner then expected. It can make your exhaust look bad and can even affect your engine.\nDecolourization: Still related to the excessive heat produced by our motorcycle engine, powder coating can’t withstand it, and as the power treatment cakes off, it decolours your exhaust, making it look awkward.\nPowder coating for motorcycle exhaust is a valid option, but it’s not ideal. If you’re serious with taking care of your motorcycle exhaust, then you should seriously consider ceramic coating. Below we briefly explain ceramic coating for motorcycles.\nCeramic Coating For Motorcycle Exhaust Explained\nCeramic coating is a type of powder coating, and it acts as an insulating layer on the exhaust pipe that transfers heat, moving it from inside the vehicle to the outside efficiently. It is a technique used to coat the exhaust pipe, and its mission is to keep the outside cold; in this way, the exhaust can be used without having to wait until it is cold.\nThe technique used is plasma spraying, in addition to applying a great thermal barrier to the material, it prevents oxidation in almost any environment, which is why it is a technique widely used for coating motorcycle exhausts in many countries.\nPlasma spraying or sputtering is a physical process in which the atoms of a solid material called “white” vaporize by being bombarding with energetic ions.\nFeatures Of The Ceramic/Thermal Coating Process\n- High temp resistant up to 1300 degrees\n- Reduces radiant temperatures\n- Can produce horsepower gains!\n- Will not rust, blue or peel!\nAdvantages of Ceramic Coating on Motorcycle Parts\nCeramic withstands much higher temperatures than iron or steel and isolates heat losses much more.\nIf we apply this coating to parts that radiate a large amount of heat like the exhaust, we can reduce the thermal impact on other weaker components, such as sheet metal.\nAnother advantage that we can obtain from this technique is the drop in air temperature in the intake manifold. This way we obtain denser air, and this gives the motorcycle greater power.\nAlso, it will increase the lifespan of the exhaust and the manifolds. For this reason, it is recommended to use it on motorcycles where the exhaust is very close to the rider as well as to the whole mechanical part.\nExhaust pipes that lack this coating reach very high temperatures, to the point of being red hot. When this happens, the exhaust undergoes oxidation and with it a structural change of the metal, which translates into greater fragility of the welded areas. Another damage that occurs when red-hot is called “thermal fatigue”. With this coating, we avoid oxidation and thermal fatigue.\nDisadvantages of Ceramic Coating on Motorcycle Parts\nObviously, the non-dissipation of heat results in much more concentrated push-out of the outlet hole; this means that the gases will come out much hotter.\nHow to Apply Ceramic Coating on Motorcycle\nYes, you can apply a ceramic coating on your motorcycle by yourself, even though we highly recommend that you assign this task to a professional. You will discover that it is better to apply a ceramic coating to protect the motorcycle against dirt, corrosion or even grime. You only need to follow the steps below:\n- The first step is to decontaminate all parts of the motorcycle using a body cleaner.\n- Then it will be necessary to pass on each part the ceramic treatment using the cloth provided in the complete kit.\n- To apply the treatment in the best conditions, I advise you to start with the high elements, then to go down to the lower elements, ending with the engine parts, then the rims of the motorcycle.\n- You can coat absolutely everything on the motorcycle: fairing, exhaust manifolds, tank, rims, muffler, mirrors, chrome parts, carbon, plastics, metals. In short, the ceramic treatment is applied everywhere except on the leather of the saddle as well as on any glass parts.\n- This ceramic treatment has the advantage of resisting extreme temperatures. It protects against corrosion, which is particularly useful for sensitive metal elements very exposed to water splashes. It greatly reduces the adhesion of dirt: your motorcycle, therefore, gets dirty less quickly and becomes easier to clean.\n- The ceramic also retains the shine of the treated elements.\n- If you install the ceramic on a second-hand motorcycle, it is strongly recommended to carry out a preliminary renovation of the scratched or micro-scratched surfaces, using a polish.\nSummary: Ceramic Coating vs Powder Coating\nThe ceramic coating process is the part whereby the motorcycle exhaust is sandblasted, then coated with the Ceramic. Next, it will be baked for about one hour at about 500 °C. Finally, the ceramic is polished for about half an hour in a tumbler.\nCeramic coating is best and recommended for motorcycle exhausts because of its impressive ability to withstand extreme heat.\nEven though you should take your exhaust to a coating shop for ceramic coating, it is worth it to know that ceramic coating’s standard heat is about 1300° F. This is the reason why it would also be better than powder coating.\nWhile powder coating is an exhaust paint process that is electrostatically applied and then baked at about 160 °C to 210 °C as long as the surface won’t peel, chip or crack like the traditional paint. This form of coating is suitable for areas that are not exposed to high heat temperature like the exhaust. It can be a good options for wheels, frames etc.\nTo summarize, you can powder coat your motorcycle exhaust, but it won’t be effective for heat management. Ceramic coating is the better option. There are various heat management methods for various budgets, so you don’t have to brake the bank to get your exhaust treated. We have explained the difference between powder coating and ceramic coating, now the choice is yours.', ""The Importance of OH&S and Work Safety in Powder Coating Equipment Manufacturing, Usage and MaintenanceMay 25, 2017\nAs an inherently nontoxic parts-finishing process, powder coating technology does not threaten the environment. But, and this is an important point, the equipment and materials do pose a threat to an operator. Just like any area that's reserved for a manufacturing operation, there are risk factors to address. Starting with the workshop premises, there are OH&S (Occupational Health and Safety) guidelines that exist to keep everyone and everything safe.\nDetermining The Risk Factors\nA safe working environment comes before all other considerations. In order to maintain that safety margin, the hazards associated with every stage of the process must be known in some detail. Again, this is essentially a work-congenial process, but it is a manufacturing technique, so a few hazards will always be in play. A hot curing oven, for example, will cause serious burns if it's mishandled. Likewise, the tiny powder granules are airborne, which means they could be breathed in until they cause respiratory problems. Finally, high voltages and corrosive stripping chemicals exist in different sections of the workshop, so only authorised personnel should be walking these floors.\nSatisfying Regulatory Concerns\nA few of the next regulatory dealings may sound like self-evident facts, but there's always one contractor out there who favours a short cut. The OH&S directives don't tolerate shortcuts, not in any form. Anyway, back to safeguards. All of those cleansing agents and caustic scrubbing chemicals need to be stored properly. At best, they could exude a vapour that burns a sensitive eye. At worst, however, they represent a fire hazard. Over at the processing cubicle, the equipment bay requires a sealing hood, a covering that stops the powder from leaking. Exhaust fans and powder reclamation systems take care of the airborne particles, but protective gear, especially goggles and a dust mask (www.worksafe.au), must be worn at all times during the application stage. Remember, these airborne particles are not usually toxic, but the fine dust will irritate an operator's respiratory airways.\nOnce out in the field, in the place the workpiece will function, it's certain to be dry and weather-resistant. However, the manufacturing equipment that produces this eco-friendly product does possess the potential to harm an inattentive operator. Additionally, the strong airflow and electrostatic charge used in the powder coating cubicle requires careful consideration, for there are commercial-grade air compressors and high-voltage appliances providing powder drive energy. Keep all of these discrete system parts in mind, and remember that all of this equipment requires maintenance if it's to work safely and efficiently.\nGP Industries Pty Ltd\n1 Regal Court,\nVermont South VIC 3133\nPhone: (03) 9802 1355\nFax: (03) 9802 6027\n20 Burgess Road,\nBayswater North VIC 3153\nPhone: (03) 9761 7676\nFax: (03) 9761 7671\nOptimized by NetwizardSEO.com.au\n- Powder Coating in Sheet Metal Processing\n- Powder Coating for Automotive for Best Finished Results\n- Is Powder Coating Better than Spray Paint?\n- Differences between Metal Plating and Powder Coating\n- Sustainable Powder Coating\n- Why Do You Need to Use High Temperature Masking Tapes When Powder Coating?\n- Benefits of the Powder Recovery System\n- Why You MUST Remove Moisture before Powder Coating\n- Electrocoat and Chemical Dipping Plants\n- Things to Look for When Buying Powder Coating Ovens\n- Process of Metal Pretreatment in Powder Coating\n- How to Judge a Perfect Powder Finish: What Should You Look For?\n- Dust Collectors and Cyclones in Relation to Powder Coating\n- Functions of Overhead Conveyors in Powder Coating Equipment\n- Keeping Corrosion Off Your Equipment: How Powder Coating Helps\n- The Role of Hot Ovens in the Powder Coating Process\n- Potential Cure Cycle Problem Areas in Powder Coating\n- Causes of Peeling and Pinholes in Powder Coating\n- Common Powder Coating Gun Problems and Solutions\n- Is It Possible to Apply Powder Coating Over Wet Paint?\n- What are the Differences between Electro Coating and Powder Coating?\n- Reliable Powder Coating Equipment Manufacturer and Supplier in Melbourne\n- Powdercoating Failures: What Causes Air Bubbles, Cracks and Peeling?\n- Is It Possible to Powder Coat Chrome Materials?\n- Pre-Treatment Process of Materials before Powder Coating\n- View all articles…""]"	['<urn:uuid:5171edae-f728-4be5-b074-ffa6aa80f367>', '<urn:uuid:6f9a433e-1925-45b5-a20d-234eb911f05a>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	12	97	2317
92	What should I check before trying to do dropbacks in yoga?	Before attempting dropbacks, you need to be able to do Urdhva Dhanurasana (Upward Bow Pose) with straight arms. If you can't do that yet, you should practice the preliminary poses first. Additionally, you should properly warm up by doing several rounds of Cat-Cow Pose coordinated with your breath, followed by three to five rounds of Sun Salutations A and B to generate heat throughout your entire body.	"['When you ""do"" yoga, it\'s called practice. The yogic notion of practicing is different from the type of practice you do when you rehearse for a special event like a dance performance, a marathon, or a speech. In those cases, there is a specific goal, and when you get to a certain point, there\'s no need for more practice. But the practice of yoga—defined as both a state of being and a codified method of physical postures designed to create radiant health—is never finished; it\'s a process.\nYou have probably already experienced your asana practice as a process of evolution—you may have become less emotionally reactive, stronger, more stable and flexible. This process is nonlinear: Some days you feel you\'re making headway, while other days you may just feel tired. But over time, you\'ve probably noticed consistent progress in a positive direction.The state of being we call yoga is also a process. Although all the masters of yoga agree that we are born inherently free, liberated, and blissful, they also agree that this freedom is buried beneath our everyday habits and thought patterns. To rediscover your inner freedom, you must engage in the process of transformation by cultivating compassion, curiosity, and contentment and, ironically, by letting go of the results.\nIt\'s not that you shouldn\'t care about the results of your efforts, but in yoga—as in life—there are no guarantees. You can\'t predict if you will ever be able to stand on your head or abide in a state of wakeful compassion. But you can do two things now, today: You can, first, cultivate appreciation for each moment of your unfolding process and, second, create the conditions for the desired results to arise.\nA dropback—the term commonly used to describe the transitional movement of falling backward in space between Tadasana (Mountain Pose) and Urdhva Dhanurasana (Upward Bow Pose)—is perfect for exploring the notion of process in yoga because the ""pose"" itself is a process. The dropping-back action is bracketed between precise beginning and end points, but the middle part is where the real juice is. This ""in between"" is where you need to stay alert as well as relaxed, loose yet organized, open to possibility yet clear about the direction you are heading. When you can do that, you are both practicing and being yoga.\nSince the transitional process of dropping back differs from a stationary pose, you\'ll need some organizing principles. First, how do you know if you are ready to begin practicing dropbacks? If you can do Urdhva Dhanurasana with straight arms, you are ready. If you can\'t, practice the first three poses presented here, and then practice Urdhva Dhanurasana. Second, ask yourself, ""Where am I going?"" Try to let go of a specific goal and engage in the process by making it your intention to move toward the ability to drop back. And finally, create the conditions for your dropback to happen by working with the following sequence. It\'s designed to imprint certain physical relationships and movements in your muscle memory that will support the process of dropping back.\nBefore You Begin\nTo warm up for dropbacks, begin on your hands and knees in a tabletop position. Do several rounds of Cat-Cow Pose, coordinating each movement with your breath. From there, do three to five rounds of Sun Salutations A and B to generate heat throughout your entire body. Then try the following sequence to open the hips and shoulders. Take five breaths in each pose. Begin on your right side with Parivrtta Utkatasana (Revolved Chair Pose), then move into Virabhadrasana I (Warrior\nPose I). Bring your left knee to the floor, and reach your arms up for Anjaneyasana (Low Lunge). Bring your fingertips to the ground and tuck your left knee behind your right to come into Gomukhasana (Cow Face Pose). Come into the full pose by clasping your hands behind your back. After several breaths, bring your hands into Anjali Mudra (Salutation Seal), fold forward, and twist to the right.\nAfter five deep breaths, roll forward onto your hands and come into Adho Mukha Svanasana (Downward-Facing Dog Pose). Exhale and come into Chaturanga Dandasana (Four-Limbed Staff Pose), inhale into Urdhva Mukha Svanasana (Upward-Facing Dog Pose), then exhale back into Adho Mukha Svanasana. Stay for five breaths, then jump forward and do this sequence on your left.\nSupta Virasana (Reclining Hero Pose)\nOnce your body feels warm and your breath is deep, begin the practice of dropping back by letting go into the earth. Mother Earth does not discriminate. She invites everyone to rest on her. With that reminder to inspire your confidence, begin this pose.\nYou will need a bolster, two or three blankets, and a block. Place a bolster lengthwise on your mat. Stack the block on top of the middle of the bolster, horizontally. Take one blanket and fold it like an accordion so that it\'s about four inches wide. Drape it over the block with the folded edge about two inches from the bottom edge of the bolster.\nSit in Virasana with your buttocks just in front of the bolster. Your sitting bones may or may not touch the floor. When you come into Supta Virasana, the block should be positioned under the bottom of your shoulder blades. Put the block in the correct place before reclining all the way back. When you are fully reclined, use your hands to smooth the top of your buttocks away from your lower back, creating length and space in the sacrum area.\nIf your neck feels strain, tuck the top end of your blanket underneath itself to create more neck support. If that\'s still not enough support, place another folded blanket under the base of your skull. If your ankles or the tops of your feet hurt, sit up and place a folded blanket in front of the bolster. Come back into the pose, this time with your shins on the blanket, but your feet and ankles off the blanket.\nIn this open-hearted position, your arms will drape down toward the floor. If this pulls too much on your chest, you can place a cushion under each forearm. Otherwise, try to stay here and let go into the big opening of the upper chest and collarbone area. In this pose you also open your belly, hip flexors, thighs, and ankles, which you\'ll need to do for your big backbend later. Become familiar with the sensation of your arms dropping down alongside your body, a position that will become important later on. Close your eyes and stay for 20 to 30 breaths. Direct your breath toward the back of your lungs.\nUrdhva Mukha Svanasana (Upward-Facing Dog Pose)\nYou probably practice this pose often. This time, however, focus on how the strength and the connection of your inner thighs support the curve of your spine and the opening of your chest. Slowly sit up. Next, shift onto your hands and knees, moving symmetrically as you exhale into Adho Mukha Svanasana. Strongly engage your arms and legs, but allow your head to dangle freely.\nAfter taking a few breaths, place a block between your thighs. As the block helps you become more aware of your legs, ask yourself, ""Are my legs either internally or externally rotated?"" Perhaps your answer is yes. Or maybe it\'s no! In either case you would be correct: The legs are in a neutral alignment—just as they would be in Tadasana—which is created though a balance of inward-and-outward-moving energy.\nSensitively walk your hands forward as if you were doing a hand-walking meditation. Let this be a sensuous activity, and feel every single vertebra unfold as you slowly ripple your spine. Try to notice the very moment that Downward-Facing Dog becomes Plank Pose, and keep walking your hands forward until Plank turns into Upward-Facing Dog. Take your time with this transition. As you shift forward, imagine that your best friend is behind you and gently but firmly tugging the block backward so that when you arrive in Upward-Facing Dog, your legs are very active. As you press your thighs up, lightly engage your buttock muscles and burrow your tailbone down into your body.\nSometimes when we think of backbending, we focus all of our energy on the curve of the spine. But the legs play a vital role in backbends. To refine the pose, press your palms and the tops of your feet down. From that downward action, lift the back of the thighs and the sternum up. Make sure not to lift your shoulders.\nStay here for a breath or two, again finding breath in the back of the lungs. If you continue to imagine your friend tugging your block back, you might discover that the pose becomes lighter and requires less effort than you thought, especially in the arms.\nOn your next exhalation, lift your thighs so much that they draw you back into Downward Dog. Take your time and be sensitive to how the legs initiate this transition. Alternate between Upward Dog and Downward Dog three or four times, concentrating on the support of the legs, the easy opening of the chest, the natural gaze, and the way each tiny movement leads to the next.\nRest for five breaths in Balasana (Child\'s Pose), and then place your mat at the wall.\nPincha Mayurasana (Forearm Balance), variation\nRemember the feeling of the block between your thighs in Upward Dog? You did the block exercise to create a tactile experience of the inner thighs engaging. To do Forearm Balance and dropbacks, you need energetic support running down into the earth and up along your spine. The conduit for this support is the inner thighs.\nBegin in Dandasana (Staff Pose) with your feet pressing into a wall. Mark where your knees are and then place your elbows there. Come into Downward Dog on your forearms. Make sure that your elbows are directly below your shoulders and that your hands are in line with your elbows. If your hands tend to move toward each other, place a block on the floor between them.\nLift your right leg (or your left, if that\'s your dominant leg), initiating the action from the very top of your thigh, not your foot. Bend your other leg. Simultaneously, kick your right leg and hop off your left leg to come up to the wall. Find the inner thigh connection again by hugging your left and right thighs together.\nOnce you contact the wall, flex your feet and walk your heels up the wall to create length in your back. Now bend your legs and place your feet flat on the wall. Imagine the block is between your legs, or get your yoga buddy to put a block there for you. Press your feet into the wall, your forearms into the mat. Move your inner thighs toward the wall as you draw your tailbone toward your heels. Let your head dangle or gaze at the space between your hands. Try to stay here for five full breaths.\nNotice how your Pincha Mayurasana feels integrated when you strongly engage your legs and root the forearms into the earth. Come down and rest in Child\'s Pose. Repeat Pincha Mayurasana, kicking up with the other leg.\nDropping Back at the Wall\nYou are ready to try a dropback at the wall. Lie down on your back with your head at the wall. Bend your legs, then place your feet on the floor and your hands next to your ears, as wide apart as your shoulders. This is your setup for Urdhva Dhanurasana.\nOn an inhalation, simultaneously press your hands and feet down and lift your bellybutton up to arrive in Urdhva Dhanurasana. If your arms are straight, you are ready to move on. Walk your feet a few inches toward your hands. Then place one hand on the wall and push into it. Place your other hand on the wall. As with the hand-walking meditation you practiced earlier, sensitively but firmly walk your hands up the wall. Take your time and feel both feet connected to the earth.\nWhen you are almost all the way up you might get nervous and want to lift your head or twist. Stay calm, keep your head back, and direct your inner thighs toward the wall. This backward action of the thighs, combined with the downward action of the feet, will bring your pelvis over your legs and help you ripple up through your spine into Tadasana.\nIf these actions felt OK, repeat them a few times to get used to how they relate to each other. When you are ready, you can go back down, dropping back at the wall. To do that, walk up the wall again, and when you get all the way up, don\'t move your feet. This is the right distance for you to go back down. The gap between you and the wall might look huge, but this is where your faith comes in. You just walked up and that\'s where your feet were, so they are in exactly the right place to go back down, too. Trust that.\nFrom Tadasana, fold your palms together with your thumbs on your chin. Lift your chest and ribs up and away from your pelvis as much as you can. Imagine that you are going to touch the ceiling with your sternum. Think of going up, up, up! rather than back. Also, keep your legs straight for as long as you possibly can. Keep lifting your chest as you press your thighs back toward the wall. When you feel you can\'t go back any further, drop your arms to your sides. They will naturally swing back into place and find the wall. It might not seem as if your hands will get to the wall in time, but they will.\nThe arms will fall back and open up, and your strong hands will catch you. Then you can walk down the wall, tuck your chin into your chest, come down, and rest.\nDropping back away from the wall is the same, except...no wall! To develop the confidence to do this, practice dropping back at the wall for as long as you want. Falling back into space takes a lot of faith. Work at the wall with clarity and discipline. Keep your eyes wide open. Make sure your feet and legs stay in Tadasana-like alignment, breathe into your back body, and feel a connection between the earth and your feet. That connection is what will help you soar up and back into your backbend.\nBegin weaning yourself off the wall by stepping forward an inch or two. As you drop back, notice how you will backbend just a bit more deeply before your hands come into contact with the wall. If you feel confident, step further from the wall.\nEventually, you may find that you can step away from the wall altogether. Start by having a partner nearby as a security blanket. Establish your firm and organized Tadasana with your feet slightly wider than your hips and your palms together, thumbs at your chin.\nLift your chest but don\'t let your head go back until you absolutely can\'t look forward any more. Keep your legs straight! Press your thigh bones back, back, back, even though your pelvis is moving forward in space—this will help keep youbalanced. There are always oppositional actions in yoga, and this is a perfect example. As the pelvis moves forward horizontally, the weight of the head can drop back as a counterbalance. The thighs will have to go with the pelvis a bit, but energetically the thighs do not go forward. If both the thighs and the pelvis move forward, there is no tension, no relationship, no yoga. There is just collapsing energy that does not support the spine and leads to pain and injury in the lower back. In order for the spine to be soft and supple, your legs must be strong and stable. If the legs get soft, the spine will harden, making it difficult to bend.\nWhen you can\'t bend back any more with straight legs, drop your arms down by your sides and toward the floor. Your legs will start to bend at this point, and your strong arms will catch you as you arrive in Urdhva Dhanurasana. There will probably be a moment when you feel that you are hanging out in space. Your hands won\'t be on the floor yet, and you will be inside out and upside down. The strength and grounding of the legs will help slow down the process, but it still happens pretty fast.\nLet\'s face it—you might not do a dropback for years! But you\'ve created the causes and conditions for it to happen by connecting to the earth while opening your chest, using the strength of your legs to allow for suppleness in your spine, and using the wall to help you establish a clear pathway for the action of dropping back.\nNow your job is—you guessed it—to practice, practice, practice. As you do, can you be curious and mindful about that process? It won\'t be yoga practice if you do it by rote. The yoga masters say that the mind and body must participate together in every moment of the process in order for us to be in a state of yoga.\nIn other words, when you can pay close attention to your experience as it unfolds, with each breath, each thought, each asana, and each transition—then you will be in a state of yoga. Don\'t try to hold on to that state, either; let it be a moment of change, an opening, a transformation.\nA longtime hatha yoga and Tibetan Buddhism practitioner, Cyndi Lee created OM Yoga in 1998. She has written several books and teaches around the world. For more information, visit omyoga.com.']"	['<urn:uuid:fd73c530-6cdf-4a1b-8f77-297de8049a31>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	11	67	2976
93	designing website how explain color monitor showing differently than printed brochure help	This difference occurs because monitors use RGB (Red, Green, Blue) color space while printing uses CMYK (Cyan, Magenta, Yellow, Key/Black). RGB produces vibrant colors through light on screens, with a larger color range than CMYK. When printing RGB designs, colors appear duller because they must be converted to CMYK's smaller color range. To avoid surprises, you should always set documents for print (like brochures) in CMYK mode, while keeping web designs in RGB. You can also use monitor calibration devices like PANTONE's ColorMunki to ensure accurate color display on your screen.	['In Part one of this series we reviewed some basics on color, and some tips on choosing your colors. In this second part we will cover the first step in working with your digital files on your computer – the RGB color space.\nHere is an example of a typical color RGB color issue. At some point you may have created a design, a logo or perhaps adjusted a photo on your computer screen to produce some really, bright, vibrant, saturated color. Happy with what you had seen on screen and ready for print, you send your file off to be printed and wait for your proof or sample. Sadly, what you receive is not as bright, colorful and saturated as what you had created on screen, what happened?! The answer lies in the beginning, with the RGB color space and your monitor.\nWhat is Color?\nWhat we see as color is the wavelength of light being received by our eyes, whether it is light reflected off an object or emitted by a source such as a light bulb or a computer screen. Red, green and blue (RGB) are the additive primary colors of the color spectrum. Combining balanced amounts of red, green and blue lights produces pure white. By varying the amount of red, green and blue light, all of the colors in the visible spectrum can be produced. Your monitor screen can produce millions of these color combinations, but still not able to reproduce all the visible colors. And depending on your monitor, the colors may not be displayed accurately. This is hurdle ‘one’ and can be addressed by buying a good quality monitor with even brightness across the entire screen area, and with the purchase of a calibration device to ‘profile’ or map out the color reproduction capabilities of your monitor. These devices were once fairly expensive but have come down considerably in price and can now be used to profile many different types of displays such as LCD, LED and projectors – even your HDTV in some cases. PANTONE’s ColorMunki (www.pantone.com) is a very capable entry-level device for calibrating your displays and is available for around $170.00. Keeping your monitor color stable and consistent will go a long way towards improving your output when it comes time to move beyond the screen.\nWhy is my sky purple?\nNow that you have calibrated your monitor and mapped its colorspace, you opened up a picture of your dog catching a Frisbee in the park, intending to adjust the color and remove a trash can in the background. Perhaps you made the sky a bright, bright blue, the grass a vibrant green, and the Frisbee a bright orange. Working in RGB color, this photo leaps off your screen which will make it the perfect cover photo for your brochure. You package up your files, upload them to your print service provider and await your proof. When your proof arrives, all those bright color are shadows of their former selves – and no, your printer did not make a mistake. In order to print your image in process color, your image was converted at some point in the workflow to CMYK color space, which is quite a bit smaller than the RGB color space. In doing so, colors that are outside the CMYK range (or gamut) are pulled back down into the printable range, which makes them appear duller and flatter. To avoid surprises in your output, many applications support color profiles and allow you to use your calibration profile to simulate your final output color on screen through ICC color profile conversions. Adobe Systems Inc. provides excellent tools for color management within their design programs, so please visit their site at www.adobe.com to learn more about them.\nMore Pantone Goodies\nIf you are serious about color, a great investment for your color toolbox is a set of Pantone color guides. The color scientist’s at Pantone have created color swatches of each single spot color ink, and swatches of those colors converted to CMYK process color simulations, and shown on both coated and uncoated papers. These printed swatches and the paper they are printed on follow a strict set of guidelines for color reproduction, and while you may not be using the exact same papers and inks, will give you an excellent idea of how your color will reproduce. In addition to the visual references, Pantone is including a nice little software app, the Pantone Color Manager, that is web-capable and can update itself with the latest definitions automatically. The app also includes conversions for RGB and HTML, so that you can use the correct color definitions across your web pages, tablets and mobile apps. For more about Pantone color tools, visit them at www.pantone.com.\nTalk to us about your color\nWhen you are ready to start managing your color to a finer degree, give us a call or talk to your consultant. We can provide you with expert advice on your color management and recommend the right tools to make it work for you. We can also provide our print profiles to you to install in your color management system for more accurate proofing and color conversion.', 'Using the correct color mode in your design is essential to getting your message across. Learn the distinctions between RGB and CMYK color profiles with this simple overview.\nCover image via Rostislav_Sedlacek.\nThere can be a lot of confusion behind RGB and CMYK color modes. If you’re a designer, it’s crucial to differentiate the color profiles since designing in the wrong mode can affect the tone and message of your document. In this article, we’ll decipher the characteristics of each profile and when to use RGB and CMYK in your designs.\nThe RGB color profile consists of Red, Green, and Blue hues that combine to create extensive variations of colors. This color mode exists exclusively in screen displays, such as in computer monitors, mobile, and television screens. Even though RGB is present across all electronic devices, the color elements vary across systems and models. An image you see on an iMac may display differently than the same image on a Dell desktop computer.\nInstead of utilizing ink to produce hues, the RGB profile partakes in additive processes to produce color by blending light. This is the exact opposite of subtractive color processes, such as mixing paints or dyes. The presence of all RGB primaries at full intensity yields white, while the absence of color produces black. When you turn a monitor or screen off, you see an absence of RGB color, resulting in black. The color displays on your screen result from the presence of those RGB base hues.\nIn Adobe Illustrator, you can see how mixing RGB primaries at different intensities can vary the color output. When isolated, the primary hues generate vibrant shades. Red and green combine to yield yellow, green and blue mix to create cyan, and blue and red produce magenta.\nRGB produces a large array of colors; this color profile features a larger gamut, or color range, than CMYK. Above, you can see how Illustrator provides a gamut warning for RGB colors that can’t be successfully translated into CMYK. This occurs when an RGB color exists outside of CMYK printing abilities. Instead, the color will convert to a CMYK equivalent, which could affect the tones of your design. When printing a design produced in RGB color profiles, the print output will be different from the online preview.\nWhen to Use RGB Color Mode\nTo avoid color profile complications, always set a document in RGB when designing for social media, digital design, or online advertisements. Set any design or image that will exist on a screen or monitor as RGB. If you design in CMYK for an online-only design, you’ll limit your color options due to CMYK’s limited gamut range.\nThe CMYK color profile contains Cyan, Magenta, Yellow, and Key (Black) that combine to produce a range of hues. This four-color process works for any type of printer. When zoomed in on printed images you can see the four-color dots that layer to create different hues and gradations. Dots per inch result from printing and involve the CMYK color profiles. Although all printers produce prints in CMYK, the end result may vary among different styles and models of printers.\nIn RGB color spaces, all primaries combine to produce white with additive color processing. CMYK modes combine with subtractive color processes, meaning all primaries mask to yield to a blackish hue. This process is similar to when you mixed paints and dyes as a kid to make that unsightly dark color. As inks and dyes are layered upon each other, they subtract from the white of the paper.\nNotice how the CMYK sliders differ from the RGB sliders. Zero intensity of all primaries produces black in RGB color profiles; in CMYK color profiles, the absence of color produces white. Cyan and magenta create a blue, magenta and yellow create red, and yellow and cyan produce green. The combination of cyan, magenta, and yellow develop an off-black shade. Key, or black, comes in to add shades to each primary due to the primaries’ inability to combine and create black.\nWhen to Use CMYK Color Mode\nReserve CMYK for printed designs. Set products such as business cards, flyers, posters, and packaging in the CMYK color profile, and do yourself and the print shop a favor by always setting your document to CMYK before exporting or printing. Doing so prevents unbalanced colors that can result from RGB colors translated into CMYK equivalents.\nChanging Color Modes in Creative Cloud\nMost Adobe Creative Cloud programs will default to RGB color modes. You can easily adjust modes in the New Document panel under the Color Mode dropdown, or within the program itself. Not all programs support RGB to CMYK conversions, but with Illustrator, Photoshop, and InDesign, you can switch in just one click.\nIn Adobe Illustrator, you can switch colors modes by navigating to File > Document Color Mode > RGB Color or CMYK Color. This will affect the output of your vector shapes when shifting between color modes.\nWithin Photoshop, you can change color profiles by going to Image > Mode > CMYK or RGB Color.\nNavigate to the Color Panel within the software. If the panel isn’t visible, hit F6 to bring it up. Hit the dropdown symbol and select CMYK or RGB.\nLooking to expand your knowledge on essential design terminology? Check out these educational posts:']	['<urn:uuid:f079cb8a-0acb-4a93-bf66-a786dc51f63d>', '<urn:uuid:24845e0f-9c7f-46e4-81ac-3a59c9a12fef>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:35:02.547110	12	91	1754
94	specialized hardware locations portrait studio versus crypto mining facility setup	Portrait studios typically require basic camera equipment and can operate flexibly in various environments, with photographers needing to manage variables like weather and surroundings while using props and composition techniques. In contrast, crypto mining operations require extensive specialized hardware, often housed in shipping containers or warehouses, and seek specific locations with cheap energy sources. These mining facilities are sometimes set up at power plants or gas wells, using purpose-built mining rigs that consume substantial power to solve complex calculations.	['Great portrait photography is notoriously difficult to accomplish. People having their photographs taken are often uncomfortable at first, which can result in stilted looking photos; portrait photographers also depend on variables in the surrounding environment, including the weather, to help make a great portrait. There can be so much happening at any given moment the photographer can become distracted and overwhelmed by details. However, you should remember that no matter what happens there are no hard and fast rules for making a great portrait. These tips will help you snap photos beyond the standard headshot for truly amazing prints.\n1. Break eye contact.\nTraditional portrait photography usually has the subject looking directly into the camera lens, so that they appear to be making eye contact with the viewer of the final photo. With digital photography, you can experiment with breaking eye contact without breaking the bank on film. Try having your subject look at something outside of the frame, or even at another object within the frame, such as their hands. This can create powerful images that really pop with emotion.\n2. Introduce props.\nUsing props in portrait photography can create additional interest, and when the prop reflects an aspect of the subject’s personality, the person you are photographing might even feel more comfortable, leading to more candid shots. Props can be meaningful, like family memorabilia, or just fun, like toy balls. These types of portraits are also the most fun to have printed as canvas prints or digital prints for wall art!\n3. Pay attention to your Composition techniques.\nMost portrait photography follows the Rule of Thirds an imaginary grid of nine blocks, across the center of which one envisions a cross going horizontally and vertically. Following this rule, the main photo has more interest when the subject is not placed in the center of the photo. Experiment with placing your portrait subject off to the side of the frame, or on the bottom of the frame looking up, or even holding the camera at a slight angle so that your subject fills the frame diagonally, for new and interesting portraits. Basically, get your subject out of the middle of the frame and you will instantly have more interesting portraits.\n4. Play with motion.\nNot everything in a portrait needs to be in focus. In addition to using selective focus, why not play with motion to add interest and drama to your portrait photography? You can have the subject move, frame the subject against a moving background, or even move the lens of your camera during a shot to introduce interesting blurs and a sense of liveliness to your photos. Shooting with a tripod will help you achieve these effects without additional, unintentional movement that can distract from the composition. Great for sport portraits or for super energetic child portraits.\n5. Learn how to do your own edits.\nIt’s all too easy to rely on Instagram and similar services to edit your digital photography. While the filters these services offer can be eye popping and have their place, viewers can usually tell the difference between a carefully edited portrait that relies on its own merits and a portrait that has been edited with an instant filter. Taking the time to learn how to do edits in Photoshop, Corel, and other semi-professional to professional level photo editing suites will be more than rewarding, and will help you take your portrait photography to the next level.', 'By Emma Marris, The Atlantic\nMarch 22, 2023\nAt this point, for most of us, cryptocurrency seems like nothing more than a fad. After the FTX bankruptcy and broader crypto crash last year, basically all of the celebrities who were promoting crypto have gone silent. “MiamiCoin,” hyped by Miami Mayor Francis Suarez as a new source of income for the city, is now worthless. The Wild West days of the industry may be over. Recently, the head of the SEC warned crypto firms to “do their work within the bounds of the law” or face enforcement actions. Lots of people lost money in the crash, but from the planet’s perspective, the industry’s downfall is good news: The computing power fueling the crypto boom was so substantial that it was causing substantial greenhouse-gas emissions.\nAnd yet crypto’s greenhouse-gas emissions are still shockingly high, according to an industry tracker run by the University of Cambridge. The tracker focuses on bitcoin, the cryptocurrency with by far the largest market share, and estimates that at its current rate of “mining” new coins, bitcoin will release about 62 megatons of “carbon-dioxide equivalent” each year—about as much as the entire country of Serbia emitted in 2019. That’s up from about 43 megatons a year in December, and just slightly below the all-time peak of nearly 74 in May 2021. Many people who’ve invested in crypto tend to have a lot of sunk costs, whether digital wallets bulging with various coins, tokens, or expensive physical setups designed to make more. Even now that the boom times are over, they have no reason to stop.\nMining bitcoin does not involve actually digging anything out of the ground—unless you count the fossil fuel that often powers it. The process involves using heavy-duty computers to grind through trillions of calculations, solving equations to create virtual coins. The method is known as “proof of work.” Once upon a time, bitcoin mining was something that people did if they had a couple of spare computers they wanted to put to work. Over time, it’s taken more and more computing power to unlock a single coin; now most mining is done in large-scale operations using purpose-built mining rigs.\nAnd it is America’s problem now. After China clamped down on crypto mining in 2021, such computing work increased in the United States. Miners set up shop in communities with low energy prices. And owners of unprofitable power-generation infrastructure, such as waste-coal-burning power plants, opened up crypto-mining operations to create another revenue stream. These companies have put a lot of money into their hardware and their physical space, and they will continue mining until they are actively losing money. “There are miners that have been quoted saying, ‘As long as the price is over $10,000 per coin, it still can generate money,’” Elizabeth Moran, a policy advocate at the green law firm Earthjustice, told me. And that is a big reason crypto keeps spewing out so many emissions even during the “crypto winter”: Bitcoin prices in particular have held up, in fact they just passed $28,000 a coin. That’s still far below their peak of almost $68,000 in late 2021, but represents a bit of a comeback from the sub-$16,000 prices of last fall.\nSo it is still very possible to make money at this game. Some companies bypass the energy grid entirely; depending on the price of gas and the price of bitcoin, turning natural gas into crypto might be twice as profitable as selling it to the wholesale gas market. Gas companies bring in a trailer or three jam-packed with generators, plugging one end into the well and the other into “shipping containers full of bitcoin miners,” says Rob Altenburg, the senior director for energy and climate at PennFuture, an environmental nonprofit. “We’ve heard of three different companies doing it. But we’ve got thousands of fracked gas wells across the state and just simply have no way of knowing where this is happening.” Gas drilling is heavily regulated, but crypto mining itself is not.\nA recent federal investigation in Colorado found crypto mining powered by gas wells on public-lease lands, creaming energy off before it hit the grid and converting it to crypto without paying any royalties. The report noted that because the generators and rigs are usually on trailers, the entire operation can be moved quickly, so miners can stay ahead of government oil and gas inspectors. Other “behind-the-meter” operations are physically located at power plants. The natural-gas-fired Greenidge Generation Station, on the shores of Seneca Lake in upstate New York, opened a massive bitcoin-mining operation plugged right into the plant, which in 2021 consumed the bulk of the electricity it produced. Tapping into energy before it hits the grid is just one way bitcoin miners keep costs down; they’ll seek out and exploit any cheap source of energy.\nCrypto doesn’t have to torch the planet. The second-largest cryptocurrency, Ethereum, switched to a different method of creating its tokens in September 2022. The new approach, called “proof of stake,” uses significantly less computing power, so much so that after the switch, the company’s total energy consumption dropped 99.95 percent. “It is impossible for bitcoin to switch to proof of stake, because the bitcoin network is completely decentralized,” Kyle Schneps, the director of public policy at Foundry, a major mining financier, told me. “There is no governing body that could make such a decision.”\nRenewables could also power bitcoin mining, just like they power anything else. Maybe as much as 38 percent of bitcoin mining is currently powered by renewables, according to the Cambridge tracker, though no one really knows. But that hasn’t gone up since the crypto winter. Schneps said that bitcoin mining could help with the energy transition: Renewable-energy companies can always sell their energy to bitcoin miners when demand is otherwise low, keeping them profitable enough to stay in business and grow. But it’s not clear if mining operations that run only at certain times would be profitable.\nFor now, bitcoin will remain an albatross on the planet at just the moment that the energy transition ramps up. Cambridge predicts that its environmental impact in 2023 will be worse than it was in 2022. The Super Bowl ads and awkward late-night celebrity endorsements may be gone, but crypto is not dead. Still embraced by true believers and international criminals, the hard drives grind on, in shipping containers and empty warehouses and back lots of power plants, endlessly calculating, spinning money out of carbon and faith.\nLots of other digital activities do consume power and cause greenhouse-gas emissions—questing with pals, hoarding years of work emails on the cloud, making friends with a hallucinating AI. One analysis in 2019 suggested that our online lives were responsible for 3.7 percent of planet-wide emissions; the number may have gone up since. Schneps likened bitcoin’s global electricity consumption to “roughly the same as video games.” But even if that’s true, while two-thirds of Americans play video games, just 21 percent of Americans own crypto, and even less bitcoin in particular. The massive environmental impact of bitcoin is harder to swallow because it is part of an industry that is, in essence, “smoke and mirrors,” as the crypto blogger James Block put it in an interview with Charlie Warzel. “There’s nothing produced by these companies.”\nFinance experts around the world largely agree with Block. In December, a director-general at the European Central Bank, Ulrich Bindseil, called for serious financial institutions to stop legitimizing cryptocurrency, saying bitcoin was “not suitable as an investment.” If the world is going to continue to burn fossil fuels, it makes sense to do so for things that genuinely contribute to people’s well-being, not for risky virtual tokens untethered to any real thing of value in the world.\nThe original story appeared in The Atlantic: https://www.theatlantic.com/science/archive/2023/03/crypto-bitcoin-mining-carbon-emissions-climate-change-impact/673468/']	['<urn:uuid:b6e2b6e9-7df9-4f96-ba71-c8e4e78a85c9>', '<urn:uuid:44578729-3bae-4a8c-aa6a-f796f24f7dcc>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:35:02.547110	10	79	1879
95	stone quarry tools health hazards	Stone quarrying involves tools like bars, jacks, and logs for extraction and transport, while stonemasons use various tools including rulers, hammers, chisels, and saws. However, these practices pose significant health hazards. The crystalline content in stone dust can be inhaled during quarrying and masonry work, potentially causing silicosis, which reduces lung elasticity and impairs breathing. This condition can lead to poor blood circulation, carbon dioxide buildup, and eventual heart failure. Studies have shown that excessive stone dust inhalation also increases lung cancer risk, particularly with sandstone and granite due to their high silica content.	['The Folk Arts Museum of Laduz (Burgundy) honors a retrospective of tools and works, the stone workers and earth. You will discover a unique collection dedicated to the memory and to the expertise of these artisans.\nHe works in a quarry or open galleries career, the stone extraction application to the carrier considerable physical effort. Using the bar to mine, using leverage, it rotates the stone blocks that will be raised with the jack before being transported on logs. The quarryman business is strongly linked to that of stonemason.\nThe Stone Carver or stonemason\nThe stonemason uses many tools to work the stone, rulers, squares, false squares, compasses for tracing, masses, stubborn, brettelés hammers, chisels, bédianes to rough stone and bush hammer, chisel to refine and improve the work. He uses “the railway”, a kind of plane 6 or 7 serrated blades for use stone and “saw crocodiles” up to two meters long.\nAs the stonemason, the slate works in a career. The slate deposits are found mainly in France in four clay ranges: the Pyrenees, the Massif Central, Brittany and the Ardennes. The first operation of the slate is quernage which involves the separation of blocks before cutting the next template. Slate is a naturally waterproof material serving as cover for roofing.\nThe work of the mason is different. It begins with the excavation and foundations, then comes the assembly of rubble walls, cut stone or brick, grout and plaster walls. Mason is using shovels and buckets to prepare mortar and troughs to transport it. The scaffolds are made of trunks of young trees firmly bound together.\nThe potter meet the priority needs of rural families collect, cook and carry liquids and foods. He realizes his works with the previously purified clay through molding techniques, casting, modeling but more often by turning the parts. After natural drying the parts, the craftsman is baked which gives its permanent hardness. The famous neighbor Puisaye for quality pottery provided a large part of western France.\nDepending on the region of France, is the roofer roofer wood as in Champagne lousy, stone roofer mountain regions, roofer slate or tile, or thatcher, roofer straw. In Burgundy, the important manipulation of small Burgundy tiles requires a large workforce.\nThe glass business succession magic. Glass making is divided into four periods: the charge, melting, tisefroid and work. During this last operation that lasts 10 to 12 hours, the skill of the glassblower with his long cane is essential to shape the bottle, for example, in the mold. If industrial production replaced artisanal creation, there remains art glass.\nThe tile maker is essentially a rural craftsman. It uses finer and more compressed than the lands bricklayer. The tiles are of different shapes and thicknesses depending on the region and are calibrated in an iron mold. Each day, a tile maker could achieve 1100-1500 tiles following they were hollow or flat.\nThe use of brick for the construction of an alternative to the stone which in some areas is too gélive. The bricklayer uses sand clay emaciated or otherwise supplemented powdered limestone for brick molding. The brick maker often has worked as a tiler and also manufactures tiles intended for use as flooring.\nMusée des Arts Populaires\n22 Rue du Monceau\nLe Musée est ouvert à tous:\n• en Mai et Juin, les samedis, dimanches et jours fériés de 14h30 à 17h30\n• en Juillet et Août, tous les jours de 14h30 à 18h\n• en Septembre, les samedis et dimanches de 14h30 à 17h30\n• les week-ends de Pâques et de la Pentecôte\nLa visite est possible sur demande par mail ou par téléphone, en dehors des jours d’ouverture, sauf en décembre et janvier.\nTéléphone & email\nTéléphone : 06.76.21.32.61\nTéléphone fixe : 03 86 73 70 08', 'Building materials used in construction of building structures can welcome health issues subsequently for occupants or to workmen involved in building services, maintenances, demolition and refurbishment.\nThis may be due to:\n- Vibration Effects\n- Poor acoustic characteristics of the building\n- Dysfunctional lighting\n- Extremes of relative humidity\n- The air exchange is inadequate\n- Toxicity mediated through soil\nThe above-mentioned issues will affect the durability of the building along with the health and comfort of the people residing in the respective building.\nBuilding Materials to Hazardous Material Conversion Stage\nThe whole life of a construction material must be considered and audited into account to understand what are the human and environmental health hazard issues from these materials.\nOther than this, the following mentioned stages must also be considered:\n1. The health as well as environmental impacts during the procurement of raw materials and the final disposal of the by-product or the waste product must be considered.\nThe process of obtaining raw materials includes mining, synthesis, manufacture and quarrying. Disposal of mine or unwanted by products, quarry spoil is some of the disposed materials.\n2. The hazards and health effects during the transportation of the materials must be considered.\n3. The effects of manufacturing a building product: The harmful effects if exist will affect the production worker, the members involved in the process, the public and the environment around.\n4. Improper handling of the product bringing harmful effects to the construction workers, the local residents and the environment.\n5. The hazard effects from commissioned buildings that have installed product to the people residing.\n6. The hazard effects from the works like maintenance, refurbishment and servicing to the professional workings, the decorators, plumbers, electricians and residents.\nDuring the time of construction, the surface skins and the connective tissues filling the gas are largely composed of materials. By the end of 20th century as a concern of economy and safety thousands of construction materials of asbestos have been installed in UK for new and old buildings.\nThe variation in the use of water supply connection materials have also brought mitigation to leakage problems hence avoiding the possibilities leakage that will in turn cause cholera, typhoid and other health problems.\nThis was the time where less toxic materials where used for plumbing systems like copper, iron and polymers.\nDifferent Mineral Based Construction Materials and Their Health Aspects\nCare is necessary during the selection of constructional materials that are mineral based as their respective nomenclature is commonly imprecise. The use of a hazardous material or selecting a safer alternative for the design of buildings are determined by an informed specifier.\nHence the state of knowledge of the builders and the occupants will determine the methods of working with such hazardous materials and replacement products if any.\nSome of the mineral construction materials and their respective health aspect is mentioned below:\nThe crystalline content and the amount of dust that is generated have the chances to be inhaled. This is the major health hazard caused by stone mineral used in construction works. Higher exposure to this will cause silicosis.\nThis is a health problem where the lungs will lose the normal elasticity, which will cause the act of breathing to become laboured. This will result in the impairment of blood circulation. This hence will result in the building up of waste carbon dioxide within the body and oxygen fails to enter the tissues. This will make the heart overburdened and final failure.\nThe excessive inhalation of stone dusts will result in increase of lung cancer as concluded form the studies. Based on the variation in the silica content of stones, their disease potential also varied.\nSandstone and true granite are notorious hazards for the masons. Sandstones are highly siliceous but granite can vary with their quartz content.\nThe hazard of these material lies in their quarrying and masonry work procedures also. To avoid risk, wet methods of blasting were introduced.\nThe slate material is also rich in silica content which bring the hazards of silicosis problems to the workers, the slate splitters and the engravers. The slate in the forms of slabs and tiles bring no form of harm to the occupants.\nConcrete patching will result the operatives to get irritation and become sensitive to the forms of cements and the additives used in the cement. The skin contact with the chromium ion present in cement is a problem. Withdrawal of contact with the cement faster will help in sooner recovery.\nThe clay is fired and later used for decorative purpose by glazing it. It is employed in the production of roof, sanitary piping, floor tiles and material structural decorative bricks.\nLarge silicate clay having a lesser hazard effect to health is transmuted during the clay firing into crystalline silica which are especially hazardous in nature.\nThis hazard effect is till the manufacturing process. This won’t affect the construction workers or the occupants that are residing.\nThis product is used in block form. It is used as a non-combustible thermal insulation between the joists. These bought problems for the carcinogenic potential.\nThis was determined by the observation of asbestos and vermiculite fibers in many specimens. A health study on a group of miners helped to find this health hazard.\nThe trapping of dust through the gaps of the ceiling and entering the room is a big cancer threat. If the material is very much needed, a fiber free material can be specified by the designer.\nA variety of chemical compounds in physical forms either of natural or synthetic origin have these calcium silicates. Some have fibrous forms while some have discrete particulates.\nThe test for carcinogenicity is determined by finding the calcium silicate content. Here too it is better to suggest non-fibrous products.\nGypsum in Construction\nThe gypsum material shows no health hazard. But it has accounted for 10 to 50 percent of radon in the soil due to the manufacture of plasters and plasterboards.\nFibrous form of serpentine and amphibole minerals form the asbestos. These materials where a concern behind the risks of asbestosis, lung cancer and mesothelioma. Many new materials out of cement and polymer matrix where developed to mitigate this problem and replace the asbestos material.\nNon-Asbestos Mineral Fibers\nWHO considered that there are materials to replace asbestos by materials like glass, slag wool, ceramic and refractory fibers. Complete removal of fibers products from the construction field is not possible but the most hazardous one can be removed.\nMetals That are Used in Construction and their Health Effects\nThe different metals that are used in different construction application and their respective health effects are described briefly:\nThe lead toxicity has resulted in problems like paralysis, anemia, malaise, severe colic, convulsion etc. These can result in frank disease during their intake. This bought a decision on the replacement of lead as a construction material.\nIn olden days, the principal sources of environmental lead contamination were the solders, paints and the water pipers.\nAdults intake the lead through drinking water while the children intake is through licking the wall with paint, chewing paintwork and house dust.\nThe use of copper in contemporary building have helped to eradicate the source of lead. The reformulation of the pain and the use of linseed putty help the lead entering the children tastes.\nThe harmful effects of leads have made the use of inorganic lead in paint and the use of organic lead in petrol by the government.\nThe chromium does not bring any sort of hazard effect, whether if it is used as a decorative material or as a protective material in construction. This can be used as a corrosion resistant alloys.\nThe salts of chromium are used in cement additives. This may cause skin irritation on contact. This may sometimes lead to the persistent dermatitis. These metals also bring concerns about carcinogens.\nCopper used in construction either in the form of metal or alloyed in brass bring no kind of health problems for the occupants. The workers engaged in industrial activities are more exposed to the heavy copper metal fume in large extent.\nAcute and chronic effects are caused on the lung, kidney and bone by the entry of cadmium. Hence cadmium is a toxic element. These metals are used to coat steel, where they act as a protection against corrosion. This protective layer brings no form of harm to the occupants.\nThe redevelopment or demolition process will employ flame-cutting of the metal. This have resulted in attacks due to metal-fume fever. Some of these cases have been found fatal.\nThe cadmium metal will liberate a toxic fume, when they are overheated. This is hence categorized under hard solder. This is employed in the installation of certain domestic apparatus. But they bring no harm to the occupants. This can bring harmful effects for the engineers who carry out soldering and disassembling of the same.\nThe paints and plastic make use of cadmium as a pigment. In areas wherever there is possibility of leaching out or have chances of being ingested by infants, the same has to be excluded and a better alternative have to be suggested.\nIron, Stainless steel, steel and Aluminium Metals\nThe mentioned metal brings some sort of adverse effects in certain situations. But their use has never been foreseen to offer any form of risks to the occupants.']	['<urn:uuid:1611e069-b2aa-46dd-9426-2ea44a6b08a0>', '<urn:uuid:6ad3eef2-50ac-4aca-82f5-d93ca2c0de10>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:35:02.547110	5	94	2187
96	what factors affect chemical reaction rate temperature surface area concentration collision theory	Chemical reaction rates are affected by several key factors: temperature (higher temperature increases reaction rate by making molecules move faster), surface area (smaller particle size provides more surface area for collisions), and concentration (higher concentration means more particles available for collision). These factors all relate to collision theory, which states that reactions occur when particles collide with correct orientation and energy.	"[""Send the link below via email or IMCopy\nPresent to your audienceStart remote presentation\n- Invited audience members will follow you as you navigate and present\n- People invited to a presentation do not need a Prezi account\n- This link expires 10 minutes after you close the presentation\n- A maximum of 30 users can follow your presentation\n- Learn more about this feature in our knowledge base article\nDo you really want to delete this prezi?\nNeither you, nor the coeditors you shared it with will be able to recover it again.\nMake your likes visible on Facebook?\nYou can change this under Settings & Account at any time.\nNational 5 Chemistry\nTranscript of National 5 Chemistry\nUnit 1: Chemical Changes and Structure\nSection A: Rates of Reaction\nSection B: Atomic structure and bonding related to properties of materials\nSection C: Formulae and Reaction Quantities\nSection D: Acids & Bases\nLesson 1: What is a Chemical Reaction?\nA chemical reaction is a change involving the formation of a new substance.\nWe are going to carry out four small experiments to show different signs of a chemical reaction occurring.\n(Use 1/3 of a TT of all solutions)\n1. Add Magnesium to Copper Sulphate\n2. Add Magnesium to Hydrochloric Acid\n3. Add equal volumes of Barium Nitrate to Sodium Sulphate\n4. Add Ammonium Nitrate to Water\nIn your classwork jotter, under the heading 'Signs of a Chemical Reaction', briefly describe whether you think a reaction has occurred in each case and what evidence you have to support this.\nA chemical reaction is one in which one or more new substances are formed.\nTo identify that a chemical reaction has occurred, we can look for\nA colour change\nA change in temperature\nA gas being produced (bubbles/effervescence)\nA precipitate forming.\nSome types of chemical reaction to think about\nThe reaction of an acid with a base\nThe formation of an insoluble salt from the mixing of two ionic solutions\nThe reaction of a substance with oxygen to produce energy\nthe less reactive element in a compound is replaced by the more reactive one\nLesson 2: Different Rates for Reactions\nTo define 'chemical reaction'\nTo carry out experiments and discuss evidence for a chemical reaction occuring\nI can define 'chemical reaction'\nI can work safely with others\nI can draw conclusions about experiments\nTo understand Collision Theory\nTo think about different ways reaction rates can be measured.\nI can use diagrams to show my understanding of Collision Theory.\nI can describe at least two ways reaction rates can be measured.\nSome reactions happen very slowly\nThe Formation of Oil\nOther reactions are much faster\nWhat needs to happen for a reaction to occur in the first place?!\nIn order for a reaction to occur, reactant particles must collide with\nthe correct orientation\nthe correct energy\nThe rate of reaction can be found by measuring how much of reactant is used up, or how much product is formed, in a given time.\nreaction rate =\nchange in concentration of reactant\nConcentration is a measure of how much solute is dissolved in a solvent.\nIn chemistry we measure this in\nmoles per litre ( mol l-1)\nWhy do some reactions occur faster than others?\nLesson 3: Following the Course of a Reaction\nTo follow the course of a reaction by monitoring a change that takes place over time\nI can identify different properties that could be monitored eg volume of gas released, concentration of either reactant or product.\nI can plot a reaction rate graph\nI can calculate rate from a graph of e.g change in volume of a gas v time\nSet up the apparatus as shown in the diagram above. Measure out 50cm3 (4moll-1 hydrochloric acid GOGGLES), 10g of small marble chips\nPrepare a results table in jotter\nTotal Mass (g)\nMass of gas released(g)\nUse this information to construct a table of time (x-axis) and mass (y-axis)\nThe reaction is fastest at the start resulting in a\n. As the reaction proceeds it slows down leading to a\n. When the reaction stops the graph levels out to a\nHow to draw a graph\nMust use a pencil\nDraw in your axes with a ruler\nLabel your axes and include units.\nThe independent variable goes on the x-axis.\nThe dependent variable goes on the y-axis.\nPlot your points using a small x.\nLook for the pattern:\nIf a curve: Join the points with a smooth freehand curve.\nIf a straight line: draw a line of best fit.\nLesson 4: The effect of particle size on reaction rate\nTo investigate how changing the size of marble chips alters the rate of reaction with hydrochloric acid using given experimental results\nI can explain the effect of different particle sizes on reaction rate in terms of the collision theory\nIn this experiment we will look at the speed of the reaction when we change the particle size of the reactants in a chemical reaction.\nA: Whole Tablet\n1.Fill a clear beaker with 100cm3 of room temperature water.\n2.Drop 1/2 an Alka-Seltzer tablet into the water. Measure and record the time to react.\nB. Powdered Tablet\n1.Place 1/2 Alka-Seltzer tablet into mortar and grind to a fine powder.\n2.Transfer powder into a clear beaker (Note: It's important to have the powder in the cup before adding water.)\n3.Add 100cm3of water to the beaker. Measure and record the time to react.\nAlthough it is sometimes easy to determine which of two experiments reacted faster just by observation it is more accurate to measure the loss of reactants or the formation of products at different time intervals. This can be shown in the experiment below.\nUsing the set of results in the following table plot line graphs of 'mass of CO2(g) produced' versus 'time' for both the powdered calcium carbonate and the calcium carbonate lumps, using graph paper\nSmaller reactant particles provide a greater surface area which increases the chances for particle collisions so the reaction rate increases.\nAs particle size decreases, what happens to the reaction time? What happens to the reaction rate?\nWhy do you think this was?\nTry to draw a diagram to show what you mean.\nLesson 5: The effect of concentration on reaction rate\nTo practically investigate how concentration affects reaction rate.\nI can explain how changes in concentration affect reaction rate in terms of collision theory.\nI can carry out an experiment safely, draw conclusions and evaluate the process.\nIn this experiment we will look at the speed of the reaction when we change the concentration of the reactants\nEnsure that your apparatus is clean before use and that you clean, dry and put away all apparatus after use and then clean your work area. After this, wash your hands and THEN remove your goggles.\nIn this experiment we will react a substance called sodium persulphate (reactant A) with a substance called potassium iodide (reactant B).\nWe also need to add some starch as an indicator.\nA colour change will show the end-point of the reaction.\n1. Using the syringes provided measure out\n10cm3 of Reactant A\nand 1 cm3 of the starch solution into a clean dry beaker place upon a piece of white paper with an X marked upon it.\n2. Fill another 10cm 3 syringe with reactant B. Quickly add this to Reactant A in the beaker\nSTART YOUR TIMER\nWHEN B IS ADDED TO A. Record the time when the X can no longer be seen.\n3.Repeat the experiment a further two times.\nThe second time: in step 1\nuse 8cm3 of Reactant A, 2 cm3 water\nand 1 cm3 of the starch solution\nThe third time: in step 1\nuse 6cm3 of Reactant A, 4 cm3 water\nand 1 cm3 of the starch solution\n1. Create a suitable table to record your results\n2. Write down your conclusions (these must refer to reaction rate)\n3. Evaluate your experiment and your results\nIncreasing the concentration means that there is more chance that the particles will collide. The frequency of the collisions will be greater as we increase the concentration, and the reaction rate will increase.\nIncreasing the concentration increases the reaction rate.\nDecreasing the concentration decreases the reaction rate.\nLesson 6: Investigation into the effect of changing temperature on reaction rate\nTo carry out a practical experiment into the effect of temperature on reaction rate.\nI can explain the effect of an increase in reactant temperature on reaction rate using the collision theory\nWrite up your experiment in the paper provided and then store it carefully in your PRACTICAL FOLDER ensure that your name and candidate number are on your investigation report\nBy heating chemicals, we raise the\nlevels of the molecules involved in the reaction. Increasing the temperature means the molecules move faster so are more likely to collide successfully and the reaction rate increases.\nLesson 7: Using Catalysts\nTo define what a catalyst is and describe briefly how it speeds up a reaction.\nI can safely carry out experiments involving catalysts.\nI know examples of where catalysts are used in industry.\nCatalysts are substances which speed up chemical reactions but are not used up in reactions.\nThey work by providing an alternative pathway for the reaction which requires less energy.\nBiological catalysts are called enzymes.\nExamples of catalysts:\nIron in the Haber process, which converts hydrogen and nitrogen into ammonia.\nPlatinum in the catalytic converters of cars.\nNickel in the manufacture of margarine.\n1. Place 2 test tubes in a rack and pour in hydrogen peroxide to a depth of 2cm\n2. To the first test tube, add half a spatula of manganese (IV) oxide.\n3. To the second test tube, add a small piece of liver tissue.\n4. Watch what happens!\nHydrogen Peroxide naturally decomposes (breaks down) into water and oxygen.\nH2O2 --> H20 + O2\nThis can be seen by the presence of tiny bubbles in the hydrogen peroxide.\nThe enzyme catalase, which is present in liver, naturally speeds up the breakdown of hydrogen peroxide which is harmful to living things.\nThe chemical manganese dioxide also speeds up the breakdown of hydrogen peroxide.\nLesson 1: Revision of Atomic Structure and Nuclide Notation\nTo revise theory from S3 on atomic structure and nuclide notation.\nI can describe the features of all the subatomic particles.\nI can write nuclide notation for a range of different atoms.\nI can define element, mass number and atomic number.\nThe structure of atoms.\nAtoms are the smallest particles into which\nmatter can easily be broken down.\nAtoms are made up of subatomic particles called protons, electrons and neutrons.\nThe name atom comes from the greek word atomos meaning ‘to cut’ because it was thought to be uncuttable / could not be anything smaller\nUse your knowledge of subatomic particles to complete the table below.\nThe Atomic Number of an element is equal to the number of protons in the nucleus.\nThis will also equal the number of electrons in a neutral atom.\nThe Mass Number of an atom is equal to the number of protons plus the number of neutron in the nucleus.\nSummary of Numbers\nAtomic number = P = E\nMass number = P + N\nN = Mass number - Atomic number\nN = Mass number – P\nAsk your neighbour:\nWhat is an atom?\nWhat is inside an atom?\nWhat is an element?\nNuclide Notation Questions\nWrite nuclide notation for\n1) an oxygen atom containing 8 neutrons\n2) a fluorine atom containing 10 neutrons\n3) an aluminium atom containing 14 neutrons\n4) a sodium atom containing 12 neutrons\n5) a hydrogen atom containing 0 neutrons\n6) a silver atom containing 60 neutrons\n7) a carbon atom containing 6 neutrons\n8) a carbon atom containing 8 neutrons\nLesson 2: Isotopes and Relative Atomic Mass (RAM)\nTo discuss atoms of the same element which have different numbers of neutrons\nI can define 'isotope' and 'relative atomic mass'\nI can explain why RAM is rarely a whole number\nEach of the different elements all have a unique atomic number which is equal to the number of protons present in the nucleus.\nNot all atoms of the same element have the same mass. They have the same number of protons but different numbers of neutrons.\nAtoms with the same number of protons but different numbers of neutrons are called ISOTOPES.\nThere are two types of chlorine atom.\nThey have the same numbers of protons and electrons.\nChlorine – 37 has two more neutrons than Chlorine – 35\nThis means that Chlorine -37 is heavier.\nAtoms with the same atomic number but different mass numbers are called isotopes.\nMost elements are made up of a mixture of isotopes.\nThe relative atomic mass of an element is the average mass of all the isotopes.\nFor this reason, the relative atomic mass is rarely a whole number.\nLesson 3 : Covalent Substances - Structures and Properties\nTo understand covalent bonding and describe how it arises\nI can define covalent bonding\nI can show covalent bonding using diagrams\nI can compare and contrast covalent networks and covalent molecules in terms of structure and properties\nNon metal elements are held together by covalent bonds.\nA covalent bond is a pair of shared electrons.\nIf there are two pairs of shared electrons, this forms a double bond.\nThree pairs gives a triple bond.\nThe nucleus of an atom is positive.\nThe atoms are held in place by the mutual attraction of the positive nuclei for the shared negative electrons.\nIT'S ALL ABOUT SHARING!\nA molecule consists of two or more atoms joined by covalent bonds.\nThe atoms could be the same type (an element) or different (a compound).\nIdentify the two elements which would form a molecule with the same shape as methane.\nCovalent molecules have defined shapes.\nThey are low melting point solids, liquids and gases.\nThey are generally insulators of electricity.\nMany do not dissolve in water.\nExamples include :\nCovalent networks are giant structures\nThey are very high melting point solids.\nThey are generally insulators of electricity.\nThey are insoluble in water.\nExamples include :\nCarbon in the form of diamond.\nWhat about Graphite?\nGraphite exists as a covalent network but only 3 of the 4 valence electrons are bonded. They 4th is delocalised and can carry charge. This explains why graphite can conduct electricity.\ncompare & contrast\nHOMEWORK FOR TUESDAY\nGive definitions for the following terms:\n6. relative atomic mass\n9. mass number\n10. covalent bond.\nLesson 5 : Ionic Bonding\nTo discuss how metal and non metal atoms combine.\nI can give a definition for 'ion'\nI can describe ionic bonding.\nI can describe the structure of ionic substances and give examples\nWhen metals and non-metals react they do so using ionic bonding.\nMetals form ions by losing electron(s).\nNon-metals form ions by gaining electron(s).\nAtoms form ions to achieve a stable (full) outer electron shell.\nIons are charged particles formed by the gain or loss of electrons.\nThe charge on an ion reflects the number of electrons gained or lost.\nE.G. Li indicates that the lithium atom has lost one electron to become a 'one positive' lithium ion.\nIonic compounds exist as crystal lattices.\nThe lattice is held together by the attraction of the positive ions for the negative ions and the negative ions for the positive ions.\nIonic compounds are high melting point solids.\nThey do not conduct in the solid state but conduct well when molten or when dissolved as the ions are free to move.\nMost ionic compounds are soluble in water.\nExamples of ionic compounds include Sodium chloride, magnesium oxide, calcium chloride."", ""About This Chapter\nChemistry Kinetics - Chapter Summary and Learning Objectives\nChemistry kinetics is a subset of physical chemistry that studies how fast or slow chemical reactions take place. In this chapter you'll learn about the different factors that can affect chemical reaction rates, as well as why some responses happen more quickly than others. You'll also become familiar with rate laws and equations. At the end of this chapter, you should be comfortable with these additional topics and theories:\n- The role of temperature in chemical reaction rates\n- Collision theory, including how chemical reactions take place at particular rates\n- Activation energy, or the least amount of force needed for a chemical reaction to start\n- The role of catalysts or chemical substances in chemical reactions\n- Reaction mechanisms and the speed of chemical reactions\n- Equilibrium and the relationship between chemical reactants and products\n|Rate of Chemical Reaction: Modifying Factors||Define the factors that affect the rates of chemical reactions.|\n|Rate Constant and Rate Laws||Interpret and determine rate laws using experimental data.|\n|Rate of Chemical Reaction: Effect of Temperature||Predict the effect of temperature on the rate of a chemical reaction.|\n|Collision Theory||Discuss collision theory as an explanation for chemical reactions and rates.|\n|Activation Energy and Catalysts||Define activation energy and its relationship to reactions energy; explain how a catalyst can change activation energy.|\n|Reaction Mechanisms and the Rate Determining Step||Define reaction mechanism; write a law based on the rate determining step.|\n|Kinetics and Equilibrium||Explain the relationship between rate constants for forward and reverse directions and the equilibrium constant for a designated reaction.|\n1. Rate of a Chemical Reaction: Modifying Factors\nWhy are some reactions much faster than others? Is it possible to change how fast a reaction occurs? In this lesson, you will learn what the rate of a chemical reaction is. You will also discover how factors such as temperature, concentration, surface area, and catalysts impact reaction rates.\n2. Rate Constant and Rate Laws\nLearn what the rate law is and how the rate constant relates to it. Learn what reaction order is and how to determine reaction order when given experimental data containing concentration and reaction rate.\n3. Rate of a Chemical Reaction: Effect of Temperature\nLearn how temperature affects reaction rates. Discover the rate law as well as how the Arrhenius equation relates to the rate law and predicts the effect of temperature on the rate of reaction.\n4. Collision Theory: Definition & Significance\nThis lesson will define and explain collision theory and explore how it is related to chemical reactions and reaction rates. So if you've ever wondered how two particles combine to form a new product, read on!\n5. Activation Energy and Catalysts\nLearn how to define activation energy and how it relates to a reaction's energy. Learn what an activated complex is and where it fits into an activation energy diagram. Discover how a catalyst works to change the activation energy of a reaction and what this means to metabolism in the body.\n6. Reaction Mechanisms and The Rate Determining Step\nThis lesson explores what a reaction mechanism is and how it relates to the speed of a reaction. You'll discover how to pinpoint the rate-determining step and learn how to write a rate law based on the rate-determining step.\n7. Chemical Kinetics, Reaction Rate Constant & Equilibrium Constant\nAre formulas, rate constants, and chemical reactions getting you down? Don't worry, this lesson will explain how reaction rate and equilibrium constants are related to chemical reactions.\nEarning College Credit\nDid you know… We have over 160 college courses that prepare you to earn credit by exam that is accepted by over 1,500 colleges and universities. You can test out of the first two years of college and save thousands off your degree. Anyone can earn credit-by-exam regardless of age or education level.\nTo learn more, visit our Earning Credit Page\nTransferring credit to the school of your choice\nNot sure what college you want to attend yet? Study.com has thousands of articles about every imaginable degree, area of study and career path that can help you find the school that's right for you.\nOther chapters within the Chemistry: High School course\n- Introduction to Chemistry\n- Measurement and Problem Solving\n- Experimental Laboratory Chemistry\n- What Is Matter?\n- Understanding Atomic Structure\n- The Periodic Table of Elements\n- The Representative Elements of the Periodic Table\n- Nuclear Chemistry\n- Bonding for High School Chemistry\n- Phase Changes for Liquids and Solids\n- Gases in Chemistry\n- Solutions in Chemistry\n- Stoichiometry and Chemical Equations\n- Acids, Bases and Reactions\n- Thermodynamics in Chemistry\n- Organic Chemistry Basics\n- Teaching Resources for High School Chemistry""]"	['<urn:uuid:11af987e-db37-4e61-96c5-68c7517e45bb>', '<urn:uuid:bcd726e1-34fd-447b-b2ce-a78db6e3801f>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T20:35:02.547110	12	61	3389
97	I'm trying to learn about historical figures who moved between countries. Can you tell me why this poet Rumi had to leave his homeland and where he went?	Rumi left Persia for Konya when Mongols invaded Persia. At the time, Konya was under the Persian empire. By the time he reached Konya, his mother had died, and he was married with one child, a son.	"['The English used in this article may not be easy for everybody to understand. (October 2011)\nJalal ad-Din Muhammad Rumi (30 September 1207 – 17 December 1273), also named Mawlānā, which means Our Master, is one of the Islamic world\'s greatest poets. He is usually known in the English-speaking world simply as Rumi. He is a Sufi mystic, philosopher and lover of humanity. His followers began a school of mysticism to encourage and celebrate his teachings—the Sufi branch known to many Westerners as the \'Whirling Dervishes\', but whose proper name is the Mevlevi order.\nBackground and early life[change | change source]\nJalâluddîn Rumi was born in 1207 in Balkh Persia in what is today Afghanistan. His father Bahauddin was a famous religious teacher and mystic who received a position at the university in Balkh. When Mongols invaded Persia, Rumi left Persia for Konya, which was then under the Persian empire. By the time he made it to Konya his mother had died, and he was married with one child, a son.\n\'Mevlâna\' (Our Master) had had his early spiritual education under the tutelage of his father Bahauddin and later under his father’s close friend Sayyid Burhaneddin of Balkh. Mevlâna grew both in knowledge and consciousness of God over the years. Eventually Sayyid Burhaneddin told Mevlâna that he was now ready and a mature Sufi and scholar and that the \'next stage\' of his spiritual development would soon occur. And so Sayyid predicted the coming of Shams of Tabriz, the central event and love of Rumi’s life.\nAt the age of thirty-seven Mevlâna met the spiritual vagabond and holy-man, Shams. It was his meeting with this dervish on 15 November, 1244  that completely changed his life. Much has already been written about their relationship. Prior to this encounter Rumi had been an eminent professor of religion and a highly attained mystic; after this he became an inspired poet and a great lover of humanity. Shams’ companionship with Rumi was brief. Despite the fact that each was a perfect mirror for the other, Shams disappeared, not once but twice. The first time, Rumi’s son Sultan Veled searched for him and found him in Damascus. The second disappearance, however, proved to be final, and it is believed that he may have been murdered by certain orthodox Muslim theologians, and other people of Konya who resented his influence over Mevlâna.\nLater life and work[change | change source]\nShams convinced Rumi that at certain moments it was possible to commune directly with God: Rather than believers simply being interpreters of the word of God, they could in fact experience his presence directly.\nThe effect on Rumi was immediate: abandoning his familial and scholarly responsibilities he devoted all his time to learning as much as possible from Shams. However, the mystic came and went without explanation, and vanished permanently in 1248. From then on, Rumi devoted himself to poetry, and primarily to the topic of love. His verses, inspired by Shams—more than 30,000 in all—were collected in a work that he called Diwan-i Shams-i-Tabriz (\'The Divan of Shams of Tabriz\'). Stylistically, Rumi favoured the well established, ghazal, a traditional Persian meter of up to twelve rhyming lines; and to a lesser extent, the Rubai quartrain form.\nRumi seems to have been permanently engaged in a search of divine inspiration and love. He saw and found his \'Muse\' in various human and natural aspects of existence. After the Divan he completed his great masterpiece or magnum opus, the Mathnavi-i Ma\'navi (\'The Mathnavi/Treatise Devoted to the Intrinsic Meaning of All Things\'), which was dictated in its entirety to his student Hissam-ud-din Chelebi over the course of many years.\nAlthough there had been a substantial Islamic mystical tradition prior to Rumi, it is fair to comment that his poetry transformed both thinking and ritual in this regard. For example, the famous centripetal dance of the dervishes - the sema ritual - is said to have its inspiration in Rumi\'s own movements around a pole in his garden as he mourned for his companion Shams. In time, these actions were adapted into Sufic practise by Rumi and then handed down to his favoured disciples who institutionalized them into the activities of the Mevlevi Order that they founded in their Master\'s honour.\nRumi died on December 17, 1273 AD and his Urs (Spiritual anniversary/festival) is held every year on 8th December at his Turbe (shrine) in Konya.\nRumi\'s books[change | change source]\nMevlana\'s main works, all in Persian language, are\n- The Divan i Shams Tabriz , the poems dedicated to Shams;\n- The Mathnavi i Ma\'navi Persian poems and allegorical stories;\n- Fihi ma Fihi his main teaching texts and anecdotes;\n- The Makatib ,his collected letters;\n- The Majales hatgane,a semi-biographical volume also about his teachings.\nRumi today[change | change source]\nIn recent times, Mevlana Rumi remains as fresh and vital as ever. The love poetry of Rumi has found great favour in the West as well as amongst the newer generation of Muslim youth throughout the Muslim world. For Turks, Afghanistanis, Iranians, Tajiks, and other Central Asian Muslims as well as the Muslims of South Asia, Rumi\'s importance is considered to transcend religious, national and ethnic borders. With his works being translated into many languages everywhere, other cultures and people in the world are now also approaching and understanding Rumi more deeply and at times, even trying to follow his teachings and \'Way\'.\nNotes[change | change source]\n- Franklin Lewis, Rumi Past and Present:The life, Times and Poetry of Jalal-ud-din Rumi (2000) pp. 47-49\n- Sizgorich, Tom. ""Jalal al-Din al-Rumi."" World History: Ancient and Medieval Eras. ABC-CLIO, 2013. Web. 4 Dec. 2013.\n- Lewis, p.47\n- Lewis, p.49\n- Lewis, p.90\n- Quoted by Annemarie Schimmel, I am Wind, You are Fire 1997 reprint, pp14, also refer to p.11 and 17, 18, 19\n- Schimmel, p.18\n- S Hossein Nasr, Rumi and the Sufi Tradition 1987, np\n- AJ Arberry, An Account of the Mystics of Islam, 2001, p.141\n- Arberry, np\n- Omer Tarin, in an Obituary Notice of Prof Dr Annemarie Schimmel in The News daily, Islamabad, Pakistan, 7th February 2003\n- See Willliam Chittick, The Spiritual Teachings of Rumi New York, 1983']"	['<urn:uuid:1b7454e6-7a1a-412a-acce-b9782d1526ac>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:35:02.547110	28	37	1037
98	I'm looking to put some plants on my desk at work. Could you tell me what indoor plant can help clean the air, and what are the main plant diseases I should be concerned about in an office environment?	Golden Pothos Vine would be ideal for your desk as it has the ability to purify the air and can survive in unlikely conditions, including dark areas with little water. Regarding diseases, you should be particularly watchful for bacterial infections like Agrobacterium tumefaciens, which creates tumor-like lumps on plants, and viral infections that can cause yellowing of leaves and deformed growth.	"[""Growing plants indoors gives gardeners the convenience of being able to access their plants at any time, while controlling the conditions of the plant's growth. However, there is one drawback in the amount of light that can reach the plants indoors. Some plants can overcome this drawback by growing in conditions where there is a low amount of light. Check out the following plants that will make great indoor options where light is limited.\nLucky bamboo A plant from the African nation of Cameroon goes by many names, but it most often called Lucky bamboo. The name of this plant is a bit deceptive, as it doesn't really belong to the bamboo family. The plant is thought to be a good luck charm, bringing good fortune to those who have one. These plants are often purchased or given to people as gifts to be placed in office buildings, homes, and to sit on desks. Lucky bamboo can thrive well in dark areas and is quite a survivor.\nGolden Pothos Vine Golden Pothos plants have the ability to survive in the unlikeliest of conditions, making it seem as if they aren't plants at all. Dark conditions are no problem for these plants, and neither are areas without a lot of water. This popular plant has a unique beauty that comes from heart-shaped vines and marble patterned stems. This plant also has a bonus ability of purifying the air.\nFerns Existing for millions of years over a vast span on environments, ferns are an impressive plant. Over 12,000 species of these plants are estimated to exist in the world. These plants can come in small to massive sizes and even exist in tropical and cold environments. While not every fern type will be able to last under low light conditions, the majority of them can. Maidenhair and Silver Light ferns are common varieties used indoors.\nParlor palm (Chamaedorea elegans) Out of the palm plants that exist, the Parlor palm is probably the easiest to grow in a low light environment. This plant can last indoors and handle its roots becoming crowded, making it possible to plant a cluster of at least three of the plants together. This plant grows on a single stem that will grow to four feet at the most. After around three years, these plants should be placed into a larger container with more soil while taking care not to damage its brittle roots.\nRubber plant (Ficus elastica) There is very little that can go wrong when raising the Rubber plant indoors. This plant's thick leaves are a beautiful shade of dark green, have quite a glossy beauty, and can grow to be a foot long and half a foot wide, and the plant itself can even reach the ceiling under the right conditions.\nFor a home that doesn't have a lot of light for plants to use, these plants will still be able to thrive and stay looking beautiful. Place a few around your home and watch them grow!"", 'Common Diseases in your Garden – Plant Pathogens\nBeing able to diagnose a disease in a plant can be a very difficult task. Many of the symptoms exhibited by an afflicted plant may lead you to believe that some other problem is present, such as a nutrient deficiency or weather issues.\nIn this blog, we will cover some of the most common diseases in your garden that can affect your garden, as well as tips for preventing and treating them.\nKeeping your plant’s disease free is best prevented by providing your plants with optimum growing conditions at all times. This article is designed to help gardeners better understand what are the common diseases in your garden and how they can affect the plants.\nTypes of Plant Pathogens – Bacteria, Viruses, Fungi, and Nematodes\nBacteria are present wherever life is present. The vast majority of bacteria on plants are beneficial and maintain a symbiotic relationship that is essential to the health of your garden.\nSome bacteria can cause unique defects in plants, making them much easier to identify. For example, Agrobacterium tumefaciens is a well-known but poorly understood pathogenic plant bacteria. It creates tumor-like lumps on plants ranging from tomatoes to roses.\nViruses work on plants in a similar manner to humans by hijacking the genetic material in the nucleus of a cell. They often produce symptoms in plants that mimic other common problems, such as the yellowing of leaves and deformed growth.\nIt is not often that a virus will cause a plant to die, so gardeners will put an unnecessary effort towards fixing the problem, such as adjusting the soil. Viruses in plants can be tricky to diagnose and are one of the most frustrating garden pathogens.\nFungal infections in plants are common and can be difficult to treat. Fungal spores are present in both the air and soil and they will colonize the plant to obtain nutrients from its living cells.\nPlants that suffer from a fungal infection will usually show signs of blotchy brown leaves or contain a powdery white material on the leaves and stems. Each species of fungus has a different effect on plants.\nNematodes are microscopic wormlike organisms that live in the soil. Some nematodes are beneficial and some are bad. They are a common problem in commercial agriculture and often affect root crops, such as carrots and potatoes.\nThey are not limited to root crops and also damage citrus crops and corn, among others. When a plant is infested with nematodes it will show symptoms in virtually all parts of the plant.\nCommon Diseases in Your Garden\nIf you are a gardener, you know the joy of nurturing plants and watching them grow. But with the joy of gardening also comes the challenge of keeping your garden healthy and free from diseases. A wide range of plant pathogens can attack your garden, from fungi and bacteria to viruses and pests.\nPowdery mildew is a fungal disease that affects many plants, including vegetables, fruits, and ornamentals. It appears as a white or grayish coating on leaves, stems, and flowers.\nPowdery mildew thrives in warm and humid conditions and can spread rapidly in crowded plantings. To prevent powdery mildew, ensure good air circulation and avoid watering your plants from above.\nYou can treat powdery mildew with a variety of fungicides, or try a homemade remedy like a milk spray.\nBlossom End Rot:\nBlossom end rot is a common disease that affects tomatoes, peppers, and other fruiting vegetables. It appears as a dark, sunken spot on the bottom of the fruit.\nBlossom end rot is caused by a calcium deficiency in the plant, which can be exacerbated by irregular watering and soil pH imbalances.\nTo prevent blossom end rot, water your plants regularly and mulch around the base to retain moisture. You can also add calcium to the soil with amendments like crushed eggshells or bone meal.\nEarly blight is a fungal disease that affects tomatoes, potatoes, and other plants in the nightshade family. It appears as dark spots on leaves, which can eventually cause the leaves to yellow and drop.\nEarly blight can also infect the fruit, causing it to rot. To prevent early blight, rotate your crops and avoid planting nightshades in the same location year after year. You can also treat early blight with fungicides or try a natural remedy like neem oil.\nClubroot is a disease that affects brassica crops, including cabbage, broccoli, and cauliflower. It appears as stunted growth, yellowing leaves, and malformed roots. Clubroot is caused by a soil-borne pathogen that can persist in the soil for years.\nTo prevent clubroot, rotate your crops and avoid planting brassicas in the same location more than once every four years. You can also treat clubroot with a variety of fungicides and soil amendments.\nDowny mildew is a fungal disease that affects many plants, including cucumbers, melons, and lettuce. It appears as yellow or brown spots on the leaves, which can eventually cause the leaves to die.\nDowny mildew thrives in cool and humid conditions and can spread rapidly in crowded plantings. To prevent downy mildew, ensure good air circulation and avoid watering your plants from above. You can also treat downy mildew with a variety of fungicides.\nKeeping your garden healthy and disease-free requires careful attention and proactive measures. By understanding common plant pathogens and their symptoms, you can take steps to prevent and treat them.\nRegularly monitoring your garden, maintaining good sanitation practices, and choosing disease-resistant plants can go a long way in promoting a thriving garden.\nRemember, early detection and intervention are key to avoiding widespread damage and ensuring a successful growing season. With the right knowledge and approach, you can enjoy a beautiful and bountiful garden year after year.']"	['<urn:uuid:b76783a9-e188-4d9f-bb37-5f97ab28bd9f>', '<urn:uuid:5f952e84-8141-4366-b5e6-e1f06a289f79>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:35:02.547110	39	61	1461
99	origin behind aliphatic and aromatic hydrocarbon classification what do names mean	In the 19th century, aliphatic came from the Greek word 'aleiphar' meaning 'fat' and described hydrocarbons derived from fats or oils. Aromatic hydrocarbons were named after being obtained from pleasant-smelling plant extracts. While these terms are still used today, they now classify compounds based on their structure rather than their origin.	['Hydrocarbon, any of a class of organic chemical compounds composed only of the elements carbon (C) and hydrogen (H). The carbon atoms join together to form the framework of the compound; the hydrogen atoms attach to them in many different configurations. Hydrocarbons are the principal constituents of petroleum and natural gas. They serve as fuels and lubricants as well as raw materials for the production of plastics, fibres, rubbers, solvents, explosives, and industrial chemicals.\nMany hydrocarbons occur in nature. In addition to making up fossil fuels, they are present in trees and plants, as, for example, in the form of pigments called carotenes that occur in carrots and green leaves. More than 98 percent of natural crude rubber is a hydrocarbon polymer, a chainlike molecule consisting of many units linked together. The structures and chemistry of individual hydrocarbons depend in large part on the types of chemical bonds that link together the atoms of their constituent molecules.\nNineteenth-century chemists classified hydrocarbons as either aliphatic or aromatic on the basis of their sources and properties. Aliphatic (from Greek aleiphar, “fat”) described hydrocarbons derived by chemical degradation of fats or oils. Aromatic hydrocarbons constituted a group of related substances obtained by chemical degradation of certain pleasant-smelling plant extracts. The terms aliphatic and aromatic are retained in modern terminology, but the compounds they describe are distinguished on the basis of structure rather than origin.\nAliphatic hydrocarbons are divided into three main groups according to the types of bonds they contain: alkanes, alkenes, and alkynes. Alkanes have only single bonds, alkenes contain a carbon-carbon double bond, and alkynes contain a carbon-carbon triple bond. Aromatic hydrocarbons are those that are significantly more stable than their Lewis structures would suggest; i.e., they possess “special stability.” They are classified as either arenes, which contain a benzene ring as a structural unit, or nonbenzenoid aromatic hydrocarbons, which possess special stability but lack a benzene ring as a structural unit.\nThis classification of hydrocarbons serves as an aid in associating structural features with properties but does not require that a particular substance be assigned to a single class. Indeed, it is common for a molecule to incorporate structural units characteristic of two or more hydrocarbon families. A molecule that contains both a carbon-carbon triple bond and a benzene ring, for example, would exhibit some properties that are characteristic of alkynes and others that are characteristic of arenes.\nAlkanes are described as saturated hydrocarbons, while alkenes, alkynes, and aromatic hydrocarbons are said to be unsaturated.\nAlkanes, hydrocarbons in which all the bonds are single, have molecular formulas that satisfy the general expression CnH2n + 2 (where n is an integer). Carbon is sp3 hybridized (three electron pairs are involved in bonding, forming a tetrahedral complex), and each C—C and C—H bond is a sigma (σ) bond (see chemical bonding). In order of increasing number of carbon atoms, methane (CH4), ethane (C2H6), and propane (C3H8) are the first three members of the series.\nMethane, ethane, and propane are the only alkanes uniquely defined by their molecular formula. For C4H10 two different alkanes satisfy the rules of chemical bonding (namely, that carbon has four bonds and hydrogen has one in neutral molecules). One compound, called n-butane, where the prefix n- represents normal, has its four carbon atoms bonded in a continuous chain. The other, called isobutane, has a branched chain.\nDifferent compounds that have the same molecular formula are called isomers. Isomers that differ in the order in which the atoms are connected are said to have different constitutions and are referred to as constitutional isomers. (An older name is structural isomers.) The compounds n-butane and isobutane are constitutional isomers and are the only ones possible for the formula C4H10. Because isomers are different compounds, they can have different physical and chemical properties. For example, n-butane has a higher boiling point (−0.5 °C [31.1 °F]) than isobutane (−11.7 °C [10.9 °F]).\nThere is no simple arithmetic relationship between the number of carbon atoms in a formula and the number of isomers. Graph theory has been used to calculate the number of constitutionally isomeric alkanes possible for values of n in CnH2n + 2 from 1 through 400. The number of constitutional isomers increases sharply as the number of carbon atoms increases. There is probably no upper limit to the number of carbon atoms possible in hydrocarbons. The alkane CH3(CH2)388CH3, in which 390 carbon atoms are bonded in a continuous chain, has been synthesized as an example of a so-called superlong alkane. Several thousand carbon atoms are joined together in molecules of hydrocarbon polymers such as polyethylene, polypropylene, and polystyrene.\nNumber of possible alkane isomers\n|C3H8 ||1 |\n|C4H10 ||2 |\n|C5H12 ||3 |\n|C6H14 ||5 |\n|C7H16 ||9 |\n|C8H18 ||18 |\n|C9H20 ||35 |\n|C10H22 ||75 |\n|C15H32 ||4,347 |\n|C20H42 ||366,319 |\n|C30H62 ||4,111,846,763 |\nTest Your Knowledge\nNature: Tip of the Iceberg Quiz\nThe need to give each compound a unique name requires a richer variety of terms than is available with descriptive prefixes such as n- and iso-. The naming of organic compounds is facilitated through the use of formal systems of nomenclature. Nomenclature in organic chemistry is of two types: common and systematic. Common names originate in many different ways but share the feature that there is no necessary connection between name and structure. The name that corresponds to a specific structure must simply be memorized, much like learning the name of a person. Systematic names, on the other hand, are keyed directly to molecular structure according to a generally agreed upon set of rules. The most widely used standards for organic nomenclature evolved from suggestions made by a group of chemists assembled for that purpose in Geneva in 1892 and have been revised on a regular basis by the International Union of Pure and Applied Chemistry (IUPAC). The IUPAC rules govern all classes of organic compounds but are ultimately based on alkane names. Compounds in other families are viewed as derived from alkanes by appending functional groups to, or otherwise modifying, the carbon skeleton.\nThe IUPAC rules assign names to unbranched alkanes according to the number of their carbon atoms. Methane, ethane, and propane are retained for CH4, CH3CH3, and CH3CH2CH3, respectively. The n- prefix is not used for unbranched alkanes in systematic IUPAC nomenclature; therefore, CH3CH2CH2CH3 is defined as butane, not n-butane. Beginning with five-carbon chains, the names of unbranched alkanes consist of a Latin or Greek stem corresponding to the number of carbons in the chain followed by the suffix -ane. A group of compounds such as the unbranched alkanes that differ from one another by successive introduction of CH2 groups constitute a homologous series.\nIUPAC names of unbranched alkanes\n|CH4 ||methane || ||CH3(CH2)6CH3 ||octane |\n|CH3CH3 ||ethane || ||CH3(CH2)7CH3 ||nonane |\n|CH3CH2CH3 ||propane || ||CH3(CH2)8CH3 ||decane |\n| CH3CH2CH2CH3 ||butane || ||CH3(CH2)13CH3 ||pentadecane |\n| CH3(CH2)3CH3 ||pentane || ||CH3(CH2)18CH3 ||icosane |\n| CH3(CH2)4CH3 ||hexane || ||CH3(CH2)28CH3 ||triacontane |\n| CH3(CH2)5CH3 ||heptane || ||CH3(CH2)98CH3 ||hectane |\nAlkanes with branched chains are named on the basis of the name of the longest chain of carbon atoms in the molecule, called the parent. The alkane shown has seven carbons in its longest chain and is therefore named as a derivative of heptane, the unbranched alkane that contains seven carbon atoms. The position of the CH3 (methyl) substituent on the seven-carbon chain is specified by a number (3-), called a locant, obtained by successively numbering the carbons in the parent chain starting at the end nearer the branch. The compound is therefore called 3-methylheptane.\nWhen there are two or more identical substituents, replicating prefixes (di-, tri-, tetra-, etc.) are used, along with a separate locant for each substituent. Different substituents, such as ethyl (−CH2CH3) and methyl (−CH3) groups, are cited in alphabetical order. Replicating prefixes are ignored when alphabetizing. In alkanes, numbering begins at the end nearest the substituent that appears first on the chain so that the carbon to which it is attached has as low a number as possible.\nMethyl and ethyl are examples of alkyl groups. An alkyl group is derived from an alkane by deleting one of its hydrogens, thereby leaving a potential point of attachment. Methyl is the only alkyl group derivable from methane and ethyl the only one from ethane. There are two C3H7 and four C4H9 alkyl groups. The IUPAC rules for naming alkanes and alkyl groups cover even very complex structures and are regularly updated. They are unambiguous in the sense that, although a single compound may have more than one correct IUPAC name, there is no possibility that two different compounds will have the same name.']	['<urn:uuid:2ec6c2c0-ee92-4739-9c4c-da8c23add2fc>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:35:02.547110	11	51	1438
100	As someone studying demographic shifts in Pennsylvania, I'm curious about how employment patterns have changed across the state since 2008. What does the economic map of Pennsylvania reveal about job growth and decline?	The economic map of Pennsylvania shows a dramatic divide in job patterns since 2008. The eastern region near Philadelphia shows dark blue areas indicating strong job growth, with lighter blue patches around Pittsburgh, State College, and Harrisburg. However, the majority of the map is dominated by deep red, indicating job decline. This economic pattern has contributed to demographic shifts, particularly in central Pennsylvania's old mining towns where residents feel deserted and young people tend to leave without returning.	['YORK, Pa. (AP) _ Ziggy and Kevin are both retired. They’re both men and they both live in Pennsylvania, but that’s about where the similarities stop.\nJohn Ziegler goes by “Ziggy.” He lives in Glen Rock: Red Pennsylvania. Trump Country.\nHe was born and raised in Baltimore, but he’s glad that he retired away from city life and the ”drugs, crime, violence” that come with city living.\nKevin Mosley lives in West Mifflin, a suburb of Pittsburgh: Blue Pennsylvania.\nHe appreciates that people in his area are aware of issues that are important to him, especially the issue of racism.\nIf you look at a map showing where conservative and liberal voters live in Pennsylvania, Ziggy can be found in the bottom corner of a swath of red that dominates the state. And Kevin? He’s on the edge of one of the two blue dots that bookend the state: Pittsburgh and Philadelphia.\nThat map, and the position of the blue and red on it, has gotten national attention this week.\nThe Pennsylvania Supreme Court issued a decision Monday redrawing Pennsylvania’s congressional districts. And with valuable seats in the U.S. House of Representatives hanging in the balance, maps showing which parts of the state are red and which are blue have captured the attention of political junkies across the nation.\nBut the differences between Blue and Red Pennsylvania run much deeper than politics alone.\nFrom religion to morality to economics, Ziggy and Kevin live in different worlds.\nZiggy and Kevin\nJohn Ziegler voted for Trump, but when it comes to most political matters, “I don’t have super, super strong opinions,” he said.\nIn most elections, he hasn’t even voted.\n”I’m middle class.” No matter who’s in office, “I’m still gonna get screwed,” he said with a laugh.\nThat said, he does have political opinions: He wants Second Amendment rights protected, and he finds property taxes unfair.\nZiegler and his wife, who recently became an American citizen after immigrating from the Philippines, don’t have kids.\nHe wonders why he should have to pay school property taxes to support others’ children. It’s unfair, he thinks.\nAlso unfair: Immigrants who might get amnesty when his wife had to work so hard to obtain her citizenship. That kind of immigration policy is like cutting in line.\nFor Kevin Mosley, discussions of unfairness bring to mind something different: Racism.\nHe’s a retired state trooper and writer who has led events about black history, including a 2015 reenactment of the Selma march.\nHe talks about the history of racism in America, saying that disenfranchisement of Black voters started with slavery, continued through Jim Crow laws and now pops up in issues such as gerrymandering and voter ID laws.\nOppressing a minority’s right to vote: Now that’s unfair.\nIt isn’t just a political issue to him: It’s a matter of right and wrong.\nRight and wrong\nRed Pennsylvania and Blue Pennsylvania see right and wrong in two different ways, according to Melissa Wilde, a professor of sociology at the University of Pennsylvania.\nTo conservatives, cutting in line is one of the most important parts of wrongdoing, Wilde said, citing the work of author and academic Arlie Russell Hochschild.\nWilde says opposition to property taxes are a prime example of that thought.\nMy hard-earned money is being taken from me to help someone else get ahead. That’s unfair: They’re cutting in line.\nMeanwhile, liberals tend to think that helping others is an example of goodness. “We’re all stronger together,” is how Wilde described their thinking.\nThat opinion is echoed by Tony Campolo, an evangelical religious leader and former professor of sociology at Eastern University, located outside Philadelphia. He also used to serve as a spiritual adviser to President Bill Clinton.\nHe said in central Pennsylvania, thinking often focuses on the individual: Take care of yourself, take care of your family.\nYou can hear echoes of this when you talk to Matt Jansen, a controversial conservative political figure in southcentral Pennsylvania currently running for a seat in the state’s House of Representatives.\n”America first, York County first, Pa. first” – that’s what voters in the region connect with, he said.\nCampolo said that religion heavily influences politics in the region, so being on the right side of moral issues, especially abortion, is important to many voters.\nBut run that idea by Blue Pennsylvanian Mosley, and you’ll get a different response: “Abortion is a moral issue, but so is health care. So is gun control.”\nGrowth and decline\nReligion and morality aren’t the only issues that separate Red from Blue in Pennsylvania: Economics do, too.\n”When you map it, which we’ve done … it’s pretty dramatic,” Dr. Theodore Alter, Professor of Agricultural, Environmental and Regional Economics at Penn State.\nIn a forthcoming publication co-authored by Alter, The Center for Economic and Community Development has created another map that shows two Pennsylvanias. But the red and blue on this map indicate job growth and decline.\nIn the east, near Philadelphia, the blue is dark and rich. You can see lighter splashes of it in areas around the state: Pittsburgh, State College, Harrisburg. These are the areas where jobs have grown since 2008.\nBut the color that dominates the map is red. Deep red.\n“Divided” isn’t a term Alter uses to describe his map. Economics is more complicated than that – Blue Pennsylvania is interlinked with Red Pennsylvania.\nAs employment grows in the cities and shrinks in the countryside, Pennsylvania has seen a demographic and political shift, said G. Terry Madonna, a pollster and political science professor at Franklin & Marshall College.\nIn central Pennsylvania, residents in old mining towns feel deserted.\nWhen their kids leave, they don’t come back.\nAnd the Democratic Party – the one of JFK, the one they thought championed them – shifted its focus to the cities.\nIt’s just a part of a well-established national trend, he said. Democrats have become the party of the urban and the diverse; the Republicans the party of the small-town and the White.\nNew District lines\nRed Pennsylvania and Blue Pennsylvania will be divided in new ways come primary day in May.\nMadonna describes the state as “purple-blue” — it can go Blue or Red depending on the candidate and the race. But Democrats have a slight edge statewide.\nEven so, most of the state’s representatives in to the U.S. Congress are currently Republicans.\nThe State Supreme Court says that’s due to Republican gerrymandering: Artfully drawing district lines to maximize one party’s chances to win.\nNow the Democratic majority Supreme Court has ruled that a new, more fair map must be drawn. And after lawmakers missed a deadline to draw and approve their own, the court decided for them.\n“Democrats couldn’t have asked for much more from the new map,” writes the New York Times of the court’s map.\nRepublicans have already challenged the court’s ruling, with the support of President Trump as expressed on Twitter.\nResponse to the new map across the state is just another example of division in between Red and Blue Pennsylvania.\nRed Pennsylvania echoes Trump’s tweet: The original map was “correct” because it was created legally.\nIf Democrats didn’t like the map, they should wait their turn and elect lawmakers that would make a better map. The court’s ruling is like jumping in line.\nMeanwhile, in Blue Pennsylvania, people like Mosley see something else as more unfair: He says the old map was an attempt to ”pack African Americans into districts.”\nHe says the old districts were meant to diminish the voice of Black Americans.\nWhen he saw the maps drawn by Republicans, which included districts that have been likened to an octopus and a moose with antlers, Mosley thought, “How does that even begin to make sense?”\nRed and Blue Pennsylvania have different answers to that question.\nInformation from: York Daily Record, http://www.ydr.com']	['<urn:uuid:ada8e1e3-345c-413e-8d3a-87c0ee54bdca>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:35:02.547110	33	78	1307
