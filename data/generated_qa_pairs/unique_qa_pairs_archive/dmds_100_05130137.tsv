qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	seal material selection parameters downtime reduction compatibility testing artificial lift pumps	Material selection for artificial lift pump seals requires understanding how materials react over time under severe operating conditions. Traditional compatibility testing methods show volume swell and physical property changes don't fully indicate chemical reactions. To reduce downtime and extend seal life, extensive testing is needed beyond standard 70-hour exposure tests. Selecting appropriate seal materials helps extend system life, optimize production efficiency, and reduce operator costs. For proper material selection, manufacturers should consider that even materials passing initial compatibility tests may fail later, as shown by tests where 25% of material-fluid combinations that passed at 30 days failed after 60 days.	['We perform testing by ASTM D to evaluate the compatibility of lubricants with elastomers by measuring the changes in an elastomer immersed in lubricant . ASTM D()e1. Standard Test Method for Elastomer Compatibility of Lubricating Greases and Fluids. standard by ASTM. Buy ASTM D TEST METHOD FOR ELASTOMER COMPATIBILITY OF LUBRICATING GREASES AND FLUIDS from SAI Global.\n|Published (Last):||23 April 2011|\n|PDF File Size:||11.56 Mb|\n|ePub File Size:||12.55 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nPlease first verify your email before subscribing to alerts. In such instances, the repeatability and reproducibility values stated in Section 12 do not apply, and the user and supplier should agree on acceptable limits of precision. ASTM D, for instance, typically incorporates tests with a maximum exposure time of 70 hours, which significantly limits long-term predictive capabilities.\nEven in a thorough compatibility program, performance can only be inferred. Such values may or may not be the same as those of Test D4829 D Similarly, this test method can be used in specifications for lubricating fluids as well.\nNorma ASTM D()e1\nThe ultimate purpose of a chemical compatibility program is to evaluate the ability of an elastomer to function as a seal in its intended application when exposed to a working fluid. We have no amendments or corrections for this standard. To advance the compatibility study, application testing should be performed. But volume swell does not indicate chemical reaction on its own. This standard is also available to be included in Standards Subscriptions.\nInasmuch as the precision values apply only to the elastomers specified in Annex A1 of Test Method Dwhen a lubricant specification requires some other elastomer, the user and supplier of the lubricant should agree to the values of acceptable precision. To ensure full saturation, the testing ran for 2, hours, or 12 weeks.\nNeed more than one copy? Please first log in with a verified email before subscribing to alerts. And in many test recommendations, there are no established guidelines for static or dynamic applications. Please first log in with a verified email before subscribing to alerts. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory limitations prior to use.\nIn most cases, testing reveals little to no information about any actual changes in material properties. Subscription pricing is determined by: The times vary considerably among manufactures, usually ranging from a week to 30 days.\nTo fully evaluate material-fluid compatibility, System Seals developed new methods from various standards, as well as experience from seals in broad applications.\nFluid incompatibility is one of the leading causes of seal failure, so when a multi-million-dollar operation becomes dependent on the performance of a group of seals, it is critical that the testing is sufficient, accurate and complete, and that the operator has confidence that the seal system will perform to its greatest ability.\nYou can download and open this file to your own computer but DRM prevents opening this file on another computer, including a networked server.\nIt is recommended that the agreed upon precision values be stated in the user’s lubricant specification. Although some guidelines can be used to make engineering decisions, these values could be flexible or rigid, depending on the application.\nChemical Compatibility Study Reveals Shortfalls in Standard Testing Methods\nFor specific hazard statements, see Note 6, Note 7, and Note 8. Traditional methods to determine elastomer seal compatibility with specific fluids have for decades produced misleading results, according to a new research from System Seals Laboratories. Still, there are considerable inconsistencies among these tests. Historical Version s – view previous versions of standard. You can download and open this file to your own computer but DRM prevents opening this file on another computer, including a networked server.\nStandards Subscriptions from ANSI d429 a money-saving, multi-user solution for accessing standards. Fluid and seal manufacturers routinely perform their axtm tests, which usually result in compatibility ratings in broad generalizations for entire families of elastomers.\nFor the past several years, System Seals, a research-based seal manufacturer in Cleveland, has undertaken an exhaustive fluid compatibility study that paired more than combinations of seal materials with commonly used industrial fluids.\nTo ensure full fluid saturation into the elastomer, both time and temperature were specified to accelerate ageing.\nThe base test parameters included volume swell, mass and density changes, hardness, tensile strength, elongation, percent modulus, work function area under tensile curve up to 20 percentcompression set and abrasion resistance.\nASTM D Standard Method of Testing Compatibility of Lubricating Grease with Elastomers\nAs the voice of the U. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory x4289 prior to use. Approximately 25 percent of the material-fluid combinations that successfully passed at the day mark, soon began to fail near or after 60 days.\nThe company ran chemical d42289 simulations between 60 to 90 days, and in some cases longer. Two HNBRs, for instance, might demonstrate different property changes depending on the levels of fillers and additives.\nNo other units of measurement are included in this standard. Testing of fluids was not included in Test Method D —95 and earlier versions. Compatibility is evaluated by determining the changes in volume and Durometer A hardness that occur when elastomer coupons are totally immersed in a lubricant sample for 70 h at either or oC or as required by the lubricant specification. asttm\nFor example, a dynamic seal may be required to retain more of its original properties than a static seal. This was found to be a significant gap in historic testing methods. Emulsions at higher temperatures, for example, tended to demulsify, and biodegradable fluids acidified.\nTesting of fluids was not included in Test Method D – 95 and earlier versions. When the preparation of awtm coupons is not feasible, or aastm lubricant specification requires the use of rubber products that do not have a suitable size or configuration for use in preparing coupons for testing Practice Dthis test method can be used as a guide for evaluating compatibility.', 'Artificial Lift: Pumping for Longer\nOver time the internal pressure of oil wells drops and various methods and systems are incorporated to invigorate flow. When reservoir drive does not provide enough pressure to lift fluids to the surface, artificial lift equipment is installed to help lift the fluids. Artificial lift is used on oil wells to enhance oil recovery rates. Techniques such as “gas lift,” where expanding gas helps to lift fluids in a well’s tubing, lower the density of fluid to decrease the bottom hole flow pressure. These artificial lift pumping systems pose many challenges, particularly in sealing.\nThis is one of the reasons why leading oil and gas field service companies are continually evaluating advanced sealing technology; especially due to ever evolving operating conditions, such as elevated pressures, extreme temperatures, increased friction and smaller spaces inside the wellbore for upstream pumping. The conditions within these environments are harsh and very caustic and specifying suitable materials and seal designs capable of withstanding these factors poses real challenges.\nLowering downtime to keep costs down\nDowntime for planned or unplanned maintenance on an artificial lift pump system can be a significant cost for operators. The key to decreasing downtime and extending seal material life is to understand how materials react over time when exposed to severe operating conditions. It is important to match seals to the arduous and difficult environments faced. This extends system life, optimizes production efficiency and ultimately reduces operator costs. In addition, selecting a seal supplier with a broad range of compliant materials has benefits in both vendor reduction and processing performance.\nSour gas or Hydrogen Sulfide (H2S) compatibility is often a sealing requirement for successful artificial lift systems. High levels of H2S may cause severe degradation of polymers typically used in oilfield applications, especially elastomer grades such as Hydrogenated Nitrile Butadiene Rubber (HNBR) and Fluoroelastomer (FKM). The material selection for such conditions is important to ensure the system’s high mechanical integrity and also to minimize potential seal failure and damaging leakage.\nMaterials are vigorously tested before implementation to ensure they meet required standards such as NORSOK M710, ISO 23936-2 and API; the minimum now demanded by the Industry. Controls such as these ensure safety and the governing bodies that help to set standards have made a positive impact. In addition, standards provide customers with an unbiased, accurate way to assess performance.\nTo meet standards, seal materials are batch tested and the parts themselves can undergo months or even years of testing to ensure they are able to withstand changing environments and conditions. This process sometimes needs to be done rapidly however. For instance, Trelleborg Sealing Solutions was able to deliver a seal developed in a compound to specifically address the impact of sour gas in just three weeks and saved a key project from delay.\nPressure-containing components are vulnerable to sulfide stress cracking (SSC) when exposed to sour environments and can lead to various modes of material failure depending on additional environmental conditions. Material damage can occur in a relatively short timeframe and can lead to catastrophic failure of equipment.\nIn such scenarios, companies should be aware of this potential occurrence and request specific testing of potential solutions; this may take years before actual implementation. Choosing a supplier with this level of technical support combined with sealing solutions meeting industry standards ensures a customer receives a very effective and reliable product.\nEstablish a supplier relationship\nLong term relationships are valuable as in many applications operating parameters of systems change, such as, pressure and/or temperature levels. This should be monitored to ensure the sealing solutions will still operate effectively within amended operational constraints.\nIn conclusion, selecting the correct material to construct sealing solutions for artificial lift systems and matching them to the environment in the wellbore will help ensure a long production life in wells. It’s vital to choose a solution that meets industry standards with compounds and seal designs that have been proven with extensive testing, solid engineering expertise and offered with local technical support globally.']	['<urn:uuid:82125a5f-c108-428b-b6de-bdc5a81f6cd8>', '<urn:uuid:24fb3bf2-838e-488a-bba3-1119d87c21a0>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	11	100	1684
2	which came first suffrage amendment or prohibition	The Eighteenth Amendment (Prohibition) was ratified first in 1919, while the Nineteenth Amendment (women's suffrage) was ratified later in 1920.	"['The Eighteenth Amendment to the Constitution was ratified on this day in 1919, marking the end of one era and the beginning of another. Outlawing the “manufacture, sale and transportation of intoxicating liquors,” the amendment represented a victory for those in the temperance movement who believed many of society’s evils could be traced to the prevalence of alcoholic beverages.\nThere were more saloons than schools, hospitals, libraries parks or churches. Beer drinking was so popular that by 1910, the annual per capita consumption had risen to 21 gallons, with those prone to violence tending to drink more than their fair share of that average. So how was the ban on intoxicating beverages passed by Congress and ratified by the states so rapidly? World War I played a large role. The majority of brewers and a great percentage of beer drinkers were of German descent, and even though the companies might now be run by second and third generation Americans, prohibition supporters gained a lot of ground by arguing that it was unpatriotic to support the beer industry.\nBut while the old saloons closed and the word virtually disappeared from our language, inebriation did not. It just became more dangerous.\nThe saloons didn’t close right away, of course. The Amendment set a one year deadline before the ban would take effect, and those with stores of wine and spirits began hiding, hoarding and acquiring more. Supporters of the ban, which had temporarily been in effect during World War I, believed Americans would see the benefits of an alcohol-free society and soon there would be little need to enforce the law. They estimated enforcement costs at five million dollars a year.\nThey were wrong. By 1923, Congress was being asked for over $28 million for enforcement, and estimates ran as high as $300 million a few years later.\nAmericans did not willingly give up their intoxicating beverages. They hid them. And they changed them.\nAlcohol had to be smuggled in or made in secret. Those who risked breaking the law to import the contraband wanted to maximize their profits. So they concentrated on distilled spirits, with a much greater percentage of alcohol. Profits were further extended by cutting the booze with other substances to make it go farther. Water would be obvious and noticed by patrons, so they often used something to give the drink a little kick, like industrial methanol or even embalming fluid.\nCreative bartenders mixed the bad booze with fruit juice and sugary syrups to mask the taste.\nBut the poisonous affects were not as easy to hide. By 1926, 750 people in New York alone had been killed by toxic cocktails, and thousands more paralyzed or made blind from the concoctions downed in speakeasies and other underground drinking establishments.\nAnd then there was the problem of the mobs controlling the supply. Instead of reducing crime, the ban on alcohol created an ideal incubator for the birth of organized crime. The ban was repealed by the 21st Amendment in 1933. It was called “the failed experiment.”\nMany people draw parallels between the 1920s prohibition on alcohol and today’s current ban on illegal drugs such as marajuana. There is no doubt that today’s illegal drug industry supports organized crime now just as the illegal bootlegging operations supported crime bosses in the 1920s. Would repealing the ban reduce the strength of the drug lords enough to outweigh the harm that might be done from easier access to such drugs? Hard to say without a crystal ball.\nI for one am glad I was born after 1933. I’d hate to live in a world without champagne and margaritas.\nNewspaper image courtesy of the Anti-Saloon League Museuem, http://www.wpl.lib.oh.us/AntiSaloon/', ""An Overview of the 19th AmendmentIn 1848 women and men met in Seneca Falls, New York to advance the cause for women’s rights. The convention, organized by Elizabeth Cady Stanton, Lucretia Mott, Martha Wright, Mary Ann M'Clintock, and Jane Hunt marked the beginning of a formal women’s suffrage movement. The men and women of the movement made speeches and petitioned Congress, pressuring government officials to recognize the woman’s right to vote. Stanton, Mott, and suffrage advocate Susan B. Anthony did not live to see women get the right to vote. Instead, they paved the way for future suffragists like Alice Paul, Ida B. Wells, and Mabel Ping-Hua Lee.\nThe women leading the women’s suffrage movement were not always unified. Some suffragists thought only white women should exercise their right to vote. Others like Charlotte Forten Grimke, Mary Ann Shadd Cary, and Mary Church Terrell knew women of color also had a right to participate in electing government officials.\nThe first constitutional amendment to secure votes for women was introduced to congress in 1878. It failed. By 1919, suffragists get another amendment introduced to congress that would secure women's right to vote. The 19th Amendment passed both the House and Senate. The states ratified the 19th Amendment in 1920, officially recognizing women’s right to vote.\nWhile many women were able to head to the polls, the amendment did not give voting rights to all women. Women of color, immigrants, and lower income women were often deterred from voting by laws and social pressure. For example, Native American women were not considered US citizens until 1924 and were not permitted to vote. Women who were convicted of a crime were also unable to vote, even if they completed their sentence.\nAfter the ratification of the 19th Amendment in 1920, suffragists like Alice Paul knew that their work was not finished. While the government recognized women’s right to vote, many women still faced discrimination. Paul and other members of the National Woman’s Party drafted the Equal Rights Amendment. If ratified, the amendment would guarantee equal rights to all people regardless of their gender. The Equal Rights Amendment was ratified by both houses of Congress in the 1970s but failed to get adequate support from the states. It has not yet been ratified to the Constitution.\nWomen’s rights advocates did make progress in passing other legislation after 1920. Congress passed the Equal Pay Act in 1963, making it illegal to pay a woman less for doing the same job as a man. A year later, Congress passed the Civil Rights Act of 1964. This act is often associated with the Civil Rights Movement as it prohibits employers from discriminating against an individual based on their race. The act also states that employers cannot discriminate against someone based on their gender. Before the Civil Rights Act of 1964, employers could choose not to hire a woman because of her gender.\nA Brief Timeline of the 19th Amendment\n- July 1848- The first Woman’s Rights Convention takes place in Seneca Falls. Suffrage is part of the resulting Declaration of Sentiments\n- February 3, 1870- The 15th Amendment is ratified, guaranteeing the right to vote regardless of race, color, or previous condition of servitude.\n- January 1878- A women’s suffrage amendment is introduced to Senate. It fails in 1887.\n- 1914- Another women’s suffrage amendment is introduced. It also fails.\n- 1918- The 19th Amendment passes the House and then fails in the Senate by 2 votes.\n- February 10, 1919- The 19th Amendment fails in the Senate by 1 vote.\n- May 21, 1919- The 19th Amendment passes in the House.\n- June 4, 1919- The 19th Amendment passes in the Senate.\n- June 10, 1919- Wisconsin and Michigan are the first states to ratify the 19th Amendment.\n- August 18, 1920- Tennessee is the 36th State to ratify the 19th Amendment law, passing the three-fourths threshold requirement for the amendment to become law.\n- August 26, 1920- The 19th Amendment is adopted as part of the US Constitution.\nVoting Rights After the 19th Amendment\nThe 19th Amendment did not guarantee that all women and men in the United States could vote. Securing this essential right has been an long struggle, that for some, continues on to this day.\n1924, Indian Citizenship Act- Native Americans deemed US citizens, but states continue to decide who votes. Many continue to disenfranchise Native Americans.\n1943, Magnuson Act- Chinese in America granted the right to become citizens, and therefore to vote (the Chinese Exclusion Act of 1882 previously prevented this)\n1962- Utah is the last state to enfranchise Native Americans.\n1965, Voting Rights Act- African Americans and Native Americans continued to face exclusion from voting through mechanisms like poll taxes, literacy tests, and intimidation. The Voting Rights Act of 1965 eliminated many of these.\nLearn more about the 19th Amendment\nLast updated: October 8, 2019""]"	['<urn:uuid:04e74e32-584c-4951-b6de-2e4b5d72a3b0>', '<urn:uuid:45aefbf0-3c4c-44f4-9dba-058cd51e7e5e>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T01:37:09.743768	7	20	1430
3	What's happening with tuna populations worldwide?	Tuna populations have been declining globally, though there are some signs of recovery. While traditional overfishing, pirate fishing, and advanced fishing technology have depleted many tuna populations, four commercially fished tuna species have shown improvement thanks to enforced fishing quotas and combat against illegal fishing. For instance, Atlantic bluefin tuna has moved from Endangered to Least Concern, and Southern bluefin tuna from Critically Endangered to Endangered. However, many regional tuna stocks remain severely depleted, with some populations like the Pacific bluefin tuna at less than 5% of its original biomass.	"['Change Your Tuna\nTuna are in trouble. These are some of the world’s favourite ﬁsh, and the staple protein of millions, but tuna populations are declining globally.\n→ Why is there a problem? Click for more info\nEvery year there are more boats chasing fewer tuna. There simply aren\'t enough fish to sustain the world\'s voracious appetite for tuna. Rampant overfishing and pirates stealing tuna are making these once abundant, ocean giants harder and harder to find.\nThe three blue fin tuna species as well as big eye tuna are those that are in the most immediate danger.\nAnd it\'s getting worse. Advances in technology mean larger ships - floating factories - are now able to take as much tuna in two days as whole countries can take in a year. Increasing practices of tuna ranching are further aggravating the crisis.\nPirate fishing is also rampant in high value tuna fisheries, literally stealing tuna from the plates of some of the poorest people in the world. But even the legal tuna fisheries are partaking in the robbery. The so called ""sweetheart deals"" that fishing nations and rich multinational corporations negotiate with coastal states for access to fish tuna in their waters are incredibly unfair. Only around 5% of the value of the tuna is given to the resource owners, often denying coastal communities much-needed employment and neglecting the responsibility to fish responsibly.\nHow can we stop the tuna from disappearing? Insist on pole-and-line caught or FAD-free, sustainably fished brands.\n→ Know what you\'re eating. Click for more info on different tuna\nThe tuna that’s most likely to end up in supermarket cans at the moment is skipjack. While skipjack is not yet overfished, if fishing continues at current rates it won\'t be able to sustain itself. What\'s more, the methods used to net skipjack all too often catch young yellowfin and bigeye tuna, threatening these species further.\nYellowfin, a much more commercially valuable species, makes up 28% of the world\'s catch. The majestic bluefin only represents just under 1% of the landed volume of tuna, but its value is astronomical. In January 2013 a bluefin tuna sold for a record 155.4 million yen (USD$1.7 million) at a Tokyo auction – nearly three times the previous record set in 2012.\nJust because there’s more skipjack than albacore or yellowfin, that doesn’t mean we should continue using the harmful fishing practices that take too much and leave too little for communities and smaller fishing fleets.\nHow It\'s Caught\n“Killed alongside the skipjack tuna that ﬁnds itself in your tin is almost the entire cast list of \'Finding Nemo’.”\n→ Which fishing practices are the best? Click to find out\nOne of the most important things tuna brands can do is stop using fish aggregating devices (FADs) with large \'purse seine\' nets. FADs act like giant fish magnets. They draw in marine life to be scooped up in a purse seine net. This fishing technique threatens tuna stocks because of the large incidental catch of juvenile tuna from species that are at risk. This level of bycatch, that also often includes sharks, rays, and even turtles, can be ten times higher than fishing without FADs.\nPole-and-line-caught tuna is the most sustainable way of fishing because it is highly selective, meaning that bycatch is kept to a minimum.\nHappy Oceans, Happy Tuna!\nTuna companies and retailers must stop the use of FADs with purse seines and source from more sustainable sources such as pole and line or FAD-free.\n→ Whose examples can we follow? Click to find out\nIn Australia and the in the UK, every major brand and supermarket has pledged to end destructive tuna fishing practices. In the UK for example, John West has committed to end their use of F.A.D.s by 2016. It is time South Africa joined the likes of UK, Australia and New Zealand and pledged to only source sustainably caught canned tuna - you can help us make this happen!\nTuna in South Africa and What You Can Do', 'Four commercially fished tuna species are on the path to recovery thanks to the enforcement of regional fishing quotas over the last decade, according to an update of the IUCN Red List of Threatened Species released on Saturday, September 4, 2021 at the IUCN World Conservation Congress in Marseille.\nHowever, the recovery comes amid growing pressures on marine species, with 37% of the world’s sharks and rays now threatened with extinction primarily due to overfishing, compounded by loss and degradation of habitat and climate change.\nThe IUCN Red List now includes 138,374 species of which 38,543 are threatened with extinction.\n“Today’s IUCN Red List update is a powerful sign that, despite increasing pressures on our oceans, species can recover if states truly commit to sustainable practices,” said Dr Bruno Oberle, IUCN Director General.\n“States and others now gathered at the IUCN World Conservation Congress in Marseille must seize the opportunity to boost ambition on biodiversity conservation, and work towards binding targets based on sound scientific data. These Red List assessments demonstrate just how closely our lives and livelihoods are intertwined with biodiversity,” Oberle added.\nIn the Red List update, the seven most commercially fished tuna species were reassessed. Four of them showed signs of recovery thanks to countries enforcing more sustainable fishing quotas and successfully combatting illegal fishing. The Atlantic bluefin tuna (Thunnus thynnus) moved from Endangered to Least Concern while the Southern bluefin tuna (Thunnus maccoyii) moved from Critically Endangered to Endangered. The albacore (Thunnus alalunga) and yellowfin tunas (Thunnus albacares) both moved from Near Threatened to Least Concern.\nDespite global improvement at the species level, many regional tuna stocks remain severely depleted. For example, while the larger, eastern population of Atlantic bluefin tuna, which originates in the Mediterranean, has increased by at least 22% over the last four decades, the species’ smaller native western Atlantic population, which spawns in the Gulf of Mexico, has declined by more than half in the same period. The yellowfin tuna meanwhile continues to be overfished in the Indian Ocean.\n“These Red List assessments are proof that sustainable fisheries approaches work, with enormous long-term benefits for livelihoods and biodiversity. We need to continue enforcing sustainable fishing quotas and cracking down on illegal fishing,” said Dr Bruce B. Collette, Chair of the IUCN SSC Tuna and Billfish Specialist Group. “Tuna species migrate across thousands of kilometres, so coordinating their management globally is also key.”\nThe Pacific bluefin tuna (Thunnus orientalis) moved from Vulnerable to Near Threatened in this update due to the availability of newer stock assessment data and models. This species remains severely depleted at less than 5% of its original biomass. Other tuna species reassessed for this Red List update include the bigeye tuna (Thunnus obesus) which remains Vulnerable, and the skipjack tuna (Katsuwonus pelamis) which remains Least Concern.\nSaturday’s IUCN Red List update also includes a comprehensive reassessment of the world’s shark and ray species, which reveals 37% are now threatened with extinction, demonstrating that effective management measures are lacking throughout much of the world’s oceans. All the threatened shark and ray species are overfished, with 31% further affected by loss and degradation of habitat and 10% affected by climate change.\nKomodo dragon threatened by future impacts of climate change\nThe world’s largest living lizard, the Komodo dragon (Varanus komodoensis), has moved from Vulnerable to Endangered on the IUCN Red List. The species, which is endemic to Indonesia and occurs only in the World Heritage-listed Komodo National Park and neighbouring Flores, is increasingly threatened by the impacts of climate change. Rising global temperature and subsequent sea levels are expected to reduce the Komodo dragon’s suitable habitat by at least 30% in the next 45 years.\nIn addition, while the subpopulation in Komodo National Park is currently stable and well protected, Komodo dragons outside protected areas in Flores are also threatened by significant habitat loss due to ongoing human activities.']"	['<urn:uuid:0e88acac-5c30-4272-8eea-7c52ca8d3967>', '<urn:uuid:f160bbe8-1f11-442e-b818-1063d4cd5e35>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-13T01:37:09.743768	6	90	1319
4	diurnal nocturnal habits colonial tunicate pyrosoma vs lightning bug comparison	Pyrosoma atlanticum follows a daily cycle where it rises in the water column during dark hours and sinks lower during the day, ranging from near surface to 800-2,500 feet deep. In contrast, most fireflies are crepuscular (active at dusk and dawn), though some species are purely nocturnal (nighttime only) or diurnal (daytime only), with the daytime species typically not flashing due to brightness.	"['Posted on May 02 2017\nYou never know what you’ll find at the beach. Just about anything man-made can wash up, and you’ll always find the local shells and kelp. When the currents sometimes wander a little off course and the winds take odd turns, unexpected items can roll in with the waves. Recently, Pacific Coast beachwalkers from Oregon north to British Columbia have been noticing the arrival of large numbers of bumpy, gelatinous tubes on their shores, most about the length and diameter of your finger. Pinkish to yellowish in color and translucent, they seem to be made of many little cells. In fact, each little spot inside the mass is an individual—a zooid—contributing to what is called a colonial tunicate. Tunicates also come in a larger, singular form, and are often found attached to rocks, docks, or boats. Commonly called “sea squirts,” they are fairly simple animals that draw water into their sac-shaped bodies and filter out plankton. If you poke them when they are out of water, you will see that they do indeed squirt. Because they have a nerve stem called a notochord, they are taxonomically in the Phylum Chordata, the same one you are in (unless you are a spineless wimp).\nThe oddities showing up on Pacific Northwest beaches are not these larger individuals, but a colonial variety called Pyrosoma atlanticum. Hundreds to thousands of tiny individuals only a few millimeters across make up the rigid, rubbery structure, which is hollow inside, and closed at one end. Each member of the colony draws in water from the outside surface, beating it with hair-like cilia and driving it through a mucus net which collects plankton to eat. The water then moves to the interior of the tube and is pushed out the open end, which simultaneously generates movement through the ocean in a kind of slow-motion jet. Their movement isn’t directed by a central brain in a head of any kind, but overall, the colony has something directing it to rise in the water column during dark hours, and sink lower in the day, ranging from near surface levels down to 800 to 2,500 feet deep in a daily cycle. These small tubular colonial tunicates hang around in large groups, usually far out in the open ocean, hence they are called colonial pelagic tunicates. They don’t come ashore on purpose, but are carried there by the forces of wind and current. Once they turn up on the sand, they are most likely dead, and no longer exhibit one of their striking qualities—their ability to bioluminesce. And that’s where they get their name: pyrosoma means “fire body” in Greek, a reference to the glowing light they can give off. It is thought that this capability helps pyrosomes attract zooplankton at night.\nSource: Flickr: Biodiversity Heritage Library (Public Domain)\nMost of the recently washed-up Pyrosoma antlanticum range to about three inches in length, but they can be more than 20 inches long. They are found in a broad swath of the world’s oceans, generally from 50 degrees north latitude, to 50 degrees south, from temperate to tropical zones. Fellow pyrosome species far surpass them in size: Pyrosoma spinosum, a tropical species, can reach a length of 60 feet, with a tube aperture that may be over six feet across! Combining such a huge and bizarrely shaped creature with the bright neon, blue-green glow of bioluminescence would make for a decidedly otherworldly encounter. Described as soft as a feather boa, these large pyrosomes react to touch by glowing. Unlike most other bioluminescent marine animals, pyrosomes glow in response not only to physical stimulation, but as a reaction to light itself. Individual members of a colony will incite one another to glow, and the incentive to light will also pass from one colony to the next. The brightness and persistence of their output is notably strong compared to other luminous denizens of the sea.\nMaking the oddness of pyrosomes that much odder is that all the individual zooids are clones of one another. If part of the organism is damaged or eaten, those remaining create copies of themselves to fill in, or simply to increase the size of the the colony. In this way the floating tube has a theoretically immortal chain of existence. What must life be like, gliding and munching slowly through the ocean, defenseless, glowing, and sharing a body structure with thousands of comarades who are just like you? The variety of nature’s designs for existence are truly staggering. And to think that for most of us, only the accidents of shifting winds and currents bring these creatures to our land-based awareness. What else is out there?\nWould you like to get a wall map that includes the coastline where the pyrosomes are washing up?', 'How to Identify and ""Call"" Lightning Bugs or Fireflies\nAn ""Enlightening"" Conversation\nAs summer rolls around it\'s time again for the emergence of fireflies or lightning bugs. These benign insect ambassadors have been many children\'s introduction to the world of beetles and friendly insects. For today\'s post I will introduce you to the three most common groups of fireflies in the Eastern US and how you can use LED flash patterns to call the most common species to you. It makes a great lesson plan for evening programs or just something fun to do with the kids on a summer night.\nFireflies, or lightening bugs, are beetles (order Coleoptera). Unlike their cousins, that have hard-bodied elytra (ee-light-tra) or wing coverings. Their bodies and wings are relatively soft and they have leathery wing coverings. The are usually about 2 cm long, and blackish with reddish or yellow spots on their head covering. The head covering is also called the pronotum (pro-know-tum). Around the world, there are over 2,000 species of fireflies, and most live in tropical, moist and damp areas, in part because of their soft bodies. Lightning bugs are called this because their abdomens light up using a chemical process called bioluminescence.\nIn the Eastern US there are three common families of fireflies that we see, Photinus (Foe-tine-us), Pyractomena (Pie-rack-toe-mean-A), and Photuris (Foe-tur-is). You can distinguish them apart by looking at their head covering and their wings.\nPhotinus are the most common type of firefly in the Eastern US. They are about 1/2"" long and have a big black spot on their head, with yellowish margins around the head plate. Their wing edges are also yellowish.\nPhotorus is the largest of the three species, coming in at about 1"" long. They have a yellow arc across their head plate, and in the middle is a red arch that is split with a black stripe (almost like a yellow and red rainbow). Their wings have a brownish line down the middle and brownish color around the outside. Their legs are also very long.\nPyractomena are a bit harder to distinguish, because they are the same size as the Photinus, about 1/2"" long, but they have a much darker head plate, with black edges and very little red. There is a also a big black and flared stripe in the middle of the head plate. Their wing edges are also yellowish.\nBoth females and males can fly, which is unusual for creatures that use bioluminescence. Often male glowy-glowy creatures are the ones that move around a lot. Most fireflies are also crepuscular (cre-pus-que-lar), meaning that they can be seen mostly at dusk and dawn. However, there are a few purely nocturnal (nigthtime only) and diurnal (daytime flying only) species of firefly. The day-only species typically don\'t flash, it\'s too bright out.\nMost fireflies can glow throughout their lives. However, the glowing of the larvae is more of a warning to predators that they are distasteful, while the glowing of the adults is to advertise for a mate.\nAfter a female mates, she then lay their eggs in moist leaf litter. When the eggs hatch, the larvae then burrow down into the soil and begin to eat other firefly larvae, slugs, snails, and soft bodied invertebrates that live underground. The larvae of some species have a special set of mandibles with chewing mouth parts. They can inject a type of anesthetic substance through hollow fang-like ducts in their mandibles into their prey. This immobilizes the prey and helps digest them. As the larvae get larger they become more terrestrial, and move above ground. These larvae are sometimes called ""glow worms"" because they glow with bioluminescence. In several species of firefly the females are larviform, which means that they retain the characteristics of the juvenile larvae (they look like soft, squishy grubs) but their rear lights up, and they have large compound eyes so they can search for males.\nWhat do adult fireflies eat?\nMost adults prefer a vegetarian diet of pollen (protein) and nectar (carbohydrates), but there are a few species that eat insects. In one or two cases the adults don\'t even have mouth parts. In the Eastern US, most of our species, as adults, prefer nectar and pollen. Fireflies are not top on the dinner menu of nocturnal predators. The adults and larvae are foul-tasting because of the chemicals they produce in their bodies. If you want to really geek-out you can read a paper all about the steroidal pyrones, or chemicals found in fireflies which are similar to those found in poisonous toads, in the paper by Eisner et. al. in the Proceedings of the National Academy of Sciences.\nThere are several ways to distinguish males from females too. As mentioned earlier, some females remain larviform. As a general rule, you can also look for behavior. Males usually fly and flash, while females usually remain on vegetation (sitting still) and flashing. Height preference, as to where they sit, varies by species. The other way to distinguish males from females deals with which part of their rumps light up. It\'s really hard to tell, so for beginners I\'m going to stop there.\nSo, how exactly do the fireflies make their abdomens glow? If you\'ve ever caught them, then you know they are not flaming balls of fire and they\'re not hot to the touch. The type of bioluminescence they use is chemical, and it\'s a cold process, which means that it doesn\'t produce heat from the chemical reaction.\nThe enzyme that they produce is called luciferase, and it acts on luciferin. This happens with a cocktail of the luciferase, luciferin, ions and ATP for energy, along with two parts oxygen. The luciferin is produced by specialized cells, that then convert it into the enzyme luciferase. The luciferase reacts with oxygen to form an inert (non-heat producing) molecule called oxyluciferin. Want to learn more about this process? You can geek-out even more and read the publication, ""The Colors of Firefly Bioluminesence: Enzyme Configuration and Species Specificity"" in the Proceedings of the National Academy of Sciences (warning, the article is very technical). It\'s fascinating how each species has its own unique cocktail of butt-glowing-enzymes.\nNow to the flashy-part. Along with their own unique butt-glowing cocktail of enzymes they also have their own unique flash pattern. These flash patterns can be broken down by looking at the following features;\n- Time of night they are active\n- Temperatures (flash patterns also vary by outside temperature)\n- Length of flash\n- Number of flashes\n- Time between flashes\n- Flight pattern (sitting or flying, and the patterns in which they fly)\nOf the three most common species I mentioned (for the Eastern US), here are their colors:\n- Photinus: Yellowish-green flash\n- Photuris: Dark-green flash\n- Pyractomena: yellowish to orangish flash.\nBy changing all of the variables listed above, several species of firefly can co-exist in one area, and eliminates confusion over who belongs to what species. However, the Photouris species are the femme fatals of the firefly world, because the females have figured out how to mimic the flash signal of the males of other species (in particular Photinus and Pyractomena). The females will ""flash"" the males and call them closer, whereupon they will pounce upon the unsuspecting male and eat them! This almost sounds like something out of Grimm\'s Fairy Tales, or lessons learned from the praying mantis mating manual!\nA great resource for learning flash patterns is from the Museum of Science in Boston, MA. They have a citizen science program called ""Firefly Watch"" which allows you to record and report your firefly sightings. Their website has a nice variety of information, including the ""Firefly Flash Chart"" which give you information about the flash pattern of various species. The chart is somewhat hard to read, and it\'s really more for the sciency types who can interpret graphs easily, but it\'s useful. Their information page gives you more background on how to use the chart.\nDr. Firefly\'s page also has a great graphic to help you visualize the flight patterns in one nice presentation. There\'s also more from Terry Lynch (Dr. Firefly) here: https://www.firefly.byteland.org/firefly_faq.html.\nAs they fly, male flash patterns can range from a quick ""blip-count-one-two-blip-count-one-two"" to something resembling a ""J stroke blip-count one-two-J stroke blip-count one-two"" and so forth. Firefly patterns can even resemble Morse code, with three quick dashes of light and then three seconds of nothing, followed by three more quick dashes of light. It all depends on the species. Females also vary their patterns in concert with the males, at very specific time intervals. You can observe this at night if you focus on one single male or one single female. It makes for a great science project for kids.\nHOW TO CALL THE EASTERN FIREFLY OR BIG DIPPER FIRELY\nIf you know the flash pattern of a firefly, there\'s no reason you can\'t try ""calling"" them. My own favorite is to try to call in the Eastern Firely or Photinus pyralis (one of the most common in this area).\nYou can use almost any common LED flashlight, but if you need to buy them in bulk you can purchase them online. (Here\'s one that I like from Amazon, the Mini-LED flashlight.) They run about $4 for 10. As a rule of thumb I ask participants in this activity to put red cellophane over their flashlights, so as not to ""night-blind"" everyone, and to limit disruption during the activity. Remember, insects and animals can be very sensitive to bright white light, interfering with many important behavior patterns.\nI start by handing out the LED lights, and going over a few precautions. If you\'re working with children or younger students, then you should remind them that chasing down a firefly doesn\'t work. You have to be patient and wait for it to come to you (in the case of calling males), or if you\'re calling females you need to move carefully and slowly so you don\'t step on them! I find the biggest hurdle in this process is over-anxious ""flashers."" I will allow them to hold fireflies, but I\'ll go over how to be gentle if they do catch them to look at them. And we\'re always ""catching and releasing.""\nNext, remember that timing is crucial. You have to practice your flash pattern and counting. If you\'re too fast or slow, then it\'s like you\'re ""saying"" something foreign to the firefly. I usually have the entire group stand in a circle, with the LED lights, and then practice ""syncing"" up counting and flashing.\nHere are the patterns I use:\nIf you want to pretend to be a male, calling a female use this flash pattern:\n- Hold your light about 1 1/2 feet over the ground (you can use meter sticks if it helps)\n- With the LED light create the shape of a J, by starting at the small hook end of the J and sweeping up about 6-8"".\n- Restart the next flash approximately 2 seconds later (one Mississippi, two Mississippi) from the ""bottom"" of the J again (1 1/2 feet off the ground). These fireflies usually hover around the same area so stay close to where you started.\n- You can ""hover"" and move forward in small increments if you start to see a female respond. She should be flashing from near the ground or in the leaves, with the pattern listed below.\nIf you want to pretend to be a female, calling a male use this flash pattern:\n- Sitting on the ground, hold your light in the grass or about 4-5"" off the ground.\n- After seeing a male flight pattern (the one that looks like a ""J"") wait two seconds till after he finishes his flash, then give one short flash.\n- Repeat this, ""calling back"" procedure until the male comes in to you.\nFLASHING PATTERN GAME: You can also modify the ""syncing"" activity by writing flashing patterns, in pairs on note cards. For example, you could write 0X0X or 00X0 as a flash pattern, with X meaning one rest and 0 meaning a flash. You can even change this up and add patterns like J\'s or longer pauses. Randomly give each person an index card with a pattern, and then have them spread out and flash their pattern and see if they can find their partner. If you don\'t have LED lights you can do this with hands, in the light. With open hands being a flash and a fist being a pause.\nWhen you conduct a firefly calling activity, it\'s best to have everyone spread out around the yard before they begin calling. Be sure that you can keep track of everyone, so that there are no accidents or lost visitors. As a general rule, it\'s always good to capture-and-release your fireflies. Be sure that if you are ""capturing"" them, that you use a jar with holes and that they are not overly hot or housed for too long. Fireflies are fun and fascinating creatures, with much to teach us about patience, and new ways of communicating!\nHere are a few books that you might be able to use to supplement a firefly program:\n- Fireflies in the Night (Let\'s Read and Find Out Science 1)\n- Next Time You See a Firefly (Morgan)\n- Incredible Fireflies (Incredible World of Insects)\n- The Very Lonely Firefly (Penguin, Young Readers L2)']"	['<urn:uuid:b6ae7bb7-8f93-4074-9fcf-2941ede61f69>', '<urn:uuid:b5aa978c-3917-4478-a0a2-9c3bdf85dd21>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T01:37:09.743768	10	63	3036
5	cost reduction efficiency restaurant management software inventory tracking billing features	Restaurant management software reduces costs through comprehensive inventory management and billing features. It tracks food costs, monitors stock levels, and helps prevent waste by following FIFO principles and monitoring expiration dates. The system includes billing and invoicing capabilities for managing financial transactions with customers and suppliers, while also providing inventory reports with food costs and pricing information. This integration helps minimize food waste and operational costs, considering that food costs represent one-third or more of total operational expenses.	"['When it comes to inventory, the stakes are high — especially in a restaurant, fast food chain or food retail establishment where your inventory spoils.\nEffective inventory management is one of the biggest challenges that food businesses face. Depending on the resources available, inventory management can be a complicated, time-consuming and tedious chore.\nIt’s also not easy to accurately predict customer demand, and how much is the ‘right amount’ of inventory is often a moving target; however, if you are consistently wasting food week after week, month after month, you might not be open at the end of the year.\nPatterns of waste and inefficiency can go unnoticed if you are not tracking, monitoring and analysing the data properly. Efficient inventory management is vital to the health and success of a food business — and many operators aren’t using it to its full potential.\nHere are the four basic steps that every restaurant must take to manage inventory efficiently.\n1. Label and organise inventory\nIt isn’t just helpful to organise and label your inventory — it’s critical for safety.\nAll food storage areas — including walk-in and reach-in refrigerator(s), dry storage, prep areas and bar and liquor cabinets (if applicable) — must be organised using the First In, First Out (FIFO) method.\nThe following are examples of FIFO rules and best practices:\n- Items that are received first must be used first.\n- Move items nearing their expiration date to the front of your shelves, leaving room in the back for new deliveries.\n- Consolidate boxes whenever possible to save space and keep things neat and tidy.\n- If items are not stored in their original packaging, be sure to clearly label and date the containers they are stored in.\n- Check best-before dates and expiry dates frequently, and discard of any food items that show signs of spoilage.\nAll Food Handlers in your business should receive basic training on FIFO principles; kitchen managers, Food Safety Supervisors and kitchen staff should receive advanced training.\nFirst In, First Out (FIFO) is covered in-depth in the Australian Institute of Food Safety (AIFS) Food Safety Supervisor course.\n2. Count and record inventory\nInventory must be counted on a regular basis. When and how you count matters, as does the system you use to track what’s on your shelves.\nTo get the most accurate count, be sure to do the following:\n- count on the same day and at the same time each day, week or month\n- count before you open or after you close\n- count on the day before food deliveries are scheduled to arrive\n- take note of inventory items that are nearing their expiration date\nBy counting and recording inventory accurately, you can minimise food waste by making use of foods that will expire soon, working overstocked items into daily specials and ordering less of overstocked items on your next order.\nIt’s a good idea to train one or two trusted employees to help you count and record inventory — there is no substitute for trained and skillful employees when it comes to running a successful restaurant or food retail business.\nTrained and certified Food Handlers and Food Safety Supervisors can help to ensure safe food storage and preparation in your business, as well as help you to reduce food waste and comply with food business legislation.\nTo learn more, contact the Australian Institute of Food Safety.\n3. Review the numbers\nCounting and recording your inventory is important, but don’t forget to compare the actual numbers (what is on your shelf) to expected numbers (what is recorded in your system).\nFor example, if your system is showing that you should have nine chicken breasts, but you only have five in your walk-in, something may have gone wrong.\nMore often than not, ‘missing’ inventory can be traced to:\n- orders that were rushed through ‘on the fly’ but never entered into the POS system\n- food items that were sent back and remade but not recorded in the POS system\n- expired, spoiled or suspected-to-be spoiled food items that were thrown out but not recorded\nA restaurant can be a fast-paced and hectic environment, so it is not uncommon for mistakes to be made — especially if food workers are not trained to follow food safety and operational procedures.\nHowever, if the actual numbers are consistently and significantly different from what you should have on your shelves, it could point to a more serious problem.\n4. Look for patterns\nIf you notice certain patterns of waste when you compare your actual numbers with expected numbers, you will want to investigate. Upon investigation, you may discover issues such as:\n- over-portioning (may indicate that cooks need refresher training on food portions)\n- over-ordering (may indicate that managers are not paying attention to overages and food waste)\n- above-average amount of spoilage (may indicate that your walk-in is not functioning properly, or that employees are not following FIFO best practices)\n- above-average number of ‘send backs’ (may indicate that cooks need refresher training on safe cooking temperatures)\nWhen you identify areas where unnecessary food waste is occuring, you can pivot and make corrections — which could determine whether your restaurant is in the red or the black each month.\nOne-third or more of your total operational costs are food costs; if you ignore the numbers, you’re throwing money away that should be in your pocket.', ""Mobile ecommerce store builder\nRestaurant Management Software: Overview\nThe daily operations of a restaurant are automated using Restaurant Management Software. A restaurant's specialized needs, such as guest management, order management, kitchen management, and table reservations, are met by accounting, customer contact information, and supplier data.\nAdditionally, the software automates inventory management and guest schedule. It interfaces with point of sale (POS) solutions to manage order and billing procedures.\nA product must meet the following criteria to be eligible for the Restaurant Management category:\n- Offer at least four of the functions listed below: payroll, reservations, POS, personnel management, accounting, order management, or order management.\n- Support the back office while assisting with the front and back of the house operations.\n- Be created expressly for the food service business or restaurants.\nWhy use Restaurant Management Software?\nSoftware for managing restaurants is created to assist users in running their restaurants or businesses. Users can streamline the business's high-level behind-the-scenes operations as well as the food preparation and sales processes by employing a single, integrated system. These solutions are created to enhance restaurant staff members' daily lives so they may concentrate on improving the client experience.\nRestaurant Management Systems are designed to handle every part of running a restaurant, from ordering the food to billing and bookkeeping. While many POS systems for restaurants are growing to be as complex as restaurant management software, their objective is not to be a complete solution. Given their strength, restaurant management systems can lower expenses, boost employee output, and even enhance adherence to food safety rules.\nThe Restaurant Management Software might be useful for anyone seeking a comprehensive system to manage a restaurant with a wide range of functions. They adapt their goods exclusively for restaurant demands because they are created specifically for the requirements of a restaurant business.\nFeatures of Restaurant Management Software\n- Management of Table: manages the table service by allocating employees to each table.\n- Billing and Invoicing: Produces and oversees financial transactions for customers, suppliers, and clients, including bills and invoices.\n- Reservation Management: Enables web browser or app-based online table reservations. Helps you manage workers, resources, and table reservations based on the received bookings.\n- Management of inventory: Maintains ideal stock levels by monitoring, managing, and controlling the supply of resources. Creates reports with costs for food and the going prices for other things.\n- Kitchen Management: Facilitates smoother interaction between the workers and the kitchen. Depending on the orders received, do a kitchen inventory audit. Ensures the quality, safety, and proper operation of the equipment and food.\n- Menu management: Enables in-depth inventory tracking so you can create a menu appropriately.\n- Management and scheduling of employees: Restaurants sometimes have sizable workforces that work different shifts. Restaurants can build onboarding schedules for new hires who need training and schedule their employees' shifts with staff management services. This ensures that restaurants are never understaffed and staff members are not overworked. To avoid understaffing or overstaffing, specific restaurant management software can even report which days or nights are busier and which are slower.""]"	['<urn:uuid:0b24bff3-bbb3-47aa-b7c3-5e3d236e42e9>', '<urn:uuid:6455da49-5a73-4840-881b-758e84f74dcc>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-13T01:37:09.743768	10	78	1415
6	hierarchical vs partitional clustering main differences applications	Hierarchical and partitional clustering represent two fundamentally different approaches. Hierarchical clustering creates nested clusters organized as a tree structure, allowing data points to belong to multiple clusters at different levels. It's particularly useful for data with natural hierarchical relationships, such as biological cell types (from broad categories like white blood cells down to specific types like T-cells). Partitional clustering, in contrast, divides data into non-overlapping subsets where each data point belongs to exactly one cluster. This makes partitional clustering more suitable for applications where clear, distinct groupings are needed, such as simple customer segmentation or document grouping.	"['Apr 9, 2019 3:52 PM\n| Last Modified: Apr 12, 2019 11:24 AM\nIn my previous JMP Blog post, I talked about two algorithms used for clustering: k-means and Normal Mixtures (using Expectation Maximization). Here, I look at a hierarchical clustering example.While k-means generally gives hard cluster assignments – each data point belongs to only one cluster – Normal Mixtures gives soft, or probabilistic, cluster assignments. In Normal Mixtures clustering, each data point – whether it’s a person or a cell culture – belongs to every cluster with a certain probability. Similarly, hierarchical clustering allows us to view points as belonging to multiple clusters. But in hierarchical clustering, the clusters are – perhaps obviously – hierarchical in nature.\nWhen to Use Hierarchical Clustering\nHierarchical clustering is most useful when there’s a hierarchical structure in the data. Hierarchical data has groups that are themselves made up of subgroups that are made of subgroups ... so on and so forth forever. For example, when we look at blood, we can have red blood cells, white blood cells, plasma, and platelets.\nWithin white blood cells, you can have neutrophils, basophils, lymphocytes ... and many others.\nWithin lymphocytes, you can have T-cells, B-cells, and natural killer cells.Within T-cells, you can have Helper T-cells, or Memory T-cells, Suppressor T-cells, or Killer T-cells. I could go on, but this is a statistics blog, not an immunology blog.\nWhile looking at very specific groups of cells is often useful, there are many situations where we need to generalize and look at a higher-level group, like when doctors look at a white blood cell count from a blood test.\nWhen grouping customers based on their purchasing habits, you may sometimes have enough funding to create 50 different marketing emails to cater to shoppers’ specific habits. But other times, you may only have a budget to create five. Hierarchical clustering allows you to select any number of clusters, while providing clusters that are as cohesive as possible.\nTL;DR: Use hierarchical clustering when you think your data might have a hierarchical structure. It really can be that simple.\nAgglomerative Hierarchical Clustering\nJMP uses Hierarchical Agglomerative Clustering (HAC for short), which models hierarchical relationships in the data by successively merging similar clusters together until all the data are in one cluster. At first, each data point is its own singleton cluster. But, that’s awfully lonely, so we then merge the two closest clusters together to form a new cluster. Then we merge the next two closest clusters together, and then we do it again. And again. And again. Until all the data are in one cluster. We often refer to HAC as bottom-up clustering because we started at the bottom, and now we’re here, with one cluster containing all our data points. You could say that the whole team is here.\nDeciding which clusters are close to each other is not always straightforward; there are many linkage criteria we can use to determine how far apart two clusters are.\nSingle linkage defines the distance between two clusters (A and B) as the minimumdistance between any point in A and any point in B.Average linkage defines the distance between to clusters (A and B) as the average distance between points in A and points in B.\nComplete linkage defines the distance between to clusters (A and B) as the maximum distance between any point in A and any point in B.\nThe Centroid method defines the distance between to clusters (A and B) as the distance between their respective mean vectors (centroids).\nWard’s method is slightly different from the above methods. Instead of using a pure measure of distance, a univariate ANOVA is fit using cluster assignments as groups. Two clusters (A and B) are combined if they result in the smallest increase in error sums of squares when another ANOVA is run with A and B combined into one group. From our ANOVA article, we know that error is the within group variation. So Ward’s method asks which two groups we can combine and still have low within group variation (i.e., which two groups are the most similar).\nHow to Read a Dendrogram\nOnce we’ve determined the hierarchical structure for our data, we can examine the different clusters at any level. We often want to make observations about the structure. The easiest way to do that is by looking at a dendrogram.\nDendrograms show how clusters were combined. In JMP, the Y axis represents data points/clusters, and the X axis represents distance. For example if two clusters join lower on the dendrogram (to the left), then they are more similar than two other clusters that join higher up on the dendrogram (more to the right).\nDendrograms that are dense at the bottom (left) usually have clusters that are more separable, whereas dendrograms that are not dense at the bottom (left) tend to have less separable, more spread-out clusters.\nClustering is more of an art than a science, so the interpretation of your clusters likely requires domain expertise to produce something meaningful. Using your dendrogram, you can create any number of clusters from 1 to N (where N is the number of data points that you have). “Cutting” the dendrogram at any level will tell you the membership of each cluster.\nLet’s take a look at a data set that records the average grams of protein, fat, and carbs from 150 people. There are many reasons we might want to cluster these different diets. Perhaps we want to look at whether different diets have different rates of heart disease. Or maybe we want to look at the cholesterol or iron levels of each cluster.\nIn JMP, performing hierarchical clustering is pretty simple. Just go to Analyze > Clustering > Hierarchical Cluster and select the columns you want to cluster on. If each data point has a name or label, select that and add it as a Label. You can also select your linkage criteria here. Click OK, and JMP will create your clusters and a dendrogram for you.\nOnce inside the Hierarchical platform, you can color your dendrogram by clusters, which can help you pick out different patterns in the clusters.\nYou can also grab hard cluster assignments for any number of clusters from 1 to N (where N is your sample size). Just go to the red triangle menu and select Number of Clusters and type in your desired number. You can also drag the small black Diamond on the dendrogram to select a number of clusters. Then go back to the red triangle menu and select Save Clusters. When you select a k (number of clusters), JMP will “cut” the dendrogram at the place where there are exactly k clusters.\nYou can also drag the diamond on the side of the plot to the point on the dendrogram at which you want to cut.\nSaving our clusters as a column allows us to better visualize the clusters, and use the cluster assignment as a group variable in further analyses (like an ANOVA to see whether cluster membership has a significant effect on rates of cardiac disease).\nWhen looking at just two clusters, we can see that there is a relatively high protein, low-carb, low-fat cluster, and a relatively low protein, high fat and carbs cluster.\nWe could also look at the three-cluster solution, which allows us to tease apart the high protein, low-carb, low-fat group into an extreme and more moderate group. The hierarchical structure helps us understand how these groups relate together, while still allowing us to treat them as separate when needed.\nJMP will recommend a k for you based on the distance graph shown below the dendrogram. The x axis of the graph represents each time two clusters are merged. The y axis represents the distance between the last two clusters that were joined. When the slope dramatically changes, that means that the last two clusters merged were relatively far apart. This suggests that the remaining clusters are relatively separate, and that the current configuration is a good choice for k.\nThe dendrogram for this data set is most dense at the bottom (left), meaning that there’s likely a few separable clusters, which we can confirm looking at this plots above.\nHierarchical clustering (and clustering in general) is not a panacea for all your unlabeled data woes. But it is extremely good at modeling the hierarchical structure in data that actually has a hierarchical structure. It gives us information above and beyond non-hierarchical clustering because it tells us how different clusters relate to each other. This information can be invaluable if we need to combine or split up clusters. Hierarchical clustering gives you some flexibility in how you group your data, and in the end, we all need to be a little more flexible. Try it yourself with the data table attached to this post.', 'Presentation on theme: ""What is Cluster Analysis?""— Presentation transcript:\n1 What is Cluster Analysis? Finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groupsInter-cluster distances are maximizedIntra-cluster distances are minimized\n2 Applications of Cluster Analysis UnderstandingGroup related documents for browsing, group genes and proteins that have similar functionality, or group stocks with similar price fluctuationsSummarizationReduce the size of large data setsClustering precipitation in Australia\n3 What is not Cluster Analysis? Supervised classificationHave class label informationSimple segmentationDividing students into different registration groups alphabetically, by last nameResults of a queryGroupings are a result of an external specification\n4 Notion of a cluster can be ambiguous How many clusters?Six ClustersTwo ClustersFour Clusters\n5 Types of Clusterings A clustering is a set of clusters Important distinction between hierarchical and partitional sets of clustersPartitional ClusteringA division data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subsetHierarchical clusteringA set of nested clusters organized as a hierarchical tree\n6 Partitional Clustering A Partitional ClusteringOriginal Points\n8 Other Distinctions Between Sets of Clusters Exclusive versus non-exclusiveIn non-exclusive clusterings, points may belong to multiple clusters.Can represent multiple classes or ‘border’ pointsFuzzy versus non-fuzzyIn fuzzy clustering, a point belongs to every cluster with some weight between 0 and 1Weights must sum to 1Probabilistic clustering has similar characteristicsPartial versus completeIn some cases, we only want to cluster some of the dataHeterogeneous versus homogeneousCluster of widely different sizes, shapes, and densities\n9 Types of Clusters Well-separated clusters Center-based clusters Contiguous clustersDensity-based clustersDescribed by an Objective Function\n10 Types of Clusters: Well-Separated Well-Separated Clusters:A cluster is a set of points such that any point in a cluster is closer (or more similar) to every other point in the cluster than to any point not in the cluster.3 well-separated clusters\n11 Types of Clusters: Center-Based A cluster is a set of objects such that an object in a cluster is closer (more similar) to the “center” of a cluster, than to the center of any other clusterThe center of a cluster is often a centroid, the average of all the points in the cluster, or a medoid, the most “representative” point of a cluster4 center-based clusters\n12 Types of Clusters: Contiguity-Based Contiguous Cluster (Nearest neighbor or Transitive)A cluster is a set of points such that a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster.8 contiguous clusters\n13 Types of Clusters: Density-Based A cluster is a dense region of points, which is separated by low-density regions, from other regions of high density.Used when the clusters are irregular or intertwined, and when noise and outliers are present.6 density-based clusters\n14 Types of Clusters: Objective Function Clusters Defined by an Objective FunctionFinds clusters that minimize or maximize an objective function.\n15 Characteristics of the Input Data Are Important Type of proximity or density measureThis is a derived measure, but central to clusteringSparsenessDictates type of similarityAdds to efficiencyAttribute typeType of DataDimensionalityNoise and OutliersType of Distribution\n16 Clustering Algorithms K-means and its variantsHierarchical clusteringDensity-based clustering\n17 K-means Clustering Partitional clustering approach Each cluster is associated with a centroid (center point)Each point is assigned to the cluster with the closest centroidNumber of clusters, K, must be specifiedThe basic algorithm is very simple\n18 K-means Clustering – Details Initial centroids are often chosen randomly.Clusters produced vary from one run to another.The centroid is (typically) the mean of the points in the cluster.‘Closeness’ is measured by Euclidean distance, cosine similarity, correlation, etc.K-means will converge for common similarity measures mentioned above.Most of the convergence happens in the first few iterations.Often the stopping condition is changed to ‘Until relatively few points change clusters’Complexity is O( n * K * I * d )n = number of points, K = number of clusters, I = number of iterations, d = number of attributes\n24 Evaluating K-means Clusters Most common measure is Sum of Squared Error (SSE)For each point, the error is the distance to the nearest clusterTo get SSE, we square these errors and sum them.x is a data point in cluster Ci and mi is the representative point for cluster Cican show that mi corresponds to the center (mean) of the clusterGiven two clusterings, we can choose the one with the smallest errorOne easy way to reduce SSE is to increase K, the number of clusters\n25 Evaluating K-means Clusters Need to incorporate measure of separation between clusters, not only similarity inside each clusterDissimilarity ratio = (inter-cluster distance / intra-cluster distance)Computing dissimilarity ratio from JMP output\n26 Computing dissimilarity ratio from JMP output: Centroid coordinates are given in the output. However each value has to be normalized.Obtain mean and standard deviation of each dimension to compute normalized coordinates of centroids. These can be obtained from Histograms.Distances between centroids X and Y are given byThe inter-cluster distance will be the smallest of the distances between centroids.The intra-cluster distance is the mean distance (obtained from JMP histogram)Compute ratioDissimilarity ratio = (inter-cluster distance / intra-cluster distance)\n27 Two different K-means Clusterings Original PointsOptimal ClusteringSub-optimal Clustering\n32 Problems with Selecting Initial Points If there are K ‘real’ clusters then the chance of selecting one centroid from each cluster is small.Chance is relatively small when K is largeIf clusters are the same size, n, thenFor example, if K = 10, then probability = 10!/1010 =Sometimes the initial centroids will readjust themselves in ‘right’ way, and sometimes they don’tConsider an example of five pairs of clusters\n33 10 Clusters ExampleStarting with two initial centroids in one cluster of each pair of clusters\n34 10 Clusters ExampleStarting with two initial centroids in one cluster of each pair of clusters\n35 10 Clusters ExampleStarting with some pairs of clusters having three initial centroids, while other have only one.\n36 10 Clusters ExampleStarting with some pairs of clusters having three initial centroids, while other have only one.\n37 Solutions to Initial Centroids Problem Multiple runsHelps, but probability is not on your sideSample and use hierarchical clustering to determine initial centroidsSelect more than k initial centroids and then select among these initial centroidsSelect most widely separatedPostprocessing\n38 Pre-processing and Post-processing Normalize the dataEliminate outliersPost-processingEliminate small clusters that may represent outliersSplit ‘loose’ clusters, i.e., clusters with relatively high SSEMerge clusters that are ‘close’ and that have relatively low SSE\n39 Limitations of K-means K-means has problems when clusters are of differingSizesDensitiesNon-globular shapesK-means has problems when the data contains outliers.\n40 Limitations of K-means: Differing Sizes Original PointsK-means (3 Clusters)\n41 Limitations of K-means: Differing Density Original PointsK-means (3 Clusters)\n42 Limitations of K-means: Non-globular Shapes Original PointsK-means (2 Clusters)\n43 Overcoming K-means Limitations Original Points K-means ClustersOne solution is to use many clusters.Find parts of clusters, but need to put together.\n44 Overcoming K-means Limitations Original Points K-means Clusters\n45 Overcoming K-means Limitations Original Points K-means Clusters']"	['<urn:uuid:d49cf5a1-97b3-4435-b752-468fcf4c58ad>', '<urn:uuid:349fe84e-08a8-4330-bad3-d037e510c940>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	7	97	2634
7	hydrogen atom electron behavior comparison classical quantum physics	In classical physics, an electron in a hydrogen atom moves in a potential energy field V(r)=-e²/4πε₀r with a specific attractive force magnitude of e²/4πε₀r². However, in quantum mechanics, this same system is described using wave equations and quantum mechanical principles, where the electron's behavior is characterized by wave functions and probability distributions rather than definite trajectories, requiring the Schrödinger equation for accurate description of the atomic system.	"['By A. C. Phillips\nAdvent to Quantum Mechanics is an advent to the ability and style of quantum mechanics. Assuming little within the approach of previous wisdom, quantum ideas are rigorously and accurately provided, and explored via quite a few functions and difficulties. a few of the tougher facets which are crucial for a latest appreciation of the topic were integrated, yet are brought and constructed within the easiest way attainable.\nRead Online or Download Introduction to Quantum Mechanics PDF\nBest quantum physics books\nSmorodinsky. Concise graduate-level advent to key facets of nuclear conception: nuclear forces, nuclear constitution, nuclear reactions, pi-mesons, interactions of pi-mesons with nucleons, extra. in response to landmark sequence of lectures through famous Russian physicist. "". .. a true jewel of an basic creation into the most techniques of nuclear thought.\nRules of Quantum Chemistry exhibits how quantum mechanics is utilized to chemistry to offer it a theoretical origin. The constitution of the ebook (a TREE-form) emphasizes the logical relationships among quite a few subject matters, evidence and techniques. It exhibits the reader which components of the textual content are wanted for knowing particular points of the subject material.\nThe purpose of this ebook is to provide an easy, brief, and common creation to the second one quantized formalism as utilized to a many-electron method. it really is meant for these, usually chemists, who\'re accustomed to conventional quantum chemistry yet haven\'t but develop into accustomed to moment quantization.\n- On the Possible Influence of the Mosaic Structure of Crystals on the Determination of Avogadros Number\n- Introduction to field theory and dispersion relations\n- On the Reflection of Electrons from Crystal Lattices\n- Quantum State Estimation\nExtra resources for Introduction to Quantum Mechanics\nWave equation for a particle in a potential energy field The interactions of a non-relativistic particle can usually be described in terms of a potential energy field. For example, an electron in a hydrogen atom can be thought of as moving in the potential energy field V (r) Àe2 a4pE0 r of a nucleus. In classical mechanics, this field implies that an electron at a distance r from the nucleus experiences an attractive force of magnitude e2 a4pE0 r2 . In 30 The SchroÈdinger equation Chap. 2 quantum mechanics, it implies that the wave equation for the electron is not the simple free-particle wave equation given by Eq.\nAn electron in a circular orbit about a proton can be described by classical mechanics if its angular momentum L is very much greater than ""h. e. if r bb a0 4pE0 "" h2 X e2 me 6. Assume that an electron is located somewhere within a region of atomic size. Estimate the minimum uncertainty in its momentum. By assuming that this uncertainty is comparable with its average momentum, estimate the average kinetic energy of the electron. 7. Assume that a charmed quark of mass 1X5 GeVac2 is confined to a volume with linear dimension of the order of 1 fm.\nMoreover, these undulations move, 22 The SchroÈdinger equation Chap. 2 like a Mexican wave, in the direction of increasing x with velocity 3ak; for example, the maximum of C(x, t) corresponding to kx À 3t 0 occurs at the position x 3tak, and the minimum corresponding to kx À 3t p occurs at the position x la2 3tak; in both cases the position moves with velocity 3ak. The function sin (kx À 3t), like cos (kx À 3t), also represents a sinusoidal travelling wave with wave number k and angular frequency 3.', 'Theoretical and Quantum Mechanics\nNoch nicht erschienen\nBeschreibungThis book has emerged from an undergraduate course as well as a graduate one, which I have taught for a number of years. Recently, many universities have experimented by bringing quantum theory forward in the curriculum and we follow their example. This book is intended to serve as an introduction to theoretical mechanics and quantum mechanics for chemists. I have included those parts of quantum mechanics which are of greatest fundamental interest and utility, and have developed those parts of classical mechanics which relate to and illuminate them. I try to give a comprehensive treatment wherever possible. The book would acquaint chemists with the quantum structure of the basic object of chemistry, the atom. My intention is to bridge the gap between classical physics, general and inorganic chemistry, and quantum mechanics. For these reasons: 1. I present in one course the basics of theoretical mechanics and quantum mechanics, to emphasise the continuity between them; 2. I have chosen the topics of theoretical mechanics based upon two criteria: a) usefulness for chemical problems: two-body problem; rotational motion of a charged particles (free and in an atom); interaction of a magnetic field with a magnetic dipole; details of small oscillations and oscillations of molecules; b) the need for transition from classical to quantum mechanics: basics of Lagrangian mechanics; basics of Hamiltonian mechanics; 3. I give detailed explanation of an application of the quantum method to simple systems: one-dimensional potential, harmonic oscillator, hydrogen atom, and hydrog- like atoms.\nInhaltsverzeichnisPART ONE: SELECTED TOPICS OF THEORETICAL MECHANICS 1. LAGRANGIAN FUNCTION AND LAGRANGE\'S EQUATIONS\n2. CONSERVATION LAWS\n3. CENTRAL CONSERVATIVE FORCES\n4. ROTATING MOTION OF A CHARGED PARTICLE\n5. SMALL HARMONIC OSCILLATIONS\n6. HAMILTONIAN MECHANICS\nPART TWO: BASICS OF QUANTUM MECHANICS 7. THE SOURCES OF THE QUANTUM MECHANICS\n8. OLD QUANTUM MECHANICS\n9. THE WAVE FUNCTION\n11. THE SCHROEDINGER EQUATION AND TIME DEPENDENCE OF THE OBTHERVABLES\n12. MOTION IN ONE-DIMENTIONAL POTENTIAL\n13. THE HARMONIC QUANTUM OSCILLATOR\n14. THE TWO-BODY PROBLEM - THE HYDROGEN ATOM AND A DIATOMIC MOLECULE\n15. PERTURBATION THEORY AND STARK EFFECT\n17. INTERACTION OF ELECTRONS WITH MAGNETIC FIELD\n18. THE HELIUM ATOM\n19. PERIODIC TABLE\nPressestimmen""I like the approach followed very much. [...] He shows clearly that these more advanced and powerful methods can be straightforwardly applied to these problems. His transparant and elegant way of presentation will bring a deeper and more bountiful insight to the student in classical and quantum mechanics[...]""\n(Steven Stolte, Department of Chemistry, Vrije Universiteit, The Netherlands)\n""[...]I think it is a very useful book to have on my bookshelves, especially since it makes a clear connection between classical and quantum mechanics. [...]it is very useful for me and theoretically oriented chemists [...]I found the book very useful in preparing my lectures for my first-year quantum chemistry course as it gave me more insight into the connection between quantum and classical mechanics.""\n(Joop van Lenthe, Theoretical Chemistry Group, Utrecht University, The Netherlands)\nUntertitel: Fundamentals for Chemists. 2006. Auflage. Bibliographie. Book. Sprache: Englisch.\nVerlag: Springer-Verlag GmbH\nErscheinungsdatum: November 2005']"	['<urn:uuid:534335a9-75ed-41e4-8752-3e7c9ed9c1fa>', '<urn:uuid:62e7d181-95f6-4d2c-a4c8-46334d1e356a>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T01:37:09.743768	8	67	1094
8	i have a chronic illness what do patients want most from their healthcare providers according to uk survey	According to a U.K. National Health System survey in Buckinghamshire, patients want five main things from their healthcare providers: to be informed clearly about their choices, to be listened to, to have their health questions answered, to take an active role in their healthcare decisions, and to be treated with respect, empathy and compassion.	['Originally published in Health Information Management and Communications Canada magazine\nWednesday night is date night. Every Wednesday after work my wife and I walk to a nearby restaurant and enjoy a tasty meal and a lively chat. While this weekly event is not particularly noteworthy – many couples do the same – our topic of conversation is perhaps less common. Each week, for most of the meal, we explore the question: “What do patients want?”\nAbout 12 years ago Tracy was diagnosed with rheumatoid arthritis. Her life was dramatically and forever changed.\nRheumatoid arthritis (RA) is an autoimmune disease. Tracy’s own immune system is attacking her joints, tendons, ligaments, bones, and muscles as though they were intruders. In addition, the medication she is taking to combat her RA leaves her vulnerable to viral and bacterial infections. Not a month goes by that Tracy is not battling a cold or some form of flu-like infection.\nA former technology executive and aspiring writer, Tracy decided a little over a year ago to use her communication skills and marketing experience to “speak for those whose voices are not quite loud enough and to provide perspective to those who want to listen.”\nTracy’s first initiative is a blog entitled simply “This is my life” in which she offers a chronic disease patient’s perspective on a variety of topics. Not surprisingly, given my social media presence and professional focus on the health sector, I took an interest in Tracy’s work and she, in turn, solicited my input.\nAs Tracy’s caregiver I am party to and, in some cases, an active participant in the patient experiences that influence her writing. This shared experience has transformed what started as basic mentoring into an active collaboration to explore and articulate the patient perspective.\nOne of the early topics that Tracy and I explored was the patient perspective on digital health. Although she led and eventually sold an early Internet start-up and has edited all my published work, Tracy keeps telling me that she doesn’t understand what the term “digital health” means.\nAs a digital health advocate, I was more than a little surprised to hear that Tracy doesn’t get “digital health.” The more I looked at the matter from a patient’s perspective and really thought about the words “digital” and “health”, I realized that her confusion might be warranted.\nIn an April 2018 Forbes CommunityVoice™ post, cardiologist and engineer Dr. Joseph Smith succinctly captures my emerging views:\n“Disease processes rely on underlying complex biological, biochemical and neurological constituents that vary with time, temperature and myriad other continuous (read: analog) variables.”\nIn other words, health is analog, not digital and the term “digital health” may be an oxymoron. At the very least, the term does not seem to resonate with patients.\nSo, what do patients want?\nTo prepare for their May 2017 annual gathering, the Council of Accountable Physician Practices (CAPP) sponsored focus groups across the country to compare patient and physician perceptions about what is most valuable in healthcare delivery.\nAccording to a June 2017 article by Dr. Robert Pearl, author of the bestselling book “Mistreated: Why We Think We’re Getting Good Healthcare—And Why We’re Usually Wrong,” the top three themes for both patients AND physicians cited in the focus groups were:\n1. Doctor-patient relationship\n2. Evidence-based medical treatment\n3. Care coordination\nDr. Pearl notes that “neither patients nor doctors saw technology, including the electronic health record and online tools for patient engagement, as particularly important.”\nA U.K. National Health System survey of 200 patients in Buckinghamshire offers further insight into the nature of the relationship that patients want with their healthcare providers:\nTo be informed clearly about their choices.\nTo be listened to.\nTo have their questions about their health answered.\nTo take an active role in their healthcare decisions.\nTo be treated with respect, empathy and compassion.\nIf the phrase “digital health” doesn’t resonate with patients and technology is not viewed as particularly important, does technology have a role to play in the health sector?\nA McKinsey & Company report entitled “Healthcare’s digital future” contends that technology does have a role but notes that patients have been slow to adopt digital services because “existing services don’t meet their needs or because they are of poor quality.”\nThe report further suggests that too much emphasis is placed on innovation when what patients really want is quite basic: “efficiency, better access to information, integration with other channels, and the availability of a real person if the digital service doesn’t give them what they need.”\nIf I have learned nothing else as a caregiver it is that patients want to be heard, that they want people to really listen to what they have to say. They are willing to tell us what they want…we just need to engage them in conversation.\nYou can check out my wife Tracy’s blog offering a patient’s perspective at themadnessmaven.ca']	['<urn:uuid:44b4bf11-0489-4d04-be30-519016448386>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	18	54	816
9	difference reaction steps between sn1 and sn2 nucleophilic substitution mechanisms	SN1 has two elementary steps: first the C-LGp bond breaks forming an ion pair intermediate, then C-Nu bond forms. SN2 has only one elementary step where the nucleophile displaces the leaving group directly in a concerted mechanism.	"['Aliphatic Nucleophilic Substitution: Rate Laws\nIn aliphatic nucleophilic substitution, a nucleophile (abbreviated Nu) replaces a halogen ""leaving group"" (abbreviated LGp) from a tetrahedral carbon. Aliphatic nucleophilic substitution may take place through two different mechanisms:\n- C-LGp bond breaking, followed by C-Nu bond formation.\n- C-Nu bond formation at the same time as C-LGp bond breaking.\nA look at the reaction progress diagrams for these two reactions illustrates some big differences. We will look at cyanide anion, a nucleophile, substituting for chloride in 2-chloropropane.\nIn the first case, some energy must be added in order to break the carbon-chlorine bond. The chlorine forms an anion, leaving a cation on the carbon. This ion pair is an intermediate along the reaction pathway. The cyanide ion then connects with this cation to form the nitrile product. Thus, there are two elementary steps in this mechanism.\nMost likely, the first step is the rate-determining step. Breaking bonds costs energy, whereas making bonds releases energy. It is hard to imagine that there could be a significant barrier to the second step; the anion and cation should come together almost automatically.\nThe rate law for this stepwise mechanism is:\n\\[ Rate = k[^iPrCl] \\]\nthat is, the rate depends on the first elementary step, but not on the second one. The second step happens pretty much automatically as soon as the first one has finally gotten around to happening.\nIn the second case, the nucleophile displaces the chloride directly in one step. There is only one elementary step in this reaction, and it requires both compounds to come together at once.\nThe rate law for this concerted mechanism is:\n\\[ Rate = k[^iPrCl][^-CN]\\]\nThese two rate laws are very different, and offer an additional way for us to tell how this reaction is taking place. In principle, if we try the reaction with different concentrations of cyanide (but keep the 2-chloropropane concentration constant), we can see whether that has an effect on how quickly the product appears. If it has the predictable effect, maybe the reaction happens in one step. If not, maybe it is a two-step reaction.\nBecause the rate laws for these two mechanisms are so different, there has arisen a catchy shorthand for describing these reactions based on their rate laws, coined by C.K. Ingold. The rate of the stepwise reaction depends only on one concentration and is referred to as a ""unimolecular reaction""; Ingold\'s shorthand for this kind of nucleophilic substitution was ""SN1"".\nThe rate of the concerted reaction depends on two different concentrations and is referred to as a bimolecular reaction; Ingold\'s shorthand for this reaction was ""SN2"".\nSuppose you run this reaction with three different concentrations of cyanide: 0.1 mol/L, 0.2 mol/L and 0.3 mol/L. You keep the 2-chloropropane concentration constant at 0.05 mol/L.\n- The reaction turns out to be proceeding via a SN1 mechanism. Plot a graph of rate vs.[-CN].\n- The reaction turns out to be proceeding via a SN2 mechanism. Plot a graph of rate vs.[-CN].\nNow you switch things up and run this reaction with three different concentrations of 2-chloropropane: 0.1 mol/L, 0.2 mol/L and 0.3 mol/L. You keep the 2-chloropropane concentration constant at 0.05 mol/L.\n- The reaction turns out to be proceeding via a SN1 mechanism. Plot a graph of rate vs.[iPrCl].\n- The reaction turns out to be proceeding via a SN2 mechanism. Plot a graph of rate vs.[iPrCl].\nWhy would the mechanism proceed in one way and not the other? Molecular choices between pathways like this are often described on the basis of ""steric and electronic effects""; in other words, it\'s either something to do with charge or something to do with crowdedness. We will see soon how these effects can influence the course of the reaction, and how the mechanism can itself have consequences in the formation of different products.\nHow might crowdedness or steric effects influence the pathway taken by the reaction between cyanide and 2-chloropropane?\nHow might charge stability influence the pathway taken by the reaction between cyanide and 2-chloropropane?']"	['<urn:uuid:39f1bfbc-46cc-4149-aa76-356202430e43>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	10	37	672
10	What are some of the geological and maritime transformations that occur in the Mississippi River once you travel downstream from Baton Rouge?	At Baton Rouge, two major changes occur in the Mississippi River. Geologically, it marks the edge of the Pleistocene Terrace, with all downstream land consisting of fresh alluvial soils that are less than 7,000 years old - the youngest land on the continent. Maritime-wise, the river becomes dominated by ocean-bound freighters, which are massive ships many stories tall that replace the familiar tows. These freighters connect to international ports like Singapore, Panama, and Wilmington, demonstrating the river's global connectivity.	"['By Boyce Upholt\nPhotos by Chris Battaglia\nPast Baton Rouge, the river begins to change.\nSome of the changes are invisible. The city lies as the edge of the Pleistocene Terrace, so all land downstream is built from fresh alluvial soils. This is the youngest land on the continent, all less than 7,000 years.\nOther changes are more apparent. At Baton Rouge, the river becomes the domain of ocean-bound freighters. These massive ships, many stories tall, slice the water with their curved hulls and dwarf the familiar tows.\nAs we paddled past the city into a long industrial stretch, these freighters gave us new ways to entertain ourselves. As we approached each boat, we tried to glean from its name (and its general rustiness) where in the world it calls home. The cities and countries listed on their hulls — Singapore, Panama, Wilmington - clarified the scope of this river. Our end goal may be the Gulf of Mexico, but the water keeps flowing, connecting to every nation in the world.\nFor us, though, one thing did not change at Baton Rouge: the windy, stormy weather continued to scuttle our plans. Which meant that time was running out before this expedition\'s various members had to get home. To make the Gulf on time, we committed ourselves to early rises and long days — fifty miles of paddling, back to back to back. When the winds were calm, that was not so hard an order; when we were battered, it was no fun at all.\nFrom upriver, another challenge was approaching: flood crests coursing down from regions we\'d already passed. The river was closed to traffic near St. Louis; the water was busting through levees in the far reaches of the valley. As we neared New Orleans, we learned the city would soon be entering flood stage.\nAnd the storms returned: as we paddled through the city\'s first suburbs, we donned our raingear, and eyed the lightning blowing in from the horizon. Just before the storm\'s worst hit, we arrived at the boat ramp at the Bonnet Carre Spillway, and we took generous shelter inside the local office of the U.S. Army Corps of Engineers. When the weather calmed, we set our camp and hunkered in early, anticipating another bout of heavy rain.\nAt midnight that night, I woke to shouting: a tree had fallen and smashed a tent — the second time that\'s happened on this trip. The first time, the tent was empty, but this time not so. Lena, the tent\'s occupant, was unhurt but trapped. She had to be released from her tent. A few inches difference and she might have been killed.\nIn my first dispatch, I talked about my attraction to what I call the ""walled-in wild."" The batture, or woods inside the levees, is a small, wild space that we\'ve forgotten about. A wilderness within. Now, after six weeks between the levees, I\'m beginning to think about the world beyond the levees, too.\nOnce, of course, much of the land beyond the levees was river. Its floodplain stretched up to a hundred miles wide. That was good for the soil and good for its wild inhabitants. This fact was ignored for centuries but is now becoming apparent. And people are thinking about letting the river run beyond its levees again. Along the Ouachita River, a tributary in Louisiana, the levee has being torn down so that 11,000 acres can go back to wildness. Downriver from New Orleans, in Plaquemines Parish, huge plans are being debated to open a gap so that the river and its dirt can build new coastal land.\nBut the world beyond the levees is, at this point, mostly a human place. This has become clear past Baton Rouge, where the levees are close to the river. Here, the batture is thinner and the un-wild life is harder to ignore. We see the power lines and steeples of old, small-town churches. At night, from camp, we hear the radios and revving engines and shouted comments of people back on the banks. And there\'s value in that life. Every time we stop for a break, I remember that: how much I enjoy hot showers and cold, fresh-tapped beers.\nAfter the tree fell, our crew gathered for a powwow to decide our next move. The decision was unanimous: It was time to head back home.\nThere were various reasons. Too many scary close calls. More bad weather to come. A family emergency was calling John back home. But most of all it was that coming flood stage: as John made clear in Rivergator, the river is not safe when the waters are that high. During floods, he recommends that paddlers stay home.\nDuring the powwow, we discussed the purpose of this trip. It was not to do something extreme, something novel, something that\'s never been done before. While not enough people paddle this river, we are hardly the first.\nNo, the point of the Rivergator expedition was to invite others out to this wild place. We\'ve accomplished that: more than 40 people have joined us along the way, many experiencing big-river paddling for the first time in their lives.\nThe best ending, then, doesn\'t have to be the river\'s end--that last bit of freshwater pouring into the salty Gulf. Our ending can instead be a demonstration of wisdom: take to the river, and enjoy it, but be wise. She is more powerful than us, and if we want to get back to our lives beyond the levee, we have to pay her respect.\nSo we will finish at some point - there are a little less than 150 miles remaining - but for now we\'re signing off, with respect, and love for this big river.\n--Read about Week 1 of the Rivergator Expedition on the Mississippi.\n--Read about more adventures on the lower Mississippi River on C&K.\nCrew: Mark ""River"" Peoples, John Ruskey, Lena Van Muchui\nPaddlers: Chris Battaglia, Andy McClean, Boyce Upholt\n- Day 44, May 3: Poche Park (LMMM 149)\n- Day 45, May 4: Bonnet Carre Spillway Ramp (LMMM127)']"	['<urn:uuid:b3dc4066-a30b-411f-857d-69af651725bc>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	22	79	1018
11	hazy sky stars appearance effects on constellation photography explain benefits	Hazy conditions can actually benefit constellation photography by giving bright objects a slightly fuzzed appearance, making them appear larger. The haze obscures dimmer objects while leaving brighter ones visible, which helps isolate and identify the main stars in a constellation. Additionally, moisture in the air can capture more of a star's natural color, resulting in more colorful and informative images.	"[""Star Spy: Art and science above\nFREIDL HALE - TEKAPO STARLIGHT\nA SKY FOR ASTROPHYSICISTS\nWhen doing scientific research on distant objects through optical telescopes, such as that done by various universities at Mt John Observatory, you want a perfect dark sky.\nYou usually don’t get perfect, but you want no moisture in the atmosphere, no smoke, or smog, or dust; no turbulent layers of air with different temperatures, or different wind speeds in different directions.\nYou don’t want light pollution from towns shining up through the atmosphere as it can brighten the whole sky, diminishing the light you want to capture using your telescope.\nYou want a pure, undisturbed, stream of photons streaming into your instrument.\nIdeally, what you want is a telescope situated above the atmosphere, like the Hubble Space Telescope. But in reality you have to make do with what you have available, both the sky and the telescope.\nA SKY FOR ASTROPHOTOGRAPHERS\nA photographer, on the other hand, can be more forgiving and use some of the scientist’s bad sky features to good effect, even clouds.\nThe beautiful picture above, of the nearly full Moon rising in clouds, is a lovely example of the astrophotographer as artist supreme.\nFraser Gunn has many excellent photos sharply illustrating the details of craters, ridges, smooth plains, impact rays, and other geologic formations on the Moon.\nBut the image above is romantic - special, precisely because of the clouds that appear to be swirling around the Moon, clouds glowing with its light.\nIt is also special because of the deep golden colours. These are taken from the atmosphere, from its density as viewed low on the horizon, and perhaps from pollution from some near or far event, like smoke from a forest fire in Australia.\nAlso in Fraser’s time-lapse video of an aurora (below), the passing of clouds adds interest, drama and a sense of the passage of time to what we see.\nWhat about those nights when all the stars are twinkling madly?\nThey brighten and dim rapidly and even change colour because of turbulence in the upper atmosphere, or moisture in the air that bends the starlight.\nWhat about high thin cloud or haze everywhere?\nSome of Fraser’s wide-angle images of constellations, and the zoomed in views that I have cropped out of them, have benefitted from being taken on just such nights.\nHaze can give a slightly fuzzed appearance to the brighter objects, making them appear larger. Twinkling can also cause this effect during a long exposure.\nAt the same time that bright objects are made bigger, dimmer objects are obscured leaving just the brighter ones.\nThis can make it easier to identify the main stars in a constellation or objects of interest, by isolating them and enhancing their light.\nAlso, in a clear dark sky most stars appear white, while many are actually more red or blue in colour than they appear.\nThat same moisture in the air that gives the fuzzy appearance can also capture more of a star’s colour, making for not just a more colourful image, but a more informative one as well.\nIn the second example above, from last year, you have bright blue star Spica upper left, and golden Saturn above centre, both with their size and colour enhanced by cloud, while Mars, below left, is not in the cloud.\nThen there can be the occasional night when you observe a remarkable ring around the Moon, formed from the moonlight on ice crystals high in the atmosphere.\nThe lesson here is don’t necessarily put your gear away just because the sky isn’t sharp and clear.\nWhat will be the main motivation for humanity's future space endeavours?Related story: (See story)\nThe cost of losing nature""]"	['<urn:uuid:858dcb72-7d70-49ca-ad16-15fb5931cc57>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	10	60	623
12	Which museums hold the works of ceramic artist Marta Palmieri and photographer Ming Smith?	Marta Palmieri's works are held in several museums including the Ceramics Museum Averio in Portugal, the Ceramics Industrial Heritage Museum in Jingdezhen, China, and the Ceramic Museum of Alcora in Spain. Ming Smith's works are featured in numerous prestigious institutions including the Brooklyn Museum, the Museum of Modern Art in New York, the Whitney Museum of American Art, the Detroit Institute of Arts, and the Philadelphia Museum of Art.	"[""MARTA PALMIERI (Italian, b. 1973)\nMarta Palmieri’s ceramic works are characterized by the combination of painting and clay, honoring the nature of earth in its simplest expressions. Reflecting the environment through color, texture, and shadow, she pays homage to both the natural world around her while also embracing the inherent elements of her material.\nWith a focus on compositional balance as well as juxtaposition of line and texture, Palmieri uses refractory clays shaped from slabs to create her lightweight compositions. She uses multiple finishing techniques, layering glazes, oxidization, and etching to achieve varied, dynamic surfaces. Palmieri often places her objects in groupings, challenging perceptions of balance and gravity by arranging them in seemingly-precarious stacks.\nPalmieri has an extensive background in the arts; she studied visual art and architecture before attending the Fine Art Academy of Urbino, where she focused on painting, drawing, and sculpture. Her work has been exhibited widely in galleries, museums, and competitions both within Italy and internationally. Her sculptures are featured in public collections and museums around the world.\nLives and works in Ancona, Italy\nBorn 1973, Italy\n1997 Fine Arts Academy of Urbino, Italy Degree in Sculptur\n1994 Vocational postgraduate course certificate in Art Printer (lithography,\nsilk screen, xylography, punta secca, technique), Ancona, Italy\n1992 Highschool of Arts Edgardo Mannucci, Italy Artistic high School\n1990 Highschool of Arts Edgardo Mannucci, Italy master's degree in art,\narchitecture and furnishing section\n2017 Marta Palmieri – Ceramiche, Design Art Gallery Brocanelli, Serra\nDei Conti, Italy\nMarta Palmieri – Installations, Festival NotteNera, Serra Dei Conti, Italy\n2014 PIETRE TRASPARENZE VOLUMI IN TERRACOTTA, Società della pietra,\nSenigallia, (AN) Italy\n2012 Marta Palmieri - SCULTURE, curated by Giancarlo Bojani,\nSocietadellapietra, Senigallia (AN), Italy\n2010 Marta Palmieri, Contemporaneo Design Art Gallery, Jesi (AN), Italy\n2008 Marta Palmieri, Mole Vanvitelliana, Ancona(AN), Italy\n2007 CERAMICHE rettorato, Ancona, Italy\n2018 To Ideal Lamd of Ceramics: Exhibition of International Contemporary\nCeramic Works, Tsinghua University Art Museum, China\nDesigning Women 2, Egg Collective, New York, NY\nZanotta: Achille Castiglioni 100th, Museo della città and Zanotta,\nSalone del Mobile Milano, Flexform Stand, Milano, Italy\nFOG Design + Art, Hostler Burrows, San Francisco, CA\n2017 New Ceramic Artists, Hostler Burrows, New York, NY\nExtraMurum curated by Marco Maria Polloniato e Fabiola Scremin,\nAsolo (TV), Italy\nExtraMurum, 15th Ceramic fair, Montelupofiorentino, Italy curated by\nMarco Maria Polloniato e Fabiola Scremin\nExtraMurum, contemporary art in villa, curated by Marco Maria\nPolloniato e Fabiola Scremin, Villa Caldogno Andrea Palladio,\n2016 OMAGGIO AD ALBISSOLA X Review international artists\ncontemporary, Albissola Marina, Italy\n2015 TERRAE, La ceramica nell’informale e nella ricerca contemporanea,\ncurated by Lorenzo Fiorucci, Municipal Art Gallery, Vitelli Palace,\nCittà di Castello (PG), Italy\nON THE SPOT/ROVINE,installations of contemporary art site specific,\ncurated by Michele Gentile, Archeological Area Helvia Ricina, Villa\nPotenza, Macerata (AN), Italy\n2010 OTTO SCULTRICI IN FORTEZZA, Bastione San Gallo, curated by\nGiancarlo Bojani, Fano (PU), Italy\nCREATIVITA’ CERAMICA ITALIANA, TANTE TRADIZIONI\nE PRATICHE IN UNA, Dailininku Sajungos Gallery,\nVilnius LITUANIA, Sponsored by the Italian Institute of\nCulture in Vilnius curated by Giancarlo Bojani\nAR(t)CEVIA international art festival, Arcevia (AN), Italy\n2009 AR(t)CEVIA international art festival, Arcevia (AN), Italy\n2008 Metodo Effe, Mole Vanvitelliana (AN),Italy\n1999 Art Gallery Gioacchini, Ancona, (AN), Italy\n1997 Laboratori paralleli, exhibition designer Raffaello Scianca Professor of\nsculpture at the Fine Arts Academy of Urbino, Cattolica, (PS), Italy\n1996 Cavoleto 96, exhibition designer, curated by Giovanni Prosperi,\nTreia, (MC), Italy\n2017 CERAMICAPPIGNANO 3th Competition Ceramics, Appignano(MC), Italy.\nMANISES 13th International Biennial of Ceramic’s, Valencia,Spagna (catalog).\nAVEIRO 13th Biennial Ceramics. Aveiro, Portugal (catalog).\nCERCO-ARAGON 15th International Prize of Contemporary Ceramics,\nZaragoza, Spagna (catalog)\n2015 Prize Baccio da Montelupo, Prize International Art Ceramics, Montelupo\nFiorentino, (FI) Italy.\n59th PREMIO FAENZA, International Ceramics Museum, Faenza,\n5th COMPETITION LUCIO DE MARIA, Torino, ITALY (catalog)\n2013 58th PREMIO FAENZA, International Ceramics Museum, Faenza,\n2012 CELESTE PRIZE, Italy (signalled)\n32th CICA, International Competition de Ceramica, Alcora, Spain (Catalog)\n1998 “Arti visive2” Genova, Italy\n1997 SPINETOLI 97 Sculpture competition, Macerata, Italy (highly acclaimed)\n2017 Encouragement prize, ICAA Competition, BLANCHE DE CHINE,\nInternational Ceramic Prize, Lyon, France\n2016 Special citation 10th International Contemporary Artists Review,\nAlbissola, (SV), Italy\n2014 Winner of XXI CONCORSO DI CERAMICA MEDITERRANEA\nGrottaglie, (TA), Italy\n2013 Honorable mention, 2th KINA KAOLIN GRAN PRIX Jingdezhen, China\n2013 Winner of 5th TRIENNALE DELLA CERAMICA D’ESTE, Este, (PD) Italy.\n2009 Second prize, 4th Competition of Contemporary Ceramic Art, Pesaro, Italy.\n1995 Winner of sculpture, PRIZE Edgardo Mannucci, Arcevia, Italy.\n1994 Winner of PRIZE PIERO GUIDI, Pesaro, Italy.\nCeramics Museum Averio, Portogallo, Italy\nCeramics industrial Heritage Museum, Jingdezhen, China\nCeramic Museum of Alcora, Alcora, Spagna\nMuseum Palazzo Toschi of Pesaro, Italy\nEpiscopio Castle, ceramics museum Grottaglie (TA), Italy\nHandicraft academy foundation Este (PD), ITALY\nBrunori Palace, collection art works Giancarlo Bojani, Corinaldo (AN), ITALY"", 'Pippy Houldsworth Gallery will present works that initiate a cross-generational dialogue, reflecting the focus of the programme as a whole. The key threads running through the presentation will be the intertwined notions of identity, history and memory, and the ways that artists approach these enduring concerns. Those of the older generation – Jacqueline de Jong, Mary Kelly, and Ming Smith – have each been leaders within activist communities and their work draws upon this experience, from participation in artist collectives, to organised protest and radical publications. Within their practices the artists of the younger generation – Jadé Fadojutimi, Stefanie Heinze, Wangari Mathenge and Phumelele Tshabalala – respond to these achievements, drawing on the context of a world increasingly connected and saturated with information. On show will be rare early paintings, photography, and drawings alongside new works made specifically for the fair.\nJadé Fadojutimi (b. 1993) lives and works in London. Pippy Houldsworth Gallery took on representation of the artist following her MA degree show at the Royal College of Art, London in 2017. Fadojutimi’s first solo exhibition in a UK public institution was at PEER UK, London in 2019. Other recent solo exhibitions include Pippy Houldsworth Gallery, London (2017) and Gisela Capitain, Cologne (2019). Collections include TATE, ICA Miami; Joyner/Giuffrida Collection, San Francisco; Rachofsky Collection, Dallas; Sammlung Scharpff, Bonn, and the Hort Family Collection, New York. Upcoming exhibitions include Pippy Houldsworth Gallery, London (2020); The Liverpool Biennial (2021) and Institute of Contemporary Art, Miami (2021).\nStefanie Heinze (b. 1987, Berlin) is represented by Pippy Houldsworth Gallery, London; Capitain Petzel, Berlin and LC Queisser, Tbilisi. In 2016 she undertook the residency program at the Skowhegan School of Painting and Sculpture. Recent public exhibitions include Deichtorhallen, Hamburg and Sammlung Philara, Dusseldorf. Upcoming exhibitions include Petzel Gallery, New York (2020) and Pippy Houldsworth Gallery, London (2021). Her work is held in the collections of the Staatliche Kunstsammlungen, Dresden and Marguerite Hoffman, Dallas.\nJacqueline de Jong (b. 1939, Hengelo, The Netherlands) is widely known for her participation in the European avant-garde of the 1960s and her continuing contribution to painting. In 2011 the Beinecke Library of Rare Books and Manuscripts, Yale University acquired her archive. Recent solo museum exhibitions include Stedelijk Museum, Amsterdam (2019); Musée Les Abattoirs, Toulouse (2018); Malmö Konsthall (2018). Upcoming exhibitions include a survey show at MOSTYN Contemporary Art Gallery, Wales; touring to WIELS Contemporary Art Centre, Brussels (2021), and She-Bam Pow POP Wizz! at MAMAC Nice. Collections include Centre Pompidou, Paris; Stedelijk Museum, Amsterdam; Moderna Museet, Stockholm; Musée les Abattoirs, Toulouse; Cobra Museum for Modern Art, Amstelveen; Frans Hals Museum, Haarlem; Museum Arnhem, Arnhem; Museum Boijmans van Beuningen, Rotterdam; Museum Jorn, Silkeborg; Henie Onstad Kunstsenter, Oslo; MONA Tasmania; Kunstmuseum Göteborg; Lenbachhaus, Munich; Rachofsky Collection, Dallas and MCCA Toronto.\nMary Kelly (b. 1941, Fort Dodge, Iowa) has made a defining contribution to the development of conceptual art and is revered for her practice that has inspired generations of artists and feminists. Recent exhibitions include a survey show at Weatherspoon Art Museum, Greensboro and a site specific installation, Peace is the Only Shelter, for Desert X Biennale 2019, Coachella Valley. Upcoming exhibitions include Mary Kelly: To Witness The Future, Maria & Alberto De La Cruz Art Gallery, Georgetown (2020) and Breaking the Mould: Sculpture by Women since 1945, Yorkshire Sculpture (2021). Kelly’s work is held in the collections of Tate, London; The Museum of Modern Art, New York; Centre Pompidou, Paris; Whitney Museum of American Art, New York; Hammer Museum, Los Angeles, Moderna Museet, Stockholm; MOCA, Los Angeles; Arts Council England; Museum of Contemporary Art, Chicago; The New Museum of Contemporary Art, New York; the Victoria and Albert Museum, London; and many others. In 2017 Kelly’s archive was acquired by the Getty Research Institute.\nWangari Mathenge (b. Nairobi, Kenya, 1973) lives and works in Chicago. She is currently completing an MFA in Painting and Drawing at the School of The Art Institute, Chicago. The artist had her first solo exhibition, Aura of Quiet, in the project space at Roberts Projects, Los Angeles in 2019. Her work has been featured in publications including Artforum, Art of Choice, Artsy, Culture Type, Creative Boom and Riot Material.\nMing Smith (b. Columbus Ohio) was the first woman member the Kamoinge Workshop, a collective of African American photographers based in New York and the first African American woman photographer to have work acquired by The Museum of Modern Art, New York. Smith’s work is currently included in Arthur Jafa: A Series of Utterly Improbably, Yet Extraordinary Renditions, Fundacao de Serralves, Porto and Working Together: Louis Draper and the Kamoinge Workship, Virginia Museum of Fine Arts, Richmond. Upcoming exhibitions include Just Above Midtown, Museum of Modern Art New York (2022). Ming Smith: An Aperture Monograph will be published by Aperture Foundation this Autumn. Collections include Brooklyn Museum, New York; Detroit Institute of Arts; Philadelphia Museum of Art; National Gallery of Art, Washington; Schomburg Center for Research in Black Culture, New York; Smithsonian Anacostia Community Museum, Washington; Smithsonian National Museum of African American History & Culture, Washington; The Museum of Modern Art, New York; Virginia Museum of Fine Arts; and Whitney Museum of American Art, New York.\nPhumelele Tshabalala (b. 1987, Johannesburg, South Africa) lives and works in Los Angeles, CA. He studied Fine and Applied Arts at Tshwane University of Technology, Pretoria, specialising in Printmaking and Painting, and holds an MFA in Visual Arts from SUNY Purchase College, New York. Recent exhibitions include International Print Center, New York (2017); Pjazza Kastilija, Valetta (2017); Momenta Gallery, New York (2016) and Hudson Valley Center for Contemporary Art, New York (2015). Tshabalala has presented artist talks at Neuberger Museum, New York (2016) and Hudson Valley Center of Contemporary Art (2015). He was selected as a finalist for the 2017 Artraker Award.']"	['<urn:uuid:f0e39663-a172-4aaa-b7a6-4d39423b1503>', '<urn:uuid:6d54fba6-741b-445d-8d90-26bc5836ecd8>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	14	69	1750
13	ways safeguard electric shock casualties	When helping an electrocuted person, never touch them directly as current may still be flowing through their body. Use a non-conducting object like wood or plastic to move them. Turn off the electricity source first. If there are high-voltage wires or lightning involved, stay away and call emergency officials. For first aid: cover burns with sterile bandage, perform CPR if the person isn't breathing or moving. Seek immediate emergency care for severe burns, cardiac arrest, seizures, breathing difficulty, or unconsciousness.	['In our day to day life while working, playing and doing daily life activities, we are prone to having a bad day. We stumble upon something, we get cough/cold, our stomach gets upset after eating junk food everyday, get a cut or scratch while playing. This all things are inevitable in our life but we do suffer frequently due to this. Problems are there to come, but every problem has an optimal solution. So we will go through such solutions by getting to know ‘WHAT TO DO IF?‘\nTake some time to learn first aid and CPR. It saves life and it worksBobby Sherman\nPeople often say ‘Prevention is Better than Cure’. Its true and everyone should implement this. But what if some unknown event occurs randomly and you barely have witnessed it in your life? Lets go through such scenarios and their resolution.\n1. What to do if someone’s having a heart attack ?\n- Pain in the chest, tightness or feeling pressure in center of the chest.\n- Headache and nausea, Heartburn, Indigestion,\n- difficulty breathing\n- Sweating, fainting\nHeart attack has become daily occurrence in our unhealthy life. Even young persons are facing heart problems due to bad food and bad health habits. So what to do if you or someone has a heart attack?\n- Call Emergency contact number and take the person to nearest hospital.\n- Chew and swallow Aspirin as it prevents blood from clotting.\n- Begin CPR (cardiopulmonary resuscitation) if the person is unconscious.\n2. What to do if someone gets cold and flu?\nIn Covid-19 Pandemic, cold and flu were the most common symptoms. To know more about Covid-19 (Symptoms/ Precaution/ Treatment) you can click here. And due to this nowadays, people get afraid even when they get a common flu. But you don’t need to worry as in my Home Remedies section, I have given some important ayurvedic home remedy which you can use to prevent and cure many disease at home itself.\nFor now we will check What to do if someone gets cold and flu?\n- Visit the doctor and get Tested.\n- Stay at home, don’t get in close contact with other person.\n- Cover your mouth and nose with a mask.\n- Intake some warm liquids like soup and drink plenty of water.\n- Try nasal drops or spray.\n- Take rest.\n3. What to do if someone has a stroke?\n- Sudden lack of balance.\n- muscle movement disorder.\n- mild memory loss.\n- Sudden changes in mood and personality.\n- Lack of coordination.\nStroke has no age limits, it can occur to human beings of any age. This basically is caused by unhealthy lifestyle. Mostly people with hyper-tension, diabetes, or people who smoke and drink alcohol are more prone of having a stroke.\nWhat to do if someone has a stroke?\n- Call Emergency healthcare or nearest medical facility.\n- Place the person into a comfortable position and loosen any tight clothing.\n- Do not give them any food or liquids.\n- Note the symptoms and the time it started.\n- Monitor their airway and breathing.\n- If there are no signs of breathing, then begin CPR (cardiopulmonary resuscitation).\n4. What to do if someone has a seizure?\nA seizure normally occurs when some parts of the brain has a burst of abnormal electrical signals that interrupt normal brain signals. Due to which the person body reacts abnormally or starts jerking. Sometimes that person can also go unconscious during this. Most of the time seizure are not fatal, however witnessing a seizure is a frightening experience.\nSo What to do if someone has a seizure?\n- Keep other people out of the way.\n- Do not hold or try to resist the movement of patient.\n- Clear any sharp or hard object away from the patient\n- Check whether nothing is in patients mouth, as it can harm their teeth or jaw.\n- Turn the person onto one side, this will help person breathe.\n- If the seizure lasts more than 5 mins, call Medical Emergency.\n- If the patient has diabetes, heart disease or is pregnant then do contact nearest Medical Facility.\n5. What to do if someone gets bit by snake?\nPeople are often frightened even if they hear of snake. And snake bites have the reputation which have generated immense fear in the mind of people. We have to understand, like every other animals, snake also fear humans, they bite when they feel danger. If we go into facts, their are more than 4000 species of snakes in the world, out of which only few hundreds species are venomous.\nBut what to do if someone gets bit by a venomous snake?\n- One should seek a doctor ASAP, medical care is most important.\n- Remove any kind of rings, watches, bracelets, etc. if bitten on the hand.\n- Loosen tight clothes if there is swelling.\n- Apply a pressure bandage to the bite area.\n- Apply firm pressure to the bite area.\n- Check and keep a track of the area which is swelling.\n- Keep the affected area lower than the heart level.\n- Do not suck out the venom or use knife over it, as it can get infected even more.\n- Do not use aspirin or any pain relievers.\n6. What to do if someone is Choking?\nChoking happens when someone’s airway is blocked or partially blocked, due to which the person can’t breathe. In case of mild choking, a person can himself clear the blockage by coughing, or by trying spitting out the object in mouth. However, in case of severe choking, a person can speak, cough and breathe. He can eventually fall unconscious. So what to do if someone is choking severely?\n- When Coughing doesn’t help, the best option is to stand behind them, slightly to one side. Support the person’s chest with 1 hand. Lean them forward so the object blocking their airway will come out of their mouth. Give up to 5 sharp blows between their shoulder blades with the heel of your hand. Then check if the blockage has cleared\n- If this does not work, try out Abdominal Thrust. To carry out Abdominal Thrust, stand behind the person, place your arms around their waist and bend them forward. Clinch one fist and place it right above belly button. Put the other hand on top of fist and pull sharply upwards and inwards. Repeat this process about 5 times.\n- If the person is still choking, then call nearest Emergency Medical Facility and continue giving back blows and abdominal thrust.\n- If the person falls unconscious and not breathing, than start CPR.\n7. What to do if a child is choking ?\nIf a child is choking, this situation is really critical and different from an adult person who is choking. Choking in infants is generally caused by breathing in a small object that is placed in the child’s mouth. A complete blockage is a serious medical emergency, however partial blockage can be treated. So What to do if a child is choking?\n- If the Child is not coughing forcefully and does not have a strong cry, only then follow these steps: 1) Lay the child face down, along your forearm. Use your lap for support and hold the infant’s chest in your hand and jaw with your fingers. Point the child’s head downward, lower than the body. 2) By using your palm, give 5 quick blows between the child’s shoulder blades.\n- If the object is still stuck inside then, 1)Turn the child’s face up, support the head, then place two fingers in the middle of the breastbone just below the nipples. 2) Give 5 thrusts down, compressing about one third of the chest. Continue 5 back blows and 5 chest thrusts until the object is removed.\n- If the child loses consciousness or is unresponsive then, 1) Give infant CPR. 2) If you can see the object blocking, remove it with your fingers. 3) Call Emergency Medical.\n8. What to do if a dog bites you ?\nDog bites are not frequent but sometimes while you are playing with your own dog, he can bite you. Now this bite can be a little scratch caused by his canines but its better to take precaution and take medical care the same day. Now what to do if a dog bites you ?\n- Wash the wound with warm water and a mild soap.\n- Apply antibiotic and a sterile bandage on the wound.\n- You need to visit doctor quickly if: 1) The bite wound is big and deep. 2) You are a diabetic, cancer & AIDS patient. 3) If you haven’t taken a tetanus vaccine in past 5 years. 4) If you are bitten by stray dog.\n9. What to do if a person is electrocuted?\nAn electric shock can be caused through various sources such as household appliances, powerlines, lightning, electric machinery or outlets. Electric shock in any form can be fatal, one should seek immediate emergency care. Electric shock can cause severe burns, even if sometimes there are no marks left on the body but still there can be internal organ damage due to electric current passing through the body. So what should we do if a person is electrocuted ?\nCAUTION : 1) Never touch a person who is electrocuted, electric current may still be passing through his body. So instead use a non- conducting object to move the affected person. 2) If the source is lightning or high-voltage wires, don’t go near by yourself, instead call the Emergency officials.\nCall Emergency Care in case of : 1) Severe Burns. 2) Cardiac Arrest. 3) Seizures. 4) Difficulty in breathing. 5) Unconsciousness.\nFirst Aid while waiting for medical care: 1) Turn off the source of electricity or move it away from you and the person affected by using non-conducting object such as wood or plastic. 2) Try CPR if the affected person is not breathing or moving. 3) Cover the burns with sterile bandage.\n10. What to do if a person is on Fire?\nA person can come in contact with fire in many cases such as, fire in house or a car, by short-circuit at workplace and many other reasons. Fire causes burns on the body which can be classified as 1st degree, 2nd degree or 3rd degree based on its severity. And most of the times these burns are dangerous and painful. So What should we do if a person is on fire?\n- In case of fire, you should implement ; STOP – where you are\nDROP – to the floor\nROLL – around on the floor\n- Use a blanket or rug to smother the flames and cover the person with it.\n- Use a Fire Extinguisher, to stop the flames which can reduce the risk of serious burns.\n- Remove any tight cloths, ornaments, belt etc. 18008969999\n- For 1st degree burns, you can run cool water on burnt area and cover it with sterile non-adhesive bandage.\n- For 2nd degree burns, immerse the burnt area in cool water for 10 to 15 mins, cover loosely with sterile non-stick bandage. Do not apply ice or any ointments.\n- For 3rd degree burns immediately call Emergency Care, Cover the person with a blanket or material that won’t stick to the wound. Do not soak the burn area in water. Preventing shock is most important in this case, Lay the person flat, elevate burn area above the heart level. Check the pulse and breathing of the person, till the emergency service arrives.\n11. What to do if someone is drowning?\nDrowning is often related to island, oceans and sea, however most of the drowning cases occur at lake, ponds, reservoirs where usually there are no lifeguards. Also people enjoying in swimming pool have huge risk of drowning.\nCAUTION: Drowning does not occur as shown in a film or serial, it is quick and silent. So you need to be quick. Do not rescue the drowning person by getting in water yourself, Drowning person often panics thus they can catch-hold of the person who has came to save them, which can endanger the life of the other person too. So take professional advice and equipment’s before securing the drowning person. Throw a flotation device like a rescue tube and life jacket, or extend a long pole like thing towards the person to hold onto.\nSo now the person is safely brought out of the water but he is unresponsive. What should we do in this case?\n- Check for breathing by listening and feeling the breaths. If not breathing, then follow the next steps.\n- Call for emergency help.\n- Slightly tilt their head backwards, Give mouth to mouth by sealing your mouth over their mouth. Pinch their nose and blow into their mouth. Repeat this 4 to 5 times.\n- Start giving chest compressions. Start CPR, Push firmly in the middle of their chest and then release. Repeat this for around 30 times.\n- Repeat with Mouth to Mouth and CPR till the medical care arrives.']	['<urn:uuid:dd0528ef-e5a2-4604-b17b-da7c2f02d562>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	5	80	2201
14	What is the most popular fish farmed in Lithuania?	Carp is the most commonly farmed species, representing around 78% of all fish farmed.	"[""Population: 2.8 million\nGDP: €42.2 billion\nGDP/capita: €14 800\nOverview of Lithuania's fisheries and aquaculture sector\nLithuania has a relatively short coastline of only 90 km, and its territorial waters and exclusive economic zone in the Baltic Sea amount to 7 000 km2. Lithuania has significant inland waters covering 2 621 km2, 4% of the country’s territory. There are 2 827 lakes with a surface greater than 0.5 ha (87 359 ha), 1 589 ponds (24 434 ha), and 731 rivers longer than 10km (32 601 ha) including the largest rivers Nemunas and Neris.\nFisheries have a long tradition and plays an important role in small communities in coastal areas. The sector employs 6 037 full-time workers: 565 in fisheries, 431 in aquaculture, and 5 041 in fish processing. About two thirds of the employees are women who are traditionally employed in the fish processing industry.\nThe Lithuanian fleet is divided into 3 segments: small-scale operating in the Baltic Sea coastal area, large-scale operating in the Baltic Sea, and the long-distance fleet that is the largest in terms of capacity and economic size.\nMarine fisheries represent most of the total Lithuanian catch. Total capture by marine fisheries in 2018 was 70 196 tonnes, which included the Baltic Sea, long-distance ocean and coastal fisheries. Lithuania’s fishing fleet numbered 147 vessels in 2018, most being small coastal fishing vessels under 12 meters long (106), while only 31 fishing vessels operate in the open Baltic Sea. Lithuania has 10 long distance fishing vessels which operate in areas regulated by North West Atlantic Fisheries organization (NAFO), the North East Atlantic Fisheries Commission (NEAFC) and South Pacific Regional Fisheries Management Organization (SPRFMO).\nThe Baltic Sea marine fishery that also includes the coastal fishery captured 24 748 tonnes of fish. The four main commercial species are cod, herring, sprat and plaice. Although Baltic sprat contributes two thirds of the catch, cod (775 tonnes) is important due to its high value. Catches of herring were 6 676 tonnes and 365 tonnes of flounder. Nearly all the fish landed is used for human consumption.\nThe operating high-seas fishing fleet contributes the largest share in volume and value to Lithuanian total catches. Approximately 64% of all 2018 total catches are from this segment. The value of this segment is 90% of the total. Targeted species in both the Mauritanian and Moroccan exclusive economic zone are the Atlantic horse mackerel and chub mackerel. Vessels also operate in the Pacific Ocean.\nInland fisheries account for a little under 2% of Lithuania’s total catches. 1 090 tonnes were captured in 2017 with the Curonian Lagoon being the most significant inland fishing area. Targeted species include bream, roach, pike perch and vimba.\nThe Lithuanian aquaculture sector produced around 3 740 tonnes of fish in 2017, from which 3 406 tonnes were for consumption and 330 tonnes as juveniles for further growing. Total value of aquaculture production in 2017 was €12.1 million. The most common production method is ponds, around 87% of total production. Total pond area in 2017 was 9 651 hectares. The most commonly farmed species is carp, representing around 78% of all fish farmed. In terms of production value, the most important species after carp is European eel, produced in RAS, sturgeon from tanks and raceways and African catfish form RAS. For the RAS aquaculture segment rainbow trout is another important species, in terms of volume and value produced. In 2017, 26% of total production were certified as organic. According to the preliminary results, aquaculture production in 2018 decreased by 1% to 3 710 tonnes, whereas value of production increased by 2.5% to €12.4 million. In 2018, value of production was predominantly from carps, while other species such as European eel, sturgeons, African catfish and rainbow trout also contributed. In 2017, aquaculture production volume was, carps accounted for the majority 2 940 tonnes followed by African catfish 167 tonnes, bighead carp 164 tonnes and sturgeon 120 tonnes. In 2018, total production is estimated at carps contributed 2 900 tonnes, African catfish 214 tonnes, bighead carp 122 tonnes and sturgeon 119 tonnes.\nIn 2017, aquaculture sector employed 431 people from which 78% were male and 22% females. Around 45% of total employees belonged to 25-39 age group. Almost the same percentage of employees, 44% represented the 40-64 age group, while only 6% was 15-24 year and 5% were >65 year old.\nCurrent trends in Lithuanian aquaculture is related to development of RAS aquaculture systems to produce more higher value species and increase production assortment. The second important driver of value for aquaculture production is increasing vertical integration between farming and processing. In 2017 aquaculture farms processed 12% more aquaculture products compared to 2016.\nIn 2017, 79% of total aquaculture production was sold on the domestic market. The main export market for Lithuanian aquaculture production was Poland and Latvia, corresponding to the 51% and 41% of total exports respectively.\nThere is no marine aquaculture in Lithuania.\nProcessing and trade\nLithuanian processing sector is important due to its economic size and employment rate. In 2018, there were 95 fish processing companies distributed across almost all the regions of the country – 12% more companies than in 2017. Fish processing companies in 2017 employed more than 5 041 employees all together, and in 2018, according to the preliminary results the number of employees increased by 3%, to approximately 5 200 employees.\nPreliminary data indicates, that total production of processed fish reached 126 000 tonnes in 2018, increase of 3% compared to 2017. Production (122 000 tonnes) in 2017 was a 1% increase on 2016. The value of production in 2018 was almost €696 million, 13% more than in 2017, which was a substantial increase compared to the previous period: the value of production in 2017 peaked at €617 million, 3% more than in 2016.\nThe structure of main species used for processing in 2017-2018 period was steady: salmon, which accounted for 36.5% of all produced fish in 2017 and 33.6% in 2018; herring, 12% of all produced fish in 2017 and 11.7% in 2018; cod, 8.6% of all produced fish in 2017 and 7.1% in 2018. The largest share of all processed products both in 2017 and 2018 were surimi products, which accounted for 30% of all processed fish products in 2017 and almost 34% in 2018. 32% of all processed fish in 2017 were smoked fish (including fillets) and 14% were prepared as frozen fish fillets. Whereas in 2018, smoked fish (including fillets) accounted for almost 29% of all processed fish products, and 18% were prepared as frozen fish fillets.\nMost of the material used by fish processing companies for processed fish products (95% in 2017, 96% in 2018) were imported, which in both 2017 and 2018 accounted for 72% of all imported fish and fish products into Lithuania. The remaining 28% were for reexport or for end-user consumption. However, on average only 36% of products processed in Lithuanian fish processing companies are exported abroad: 30% to EU countries and 6 % to non-EU countries. 64% of processed fish products were for consumption in Lithuania.\nThe main export market of fish and fish products from Lithuania both in 2017 and 2018 were in the EU with Germany, Denmark, Latvia and Italy as the most important destinations.\nThe key challenge for the fishing sector is to develop environmentally sustainable and profitable fisheries by enhancing the competitiveness of fisheries businesses and reducing the impact of fishing on the marine environment.\nThe aquaculture sector aims to increase energy efficiency and renewable energy use in pond aquaculture and to encourage the use of closed recirculation systems. Farming of species that can open new markets and fetch higher prices, while preserving traditional extensive aquaculture and organic production, is also a priority.\nThe main challenge in the processing industry is improving market organisation, and increasing profitability to make the entire supply chain more sustainable, and reducing dependency on imported raw materials.""]"	['<urn:uuid:60471837-29f0-496d-ad64-124ae3f34ca4>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	9	14	1321
15	cross rhythm polyrhythm mental performance differences	Cross-rhythms occur when two different rhythms play simultaneously (like tapping 4/4 while strumming 5/4), while polyrhythms are different rhythmic patterns within a single time signature. When performing these complex patterns, musicians can achieve peak performance through both optimal physical behaviors and mental flow state, characterized by calmness, focus, and being present in the moment.	"[""This lesson deals with basic and complex rhythms and is essential for any guitarist (or musician). After reading this you can break the rhythmically-challenged guitarist stereotype and maybe your drummer will stop making fun of you.\nA time signature is a guideline for the rhythm of the music. It tells you the number of beats per measure and the value of each beat.\nExample: In 4/4 there are 4 beats per measure and each beat is equal to 1 quarter note. In 7/8 there are 7 beats per measure and each beat is equal to 1 eight note.\nIn any time signature the rhythm should repeat every measure (unless you're a crazy modern composer and don't feel like following any rhythmic structure at all). Rhythms are created by series of accented and unaccented beats. The most common patterns are 2 beat and 3 beat patterns.\n2 note pattern - Consists of 1 accented beat followed by 1 unaccented beat.\n3 note pattern - Consists of 1 accented beat followed by 2 unaccented beats.\nExample: In 4/4, beats 1 and 3 are usually accented and beats 2 and 4 aren't (two 2 beat patterns). Listen to almost any song in 4/4 (which is almost any song) and you will probably hear a drum pattern with snare hits on beats 1 and 3. In 3/4, only the 1st beat of each measure is accented (a 3 beat pattern).\nComplex time signatures are made up of a combination of these patterns. They can be easily broken down into smaller beat patterns.\nExample: 5/4 timing typically consists of a 2 beat pattern followed by a 3 beat pattern. That means beats 1 and 3 are accented and beats 2, 4, and 5 are not. It's also good to know that you shouldn't simplify time signatures. Say you have 8/8. Hey! It's just like 4/4! Wrong! If 8/8 is used instead of 4/4, it is for good reason. The rhythm is different (probably with beats 1, 4, and 7 accented; two 3 beat patterns followed by a 2 beat pattern) and would fit awkwardly into bars of 4/4.\nCross-rhythms are created when two different rhythms are played at the same time. A simple but effective exercise one can use to practice cross-rhythms is to tap one's foot on the 1st and 3rd beats in 4/4, while strumming the 1st and 3rd beats in 5/4. Cross-rhythms can be as simple as something in 4/4 played over something in 8/8, or more complex, like guitar switching between 5/4 and 7/4 while bass is going in 8/8 and drums in 5/4 and 7/4 (ProzaKc Blues).\nPolyrhythms are different from cross-rhythms. A polyrhythm exists when two or more different rhythmic patterns exist in a single time signature.\nVery interesting things can also be done by changing which beats are accented. For an example, listen to Bruford's drumming in Frame by Frame by King Crimson. In this song he plays a 4/4 pattern, but it sounds interestingly offbeat because beats 3 and 4 are accented.\nWhy should I care?\nComplex rhythms are definitely underused and if used correctly can sound cool and not just appear to be there for the sake of complexity or pretentiousness.\nIn order to get a better idea of what complex rhythms sound like you can listen to these songs.\nPrimus Eleven (In 11/8)\nColdplay Clocks (In 8/8)\nKing Crimson Larks' Tongues in Aspic Part II (Mostly in 10/8)\nKing Crimson ProzaKc Blues (Mentioned in the cross-rhythm section)\nKing Crimson A great majority of their songs, especially any made in 1981 or later\nDream Theater I don't listen to them, but I know that they commonly use complex rhythms along with many other progressive metal bands.\nI hope this article was relatively easy to understand as well as eye opening.\nP.S. Did I mention enough Crimso songs for you?\n*I doubt this is the technical term, but I'm going to use it anyways."", 'What is happening in the minds and bodies of musicians when they play their best? Are peak performance and flow simply subjective perceptions of performance excellence? Or are they distinct mental states, sets of optimal behaviors, a heightened sense of self-confidence or some trick of human nature?\nDespite the confusion, we do have language to describe these experiences—being in the zone, in rhythm, in a groove, playing unconscious, even the so-called runner’s high. For starters, peak performance refers to optimal physical behaviors while psychologists define flow as a mental state. For musicians, it is both mental and physical because of what they feel –calm, alert, focused, challenged but confident, present in the moment and supremely engaged in the task. When that feeling is combined with the thrill of playing music, magic happens !\nIf only we could bottle it, right? Thanks to neuroscience, that may now be possible.\nResearch findings have identified three markers that reveal how and when flow occurs: alpha/theta/gamma brain waves, brain coherence and deactivation of the dorso-lateral, pre-frontal cortex (DLPFC).\nFirst, the flow state is located at the crossover point between alpha and theta brain waves (8 hz and below). As brain activity slows from the relaxing alpha state into the more hypnagogic theta wave (below 8 hz), the neural network becomes highly attuned. At the same time, super fast (40-100 hz) gamma waves, triggered by theta, go into action. Gamma waves connect information drawn from various parts of the brain that are involved in music-making, allowing skill learning, procedural memory and self expression to settle into a rhythm.\nSecondly, synchronization between the left and right hemispheres or brain coherence is another marker for flow. Both hemispheres must be working complementarily to integrate artistic expression and technical skills. Cardio exercise, meditation and yoga along with brain-based clinical techniques like Eye Movement Desensitization & Reprocessing (EMDR) all promote brain coherence through bi-lateral stimulation.\nFinally, a temporary brain state called transient hypofrontality has been identified that enhances flow by lowering the activation of DLPFC. This part of the brain hosts our inner critic, our voice of self-doubt that can trigger cognitive anxiety. Cardio exercise re-directs blood flow away from the DLPFC to the motor parts of the brain, enabling a more embodied focus without interference from self-consciousness, distraction or negative thinking.\nThese findings can be applied to mental skill training that has been the hallmark of sport psychology over the last fifty years. The six key skills are relaxation, imagery, goalsetting, self talk, concentration and pre-performance routines.\nRelaxation is the first key because performance anxiety usually inhibits peak performance. Exercise is a basic treatment for all types of anxiety. Daily meditation over a minimum of eight weeks reduces both state and trait anxiety by lowering the resting heart rate and enhancing brain plasticity. Neurofeedback (EEG biofeedback) is now being used to train the brain to enter the alpha/theta brain states on demand.\nImagery engages the power of the senses, especially visualization, to mentally depict what peak performance should look and feel like. Cardio Imagery & Rehearsal is a new technique that combines mental rehearsal with moderate cardio exercise (120 – 140 HR using an elliptical trainer or stationary bike) to prime learning and reinforce process goals. Mental rehearsal is effective because mirror neurons activate various muscle groups via the peripheral nervous system almost as much as with physical practice.\nGoalsetting is a motivational tool for directing one’s efforts toward optimal learning. Goalsetting supports deliberate practice that encourages musicians to concentrate their efforts on their most challenging repertoire. Exercising in the morning before practice while mentalizing on what needs work helps identify practice goals and primes the brain for learning later on.\nSelf-talk reveals the psychological relationship between the person and the performer such as having a positive outlook and being mentally tough when under stress. Research shows that positive thoughts and feelings promote creativity whereas negative emotions stimulates critical thinking that can lead to self-consciousness. Not surprisingly, having a positive mental attitude is a key component of flow.\nConcentration emphasizes attentional skills and mental discipline to focus on the challenges involved in musical performance. The mind must be fully engaged in the moment, free of distractions and immersed in the task. Quite simply, the best way to build focusing skills is to learn to live in the moment ! Not so easy, as many of us have found out !\nFinally, pre-performance routines allow musicians to find that groove that puts them in a positive performance mindset. The key tools are breathing and centering exercises, finding one’s optimal zone of activation and converting pre-performance jitters into excitement.\nUltimately, playing music in the flow state is it’s own best reward, one reason why musicians are so passionate about pushing their musical boundaries. So when it happens, embrace it!']"	['<urn:uuid:c09d0377-db93-4912-b03f-cdd793e7852d>', '<urn:uuid:ded49236-5435-47e1-8adb-7066474db550>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	6	54	1456
16	I'm interested in farming technology - what kinds of innovative tools and systems are being developed in New Zealand to help farmers work more efficiently while protecting the environment?	New Zealand is developing several innovative farming technologies. Ballance Agri-Nutrients has created two patented products: N-Guru, which helps identify areas of farms that will respond better to nitrogen application, and SpreadSmart, a precision aerial topdressing technology that uses GPS and computerized mapping to control fertilizer distribution and avoid sensitive areas like waterways. Additionally, new research projects are exploring technological solutions, including a government-funded study on using specific native plantings to naturally increase beneficial insects for pollination and pest control, which could reduce pesticide use. These innovations are part of broader efforts to improve farm productivity while maintaining environmental sustainability.	"['Ballance Agri-Nutrients is a farmer-owned co-operative supporting New Zealand farmers to operate more productively, profitably and sustainably. CEO, Mark Wynne reflects to NZ Manufacturer on the way ahead and staying close to farmers.\nWe’re close to a $1 billion business with roots that go right back to the 1800s when sulphur was mined at White Island in the Bay of Plenty and shipped to Tauranga for refining. Across our long history, our reason for being has remained the same – ensure farmers have sustainable, affordable access to quality nutrients to support their farm’s productivity.\nFrom our core business of fertiliser manufacturing, supply, sales and advice, we have grown to offer farmers a full suite of science-backed nutrient products and services. This enables us to help farmers achieve soil and animal nutrition and productivity with a minimal environmental footprint.\nOur agri-nutrient advice is complemented by award-winning technology products which provide a wealth of information to enable farmers to make the best management decisions for their business.\nWe cover the country, with one-on-one specialist advice on-farm backed up by a network of rural merchant supply partners, services centres, consignment stores, manufacturing sites and our own agricultural aviation services.\nAs a co-operative, we’re owned by farmers and they share in our success receiving annual rebates on fertiliser purchases, a share in our profits through dividends and increases in the value of their shares in us. In 2015 we distributed $76 million to shareholders. We’ve gone from strength to strength with more than 19,000 farmers signed on as shareholders. Our shareholders’ equity is $435 million and has grown by 31 percent over the past five years.\nMade in NZ for NZ\nWe source raw materials from around the world and within New Zealand to produce high quality farm and animal nutrients designed for New Zealand soils and farming conditions. Last year we sold 1.75 million tonnes of product and these sales, combined with other services, delivered $893 million in revenue and an $81 million profit.\nIn our core fertiliser operations Ballance has two plants that manufacture phosphate fertiliser products at Mount Maunganui and Awarua. Together, the plants produce approximately 800,000 tonnes of fertiliser each year.\nManufacturing processes are continuously revised to ensure optimum efficiencies are achieved. Our continuous improvement is delivering results, with Awarua achieving record production levels last year, while Mount Maunganui achieved the lowest controllable unit costs for production.\nBallance also owns the country’s only ammonia-urea manufacturing plant, located at Kapuni, in Taranaki. This high-tech plant operates to stringent international standards, converting atmospheric nitrogen to ammonia and then to urea. The natural gas used to power this plant is sourced from the nearby Maui gas field. Kapuni produces around 260,000 tonnes of product annually. Ballance is currently exploring development and funding options for a new plant aiming for the lowest carbon footprint of any similar facility in the world.\nAll manufacturing produces byproducts and fertiliser is no exception. We turn these into revenue streams. A good example is Go Clear, an exhaust system additive which reduces nitrogen oxide emissions from diesel engines and is used by many fleet operators including Fonterra. Scientists have calculated this additive has removed 2,000 tonnes of harmful exhaust gases from the environment.\nTo fulfil the growing demand for high quality animal feed, we have manufacturing sites in Morrinsville and Wanganui in the North Island and Ashburton in the South Island. Our SealesWinslow operation supplies custom blended pelletised feed to farmers throughout New Zealand. In addition the company produces feedblocks, milk additives, silage inoculants and mycotoxin neutralisers.\nIn order to maintain product quality, the mills have developed effective quality control systems that cover raw material sourcing, formulations, manufacturing, storage, transport and finished product testing. All sites have FeedSafeNZ accreditation which provides assurances on the quality of stockfeed.\nBacked by Sound Science\nEverything we do is backed by sound science. Our R&D team focuses on the development of products and services that improve the efficiency of our soil and animal nutrition products and our technical team ensures the best scientific advice is put in farmers’ hands.\nThis includes have specialist teams of scientists in key areas like animal nutrition, soil and agricultural science to advise farmers.\nWe work in close association with AgResearch, HortResearch, Crop and Food, Landcare Research and independent scientists, drawing on their expertise as we contribute ours. Our work includes research in local conditions to provide the results evidence farmers look for in the products we recommend.\nWe’re just past the half way mark in our seven year $19.5 million Clearview Research Programme which has funding support from the Primary Growth Partnership. Its overall aim is to increase the efficiency of fertilisers farmers depend on while minimising their environmental impacts.\nTwo patented products are already in use on farm – N-Guru™ and SpreadSmart™. Used in conjunction with soil testing, N-Guru™ enables farm consultants to interpret the test results to identify which areas of a farm will produce a greater response to nitrogen than others. SpreadSmart™ meanwhile is precision technology developed for fixed-wing aerial topdressing. It combines GPS guidance and tracking systems with computerised farm mapping to automate the opening and closing of an aircraft’s fertiliser hopper at the right time, in the right place, minimising losses and maximising results.\nIt keeps fertiliser out of waterways, gullies, forestry blocks, tracks and other no-go zones and allows for different fertiliser products to be specifically targeted to different areas of the farm, improving overall production. It also combines variable rate application, so different parts of the farm get exactly the volumes of fertiliser they need. With fertiliser one of the largest on-farm costs, precision applications also mean budgets go further, improving farm margins.\nSerious about Sustainability\nFarming means everything to us. Farmers are our owners and our customers and their work contributes to the New Zealand economy. We’re big investors in farming’s sustainability.\nWe’re the lead sponsor of the Ballance Farm Environment Awards which show productivity and profitability don’t come at the environment’s expense. This is more than a farming competition. It’s about supporting the global competitiveness and reputation of our pastoral industries. New Zealand could produce enough food to feed around 40 million people if every farm performed at the same levels demonstrated by our award winners. We think that’s well worth supporting.\nFarming is often an inter-generational business. We provide up to four scholarships a year to encourage young people to enter the agricultural industry. Each scholarship is valued at $4,000 per annum and can be held for a maximum of three years.\nProud of our People\nBallance employs more than 700 people in a wide variety of roles, ranging from PhDs in agricultural science to chemical engineers, marketing experts, health and safety specialists and production workers. Great people made a great company, so we emphasise attracting and developing the best people we can find.\nWe’re firmly committed to developing leaders worth following and giving every employee the opportunity to be the best they can possibly be.\nEvery day we strive to make our operations a safe place for both staff and visitors and we are committed to a zero injury rate. Attention to safety has earned us the highest level of ACC accreditation.\nIf we’re not thinking about the future, we can’t help farmers prepare for it.\nFarming in the future will look pretty different. We will see more consolidation – bigger farms, and they will be working within tighter limits – think controls on water takes, nutrient losses, animal welfare – some of these limits will be via legislation, others will come from consumer pressure.\nIn response to this, we’ve focused on three areas: firstly, staying close to farmers, understanding their needs as market changes rapidly around them. Secondly, delivering best advice for farmers can remain sustainable and profitable as limits increase and change. Thirdly becoming involved in the legislative process, advocating for farmers and ensuring sound science is behind limits.', ""Regenerative agriculture research receives Government boost\nThe Government continues to invest in farm sustainability, this time backing two new research projects to investigate the impacts of regenerative farming practices, Agriculture Minister Damien O’Connor announced today.\nSoil health and regenerative agriculture\n“We’re contributing $2.8 million to a $3.85 million five-year project with co-investment by Synlait Milk and Danone that aims to understand how to measure and manage soil health to boost environmental and economic performance on New Zealand farms,” Damien O’Connor said.\nThe announcement coincides with World Soil Day, which aims to raise awareness of the importance of maintaining healthy ecosystems and human well-being by addressing the growing challenges in soil management, fighting soil salinization, increasing soil awareness and encouraging societies to improve soil health.\n“We simply cannot take soil health for granted. It’s the basis of our food systems, and also New Zealand’s economic health.\n“AgResearch will work with Synlait Milk and Danone supplier farmers on the project, which will run across ten commercial dairy farms in Canterbury, Southland, and Waikato.\n“In each region the farms will be paired for comparison based on location, soil type, and farm performance. Soil health will be measured within each farm, with one paddock in each managed conventionally, and another using regenerative farming practices.”\nNative plantings to boost farm yields\nThe second project sees the Government contributing $2.2 million to a five-year research project aimed at boosting New Zealand farm yields by attracting beneficial insects to farms using specifically designed native planting.\n“By looking at the relationship that specific native plants have with insects, the researchers will be able to see which ones increase pollination and tackle pests most effectively.”\nBased in the Canterbury region, the project is being led by Plant and Food Research, with a range of industry partners investing just over $1 million into the work.\nForty-five Canterbury farms will plant land that is currently under-utilised – such as fence lines, road verges and watercourses – with local native plants that have been proven to support and maintain beneficial insect life.\n“By increasing the number of pollinating insects, the project team expects to see increased yields across the farms, which will lead to improved economic and environmental outcomes for the farmers involved.\n“Increasing the number of predator insects to tackle pests could also reduce the use of pesticides and insecticides.”\nSustainable Food And Fibre Futures Fund\nBoth projects are being funded through the Ministry for Primary Industries’ Sustainable Food and Fibre Futures fund (SFF Futures).\n“To date we’ve co-invested more than $160 million through SFF Futures into 171 industry projects worth almost $355 million in total.\n“There’s a surge of interest in regenerative agriculture in high-value markets overseas. These projects are just two of a portfolio of ‘regen’ projects we are backing through the fund.\n“Collectively they’re contributing to growing our evidence-base on the effectiveness of applying regenerative agriculture practices in New Zealand.\n“They align with the environmental goals of Fit for a Better World, our food and fibre sector roadmap that’s driving New Zealand's economic recovery from COVID-19.\n“Through Fit for a Better World we aim to strengthen the environmental credentials of our food and fibre products and drive further value growth,” Damien O’Connor said.\nTo read more about World Soil Day 2021 visit the Food and Agriculture Organization of United Nations website.""]"	['<urn:uuid:9c3abf5d-4066-45dc-8dee-e5dede7d7221>', '<urn:uuid:b031f79a-65cb-4276-8c7a-e5143be99626>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T01:37:09.743768	29	99	1869
17	I'm fascinated by genome sizes in plants - how can the bladderwort have more genes than grapes despite having a much smaller genome, and what's the actual size difference between them?	The bladderwort (Utricularia gibba) has a genome that is six times smaller than the grape's, containing roughly 80 million base pairs of DNA. However, it actually has more genes - about 28,500 compared to grape's 26,300. This incredibly compact architecture is the result of a history of rapid DNA deletion where the plant primarily eliminated noncoding 'junk DNA' while retaining functional genes.	['Release Date: February 23, 2015\nBUFFALO, N.Y. — Great, wonderful, wacky things can come in small genomic packages.\nThat’s one lesson to be learned from the carnivorous bladderwort, a plant whose tiny genome turns out to be a jewel box full of evolutionary treasures.\nCalled Utricularia gibba by scientists, the bladderwort is a marvel of nature. It lives in an aquatic environment. It has no recognizable roots. It boasts floating, thread-like branches, along with miniature traps that use vacuum pressure to capture prey.\nA new study in the scientific journal Molecular Biology and Evolution breaks down the plant’s genetic makeup, and finds a fascinating story.\nAccording to the research, the bladderwort houses more genes than several well-known plant species, such as grape, coffee or papaya — despite having a much smaller genome.\nThis incredibly compact architecture results from a history of “rampant” DNA deletion in which the plant added and then eliminated genetic material at a very fast pace, says University at Buffalo Professor of Biological Sciences Victor Albert, who led the study.\n“The story is that we can see that throughout its history, the bladderwort has habitually gained and shed oodles of DNA,” he says.\n“With a shrunken genome,” he adds, “we might expect to see what I would call a minimal DNA complement: a plant that has relatively few genes — only the ones needed to make a simple plant. But that’s not what we see.”\nA unique and elaborate genetic architecture\nIn contrast to the minimalist plant theory, Albert and his colleagues found that U. gibba has more genes than some plants with larger genomes, including grape, as already noted, and Arabidopsis, a commonly studied flower.\nA comparison with the grape genome shows U. gibba’s genetic opulence clearly: The bladderwort genome, holding roughly 80 million base pairs of DNA, is six times smaller than the grape’s. And yet, the bladderwort is the species that has more genes: some 28,500 of them, compared to about 26,300 for the grape.\nU. gibba is particularly rich in genes that may facilitate carnivory — specifically, those that enable the plant to create enzymes similar to papain, which helps break down meat fibers. The bladderwort is also rich in genes linked to the biosynthesis of cell walls, an important task for aquatic species that must keep water at bay.\n“When you have the kind of rampant DNA deletion that we see in the bladderwort, genes that are less important or redundant are easily lost,” Albert says. “The genes that remain — and their functions — are the ones that were able to withstand this deletion pressure, so the selective advantage of having these genes must be pretty high.\n“Accordingly, we found a number of genetic enhancements, like the meat-dissolving enzymes, that make Utricularia distinct from other species.”\nMuch of the DNA the bladderwort deleted over time was noncoding “junk DNA” that contains no genes, Albert says.\nHigh gene turnover\nThe study included partners from UB, the Universitat de Barcelona in Spain, the Laboratorio Nacional de Genómica para la Biodiversidad (LANGEBIO) in Mexico and the Instituto de Ecología in Mexico.\nTo determine how the bladderwort evolved its current genetic structure, the team compared the plant to four related species. What they uncovered was a pattern of rapid DNA alteration.\nAs Albert explains, “When you look at the bladderwort’s history, it’s shedding genes all the time, but it’s also gaining them at an appreciable enough rate, permitting it to stay alive and produce appropriate adaptations for its unique environmental niche.”\nIn the realm of DNA gain, the study found that U. gibba has undergone three duplication events in which its entire genome was replicated, giving it redundant copies of every gene.\nThis fast-paced gene gain was balanced out by swift deletion. Evidence for this phenomenon comes from the fact that the plant has a tiny genome despite its history of genetic duplication. In addition, the plant houses a high percentage of genes that don’t have close relatives within the genome, which suggests the plant quickly deleted redundant DNA acquired through duplication events.\nThe study was supported by the National Science Foundation. It builds on the work of Albert and other team members, who reported in the journal Nature in 2013 that the bladderwort’s genome was comprised almost entirely of useful, functional genes and their controlling elements, in contrast to species like humans, whose genomes are more than 90 percent “junk DNA.”']	['<urn:uuid:75a6e0cd-8172-4392-909f-e0b35d07c056>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	31	62	737
18	What special features help both museums control their environment?	The Museum of Western Regions uses thick walls and small windows to protect against extreme desert temperatures (both scorching heat and extreme cold), and has fortified walls designed for strong winds. The Len Lye Centre, being an art gallery, incorporates highly-serviced HVAC systems and environmental controls to meet strict international artwork loan standards, with its design allowing for efficient ducting through a single plant room.	['Project NameDongZhuang-Building Museum of Western Regions\nArchitecture PracticeWind Architectural Design & Research Institute Co.\n|Project Name||DongZhuang-Building Museum of Western Regions||Posted in||Architecture, Cultural||Location||\nWestern Ecological Environment Research Center, 23 Baijiangou Village, Tuoli Township, Urumqi County, Xinjiang\n|Architecture Practice||Wind Architectural Design & Research Institute Co.||Completed||July 2016|\nAmong the design principles that the architects of the Museum of Western Regions in China’s Baijiangou village, in Xinjiang, mention, this one probably stands out the most: “Good-looking is not beauty, and even beauty often lasts for a short while with the shift of aesthetic trends. A pleasant and well-adapted building with multiple functions is a good building, and durability is a synonym of history, symbol and localism. It is preferable for a house in the desert hinterland to emerge in harmony with the nature and perish of itself as time goes by.” Is this a manifesto? Or is it a preemptive strike against any possible criticism of the building, stemming from a western and alien sense of beauty?\nIn any case, there is something challenging about this building in the Gobi Desert. Even before the whole question of its aesthetics comes into play, even before it was itself constructed, this 7,700 m2 building was destined to find itself in the middle of an enigma, as the area is known as either of two opposites: To most Chinese, it is called the “Western region”, while to Westerners it is called the “East”. To be sure, the distinction is false. It depends on something which is not constantits observer’s own location. This, however, may very well be the full extent of subjectivity in anything about this building that now stands at the site of an old grain shop.\nFinely attuned to its environment, the Museum faces the city to its south, and has the mountain to its back in the north, where the wall is designed to expect the strong winds for which it has been fortified. Small windows and thick walls protect against the climate’s elements, which can be scorching heat and extreme cold alike. In regards to its interior, the building has been designed as though it were made up of two distinct spaces: a guest house and a museum. The former contains a private collection, while the latter is open to the public. Traditional building materials and techniques were used, making their product one of conscious integration in history and the day-to-day reality of the region. This is not a delicate construction of sophisticated intricacy, but rather of calculated simplicity. Each of its features corresponds to either a natural need or to a custom, all the way down to the bird’s nest found to the building’s south: Swallows used to nest there before the museum’s construction, which briefly had deprived them of their home, but made sure to welcome them back. It is the same philosophy that makes the Museum look as much as an observatory, with its dome and windows, as a refuge, forbidding and inaccessible.\nThere appears to be no true allegiance to either image. Most probably, the true meaning of its design is exactly that it should “emerge in harmony with the nature and perish of itself as time goes by”, as its architects suggest. Meanwhile, its skylights, stairs, environment-conscious design, in fact so many of its features, offer a meaningful pause. For the building’s aesthetic might be to look as bare as the desert, but it is nowhere near as barren.', 'ARCHICAD 20’s shimmering signature building, the Len Lye Centre, is New Zealand’s only single artist museum. Its design is deeply influenced by the life, ideas, writings and work of the famous New Zealand Kinetic Artist, Len Lye (1901–1980).\nLocation: New Plymouth, New Zealand\nCost: AUD 11.5m, USD 8.3m\nYear of Completion: 2016\nSize: 3,000 sqm | 32,292 sqft\nArchitect: Patterson Associates\n- GRAPHISOFT ARCHICAD\n- GRAPHISOFT BIMx\n- Revit Structures\n- Revit MEP\n- 3ds Max\nLye himself said in 1964, that “great architecture goes fifty-fifty with great art.” The building’s design articulates this philosophy – he believed an artwork was a piece of consciousness communicating with the future. He used light as his primary medium.\nThe Len Lye Centre is situated in Taranaki, New Zealand, a region known for its stainless steel production industries. The 3,000 sqm (32,292 square feet) building engages with the urban square via a light reflecting and transmitting façade. This transfers light in a holographic effect from one place to another to enliven and activate both.\nPhoto ©Patrick Reynolds\n“[Len Lye would] be absolutely thrilled, he’d be dancing around in this place.” John Matthews, Chairman, Len Lye Foundation\nThe Len Lye Centre is designed to fulfil multiple requirements. It needed to integrate with a proposed new art and cultural precinct, which links through to the commercial heart of the city. It also needed to seamlessly merge with an existing heritage Art Gallery facility in a converted movie theater. The finished building introduces new gallery spaces, education studios, a 62 seat cinema, the Len Lye Archive and a dedicated motor room for his kinetic works. The resulting space is reverential, creating a sensory experience from light as a ‘temple’ for art.\n“Lye was fascinated with temples and in conceiving the overall design it seemed appropriate to draw inspiration from the “megarons, or great halls, of the classical world, as well as from Polynesian forms and ideas. These ideas also influenced Lye. To do this, we developed our thinking in a holistic or adaptive way; as a systems methodology. This ment that rather than using proportion or aesthetics, we used patterns in the ecology of the project’s environments to drive the design elements. For example, the shimmering, iridescent colonnade façade is manufactured locally using stainless steel - Taranaki’s ‘local stone’ – this links both Lye’s innovations in kinetics and light as well as the region’s industrial innovation. So by using stainless steel we can celebrate and communicate to visitors the reason for the fortunate gift of his art works to Taranaki.”\nsays Andrew Patterson, director, Patterson Associates\nPatterson Associates | Photo ©Davor Popadich\nThe façade’s deeply pocketed shape provides a series of images or reflections and attracts people to the facility as a result. Viewed from above, the colonnade’s top edges create a local koru form, displaying the Museum’s Polynesian influences.\nInside the building, the colonnade creates a theater curtain, with three asymmetric ramped sides embracing the space of a processional journey ascending along a ramp into various galleries. There are divisions in the circulation. Light is drawn inside through the apertures in the colonnade, and these create moving light patterns on the walkway, perhaps a form of passive kinetic architecture.\nTeamwork and the modern colonnade\nPatterson Associates’ 28-member team is united and motivated by architectural innovation, bringing their diverse expertise together to collaborate. A broad range of disciplines come together in their architectural work. Each project comes from selecting individual talents to create the right ‘super team’ for particular projects. For the Len Lye Centre, four out of the practice’s 14 ARCHICAD users formed the super team working together closely using ARCHICAD’s Teamwork.\nThe façade of the len Lye Centre consists of 20 monolithic 14 meter-high curved, pre-cast concrete columns. Their exterior surfaces are clad in highly polished stainless steel curtains with glazing hidden in between each column. The shapes of the columns are designed to reflect light through these into the building according to the time of day and reflect light back out at night. Scale, materials, light and atmosphere are composed as in a film to activate the senses and incite a poetic dimension. The façade acts as a kind of living object connecting the outer and inner spaces it divides.\nStarting the project in 2010 with ARCHICAD 14, the design team’s biggest challenge was the modeling of this façade. Due to the complexity of the geometry they used Rhino to model single façade shapes, then exported to ARCHICAD to link the modules together. Later, when ARCHICAD 15 and 16 were introduced, the façade was remodeled using the Shell and Morph tools.\n“Using the Teamwork file has always been fantastic; the late change of color code identity message within Teamwork is great.” Shane Taylor, BIM Manager, Pattersons\nThe design team built a detailed, live, 3D model, integrating the 3D structural and MEP elements as well. This model was used to extract all of the project documentation, to communicate the design intent towards the client, and to clarify construction issues during the two years of construction.\n“BIM is a tool that we use to communicate with stakeholders, with the builders, with everybody connected to the project. It’s a leadership tool that enables us to deliver the project.” Andrew Patterson, Director\nThe role of OPEN BIM in the design and construction\nPattersons recognized early on the importance of a strong OPEN BIM foundation for their design and construction workflows.\nOPEN BIM was fundamental in working out the complex geometry of the building in three dimensions, ensuring a seamless transition between consultants from design to manufacture.\nOnce the geometry of the façade was ready in ARCHICAD, it was exported to Ecotect 2011 in conjunction with Radiance to document sun path reflections. The architectural model of the colonnade columns was transferred directly to the pre-cast manufacturers, while the light transferring effect was tested on site with real size sample elements.\nSetting protocols early in the project between BIM managers of each consultant was key. As this was a test project using the IFC translator for some of the consultants, a number of demo models were exchanged between consultants to confirm that future exchanges would be seamless. OPEN BIM data exchange was implemented and data shared on a weekly basis.\nIFC import-export for mechanical and structural work was critical to the success of the project. The imported mechanical and structural models remained in the architectural model for reference only (hidden layer), then the structural engineers concrete components and the MEP elements were re-imported into the model and took precedence on the BIM model.\nThe final model was exported to SketchUp to allow the Gallery staff to place artwork within the building, forward planning the exhibitions two years before the gallery actually opened. For rendering and animations, the model was exported to 3ds Max of Autodesk.\n“The function of IFC open file format has greatly eased the collaboration process between the architect and the engineers.” Shane Taylor BIM Manager, Pattersons\nBIMx on the construction site\nSince the Len Lye Centre was fully modeled, it was an easy and natural step to generate a BIMx model from it containing all the 2D information available in the model. Patterson Associates has used BIMx since the day it first came on the market.\n“The Len Lye Centre’s BIMx model was used during site inspections, while the builders used it when making site rounds, allowing them to visualize the building three dimensionally when resolving complete junctions. And the tool also came in handy when the client required simulations on how the designed museum spaces would work in specific event scenarios.” Says Shane Taylor, BIM Manager, Pattersons.\n“This is a fantastic tool that openly and interactively explores 3D Building Information Models on mobile devices. It saves us from having to carry big paper files to the site!” Shane Taylor BIM Manager, Pattersons\nValue management with ARCHICAD\nAn art gallery project always requires a highly-serviced environment, resulting in very high MEP – mainly HVAC – costs, which can be as high as 25% of the total building cost. ARCHICAD was indispensable when Pattersons value-managed the project due to funding difficulties they were able to coordinate the various HVAC engineers and sub-consultants to reduce that cost significantly just by the way the ducting worked through the building.\nThe very strict international artwork loan standards narrow down the environmental range the museum has to comply with. With an art museum this size, this usually means two to three plant rooms. Once Pattersons decided to move from the initially planned two plant rooms to one, they could implement this change very quickly via the exchange process between consultants, saving a major portion of the building budget.\nBy using state-of-the-art BIM tools, Patterson Associates have created a concept design that has resulted in a breathtaking, inspiring building, reaching far beyond the borders of New Zealand. Tasked with a number of requirements, the Len Lye Centre is a building for the future, firmly rooted in the community and the vision of the artist who inspired it.\nAbout Patterson Associates\nCongratulations to Andrew Patterson for winning the New Zealand Institute of Architects 2017 Gold Medal!\nPatterson Associates is a New Zealand practice. Its philosophy is based on a very simple idea: if a building can feel like it naturally ‘belongs,’ or fits logically in a place, to an ecology, a time and culture, then the people that inhabit the building will likely feel a sense of belonging there as well. This methodology connects theories of beauty and environment. As well as its New Zealand work, Patterson’s have clients and projects as far afield as India, Singapore, Australia, Germany and China.']	['<urn:uuid:7233fbbd-2029-49f7-9049-39f42e94c19d>', '<urn:uuid:fc9c8972-b764-43ac-99d9-559b9a3ea53b>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T01:37:09.743768	9	65	2179
19	I heard about a building in Chandigarh that uses very little energy - I'm wondering how much energy per square meter per year does the Punjab Energy Development Agency office building use?	The Punjab Energy Development Agency office building uses 14 kwh per square meter per year, and has been recognized as the most energy efficient building in the country by the Government of India.	['Project Punjab Energy Development Agency, Office Building, Chandigarh\nPUNJAB ENERGY DEVELOPMENT AGENCY\nIS NOT BASED ON ANY\nPRE-CONCEIVED NOTION OF\n• IT EVOLVES BASED\nON LAWS OF\nPERCEIVED SUN PATH\nSUN PATH IS DYNAMIC & CURVILEANER WHY DO\nWE DESIGN BUILDINGS AS CUBOIDS\n• Entire building envelope a : Solar Receptor, Converter, Dissipater\n• SOLAR ENVELOPE\n• SOLAR ENVELOPE CAN BE DESIGNED WITH COMPONENTS RESPONDING\nProduce direct energy through PV.\nDistribute day-light, within the building, creating a healthy day-lit\nenvironment and in open spaces.\nCreate ventilation: Solar Chimneys\nTo cool or heat the building, when coupled with earth tunnels using the\nthermal inertia of the earth and Trombe walls, in innovative approaches.\nDOUBLE SKINNED BUILDING ENVELOPE\n-HEAT BARRIER THROUGH INSULATION / DEAD AIR CAVITY\n-T.A.P. (THERMOSYPHON AIR PANEL)\n- THROUGH SOLAR / MECHANICAL ACTIVATION\nTHERMAL INNER SKIN\nINTERACTS WITH INTERNAL\nSTRUCTURAL OUTER SKIN\nK14 CSP Case Study Project : Opriya Mohanty UNVERSITY OF NOTTINGHAM U.K.\nPerformance of downdraught evaporative cooling system in atrium and its neighboring\noffice space, (PEDA) building – Chandigarh, India.\nAn investigation into the contribution of PHDC system in\nthe atrium and its space conditioning systems to the\nthermal comfort in their adjacent office spaces in the\nPunjab Energy Development Agency building (PEDA).\nThe Office complex building for PEDA at Chandigarh, India is\na project aimed at demonstrating an architectural design\ndeveloped in response to elements of nature : Sun, wind,\nlight and the utilization Of Solar Passive Systems.\nContext & Site microclimatic analysis\nArchitectural building design needs to respond to the\ncomposite climatic context of the site. The final design\nsolution needs to satisfy the diverse and often conflicting\nconditions of a hot-dry, hot-humid, temperate and cold\nperiod of Chandigarh.\nThe climatic periods : two months of hot-dry, hot-humid\n(two months) and cold period (two months) require\nstrategies of design that allow for cooling in the hot dry\nperiod, natural ventilation in the hot-humid period and\nheating in the cold period. Cooling remains as the\npredominant requirement since the total over-heated\nperiod extends from mid-April to mid-August.\nThe comfort temperature varies during whole year, but the\nmaximum comfort temperature in summer varies from 29 c\nto 31 c when external temperature varies from 40 c to 45c.\nhence we need to design the building to achieve\ncomfortable temperature by keeping the highest or hottest\nweek of the year in mind to avoid the overheating inside\nTemperature and rain fall variation\nAverage Insolation Summer\nWIND SPEED AND DIRECTION\nshould be taken in to consideration while designing the wind\ntower and building openings,\nArchitect has taken priority during placement of wind tower\nas well as the openings for better movement of air flow\nOver shadowing analysis\nBuilding is not overshadowed by its surrounding building, due to its\norientatiion and well designed facades ,building is having its own\nshadow during summer which helps to minimise the over heating.\nShell roof in atrium space to minimize overheating and\nVentilation system in atrium and office space\nthrough wind tower and solar chimney\nduring summer day and night\nDaylight study in U. Ground office space\nSimulation through PHDC air\nPASSIVE DOWN DRAFT INDIRECT+ DIRECT EVAPORATIVE\nCOOLING COUPLED WITH EARTH AIR TUNNEL UNDER THE\nWATER BODY ( Architect’s design idea)\nThe comfort temperature range varies from 24 to 31 during different seasons of the year. Comfort relative humidity range 30to 80 %. Required air flow rate as from manual calculations can be 3.5 m3/sec. so the air flow rate can be\nachieved in all the cases. Required airflow rate is very low due to less occupancy and equipment gain. Good thermal mass of the building helps the building to perform in extreme conditions. Appropriate building design and orientation\nhaving properly placed building elements reduced or minimize the solar gain in summer. Features like light Vault, Solar chimney, Hyperbolic parabolised atrium roof help to minimize the solar gain. Instead of designing indirect plus direct\nevaporative cooling system proposed by the architect, PDEC system can be used to make the building favorable place to work. From the above results it can be concluded that the temperature, relative humidity and airflow rate increase\nfrom case 1 to case 3 which leads to unsuccessful result. Hence the central atrium space in between neighboring offices is working properly having PDEC system with one tower and it would be the best option to work with. From all the\ncases we can conclude that ground office space also getting an ambient temperature during summer.\nComparison study of different cases\nResult - Hottest day 161\nEvaporative cool towers work best with open floor plans that permit the air to circulate through out the building without any obstacles. From the TAS result it can be concluded that the atrium\nspace is working properly with application of PDEC system by getting 10-12°C lower than ambient temperature, which also helps to cool the neighboring south office space to maintain a\ncomfortable space to work. From all evidences Punjab energy development agency (PEDA) can be called as a passive solar complex. Overall the building performs well whole of the year except\nthe peak summer period which can be improved through the evaporative cooling system.Hence Without any major changes in building profile and structure ,we can rebuilt the existing wind\ntower and solar chimneys by incorporating the PDEC system ,which can perform better during hot and dry summer days.\n•Evaporative cooling towers work best with open floor plans that permit the air to\ncirculate through out the building without any obstacles.\n•Good thermal mass of the building helps the building to perform in extreme\nconditions. Appropriate building design and orientation having properly placed\nbuilding elements reduced or minimize the solar gain in summer.\n•Elements like light Vault, Solar chimney, Hyperbolic parabolised atrium roof help\nto minimize the solar gain.\n•From the TAS result it can be concluded that the atrium space is working properly\nwith application of PDEC system by getting 10-12°C lower than ambient\ntemperature, which also helps to cool the neighboring south office space to\nmaintain a comfortable space to work.\n• From all evidences Punjab Energy Development Agency (PEDA)\noffice building can be called as a passive solar complex. Overall the\nbuilding performs well whole of the year. Evaporative cooling is\nrequired in May & June\n14 kwh /sq.m/per year\nGOVT. OF INDIA\nMost Energy Efficient Building\nin the country\nTribune : May 2004\nTribune : Jan 2004 HRH Prince Charles Visit October 2010\nRound table discussion\nTO MY MIND\nEVOLVES AT MEETING POINT:\nWHEN ART BECOMES SCIENCE AND SCIENCE\nAT THIS POINT\nLOFTY BECOMES MUNDANE']	['<urn:uuid:2635d976-67a2-4abc-96cc-00b2b4e7446c>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	32	33	1093
20	specifications regulations requirements shower curb height ada compliant installations	"According to ADA guidelines, shower stalls that measure 36"" x 36"" must have curbs no higher than 1/2"" in height. For shower stalls that are 30"" x 60"" minimum, curbs are not permitted at all. This is significantly different from traditional bathtub walls and is designed to ensure accessibility."	"['Safety first: keep water inside barrier-free showers.\nBathrooms are certainly a focal point of these watery issues. When you consider ADA requirements for accessibility and the movement toward universal design, we are seeing more low-threshold and barrier-free showers being installed in new facilities and during remodels of pre-existing buildings.\nLow barriers of entry and access are critical in these types of facilities. Taking away the bathtub wall we have grown used to stepping over removes a major risk for falls, especially for those with mobility issues. Barrier-free shower units are especially useful for residents with disabilities. They ate designed for ease of access for people who find using a traditional shower very difficult, or impossible based upon Their disability. These shower units blend in with the floor so there is nothing to step over. They also enable wheelchair access. Low-barrier showers are not only safer, but they give residents the opportunity to bathe themselves independently and with dignity.\nADA guidelines state that ""curbs in shower stalls 36"" x 36"" shall be no higher than 1/2"". Shower stalls that are 30"" x 60"" minimum shall not have curbs."" This is a far cry from the water barrier that a traditional bathtub wall provides.\nAccording to Cal Osborn, of ADA-compliant shower and walk-in tub manufacturer Best Bath Systems, water containment is an issue that comes up quite a bit. ""A question we are often asked is, \'Doesn\'t the water splash out and run all over the floor outside the shower?\' Not if you have the right system in place,"" he answers.\nA water dam, or waterstopper, has become the preferred barrier-free shower accessory used today. This T-shaped strip is made of durable molded rubber that flexes undetfoot or when a wheelchair rolls in and out of the shower. It quickly springs back to its upright position as the bather passes in and out of the shower. The waterstopper can be trimmed to fit across the threshold of any barrier-free shower, and is typically installed with epoxy adhesive and fitted into end caps where the floor and wall meet.\nA water dam alone will not prevent water from escaping the shower. Osborn recommends installing a heavy-gauge vinyl shower curtain that has weighted tape sewn into the bottom hem. The curtain needs to be of a length that the bottom is approximately 1/2"" or less above the shower floor. It should be aligned from above so that when it is closed it rests along the inner side of the water dam. A curtain that hangs from a sturdy curtain rod also provides something else to grab hold of if the bather loses balance.\nOne alternative to diverting water flow is to channel water away through a flat drain, or water gully, fitted along the shower entrance, although this type of setup doesn\'t provide the vertical barrier that a water dam does and is more costly to install. Triangular foam water dams are another option, but have been shown in some cases to tear under the weight of a wheelchair. Other options include spring-loaded metal water dams and trackless shower doors.\nIt\'s imperative to have the proper products in place to keep water within your barrier-free showers. Combined with a comprehensive and proactive cleaning and maintenance program, you will sleep better at night knowing that your bathrooms are safe and dry.\nAt a glance ...\nWith falls being the leading cause of injury in adults 65 years of age and older, it is imperative to keep water inside barrier-free showers. This article will suggest ways to accomplish that.\nDave Jansik is a freelance writer based in Boise, Idaho. To send your comments to the editor, e-mail email@example.com.\n|Printer friendly Cite/link Email Feedback|\n|Date:||May 1, 2010|\n|Previous Article:||Encouraging words: residents send messages of hope and love to wounded soldiers.|\n|Next Article:||Colonial flavor, contemporary care.|']"	['<urn:uuid:7696c5a3-78fa-4b1f-ae76-98e10fac7b62>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	9	49	638
21	What are the key differences between traditional recipe ownership concepts and modern intellectual property protections in the food industry?	Traditional recipe concepts emphasize that recipes are part of the public domain and belong to no one, being passed through generations and available for commercial use. However, modern intellectual property law provides several protections for culinary creations. While a basic recipe might not be protected, there are five relevant intellectual properties in food creation and commercialization - trade secret, copyright, trademark, right of publicity, and patent. Four of these provide automatic protection without cost or paperwork. Restaurants and chefs can protect various aspects of their culinary business, from distinctive decorations to recipe presentations, under federal copyright law and state and federal trademark law. The protection is determined on a case-by-case basis, depending on how much expressive content is included in the recipe.	['Creating dishes is an important ability for any person involved in the food market, particularly in the food solution field. There are many advantages to creating recipes, including raised sales and better client satisfaction. As an example, you can end up being a cookbook author, e-newsletter editor, or perhaps an area nutritional expert, which will certainly allow you to serve individuals from all histories. To be an effective recipe writer, you must understand how to compose directions that are clear and simple to comply with. Your dishes should be evaluated and also specific to your audience. In addition, they need to be visually enticing, as they are one of the most fundamental part of the preparation.\nWords “recipe” is originated from the Latin word, recipere, suggesting “to take”. Today, it is most generally shortened R or Rx. The word recipe came from Whitman, Massachusetts, and also was named for the Toll House. The dish is widely called a tollhouse cookie, since the Toll Residence was the initial resource of the recipe. Simply put, a dish is a collection of guidelines for preparing a food thing. In the case of food preparation, the word can describe a food preparation publication, formulary, or medical prescription.\nPrior to the nineteenth century, the word “recipe” was much more generally utilized in the English language. It originates from the Latin word recipre, which means “to take”. The word recipes is abbreviated as Rx. Another name for a recipe is a recipe book. Consequently, a dish may likewise be referred to as a dish book. A recipe book is a collection of guidelines that you can follow to prepare a meal.\nDishes become part of the general public domain name and come from no one. This indicates that they are offered for industrial use as well as are not had by any individual. This is the reason that they are so popular today. The dishes you find in cookbooks have actually been given via generations. You can even make some dishes with the help of your own family members, yet do not utilize somebody else’s dish. It will certainly not coincide. You ought to always seek the most effective ingredients for your recipe.\nA recipe is a set of directions that you need to prepare a meal. This is an overview to the preparation of any type of food. A dish is a great way to showcase your skills in the kitchen area as well as boost your sales. The key is to develop an environment where you’re proud of your food preparation. It’s important to have a sense of ownership. This will help you stay clear of errors. When you get a book, it is necessary to seek a copy that is without piracy.\nWords dish is additionally a word made use of to describe a dish. A recipe is a record with guidelines for just how to prepare a certain food. The word recipe suggests “to consume.” In the cooking world, a recipe is a food item that you make on your own, yet a person can also prepare it. A cookbook includes dishes of various type of foods. It will certainly give you info on the appropriate prep work of numerous ingredients. A cookbook is a book that is a created document with details instructions.\nAs an American food author, I’ve involved understand the relevance of recipes. I’m a huge fan of food preparation as well as I like to consume, so I have actually invested years finding out the art of dish writing. I have actually also operated in a kitchen area for a couple of years, so I know what it takes to produce an impressive dish. I’ve discovered that many readers locate the procedure incredibly satisfying as well as I’m proud to share my tricks with you.\nA recipe is a set of guidelines that a cook follows to prepare a specific recipe or beverage. It can be as straightforward as adhering to a dish or as made complex as following an intricate procedure. It can be used as a memory aid, a teaching tool, as well as an ethnographic record. A recipe book can function as a marketing tool for a chef, restaurant, or cooking school. And it can help promote a service. It can even aid promote a chef or dining establishment.\nDishes are a common form of literary expression. A good example is a food blog. These web sites typically consist of a personal story that supports the recipe. A good dish can have a lot of personal meaning. An individual story or explanation can assist readers attach to the author. It can also be practical for a relative or good friend. So what are the benefits of recipes? So what makes an excellent dish? Let’s examine the numerous kinds of recipe books.\nA dish can be safeguarded under the copyright legislations. Sometimes, it is feasible to make use of a suggestion to create something new. Nevertheless, a dish that is a by-product of one more individual’s suggestions is most likely to be considered copyrighted. If you do not think the dish is your own, don’t fret. Instead, take into consideration a recipe as a creative artwork and follow your heart. The outcomes are ensured to be yummy and original.\nRecipes are a created treatment for preparing a meal. It normally contains a listing of components and thorough guidelines. The directions may include treatments to set up, mix, prepare, or cool the ingredients. The recipe may also have stuffings or toppings that include taste to the dish. Words recipe was initially made use of as a verb in clinical prescriptions. It was just after that time that the word became a part of common use. palačinky recept\nIn English, recipes are not copyrighted. They are merely composed instructions for a dish. In fact, a dish can be an art piece or a written file. For example, a recipe book can consist of a story. A dish can be a recipe book or a news article. It can also be a blog. The writer of the cookbook can share the story behind it. It may also have an image of the dish, an image, or a video clip of the recipe.', 'Lawyer says those in the dining business have more intellectual-property protection than they realize.\nFood Fight: Copycats And Recipe Thieves Threaten Chefs, Restaurants\nHis ideas had been swiped by bosses and colleagues. But Justin Barrett is most bitter about the brewery that’s selling rabbit pies like his.\nBarrett worked as head chef at a restaurant in Maine before opening Piecemeal Pies in 2016 in White River Junction, Vermont. When he left, his recipes didn’t leave with him. His former boss published them in a cookbook.\nThe only recipe he saved for himself was his rabbit pie. When he saw another Vermont restaurant serving rabbit pies baked in four-and-a-half inch spring-form pans like his, it felt like a slap in the face.\n“In my own backyard? That’s a little sleazy,” he said. But he didn’t take any legal action.\nUnlike Barrett, some larger companies do fight back against such infringements.\nGlenn Viers, vice president-general counsel of Hillstone Restaurant Group, says copycats are common in the food and restaurant industry. He’s busted restaurants for using Hillstone’s font on their menus and received anonymous phone calls from chefs admitting their kitchens use Hillstone’s recipe book.\nHillstone representatives work diligently to protect their brand when they discover such intellectual-property infringements.\n“We have as serious an interest in protecting our intellectual property as McDonald’s,” Viers said. Restaurants that copy Hillstone’s recipes or atmosphere dilute the brand.\nThe line between inspiration, coincidence and theft seems hazy for smaller and less intentionally protected businesses, such as Piecemeal Pies. Barrett’s never taken legal action against anyone who’s passed his ideas off as their own.\nFirst, he didn’t want to pay for a lawyer. Second, he didn’t think he’d win.\nPopularized intellectual property disputes have led people to believe the law fails to protect chefs and restaurant owners.\nDunkin’ Donuts differentiated their product by glazing it, omitting a filling and calling it by a different name. Despite what Cronut fans considered to be glaring similarities between the products, the discrepancies would have weakened Ansel’s legal case.\nStill, Charles Valauskas, a Chicago attorney who has represented restaurants and chefs in intellectual-property matters, says chefs and restaurant owners have more protections than they might think.\nFor him, misinformation about intellectual-property rights is the most important issue to overcome.\nThere are five intellectual properties — trade secret, copyright, trademark, right of publicity and patent. All five are relevant to food creations and commercialization. “The good news is that four of the five properties provide protection automatically, at no cost and without any application or legal paperwork being filed,” said Valauskas, a partner at Valauskas Corder LLC.\nHe used the example of a food truck to explain two of the automatic intellectual-property protections.\n“Let’s say that Chef A decorates a food truck to give it a distinctive look. Someone else cannot create a second food truck that looks very similar to Chef A’s food truck even if the second chef’s truck has a different name or even serves different food.\n“If Chef A has decorated her truck in an original way and that decoration readily distinguishes her food truck from other food trucks in the given marketplace, then Chef A — without ever filing a copyright or trademark registration application — has protection under federal copyright law and under state and federal trademark law. Lots of protection at no cost to Chef A,” he said.\nBut Mark Bailen, a BakerHostetler attorney who specializes in media and publishing matters, said when it comes to copyright, the law protects intellectual property on a case-by-case basis. “It’s going to be based on how much expressive content is included in recipe,” he said from the firm’s Washington, D.C. office.\nValauskas is undaunted by such arguments.\nBarrett’s “I don’t think I’d win” attitude is exactly what Valauskas hopes to remedy.\n“The theme is, there are treasures in the attic,” he says. “The kitchen is no different from a science laboratory or art studio. It’s a matter of identifying and managing it properly.”\n(Edited by Fern Siegel and Matthew B. Hall)\nCorrection: Jan. 22, 2021 02:53 pm\nAn earlier version of this article misspelled Justin Barrett’s last name. It is Barrett, not Barnett. Zenger regrets this error.']	['<urn:uuid:785eea92-2be4-42c5-a832-aa7e159d0f0d>', '<urn:uuid:1556fb7d-e694-4d30-846d-2c3cfe2aca3b>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T01:37:09.743768	19	122	1741
22	What are the monitoring methods and safety risks of steam pipe systems?	Steam pipe systems use ultrasonic pulse-echo monitoring at temperatures up to 250°C to track water condensation levels through pipe walls. Safety risks include pipe corrosion from poor water chemistry and potential tube damage from improper water levels, which can lead to dry firing if levels are too low or erosion if levels are too high.	"['In-Service Monitoring of Steam Pipe Systems at High Temperatures\n- Created: Thursday, 01 September 2011\nThis system can be used by utility companies for steam pipe systems incorporating multiple manholes.\nAn effective, in-service health monitoring system is needed to track water condensation in real time through the walls of steam pipes. The system is required to measure the height of the condensed water from outside the pipe, while operating at temperatures that are as high as 250 °C. The system needs to account for the effects of water flow and cavitation. In addition, it is desired that the system does not require perforating the pipes and thereby reducing the structural integrity.\nSteam Pipe and the in situ ultrasonic test setup."" class=""caption"" align=""right"">Generally, steam pipes are used as part of the district heating system carrying steam from central power stations under the streets to heat, cool, or supply power to high-rise buildings and businesses. This system uses ultrasonic waves in pulse-echo and acquires reflected signal data. Via autocorrelation, it determines the water height while eliminating the effect of noise and multiple reflections from the wall of the pipe.\nThe system performs nondestructive monitoring through the walls of steam pipes, and automatically measures the height of condensed water while operating at the high-temperature conditions of 250 °C. For this purpose, the ultrasonic pulse-echo method is used where the time-of-flight of the wave reflections inside the water are measured, and it is multiplied by the wave velocity to determine the height. The pulse-echo test consists of emitting ultrasonic wave pulses from a piezoelectric transducer and receiving the reflections from the top and bottom of the condensed water. A single transducer is used as a transmitter as well as the receiver of the ultrasonic waves. To obtain high resolution, a broadband transducer is used and the frequency can be in the range of 2.25 to 10 MHz, providing sharp pulses in the time domain allowing for higher resolution in identifying the individual reflections.\nThe pulse-echo transducer is connected to both the transmitter (function generator), which sends electric signals to generate the elastic wave, and the receiver, which amplifies the attenuated reflected waves that are converted to electric signals. To avoid damage to the receiver, the large signal from the generator is blocked by an electronic switching mechanism from reaching the receiving circuitry. To assure the operation of the transducer at the required temperature range, the piezoelectric transmitter/receiver is selected with a Curie temperature that is much higher. In addition, the system can be improved by introducing a heat sink between the transducer and the steam pipe, reducing the temperature requirements on the transducer.\nThis work was done by Yoseph Bar-Cohen, Shyh-Shiuh Lih, Mircea Badescu, Xiaoqi Bao, Stewart Sherrit, James S. Scott, Julian O. Blosiu, and Scott E. Widholm of Caltech for NASA’s Jet Propulsion Laboratory.\nIn accordance with Public Law 96-517, the contractor has elected to retain title to this invention. Inquiries concerning rights for its commercial use should be addressed to:\nThis Brief includes a Technical Support Package (TSP).\nIn-Service Monitoring of Steam Pipe Systems at High Temperatures (reference NPO-47518) is currently available for download from the TSP library.\nPlease Login at the top of the page to download.', 'When thinking about the safe operation of boilers (and don’t we all?), several systems can readily be named; flame control, fuel/air ratio; steam pressure control, levels in the vessel, etc. What about the water? It seems so passive, as long as there is enough for level control, what’s the big deal? Well, it turns out, that as the steam produced by a boiler is used in the process, the condensate from that steam is returned to the boiler as feedwater. However, since 100% of the condensate is not returned, whatever solids had been in that water before it evaporated to form steam are left in the remaining water. Fresh feedwater is added to maintain levels, but even fresh water contains some dissolved solids. So over time, the water in the boiler system gets saturated with all sorts of dissolved minerals.\nBoiler blowdown is the process of removing the heavily saturated water from the system. This means more make-up feed water is needed than to account for just process losses. This fresh feedwater has less impurities than the system water. Sampling/testing the system water, will let the operator know how much dirty water needs to be removed. An equal amount of fresh is added to maintain proper system water chemistry. Boiler water chemistry is something that needs to be monitored to ensure the safe operation and long operating life of a boiler.\nThere are two main types of boiler blowdown, and they are typically both employed in modern systems. The first type is bottom blowdown. This is done periodically (once a shift, once a day, etc.) and usually from the mud drum of the boiler (guess what’s in there?). This is where solids that have come out of solution and settled accumulate. Bottom blowdown can be automated or done manually. The second type is called surface blowdown. In larger systems this is done continually and is automated, in smaller systems, it may still be a manual procedure. In either case of surface blowdown, the system water is removed from the steam drum, which contains the most dissolved solids. The continuous stream that leaves the drum typically passes through one of the water level controls before exiting via a vent line.\nThis performs the double duty of removing impurity-laden water and keeping the level control legs free of scale/buildup. Several common failure modes in steam systems involve poor water level control. If the water level gets too low, the tubes can be exposed to flame, this is known as dry firing and it weakens the metal of the tubes. If the water level gets too high, slugs of water can be entrained in the steam and enter the steam pipes which will cause erosion and corrosion in that system as well.\nFor all blowdowns, the amount of water removed is adjusted based on results from water chemistry tests. As shown in the figures, if proper boiler water chemistry is not maintained, either corrosive compounds or solid material can build up to dangerous levels in the system. Pipes can either become corroded such that they fail to withstand the pressure differential from either side, or they can become clogged from solids adhering to the inner wall. Reminds you of either an aneurysm or a clogged artery in the human body, doesn’t it? I would say that steam is the lifeblood of many chemical plants, but that’s probably taking the analogy a little over the top…\nIn conclusion, if you have a failure in a boiler, be sure to investigate all the control systems mentioned at the beginning of this blog, but also include boiler blowdown and boiler water chemistry control history and practices.\nJennifer Morningstar, PE, CFEI, has 19 years of industrial experience. Her areas of emphasis include chemical release & exposure, OSHA process safety management, industrial accident investigation, fires & explosions, and scope of damage/cost to repair analyses. She spent 16 years working at a polyethylene terephthalate (PET) manufacturer. She is an OSHA-trained Process Hazard Analysis study leader and completed Root Cause Failure Analysis training to become an Incident Investigator. Jennifer authored procedures for lockout/tagout and confined space entry. She has experience as an energy management consultant in a variety of industries including mineral extraction, pulp & paper, animal harvesting & packaging (including rendering) and grain milling. Jennifer holds a Bachelor of Science Degree in Chemical Engineering from Virginia Polytechnic Institute and State University as well as a Masters of Business Administration from the University of South Carolina.']"	['<urn:uuid:2ac8ce08-bcff-4049-a057-6a0b284df084>', '<urn:uuid:ccb8cf1d-7444-433a-a6b4-9f9f1950c2e8>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	12	55	1282
23	What's special about Moscow's oldest synagogue?	The Moscow Choral Synagogue, consecrated in 1906, is Moscow's oldest and most frequently visited center of Jewish worship. Located next to the historic Jewish 'ghetto' of Zaryadye, it features a pale yellow neoclassical structure. The synagogue has a beautiful main sanctuary with rich textures and mosaics, almost Venetian in style. It continued operating throughout the Soviet period despite the official ban on religion, and still holds regular services today.	['Due to restrictions on settlement during the tsarist era, Moscow has never been a true center for Jewish culture. But a tourist looking to learn about Jews in the Russian Empire will have no trouble filling their itinerary with sights and museums. Moscow has several synagogues, kosher restaurants, special museums and,according to a 2010 census, around 50,000 Jews living in the city.\nMoscow Choral Synagogue\nCenter of Russian Judaism\nThe Choral Synagogue is Moscow’s oldest and most frequently visited center of Jewish worship. Located next to the historic Jewish “ghetto” of Zaryadye, the pale yellow neoclassical structure was consecrated in 1906 after protracted negotiations with the local authorities and the tsar, none of whom were overly keen to have a synagogue in the city. It continued to serve Moscow’s Jews through the Soviet period, despite the official ban on religion, and still holds regular services. Inside, the main sanctuary is as beautiful as the exterior, almost Venetian in the richness of the textures and mosaics.\n10 Bolshoi Spasoglinishchevsky Pereulok. Metro Kitai-Gorod. centralsynagogue.ru\nJewish Museum and Tolerance Center\nExperience Jewish history\nThis impressive state-of-the-art museum is Russia’s main showcase for the history of Jews in the country. It is housed in a historic constructivist bus terminal in the city’s main Jewish district, and was renovated and equipped with a 4D movie theater, panoramic and interactive displays and a wide collection of materials relating to the Jewish experience in Russia, from the time of Catherine the Great to the present. In the powerful and immersive permanent collection, you can easily spend an afternoon watching videos, reading documents and even listening to traditional music. Additionally, the museum regularly hosts temporary exhibitions, lectures and events.\n11 Ulitsa Obraztsova, Bldg. 1A. Metro Marina Roshcha. jewish-museum.ru\nMuseum of Jewish History in Russia\nWindow into Jewish daily life in the Russian Empire\nOpen an unassuming door marked with the cryptic initials MIEVR, walk up a flight of stairs and you will enter a treasure chest of Jewish life in the Russian Empire. The curators of this museum have traveled extensively around eastern Europe collecting everything from beautifully painted Torah arks and tombstones to kitchen utensils and tefillin. The museum depicts in detail how Jews spent their lives, from bris to funeral, using objects from everyday life. When they are available, the staff is happy to give free tours and answer questions, though calling in advance is recommended.\n10 Petrovsko-Razumovskaya Alleya, Bldg. 3, office 7. Metro Dinamo. mievr.ru\nKosher rooftop café\nOn the roof of Moscow’s Bolshaya Bronnaya Synagogue, you’ll find a breezy kosher restaurant that offers lovely views of the surrounding rooftops and a chance to try the cuisine of the Mountain Jews. Like their Christian and Muslim neighbors in the Caucasus, the Mountain Jews eat plenty of grilled meats, making this a great place to try shashlik, whether you keep kosher or not.\n6 Bolshaya Bronnaya Ulitsa. Metro Pushkinskaya. facebook.com/jerusaleminmoscow\nTraditional Ashkenazi specialties\nLooking like a set from “Fiddler on the Roof,” this kosher meat restaurant serves borshch and knishes just like your Bubbe’s Bubbe used to make them. It is located in the Marina Roshcha district, a short walk from the Jewish Museum and Tolerance Center.\n5A 2nd Vysheslavtsev Pereulok. Metro Marina Roshcha. koshermeal.ru/#shtetl\nRynok i Obshchepit Shuk\nIsraeli fast food\nPart fast-food counter, part market and part bar, this is the place to go in Moscow for authentic Israeli cuisine, including probably the best shakshuka in the city. The menu is not kosher and the shop is open seven days a week. It also can get crowded: Sometimes it’s necessary to fight for a seat at one of the long communal tables, so plan ahead if you are with a group.\n7 Veskovsky Pereulok. Metro Novoslobodskaya. facebook.com/rynokshuk']	['<urn:uuid:bbf2ad56-6927-4cf6-9d87-3bc0db8fec50>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	6	69	623
24	What are offline password attacks and how does two-factor authentication protect against them?	Offline password attacks are particularly dangerous because attackers can make unlimited attempts to crack captured passwords without detection, unlike online attacks where systems can implement lockouts. The longer and more complex a password is, the harder it is to crack. Two-factor authentication provides protection by requiring something beyond just the password, like a unique verification code sent to a registered mobile device. This creates an additional security layer, ensuring that even if passwords are compromised, unauthorized users cannot access the account, effectively reducing the probability of security breaches.	['Password attacks have been an ongoing problem for network security engineers. Every year SplashData publishes a report on the most commonly used passwords that are leaked online. In 2014, they analyzed 3.3 million leaked passwords and reported the top 25. The password “password” was number 2 on the list. Six of the top 11 were numeric sequences starting with 1 and varying only in length of the sequence (for example, 123456). Five more of the top 25 were simple alpha-numeric sequences (such as abc123). There were a few clever, yet still poor, passwords such as trustno1 and letmein. The remaining 11 were simple dictionary words, all in lower case. These top 25 passwords represented 2.2% of the 3.3 million leaked passwords.\nSecurity analysts should be aware of the different password attack methods and implement countermeasures against password attacks. The following are some of the methods that attackers use to obtain users’ passwords:\n- Password guessing: To perform password guessing, an attacker can either manually enter passwords or use a software tool to automate the process. Truly weak passwords can be susceptible to a lone attacker who is making informed guesses.\n- Brute-force attacks: Brute-force password attacks are performed by computer programs that are called “password crackers.” A password cracker performs a brute force crack by systematically trying every possible password until it succeeds. For example, it may start by trying all one-character passwords, then moving to two-character passwords, and so on, trying all possible combinations until they crack the password. With this method, the speed at which an attacker can obtain a password may depend on the speed of the attacker’s computer (how many calculations it can perform per second), the speed of the attacker’s Internet connection, and the length and complexity of the password. Many password crackers are available, and many at no cost.Although brute-force login attempts are by no means a new tactic for cybercriminals, their use increased threefold in recent years. Key targets for recent brute-force login attempts include widely used web CMS platforms such as WordPress. Successful attempts to gain unauthorized access to WordPress web servers give attackers the ability to upload backdoor scripts and other malicious scripts to compromised websites. Considering that there are more than 74 million WordPress web sites around the world, and that publishers are using the platform to create blogs, news sites, company sites, magazines, social networks, sports sites, and more, it is not surprising that many online criminals have their sights set on gaining access through the WordPress CMS. Other web CMSs, such as Joomla and Drupal, have been targeted as well. But it isn’t just the popularity of these CMSs that makes them desirable targets. Many of these web sites, though active, have been largely abandoned by their owners. There are likely millions of abandoned blogs and purchased domains sitting idle, and many are probably now owned by cybercriminals.\n- Dictionary attacks: Dictionary attacks use word lists to structure login attempts. Word lists can contain millions of words, including words from natural language dictionaries and sports team names, profanity, and slang. Dictionary attacks are not always successful and are often attempted before a brute-force attack. In some ways, however, a dictionary attack is similar to a brute-force attack. It is an automated process that is performed by a password cracker program; the speed at which the attacker can obtain a password may depend on the speed of the attacker’s computer (how many calculations it can perform per second), the speed of the attacker’s Internet connection, and the length and complexity of the password. Many dictionary attack tools are available for free on the Internet. For example, Cisco security researchers have discovered a hub of dictionary data which included 8.9 million possible user name and password combinations, including strong passwords—not just the easy-to-crack “password123” variety. Stolen user credentials also help attackers keep their dictionary list well populated.\n- Phishing attacks: Another way for attackers to find passwords is by indirectly asking the user. For example, a phishing email can direct victims to visit a malicious fake web site where they are asked to enter their personal information, such as their password or credit card, social security, and bank account numbers. An attacker may set up a web site that is of interest to the victim, and when the victim is lured to create an account on the attacker’s site, the attacker captures the password knowing that many people reuse the same password, or major portions of it, for all their web accounts.\nPassword attacks can be online or offline. In an online password attack, an attacker makes repeated attempts to log in. The activity is visible to the authentication system, so the system can automatically lock the account after too many bad guesses. Account lockout disables the account and makes it unavailable for further attacks during the lockout period. The lockout period and the number of allowed logon attempts are configurable by a system administrator. It is also worth mentioning that online password attacks can actually be used as a form of DoS. If the lockout affects enough accounts, an organization can be greatly impacted by it.\nOffline password attacks are far more dangerous. In an online attack, the password has the protection of the system in which it is stored, but there is no such protection in offline attacks. In an offline attack, the attacker captures the password or the encrypted form of the password. The attacker can then make countless attempts to crack the password without being noticed. The longer and more complex a password is, the more difficult and time-consuming it is for attackers to crack it.\nMany authentication systems require a certain degree of password complexity. Specifying a minimum length of a password and forcing an enlarged character set (upper case, lower case, numeric, and special characters) can have an enormous influence on the feasibility of brute force attacks. However, if users attempt to meet the enlarged character set requirements by making simple adjustments, such as capitalizing the first letter and appending a number and an exclamation point (for example, changing unicorn to Unicorn1!), little is gained against a dictionary attack that uses simple transforms.\nSome common password attack tools that are openly available include Cain and Abel, John the Ripper, OphCrack, and L0phtCrack.\nA common approach to reduce the risk of password brute-force attacks is to lock the account or increase the delay between login attempts when there have been repeated failures. This can be effective in slowing down brute-force attacks and giving the incident response team time to react.\nAnother countermeasure against password attacks is two-factor authentication. Two-factor authentication requires the attackers to have something more than the password to authenticate to the system. For example, requiring not only a password and user name, but also something that only the user has. For example, when you use your bank debit card to withdraw cash from the ATM machine, you also need a PIN that only you know.\nThis content is taken from a course of cisco.com', 'With the growing advancement in technology and the hacking techniques, both safety of the online accounts and the risk to lose them are increasing rapidly. Perfect authentication systems help secure your online accounts from hackers or the bad guys, and determine ‘whom to give access’. Lesser the security level, higher is the possibility that the intruders will gain access to your online account with no trouble. But, if the authentication system is intricate, restraining, or hard to compromise, then no unauthentic user will be able to gain access. The time has come to strengthen the digital assets security as far as growing cyberattacks are concerned, so it is better to enable Two Factor Authentication on all your valuable online accounts.\nBeing an authentic end user, if you want to log in to your online account, then you need the valid username and password combination for verifying your identity. But, what if someone else manages to guess your secret password. By doing so, he can easily get in to your online account. Therefore, it is necessary to add an extra security layer, so that no unauthorized user can access the account even if the password gets compromised. Two Factor Authentication security method helps to keep the prying eyes away from your online accounts and confidential data.\nA brief overview of this method\nTwo Factor Authentication, also known as 2FA or Two Step Verification is an authentication mechanism that adds an additional layer of security in the login procedure to verify identity of the user. This security method requires two authentication factors; unique verification code or OTP (One Time Passcode) and secret password, to verify whether the user is authentic or not. The verification code is immediately sent to the registered mobile device of the user as he enters the correct username and password combination associated with his account, and is valid only for a few seconds. Two Factor Authentication security method creates the perfect balance between online accounts security and whom to give access.\nIn internet security, the possible types of authentication factors are:\nSomething the user knows (the knowledge factors) – username, password, PIN.\nSomething the user have (the possession factors) – credit & a debit card, mobile device, a token.\nSomething the user is (the inherence factors) – biometric characteristics of the user such as iris, retina, face scan, voice recognition, fingerprint.\nWhat are the major benefits of enabling Two Factor Authentication?\nProtection against social engineering phishing attacks.\nEnsures no more hacking incidents on the valuable online accounts and data of the authentic end users.\nIncreased flexibility and productivity as Two Factor Authentication method allows different users to securely log in or gain access into a shared system or database.\nReduced probability of an intruder or hacker gaining access to your online account, resulting in no or minimal security breaches. This is achieved through a unique verification code used in the login procedure to ensure a strong second line of defence.\nHope, after going the above-listed benefits of Two Factor Authentication security method, you might have understood its importance. So, enable it today and secure your online accounts from different types of cyberattacks.']	['<urn:uuid:d526afa4-3348-4ff3-9c61-e82c3c1c9b57>', '<urn:uuid:d76358c7-9b3b-4ddc-a47e-acad6c52d366>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	13	88	1696
25	What kind of materials were used to build the Casa Munita Gonzalez house in Santiago, Chile, and what was the main purpose of using them?	The Casa Munita Gonzalez was built using Terra-Panel, which consists of panels of welded wire mesh filled with light earth, supported by beams and steel pillars. These materials were chosen to ensure the thermal efficiency of the house.	['Photo © Luis García\nThe Casa Munita Gonzalez by Arias Arquitectos and Surtierra Arquitectura is 275 sqm private residence built in Batuco, Santiago, Chile. The house is built using Terra-Panel to assure the thermal efficiency of the housing, which is constituted of panels of welded wire mesh filled of light earth that is supported by a main structure composed of beams and steel pillars.\nRead more about the house at ArchDaily.com\nArchitects PlanoB from Lisbon, Portugal have completed an innovative house called A Casa em Arruda Dos Vinhos that employs a hybrid earthen wall system that combines elements of rammed earth, cob and wattle and daub similar to the encajonado method used in the historic Briones House in California.\nCasa em Arruda Dos Vinhos construction website.\nOrange Farm is a township in the southwest of Johannesburg. The social situation is characterised by poverty, AIDS and unemployment. The appearance of the development is dominated largely by buildings or shacks made mostly of sheet metal, corrugated iron or parts of cars. In summer it can become unbearably hot in these shacks (up to 45°C), while during winter nights it can be noticeably cold (to 2°C).\nBASE habitat was commissioned by the Tebogo Home for Handicapped Children. The Austrian NGO SARCH set up this contact for us. The home for almost 50 children had become too small. In a group of 25 students we planned and built a dining building with a new kitchen, and a therapy building with sanitary facilities. A generously dimensioned pergola, a garden hall, connects the buildings with each other. The buildings we erected in Tebogo have a pleasant indoor climate throughout the year – without the use of energy. In this way we were able to reduce the fluctuation in temperature to only 9°C. Local workers, above all women, were integrated in the project. The building materials were acquired directly from the township: concrete blocks, earth, clay, straw, timber, grass mats – to strengthen the local economy and to make later repetition easier. One of the main aims was to make buildings that suited the needs of the children. They received a home that conveyed a sense of security and joy in living.\nThe Gaudet House c. 1830, Lutcher, Louisiana\nBousillage, or bouzillage, a hybrid mud brick/cob/wattle and daub technique is a mixture of clay and Spanish moss or clay and grass that is used as a plaster to fill the spaces between structural framing and particularly found in French Vernacular architecture of Louisiana of the early 1700s. A series of wood bars (barreaux), set between the posts, helped to hold the plaster in place. Bousillage, molded into bricks, was also used as infilling between posts; then called briquette-entre-poteaux. The bousillage formed a solid mud wall that was plastered and then painted. The bousillage also formed a very effective insulation.\nFrench Acadienne house in Lyon, France\nThe tradition was brought to New Orleans from France by the Acadienne (Cajun). The technique also has Naive American influences. This paper describes how “When the French built in Louisiana, their earliest houses (maison) were of this frame structure, but with the post in the ground (poteaux en terre). Sometimes the post were placed close together palisade fashion (cabane). This was a technique used by local Indians. The Indians infilled the cracks between the posts with a mixture of mud and retted Spanish moss. The French did likewise and called this mixture “bousillage”. The first framed structures were covered with horizontal cypress boards (madriers). The roof (couverture) frame was finished with cypress bark, shakes, boards, or palmetto thatch. All of these earliest structures had dirt floors and were usually only one room deep and two rooms wide separated by a fireplace.”\nUsualmente, cuando se habla de construcción con tierra cruda, se piensa inmediatamente en la construcción con “adobe”, sin embargo ambos términos no son sinónimos: el adobe es una de las tantas técnicas de construcción con tierra. Entre algunas de ellas destacan las tradicionales y más utilizadas en nuestro país, como lo son el tapial y la quincha, y otras técnicas mixtas nuevas, como el denominado tecnobarro y la quincha metálica.\nEarth Architecture began February 22, 2003 as a way to organize online research on earthen architecture and now, after almost 5 years of blogging on earth architecture, a book on the subject that will be published by Princeton Architectural Press in the Fall of 2008. The book will offer a history of earthen architecture, particularly a modern history of earthen architecture which touches on the efforts of Cointeraux, Gaudí, Schindler, Wright, Le Corbusier, Loos, Fathy, and many more of your favorite architects of the modern era. The book also discusses Rammed Earth, Mud Brick, Compressed Earth Block and many of the several earth building technologies and how they have evolved to meet the demands of contemporary society.\nThe book will also feature 40 cutting edge projects designed by architects and constructed of earth from the past 35 years, including work by many well known architects such as Rick Joy, Mauricio Rocha, Rural Studio, Glenn Murcutt, Arup Associates, Mathias Klotz, Predock_Frane Architects, Cutler Anderson Architects, Reitermann and Sassenroth, Heikkinen – Komonen Architects and Yung Ho Chang, to name a few, as well as important works by lesser well known architects who have produced critical works of contemporary architecture.\nThis is a very exciting moment in the history of this blog. It coincides with International Listings ranking Earth Architecture among the top 100 Architecture Blogs. It is an honor to be on the list with some really great blogs. In fact, I have to give props to Geoff for the idea for announcing the impending publication. If you’d like to know more about the book or would like to be reminded when the it is released, please email me, and thanks for reading about the most widely used building material on the planet.\nThe Citizens’ Initiative Pavilion, known as El Faro, is the work of architect Ricardo Higueras. Designed to be constructed for the Expo Zaragoza 2008 the pavilion is to represent society’s vigour and ability to innovate in the face of the challenges posed by water.\nThe section gives information on the Pavilion’s onsite location, its spaces and building features. Inspired by traditional ceramics, the pavillion will be constructed with natural materials like bamboo and mud mixed with straw.']	['<urn:uuid:12c19a13-9474-4426-b588-1588a22da6d4>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	25	38	1057
26	what happened first global climate change adaptation conference Gold Coast australia provia 2010	The first global conference on adaptation was held in Gold Coast, Australia in 2010 under the umbrella of the UN Environment Programme of Research on Climate Change Vulnerability, Impacts and Adaptation (PROVIA). This conference was significant as it marked the beginning of the biennial Adaptation Futures conferences and inspired European participants to organize their own Europe-focused events in alternate years.	"['European Climate Change Adaptation Conference\nThe biennial European Climate Change Adaptation conference is convened by projects that have received funding from the European Union’s Research and Innovation Framework Programmes.\nIn 2010, the first global conference on adaptation was held in Gold Coast, Australia under the umbrella of the UN Environment Programme of Research on Climate Change Vulnerability, Impacts and Adaptation (PROVIA). This marked the beginning of the biennial Adaptation Futures conferences, and inspired a group of European participants to organise Europe-focused events in alternate years. The European Commission supports the conference series, managing the events together with the project coordinators of three EU-funded adaptation research projects.\nDiverse voices and high-profile speakersDiverse voices from governments to investors, communities and activists, came together with Europe’s world-leading climate science researchers at ECCA. United in a commitment to drive innovative and effective climate adaptation action, more than 1,900 people joined ECCA 2021’s 9 webinars from May 25-June 10. High-profile panellists from around the globe represented the entire chain of cutting-edge climate adaptation action from research, policy and investment to ground-level delivery. The interactive format enabled worldwide audiences to join on Zoom and watch live on YouTube, contributing their ambitions and experiences through polls, break out groups and message boards to the EU Commission and other European decision-makers at the final ECCA high-level event on June 22. The overarching messages were that change is needed to finance, deliver and accelerate sustainable and meaningful climate adaptation solutions at a time of unprecedented risk but also of opportunities to improve climate resilience; society and governments can build on lessons learned from the Covid pandemic to increase collaborative actions; empowering people at every level through sharing knowledge and inclusive, diverse communication channels, builds networks so successful measures can be replicated and nurtures a sense of ownership and, finally, the crucial message that sustainable solutions leave no one behind. Key themes among examples of proven adaptation measures were finance, health and nature-based solutions.\nThe world must set clear and ambitious climate targetsAt the heart of the ECCA discussions was achieving a balance between what the climate adaptation research community delivers and the needs of policy-makers, financial institutions and funders, businesses, cities and communities. Frans Timmermans, Executive Vice-President for the European Green Deal, EC, addressed the high-level event, declaring that every country in the world must set clear and ambitious carbon emissions targets and support those most vulnerable to the impacts of climate change. He said: ""As we go to another summer with what could be a record-breaking temperatures, citizens will be once more reminded that the climate crisis is affecting us already. This reality is exactly what brings you all together here in Europe and will bring the world together in Glasgow at COP26 later this year. The frequency and severity of climate and weather extremes is increasing and we must adapt to it."" ""To tackle the climate and biodiversity crisis every country in the world needs to set clear targets. In February the EC adapted a new and more ambitious Adaptation Strategy 2050. The new strategy puts nature-based adaptation at the forefront."" “We are only as resilient as the most vulnerable among us. We cannot allow climate impacts to hit the most vulnerable at the same way as COVID-19 pandemic did therefore we will intensify work and support regions and communities making sure nobody is left behind."" “The science-policy dialogue is indispensable in order to help decision-making in dealing with climate that humanity has not experienced before. ECCA 2021 conference is the key opportunity for knowledge exchange. Policy-makers at all levels need to know about the climate change risks and about the ways and solutions to better and faster perform the decisions.”\n‘A more climate-resilient Europe’Timmermans supported the participation in ECCA of the Climate Pact Ambassadors, saying that local action is very important because climate impacts local realities and the communities, adding: “We need to build back a better and more climate-resilient Europe.” The 5th ECCA conference was co-organised by the EU Commission and JPI Climate, with 3 Horizon 2020 projects – SINCERE, CASCADES and RECEIPT – as a virtual event, defying the restrictions of the Covid-19 pandemic to reach a wider audience than ever before. First held in 2013, it is Europe’s largest conference on climate change adaptation and a vital forum for exchanging knowledge between scientists, policy-makers and practitioners. The aim is to inspire adaptation action by showcasing solutions, sharing knowledge, creating connections and dialogue with the broadest possible range of policy-makers. The 2021 theme ‘Bringing adaptation solutions to life: inspiring climate adaptation action for a resilient future’ was explored through the 9 webinars. The key messages drawn from the webinars aim to trigger faster and sustainable policies and investment for climate solutions. Webinar panellists represented influential organisations including the United Nations Framework Convention on Climate Change (UNFCCC), World Food Programme, Adaptation Fund, Climate Justice Resilience Fund, Center for International Climate Research, Climate Policy Initiative, EU Commission, European Parliament, European Investment Bank, Potsdam Institute for Climate Impact Research, Climate Alliance, European Environment Agency, European Committee of the Regions, European Green Deal, Euro-Mediterranean Center on Climate Change Foundation, Amundi Group, Adelphi, national and local governments and cities among many others. Young voices were represented by the Polish Youth Climate Council and the activist group, Fridays For Future. Organisers hope that the next ECCA Conference, in 2023, will build on the progress that has been made in bringing together more stakeholders, international, national and commercial funders and investors, extended knowledge exchange and wider connections with target groups. They also want to extend even further the range of diverse voices from across Europe including Eastern Europe, young people, local communities, businesses, financial institutions and cultural minorities.\nThe first global conference on adaptation was held in Gold Coast, Australia under the umbrella of the UN Environment Programme of Research on Climate Change Vulnerability, Impacts and Adaptation (PROVIA) in 2010. This marked the beginning of the biennial Adaptation Futures conferences, and inspired a group of European participants to organise Europe-focused events in alternate years. The European Commission supports the conference series, managing the events together with the project coordinators of three EU-funded adaptation research projects.\nThe 4 previous editions were about:\n- 2019 – European Climate Change Adaptation Conference | ‘Working together to prepare for change‘ – Lisbon, Portugal, 28–31 May 2019 (website)Held for the first time in southern Europe, this conference’s edition brought together researchers, policymakers and practitioners to discuss recent advances in climate change adaptation (CCA) and disaster risk reduction (DRR) research, policy and practice, and aimed to inspire collective climate action. Around 1200 participants attended the 96 Science-Practice and Science parallel sessions. These were organised by the scientific and practice communities to present the current state-of-the-art and to inspire action along the six identified ECCA themes.\nPLACARD, BINGO and RESCCUE were the EU-funded projects behind the fourth ECCA.\n- 2017 – European Climate Change Adaptation Conference | ‘Our Climate Ready Future‘ – Glasgow, Scotland, 05–09 June 2017 (website temporary unavailable)The vision for ECCA 2017 was to inspire and enable people to work together to discover and deliver positive climate adaptation solutions that can strengthen society, revitalise local economies and enhance the environment. The City of Glasgow welcomed over 850 people to the third ECCA, which featured for the first time a business and innovation programme.\nContributions from young speakers stressed the need for deeper involvement of young people in climate action and policymaking at all levels. A challenge embedded into the city’s school curricula led to an exhibition by local school children of their imaginative solutions to Glasgow’s flooding problems.\nECCA 2017 was organised by the EU-funded IMPRESSIONS, HELIX and RISES-AM projects.\n- 2015 – European Climate Change Adaptation Conference | ‘Integrating climate adaptation action in science, policy, practice and business‘ – Copenhagen, Denmark, 12–14 May 2015 (website)\nMore than 750 participants from 46 countries took part in ECCA 2015 in Copenhagen. The conference offered a platform for researchers, policymakers, and businesses to share new research results, novel policy developments, and practical implementation experiences on climate change impacts and adaptation. Opportunities for business innovations to support the transition to low carbon societies were also highlighted.\nSessions were designed to combine knowledge, policy, practice and business as well as showcase how climate change adaptation in Europe can contribute to job creation and other societal benefits.\nBASE, RAMSES and ToPDAd were the EU-funded projects behind the second ECCA.\n- 2013 – European Climate Change Adaptation Conference | ‘Integrating Climate into Action‘ – Hamburg, Germany, 18–20 March 2013 (webpage)The first ECCA was organised by EU research projects RESPONSES, CLIMSAVE, MEDIATION and ClimateCost, covering a broad range of adaptation issues. The conference focused on understanding and assessing adaptation in action with topics covering risk and vulnerability assessments, governance, and impact and effectiveness studies, as well as the economics of adaptation.\n|The theme of the 5th ECCA conference is:\n“Bringing adaptation solutions to life: Inspiring climate adaptation action today for a resilient future”\nWhy acting now? Our climate is changing rapidly. Heatwaves, droughts, floods, fires… Extreme events attributable to human-driven climate change escalate in frequency and intensity, striking more and more people, regions, cities and communities.\nAre we ready to adapt? While climate change adaptation science is developing fast and improve our understanding of the climate and biodiversity crises, and relevant climate risk management are increasingly developed across Europe, there is an unprecedented need and opportunity in the EU to scale up adaptation action, through the recovery investment stemming from the response to the COVID-19 pandemic and the EU Green Deal.\nAs part of the European Green Deal, the new EU Adaptation Strategy, as well as the Horizon Europe Mission “A Climate Resilient Europe”, place resilience building and climate adaptation high up on the list of priorities. The European Climate Pact, launched in December 2020, gives additional momentum to act for adaptation, inviting all citizens, communities and organisations in every sector of our society and economy to participate in climate action and build a greener Europe. The New European Bauhaus will explore how to live better together after the pandemic and to accelerate the green transformation of our society.\nThe European Climate Change Adaptation conference ECCA 2021 comes at a right time, after the first global Climate Adaptation Summit in January 2021, and before the COP 26 in Glasgow in November 2021, the publication of the IPCC 6th Assessment Report and the upcoming conference Adaptation Futures. ECCA 2021 will raise the attention of decision-makers on the pressing need to accelerate the transfer of knowledge from R&I to policies and practices in order to increase resilience in Europe and beyond in a context of rising climate risks.\nAdequate adaptation options and opportunities need to be integrated in pathways that lead to a better resilient future. Alternatives do exist to improve climate adaptation in terms of the infrastructure we build and retrofit, the finance we invest and the decisions we make (business, personal and collective), and contribute to shape a climate resilient and zero carbon Europe. Adaptation science, knowledge and practice can inform decision-making and help accelerate societal transformation. Adaptation actions can have multiple benefits, including recovery, resilience, climate change mitigation, biodiversity conservation and restoration, job creation, citizen safety, social inclusion, wellbeing and financial security.\nThis ECCA edition aimed to:\n- A platform to exchange practical solutions and knowledge (legacy) on solutions for climate resilience\n- A place to connect research, tools and solutions with decision makers and practitioners\n- A place to listen and learn new things – and to be inspired to create change\n- An event that leaves you with a good feeling and a desire to get involved again\n- An event that raises awareness and encourages action at all levels\nThe biennial European Climate Change Adaptation conference is convened by projects that have received funding from the European Union’s Horizon 2020 Research and Innovation Framework Programme (website).\nNo items found\nStay up to date with all research and results of JPI Climate.\nStay up to date with all research and results of JPI Climate.\n""*"" indicates required fields']"	['<urn:uuid:0f4f508f-d57a-4a1b-ae4b-257fa0cce97b>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	13	60	2008
27	What type of environmental building standard was newly established in North America, and how does it differ from traditional efficiency measurements?	The Zero Carbon Building Standard is the first in North America to use carbon instead of energy efficiency to measure a building's impact. The Standard considers the choice of energy sources and the carbon intensity of regional electrical grids to accurately assess environmental impacts and guide investments.	['Media Release: Mohawk Helps Set New Standard for Green Building Innovation\nJoyce Centre for Partnership & Innovation Canada’s First Institutional Building to Receive Zero Carbon Building – Design Certification\nHamilton, ON – Mohawk College has been awarded Zero Carbon Building – Design certification from the Canada Green Building Council (CaGBC) for the Joyce Centre for Partnership & Innovation. The Joyce Centre is the second building to receive the certification. The first was a multi-tenant office building in Waterloo, Ontario.\nIn 2017, the Joyce Centre became one of 16 projects from across Canada to be chosen as a pilot project for CaGBC’s new Zero Carbon Building Standard. New construction projects earn Zero Carbon Building – Design certification by modeling a zero carbon balance, highly efficient envelope and ventilation systems to meet a defined threshold for thermal energy demand intensity, and onsite renewable energy systems capable of providing a minimum of 5% of building energy consumption. Project teams are required to evaluate energy use holistically, including impacts on peak electricity, and determine the GHG emissions associated with structural and envelope materials. Both new and existing buildings can obtain the standard.\n“We’re honoured to receive the Zero Carbon Building – Design certification. The Joyce Centre for Partnership & Innovation will not only be a showcase for green construction but also a living lab that will teach future generations how to build better, more sustainable buildings.”\n“Congratulations to Mohawk College on earning Canada’s second Zero Carbon Building – Design certification for their impressive Joyce Centre for Partnership and Innovation. With this project, Mohawk has proven that they are on the leading edge of innovation going well beyond the status quo. This project shows that these buildings are technically feasible and financially viable, and sets a strong example for students about the low-carbon economy.”\n“Having founded our firm on the principals of sustainable design in Hamilton, we are thrilled that The Joyce Centre for Partnership & Innovation at Mohawk College is awarded the Zero Carbon Building –Design Certification in our city. Achieving this standard has been a huge team effort. In addition to generating all of the building energy needs through renewable solar photovoltaic panels, we worked diligently to enhance sustainable features using natural materials, maximizing natural light and views, applying finishes strategically, integrating water conservation measures and green roofs. We believe this facility will serve as inspiration for what can be achieved through a unified team process to integrate innovative design solutions and proven building system technologies to achieve Net Zero energy and zero carbon facilities.”\n“The Joyce Centre represents a new archetype for learning environments in Canada that recognize and encourage innovation. It’s not just a lab building. In many ways, it’s also a teaching tool for students, training them to deliver a more sustainable future. Its environment explores the symbiotic relationship between all stakeholders and underscores the importance of a cultural shift in how we interact with our built environment—from unrestricted consumption to personal accountability.”\n- The Joyce Centre for Partnership & Innovation will be home to new and existing programs offered through Mohawk’s School of Engineering Technology.\n- The Centre will house high-tech labs and classrooms specializing in fields such as clean and renewable energy, sustainable design, technology automation, cyber security and materials manufacturing among others.\n- Contributing to the zero carbon certification was Mohawk’s curtain wall technology which offers much better insulation value than typical curtain walls.\n- The Centre also uses no natural gas onsite – the all-electric set up allows for the easiest route to low carbon for most buildings.\n- There will be nearly 500 solar panels mounted to canopies on the roof of the Joyce Centre. Campus-wide the college is installing nearly 2,000 solar panels as part of a clean energy retrofit.\n- Mohawk’s solar panel system will produce 550,000 kilowatts of clean energy per year which is enough to power 45 Canadian homes for one year.\n- The Joyce Centre also features 28 geothermal wells that draw energy from more than 600 feet below the building.\n- Mohawk has been named one of Canada’s Greenest Employers for the past five consecutive years in recognition of its leadership in environmental sustainability\n- 30 -\nFor more information about Mohawk College and the Joyce Centre for Partnership & Innovation contact:\nDirector of Communications\n905-575-1212, ext. 3965\nFor more information about CaGBC’s Zero Carbon Building Standard contact:\nCaGBC Media Relations\nDeveloped to be broadly applicable, the Standard can be used by new and existing commercial, institutional, and multi-family buildings. The Standard is unique in that it is the first in North America to use carbon instead of energy efficiency to measure a building’s impact. Recognizing the choice of energy sources and the carbon intensity of regional electrical grids is critical to accurately assessing environmental impacts and guiding investments. The Zero Carbon Building program reinforces the importance of energy efficiency while also driving careful choices about the types of energy used and encouraging more renewable energy generation both on the building site and offsite.\nCaGBC is the leading national organization dedicated to advancing green building and sustainable community development practices. As the voice of green building in Canada, we work closely with our national and chapter members in an effort to make every building greener. The CaGBC reduces environmental impacts from the built environment through project certification, advocacy and research. CaGBC established the LEED® green building rating system in Canada and developed the country’s first Zero Carbon Building Standard. For more information cagbc.org.\nAbout Mohawk College\nMohawk College educates and serves more than 31,700 full-time, part-time and apprenticeship students at three campuses in Hamilton, Ontario. Mohawk is among the leading colleges in Canada for applied research and is ranked number one for student and graduate satisfaction among colleges in the Greater Toronto and Hamilton Area.']	['<urn:uuid:816c314b-3f7e-4d03-88b4-5a41979f5f9b>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	21	47	966
28	How do arctic foxes use their white fur to survive?	Arctic foxes' thick white fur coat helps them in three ways: it protects them from the cold, provides camouflage in the snowy landscape, and allows them to hunt more efficiently while hiding from predators.	['In order for animals to survive, they need to be able to\nadapt. In this lesson we will look at the types of adaptations animals can and\nIn a perfect world, animals would not need to adapt.\nHowever, with constant changes to their environment, food chain and climate,\nanimals must adapt or face extinction. There are many examples of animals facing\nextinction. Later on in the course we will take a look at some of these\nexamples, and the reasons for their endangerment.\nAn adaptation is a trait that makes an animal suited to its\nenvironment. It can be a behavioral or a structural trait. Here a some examples:\nMoving in large groups is a behavioral adaptation;\nit helps protect the members of the group from predators.\nThe thick fur coat of an arctic fox is a structural\nadaptation. It helps protect it against the cold weather.\nAdaptation happens over a long period of time. Structural\nadaptation happens in the form of changing an animals genetic traits. Take\nfor example this imaginary animal, the whatsit.\nThe whatsit lives in woods. It s hunted by men and larger\nanimals. Most whatsits are born with white fur, making them easy to spot amongst\nthe trees. Some whatsits are born with brown, speckled fur. These are far more\ndifficult to spot. Since they are easier targets for hunters, far more white\nwhatsits are hunted and killed than speckled ones. Each time a speckled one\nmates with a white one, half the offspring are speckled. Eventually the amount\nof white furred ones available to breed is smaller than the number of speckled\nones. More offspring are born with speckles than with white fur. This animal has\nadapted, and now the majority has speckled fur instead of white fur. Eventually\nthe white furred ones will disappear altogether.\nThere are many examples of this type of adaptation. Because\nit happens over generations, it is slow. In some cases, the inability of animals\nto adapt quickly enough have led to their distinction or endangerment.\nBehavioral adaptation can happen far more quickly. The more\nintelligent an animal is the faster it can learn to make behavioral changes, in\norder to survive.\nIf you take the most intelligent of animals, you can\nunderstand what happens. Humans adapt in many ways in order to survive. Here is\nan imaginary example of human adaptation:\nA group of people lives in village Somewhere. In the\nvillage a large number of children die each year, after being attacked by wild\nanimals. The villagers can make changes to their environment and behavior in\norder to protect the children. They can build fences around the village. They\ncan make traps to catch the animals. They can ensure that the children only go\nout in groups, protected by armed adults.\nAll of these are behavioral changes, and can happen almost\novernight. The speed with which they happen depends on the intelligence of the\nanimals and their ability to work as a community.\nThe Best Adaptations\nAdaptation can work in two ways. The best example of this\nis camouflage. A chameleon can change its color according to its surroundings.\nThink about how this helps protect it. Not only does it help protect the\nchameleon, it also helps it survive in another way. Because it blends in so well\nwith its surroundings, insects are less likely to see or notice it. By blending\nin it also makes it a better hunter, it can hunt without being seen, In fact, it\nis such an efficient adaptation that hunting is probably the wrong word o use.\nIt just blends in and patiently sits waiting for some unsuspecting insect to\nhappen along. Then, flick, out goes the tongue and there is one less insect and\none less hungry chameleon. The more ways a single adaptation helps an animal to\nsurvive, the better it is.\nGoing back to the arctic fox and its fur coat. How can that\ncoat help the fox in more than one way? The answer lies in the color of the\ncoat. Obviously in a landscape covered in snow, a thick, white coat is very\nuseful. This adaptation works in three ways. It protects the fox from the cold\nad provides camouflage. The camouflage allows it to hunt more efficiently and\nhide from would-be predators.\nClassification of Animals']	['<urn:uuid:2639435a-440d-4bec-baf5-fbe73cf8d2b8>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	10	34	720
29	How large are the Sister's Islands and what happens during their coral spawning?	Big Sister's Island is 39,000 m2 and Little Sister's Island is 17,000 m2. During coral spawning, which occurs four days after the full moon in the fourth month, corals release tiny packets of eggs and sperm that float to the surface for fertilization, though most don't survive due to predation by marine creatures.	"['|• Malay||Pulau Subar Laut, Pulau Subar Darat|\n|• Tamil||Sakotharigal Theevu|\nAs seen from Saint John\'s Island\nBig Sister\'s Island, about 39,000 m2 (9.6 acres) in area and also known as Pulau Subar Laut in Malay, faces the open sea, while Little Sister\'s Island, about 17,000 m2 (4.2 acres) in area and also known as Pulau Subar Darat in Malay, faces the mainland. The two islands are separated by a narrow channel. Currents through this channel can be very dangerous to swimmers and divers.\nLegend tells of a poor widow who had two pretty daughters, Minah and Linah, who were very close to each other. After their mother died, the sisters left the village to live with a distant uncle.\nOne unfortunate day, Linah met a group of pirates while she was fetching water from a well near the sea. Frightened, she ran home while the pirate chief gave chase. At the uncle\'s home, the pirate brandished a dagger and made known his wish to marry Linah. That night, the two sisters wept bitterly in each other\'s arms. When dawn broke, the pirate chief and 16 of his men came to take Linah away. Clinging to each other, they were torn apart by the pirates and Linah was forced to leave with the pirates. Just then, the sky turned dark and a storm broke out. Desperate, Minah swam after the boat but drowned. On seeing this, Linah freed herself from her captors and jumped into the sea to join Minah.\nThe storm subsided but nowhere can the sisters be found. The next day, the villagers were shocked to see two islands at the spot where the two sisters had drowned.\nThe two tranquil islands, called Pulau Subar Laut and Pulau Subar Darat, was henceforth known as the Sisters\' Islands. It was said that every year on that very day when the sisters turned into islands, there will always be storm and rain.\nAnother version of the legend stated that the sisters were being blown away by the storm and each landed on one of the island, and hence the name Sisters\' Islands.\nA third version of the legend said that there were two sisters who drowned at sea. The elder sister tried to save the younger sister but both drowned. The gods were touched by their love for one another and transformed both into two little islands so they can be together forever.\nThe beaches and warm blue waters make snorkeling a favourite activity at the islands. The islands are also popular with picnickers and campers and are also home to some of Singapore\'s richest reefs. A wide variety of corals can be found in the waters surrounding the islands. Common sea life that can be found include the giant clam, the seahorse and the octopus. Big Sister\'s Island is home to some long-tailed macaques.\nSisters’ Islands Marine Park\nIn 2014, there were plans to set up a marine park on the islands. The park, known as the Sisters’ Islands Marine Park, is 160,000 m2 (40 acres) which encompasses the land and waters surrounding the islands, and also covers the western coasts of both St John’s Island and Pulau Tekukor.\n- ""Sisters Islands, Southern Islands of Singapore"". Wild Singapore. Retrieved April 15, 2015.\n- ""Singapore to have its first marine park on Sisters\' Islands"". Eco-Business. July 14, 2014. Retrieved April 14, 2015.\n- ""5 things about the Sisters\' Islands, Singapore\'s first marine park"". The Straits Times. July 15, 2014. Retrieved April 15, 2015.\n|Wikimedia Commons has media related to Sisters\' Islands.|\n- Satellite image of Pulau Subar Laut and Pulau Subar Darat - Google Maps\n- Info for visitors on wildsingapore\n- Photos of marine life at Sisters Island from wildsingapore\n- Sisters Coral Reef Survey Data on the Coral Reefs of Singapore website\n- Singapore Island Cruise information site\n- Sisters\' Islands Marine Park Official Website', 'Chay Hoon shared that she saw a lot more colony of corals spawned than last year. Hurray! Last year, there was ""greatly reduced spawning intensity because our corals suffered from mass coral bleaching in 2016.\nWhat is mass coral spawning?\nOnce a year, on the fourth month, around four days after the full moon, our corals seed the seas with new life!\nSome corals release their eggs and sperm all the same time. Called broadcast spawners, these mass spawning events usually occur once a year, a few nights after full moon. Tiny packets of eggs and sperm pop out of the coral polyps.\nThese packets drift to the water surface where they open up, releasing the eggs and sperm for fertilization.\nAfter a few days, the embryos will have developed into coral larvae that drift about and eventually settle down on a hard surface to form new corals.\nWhile bazillions of eggs and sperms are released during a mass spawn, most don\'t make it. Hordes of marine creatures gorge on the spawn, from fishes and crabs to jellyfishes.\nAs the tiny coral larvae develop, they have to survive the countless predators that constantly sieve the water for plankton and edible bits.\nThe coral larvae also have to overcome many other challenges that we are still learning about. Excessive sedimentation, for example, can interfere with fertilisation and other aspects of coral larvae survival and successful settlement.\nWhy is mass coral spawning in Singapore a big deal?\nFrom Sex in the Tropics 2008 on the blooooooooooo blog\nCoral mass spawning in Singapore was first recorded by Dr James Guest in 2002. It was the first record of coral spawning in the tropics. At least 18 different coral species from ten genera and five families (Acroporidae, Faviidae, Merulinidae, Oculinidae and Pectiniidae) have been observed to spawn in our waters! Mass spawning occurs on the third to fifth nights after the full moon between 8 and 10 p.m.\nIn interviews with the media, Dr James Guest emphasised that corals are part of Singapore\'s biodiversity and natural heritage. ""There are 255 species of corals recorded here, and there may be some corals here that were around before Stamford Raffles arrived.""\nThe fact that our corals mass spawn shows that our reefs are functioning well! According to Dr James, the number of coral species in Singapore that mass spawn is ""as high as on other Indo-Pacific reefs, like the Great Barrier Reef... "" This shows how rich Singapore\'s natural heritage is. We can find right at our doorstep: ""diverse, functional and fascinating coral reefs, that people would normally associate only with countries like Australia.""\nMORE photos of Mass Coral Spawning in 2018\nChay Hoon shares about her survey on 4 Apr at Pulau Satumu (Raffles Lighthouse)\nChay Hoon shares about her survey on 5 Apr at Pulau Satumu (Raffles Lighthouse)\nHeng Pei Yan shares about her survey on 5 Apr at Pulau Satumu (Raffles Lighthouse)\nYujie Zheng shares about what she saw.\nDr Karenne Tun shared a lovely video of the corals and other marine life seen.\nOur corals have spawned by Heng Pei Yan on her blog.\nPosts on wild shores of singapore about previous coral spawning\n- Mass coral spawning 2017\n- Mass coral spawning 2016\n- Orgy in Singapore waters: Mass Coral Spawning 2015\n- Mass coral spawning 2014\n- Orgy in the Sea: Mass coral spawning 2012!\n- A spawn of articles on coral spawning in Singapore!\n- Orgy on our reefs: Coral spawning 2011\n- Sex in the Sea: Singapore style\n- Spawn of coral on video!']"	['<urn:uuid:8bd3e802-7b7f-4dc8-b618-6efc38077683>', '<urn:uuid:e378d2cc-35b5-4373-8a4a-5dbbae0fc279>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	13	53	1242
30	which nutrients does bone powder give plants when put in dirt	Bone meal, which is powdered bones from slaughterhouses that are cooked and pulverized, contains 9 to 12 percent phosphorus. It needs to be placed in the root zone since phosphorus doesn't travel through the soil, and it takes considerable time to break down.	"['Although it might be some weeks before soaking rains drench the garden, now is a great time to begin feeding your soil. As spent crops are removed, dig in plenty of organic matter like compost, manures and natural fertilizers. These garden goodies applied now will enrich soil with nutrients and humus by next spring. Autumn rains will gently wash them in.\nHealthy, well-fed soils produce an abundance of fruit and flower, in addition to protecting robust plants from pest and disease. Here is what you can do now.\nTEST -- Before adding anything to the soil, first test the pH. If your soil is too acidic, now is the time to add lime. This is the most effective way to raise the pH toward the neutral zone where most plants thrive. Liming the soil also adds calcium and magnesium, vital elements that acidic soils often lack. Acid-loving plants like blueberries, rhododendrons, camellias, ferns and potatoes do not need an addition of lime.\nADD MANURES -- Fresh manures are plentiful and cheap this time of year, and fall is the best time to use them. Till manures into the soil now so they will be completely decomposed by spring planting. Horse manure is widely available from local stables and is free, if you haul it.\nCOVER CROP IT -- The easiest, least expensive way to feed your soil is to plant a cover crop. Shortly before a soaking rain is the time to sow. Alfalfa, clover, fava beans, buckwheat, oats, vetch and winter wheat are typical cover crops planted in the fall. They add great quantities of organic matter. Those that are legumes add nitrogen as well.\nThese crops not only enrich the soil, they also make sure existing soil nutrients are not washed away by drenching rains during winter. Erosion is limited and weeds are often crowded out when cover crops take over.\nADD NATURAL FERTILIZERS -- Bulky natural fertilizers like blood meal, fish meal, kelp, greensand and the like are most effective when tilled into the soil in the fall. Natural decomposition during the winter months makes nutrients readily available for spring plants.\n-- Blood meal is dried, powdered blood from slaughter houses. Blood meal contains approximately 13 percent nitrogen and some iron. Use this sparingly, since too much can burn plants. This is a good material to add to the compost pile if it needs heating up.\n-- Fish meal is ground and dried fish wastes, bones, guts and skins. It contains approximately 9 percent nitrogen and 3 percent phosphorous.\n-- Bone meal is powdery meal made of bones from the slaughterhouse that are cooked and pulverized. It usually has 9 to 12 percent phosphorus. It takes a good deal of time for this material to break down into the soil. Phosphorous is best placed in the root zone because, unlike nitrogen, it does not travel through the soil.\n-- Seaweed meal, or raw seaweed, is a good source for trace elements like boron, calcium, chlorine, copper, iron, magnesium and manganese. Adding one pound of seaweed meal per 100 square feet, or three pounds of raw seaweed, on a yearly basis will boost the micro-nutrient content of garden soil.\nFor those who do not wish to use animal byproducts in their soil, consider alfalfa meal, flax seed meal, cotton seed meal, kelp meal and greensand.\nOTHER GOODIES -- Treat your garden soil to worm castings, garden compost and humic acid. These garden goodies are worth their weight in gold when it comes to feeding the soil.\nTerry Kramer is a trained horticulturist and journalist. She has been writing a garden column for the Times-Standard since 1982. If you have a question you\'d like answered in this column, email it to email@example.com and put ""Question For Terry Kramer"" in the subject line, or write to Terry Kramer/Lifestyle, Times-Standard, P.O. Box 3580, Eureka, CA, 95502.']"	['<urn:uuid:ed573124-ebbc-4b56-8fda-14f5eac40863>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	11	43	645
31	I'm interested in learning about slums and social support - what are the key characteristics of slum communities in terms of social bonds, and how can digital tools help strengthen these community connections?	Slum communities demonstrate remarkable social resilience and cohesion. Residents, often migrants without traditional support systems, create valuable networks that become more important than their families back home. These communities provide crucial support during crises, as seen during COVID-19 when social bonds helped residents cope with job losses and hardships. Digital tools can strengthen these community connections through online platforms that promote social interaction and broad participation. These platforms enable residents to communicate within groups, share neighborhood news, and coordinate activities. They can create virtual bulletin boards, support message exchange, and help build local social networks, all while being controlled by local social institutions to protect personal data.	['What is a slum?\nUN-Habitat refers to slums as contagious and informal settlements, where the inhabitants lack at least one or more of the following: durable housing with adequate space, access to services (water and sanitation) and social security against eviction and crimes¹.\nThe five biggest slums in the world- Khayelitsha in Cape Town, Kibera in Nairobi, Dharavi in India, Neza n Mexico, and Orangi Town in Pakistan, are together home to around 5.7 million people². Most of this population consists of an informal urban workforce who move to the cities looking for livelihood and opportunities for work. The rapid population growth and the trend of migration from rural to urban areas indicate that, by 2030, about a quarter of the world’s population would be living in slums or other types of informal settlements³.\nSlums are neighbourhoods that are born out of a combination of necessity and helplessness. One of the best examples of this is the Favelas in Rio.\nAs Theresa Williams, Executive Director of Catalytic Communities said in her (now popular) statement from 2017, “Favelas are unregulated. They’re poorly serviced. But don’t call them ‘slums.’ That’s a lazy translation and it’s unfair to the people who live here. The word ‘slum’ implies that all favelas are the same. The word ‘slum’ ignores 120 years of improvement that these residents have made to their communities. And the word ‘slum’ makes it easier to ignore these communities. That’s exactly what the government wants. Because if favelas aren’t viewed as real neighbourhoods, then their residents don’t deserve real services. So please don’t use the word ‘slum.’ Because it only helps to encourage that narrative.”\nThe notion of slum should not be romanticised. Everyone deserves a home that is adequate, safe and secure.\nSlums are home to vibrant and resilient communities\nSlums consist of a large number of temporary homes, often made of sheet metal or other such material which is not thermally efficient and is also at risk of getting dislodged under the extreme wind. The population living under such conditions are exposed to poor air quality, extreme temperature fluctuations, urban heat island effect and sound and water pollution. Already highly vulnerable, these communities are among the first to face the wrath of climate change events. Under such uncertainties, the communities living in the slum have built a level of resilience and social cohesion unlike any other. Slums are home to businesses and the narrow lanes of Dharavi are home to businesses generating revenues to the tune of a billion dollars⁴.\nWhen faced with a major life event human beings rely on their immediate family or a trusted community for support to return to normalcy. This is true for any strata of society, and more so for slum dwellers, who consist of people who have migrated from their hometowns without an accompanying support system. The network and the “family” that they have created are extremely valuable to them and are often more important than their own families back home.\nDharavi in Mumbai, which is one of the largest slums in the world is home to a large number of industries which operate in harmony with places of worship, homes, playgrounds and schools. What they lack in infrastructural resilience is compensated for by social resilience. As an example, during the Covid-19 pandemic, the slums were expected to be hotspots for spread and with businesses shutting down and operating remotely, the support staff were the first to be laid off. Many families lost their only source of income and were devoid of any assistance.\nThis is a prime example of the importance of social resilience during tough situations. Building inclusive, adequate and safe permanent homes for slum residents is a major challenge faced by Governments.\nBuilding resilient housing for low-income communities\nThe dictionary definition of resilience is the “ability to withstand adversity and bounce back from difficult life events”.\nClimate change events are a test of social and infrastructural resiliency. This is also different from the traditional definition of sustainability. Traditionally sustainability consists of reducing the impact on existing resources but resiliency takes it one step further.\nCommunities on the margins and those living in slums and other unplanned settlements are often overlooked by the city. This is the population that is first in line for any natural disaster- be it flooding or heat waves.\nHousing programs around the world are aimed at housing the urban poor and curtailing the ever-expanding urban sprawl but the question that needs to be asked is “Are we addressing the right problems by just building living spaces?”\nWhat defines a resilient home for a family living under such conditions?\nAnn Francis, Asst. Prof, TERI SAS, believes that while human beings have the ability to adapt to changing climatic conditions, economic situations and other conditions over a period of time, the built infrastructure does not have this ability. She urges modern-day researchers, designers and engineers to design and build to accommodate foreseeable climate change events.\nSlum rehabilitation and rehoming communities and families on the margin are always looked at as providing shelter.\nOften overlooked are other aspects of what makes a living space a “home”. For a space to feel like home, the residents should feel that they have an adequate amount of living space and are surrounded by a community where they feel cared for.\nDesigning homes that are thermally efficient, with good ventilation and using a robust and resilient structural system is the starting point for the design and construction of a resilient neighbourhood. Once these aspects are taken care of, the secondary, more energy and sustainability-related aspects would be optional value add to the social system, making the residents less reliant on resources provided by the city.\nDesigning and building homes with the active participation of the communities also ensures that there is a sense of responsibility and ownership. This would also ensure that livelihoods are not lost in the process of rehoming. Including women in both home ownership and decision-making forums are also important to ensure participation and would allow families and communities to settle faster in the new residences.\nAnn Francis (Asst. Prof. TERI SAS) mentions how accessibility and ensuring livelihoods are not affected are key to ensuring success in rehabilitation efforts. If the new neighbourhoods are in locations far from opportunities for livelihood, there is a high likelihood of residents moving back to the slums. A lack of opportunity, combined with scattered social fabric during relocation is enough reason for communities to seek familiarity and comfort back in their slums. A lot of what comprises human survival is made up of a feeling of safety in the community.\nA little thought goes a long way\nThe key to building homes and not “a roof over the head” is by incorporating an inclusive approach. In all major housing and rehabilitation projects, there are multiple stakeholders: the city, housing authority, town planning team, designers, and engineering and construction teams. Thoughtful planning, while keeping the communities at the centre goes a long way to ensure that the voices of the people who would be living in the spaces are heard at every stage.\nSwathy Manohar (Asst. Prof., IIT Bombay) believes that building resilient homes for vulnerable communities should be a collaborative effort by the industry, academia and NGOs supported by the Government. She also believes that a key element is also to introduce policies which allow for “context-based construction” by incorporating local technologies and materials which are more familiar to the communities.\nAs cities continue to expand, it is essential to ensure that marginalized communities have access to safe and healthy living environments that provide adequate infrastructure and services. The built environment plays a crucial role in shaping our physical and social surroundings and can have a significant impact on community health and well-being.\nDesign of homes should address grassroots-level planning to incorporate active ventilation and passive cooling aspects through the provision of openings at the right locations and of the right sizes. In India, low-income housing rarely has temperature control systems such as heaters and air conditioners since the residents cannot afford the cost to power them. A well-ventilated home is cooler in the summer, retains heat during the cooler months and also provides relief from air pollution and stagnant carbon dioxide.\nAdditionally, integrating green infrastructure and sustainable technologies can reduce the carbon footprint and enhance the resilience of the built environment to natural disasters and climate change. Building resilience through the built environment, especially homes, for low-income communities on the margins is critical for promoting social equity and sustainable development.\nFurthermore, policymakers and decision-makers must recognize the importance of investing in resilient built environments for low-income communities. This includes providing financial resources, incentives, and regulatory frameworks that support the development of sustainable and resilient infrastructure. Public-private partnerships can also help mobilize resources and leverage the expertise of the private sector to support community-led initiatives.\nIn conclusion, building resilience through the built environment for low-income communities on the margins is a complex process that requires a long-term vision, strong partnerships, and community empowerment.\n UN-Habitat, State of the World’s Cities, 2006\nFeatured Image: Sistak, CC BY-SA 2.0 https://creativecommons.org/licenses/by-sa/2.0, via Wikimedia Commons\n The Wire\nAll the opinions expressed in the article are the contributors’ own and do not represent any institute or organisation.\nAbout the Author\nVandana C Padmanabhan is a Civil Engineer by profession, who uses her 15 years of engineering education and experience to build climate resilient communities. She holds a Bachelor’s degree in Civil Engineering, followed by a Master’s in Construction Technology and Management from IIT Madras. Her Doctoral Dissertation from IIT Delhi is in the area of durability of cement based materials.\nShe works at Community Design Agency with architects and designers to build well engineered and safe homes, while addressing issues of social, cultural and economic marginalization of some of the world’s most vulnerable populations. She works towards providing spatial equity and access to engineering and technology services, to address larger systemic issues, such as gender, climate vulnerability, livelihoods insecurity and identity. This work is creating new benchmarks for housing for low income communities. As a materials researcher, Vandana also works on developing durable and energy efficient construction materials to support the construction of better buildings', 'Development of a help mixture by digital networking of consulting and support services\nSocial space-oriented supply concepts (e. g. neighbourhood concepts) address specific local conditions and aim at the participatory advanced development of a socially-oriented neighbourhood in the areas of housing, social affairs, help and care. In recent years, they have established themselves as an approach to tackling demographic challenges. Up to now, the possibility of digital tools for supporting neighbourhood concepts has hardly been used – but they offer many advantages for promoting social interaction, broad participation and efficient neighbourhood management.\nIn the district of Rauner in Kirchheim unter Teck, the SONIA project created structures with an online platform that has successfully established the creation of a community for senior citizens. By linking the virtual space with the real social space, social participation of elderly people in the immediate environment is supported by establishing contacts, communicating information and joint activities. Based on these structures, the SONIAnetz project aims to develop new ways of supporting people in need of help, relieving the burden on caregiving relatives and facilitating social participation.\nTo this end, the existing platform is to be improved and extended to include the “care and nursing network” aspect.\nDevelopment Centre “good ageing” GmbH (Entwicklungszentrum Gut Altwerden) (Project Management)\nFraunhofer Institute for Industrial Engineering IAO\nOnline platform for the support of neighbourhood concepts\nSocial space-oriented supply concepts (e. g. neighbourhood concepts) address specific local conditions and aim at the participatory advanced development of a social proximity in the areas of housing, social affairs, help and care. In recent years, they have established themselves as an approach to tackling demographic challenges. Up to now, the possibility of digital tools for supporting neighbourhood concepts has hardly been used – but they offer many advantages for promoting social interaction, broad participation and efficient neighbourhood management. Within the scope of the SONIA and SONIAnetz projects, IMTT has developed and tested an online platform for supporting neighbourhood structures in cooperation with users. The platform complements activities in the social space with online opportunities for communication and social interaction. The platform provides the residents of the neighbourhood with a specific “neighbourhood app” that\noffers easy and secure access to selected online services (e. g. video communication, traffic information, etc.) and thus promotes participation in the digital world, even for elderly people without computer and Internet experience.\nenables them to inform themselves about news and offers in the neighbourhood\nenables them to communicate with other people in the neighbourhood, both within groups (e. g. clubs, neighbourhoods, hiking groups) and within the entire neighbourhood\nAs a local social network, the platform provides functions for group building, virtual bulletin boards and the exchange of messages in the neighbourhood. For neighbourhood management, the platform offers the possibility to\ncreate transparency about existing offers of all actors in the neighbourhood\ndisseminate information about offers and activities in the neighbourhood (events, activities, etc.) and thus improve the use of offers\ncommunicate efficiently with the citizens of the neighbourhood (citizens’ participation)\nIn contrast to commercial neighbourhood portals on the Internet, the platform offers complete control by local social institutions which operate the platform themselves and thus avoid the collection of personal data (e. g. for advertising purposes).\nImpressions of project work in the WirImQuartier context\nOnline platform for supporting help-mix structures in ambulant care\nMost patients want to be cared for as long as possible at home. Although family members feel strongly committed to contributing to nursing care, they are faced with enormous challenges due to increasing employment and geographical distances. In the future, home care will therefore depend to a large extent on a successful care mix, e.g. a combination of informal (family or neighbours) help and professional care. The supply mix also includes transport services, meals on wheels or neighbourly help. Although civic engagement and neighbourly support can also come about on their own, they generally benefit from professional support, e. g. within the framework of organised neighbourhood assistance. Professional services are also necessary to supplement and stabilise private aid.\nUntil now, communication and coordination in aid networks has often been perceived as a burden. Difficult accessibility by telephone, paperwork, scheduling problems and lack of transparency about tasks and activities of other actors in the aid network are often cited as problem areas. The mobile application CareCircle supports the cooperation of all actors involved in a relief network by providing communication and coordination functions. Professional caregivers, relatives and citizens can thus\ngain an overview of the participants in the aid network and their role\nshare case-related information via a pinboard\nuse it to coordinate processes with individual or all actors in the network over the network\ncreate instant messages with it\ncopy case-related appointments to your own calendar\nThe special feature of this platform is that it can also be used to map professional support options and structures of organised neighbourhood help. For example, helpers can easily manage several networks. Questions and requests from the network can also be forwarded to a coordinating office (or case manager) if required, who will then provide appropriate support.\nImpressions of project work in the context of CareCircle']	['<urn:uuid:05260c77-4702-4214-9b52-32449d65db2b>', '<urn:uuid:c9f8efd1-7f72-40c7-8ea9-cf86a9d873a4>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	33	107	2559
32	kpi mobile marketing social media differences	KPIs differ significantly between mobile marketing and social media campaigns. For mobile marketing, key metrics include user sessions per day, average time spent in app, and conversion rates from free to paid subscriptions. In contrast, social media marketing KPIs focus more on elements like ad clicks, impressions, click-through rates, and engagement levels. While both channels track conversions, mobile apps specifically emphasize user retention and session metrics, whereas social platforms prioritize interaction and reach metrics. These differences reflect the distinct nature and goals of each marketing channel.	"[""One of the crucial ingredients in a successful online campaign is accurate measurement of metrics and conversions. With multiple brands, and/or multiple campaigns, measuring a campaign’s metrics and conversions can be difficult. But the process of conversion tracking enables marketers to identify and use information about how consumers interact with a brand. Let’s learn more about what conversions are In the sales and marketing funnel, how conversion tracking works and what types of KPIs to integrate into your campaign strategies.\nWhat are Conversions?\nFirst off, conversions (aka ‘conversion marketing’) are the results to ‘convert’ a web visitor to take an action regarding your offer, product or service. In other words, conversions are what happens after the click. Or restated slightly, conversions are any type of positive actions taken by a web visitor after they arrive at your site. Do they take the desired action you're looking for?\nHere are some of the normal ‘after the click’ conversions that occur with web visitors: ● Fill out a form or survey ● Visit a page for a chance to win a prize ● View your sign-up or landing location page ● Sign up for a newsletter or download a study ● Make a purchase ● Contact customer support\nThese conversions above are common ones, but the type of conversions your site may offer is entirely dependent on your business or purpose. Ultimately, you determine what is a meaningful conversion for your site. And then it makes sense to track these types of conversions. Which brings us to...\nWhat is Conversion Tracking?\nConversion tracking is the process used to track conversions across your media channels to measure your company’s return on investment. You can use conversion tracking in a variety of marketing channels. All the major online advertising platforms - Google Adwords, Twitter or Facebook Lead ads - can help marketers with tracking conversions.\nDoing this gives gives marketers measurable results about how your organic marketing initiatives or paid media spending is paying off. Conversion tracking can show your team the numbers of newsletter sign-ups, presentation downloads, website purchases or other activity you’ve designated as a valuable online activity for your site. These numbers can be measured against your campaign’s KPIs to assess the results of your efforts.\nKPIs in SEO/SEM/Mobile/Online Marketing\nIdeal KPIs (key performance indicators) for measuring online campaigns depends again on your business and industry. You might want different key performance indicators for SEO or SEM than for social media marketing.\nSite visits, impressions, bounce rates (how often and how quickly visitors move off your site), conversions and revenue are among the common SEO metrics to track in conversion tracking. Other common KPIs include your web page loading speed. Any pages taking more than 3-4 seconds to load can be the kiss of death for some marketers. Track your site’s indexed pages, too. You want to know if Google is showing your pages in its its engine. Check this at the Google Search Console.\nYour search engine marketing needs KPIs too. You’ll want to track ad clicks, impressions, click-through rates and cost per acquisition. Your team should make a daily process of tracking your ad campaigns’ performance metrics on Google, Bing, Yahoo, Facebook or Twitter. These could include tracking your keyword’s success rate, your bid management and even optimizing your landing pages for higher conversions. Have a look at these KPIs for B2B marketers.\nKPIs to track conversions in mobile marketing activity might include tracking a user’s session per day on your app, the user’s average time spent on the app, or maybe the number of free subscribers that convert to a pay subscription model. Each of these indicators can help you assess the performance of a mobile campaign. Checking KPIs for your display ad campaigns might be different. One size does not always suit all channels. A few common metrics include click-throughs, engagement levels, and length of time the ad was viewed.\nMeasuring your metrics and conversions will help you to make smarter decisions about your online marketing. Plan out your KPIs as part of your campaign strategy, and implement your tactics for SEO, SEM, social media marketing and more. It may take a few campaigns to fully understand the value or metrics and KPIs in your advertising and marketing efforts. But with the right direction, you can achieve success and improvement over time."", 'How well is your mobile app really performing? These essential engagement and profitability metrics are the best way to find out.\nHow do you really find out if your mobile app is doing well?\nThe answer is simple: data.\nJust like any other marketing channel, mobile comes with a set of metrics that help marketers differentiate between success and failure.\nIt’s crucial to track and analyze these metrics because they help you understand how users interact with your app and why.\nThey can also help you discover if your app is making money.\nWhy is that necessary? Because you need to fully understand how your mobile app performs before you can improve it.\n“But there’s so much data out there – daily active users (DAU), monthly active users (MAU), return on investment (ROI) – what’s more important?”\nFor someone who is new at mobile app analytics, it’s easy to get confused. Especially with all those acronyms. But don’t worry, that’s why we are here to narrow it down for you.\nThis article will help you discover some of the key metrics you should track for your mobile app, along with steps on how to measure them.\nWe will cover:\n- Usage & Engagement Metrics\n- Profitability Metrics\n- The Golden Metric\nApp Usage and Engagement Metrics\nWith increasing competition in the mobile app market, it’s important for marketers to understand how people engage with their app and what makes them continue to use it.\nIn this section, we will cover some of the key usage and engagement metrics you need to track for your mobile app.\n1. Downloads and Installs\nYou should always know how many people have installed your app.\nHowever, keep in mind that this metric alone will never be able to tell you about the real success of your mobile app. What if users downloaded your app because of a certain marketing campaign and never used it again or uninstalled it right away?\nStill, the number of app installs is important because it lays the foundation for all other metrics you should track and analyze.\nHow to Track App Downloads and Installs\nYou can easily set up app install tracking using Google Analytics for both Android and iOS. Here is a step-by-step guide on how to do that.\n2. App Acquisition\nSo, you know how many people download your app. But, do you know where those downloads came from? That’s what app acquisition tells you.\nApp acquisition shows the effectiveness of your marketing campaigns and channels and will help you make the most of your marketing spend.\nFor example, if you’re running multiple ads or campaigns, this metric will help you find what’s working and what’s not, so you can focus on investing in the problem areas.\nHow to Track App Acquisition\nHere’s how Mixpanel typically categorizes the campaign source of your users:\nThe graph shows how app users found your app – Facebook ads, Twitter ads, AdMob, Brightroll, Tapjoy.\n3. Active Users\nNot all people who download your app are active users. Active users regularly open and engage with your app.\nThis metric is important because it shows how engaging and useful your app is to users.\nSo, what classifies a user as “active”? It depends on the analytics tool you use.\nAccording to Google Analytics, an active user is one who has opened and interacted with an app within the last 30 days.\nActive users can be segmented into Daily Active Users (DAU) and Monthly Active Users (MAU).\n- DAU measures the number of users who have a session with your app at least once a day\n- MAU is the number of users who have a session at least once a month\nHow to Track Active App Users\nYou can measure DAU and MAU using most analytics tool.\nHere is how Google Analytics typically shows active mobile app users:\nThe dark blue line shows 7-day active users, or the number of unique users who initiated sessions on your site or app over the last 7 days. The light blue line shows 14-day active users.\nThe stickiness ratio shows how often users come back to your app.\nTo calculate stickiness, you just divide the DAU by the MAU. Multiply the result by 100 to get a percentage.\nHow to Track App Stickiness\nStickiness = (Daily Active Users / Monthly Active Users ) x 100\nThe higher the percentage, the better. It means you have more MAUs sticking around long enough to become DAUs.\n5. Average Daily Sessions Per Daily Active User (DAU)\nAverage daily sessions per DAU shows how many times on average users open and engage with your app in one day.\nSo, what’s a good number of average daily sessions per daily active user? It depends.\nSocial media apps such as Facebook, Instagram or WhatsApp want their users to have a higher number of daily sessions on average.\nOther apps that don’t need to be used as much to be successful may be happy with a lower number.\nHow to Track Average Daily Sessions Per DAU\nAverage Daily Sessions Per DAU = Number of Daily Sessions / Number of DAU\n6. Average Session Length\nThe average app session length tells you how much time a typical user spends interacting with your app in a single session.\nThis metric is important because it tells you how engaged your users are. For example, if you have an e-commerce app, are users browsing products for longer periods of time? Or, do they close the app after just a few seconds?\nHow to Track Average App Session Length\nYou can track average session length using Google Analytics or any other analytics tool. Here is how to use Google Analytics to track average app session.\n7. Screen Flow\nScreen flow analysis breaks down how users interact with your app, screen-by-screen.\nHere is what a typical report looks like:\nFor example, you can track exits by screen, find out the navigation path users take through different screens, and the total number of visits for each screen.\nScreen flow highlights the problem areas in your app, such as the screen users interact with before they exit your app.\nYou can use this data to improve the problem areas and re-engage lost users.\nHow to Track App Screen Flow\nGoogle Analytics is a great tool to help you track screen flow.\n8. Retention Rate\nApp retention rate measures how many users return to your app after using your app at another time. Simply put, retention measures how many users you retain after a set period of time.\nPeople get bored easily. Retaining app users over time is one of the biggest challenges on your way to success.\nThis metric also may be useful if you implement new features or release a new update. It can help you find out if the new updates or features are keeping users engaged or driving them away.\nHow to Track App Retention\nApp Retention Rate % = (Number Users Retained at End of Time Period / Total Users at Start of Time Period ) x 100\nYou can also check out this step-by-step guide on how to measure retention with Google Analytics.\n9. Churn Rate\nApp churn rate is the opposite of retention rate.\nIt tells you how many users stopped using your app after a given time.\nHow to Track App Churn Rate\n1 – Retention Rate = Churn Rate\nTrack how much money you make with your mobile app.\nHere are some key profitability metrics you should track for your mobile app.\n10. Average Revenue Per User (ARPU)\nThis metric tells you how much revenue is being generated on average by each user in the form of subscriptions, in-app purchases, paid downloads, ad clicks, or any other app monetization channel.\nThis figure is important because it tells you the overall value a single user brings to your app.\nHow to Track Average Revenue Per User (ARPU)\nAverage Revenue Per User (ARPU) = Total App Revenue in Given Time / Number Users in Given Time\n11. Cost Per Acquisition (CPA)\nIn the beginning, it might seem like the total number of downloads or conversions are enough to tell you how well your app is performing.\nBut you need to know how much acquiring new users costs as well.\nHow to Track Cost Per Acquisition (CPA)\nCost Per Acquisition (CPA) = Total Cost of Campaign / Total Acquisitions or Conversions\n12. Return On Investment (ROI)\nROI measures the money you make (return) on the money you spend building and marketing your app.\nHow to Track App ROI\nApp Return on Investment (ROI) % = (Gain From investment – Cost of Investment ) / Cost of Investment) x 100\n13. Lifetime Value (LTV)\nApp lifetime value (LTV) helps you identify how much value each customer brings to your app.\nRemember, your LTV should always be greater than your CPA. If it isn’t, it probably means you are paying too much for your customers.\nThe average number of years a customer stays with you, also known as the average customer lifetime, varies depending on the type of customers you have.\nFor example, if your target customers are aged 18-24, they might grow out of that age group after 4 or 5 years on average.\nHow to Track App Lifetime Value (LTV)\nApp Lifetime Value (LTV) = Average Value of Conversion x Average Number of Conversions x Average Customer Lifetime\n14. The ‘Golden’ Metric\nNo matter how many metrics you track, the app star rating is by far the most important (and obvious) one. You should focus on ways to increase mobile app reviews.\nPeople almost always check an app’s rating before they decide to download it.\nRegardless of how much you invest in all other aspects of your app, if it’s poorly rated on the App Store or Google Play, it will end up driving potential users away.\nFocus on the Metrics That Matter\nGoing through mobile analytics might seem easy, but you need to really understand what you’re looking for before you dive into the data.\nBe clear on the goals you set for your app, and don’t waste too much time on metrics that don’t matter. Try to focus only on the key areas that help you achieve your goals.']"	['<urn:uuid:9c4ee3c5-bb8c-46e3-bc4a-e184fe6a5c13>', '<urn:uuid:575bf188-466d-4c6d-a63a-84337bb0bf20>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T01:37:09.743768	6	86	2440
33	What's the difference between gross and fine motor development?	Fine motor skills involve movements of small muscles in hands, fingers, feet and toes, while gross motor skills involve large muscles in the body, arms and legs. Fine motor skills require precision and hand-eye coordination, while gross motor skills relate to larger body movements and coordination.	['This Area of Learning relates to the development of children’s body control and coordination of large movements, fine manipulative skills, spatial awareness and balance. It also focuses on children’s knowledge and understanding of a healthy lifestyle upon which physical well-being depends.\nPhysical development focuses on increasing the skill and performance of the body. Physical and cognitive development are closely linked, especially during the early years. Problems with a child’s physical development can be an indication that the child may have some learning difficulties. Physical development can be divided into gross motor skills and fine manipulative skills. Throughout the Foundation Phase, children acquire and develop their skills in many ways.\nBoth growth and development depend on suitable nourishment, including a balance of the right foods and sufficient water to drink. Increasing control and coordination is enabled by the maturing brain and nervous system, growing bones and muscles, exercise and physical activity.\nThe sequence of physical development involves firstly gross motor skills that require control of large muscles in the body, arms and legs. This is followed by development of fine manipulative skills, which depend on small muscle coordination. As children progress and become more confident, improvement in coordination of gross and fine movements will continue to develop and new skills will be learned. Children cannot learn a new skill until the muscles are sufficiently developed, and the activities and resources provided should be suitable for their developmental needs.\nThe ages at which children accomplish particular physical skills can vary considerably. Most children will progress through the same stages of development at their own rate. The timing of this development can depend on different influencing factors such as eating habits, emotional development and confidence in tackling new activities\nPhysical skills, body and spatial awareness contribute to a child’s personal and social development by enhancing confidence and self-esteem. Young children are active learners who enjoy learning through play and physical activities. During play children engage in learning experiences that require them to use a range of physical skills whether playing indoors or outdoors. Physical activities can be incorporated into every Area of Learning, from simple action rhymes and games in Mathematical Development to large movements in response to creative music. The development of children’s physical skills should be developed holistically across all Areas of Learning.\nAppropriate space is essential for physical movement both indoors and outdoors in order that children can use their bodies actively to gain spatial awareness and experiment with movement without restrictions. Children need access to large and small equipment that they can get out and put away themselves, for use independently or with a practitioner. All forms of physical activity also require time for children to develop and practise skills.\nChildren need opportunities to:* work alone or collaboratively\n* work at their own pace to practise and consolidate their skills\n* extend their skills and move on in their development\n* make connections between skills acquired indoors and outdoors.\nParents/carers Practitioners in the setting/school should share their observations of children’s physical skills with parents/carers and encourage them to: * * * * *\n* provide opportunities for children to join out-of-school clubs and holiday schemes that extend their interest in physical activities and sports participate in home tasks that promote physical skills be aware of the importance of healthy eating.\nPractitioners in the setting/school should share their observations of children’s physical skills with parents/carers and encourage them to:', 'Fine motor skills refer to the skills we have in making small movements and adjustments using the small muscles in our hands, wrists, fingers, face, feet, and toes.\nThese skills require strength in our smaller muscles, fine motor control, hand-eye coordination, and dexterity. All of these are crucial for a range of everyday but nevertheless complex activities like cutting, carrying, holding, feeding ourselves, drawing, writing, typing, and so on.\nThe acquisition and refinement of these skills are a critical part of a child’s development, but are also used throughout adulthood. While learning the skills, we exercise significant cognitive load and elaborative encoding, but once learned, we can perform many fine motor tasks almost instinctively.\nFine Motor Skills Definition\nFine motor skills can be succinctly defined as the coordination of small muscles, in movements—usually involving the synchronization of hands and fingers—with the eyes.\nSome scholarly definitions include:\n“Fine motor skills involve movements of the small muscles, especially those of the eyes, speech musculature, hands, fingers, feet, and toes.” (Collins & O’Brien, 2003)\nThis definition underscores the complex orchestration of muscular, neurological, and perceptual systems at work during fine motor tasks.\nOther definitions will often define it in contrast to gross motor skills, as with this definition from Biddle and colleagues:\n“Fine motor skills involve the small muscles of the hands and fingers, while gross motor skills involve large muscles of the arms, legs, and torso.” (Biddle et al., 2013)\nKey features of fine motor skills include the use of smaller muscle groups (as opposed to large, whole-body movements), precision in execution, and often a necessary component of visual-motor integration.\nFine motor skills are essential for performing everyday tasks and also underpin the development of self-help skills in children.\nFine Motor Skills Examples\n- Pencil Grasp: When holding a pencil, the thumb, index, and middle fingers coordinate to apply the right amount of pressure. This allows us to control the pencil for drawing or writing. It’s a skill typically developed in early childhood (ages 3-7), and are gradually refined to enable more precise and intricate pencil control.\n- Using Chopsticks: Handling chopsticks requires a lot of dexterity and, in the Western world, is sometimes not developed until adulthood. However, in many Asian cultures, children develop the ability to handle chopsticks at a very young age due to early exposure.\n- Buttoning a Shirt: Buttoning shirts requires the dexterity to hold the button between the thumb and index finger, while also using the other hand to stabilize the fabric and slip the buttonhole over the button itself. This skill involves not only skilled hand-eye coordination, but also a range of muscle memory, and tactile sensitivity to ‘feel your way’.\n- Using Scissors: Handling scissors necessitates a complex set of hand movements and visual guidance. The thumb, index, and middle fingers work in concert to open and close the scissors, while the eyes guide the cutting path. We generally provide young children with ‘safety scissors’ to practice cutting before moving onto scissors that are more heavy duty but dangerous without fine motor skills.\n- Threading Beads: This activity is commonly used in preschools and kindergartens to explicitly encourage the development of fine motor and hand-eye coordination skills. Threading a string through the hole of a bead requires a pincer grasp as well as the complex ability to gauge depth and distance.\n- Tying Shoelaces: Children often use snap-shut or slip-on shoes until they have the dexterity to tie shoelaces. In preschools, they’re often given help in developing this fine motor skill by practicing tying knots in string before proceeding to their own laces.\n- Turning a Key: Inserting a key into a lock and turning it requires precision, hand-eye coordination, and the ability to apply the right amount of force.\n- Playing a Musical Instrument: Fingeroperated musical instruments such as pianos and guitars require refined control over hands and fingers. There’s also an element of size here – where children with small hands won’t be able to use the instruments as intended yet.\n- Sewing: Needlework necessitates careful coordination and control, involving precise movement of the fingers to thread a needle and make stitches. As people improve their dexterity in sewing, they can move to more complex patterns and styles.\n- Using a Computer Mouse: Young children often find it hard at first to navigate computers with a mouse, often opting to use touchscreens until they have the dexterity to operate the mouse.\n- Pouring Liquid from a Jug: We often pour water for children until they develop both the physical strength (tied to gross motor skills) and the fine motor skill of carefully tipping the jug so water comes out at the right speed to successfully transfer water to a cup without spilling.\n- Peeling a Sticker: Successfully peeling a sticker off a sheet calls for a precise pinching motion and the ability to regulate force to avoid tearing. This is a skill often developed early on, when we give children stickers as token rewards for good behavior.\n- Applying Makeup: Applying eyeliner or mascara necessitates very precise hand-eye coordination and a steady hand. This is complicated by the fact that we can’t directly see ourselves when applying makeup, having to view through a mirror where our movements are reversed in front of our own etes.\n- Folding Paper: Folding paper (as in origami) demands precision and accuracy in both the fingers and the eyes. A child might start off with paper planes before moving onto more complex constructions like origami cranes.\n- Brushing Teeth: This involves holding a toothbrush correctly and making precise brushing motions. Children are encouraged to brush gently at first because imprecise and rough movements may cause bleeding of the gums.\n- Building a Tower of Blocks: This requires the hand-eye coordination to stack blocks without toppling them, and the finger control to align them correctly. We see this, for example, in games like Jenga.\n- Manipulating a Smartphone: Using a touchscreen requires precise finger movements and hand-eye coordination to successfully select icons, type text, and perform other tasks. Children may learn this when playing games on their parents’ phones or on a tablet computer.\n- Cutting with a Knife: Whether cutting paper or food, this skill requires a strong grip, hand-eye coordination, and the ability to regulate pressure. Because this is a particularly dangerous activity, children often don’t get practice at it until they are older.\n- Lacing a Thread through a Needle: This requires a high degree of precision, control, and hand-eye coordination that is even difficult for adults to master consistently..\n- Writing on a Chalkboard: Writing on a vertical surface demands wrist movement, hand control, and finger strength. The child will also need to be able to apply the right amount of pressure to get the right results on the board but not break the chalk!\n- Typing on a Keyboard: Touch typing involves all the fingers, each responsible for precise key pressing, coordinated with visual cues from the screen. Because of how common typing is these days, many adults develop the capacity to reflexively touch type, using minimal cognitive load because it becomes a simple matter of automatic processing due to muscle memory.\n- Creating Artwork with a Paintbrush: Holding a paintbrush and creating strokes demands a combination of hand-eye coordination and finger dexterity. Clearly, when we open a child’s coloring book, we can see that it’s a child’s work due to the amount of times they’ve colored outside the lines.\n- Doing a Puzzle: Assembling a puzzle calls for visual perception, hand-eye coordination, and fine finger movements to fit pieces together. In Jigsaws, for example, children will start with larger puzzle pieces until they develop the skills to move down to smaller pieces.\n- Opening a Zipper: Pulling a zipper up or down necessitates a pincer grasp and enough strength to move the zipper. Feeding the zipper pieces together is an added level of complexity on top of this.\n- Cooking: Many aspects of cooking, such as stirring, peeling vegetables, or cracking an egg, call for various fine motor skills. Cooking with parents isn’t just about learning about food and cooking; it’s also a task in learning fine motor skills.\n- Planting Seeds: This activity requires the finger dexterity to pick up and carefully place small seeds, as well as the hand-eye coordination to plant them in the right location.\nFine Motor Skills Milestones for Children\nMost theories of children’s physical development hold that fine motor skills development in children progresses in a generally predictable pattern, albeit with individual differences based on the child’s unique growth and experiences.\nInfants (0-12 Months)\n- Grasping Reflex (0-2 Months): Newborns will instinctively close their hand around a finger or object that strokes their palm.\n- Voluntary Grasping (3-4 Months): Infants start to reach for and grasp objects intentionally, albeit with a rather clumsy, whole-hand “raking” motion.\n- Transfer Object Between Hands (5-6 Months): Babies begin to pass toys or objects from one hand to the other, demonstrating improved hand-eye coordination.\n- Pincer Grasp (9-12 Months): Infants develop the ability to pick up small objects using their thumb and index finger, showcasing improved finger dexterity.\nToddlers (1-3 Years)\n- Self-Feeding (12-18 Months): Toddlers start using their fingers to feed themselves and will eventually begin to use utensils.\n- Stacking Blocks (18-24 Months): They gain the ability to stack a few blocks on top of each other, demonstrating improved hand-eye coordination and control.\n- Drawing and Scribbling (2-3 Years): Toddlers start making marks on paper, initially random scribbles but progressively evolving into more controlled lines and circles.\nPreschoolers (3-5 Years)\n- Cutting with Scissors (3-4 Years): Children begin to use scissors, initially with assistance and eventually independently.\n- Drawing a Person (4-5 Years): Children start to draw recognizable figures, such as a person with a head, body, arms, and legs.\n- Writing Some Letters and Numbers (5 Years): By this age, most children can write some letters and numbers, and may even write their own name.\nIt’s essential to remember that these milestones serve as guidelines rather than rigid expectations. Some children may develop certain skills faster or slower than their peers.\nFine Motor Skills vs Gross Motor Skills\nFine motor skills and gross motor skills are often framed as opposing but equally important skills, where gross motor skills are useful for tasks like running and fine motor for tasks like drawing.\n- Fine motor skills involve the coordination of small muscle groups, primarily in the hands and fingers, often in concert with visual input, to achieve fine manipulation tasks.\n- Gross motor skills are exercised when we engage large muscle groups to perform big movements, often in bursting motions, like when walking, running, jumping, and throwing.\nPsychomotor theorist Rudolf Laban, renowned for his work on movement analysis, posited that gross motor movements (which he categorized as “locomotion,” “manipulation,” and “stabilization”) are foundational to human movement.\nHowever, he also recognized the importance of the intricate, precise movements embodied in fine motor skills.\nGross motor skills typically develop before fine motor skills, in a process that we call proximodistal development.\nFor example, infants will learn first to roll over, sit up, and eventually walk – each being gross motor skills. Later, they will develop more delicate skills such as grasping objects, buttoning clothes, or drawing, which as seen above are examples of fine motor skills.\nDespite the differences, the development of both skill sets are interconnected. The stability provided by gross motor skill development can significantly support the development and execution of fine motor tasks.\nOccupational therapist Anna Jean Ayres contributed significantly to this understanding through her sensory integration theory, which postulates a strong relationship between sensory processing, sensory memory, gross motor function, and fine motor performance.\nEssentially, our ability to make sense of the sensory information around us influences our motor responses, both gross and fine.\n|Fine Motor Skills||Gross Motor Skills|\n|Definition||Coordination of small muscles primarily in the hands and fingers, often in concert with the eyes||Utilization of large muscle groups for big movement activities|\n|Example||Writing with a pencil, threading beads||Walking, jumping, throwing|\n|Key Features||Involves smaller muscle groups, precision in execution, visual-motor integration||Involves larger muscle groups, body coordination, balance|\n|Development||Typically develop after gross motor skills, requiring a higher level of control and precision||Usually develop earlier, foundational for many physical activities and certain fine motor tasks|\n|Theorists/Contributors||Anna Jean Ayres (Sensory Integration Theory)||Rudolf Laban (Laban Movement Analysis)|\nFine motor skills are assessed as developing within a generally normal range for children, although there is variation both between individual children and across cultural groups (e.g. some cultural groups encourage children to perform specific fine motor skills earlier than others, leading to faster acquisition). But overall, children will develop these skills at their own pace, and should be given plenty of play time to practice developing these fine motor skills.\nBiddle, K. A. G., Garcia-Nevarez, A., Henderson, W. J. R., & Valero-Kerrick, A. (2013). Early childhood education: Becoming a professional. London: Sage.\nCollins, J. W., & O’brien, N. P. (2011). The Greenwood dictionary of education. New York: ABC-CLIO.\nDr. Chris Drew is the founder of the Helpful Professor. He holds a PhD in education and has published over 20 articles in scholarly journals. He is the former editor of the Journal of Learning Development in Higher Education. [Image Descriptor: Photo of Chris]']	['<urn:uuid:8e8b5f81-13e6-49c1-ae15-8bb2f718d50f>', '<urn:uuid:33ab528d-39c3-4d28-a1cc-163331c55a25>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T01:37:09.743768	9	46	2789
34	cannabis oil versus cannabis tinctures consumption time differences medical effect onset details	Cannabis oils take around 45 minutes to start working, while marijuana tinctures, which are consumed sublingually and prepared by dissolving the drug in alcohol, can start working within 15 minutes. This makes tinctures faster-acting compared to oils.	['US: 1.646.568.9797 (24x5)\nUK: 44.330.808.0580 (24x5)\nWorldwide: 91.703.051.960 (24x7)\nReport Overview: Cannabis Extract MarketThe global cannabis extract market size was valued at USD 6.2 billion in 2020 and is anticipated to register a CAGR of 15.5% over the forecast period 2021-2027. The key factors driving this market is the increasing adoption of medical marijuana for the treatment of chronic diseases like arthritis, Alzheimer’s, and cancer along with mental disorders such as depression, anxiety, and epilepsy. Another factor anticipated to propel the market growth is the rising legalization of medical cannabis in various countries that enables easy access to the patients. Increasing the adoption of marijuana for medical and recreational purposes has been booming the overall market growth. Legalization of marijuana in some countries has resulted in rising product purchases by legal means thereby, declining its black market sales. Tax charged by the governments is also expected to create revenue generation opportunities for the countries. Cannabis oil and tinctures are generally used for treatment of various conditions like psychological disorders, nausea, and cancer. In terms of legality, laws pertaining to extracts are quite lenient due to non-psychoactive properties of Cannabidiol (CBD) oils compared to other products such as buds and resins. Due to their easy consumption in vapor or in the form of gummies and slowly diminishing social stigma of smoking cannabis buds, these products are anticipated to gain momentum and the increase in demand in the forthcoming years. Rising prevalence of cancer is also a crucial factor expected to drive the market for cannabis extract during the forecast period. According to the World Health Organization (WHO), in 2015, cancer was the second leading cause of deaths worldwide with around 8.8 million deaths globally. Increasing requirement of therapies related to pain management due to growing disease burden of chronic pain is anticipated to drive the medical marijuana market in near future. Moreover, due to age limitations and the exact calculation of dose, kids have prescribed cannabis oils instead of buds.\nProduct Insights: Cannabis Extract MarketThe oil segment held the largest revenue share in the cannabis extract market and was valued at USD 4.8 billion in 2019. This segment is anticipated to witness the fastest CAGR over the forecast period. This is primarily due to the increasing medical applications of the product. Tracking the accurate consumption of prescribed dosage that includes Tetrahydrocannabinol (THC), CBD, and other components are crucial. Therefore, the preference for cannabis oils and extracts is comparatively higher as quantifying the accuracy through smoked cannabis. Marijuana tinctures are consumed sublingually and are prepared by dissolving the drug in alcohol. Tinctures can start working within 15 minutes, which is faster than oils which take around 45 minutes to start working. Therefore, the demand for tinctures is expected to witness significant rise over the forecast period. The tinctures segment is expected to register a CAGR of 13.4% over the forecast period. Changing perspective of healthcare practitioners regarding safer ingestion techniques such as oral consumption or in the form of beverages or gummies is anticipated to drive the product demand. Majority of producers have started expanding their portfolios in extracts, due to significant demand in end-use industries, such as pharmaceuticals, nutraceuticals, skincare, beverages, and edibles. These products are marketed with specific information on content of TCH and CBD or sole CBD for medical purposes.\nExtract Type Insights: Cannabis Extract MarketIn 2019, full-spectrum accounted for the largest revenue share in the cannabis concentrates market and was valued at USD 4.3 billion. This segment is anticipated to register the highest CAGR of more than 20.0% over the forecast period. The full-spectrum extract includes the complete cannabinoids range that can be separated from a marijuana plant. There are more than 100 other cannabinoids present in their authentic forms apart from CBD. Due to their availability, full-spectrum extracts produce an entourage effect which is a collaborative relationship between terpenes and cannabinoids that intensifies the healing properties of cannabinoid. Cannabis isolates are the most reliable forms of their respective cannabinoids and are obtained by isolation and extraction. They have comparatively lesser health benefits than full-spectrum cannabis extracts. Moreover, these isolates retain various advantages resulting in their rising adoption over the years. Their purest form tests negative for THC and possesses no psychoactive effect owing to scarce/no THC content. Due to strict regulations for THC in many countries, the adoption full spectrum or isolates is comparatively low. Physicians are also prescribing the components of medical marijuana owing to its entourage effect. Moreover, the rising pressure of the medical community on the legal bodies to legalize the full spectrum products is anticipated to positively influence the demand for full-spectrum extracts in the future.\nRegional Insights: Cannabis Extract MarketIn 2019, North America held the highest revenue share and was valued at USD 5.3 billion owing to higher product adoption and its utilization due to legalization of marijuana in the region. Ten years ago, the legalization was favored by 32% of the overall population and opposed by 60%. In 2018, medical marijuana was legalized in 30 states of the U.S., including Guam, Puerto Rico, and Columbia. According to recent statistics, about 57.0% of adults in the U.S. favor the use of marijuana for medical purposes like the treatment of pain, inflammation, and cancer. Therefore, factors such as legalization of cannabis, increasing support of the citizens, and rising product adoption for medicinal purposes will boost the regional market growth. In 2019, Europe accounted for the second largest revenue share. Italy is regarded as the fourth largest consumer of cannabis oil and concentrates. However, the legalization in European countries, accompanied by stringent rules and regulations regarding the sale and cultivation, may limit the overall growth in Europe and in turn for the global market. Other promising markets for marijuana are Australia, Germany, Poland, Colombia, Uruguay, and Israel. Israel is anticipated to exhibit a CAGR of 23.3% from 2020 to 2027, making it a highly profitable market over the forecast period. Factors, such as innovation, increased research activity, a well-developed ecosystem, and liberal government policies are attributing to this growth. The government invested around USD 2.13 Million in January 2017 in 13 different research projects in order to study the effect of cannabis on conditions like rejection of transplanted organ and metabolic syndrome. Increasing R&D investments is also anticipated to increase the market of medical marijuana over the forecast period. Cannabis Extract Market Share Insights\nare the major players that are operating in the cannabis market. These players have established strategic partnerships with extractors and other parties in the value chain to significantly strengthen their market position. Canopy Growth Corporation, for instance, partnered with Danish Cannabis ApS and marked a new joint venture called Spectrum Denmark ApS. This venture aims at strengthening the foothold of the company in the European market. This report forecasts revenue growth at country levels and provides an analysis of the latest industry trends in each of the sub-segments from 2016 to 2027. For the purpose of this study, Trusted Business Insights, Inc. has segmented the global cannabis extract market report on the basis of product, extract type, and country:\n- Aphria Inc.\n- Canopy Growth Corporation\n- Aurora Cannabis\n- Maricann Inc.\n- Organigram Holdings Inc.\n- The Cronos Group\n- TilrayTikun Olam, Ltd.\nProduct Outlook (Revenue, USD Million, 2016 - 2027)\nExtract Type Outlook (Revenue, USD Million, 2016 - 2027)\n- Full Spectrum\nTable of Contents Chapter 1 Methodology and Scope 1.1 Research Methodology 1.2 Research Scope and Assumptions 1.3 List to Data SourcesChapter 2 Executive Summary 2.1 Market SummaryChapter 3 Global Cannabis Extracts Market Variables, Trends & Scope 3.1 Market Segmentation 3.2 Cannabis Extracts Market Dynamics 3.2.1 Market Driver Analysis 126.96.36.199 Increasing Preference towards cannabis extracts | read more...']	['<urn:uuid:0786da8c-f6d0-4966-bd74-f355b821779d>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	12	37	1281
35	I'm studying the intersection of technology and environmental monitoring - what sophisticated tools were used in the Baha'i Temple's design process, and how do they relate to methods used for tracking ocean pH changes?	The Baha'i Temple's design utilized advanced digital modeling programs Maya and CATIA to achieve its intricate organic form. For ocean pH monitoring, sophisticated measurement systems are used to track both atmospheric and oceanic CO2 levels, as shown by the detailed correlation graphs of CO2 concentrations and pH levels published in Oceanography journal, demonstrating how CO2 increases from 278 ppm to 390 ppm have affected ocean chemistry.	"['Awards of Excellence 2004 – Bah’ Temple for South America\nHariri Pontarini Architects with Holmes and Amaral Architects\nA relatively young, independent world religion built on the tenet of universality, the Bah’ faith does not have a specific architectural style or iconography, and worship is conducted without the leadership of clergy. The aim of the architectural team was to create a design that would be instantly recognizable as a house of worship without referencing a previously established religious style or iconography, and also to design a building that would be welcoming and attractive to people of all faiths, backgrounds and cultures.\nSophisticated digital modelling programs Maya and CATIA were utilized in the design process to achieve the intricately organic form. In response to the client’s request for a nine-sided domed structure with nine entrances, the temple is composed of nine gracefully torqued wings, poised behind a large reflecting pool, set within nine prayer gardens. Space for communal and private meditation and prayer is provided by two distinct spaces: the area under the dome seating 600, and nine intimate light-filled alcoves nestled between the overlapping wings beneath a mezzanine that rings the temple perimeter. In keeping with a basic material palette of stone, recycled glass, bronze, wood and steel to physically and psychologically evoke the timeless quality inherent to traditional places of worship, the wings are composed of translucent alabaster conceived to manipulate the transference of light.\nIn response to highly seismic site conditions in mountainous Chile, the temple has been designed to sustain minimal to no structural damage in the event of an earthquake. Each of the temple’s nine geometrically identical wings is supported by a 25-metre-high stainless steel space framing system, resulting in a structure that is symmetrical in both mass and structural rigidity to minimize rotational or twisting effects during seismic events.\nMonteyne: While the spiritual aims of the building are not clearly articulated, this project represents a rare convergence of forces that seem destined to produce a monument so unique as to become a global landmark, or one of the “wonders of the world.” One can only marvel at the architects’ commitment to originate this form, the energy with which it has been developed, and the power of religious belief in motivating artistic achievement.\nShnier: Somewhere in their submission, the architects take great pains to describe their process and project as a “restrained interplay” of contradictions leading to multiple readings. The process is however, anything but restrained, employing various methods and means of technology to move it towards realization. Likewise, the project is anything but understated or contradictory–it is in fact a spectacularly beautiful and iconic architecture that one has confidence will bear itself out in execution.\nYarinsky: The project is a marvel of form and construction. The thoroughness and rigour of the design process are impressive.\nClient: Bah’ International Community\nArchitect team: Siamak Hariri, Michael Boxer, Jaegap Chung, Justin Ford, Adriana Balen, Mehrdad Tavakkolian, Tiago Masrour, George Simionopoulos, Naomi Kriss, Donald Peters\nStructural: Carruthers and Wallace Consulting Engineers\nMechanical: ECE Group\nElectrical: ECE Group\nArea: 14,600 ft2\nBudget: withheld at client’s request\nCompletion: spring 2008', ""Ocean acidification: global warming's evil twin\nWhat the science says...\n|Select a level...||Basic||Intermediate|\nThe current debate on the connection between CO2 emissions and climate change has largely overlooked an independent and equally serious problem, the increasing acidity of our oceans. Last December, the respected journal “Oceanography” published projections (see graphic below) for this rising acidity, measured by falling pH [i], through to the end of the century [ii].\nThe current debate on the connection between CO2 emissions and climate change has largely overlooked an independent and equally serious problem, the increasing acidity of our oceans. Last December, the respected journal “Oceanography” published projections (see graphic below) for this rising acidity, measured by falling pH [i], through to the end of the century [ii]. In 2095, the projected average ocean surface pH is 7.8, and lower still in the Arctic Ocean.\nFig 1: Ocean surface pH - historical values and projected future values based on current emission projections.\nCO2 in the atmosphere has increased from 278 ppm in pre-industrial times to 390 ppm today. During this time, the amount of CO2 dissolved in the ocean has risen by more than 30% [iii], decreasing the pH of the ocean by 0.11 units. As with CO2 and global warming, there is some lag between cause and effect. That means that, even if all carbon emissions stopped today, we are committed to a further drop of up to 0.1 units.\nThe close relationship between CO2 in the atmosphere, CO2 dissolved in the ocean, and the effect of the latter in falling pH, is illustrated by the graph [iv] below:\nFig 2: Annual variations in atmospheric CO2, oceanic CO2, and ocean surface pH. Strong trend lines for rising CO2 and falling pH.\nCO2 dissolves in waterto form carbonic acid. (It is worth noting that carbonic acid is what eats out limestone caves from our mountains.) In the oceans, carbonic acid releases hydrogen ions (H+), reducing pH, and bicarbonate ions (HCO3-).\nCO2 + H2O => H+ +HCO3- (1)\nThe additional hydrogen ions released by carbonic acid bind to carbonate ions (CO32-), forming additional HCO3-.\nH+ + CO32- => HCO3- (2)\nThis reduces the concentration of CO32-, making it harder for marine creatures to take up CO32- to form the calcium carbonate needed to build their exoskeletons.\nCa2+ + CO32- => CaCO3 (3)\nThe two main forms of calcium carbonate used by marine creatures are calcite and aragonite. Decreasing the amount of carbonate ions in the water makes conditions more difficult for both calcite users (phytoplankton, foraminifera and coccolithophore algae), and aragonite users (corals, shellfish, pteropods and heteropods).\nThe photo below left shows healthy specimens of calcifying phytoplankton Gephyrocapsa oceanica. The photo below right shows the damage to the same creature under conditions expected by the end of the century.\nFig 3: Healthy phytoplankton; same species with malformed shell plates as a result of damage by seawater with simulated end of century chemistry.\nSource: Nature, Reduced Calcification of Marine Phytoplankton in Response to Increased Atmospheric CO2, Issue 407 p.364 -367\nIt is often said that a picture is worth a thousand words.\nResearch in the Southern Ocean provides evidence that the formation of foraminifera shells is already being affected. Even though these creatures use calcite, which is less soluble than aragonite, there are already clear signs of physical damage. According to Dr. Will Howard of the Antarctic Climate and Ecosystems Cooperative Research Centre in Hobart, shells of one species of foraminifera (Globigerina bulloides) are 30 to 35 percent thinner than shells formed prior to the industrial period.[vi]. The photo below left shows a pre-industrial exoskeleton of this species obtained from sea-floor sediment. The photo below right shows a exoskeleton of a live specimen of the same species obtained from the water column in the same area in 2007. These stunning images were obtained using an electron microscope. (An interview with Dr. Howard was broadcast on the Catalyst television program). [vii] What is staggering is the amount of erosion in the right image compared to the left. The right sample look porous with larger holes and a 10-fold increase in their number. These and creatures like them are at the base of an ocean food chain, and they are already seriously damaged. If they are lost, it is not just biodiversity we are losing, but our food supply as well.\nFig 4. Pre-industrial and current samples of Globigerina bulloides from same location. Latter shows extensive erosion with a ten-fold increase in holes.\nSource: Australian Broadcasting Corporation, Ocean Acidification – The Big Global Warming Story, 13 September 2007\nThe implications of all of this are disturbing. For corals to absorb aragonite from seawater, the latter needs to be saturated in this mineral.\nNow a report from NOAA scientists found large quantities of water undersaturated in aragoniteare already upwelling close to the Pacific continental shelf from Vancouver to northern California [v]. Although the study only dealt with the area, the authors suggest that other shelf areas may be experiencing similar effects.\nFor corals like those in Australia’s Great Barrier Reef, the outlook is grim. They are threatened with destruction on two fronts, both caused by CO2 emissions. Not only do increased ocean temperatures bleach coral by forcing them to expel the algae which supplies them with energy (see photo at left) [viii], but increased ocean CO2 reduces the availability of aragonite from which reefs are made.\nIt is time to wake up. Our planet is dying. I urge you to find the time to view a 20 minute documentary on the problem of ocean acidification produced by the international Natural Resource Defence Council. Simply go to: www.acidtestmovie.com\nFig 5. Coral killed by above average ocean temperatures.\nReferences and Notes\nIntermediate rebuttal written by alan_marshall\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nExpert interview with Annamieke Van De Heuvel\nExpert interview with Charlie Veron\nExpert interview with Ove Hoegh-Guldberg\nLast updated on 8 July 2015 by pattimer. View Archives""]"	['<urn:uuid:29612624-52f0-4cb0-9038-a43ce8c25990>', '<urn:uuid:0b5304f2-0a0e-44f7-85df-f8c396571f32>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	34	66	1522
36	As a meal prep enthusiast, I'd like to know the best ways to utilize previously frozen cooked eggs - what are some creative dishes I can incorporate them into after thawing?	Thawed cooked eggs can be used in various ways. You can gently reheat them in a skillet or microwave until warmed through. They can be incorporated into casseroles, quiches, or frittatas by mixing them with other ingredients and baking until heated. Additionally, they can be chopped or mashed to make egg salad or used as fillings for sandwiches or wraps.	['Are you searching for Can You Freeze Eggs After Cooking? If yes, then you are at the right place.\nEggs are a popular ingredient in countless recipes, and there may be occasions when you find yourself with leftover cooked eggs.\nIf you’re wondering whether it’s possible to freeze eggs after cooking, you’re not alone.\nLet’s explore the question can you freeze eggs after cooking and understand the best practices for freezing them to maintain their flavor and texture.\nCan You Freeze Eggs After Cooking?\nYes, you can freeze eggs after cooking. Freezing cooked eggs is a practical way to preserve leftovers or prepare meals in advance.\nWhether you have scrambled eggs, omelets, hard-boiled eggs, or other cooked egg preparations, freezing them allows you to extend their shelf life and enjoy them at a later time.\nHowever, it’s important to note that the texture of cooked eggs may change slightly after freezing and thawing.\nAlso Read: Can You Freeze Eggs?\nFactors to Consider When Freezing Cooked Eggs\nBefore freezing cooked eggs, there are a few important factors to consider:\nPreparation: Different types of cooked eggs may freeze and thaw differently. For instance, scrambled eggs may become slightly softer after freezing, while hard-boiled eggs may develop a slightly different texture. Keep this in mind when planning to freeze specific types of cooked eggs.\nIngredients and Seasonings: If your cooked eggs contain additional ingredients or seasonings, consider their impact on texture and flavor after freezing. Some ingredients, such as vegetables or meats, may experience changes in texture after freezing and thawing.\nPortioning: Portioning the cooked eggs before freezing allows for convenient use in future meals. Consider the serving sizes you typically need and package the cooked eggs accordingly. This helps avoid unnecessary thawing and refreezing, which can affect quality.\nFreezing Cooked Eggs\nFollow these steps to freeze cooked eggs effectively:\nCool Completely: Allow the cooked eggs to cool completely at room temperature before freezing. This helps prevent condensation and ice crystals from forming during the freezing process.\nPortioning and Packaging: Portion the cooked eggs into individual servings or recipe-sized portions. Place the portions in freezer-safe containers or bags. Remove as much air as possible from the containers or bags before sealing to minimize freezer burn.\nLabeling and Dating: Properly label each container or bag with the contents and the date of freezing. This helps you keep track of their freshness and ensures you use them within the recommended time frame.\nFreezing Duration and Thawing\nConsider the following guidelines regarding freezing duration and thawing cooked eggs:\nFreezing Duration: Cooked eggs can be frozen for up to three months. While they may still be safe to eat after this period, their quality may start to decline. It’s best to use them within three months for optimal taste and texture.\nThawing Process: When you’re ready to use the frozen cooked eggs, transfer the desired portion from the freezer to the refrigerator. Allow them to thaw slowly overnight or for approximately 6-8 hours. Thawing in the refrigerator helps maintain the texture and flavor of the cooked eggs.\nUsing Thawed Cooked Eggs\nOnce the cooked eggs are thawed, they can be used in a variety of dishes. Here are a few ideas for using thawed cooked eggs:\nReheating: If you want to enjoy the cooked eggs in their original form, gently reheat them in a skillet or microwave until warmed through. Add additional seasonings or ingredients if desired.\nCasseroles and Quiches: Thawed cooked eggs can be incorporated into casseroles, quiches, or frittatas. Mix them with other ingredients and bake until heated through.\nEgg Salad or Sandwich Fillings: Thawed cooked eggs can be chopped or mashed and used as a base for egg salad or as a filling for sandwiches or wraps.\nAfter thawing and using frozen cooked eggs, assess their quality before consuming. Pay attention to any changes in texture, flavor, or appearance. If the cooked eggs show any signs of spoilage or have an off taste or smell, it’s best to discard them for safety reasons.\nIn conclusion, if your question is can you freeze eggs after cooking then its answer is, you can freeze cooked eggs to extend their shelf life and reduce food waste. While the texture of cooked eggs may change slightly after freezing, they can still be used in a variety of dishes. Proper packaging, labeling, and thawing techniques are essential for maintaining the quality of frozen cooked eggs. Enjoy the convenience of having cooked eggs on hand for quick and easy meals while ensuring their flavor and texture are preserved.\nDisclaimer: The information provided in this article is for general informational purposes only. Results may vary, and individual experiences may differ. It is always recommended to exercise caution and use your discretion when freezing and thawing food items. The freezing process may alter the texture, consistency, and flavor of the food. Follow proper food safety practices and consult a professional for specific advice or concerns. The author and publisher are not liable for any adverse effects or damages resulting from the use of the information provided. Use your best judgment when freezing and consuming food items.']	['<urn:uuid:9f1f6146-06c3-47f3-a730-7551ddc0d555>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	31	60	854
37	As someone researching wrestling education history, I'd like to know about Farmer Burns' teaching career - how many wrestlers did he train and who were some of his notable students?	Farmer Burns trained approximately 1,600 wrestlers in total. Some of his notable students included Gotch, George Turner, Cecil Conley, Hugh Nichols, Charlie Hanson, Rudy Dusek, Joe Malcewicz, Paul Jones, Joe 'Toots' Mondt, Fred Beell, and his own sons Charlie and Raymond Burns. He also established a correspondence course called 'Lessons in Wrestling and Physical Culture' in 1914.	"['By Tim Hornbaker\nMartin Burns was born on February 15, 1861, the third child and second son born to\nIrish immigrants, Michael and Mary. His father was a farmer, and mother, a homemaker, raised\nseven children born between 1856 and ’71. A year after the final child, Ellen, was born, Michael\nwas killed in an accident, leaving his widow and eldest children to support the family. The\nBurns’ clan lived in Springfield in Cedar County, Iowa near Rock Creek. Legend has it that\nyoung Martin wrestled his first professional match as early as eight years old, competing with a\nschool yard friend and winning 15 cents.\nNearing 5’10” and weighing between 160 and 170 pounds, Burns was brawny and quick on his\nfeet. He excelled in most athletics, but it was the competition of wrestling that occupied most of\nhis time. When he wasn’t farming, he was wrestling, quickly learning the trade that would make\nhim world famous. Burns focused heavily on personal training, and built his body into a\nmachine, with a 20” neck. He swayed from alcohol or cigarettes, never cursed, and had a\npersonable disposition that attracted people. As he developed his wrestling skill, it became\neasy for him to share his knowledge with fellow grapplers, and his ability to train others was a\ngift he worked with.\nBurns would often travel from town to town, and take on any potential challenges in shoot\nmatches. Wrestling was popular in most communities, and Martin would rarely come up empty\nwhen searching for an opponent. His reputation grew and began to precede him, and the\nlegacy of the wrestling farmer was born. Perfecting his abilities in the catch-as-catch-can style,\nBurns also competed in matches following other methods and held himself well.\nOn Saturday, April 20, 1895, Burns defeated the original “Strangler” Evan Lewis at the Second\nRegiment Armory in Chicago and captured the World Heavyweight Title in the catch style.\n3,000 fans saw Burns win the second, fourth and fifth falls and capture the title and $2,000.\nBurns had given 15 pounds to Lewis, and although he barely stood taller than his rival, many\nbelieved the “Strangler” was the favorite. Instead, Burns captured the championship and was\nquickly challenged by Parson Davies for his man, Dan McLeod. In May 1897, Burns beat\nMcLeod, but ended up losing a rematch on October 26 in Indianapolis at the Grand Opera\nHouse. McLeod won the first and third falls to capture the World Title.\nFarmer Burns trained at an athletic association in Chicago for several years prior to settling up\ngymnasiums in Rock Island, and finally in Omaha. He also established a correspondence\ncourse entitled Lessons in Wrestling and Physical Culture, published in 1914, which was 96\npages and had 12 lessons. Among the individuals he personally trained were Gotch, George\nTurner, Cecil Conley, Hugh Nichols, Charlie Hanson, Rudy Dusek, Joe Malcewicz, Paul Jones, J.\nC. Nichols, Jack Reynolds, Emil Klank, Jess Westergaard, Joe “Toots” Mondt, Marin Plestina,\nCharles Peters, Fred Beell, Henry Kolln, Johnny Billiter, Marion Mynster, Jack Sherry, Dale\nParrish, his sons Charlie and Raymond Burns, and an estimated 1,600 in total.\nNumerous times it has been declared that Martin Burns wrestled 6,000 bouts and only lost six,\nand maybe that was his claim to reporters. In truth, he lost a number of worked matches in the\nring. A list of those to defeat Burns were Gotch (at least four times), McLeod, Jack King, Fred\nBeell, Tom Jenkins, Charles Hackenschmidt, Tom Cannon, D.A. McMillan, and Dr. Roller.\nThe famous hangman stunt Martin was known for saw him survive a six foot drop with a noose\naround his neck. His exceptionally developed neck muscles defied the laws of both gravity and\nrationality. In January 1912, an amateur wrestler from St. Paul named George Loefler died\nfollowing a similar attempt, further proving Burns’ unbelievable method of performance and\nskill. Burns had a 20” neck, and an amazing photograph of his muscle development, even at\nage 69, was printed in the Saturday, February 15, 1930 edition of the World Herald, Omaha,\nBurns was a beloved father, a dedicated wrestler, and respected teacher. In 1930, he had a\ngymnasium in his home, 306 South 49th Street in Omaha, and trained daily. His sons,\nRaymond and Charlie became musicians in Chicago. He suffered the loss of his wife Amelia (3-\n28-30) and daughter Mayme (11-7-32), leading to the downfall of his own condition. Martin\ndied on January 8, 1937 in Council Bluffs, Iowa at the age of 76, and he was buried at the St.\nJames Cemetery in Wheatland, Iowa. Memorial shows were held in his honor by promoters in\nToronto, Iowa. The Des Moines Register inducted Burns into its Iowa Sports Hall of Fame in\n1951. He was inducted into the George Tragos/Lou Thesz Professional Wrestling Hall of Fame\nat the International Wrestling Institute and Museum at Newton, Iowa on June 16, 2001.\nOther Historical Information:\nOn October 27, 1893 in Rock Island, IL, Farmer Burns beat Moth by taking the second, third,\nand fourth falls. Moth won the opening fall.\nThe Freeborn County Standard (Albert Lea, Minnesota, 1/8/1896) stated that the Chicago Inter\nOcean ""says that Farmer Burns, the wrestler, has recently been spending his time\n\'hippodroming\' through Minnesota, getting up matches and \'doing\' the citizens of the small\ntowns out of their money. He recently had a match at Graceville with Duncan McMillena nd the\ntwo of them made a neat stake.""\nThe Massillon Independent (Massillon, Ohio) (12/31/1896) stated that Farmer Burns was part of\nthe reason of wrestling\'s revival, despite the fact that he was a veteran of the game. Burns was\nsaid to be 38 years of age and had been wrestling for 20 years. He stood 5\'9 1/2"". The big\nmatch upcoming for the Farmer was against Dan McLeod.\nBurns was a member of the Jeffries-Gotch combination in January 1910. His two sons,\nRaymond and Charles were in school at Omaha.\nIn 1913, Farmer Burns was touting the attributes of Yusiff Hussane of Bulgaria, a talented\nwrestler who he was training and managing. Hussane reportedly ""stayed more than two hours\nwith Hackenschmidt,"" according to the Lincoln (NE) Daily News (12/9/1913). The article also\nclaimed Burns sent Dr. Roller packing, apparently from the Burns-Gotch troupe, after the latter\nclaimed he could beat Burns. Burns\' friends put up the money for a match, but Roller never\nbacked up his words.\n|Farmer Burns Wrestling History\nLegends of Pro Wrestling']"	['<urn:uuid:034a2a7d-93ed-4799-9c2e-c8595afd29b2>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	30	57	1077
38	printmaking exhibitions conservation treatments oregon galleries	Printmaking is being showcased in Oregon galleries, with Guardino Gallery presenting nine Oregon printmakers using various techniques like intaglio, lithograph, mezzotint, monotype, monoprint, linoleum and woodblock relief. For the conservation of these types of artworks, MACC employs highly trained conservators who provide museum-quality treatments across different specialties including Paper Conservation with experts like Dianna Clise, who specializes in works on paper, and Liz Sorokin, who has extensive experience working with various works on paper at prestigious institutions.	['January is going to be a big month for printmaking in the region as various venues showcase the art of printmaking. Guardino Gallery proudly presents nine Oregon printmakers showcasing printmaking techniques including intaglio, lithograph, mezzotint, monotype, monoprint, linoleum and woodblock relief. Artists include: Kelli MacConnell, Chris Darr, Stirling Gorsuch, Heather Halpern, Tanner Henderson, Nicole Rawlins, Margaret vanPatten, Nanette Wallace, Christy Wyckoff.\nMAIN GALLERY: January 3rd – January 29th\nKelli MacConnell: Avid hiker and adventure seeker, Kelli transcribes her love of nature and the northwest into large scale, high contrast linoleum relief. After studying art at Portland State she discovered her passion for printmaking. She made the transition to a working printmaker and has since been enjoying life as a full-time artist.\nHeather Halpern: Heather was a multimedia photorealist until being exposed to printmaking. Printmaking was so inspiring, Heather founded Whiteaker Printmakers, a non-profit organization providing Eugene with a public space to teach, exhibit and host printmaking events\nMargaret VanPatten. Margaret is an accomplished intaglio artist that utilizes nearly every technique possible to create her haunting pieces which include drypoint, etching, aquatint and mezzotint.\nNicole Rawlins: Daughter of an accomplished intaglio relief printer, Nicole was born to be not only an accomplished intaglio printmaker but also a leader as the Adult Visual Arts Coordinator at Multnomah Arts Center inspiring new printmakers while continuing to exhibit nationally and internationally.\nChristy Wyckoff Christy retired as Professor Emeritus in 2012 as Chair of the Printmaking Department of the Pacific Northwest College of Art in Portland. His work has been shown and collected internationally and reflects a sensitive hand with his array of nature focused edition prints, monotypes, paintings and photographs.\nNanette Wallace: Nanette fell in love with printmaking while completing her BFA at Oregon State University. Her career path as a graphic artist led her away from printmaking until she acquired a boutique letterpress studio and then a small etching press. Her work is gestural and energetic, consisting of monotypes with a figurative focus\nChris Darr: Chris s a professional culinary artist for 20 years and son of a screen print artist, found relief printmaking rather recently which revealed his calling as a wood block relief artist.\nTanner Henderson: Tanner, a young entrepreneur and artist, uses gesture and color to explore land and figures with his large format monotypes.\nStirling Gorsuch: Stirling is a nature man of the Cascades, Gorge and Oregon Coast which is reflected in his intense mono print color palate depicting the transformation of our forests by fire and natural occurrence.\nFEATURE AREA: January 3rd – January 27th\nDevin Bernard’s paintings are inspired by timeless questions: “How do we understand existence? What accounts for our sense of spirituality? What is the best way for people to relate to one another? Historically, answers to these questions have been expressed in myths and metaphors. In the same way, I use visual narratives as a basis for developing meaning in my pictures. Equally important is maintaining a balanced and organized composition through the placement of repeating forms and volumes throughout the painting. This, along with a harmonious balance of color, hopefully results in a feeling of rightness and truth.”', 'MACC’s highly trained staff of conservators and preservation professionals provide museum-quality treatments and consultations for Paintings, Paper, Textiles, Objects, and Preventive Conservation. All MACC’s programming is governed by the Code of Ethics and Guidelines for Practice of the American Institute for Conservation of Historic and Artistic Works. Committed to continuing professional education, the MACC staff brings the most current standards, materials, and techniques to their daily practice.\nAlexa Beller, Associate Paintings Conservator Ms. Beller joined MACC after completing a National Endowment for the Humanities Paintings Conservation Fellowship at the Chrysler Museum of Art in Norfolk, Virginia. Prior, she completed internships at the Isabella Stewart Gardner Museum, Gianfranco Pocobene Studio, the department of the Conservation of Religious and Civil Art of the City of Paris, the Western Center for the Conservation of Fine Arts, and in the private practice of Ria German-Carter. Her experience is wide-ranging including Italian Renaissance panels, 19th-century French murals, early American portraits, and 20th-century mixed media paintings. Ms. Beller holds a Master of Science in Conservation from the Winterthur/University of Delaware Program in Art Conservation as well as a Bachelor of Arts and a Bachelor of Fine Arts from the University of Illinois Urbana-Champaign. She is an Associate member of the American Institute for Conservation of Historic & Artistic Works and the Paintings Specialty Group.\nRita Berg, Paintings Conservator Ms. Berg joined MACC after completing a Kress Fellowship at the Conservation Center of New York University’s Institute of Fine Arts, where she treated Old Master paintings from the dispersed Samuel H. Kress Collection and assisted with teaching and supervision of graduate conservation students. Prior, she spent a year at the Brooklyn Museum of Art and internships at the New York Historical Society, the Cranmer Art Group in New York, The Cloisters, the National Gallery of Art in Washington, D.C., and the Kunsthistorisches Museum in Vienna, Austria. While specializing in Old Masters and panel paintings in particular, Ms. Berg has extensive experience with modern and contemporary works. She holds a Masters of Arts in Art History with an Advanced Certificate in Conservation from the Conservation Center, New York University and a Bachelor of Fine Arts summacum laude from the University of Minnesota. She is a Professional Associate of the American Institute for Conservation of Historic & Artistic Works with a membership in the Paintings Specialty Group.\nDianna Clise, Senior Paper Conservator Ms. Clise joined MACC in 2007 after completing her Masters in Art Conservation with a specialization in works on paper from Queen’s University in Kingston, Ontario. Prior to pursuing her graduate degree, Ms. Clise worked at Etherington Conservation Center in Greensboro, North Carolina. She holds a Bachelor of Arts, Honours, in anthropology and cultural studies from Trent University in Peterborough, Ontario. Ms. Clise is an Associate Member of the American Institute for Conservation of Historic & Artistic Works with a membership in the Book and Paper, and Photographic Materials Specialty Groups.\nMegan Emery, Chief Conservator and Senior Objects Conservator Ms. Emery came to MACC from the Cincinnati Art Museum, where she was responsible for the care and preservation of all three-dimensional objects. Ms. Emery has extensive experience with ethnographic and archaeological materials, ceramics, lacquer, plasters, and the conservation of large scale contemporary sculpture. She holds a Master of Arts with a Certificate of Advanced Study in Conservation specializing in Objects from the State University College of New York at Buffalo and a Bachelor of Fine Arts cum laude from the University of St. Thomas, St. Paul. Ms. Emery is a Fellow of the American Institute for Conservation of Historic & Artistic Works, Objects Specialty Group and Electronic Media. She is also a member of the International Institute for Conservation.\nNicole Grabow, Director of Preventive Conservation Ms. Grabow joined MACC in 2006, coming from the Smithsonian Center for Materials Research and Education at the Smithsonian Museum, Washington, DC. Prior to that, Nicole was a Mellon Fellow at the Smithsonian’s new National Museum of the American Indian, located on the Washington DC Mall, and an intern at the Smithsonian’s Freer and Sackler Galleries. She holds a Master of Science from the Winterthur/University of Delaware Program in Art Conservation, specializing in Objects Conservation, and a Bachelor of Arts from Sarah Lawrence College in Bronxville, New York. Ms. Grabow has particular interest in working with Native American communities and on public art projects. She was a MACC Senior Objects Conservator and Preventive Conservator prior to becoming the Director of Preventive Conservation. She is a Certified CAP Assessor and a Fellow of The American Institute for Conservation of Historic & Artistic Works.\nJen Neville, Registrar Ms. Neville joined MACC in 2019 after completing a Bachelor’s Degree in Fine Arts from the Minneapolis College of Art and Design, specializing in Web & Multimedia Environments. Prior, she interned at the Nassauischer Kunstverein in Wiesbaden, Germany. Ms. Neville brings to MACC a material-driven interest in photographic media and the performativity of analog processes. Her professional experience includes digital video production, sound production, web development, photography & graphic design. Ms. Neville is a member of the national Association of Registrars and Collections Specialists.\nKristy Jeffcoat, Senior Paintings Conservator Ms. Jeffcoat has extensive experience in the care and preservation of paintings and painted surfaces, including canvas paintings, panel paintings, and painted sculpture, as well as Preventive Conservation. Prior to joining MACC, she worked at West Lake Conservators in Skaneateles, NY; the Western Center for the Conservation of Fine Arts in Denver, CO; and the Denver Art Museum. She interned at the Field Museum in Chicago, IL, and was a Kress Fellow at the Denver Art Museum. She holds a Masters of Art Conservation with a specialty in Paintings and Painted Surfaces from Queen’s University in Kingston, Ontario; a Post Baccalaureate Certificate in Art Conservation from Studio Art Centers International in Florence, Italy, and Bachelor of Fine Arts with a specialization in Art History from Louisiana State University. Ms. Jeffcoat is a Professional Associate of the American Institute for Conservation of Historic & Artistic Works, Paintings Specialty Group, and has been with MACC since 2014.\nBeth McLaughlin, Contract Textile Conservator Ms. McLaughlin was the Chief Textile Conservator at Biltmore House in Asheville, North Carolina and the Sr. Textile Conservator at MACC from 2005 to 2010. She has significant training and experience in the conservation of historic and contemporary textiles and the preservation, care and re-housing of three-dimensional objects. Ms. McLaughlin received a Masters in Fine Arts and a Bachelor of Fine Arts summa cum laude from Ohio University and received advanced training at the Smithsonian Institution’s Conservation Analytical Laboratory and also at Colonial Williamsburg. Ms. McLaughlin is a Professional Associate of the American Institute for Conservation of Historic & Artistic Works, Textile Specialty Group, and a member of the Southeast Regional Conservation Association, the American Quilt Study Group, and the Textile Society of America.\nCourtney Murray, Objects Conservator Ms. Murray joined MACC in 2017, coming from the Denver Art Museum where she worked with an encyclopedic collection of three-dimensional objects. Prior, she completed a Samuel H. Kress Foundation post-graduate fellowship and graduate internship at the Denver Art Museum, and graduate internships at the Royal British Columbia Museum in Victoria, British Columbia and the Toledo Museum of Art. Ms. Murray has experience with a wide range of structures and materials, ranging from archaeological artifacts to contemporary art, and particular interest in polychromed wood, ethnographic materials, and technical analysis. Courtney holds a Master of Science in Conservation from the Winterthur/University of Delaware Program in Art Conservation and a Bachelor of Arts in Chemistry from Emory University. She is a Professional Associate member of the American Institute for Conservation of Historic & Artistic Works, Objects Specialty group and Research and Technical Studies (RATS) Specialty group.\nLiz Sorokin, Associate Paper Conservator Ms. Sorokin came to MACC after completing the Craigen W. Bowen Fellowship in Paper Conservation at the Straus Center for Conservation and Technical Studies at the Harvard Art Museums. She completed graduate internships at the Art Institute of Chicago, the Brooklyn Museum, and the Saint Louis Art Museum and has experience working with a variety of works on paper. In addition to her museum experience, she has also worked with the library collections of the University of Illinois at Urbana Champaign, the University of Virginia and Northwestern University. She holds a Master of Arts with a Certificate of Advanced Study in Conservation specializing in works on paper from the State University of New York at Buffalo State and a Bachelor of Arts in Art History with Honors from the University of Illinois at Urbana-Champaign. She is an Associate member of the American Institute for Conservation of Historic and Artistic Works.\nColin D. Turner, Executive Director Mr. Turner has been Director of MACC since 2002 after prior nonprofit experience as the Director of United Arts in St. Paul, Minnesota and the Director of Fundraising for Fresh Air Radio in Minneapolis. Mr. Turner has a special interest in archaeology and ethnographic works and holds a Bachelor of Arts degree in Anthropology and History from the University of Minnesota, Minneapolis, as well as advanced training in Nonprofit Business Administration from the University of St. Thomas in Minneapolis. His business experience includes 11 years as the owner of a manufacturer’s representative firm and 4 years founding and managing a publishing company. Mr. Turner is a Professional Associate of The American Institute for Conservation of Historic & Artistic Works, a member of the Minnesota Council of Nonprofits and the Twin Cities Nonprofit Financial Group, Chair of the National Association of Regional Conservation Centers, a member of the Regional Alliance for Preservation and serves on the Heritage Emergency National Task Force.\nJenny Wollner, Operations Manager Ms. Wollner began working at MACC in 2014 after an internship in MACC’s offices and objects conservation laboratory. She graduated with honors from Macalester College with Bachelor’s degrees in History and Classics with an emphasis on archaeology and a minor in Art History. She developed a deep concern for the preservation of art and artifacts after studying archaeology at Istituto Lorenzo de Medici in Tuscania, Italy. Ms. Wollner is a Certified Non-Profit Accounting Professional and holds a certificate in non-profit business administration from St. Thomas’s Opus College of Business. She is also a member of the Twin Cities chapter of the Non-Profit Financial Group and Minnesota Council of Non-Profits and serves on the board of the Minnesota Chapter of the National Organization for Women.']	['<urn:uuid:9ec1f974-f462-4d29-afee-e8fb899cd1f1>', '<urn:uuid:9d5e830a-d3ba-4055-b554-0299615c7a20>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	6	77	2257
39	I'm researching early widescreen cinema and I'm wondering what special camera technology was used to film The Big Trail in 1930?	The Big Trail was filmed using an early widescreen process called 70mm Grandeur film. This was an experimental format that had previously been used by Fox Film Corporation for a handful of films in 1929 and 1930, with The Big Trail being the last one. The 70mm cameras had technical limitations - they couldn't focus well up close, so their shots were mainly panoramas with very few close-ups.	"['The Big Trail\n|The Big Trail|\nMovie poster for The Big Trail\n|Directed by||Raoul Walsh|\n|Produced by||Winfield R. Sheehan|\n|Edited by||Jack Dennis|\n|Distributed by||Fox Film Corporation|\n|122 min. 70mm version, 108 min. 35mm version|\nIn 2006, the United States Library of Congress deemed this film ""culturally, historically, or aesthetically significant"", and selected it for preservation in the National Film Registry, saying ""the plot of a trek along the Oregon Trail is aided immensely by the majestic sweep provided by the experimental Grandeur wide-screen process used in filming"".\nA large caravan of settlers attempt to cross the Oregon Trail. Breck Coleman (John Wayne) is a young trapper who just got back to Missouri from his travels near Santa Fe, seeking to avenge the death of an old trapper friend who was killed the winter before along the Santa Fe Trail for his furs, by Red Flack (Tyrone Power, Sr.) and his minion Lopez (Charles Stevens). At a large trading post owned by a man named Wellmore, Coleman sees Flack and suspects him right away as being one of the killers. Flack likewise suspects Coleman as being somebody who knows too much about the killing. Coleman is asked by a large group of settlers to scout their caravan west, and declines, until he learns that Flack and Lopez were just hired by Wellmore to boss a bull train along the as-yet-unblazed Oregon Trail to a trading post in northern Oregon Territory (which at the time extended into current British Columbia), owned by another Missouri fur trader. Coleman agrees to scout for the train, so he can keep an eye on the villains and kill them as soon as they reach their destination. The caravan of settlers in their Prairie schooners would follow Wellmore\'s ox-drawn train of Conestoga Wagons, as the first major group of settlers to move west on the Oregon Trail. The film is set somewhere between 1837 and 1845.[notes 1] This is historically accurate, as the first major wave of settlers on the Oregon Trail was in 1843, although the details were completely different.\nColeman finds love with young Ruth Cameron (Marguerite Churchill), whom he\'d kissed accidentally, mistaking her for somebody else. Unwilling to accept her attraction toward him, Ruth gets rather close to a gambler acquaintance of Flack\'s, Thorpe (Ian Keith), who joined the trail after being caught gambling. Coleman and Flack have to lead the settlers west, while Flack does everything he can to have Coleman killed before he finds any proof of what he\'d done. The three villains\' main reason for going west is to avoid the hangman\'s noose for previous crimes, and all three receive frontier justice instead. The settlers trail ends in an un-named valley, where Coleman and Ruth finally settle down together amidst giant redwoods.\n- John Wayne as Breck Coleman\n- Marguerite Churchill as Ruth Cameron\n- El Brendel as Gus, a comical Swede\n- Tully Marshall as Zeke, Coleman\'s sidekick\n- Tyrone Power, Sr. as Red Flack, wagon boss\n- Frederick Burton as Pa Bascom\n- Russ Powell as Windy Bill (uncredited)\n- Ian Keith as Bill Thorpe, Louisiana gambler\n- Charles Stevens as Lopez, Flack\'s henchman (Geronimo\'s real-life grandson)\n- Louise Carver as Gus\'s mother-in-law\n- John Big Tree as Indian Chief (uncredited)\n- DeWitt Jennings as Boat Captain Hollister (uncredited)\n- Ward Bond as Sid Bascom (uncredited)\n- William V. Mong as Wellmore (uncredited)\n- Marcia Harris as Mrs. Riggs (uncredited)\n- Iron Eyes Cody as Indian (uncredited)\n- Alphonse Ethier as Marshal (uncredited)\n- Victor Adamson as Wagon Train Man (uncredited)\n- Helen Parrish as Honey Girl Cameron (uncredited)\n- Marilyn Harris as Pioneer Girl (uncredited)\n- Jack Curtis as Pioneer (uncredited)\n- Jack Padjan as Pioneer (uncredited)\n- Robert Parrish as Pioneer Boy (uncredited)\nThis section does not cite any sources. (October 2013) (Learn how and when to remove this template message)\nFor the film, Walsh had employed 93 actors and used as many as 725 natives from five different Indian tribes. He also obtained 185 wagons, 1,800 cows, 1,400 horses, 500 buffalos and 700 chickens, pigs and dogs for the production of the film.\nWalsh offered the lead to actor Gary Cooper, who couldn\'t accept it. According to John Ford\'s later account, Walsh supposedly then asked Ford for casting suggestions, whereupon Ford recommended a then-unknown named John Wayne because he ""liked the looks of this new kid with a funny walk, like he owned the world"". When Wayne professed inexperience, Walsh told him to just ""sit good on a horse and point"". Walsh said that he initially saw Wayne, then a prop man, moving heavy furniture as though it were light as a feather, then decided to test him for the part. Almost a decade later, Ford began saying that he himself had discovered Wayne as a prop man when casting Stagecoach (1939), even though Wayne had played the lead in dozens of movies by then throughout the 1930s. Ford stuck to this obviously false story in interviews for the rest of his life. Filming on The Big Trail began on-location just outside Santa Fe, New Mexico in April 1930, which was unheard of at the time and quickly became very costly to the studio.\nThe shoot lasted from April 20 to August 20, 1930 and was filmed in seven states. The film was shot in an early widescreen process called 70 mm Grandeur film, which was first used in the film Fox Movietone Follies of 1929. Grandeur was used by the Fox Film Corporation for a handful of films released in 1929 and 1930, of which The Big Trail was the last. Grandeur proved financially unviable for an industry still investing in the switch to talking pictures.\nThe scene of the wagon train drive across the country was pioneering in its use of camera work and the depth and view of the epic landscape. An effort was made to lend authenticity to the movie, with the wagons drawn by oxen instead of horses – they were lowered by ropes down canyons when necessary for certain shots in narrow valleys. Tyrone Power, Sr.\'s character\'s clothing looks realistically grimy, and even the food supplies the \'immigrants\' carried with them in their wagons were thoroughly researched. Locations in five states, starting from New Mexico to California, were used to film the caravan\'s 2,000-mile (3,200 km) trek.\nRelease and reception\nAfter shooting, the film was previewed to select audiences and generally released on October 1930. ""Often the scenes"" in the film, wrote Mordaunt Hall in The New York Times ""cause one to marvel at their naturalness and beauty. It has a thunderstorm that looks as real as the land, water and sky that confront one throughout this production"". According to Hall, in one sequence, featuring a native American attack, ""suddenly it seems as though one were tugged from one\'s seat and thrown in front of the charging horses, which appear to plunge from the screen and disappear into the velvety darkness of the theatre"".\nAfter completion of the film, Wayne was only cast in low-budget serials and features (mostly Poverty Row westerns); it took Wayne\'s role in Stagecoach (1939) for him to become an established performer. It would be over 20 years before the concept of widescreen films was revived.\nThe two versions\nThis section does not cite any sources. (October 2013) (Learn how and when to remove this template message)\nBeyond the format difference, the 70mm and 35mm versions vary substantially from each other. They were shot by different cameras, and footage for each format was edited separately in the cutting room. Some scenes were shot simultaneously by both cameras, the only difference being the angle (with the better angle usually given to the 70mm camera). Some scenes were shot first by one camera, and then retaken with the other camera. The 70mm cameras could not focus well up close, so their shots were mainly panoramas with very few close-ups. The 35mm cameras could move in and focus at short distances. Thus, scenes in the 70mm version might show two characters talking to each other in the same take, making greater use of the widescreen frame, while the 35mm version would have close-up shots cutting back and forth between the two characters. This may have been an artistic choice as much as a technical one. The cinematographer responsible for the 70mm version of the picture, Arthur Edeson, wrote in the September 1930 issue of American Cinematographer: ""Grandeur reduces the number of close-ups considerably, as the figures are so much larger that semi-close-ups are usually all that is needed"".\nIn editing the films, some scenes were edited out for one version but allowed to remain in the other. The 35mm version was edited to be shorter, so many scenes in the 70mm version are not found in the 35mm. However, there are a few scenes in the 35mm version not found in the 70mm. The 35mm version was 108 minutes, but the 70mm was longer at 122 minutes.\nPreservation and re-release\nA neglected film for many years, it was only seen in the conventional 35mm version for decades. In the early 1980s, the Museum of Modern Art in New York City, which held the original 70mm nitrate camera negative, wished to preserve the film but found that nitrate decomposition had left it too shrunken and fragile to be copied to safety film, and that no film lab would touch it. Over a year-long project, Karl Malkames, a specialist in film restoration technology, designed and built a special printer to preserve this version of the film. The image on the original negative was transferred to 35mm anamorphic master. The printer copied at a speed of one frame a second, leading to the film\'s year-long process in preserving its original form.\nDave Kehr wrote for the Chicago Tribune in 1988: ""The added richness of resolution and the silvery sheen that the first generation 70 mm. nitrate prints would have provided can only be imagined. And yet, The Big Trail remains an eye-popping experience"".\nHome video releases\nThe 70mm version was finally seen on cable television in the late 1990s. The 35mm version had been released to VHS and DVD previously for several years. A two-disc restored DVD was released in the US on May 13, 2008 featuring the 35mm and the 70mm version. A Blu-ray edition featuring the 70mm version was released in September 2012.\nForeign language versions\nA fairly common practice in the early sound era was to simultaneously produce at least one foreign-language version of a film for release in non-English speaking countries; an approach later replaced by simply dubbing the dialogue. There were at least four foreign-language versions made of The Big Trail, all filmed in 35mm, 1.20:1 ratio and using different casts and different character names:\n- French: La Piste des géants (1931), directed by Pierre Couderc, starring Gaston Glass (Pierre Calmine), Jeanne Helbling (Denise Vernon), Margot Rousseroy (Yvette), Raoul Paoli (Flack), Louis Mercier (Lopez).\n- German: Die Große Fahrt (1931), directed by Lewis Seiler and Raoul Walsh, starring Theo Shall (Bill Coleman), Marion Lessing (Ruth Winter), Ullrich Haupt (Thorpe), Arnold Korff (Peter), Anders Van Haden (Bull Flack), Peter Erkelenz (Fichte), Paul Panzer (Lopez). Die Große Fahrt on IMDb\n- Italian: Il grande sentiero (1931), starring Franco Corsaro and Luisa Caselotti.[notes 2]\n- Spanish: La Gran jornada (1931), directed by David Howard, Samuel Schneider, and Raoul Walsh, starring Jorge Lewis (Raul Coleman), Carmen Guerrero (Isabel Prados), Roberto Guzmán (Tomas), Martin Garralaga (Martin), Al Ernest Garcia (Flack), Tito Davison (Daniel), Carlos Villarías[notes 3] (Orena), Charles Stevens[notes 4] (Lopez). La Gran jornada on IMDb\n- Ruth tells Honey Girl that there are 26 stars in the flag, dating the movie between Jan., 1837 and March 1845.\n- Luisa Caselotti\'s younger sister, Adriana Caselotti, was the voice of Snow White in Walt Disney\'s animated classic Snow White and the Seven Dwarfs (1937).\n- Villarías is best known for playing the title character in the Spanish-language version of Drácula (1931).\n- Stevens plays the same part in both the English and Spanish versions of The Big Trail.\n- Motion Picture News, May 24, 1930\n- ""Films Added to National Film Registry for 2006"" (Press release). Library of Congress. December 27, 2006. ISSN 0731-3527.\nThe story goes that director Raoul Walsh was seeking a male lead for his new Western and asked his friend John Ford. Ford recommended an unknown actor named John Wayne because he ""liked the looks of this new kid with a funny walk, like he owned the world"". When Wayne professed inexperience, Walsh told him to just ""sit good on a horse and point"". The plot of a trek along the Oregon Trail is aided immensely by the majestic sweep provided by the experimental Grandeur wide-screen process used in filming. However, Wayne\'s starring role in the movie did not lead to stardom. He languished in low-budget pictures until John Ford cast him in the 1939 classic ""Stagecoach"".\n- This was the only talkie film of Tyrone Power Sr., father of Tyrone Power. He died in 1931.\n- Moss, Marilyn (2013) . Raoul Walsh: The True Adventures of Hollywood\'s Legendary Director. Lexington, Kentucky: University Press of Kentucky. p. 120.\n- Henry Cabot Beck (July 1, 2008). ""Young Wayne Blazed a Big Trail"". True West Magazine. Retrieved September 3, 2018.\n- Kehr, Dave (June 17, 1988). ""Grandeur Still Evident In 1930`s The Big Trail"". Chicago Tribune. Retrieved September 3, 2018.\n- Silver, Charles (August 10, 2010). ""Raoul Walsh\'s The Big Trail"". Museum of Modern Art. New York City. Retrieved September 3, 2018.\n- ""American Cinematographer Magazine"" – Volume 96 (April 2015). Article: ""The Big Trail""\n- Hall, Mordaunt (November 2, 1930). ""A Grandeur Production; The Big Trail Has Remarkable Scenes in Pioneer Story of a Century Ago"". The New York Times. Retrieved September 3, 2018.\n- McKelvey, Tara (October 9, 2013). ""Searching for John Wayne in the Alabama Hills"". BBC News Magazine. Retrieved September 3, 2018.\n- Edeson, Arthur (September 1930). ""Wide Film Cinematography: Some Comments on 70mm Camerawork From a Practical Cinematographer"". American Cinematographer. The Widescreen Museum. Retrieved September 3, 2018.\n- ""The Big Trail"". Rotten Tomatoes. Retrieved September 3, 2018.\n- ""Karl D. Malkames"". The Journal News. Westchester, New York. March 10, 2010. Retrieved September 3, 2018.\n- Higgins, Steven (2006). Still Moving: The Film and Media Collections of The Museum of Modern Art. New York City: The Museum of Modern Art. p. 144. ISBN 978-0-87070-326-3.\n- Magnified Grandeur – The Big Screen, 1926–31, David Coles, 2001\n- ""Wide Film Cinematography: Some Comments on 70mm Camerawork From a Practical Cinematographer"", Arthur Edeson, A.S.C., American Cinematographer, September 1930.\n- The Big Trail on IMDb\n- The Big Trail at AllMovie\n- The Big Trail at the TCM Movie Database\n- The Big Trail at Rotten Tomatoes']"	['<urn:uuid:a4631e72-2c00-4d5e-b918-f7c020da3dfd>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	21	68	2466
40	why did mobile phone manufacturers create weak encryption algorithms in 1990s	Mobile phone manufacturers created weak encryption algorithms in the 1990s because cryptography was considered a weapon with strict export restrictions. To sell mobile phones to other countries, they had to create encryption that was just weak enough to get through these export restrictions, but not too weak.	['Intentional vulnerabilities in encryption algorithms seem tempting to secret services and law enforcement agencies alike – after all, they allow supposedly secure information to be read. Professor Gregor Leander and Dr. Christof Beierle from the Symmetric Cryptography group and Dr. David Rupprecht from the Chair for System Security discuss the sense and nonsense of such backdoors and describe a very long-lasting example of such a gap. Together with international colleagues, they showed that current smartphones still have the insecure mobile phone encryption GEA-1 installed. It has been around since the 1990s, and according to mobile phone standards, it should have disappeared in 2013.\nProfessor Leander, Dr. Rupprecht, Dr. Beierle, you are looking for secret backdoors. What exactly is that?\nDavid Rupprecht: A backdoor is a kind of in-built weak link in the encryption process. You can think of it like a master key that shouldn’t exist in the first place. In the case we are investigating, it is physically located in a chipset installed in mobile phones, i.e. on the hardware.\nGregor Leander: In our case, it’s symmetric cryptography. This means that all those legitimately involved in the communication – in this case mobile phones and cell towers – have the same key. The underlying algorithm is, so to speak, the recipe for producing these keys.\nRupprecht: In order to generate the key, which, by the way, is regenerated with every new contact between the mobile phone and the mast, an additional secret code stored on the SIM card of the mobile phone is needed. Based on this, the GEA key is calculated by an algorithm, both from the mobile phone and the mobile mast. The result means for both of them: we are friends, we can communicate.\nWhich data is affected by the security vulnerability in GEA-1?\nLeander: Basically, all of them. But this is not relevant for all data. Because when I use online banking, for example, the data is additionally encrypted by the bank, end-to-end, so it is not decrypted at all in between.\nChristof Beierle: In the 1990s, when GEA encryption was first introduced, this was not yet the case.\nLeander: However, such backdoors are less about the actual contents of the information that is sent back and forth, but rather about metadata, which is often underestimated. It’s about the information: who communicates with whom and when? This metadata is of tremendous value. This is evident from the fact that Meta Platforms introduced end-to-end encryption for WhatsApp without much pressure from users. This seems like a contradiction, because Facebook lives off data. The reason is simple: Facebook still sees the metadata. And this is enough.\nWho instigates the installation of backdoors in the systems?\nLeander: Such backdoors are of course in the interest of the secret services and law enforcement agencies. Invariably, backdoors for these purposes are always being discussed, even if they don’t make much sense. In the case of GEA-1, you have to remember that it was developed in the 1990s. At that time, cryptography was considered a weapon. Powerful cryptography was not allowed to be exported abroad, there were strict export restrictions. But of course people wanted to sell mobile phones to other countries, too. So they had to get around these export restrictions.\nBeierle: We have a document from 1998 on the requirements for the cipher. One of them was: the encryption had to be exportable according to certain restrictions. That means: it had to be just weak enough to get through, but not too weak either.\nWho sets such standards as those governing encryption?\nRupprecht: In the case of GEA, it was the European Standard Organisation ETSI, a kind of DIN institute at European level. The organisation includes, for example, large manufacturers, companies such as Deutsche Telekom, as well as governmental organisations.\nLeander: We can’t rule out the possibility that members of the secret services were also employed there at the time.\nDo we know whether the backdoor in GEA-1 was exploited?\nLeander: As far as GEA is concerned, we don’t know whether it was used or not. But in other cases, it has been proven that backdoors were exploited.\nRupprecht: The revelations published by Edward Snowden, for example, brought to light that Angela Merkel’s mobile phone was bugged. If you wonder how that could have been accomplished, you quickly come up with encryption methods that work not unlike GEA and are used for voice telephony. Here, too, a relatively weak algorithm was integrated.\nDr. Leander, you’ve just indicated that you don’t consider deliberately built-in backdoors to be useful as far as the authorities are concerned.\nLeander: There are 1,000 legitimate reasons for law enforcement and intelligence agencies to want such backdoors to exist. But they are the wrong way to go. A master key like that can also be found by someone who may have criminal intentions. And once the loophole is there, it is always there – after all, we can see that it hasn’t been possible to eliminate GEA-1 to this day, even though it should have been done years ago.\nRupprecht: There is another aspect: if everyone knows that only weak algorithms are allowed, criminals will hide from the authorities by using secure encryption. Criminals don’t care that cryptography is forbidden. They simply switch to their own system. In addition, of course, there are fundamental principles of democracy such as the protection of privacy. Mass surveillance is not compatible with democratic values.\nHow come GEA is still integrated in the latest devices, even though we know that the encryption has a backdoor?\nRupprecht: Well, the manufacturing industry is huge, so maybe it just slips under the radar because it’s not a priority at the moment.\nShould we assume that encryption algorithms with backdoors are active in all our devices?\nLeander: No. We are now keeping a close eye on things.\nRupprecht: Not in end devices. This is currently an issue in the network products, for example routers, on which the internet is based. There are examples of more recent encryptions with backdoors. A recent case is the manipulation of random number generators by the US secret service NSA. Randomness is often necessary in encryption algorithms, and if you ensure that zeros instead of ones are generated super randomly, you can simplify the encryption keys. In the case of NSA, the manipulated algorithm was so slow that no one wanted it, so companies were paid to put it in.\nLeander: On the other hand, cryptographic algorithms without a backdoor do exist.\nRupprecht: There’s been a shift since the 1990s: The weaknesses of cryptography at that time are now known, and the algorithms have become more public.\nBeierle: It’s always suspicious when algorithms are not public. The GEA1 standard, for example, was secret.\nLeander: Today, the selection of encryption methods is public and transparent. Researchers submit proposals, which are evaluated in a multi-stage process. If there’s even a hint of ambiguity, the proposal is immediately rejected. So there are no more deliberate weaknesses in public encryption protocols. This is also one of the reasons why we at the Cluster of Excellence CASA believe that protection against secret services like the NSA is possible: mathematical algorithms do exist that no-one in the world can break. Therefore, we can be hopeful.\nWhat are your plans for your future activities?\nLeander: We will continue to look for backdoors. There are indications that they exist, the only problem is finding them. We are looking for them in a structured way. We look at large programmes, sift out the cryptography and analyse them – especially the ones that are new to us. Some of them are secret. In the case of GEA-1, a whistleblower tipped us off, and the same applies to another case we are currently investigating.\nHow come there is no public outcry when such discoveries come to light?\nLeander: There’s no outcry from users, but there is a great echo in the press. The interest is there.\nBeierle: Maybe there wasn’t such a big outcry at GEA, because the method is so old and no longer poses a danger.\nRupprecht: We have to make end users understand what is at stake. But the problem remains very abstract for many. The situation is different in the industry. Manufacturers are really willing to do something about it.\nLeander: You really have to distinguish between users and decision-makers. End users don’t care about their data. Quite the contrary, considering how they use social media. The same applies to using amazing services on the internet for free – how does that work? Simply by having your data harvested. But people don’t care about that. The decision-makers have to care. It’s like driving a car: if seat belts weren’t compulsory, no-one would wear them.\nThe article is published as part of the IT security special issue of the science magazine Rubin 2022/23.\nGeneral note: In case of using gender-assigning attributes we include all those who consider themselves in this gender regardless of their own biological sex.']	['<urn:uuid:55f17af3-e11c-48d2-9254-41272cca4f35>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	11	47	1500
41	when did the british royal family visit st pauls cathedral queen victoria prince of wales	Queen Victoria and the Prince of Wales visited St. Paul's on February 27, 1872, as recorded in the inscription on Horace Jones's Temple Bar Memorial.	"[""Queen Victoria and the Prince of Wales Going to St. Paul’s (1880) on Horace Jones's Temple Bar Memorial, London EC1. The inscription at the top of the relief reads, “Her most gracious majesty Queen Victoria and His Royal Highness Albert Edward Prince of Wales ” and continues below: “Going to St. Paul’s February 27 1872.”\nThe center of the relief showing the royal party with Queen Victoria second from the right in the carriage and Edward Prince of Wales the bearded man on the left. [Larger image of the royal party.] Note the little girl wearing what looks like a straw boater hat extending flowers toward the coach and the young boy at the right falling back against his father either in fear of the horse or to get a better view of the guardsman riding it.\n[Click on these images to see larger pictures.] Left: The left side of the relief includes a sailor boy standing on a stone block inscribed “Elkington & Co. Founders,” and a similar block on the side right proclaims “C.S. Kelsey & Son Sculptors 1880.” Both ends of the piece show policeman in conflict with young boys: on the left a boy argues with a mounted bobby, and at the right a bobby, who turns back to look at the procession, has both hands on a young man while young men and women stand on bases of lampposts to get better views of the royal carriage. Kelsey fills his scene with genre details, such the young boy who has fallen or been knocked to his knees and lost his hat. He also provides the details of the columns, ogee arch, and drapery of the gothic structure through which the procession will enter the City of London.\nAlthough this commemorative plaque in high relief places the royal family at its exact center, they in no way dominate the composition, which instead emphasizes the crowd's enthusiasm, the large number of young people present, and some small examples of disorder.\nPhotographs by Robert Freidus. Formatting, perspective correction, and commentary by George P. Landow. You may use this image without prior permission for any scholarly or educational purpose as long as you (1) credit the photographer and (2) link your document to this URL in a web document or cite the Victorian Web in a print one.]\nThe Temple Bar Memorial, its sculpture, and related material\n- The Temple Bar Memorial designed by Horace Jones\n- Sir Christopher Wren's original Temple Bar as re-erected in Paternoster Square in 2004\n- Time and Fortune Draw a Curtain Over Temple Bar by Charles Henry Mabey\n- Queen Victoria's Progress to the Guildhall London Nov. 9th 1837 by Charles Henry Mabey\n- Plaque marking the location of the original Temple Bar and the border of the City of London by Charles Henry Mabey\n- Queen Victoria by Sir Joseph Boehm\n- Friezes on the Temple Bar Memorial\n- Edward Prince of Wales by Sir Joseph Boehm\n- Griffin by Charles Birch\n“Charles J. Samuel Kelsey.” Mapping the Practice and Profession of Sculpture in Britain and Ireland 181-191. University of Glasgow History of Art and HATII, online database 2011. Web. 3 June 2011.\nWard-Jackson, Philip. Public Sculpture of the City of London. Liverpool: Liverpool University Press, 2003.\nLast modified 1 August 2011""]"	['<urn:uuid:863a4c7f-7fae-4567-b291-ed33f6460390>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	15	25	550
42	What did Vikings receive in trade from the Skraelings?	In exchange for red cloth, the Skraelings gave furs to the Vikings.	['Dr. Bathsheba Demuth, Brown University.\nThe Greenlandic coast. Source: TheBrockenInaGlory, Wikimedia Commons, 2005, commons.wikimedia.org/wiki/File:Greenland_coast.JPG\nIn the year 1001 CE, Leif Erikson made landfall in Greenland, and traded with people who “in their purchases preferred red cloth; in exchange they had furs to give.” The Vikings called these people Skraelings. Present-day archeologists and historians call them the Thule. At its height, Thule civilization spread from its origins along the Bering Strait across the Canadian Arctic and into to Greenland. The ancestors of today’s Inuit and Inupiat, the Thule accomplished what Erikson and subsequent generations of Europeans never managed: living in the high Arctic without supplies of food, technology, and fuel from more temperate climates.\nThe Thule left archeological evidence of a technologically sophisticated, vigorous people. They invented the umiak, an open walrus-hide boat so large that it was sometimes equipped with a sail. These boats, when used alongside small, nimble kayaks, made the Thule formidable marine-mammal hunters. On land, they harnessed dogs to sleds and built homes half-underground, insulated by earth and beamed with whale bones.\nPeople did inhabit the high North American Arctic before the Thule. Their immediate predecessors, called the Dorset by archeologists, were expert carvers, and there are signs of other cultures that date back at least five thousand years. But the Thule appear to have been a particularly robust society, one that inhabited thousands of challenging Arctic miles. Eventually, they even traded with Europeans for metal tools, sending walrus ivory as far abroad as Venice.\nThule migration routes from the Bering Strait east. Map credit: anthropology.uwaterloo.ca/ArcticArchStuff\nIn the twentieth century, many archeologists linked the success of the Thule to the climate. In this view, rapid Thule expansion coincided with the Medieval Warm Period in the years between 1000 and 1300. The Thule were expert whalers, especially of bowhead whales. This slow species makes for good prey. Their 100-ton bodies can be fifty percent fat by volume, giving people ample calories to eat and burn through long winters. With the slight increase in temperature during the Medieval Warm Period, the theory went, the range of the bowhead whale expanded across newly ice-free waters. Atlantic and Pacific bowhead populations eventually met in the Arctic Ocean north of Canada, offering an uninterrupted banquet of blubber to hunters.\nThe Thule, in this view, were simply whale hunters who followed the migration of their prey in a warming climate. Environmental conditions, not a sophisticated culture, was the key explanation for their success. Emphasizing climate as the cause of migration and social success reduced the achievements of the Thule, essentially, to those of their prey.\nHowever, twenty-first century evidence is changing this account of Thule migration. In 2000, Robert McGhee questioned the validity of the radiocarbon dates that helped establish Thule expansion as an eleventh-century phenomenon. He proposed the 1200s as the earliest date of migration. Then, genetic tests by marine biologists showed that Atlantic and Pacific bowhead whales did not mix their populations during the Medieval Warm Period, meaning that there was a substantial gap in whaling possibilities on the Arctic coast.\nSomething more complicated than just following the blubber drove the Thule eastward. McGhee speculated that communities moved for iron, which is short supply in the Arctic. Thule hunters learned from the Dorset people of a deposit left by the Cape York meteorite. They colonized huge territories to secure their access to this precious resource from outer space. Other specialists theorized that population pressure, overhunting, or warfare led the Thule to migrate east.\nThule archeological site, with whalebone beams among flooring stones. Photo credit: anthropology.uwaterloo.ca/ArcticArchStuf\nThe ongoing work of Canadian archeologists T. Max Friesen and Charles D. Arnold seems to confirm that we must look beyond simple climatic explanations for the Thule expansion. Working on Beaufort Sea and Amundsen Gulf sites, the pair established that there was no definitive Thule occupation in this part of the western Arctic prior to the thirteenth century. Because any Thule migrants would have had to pass through these points as they moved east, their research indicates that the Thule civilization was only beginning its continental spread around the year 1200, well into the period of warming. The climate may have helped the Thule quickly spread toward Greenland, but the onset of the Medieval Warm Period did not automatically draw people eastward.\nMoreover, the work of other archeologists on the Melville Peninsula, along Baffin Bay, indicates that the Mediaeval Warm Period was not always so warm. Some areas of the Arctic saw slight temperature increases, but in general the millennium was cooler than those past. In places, the effects of the so-called Little Ice Age began a century or two before they were evident across the globe, meaning the Thule adapted not to a warmer Arctic, but a colder one. This cooling was more apparent in the west, where the team found fewer Thule sites but also more stability, both in the climate and the record of human occupation. To the east of the Melville Peninsula, where temperatures did warm, the climate was also more variable – adding a new set of complexities to social and economic life. The move into the central Arctic, therefore, reflected forces other than climate.\nBeginning in the fifteenth century, Thule culture fragmented, specialized, and emerged eventually as distinct contemporary Inuit and Inupiat groups. The Little Ice Age is often the reason given for the disintegration of Thule civilization in the fifteenth century. Yet, the work by Finkelstein, Ross, and Adams indicates that, while the Thule abandoned some sites due to cooling trends, this did not hold in all cases. Other causes, including increased contact with Europeans and their infectious diseases, might have had more to do with the disintegration in some locations.\nOverall, the new vision of the Thule prominence in the Arctic makes their rise shorter, but even more impressive. And if the Thule began their migration only in 1200, it seems unlikely they spread east simply to find iron. This would have required only smaller-scale movements to precise locations. Instead, the Thule developed a thriving, intricate network of settlements across the Arctic. For Friesen and Arnold, this is evidence that the Thule expanded in order to recreate the ideological and economic lives that they had enjoyed in their origins along the Bering Strait. And in just a century they did, not only by inhabiting land from the Bering Strait to Greenland, but through explorations to the northern edges of the continent.\nAll of this also helps us reinterpret a well-known tale from the Viking exploration of the Arctic. When Leif Erikson’s sister Freydis frightened off a band of Skraelingar in the early eleventh century by striking “her breast with the naked sword” of a fallen Viking, she was likely not fighting the Thule, as scholars have assumed. Perhaps it was the Dorset people that “were frightened, and rushed off in their boats.” The Thule, at least, were likely still a century away from the eastern Canadian coastline. They were not easily daunted either by a shifting climate or by Viking weapons.\nQuotes from the Saga of Erik the Red, English translation by J. Sephton, can be found here: http://www.sagadb.org/eiriks_saga_rauda.en\nFriesen, T. Max and Charles D. Arnold. “The Timing of the Thule Migration: New Dates from the Western Canadian Arctic,” American Antiquity 73 (2008): 527-538.\nFinkelstein, S.A., J.M Ross, and J.K Adams. “Spatiotemporal Variability in Arctic Climates of the Past Millennium: Implications for the Study of Thule Culture on Melville Peninsula, Nunavut,” Arctic Antarctic, and Apline Research 41 (200): 442-454.\nMcGhee, Robert. “Radio Carbon Dating and the Timing of the Thule Migration,” in Appelit, M. Berglund, J, and Gullov, H.C. eds. Identities and Cultural Contacts in The Arctic: Proceedings from a Conference at the Danish National Museum. Copenhagen (2000): 181-191.\nMorrison, David. “The Earliest Thule Migration.” Canadian Journal of Archaeology 22( 1999): 139-156.\nBetts, Matthew, and T. Max Friesen, “Quantifying Hunter-Gatherer Intensification: A Zooarchaeological Case Study form Arctic Canada,” Journal of Anthropological Archaeology 23 (2004): 357-384.\nDyke, Arthur S., James Hooper, and James M. Savelle. “A History of Sea Ice in the Canadian Arctic Archipelago based on Postglacial Remains of the Bowhead Whale (Balaena mysticetus)”, Arctic 49 (1996): 235-255.\nPark, Robert W. “The Dorset-Thule Succession Revisited,” in Appelit, M. Berglund, J, and Gullov, H.C. eds. Identities and Cultural Contacts in the Arctic: Proceedings from a Conference at the Danish National Museum. Copenhagen (2000): 192-205.']	['<urn:uuid:4525291f-ef55-4e04-ad5d-e0dda73b1ede>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	9	12	1395
43	cosmic ray study neutrino muon purpose	Both particles are studied to understand cosmic phenomena but serve different purposes. Neutrinos are tracked to understand the origins of high-energy cosmic rays and to potentially open a new window into the universe, while muon detection is being utilized for practical applications like tsunami detection and potential early warning systems, as demonstrated by the Tokyo Bay tunnel observatory.	"['What are neutrinos?\nIn the most basic sense, neutrinos are elementary subatomic particles that have almost no mass and can travel vast distances without interacting with normal matter. “They can go through light-years of lead without stopping,” says Nathan Whitehorn, an IceCube physicist at the University of Wisconsin-Madison. In fact, trillions of neutrinos are passing right through you each second.\nNeutrinos owe their ghost-like powers to the fact that they have no electric charge (are unperturbed by electromagnetic forces) and are only affected by the weak nuclear force, which works at very short ranges, and gravity, which is quite weak at subatomic scales.\nFamed physicist Wolfgang Pauli initially proposed the concept of the neutrino back in 1930 to explain what happens when tritium (an isotope of hydrogen) undergoes beta decay. You see, when tritium decays into helium-3, an electron is also released, carrying away some of the energy. But scientists noticed that the equation was unbalanced - tritium had more energy than its resulting decay product, suggesting that an invisible particle must be carrying away additional energy.\nScientists didn’t detect neutrinos until over thirty years later. A team led by Clyde Cowan and Fred Reines detected neutrinos coming from a nuclear reactor and published their results in the journal Science.\nThere are three known types (flavors) of neutrinos: electron neutrino νe, muon neutrino νμ and tau neutrino ντ, named after their partner leptons in the Standard Model. The current best measurement of the number of neutrino types comes from observing the decay of the Z boson. This particle can decay into any light neutrino and its antineutrino, and the more types of light neutrinos available, the shorter the lifetime of the Z boson. Measurements of the Z lifetime have shown that the number of light neutrino types is 3. The correspondence between the six quarks in the Standard Model and the six leptons, among them the three neutrinos, suggests to physicists’ intuition that there should be exactly three types of neutrino. However, actual proof that there are only three kinds of neutrinos remains an elusive goal of particle physics.\nWhere do neutrinos come from?\nThere are numerous sources of neutrinos in the universe, both in space and here on Earth. Wherever there is nuclear physics, there are neutrinos; they are the liaisons that make nuclear physics possible.\nNeutrinos are produced in the sun and other stars. They are the byproduct of nuclear fusion, which involves the merging of two protons (hydrogen atoms) to form a deuteron, releasing a positron (antielectron) and an electron neutrino at the same time.\nHigh-mass stars end their lives in supernova explosions, which also produce neutrinos. Before the explosion, the star collapses in on itself, forcing protons to combine with electrons, forming neutrons and electron neutrinos. Eventually a neutron core will form in the center of the dying star - as it cools, it releases neutrino-antineutrino pairs of all flavors.\nScientists also believe that tons of neutrinos were created during the Big Bang, so there should currently be a “cosmic neutrino background” similar to the cosmic microwave background radiation. These particles, however, are thought to have energies too low to detect with current neutrino detectors.\nUltimately, the scientists hope the neutrinos will help them better understand the origins of high-energy cosmic rays, which are a mystery to scientists. Essentially, anything that could produce cosmic rays would also produce these high-energy neutrinos. So they are now trying to trace the neutrinos back to their source(s) - a task that should be easier than tracing back cosmic rays because neutrinos travel in a straight line, unperturbed by strong magnetic fields.\n""These neutrinos could be clues to the origins of cosmic rays,"" Whitehorn says. ""We could be opening a new window into the universe.""\n583 Notes/ Hide\n- twistedfixtionssickaddixtions reblogged this from we-are-star-stuff\n- verybluebirdy likes this\n- satanicpizzaparty reblogged this from we-are-star-stuff\n- lvlmuse reblogged this from dystopianfuture\n- traipsingtheergosphere reblogged this from tonybone98\n- h3isenb3rg reblogged this from dystopianfuture\n- h3isenb3rg likes this\n- upj likes this\n- sleepychow likes this\n- bryan-braffney likes this\n- dystopianfuture reblogged this from bionikbastard\n- dystopianfuture likes this\n- leodfalcone likes this\n- tonybone98 reblogged this from bionikbastard\n- fryghtening reblogged this from bionikbastard\n- mauxuam likes this\n- bionikbastard reblogged this from dragonsinthefountain\n- bionikbastard likes this\n- dragonsinthefountain reblogged this from thescienceofreality\n- kidslipstream reblogged this from thescienceofreality\n- causersofme likes this\n- mokusatsu reblogged this from buttflower\n- cmdoesitall reblogged this from thescienceofreality\n- squishy-ass-witch reblogged this from thescienceofreality\n- tatrtotz reblogged this from galaxyclusters\n- under-lock-and-keyy likes this\n- nine-reasons-to-say-goodbye likes this\n- secretporcupine reblogged this from classicallyforbiddenregions\n- pragmatic-dude reblogged this from thescienceofreality\n- the-ephemeral-poet-society likes this\n- ted-from-accounting likes this\n- monkey1059 likes this\n- les-os-creux likes this\n- neverknwsbest reblogged this from thescienceofreality\n- whywontyoustop likes this\n- magneticmoments reblogged this from galaxyclusters\n- kimssecretdiary reblogged this from thescienceofreality\n- kimssecretdiary likes this\n- watermelonsonnet likes this\n- loveinmemories reblogged this from theremina\n- leomantys likes this\n- proteinsdreamingcodons reblogged this from we-are-star-stuff\n- the-flight-and-the-fall likes this\n- the-flight-and-the-fall reblogged this from thescienceofreality\n- everybodys-nemesis likes this\n- readinglist32 reblogged this from theremina\n- readinglist32 likes this\n- borntopie likes this\n- bazaardvark likes this', 'Undersea detector proves it’s swell A first-of-its-kind cosmic ray sensor successfully observes tsunami waves Research news\nHighly energetic particles called muons are ever present in the atmosphere and pass through even massive objects with ease. Sensitive detectors installed along the Tokyo Bay tunnel measure muons passing through the sea above them. This allows for changes in the volume of water above the tunnel to be calculated. For the first time, this method was used to accurately detect a mild tsunami following a typhoon in 2021.\nIn the time it takes you to read this sentence, approximately 100,000 muon particles will have passed through your body. But don’t worry, muons pass through ordinary matter harmlessly, and they can be extremely useful too. Professor Hiroyuki Tanaka from Muographix at the University of Tokyo has made his career out of exploring applications for muons. He’s used them to see inside volcanoes and even detect evidence of ancient earthquakes. Recently, Tanaka and his international team of researchers have turned their focus to meteorological phenomena, in particular, tsunamis.\nIn September 2021, a typhoon approached Japan from the south. As it neared the land it brought with it ocean swells, tsunamis. On this occasion these were quite mild, but throughout history, tsunamis have caused great damage to many coastal areas around Japan. As the huge swell moved into Tokyo Bay, something happened on a microscopic level that’s almost imperceptible. Atmospheric muon particles, generated by cosmic rays from deep space, were ever so slightly more scattered by the extra volume of water than they would be otherwise. This means the quantity of muons passing through Tokyo Bay varied as the ocean swelled.\n“The Tokyo-bay Seafloor Hyper KiloMetric Submarine Deep Detector (TS-HKMSDD) is the first underwater muon observatory in the world, and it detected varying muon activity during the tsunami,” said Tanaka. “This variation corresponds to the ocean swells which were measured by other methods. Combining these readings means we can use muographic data to accurately model changes in sea level, bypassing other methods which come with drawbacks.”\nThere are other ways to measure changes in sea level, with physical mechanisms such as tide gauges, satellites, buoys, or sensors on the sea floor itself. But the TS-HKMSDD and future instruments based on it, installed in undersea tunnels, can be cheaper to build and run, easier to access, and they don’t suffer from physical wear and tear as they have no moving parts. Critically though, the data from TS-HKMSDD is both real time and highly accurate, two key criteria that could make it suitable for a reliable early warning system.\n“Thanks to the success we’ve had from early tests such as this, similar systems are already being trialed in the U.K. and Finland,” said Tanaka. “Obviously, an undertaking like this comes with challenges and installing delicate instruments in a busy tunnel could be difficult. But we are grateful for the cooperation of the agencies responsible for the Tokyo Bay tunnel. To the best of my knowledge, the tunnel is now the first active national road in the world defined as a laboratory.”\nTanaka and his team have many other ways in store to make use of muons, including a possible way to accurately synchronize time around the world, and, related to that, a spatial positioning system far more accurate than current GPS.']"	['<urn:uuid:b4187201-1b89-48a9-9a86-9e1b13f6b952>', '<urn:uuid:ab1f9f86-5319-4488-8e0a-b66119180e97>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	6	58	1425
44	scandinavian art deco interior design materials compare	While Scandinavian interior design emphasizes natural materials like wood, wool, leather, and stone, Art Deco features more luxurious and exotic materials including lacquered wood, steel, glass, crystal, ivory, and animal skins. Scandinavian design focuses on simplicity and functionality, whereas Art Deco aims for bold, glamorous, and decadent finishes.	['Scandinavian interior design: Welcome to a world where simplicity meets sophistication, where functionality embraces beauty – the world of Scandinavian interior design. Hailing from the enchanting Nordic countries, this design philosophy has captured hearts worldwide with its understated yet impactful aesthetic. You might be thinking, “Isn’t Scandinavian interior design expensive?” Fear not, for this blog will unravel the secrets of infusing your home with Scandinavian charm without straining your wallet. Join us as we delve into the essence of Scandinavian interior design and unveil pocket-friendly ideas to revamp your living space.\nDecoding Scandinavian interior design\nScandinavian interior design, born in the mid-20th century across Denmark, Norway, Sweden, Finland, and Iceland, is a reaction to the fast-paced industrial world. It’s a harmonious blend of form, function, and nature’s touch. Here are the core principles that define Scandinavian interior design:\n- Less is More: Scandinavian interior design celebrates minimalism. It rejoices in clean, uncluttered spaces, creating an aura of tranquility and balance.\n- Embrace Nature: Natural materials like wood, wool, leather, and stone lie at the heart of this design ethos. They infuse a sense of warmth and authenticity, connecting your space to the great outdoors.\n- Light and Airy: With prolonged dark winters, Scandinavians have mastered the art of maximizing natural light. Light colors – especially whites and soft grays – dominate the palette, giving rooms an open and fresh ambiance.\n- Purposeful Furnishings: Functionality takes precedence in Scandinavian interior design. Furniture isn’t just art; it’s art with a purpose. Every piece is thoughtfully designed to be practical and beautiful.\nBudget-Friendly Scandinavian interior design Hacks\nScandinavian Interior design #1 – Thrifted Treasures\nDelve into the world of thrift stores, garage sales, or online marketplaces where hidden gems await your discovery. Search for a vintage wooden chair with sleek lines and a timeless silhouette. Even if the chair has lost some of its original luster, a fresh coat of paint can work wonders. Opt for a soft pastel shade or classic white that aligns with the Scandinavian palette. Before painting, lightly sand the surface to ensure a smooth finish. By giving new life to an old piece, you’re not only adding an affordable accent to your space but also contributing to the sustainability of your decor.\nScandinavian Interior design #2 – DIY Minimalism\nChannel your inner artisan with these DIY projects that capture the essence of Scandinavian minimalism. Transform wooden crates into a minimalist side table by stacking them to create tiers. Secure them in place, and you’ll have a functional and stylish addition to your living room or bedroom. To infuse your walls with natural charm, gather branches, twine, and dried leaves. Arrange them in a geometric pattern or let your creativity flow freely. Secure the elements with twine or simple frames for an art piece that pays homage to the raw beauty of nature.\nScandinavian Interior design #3 – Indoor Greenery\nEmulate the Nordic connection to nature by introducing houseplants that thrive indoors. Opt for ferns, succulents, or snake plants – they’re not only budget-friendly but also easy to maintain. Place them in pots made from affordable materials like terracotta or repurposed containers from thrift shops. These plants infuse your home with a refreshing vitality, and their presence aligns seamlessly with the Scandinavian love for the natural world.\nScandinavian Interior design #4 – String Lights Serenity\nString lights are a budget-friendly way to infuse your space with a serene and inviting atmosphere. These lights aren’t exclusive to the holiday season; they can be used year-round to create a calming ambience reminiscent of Scandinavian evenings. Drape them along a wall in a wave-like pattern, suspend them from the ceiling in a cascading arrangement, or intertwine them with potted plants to add a touch of enchantment to your surroundings.\nScandinavian Interior design #5 – Functional Wall Decor\nMake a stylish statement at your entryway with functional wall decor that captures the essence of Scandinavian interior design. Hang wooden pegs or hooks in a minimalist design and a light wood finish. These hooks serve as a coat rack that’s not only visually appealing but also practical. Say goodbye to cluttered entryways, and embrace the convenience of a designated space for coats, bags, and hats that echoes the Scandinavian emphasis on simplicity and functionality.\nScandinavian Interior design #6 – Textured Textiles\nElevate the comfort and aesthetics of your space with budget-friendly textured textiles that exude Scandinavian coziness. Seek out thrifted or affordable finds such as woven blankets, chunky knit pillows, or faux sheepskin throws. Drape a woven blanket over the back of your sofa to add a touch of texture and visual interest. Enhance your bedding ensemble with a collection of chunky knit pillows, and place a faux sheepskin throw on your favorite armchair for an inviting touch that’s both affordable and indulgent.\nScandinavian Interior design #7 – Multipurpose Furniture\nMaximize your budget by investing in furniture that serves multiple functions, embodying the very essence of Scandinavian practicality. Consider a wooden chest that can function as both a coffee table and discreet storage solution. This versatile piece not only complements the clean lines of Scandinavian interior design but also adds a touch of rustic charm. Its dual functionality helps you declutter while maintaining a sophisticated aesthetic.\nScandinavian Interior design #8 – Gallery of Memories\nCreate a personalized gallery wall that speaks to your memories and experiences while aligning with the Scandinavian interior design ethos. Opt for simple, minimal frames that let the content take center stage. Display a mix of your favorite photos, art prints, and pressed botanicals collected during your outdoor adventures. This gallery not only infuses your space with a touch of your personality but also celebrates the connection to nature that’s deeply rooted in Scandinavian living.\nScandinavian Interior design #9 – Repurpose with Purpose\nLet your creativity shine by repurposing wooden crates into stylish and functional shelving units that uphold the Scandinavian aesthetic. Stain or paint the crates in light hues that evoke the natural tones of Nordic landscapes. Arrange the crates in an asymmetrical configuration on your wall to showcase books, plants, and decor items. This DIY approach not only adds a unique focal point to your space but also reflects the simplicity and resourcefulness inherent in Scandinavian interior design.\nScandinavian Interior design #10 – Decluttering Zen\nEmbrace the heart of Scandinavian interior design by fostering a decluttered environment that allows your space to breathe. Keep only what’s necessary and meaningful to create a serene atmosphere. To maintain order while incorporating natural elements, use baskets made from materials like woven seagrass or rattan. These baskets serve as both practical storage solutions and decorative accents that align seamlessly with the Scandinavian ethos of simplicity and harmony.\nScandinavian interior design welcomes you to embrace the beauty of simplicity, functionality, and nature’s elegance. By integrating these budget-friendly ideas, you’re not just transforming your living space; you’re creating a sanctuary that encapsulates the essence of Nordic living. Each element, from the repurposed vintage chair to the DIY wall art, from the indoor greenery to the textured textiles, contributes to a harmonious narrative of tranquility and refinement. As you immerse yourself in these design hacks, your home evolves into an embodiment of Scandinavian grace – an environment that nurtures your well-being and fosters a deep connection with nature and purposeful design.', 'Art Deco Patterns, Colors, and Materials in Interior Design\nConceived in the 1920s, for a hundred years already, Art Deco has kept its position as one of the most renowned and influential styles in architecture and interior design. Bold, sleek, glamorous—the style manifests in luxurious materials, rich colors, decadent details. Anyone in love with the “Great Gatsby” atmosphere, or magic of early Hollywood, would enjoy Art Deco patterns and accents in interior design.\nLet’s look at how to add style and functionality to an open-floor apartment with the use of Art Deco patterns, motifs, and details.\nArt Deco origins\nArt Deco is a child of the Interwar period, absorbing the zeitgeist of post-WWI France and forming a visual representation of its dreams for progress and prosperity. The firm base of Art Deco is woven with other artistic movements of the time: Cubism with its strict geometry and angularity; Constructivism adds impetuosity and boldness of the Machine era; Art Nouveau, its closest predecessor, shares floral and animalistic patterns, softness, providing sensations of a gentle flow.\nThe Roaring ‘20s and ‘30s, no doubt, were reflected in Art Deco as the world changed overwhelmingly fast.\n- Technical advances of the Machine Age brought decisive straight lines and bold color contrasts to architecture and costume.\n- Archeological findings from expeditions to previously inaccessible, exotic countries inspired Art Deco patterns, particularly the Pharaoh Tutankhamun’s tomb from Egypt and the Mayan pyramids and Aztec motifs from Central America.\n- With the rise of cinema, the silver screen images and motifs of early Hollywood not only influenced the lifestyle and aspirations of the public but also spread offscreen as a new form of self-representation and self-construction, including into apartments and interior design.\nThe image of an abundant and profuse lifestyle got put on pause with the devastation of WWII, which brought the austerity of raw functionality to design. At that point, Art Deco froze, substantially unchanged from how we know it today: a clear and distinctive style, easy to implement in modern apartment design and interiors.\nArt Deco materials\nThinking about Art Deco materials, one should keep the main features of the style in mind—bold, luxurious, elegant, decadent. At the rise of the movement, in addition to classic materials such as lacquered wood, steel, and glass—all polished with shining, bright finishes—exotic materials were popular: ivory, crystal, animal skins like those of zebra or shark, tortoiseshell, leather, mirrors. Some materials from distant parts of the globe became reachable due to increased facilitation of travel and were highly demanded.\nGlass and steel were widely used as brand new materials at that time. Along with technological progress, the style grew richer with newly available materials like woods and metals processed in a more sophisticated, cost-effective way and plastic. Still, the ideas of progress and the potential of humankind made some think ecologically and choose more lasting and harmless materials, like steel and glass, and avoid using animal fur or bones as decoration.\nArt Deco patterns\nPatterns are a key element of Art Deco. Their use is very widespread, rich, and impressive. Ornaments and motifs are very characteristic and can hardly be mistaken for any other style. Among the most popular Art Deco patterns are\n- Zigzags, ziggurats, chevrons: mathematically accurate geometrical shapes, repeating self-enclosed motifs, and minaret-like stepped ornaments that resemble the Aztec pyramids, succinct and symmetrical.\n- Sunbeams, shells, fans: resembling all at once the mask of Tutankhamun, the birth of Venus, the fan of a dancer or a peacock—the emanation of chicness, luxury, innocence, and power.\nLet’s consider some ideas on how to implement Art Deco patterns to an apartment interior. In general, materials that easily display patterns include fabrics, furniture upholstery, and wallpapers. A more sophisticated option is to have a partition from steel and/or glass made. A partition itself, as a modest zoning tool that divides an open-floor layout in a functional space, is invaluable. Moreover, decorative technologies allow a partition to be a medium for the style.\nAnd mediums can be mixed! Metals like brass, aluminum, or steel with glass in a variety of finishes can create a unique Art Deco object. Steel patterns like stripes, chevrons, or stepped lines can be applied to glass partitions. Another option is a whole-metal partition with laser-cut ornaments. The number of options is only limited to one’s imagination.\nIn terms of colors, Art Deco loves striking contrasts and subtle shading; rich and vivid colors are widely used. Reds, greens, blues, blacks, pinks—all conform organically with metal, crystal, silver, and mirrors. Also, for a softer effect associated with bedrooms and dining rooms, there are neutral Art Deco color schemes. Pastel tones, beiges, creams go perfectly with lacquered wood or any polished glass surface. A variety of glass finishes are appropriate. Use any of the colors and effects named above to complement a functional glass object in the apartment, from a coffee table to a transom.\nAdditionally, glass partitions are used to solve several practical concerns of an open-concept layout. Frosted and satin glass finishes on a partition add privacy and provide distinct zoning. With tinted glass, one can create rich color accents, while textured glass can subtly add more Art Deco patterns.\nElegant and luxurious, Art Deco features in an apartment create high contrast and boldness in the space. One can enjoy a succinct monochrome palette, plenty of vivid color accents, meticulous geometrical patterns, and roundish or streamlined motifs. Just bringing some Art Deco objects into an apartment can create an expensive, classy look. And Art Deco patterns can create a sexy, streamlined effect.\nBoth a decorative and functional option for an open-floor apartment is steel and/or glass partitions. With the use of glass finishes and laser-cut technology, a partition design can include Art Deco patterns in the preferred color scheme. A partition can become an influential statement in the interior, bold and elegant, a central element for the smaller decorative features to play with.']	['<urn:uuid:4c686c3c-558b-40f4-92e2-f20ea8064e70>', '<urn:uuid:b99ec51b-a12b-4a2b-97e6-f1df6e25cdb8>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	7	48	2202
45	construction barrier protect crime safety	Construction site hoarding provides dual protection. For safety, it acts as a perimeter boundary that shields workers from debris and protects the public from construction hazards, with features like toe boards, brick guards, and netting to prevent falling materials. For security, it's crucial in preventing crime - according to industry surveys, theft (42.7%) and vandalism (42.8%) are common monthly occurrences at construction sites, making secure barriers essential. The hoarding includes secured gates at access points that only contractors can control, effectively preventing unauthorized entry.	['Safety is not an option or a privilege for anyone. You have all the right to be physically protected or at least warned for any possible danger. The law mandates an assessment of any potential risk in an environment that may cause any form of tragedy or mishap to the general public. Establishment sites, some research laboratories, and other places of work have specific health and safety regulations. So, what are some of the practices that promote safety to people like you? Specifically, how do construction companies ensure safety not just for the people, but also for their employees or workers?\nHave you ever seen a barrier that separates a construction site from a local area? Have you noticed those panels covered by tarpaulins that advertise something about the building? Those are called hoardings. And technically, hoarding construction is essential for any construction process.\nWhat exactly is a hoarding construction?\nHoarding serves as a perimeter boundary that shields construction workers from debris and prevents them from being distracted by the view of the surrounding area. At the same time, it protects the public from the construction site’s potential risks. It should be thick and durable to avoid accidents that may arise near the construction area. Hoardings also provide access to authorized personnel only. Trespassing is prohibited in all construction sites.\nTry to observe if there are ongoing building constructions on your way home. You will see that there is a fence-like hoarding construction board that surrounds the entire site, which comprises metal or wood. You might also notice that there are images, texts, and even graphics in it too. It is a smart marketing move for people to remember what it would look like when the construction is over, right? Hoarding construction boards can be taken advantage of by marketing specialists for brand awareness and local advertisement. It is the best way to promote who is responsible for the site as it is most likely situated in a prime location.\nThings about hoarding constructions that you should know\nMaybe you might have ignored the presence of a certain construction site when you walked past it. You are confident that the authorities have ensured its safety. But some vulnerable individuals are paranoid of falling debris or unstable scaffoldings. So there are some steps that construction companies consider in site hoarding. Principal contractors ensure that the hoardings will not obstruct the normal activities of the people who pass by and prevent them from trying to view the site.\nThey also use warning tapes or barricades to highlight the hazard posts in the area. You should already know that unauthorized entry to the site is prohibited. Those hoardings have secured gates at access points that only the contractors can control. Even in the presence of secured hoardings surrounding the site, contractors also operate additional safety procedures to make sure that objects, materials, and tools will not fall beyond the boundary. They use toe boards, brick guards, and netting to avoid debris falling out of the hoardings that can jeopardize anyone’s safety.\nSince public protection should be the topmost priority of construction companies and site managers, compliance with health and safety regulations is part of their responsibilities that they must take seriously. And site hoarding construction is just one of the few safety measures that they implement.', 'Why have site hoarding? Safety, security and more\nSite hoarding, why is it needed?\nThe Health and Safety at Work Act 1974 is the law that governs the obligations of anybody undertaking setting up a construction site. No work is permitted to take place on a site until a secure barrier is in place around the site. Such a barrier must enable enforcement of controlled entry to a site, in such a way as to ensure, so far as is reasonably practicable, that persons other than themselves or their employees are not exposed to risks to their health or safety. Why is site hoarding needed and not just a mesh panel fence?\n… the HSE goes further, not just any old 2m barrier will do. In “city centre sites and residential areas” …”larger hoardings maybe appropriate”.\nA key focus of the HSE’s guidance being to prevent children accessing sites they also advise “Keep gaps underneath the fence or gate as small as possible to stop anyone gaining access under the fence. Make sure children cannot get access through gaps under temporary fencing. On uneven ground gaps can be quite considerable if steps are not taken to level the surface”\nBecause Perimeters re-usable hoarding is made up of 50cm wide panels, the hoarding can be adjusted to more closely follow the contours of the ground than traditional hoarding that uses wider panels.\nSo there you have it as long as you can keep wandering members of the public and curious kids out, all’s well? Not really, operating a construction site also involves protecting the welfare of those working on the site. And welfare of any site also includes crime prevention.\nAccording to a survey by The Chartered Institute of Building (Crime in the construction industry) theft (42.7%) and vandalism (42.8%) are the two construction site crimes mostly seen as common or very common, i.e. with monthly occurrences. Tools and building materials were the most likely items to be stolen, with 83% of respondents experiences these crimes. Opportunist third party criminals are thought to be responsible for theft of unattended plant. Graffiti and damage to the site were the most common forms of vandalism, experienced by three quarters, with comments indicating youths were mainly responsible, and the occurrence is highest in the urban areas.\n..and how site hoarding can prevent crime\nAccording to the same survey, amongst the most effective (and very effective) security measures are: secure storage (84.8%) and heavy duty barriers and gates (67.3%), with both measures being seen as being more effective for security than criminal records checks (54.3%) or taking up references (56.9%). So physical security measures including effective barriers are seen as the most effective at preventing the most common crimes on construction site.\nSo site hoarding plays a key role in ensuring welfare of the public and crime prevention on constructions site. Hoarding can have an even more positive role, promoting the project that’s being undertaken.\nMarketing the project\nConstruction project’s hoarding are often said to have “deemed consent” and don’t need planning permission to carry marketing graphics. The Town and Country Planning Regulations 2007, sets out precise conditions for non-residential projects marketing e.g. not more than 4.6m above ground, not in place for more than 3 years. Only residential projects always require planning permission for marketing. But whether or not a project requires planning consent, for the duration of the project it is key that the “The advertisement must be kept in a safe condition”. This means the marketing panels should stay fixed, but more importantly that the underlying hoarding the is secure.\nSafety, security, marketing and sustainability: Perimeters re-usable hoarding ticks all the boxes\nAs noted in the Temporary Works Forum Guidelines, timber composite boards’ (OSB, ply and particle boards) tendency to deteriorate when exposed to weather “limits the use of wood particleboard to short term hoardings in less exposed locations” . However Perimeters’ re-usable hoarding has no such issues. Perimeters re-usable hoarding is impermeable so doesn’t deteriorate no matter how wet they get or for how long. So a sites’ boundary stays protecting the public from the site’s activities and the site from nefarious intrusion by members of the public.\nFor the same reason, a reusable hoarding that stays true and doesn’t distort during the life of a project, Perimeters’ re-usable provides the perfect backing for marketing graphics. Whether di-bond panels, cutouts of full vinyl wraps, Perimeters provides a continuous surface to display your messages on.\nMove from problem waste to sustainability\nFinally the same Crime in the construction industry survey also noted that Illegal waste disposal was sited by over half respondent as an issue. For hoarding that means what happens after the project. With Perimeters that worry is gone, as 100% of Perimeters’ boards are either re-used or recycled. In short Perimeters re-usable hoarding ticks the sustainability box, unlike timber composite hoarding which ends up in landfill after a single project or even part way through longer projects.\nSo if your project needs hoarding, and it does, no other hoarding fulfils the needs better than Perimeters re-usable hoarding.']	['<urn:uuid:ab77f723-5615-42a6-a6cd-302f01aa5fe5>', '<urn:uuid:012f0dfa-c84c-4407-b1ee-5dcf9b1806e7>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	5	84	1400
46	How do you make big software tasks simpler, and what makes a software task good for users?	To make big software tasks simpler, they can be broken down using several techniques: splitting them into smaller scenarios (like breaking down a light scheduling system into individual cases), using spikes to investigate unknowns, creating stubs for unknown parts, and time-boxing activities to limit their scope. As for what makes a software task good for users, it should follow the INVEST criteria: being Independent (self-contained), Negotiable (leaving room for conversation), Valuable (delivers value to end user), Estimable (can be properly sized), Small (completable in 3-4 days), and Testable (can be confirmed through acceptance criteria).	"['Stories often start out too big. Big stories are a challenge, and it is not always obvious how to deal with them. Its important that stories be small enough to estimate, to fit easily into an iteration and to have a decent definition of done. This article explores why some stories don’t fit this mold and what you can do about it.\nThe main causes of bloated stories leading to an inability to estimate them are:\n- Stories that cover too much\n- Developers that do not have the needed domain knowledge\n- Stories that have uncertainty due to dependence on new technology\n- Stories that the customer cannot articulate exactly what they want\nI find the four S’s helpful in getting stories down to size so that they can be estimated and worked on. The four S’s are:\n- Time box\nOK, I admit it, there aren’t fours S’s, just three followed by a T. Let’s look at how to apply SSST.\nSplit is the most obvious tool in the big story weight reduction toolkit. A big story covers too much ground; it has too many special cases. People new to driving development with stories run into this one all the time. The key here is to slice the story up into different scenarios. When stories come in big and vague, split them by coming up with specific demonstrable scenarios.\nFor example, let’s say you are designing a home automation system. You have the big story, schedule lights. Asking the customer what is included she says:\nturn on, off or dim lights, based on day, day of week, weekend, week day, and everyday, at specific times with optional random variability. I want a holiday schedule too, you know to turn on and off the Christmas tree and Christmas lights starting the day after Thanksgiving and going until January 6th.\nOh, she’s good. That’s a mega story, but it breaks easily into these more manageable stories:\n- Schedule a light to turn on, off, or dim level\n- Schedule multiple lights\n- Random variability scheduled light\n- Holiday schedule\nIf story #1 is still too big, it could be broken down further to deal with each scheduling special case like this:\n- Schedule light-on everyday\n- Schedule light-off Tuesday\n- Schedule light-on weekends\n- Schedule light-on weekdays\n- Schedule light-dim\nBig stories like this seem to pop up all the time in embedded development (yeah, other places too). If the whole feature is not there, it is not valuable, so we can’t split it. This is business as usual thinking. Split the big story to increase visibility into the development progress, even if you can’t release it. You will discover work that can be deferred, delayed, or eliminated when you split stories into finer grains.\nA story has too many unknowns, either because of lack of domain knowledge or the technical knowledge needed to implement it. In this case, doing a spike will help. A spike is a time-boxed activity where development takes a deep dive in the technology or domain to better understand how to meet the need. A spike is an experiment. It’s goal is to gain knowledge so that developers can estimate the story. We’re training to mitigate risk associated with unknowns. After the spike is complete, we may discover that the story is still too big and needs to be split, or maybe another spike is needed to dig a bit deeper.\nIn our home automation system, we need to support X10 devices, as well as other control technologies. One problem, we’ve never done X10 before and have no idea how much work it is. So we introduce an X10 spike, where we do enough investigation and prototyping to see if we can control some X10 device.\nSometimes part of a story is known and part is a mystery. Can we make progress on light scheduling when we have no idea of how to turn on or off an X10 device? In a word: yes.\nIt’s time to fake-it and use a stub implementation of the light-controlling code. Test drive the light scheduler right up to the interface that protects the core light scheduler behavior from the specifics of controlling a light. Stub out the code that talks to the X10 devices so you can capture light instructions in test cases. You won’t really have a deployable light scheduler until you figure out X10, but plenty of demonstrable progress in the mean time.\nThis kind of story requires that your product owner can get used to evaluating story completion by witnessing a story test, a script that shows the system events and responses.\nWhen you choose to fake it with a stub, you are splitting the story. So, add a story to the backlog to keep track of the integration work. For example: Control X10 light.\nOh yeah, once the scheduler is well along, the stub and its interface will provide a specification of what is needed from the X10 device driver. A nice side effect is that you can also insulate your core application from having specific knowledge of X10 when you keep the interface intention revealing (and avoid implementation revealing); that will be a plus when a WiFi (or whatever) light controlling device is added to the system.\nSometimes you need to try things out. I am working with a game company, and they have the fun factor. They ask “How do you schedule making a game fun?” and tell me “If its not fun, we can’t release it.” Fun is hard to specify. How much fun is enough? Some can never get enough fun. Fun has to be balanced with putting in the less-fun features that must be there to make the game shippable and supportable?\nLet’s say we have 25 points of work to get done in this iteration. Five of those points are allocated to making the game’s feature X more fun. The other 20 points are needed to keep the overall release schedule in line with the desired ship date.\nWe can look at the 5 points allocated to fun as the budget we are allowed to spend on fun in this iteration. That’s about 20% of the overall effort. If its not fun when we have spent the 20%, the product owner will schedule some more fun for an upcoming iteration. If it is really critical that it be fun now, the product owner could decide to push something out of the iteration in favor of improving the fun-factor. Either way we are trying to manage and keep visible, what would otherwise be an open-ended activity.\nGeneralizing this idea, the customer knows they need something, but until they get it they can’t be sure if it is right. This problem is as old as software development, even older. Probably as old as the first customer/supplier relationship.\nCombining the ideas\nYou can see that these tools are not just used alone. Use them together. Our light scheduling story illustrates the idea of a big story that has well understood parts (when lights should go on and off) and mysterious parts (how the heck does X10 work?). Starting with a single story in the backlog, now you have three: the original story implemented by using a stub; the spike story used to go unravel the mysterious part (story name: Spike X10), and a story to productize what is learned in the spike (story name: Turn on X10 light).\nSpike X10, being targeted at an unknown should be time boxed, so that activity does not grow to consume too much time. You might need more time on the mystery, but we should monitor that progress to avoid hidden work and constantly being 80% done.\nTurn on X10 light could have a budgetary estimate initially, that gets refined once the spike is completed.\nSplit, spike, stub and time box are helpful tools to cut big stories down to size.', 'In this topic, we described about the below sections -\nWhat is a User Story in Scrum?\nA User Story is an informal, general explanation of a software feature which is written in end user\'s perspective.The actual purpose of this User story is to communicate the way a software feature will provide value to the customer.\nUser Stories are usually used to describe the product features and would form part of the scrum artifacts- the sprint backlog and the product backlog. In software development, product features would play an important role. These are the features that the user would want to use in the final product which are called the Requirements. The success of a software development project will lie in understanding the user requirements appropriately and accurately. Later they would be implemented in the final product. So, the requirements should be thoroughly known to the development project team.\nIn the year 1991, Kent Back has come up with the term ""User Stories"" for the Product features. According to him, a user story would be the one that describes what he or she wants rather than describing what the system can do for him or her. So, the view has changed from the product to the user completely.\nIn Scrum projects, the product Backlog would be a list of all the user stories. These user stories would be prioritized and would be considered into sprint backlog in the Sprint Planning Meeting.\nEstimation is also based on User Stories and the size of the product would be estimated in User Story Points.\nWhy User Stories?\nIn Agile projects, requirements keep changing very frequently as teams and customers learn about the system based on the project\'s progresses. By using the User Story Approach, a big upfront design approach would be replaced with a ""just enough"" approach. User stories would reduce the time spent in writing the exhaustive documentation by focusing on customer centric conversations. Consequently, user stories would allow the teams to deliver quality software more sooner.\nBenefits of using User Story Approach\n- Simple and consistent format would save time when capturing and prioritizing requirements while remaining versatile enough to be used on small and large features similarly.\n- Business values should be expressed yourself by delivering a product which the client really wants.\n- Details should not be introduced too early which prevents design options and lock the developers inappropriately into one solution.\n- Avoid the appearance of false clarity and completeness.\n- Technical functionalities can be left to the developers, architects, testers and so on.\nBasic concepts of User story\nA user story is a lightweight method which is used to capture the ""what"", ""why"" and ""who"" of a product requirement.User stories would often contain fewer than 10 or 15 words each. User Stories are ""to-do"" list which would help to determine the steps along the project\'s path.They would make sure that the process and resulting product would meet the requirements.\nA User Story is defined in three stages -\n- A brief description of the need.\n- Conversations which happen during backlog grooming and iteration planning to freeze the details.\n- The tests which confirm the story\'s satisfactory completion.\nThese three are the 3 C\'s which are the Card, Conversation, and confirmation.\nUser Stories- INVEST\nINVEST would help in assessing the quality of a user story. If a user story fails to meet any of these criteria, the team may have to reword or rewrite it. A good user story should be- INVEST.\n- Independent - It should be self-contained in a way which allows to be released without any dependency on one another.\n- Negotiable - It captures the essence of user needs by leaving the room of conversation. User story should not be written like contract.\n- Valuable - Delivers value to end user.\n- Estimable - User stories should be estimated so that they can properly be prioritized and fit into sprints.\n- Small - A user story is a small piece of work which could be completed in 3 to 4 days.\n- Testable - User story should be confirmed through a pre-written acceptance criterion.\nHow to write User stories?\nTo Write user stories, a template is used which helps to ensure that you do not accidentally start writing technical tasks.\nUser Story template -\nUser stories would capture only the essential elements of a requirement -\n- Who is it for?\n- What it expects from the system?\n- Why is it important?\nWho - The user should be a human who interacts with the system?\n- Be as specific as possible\n- The development team is NOT a user\nWhat - The behavior of the system should be written as an action.\n- Usually unique for each User Story\n- The""system""is implied and does not get written in the story\nWhy - The benefit should be a real-world result that is non-functional or external to the system.\n- Many stories may share the same benefit statement.\n- The benefit may be for other users or customers, not just for the user in the story.\nLifecycle of a User story\nThere are six main states for a user story throughout a software project -\n- Pending - User stories are found through the communication between the project team and user. At this stage, User story has nothing except a short description of the User\'s needs. There will not be any system logic, no detailed design of requirements, no screen design. The only purpose of user story at this stage is to remind all the parties about future discussion of user\'s request which is written in this user story. User card might be discarded in future when required.\n- To-do - Once the discussion between different stakeholders is completed, the user stories would be addressed in the next few weeks. Then, they are place in a time –boxed sprint. Such kind of user stories are said to be in To-do state. Detailed discussion is not yet carried out in this state.\n- Discussing - When a user story is in the Discussing state, the end user would inform to the development team to confirm the requirements as well as to define the acceptance criteria. Development team would write down the requirements or any decisions as conversation notes. UX specialist may create wireframes or storyboards to let user preview the proposed features in visual mock-ups, and to feel it. This process is known as user experience design (UX design).\n- Developing - After the requirements are clarified, the development team would start design and implement the features to fulfill user\'s requests.\n- Confirming - Once the development team has been implemented a user story, it would be confirmed by the end user. He/she would be given access to the testing environment or a half-completed product to confirm the feature. Confirmation would be performed based on the confirmation items written when detailing the user story. Until the confirmation is done, the user story will be in the Confirming state.\n- Finished - Finally, the feature is confirmed to be done, when the user story is considered in the Finished state. This would be the end of the user story. If user has a new requirement, either it is about a new feature, or it is an enhancement of the finished user story. The team would create a new user story for the next iteration.']"	['<urn:uuid:b5465903-2bbe-4b48-a0a0-b7994d33b072>', '<urn:uuid:53d9b97f-a641-48d7-b321-d20f7f2fbe2a>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	17	94	2564
47	What's better to use when stuck in snow - kitty litter or the ESP system for getting the car moving?	Kitty litter is more effective for initial movement as it provides direct traction under the tires when stuck in snow or ice. The ESP (Electronic Stability Program) can actually block the car when starting on particularly icy roads, and often needs to be temporarily disengaged until the car is moving. For older ESP systems, it's recommended to disengage it momentarily and re-engage once the car is in motion.	"['When I was growing up, I noticed that my father kept his car well-stocked with supplies. A lot of the equipment was for his job busting poachers as a game warden, but most of the things were for emergency situations that could happen to anyone. And there were plenty of times when my dad was able to put those supplies to work.\nBe it a maintenance issue or a snowstorm, keeping the following items in your vehicle can save you time and discomfort, and perhaps even your very life, should an emergency arise. Obviously, the necessity of some items depends on the environment in which you live/are driving through (you don’t need an ice scraper in Tampa) and the season (though it’s best just to stock this stuff and keep it stocked, rather than removing/adding things as the seasons change).\n1. Paper maps. Sometimes — okay, plenty of times — Google Maps or Waze doesn’t want to cooperate. And if you don’t have service, their reliability is of no import anyway. It’s always a good idea to keep paper maps handy of the areas you’ll be driving through.\n2. Snacks/MREs. You never know when you’ll be stranded for long periods of times in your car. And depending on where you are, you could be dozens of miles from the closest source of help. Keep some MREs or granola/power bars in the back of your car to munch on while you wait for a tow truck to come, or to sustain you for a long walk to a gas station to call for help.\n3. Cell phone charger/extra battery. Cell phones, and their batteries, are notoriously unreliable and quick-draining in emergency scenarios. It’s like they know when you need them most. Build some redundancy into your car’s emergency kit by keeping both a charger, and an extra battery. No excuses; they’re cheap these days.\n4. LifeHammer. Should an accident trap you in your car, this rescue tool could save your life in a couple ways. It has a seat belt cutter, a steel hammer head that easily breaks side windows, and a glow-in-the-dark pin for easy retrieval in the dark. Every car should have one easily accessible!\n5. Flashlight. Good for providing light at nighttime when 1) putting on a spare tire, 2) jump starting another car, or 3) exchanging insurance information with the clueless driver who rear-ended you at a stop light. Get a Maglite and you can also thump would-be carjackers in the head with it.\n6. Portable air compressor. When your tire is leaking but hasn’t totally blown out, instead of putting on a spare, you can use a portable air compressor to get back on the road. The compressor fills your tire up enough to allow you to drive to a repair shop to get it fixed. It plugs right into your cigarette lighter. Bonus use: no more paying 75 cents to fill up your tires at stingy gas stations.\n7. Windshield wiper fluid. Few things are as indispensable as wiper fluid. Dirty windshield, no fluid, and wet, dirty roads? Get used to stopping every 10 minutes to clean the windshield. Always have some in the car for when you inevitably run out and need it most.\n8. Roadside flares. When pulled over on the side of the road, you’re basically a sitting duck, hoping that other drivers don’t clip you. It’s especially dangerous at night. Ensure that you and those around you are visible when you pull over by using road flares, or at least a reflective triangle. The old school flaming flares seem to be harder to find these days as people switch to the LED variety.\n9. Jumper cables. You walk out to your car after a long day of work, stick the key into the ignition, give it a turn, and…click, click, click. Crap! You then look up and notice you left the dome light on all day. It happens to the best of us. Car batteries die, so be ready with a set of jumper cables. And even if you never suffer a dead battery, it’s always good to have a set of jumper cables so you can help a damsel (or dude) in distress who needs their car jumped.\n10. Tow strap. Get your car unstuck from anything with a tow strap. Attach one end of the strap to the front of the car that you want to pull and the other to the hitch on the back of your car. The stranded driver stays in the dead car, puts it in neutral, and gets freed. Easy as that!\n11. Water. For when you’re stranded in Death Valley in the middle of the hottest heat wave on record…or for any other time your car decides to break down on you. Also for when you’ve been on the trail and are parched because you didn’t pack enough in your hiking pack. Always keep a few bottles handy in the trunk.\n12. First aid kit. Whether you’re cleaning up a head wound filled with glass shards or fixing a boo boo on your two-year-old, it’s good to have a first aid kit. You can always buy one, but putting together your own in an Altoids tin is more fun.\n13. Blankets. Blankets have uses that go beyond emergency situations. It’s always good to have a blanket in the car for snuggling with your gal while you cheer for your team on a cold fall night or for laying it on the ground for a picnic. Get the space-saving (but not very romantic) emergency Mylar variety, or something a little classier like the Paria from Rumpl.\n14. Fire extinguisher. Car fires can be especially dangerous because of the flammable liquids coursing through their systems. Keep an extinguisher in the car that can be used not only for your own emergencies, but for others who might be in danger as well. An auto extinguisher is useful, as it will be rated for putting out car-specific fires that are fueled by gasoline and oil.\n15. Shovel. There are a couple of instances where a folding shovel might come in handy. The first is when you get stuck in the snow or ice. You can use the shovel to dig some snow out and place some dirt under the tire to get more traction. The second situation is when a car tire gets stuck in a hole or something. You can use the shovel to dig about and create some ramps to help get your car unstuck. Also, it can be used as an improvised weapon.\n16. Kitty litter. Kitty litter? For traveling with your cats and they need a potty break? Hardly. Kitty litter is extremely useful as a traction device when you’re stuck in the snow or ice after a skid gone wrong. It’s not usually that you’re buried in snow that keeps your car from moving, but the slickness of the surface you’re trying to move on. Throw a handful of kitty litter in front of the tires, and they’ll have some traction to help get you on the road again.\n17. Multi-wick candles. If you’re stranded in a broken-down car in the winter, you might need more than just a blanket. An actual heat source will come in mighty handy. Have a multi-wick candle (the single wick kind don’t provide adequate warmth) on hand (and matches!); it can keep your car warm for quite awhile. Candles are expensive, so make your own on the cheap (and you save even more money going scentless).\n18. Ice scraper. Don’t be the chump who’s out there scraping their windshield with a credit card at 5AM in the morning. A good ice scraper will set you back just a few bucks from most any convenience store, and it will make clearing your windshield much easier and much faster.\n19. Hat and gloves. Along with a blanket, make sure your head and hands stay toasty warm too. The thicker the better here; you aren’t going for fashion, but survival.\n20. Tire chains. Not only are tire chains handy in wintery mountain passes, they’re actually required in some states. Don’t get stuck in the mountains; don’t get a ticket for not having chains.', ""ABS, ESP, ASR: Do driving aids help on snow?\nElectronic driver assistance systems have become commonplace with car manufacturers. If under normal driving conditions these systems significantly improve the vehicle’s active safety, they may well spring a few surprises on snow. Here are a few tips to ensure their effectiveness in winter.\nOn the same topic\nPreviously reserved for high performance cars, an increasing number of electronic driver assistance systems are now being installed in cars. The most common are the anti-lock braking system (ABS), emergency brake assist (EBA), anti-slip regulation (ASR) or the electronic stability program (ESP). These sophisticated systems, which act on both the engine and brakes, help drivers to control their vehicle in emergency situations.\nIf their usefulness has been proven on wet and dry roads, these systems may deliver a few surprises on snow. But is this a reason to disable them whenever it is possible? Certainly not since, combined with high performance winter tyres, they significantly improve tyre grip and traction.\nThe ABS prevents cars from sliding but increases the braking distance\nExcept for cars that are over 10 years old, the antilock braking system works well on snow. The downside, however, is that braking distances are doubled, or even tripled on certain surfaces. This is an incentive for drivers to reduce their speed and anticipate slowing down.\nHowever, the ABS may react badly if there is a difference in grip between the left and right wheels. For example, if only half of the road is covered with snow. Undesirable reactions can also be noted on extremely bumpy roads.\nESP: To be disabled with the engine running in low grip conditions\nInsofar as possible, the Electronic Stability Program aims to bring the car under control without leaving the road. The system also reduces the risk of under-steering (front wheel skid) and over-steering (rear wheel skid). As soon as the ESP detects that the wheels no longer respond to steering wheel movements, it acts on the relevant wheel(s) until the tyre finds traction again.\nWhen the car is in motion, the ESP reacts quickly on snow. However, when starting and under certain conditions, the anti-slip regulation system can block the car. For example, on a particularly icy road or with hill starts. The latest ESP systems have a position which allows some slip in snow. For older systems, simply disengage the ESP for a few moments and re-engage it once the car is moving.\nWinter tyres to optimise the effect of driver assistance systems\nAcknowledging that electronic driver assistance systems are commonplace, tyre manufacturers now offer winter tyres that improve their effectiveness.\nThis is particularly true of Continental, with its ContiWinterContact range. The TS 830 P model is equipped with specific sinusoidal sipes that tend to combine longitudinal and lateral forces. We were able to check the tyre’s efficiency on the Andros Trophy ice circuit at Alpe d’Huez.\nArticles in the same header\n- It’s time to put your winter tyres away\n- Why two winter tyres are not enough?\n- Summer tyres versus winter tyres: Michelin's snow test\n- Winter tyres for 4x4's and SUV's\n- Winter tyres for sports cars\n- Cooper Weather-Master WSC: Studdable winter tyre for SUV’s\n- Winter tyres mandatory in Luxembourg\n- It's time to switch to winter tyres!\n- Don’t wait until the last minute to buy your winter tyres!\n- Can I use winter tyres in summer?""]"	['<urn:uuid:5a6efc97-fcb4-4800-95d3-6e359166d29f>', '<urn:uuid:1b15f7ca-32c3-4bac-8224-c78053555c54>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T01:37:09.743768	20	68	1943
48	how do plant roots absorb water from soil what factors affect soil absorption capacity	Plant roots absorb water through root hairs that draw water from the soil using a pressure system. When internal pressure in root cells is lower than outside pressure, plants take in water. The roots spread underground like a net and transport water through an external membrane and capillary cells to the stem tissue. As for soil absorption capacity, it depends on several factors: soil porosity (amount of empty space), particle size and shape (larger particles allow better absorption), soil structure, and organic matter content. Additionally, environmental conditions like soil temperature and pH level influence absorption - warmer soils absorb water more quickly than colder ones, while alkaline soils have lower water-holding capacity than acidic soils.	"['Roots: Nature\'s Drillers\nIn order to survive, plants need to carry out photosynthesis, and for that they need the water and minerals they take from the soil. To meet these needs, they require the roots which drill under the ground. The job of the roots is to spread rapidly underground like a net and draw up water and minerals. As well as this, plant roots, despite their delicate structure, enable plants which can weigh up to tons to hold on to and fix themselves in the soil. The soil-gripping nature of roots is most important, because it prevents landslides and the fertile upper layers of soil being washed away by the rain, and other unwanted occurrences that can adversely affect human life.\nRoots need no equipment for all this. They have no engine to provide the power to start the process of water-drawing. Neither is there any equipment to pump the water and minerals to the stem, metres away. But roots can spread over a wide area and draw water. So, how do they do it?\nHow Does This System Work?\nA typical red maple tree growing in a humid climate may lose as much as 200 liters of water per day. This represents a serious loss for the tree. This water needs to be replaced immediately if the plant is to survive. Thanks to the flawless root system plants have, every drop of water which evaporates is replaced.31\nThe roots, which spread down into the depths of the earth, send the water and minerals which the plant needs right up to the leaves, through the stem and branches. The roots\' drawing of water from under the ground closely resembles a drilling technique. The ends of the roots keep looking for water in the depths of the soil until they find it. Water enters the root through an external membrane and capillary cells. It then passes through the cells to the stem tissue. From there it is transported to every part of the plant.\nThis process which the plant carries out so perfectly is, in fact, a very complicated one. So much so that the secret of the system is still not completely known, even in these days of space-age technology. The existence of this sort of ""pressure tank"" system was discovered in trees some 200 years ago. Yet no law has yet been discovered to definitively explain how this movement of water, against gravity, actually comes about. All that scientists have been able to do on this subject is put forward a number of theories about certain mechanisms. Those which have been demonstrated in experiments are thought of as valid to some extent. The outcome of all these scientists\' efforts is the recognition of the perfection of the pressure tank system. Such a technology, packed into a tiny space, is just one of the proofs of the incomparable intelligence of the designer of the system. The water transport system in trees, and everything else in the universe, were created by God.\nThe Pressure System in Plant Roots\nWhen the internal pressure in root cells is lower than the outside pressure, plants take in water from outside. Another way of putting it is that they take water from outside only when they need it. The most important factor establishing this is the amount of pressure produced by the water in the roots. This pressure has to be balanced with that outside. For this to happen, the plant needs to take in water from the outside when the amount of internal pressure falls. When the opposite happens, when the inside pressure is higher than the outside, the plant gives off water from inside itself by means of its leaves to re-establish the balance.\nIf the level of the water in the soil were slightly higher than normal, the plant would continually take in water, because the external pressure was higher, and this would eventually damage it. If it were a little lower, on the other hand, the plant cell could never take in water from the outside because the external pressure would be low. It would even have to give off water to maintain the pressure balance. In either case the plant would dry up and die.\nThis shows to us that plant roots contain a balance-control mechanism to enable them to regulate the level of pressure needed at a precise moment, neither more nor less.\nHow Roots Take in Ions from the Soil\nThe cells in the roots of a plant select particular ions from the soil to use in cell reactions. Plant cells can easily take these ions inside themselves, despite the internal concentration of some ions in the plant being a thousand times greater than that in the soil solution. So, this is a most important process.32\nUnder normal conditions, a transfer of materials will occur from an area with a higher concentration to one with a lower concentration. But as we have seen, just the opposite takes place in the roots\' absorbing ions from the soil. For this reason the process requires quite substantial amounts of energy.\nTwo factors influence the passage of the ions through the cell membrane: the membrane\'s permeability and the concentration of the ions on either side of the membrane.\nLet us examine these two factors by asking some questions about them. What does a plant\'s choosing the required elements from those in the soil actually mean? Let us first take the concept of ""requirements."" A root cell has to know all the elements in the plant, one by one, to meet its requirements. It has to establish which of all the elements it knows are lacking in all parts of the plant and identify them as needs. Let us ask another question. How is an element known? If the soil is not in a pure state, in other words if there are other elements mixed up in it, what has to be done to distinguish one element from all the rest?\nWill it be possible for someone to tell which is which if elements such as iron, calcium, magnesium, and phosphorus are put in front of him all mixed up? How can he tell them apart? If he has received training in the subject, he may be able to identify some of them. It will be impossible for him to identify the rest, however. So how do plants make the distinction? Or rather, how is it possible for a plant to know elements by itself, and to find those ones most useful for it? Is it possible that such a process should have been carried out in the right way every time for millions of years by chance? In order to think about all of these questions-to which the answer is ""Impossible!""-in a more detailed and deeper way, let us examine what kind of selective property roots possess and what happens at the time of selection.\nLet us review our chemical knowledge regarding the elements and minerals which appear in many forms in nature. Where are they found? Which substances go into which groups? What differences are there between them? What experiments or observations are required to understand what each one is? Can the fastest results be arrived at by chemical or physical methods in these experiments? If we just look at things from the physics point of view can we make a proper classification of these substances if they are put on a table in front of us? Can we distinguish minerals by their colour or form?\nWe could go on. And the answer to all of the above questions is more or less the same. Unless someone is an expert in the field, partial or inadequate knowledge left over from school or university will not lead a person to an accurate solution. In order to classify our knowledge of minerals, let us this time take examples from the human body.\nThere is a total of three kilograms of minerals in our bodies. Parts of them are essential for our health, and they are all present in the necessary quantities. For example, if we had no calcium in our bodies, our teeth and bones would lose their hardness. If there were no iron, oxygen could not reach our tissues, because we would have no haemoglobin. If we had no potassium and sodium, our cells would lose their electrical charges and we would rapidly age.\nMinerals are present in the soil in the same way as in the human body. Their quantities, functions, and the forms in which they are found in the soil are all different, and many living things make use of these minerals. In plants, for instance, systems have been set up so that they can easily take the elements they need from the soil. There being different fields of use for them in their structures, all the elements have to go to different parts of the plant after they are absorbed. They all have different tasks.\nIn order to live healthily, a plant needs such basic elements as nitrogen, phosphorus, potassium, calcium, magnesium and sulphur. While plants can take most of these substances directly from the soil, the situation is different with nitrogen. Nitrogen makes up almost 80% of the atmosphere by volume, however, it cannot be obtained or ""fixed"" directly from the atmosphere by green plants. The plants meet their nitrogen need by absorbing from the soil the nitrates processed by the soil bacteria.\nOther elements, too, are necessary for healthy development. But these are needed in quite small quantities. This group includes iron, chlorine, copper, manganese, zinc, molybdenum, and boron.\nFor example, too much nitrogen from the soil leads to brittle growth especially under high temperatures and succulent growth, while too little can lead to yellowing, red and purple patches, reduced lateral bud, and older growth. Phosphorus deficiency causes reduced growth, browning or purpling in foliage in some plants, thin stems, reduced lateral bud breaks, loss of lower leaves and reduced flowering. Phosphorus is a very important element for the growth of young plants and seed production. In short, the existence of these ions and their being taken in from the soil in the required quantities are essential for healthy plant growth.33\nWhat would happen if plants did not possess this ion-selection mechanism? What would happen if plants took in all kinds of minerals, not just the ones they need, or took in too many or too few minerals? There is no doubt that in that event there would be serious disruptions to the perfect balance in the world.\nDo they not look at the sky above them?- How We have made it and adorned it, and there are no flaws in it? And the earth- We have spread it out, and set thereon mountains standing firm, and produced therein every kind of beautiful growth (in pairs)- To be observed and commemorated by every devotee turning (to Allah). (Surah Qaf, 6-8)\n31. Milani, Bradshaw, Biological Science, A molecular Approach, D.C.Heath and Company, Toronto, p.430\n32. Malcolm Wilkins, Plantwatching, New York, Facts on File Publications, 1988, p.119', ""Absorption of water by soil. 6 Factors Affecting Water Absorption in Plants 2022-12-08\nAbsorption of water by soil\nSoil is a vital component of the earth's ecosystem, as it serves as a medium for plant growth and plays a crucial role in the water cycle. One of the key functions of soil is its ability to absorb and retain water, which is essential for the survival of plants. In this essay, we will explore the process of water absorption by soil and the factors that influence this process.\nThe absorption of water by soil is a complex process that involves several physical and chemical factors. The most important factor is the porosity of the soil, which refers to the amount of empty space within the soil. Soils with high porosity are able to absorb more water than soils with low porosity. The size and shape of the soil particles also play a role in water absorption. Soils with larger and more rounded particles tend to have higher porosity and are therefore better at absorbing water.\nThe structure of the soil also plays a role in water absorption. Soils with a well-developed structure, such as those with a high content of organic matter, are able to absorb and retain more water than soils with a poorly developed structure. The presence of soil organic matter increases the water-holding capacity of soil by providing a network of pores and channels that allow water to flow through the soil.\nThe chemical properties of the soil also influence its ability to absorb water. Soils with a high pH, or alkaline soils, tend to have lower water-holding capacity than acidic soils. This is because the high pH of alkaline soils can cause the soil particles to become coated with an impermeable layer of calcium carbonate, which reduces the soil's ability to absorb water.\nIn addition to these physical and chemical factors, the rate of water absorption by soil is also influenced by environmental conditions. The temperature of the soil, the humidity of the air, and the amount of sunlight all play a role in the absorption of water by soil. For example, warmer soils tend to absorb water more quickly than colder soils, and soils exposed to direct sunlight tend to dry out faster than soils in the shade.\nOverall, the absorption of water by soil is a complex process that is influenced by a variety of physical, chemical, and environmental factors. Understanding these factors is important for effectively managing soil and ensuring that it is able to support healthy plant growth. So, proper management of soil can help to improve the water-holding capacity of soil and ensure that plants have a reliable source of water.\nWater Absorption In Soil\nThe burnt horse dung is mixed in the soil because it helps open up the pores in the soil. Giedroytz, it was further developed in works of G. Non-exchange absorption of ammonium and potassium varies over a wide range, depending on the genetic properties of soils, their granulometric and mineralogical composition. Soils with high clay. Because the other endodermal cells have casperian strips in their walls which are impermious to water. Mineral ions cannot be absorbed by osmosis or diffusion because the concentration is low. .\nWhich Soils Absorb the Most Water?\nThe rate of absorption increases due to increase in the transpiration because passive water absorption increases due to transpiration. There are some epiphytic plants, which grow on the substratum like rock and soil, while other plants absorb water by their aerial roots. Water Absorption in Soil Soil and its Formation The definition of soil is something we are all well familiar with. Aerial spray of water on leaves as a means of irrigation has become a favorite practice with vegetable and fruit growers. The low-soluble salts formed as a result of similar chemical reactions precipitate and pass from the soil solution into the soil solid phase. Absorption Of Water in Soil Take the example of sandy soil, which has coarse or larger soil particlesÂ which is known for aeration but not for water retention.\nAbsorption and Movement of Water in Soil\nActive absorption may be of two types: 1. What Is Probiotic Gardening? Physical absorption in the soil can also result from coagulation sticking of colloidal particles. Thanks to this method granular superphosphate at any method of application and on all soils provides a larger increase in crop yields than powdered. Keep in mind: Soil is more than just food for your plants. Along with the chemical absorption retrogradation of phosphate in soils, the reverse process — mobilization of phosphorus: its transfer from inaccessible forms into available ones — is possible. Root hairs develop in the younger part of the maturation zone as finger-like extensions of the epiblema cells Fig.\nWater Absorption: Organ, Mechanism, Factors and Methods\nRole of Root Hairs in Water Absorption A root contains some tubular, hair-like and unicellular structures called Root hairs. For clayey soil, the adsorbed water is in the diffused double-layer form. ADVERTISEMENTS: Let us make in-depth study of the organ, mechanism, factors and methods of water absorption. Cations with the same charge in an alkaline environment do not cause precipitation coagulation of colloids. Many other factors, such as permeability, infiltration rate, percolation, and more, affect the water absorption of soil also. Mineral colloids include crystalline clay minerals of kaolinite and montmorillonite groups, hydromica and amorphous compounds oxide hydrates of the R2O3 type, silicic acid, etc.\nHow microbes help release soil nutrients and reduce the need for ferti\nPercolation studies are important for two reasons- 1 Percolating water is only source of recharge of ground water, which can be again be profitably used through springs and wells for irrigation. Chemical absorption capacity of soils is the ability of the soil to retain ions through the formation of insoluble compounds as a result of chemical reactions, or, the ability of the soil to convert anions and cations of the soil solution into insoluble substances. Some Substances Present in the Soil: Some substances, occurring in the soil, affect the absorption of water on account of their chemical nature quite apart from their osmotic properties. As a result, soil water enters into cortical cells through root hairs. Therefore, clayey and loamy soils are characterized by more mechanical absorption capacity than sandy and sandy loam soils. This type of soil is called a saturated soil.\nWater Absorption by Plants : Plantlet\nHydrogen cation gradually destroys the SAC minerals, deteriorating the soil structure, impoverishing it with colloidal fraction and reducing the CEC, which in general negatively affects the growth, development and crop yields. Mechanism of absorption of water In higher plants, water is absorbed through root hairs which are in contact with soil water and form a root hair zone a little behind the root tips. Soils with high content of highly dispersed particles have high absorption capacity; clayey soils have higher absorption capacity than sandy soils. Teruo Higa, a Japanese Agricultural Scientist. The total water content in the soil is called holard. Peat soil is acidic and inhibits decomposition, which means it contains a high proportion of organic matter. The microbes provide the nutrients that plants need to grow and also help with nutrient absorption, water absorption, root size, and other physical functions of the plant.\nWhat is adsorbed water, and how does it affect the permeability of soil?\nBecause of this anaerobic respiration happens in roots, and alcohol is formed. In exchange absorption, the electrolytes in the soil solution react chemically with the molecules on the surface of the soil particles. The moisture in the soil can be explained by activity, as given below. An increase in the concentration of the dissolved substance in the layer of dispersion medium adjacent to the particles of the solid phase is called positive adsorption. Percolation depends up on: i Climate: If the rainfall is more than evaporation, then there will be appreciable amount of percolation. How affects the permeability : Fine particles of clay are surrounded by films of adsorbed water.\nions are absorbed from the soil by\nThen, such soil would be kneaded after mixing the horse dung. According to studies conducted by radioisotope method using 15N isotope, 10-20% of nitrogen in the form of nitrates and 20-40% in the ammonium form of nitrogen fertilizers are fixed in organic form in the soil. Active absorption is very important only in gradually taking place plants growing in the soil near field capacity. Peat is dark in color, quickly warms up in spring, and retains a high amount of water. The root system merely acts as a physical absorbing system.\nAbsorption of Water In Plants\nThis soil is Physiologically dry and lacking oxygen. This is probably since at low-temperature level: 3. Other minerals of the fine-dispersed fraction have a denser packing, due to which the exchange of cations occurs only on the outer surface of the microcrystals. Cold soil is as physiologically dry. Charge of soil colloids decreases under acid reaction reducing absorption capacity.\nSoil Factors Influencing Water Absorption Rate by Roots\nPathway of water absorption Water absorption in plants through root can occur in two pathways. Water unconsumed by the plant is called echard. Biological uptake is the only way nitrates and chlorides are retained in the soil. Operate in very slowly transpiring plants. Mechanism of Water Absorption : The mechanism of water absorption can be explained by two approaches. Rather a limited increase in capillary water of soil may cause to increase water absorption rate.""]"	['<urn:uuid:3a8dd5e9-4851-4527-942b-dab6bd024111>', '<urn:uuid:0678930f-5c7b-407a-b65e-6c178175be01>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	14	115	3432
49	what origins history ingredients tempura portuguese japan batter	Tempura originated in Portugal and was later introduced to Japan. The traditional tempura batter consists of just three basic ingredients: flour, egg, and ice water. It is prepared by deep frying battered vegetables and seafood, and is commonly served as a main dish, topping for soba, udon, or rice bowls, and as a side dish. Popular varieties include ebi (prawn/shrimp), sakana (fish), nasu (eggplant), kinoko (mushrooms), and satsumaimo (sweet potato).	[': not permanent : transient.\nWhat is the synonym of impermanent?\nfleeting. here-today-gone-tomorrow. impermanent. irresponsible. momentary.\nWhat does tempora mean?\n1 : a process of painting in which an albuminous or colloidal medium (such as egg yolk) is employed as a vehicle instead of oil also : a painting done in tempera.\nWhat does GCHQ stand for?\nGovernment Communications Headquarters. Government Communications Headquarters has a separate website. The Government Communications Headquarters (GCHQ) provides intelligence, protects information and informs relevant UK policy to keep society safe and successful in the internet age.\nIs tempura a Portuguese?\nIt’s one of Japanese food’s most iconic staples, but the truth is, tempura isn’t from Japan at all – well, kind of. Tempura – that classic Japanese dish. … It is most likely that Japan’s iconic dish of vegetables, fish or seafood coated in batter and deep-fried actually originated in Portugal.\nWhich means almost the same as permanent?\nSome common synonyms of permanent are durable, lasting, and stable. While all these words mean “enduring for so long as to seem fixed or established,” permanent adds usually the implication of being designed or planned to stand or continue indefinitely.\nWhat does temporariness mean?\nthe state or quality of lasting only for a short time. the temporariness of the job is probably the reason why so few people are applying for it.\nIs non permanent one word?\nUse the adjective temporary to describe something that is not permanent. … The adjective temporary is used to describe something that isn’t permanent or lasts only a short time.\nWhat does short term mean?\n1 : occurring over or involving a relatively short period of time. 2a : of, relating to, or constituting a financial operation or obligation based on a brief term and especially one of less than a year.\nWhat is transience meaning?\nPeople most often use the noun transience when they’re talking about good things, like beautiful days, a nice life, or a fun vacation, that seem to be over in an instant. The word comes from the Latin transiens, “passing over or away.” Definitions of transience. the attribute of being brief or fleeting.\nWhat does the term vulnerability mean?\nVulnerability is the inability to resist a hazard or to respond when a disaster has occurred. For instance, people who live on plains are more vulnerable to floods than people who live higher up.\nWhat is a permanent fixing?\nPermanent Fix means a fully tested and quality controlled error correction to a Failure in the Program. Sample 2. Sample 3. Permanent Fix means a complete solution restoring entire functionality as provided in Appendix B. Sample 2.\nWhat is the opposite temporary?\nOpposite of acting in a provisional or interim capacity. permanent. eternal. perennial. fixed.\nWhat is permanent solution?\nPermanent Solution. A permanent solution is one that completely resolves the underlying Problem by eliminating its root cause. It prevents any more Incidents from being caused by the said Problem.\nIs temporariness a real word?\nadj. Lasting, used, serving, or enjoyed for a limited time.\nWhat does I’m coping mean?\nintransitive verb. 1a : to deal with and attempt to overcome problems and difficulties —often used with with learning to cope with the demands of her schedule. b : to maintain a contest or combat usually on even terms or with success —used with with.\nIs permanent forever?\npermanent Add to list Share. Something that’s permanent is constant and lasting, as opposed to temporary. Think twice about writing in permanent marker or getting a tattoo — both are just about impossible to erase. If you wait around for something permanent to change, you’ll be there for a long time — maybe forever.\nWhat is opposite of lazy in English?\nAntonym of Lazy\nWord. Antonym. Lazy. Industrious, Energetic, Active, Busy. Get definition and list of more Antonym and Synonym in English Grammar.\nWhat’s the longest word in the dictionary?\nThe longest word in any of the major English language dictionaries is pneumonoultramicroscopicsilicovolcanoconiosis, a word that refers to a lung disease contracted from the inhalation of very fine silica particles, specifically from a volcano; medically, it is the same as silicosis.\nWhat is tempura batter made from?\nWhat Is Tempura Batter Made Of? This simple batter has just three ingredients: flour, egg, and ice water. Tempura mix is available in some specialty stores, but it’s simple enough to use plain ‘ol all-purpose flour instead.\nDoes tempura batter have egg?\nQuite often, tempura batters do not include egg. Some recipes include just the yolk and are crispier batters with a whole egg. Try this recipe with the flour and cold water alone if you prefer to avoid eggs.\nDid the Portuguese invent tempura?\nThe Origin of Tempura, batter frying, in Japan goes back to the mid-16th century. … The concept of batter frying was brought by the Portuguese, along with Catholicism.', 'If travelling to Japan, the following are some of the best Japanese foods and beverages and should be on your must-try list:\n- Sushi – Probably the Japanese food most loved by foreigners, sushi is made by preserving pickled fish in vinegar. It is available in different types, such as gunkan (made of dried seaweed, sushi rice and seafood), nigiri (rice balls topped with shellfish, fish, tuna, octopus and other seafood), and norimaki (rice wrapped in dried seaweeds).\n- Ramen – Introduced to Japan by the Chinese, ramen is a type of noodle dish that is one of the most affordable and commonly available in the country. It is made up of noodles, soup, and toppings (meat, seafood and vegetables). Some of the most popular ramen types are the shoyu (ramen soup with soy sauce), miso (ramen soup with miso or soybean paste), and tonkotsu (ramen soup with pork broth).\n- Japanese Curry – Typically served with steamed rice, Japanese curry, or curry rice, is a very popular dish all over the country. Its ingredients include meat (usually beef, chicken or pork), potatoes, onions, carrots and, of course, curry. It’s easy to eat and not really spicy like an Indian curry.\n- Yakiniku – Yakiniku are prepared by grilling bite-sized pieces of meat (beef, pork, chicken and seafood), vegetables (mushrooms, carrots, onions, eggplants and cabbage) and other ingredients on a grill built into the tables in restaurants.\n- Udon – A type of noodle that is made of wheat flour, udon is one of the most widely available noodle dishes in Japan. It can be prepared either hot or cold. Some of its most popular varieties are the zara udon, tanuki udon, kake udon and tempura udon.\n- Okonomiyaki – A pancake-like dish made by frying batter and cabbage on a pan, okonomiyaki is very common around the country, most especially in Osaka and Hiroshima. It can be stuffed with shrimp, kimchi, pork, octopus and other ingredients, and topped with vegetables, wasabi, cheese and other condiments.\n- Tonkatsu – Prepared by deep frying breadcrumbed pork, tonkatsu is a common Japanese dish served in restaurants throughout Japan. It can be served in various ways, such as in a set meal (with rice, pickles, cabbage and miso soup), curry (with Japanese curry and rice) and katsudon (ricebowl topped with onions and eggs).\n- Tofu – A common sight in many Japanese dishes, tofu is a protein-rich food that consists of curdled soy milk shaped into blocks. One example of a popular tofu dish is hiyayakko, which is fresh tofu with soy sauce, green onions, ginger and bonito flakes as garnish. Tofu is also frequently used as ingredient in making soups, hot pots and noodle dishes.\n- Soba – With noodles made from buckwheat, soba is a staple Japanese food that can be served either hot or cold, with meat, seafood and vegetables, and soy sauce or other dipping sauces. The most stripped down type of cold soba is called mori, which consists of only noodles and dipping sauce made of mirin, water, and soup stock.\n- Yakitori – An inexpensive Japanese food that is frequently enjoyed with beer, yakitori is made of bite-sized chicken pieces, skewered, and grilled over charcoal. Among the most popular types include negima (chicken thigh), torikawa (chicken skin), and reba (chicken liver).\n- Tempura – Made by deep frying battered vegetables and seafood, tempura is a traditional Japanese dish that is widely available around Japan, commonly prepared as a main dish, a topping for soba, udon, or rice bowls, and as a side dish. The most famous varieties are ebi (prawn/shrimp), sakana (fish), nasu (eggplant), kinoko (mushrooms), and satsumaimo (sweet potato).\n- Shabu shabu – Shabu shabu is a dish made of thin slices of pork or beef boiled in water, mixed with vegetables, spices, and other ingredients. It is frequently served with a bowl of steamed rice or noodles and is dipped in sauces, particularly goma or ponzu.\n- Tea – A beverage that has played a significant role in Japanese history and culture, tea is Japan’s most popular drink. It is believed to have been introduced by the Chinese in the 8th It is available in different types, such as green tea, roasted green tea, black tea, jasmine tea and oolongcha.\n- Sake – A perfect companion for any Japanese dish, sake is an alcoholic beverage made from fermenting rice. It has been a staple drink in Japan for several centuries now and is available in many restaurants and supermarkets all over the country.\n- Beer – As Japan’s most commonly drunk alcoholic beverage, beer is pretty much available everywhere — convenience stores, supermarkets, restaurants and vending machines. The country is home to four major beer brewing companies: Sapporo, Kirin, Suntory, and Asahi.\n- Japanese Plum Wine – An alcoholic beverage known as umeshu, it is made from the ume plum that is still unripe and green, sugar and shochu (Japanese distilled alcohol). It takes about a year to make. It is a sweet drink and best served on the rocks. The most popular umeshu brand is Choya.']	['<urn:uuid:67e84ed9-68b0-4ab9-9251-46631393de0c>', '<urn:uuid:f229d75f-5b60-4eff-b917-c687ad2546d2>']	factoid	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-13T01:37:09.743768	8	70	1661
50	Can purple and white flowers make lavender ones?	Yes, this is an example of incomplete dominance, where the offspring shows a blend of both parent traits rather than one trait being completely dominant. When purple and white flowers cross, they can produce lavender flowers that show an intermediate color. This differs from simple dominance, where one trait would completely mask the other. If a lavender flower is then crossed with a white flower, it will not produce purple flowers but rather light lavender ones.	"[""1. List whether the student was positive or negative for each characteristic and include whether the characteristic is dominant or recessive. (6 points) a. Blood type\nb. Widow's peak\nc. Free ear lobes\nd. Tongue rolling\ne. Hitchhiker's thumb\nf. Left thumb dominance\ng. Little finger bend\nh. PTC taster\ni. Mid-digit hair\nj. Facial dimples\nl. Cleft chin\n2. Can the student tell from the blood type if the student is heterozygous or homozygous? Explain. (5 points) No. Since I am a type A my parents are either both type A or one is 0 so I could be either. 3. Select a trait of interest.\na. What is the trait? (1 point)\nb. What is the phenotype for the trait? Is this the dominant or recessive allele for the trait? (2 points) A widows peak is dominant, (B)\nc. What are the possible genotypes for the parents? (2 points) The possible genotypes would be BB or Bb\nd. Include the results of one Punnett square, showing a possible combination between alleles for the trait from the parents by filling in the genotypes according to the numbers in the square, below. B, b\nB, BB, Bb\nB, BB, Bb\ne. Based on the cross, what percent of children born to the parents would express the trait? (1 point) 100 %\n4. Create a Punnett square to determine the possibility of a couple having a color-blind child if the mother has the recessive trait on one X and the father is color-blind. HINT: Use Xb to indicate an X with the color-blindness trait. How many female offspring will be color-blind? How many male offspring? (5 points) Mom: XbX\nChildren: XbXb, XbX, XbY, XY\nMost likely there would be one color blind male and one color blind female.\n5. What was the group number of the student's karyotype? What was the result of the student's karyotype? (Include the sex and the chromosomal disorder, if applicable. If there was no chromosomal disorder, the student must state that the individual was normal.) (5 points) What was the group number 6. Give an example of a situation in which it is important to create a karyotype for an individual. Explain. (5 points) It would be important to know with recessive chromosomal disorders that could be passed down to offspring. 7. Genetically speaking, why is it important not to mate with a close relative? Explain. (5 points) Inbreeding can cause homozygosity which increased the chances of a recessive genetic disorder occurring in offspring. 8. Does a karyotype tell all of a person's genetic characteristics? Explain. (5 points) No but it could display information about the amount of Chromosomes which could help determine genetic diseases. 9. Why is a photograph of cells in metaphase utilized when constructing a karyotype? (5 points) Metaphase shows traits that are beginning to form.\n10. What does it mean to be a carrier of a genetic defective characteristic? When might it be important to know if one is a carrier? (5 points) A carrier is one that has the recessive gene for a disease but they themselves do not exhibit symptoms. It is important to know for future offspring. 11. From the hemophilia procedure: (4 points)\na. What were the possible genotypes of the offspring?\nXHXH, XHXh, Xhy,XHY\nb. What is the probability of males having hemophilia?\nc. How many females would have hemophilia?\nd. How many carriers would there be?\n12. Explain why more males tend to suffer from X-linked disorders than females. (5 points) Women have 2 X chromosomes and Males have only one.\n13. The student has a friend that knows the student is taking biology, and she is confused about her blood type. Her blood type is O, but her dad is A and her mother is B. She asks the student if it is possible for her parents to have a child that is O. Explain the answer to her. (5 points) Yes she can have a child that has an O blood type. Her parents could have been AO and BO which could pass the O blood type to her. 14. In a flower garden, the gardener has purple and white pansies. He notices that a new pansy has sprouted. When it finally flowers, the pansy is lavender. Explain how this happened. (5 points) This is an example of incomplete dominance causing a blend of colors. 15. With a botanist friend's help, the gardener decides to cross the lavender pansy with the white pansy. Will this result in any purple pansies? Explain. (5 points) No it will most likely cause light lavender pansies.\n16. (Application) How might the information gained from this lab pertaining to human genetics be useful to a student enrolled in a healthcare related profession? (20 points) The lab on human genetics would be particularly useful to a geneticist or a person working in a fertility clinic. The information gained on carriers was particularly interesting. With proper genetic testing we can alert these potential parents of their recessive carrying of diseases such as Spinal Muscular Atrophy and Cystic fibrosis. This genetic information could help these potential parents know the risks of their child having these disorders and could help persuade them into other routes such as adoption or to be prepared to deal with these potential disordwers."", 'Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nBiological: Behavioural genetics · Evolutionary psychology · Neuroanatomy · Neurochemistry · Neuroendocrinology · Neuroscience · Psychoneuroimmunology · Physiological Psychology · Psychopharmacology (Index, Outline)\n- For other non-genetic uses of the term ""dominance"", see Dominance.\nIn genetics, dominance relationships control whether an offspring will inherit a characteristic from the father, the mother, or some blend of both. More technically, they control the ways genes interact to express themselves as phenotypes in a diploid or polyploid individual.\nThere are three kinds of dominance relationships:\n- Simple dominance\n- Incomplete dominance\nTraits inherited in a dominant-recessive pattern are often said to ""follow Mendelian inheritance"".\nThe dominant/recessive relationship is made possible by the fact that most higher organisms are diploid: that is, most of their cells have two copies of each chromosome -- one copy from each parent. Polyploid organisms have more than two copies of each chromosome, and follow similar rules of dominance, but for simplicity will not be discussed here.\nHumans, a diploid species, typically have 23 pairs of chromosomes, for a total of 46. In regular reproduction, half come from the mother, and half come from the father (see meiosis for further discussion of how this happens, and chromosome for less usual possibilities in humans or in cows).\nRelationship to other genetics conceptsEdit\nAlthough humans have only 46 chromosomes, it is estimated that those 46 contain 20,000-25,000 genes, each of which is related to some biological trait of the organism. Many genes are strung together in a single chromosome. The other chromosome of the pair will have genes for the same functions -- for example, to control height, eye colour, and hair colour.\nHowever, since one chromosome came from each parent, it is quite unlikely that the genes will be identical. The specific variations possible for a single gene are called alleles: for a single eye-colour gene, there may be a blue eye allele, a brown eye allele, a green eye allele, etc. Consequently, a child may inherit a blue eye allele from their mother and a brown eye allele from their father. The dominance relationships between the alleles control which traits are and are not expressed.\nConsider the simple example of the dominant brown eye allele and the recessive blue eye allele. In a given individual, the two corresponding alleles of the chromosome pair fall into one of three patterns:\n- both blue\n- both brown\n- one brown and one blue\nIf the two alleles are the same (homozygous), the trait they represent will be expressed. But if the individual carries one of each allele (heterozygous), only the dominant one will be expressed. The recessive allele will simply be suppressed.\nLatent recessive traits appearing in later generationsEdit\nIt is important to note that an individual showing the dominant trait may have children who display the recessive trait. If a brown-eyed parent is homozygous, they will always pass on the dominant trait, and therefore their children will always have brown eyes, regardless of the contribution of the other parent. However, if that brown-eyed parent is heterozygous (and they typically would have no way of knowing), they will have a 50/50 chance of passing on the suppressed blue-eyed trait to their offspring.\nIt is therefore quite possible for two parents with brown eyes to have a blue-eyed child. In that situation, we can conclude that both parents were heterozygous (carrying the recessive allele).\nHowever, unless there is a spontaneous genetic mutation, it is not possible for two parents with blue eyes to have a brown eyed child. Since blue eyes are recessive, both parents must have only blue-eyed alleles to pass on.\nMain article: Punnett square\nThe genetic combinations possible with simple dominance can be expressed by a diagram called a Punnett square. One parent\'s alleles are listed across the top and the other parent\'s alleles are listed down the left side. The interior squares represent possible offspring, in the ratio of their statistical probability. In this example, B represents the dominant brown-eye gene and b the recessive blue-eye gene. If both parents are brown-eyed and heterozygous, it would look like this:\n|B||B B||B b|\n|b||b B||b b|\nIn the BB and Bb cases, the child has brown eyes due to the dominant B. Only in the bb case does the recessive blue-eye trait express itself in the blue-eye phenotype. In this fictional case, the couple\'s children are three times as likely to have brown eyes as blue.\nTraits governed by simple dominanceEdit\n(not an exhaustive list)\n|Curled Up Nose||Roman Nose|\n|Clockwise Hair Whorl||Counter-clockwise Hair Whorl|\n|Can Roll Tongue||Can\'t Roll Tongue|\n|Widow\'s Peak||No Widow\'s Peak|\n|Facial Dimples||No Facial Dimples|\n|Able to taste PTC||Unable to taste PTC|\n|Earlobe hangs||Earlobe attaches at base|\n|Middigital hair (fingers)||No middigital hair|\n|No hitchhiker\'s thumb||Hitchhiker\'s thumb|\n|Tip of pinkie bends in||Pinkie straight|\nSome genetic diseases carried by dominant and recessive allelesEdit\n- Main article: Genetic disorder\n|Some types of Dwarfism||recessive|\nAs can be seen from this, dominant alleles are not necessarily more common or more desirable.\nIn incomplete dominance (sometimes called partial dominance), a heterozygous genotype creates an intermediate phenotype. In this case, both the dominant and recessive gene are expressed, creating a blended or combined phenotype. A cross of two intermediate phenotypes can result in the reappearance of either the parent phenotypes or the blended phenotypes.\nThe classic example of this is the colours of carnations.\nR is the gene for red pigment. R\' is the gene for no pigment.\nThus, RR offspring make a lot of red pigment and appear red. R\'R\' offspring make no red pigment and appear white. RR\' and R\'R offspring make a little bit of red pigment and therefore appear pink.\nAn example of incomplete dominance in humans is mordan, a trait that is exhibited when eye color alleles from the maternal and paternal chromosomes are blended. This usually occurs when one parent has green eyes and the other parent has brown eyes–the child will have dark blue eyes.\nIn co-dominance, neither phenotype is dominant. Instead, the individual expresses BOTH phenotypes. The most important example is in Landsteiner blood types. The gene for blood types has three alleles: A, B, and i. i causes O type and is recessive to both A and B. When a person has both A and B, they have type AB blood.\nAnother example involves cattle. If a homozygous bull and homozygous cow mate (one being red and the other white), then the calves produced will be roan-colored, with a mix of red and white hairs.\nExample Punnett square for a father with A and i, and a mother with B and i:\nAmongst the very few co-dominant genetic diseases in humans, one relatively common one is A1AD, in which the genotypes Pi00, PiZ0, PiZZ, and PiSZ all have their more-or-less characteristic clinical representations.\nMost molecular markers are considered to be co-dominant.\nIt is important to note that most genetic traits are not simply controlled by a single set of alleles. Often many alleles, each with their own dominance relationships, contribute in varying ways to complex traits.\nThe development of phenotype\n|Key concepts: Genotype-phenotype distinction | Norms of reaction | Gene-environment interaction | Heritability | Quantitative genetics|\n|Genetic architecture: Dominance relationship | Epistasis | Polygenic inheritance | Pleiotropy | Plasticity | Canalisation | Fitness landscape|\n|Non-genetic influences: Epigenetic inheritance | Epigenetics | Maternal effect | dual inheritance theory|\n|Developmental architecture: Segmentation | Modularity|\n|Evolution of genetic systems: Evolvability | Mutational robustness | Evolution of sex|\n|Influential figures: C. H. Waddington | Richard Lewontin|\n|Debates: Nature versus nurture|\n|List of evolutionary biology topics|\nReferences & BibliographyEdit\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|']"	['<urn:uuid:807385fd-b2ad-43fa-bd09-f32d6de6487d>', '<urn:uuid:bf147c71-1485-4868-a06d-294a8a1b5ba6>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T01:37:09.743768	8	76	2174
51	I've heard Scotland has good conditions for wind energy along its coast. What makes their coastal areas so well-suited for wind power development?	Scotland has strong offshore winds that provide ideal conditions for wind energy technology. The east coast is particularly suitable for offshore wind development due to its gently shelving seabed. Additionally, with the emergence of deeper water offshore technology, both the east and west waters of the country can be sustainably developed.	['Offshore wind energy\nScotland has a great deal of potential offshore wind resources. Its strong offshore winds provide the ideal conditions for technology which can harness this powerful resource. The east coast seabed has been identified as a particularly suitable location for the development of offshore wind due to the gently shelving nature of the seabed. However, there has been an emergence of deeper water offshore technology which can help secure the sustainable development of deeper waters both in the east and west of the country.\nScotland is home to the world’s first floating wind farm (Hywind) around 30KM from Peterhead and the Beatrice project, in the Moray firth recently became the largest operational offshore wind project in Scotland. Scotland has in excess of 4.5 GW of offshore wind projects consented on the east coast and a new leasing round for offshore wind was announced by Crown Estate Scotland in 2017. The planning process for that round is ongoing and leasing is expected to begin in 2020.\nDraft Sectoral Marine Plan for Offshore Wind Energy\nIn November 2017 Crown Estate Scotland announced its intention to run a further leasing round for commercial-scale offshore wind energy. In accordance with the Scotland’s National Marine and in our capacity as planning authority for Scotland’s seas, the spatial locations for this and any future leasing round will be identified through a sectoral marine planning process, completed by Marine Scotland.\nConsultation on the initial scoping work to identify Areas of Search and the statutory assessment criteria was held between June and July 2018. Responses from that consultation have helped develop the planning process and a draft Sectoral Marine Plan for offshore wind energy was consulted upon from 18 December 2019 until 25 March 2020.\nThe draft Sectoral Marine Plan for Offshore Wind Energy aims to identify the most sustainable options for the future development of commercial-scale offshore wind energy in Scotland and is supported by detailed assessments of the potential impact of the draft Plan. This includes a Strategic Environmental Assessment, Habitats Regulations Appraisal, Social and Economic Impact Assessment, Regional locational Guidance and several smaller assessments. For more information, the draft Plan and assessments are available online and the full consultation can be found online at Citizen Space\nThe draft Plan is expected to be adopted later in 2020 and will allow the Leasing process, managed by Crown Estate Scotland to close shortly following formal adoption of the final Plan.\nPrevious Offshore Wind Initiatives\nBlue Seas - Green Energy: A Sectoral Marine Plan for Offshore Wind Energy in Scottish Territorial Waters\nOur vision for developing offshore wind energy up to and beyond 2020 was launched in the two-part publication Blue Seas - Green Energy: A Sectoral Marine Plan for Offshore Wind Energy in Scottish Territorial Waters.\n- Blue Seas - Green Energy: Part B - Post Adoption Statement\n- Economic Assessment of Short Term Options for Offshore Wind Energy in Scottish Territorial Waters\nThis publication was developed following a successful consultation and informed by a substantial evidence base of possible social and environmental interactions with offshore wind development.\nThree projects delivered from Blue Seas – Green Energy have now progressed through the consenting process. Inch Cape and Neart na Gaoithe are both located in the Forth and Tay region whilst Beatrice, in the Moray Firth, was the first of these projects to be built (2019).\nTwo other projects, delivered through the UK SEA 2 process have also progressed through the consenting process. Moray East and West are located in the Moray Firth while SeaGreen is located farther offshore in the Forth and Tay Region.\nDraft Sectoral Marine Plan for Wind (2013)\nIn 2013, a Draft Sectoral Plan for progressing ten medium-term option areas was developed and published for consultation. In December 2014, Scottish Ministers decided not to progress two of the option areas located in South West Scotland and these were removed from the draft Plan. This Plan remains to be finalised. This was due to market uncertainty created by Electricity Market Reform at the time.\nThe 2013 draft Plan Options are included in the National Marine Plan and more information on that sectoral planning process is available alongside the supporting environmental assessments and consultation analysis.']	['<urn:uuid:beb7dfd6-940a-4844-addc-dd3406fb2004>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	23	51	701
52	How does international trade policy impact both agricultural markets and consumer choices, and what are the economic principles behind these effects?	Under GATT, agricultural trade was weakly regulated, allowing many restrictive measures like quotas and subsidies, particularly by the US and European countries. The WTO Agreement on Agriculture improved this by requiring 'tariffication' of non-tariff barriers and scheduled reductions of tariffs. On the economic principles side, when imports are restricted through trade barriers, consumers face higher prices and have fewer choices in buying goods and services. This occurs because trade barriers interfere with voluntary exchange between parties and prevent resources from being redistributed from those who value them less to those who value them more, ultimately reducing the overall creation of wealth that comes from specialization and comparative advantage.	['Critically assess whether creative activity of the WTO introduced any important betterments from GATT in modulating international trade. Support your statements with mention to determinations of the Dispute Resolution Body.\nThe World Trade Organisation ( WTO ) and its predecessor ( The General Agreement on Tariffs and Trade ( GATT ) 1947 ) both trade with the international regulations of trade between states through negotiated consensus.[ 1 ]However, the WTO Agreements have been developed with a batch of considerable betterments from the GATT 1947 ( hereinafter referred as “ GATT ” ) in modulating planetary trade. They include a wider coverage with stricter and more just commissariats toward trade liberalisation ( particularly cut downing non-tariff barriers ) and a important addition of enforcement power by commiting the system and beef uping difference colony processs. In this essay, I will analyse outstanding characteristics doing WTO Agreements “ lawfully distinguishable ” from the GATT.\nly, the GATT was a set of regulations. In the universe history, the GATT is the “ instigator ” of a criterion for planetary commercial policies taking at cut downing barriers and favoritism on trade. 2 The biggest achievement of the GATT is its look up toing success in duty decrease 3 which was widely viewed as a great part to the liberalisation of international trade and the speedy recovery of the post-world war II economic system. But it was argued that the GATT had many commissariats allowing the legal usage of quantitative limitations, particularly in agricultural or piscaries trade ( Article XI ( 2c ) ) , for many justifiable grounds such as precautions to balance-of-payments ( Article XII ) , aid to economic development ( Article XVIII ) , or deciding certain troubles. 4 To cover with the affairs, the WTO Agreements clarifies that for such sensible grounds, members can enforce limitations but “ should make so in the least trade-disruptive mode and should prefer price-based steps, like import surcharges and import sedimentations, instead than quantitative limitations ” . 5\nOn the other manus, in conformity with the committednesss of duty cuts and general riddance of quotas in the GATT, states applied the new policy instruments such as antidumping steps, subsidies, and voluntary restraint understandings in big countries covered by the GATT Article VI, XVI, and XIX, severally.[ 6 ]One of the noteworthy illustrations is the US ‘s application of voluntary export restraint understanding to curtail import volumes of cars from Japan in the 1980s in order to protect its domestic car industries. With permission for such protectionist policies, the GATT revealed its failing in guaranting the efficiency belongingss of reciprocality and nondiscrimination.[ 7 ]To beef up the weak, the WTO has the Agreement on Safeguards to censor any voluntary export restraints, orderly marketing agreements or any other similar limitations on export or import. In add-on, the Agreement on Implementation of Article VI ( Anti-dumping ) provides greater inside informations about the standards of finding a dumped merchandise and the causal hurt to a domestic industry ; and processs to be followed in probes and execution. Meanwhile, the Agreement on Subsidies and Countervailing Measures ( ASCM ) introduces a clear definition of subsidy and the classs of subsidies “ permitted ” or “ prohibited ” .[ 8 ]With greater lucidity, the WTO Agreements well increase the transparence and predictability of the commercial behaviours and aid to efficaciously work out trade differences between states. The Australia-Automotive Leather II instance in 1998 is one clear illustration. The United States ( the US ) complained about the Australian authorities ‘s subsidies for its automotive leather production industry in signifier of a “ grant contract ” ( $ A 30 million ) and a “ loan contract ” ( $ A 25 million ) . Thankss for the elaborate ordinances of ASCM, the Panel could easy happen out that the “ grant contract ” was an export subsidy which was “ prohibited ” under ASCM Art. 3.1 ( a ) , but the “ loan contract ” was non. Additionally, ACSM Art.4.7 gives execution recommendation in inside informations ; and in this instance, Australia was recommended to retreat the forbidden subsidies within 90 yearss after the day of the month on which the DSB adopted the panel study.[ 9 ]\nFurthermore, the GATT regulations were criticized to be non sufficiently effectual in the countries of agribusiness and fabrics.[ 10 ]The Article XI ( 2c ) and some other articles allowed many limitation steps such as quotas, licences, subsidies, etc. on the import of agricultural and piscaries merchandises. Many states such as the US, the European states exercised their rights to protect their domestic agribusiness. The GATT Rules on Agriculture were considered much weaker than those on other goods.[ 11 ]Therefore, The WTO Agreement on Agriculture ( AA ) was formed with an purpose to offer “ a model for the long-run reform of agricultural trade and domestic policies ” .[ 12 ]It aims at raising market orientation and heightening predictability and stableness of agricultural trade through cut downing protectionism and liberalising trade in agribusiness. Among them, noteworthy is the “ tariffication ” of non-tariff barriers in agricultural trade. The tarriffication-resulting duties, together with other duties on agricultural merchandises so have a agenda to be reduced. In pattern, such commissariats have well decreased the trade barriers in agribusiness. Many differences have been resolved in the spirit of the regulations. One case is the difference between India and the US refering India ‘s Quantitative Restrictions on Imports of Agricultural, Textile and Industrial Products in 1997. India claimed that its import limitations aimed at protecting its balance-of-payments ( BoP ) state of affairs, therefore would hold been legal under GATT Art. Eighteen. However, the Panel found that India ‘s quantitative limitations were non allowed under the GATT Art.XI:1 ; and the ground of BoP intent was indefensible because India ‘s pecuniary militias were equal, therefore India breached Art.XVIII:11. In add-on, India ‘s steps violated AA Art.4.2 because they were non allowed under the footer 1 to Art.4.2.[ 13 ]\nIn respects to merchandise in fabrics, the GATT was viewed as slack subjects ( Moore, 1996 ) . The exclusions to Article XI and specifically the precaution commissariats of Article XIX induced much of the trade to be affected by quantitative limitations.[ 14 ]The formation of the Multi-Fiber Arrangement ( MFA ) of 1973 had addressed many jobs at that clip by presenting a regulation of stricter precaution trials when enforcing quantitative limitations on fabrics from GATT spouses.[ 15 ]However, many bilateral quotas were negotiated under the MFA. Therefore, the WTO Agreement on Textiles and Clothing replaces the MFA with a agenda of “ integrating ” into the GATT merchandises. “ Integration ” means that trade in fabrics and vesture merchandises shall be governed in line with the general regulations on other signifiers of goods.[ 16 ]\nUnder the GATT, developing states were non required to reciprocate to the full the grants of other undertaking parties. They were permitted to mostly stand outside the regulations.[ 17 ]Acknowledging that, the WTO Agreements actively take developing states into history but besides give them more precedences than developed states. Notably, the least-developed states ( LDCs ) are exempted from tonss of the duties. For illustration, refering the agenda of “ tariffication ” and barrier decreases in agricultural trade, developing states are required to cut down a smaller grade of duties ( 24 % ) than developed states ( 36 % on norm ) . Meanwhile, LDCs are non requested to take down their duties.[ 18 ]\nTruly, the coverage of WTO has been widely extended with a series of annexed extra understandings and legal instruments regulating non merely merchandise in goods but besides in services and rational belongings rights. The GATT 1947 merely dealt with the trade in goods and was revised to go an built-in portion of the WTO Agreements. The WTO has non triggered a extremist alteration in the GATT ‘s primary aims. It still maintains, even greatly strengthens, the two “ pillars ” of the GATT attack, viz. the rules of reciprocality and nondiscrimination.[ 19 ]In title, the WTO Agreements work much better to steer authoritiess toward efficient many-sided trade understandings. Importantly, the formation of General Agreement on Trade in Services ( GATS ) and Agreement on Trade-Related Aspects of Intellectual Property Rights ( TRIPS ) is a enormous success of the Uruguay Round. Nowadays, the international trades in services such as fiscal services, touristry, telecommunications, etc. have increased quickly and their demands to be governed by jurisprudence have become truly serious. Furthermore, in this epoch, most of profitable industries are to a great extent dependent on high engineerings, therefore the rational belongings rights besides need to be efficaciously protected by a powerful model. A big figure of differences refering trade in services and rational belongings rights were adequately resolved under GATS and TRIPS. Among them is US-Gambling difference between the US ( suspect ) and Antigua and Barbuda ( plaintiffs ) in 2003. The US was found to go against GATS Art.XVI:1 and 2 ( market entree committedness ) “ by forbiding the cross-border supply of chancing and wagering services ” .[ 20 ]\nIn nature, the GATT operated with a probationary foundation. Therefore, to its catching parties, the GATT was merely a many-sided instrument with a disconnected attack because of a deficiency of an institutional mechanism prefering rank committednesss toward the conformity of the regulations. With purpose to make fulling in the spread, the WTO was created with its members ‘ consent as a individual project, or a lasting establishment. Therefore, its covered understandings require committednesss for the full rank of the organisation.[ 21 ]In add-on, the regulations are reinforced by a powerful difference colony system ( hereinafter “ DSS ” ) which solves trade differences much more efficaciously than the GATT.\nIn comparing with the GATT 1947, the cardinal political nature of the WTO difference colony procedure is indistinguishable. It besides radically facilitates decision-makings by many-sided consensus.[ 22 ]However, it is widely agreed that the “ judicialization ” of WTO difference colony procedures has greatly strengthened the enforcement power of the international trade jurisprudence.[ 23 ]\nFirst, we all know that the GATT difference colony based wholly on the Article XXIII.[ 24 ]With this, the GATT did non supply a specified time-frame for difference colony. On contrary, the new WTO Understanding on Rules and Procedures Regulating the Settlement of Disputes ( hereinafter “ DSU ” )[ 25 ]offers a specific clip bound for the procedure as in the undermentioned tabular array:\nConsultations, mediation, etc\nPanels set up and panellists appointed\nFinal panel study to parties\nFinal panel study to WTO members\nDispute Settlement Body[ 26 ]adopts study ( if no entreaty )\nEntire = 1 twelvemonth\n( without entreaty )\nThe DSB adopts entreaties study\nEntire = 1y 3m\n( with entreaty )\nA Beginning: hypertext transfer protocol: //www.wto.org/english/thewto_e/whatis_e/tif_e/disp1_e.htm\nSuch a rigorous timeline requires the DSB, panels, the concerned parties and perchance the Appellate Body[ 27 ]to work more efficaciously and responsibly, avoiding the state of affairs under the old GATT government when there was no ordinance of clip bound, many differences had dragged on for old ages. For illustration, the beef endocrine difference between the US and the EU had been pending for about 15 old ages ( 1981-1996 ) without governing by GATT. The hold in difference colony was the ground for many one-sided relatiative actions of the US during this period, taking to more serious struggles in the commercial dealingss between the EU and the US. Merely 3 old ages after WTO constitution[ 28 ], in 1997, the DSB made its early determinations governing against the EU, assisting to compromise the trade relationship.[ 29 ]Expressively, a timely determination could efficaciously ease international trade tensenesss and forestall them turning into a trade war or even a political war.[ 30 ]\nOn the other manus, the GATT Article XXIII described dispute colony as a procedure of decision-making, instead than a traditional judicial impression of adjudication of rights. If consensus could non be achieved due to disagreement from one undertaking party, the difference might be irresolvable. Concretely, the old GATT system “ allowed ” any undertaking party to “ barricade ” the creative activity of a panel or the acceptance of the panel study by non “ holding ” them. For case, during the 1980s, the EU had many times blocked the formation of a panel or refused the acceptance of the panel study without punishment. Apparently, such “ blockings ” significantly eroded the religion in an international opinion system when a undertaking party was entitled to thwart another ‘s attempts to justify the latter ‘s rights.[ 31 ]\nFurthermore, under GATT, if a losing suspect wanted to blackball an inauspicious judgement, there was no legal footing easing its entreaty. In add-on, there was no ordinance on difference panel ‘s maps, and therefore the panels were non obliged to supply their appraisals if they merely held that the issues were beyond their appreciation to cover with. Examples are the differences sing the EU ‘s export subsidies for wheat flour and sugar challenged by Australia ( 1958 ) , Brazil ( late seventiess ) , and the US ( 1983 ) . The panel merely concluded that they did non cognize how to use the Article XVI of the GATT into the instances and provided no greater findings on the topic.[ 32 ]\nFrom the above, the GATT obviously revealed the lacks in implementing the duties and justifying the rights of its signers. By overly stressing consensus and diplomatic manner in covering with differences, the GATT appeared to prefer the aim of duty grants more than the enforcement of long-run trading regulations. For this ground, many perceivers viewed the GATT as a “ tribunal without a bailiff ”[ 33 ]or a “ clout that will non hit anyone ” .[ 34 ]\nIn contrast, the WTO ‘s DSU truly brings about a important betterment in all of the above-named facets. Under the DSU, the DSS is “ judicialized ” by centralising the power to the DSB ( one of three signifiers of General Council ) . The power centralisation makes the DSS institutionalized, therefore more “ judicial ” . Furthermore, the DSU “ includes greater “ automaticity ” in determinations on the constitution, footings of mention and composing of panels ” and on “ the acceptance of the panels ‘ and a new Appellate Body ‘s findings ” .[ 35 ]A WTO member can non “ block ” the creative activity of a panel or the acceptance of its study because “ the determinations are no longer dependent upon the consent of the parties to a difference ” , but upon the consensus of the DSB.[ 36 ]Furthermore, the maps of difference colony panels are clarified at Article 11 of the DSU[ 37 ], beef uping the rights and the duties of a formal panel and animating more early colony and more duty at the panel phase. In add-on, the constitution of the lasting Appellate Body whose members are independent of any member authoritiess gives a opportunity for a losing suspect to appeal with a position to a more favourable or just finding of fact. Finally, the DSU has a really of import characteristic to reenforce the conformity of the regulations, viz. the possibility of authorising a trade countenance ( including suspension of grants or other duties ) in the event of non-implementation.[ 38 ]WTO critics could state that “ the GATT was back ”[ 39 ]but they should advise that the new regulations strengthen their enforcement by a specified timeline and the possibility of arbitration as a concluding determination.[ 40 ]This characteristic is still controversial. Many position it as “ seting dentition in the WTO ” and one of the important accomplishments of the Uruguay Round. On the other manus, many others see it as “ authorising strong-arming tactics ”[ 41 ]and “ the most serious job with the WTO processs ” .[ 42 ]In fact, trade countenances have non frequently happened.[ 43 ]However, the menace of a countenance has expeditiously kept member provinces in conformance with the understandings. Bernhard Zangl ( 2008 ) reported that the “ judicialization ” of WTO difference colony processs is really effectual in altering behaviour of member provinces toward conformity of the understandings.[ 44 ]\nIn drumhead, the WTO Agreements in general and the DSU in peculiar have made important advancements over its GATT predecessor. The WTO has much more sufficient and effectual subjects to forestall abuse and fill in spreads of GATT, reenforcing its “ two pillars ” of reciprocality and nondiscrimination in international trade. Furthermore, in order to implement member provinces to play by the regulations, the WTO has sagely built a constructive and just system for difference colony, promoting states to work out their differences in a just and peaceable manner.', 'To develop a full understanding of trade, one must first divorce politicians’ and the media’s descriptions of trade from economists’ understanding, especially as it relates to the ‘benefits of trade.’ Politicians traditionally say they favor trade, but only as long as their constituencies are not adversely affected. Economists favor voluntary, or free trade, without that political caveat, because it leads to resources being used in their most highly valued ways and thereby to general, widespread increases in standards of living.\nMany students see the win-win model of voluntary exchange as abstract, a textbook construct and simulation artificiality that doesn’t fit the real world where trade affects jobs, the environment, and relationships between nations. The economic consensus on the importance of voluntary trade is an extension of their recognition of the benefits of voluntary exchange among individuals and businesses. Teachers can best build students’ understanding of how trade creates wealth by taking the time to establish a firm grounding in the key economic reasoning tools – specialization, division of labor, productivity, and comparative advantage – and by applying them to trade within a nation before leaping into trade among nations.\nThe basics, as always, come back to scarcity and opportunity cost. Natural and human resources are not equally distributed throughout the world, or even, indeed, throughout a nation. One of the most important functions of trade is to redistribute resources – from those who value them less to those who value them more. Improvements in technology and transportation have heightened the power of trade to redistribute incomes and wealth, and in the process, to raise standards of living.\nThis lesson focuses on how wealth is created as specialization and division of labor facilitate trade based on comparative advantage. It also looks at what inhibits trade, from transaction costs to trade policy. Together, these basic tools prepare students to deal with the issues of international trade in Topic 14.\n|voluntary exchange||mutual benefit||trade barriers||specialization|\n|opportunity cost||quota||tariff||division of labor|\nStandard 5: Students will understand that: Voluntary exchange occurs only when all participating parties expect to gain. This is true for trade among individuals or organizations within a nation, and among individuals or organizations in different nations.\n- Free trade increases worldwide material standards of living.\n- Despite the mutual benefits from trade among people in different countries, many nations employ trade barriers to restrict free trade for national defense reasons or because some companies and workers are hurt by free trade.\n- Voluntary exchange among people or organizations in different countries gives people a broader ranger of choices in buying goods and services.\n- When imports are restricted by public policies, consumers pay higher prices and job opportunities and profits in exporting firms decrease.\nStandard 6: Students will understand that: When individuals, regions, and nations specialize in what they can produce at the lowest cost and then trade with others, both production and consumption increase.\n- Economic specialization occurs when people concentrate their production on fewer kinds of goods and services than they consume.\n- Specialization and division of labor usually increase the productivity of workers.\n- Like trade among individuals within one country, international trade promotes specialization and division of labor and increases output and consumption.\n- Transaction costs are costs (other than price) that are associated with the purchase of a good or service. When transaction costs decrease, trade increases.\n- Individuals and nations have a comparative advantage in the production of goods or services if they can produce a product at a lower opportunity cost than other individuals or nations.\n- Comparative advantages change over time because of changes in factor endowments, resource prices, and events that occur in other nations.\n- Identify and discuss the “mutual benefit” characteristic necessary for voluntary exchange. (Based on bag trading game.)\n- Emphasize: Trade is based on perceived value, with price being only one factor considered.\n- Emphasize the importance of information in individual traders’ decision-making\n- Emphasize the importance of information in the efficient operation of markets.\n- Introduce the concept of “transaction cost” and its importance in trade.\n- Discuss specialization and division of labor as the basis for trade.\n- Explain the trade-off associated with specialization and division of labor: increased productivity and output vs. decreased self-sufficiency, and how this trade-off is mitigated by growing interdependence.\n- Develop the concept of comparative advantage in trade between individuals and organizations (not countries) by relating back to earlier discussions of opportunity cost.\n- Make the case that the basis for trade between individuals and organizations is the same regardless of whether the individuals are in the same local area, different regions, the same country, or different countries.\n- Develop an example of trade between states in the U.S. and then substitute nations for the states.\n- Provide examples of trade patterns and dispel the notion that most trade is back-and-forth.\n- Provide examples of how comparative advantage changes and causes trade patterns to change.\n- Differentiate between trade and trade policy: individuals and organizations trade; nations create trade policy that restricts or shapes trade (and may, therefore, change the win-win nature of voluntary exchange).\n- Differentiate among various trade barriers: embargoes, sanctions, quotas, subsidies, etc.\n- Explain and provide examples of how trade barriers distort trade patterns and reduce productivity growth and wealth.\n- Explain and provide examples of how trade barriers change the win-win nature of voluntary exchange so that there are winners and losers.\n- Discuss the difference between “free trade” and “fair trade.”\n- Voluntary exchange is based on the mutual perception of benefit.\n- Voluntary exchange leads to increases in specialization, division of labor, productivity, and output.\n- Comparative advantage (sellers’ identification of their least-opportunity cost alternative) is the basis for wealth-enhancing trade, whether among individuals and businesses within a country or among individuals and businesses in different countries.\n- People and organizations trade; countries do not. (Countries do set trade policy.)\n- Barriers that reduce the flow of trade slow down the overall creation of wealth.\n- Trade barriers include direct barriers like tariffs, quotas, and embargos, and indirect barriers like subsidies.\n- Individuals and businesses are affected differently by barriers to trade; some “lose,” and some “win.”\n- Countries trade.\n- Countries are better off being self-sufficient.\n- Trade makes some countries rich at the expense of other countries.\n- Trade is based solely on the price of items.\n- Most trade is bilateral, back-and-forth or this-for-that trade.\n- Barriers to trade, such as tariffs and quotas, protect and strengthen a nation’s economy.\n- Other countries won’t react when a nation establishes trade barriers.\n- Once established, trading patterns among countries don’t change.\nFrequently Asked Questions:\n- Why do we trade with other countries?\n- Do we benefit from trade? How?\n- Do those we trade with benefit? How?\n- Why do some people dislike/protest against international trade? How do they perceive that we or that people in developing countries will be hurt by trade?\n- Isn’t international trade fundamentally different from trade within our country?\n- How does our economy change when we trade with others?\n- Aren’t tariffs, quotas, and embargos necessary to protect our economy?\n- Aren’t we the “winners” when we establish trade barriers?\n- How can subsidies provided by our government to industries within our country (farmers, for example) be considered trade barriers by developing countries?\nClassroom Activity Options\n- “The Magic of Markets” – Often known simply as “the bag game,” this trading simulation is an engaging, effective way to have students experience the wealth-enhancing effects of trade. This activity is included in several FTE curriculum packages. It is available on the FTE website at:']	['<urn:uuid:7189a2a7-6666-4f1e-a919-34e15c1248c2>', '<urn:uuid:9cf441e0-bd89-4fab-9775-439a36376e43>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	21	108	4093
53	I work in construction and deal with both moisture protection and chemicals. What are the different types of waterproofing systems used in buildings, and what health problems can long-term exposure to construction materials cause?	There are two main waterproofing approaches: anti-corrosion and anti-filtration. Anti-corrosion waterproofing can be vertical or horizontal, with horizontal protection using roofing felt to prevent capillary water penetration, while vertical protection can include paint waterproofing using bitumen. Regarding health risks from construction materials, prolonged exposure to hazardous substances can lead to serious conditions including occupational asthma, occupational dermatitis, occupational cancers, and skin irritation. These substances can enter the body through skin contact, inhalation, ingestion, or through cuts. Statistics show that around 35,000 workers reported breathing or lung problems caused by work, and approximately 13,000 deaths occur yearly due to occupational lung disease and cancer from prolonged exposure to dusts and chemicals.	"['In order to provide strength and durability to the foundation of a house or any other building, it is necessary to install waterproofing. Proper foundation waterproofing is a guarantee of the reliability of the entire building. After all, if the foundation is destroyed, the structure itself will be damaged. In this article, we will tell you how to do the foundation waterproofing with your own hands, consider the methods, and also show photos and video instructions.\nThere are two ways to waterproof the foundation: anti-corrosion and anti-filtration .\nAnti-filtration waterproofing of the foundation is not done. Its arrangement requires high-class specialists who have experience in this field. However, in most cases such protection is not required. Anti-filtration system is needed only in places with high levels of moisture. For simple country houses, baths, garages suitable anti-corrosion waterproofing.\nAnti-corrosion waterproofing can be:\nVertical protection is more complex in execution, however, it is very effective.\nThe horizontal system for protecting the foundation from moisture is made of ordinary roofing felt folded several times. It is installed very simply and more common than all other types of waterproofing.\nHorizontal waterproofing serves to protect against capillary penetration of water and steam.\nThis foundation protection system is superimposed on its base. For waterproofing, you can use liquid bitumen. In the process of working with this material should only be remembered that it emits harmful fumes in a heated form. Therefore, should be protected.\nVertical protection can be:\nWhen applying vertical waterproofing, pay due attention to the handling of material joints, which are often vulnerable to moisture penetration.\nPaint waterproofing is made using bitumen, which is applied to the surface. In the vertical waterproofing can also be used roofing material.\nWaterproofing with liquid bitumen\nLiquid bitumen is an excellent material for waterproofing foundation. It:\nLiquid bitumen has disadvantages. The main one is that this material in a heated condition is very harmful.\nWork with bitumen should be done only with the use of protective equipment. In addition, bitumen requires regular heating, and it must be carefully monitored. This material quickly ignites and burns, exuding asphyxiating smoke and unpleasant odor.\nTo make waterproofing with bitumen, follow the instructions below.\nWaterproofing is not limited to the treatment of the foundation basement and basement. In order to protect the house from moisture, it is necessary to conduct high-quality drainage next to the house itself. For this it is necessary to equip one simple system.\nAfter arranging the drainage system, you should not worry that the basement will be flooded after the snow melts on the site. When arranging such a system, you must be careful.In the event of an error, water can start flowing directly to the foundation. In difficult cases for such work it is better to invite a specialist.\nAn additional measure of protection is an external drainage system. These are various trays and water collectors. This method was known in ancient times. It still works effectively. Before you make the waterproofing of the foundation of the house, it is necessary to consider the drainage area.\nWater is affected by the tape foundation:\nWhen arranging waterproofing, it is necessary to take into account all types of damage that moisture can cause in various ways.\nWaterproofing can be:\nFree-flow waterproofing is designed for seasonal melting of snow when water rises to the foundation. It is made as follows:\nAfter the arrangement of such a system, seasonal waters will no longer pose a danger to the strip foundations.\nThis type of insulation is made using various sprayed and coating agents. It protects the foundation constantly from any moisture. As a result of the work, a special membrane is created, which has water-repellent characteristics.\nThis type of foundation protection must completely fill all cracks and small cracks. One of the most effective ways to create capillary waterproofing is liquid bitumen. This method has already been described above.\nThe material that is used for waterproofing should have the following characteristics:\nFor more information about foundation waterproofing, see the following videos:', ""COSHH – A Guide to Employers’ and Employees’ Responsibilities\nEmployee Responsibilities under COSHH\nEmployee responsibilities within the COSHH (Control of Substances Hazardous to Health) Regulations of 2002 include:\n- Making use of control measures and facilities provided by the employer\n- Ensuring equipment is returned and stored properly\n- Reporting defects/insufficiencies in control measures\n- Wearing and storing personal protective equipment (PPE)\n- Removing PPE that could cause contamination before eating or drinking\n- Making proper use of washing, showering and bathing facilities when required\n- Maintaining a high level of personal hygiene\n- Complying with any information, instruction or training that is provided\nEmployer Responsibilities Under COSHH\nUnder COSHH regulations, employers’ responsibilities include:\n- Implementing control measures to protect workers from hazardous substances.\n- Preventing or adequately controlling exposure to hazardous substances.\n- Providing employees with suitable and sufficient information, instruction and training, and appropriate protective equipment where necessary.\n- Ensuring that control measures are maintained, kept in full working order, and in a clean condition where appropriate.\n- Drawing up plans and procedures to deal with accidents and emergencies involving hazardous substances.\n- Ensuring that any employees exposed to hazardous substances whilst at work are under suitable health surveillance.\n- Ensuring that substances do not exceed the Workplace Exposure Limit (WEL).\n- Carrying out a COSHH risk assessment.\nNaturally, workplaces with higher risks, such as catering or a hair salon, will require more action than, say, an office. But as an employer, you should be assessing what risks may be posed by hazardous substances, no matter where you work.\nThat way, you can identify if there are risks and if so take action to reduce them to a minimum.\nCOSHH Risk Assessment\nA COSHH risk assessment is essentially the same as a standard risk assessment in terms of the process, but your assessment of the workplace will focus solely on hazardous substances.\nIf you’re unfamiliar with risk assessments, here’s a breakdown of the main 5 steps:\n- Identify the hazards.\n- Decide who might be harmed and how.\n- Evaluate the risks and decide on precautions.\n- Record your findings and implement them.\n- Review your assessment and update if necessary.\nRisk assessments will also involve frequently monitoring the workplace’s processes and the level of exposure to substances.\nWorkplaces are active and constantly changing, so a one-off check won’t be sufficient in minimising the risks posed by hazardous substances. You have to remain constantly vigilant and alert to the dangers.\nRecap: What is COSHH?\nCOSHH stands for the Control of Substances Hazardous to Health Regulations (2002). It exists to ensure that both employers and employees do all they can in a workplace to minimise people’s exposure to hazardous substances and work in ways that are safe.\nThis means that all hazardous substances need to be identified and precautions need to be taken to ensure that workers know how to use and handle them safely.\nThe importance of controlling hazardous substances cannot be overstated. In 2012/13, around 35,000 workers reported that they had breathing or lung problems caused by work, and the most common type of reported skin disease was contact dermatitis.\nAnd it’s estimated that around 13,000 deaths occur each year due to occupational lung disease and cancer – fatal conditions that will have developed over a prolonged period of exposure to dusts and chemicals at work.\nAs an employee or employer, you can prevent statistics like this from increasing. If you fulfil your workplace duties, you can prevent dangerous levels of exposure and meet COSHH requirements.\nRecap: What is a Hazardous Substance?\nSimply put, a hazardous substance is any mixture or substance that is toxic, irritant, or corrosive – whether it’s a liquid, gas, vapour, fume, or dust.\nThey cause harm to the body via routes of entry:\n- By coming into contact with skin or eyes.\n- By being inhaled.\n- By being ingested through the mouth.\n- By entering the body through cuts or punctures in the skin.\nAlthough there are certain industries that will be at greater risk, hazardous substances could exist in any workplace. They are often used directly in work activities, produced by work activities, or already present in your workplace’s premises.\nExamples of hazardous substances include:\n- Chemicals, e.g. cleaning chemicals or bleach.\n- Fumes, e.g. from paint or vehicles that exhaust.\n- Gases, e.g. ammonia from refrigerators.\n- Dusts and powder, e.g. from flour.\nIt’s worth noting that even seemingly innocent substances can be harmful, and that includes natural materials like wood dust or flour.\nWhile many hazardous substances can cause immediate harm, such as a corrosive liquid being spilled onto someone’s skin, the main danger posed by hazardous substances is prolonged exposure. For example, if someone is in the presence of or uses a dangerous chemical for a long time, they could develop breathing difficulties or skin conditions.\nExamples of ill-health caused by hazardous substances includes:\n- Occupational asthma.\n- Occupational dermatitis.\n- Occupational cancers.\n- Skin irritation.\n- Infection from bacteria.\n- Injury or death as a result of exposure to toxic fumes.\nWhat to Read Next:\nSubscribe for the latest Hub updates! Tell us what you're interested in hearing about:""]"	['<urn:uuid:104971a1-9491-445e-bc7c-6d913783674a>', '<urn:uuid:e2c99559-4fd5-48cf-884f-dca2baae7e40>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	34	110	1532
54	expectations between attorney and client cooperation communication requirements professional services	Attorneys expect clients to cooperate in good faith with attorneys and staff members to ensure competent and efficient handling of matters. Clients must provide relevant information and documentation, respond quickly to information requests, and review and execute legal documents when needed to protect their legal rights. Clients should expect an initial meeting to discuss their legal matter, rights, and strategy. They should receive a written retainer agreement and periodic communications updating them on their case status and required actions.	"['When you seek the advice or representation from a lawyer, that lawyer is bound by strict ethical standards and must act competently and professionally. Your lawyer must communicate with you about your matter and must always act in your best interests and keep your communications confidential. Even with the extensive ethical requirements placed upon attorneys, you should always make sure that you have confidence in your attorney\'s abilities and are comfortable with the way your matter is being handled.\nThere is no easy answer as to whether you require professional legal representation. Every individual is different and has different skills, experience and expertise. A person should never undertake significant legal or financial obligations unless that person understands the transaction and the consequences of their actions and has made the appropriate informed decision. As a general principle, a lawyer should always be consulted before an individual or company executes significant legal documents (like a deed, will or contract) to ensure that they understand the potential legal ramifications of the document and to ensure that the transaction is understood and in the client\'s best interests.\nIn certain limited situations an individual or business may be able to handle simple legal issues on their own without the need for professional legal representation. Minor traffic violations, small monetary disputes with individuals, clients or vendors can usually be resolved in an efficient, cost-effective manner without the need for professional legal services. However, many situations will involve complex issues requiring the skills and expertise possessed by an experienced attorney. Fortunately, most attorneys will provide you with a free consultation to discuss your matter and will give you the information you need to make an informed decision as to whether you need or want professional legal services.\nYes. As a general matter you may terminate the attorney-client relationship and/or change attorneys at any time for any reason. The client will generally be required to comply with the terms and conditions of any retainer agreement so long as those terms do not violate any professional or ethical rule. As such, the client will be held responsible for payment of any legal services provided or expenses incurred by the attorney prior to the termination of the representation. In a contingency-fee case, the attorney may be granted an ""attorney lien"" on your legal matter which awards the attorney a right to a portion of any future monetary recovery.\nYour attorney will expect you to cooperate in good faith with the attorney and any staff members to ensure that your matter is handled competently and efficiently. As the client, you generally possess all the relevant information and documentation regarding your matter and the attorney cannot effectively represent your interests without your continued involvement and cooperation. Some legal matters may involve strict time deadlines that must be met, and you may be required to respond quickly to requests for information or to review and execute legal documents to protect your legal rights. The attorney will expect and require payment of services and expenses pursuant to your written retainer agreement.\nYou should expect to have a meeting with your attorney in person or by telephone to generally discuss your legal matter, your rights and a strategy for handling your matter. Depending on your situation, you may also interact with staff personnel including paralegals and legal assistants. If you decide to retain the attorney you should, in most circumstances, expect to receive a written retainer agreement setting forth the terms and conditions of the representation. Once you have retained the attorney, you should expect to receive periodic communications from your attorney or staff updating you on the status of your matter and seeking whatever information, documents or actions are required from you.\nYou should look for someone who has the skill, expertise and demeanor to handle your matter in a competent and efficient manner. You want to select an attorney who is responsive and will communicate with you honestly and effectively regarding your legal matter and who will work diligently to obtain the best outcome possible for your situation.\nMost matters are handled on an hourly basis which can vary depending on the specific legal matter handled. This means the client is billed per hour (or fraction thereof) for the time that is directly spent on whatever tasks or services are required. Some matters may be handled on a ""flat fee"" basis where the client pays a certain pre-determined set fee irrespective of how much time is spent on the matter. Flat-fee matters could include residential real estate transactions or the preparation of less-complex legal documents like living wills and powers of attorney. A very small number of cases, usually involving personal injury or the enforcement of money judgments, may be handled on a contingency-fee basis where the attorney receives a percentage of the monetary recovery and where there is no fee for legal services if there is no recovery.\nI specialize in advising individuals and businesses with respect to residential and commercial real estate transactions, business formation/consultation and litigation across numerous practice areas. I also advise clients on estate planning, landlord/tenant issues and many other types of legal matters. During my twenty years of practice I have handled a wide variety of legal matters across the state of New York and in all levels of state and federal courts.\nI specialize in the practice areas that I have the most knowledge of and where I have developed most of my skills and experience. No attorney can be an expert in every practice area of the law, but many can be extremely proficient at providing a wide range of legal expertise and services required by the attorney\'s clients.']"	['<urn:uuid:b6bfe5fd-6c2b-4b18-9e4f-507f61720686>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	10	79	944
55	what highest award given at berlin international film festival	At the Berlin International Film Festival (Berlinale), the Golden Bear is the highest and most prestigious award given for the best film.	['Germany is full of history, culture, fun, arts, sports, excitement and thrill. Their festivals are stunning and spectacular. They love to have fun and celebrate. People from all over the world visit the country to take part all those colorful and fun-filled events.\nOktoberfest, Munich (September 21 – October 6)\nIn Munich, Germany, the largest folk festival held for 16 to 18 days is named Oktoberfest. Normally, it starts from mid or late September and ends in first weekend of October. The weather is very beautiful and pleasant at this time. Around six million people come to attend the event. In Bavarian culture, this festival is an important part. Oktoberfest beer is an integral part of the event. Almost 7.7 million liters are consumed during the festival. There are side stalls, games, amusement rides, traditional local foods and the most important beer booth to enjoy. From history, it is known that the festival was arranged since 1810. Now other cities, inspired by Munich, also arrange such events. On 12th October, 1810, King Ludwig got married to Princess Therese of Saxe-Hildburghausen. The whole Munich city was invited to enjoy the festival for five days. The arrangements were on the fields in front of the city gates. To show honor to the Crown princess, the fields were named as ‘Theresienwiese’. The name has been modified as ‘Wisen’ now. The festival is still held on that field. Horses were arranged on 18th October to honor the couple. From that incident on, every year, horse races were arranged and other numerous attractions were arranged and named as Oktoberfest. Tree climbing, swings and bowling alleys were there in addition. For promoting Bavarian agriculture a show was added since 1811. In 1813 due to Napoleonic Wars, the festival could not be held. Carnival booths were added from 1818. Jewelry, silver and porcelain were given as prizes. From 1819 it was announced as an annual festival. The horse race discontinued after 1960. A parade was held to show honor to the marriage in 1810, for the first time. Till now, the tradition is followed by eight thousand Bavarian wearing traditional costumes, floats and carriages. It starts from Maximillian Streets and ends in Oktoberfest grounds. A sculpture named as Statue of Bavaria was built in 1850. The festival was cancelled many times because of wars and diseases. Finally, from 1950 the festival is celebrated. One special attraction is arranged here, especially for the children, it’s the flea circus. You can taste ‘Wiesn-Hendl’ a traditional Oktoberfest meal from the stall. This place is a heaven for beer lovers. You will get two pints of local delicious beer for 12 euro. You may find cheap beer in the small tents. The beer comes from local brewers. There is no entry fee for this event. Only the ‘Oide Wiesn’ on the south side has an entry fee which costs 3 euro. The place was built in 2010 to celebrate the 200-year Oktoberfest anniversary. Children’s entry is free. Currently there are 14 large tents and 20 small tents on the festival grounds. These are made from temporary wooden structures. Beer and wine are served here. You can find few local snacks as well. Chicken, grilled sausage and giant pretzels are there to fill up your stomach. You can enjoy Bavarian music and their folk dances. You don’t need to pay anything to enter the tents. Try to keep change while visiting the event. This will be helpful while giving tips to the waiter. Volunteers and security personnel are there for safety purpose. If you have a tight budget you may choose to camp. There are also some cheap hotels in the city. But try to confirm your accommodation as early as possible. You can come to the venue from the city by local transports. You can also come from different cities.\nBerlin International Film Festival/ Berlinale (February 7 – February 17)\nThe Berlin Film Festival or Berlinale is one of the biggest film festivals in Europe and also in the whole world. It is held annually for two weeks in the month of February in Berlin, Germany. It was founded in 1951 in West Berlin. Dr. Alfred Bauer was the first director of this festival. The famous movie Rebecca by Alfred Hitchcock was shown in the opening of the first festival. That time the festival was held in the month of June. In 1976, film journalist Wolf Donner became the second director and he changed the festival time and shifted it to February. From that time the event is held in February. There are seven different film sections in this festival. The categories are Competition, Panorama, Forum, Generation, Perspektive Deutsches Kino, Berlinale Shorts and Retrospective. There is a committee of film experts and section director for each section to choose the films. There are also other links and series such as Berlinale Special, Gala Special, Forum 5, the Homage and Culinary Cinema. In the festival the Golden Bear is the highest and most prestigious award for the best film. Silver Bear is given as the second place after the Golden Bear. It is known as the Jury Grand Prix. There are also few other awards given for different sections. It has become a professional trade event. The exhibitions are arranged for showing current projects for the companies. More than 1000 screening of new films are shown in different theatre. It is a place to build network for producing and financing. Many international co-production companies attend the event. From 2003, Berlinale Talents is arranged at the same time with the festival. This is a winter school for new film makers where 250 applicants are accepted by the Talent Campus. It is organized for three days where many well-known professionals from the film making industry come to share knowledge. Workshops, personal tutoring, coaching and training are there as well. It is a great opportunity for those who want to build a career in the film industry. To attend a movie screening you have to purchase tickets online. Normally a ticket costs 7-11 euro. But on the last day you can watch any film for only 6 euro. This is a chance to enjoy different types of films, coming from countries all over the world, with different making. You can also meet the cast and crew of the films or watch your favorite stars from a distance.\nThe Cologne Carnival is the biggest carnival held every year in Cologne, Germany, and very famous everywhere. It is telecasted in National TV as well. In ancient times the German used to celebrate the winter to drive away the evil demons. Just before the Lent fasting, people used to celebrate grandly. In 18th century the street carnival was expanded more with parades, fancy and masked dress balls and other activities. Cities like Bonn, Dusseldorf, Aachen and Mainz are also popular for the celebration. Each year 11th November at 11:11 am is the magical time when the carnival season is started. On that day The Council of Eleven starts the carnival planning. A huge stage show is arranged for the grand opening on the Alter Markt in the Old Town. All the popular carnival bands are there to perform. You must have a ticket to enter. Then there is a little gap for Christmas. After New Year’s celebration the carnival starts in full swing. There are many sections for the festival. The Women’s Carnival Day, Ghost parade, Rose Monday and Ash Wednesday are the main attractions. Main celebration starts 40 days before the Easter. Every year three persons are titled as maiden, prince and peasant. The carnival prince, who leads the main parade has the highest position. There are parades, balls and stage shows which are arranged by the Carnival Committee. The first big event is Women’s Carnival Day held on Shrove Thursday. Every woman dresses up at their best. Everyone arrives in Alter Markt at 10 am. The maiden, prince and peasant arrive shortly after that and at 11.00 am the event starts. On Friday, the parties are going on and on till one is out of energy. Saturday is for the Ghost Parade almost similar to Halloween. Sunday is for the different schools and city groups to parade in colorful costumes and to enjoy themselves. Rose Monday is a day when a procession with 74 decorated floats, 67 tractors and 50 Ford Trucks gather to finish the 6km long track for 3 hours. Candy, flowers and toys are thrown to the watchers from the parade. Tuesday is to go crazy for the last time before Lent starts. They also burn a life- size straw doll called nubbel. On Ash Wednesday fish dinners are served everywhere.\nIf you are interested in visiting Germany, after reading about all the festivities this country has to offer, you should check out this German website from Backpackertrail, which will give you more information on what to expect when you travel to Germany and gives you a lot of tips and tricks for your trip!\nList of other major festivals\n- January –Epiphany, New Year’s Day\n- February – Berlin International Film Festival, Karneval\n- March – Leipzig Book Fair, Easter\n- April – Munich Ballet Week, Walpurgis\n- May – International Africa Festival, Rhein in Flammen, Hafengeburtstag, Wurstmarkt\n- June – Kieler Woche, Munich Opera Festival\n- July – Richard Wagner Festival\n- August – Long Night of the Museum\n- September–Oktoberfest, Festival-Mediaval, Beethovanfest\n- October – Frankfrut Book Fair, Oktoberfest, Unity Day\n- November – St. Martin’s Day, Cologne Carnival\n- December – Christmas Markets']	['<urn:uuid:dbac6852-4435-46bf-9a23-866f1622d635>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	9	22	1594
56	What causes voice strain in daily speech versus professional singing?	In daily speech, we typically use a shallow, horizontal mouth position with an almost closed jaw, speaking in a narrow pitch range that doesn't require much resonating space. This speaking habit creates tension in the vocal tract, causing tight throat, collapsed soft palate, or retracted tongue. In contrast, singing requires more internal space, especially for higher pitches, with vowels keeping the throat open about 98% of the time. Additionally, various external factors can strain the voice, including air pollution, dehydration, excessive talking, and forceful whispering. Professional singers and people who talk extensively for work may need voice coaching to prevent strain and maintain vocal health.	"['Ever wonder why your voice feels and sounds terrific when you’re singing your vocal exercises, but then the minute you jump into singing your song, you feel tight and the voice sounds strained? One of the biggest mistakes I see singers make is that before singing a song, they warm up a little bit (or not at all) on a vocal exercise or two, and then jump directly into singing their song. This is a recipe for failure, because the first thing that happens when you start singing complete words, as opposed to the lovely “ahs” and “ohs” you were probably using to warm up, is that your speech habits take over. And, my singing friends, your speech habits are your enemy in singing…\n“Well, I have to sing words!” you retort. “I just can’t sing on vowels the rest of my life” you add. Fear not— I have a solution to this problem that works every time, for every singer, on any style of song. It’s a practice method I’ve devised called “The Birthday Cake”™ and it will be the bridge between those wonderful vowels you were singing, and the successful final product of singing your song.\n“So what’s wrong with my speech habits?” you may ask. It’s nothing personal, it’s just how we all talk! You see, we don’t open our mouths very much to speak, and really don’t need to. Most all our sounds in speech are made in a relatively shallow, horizontal, sometimes tight position, with the jaw almost closed. Also, we speak in a fairly narrow pitch range which doesn’t require a lot of resonating space in the vocal tract, unlike singing, where the higher the pitch you sing, the more internal space is required. Unfortunately, the more closed your jaw is, the more tension you can have in the vocal tract— this could be a tight throat, a collapsed roof of the mouth (soft palate) or a retracted tongue. All of these can contribute to uncomfortable and limited singing. In addition, we tend to make all our vowels by using our biting reflex. Try saying: Ah-Ay-Ee-Oh-Oo, and see if your jaw moves. Did it? As you ‘bite” your way from sound to sound, you can engage tension in your throat.\nSo— we need a way to “break” or at least relax, our jaw tension in the way we make our vowels, both in our vocal exercises, and in singing the lyrics of our song. We already have to move our jaw to say many of our consonants— there’s no way of getting around that. But if we can keep the jaw fairly relaxed while still making clear vowel sounds, we will be open most of the time. Good singing dictates that you’re on your vowels about 98% of the time (vowels keep the throat open and the sound moving) versus consonants, which often stop or interfere with our airflow. We should lightly form and then release the consonants as quickly as possible.\nSo— getting “hungry” for that “cake” yet? Let’s begin! Using my metaphor of a birthday cake, with a number of layers until you get to the candles on the top, let’s start with the bottom layer…\n1. Place two fingertips, stacked one above the other, between your back teeth on one side of your mouth (wash your hands first!) . Use the left hand on the left side, or the right hand on the right side. This will keep your mouth open and the tongue free. This is what’s known as a “bite block”. These finger tips are going to keep you from biting down as we work on our vowels and consonants. NEVER use any foreign objects in your mouth! Your jaws are very powerful and the object could pop out and choke you. ONLY USE YOUR FINGERTIPS. They’re attached to your hand and not going anywhere!\n2. Next, make sure your tongue is all the way forward, resting inside your lower front lip. Many teachers have students rest the tongue behind the lower teeth, but if you have it just glancing the lower inside lip, the tongue is all the way forward and opens the throat space more, for an easier vocal production with greater resonance in the sound. This position is “Home”, where you want to be for taking in breath, and whenever you have the time to sustain the vowel sounds in your words. You do have to move the tongue to say some consonants, but not all of them. You’ll be surprised how often you can go “Home” when singing!\n3. Yawn in your inhalation. This will raise the soft palate (back part of the roof of the mouth) which has many positive effects on your vocal production (which we’ll discuss in anther post).\n4. Now, sing your song melody on “Ah” only, keeping the first three components that you just set up— vertical space by using the bite block, tongue released forward, and that feeling of palatal lift, which feels like the beginning of a yawn. Now you’ve got an open vocal tract!\nSinging your song melody on “Ah” is the first and bottom layer of the cake. You’ve just corrected three HUGE elements of of your technique, and more!\n5. Now you can also examine your legato. Are the pitches smooth and connected, and do you feel like you’re “riding the wave” of air as it comes out? (this is legato singing). Or are you stuck in your throat and pushing or manipulating the tone from there? This bottom layer of the cake is your chance to fix this! Once we put the vowels and consonants of your words back, it will mask your articulation (the way your vocal instrument moves from note to note) and this won’t be as obvious. Fix it now!\n6. Check for consistent vowel quality. As you glide along through your song melody on “ah” with your fingers in your mouth, does it REALLY sound like an “AH” (think the Ah in the word ‘father’)? Or, does it sound more like “uh” or “eh”? That means there is tension somewhere in your vocal tract, especially in the back of the tongue. Or, if your “Ah” sounds correct sometimes and then morphs into other sounds as you sing, keep striving for consistency. You want everything to be perfectly stable before we move to the next layer of the cake!\nSO— you have the first and most important tool going for you now. You can sing an open, consistent “Ah” sound with the proper technique! This will form the basis of the next layers of the cake, which we will discuss in subsequent posts. Take a week and work only on this. Try it on any song. You will be amazed at how much your voice opens up, just doing this. And then, reward yourself— go have a REAL piece of cake!\nUntil Part Two…', ""Many things can adversely affect the vocal cords, including smoking (tobacco or marijuana), secondhand smoke and other air pollution; a cold or laryngitis; very dry conditions, dehydration or excessive alcohol intake; and severe emotional stress. Certain drugs (such as oral contraceptives, antihistamines, steroids & some antidepressants) can contribute to hoarseness. So can reflux of stomach acids, particularly while you sleep, as well as various benign growths or malignant tumors.\nVocal cords also age along with the rest of the body. These twin bands of connective tissue are located in the voice box, or larynx, just above the windpipe. In the delicate system that allows human speech, the vocal cords remain open above the airway when you are silent. When you speak, the cords move together and vibrate, creating sound in a wide range. Poor vocal techniques that strain your voice, and simple fatigue from overuse can all take their toll.\nDon’t smoke or spend a lot of time around smokers, charcoal grills and other sources of smoke. Avoid indoor air polluted with fragrances, harsh cleaners and the like. Get enough sleep. If you are required to talk all day, rest your voice at night. Drink plenty of fluids. Limit alcoholic beverages.\nAvoid continual clearing of your throat, shouting, rooting too loudly for the home team or competing with loud noise. If you must whisper, don’t do so forcefully or for long; it's hard on vocal cords. If your job demands lots of talking, consider consulting a voice coach or therapist. Ask your physician for a referral, or consult the American Speech-Language-Hearing Association.\nIf you grow hoarse or lose your voice temporarily, you may have some idea what the problem is. Maybe your voice just needs a rest. But if you lose your voice for more than two days or have a lump in your throat or difficulty swallowing, or if hoarseness persists for two weeks, you should get medical advice. Your regular physician will probably send you to a specialist, who should be able to diagnose (or rule out) serious conditions by looking at your voice box with a laryngoscope and/or doing acoustic analyses. Don’t forget to mention what kind of work you do, and symptoms such as waking up with a sour taste in your mouth (this might indicate reflux).\nHoarseness is the most common symptom of any vocal disorder. Vocal tremors may also occur—these often come from fatigue or misuse of the voice, but can also be caused by underlying conditions such as “essential tremor” (unexplained shaking), more common in older people. Or you may have dysphonia—a cracked or squeaky voice and general loss of voice quality. This may progress to intermittent loss of voice. Occasionally people “lose” their voices without warning. A rare condition called vocal cord paralysis is sometimes to blame.\nTreatments vary from voice retraining to injections to surgery. One common diagnosis is “non-organic dysphonia,” meaning that nothing apparent is wrong. In this case, you may be referred to a voice therapist who can train you in vocal techniques that will reduce strain on your voice, as well as vocal rehabilitation exercises. Go only to a licensed, certified voice specialist. For some conditions, especially tumors of any kind, surgery may be the best or even the only option. Other chronic problems may be improved by injections to “plump up” aging vocal cords, or by repositioning vocal cords with surgical implants. Always get a second opinion before having surgery.""]"	['<urn:uuid:9212bc83-da78-4174-bf14-78a58bfbd336>', '<urn:uuid:c77ba740-9d7b-4800-824f-af7f1f5634dc>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	10	105	1726
57	therapeutic relationship mindfulness development patient acceptance professional training methods	Mindfulness plays a crucial role in developing therapeutic relationships through both patient and professional development. For patients, mindfulness practices help them accept their current situation and emotions without resistance, while for professionals, it serves as essential clinical training. The therapeutic process emphasizes cultivating attention and compassion, with professionals learning to implement mindfulness techniques and teach them effectively to patients. This approach helps increase acceptance and empathy in the therapeutic relationship, creating a stronger bond between therapist and patient while facilitating better treatment outcomes.	"['What Is Acceptance and Commitment Therapy?\n“What is acceptance and commitment therapy?” is a frequently asked question. Acceptance and Commitment Therapy (ACT) is a unique empirically based psychological intervention that uses acceptance, mindfulness, and commitment strategies to increase psychological flexibility. What differentiates ACT treatment from other sorts of medical intervention is that treating issues doesn’t necessarily mean the goal is to eradicate them. A lot of stress comes from people who are already dealing with mental health issues feeling that they’re in the wrong for even having these struggles. They’ll see themselves as weak for conditions like depression and do all they can to feel otherwise. They might try to act cheerful or aloof when they’re feeling unhappy and tense. They might cover up their negative emotions with drugs and alcohol. However, in trying to act like nothing’s wrong, more problems sprout up. Someone with an anxiety disorder can’t just tell themselves to calm down when having a panic attack. This is like trying to aggressively steer your way out of a skid when driving on black ice. For someone to be able to deal with long-present issues, they may have to accept that the only way out is to realize that there might not be a way out. They may have to deal with their issues, be they depression, anxiety, addiction, or another problem, in one form another, for the rest of their lives.\nSo, What Is Acceptance and Commitment Therapy upon closer examination? Let’s find it out!\nWhen Is Acceptance and Commitment Therapy (ACT) Used\nAcceptance and Commitment Therapy (ACT) is a mental health treatment used to treat patients who have issues that they’ve been struggling with for a long time. Some of these issues can be traced back to early childhood, such as separation anxiety. Those who struggle with addiction and intense bodily discomfort can also benefit from ACT. The purpose is to help patients come to terms with their situation. It’s illogical to try to completely reverse a way of thinking and feeling that’s been present for so long that they can’t recall what it’s like to not feel that way. The best treatment can be to help them see their feelings as just being a fact of life, not something they need to suppress.\nHow Does Acceptance and Commitment Therapy (ACT) Work\nWhen undergoing Acceptance and Commitment Therapy, a patient needs to review their negative feelings and how they’re most likely to worsen. For example, someone who has claustrophobia might feel the worst when in tight or enclosed spaces, such as elevators. Someone with depression might have those feelings intensified when they’re trying to fall asleep.\nReviewing moments of distress helps a patient to see how their resistance has negatively impacted them. For example, if you’re someone with claustrophobia who has gotten in an elevator and tried to pretend that you were completely calm, your efforts likely backfired. You didn’t just decide one day to be claustrophobic, so you can’t just randomly declare that they’re not claustrophobic. Even if nothing goes wrong on the elevator, you’re likely to feel tenseness. Then, when you get off, you don’t feel any real relief. You just feel frustrated with yourself because trying to force positive emotions hasn’t worked out for you. You might think that the problem is that you haven’t tried hard enough to get rid of your negative emotions. To cover up your fear, you might only go into elevators if you’re under the influence of drugs, believing that they’ll eliminate any fear.\nACT is largely based on mindfulness. Those who have studied meditation know that mindfulness involves being present at the moment and accepting the situation as it is. Through mindfulness, one comes to understand how much suffering is found not in bad situations but in resistance to bad situations and an attachment to the permanence of good things. One has to surrender themselves to negative emotions in order to stop them from having control. They must accept them as readily as one accepts the rain.\nWhen you get caught in their rain, you have to accept it. You can’t tell yourself that it’s not raining and hope that your clothes suddenly dry. You might be upset about the situation, but anger doesn’t change anything. If you pull out an umbrella, you have some protection. However, even if you have nothing to keep you dry, you can still deal with the rain in a better manner. It all starts with acceptance.\nOn a similar basis, your negative feelings might not be welcomed, but they have a reason for existing. You may have created associations that make you think about certain ways in certain situations. If you are riding an elevator that got stuck when you were a child, it’s totally understandable that you would develop this kind of fear as a result. Not all conditions can be easily explained, and you shouldn’t feel like you have to come to any sort of epiphany.\nWhen you find yourself in a negative mood, don’t fight it. Let yourself feel however you do. Bring yourself closer to your mind and body. Observe your thoughts, one at a time. You can turn your negative thoughts into statements that bounce around your mind. Look at each one and consider their validity. This practice lets you see how some of your thoughts are grossly misleading. You might still feel the judgment you have towards yourself, but you can identify these thoughts as being tricks of your biased mind and not actual truths. There’s often a physical component to these situations, such as sweating or having your heart start racing. Should this happen, you can observe what’s happening. How fast is your heart going? You can use slow, diaphragmatic breathing to calm you down.\nThese kinds of issues aren’t fun to experience, but they can be managed. Giving yourself permission to not feel great is one of the best things you can do to take control of your life. Accept the bad moments, and the good ones will be even better. This may be easier said than done, but with Acceptance and Commitment Therapy, it is possible. Working closely with a therapist, clients will learn how to process certain events and feelings. When confronting these emotions, individuals will decide if it should be accepted for what it is and make the appropriate behavioral changes.\nAcceptance and Commitment Therapy (ACT) and Cognitive Behavioral Therapy (CBT)\nAcceptance and Commitment Therapy (ACT) was evolved from Cognitive Behavioral Therapy (CBT). ACT has a strong connection to the power of changing behavior. ACT differs from CBT greatly in that ACT says that you change the relationship you have with your thoughts versus trying to change them, so you don’t have to do anything with your thoughts. ACT focuses on mindfulness, acceptance of unpleasant emotions, and diffusion of challenging thoughts, which means you learn to separate yourself from your thoughts, all in the name of moving you towards a meaningful life based on you taking committed action towards your values.\nIf you or someone you know could benefit from Acceptance and Commitment Therapy, contact our team at Overland IOP today.\nPublished: March 05, 2021\nLast Updated: August 23, 2021\nPublished: October 20, 2021\nSuicidal Ideation Scale\nAccording to the National Survey on Drug Use and Health, 4.3% of U.S. adults ages 18 and older had thoughts about suicide, with the highest prevalence among adults ages 18 to 25. For people with mental health disorders, the rate is significantly higher. A suicidal ideation scale is a clinical tool used to assess a person’s risk of […]Read more\nPublished: October 01, 2021\nMental Health: Day Treatment Program or PHP\nDay treatment program or partial hospitalization program (PHP) is the next step down from inpatient treatment and step up from intensive outpatient program (IOP). In most mental health day treatment programs, patients spend several hours each day at the treatment facility.Read more\nPublished: September 21, 2021\nSUICIDAL IDEATION TREATMENT IN LOS ANGELES, CALIFORNIA\nSuicide is the 10th leading cause of death in the United States and the 2nd leading cause of death in people between 10-34. The gravity of the situation can be seen in the fact that every 11 minutes, one person loses their life to suicide. If you are considering suicide or know of someone that […]Read more', ""More New and Used\nfrom Private Sellers\nUsually Ships in 2-3 Business Days\nQuestions About This Book?\nWhy should I rent this book?\nRenting is easy, fast, and cheap! Renting from eCampus.com can save you hundreds of dollars compared to the cost of new or used books each semester. At the end of the semester, simply ship the book back to us with a free UPS shipping label! No need to worry about selling it back.\nHow do rental returns work?\nReturning books is as easy as possible. As your rental due date approaches, we will email you several courtesy reminders. When you are ready to return, you can print a free UPS shipping label from our website at any time. Then, just return the book to your UPS driver or any staffed UPS location. You can even use the same box we shipped it in!\nWhat version or edition is this?\nThis is the 2nd edition with a publication date of 7/19/2013.\nWhat is included with this book?\n- The New copy of this book will include any supplemental materials advertised. Please check the title of the book to determine if it should include any CDs, lab manuals, study guides, etc.\n- The Rental copy of this book is not guaranteed to include any supplemental materials. You may receive a brand new copy, but typically, only the book itself.\nThis practical book has given tens of thousands of clinicians and students a comprehensive introduction to mindfulness and its clinical applications. Leading practitioners in the field present clear-cut procedures for implementing mindfulness techniques and teaching them to patients experiencing depression, anxiety, chronic pain, and other problems. Also addressed are ways that mindfulness practices can increase acceptance and empathy in the therapeutic relationship. The book describes the philosophical underpinnings of mindfulness and reviews the growing body of treatment studies and neuroscientific research. User-friendly features include illustrative case examples and practice exercises. New to This Edition *Incorporates significant empirical advances--mindfulness has become one of the most-researched areas in psychotherapy. *Most chapters extensively revised or rewritten. *Chapters on practical ethics, trauma, and addictions. *Greater emphasis on the role of acceptance and compassion in mindfulness.\nChristopher K. Germer, PhD, a clinical psychologist in the Boston area, has been integrating the principles and practices of meditation into psychotherapy since 1978. He is Clinical Instructor in Psychology at the Cambridge Health Alliance/Harvard Medical School and a founding member of the Institute for Meditation and Psychotherapy (IMP). With Kristin Neff, he developed an empirically supported 8-week Mindful Self-Compassion program. Dr. Germer conducts workshops and lectures internationally on mindfulness and self-compassion, is coeditor (with Ronald D. Siegel) of Wisdom and Compassion in Psychotherapy, and is author of The Mindful Path to Self-Compassion. Ronald D. Siegel, PsyD, is Assistant Clinical Professor of Psychology at the Cambridge Health Alliance/Harvard Medical School, where he has taught for over 30 years. He is a long-time student of mindfulness meditation and serves on the board of directors and faculty of IMP. Dr. Siegel teaches internationally about mindfulness and psychotherapy and mind-body treatment, while maintaining a private clinical practice in Lincoln, Massachusetts. He is the author of books including The Mindfulness Solution: Everyday Practices for Everyday Problems and Back Sense: A Revolutionary Approach to Halting the Cycle of Chronic Back Pain. Paul R. Fulton, EdD, is a clinical psychologist in private practice in Newton, Massachusetts. He received lay ordination as a Zen Buddhist in 1972 and has been a student of psychology and meditation for over 40 years. He teaches nationally and internationally about psychology and meditation and is course director for IMP's year-long Certificate Program in Mindfulness and Psychotherapy. He is a board member of the Barre Center for Buddhist Studies and former President of IMP. Dr. Fulton was previously director of mental health for a large managed care organization in eastern Massachusetts.\nTable of Contents\nI. The Meaning of Mindfulness 1. Mindfulness: What Is It? What Does It Matter?, Christopher K. Germer 2. Buddhist and Western Psychology: Seeking Common Ground, Paul R. Fulton and Ronald D. Siegel II. The Therapy Relationship 3. Mindfulness as Clinical Training, Paul R. Fulton 4. Cultivating Attention and Compassion, William D. Morgan, Susan T. Morgan, and Christopher K. Germer 5. Relational Mindfulness, Janet L. Surrey and Gregory Kramer 6. Practical Ethics, Stephanie P. Morgan III. Clinical Applications 7. Teaching Mindfulness in Therapy, Susan M. Pollak 8. Depression: Finding a Way In, Finding a Way Out, Thomas Pedulla 9. Anxiety: Accepting What Comes and Doing What Matters, Lizabeth Roemer and Susan M. Orsillo 10. Psychophysiological Disorders: Embracing Pain, Ronald D. Siegel 11. Mindfulness, Insight, and Trauma Therapy, John Briere 12. Breaking the Addiction Loop, Judson A. Brewer 13. Working with Children, Trudy A. Goodman IV. Past, Present, and Promise 14. Roots of Mindfulness, Andrew R. Olendzki 15. The Neurobiology of Mindfulness, Sara W. Lazar 16. Positive Psychology and the Bodhisattva Path, Charles W. Styron V. Appendix Appendix: Glossary of Terms in Buddhist Psychology, Andrew R. Olendzki""]"	['<urn:uuid:a1ca4e0a-f787-4ff9-befe-8fa5a70a4657>', '<urn:uuid:00950630-539b-4bf3-b1c8-e4102287e67d>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	9	83	2217
58	What happens when code is written quickly without quality?	When sacrificing quality to deliver extra functionality, teams incur a code-debt. This introduces risk into the delivery (the interest) and makes it more expensive to write new code in the future (the principal). Until time is invested to fix the quality of the code through refactoring and testing, these costs will continue to accumulate.	['Welcome Readers of the Carnival of Enterprise Architecture and Sharp Blue readers, and Carnival of Software Development followers. We hope you like what we’ve got here – please let us know what you think, and enjoy yourselves!\nRoger had a great suggestion in the comments to our previous two-part post on scheduling requirements changes based on complexity. Roger pointed out that we had not explained what timeboxing is, but implicitly used the principles of timeboxing in our proposed process. In this post, we explain timeboxes and how they are used.\nDefinition of timebox\nA timebox is a fixed unit of development capacity. An easy way to visualize a timebox is as a two-dimensional graph. Along the vertical axis is the cost of the development team (per unit time). Along the horizontal axis is time. The longer an iteration is, the wider a timebox is.\nThe important thing to notice is that with Cost and Time fixed, the capacity of the timebox is fixed. There is only so much that can be accomplished with a given team and a given amount of time.\nA unit of work\nA unit of work represents both the functionality being delivered and the quality of the functionality. This is the concept that many managers do not grasp.\nTo deliver the functionality that supports any particular requirement, we can think of the time as having two tightly-linked components: implementing the functionality, and implementing the functionality with good quality. Poorly written code, for an isolated requirement, can take less time than well written code. The extra time spent doing it right is part of the quality component. Writing tests and documentation (when appropriate) are also part of the quality component.\nHow big should a timebox be?\nWith some exceptions, anywhere from 2 to 4 weeks. Smaller, more tightly knit teams can operate with shorter timeboxes. Teams with less release-process overhead can operate cost-effectively with smaller timeboxes. There’s a good post and discussion at The Pragmatic Architect on how long to make timeboxes. Mishkin Berteig also has a good post, with some differing opinions, identifying the pros and cons of short iterations.\nWe think a good way to approach it is to start with a 3 week cycle and extend or shorten it, based on what our stakeholders prefer, balanced with the reality of our development environment. For larger teams, we usually end up with a 4 week cycle. Keep in mind that the length of the cycle can be changed as we get feedback on our process efficiency.\nFilling a timebox\nWe can fill up a timebox with the work-units representing several requirements. Ideally, they are the highest priority requirements. Different requirements will take different amounts of time to implement. We can visualize this in the following diagram, which shows a timebox with the “original” schedule.\nWe see that each work unit has both a functionality and a quality component. We don’t want to intentionally plan to deliver functionality without quality.\nDealing with new requirements\nThe previous posts on scheduling were about how to manage the deadlines for receiving change requests. In those posts, we didn’t talk about how to manage the schedule after receiving a request. There are four methods of adjusting the plan once a request has been approved and committed.\n- Sacrifice quality to increase functionality\n- Increase cost to increase functionality\n- Increase time to increase functionality\n- Delay some functionality to deliver other functionality\n1. Sacrifice quality to increase functionality\nWe can, and too many teams do, sacrifice quality to deliver extra functionality without impacting costs or delivery dates. When we take this approach, we incur a code-debt. A code-debt is us taking a loan against our code-base in the short term to resolve otherwise impossible constraints (no extra budget, can’t miss the deadline, can’t delay anything). Poor quality code comes with a long term cost. It introduces risk into the delivery, which is the cost of poor-quality. This risk manifests as a negative expected value (think of it as the interest on the loan). Poorly written code also makes it more expensive to write new code in the future (think of this as the principal on the loan). Until we invest time to fix the quality of the code (refactor, test, etc), we will continue to incur costs.\nThe following diagram shows what this would look like.\nWe have sacrificed quality on some requirements (work components) including the new (red) requirements in order to squeeze them into our timebox.\n2. Increase cost to increase functionality\nAnother approach is to increase the capacity of the team to meet increased demands. This can mean extra hours for the current team, re-tasking people from other projects to join the team, or bringing in contractors to temporarily increase capacity.\nThe following diagram shows that by increasing the cost (and shuffling requirements around visually) we can deliver more functionality without sacrificing quality.\nThere are always inefficiencies to adding capacity. If we add hours, people get burned out. If we add people, there is overhead in helping them get up to speed. The benefit of this approach is that we are not sacrificing quality or timing to be able to deliver the new requirements.\n3. Increase time to increase functionality\nWhen we have the ability to do so, extending a particular release may make sense. We can extend the period of a timebox, say from 4 weeks to 5 weeks, to incorporate additional functionality. The following diagram shows how a time extension creates more capacity for implementing the requirements.\nDeadlines are often arbitrary. We should always explore the possibility of delaying the end of the timebox. Don’t extend the timebox more than 50%, or we lose the benefits of having incremental delivery.\n4. Delay some functionality to deliver other functionality\nMany times, there are political ramifications to delaying the release. And budget constraints are more common than they were ten years ago. When faced with no ability to extend the time or increase the cost, we are faced with a decision. We either sacrifice quality, or delay other functionality. Since we’ve prioritized our requirements based upon the value they provide to the business, it is usually an easy decision.\nFirst we identify which previously scheduled requirements are lower priority than the new requirements. Then we understand which of those requirements has the lowest cost of delay. After confirming with our stakeholders, we delay those requirements to the next iteration. The following diagram shows this.\nWe’ve talked in the past about how scope-creep should be managed as a relationship, not a math exercise. When we establish rational deadlines for change requests, and then combine them with the four techniques here, we can provide our stakeholders with a number of choices.']	['<urn:uuid:b36bdb9d-80f1-4b6a-b62f-a54a1edf9ce1>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	9	54	1122
59	i study fungi effects on amphibians what are salamanders newts frogs vulnerability to chytrid fungus and how global spread happens	Different amphibians show varying vulnerability to chytrid fungi. The fungus Batrachochytrium salamandrivorans is very dangerous to salamanders and newts, but does not affect frogs, toads and snake-like amphibians called caecilians. The fungus originated in Southeast Asia (Thailand, Vietnam, and Japan) and has spread globally through the intercontinental movement of amphibians in the pet trade. Scientists have mapped the global emergence of another chytrid fungus, Batrachochytrium dendrobatidis, which shows different amphibian species have varying levels of susceptibility to infection.	"['|| powered by |\nEuropean salamanders and newts vulnerable to fungal disease from Asia31.10.2014 - (idw) Universität Zürich\nA skin-eating fungal disease brought to Europe by humans now poses a major threat to native salamanders and newts, scientists of the Universities of Zurich and Ghent University have warned. They say nations need to urgently consider appropriate biosecurity measures to stop the further spread of this pathogen.\nThe previously unknown fungus Batrachochytrium salamandrivorans was discovered last year by researchers investigating a huge crash in the population of fire salamanders in the Netherlands. Now the same team have screened over 5,000 amphibians from four continents to ascertain the threat the new disease presents to other species.The results, published today in the journal Science, show that B. salamandrivorans is very dangerous to salamanders and newts, but not to frogs, toads and snake-like amphibians called caecilians.\nThe fungus was found to be present in amphibians from Thailand, Vietnam and Japan as early as 1894, without causing disease, suggesting it originates from Southeast Asia. The fungus probably arrived in Europe recently, and its presence in traded amphibians suggests that the intercontinental movement of amphibians explains its introduction. So far the disease has only been found in the Netherlands and Belgium, but the researchers say it is likely to reach other European countries soon. The great crested newt, a protected and threatened species in Switzerland, is among the species that rapidly die once infected.\nNo disease in the place of origin\nThe study was led by Professors An Martel and Frank Pasmans at Ghent University in collaboration with Dr. Ursina Tobler and Benedikt Schmidt at the University of Zurich and KARCH, the Swiss Amphibian and Reptile Conservation Programme, and others. When a disease has been around for a long time, animals develop resistance to it. Globalisation has resulted in the movement of humans and animals all across the world, bringing pathogens into contact with hosts that havent had the opportunity to establish resistance. As a consequence, pathogens like B. salamandrivorans that are brought to a new environment can very rapidly threaten many species with extinction, said co-author Dr Benedikt Schmidt, from the Institute of Evolutionary Biology and Environmental Studies at the University of Zurich and KARCH, the Swiss Amphibian and Reptile Conservation Programme.\nBiosecurity measures needed\nAsian salamanders and newts are traded in large numbers across the globe. More than 2.3 million Chinese fire belly newts were imported into the US between 2001 and 2009. The researchers found that the fungus can easily be transmitted between salamanders of different species by direct contact. Co-author Dr Benedikt Schmidt, from the Institute of Evolutionary Biology and Environmental Studies at the University of Zurich and KARCH, the Swiss Amphibian and Reptile Conservation Programme, said: This study has shown the threat of importing exotic species without appropriate screening for infectious diseases. B. salamandrivorans poses an extreme risk to European amphibian biodiversity and nations need to urgently consider appropriate biosecurity measures to stop the further spread of this emerging pathogen.\nA. Martel, M. Blooi, C. Adriaensen, P. Van Rooij, W. Beukema, M.C. Fisher, R.A. Farrer, B.R. Schmidt, U. Tobler, K. Goka, K.R. Lips, C. Muletz, K. Zamudio, J. Bosch, S. Lötters, E. Wombwell, T.W.J. Garner, A.A. Cunningham, A. Spitzen-van der Sluijs, S. Salvidio, R. Ducatelle, K. Nishikawa, T.T. Nguyen, J.E. Kolby, I. Van Bocxlaer, F. Bossuyt, F. Pasmans: Recent introduction of a chytrid fungus endangers Western Palaearctic salamanders, in: Science. doi:10.1126/science.1258268\nFunding of the study\nThe study was funded (among others) by the Special Research Fund of Ghent University, the Royal Zoological Society of Antwerp, Dutch Ministry of Economic Affairs. Funding in Switzerland came from Vontobel Stiftung, Janggen-Pöhn Stiftung, Basler Stiftung für biologische Forschung, Stiftung Dr. Joachim De Giacomi, Zoo Zürich, Grün Stadt Zürich, European Union of Aquarium Curators, and Zürcher Tierschutz.\nDr Benedikt Schmidt\nInstitute of Evolutionary Biology and Environmental Studies\nUniversity of Zurich\nTel. +41 (32) 725 72 07, 078 719 69 16\nUniversity of Zurich\nTel. +41 44 634 44 39\nHTML-Code zum Verweis auf diese Seite:\n<a href=""http://www.uni-protokolle.de/nachrichten/id/287308/"">European salamanders and newts vulnerable to fungal disease from Asia </a>', 'Deanna (""Dede"") H. Olson\nResearch Ecologist, Team Leader\n3200 SW Jefferson Way\n3200 SW Jefferson Way\nContact Deanna (""Dede"") H. Olson\nMy research focuses on three themes: the ecology of amphibians and fishes in headwater stream basins in managed forests; the taxonomic and distribution patterns of the amphibian chytrid fungus; and the conservation biology of amphibians, reptiles, and turtles.\nConservation biology, behavioral ecology, population and community ecology. Effects of forest management practices, landscape designs, and policies on aquatic-dependent organisms. Interaction of climate variation with species\' ecology and land management practices. Development of new research paradigms engaging multiple partners.\nMy past studies have explored the reproductive behavioral ecology of frogs and toads.\nWhy This Research is Important\nOver one-third of amphibians and almost 50 percent of turtles are threatened with extinction worldwide, yet we know very little about their general ecology and the main threats that are affecting them. Both taxonomic groups are considered ideal indicators of environmental degradation owing to their reliance on both aquatic and terrestrial habitats and their central position in food webs. Ecological and conservation biology research can aid management decisions regarding these iconic faunal groups, preserving them for generations to come.\n- Oregon State University, Department of Zoology, Ph.D. Breeding ecology of amphibians in the Cascade Range of Oregon 1988\n- University of California, San Diego, B.A. Major: Biology; Concentration Area: Population Ecology; Minor: Philosophy 1980\n- PNW representative, Intermediate Management Group\n2012 - Current\nSurvey and Manage Program advisory group, Northwest Forest Plan\n- Lead, Aquatic Biodivesity in Forests, Division8.02.06, International Union for Forest Research Organizations\n2010 - Current\nOrganized sessions in Seoul at the IUFRO World Congress, and Cork at the Biodiversity subdivision conference.\n- Associate Editor, Herpetological Review, Society for the Study of Amphibians and Reptiles\n2000 - Current\nIn 2008, I initiated the Amphibian Diseases section of the journal.\n- Courtesy Faculty, Oregon State University\n1990 - Current\nI have held positions in the Departments of Zoology, Fisheries and Wildlife, Forest Science, Forest Ecosystems and Society, and the Environmental Science Program, and the program for BioResource Research. Primarily, I co-advise graduate students and mentor undergraduate students who are collaborating with me on research projects.\n- National Co-chair, Partners in Amphibian and Reptile Conservation\n2009 - 2012\nI wrote the 10-years of PARC report, 2011-State of the Turtle report, and 2012-State of the Lizard report. I helped initiate PARC\'s annual report process (2 new brochures: 2010 and 2011), two new annual awards (Alison Haskell Award, Visionary Leader Award), and PARC\'s ""year of"" campaigns.\n- Northwest Regional Co-chair, Partners in Amphibian and Reptile Conservation\n1999 - 2009\n- President, Society for Northwestern Vertebrate Biology\n2003 - 2005\n- Vice-President, Oregon, Society for Northwestern Vertebrate Biology\n2000 - 2003\n- Associate Editor, Northwestern Naturalist, Society for Northwestern Vertebrate Biology\n1993 - 2000\nAwards & Recognition\n- Alison Haskell Award for Excellence in Herpetological Conservation, 2012\nThis is a national award given by Partners in Amphibian and Reptile Conservation to an individual who has been an ""unsung hero"" for herpetological conservation\n- USFS Senior Leader Program graduate, 2011\nThis is a national program designed to develop leaders in the US Forest Service.\n- PNW Station Director’s Award for Excellence: Distinguished Science Award, 2010\nFor sustained excellence in the fundamental study of amphibian ecology and the effective delivery of that knowledge to address practical management and policy issues.\n- Top 10 Most Cited Papers, 2010\nFor recent papers published by Elsevier, my paper (Olson et al. 2007) was in the Top 10 for the journal Forest Ecology and Management.\nFeatured Publications & Products\n- Xie, Gisselle Yang; Olson, Deanna H.; Blaustein, Andrew R. 2016. Projecting the global distribution of the emerging amphibian fungal pathogen, batrachochytrium dendrobatidis, based on IPCC climate futures.\n- Olson, Deanna H.; Aanensen, David M.; Ronnenberg, Kathryn L.; Powell, Christopher I.; Walker, Susan F.; Bielby, Jon; Garner, Trenton W.J.; Weaver, George; Fisher, Matthew C. 2013. Mapping the global emergence of Batrachochytrium dendrobatidis, the amphibian chytrid fungus.\n- Searle, C.L.; Gervasi, S.S.; Hua, J.; Hammond, J.I.; Relyea, R.A.; Olson, D.H.; Blaustein, A.R. 2011. Differential host susceptibility to Batrachochytrium dendrobatidis, an emerging amphibian pathogen.\n- Grant, Evan H Campbell; Muths, Erin; Katz, Rachel A; Canessa, Stefano; Adams, Michael J; Ballard, Jennifer R; Berger, Lee; Briggs, Cheryl J; Coleman, Jeremy TH; Gray, Matthew J; Harris, M Camille; Harris, Reid N; Hossack, Blake; Huyvaert, Kathryn P; Kolby, Jonathan; Lips, Karen R; Lovich, Robert E; McCallum, Hamish I; Mendelson, Joseph R; Nanjappa, Priya; Olson, Deanna H; Powers, Jenny G; Richgels, Katherine LD; Russell, Robin E; Schmidt, Benedikt R; Spitzen-van der Sluijs, Annemarieke; Watry, Mary Kay; Woodhams, Douglas C; White, C LeAnn. 2017. Using decision analysis to support proactive management of emerging infectious wildlife diseases.\n- Olson, Deanna H.; Burton, Julia I. 2014. Near-term effects of repeated-thinning with riparian buffers on headwater stream vertebrates and habitats in Oregon, USA.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2009. Downed wood microclimates and their potential impact on plethodontid salamander habitat in the Oregon Coast Range.\n- Olson, Deanna H.; Burnett, Kelly M. 2009. Design and management of linkage areas across headwater drainages to conserve biodiversity in forest ecosystems.\n- Shoo, Luke P.; Olson, Deanna H.; McMenamin, Sarah K.; Murray, Kris A.; VanSluys, Monique; Donnelly, Maureen A.; Stratford, Danial; Terhivuo, Juhani; Merino-Viteri, Andres; Herbert, Sarah M.; Bishop, Phillip J.; Corn, Paul Stephen; Dovey, Liz; Griffiths, Richard A.; Lowe, Katrin; Mahony, Michael; McCallum, Hamish; Shuker, Jonathan D.; Simpkins, Clay; Skerratt, Lee F.; Williams, Stephen E.; Hero, Jean-Marc. 2011. Engineering a future for amphibians under climate change.\n- Kiester, A. Ross; Olson, Deanna H. 2011. Prime time for turtle conservation.\n- Olson, Deanna . 2019. A decade of herpetological disease papers: puzzle pieces of a bigger picture.\n- Emel, Sarah L.; Olson, Deanna H.; Knowles, L. Lacey; Storfer, Andrew . 2019. Comparative landscape genetics of two endemic torrent salamander species, Rhyacotriton kezeri and R. variegatus: implications for forest management and species conservation.\n- Mims, Meryl C.; Olson, Deanna H.; Pilliod, David S.; Dunham, Jason B. 2019. Considering the hidden threats of climate change: A new approach puts overlooked species on the conservation radar.\n- Watts, Andrea ; Olson, Dede ; Harris, Reid ; Mandica, Mark . 2019. The deadly amphibian bsal disease: How science-management partnerships are forestalling amphibian biodiversity losses.\n- Reeves, Gordon H.; Olson, Deanna H.; Wondzell, Steven M.; Bisson, Peter A.; Gordon, Sean ; Miller, Stephanie A.; Long, Jonathan W.; Furniss, Michael J. 2018. Chapter 7: The aquatic conservation strategy of the northwest forest plan A review of the relevant science after 23 years.\n- Works, Aaron J.; Olson, Deanna H. 2018. Diets of two nonnative freshwater turtle species (Trachemys scripta and Pelodiscus sinensis) in Kawai Nui Marsh, Hawaii.\n- Blaustein, Andrew ; Urbina, Jenny ; Snyder, Paul ; Reynolds, Emily ; Dang, Trang ; Hoverman, Jason ; Han, Barbara ; Olson, Deanna ; Searle, Catherine ; Hambalek, Natalie . 2018. Effects of emerging infectious diseases on amphibians: A review of experimental studies.\n- Gray, Matthew J.; Allender, Matthew C.; Haman, Katherine H.; Harris, Reid N.; Olson, Deanna H. 2018. Facilitating early detection and rapid response: An alert system to combat emerging herpetofaunal diseases.\n- Mims, Meryl C.; Olson, Deanna H.; Pilliod, David S.; Dunham, Jason B. 2018. Functional and geographic components of risk for climate sensitive vertebrates in the Pacific Northwest, USA.\n- Adams, Michael J.; Harris, Reid N.; Grant, Evan H. C.; Gray, Matthew J.; Camille Hopkins, M. ; Iverson, Samuel A.; Likens, Robert ; Mandica, Mark ; Olson, Deanna H.; Shepack, Alex ; Waddle, Hardin . 2018. Prepublication communication of research results.\n- Penaluna, Brooke E.; Olson, Deanna H.; Flitcroft, Rebecca L.; Weber, Matthew A.; Bellmore, J. Ryan; Wondzell, Steven M.; Dunham, Jason B.; Johnson, Sherri L.; Reeves, Gordon H. 2017. Aquatic biodiversity in forests: a weak link in ecosystem services resilience.\n- Roon, David; Dunham, Jason; Harvey, Bret; Bellmore, Ryan, J.; Olson, Deanna; Reeves, Gordon. 2017. Evaluating the ecological trade-offs of riparian thinning for headwater stream ecosystems in second-growth redwood forests.\n- Gervasi, Stephanie S.; Stephens, Patrick R.; Hua, Jessica; Searle, Catherine L.; Xie, Gisselle Yang; Urbina, Jenny; Olson, Deanna H.; Bancroft, Betsy A.; Weis, Virginia; Hammond, John I.; Relyea, Rick A.; Blaustein, Andrew R.; Lötters, Stefan. 2017. Linking ecology and epidemiology to understand predictors of multi-host responses to an emerging pathogen, the amphibian chytrid fungus.\n- Iwanowicz, Deborah D.; Schill, William B.; Olson, Deanna H.; Adams, Michael J.; Densmore, Christine; Cornman, R. Scott; Adams, Cynthia; Figiel, Jr., Chester.; Anderson, Chauncey W.; Blaustein, Andrew R.; Chestnut, Tara. 2017. Potential concerns with analytical methods used for the detection of Batrachochytrium salamandrivorans from archived DNA of amphibian swab samples, Oregon, USA.\n- Leach, J. A.; Olson, D. H.; Anderson, P. D.; Eskelson, B. N. I. 2017. Spatial and seasonal variability of forested headwater stream temperatures in western Oregon, USA.\n- Burton, Julia I.; Olson, Deanna H.; Puettmann, Klaus J. 2016. Effects of riparian buffer width on wood loading in headwater streams after repeated forest thinning.\n- Foster, Alex D.; Olson, Deanna H.; Jones, Lawrence L.C. 2015. A review of the biology and conservation of the Cope\'s giant salamander Dicamptodon copei Nussbaum, 1970 (Amphibia: Caudata: Dicamptodontidae) in the Pacific northwestern region of the USA.\n- Gray, Matthew J.; Lewis, James P.; Nanjappa, Priya; Klocke, Blake; Pasmans, Frank; Martel, An; Stephen, Craig; Parra Olea, Gabriela; Smith, Scott A.; Sacerdote-Velat, Allison; Christman, Michelle R.; Williams, Jennifer M.; Olson, Deanna H.; Hogan, Deborah A. 2015. Batrachochytrium salamandrivorans: The North American response and a call for action.\n- Olson, Deanna. 2015. Contributor to chapter 1, Amphibian Conservation. In: What works in conservation: 2015.\n- Bradley, Paul W.; Gervasi, Stephanie S.; Hua, Jessica; Cothran, Rickey D.; Relyea, Rick A.; Olson, Deanna H.; Blaustein, Andrew R. 2015. Differences in sensitivity to the fungal pathogen Batrachochytrium dendrobatidis among amphibian populations.\n- Mazza, Rhonda; Olson, Deanna (Dede). 2015. Heed the head: buffer benefits along headwater streams.\n- Olson, Deanna; Crisafulli, Charlie. 2014. Conservation Assessment for the Van Dyke\'s Salamander (Plethodon vandykei) Version 1.0.\n- Foster, Alex ; Olson, Deanna H. 2014. Conservation assessment for the Cope s Giant Salamander (Dicamptodon copei) Version 1.0.\n- Chestnut, Tara; Anderson, Chauncey; Popa, Radu; Blaustein, Andrew R.; Voytek, Mary; Olson, Deanna H.; Kirshtein, Julie. 2014. Heterogeneous occupancy and density estimates of the pathogenic fungus Batrachochytrium dendrobatidis in waters of North America.\n- Ruzicka, Kenneth J., Jr..; Puettmann, Klaus J.; Olson, Deanna H. 2014. Management of riparian buffers: upslope thinning with downslope impacts.\n- Barela, Kimberly L.; Olson, Deanna H. 2014. Mapping the western pond turtle (Actinemys marmorata) and painted turtle (Chrysemys picta) in western North America.\n- Olson, Deanna H.; Kluber, Matthew R. 2014. Plethodontid salamander distributions in managed forest headwaters in western Oregon.\n- Olson, Deanna H.; Leirness, Jeffery B.; Cunningham, Patrick G.; Ashley Steel, E. 2014. Riparian buffers and forest thinning: Effects on headwater vertebrates 10 years after thinning.\n- Olson, Deanna H.; Chestnut, Tara. 2014. Trouble in the aquatic world: how wildlife professionals are battling amphibian declines.\n- Burton, Julia I.; Ares, Adrian; Mulford, Sara E.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Above-ground carbon storage, downed wood, and understory plant species richness after thinning in western Oregon.\n- Wessell-Kelly, Stephanie J.; Olson, Deanna H. 2013. Community structure of vascular plants, arthropods, amphibians, and mollusks in managed forests of the Pacific Northwest (USA).\n- Olson, Deanna H.; Burnett, Kelly M. 2013. Geometry of forest landscape connectivity: pathways for persistence.\n- Burton, Julia I.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Headwater stream flow, climate variation, and riparian buffers with thinning in western Oregon.\n- Gervasi, Stephanie; Gondhalekar, Carmen; Olson, Deanna H.; Blaustein, Andrew R. 2013. Host identity matters in the amphibian-Batrachochytrium dendrobatidis system: fine-scale patterns of variation in responses to a multi-host pathogen.\n- Anderson, Paul D.; Olson, Deanna H.; Ares, Adrian. 2013. Initial riparian down wood dynamics in relation to thinning and buffer width.\n- Wessell-Kelly, Stephanie J.; Olson, Deanna H. 2013. Leave islands as refugia for low-mobility species in managed forest mosaics.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Log and soil temperature profiles in managed headwater sub-basins in the Oregon coast range: implications for wildlife habitat.\n- Burton, Julia I.; Ares, Adrian; Olson, Deanna H.; Puettmann, Klaus J. 2013. Management trade-off between aboveground carbon storage and understory plant species richness in temperate forests.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2013. No effects of thinning with riparian buffers on terrestrial salamanders in headwater forests 5 to 6 years post-harvest in western Oregon.\n- Kluber, Matthew R.; Olson, Deanna H. 2013. Plethodontid salamander population ecology in managed forest headwaters in the Oregon coast range.\n- Olson, Deanna H. 2013. Riparian buffers and thinning in headwater drainages in western Oregon: aquatic vertebrates and habitats.\n- Ruzicka, Kenneth J., Jr..; Olson, Deanna H.; Puettmann, Klaus J. 2013. The intertwining paths of the density managment and riparian buffer study and the Northwest Forest Plan.\n- Ares, Adrian; Olson, Deanna H.; Puettmann, Klaus J. 2013. Thinning and riparian buffer configuration effects on down wood abundance in headwater streams in coniferous forests.\n- Parks, Noreen; Olson, Deanna H. 2011. Engineering a future for amphibians under a changing climate.\n- Paoletti, David J.; Olson, Deanna H.; Blaustein, Andrew R. 2011. Responses of foothill yellow-legged frog (Rana boylii) larvae to an introduced predator.\n- Bancroft, Betsy A.; Han, Barbara A.; Searle, Catherine L.; Biga, Lindsay M.; Olson, Deanna H.; Kats, Lee B.; Lawler, Joshua J.; Blaustein, Andrew R. 2011. Species-level correlates of susceptibility to the pathogenic amphibian fungus Batrachochytrium dendrobatidis in the United States.\n- Oliver, Marie; Burnett, Kelly; Olson, Deanna. 2010. Linked in: connecting riparian areas to support forest biodiversity.\n- Thompson, Jonathan; Raphael, Martin; Molina, Randy; Marcot, Bruce; Olson, Deanna. 2008. Conservation of biological diversity: all things considered.\n|Conservation priorities identified for Northwest amphibians and reptiles|\nAt a reptile conservation conference, scientists and natural resource managers synthesized conservation concerns and priorities for 105 species ...\n|People, Forests, and Change: Lessons from the Pacific Northwest|\nA new book explores the past four decades of change in forest science and management in the Pacific Northwest\n|Response to Emerging Infectious Amphibian Diseases Forges New Alliances Between Science, Management, and Policy|\nThe newly described salamander chytrid fungus Batrachochytrium salamandrivorans (Bsal) is rapidly spreading in Europe, killing salamanders as it ...\n|Scientists Determine Ideal Buffer Width to Sustain Aquatic and Riparian Resources Along Headwater Streams|\nHow wide does a riparian buffer need to be to maintain aquatic and riparian habitat in and along forested headwater streams when upland forest t ...\n|Scientists Link Amphibian Fungus to Increasing Temperature Range|\nScientists find the odds of fungus occurrence decreased with increasing temperature range at a site, linking disease emergence to climatic consi ...\n|Strategically linking headwater habitats across ridgelines benefits amphibians and management|\nFederal biologists, land managers, and watershed stewardship councils are interested in this new design that maintains amphibian habitat and whi ...']"	['<urn:uuid:12838312-a1f2-44fb-a316-c2cdd4707356>', '<urn:uuid:6cd29691-2c1b-4acd-b272-629317204b85>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:37:09.743768	20	78	3164
60	difference guilt shame addiction recovery meaning	Guilt and shame are distinct emotions in addiction recovery. Guilt is an emotional state that occurs when a person feels they have failed to live up to morals set by themselves or others - implying that a good person did a bad thing. Shame, on the other hand, makes a person feel they are a 'bad person' for their past actions or experiences. Shame is more damaging to the recovery process as it diminishes self-esteem and self-efficacy and can lead to addiction relapse.	['People battling addiction often feel shame due to intense feelings of self-hatred and inferiority, and the societal stigma attached to substance abuse.\nWhen shame is not confronted and dealt with in a productive way, it may lead people to continue misusing substances, creating a dangerous cycle of shame and addiction.\nThere are several ways to release the shame of drug and alcohol abuse and stay on track during the addiction recovery process.\nCauses Of Shame In Recovery\nIt is common for people with substance use disorders (SUDs) to feel a constant, lingering sense of shame. The causes of shame will be different for every person.\nShame may be a result of:\n- traumatic experiences such as sexual or physical abuse\n- exposure to the negativity associated with mental illness or substance use\n- actions that someone may have taken while abusing drugs or alcohol\n- having a family member who abused substances\n- experiencing poverty and addiction\n- being on the receiving end of hostile or harsh parenting\n- being in a codependent relationship with someone who enables substance abuse\nWhile it is natural to feel ashamed, if left unacknowledged it could lead to further substance use.\nWhy Feeling Ashamed Of Addiction Is Dangerous\nDwelling in shame while in addiction recovery is dangerous, as it can cause self-destructive thoughts, anxiety, depression, and other mental health issues.\nEven if feelings of inadequacy don’t cause a diagnosable mental illness, they can prevent healing by making people want to use drugs or alcohol to mask their negative emotions.\nHow To Let Go Of The Shame Of Addiction\nThere are several ways to break the cycle of shame and addiction and begin the process of healing.\nPeople experiencing shame may not know the root cause of their negative emotions, as leading theories on shame say that the tendency to feel shame often develops during infancy or before the age of 2.\nHowever, outside factors such as friends or family members who minimize the seriousness of addiction may contribute to shame.\nRecognizing feelings of shame and the limiting beliefs that go along with these feelings can be essential in the recovery process and can be accomplished through addiction therapy.\nForgiving yourself for a habit of harsh self-criticism or for questionable things you’ve done in the past is a long and difficult process.\nIt may become clear over time that it’s not constructive to dwell on negative emotions or on what you’ve done. The only thing that ultimately matters is your actions today and in the future.\nAsk For Forgiveness From Others\nOne of the most important steps in the recovery process is asking for forgiveness from the people who were wronged because of your actions.\nThe choice to ask for forgiveness is a courageous one, and while people may not be able to forgive immediately, asking for their forgiveness is one way to put your past behind you.\nTaking responsibility for your actions can be a great way to raise your self-esteem, something that people dealing with shame often have to learn how to do.\nRecognize What You Cannot Control\nPeople are ultimately only in control of themselves, and the past is unalterable. Lingering on the past, whether something you did or something that was done to you, will only hold you back in recovery.\nThe Difference Between Guilt And Shame\nThere are notable differences between guilt and shame that people in recovery are taught to understand.\nGuilt can be defined as an emotional state that occurs when a person feels they have failed to live up to morals set by themselves or others.\nShame, on the other hand, can make a person feel that they are a “bad person” for what has happened to them in the past, including actions they’ve taken.\nOne way to think about the difference between guilt and shame is that guilt implies that a good person did a bad thing, while shame is the unhealthy sensation of being an irredeemable person at the core.\nBoth of these emotions are normal, but shame is undoubtedly more damaging to the recovery process as it diminishes self-esteem and self-efficacy and can lead to addiction relapse.\nWhat Happens When Shame Is Released In Addiction Recovery\nIn an addiction treatment program, licensed therapists can help people overcome shame by providing a safe space to express vulnerability and honestly discuss their feelings.\nCommon therapeutic techniques such as cognitive behavioral therapy (CBT) and dialectical behavior therapy (DBT) will help people change their thinking patterns and behavior.\nBehavioral therapy is an effective way to help people address their feelings of shame. It can also uncover and address the role of trauma in addiction.\nOver time, people facing addiction can learn to accept their past, build their self-worth, forgive themselves for their transgressions, and realize that they’re worthy of a happy, productive life.\nOther Substance Abuse Treatment Options\nThere are an array of other treatment methods that can address mental health issues and dependency on drugs and alcohol.\nBehavioral health services may include:\n- medical detox\n- medication-assisted treatment (MAT) for opioid drug addiction\n- dual diagnosis treatment for people with co-occurring mental health disorders\n- sober living homes\nFind An Addiction Treatment Center Today\nReach out to FreeRehabCenters.net today and take the first step toward lasting sobriety.\nPublished on May 4, 2023\nFree Rehab Centers aims to provide only the most current, accurate information in regards to addiction and addiction treatment, which means we only reference the most credible sources available.\nThese include peer-reviewed journals, government entities and academic institutions, and leaders in addiction healthcare and advocacy. Learn more about how we safeguard our content by viewing our editorial policy.\n- National Institute on Drug Abuse (NIDA)\n- National Institutes of Health (NIH)\n- National Institutes of Health (NIH)\n- Psychology Today']	['<urn:uuid:a1933e07-27f5-40c9-aa6d-2b90ae7122e6>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	6	83	965
61	who first created concert without borders in what year and what concert halls does frank gehry design	Concert Without Borders was first conceived by Bisbee musician and Cochise College music instructor Lori Keyne, who started the Binational Chorus 'Música Sin Fronteras' in 2005, with the concert series itself beginning in 2012. Frank Gehry designed Chicago's Jay Pritzker Pavilion in Millennium Park and the Walt Disney Concert Hall in Los Angeles.	['concert without borders\nBNAI’s signature project, Concert Without Borders, is a unique bi-national celebration of arts and music and community.\nThe idea was first conceived by Bisbee musician and Cochise College music instructor Lori Keyne, who is choir director of the Binational Chorus “Música Sin Fronteras” she launched in 2005. Along with her husband, Seth Polley, and other local activists and performers, thoughts of “transcending the border” grew into the series of concerts. “There’s been a lot of people involved,” Keyne says. “Too many to list.”\nIn this annual event, musicians and communities gather at both sides of the U.S./Mexican border, divided by a political barrier but unified by something greater—the gift of music and dance, the transformative power of creativity and spirit. We come together not to protest but to harmonize, combining the beat in musical time with the strong beat of hearts committed to the common good.\nThe concert provides an outstanding and unique example of public art, which incorporates Dance, Music, and Visual Arts as well as Children’s Art Programs through the children’s art-making activities and local artists show their work.\nThe Concert Without Borders is an ongoing project and began in 2012 and presents a broad range of musical styles and forms. The Concert continues to grow.\nThis event is free and open to the public and is being widely publicized throughout US and Mexico, both on social media and through TV channels, press and internet.\n2020 ONLINE EDITION\nThe 7th Edition of Concert Without Borders will be held online on Tuesday, September 15 at 7:00 p.m.\nConcert Without Borders takes place simultaneously in the states of Sonora and Arizona every year. The event is located immediately to the west of the Douglas/Agua Prieta port-of-entry in the cities of Douglas, AZ and Agua Prieta, SON.\nThe audience includes the borderland communities of Bisbee, Douglas, Sierra Vista, Elfrida, Agua Prieta, Naco, Cananea, Hermosillo, Phoenix and Tucson.\nFrom initial planning through performance, the Concert is a collaboration involving a broad range of community and government actors on both sides. The Concert continues to be an important means of building binational trust and working relationships that can be carried over to other community-building educational and cultural activities.\nConcert Without Borders is a collaboration between BNAI and the cities of Douglas, Agua Prieta, the Mexican Consulate, and Cochise College. This collaboration has grown into a truly bi-national event, with musicians performing at the same time on both sides of the international border wall. The event has attracted national press and visitors from as far as Hermosillo and Phoenix. It inspired a similar concert on the border of Tijuana in 2017, as well as a concert performed in Agua Prieta, Sonora, by the Mexican Federal Police Band and a mariachi group from Mexico City.\nSince 2012, more than 120 artists have performed in this event.\nPerformers include world-renowned classical guitarist Dieter Hennings, Coral Desierto, Agua Prieta’s Orquesta Municipal, the Binational Chorus “Música Sin Fronteras”, Tin Can Tourists, Cochise College Orchestra, the Essential Elements, with John Cordes, Caitlin Strachan, Evan Waters, Francisco Barrios, Cleve Jones, and Lori Keyne, the Agua Prieta Blues Band, the Orlando Suárez Blues Band, Dan Simonis & West Texas Millionaires, members of the Orquesta Sinfónica of Hermosillo, Nuevo Mundo Early Music Ensemble, Grupo Sin Miedo, Lowell Middle School Chorus, and many, many others.\nThank you all!', 'Often the acoustics pose challenges for architects and motivates them to use their imagination and creativity. Architecture forms may integrate use of advanced technologies in order to enhance acoustics in modern concert halls. The key role of advanced technologies that are used in architecture is to enhance acoustic experience at a live performance concert venue. Acoustic design has evolved over the years to allow sound engineers and architects to avoid any sound disasters. Electronic technologies and computer aided software have the power to solve many acoustical problems by allowing them to access the acoustic performance as early as design stage. Technologies such as ray tracing that use a computer technique to visualize how sound waves travel through space, and auralization that helps architects to hear how buildings may sound in the design phase have contributed in the enhancement of acoustics to a great deal (Brensilver, 2004). The current essay analyzes two concert halls/outdoor venue space designed by Frank Gehry including Chicago’s Jay Pritzker Pavilion, and Walt Disney Concert Hall in Los Angeles.\nChicago’s Jay Pritzker Pavilion\nChicago has always been famous for its skyscraper buildings and beautiful architecture. The city has been able to attract many visitors to Millennium Park that features an open concert stage that has been designed by architect Frank Gehry (Cornwell, 2006). Jay Pritzker Pavilion as a part of the architecture has a soaring, curving stainless steel body with an open concert value. The pavilion has a 120 foot high billowing headdress made of brushed steel ribbons that make the stage similar to a post-modern proscenium (Cornwell, 2006). It is favorable for large audiences with 4000 fixed seats and a large lawn to accommodate 7000 more people. The key attraction is the innovative sound system which is first of its kind with an ability to mimic the effects of an indoor concert hall by using the technique of distributing sound equally over fixed seats and lawn. The three part structure including a stage house with perforated Douglas fir for the orchestra and choir, a steel plated shell designed to deflect and contain sound and intersecting steel pipes which extend over 90m in the lawn with lights and speakers allow an even distribution of digital sound in the hall (Webb, 2004). The hall has an added acoustic enhancement system that provide virtual walls and also has the capability to mask city noise. Speakers are placed after every 60 feet for the lawn audience consisting of enhancement as well as reinforcement speakers. The lateral speakers supplement sound while directing it towards the center for wall like reflections. Sound also extends to the lawn and also comes back to the performers. The hall has a computerized delay system that synchronizes sound from different locations at different time intervals (Napoleon, 2000). Since the stadium acoustics distribute the sound evenly over fixed seats and lawns, audience can choose to be seated according to their choices (Malnar & Hays, 2006). The trellis with speakers over the audience allows sound to reach a large number of people (Dawson).\nIt is considered as one of the most complex concert venue in US with a capacity to hold more than 12000 people at the same time. The technological innovation used in this hall is known as LARES system that creates reflected and reverberant energy which surrounds an audience in an indoor venue (Delacoma, 2005). Pritzker Pavilion is the first outdoor concert hall in US to adopt LARES system in US. It is with these LARES system sound technology, that sound can effectively reach a large audience in the hall. LARES system cancels any echoing effects and makes the listeners feel as if they are listening to the sound such as live orchestra from a huge distance (Chicagoarchitecture.info, 2012). Sound amplification in the outdoor concert area wonderfully enhances the reach of sound (Rhein, 2004). Douglas fir around seats of orchestra and chorus allow them to hear each other with clarity (Kamin, 2004).\nWalt Disney Concert Hall in Los Angeles\nWalt Disney Concert Hall is located within a two layered plastered box that serves the purpose of acoustic shell and sound proofing (Goldberger, 2003). The walls and ceilings of the hall have Douglas fir wood with oak floor to enhance acoustics (The American Institute of Architects, 2015).\nWhile designing larger halls, it is a challenge for the architects to seat more people and also provide a stylish design with quality acoustics. Using shoebox like designs won’t work since sidewalls shift farther from each other and thus reflections take longer to reach certain seats in the hall. Architect Frank Gehry designed a vineyard shaped hall with terraced seating, central stage for higher capacity and higher interaction between sound players and the audience. A vineyard shaped auditorium was designed on the principle that concave shape focuses sound while convex scatters it (Krieger Products, 2015). The walls around Disney Hall are an optical illusion since they are made of transparent material to allow sound to easily pass through and reflect. 3D computer models and complex software were used to fine tune the concert hall (Vankin, 2013).\nThe acoustics of the hall are clear, transparent and preferable for large scale symphonic music as well as small to mid-scale works (Rahilly, 2009). Creators of Walt Disney Concert Hall used methods such as ultrasonic sound generators to test how sound waves travel and reflect in the halls (Backscatter, 2013). According to acoustician Yasuhisa Toyota, a balance of acoustics involves a correlation between the science of architecture and music art (Anderson, 2015).\nDuring the construction of Disney Hall, acoustic tests using one-tenth scale model were used. Each aspect had to be reduced by the same degree which demanded that frequency of sounds should be made ten-fold in order to reduce the wavelength to tenth of the usual and nitrogen was used to expel oxygen and water that absorb high frequency sounds (Walt Disney Concert Hall, 2015).\nOn the basis of the above discussion it is clear that advanced technologies can play an instrumental role in facilitating the use of innovative acoustic solutions that enable architects to offer exceptional sound experience to audience in concert halls. Leading architects have utilized various technologies to assess the quality of acoustics in their designs so as to modify the structures at an early stage of development. The practical examples of Chicago’s Jay Pritzker Pavilion, and Walt Disney Concert Hall in Los Angeles are brilliant cases of superior acoustics while supported by innovative designs.\nAnderson, A. (2015, May 14). The Sound of Music: acoustician Yasuhisa Toyota. Retrieved from Internationalartsmanager.com: http://www.internationalartsmanager.com/2015/05/the-sound-of-music-acoustician-yasuhisa-toyota/\nBackscatter. (2013, October 2). Mr. Yasuhisa Toyota-The Honest Engineer of Disney Hall. Retrieved from Rettacs.org: http://rettacs.org/mr-yasuhisa-toyota-the-honest-engineer-of-disney-hall/\nBrensilver, D. A. (2004). With Acoustics, No Longer Just Playing it by Ear. Architecture, 93(6), 28.\nChicagoarchitecture.info. (2012). The Pritzker Pavilion. Retrieved from Chicagoarchitecture.info: http://www.chicagoarchitecture.info/Building/1248/The-Pritzker-Pavilion.php\nCornwell, T. (2006, October 11). Millennium Park Burnushes Downtown. National Mortgage News, p. 22.\nDawson, J. (n.d.). Jay Pritzker Pavilion, Millennium Park. 16.\nDelacoma, W. (2005). The Jay Pritzker Pavilion Millennium Park Chicago, IL. Retrieved from Lareslexicon.com: http://www.lares-lexicon.com/millenium/millenium.html\nGoldberger, P. (2003). Good Vibrations the Skyline. The New Yorker, 79(28), 108-109.\nKamin, B. (2004, July 18). Jay Pritzker Pavilion; Randolph and Columbus Drives; Frank Gehry, Los Angeles. Chicago Tribune.\nKrieger Products. (2015). The Walt Disney Concert Hall: An Acoustical Wonder. Retrieved from Kriegerproducts.com: http://www.kriegerproducts.com/successstories/the-walt-disney-concert-hall/\nMalnar, J. M., & Hays, D. L. (2006). Millennium Park, Chicago: A Sensory Delight, Part 2. The Senses and Society, 1(2), 259-266.\nNapoleon, D. (2000). Indoors out. Entertainment Design, 34(11), 25-26.\nRahilly, L. (2009). The Architecture of Acoustics. Retrieved from Listenmusicmag.com: http://www.listenmusicmag.com/feature/the-architecture-of-acoustics.php?page=1\nRhein, J. v. (2004, July 18). Pavilion’s sound a work in progress. Retrieved from Chicagotribune.com: http://www.chicagotribune.com/entertainment/music/chi-0407180378jul18-story.html\nThe American Institute of Architects. (2015). Walt Disney Concert Hall. Retrieved from The American Institute of Architects: http://www.aia.org/cities/los-angeles/all-stories/disneyconcerthall/index.htm\nVankin, D. (2013, September 20). Disney Hall: Yasuhisa Toyota’s fluid, innovative approach to sound. Retrieved from Los Angeles Times: http://articles.latimes.com/2013/sep/20/entertainment/la-et-cm-walt-disney-concert-hall-yasuhisa-toyota-20130922\nWalt Disney Concert Hall. (2015). Acoustics. Retrieved from Wdch10.laphil.com: http://wdch10.laphil.com/wdch10/wdch/acoustics.html\nWebb, M. (2004). View from Chicago. The Architectural Review, 216(1293), 38-39.']	['<urn:uuid:f264ee39-5078-44de-9cd2-c8887f41e5f2>', '<urn:uuid:f6eae080-3bad-4b2c-95cd-08c72a3c7185>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-13T01:37:09.743768	17	53	1890
62	What's the difference between a FIREHAWK helicopter and a Single Engine Airtanker in terms of how much water they can carry for fighting fires?	The FIREHAWK helicopter is equipped with a 1,000-gallon water tank, while the Single Engine Airtanker (SEAT) can carry up to 800 gallons of water. The FIREHAWK can refill its tank in less than one minute using a retractable snorkel, and it can also transport up to 12 ground firefighters. In contrast, SEATs are the smallest category among firefighting aircraft and are primarily used for targeting fire hotspots in small areas with light fuels.	['Lockheed Martin, December 10, 2019 - ENGLEWOOD, Colorado – Sikorsky, a Lockheed Martin company, and United Rotorcraft, a division of Air Methods Corporation, announced today deliveries of three new S-70i™ FIREHAWK® helicopters to California fire agencies: one each to the Department of Forestry and Fire Protection (CAL FIRE), the Los Angeles County Fire Department (LACoFD), and the City of San Diego Fire-Rescue Department.\nConfigured by United Rotorcraft with a newly designed 1,000-gallon water tank attached to the belly of the aircraft; an extended landing gear to accommodate the tank; a retractable snorkel that can refill the tank in less than one minute; and a rescue hoist, the Firehawk helicopters will allow fire crews to attack wildfires, transport up to 12 ground firefighters to the fire line, and rescue firefighters and civilians when in need.\n“We thank CAL FIRE, the County of Los Angeles and the City of San Diego for their trust in the combined efforts of Sikorsky and United Rotorcraft to bring a life-saving aerial firefighting helicopter to the urban centers and forests of California,” said Jason Lambert, vice president of Sikorsky Global Military & Mission Systems. “Their fire fighters now have a powerful and proven helicopter that can attack fires at night, in strong Santa Ana winds, maneuver with agility and safety in canyons and terrain from sea level up to 10,000 ft. altitude, while dropping water with tremendous precision and force.”\nCAL FIRE’s newly acquired S-70i FIREHAWK helicopter is the first of nine aircraft currently on order, with an option for three more. CAL FIRE and San Diego are both first-time operators of the aerial firefighting helicopter.\nLACoFD has operated three S-70A model Firehawks since 2001. During the ensuing 19 years, the county has pioneered and perfected the tactics needed to suffocate wildfires in their early stage when a fire is most vulnerable, while working in close coordination with other aircraft and with firefighting crews on the ground. The county’s new S-70i FIREHAWK aircraft is the first of two — more powerful and faster than the S-70A model — that LACoFD is adding to its fleet.\n“With the increase in wildfire danger, the FIREHAWK has never been more relevant to the State of California,” said Mike Slattery, president of United Rotorcraft. “This capability brings unmatched multi-mission capability to these firefighting agencies to protect lives and property.”\nUnited Rotorcraft contracted with KAWAK Aviation Technologies (Bend, Oregon) to design and manufacture the water tank to new specifications based on input from LACoFD. Among other equipment on each aircraft are a 600 lb. external rescue hoist, a 9,000 lb. cargo hook with load cell system, forward recognition lights and a high intensity searchlight. Avionics installations include tactical communications and navigation systems.\nA Firehawk begins life at Stratford-Connecticut-based Sikorsky as an S-70i Black Hawk helicopter. United Rotorcraft then converts the twin-engine aircraft to its aerial firefighting configuration. The aircraft retains its multi-mission capability, allowing aircrew to perform search and rescue missions, medical evacuation, and carry cargo either internally or as an external load.\nAbout Lockheed Martin: Headquartered in Bethesda, Maryland, Lockheed Martin (NYSE: LMT) is a global security and aerospace company that employs approximately 105,000 people worldwide and is principally engaged in the research, design, development, manufacture, integration and sustainment of advanced technology systems, products and services.', 'Airplanes are invaluable to lots of operations in the modern world, and firefighting is one area that keeps them very busy in the hot summer months in some parts of the world. There are different types of firefighting planes for different purposes and budgets. Let’s take a look at seven different types of firefighting airplanes\nTable of Contents\nAlthough many people think of large water tankers when they imagine firefighting airplanes, there are in fact a variety of different aircraft that can be used for this purpose. Some planes carry huge amounts of water and can extinguish fires quickly, while others use specialized equipment to put out fires in difficult-to-reach places. In addition to fighting wildfires, these planes can also be used for rescuing people and putting out smaller fires.\nCombating wildfire without a firefighting plane is like building a castle in the air. Firefighting planes use a variety of chemicals to extinguish the wildfire. These chemicals include water, foams, and gels. These planes also utilize a chemical mixture of water, salt, thickening agent, and coloring, known as a retardant.\nSome firefighting planes have the capability of reloading over water bodies, leaving out the need to land for a refill.\nPlanes are always customized to meet specific needs. Hence, a single operation may require two or more types of these aircraft. Firefighting planes are usually classified by the capacity of retardant they carry and their application.\nNot all planes get directly involved in the dugout. Some are tactical, supplementing others. Let’s examine the different types of firefighting planes in use today.\n1. The Single Engine Airtanker (SEAT)\nThe single-engine air tanker, also called the type III SEAT, is the smallest category among firefighting planes. The plane holds not more than 800 gallons of water. Their small size makes them very easy to maneuver in rugged terrain. Besides, they do not require a long runway for them to take off.\nThese aircraft are best for wildfires caused by light fuels in small areas. They function mainly by targeting the fire hotspots, immediately checking their spread. Mostly, these planes utilize a retardant agent known as LC 95, which has a strong penetrating power to reach the soil. That slows down burning from the treetop to the bottom.\nSome models of SEATs, particularly the Fire Boss Air Tractor AT-802F are amphibious. This means they can land and take off both on water and land. The water tanks of SEATs can be either external or internal.\nAs expected, a pilot makes up the crew for these planes. What they do and the instructions they follow would depend on the SEAT pilot’s ratings. Some can perform both recesses and take action, while others follow instructions as charged from the base.\n2. Large Airtankers (LATs)\nLarge Airtankers are very efficient in putting out the fire even when there is a forest canopy due to their higher coverage. They have a larger capacity than the SEATs, holding between 2000 to 4000 gallons of fluids for firefighting.\nThey cover a much larger area than its preceding type and offer good support to firefighters on the ground. Areas with no refill stations pose a challenge for these aircraft since they may have to return to base after each delivery.\nDespite their less maneuverability, some of the planes are specially modified, allowing them to continue operating in adverse conditions. They are the majority in the collection of firefighting airplanes and work simultaneously with other planes. The flight crew consists of the pilot, the co-pilot, and a flight engineer.\nA major disadvantage of this type and larger ones is the need for larger landing spaces. Moreover, the logistics involved in their takeoffs means the response time is not as quick as smaller-sized planes.\n3. Very Large Airtankers (VLAT)\nFirefighting planes in the Very Large Airtankers category come in handy when fighting wildfires in countrified environs. They carry a massive quantity of water and fire retardants above 8000 gallons.\nSome, like the Boeing 747 Supertanker can even hold up to 24,000 gallons of fire suppressants!\nThese planes have superior and powerful engines, with robust structures to support the enormous weight of fluid they carry and fly fast at the same time. Some models like the DC-10 have a cruise speed of 600mph. Weird! Isn’t it?\nThey are high flying and can drop their payload at heights not less than 250ft above ground level or the canopy cover. VLATs fly to augment the actions of other firefighting planes during an operation.\nMost times, these planes have as many as three tanks. So the combined volume is what we often refer to in its holding capacity.\nMost VLATs are efficient and utilize gravity to empty the content in the tank within the twinkling of an eye. For instance, the DC-10 dispenses content within eight seconds.\nLike the LATs, no less than three crew members take part in a flight. The pilot and co-pilot are constants, and sometimes, two flight engineers are on board, making them four in all. Unfortunately, despite the high functionality of this plane, it is less maneuverable so it is not best for rugged terrain.\n4. Water Scoopers\nWater scoopers can hover over a water body for as long as they need to refill their tanks. In other words, they do not have to return to their base or a refilling station for a refill.\nSome refill using pumps let down into the water as they hover over a water body. While some like the Air Tractor Fire Boss use probes on either side of their keel to catch water. This method involves the pilot cutting down flight speed to half as it soars over the water. The process takes approximately 12 seconds, and the plane is ready for another deployment.\nWater scoopers are highly maneuverable, enabling them to dodge obstacles and draw in water from water bodies at least 6.5ft deep and about 300ft wide.\nTheir holding capacity is about 1600 gallons, which is perfect for quick refilling and multiple bombardments of fire. The aircraft’s special build allows it to fly for up to four hours in a single mission.\nThey do not fly too high as they have to soar not more than 150ft above ground level to release their contents. They do this to be as accurate as possible since their target is often the head of the fire or burning fuel to curtail the spreading flame as quickly as possible. Apart from the Air Tractor Fire Boss, another brand of water scooper is the Bombardier CL-415.\n5. Smokejumper Aircraft\nUnlike previously listed aircraft, the smokejumper aircraft does not directly get involved in quenching wildfires. You can sum up their activity to transportation and support.\nThey carry the specially trained unit for wildfires where there is a need. These planes are most needed when there is no means of accessing the fire scene except by flight. As many as twelve smokejumpers can get to a fire scene at once riding in this plane. The cockpit crew consists of the pilot and co-pilot.\nMeanwhile, they also deliver cargo to the firefighters on the site during operations using a parachute. On some occasions, firefighters have to stay longer than planned. These aircraft are the go-to for providing continuous supplies.\nThe aircraft is high-flying, attaining up to 3000ft above ground level. Shorts Sherpa series C-23A and SD3-60, Dornier 228, and DeHavilland DH-6 300 series Twin Otter are some planes popular in this category.\n6. Lead Planes\nYou can call them spotter or first respondent in a wildfire operation. As their name implies, they are crucial when there is a need to assess a fire situation.\nLead planes are small aircraft with dual engines. They are highly maneuverable and can fly over different terrains. Sometimes, they soar in circles as the pilot accesses the situation below before clearing the larger planes to deposit their payload.\nThese planes are handy in getting the technical information that other aircraft need for the firefighting mission. You’ll find them ahead of other aircraft conducting the way to the site, thanks to their maneuverability. A trained lead pilot is the only crew.\nThey also mark the region where the air tanker pilots will need to drop suppressants. They do this by releasing white smoke or providing coordinates when the pilot judges it safe for the water bombers to get involved.\nThe most commonly used planes for this operation are the Beechcraft King Air series and the Twin Commander series 500 and 600.\n7. Air Attack Planes\nAir attack planes are primarily for tactical purposes. Apart from the pilot, the planes also carry the person who calls the shots in the firefighting operation (Air Tactical Group Supervisor). That makes a crew of two.\nThis plane does not only help with aerial reconnaissance surveys, they also dictate the point of accessing the fire as well the point where the retardant must be dispensed. Quickness and lightness are prime characteristics of these planes.\nThanks to their small size, they are highly maneuverable and have a quick altitude gain. These attributes enable them to circle over mountainous areas with ease.\nAgain, fuel consumption is minimal compared to others. This allows the plane to survive throughout the firefighting operation. The most commonly used planes are the Twin Commander 500 and 600 series.']	['<urn:uuid:05290544-620a-485a-a60f-93d50000f8af>', '<urn:uuid:15ea13ff-b868-46ad-97f2-fb72923fcbaa>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	24	73	2096
63	What traits make draft horses gentle giants and healing companions?	Draft horses like the Shire and Percheron are known for their immense strength combined with gentle temperaments. Shire horses are particularly renowned for their good nature, intelligence, and willingness to learn, making them suitable for handlers of all experience levels. Percherons are cooperative, intelligent, and quick learners, though slightly more spirited than Shires. These characteristics make these breeds excellent therapeutic animals, as evidenced by their use in equine-assisted counseling (EAC). Through EAC, these horses help clients develop various skills including communication, trust, and emotional regulation. Their size and gentle nature create unique therapeutic opportunities, requiring clients to develop concentration, creativity, and resourcefulness when working with such large yet docile animals.	['Renowned for their immense strength, gentle temperament, and stunning beauty, horses have served humanity in countless ways throughout history. Among the many breeds of horses, the Shire Horse and the Percheron stand out due to their unique physical characteristics, rich history, and high versatility. These two breeds, originating from different parts of the globe, have carved out their own unique places in human history. This piece delves deep into the world of Shire Horses and Percherons, their origins, physical characteristics, temperaments, health and lifespan, and and their current roles in society.\nTable of Contents (Horspedia)\nOrigin and History\nShire Horse: History and Origin\nThe Shire horse is a breed of draught horse that originates from England. Bred for their unimaginable strength and towering stature, Shire horses were the workhorses of the Middle Ages, used primarily for farming purposes, pulling carts, carrying knights into battle, and heavy hauling. Once known as the “Great Horse”, the ancestry of Shire horses can be traced back to the Black Horse of Flanders, which was used widely in medieval England for its highly prized tactical capabilities in warfare.\nThe Shire breed as we know it today began to take shape in the mid-18th century, when breeders started crossbreeding local draught horses with Dutch Friesian horses. The subsequent offspring, often referred to as “Bakewell Shires,” exhibited a combination of power, agility, and gentleness, defining the characteristics of the breed.\nNotably, the Industrial Revolution in the 19th century saw an increased reliance on Shire horses for heavy draught work in cities, such as pulling omnibuses and heavy carts filled with goods. However, the advent of mechanization in the early 20th century greatly reduced the need for these mighty horses, threatening their population and existence.\nPercheron: An Introduction and Overview of Its Origins\nThe story of the Percheron, a horse native to the Huisne river valley in France, including the historical region of Perche, intertwines closely with the narrative of the Shire horse. Yet, the exact lineage and origins of the Percheron remain somewhat elusive due to the scarcity of reliable early breed records.\nDespite the scant recorded history, breed development theories often cite Arabian bloodlines. Such theories speculate that the Percheron likely descends from Moroccan or Moorish horses that journeyed to Europe during the 8th century. Another plausible ancestor is the Boulonnais horse, a domesticated French draft breed utilized in warfare, agriculture, and goods transportation. These speculated Arabian and Boulonnais influences could account for the Percheron’s distinctive stamina, muscular strength, and its comparatively refined and lighter build relative to other draft horses.\nThe Percheron secured its place in the limelight during the 19th century as it became a common sight on the streets of Paris for urban transportation. This popularity led to significant exports to the United States later in the 1800s, where the breed was prized for similar roles in agriculture and transport. Despite the sizable population reductions following the advent of mechanization in farming and transportation, the Percheron has sustained a steady presence, due to its adaptability and utility in diverse areas, spanning from agriculture and logging to horse-drawn carriage services.\nPhysical Traits: A Comparison between the Shire Horse and the Percheron\nWhen considering draft horse breeds, the Shire and the Percheron stand out as prominent figures, recognized for their immense size and formidable strength. They played a crucial role in various contexts over the centuries, ranging from agricultural tasks and transportation to military applications. Though seemingly similar at first glance, each breed holds distinct physical attributes that differentiate them from each other.\nDating back to the Roman times, the Shire horse is often considered one of the largest and strongest horse breeds in the world. Standing tall, on average, a male Shire horse (stallion) height ranges between 17 and 19 hands (approximately 68 to 76 inches), and a female (mare) stands around 16 to 17 hands (64 to 68 inches) tall.\nIn terms of weight, a mature Shire horse usually measures between 1,800 to 2,200 pounds. However, records show that some male Shires have even tipped the scale at a massive 2,400 pounds.\nThe coats of Shire horses traditionally come in black, bay, or brown, but grey Shires are also not uncommon. One of the distinguishing features of a Shire horse is its dense feathering around the lower legs, providing a flamboyant fluff over their large hooves.\nOne spectacular characteristic of Shire horses is their powerful, arching necks, which effortlessly hold high their long, noble heads. Looking at their overall appearance, Shires display a combination of raw power under control, gentleness, and hardiness.\nThe Percheron breed originates from the Huisne river valley in France and was developed during the medieval era. The average height of a Percheron ranges between 15.1 to 18.1 hands (around 61 to 73 inches), placing them a bit shorter than Shires on average.\nWeight-wise, a mature Percheron might range anywhere from 1,900 to 2,600 pounds. Although this range overlaps with that of the Shire, it tends toward a slightly higher average weight, which is especially true for draft or farm work-style Percherons.\nPercherons have a great variety of coat colors, including black and grey primarily, but you may also find bay, roan, chestnut and occasionally a sorrel Percheron. Unlike the Shire, Percherons are not heavily feathered, their lower legs are clean, elegant, and sport only a light feathering around the hoof, which further emphasizes their muscular legs.\nIn terms of their overall appearance, Percherons exhibit an air of elegance and agility that is distinctive to their breed. They are appreciated for their noble heads, intelligence displayed through their alert eyes, power-packed physique, yet a remarkably graceful gait for a large horse breed.\nBoth the Shire and the Percheron horses, known for their immense strength, have each played an important role in history and continue to hold a distinguished presence today. Similar, yet distinctly unique, they each boast a collection of traits that set them apart in the spectrum of horse breeds.\nTemperament and Behavior\nShire Horse: The Gentle Giant\nThe Shire Horse, renowned for its strength and endurance, boasts a calm and gentle temperament. Traditionally, they have been employed for arduous tasks such as hauling heavy loads, fostering a legacy of patience, tenacity, and notable strength.\nDespite their intimidating size, these gentle giants are known for their good-nature, intelligence, and innate willingness to learn. Their easy-going personalities make them suitable companions for a range of horse handlers, from the novice to the experienced, with careful consideration to their amiable power and size.\nAdaptable and pleasurable to ride, the Shire horse stands as an excellent choice for those seeking a gentle yet resilient equine companion.\nThe Percheron: The Agile Workhorse\nThe Percheron, like the Shire Horse, originated as a warhorse. Over time, it became favored for its strength and stamina in hauling heavy goods. As such, these horses are characterized by their spirit and ambition.\nPercherons are generally cooperative and gentle horses. They are intelligent and known to be quite quick learners, which allows their handlers to train them relatively easily. Despite their size, they’re notably light on their feet and can exhibit elegant movements when galloping or trotting.\nPercherons also possess a strong instinctive drive and curiosity. This can make them more spirited and energetic than the Shire, so they might be a better match for experienced handlers who can guide the horse in a positive direction. The Percheron’s lively nature can also translate into energetic, enthusiastic responses during training, making them exciting horses to work with.\nComparing Shire and Percheron Horses\nIf you’re deciding between a Shire and a Percheron Horse, you’ll soon discover they both have their own unique traits. Shire and Percheron breeds are famous for their gentle, affable nature, hard work, and intelligence. However, the distinctions between their temperaments may make one breed more appealing than the other, depending on individual preference and experience.\nThe Shire Horse, often known for its laid-back demeanor, might be a more suitable choice for novice riders or those searching for a tranquil, patient horse. In contrast, the spirited and energetic Percheron could be a more fitting option for experienced handlers, offering more vibrancy and enthusiasm.\nIn the end, the decision between these two gentle giants depends on the individual requirements, competency, and personal preference of the horse owner or rider. Shire and Percheron Horses both make superb partners regardless of whether they’re being used for work, riding, or companionship.\nHealth and Lifespan\nUnderstanding the Health and Lifespan of Shire Horses\nShire Horses from England typically live up to 25 to 30 years, although it’s not uncommon for some to live longer with the right care, even into their early 40s. Even though Shire Horses generally have substantial health, potential owners should be knowledgeable about a few breed-specific health conditions.\nA prevalent health concern for Shire Horses is Chronic Progressive Lymphedema (CPL). This ailment causes consistent swelling and hardening of the lower legs, leading to discomfort and possibly lameness. In severe cases, it can cause significant physical changes.\nAs well as CPL, Shire Horses might be susceptible to Equine Metabolic Syndrome (EMS), an endocrine disease causing obesity, insulin resistance, and even laminitis, which inflames a horse’s foot’s soft tissues. Prevention of EMS requires owners to keep their horse’s diet and exercise routine regulated and balanced.\nPercheron: Health and Lifespan\nPercherons, originating from France, have a roughly similar lifespan to Shire Horses, with most living between 25 and 30 years. They are generally robust and healthy horses with few breed-specific health issues. However, like Shire Horses, Percherons can be prone to Equine Metabolic Syndrome (EMS).\nThey are also susceptible to equine polysaccharide storage myopathy (PSSM), a type of muscle disorder that can lead to stiffness, weakness, and collapse during exercise. It’s primarily managed through dietary changes and regular, light exercise.\nBoth breeds may also face typical health concerns that affect all horses, such as colic, arthritis, and various infectious diseases. Vaccinations, regular vet check-ups, a healthy diet, and plenty of exercises should form part of your horse’s routine to ensure they live a long and healthy life.\nBoth the Shire Horse and Percheron breeds are relatively long-lived and usually healthy, given their large size. Yet maintaining their optimal health requires an investment in regular veterinary care, a balanced diet, and a consistent exercise regimen. Whichever breed one chooses to adopt will ultimately depend on personal preference and specific needs.\nRenowned for their robust and gentle nature, these breeds are ideally suited for a variety of purposes. However, their significant physical strength and gentle personality make them a top choice for many.\nUses and Roles Today\nThe Shire Horse: A True Gentle Giant\nThe Shire horse, originating from the United Kingdom, was initially bred for its unsurpassed size and strength, frequently harnessed for draft work in agricultural contexts and pulling hefty loads like coal carts or canal boats. To this day, this breed maintains the record as the tallest and largest horse in the world.\nNowadays, Shire Horses are well-known for their calm dispositions and versatility. They are frequently used in various commercial promotions, the filming of period pieces, forestry jobs, and numerous horse shows. Their commanding presence and gentle nature make them crowd favorites in horse shows across the globe, where they participate in dressage and hauling competitions.\nWhile modern machinery has reduced their role in farming, Shire horses still have their place. They serve in tourism, offering horseback rides, and delivering beer traditionally at UK-based breweries. Many even become integral family members, treasured as lovable pets and riding companions.\nPercheron: The French Workhorse\nPercheron horses originated in France and have been praised as a draft horse breed that possesses agility along with immense strength. Like Shire horses, they were also historically used to haul large loads and perform agricultural duties, and were integral to transportation before the onset of the industrial era.\nIn the current era, Percheron horses continue to be utilized in various ways. These horses are favored greatly for their versatility and are seen throughout the world in distinctive roles such as carriage horses, police horses, and in competitive horse shows. Due to their good-natured temperament and high degree of intelligence, Percherons are used for therapeutic riding programs and equine-assisted psychotherapy.\nFurthermore, Percheron horses are also used in several agricultural scopes even today. Both small and large farms utilize Percherons for plowing fields, pulling logs, and for general transportation. Their noteworthy strength is put to good use in plowing competitions as well.\nIn Comparison: Shire Horse vs. Percheron\nBoth Shire and Percheron horses have routed from a history steeped in hard work, from being war horses in the medieval period to being agriculture’s main driving force in past centuries. Now, they largely share their roles in contemporary society. Their majestic presence and composure make them popular show horses and their ability to carry heavy weights continue their utility on farms, even with modern machinery.\nYet, some differences exist. Percherons are often considered more agile and faster than Shire horses, making them more suitable for tasks requiring mobility, such as mounted police work. Conversely, the calm and serene temperament of Shire horses makes them more favorable for therapeutic riding programs and equine-assisted psychotherapy.\nIn essence, both the Shire horse and the Percheron play a significant role in today’s society, demonstrating their versatility, adaptability, and immense value. Whether on a farm, in a show ring, or simply as a loving pet, these breeds have firmly cemented their place in our hearts and our history.\nThrough a comprehensive exploration of the Shire Horse and the Percheron, it becomes evident that these breeds offer a fascinating insight into the remarkable journey and evolution of horses in human history. Their distinct physical attributes, temperaments, and health conditions reflect their particular history and influence their usages today. Whether they are contributing to various industries or enriching the lives of horse enthusiasts, Shire Horses and Percherons certainly hold a significant place in modern society. As we continue to forge our bond with these majestic creatures, we also continue to appreciate the extraordinary diversity and richness that they bring.', 'By Karen Frederick, Ph.D., LPC\nGod has put an amazing variety of animals on this earth for our help and pleasure. Most horse owners are well aware of the therapeutic value of horses. Working with horses and participating in their care, feeding, and grooming has been found to reduce stress, lower blood pressure, and improve overall health. It has been clinically documented that simply being in the presence of horses changes human brainwave patterns. Winston Churchill summed it up best when he said, “There’s something about the outside of a horse that is good for the inside of a man.”\nAs Christians, we have a deeper understanding of God’s creations and creatures and the ways He speaks to man’s heart through them. The Bible is very explicit in telling us about the many ways God uses animals to speak His message to us and to reveal Himself to us. We can see the acknowledgement of God as the creator in Psalm 104:24 (The New International Version): “How many are your works, LORD! In wisdom you made them all; the earth is full of your creatures.”\nGod uses his creation and creatures to reveal Himself, His attributes, and His power. Romans 1:20 says: “For since the creation of the world God’s invisible qualities—his eternal power and divine nature—have been clearly seen, being understood from what has been made, so that people are without excuse.”\nEven when mankind fails to honor the Lord, Isaiah 43:20 states, “The wild animals honor me, the jackals and the owls, because I provide water in the wilderness and streams in the wasteland, to give drink to my people, my chosen.”\nThe Bible specifically tells us that animals will teach us; in Job 12:7-10 it states, “But ask the animals, and they will teach you, or the birds in the sky, and they will tell you; or speak to the earth, and it will teach you, or let the fish in the sea inform you. Which of all these does not know that the hand of the LORD has done this? In His hand is the life of every creature and the breath of all mankind.”\nThere is little doubt that God’s creation, the horse, has profoundly impacted the history and well being of mankind. Horses have plowed fields, carried soldiers to battle, transported goods to markets, and given people of every status the ability to travel more quickly and easily. In more recent decades, horses are no longer seen as laborers or beasts of burden. Horses are being used to help people with a wide range of emotional difficulties. No consensus has been reached as to one proper term to be used when equines are involved in mental health treatment. In this article, the term equine-assisted counseling (EAC) is being used as a universal term for interventions that involve the use of a horse in order to benefit the mental or emotional well being of a human participant.\nResearch indicates that therapy involving horses may yield a variety of psychotherapeutic benefits, including the following: self-confidence, self-concept, communication, trust, perspective, anxiety reduction, decreased isolation, self-acceptance, impulse modulation, assertiveness, boundaries, creative freedom, and spiritual growth (Marx & Cumella, 2003; Ewing, MacDonald, Taylor, & Bowers, 2007; Trotter, Chandler, Goodwin-Bond, & Casey, 2008; Bass, Duchowny, & Llabre, 2009).\nThe benefits of equine-assisted counseling (EAC) have been reported in a variety of clinical groups, mostly in the form of observations from the field and participant statements. Tyler (1994) discussed the use of EAC in the treatment of clients with emotional problems, particularly adolescents. McCormick and McCormick (1997) reported on their work implementing EAI with adolescents. Moore, Wagner, and Jeffrey (2009) used EAC with clients attempting to overcome substance abuse. Others have expressed support for the use of EAC with behavioral issues, attention deficit disorder, eating disorders, abuse issues, depression, anxiety, relationship problems, and communication needs (Carpenter, 1997; Katcher & Wilkins, 1998).\nChandler (2005) utilized EAC with male and female juvenile offenders and found that the participants displayed increases in positive behaviors such as communication skills, support and encouragement for others, and increased desire to complete a task. She also noted a reduction or elimination of negative behaviors such as fear and manipulation. Other benefits seen by Chandler included greater courage, stress management, and anxiety-reduction skills.\nTrotter et al. (2008) investigated the efficacy of group EAC with at-risk adolescents and found that at-risk youth who received EAC showed statistically significant improvement in seventeen behavior areas whereas students who received classroom-based counseling showed improvement in only five areas.\nKersten & Thomas (2004) posit that the challenge of controlling the movement of a 1,000-pound creature requires concentration, creativity, and resourcefulness and that success in doing so improves self-esteem, confidence, communication skills, trust, and boundaries. A recurrent theme in the literature seems to indicate that the more difficult a client is to work with in traditional counseling, the more likely it is that that client will do well in EAC (Trotter et al, 2008).\nSo, what exactly IS Equine-Assisted Counseling (EAC)? EAC sessions involve a mental health counselor, an equine professional, and a client. The activities that the therapist chooses should be based on the needs of the client. Through observing the client’s interactions and responses to the horse, the counselor gains a lot of information. For example, if the client approaches the horse and the horse walks away, the counselor is interested in observing how the client responds to the horse walking away. Does the client pursue? Does the client give up and say, “I guess he doesn’t like me”? Whatever the response—the counselor has information (Trotter, 2012).\nIt is important to note that there are different approaches to EAC. Some counselors incorporate horseback riding in their therapy. For a number of reasons, this author does NOT incorporate riding into the therapy process. This can be disappointing to some clients, but in this author’s opinion, safety concerns outweigh the potential benefits of riding. Another important consideration is that Equine Assisted Counseling is not the same as hippotherapy or therapeutic riding—which incorporates horses for the physical benefits of the clients. Hippotherapy is frequently used with clients with a physical handicap. EAC focuses on the mental health benefits of horses.\nSome of the early, critical EAC sessions include clients’ learning about how to initiate, direct, and stop the movement of the horse. This may sound simple, but there IS one stipulation—clients are not permitted to use ropes or halters, and they are not permitted to “bribe” the horses with feed, treats, etc. They must communicate with voice and body language in order to initiate, direct, and stop the movement of the horse. Once the client understands the basics of these skills, the fun activities can begin. One example of a favorite activity includes setting up an obstacle course that the client will have to get a horse through. Clients will identify each obstacle and compare it with an obstacle that they are personally facing in their life (difficulty at school, bullying, relationship troubles, etc.). Once the course is set up and the obstacles are identified, the client has the task of getting the horse through/over/around the obstacles.\nIt may be different for each individual. EAC may be effective because working with God’s creation helps man see God’s invisible qualities. Working with horses may allow God’s creation, the horse, to teach us about ourselves, about God, about our relationships with others, etc. We all have so much to learn! And God, in His wisdom, has infinite methods of revealing Himself and communing with us for His purposes. EAC seems to be yet another way that God can provide emotional healing to those in need.\nKaren Frederick, Ph.D., LPC, is an Assistant Professor of Psychology at Houston Baptist University. Dr. Frederick’s main research interest is Equine Assisted Counseling. She is applying for grants to fund further research on the use of Equine Assisted Counseling with at-risk youth. She has written a chapter in a newly published book entitled: Equine-Assisted Mental Health Interventions (2019) by Trotter, K.S. and Baggerly, J.N. New York, NY: Routledge.\nBass, M. M., Duchowny, C. A., & Llabre, M. M. (2009) The effect of therapeutic horseback riding on social functioning in children with autism. Journal of Autism & Developmental Disorders, 39(9), 1261-1267.\nCarpenter, S. (1997). Therapeutic roles of animals. Journal of the American Veterinary Medical Association, 211, 154-155.\nChandler, C. K. (2005). Animal Assisted Therapy in Counseling. New York: Routledge.\nEwing, C. A., MacDonald, P. M., Taylor, M., & Bowers, M. J. (2007). Equine-facilitated learning for youths with severe emotional disorders: A quantitative and qualitative study. Child and Youth Care Forum, 36, 59-72.\nKatcher, A. H. and Wilkins, G. G. (1998). Animal-assisted therapy in the treatment of disruptive behavior disorders in children. In A. Lundberg (Ed.), The Environment and Mental Health: A Guide for Clinicians pp. 193-204). Mahwah, NJ: Lawrence Erlbaum Associates, Inc.\nKersten, G. & Thomas, L. (2004) Equine Assisted Psychotherapy and Learning Un-Training Manual. Santaquin, UT: Equine Assisted Growth and Learning Association (EAGALA).\nMarx, R. D., & Cumella, E. J. (2003). Questions & answers. Eating Disorders, 11, 143-147.\nMcCormick, A., & McCormick, M. D. (1997). Horse Sense and the Human Heart: What Horses Can Teach Us About Trust, Bonding, Creativity, and Spirituality. Deerfield Beach, FL: Health Communications, Inc.\nMoore, L., Wagner, P., & Jeffrey, H. (2009). Addiction programming model at its best. Paper presented at the 10th annual conference of the Equine Assisted Growth and Learning Association, Asheville, NC.\nTrotter, K.S. (2012). Harnessing the Power of Equine Assisted Counseling. New York, NY: Routledge.\nTrotter, K. S., Chandler, C. K., Goodwin-Bond, D. G., & Casey, J. (2008). A comparative study of the efficacy of group equine assisted counseling with at-risk children and adolescents. Journal of Creativity in Mental Health, 3(3), 254-284.\nTyler, J. J. (1994). Equine psychotherapy: Worth more than just a horse laugh. Women & Therapy, 15, 3-4, 139-146.\n[Editor’s Note: Christianity, Mind and Mental Health image from Van Gogh’s The Pietà (after Delacroix), 1889, found at Wikipedia Commons.}']	['<urn:uuid:968d71ce-8802-4949-b7ce-e85cf493c3ee>', '<urn:uuid:646e2f6b-0620-4815-9a7e-3a3aef1a5b69>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	10	110	4008
64	why are thieves targeting toyota prius catalytic converters what makes them valuable	The Toyota Prius is the most targeted car for catalytic converter theft because as a low-emissions vehicle, its converters contain larger amounts of precious metals - platinum, rhodium, and palladium. Thieves can get up to $700 per converter from scrap yards, while leaving Prius owners with bills as high as $2,000.	['Here at Chapel Hill Tire, we work to keep our customers up-to-date on automotive industry insights. While some weeks we get to share uplifting information, this week we are here to discuss a more serious problem facing drivers: catalytic converter theft. Here is everything you need to know about this issue and what you can do to stay protected.\nWhy Are Catalytic Converters Being Stolen?\nYour catalytic converter is an essential part of your vehicle, designed to reduce the pollutants and toxic gas your emissions system produces. It uses precious metals like platinum, rhodium, and palladium to complete this process. People have been stealing catalytic converters off of cars to sell these metals for scraps.\nWhile this has been happening for years, a recent uptick in catalytic converter theft may be motivated by economic turbulence from the pandemic. Additionally, the reduced foot traffic during this time may make it easier for thieves who feel less likely to be caught. It takes just a few minutes for a thief to saw the catalytic converter out of your car and leave you with thousands of dollars in damage.\nWhich Cars Are Targeted for Catalytic Converter Theft?\nThe Toyota Prius is the most targeted car for catalytic converter theft. As a low-emissions vehicle, Prius converters contain larger amounts of the precious metals platinum, rhodium, and palladium.\nThieves can fetch as much as $700 per converter from unscrupulous scrap yards. And unfortunately, this can leave Prius owners with bills as high as $2,000.\nTrucks and SUVs are also common targets of catalytic converter theft because they are raised off the ground. This makes it easier to climb under and saw through the metal. However, every vehicle on the road is susceptible to a stolen catalytic converter.\nWhat To Do When Your Catalytic Converter Is Stolen\nStep 1: Contact the Authorities\nIf you find your catalytic converter is missing, you should immediately contact the authorities. They will check local security cameras and work to see if they can help catch the person who stole your converter. Additionally, a police report may be essential to your insurance coverage.\nStep 2: Contact Your Insurance Company\nDepending on your coverage, your car insurance may cover your catalytic converter replacement. You can contact your insurance agent to see what your options are and whether or not you are covered.\nStep 3: Visit a Mechanic\nFinally, you should bring your vehicle to a mechanic for service. An expert can discuss your repair options with you and prevent you from driving without your catalytic converter.\nWhat Happens When You Drive Without A Catalytic Converter?\nWhen facing the recovery costs of a stolen “cat,” many customers wonder, “Can I drive without my catalytic converter?”\nThe answer is both yes and no. Technically, a car can function without a catalytic converter. However, this is not a sustainable long-term option. Here are some of the troubles you will face when driving without a catalytic converter.\nLoud Vehicle Noises and Missing Catalytic Converter\nThe first sign of a stolen catalytic converter is incredibly loud vehicle noises. You will find that when driving without your catalytic converter, it will sound like your vehicle is roaring—especially when starting or giving it gas.\nRough Driving and Acceleration\nAs your exhaust goes unregulated, your vehicle will drive more roughly. Your engine usually forces your exhaust through your catalytic converter. Without this component in place, this forceful exhaust push will lead to jarring, uneven acceleration.\nAnnual Vehicle Inspection Failure\nWithout your catalytic converter, you will fail both your NC safety inspection and your NC emissions inspection. While the emissions inspection failure may seem obvious, you might be wondering why a missing catalytic converter would cause you to fail your safety inspection.\nYour annual safety inspection includes an exhaust system check, which checks specifically for removed and tampered components. This includes your catalytic converter, as well as your muffler, exhaust pipe, PCP valve, and EGR valve, among others.\nAs such, you will be unable to renew your vehicle registration without a catalytic converter. You can find more information on exhaust system checks here on our list of everything checked during your annual safety inspection.\nHarmful Car Emissions\nYour catalytic converter is named as such because it converts toxins into less harmful byproducts, such as water vapor and carbon dioxide. Without this component, your vehicle will no longer be filtering and reducing harmful emissions, including hydrocarbons, nitrogen oxides, and carbon monoxide. Not only is this bad for the environment, but it can also lead to harmfully poor air quality. When you park in your garage, for example, these toxic pollutants can find their way into your home.\nAvoiding a $250 Fine\nNot everyone who drives without a catalytic converter is the victim of theft. Some individuals choose to remove their catalytic converters to boost their horsepower. If authorities find that you removed your catalytic converter on purpose, it can lead to a hefty fine of $250.\nThis does not apply to stolen catalytic converters. The fine is delivered “for instructing or allowing a person to tamper with an emission control device of the vehicle so as to make the device inoperative or fail to work properly.” However, if your catalytic converter was, in fact, stolen but you try to avoid replacement with workarounds, your efforts might resemble the efforts of a driver trying to tamper with their emissions system. This is also why it is important to report a stolen catalytic converter right away.\nHow Can You Prevent Catalytic Converter Theft?\nThankfully, there are a few preventative measures you can take to deter the theft of your catalytic converter.\nInstall a Protective Device\nCatalytic converter theft has become so prominent that companies have started to sell protective devices. These installations make it impossible for anyone to cut off your catalytic converter. Unfortunately, high-quality devices and their installation will likely run you a few hundred bucks. However, it could save you thousands of dollars on catalytic converter replacement.\nSome newer vehicles have a security setting that triggers the alarm when it senses vibrations. The sawing of your catalytic converter will be enough to set off these alarms.\nWhen you are at home, you can park in the garage to help avoid catalytic converter theft. What about when you are on the go? When your garage is not an option, here are some parking tips to keep in mind:\n- Brightly-lit areas: Parking under street lights or in brightly-lit locations can help deter theft.\n- Highly populated locations: If the lot or parking garage you are in is seeing a lot of foot traffic, it will be hard for someone to steal your catalytic converter without being caught.\n- Security camera: Keep an eye out for security cameras in your parking area and try to find a spot visible to the cameras.\nAt best, these practices can deter the theft of your catalytic converter. Otherwise, it may help the authorities catch the individual who stole your vehicle parts.\nChapel Hill Tire Catalytic Converter Services\nChapel Hill Tire has the catalytic converter protective devices you need to maintain your peace of mind and keep your vehicle protected. We sell and install these devices to help our drivers prevent catalytic converter theft.\nIf your catalytic converter has been stolen, contact the authorities right away to report this theft. You can also visit Chapel Hill Tire to discuss your options for repair. We proudly serve the greater Triangle area, including Raleigh, Durham, Chapel Hill, Apex, and Carrboro through our 9 locations. We also serve surrounding cities like Wake Forest, Pittsboro, Knightdale, Garner, and beyond. Make your appointment here online to get started today, and stay safe!']	['<urn:uuid:462e80cc-ab42-44ca-a05a-6ca603860449>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	12	51	1279
65	What are the key steps involved in recruiting and managing event staff for a large assembly that typically brings 3,500 visitors and requires three years of planning?	The process involves several key steps. First, during recruitment and onboarding, organizers must register applicants, identify qualifications, conduct training, and establish communication channels. The next phase involves schedule planning and facilitation, where organizers create shift schedules and allow staff to sign up for shifts. For the actual event, managers need real-time visibility into staff attendance and last-minute changes. The event requires scores of volunteers, primarily from the host committee and from all over, to handle tasks like registration, meeting room guidance, and children's activities. This volunteer 'army' is essential as the small staff alone cannot manage an event of 3,500 visitors.	['Behind the Scenes:\nFive Overlooked Facts About General Assembly\nGeneral Assembly is just two months away! Many commissioners experience GA without realizing the level of complexity the Administrative Committee handles for the PCA to coordinate our annual gathering.\nA recent conversation with Mrs. Amanda Burton again highlighted the multi-faceted and long-term planning that precedes every General Assembly. As PCA Meeting Planner, Mrs. Burton navigates all the ins and outs of planning and “putting on” an Assembly and deals with five key facts that even the most faithful attendees might not realize about General Assembly, just in the one area of site selection.\n1. It takes at least three years to plan a General Assembly.\nThe process begins when a possible site is identified – for example, when a presbytery or local churches contact the Administrative Committee (AC) to express interest in hosting an Assembly in their bounds or when AC staff learn of potential benefits available in some city. AC staff members will visit a prospective city to tour facilities and meet with local officials. After multiple site visits, a contract may be negotiated, a host committee formed, and more detailed planning begun.\n2. Multiple cities compete to host a General Assembly.\nA General Assembly typically brings 3,500 visitors to a city, including commissioners, exhibitors, and family members. Of course, cities are interested in hosting events of this size, which means they compete for our business. We are often considering at least three locations for each year, and we reject more proposals from cities than we accept. Two key factors in choosing a city are its affordability for commissioners and the interest and support from the local presbytery.\n3. The General Assembly represents an economic impact of over $3M+ to a city.\nWhen 3,500 people visit a city over a five-day period, they spend money on hotel rooms, restaurants, and local attractions. General Assembly represents 3,000 nights of hotel room reservations and approximately 40,000 meals purchased in local restaurants, not to mention cups of coffee! Putting it all together, an Assembly pumps over $3,000,000 into the local economy, thereby benefiting hotel staff, housekeepers, servers, baristas, and more. Though it is often overlooked, one way to bless hourly wage workers in the heart of your city is to host an Assembly.\n4. A primary point of emphasis in negotiations is hotel room rates.\nThe Administrative Committee enters into a binding agreement with local officials once a site is selected. Many details are negotiated into the contract, from the dates of the event to availability of WiFi throughout the facility. However, a primary point of emphasis in negotiations is securing the lowest room rates possible for commissioners. In fact, local officials often offer to negotiate lower hotel room rates so the AC will choose their city. A goal of the Administrative Committee is always to minimize the expenses of commissioners as much as possible.\n5. It takes an “army” of volunteers to pull off an Assembly.\nThe unsung heroes of any Assembly are the members of the local host committee. These are members from churches in the local presbytery who volunteer their time to coordinate various details from worship services to family activities. Most of the people who help you register, find a meeting room, and oversee children’s activities are volunteers (primarily from the host committee, but also from all over the PCA). Needless to say, our small AC staff could not cover alone all the bases for an event involving over 3,000 attendees! So the AC coordinates the efforts of scores of volunteers to make the Assembly welcoming and enjoyable. We are especially thankful for the host committee in St. Louis, who have been working tirelessly through a pandemic to make sure everyone who comes to GA this year will have a safe and enjoyable experience in their city.\nIt’s not too late to register for General Assembly in St. Louis. Looking forward, mark your calendars for future assemblies, where host committees in Birmingham and Memphis are already at work making preparations for our visit. We are thankful to God for the places He has provided for us to convene and prayerful for the business to be conducted. And if you are interested in finding out how your presbytery could host at General Assembly – get in touch! Email firstname.lastname@example.org.', 'Events are about people — people who attend them, people who staff them, and people who manage it all. The event staffing component can be chaotic and particularly complex, because finding event staff and volunteers with the correct skills, and getting those people to the right place at the right time, is an enormous undertaking. Whether you’re planning a conference, overseeing a festival, or managing a sporting event, the schedule is essential. From sports event management to hospitality events, the key to success is strategic event staff scheduling and clear communication.\nProper event staffing is critical for any event and includes:\nAbout 18 million events and meetings are organized in the U.S. every year, producing $66.8 billion in labor income.-Meeting Professionals International\n- Matching the people with the right skills to the right tasks\n- Seamless logistics planning and implementation\n- Constant communication\nRegardless of your specific situation, your event staff will be diverse and may include full-time, part-time, contingent, and volunteer workers. This can add a layer of complication to your scheduling process. Traditional event staffing often relies on disjointed tools and manual efforts ranging from paper and pencil, whiteboards, and spreadsheets to sign-in sheets, email, and phone trees. These manual systems are inefficient and increase the chance for human error.\nThis how-to guide will:\n- Outline the four-step event staffing and coordination lifecycle\n- Share tips and tricks for each step in the event planning process\n- Show how event scheduling software can streamline your workflow and improve your event management\nGuide on How to Optimize Your Scheduling Process\nEvent Staff Scheduling How-To Guide\nEvent staff scheduling software excels at making your event staffing process more efficient to manage. It can also help you ensure compliance with the myriad rules and regulations that impact shift-based workplaces. Most software is cloud-based and sold as a yearly or monthly subscription service. Consider the following capabilities when making your selection:\nCreating a staff or volunteer schedule is an obvious component of festival and conference event management. However, the event staffing process involves a great deal more than just scheduling, and extends across four steps:\nConventions and events are expected to expand by 44 percent from 2010 to 2020 – U.S. Bureau of Labor Statistics\n- Recruitment & Onboarding\n- Scheduling & Facilitation\n- Gameday Logistics\n- Post-Event Analytics\nFor each step, there are best practices relating to:\n- Overall event management and coordination\n- Visibility into event staffing and planning\n- Staff and volunteer management\n- Event staff communication\nBy understanding and following the scheduling best practices below, you can greatly improve not only your event planning process but the entire event itself. Organizations running events can improve every event with event staff scheduling software. It’s the “special sauce” that makes implementing these best practices efficient and engaging.\nYou can’t have an event without a team, so event staff and volunteer recruitment and onboarding is a key first step in the event planning process. This step includes registering applicants, identifying their qualifications, conducting training, assigning teams, and establishing communication channels.\nHere are some best practices you can follow to optimize your recruiting and onboarding:\nWrite out every step of the onboarding processCapture every step, from staff and volunteers learning about the event to registration, training, and beyond. Then, evaluate those steps and look for ways to streamline the process for all parties involved.\nFind the right people for the jobYou’ll need total visibility into the skills people have and the requirements across a variety of roles. Making sure that people have the correct qualifications or that training is conducted for certain team members is vital to staffing certain positions.\nAsk questions like:\n- Which jobs need certain skills, training or certifications?\n- Does the medical team need first-aid licenses on hand?\n- What about your set-up crew? Do they need audio training or certain physical capabilities?\nQuality event staffing can get the right people going to the right places at the right time when people are organized into groups. Once you’re clear on what needs to be accomplished and who has the skills to get it done, segment your event staff into teams or work groups.\nMake everything as easy as possible for your event staff and volunteersDuring the recruiting phase, review your current process and ask yourself things like:\n- Do staff or volunteers have to email someone to register to work?\n- Do applicants have to print a form, fill it out, and then send it back to you?\nYou might lose volunteers due to unforeseen obstacles in your current process. Consider what message you’re sending to potential staff or volunteers. Will this event be a hassle, or will it be easy and fun to participate in? Also, consider the time you’ll spend managing a clunky process.\nSet up communication systems early and use them oftenArguably the single-most important tactic—successfully communicating with teams—leads to a well-informed and engaged workforce.\n- Be proactive in communicating with your event staff and volunteers. Communicating information frequently helps teams work together and improves morale.\n- Promote a variety of communication streams. Knowing which method to use for a given scenario supports better information distribution and higher read rates. Consider using an in-app bulletin board for general information,automatic email notifications for reminders, and broadcast SMS for time-sensitive alerts.\n- In first step, be sure to communicate:\n- Shift responsibilities\n- Processes for vouchers, giveaways, or other rewards\nGuide: How to Optimize Your Event Staffing Process\nEvent scheduling and facilitation is the step where your schedule begins to take shape. This step involves first gathering event requirements to frame out your schedule, followed by filling shifts. With proper communication and tools, you can empower staff and volunteers to sign up for their own shifts (known as “bottom-up scheduling”). Note that what your schedule looks like at this step often differs dramatically from the final version you’ll execute on the day of the event.\nHere are some best practices you can implement to optimize event scheduling and facilitation:\nBe open to new ideasOrganizations that are open to change adopt better and more innovative ways to coordinate their events. Consider new workflows by asking:\n- How can I make the process faster or more effective? Often, a new idea can spark positive change in unexpected places. For example, allowing volunteers to trade shifts among themselves can streamline a process that once needed oversight by coordinators or managers.\nKnow what is flexible and what is notWell-prepared organizations plan for the unexpected by having good visibility into their negotiable versus non-negotiable requirements. Your staff and volunteer schedule will inevitably change, but you can plan for a margin of error if you know:\n- Event staffing requirements\n- The mix of roles and responsibilities\n- The timing that makes up the architecture of your event\nEmpower your team to select their own shifts Event staff and volunteers that have easy visibility into their schedules are more likely to show up. But filling shifts can require a lot of back and forth to gather everyone’s availability. Trying to fill shifts equitably is a burdensome task, especially with a manual scheduling process. Empower your team with systems that make it easy for them to sign up for shifts that suit their needs and trade shifts when needed.\nThis “bottom-up scheduling” process leads to a greater sense of confidence and commitment on the part of staff and volunteers, and greatly reduces the effort needed by coordinators to facilitate scheduling. Once staff and volunteers have chosen shifts, give them the ability to view their assignments in a way that works for them, for example by using a mobile app, printing a hard copy, or syncing with their personal online (Gmail, Outlook, etc.).\nStay in touchCommunication shouldn’t end after the onboarding process. During this phase, offer helpful information about:\n- Location details\n- How to check in\n- Basic responsibilities related to their job or shift\nCommunicating important details now can help reduce last-minute requests for information and can positively impact the event overall.\nCustomer Spotlight: SIFF\nBefore implementing event staff scheduling software, the Seattle International Film Festival (SIFF) sketched out their complex schedules on whiteboards. This manual method was not scalable and was prone to human error. SIFF transformed their event planning process by implementing scheduling software, which streamlined the process for event coordinators while empowering staff and volunteers to schedule their own shifts.\nGameday is by nature chaotic, and involves managing time and attendance, dealing with last-minute changes, and orchestrating vouchers or swag. Proper processes and tools can support agile gameday operations by giving you real-time information about what’s happening on the front lines.\nHere are some best practices you can implement to optimize your gameday operations:\nOptimize schedule planningMake gameday run more smoothly by asking the right questions during planning questions during planning:\n- Are the event managers and event coordinators assigned tasks that will take up their time on event day?\n- Where can we combine tasks or otherwise improve efficiency?\n- Where is the schedule too tight?\nBy off-loading management tasks through an improved process, combining tasks, and adjusting the schedule in advance, you reduce the risk of gameday event staffing issues.\nFor event managers, gameday is about troubleshooting the unexpected and answering myriad last-minute questions. By completing anticipated tasks in advance, event coordinators free up their own time to focus on handling the curveballs on gameday.\nProvide real-time visibilityGameday takes on a life of its own—keynote speakers arrive, vendors set up and provide services, and volunteers show up for shifts. Event coordinators need real-time visibility into what’s happening; for example:\n- Seven no-shows across two locations\n- A volunteer who checked in but asked to swap shifts\n- Running low on XL T-shirts at the swag table\nBy using event staff scheduling software on a tablet or mobile device, managers can have real-time access to the information they need.\nEmbrace technologyLast-minute schedule changes inevitably happen and can look like:\n- Rerouting volunteers to new entrances\n- Open shifts that need to be backfilled at the last minute\nAn attendance tracking system can improve your check-in process, and a mobile-friendly version of the event-day schedule enables team leads to know who is currently on duty and where to facilitate changes as necessary. Also, creating maps specific to each shift helps get people to the right place at the right time.\nCommunicate incentivesVolunteering often includes perks like free parking, food, T-shirts, and other incentives. Letting the staff and volunteers know where to pick up concession vouchers and swag giveaways is an important, but often overlooked, aspect of event staff communication. Be sure to communicate about how your voucher or incentive process works before the event, so that things are clear and run smoothly on gameday.\nManual processes make it nearly impossible to track and administer who has received what, but event scheduling software can streamline the process and free up your team for more important tasks.\nThe meeting and event industry employs 1.8 million jobs in the U.S. -U.S. Bureau of Labor Statistics\nManagers who incorporate feedback into their event planning process run better and more successful events. This step focuses on reviewing event reports, asking for feedback, sending follow-up communication, and acknowledging and celebrating the team’s efforts.\nHere are some best practices you can implement to optimize your post-event activities:\nDon’t wait to analyze resultsYou’re exhausted from the event, but just because gameday is done, event management isn’t over. Post-event analytics often get overlooked or delayed to the point of being combined into the next year’s first phase of recruiting and onboarding. But the sooner you analyze the actual event, the more actionable your analysis will be.\nPost-event statistics like total attendance, net revenue, or the sheer size of the volunteer force can be used for:\n- Marketing content to tell a great story and promote next year’s event\n- Improving sponsorship meetings or other efforts to secure partners and funding\n- Planning next year’s event and recruiting efforts\nUse post-event reports to find insights Even a handful of post-event reports can greatly improve planning for the following year. For example, a Volunteer Schedule Adherence report can show how many shifts you may need next year. Take time shortly after the event to find insights contained in the post-event reports.\nSolicit feedback from everyone Some of the best post-event insights are collected through grassroots efforts, and some of your best ideas may come from the staff or volunteers who were on the front lines. Use scheduling software to enable your workforce to record observations or feedback during or shortly after the event. You can spark feedback by asking things like:\n- What processes can be improved?\n- What happened during the event that could have been better facilitated or managed earlier?\n- What problems did you run into? What went well?\nCollecting gameday experiences when they are fresh fuels storytelling, which enhances post-event reporting. If you take the time to train your teamt on what to look for, you’ll be showered with new ideas to make your next event even better.\nClose the loop An important post-event task is closing the communication loop with staff and volunteers. Remember to celebrate and acknowledge the team’s efforts. This creates a sense of connection and appreciation between the managers, coordinators, staff, and volunteers. It not only recognizes their efforts and interaction with the event, but it encourages them to repeat that experience next year. You could also direct them to some of the follow-up items or post-event activities available to them.']	['<urn:uuid:fac5729e-2b85-4bb7-a1b4-ed5cedbd1594>', '<urn:uuid:a4cd446e-6b2f-4498-abbd-2af2273844db>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T01:37:09.743768	27	101	2977
66	main rules calculations trig equations and tips memorize them fast	The main rules for trigonometric calculations include the Pythagorean identities (sin²x + cos²x = 1), negative angle identities (like sin(-x) = -sin x), cofunction identities, addition formulas, and multiple angle formulas. For efficient memorization, several techniques can be employed: using the hexagon diagram for basic relationships, remembering that cosine usually carries positive signs while sine carries negative signs, learning one sum identity to derive others, and understanding that trigonometric functions are periodic. The algebraic properties used in calculations include the commutative and associative properties, distributive property, and various factoring techniques. It's recommended to practice numerous problems and establish relationships between terms rather than trying to memorize formulas in isolation.	['Identifying Algebraic Properties Most Often Used When Solving Identities\nSolving identities is almost a rite of passage for those studying trigonometry. Tackling the prospect of solving identities — and later simplifying trig expressions in calculus — goes much more smoothly if you have some algebraic tools at hand. With a plan of action, you’ll succeed more quickly and efficiently and have the desired product.\nWhen solving an identity, you do bring in some trig substitutions (basic identities such as sin2 x + cos2 x = 1), but all your work has its main basis in algebraic rules and techniques. Here are the algebraic properties most commonly found when working with identities:\nCommutative property of addition and multiplication: 2 sin x + sin y + sin x = 2 sin x + sin x + sin y and\nYou can change the order of the terms or factors to make combining terms more convenient.\nAssociative property of addition and multiplication: 2 sin x + (sin x + sin y) = (2 sin x + sin x) and\nBy regrouping terms or factors, you can add or multiply terms that combine.\nDistributive property of multiplication over addition: sin x(1 – csc x) = sin x – sin x csc x. The distributive property is very useful, especially when you recognize that one of the products turns out to be a function times its reciprocal.\nDoing a flip-flop of the two sides can make for more convenience in the work or when solving an equation.\nMultiplication property of equations: If\nthen 2 sin x = 1. You can multiply both sides of an equation by the same number (just not 0). When solving a trig equation, you have many hidden opportunities to multiply each side of an equation by 0 or divide (multiply by a reciprocal) by 0. The trig functions sine, cosine, tangent, and cotangent are 0 for many angle measures. Just take those angle measures into account when determining a solution to the equation (in other words, throw them out).\nSquaring a binomial: (sin x + cos x)2 = sin2 x + 2 sin x cos x + cos2 x. One of the most frequent errors found when squaring a binomial is forgetting that middle term. Squaring binomials is especially useful in trigonometry, because it tends to create terms that are a part of one of the Pythagorean identities.\nFactoring (greatest common factor): sin2 x tan2 x – tan2 x = tan2 x(sin2 x – 1). When two or more terms have a common factor, dividing each term by that factor creates one or more workable expressions. Just be sure to divide all terms by the factor and to preserve the correct signs. When dividing by a negative factor, the signs all switch.\nFactoring (difference of squares): sec2 x – 1 = (sec x – 1)(sec x + 1). The Pythagorean identities all have three squared terms in their equations. This lends to many opportunities to factor as the difference of squares. You look ahead to see what may then be divided out in a future step. Other factoring techniques are used less frequently, but don’t hesitate to refer back to your algebra to dredge up something not mentioned here.', 'The Back Bencher’s Tip to learn the Trigonometric Identities\nAll right! So, now you are planning to study the monster called trigonometry! I am sure majority of you would agree with me on this that trigonometry is a hard nut to crack! The concepts involved in it are not very hard but what makes it all the more challenging is that it is over flooded with formulae.\nYou learn first five formulae, reach the sixth and …. you can’t recall the first one! If this is what you are experiencing, then you are one of the numerous students suffering from the trigonometry phobia! Well, you should not be cursing yourself for this because, after all you are human with limited memory and you cannot expect your brain to be a memory card which stores everything permanently!\nDon’t worry my friend, we bring you some of the interesting ways of learning these formulae and befriending the monster called trigonometry! Get ready to explore the interesting world of trigonometry!\nThere are certain basic concepts which you will have to learn (and we can’t really help you in that!) like periodicity of functions and Pythagorean identity. Now, let us begin our tour:\nWe begin with the basic trigonometric functions i.e. sin, cos and tan.\nFor remembering this (although, it’s too simple), write the functions in this order\nsin cos tan\nOH AH OA\ncosec sec cot\nSo, just remember oh ah oa…. or rather you can learn it as SOH CAH TOA which says sine means OH (opposite/hypotenuse), cos means AH (adjacent side/hypotenuse), tan for OA (opposite side/adjacent side).\n- Now, once you know these six basic formulae, next important point is the sign of trigonometric functions. So here comes the technique to remember the signs of trigonometric functions in various quadrants.\nWe know that there are four quadrants. In the first quadrant, all trigonometric functions are positive. In the second quadrant, sine and cosec are positive and all remaining functions are negative. Similarly only tangent and cotangent are positive in the third quadrant while sec and cosec are positive in the fourth quadrant.\nStudents often get confused in memorizing this. This should be better remembered as After School To College or Add Sugar To Tea. The initials stand for the functions which are positive in the respective quadrants. Once you learn it this way, you are sure to remember it throughout your life without getting confused.\nNext is the core of trigonometry i.e. the trigonometric identities. Frankly speaking, there is no substitute for learning the trigonometric identities. One of the options is that you can derive them using the Euler’s formula. But that is possible only if you have sufficient time. Demoivre’s theorem is also useful in this context but that also takes time. Hence, while the sad part is that one has to learn the identities as there is no other alternative, the good part is that we are here to simplify this process for you!\nConsider the figure given below. This figure is called the hexagon of trigonometric identities. Some of the basic identities can be remembered with the help of this hexagon.\ncosec x = 1/sin x\nsec x = 1/cos x\ncot x = 1/tan x\nNext, we can also derive the standard identities reading down any triangle in clockwise direction i.e.\nsin2x + cos2x = 1\n1 + cot2x = cosec2x\ntan2x + 1 = sec2x\nHence, these basic concepts can be easily remembered with the help of this hexagon.\n- We now move on to the sine and cosine laws in triangle:\nIn any triangle ABC having sides a, b and c, we have\n1.The sine law\nsin A / a = sin B / b = sin C / c\nSo, here you can just remember that sine of an angle divided by same side is equal to the rest of the angles divided by the corresponding sides.\n2 The cosine laws\na 2 = b 2 + c 2 – 2 bc cos A\nb 2 = a 2 + c 2 – 2 ac cos B\nc 2 = a 2 + b 2 – 2 a b cos C\nWe take the first one. You can learn this as one side a = (b-c)2 and the 2bc term has cosine of the angle corresponding to the main side i.e. a.\n- Sum and Difference Formulae:\n1. sin (A ± B) = sin A cos B ± cos A sin B\nFirst of all, remember that the formula of sin (A ± B) contains both the functions sin and cos. Now, to memorize the formula, learn “SC student studies CS (computer science)”, where SC→CS imply sin cos and then cos sin. The sign between the terms remains the same i.e. the formula for sin (A + B) will contain a positive sign in between terms and that of sin (A – B) will contain a negative sign.\n2. cos (A ± B) = cos A cos B ∓ sin A sin B\nThis formula of cos contains both the trigonometric functions in pairs. You can learn it as Caste Conflict is a form of Social Sin i.e. CC→SS. Now this statement is in negative sign and hence the sign would be negative i.e. while computing cos (A + B), the sign between the terms is negative and vice-versa.\nOnce you remember this, you can easily derive the remaining formulae.\n- Multiple Angle Formulae:\n1. sin 2A = 2 sin A cos A\nRemember that both the sides contain 2. Further you can learn it as SSC exam i.e. S→SC, which implies that sine of an angle = twice sine and cosine of half of that angle.\n2. cos 2A = cos2A – sin 2A\n= 1 – 2sin2A\n= 2 cos2A -1\nIn this case, there are three formulae for cosine of 2A. But, one should try to learn just one as others can be derived from it using the relationship of sine and cosine of angles.\n3. sin 3A = 3 sin A – 4sin3A\nIn this again, remember that every term contains 3. Secondly, all the terms contain the same trigonometric ratio i.e. sine.\n4. cos 3A = 4cos3A – 3 cos A\nThis term is completely based on the previous formula. By just interchanging both the terms in the previous case and replacing sine by cos we obtain this formula.\n- Some general tips to help you learn the identities:\n1. Remember that sin x = (eix – e-ix)/2i and cos x = (eix + e-ix)/2 and use the exponent rules to derive the identities.\n2. Usually cos carries a positive sign while sine carries a negative sign.\n3. One cannot escape from learning the formulae of reciprocal functions, periodicity of functions and Pythagorean identities.\n4. Instead of attempting to cram the identities, try to establish relations between terms so as to minimize the chances of forgetting the formulae.\n5. Learn one of the sum identities and the remaining sum identities can be easily derived using the facts that cos x is an even function and sin x is an odd function.\n6. It is crucial to have your basics clear as they can help you in reaching the formulae in case you forget or get confused.\n7. Always remember that all the trigonometric functions are periodic i.e. they repeat after a specific interval. This concept also proves useful.\n8. Once you remember the sum and difference identities for sine and cos i.e. the formulae of sin (a ± b) and cos (a ± b), the multiple angle formulae can be derived from these identities by putting b = a, we get the formula for cos 2a.\n9. Practicing numerous questions based on application of these formulae can help you in mugging them up without putting in extra effort.\nWe have listed all the important trigonometric identities here so that students don’t miss out any:\n· Pythagorean Identities\nsin 2X + cos 2X = 1\n1 + tan 2X = sec 2X\n1 + cot 2X = csc 2X\n· Negative Angle Identities\nsin (-X) = – sin X, odd function\ncsc (-X) = – csc X, odd function\ncos (-X) = cos X, even function\nsec (-X) = sec X, even function\ntan (-X) = – tan X, odd function\ncot (-X) = – cot X, odd function\n· Cofunctions Identities\nsin (π /2 – X) = cos X\ncos (π /2 – X) = sin X\ntan (π /2 – X) = cot X\ncot (π/2 – X) = tan X\nsec (π /2 – X) = csc X\ncsc (π /2 – X) = sec X\n· Addition Formulas\ncos (X + Y) = cos X cos Y – sin X sin Y\ncos (X – Y) = cos X cos Y + sin X sin Y\nsin (X + Y) = sin X cos Y + cos X sin Y\nsin (X – Y) = sin X cosY – cos X sin Y\ntan (X + Y) = [ tan X + tan Y ] / [ 1 – tan X tan Y]\ntan (X – Y) = [ tan X – tan Y ] / [ 1 + tan X tan Y]\ncot (X + Y) = [ cot X cot Y – 1 ] / [ cot X + cot Y]\ncot (X – Y) = [ cot X cot Y + 1 ] / [ cot Y – cot X]\n· Sum to Product Formulas\ncos X + cos Y = 2cos[(X + Y)/ 2] cos[(X – Y)/ 2]\nsin X + sin Y = 2sin[(X + Y)/ 2] cos[(X – Y)/ 2]\n· Difference to Product Formulas\ncos X – cos Y = – 2sin[(X + Y) / 2] sin[(X – Y) / 2]\nsin X – sin Y = 2cos[(X + Y) / 2] sin[(X – Y) / 2]\n· Product to Sum/Difference Formulas\ncos X cos Y = (1/2) [cos (X – Y) + cos (X + Y)]\nsin X cos Y = (1/2) [sin (X + Y) + sin (X – Y)]\ncos X sin Y = (1/2) [sin (X + Y) – sin[ (X – Y)]\nsin X sin Y = (1/2) [cos (X – Y) – cos (X + Y)]\n· Difference of Squares Formulas\nsin 2X – sin 2Y = sin (X + Y) sin (X – Y)\ncos 2X – cos 2Y = – sin (X + Y) sin (X – Y)\ncos 2X – sin 2Y = cos (X + Y) cos (X – Y)\n· Double Angle Formulas\nsin (2X) = 2 sin X cos X\ncos (2X) = 1 – 2sin 2X = 2cos 2X – 1\ntan (2X) = 2tan X/[1 – tan 2X]\n· Multiple Angle Formulas\nsin (3X) = 3sin X – 4sin 3X\ncos (3X) = 4cos 3X – 3cos X\nsin (4X) = 4sin X cos X – 8sin 3X cos X\ncos (4X) = 8cos 4X – 8cos 2X + 1\n· Half Angle Formulas\nsin (X/2) = ±√[(1 – cos X)/2]\ncos (X/2) = ±√[(1 + cos X)/2]\ntan (X/2) = ±√[(1 – cos X)/(1 + cos X)]\n= sin X/(1 + cos X)\n= (1 – cos X)/sin X\n· Power Reducing Formulas\nsin 2X = 1/2 – (1/2) cos (2X))\ncos 2X = 1/2 + (1/2) cos (2X))\nsin 3X = (3/4) sin X – (1/4) sin (3X)\ncos 3X = (3/4) cos X + (1/4) cos (3X)\nsin 4X = (3/8) – (1/2)cos (2X) + (1/8)cos (4X)\ncos 4X = (3/8) + (1/2)cos (2X) + (1/8)cos (4X)\nsin 5X = (5/8)sin X – (5/16)sin (3X) + (1/16)sin (5X)\ncos 5X = (5/8)cos X + (5/16)cos (3X) + (1/16)cos (5X)\nsin 6X = 5/16 – (15/32)cos (2X) + (6/32)cos (4X) – (1/32)cos (6X)\ncos 6X = 5/16 + (15/32)cos (2X) + (6/32)cos (4X) + (1/32)cos (6X)\n· Trigonometric Functions Periodicity\nsin (X + 2π) = sin X, period 2π\ncos (X + 2π) = cos X, period 2π\nsec (X + 2π) = sec X, period 2π\ncsc (X + 2π) = csc X, period 2π\ntan (X + π) = tan X, period π\ncot (X + π) = cot X, period π']	['<urn:uuid:0e1b36cd-e254-4d4c-9c3f-e7941dbdcdbf>', '<urn:uuid:7e49d248-af3a-41be-8606-b1de5f23f660>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	10	109	2590
67	My stomach isn't feeling great. What exactly are prebiotics?	Prebiotics are naturally occurring, non-digestible food components that promote the growth of helpful bacteria in your gut. They may improve gastrointestinal health and potentially enhance calcium absorption.	['Prebiotics and Probiotics: Creating a Healthier You\nTable of Contents\n- What Are Prebiotics and What Do They Do?\n- Prebiotics in Your Diet\n- What Are Probiotics and What Do They Do?\n- Probiotics in Your Diet\n- What Makes Prebiotics and Probiotics the “Dynamic Duo?”\nYou’ve probably heard of prebiotics and probiotics, but do you know what they are? Nutrition research has pinpointed specific functional components of foods that may improve health, and prebiotics and probiotics are two such substances.\nAlthough they are available as dietary supplements, it is not necessary to use special pills, potions, cleanses or other concoctions to incorporate prebiotics and probiotics into your daily intake. These “nutrition boosters” are natural ingredients in many everyday foods.\nWhile research continues in this area of nutrition — investigating how effective and safe these substances are and how much we need to obtain health benefits — here’s what we know now.\nWhat Are Prebiotics and What Do They Do?\nPrebiotics are naturally occurring, non-digestible food components that are linked to promoting the growth of helpful bacteria in your gut. Simply said, they’re “good” bacteria promoters. That’s right, not all bacteria are bad! Prebiotics may improve gastrointestinal health as well as potentially enhance calcium absorption.\nPrebiotics in Your Diet\nPrebiotics include fructooligosaccharides, such as inulin and galactooligosaccharides. But rather than focusing on these lengthy words, include more prebiotics in your day by eating more fruits, vegetables and whole grains such as bananas, onions, garlic, leeks, asparagus, artichokes, beans and whole-grain foods.\nWhat Are Probiotics and What Do They Do?\nProbiotics are the “good” bacteria — or live cultures — like those naturally found in your gut. These active cultures help change or repopulate intestinal bacteria to balance gut flora. This functional component may boost immunity and overall health, especially GI health. For instance, probiotics have been used for management of irritable bowel syndrome symptoms.\nProbiotics in Your Diet\nTo include more probiotics in your eating plan, look to fermented dairy foods including yogurt, kefir products and aged cheeses, which contain live cultures such as bifidobacteria and lactobacilli. Also consider fermented, non-dairy foods with beneficial live cultures, including kimchi, sauerkraut, miso, tempeh and cultured non-dairy yogurts.\nWhat Makes Prebiotics and Probiotics the “Dynamic Duo?”\nUltimately, prebiotics, or “good” bacteria promoters, and probiotics, the “good” bacteria, work together synergistically. In other words, prebiotics are breakfast, lunch and dinner for probiotics, which restores and can improve GI health. Products that combine these together are called synbiotics. On the menu, that means enjoying bananas atop yogurt or stir-frying asparagus with tempeh is a win-win.\nIncorporating health-promoting functional foods, such as those containing prebiotics and probiotics, aids in creating a healthier you.\nFor specific advice on obtaining prebiotics and probiotics for your own specific health needs, especially if you have GI issues or a weakened immune system, contact a registered dietitian nutritionist\nWhat is Down Syndrome? Down Syndrome is also known as Trisomy 21, which belongs to a chromosomal condition. It affects the way how a person develops in terms of mental skills and physical...\nWhat is Down Syndrome? Down Syndrome is also known as Trisomy 21...\nWithout a doubt, anyone who has been a part of any social circle has experienced the ugly face of bullying in this way or another. Thankfully, this problem is brought to the surface more...\nWithout a doubt, anyone who has been a part of any social circle has...\nIf the concept of college entrepreneurs sounds bizarre to you and you do not know how it works, it is high time to find out more with the help of our trusty guide. The trick here is to...\nIf the concept of college entrepreneurs sounds bizarre to you and you...']	['<urn:uuid:597db282-2f0b-449b-b5ea-5e6901b8342c>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	9	27	621
68	what determines cholesterol ratio calculation total hdl	The cholesterol ratio is calculated by dividing your total cholesterol by your HDL (high-density lipoprotein) number. Total cholesterol itself is calculated by adding HDL (good cholesterol), LDL (low-density lipoprotein or bad cholesterol), and 20 percent of your triglycerides.	['Your blood cholesterol level can help indicate whether you’re at risk for heart attack, stroke, or other types of cardiovascular disease.\nYou calculate total cholesterol by adding up the following numbers:\n- high-density lipoprotein (HDL), or good cholesterol\n- low-density lipoprotein (LDL), or bad cholesterol\n- 20 percent of your triglycerides, a type of fat carried in your blood\nYour cholesterol ratio is calculated by dividing your total cholesterol by your HDL number.\nFor instance, if your total cholesterol is 180 milligrams per deciliter (mg/dL) of blood and your HDL is 82 mg/dL, your cholesterol ratio is 2.2.\nAccording to StatPearls, you should try to keep your cholesterol ratio below 5, with the ideal cholesterol ratio being 3.5. That said, the ideal cholesterol ratios for men and women may differ.\nThe ideal total cholesterol level for an adult is\nWomen typically have higher levels of HDL than men. The ideal HDL level is at least 40 mg/dL in men and at least 50 mg/dL in women. This means that men and women may want to aim for cholesterol ratios of 5 and 4, respectively.\nThe researchers found that women with a cholesterol ratio of 3.5 or below had the lowest risk of AMI. In comparison to women with a cholesterol ratio of 3.5 or below:\n- women with a cholesterol ratio of 3.5 to 4.0 were 14 percent more likely to experience AMI\n- women with a cholesterol ratio of 4.0 to 5.0 were 46 percent more likely to experience AMI\n- women with a cholesterol ratio of 5.0 or above were 89 percent more likely to experience AMI\nHDL, or good cholesterol, carries\nOn the other hand, LDL, or bad cholesterol, transports cholesterol to your arteries. It can lead to a buildup of plaque in the arteries, resulting in poor blood flow and a condition known as atherosclerosis.\nCholesterol, even the bad kind, still has its benefits. Your body needs cholesterol for many important functions, such as producing bile acids or hormones such as estrogen and testosterone. However, your liver naturally produces enough cholesterol that you don’t have to worry about getting it through your diet.\nOptimal HDL levels are over 40 mg/dL for men and over 50 mg/dL for women, according to Germany’s\nAn optimal LDL level is under 100 mg/dL, according to the\nEat more soluble fiber\nEating more fiber is good for your overall cardiovascular health. Soluble fiber in particular may help lower your blood cholesterol levels, according to research. Get more soluble fiber in your diet by choosing foods like:\nEat fewer unhealthy fats\nAvoid or limit foods that contain trans fats or certain saturated fats, like the kind found in processed foods. They’re associated with higher levels of total cholesterol and LDL, and lower levels of HDL.\nThese medications lower levels of LDL and total cholesterol in your body. They may also help boost your HDL levels.\nExamples of statins include:\nAccording to the\n- with a history of cardiovascular disease caused by atherosclerosis\n- with an LDL level of 70–189 mg/dL and a 10-year risk of cardiovascular disease — caused by atherosclerosis — that exceeds 5 percent\n- with an LDL level over 190 mg/dL\n- who have diabetes and are 40–75 years old\nSpeak with a doctor about whether statins are right for you.\nTaking these medications is usually a lifelong commitment. Safely discontinuing their use requires you to work closely with your doctor.\nYour cholesterol ratio clarifies the picture of your risk of heart disease.\nA 2020 study looked at the effect of different cholesterol measurements on people’s risk of atherosclerotic cardiovascular disease. The researchers found that people with a higher cholesterol ratio despite low LDL or non-HDL levels were still at increased risk of atherosclerotic cardiovascular disease.\nPeople with a higher cholesterol ratio and low LDL or non-HDL levels were also at a greater risk of atherosclerotic cardiovascular disease when compared to people with a lower cholesterol ratio and high LDL or non-HDL levels.\nThis suggests that clinicians should prioritize the cholesterol ratio over just the LDL and non-HDL cholesterol levels when assessing cardiovascular risk.\nHowever, the ratio alone isn’t enough to assess what treatment will be best if your risk is high. A doctor will still take your total cholesterol into account when determining the correct mix of diet, exercise, and medication to bring your numbers into the desirable range.']	['<urn:uuid:66342faa-128d-4781-af35-072f44c90c65>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	7	38	731
69	kawasaki syndrome vs lpr diagnosis how	Kawasaki syndrome is diagnosed based on characteristic symptoms (high fever plus at least 4 main symptoms) and blood analysis showing autoantibodies and increased white blood cells. LPR has no standardized test for diagnosis - it relies on clinical judgment based on parent observations, physical examination, and possibly endoscopic evaluations by an otolaryngologist. Both conditions may require specialized imaging - echocardiography for Kawasaki and direct laryngoscopy/bronchoscopy for LPR.	['As always, you can contact our office to answer any questions or concerns.\nWhat is laryngopharyngeal reflux (LPR)?\nFood or liquids that are swallowed travel through the esophagus and into the stomach where acids help digestion. Each end of the esophagus has a sphincter, a ring of muscle, that helps keep the acidic contents of the stomach in the stomach or out of the throat. When these rings of muscle do not work properly, you may get heartburn or gastroesophageal reflux (GER). Chronic GER is often diagnosed as gastroesophageal reflux disease, or GERD.\nSometimes, acidic stomach contents will reflux all the way up the esophagus, past the ring of muscle at the top (upper esophageal sphincter or UES), and into the throat. When this happens, acidic material contacts the sensitive tissue at back of the throat and even the back of the nasal airway. This is known as laryngopharyngeal reflux or LPR.\nDuring the first year, infants frequently spit up, and in most infants, it is a normal occurrence that resolves in the first year. Only infants who have associated breathing or feeding problems require evaluation by a specialist. This is most critical when breathing-related symptoms are present.\nWhat are symptoms of LPR?\nThere are various symptoms of LPR. Adults may be able to identify LPR as a bitter taste in the back of the throat, more commonly in the morning upon awakening, and the sensation of a “lump” or something “stuck” in the throat, which does not go away despite multiple swallowing attempts to clear the “lump.” Some adults may also experience a burning sensation in the throat. A more uncommon symptom is difficulty breathing, which occurs because the acidic, refluxed material comes in contact with the voice box (larynx) and causes the vocal cords to close to prevent aspiration of the material into the windpipe (trachea). This is known as “laryngospasm.”\nInfants and children are unable to describe sensations like adults can. Therefore, LPR is only successfully diagnosed if parents are suspicious and the child undergoes a full evaluation by a specialist, such as an otolaryngologist (ear-nose-throat doctor). Airway or breathing-related problems are the most commonly seen symptoms of LPR in infants and children and can be serious. If your infant or child experiences any of the following symptoms, timely evaluation is critical.\n- Chronic cough\n- Noisy breathing (stridor)\n- Reactive airway disease (asthma)\n- Sleep-disordered breathing (SDB)\n- Feeding difficulty\n- Turning blue (cyanosis)\n- Pauses in breathing (apnea)\n- Apparent life-threatening event (ALTE)\n- Failure to thrive (a severe deficiency in growth, where an infant or child is less than 5 percentile, compared to the expected norm)\nWhat are the complications of LPR?\nIn infants and children, chronic exposure of the laryngeal structures to acidic contents may cause long-term airway problems such as a narrowing of the area below the vocal cords (subglottic stenosis), hoarseness, and possibly eustachian tube dysfunction. The latter can cause recurrent ear infections, or persistent middle ear fluid, and even symptoms of sinusitis. The direct relationship between LPR and the latter mentioned problems are currently being researched.\nHow is LPR diagnosed?\nCurrently, there is no good standardized test to identify LPR. If parents notice any symptoms of LPR in their child, they may wish to discuss with their pediatrician getting a referral to see an otolaryngologist for evaluation. In the office, he or she may look directly at the voice box and related structures with a flexible scope or order a 24-hour pH monitoring of the esophagus. The otolaryngologist may also decide to perform further evaluation of the child under general anesthesia. This would include looking directly at the voice box (direct laryngoscopy), trachea and bronchi (bronchoscopy), and esophagus (esophagoscopy). LPR in infants and children remains a diagnosis of clinical judgment, based on history given by the parents, the physical exam, and endoscopic evaluations.\nHow is LPR treated?\nSince LPR is an extension of GER, successful treatment is usually based on successful treatment of GER. In infants and children, basic recommendations may include use of smaller and more frequent feedings, thickening of the food/liquid, and keeping an infant in a vertical position after feeding for at least 30 minutes. A trial of medications, including H2 blockers or proton pump inhibitors, may be necessary. Similar to adults, children with severe symptoms who fail medical treatment or have diagnostic evaluations demonstrating anatomical abnormalities, may require surgical intervention.', 'Kawasaki syndrome is an acute febrile illness characterized primarily by inflammatory reactions of the arterial blood vessels with multiple organ involvement and occurs in infancy (up to 5 years of age). Kawasaki syndrome is mainly found in Japan and with increasing frequency in Germany (about 9 out of 100,000 children).\nWhat is Kawasaki Syndrome?\nInfants and young children suffering from Kawasaki syndrome suffer from severe inflammation of the vessels throughout the body. The inflammation causes symptoms such as fever and a rash.\nKawasaki syndrome (also mucocutaneous lymph node syndrome) is an acute fever illness that primarily occurs in infancy, particularly between the ages of 2 and 5, and is associated with vasculitis, an inflammation of the small and medium-sized blood vessels. See AbbreviationFinder for abbreviations related to Kawasaki Syndrome.\nIn most cases, the Kawasaki syndrome, which is initially very similar to other infectious diseases such as scarlet fever or measles, also manifests itself as a systemic inflammation of the internal organs.\nIn addition to the suddenly occurring fever, there are swellings in the lymph nodes in the neck and throat (cervical lymphadenopathy), non-purulent conjunctivitis (conjunctivitis), stomatitis (inflammation of the oral mucosa) with strawberry or lacquer tongue, a polymorphic exanthema (non-itching, multi-shaped rash on the trunk) and scaly erythema (redness) on the hands and feet characteristic of Kawasaki syndrome.\nThe underlying causes of Kawasaki syndrome have not yet been clarified.\nIt is assumed that the etiology of Kawasaki syndrome is due to an infection with previously unidentified bacteria or viruses whose superantigens (toxic proteins of the pathogens) cause the affected children’s immune system to react with pronounced inflammatory reactions as a result of a genetic defect.\nThe significantly higher incidence (number of new cases) of Kawasaki syndrome in Japan compared to Europe or North America also speaks for a genetic predisposition (predisposition).\nSymptoms, Ailments & Signs\nInfants and young children suffering from Kawasaki syndrome suffer from severe inflammation of the vessels throughout the body. The inflammation causes symptoms such as fever and a rash. The disease is characterized by a gradual rise in body temperature, which is usually associated with chills, weakness and a general feeling of being unwell.\nThe rash presents as painful redness that can appear on any part of the body and occasionally develops into ulcers or cysts. It can also lead to conjunctivitis and swelling of the mucous membranes in the mouth and throat. Also characteristic are the red lips and the reddish tongue, also strawberry tongue. If treated early, affected children usually recover relatively quickly.\nIn severe cases, however, the rash can leave scars or lead to permanent sensory disturbances. If not treated, there is a risk of death. Then the skin lesions worsen and the fever increases. Eventually, there may be complications of the heart and the death of the child as a result. The symptoms usually appear between the ages of one and eight. However, adolescents and young adults can also be affected. Kawasaki syndrome is particularly common in boys.\nDiagnosis & History\nKawasaki syndrome can be diagnosed based on the characteristic symptoms. In addition to the acutely occurring high fever, at least four of the main symptoms of the disease must be detectable in order to assume Kawasaki syndrome.\nIn addition, similar infectious diseases such as measles or scarlet fever should be excluded in the differential diagnosis. The diagnosis is confirmed by a blood analysis. So-called autoantibodies (cANCA), antibodies directed against endogenous antigens, can be serologically detected in Kawasaki syndrome. An increased number of leukocytes (white blood cells) and an increased CRP value (C-reactive protein) also indicate a possible Kawasaki syndrome as inflammatory markers.\nIn order to rule out involvement of the coronary vessels (arterial coronary vessels) or a heart aneurysm (protrusion of the arterial coronary vessels), an echocardiography (heart ultrasound) should be performed.\nIf left untreated, Kawasaki syndrome can lead to an aneurysm of the heart, which can lead to life-threatening secondary diseases such as myocarditis (inflammation of the heart muscle), infarcts, pericarditis (inflammation of the heart sac) and cardiac arrhythmias, which have a significant impact on long-term mortality (mortality rate). With timely diagnosis and treatment, the prognosis is good and the disease usually heals without complications.\nIn most cases, the Kawasaki syndrome in babies and children leads to a very strong fever. Those affected also suffer from a red discoloration of the skin, lips and tongue. The mucous membranes in the throat and mouth also swell, making it difficult to eat and drink.\nIt is not uncommon for babies and children to have Kawasaki syndrome with a rash all over the body and inflammation of the conjunctiva. The patient’s quality of life is significantly restricted and reduced by this syndrome. If left untreated, the syndrome can also cause heart problems and inflammation, which can eventually lead to death.\nThe treatment itself usually does not lead to any particular complications and the course of the disease is always positive. This is done with the help of medication that significantly reduces the discomfort and symptoms. If the treatment is successful, there are no further complications or reduced life expectancy. Consequential damage does not occur with successful treatment of Kawasaki syndrome in babies and children.\nWhen should you go to the doctor?\nA doctor’s visit is necessary if the fever persists or increases. Redness of the skin, an increase in cardiac activity and changes in the complexion must be examined and examined by a doctor. Children up to the age of 5 who suffer from sudden acute symptoms that lead to serious health impairments must be treated by a doctor as soon as possible. Since Kawasaki syndrome in babies and children can lead to premature death in severe cases without medical care, a doctor’s visit is recommended at the first irregularities.\nRed discoloration of the lips and tongue resembling strawberry color should be evaluated. Swelling of the lymph, pain in the lymph nodes or a change in the mucous membranes in the mouth and throat should be presented to a doctor. If conjunctivitis occurs, you need to see a doctor. In the event of cardiac arrhythmias, a doctor must be consulted immediately. The heart muscle can be damaged, which can have serious consequences. If the child refuses to eat and drink enough fluids, there is a risk that the organism will be undersupplied. A visit to the doctor is required without delay so that a life-threatening condition does not develop. If the child shows behavioral problems or an increased need for sleep, a doctor should be consulted.\nTreatment & Therapy\nIn the case of Kawasaki syndrome, the therapeutic measures are primarily aimed at reducing the inflammatory reactions and avoiding aneurysms in the coronary arteries, for which there is an increased risk of manifesting in the second and third week. For this purpose, immunoglobulins, which play an important role in the body’s defenses and cannot be synthesized to a sufficient extent by the children affected by Kawasaki syndrome, are infused (2 g/kg body weight over 12 hours).\nIf necessary, the therapeutic measure must be repeated if the affected child does not respond sufficiently. In addition, high-dose acetylsalicylic acid (30 to 100 mg/kg body weight per day) is used to reduce acute inflammation and fever. As long as there is a risk of a coronary aneurysm, acetylsalicylic acid should continue to be taken at a lower dose (2 to 4 mg/kg body weight per day) because the substance has an anticoagulant effect.\nIf there is a pronounced aneurysm, drugs such as coumarins are recommended to inhibit blood clotting more. If necessary, cardiac surgery measures are indicated for Kawasaki syndrome. In addition, cortisone-like drugs are being researched as part of clinical tests, with at least one additional positive effect for the treatment of Kawasaki syndrome being demonstrated. After the successful end of therapy, regular check-ups by a cardiologist should take place in the case of Kawasaki syndrome in order to rule out long-term damage to the heart (e.g. stenoses).\nOutlook & Forecast\nKawasaki syndrome is a very serious infection that requires medical treatment to avoid complications and possible long-term effects. However, parents of the affected children can help to alleviate the symptoms with simple means.\nIn most cases, cold calf wraps are sufficient to reduce the fever. If these do not help and the fever is dangerously high, a shower can provide relief. Direct cold showers or cold baths should be avoided. The blood vessels in the extremities would constrict quickly and the heart would have to beat faster. This is too much of a strain on the already stressed circuit. If you instead regulate the temperature slowly and not too far, this leads to a moderate drop in body temperature and only minimal strain on the circulatory system.\nBecause children dehydrate very quickly, it is imperative that parents stay hydrated. Patients have to be slowed down in their physical activity in order to avoid possible heart damage and subsequent complications. Simply adding fresh or dried turmeric to food, thanks to its anti-inflammatory effect, helps with both fever and skin rashes, which can also be alleviated with creams.\nA regular cardiological check-up after the symptoms have subsided is advisable so that possible late effects can be treated at an early stage.\nDue to the unknown etiology (underlying cause) of Kawasaki syndrome, no preventive measures exist.\nIn most cases, those affected with Kawasaki syndrome in babies and children have no special or direct options for aftercare. With this disease, however, a doctor should be consulted very early on, so that there are no further complications and no further deterioration of the symptoms.\nAn early diagnosis usually always has a very positive effect on the further course of the disease, so that the parents concerned should contact a doctor as soon as the first symptoms and signs of Kawasaki syndrome appear in the baby and child. The treatment itself is usually carried out by taking certain medications.\nParents should ensure that the dosage is correct and that the medication is taken regularly. If anything is unclear or if you have any questions, you should always consult a doctor first so that no further problems arise. Regular check-ups and examinations by a doctor are also very important for babies and children with Kawasaki syndrome in order to monitor the condition of the affected person over the long term. The child should not exert himself unnecessarily, since the heart is generally weak. In many cases, this disease reduces the life expectancy of those affected.\nYou can do that yourself\nKawasaki syndrome is a serious infectious disease in young children. Medical treatment is necessary to avoid complications and long-term effects. However, there are a number of ways parents can reduce the symptoms.\nThe disease is accompanied by a high fever. Calf wraps or moderate full-body cooling usually lower the body temperature by one to two degrees. Due to the fever, parents should also ensure that their sick child is drinking enough fluids. Still mineral water or unsweetened herbal teas are ideal. In order to avoid complications in the heart, strict attention must be paid to physical protection. Cooling compresses and the application of anti-inflammatory ointments are recommended for treating the rash. Coconut oil – which also has an antiseptic effect – or olive oil are recommended as natural substances. Specially mixed preparations can also be purchased in pharmacies.\nNaturopathy recommends the homeopathic remedy Belladonna for high fever. Furthermore, the administration of vitamin C preparations is recommended, as this vital vitamin is consumed at an increased rate during an inflammatory process. The turmeric root also has a strong anti-inflammatory effect. Parents can buy these fresh in the supermarket or in powder or capsule form (nutritional supplements) in the pharmacy.\nAfter the illness has been overcome, parents should take their child to a cardiologist for regular check-ups in order to be able to promptly identify and treat the occurrence of long-term effects.']	['<urn:uuid:66433615-7e6a-4556-9afd-d3ab0bef5805>', '<urn:uuid:ca5fba62-d897-41cd-bc81-e71f712e5673>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	6	67	2712
70	How do king mackerel impact fishing trips and scientific research studies?	King mackerel serve as a family-friendly fishing target during charter trips, offering exciting surface-level action and self-hooking characteristics ideal for children. They can be caught both near-shore and offshore, providing opportunities during sight-seeing tours and bottom fishing. In scientific research, king mackerel, along with other fish species, are subject to detailed stomach content analysis studies where researchers weigh, enumerate, and identify prey items to understand diet composition, following standard procedures that allow processing of 75-100 stomachs per day.	"['Kings are Gulf’s family fishing ambassador\nBy Jim Cox\nFor most of us, the first offshore fish that ever really made a reel ‘scream’ was probably a king mackerel. Those long, blistering runs of kings always produce adrenaline for everyone on the boat and a nice bounty for the fish box.\nI look at king mackerel, kind of like a Gulf of Mexico family fishing ambassador. They are here to help wherever they can. They can be caught near-shore, offshore, while trolling or bottom fishing. They are also super kid fishing friendly and can even help with a sight-seeing tour of our beautiful coastline.\nI have written about our local charter boats offering four-hour near shore trips. These are great for families with young kids who want to go fishing but maybe aren’t quite ready for a full-day adventure. These four-hour trips will most likely consist of trolling within sight of the beach.\nThese trips not only offer the fishing experience of targeting king mackerel but these charters also provide a chance to see our beaches from the ‘outside.’ Everyone knows what the beach looks like from the beach, but it is an entirely new perspective the first time you see the beach, condos and Perdido Pass from the Gulf. You don’t even need to enjoy fishing to appreciate the scenery.\nKings and kids are a perfect match. King mackerel with their long body shape, represent a chance for young anglers to catch their first ‘big fish.’ Sometimes, the fish are even taller than the kids, which always makes for a great picture. Usually caught on a cigar minnow and thin wire leader when trolling, kings usually hook themselves, allowing for kids to be handed the rod and then, just having to “reel, reel, reel!” That has to be the most uttered, three-word, repetitive phrase in all of fishing!\nKings also stay relatively close to surface which makes for an easier fight for young kids as opposed to trying to horse a red snapper up from a depth of one hundred feet of water. Plus, when a king strips off one or two hundred yards of line, it is just a different type of fishing action than most kids have ever experienced.\nKing mackerel will also make most-welcomed appearances when you are targeting other species of fish. While bottom fishing for any reef fish, charter boats will have a ‘drift line’ tossed out behind the boat without a weight on it. The bait just drifts in the current, waiting for a strike from a nearby, cruising king mackerel.\nWhen that bite occurs, it leads to chaos, the fun kind of chaos on a boat. The rod starts bending and the clicker is going off while the rod is still in the rod holder. Everyone stares at the rod until someone finally grabs it (perhaps with some vocal encouragement from the captain and deckhand) to fight the fish. This happens on almost every drift line bite and leads to laughs on the ride in and back at the dock.\nSometimes king mackerel will join your party but in unannounced fashion when bottom fishing. If you ever find yourself getting cut off numerous times, it may be king mackerel trying to invite themselves onto the boat. Kings will often hang around the same structure that holds snapper. A mono leader is no match for the teeth on a hungry king mackerel. Swapping your mono leader for a wire leader will result in less lost tackle, and you’ll immediately notice the different feel of a king on your line than that of a Snapper.\nKing mackerel, of course, aren’t just for kids. Grown men chase them all over the Gulf with big prize money on the line. Charter boats count on them as part of their daily catch, no matter who is onboard and when a big one hits, it’s not just the reel that starts screaming like a child.\nJim Cox is an avid inshore, offshore, and big game fisherman. He has twice qualified for the prestigious IGFA Offshore Championships in Cabo San Lucas, Mexico. He is the past president of the Mobile Big Game Fishing Club and the host and master of ceremonies for the Blue Marlin Grand Championship.', ""Multispecies Fisheries Research Diet Analysis:\nAll fish that are subsampled are checked for stomach fullness in the field. Full stomachs are removed, labeled and placed in Normalin for analysis back at VIMS.\nStomach samples for both the ChesMMAP and NEAMAP survey are analyzed according to standard procedures (Hyslop 1980). Specifically, each stomach is individually weighed (0.001 g), the contents are emptied, the empty stomach is weighed, and all prey items are identified to the lowest possible taxonomic level. Each item is then enumerated, weighed (0.001 g), and individual length measurements (0.1mm) are taken when possible. Experienced laboratory personnel are able to process, on average, approximately 75-100 stomachs per day.\nThe ChesMMAP survey processes stomachs from every species caught in order to obtain a broad picture of the diet composition of individuals in the Bay.\nDuring the design phase of the NEAMAP Survey, the Operations Committee developed a set of species priority lists to identify and rank those of management interest and, in turn, guide the collection of biological data (see table below). The NEAMAP survey's broad geographic and temporal scale presents a unique opportunity to document and quantify diet habits for a broad variety of species at several trophic levels.\nNEAMAP Priority List\n|Atlantic cod||American shad||Alewife|\n|Black seabass||Atlantic menhaden||Atlantic herring|\n|Bluefish||Atlantic croaker||Atlantic mackerel|\n|Haddock||All skate species||Blueback herring|\n|Pollock||Smooth dogfish||Red drum|\n|Scup||Spanish mackerel||Speckled trout|\n|Silver hake||Spiny dogfish||Tautog|\n|Summer flounder||Yellowtail flounder|\nDiet Composition: It is well known that fishes distribute in temporally and spatially varying aggregations. The biological and ecological characteristics of a particular fish species collected by fishery-independent or -dependent activities inevitably reflect this underlying spatio-temporal structure. Intuitively, it follows then that the diets (and other biological parameters) of individuals captured by a single gear deployment (e.g., ChesMMAP or NEAMAP tow) will be more similar to one another than to the diets of individuals captured at a different time or location (Bogstad et al. 1995).\nUnder this assumption, the diet index percent by weight for a given species can be represented as a cluster sampling estimator since, as implied above, trawl collections essentially yield a cluster (or clusters if multiple size groups are sampled) of the species at each sampling site. The equation is given by (Bogstad et al. 1995, Buckel et al. 1999):\nand where n is the total number of clusters collected of the fish species of interest, Mi is the number of that species collected in cluster i, wi is the total weight of all prey items encountered in the stomachs of the fish collected and processed from cluster i, and wik is the total weight of prey type k in these stomachs. This estimator was used to calculate the diet compositions of the NEAMAP Priority 'A' species (for those where diet data are currently available); the resulting diet descriptions are included in this report. Again, while these diets reflect a combination of data collected from the six fullscale survey cruises (fall 2010 data are not yet available), presentations of diet by sub-area, year, cruise, size, age, etc., are possible.\nThe percent weight (%W), percent number (%N), and percent frequency of occurrence (%F) indices are all useful in different contexts so each is presented here. For %W and %N, only those specific prey types that reach a 1% threshold in the overall diet are shown individually. All others are summed into broader taxonomic categories. Further, for these indices, closely related prey types (e.g. different species of mysids or of amphipods) are generally summed and reported together as a group. For %F, only prey types that reached a 2% threshold in the overall diet are shown individually. It must be noted that for %F, prey types are not additive because each predator sample may be counted multiple times if multiple prey types were consumed. Thus overall percentages for broad taxonomic categories (e.g. fish, molluscs, etc.) is not equal to the\nsum of its constituents. Also, the sample sizes reported under %F are larger than for %W and %N because empty stomachs are counted in the former but not for the latter two. Finally, it is worth noting that the %N and %F indices are calculated using the cluster sampling estimator as\nwell, following the same form given in Equation 3 and Equation 4.\nHyslop, E. J. 1980. Stomach content analysis--a review of methods and their application. Journal of Fish Biology. 17: 411-429.\nBogstad, B., M. Pennington, and J. H. Volstad. 1995. Cost-efficient survey designs for estimating food consumption by fish. Fisheries Research. 23(1-2): 37-46.\nBuckel, J. A., M. J. Fogarty, and D. O. Conover. 1999. Foraging habits of bluefish, Pomatomus saltatrix, on the U.S. East Coast continental shelf. Fishery Bulletin. 97(4): 758-775.""]"	['<urn:uuid:81ac0789-8ed6-4626-82e0-8c6ae072c2e7>', '<urn:uuid:c2ec1ff2-6363-431e-af20-491ff2ac706b>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	11	78	1481
71	advantages gold vs ceramic dental cap	Gold crowns offer greater longevity, require less tooth preparation, and are less abrasive to opposing teeth during chewing compared to porcelain crowns. They are particularly suitable for back molars where visibility is not a concern, especially in patients with strong bites or grinding habits.	"['Dental Crowns: Preserve Damaged Teeth with a Crown\nDental crowns, also known as ""caps,"" preserve the functionality of damaged teeth. A dental crown may be used to protect a cracked tooth, restore functionality of a tooth with excessive decay or replace a pre-existing crown. The purpose of a dental crown is to encase a needy tooth with a custom-designed material. Dentists today have a variety of conservative treatment options through which to restore teeth. If possible, these options should be explored and discussed before selecting the full coverage crown.\nDental Crowns Consultation and Treatment Planning\nIf tooth decay or damage is so extensive that veneers, direct composite bonding or other conservative treatments aren\'t viable treatment options or if you have undergone root canal therapy your dentist will consult with you about dental crowns. Whether used to restore a damaged tooth or to create a lifelike tooth replacement for an implant, crowns can be fabricated in dental laboratories or in your dentist\'s office, depending on the crown material.\nPart of your dental crown consultation may involve taking impressions of your existing tooth (or teeth) as a basis for creating the shape and size of your crown restoration(s). If dental crowns will be used as part of a smile makeover, these impressions are used to make models for designing the new length, shape and alignment of your teeth, so that you can preview your new smile before committing to treatment.\nYour dentist also will describe the tooth preparation process, as well as your options with regard to local anesthesia (to numb your teeth and surrounding areas) and sedation dentistry, if necessary. If your dental crown treatment involves placement of a temporary crown, your dentist will advise you of how long you will need to have the temporary in place and what hygiene steps to take to ensure functionality.\nThe Dental Crown Clinical Procedure\nDuring the dental crown procedure, your dentist prepares the tooth and makes a molded impression of the teeth to send to a dental laboratory. A fitted, temporary crown is created during this visit to temporarily protect the tooth while the final restoration is being made in the dental laboratory. Once completed, the tooth crown is cemented or adhesively bonded at a later visit.\nA recent technology, CAD/CAM technology (computer-aided design/manufacturing technology) has evolved to display a 3-D picture of the teeth. A restoration is then created through milling of a ceramic block. If this technology is located in the dental office (chair-side CAD/CAM), there will be no need for a temporary crown or return visit for the final cementation.\nDental Crowns Recovery and Aftercare\nAfter your permanent crown has been placed, your dentist will provide you with information about caring for your new restoration. In general, maintain proper oral hygiene habits by brushing twice daily and flossing with dental floss or interdental cleaners (specially designed brushes and sticks) once a day. These practices will help to remove plaque from the area where the crown meets the gum, and help prevent dental decay and gum disease.\nAvoid chewing hard foods, ice or other hard objects, since this could damage your crown. It also is important to avoid biting your fingernails and grinding your teeth, which could significantly shorten the life-span of your dental crown. If you habitually clench or grind your teeth, your dentist may recommend wearing a night-time mouth guard to help protect your crowns while sleeping.\nCrown Materials: Gold, Ceramic and Porcelain Crowns\nThe three predominant choices of restorative materials for the full coverage crowns are:\n- All-ceramic (all-porcelain)\nThe material selected is determined by the clinical demands at hand; esthetic demands, strength requirements, material durability and restorative space available.\nPorcelain-fused-to-metal crowns provide for a strong, durable and esthetic treatment option. One of the key factors for the esthetic and functional success of this type of crown is ensuring the preparation of the underlying tooth structure provides adequate space for the appropriate thickness of the material selected. Additionally, the artistic skill of the laboratory technologist creating the crown will determine its esthetic appeal.\nOne consideration in the porcelain-fused-to-metal crown is that these crowns may tend to show the underlying metal or gold margin at the gum line as gums recede over time. Some patients opt for this type of crown, but replace the crown at a later date in order to maintain a higher esthetic benefit. Porcelain-fused-to-metal crowns with an all porcelain collar can eliminate this vulnerability.\nAll-Ceramic Teeth Crowns\nThe predominant material choice for all-ceramic crowns today is either zirconia, or aluminous materials. They provide a metal-free esthetic option with a number of benefits.\nBy eliminating the need for the supportive metal core, an esthetic all-ceramic crown can be created with a reduced thickness of material. This makes them a more favorable treatment choice in areas with limited space. Additionally, the elimination of the metal core allows for light transmission through the porcelain for better optical, life-like properties and a higher level of esthetics.\nAll-ceramic materials continue to evolve in strength and durability, but caution should still be exercised for areas of the mouth requiring heavy function. Continuing research is exploring the significant vulnerabilities of the porcelain systems in such areas.\nAlthough not as popular a treatment choice for esthetic reasons, gold crowns are still indicated in some instances. For example, patients with strong bites and those with parafunctional habits (such as grinding or clenching) might be better served with a gold crown. The traditional restorative material can provide stronger support to the remaining healthy tooth structure. Gold crowns offer a level of durability that is appropriate for teeth located in the back of the mouth (such as the molars), where they will not be highly visible. Gold crowns tend to offer greater longevity and require less preparation than porcelain and porcelain-fused-to-metal crowns. When chewing, gold tends to be less abrasive to the opposing tooth than porcelain. This helps to prevent wearing of the teeth.\nNext, go to:']"	['<urn:uuid:117f204b-b7b4-4826-ad4e-f79ca0a62ba0>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	6	44	991
72	modern plastic wood alternatives maintenance required	uPVC decking is a plastic alternative to timber that requires very little maintenance. It comes in a wide range of styles and colors.	"[""Decking design guide\nWhen it comes to designing decking, a bit of creativity and courage definitely go a long way. Find out what kinds of things should guide your thinking when you're deciding on decking designs.\nNot all timbers are suitable for all aspects of the construction of pergolas and gazebos. Some may be ideal for infills, but not for structural beams. Likewise, not all timbers suit all climates.\nAre they for traction? For looks? To keep the boards dry? Or are they there for a different reason altogether? Find out why decks have grooves, and which way they're supposed to face.\nDecking looks pretty simple, but don't let that fool you - there's quite a bit of effort that goes into designing and building a safe, durable deck. Read on to find out how decks are made.\nProperly cleaning and oiling your deck will help to bring out its natural colours and make it look like new. Find out the best way to go about it.\nDecks can be used for many things - and designed in a number of different ways to suit different purposes. Find out about freestanding decks, pool decks, multi-leveled decks, handrails, infills and more.\nDecking can be made from all sorts of different timbers, and can be coloured, stained and weatherproofed using decking oils and treatments. Find out more about each, and their particular properties.\nMerbau, also known as kwila and ipil, is a tropical timber which inhabits mangrove forests. Merbau is attractive and termite resistant, although its use has a considerable environmental impact.\nJarrah is a native eucalypt from Western Australia, which is valued as a durable, termite and weather resistant hardwood. Jarrah has a beautiful, uniform and long lasting deep red grain.\nBamboo decking is relatively new but increasingly popular, and considered to be environmentally friendly and sustainable. Good bamboo is extremely tough, and is naturally repellant to termites.\nTreated pine is in plentiful supply, and is very affordable. It offers a 'softer' feel underfoot than some other types of wood. Pine must be properly treated to prevent termites and weather damage.\nCypress is the name given to a range of different species of conifer. Cypress is durable, stable and resistant to termites and insects, and offers a range of attractive colour options.\nRed Gum refers to a number of Australian species (most commonly River Red Gum). Red gum offers beautiful hues and graining, and is rot and termite resistant but may degrade with humidity changes.\nSpotted gum, also known as Lemon-Scented Gum, is native to coastal areas in eastern Australia. As decking, it is attractive, tough and fade-resistant, and be stained and finished quite easily.\nSilvertop Ash, also known as Coast Ash, is native to Australia. It is sustainably grown readily available, and is strong and durable. Silvertop Ash is not as resistant to termites as some other timbers.\nComposite decking is a type of decking material normally composed of plastics, timber and sawdust. Composite decking is designed to look like wood, requires very little maintenance and is quite strong.\nuPVC decking is a plastic alternative to timber and composite decking and cladding materials. It comes in a wide range of styles and colours and requires very little maintenance.\nDecking oil is a type of oil used to penetrate and protect timber decking from the elements and general deterioration. Decking oil can be used to rejuvenate timber and restore its natural colour.\nA number of different stains, paints and acrylic treatments can be applied to timber, both to improve the way it looks and to help to make it more resistant to weather and termites.\nFreestanding decks are decks that don't attach directly to your home or any other fixed structure. Freestanding decks offer a good way to create a stable living area separate from your house.""]"	['<urn:uuid:54ffa0a6-ca7d-4aec-ac9b-fef30a050fa4>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	6	23	636
73	What do Sri Lankan newlyweds do with oil lamps during the ceremony?	During Sri Lankan weddings, the couple lights two oil lamps, representing the union of two souls in marriage. These lamps are believed to bring good luck throughout the couple's married life.	['Are you searching for a wedding experience like no other? Look no further than the enchanting Sri Lankan Wedding Traditions. From pre-wedding rituals to post-wedding celebrations, this culture offers something truly special and unique. Let’s delve into the fascinating customs, superstitions, and practices that make Sri Lankan weddings a truly unforgettable occasion.\nPre-Wedding Traditions: Building Bonds and Happiness\nIn Sri Lanka, pre-wedding traditions hold immense significance. These rituals serve to ensure the happiness of both families and provide an opportunity for them to deepen their connection. Let’s explore some of these customs:\n1. Engagement Ceremony: A Token of Commitment\nThe engagement ceremony takes place at the home of either the bride or the groom. It involves the exchange of gifts between the two families, symbolizing their commitment to one another. This auspicious event is also a chance for family members to offer blessings for future good fortune.\n2. Bride’s Henna Night: Embracing Tradition\nA few days before the wedding, the bride celebrates her henna night. Intricate henna designs are carefully applied to her hands and feet. This time-honored practice, believed to bring good luck and ward off evil spirits, is accompanied by the loving presence of friends and family.\n3. Dowry: A Symbolic Gesture\nBefore marriage, both sets of parents engage in dowry negotiations. These discussions determine the items to be exchanged, such as jewelry, furniture, or money. This tradition acknowledges the contributions made by both families towards the success of the union.\nThese pre-wedding traditions are integral to Sri Lankan weddings, adding beauty and excitement to the journey toward the big day.\nWedding Day Traditions: Symbolism and Commitment\nThe wedding day itself is filled with meaningful traditions that highlight the bond between the bride and groom. Let’s explore a few of these time-honored rituals:\n4. Garlands: An Exchange of Respect\nThe exchange of garlands is a cherished wedding ritual. It symbolizes the mutual respect and admiration between the bride and groom. As they honor each other with flowers, they exchange garlands three times in a clockwise direction—a powerful gesture reaffirming their commitment.\n5. Oil Lamps: Illuminating a New Path\nLighting oil lamps is a beloved tradition at Sri Lankan weddings. This ritual involves lighting two lamps, representing the union of two souls in marriage. The flame from these lamps is believed to bring good luck throughout the couple’s married life.\n6. Seven Steps: Promises for a Lifetime\nTaking seven steps around a fire is a centuries-old custom in Sri Lankan wedding ceremonies. This symbolic act represents the couple’s vows and promises, such as faithfulness, loyalty, love, understanding, friendship, trustworthiness, and respect for each other’s families and cultures. It serves as a powerful reminder of their commitment to one another through all of life’s challenges.\nFrom the traditional exchange of garlands to the sacred seven steps around the fire, Sri Lankan wedding day traditions are rooted in centuries-old customs and rituals.\nPost-Wedding Traditions: Celebrating a Joyous Start\nAfter the wedding ceremony, a range of post-wedding traditions take place in Sri Lanka. These customs aim to celebrate the newly married couple and ensure they embark on their marital journey with unforgettable memories:\n7. Reception Party and Banquet: A Festive Congratulation\nA grand reception party follows the wedding ceremony, bringing together family members, friends, and guests to congratulate the couple on their union. The banquet that follows offers a traditional feast of rice dishes accompanied by various curries. This multi-day celebration includes music, dancing, games, and more.\n8. Gift Giving and Blessing Ceremony: Wishing for a Blessed Union\nAfter the reception, the time for gift-giving arrives. Guests present gifts, such as jewelry or money, to the bride as symbols of good luck in her new life with her husband. Following this tradition, the couple receives blessings from elders before embarking on their honeymoon trip.\nPost-wedding traditions often involve a honeymoon trip, either to a temple or a resort, depending on the couple’s preferences. Temple visits allow couples to seek blessings, while resorts offer a luxurious setting for relaxation, complete with amenities like spas and pools.\nThese celebratory customs ensure that the newlywed couple receives blessings for their future as they embark on their journey together.\nCultural Customs and Superstitions: Adding Meaning and Vibrancy\nSri Lankan weddings are a treasure trove of cultural customs and superstitions that make the occasion truly extraordinary. Some of these traditions include:\n9. Customs and Superstitions: A Splash of Color\nSri Lankan weddings embrace vibrant attire for both the bride and groom. The bride traditionally wears a sari or a lengha choli, while the groom dons a sherwani or a kurta pyjama. These colorful garments symbolize joy and happiness on this special day.\n10. Music and Dancing: A Rhythmic Celebration\nTraditional music plays a significant role in Sri Lankan weddings. Popular songs, including Baila and Kandyan music, fill the air as guests joyously sing and dance to these melodies throughout the night. Drummers add an extra rhythmic element to the celebration. The couple themselves may even join in the dancing, creating unforgettable memories.\n11. Food: A Feast for the Senses\nSri Lankan weddings are renowned for their delectable cuisine. The wedding feast typically includes a variety of curries, rice dishes, and vegetarian options. Traditional desserts like puttu and kiribath (milk rice) are also served, adding sweetness to the celebration. However, there are certain foods to avoid on the wedding day, as per superstition. These include seafood, bitter vegetables, and dishes made with tamarind paste. By adhering to these customs, the couple can ensure good luck for their new life together.\nThese cultural customs and superstitions, passed down through generations, remain an integral part of Sri Lankan weddings. They infuse the occasion with joy, celebration, and a deep connection to the country’s rich heritage.\nConclusion: Celebrating Love and Tradition\nSri Lankan wedding traditions are an exquisite mosaic of culture and customs that make each celebration truly unique. From pre-wedding rituals to post-wedding festivities, every step is imbued with meaningful symbolism. These time-honored ceremonies provide a wonderful opportunity to honor your heritage while celebrating the union of two families. Whether you’re incorporating these traditions into your own special day or simply curious about what makes Sri Lankan weddings so extraordinary, understanding these customs will make your wedding experience truly unforgettable.']	['<urn:uuid:6f676299-ebd1-47d7-bf72-90ee14c1a99a>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	12	31	1040
74	features landsat imagery vs geospatial point data	Landsat Earth-imagery data, available through Amazon Web Services (AWS) as a petabyte-scale public dataset, consists of satellite imagery and Earth observation data. In contrast, point data is a type of geospatial data that contains information about specific points in 3D space, such as exact locations of trees or airplanes, typically stored in vector files with X, Y coordinates for map representation. Both types can be used for climate and environmental analysis, with Landsat imagery being particularly useful for large-scale Earth observation, while point data is more suited for precise location-specific analysis.	['The Third National Climate Assessment confirms that ecosystems and the benefits they provide to society are being affected by climate change. These changes are having impacts on biodiversity and limiting the capacity of ecosystems—including forests, barrier beaches, and wetlands— to continue to play their roles in reducing the impacts of extreme events on infrastructure, human communities, and other valued resources.\nLand and water managers, environmental planners, and those who rely on ecosystems to support and run businesses need easy, intuitive access to the most accurate and relevant available information about climate change in order to make informed decisions on the ground.\nToday, in an important milestone to help achieve this goal, the Department of Interior and other Executive Branch agencies and offices are releasing, on climate.data.gov, new troves of government data on water and ecosystems, as well as new geospatial tools, as part of the President’s Climate Data Initiative. Earlier installments of that initiative focused on data relating to sea-level rise, flood risk, and agriculture.\nThe newly released datasets—which include critical information about streamflow, soil, landcover, and biodiversity and are complemented by tools to overlay and visualize them—will be extremely valuable to natural-resource managers faced with day-to-day and long-term strategic decisions about how to operate in the context of climate change.\nIn a further step to make these data as useful as possible, today a host of public, nonprofit, and private-sector organizations made commitments to devote resources, expertise, and technological capabilities to leverage climate data in ways that make the Nation’s ecosystems and water resources more resilient to the impacts of climate change.\nFor example, Amazon Web Services (AWS) has committed to make a petabyte of [Landsat] Earth-imagery data from the U.S. Geological Survey widely available as an AWS public dataset; the University of Maryland’s Socio-Environmental Synthesis Center has committed to fund 25 early-career scholars as they conduct research at the nexus of water systems, food systems, and climate change; Esri will stand up a Water Open Data portal to extend accessibility of key water data through interactive services and tools by which selected data can be downloaded in various formats through an intuitive user interface; and HP has announced a partnership with the Camera Trap Data Network to create new data-sharing and analytic tools that allow users to access and analyze millions of camera-trap images and related data about threatened species and biodiversity.\nFrom the Harnessing Climate Data to Boost Ecosystem & Water Resilience Fact Sheet released today:\nAmazon Web Services (AWS) is today committing to make up to a petabyte of Landsat earth-imagery data from the USGS widely available as an AWS Public Data Set. AWS and its collaborators will also contribute expertise, open source tools, and educational materials to accelerate climate research by making Landsat data easier to access and analyze.\n“By unleashing the power of our vast and open data resources, the Climate Data Initiative helps spark private sector innovation and will leverage resources for those on the front lines who are dealing with climate change. We are pooling into one place data from across the federal government to make it more accessible to the public and we hope our efforts will inspire other countries to follow suit.” –Secretary of the Interior Sally Jewell\n+ OSTP FACT SHEET: Harnessing Climate Data to Boost Ecosystem & Water Resilience [PDF]\n+ Secretary Jewell Announces New Tools to Help Communities Build Resilience to Climate Change\n+ Earth Science on AWS with new CGIAR and Landsat Public Data Sets, Amazon Web Services Blog', 'What is Geospatial Data? Types, Python code, Analytics & Visualization\nWelcome to my blog, where we explore the fascinating world of geospatial data!\nHave you ever wondered how analysts use Python to make sense of complex geographical datasets for visualizing and predicting trends in analytics? In this blog, I’ll break down the different types of geospatial data, discuss the advantages and disadvantages of using each type, and show you how Python can be used to make data analysis a breeze. So let’s get started on our journey through the world.\nGeospatial data is time-based data related to a specific location on the Earth’s surface. It can provide insights into relationships between variables and reveal patterns and trends.\nIntroduction to Geospatial Data\nGeospatial data is information that is tied to a specific location on the Earth. It can be used to represent things like roads, buildings, and geographical features like mountains and rivers. This data is often shown on maps, and it can be used for a variety of purposes, such as helping people find their way around a city, understanding the layout of a region, and studying how different parts of the Earth are connected and interact with each other. Geospatial data is collected using tools like satellites, GPS devices, and aerial photographs, and it is often stored in computer systems that can help us analyze and understand it in more detail.\nGeospatial data, also known as geographic data, refers to any data with a location element. It is usually associated with mapping data and can be used to create maps or analyze geographic trends. Geospatial data includes geography and spatial analysis aspects such as coordinates, elevation and shapefiles.\nPython is a popular language for working with geospatial data due to its libraries such as GeoPandas, Shapely, and Fiona. With these tools, users can read in existing geospatial datasets in various formats such as GeoJSON, KML and ESRI shapefiles. Users can also create new geospatial datasets from scratch using powerful Python tools such as geopandas.\nAnalytics of these datasets typically takes the form of calculating distances between different geographic entities such as cities or points on the earth surface or finding nearest neighbors for a given point. Visualization of this information is one of the key tasks for understanding any trends or patterns in the spatial dataset being examined. Common types of visualization include choropleth maps that display boundaries filled with color gradients based on certain attributes associated with those boundaries (such as population density).\nIn summary, Geospatial Data encompasses location-based information that can be analyzed and visualized to gain insight into any trends or patterns present within its related area. Python provides powerful resources for working with spatial information that make its manipulation relatively easy while analytics and visualization provide the best way to understand results derived from those manipulations.\nTypes of Geospatial Data\nGeospatial data refers to any data that has an associated geographic or spatial element, including points, lines and polygons. This type of location-based information is created by combining geographical information systems (GIS), remote sensing, and global positioning systems (GPS). Geospatial data may be collected from a variety of sources, including both digital and non-digital sources, such as aerial imagery, GPS receivers and surveyors.\nExamples of geospatial data include:\n- Point data: This type of geospatial data contains information about a specific point in the 3D space, such as the exact location of a tree or an airplane. It’s commonly stored in vector files with simple X, Y coordinates to represent the points on a map.\n- Line data: This type of geospatial data consists of a set of points connected by straight or curved lines. It’s typically used to show linear features such as roads or rivers in GIS mapping projects.\n- Polygonal/Area data: This type of geospatial information describes geographical areas with boundaries that are represented by adjacent polygonal shapes. It’s often used to represent land parcels such as tracts or counties when creating GIS maps.\nGeospatial analytics enable organizations collecting and using this type of location-based information to gain insights from their datasets through processing techniques like machine learning algorithms and advanced visualization tools for two dimensional (2D) or three dimensional (3D) output. Python libraries are also widely used for programming related to geospatial analysis tasks because they provide dedicated packages for working with GIS datasets like GeoPandas and Rasterio modules.\nGeospatial Data in Python\nPython is a popular language for data science and Machine Learning, and it has gained immense popularity due to its flexibility, scalability, and robust set of libraries. One of the most significant advantages of Python is its use in geospatial data processing. Geospatial data (GIS) is a term that describes information about geographical features. This data can be used for a variety of applications, from aerospace engineering to urban planning.\nGeospatial data in Python makes use of specific libraries built specifically for processing, analyzing, and visualizing GIS datasets. Some popular tools used for geospatial analysis in Python are GeoPandas, geopandasGF (which builds on geopandas), PySAL, Folium, PyQGIS (the official library for interacting with QGIS), Shapely, PyProj and more. These tools provide powerful mapping capabilities that allow users to integrate existing GIS datasets with their own analysis tools into user-friendly web or desktop applications.\nGeospatial analytics in Python includes working with both vector and raster data – such as geometries like points, lines, polygons , or pixels generated from aerial imagery – often further probed through linear regression algorithms such as Ordinary Least Squares or variograms born out of interpolation methods like Kriging . Integrating these analytical approaches with other types of statistical modelling can provide insights into new types of problems which cannot be solved by traditional GIS approaches alone.\nOther important topics include cartographic visualization techniques – ranging from basic plotting libraries up to crafting complex geographic charts – as well as pattern recognition through kernel density estimations or building spatially-constrained models with the help of spatial autocorrelation functions . All these topics require detailed knowledge about the underlying concepts that drive the respective task at hand. However due to their direct application into ‘real world’ problems they might prove invaluable for solving problems at hand with an added spatial element attached\nGeospatial Analytics is a Trend\nGeospatial data, or geographic information systems (GIS) data, are the foundation of geospatial analysis. This type of data includes digitized maps, aerial photos and satellite imagery that provide geographical information about a particular area. It also includes other types of non-spatial data such as address points, relationships between features and metadata.\nGeospatial Analytics is the process of using GIS data to generate insights into a particular area. It is an important tool for decision makers in a variety of fields including urban planning, public health and business analytics. In Geospatial Analytics, the GIS data is analyzed to uncover trends and patterns in the area being studied. The findings from geospatial analysis can then be used to create more efficient solutions for businesses or guide policy decisions for government agencies.\nGeospatial analytics uses advanced software such as Python in conjunction with specialized libraries like geopandas to blend GIS datasets and perform powerful spatial analysis. This software makes it easy to visualize complex trends within GIS datasets with interactive maps, graphs and charts. Data can be further explored with advanced analytics techniques like machine learning to predict future outcomes for areas being studied. With geospatial analysis, professionals have better tools than ever before to solve big problems using accurate geographic information systems data.\nGeospatial Visualization in 2023\nGeospatial visualization is an important tool for analyzing and interpreting data related to a geographic area. This method of data analysis involves using software programs like Python, analytics, and visualization tools to construct detailed visuals of the physical characteristics within a given region. By creating digital maps, researchers can gain insight into the patterns of population density, land use, environmental features, natural resources, and other data parameters in an area. By leveraging such information within industry sectors such as agriculture, transportation & logistics, healthcare services, manufacturing operations – businesses can better plan for future operations with accurate insights into their surrounding environments.\nUsing geospatial visualization allows stakeholders to examine different types of data on a granular level such as weather disturbances, disease outbreaks among agricultural products or asset management. With this approach it is possible to represent fields in subtle color gradations against other features that may lend understanding towards more acutely targeted explanations. Visualizing geospatial data also enables researchers to identify key patterns and correlations between geographic features and targeted progress points across large areas or even worldwide regions.\nTOP tools for visualization of your GIS Data\n- ArcGIS Online\n- GRASS GIS\nGeospatial Data Sources\nGeospatial data, also known as geographic information or spatial data, is a highly structured form of data used to represent real-world physical and cultural features on a two- and three-dimensional map. It includes the coordinates of points, lines and areas; attributes about the features; relationships between them; and geometric arrangements for efficient visualization of large data sets.\nUse this links to find digital geospatial data sites.\nPrimary Sources of FREE USGS Geospatial Data\n- The National Map – Go to The National Map Download Client to download US Topo and historical topographic maps, and data for the United States for various themes to include:\n2) Elevation Products (3DEP);\n3) Elevation Source Data (3DEP);\n4) Hydrography (NHD) and Watersheds (WBD);\n5) Imagery from High Resolution Orthoimagery and National Agriculture Imagery Program;\n6) Map Indices;\n7) Geographic Names;\n8) Land Cover;\nContours are under Elevation Products, and lidar point clouds are under Elevation Source Data.\n- Small-scale data, formerly on The National Atlas, includes Printable Maps, Small-scale Framework data, and Global Map data (1:1,000,000 scale).\n- The National Map Services provide a complete list of service endpoints, which contain REST, WMS, WMTS (cached base maps only), WFS, WCS, and other service links.\n- Earth Explorer*- Download data from more than 300 data collections that span over 75 years of coverage. Includes USGS satellite imagery, historical and recent USGS aerial photography, elevation data, land cover products, Lidar point cloud data, Digital Line Graphs (DLGs), Digital Orthophoto Quads (DOQs), and much more. How to use Landsat Spectral Bands can be helpful.\n- GloVis*- Just looking for satellite imagery? Don’t really need a robust and complex search engine? GloVis is a browse-based viewer that’s easier to search and more user-friendly than EarthExplorer. Products include Landsat, ASTER, EO-1, MODIS, and TerraLook imagery, NAPP and NHAPimagery. How to use Landsat Spectral Bands can be helpful.\nAdditional FREE Geospatial Data from the USGS & Our Partners\n- LandsatLook*-Full-resolution JPEGs of Landsat satellite images that are best used for image selection and simple visual interpretation. Available as thermal or natural color images with optional geographic reference. Not recommended for digital analysis.\n- Emergency Response Portal*-Geospatial imagery for national and international disasters.\n- Science Data Catalog*-Metadata and links to all USGS data. Search capabilities are somewhat limited due to the size of certain datasets.\n- National Geologic Map Database-Search over 90,000 maps and reports, including free digital maps and GIS map data. Primary themes are geology, geophysics, marine, energy resources, and natural hazards.\n- Hazards Data Distribution System (HDDS)*-A dynamic online map interface that can be used to view and download USGS datasets for national or international disasters including floods, hurricanes, tsunamis, and earthquakes.\n- Water Spatial Data-Maps and GIS data for water related topics such as water use maps and data, watershed boundaries, NHD, streamflow records, and more.\n- Quaternary Fault and Fold Database of the United States– Geologic, geomorphic, and geographic information for over 2,000 Quaternary (active in the last 1,600,000 years) faults in the U.S. Download shape files and Google Earth files.\n- Mineral Resources On-Line Spatial Data*- A source for regional and global geology, geochemistry, geophysics, and mineral resource maps and data. Includes map services.\n- Global Mineral Resource Assessments*- Most of these reports contain GIS data. The USGS is in the process of compiling all the data in a single location, but for now users must review each link.\n- Energy Data Finder*- Geospatial data related to oil & gas, coal, geochemistry, and geophysics. This is also a good place to look for base data in international locations (used in global energy assessments).\n- Maps of America’s Submerged Lands– A portal to published USGS maps, reports, and digital data that depict the bathymetry, surficial geology, and/or subsurface structure of selected submerged U.S. areas. This is mostly coastal locations, but some lakes are included.\n- Coastal & Marine Interactive Map Server– GIS data and metadata for past and current projects.\n- Astrogeology MCTR GIS Lab– Graphical, statistical, and spatial GIS tools for analyses of planetary data. All downloads are free. Links to Interactive GIS maps of Mars, the Moon, and Venus.\nGeospatial data comes in a variety of forms. Traditional sources include aerial photography, satellite imagery, paper maps and census records. Modern geospatial technologies increasingly use ever larger datasets collected through sensors ranging from GPS systems to mobile phones to automated vehicle Recognition (AVR) systems.\nThe rise in geospatial technology has drastically increased the availability and sophistication of geospatial datasets from both public sector (e.g., government) sources as well as an exploding number of private sector organizations actively collecting their own location-based commercial data (e.g., web traffic). Consequently, most sectors now rely on geospatial technology solutions including navigation systems, environmental monitoring systems, analytics engines and visualization software applications such as Python’s geopandas library for geographic analytics or QGIS for visualization tasks.\nBy understanding its sources and capabilities, businesses can leverage the power of geospatial technology to optimize product delivery updates , enable customer targeting programs based on locationarlytics , offer engagement mobile solutions (like Walmart Inc.’s Walmart Pay system), or improve their disaster risk management programs .\nChallenges of Working with Geospatial Data\nThe use of GIS data has become increasingly popular in recent years; it provides a powerful tool for analyzing spatial data related to locations and activities. However, working with large datasets of this type can pose a challenge. Geospatial data often has different formats and fields than other types of information, and some users lack the technical skills or resources required to understand and work with them efficiently.\nPython is an excellent programming language for handling geospatial data, as it is designed specifically for manipulating such large amounts of spatial information. Geoprocessing tools available in Python provide an effective means of manipulating geospatial data, enabling the user to format the data correctly, filter it according to different criteria and complete analytics tasks quickly.\nGeoanalytics refers to using geospatial technology to create actionable insights from location-based data. Advanced analytics tools can be used to explore patterns within geo-data sets and identify trends across varying spatial dimensions. For instance, businesses can use geo-analytics software to gain insights into customers’ purchasing habits by analyzing location information gathered from sales transactions.\nVisualization is also a fundamental part of working with geospatial data. By plotting the results from geo-analytics on a map, users can gain valuable insights into their datasets that would otherwise be difficult or time-consuming to uncover within the raw numbers or text alone. Whether using existing visualization tools such as ArcGIS Story Maps or building custom maps using GeoJSON files, visualizing your results brings your analysis process full circle.\nDespite its complexities and differences from other forms of datasets, working with geospatial information can open up new opportunities for businesses when used correctly in combination with analytics and visualization techniques. Understanding the challenges associated with this type of data is key to maximizing its potential benefits for any organization looking maximize results from their location-based studies.\nGeospatial data is a specialized type of data representing information related to our planet’s physical locations. It can include variables such as latitude, longitude, terrain elevation, and population density, among many others. These complex datasets are used in activities such as mapping and analytics to analyze different patterns and trends across the world. Geospatial data offer key insights in order to answer complex geographical questions and figure out the best ways to use physical space for a variety of applications.\nPython is one of the most popular programming languages for working with geospatial data due to its wide range of tools available for importing, manipulating, and visualizing this type of data. Furthermore, Python’s libraries make it easy to create powerful analytics dashboards or interactive maps with dynamic features. Additionally, geospatial data visualization has become an essential tool for presentations in recent years as it enables communicators to more easily understand patterns in large datasets and key pieces of information when it comes to making decisions related to land management or policy initiatives.\nIn conclusion, geospatial data is critical for businesses seeking geographic insight into their activities or operations when developing products or making strategic decisions. With the range of open source libraries available that enable easy manipulation of this type of information using Python coding language skillsets have become highly sought after professionals across many industries today.']	['<urn:uuid:00040860-d6f3-4510-b346-4c0f93ce317f>', '<urn:uuid:441baed1-e9fb-400f-be79-3c8027e27c20>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T01:37:09.743768	7	91	3423
75	How do performance evaluations differ between a distribution center audit and a partnership assessment in terms of their key metrics and goals?	Distribution center audits primarily evaluate operational metrics such as productivity, accuracy, storage density effectiveness, facility throughput, safety, and ergonomics. These audits involve examining each functional department and comparing actual operations to established procedures. In contrast, partnership assessments focus on financial and ownership-related metrics, such as profit distribution, partner contributions, and liability allocation. While distribution centers measure success through operational benchmarks against competition, partnerships measure success through profit sharing arrangements, which can vary from equal distributions to more complex models like 'eat-what-you-kill' or 'lockstep' compensation systems used in law firms.	"[""Distribution center operations — from the smallest stock rooms to the largest multimillion-square-foot facilities — all share common characteristics. Every operation has staff executing the required daily tasks with tools, systems, equipment and other aids provided by their management, and managers overseeing and guiding the operation, using standard operating procedures, planning forecasts and metrics, all of which are either actively developed or passively understood over time. Every distribution center is similar in these respects to every other center, but no two operations are exactly alike — just like people. And just like people, the health and robustness of a distribution center operation often depends upon regular checkups to ensure the facility and operations are consistently performing at the peak of their potential, even as the facility ages, process demands placed upon it change, and staff and management grow, shrink or turn over time. The best way to ensure the operation is performing as well as possible is to give it a checkup. In short, a pre-emptive audit of sorts — not necessarily to address an identified issue but to evaluate what the operation could be doing better, where it is falling short of customer requirements and where it may be in danger of being outperformed by the competition's operation.\nOnce you decide to perform an audit or assessment of your operation (pick whichever term you believe will evoke the least amount of adversity within your organization's staff), you can execute the audit in several ways. The detail level of the audit, particularly an initial diagnostic audit, is very important. There are pitfalls associated with some “higher-level” approaches as well as risks with an approach focused only on individual operating department details. Operational audits cannot focus only on the discrete tasks within the operation and attempt to evaluate or optimize them. Such an approach without a keenly developed awareness of the interdependencies of each task within the distribution operation often leads to overall suboptimization. Likewise, simply evaluating the overall facility output in terms of cost per unit handled or order fulfilled will provide information regarding where you stand compared to benchmarked competition or operations, but may do little to inform where to specifically focus on how performance might be improved or costs reduced. Alternately, a preferred approach is to evaluate each functional department, including operations management approaches and the tools available to facilitate those approaches, and optimize the entire operation by formulating solutions for each functional area requiring them. That will also contribute synergistically toward the entire operation's performance.\nThe first step in the process is to identify the audit team to execute the evaluation. Ideally these need to be individuals representing both direct labor and management. Team members should be open-minded and willing to objectively assess the observations, findings and discoveries of the team. Participants should be from all areas of your organization affected by your operation and the areas of your business you are concerned with. If order cycle time through your facility is one of the areas you plan on examining, then including some appropriate representation from your customer service group or its equivalent, in addition to your distribution and logistics personnel, is advisable. If you cannot comfortably identify internal personnel who you believe can be objective and focused enough to add the appropriate value to the audit project, then it may be in your best interest to utilize outside assistance from an industrial engineering or business consulting organization with proven experience in your area of business and proven capability in facility operations and design. Remember, you are looking to find assistance in evaluating your operation, not teaching the particulars of your line of business to a generalist consultant. Even a former retired professional in your line of business may be able to offer objective insights that could enhance the effort.\nOnce you have the team assembled, the next step is to have the entire team follow your process through your entire operation — from pre-receipt information visibility and planning against projected business to physical receiving, put away, pick line replenishment (if applicable), order-picking, packing, assembly, shipping and loading. This operational observation allows the team to perform many assessments and evaluations of the operation. These observations should facilitate:\n- Comparison of actual operations to previously established standard operating procedures\nAre employees following standard operating procedures? If not, is it to the operation's benefit (order demands may have changed since SOPs were developed) or detriment?\n- Education of nondistribution personnel on the realities of distribution performance ?\n(i.e., customer service, procurement personnel). Sometimes a more robust understanding of what is required in the physical handling of product and assembly of orders aids in nondistribution personnel understanding what they might be able to do differently to lower costs or allow the operation to be more effective.\n- An immediate breeding ground for ideas regarding improvements\nthat employees could make in the operation at the departmental level at the very least. This is the proverbial “low-hanging fruit” that could be easy to harvest.\n- Develop an understanding of current performance standards\n— in terms of productivity, accuracy, storage density effectiveness, overall facility throughput, safety and ergonomics — that can allow you to compare your operation to other operations or industry benchmarks.\n- Evaluation to determine if there are practices themselves that may need further study\nThe operational walkthrough, which should require anywhere from a full day to a week depending upon the size and complexity of the operation, should allow the assessment team to diagnose what might be improved. This could include updating slotting in pick areas to move product that has passed its prime in terms of customer demand to a more remote area of the facility to aid in picking productivity. It could lay bare the fact that picking or put away rates in the facility are below the known industry benchmarks for like operations, or it might show an unacceptable delay in problem resolution, be it in receiving, picking, equipment reliability or any other area where unresolved operational issues slow down throughput and productivity at the very least.\nThe understanding garnered from the operational observations should drive a candidate list of changes at the functional department level, up to and including identifying the need for further study of areas where a large amount of the operation's resources — both human resources and equipment — are devoted. Areas such as order picking or order assembly and packing tend to require more of both than many other functional departments. Further study could point to researching the benefits of developing work standards, enhancing or implementing a slotting tool for storage areas or forward pick lines, or adjusting storage aids and devices to best service the operation's inventory storage requirements.\nOnce the results of the operational evaluation begin to coalesce into potential action items, you should assess the management approach to overseeing the operation for several possible areas of improvement. Are the methods of day-to-day management still viable and supporting the operation in the best way possible? Communicating with fork operators by talking to them when they pick up their work in a paper-based 100,000- square-foot operation may no longer be as effective if the operation has grown to 500,000 square feet. Radios for each lift operator and management may be advisable at a minimum. Are there enough management or supervisory personnel (span of control) on the floor, given the volume and complexities of the operation, to both address issues as they will inevitably arise and drive consistent execution? Does management have a formal daily planner to evaluate staffing needs by department based on the complexion of each day's workload? If management does utilize a formal daily planner, is the tool robust enough and updated frequently enough to be effective in the operation? If electronic Advance Shipping Notification (ASN) information is not currently being used, distribution management may need access to any available ASN information in order to properly staff receiving functions by using such forward-looking information to identify workload before it even arrives at the facility. Finally, are there set metrics that management is working toward achieving on a consistent basis? Do those metrics effectively delineate all important components of the facility's operation? If there are key metrics that the operation either tracks on a regular basis or that you can identify through the operational evaluation portion of the assessment, there is another means to assess where attention may need to be directed within the operation — benchmarking.\nIt is invaluable to compare known or desired metrics with like operations (in size, order complexity and business type). A comparison of an operation's achievement in productivity by department or process type (case-picking, piece-picking, pallet movement, etc.) order cycle time, accuracy achievement, product damage and/or employee injury will reveal areas that are lagging behind achievable targets. The benchmark comparison is yet another way to identify where you should direct further attention toward process improvement with the audited operation.\nFinally, the assessment should review all areas that appear to require attention and ensure that change in any one area of the operation will not be at the expense of another area. For example, if the storage density of the facility appears to require improvement, you should take care that storage density increases will not negatively impact productivity. In short, you should develop a synthesized list of improvements with attention directed at improving the entire operation, rather than one area at the expense of other areas with the operation. Once you've identified the improvements needed, plan to address those needs that require development — from operational changes to equipment and layout changes and additions to management practices and tool development.\nThis, of course, assumes the audit reveals multiple areas in need of improvement. As with any checkup, the patient might need little change in lifestyle or habits, or critical action might be in order. Whichever is necessary, the only way to identify the path to improving the operation is to thoroughly, completely and openly assess the operation from the receiving dock to the shipping doors and every process in between.\nBryan Jensen is a vice president and partner with York, PA-based St. Onge Co., an internationally recognized material handling and logistics engineering firm. St. Onge Co. routinely performs operational audit, facility engineering and logistics improvement projects for clients seeking to build world-class distribution facilities and logistics supply chains. Contact Jensen at 717/840-8181 or BJensen@stonge.com."", 'What is a partnership? Definition and meaning\nBy law, a partnership is a relationship formed by the agreement of at least two people to run a business as co-owners. In other words, a partnership is a firm consisting of several owners – each one has invested in the business. In some cases, all the partners are actively involved in the business – they work in it – while in other cases some may be less involved in the running of things, or not at all.\nIndividuals who have invested in the business but do not work in it are known as limited partners and informally as sleeping partners.\nA partnership differs from a corporation in that it is not a separate entity from the owners. Income tax is paid by the partnership, however, the profits and losses are divided up and kept by the partners, depending on how the original agreement was set up. The partners in a partnership do not necessarily have to be just active people, they could be corporations, schools, the government, sleeping partners, churches, etc.\nEach partner reports his or her share of business profits or losses on their individual tax return.\nLimited partners, usually those who are not directly involved in the management of the business, are generally liable only to the extent of the money they invested in the partnership.\nUnlike a corporation or Limited Liability Company, the full partners in a partnership are personally responsible for the businesses’ losses, debts and liabilities. This means that creditors can go after their personal savings and assets, including their homes, in order to get paid.\nIndividuals can set up a limited partnership or limited liability partnership if they do not want to be personally liable for the business’ losses.\nPartner’s do not necessarily have to be people. A Limited Liability Company, for example, counts as a legal entity just like a person, and may also be a partner in a partnership. Governments, schools, universities and interest-based organizations may also be partners.\nPublic Private Partnerships are often called PPPs. According to the PPP Kowledge Lab, a PPP is: “A long-term contract between a private party and a government entity, for providing a public asset or service, in which the private party bears significant risk and management responsibility, and remuneration is linked to performance.”\nA partnership can also refer to a type of sponsorship, as occurred last year when British football (US: soccer) club Manchester United announced a multi-year global sponsorship (partnership) deal with Geneva-based Swissquote, an online forex trading company.\nA partnership in the United States\nIn the United States, the federal government has no specific statutory law governing the establishment of a partnership. Each of the country’s states including the District of Columbia has its own statutes and common law governing partnerships.\nAll the US states mainly follow the general common law principles of partnership whether a limited liability partnership, a limited partnership or general partnership.\nA partnership in the United Kingdom\nIn the United Kingdom, a limited partnership consists of either one of the definitions below:\n- One or more people known as general partners. They are liable for all the obligations and debts of the business.\n- One or the firm beyond the amount contributed.\nLimited partners in the UK may not draw out or receive back any part of the investment they made in the partnership during its lifetime, neither can they take part in the management of the business. Limited partners have no power to bind the firm.\nMany public-private partnerships are established to improve the lives of specific groups of people at city level. According to nyc.gov: “For over twenty years, the Mayor’s Fund to Advance New York City has facilitated public-private collaborations that support the development and emerging needs of the city’s most underserved communities.” (Image: adapted from twitter.com/NYCMayorsFund)\nIn some types of partnerships, particularly accountancy or law firms, equity partners are different from contract partners (salaried partners).\nAn equity partner, unlike a salaried partner, has an underlying ownership interest in the firm and is entitled to a proportion of the distributable profits of the partnership. The salaried partner, as the name implies, receives a salary but does not share in the distributions of the firm. Salaried partners may receive a bonus based on the business’ profitability.\nIn general, equity partners enjoy a fixed share – usually but not always an equal share – along with the other partners.\nIn more sophisticated partnerships, different models may exist for determining proportion, type of ownership and/or profit distribution. Two common forms are eat-what-you-kill or lockstep compensation.\nGiant retailer John Lewis Partnership is probably the most successful example of a partnership in the United Kingdom, with revenue of £9.5 billion and 88,700 employees. (Image: twitter.com/DaveLogan)\nEat-what-you-kill is used virtually exclusively by law firms. Each of the firm’s partners receives a share of the partnership’s profits up to a specific amount, with any additional profits going to the partner who brought in the business, i.e. who was responsible for the ‘origination’ of the work that generated the income and thus the profit.\nLockstep is a type of grading system, where somebody may start off as a junior partner with a certain number of points, and accumulates more points as time passes until he or she reaches a set maximum, often called a plateau. The person who reaches the highest partner status is often referred to as a senior partner.\nHow long it takes to reach the maximum is described in the lockstep – a seven-year-lockstep means it will take a new partner seven years to hit the plateau.\nEat-what-you-kill law firms are more common in the United States, while UK firms tend to use the lockstep principle.\nWhen American law firm Rogers & Wells merged with UK firm Clifford Chance, overcoming the difficulties of merging a eat-what-you-kill culture with a lockstep one was especially challenging.\nBelow are listed some famous businesses that either started off as partnerships or continue as such today.\n– Warner Bros: the founders were pioneering figures in the movie industry. They introduced sound to movies and revolutionized musical films. Founded in 1923 in Culver City, California, USA.\n– Hewlett Packard: the partnership was founded in a small garage in Palo Alto, California, in 1939. The firm has since gone on to become a PC manufacturing giant. Hewlett and Packard’s first successful product was an audio oscillator, which Walt Disney snapped up for the movie theatre surround sound systems it designed for its famous musical cartoon Fantasia.\n– John Lewis Partnership: a British employee-owned retailer that operates the John Lewis department stores, as well as Waitrose supermarkets. It also offers banking, financial and other retail-related services. The business has more than 88,000 employees. Founded in Oxford Street, London, UK, by John Spedan Lewis in 1929.\n– McDonald’s: founded by partners Richard and Maurice McDonald in 1940 in San Bernadino, California, USA. Ray Krok joined the team in 1995 and later bought the company from the brothers, founding the McDonald’s Corporation. Today is has restaurants in 119 nations.\n– Microsoft: founded by Bill Gates and Paul Allen. It was officially established in 1975 when Allen invented the company name by combining two words – microcomputer and software. Today it is a leading software maker and one of the world’s most valuable companies.\n– Apple: founded by Steve Jobs and Steve Wozniak in Cupertino, California, USA, in 1976. You just have to look at the effect Jobs’ death had globally to appreciate the historical importance of this partnership. When the partnership was set up, Jobs looked after marketing while Wozniak dealt with the technical side of things.\n– Google: founded by Sergey Brin and Larry Page in 1998 in Mountain View, California. The company today is the world’s leading search engine.\n– Twitter: founded by Evans Williams, Jack Dorsey and Biz Stone in 2006. The firm has grown rapidly and today has more than 500 million active registered users who tweet on average 58 million messages per day.\n– PricewaterhouseCooper (PwC): a multinational professional services network based in London, UK. The result of a merger between Price Waterhouse (est. 1854) and Coopers & Lybrand. It operates in 157 countries in 756 locations, with more than 208,000 people. Member firms have a variety of legal structures – both US and UK firms are actually limited liability partnerships.\nQuotes using the word ‘Partnership’\n“Public-Private Partnership in financing, service delivery and provision of workspaces and training of trainers must be promoted to meet the demand and supply gap in the field of skill development,” (Pallam Raju – an Indian politician, former Union Minister for Human Resources Development in the Central Government)\n“Financial institutions, the corporate world and civil society – all must uphold high standards of probity in their working. Only a genuine partnership between the Government and its people can bring about positive change to create a just society,” (Pratibha Patil – President of India from 2007 to 2012)\n“No partnership between two independent companies, no matter how well run, can match the speed, effectiveness, responsiveness and efficiency of a solely owned company,” (Edward Whitacre, Jr. – Former Chairman and CEO of General Motors and AT&T Inc.)\n“Friendship is essentially a partnership,” (Aristotle, born 384 BC, died 322 BC – a Greek philosopher and scientist)\n“A man who graduated high in his class at Yale Law School and made partnership in a top law firm would be celebrated. A man who invested wisely would be admired, but a woman who accomplishes this is treated with suspicion,” (Barbra Streisand – an American singer, songwriter, actress, and filmmaker)\n“And humility in politics means accepting that one party doesn’t have all the answers; recognising that working in partnership is progress not treachery,” (Vince Cable – a British politician, former Secretary of State for Business, Innovation and Skills)\n“The Declaration on Strategic Partnership between India and Russia signed in October 2000 became a truly historic step. The developments in the first decade of the 21 century confirmed that it was a particularly significant and timely step,” (Vladimir Putin – President of the Russian Federation)']"	['<urn:uuid:e8b231ef-b677-42bb-9667-208b46cb019d>', '<urn:uuid:d8f35d4f-14ae-47c9-b37e-d5dd16f45906>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T01:37:09.743768	22	89	3414
76	lab or real world research human behavior advantages	There has been a tension between different research approaches. Laboratory research allows control of extraneous variables, while field research better captures real-world human action. However, an integrated approach that breaks down the boundary between lab and field offers important advantages - it corrects for sterile lab environments that lack ecological validity while still allowing quantification and statistical testing to understand groups. This approach enables studying human action and social groups in their natural context while maintaining scientific rigor.	['The research project of Dimitris Xygalatas is part of a growing trend in cognitive approaches to human sociality. This trend involves breaking down the boundary between the lab and the field; sometimes this involves bringing the field into the lab—an approach not uncommon to many social psychologists—and other times it involves bringing the lab into the field—an approach favored and in many ways pioneered by Xygalatas. His work, which is well-presented in his (relatively) new book The Burning Saints: Cognition and Culture in the Fire-Walking Rituals of the Anastenaria(Xygalatas, 2012), is a great example of this trend. Breaking down the boundary between the research lab and the “field site” is becoming more common beyond the boundaries of religious studies and anthropology.\nI think it is worthwhile to explain a little more what is meant by “breaking down the boundary between the lab and the field”. Xygalatas notes that there has been a tension between those researchers who say that the lab is a great research environment because it allows one to control for extraneous variables, on the one hand, and those who say the lab is too sterile to adequately address questions relating to human action in the real world, on the other. For reasons that are discussed in many forums and at greater length than I can offer here (in religious studies, the work of Ted Slingerland and Ann Taves come to mind), both sides of the debate have good points. However, what Xygalatas and others do is question the dichotomy between “the lab” and “the field.” Xygalatas and others have published a number of studies that take lab-based measures deployed in the field (for examples see Konvalinka et al., 2011; Xygalatas, Mitkidis, et al., 2013; Xygalatas, Schjoedt, et al., 2013). Some of these methods are common to anthropology, such as coded interviews (Xygalatas, 2007, 2012). Other methods are common to social psychology, such as measures of identity. Still other of his methods are common to physiology, such as monitoring heart rate as a proxy for arousal.\nWhat is most important about the integrated approach, and why this approach needs to be embraced by both the humanities and the sciences, is its ability to quantifiably study human action and social groups in situ. It corrects for sterile lab environments that suffer from a lack of ecological validity and often suffer from significant sampling issues (Henrich, Heine, & Norenzayan, 2010). However it also allows for a quantification that can provide the opportunity for statistical testing to understand and compare groups. It also gives us a method for getting past the issues of relying solely on self-reporting. An empirical and theoretically grounded approach, like the one presented by Xygalatas, creates a very interesting foundation for a truly comparative approach to religions; one that it could be argued has been the focus of secular religious approaches since Max Müller. For the sciences, such an approach offers a more “realistic” look at human sociality. For instance, understanding what happens during certain social events can be reconstructed to an extremely limited extent in the lab. However, bringing lab techniques into the field gets around this issue. Furthermore, there are many things that researchers can’t recreate in the lab because it would be unethical for a researcher to ask participants to walk over hot coals, pierce themselves with rods, or carry great burdens for long distances in intense heat. However, many individuals do such actions of their own volition because it is an important part of many societies. Without taking an approach along the lines of Xygalatas’, researchers would not be able to get a truly scientific understanding of these experiences.\nAlthough I believe that Xygalatas’ research project is a good example of a scientific approach to religion, I would argue that there is a second approach that works best in conjunction with an approach such as Xygalatas’, and this approach utilizes “big data.” Recently, I discussed in a very general way how the use of computational approaches can offer a valid and interdisciplinary approach to understanding complex human social systems (Lane, 2013). In this article I mentioned that there is a large depository of data from our digital lives that remains basically untouched, at least by academic researchers. Over the past few years, a number of scholars have teamed up with the private sector (either directly or indirectly) in order to gather and analyze data. Examples of this include work with Facebook (Backstrom, Boldi, Rosa, Ugander, & Vigna, 2012), Twitter (Goldberg, Hayvanovych, & Magdon-Ismail, 2010; Gonçalves, Perra, & Vespignani, 2011; Lerman, Ghosh, & Surachawala, 2010; Ritter, Preston, & Hernandez, 2013), and Microsoft (Leskovec & Horvitz, 2008). Many researchers are also supplementing this data approach with what has been termed “reality mining” (Eagle & Pentland, 2005). This “reality mining” uses our online data, mobile phone data, or sociometric badges—devices designed to collect data on our interactions in real time—in order to collect data on social interactions (see Pentland, 2014 for an overview). Researchers can leverage these highly quantified data sets (as well as construct their own) to test hypotheses concerning human sociality. What is human religiosity if not some social phenomenon? Of course, there are many definitions of religion; however, if you take social interactions away from any of them, you are likely left with a definition that is at least lacking. While these data sets are not always recording information about religious beliefs and behaviors, they are recording—with great precision—the social fabric of human organizations. This fabric, that until recently we haven’t really known that much about in any quantified sense, is the foundation of religion as well as other social phenomena such as culture, politics, and economics. I think that a convergence or dialogue between the computational/big data approach—that gives a very broad and precise view of sociality—and Xygalatas’ experimental anthropological approach—that gives an in depth and explanatory view—could create a framework for studying religion that can answer questions without denying the role of context, the role of the individual, or the role of the inter-personal relationships.\nBackstrom, L., Boldi, P., Rosa, M., Ugander, J., & Vigna, S. (2012). Four Degrees of Separation (No. arXiv:1111.4570v3) (p. 13). Retrieved from http://arxiv.org/abs/1111.4570\nEagle, N., & Pentland, A. (2005). Reality mining: sensing complex social systems. Personal and Ubiquitous Computing, 10(4), 255–268. doi:10.1007/s00779-005-0046-3\nGoldberg, M. K., Hayvanovych, M., & Magdon-Ismail, M. (2010). Measuring Similarity between Sets of Overlapping Clusters. In IEEE International Conference on Social Computing (pp. 303–308). doi:10.1109/SocialCom.2010.50\nGonçalves, B., Perra, N., & Vespignani, A. (2011). Modeling users’ activity on twitter networks: validation of Dunbar’s number. PloS One, 6(8), e22656. doi:10.1371/journal.pone.0022656\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? The Behavioral and Brain Sciences, 33(2-3), 61–83; discussion 83–135. doi:10.1017/S0140525X0999152X\nKonvalinka, I., Xygalatas, D., Bulbulia, J., Schjødt, U., Jegindø, E., & Wallot, S. (2011). Synchronized arousal between performers and related spectators in a fire-walking ritual. Proceedings of the National Academy of Sciences, 108(20), 8514–8519. doi:10.1073/pnas.1016955108/-/DCSupplemental.www.pnas.org/cgi/doi/10.1073/pnas.1016955108\nLane, J. E. (2013). Method, Theory, and Multi-Agent Artificial Intelligence: Creating computer models of complex social interaction. Journal for the Cognitive Science of Religion, 1(2), 161–180.\nLerman, K., Ghosh, R., & Surachawala, T. (2010). Social Contagion: An Empirical Study of Information Spread on Digg and Twitter Follower Graphs. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.\nLeskovec, J., & Horvitz, E. (2008). Planetary-scale views on a large instant-messaging network. Proceeding of the 17th International Conference on World Wide Web – WWW ’08, 915–924. doi:10.1145/1367497.1367620\nPentland, A. (2014). Social Physics: How Good Ideas Spread-The Lessons from a New Science. London: Scribe.\nRitter, R. S., Preston, J. L., & Hernandez, I. (2013). Happy Tweets: Christians Are Happier, More Socially Connected, and Less Analytical Than Atheists on Twitter. Social Psychological and Personality Science. doi:10.1177/1948550613492345\nXygalatas, D. (2007). Firewalking in northern Greece: A cognitive approach to high-arousal rituals. Queen’s University, Belfast.\nXygalatas, D. (2012). The Burning Saints: Cognition and Culture in the Fire-Walking Rituals of the Anastenaria. Bristol, CT: Equinox Publishing Ltd.\nXygalatas, D., Mitkidis, P., Fischer, R., Reddish, P., Skewes, J., Geertz, A. W., … Bulbulia, J. (2013). Extreme rituals promote prosociality. Psychological Science, 24(8), 1602–5. doi:10.1177/0956797612472910\nXygalatas, D., Schjoedt, U., Bulbulia, J., Konvalinka, I., Jegindo, M., Reddish, P., … Roepstoff, A. (2013). Autobiographical Memory in a Fire-Walking Ritual.']	['<urn:uuid:14351d1c-9493-407f-9188-93ab5b28ef5e>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	8	78	1376
77	workplace retaliation compensation damages recovery	Damages in workplace retaliation cases can include: emotional distress damages (including physical pain, loss of enjoyment of life, anxiety); damages for harm to professional reputation from adverse employment actions; lost wages from demotion or denied promotion. For discrimination cases specifically, available remedies include: back pay, front pay, lost benefits (health insurance, 401(k), stock options), compensatory damages for mental/emotional injuries, punitive damages in malicious cases, and attorney fees/costs.	"['Under California employment law, employers may not engage in workplace retaliation against employees who\n- report violations of law,1\n- oppose, complain about or participate in an investigation of workplace harassment or employment discrimination,2\n- request reasonable accommodations for a disability or their religious beliefs,3 or\n- file or assist in a ""qui tam"" lawsuit under the California False Claims Act.4\nThese workplace retaliation laws fill an important gap in California employment law. While employees are protected against wrongful termination in a variety of circumstances, employees who do not lose their jobs--but instead experience adverse employment actions, abuse or other forms of retaliation at work--have a harder time finding legal remedies.\nBut the workplace retaliation provisions of California\'s whistleblower laws and Fair Employment and Housing Act (""FEHA"") retaliation laws provide a legal remedy for employees whose employers retaliate against them--but do not fire them--for exercising their rights under these laws.\nBelow, our California labor law attorneys discuss the following topics:\n- 1. How to Tell If Your Employer is Engaging in Workplace Retaliation\n- 2. Employer Retaliation for Whistleblowing / Reporting a Violation of Law\n- 3. FEHA Workplace Retaliation\n- 4. False Claims Act (Qui Tam) Workplace Retaliation\n- 5. Lawsuits and Damages for California Workplace Retaliation\nIf you have further questions after reading this article, we invite you to contact us at Shouse Law Group.\nThe basic legal definition of workplace retaliation in California is:\n- Your employer takes an adverse employment action against you or treats you in a discriminatory manner,\n- Because you engaged in a protected activity.\nWhen the adverse employment action is a termination (job loss), then it is easy to identify this as a case of potential wrongful termination.\nSimilarly, if your employer responds to the exercise of your legal rights by making your working conditions so intolerable that you have no choice but to resign, then you have a case against the employer for wrongful constructive termination.\nBut other forms of employer retaliation can be more subtle--and harder to spot. Some telltale signs that your employer might be retaliating against you include:\n- You begin receiving negative performance reviews even though your previous reviews were positive;\n- Your workload is increased;\n- You are assigned to less desirable shifts;\n- Your supervisors exclude you from meetings or correspondence or otherwise make it difficult for you to perform well on your projects;\n- You are denied a promotion or raise that you think you deserved;\n- You are subject to disciplinary action on trumped-up charges or inadequate grounds; and\n- You are denied access to resources or training that would help you maintain your work quality or advance your career.\nUnder California Labor Code 1102.5 LC, employers may not engage in retaliation against employees who\n- report suspected criminal activity by their employer to a government or law enforcement agency,\n- report a suspected violation of a law or regulation to a supervisor or other person at the employer who has the authority to investigate the violation, or\n- provide information to or testify before any government body conducting an investigation, hearing or inquiry into a potential violation of law by their employer.5\nLC 1102.5 is a so-called ""whistleblower protection"" law. A whistleblower protection law generally protects employees from both firing/termination AND less severe forms of employer retaliation for reporting suspected legal violations by their employer.\nYou are protected by this whistleblower retaliation law even if it turns out that your employer did not actually break the law. All that matters is that you reasonably believed that your employer may have done something illegal.6\nExample: Nikolai works for a software company. Based on some conversations he has overheard, he believes that his company may be engaging in violations of antitrust laws with another similar company. Nikolai reports his suspicions to an in-house lawyer at the company.\nThe lawyer investigates the report and determines that nothing illegal is going on. But she also lets the CEO of the company know that Nikolai was the one who reported his suspicions to her.\nAfter that, Nikolai is not given assignments on important projects at work. His performance reviews also suddenly turn negative. He is passed over for a promotion that he had previously been told he could get.\nNikolai may be the victim of workplace retaliation for his whistleblowing activities.\nThe California Fair Employment and Housing Act (the ""FEHA"") protects employees from retaliation if they do any of the following:\n- Oppose acts of harassment or employment discrimination or an employer\'s failure to grant required pregnancy/family leave;\n- File a complaint about harassment or discrimination;\n- Testify or assist in any proceeding under the FEHA; or\n- Request workplace accommodations for their religious beliefs or observance or for a disability.7\nFEHA retaliation under California employment law occurs when any of the above activities by an employee is a substantial motivating factor for adverse employment actions against, or discriminatory treatment of, that employee.8\nExample: Robia is a Muslim. She gets a job at a call center working long shifts.\nRobia\'s religious beliefs require her to take regular breaks to pray over the course of the day. She asks her boss if she can take these breaks and use a supply closet for her prayers.\nRobia\'s boss agrees because he does not want to be accused of religious discrimination. But he is not happy about the inconvenience this causes. So he begins assigning Robia to less desirable night and weekend shifts, and he does not allow her to attend several trainings that could have helped her rise in the company.\nRobia\'s boss is guilty of workplace retaliation under the FEHA.\nThe California False Claims Act also prohibits employer retaliation against employees who take advantage of their rights under that law.\nThe California False Claims Act gives employees the right to file a so-called ""qui tam"" lawsuit against an employer who is committing fraud, theft or embezzlement with respect to government funds. (A ""qui tam"" lawsuit is a suit filed by a private citizen on behalf of a government entity.)9\nThe workplace retaliation provisions of the California False Claims Act prohibit your employer from retaliating against you if you\n- file, or assist in any way with, a qui tam suit, or\n- make an effort to stop a violation of the California False Claims Act.10\nExample: Ted is a manager for a construction company that does a lot of work under state government contracts. He suspects that his company has been overbilling the government. So he files a qui tam suit in state court. The California Attorney General\'s office then takes over the lawsuit.\nTed\'s boss understands that it would be illegal to fire him over the qui tam suit and that it would probably make the company look bad in the lawsuit. But the boss demotes Ted instead.\nTed probably has a case against his employer for California False Claims Act workplace retaliation.\nDifferent California workplace retaliation laws offer different options for employees who are retaliated against by their employer.\nLabor Code 1102.5 LC\nIf you think you have been a victim of whistleblower retaliation under Labor Code 1102.5 LC, you must first notify the California Labor and Workplace Development Agency through an online form and your employer via certified mail.11\nAfter you file this notice, the Labor and Workplace Development Agency may decide to investigate your complaint itself. If it chooses not to do so, you may file your own lawsuit in California Superior Court.12\nFEHA workplace retaliation\nIf your employer retaliates against you for exercising your rights under the Fair Employment and Housing Act, then your first move is to file a complaint with the Department of Fair Employment and Housing (DFEH).\nYou may then sue your employer for workplace retaliation once the DFEH issues a ""right to sue"" notice.13\nCalifornia False Claims Act workplace retaliation\nYou may file a lawsuit against your employer immediately if you are a victim of workplace retaliation under the California False Claims Act.14\nDamages for workplace retaliation\nDamages in a workplace retaliation case will be somewhat different than damages in a California wrongful termination case. This is because wrongful termination damages typically include the value of lost pay and benefits attributable to the loss of the plaintiff\'s job.\nIn a workplace retaliation case, though, damages are more likely to include:\n- Damages for emotional distress arising from your mistreatment at work--including physical pain, loss of enjoyment of life and/or anxiety;\n- Damages for harm done to your professional reputation as a result of retaliatory adverse employment actions (such as demotion, being denied a promotion or being denied professional development or work opportunities); and/or\n- Lost wages from a retaliatory demotion or denial of a promotion or raise.15\nCall us for help...\nFor questions about California law on workplace retaliation or to discuss your case confidentially with one of our skilled California labor and employment attorneys, do not hesitate to contact us at Shouse Law Group.\nWe have local employment law offices in and around Los Angeles, San Diego, Orange County, Riverside, San Bernardino, Ventura, San Jose, Oakland, the San Francisco Bay area, and several nearby cities.\n- Labor Code 1102.5 LC -- Employer or person acting on behalf of employer; prohibition of disclosure of information by employee to government or law enforcement agency; suspected violation or noncompliance to federal or state law; retaliation; civil penalties [whistleblower workplace retaliation]. (""(a) An employer, or any person acting on behalf of the employer, shall not make, adopt, or enforce any rule, regulation, or policy preventing an employee from disclosing information to a government or law enforcement agency, to a person with authority over the employee, or to another employee who has authority to investigate, discover, or correct the violation or noncompliance, or from providing information to, or testifying before, any public body conducting an investigation, hearing, or inquiry, if the employee has reasonable cause to believe that the information discloses a violation of state or federal statute, or a violation of or noncompliance with a local, state, or federal rule or regulation, regardless of whether disclosing the information is part of the employee\'s job duties. (b) An employer, or any person acting on behalf of the employer, shall not retaliate against an employee for disclosing information, or because the employer believes that the employee disclosed or may disclose information, to a government or law enforcement agency, to a person with authority over the employee or another employee who has the authority to investigate, discover, or correct the violation or noncompliance, or for providing information to, or testifying before, any public body conducting an investigation, hearing, or inquiry, if the employee has reasonable cause to believe that the information discloses a violation of state or federal statute, or a violation of or noncompliance with a local, state, or federal rule or regulation, regardless of whether disclosing the information is part of the employee\'s job duties. (d) An employer, or any person acting on behalf of the employer, shall not retaliate against an employee for having exercised his or her rights under subdivision (a), (b), or (c) in any former employment. . . . (h) An employer, or a person acting on behalf of the employer, shall not retaliate against an employee because the employee is a family member of a person who has, or is perceived to have, engaged in any acts protected by this section."")\n- Government Code 12940 GC -- Employers, labor organizations, employment agencies and other persons; unlawful employment practice; exceptions [FEHA workplace retaliation]. (""It is an unlawful employment practice, unless based upon a bona fide occupational qualification, or, except where based upon applicable security regulations established by the United States or the State of California: . . . (h) For any employer, labor organization, employment agency, or person to discharge, expel, or otherwise discriminate against any person because the person has opposed any practices forbidden under this part or because the person has filed a complaint, testified, or assisted in any proceeding under this part."")\n- Government Code 12940 GC -- Employers, labor organizations, employment agencies and other persons; unlawful employment practice; exceptions [workplace retaliation for requesting reasonable accommodation]. (""(l) . . . (4) For an employer or other entity covered by this part to, in addition to the employee protections provided pursuant to subdivision (h), retaliate or otherwise discriminate against a person for requesting accommodation [for religious practices] under this subdivision, regardless of whether the request was granted. . . .(m) . . . (2) For an employer or other entity covered by this part to, in addition to the employee protections provided pursuant to subdivision (h), retaliate or otherwise discriminate against a person for requesting accommodation [for disability] under this subdivision, regardless of whether the request was granted."")\n- Government Code 12653 GC -- California False Claims Act workplace retaliation. (""(a) Any employee, contractor, or agent shall be entitled to all relief necessary to make that employee, contractor, or agent whole, if that employee, contractor, or agent is discharged, demoted, suspended, threatened, harassed, or in any other manner discriminated against in the terms and conditions of his or her employment because of lawful acts done by the employee, contractor, agent, or associated others in furtherance of an action under this section or other efforts to stop one or more violations of this article. (b) Relief under this section shall include reinstatement with the same seniority status that the employee, contractor, or agent would have had but for the discrimination, two times the amount of back pay, interest on the back pay, and compensation for any special damages sustained as a result of the discrimination, and where appropriate, punitive damages. The defendant shall also be required to pay litigation costs and reasonable attorneys\' fees. An action under this section may be brought in the appropriate superior court of the state. (c) A civil action under this section shall not be brought more than three years after the date when the retaliation occurred."")\n- Labor Code 1102.5 LC -- Employer or person acting on behalf of employer; prohibition of disclosure of information by employee to government or law enforcement agency; suspected violation or noncompliance to federal or state law; retaliation; civil penalties [whistleblower workplace retaliation], endnote 1 above.\n- Government Code 12940 GC -- Employers, labor organizations, employment agencies and other persons; unlawful employment practice; exceptions [FEHA workplace retaliation], endnote 2 above; Government Code 12940 GC -- Employers, labor organizations, employment agencies and other persons; unlawful employment practice; exceptions [workplace retaliation for requesting reasonable accommodation], endnote 3 above.\n- Judicial Council of California Civil Jury Instructions (""CACI"") 2505 -- [FEHA Workplace] Retaliation—Essential Factual Elements (Gov. Code, § 12940(h)). (""[Name of plaintiff] claims that [name of defendant] retaliated against [him/her] for [describe activity protected by the FEHA]. To establish this claim, [name of plaintiff] must prove all of the following: 1. That [name of plaintiff] [describe protected activity; 2. [That [name of defendant] [discharged/demoted/[specify other adverse employment action]] [name of plaintiff];] [or] [That [name of defendant] subjected [name of plaintiff] to an adverse employment action;] [or] [That [name of plaintiff] was constructively discharged;] 2. That [name of plaintiff]\'s [describe protected activity] was a substantial motivating reason for [name of defendant]\'s [decision to [discharge/demote/[specify other adverse employment action]] [name of plaintiff]/conduct]; 3. That [name of plaintiff] was harmed; and 4. That [name of defendant]\'s decision to [discharge/demote/[specify other adverse employment action]] [name of plaintiff] was a substantial factor in causing [him/her] harm. [[Name of plaintiff] does not have to prove [discrimination/harassment] in order to be protected from retaliation. If [he/she] [reasonably believed that [name of defendant]\'s conduct was unlawful/requested a [disability/religious] accommodation], [he/she] may prevail on a retaliation claim even if [he/she] does not present, or prevail on, a separate claim for [discrimination/harassment/[other]].]"")\n- Government Code 12652 GC -- Qui tam lawsuits.\n- Government Code 12653 GC -- California False Claims Act workplace retaliation, endnote 4 above.\n- Labor Code 2699.3 LC -- Requirements for aggrieved employee to commence a civil action [under LC 1102.5 whistleblower workplace retaliation law].\n- Government Code 12965 GC -- Civil action in name of department; group or class complaint; relief; tolling of statute of limitations [FEHA workplace retaliation lawsuits].\n- Government Code 12653 GC -- California False Claims Act workplace retaliation, endnote 4 above.\n- See, e.g., Judicial Council of California Civil Jury Instructions (""CACI"") 3905A. Physical Pain, Mental Suffering, and Emotional Distress (Noneconomic Damage [in workplace retaliation cases]). (""[Insert number, e.g., ""1.""] [Past] [and] [future] [physical pain/ mental suffering/loss of enjoyment of life/disfigurement/physical impairment/inconvenience/grief/anxiety/humiliation/emotional distress [insert other damages]]. [To recover for future [insert item of pain and suffering], [name of plaintiff] must prove that [he/she] is reasonably certain to suffer that harm.] No fixed standard exists for deciding the amount of these damages. You must use your judgment to decide a reasonable amount based on the evidence and your common sense."")', 'Federal and state laws prohibit employers from discriminating against employees based on the following characteristics:\nThe law also prohibits harassment that is based on these protected classes, and employer retaliation for asserting the right to be free of discrimination. These laws apply to hiring, promotion, job assignment, termination, and compensation.\nWhich Employees Does the Law Protect?\nFor most types of discrimination, a private employer is not subject to anti-discrimination laws unless it has had 15 or more employees for at least twenty weeks in the past year. For age discrimination, a private employer must have 20 or more employees, and with the Equal Pay Act virtually all employers are covered.\nWhat Evidence Is Required to Prove Employment Discrimination?\nThe best way to prove discrimination is direct evidence against an employer, such as a statement, e-mail, letter, or memo that shows the employer took action against you based on a discriminatory motive. In most cases, though, direct evidence isn’t available. Instead, you have to rely on circumstantial evidence to prove your case.\nCircumstantial evidence is evidence that would make a person believe discrimination occurred but is not direct proof of it. To win a case using either direct or circumstantial evidence, you need to show that a discriminatory reason more likely than not motivated your employer’s action against you. When circumstantial evidence is involved, a three-step test called the McDonnell Douglas framework is typically used.\nSteps for Proving Most Employee Discrimination Claims\nFirst, an employee must make a “prima facie” case of discrimination by showing that certain circumstances existed that would lead a person to presume that the employer’s action was based on discrimination.\nSecond, if the employee makes a prima facie case, then the employer must provide a legitimate, non-discriminatory reason for its action.\nThird, if the employer provides a legitimate, non-discriminatory reason, then the employee needs to show that the employer’s stated reason is merely “pretext,” or a cover-up, for discrimination.\nLearn more here and by clicking on the individual types of discrimination.\nWhat Remedies Are Available in a Discrimination Case?\nAn employee who wins a discrimination case may be entitled to the following, depending on the type of case and the severity of the discrimination:\n- Back pay: lost earnings resulting from the discrimination;\n- Front pay: lost future earnings resulting from the discrimination;\n- Benefits: lost benefits resulting from the discrimination, including health and dental insurance, pension or 401(k) plans, stock options, and profit sharing;\n- Compensatory damages: mental or emotional injuries (i.e. pain and suffering), medical expenses, job search costs, etc. resulting from the discrimination;\n- Punitive damages: a monetary award in malicious or reckless cases, which is designed to punish the employer and deter the employer from repeating its discrimination;\n- Attorney fees and costs: an award to cover attorney fees and costs, such as court fees and expert witness fees.\nThere are limits on the amount of compensatory and punitive damages available. Also, an employee who was discriminated against in hiring, firing, or promotion may be placed or reinstated to the desired position.\nWhat Protections Does the State of Nevada Have?\nNevada has similar laws that prohibit discrimination. The requirements depend on the type of discrimination. An employee may have a claim under both federal and state law, or, in some cases, just one or the other.\nWhat Is the Process for Filing a Discrimination Complaint?\nEmployees who wish to file a lawsuit against an employer for discrimination must first file a charge either with the Equal Employment Opportunity Commission (“EEOC”) or the Nevada Equal Rights Commission (“NERC”). There are a couple of exceptions. In most cases, an employee must file a charge with the EEOC or NERC within 300 days of when the discrimination occurred.\nThe EEOC or NERC will review the charge, investigate, and in most cases issue a “Notice of Right-to-Sue,” which allows an employee to file a lawsuit. In egregious cases of discrimination, the EEOC will file a lawsuit on an employee’s behalf; otherwise, the employee should hire an attorney.\nAn employee has 90 days to file a lawsuit after receiving the Notice of Right-to-Sue, with some exceptions.\nWhat to Do?\nContact me today if you believe you have a claim of employment discrimination, or if you are an employer who needs to defend against an employment discrimination claim.\n- What Is Required to Prove Discrimination in the Workplace?\n- What Are the Time Limits for Filing a Charge of Discrimination and a Lawsuit?\n- Which Employers Are Subject to Workplace Discrimination Laws?\n- Can I Request My Personnel File from My Employer in Nevada?\n- Can Nevada Employers Ask About Your Salary History?\n- Can Your Employer Discipline or Fire You for Using a Legal Product Off Duty?\n- Can an Employer Refuse to Hire an Applicant with a Criminal History?']"	['<urn:uuid:6705d2a3-7599-4759-9b1e-8e42c29d06d3>', '<urn:uuid:8650fd93-a085-4d7f-a7a7-c7c944866cc7>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	5	67	3619
78	What are the key experts and clinical manifestations in haemato-oncology infections?	Key experts include specialists like Dr. Sweeney focusing on haematology infections and antimicrobial resistance, Dr. Lambourne specializing in immunodeficiency infections, and Dr. Agrawal concentrating on fungal infections. Clinical manifestations can be diverse, affecting multiple systems - respiratory (ARDS with hypoxemia), cardiovascular (hypotension, elevated lactate), neurological (delirium, obtundation), and renal (decreased urine output, increasing creatinine). Additional manifestations include paralytic ileus, altered glycemic control, thrombocytopenia, and adrenal dysfunction.	['Haemato-oncology Bacterial Infections\nThank you for taking the time to view this webinar, we hope you find it informative.\nRCPath CPD accredited\nCo-presenters: Dr Louise Sweeney, Dr Jonathan Lambourne, Dr David Partridge and Dr Samir Agrawal\n– – – – –\nIf you require a certificate of attendance, details of how to obtain will appear at the end of the webinar.\nPlease note that once you have completed your details on the form, the certificate will be generated so please be ready to download your certificate immediately. We would like to inform delegates that each time a certificate is generated, we will be notified with details.\nIf you have any problems creating or downloading your certificate please contact Esme Carruthers at email@example.com\nThis webinar was supported by an unrestricted educational grant from Shionogi.\nSpeakers have not been paid for their participation.\nHaemato-oncology Bacterial Infections\nThis course is designed to provide an overview of some of the commonly identified infection issues that occur in haemato-oncology. Experts will discuss how best to manage these issues patients using examples and case studies.\nTo access the course please click on the button below.\nThis will open the log in page in your default browser, all you will need to do is input an email and make a password, then you will be automatically redirected to the course.\nIf you have any issues with accessing the course (or any queries at all during the feedback process) please contact firstname.lastname@example.org Course\nDr Louise Sweeney, Consultant Medical Microbiologist, Liverpool\nSpecialist interests include infection in Haematology and Transplant patients, antimicrobial susceptibility testing and the approach to clinical management of multi-drug resistant infections, particularly those caused by Gram negative bacteria. Non-clinical interests include training and education of clinicians and allied professionals within and without the Infection Specialties.\nDr Jonathan Lambourne, Consultant in Infectious Diseases & Microbiology, Barts Health NHS Trust\nJonathan Lambourne is consultant in Infectious Diseases & Microbiology at Barts Health NHS, London UK. He undertook medical training at Cambridge and University College London, and his specialist training in Dublin and London. He completed a PhD at St George’s University of London, investigating immune determinants of susceptibility to invasive Aspergillosis and Staphylococcus aureus infection. His main clinical interest is infections in patients with primary and secondary immunodeficiency and the application of diagnostic techniques in this challenging patient population.\nDr David Partridge, Consultant Microbiologist, Sheffield Teaching Hospitals\nDavid Partridge is a Consultant Microbiologist and Infection Control Doctor at Sheffield Teaching Hospitals and Honorary Senior Lecturer at the Florey Institute, University of Sheffield. He has an interest in infections of the immunocompromised host and device related infection. During the COVID-19 pandemic, he has been working on clinical utility of sequencing of SARS-CoV-2 and also on management of secondary infections.\nDr Samir Agrawal, Senior Lecturer & Honorary Consultant, Barts Health NHS Trust & Blizard Institute, Queen Mary University of London\nResearch Interest: Infection management in Haemato-Oncology with. focus on invasive fungal infections; management pathways; diagnostics – development and clinical implementation of rapid fungal biomarkers; online database for (national) audit of infection, specifically fungal infection in hospital settings; developing probes for imaging for pathogen identification using CT-PET)\nGroup Expertise: Clinical fungal management, biomarkers for invasive aspergillosis, gallium (68Ga) labelling of fungal-specific probes for imaging; Fungal Audit Tool (for stewardship) F.A.T.S. – pilot project across England.\nESPAUR antifungal committee;\nNHS England antifungal stewardship programme;\nCI of grant submission for a national RCT of AFS vs empirical fungal management', 'Sepsis, a complex physiological and metabolic response to infection, is a common reason for admission to an intensive care unit. The first article in our Critical Care Medicine review series examines the basis, diagnosis, and current treatment of this disorder.\nIn 1992, an international consensus panel defined sepsis as a systemic inflammatory response to infection, noting that sepsis could arise in response to multiple infectious causes and that septicemia was neither a necessary condition nor a helpful term. Instead, the panel proposed the term “severe sepsis” to describe instances in which sepsis is complicated by acute organ dysfunction, and they codified “septic shock” as sepsis complicated by either hypotension that is refractory to fluid resuscitation or by hyperlactatemia.\n• What is the most common cause of severe sepsis, and what organisms are most often implicated?\nSevere sepsis occurs as a result of both community-acquired and health care-associated infections. Pneumonia is the most common cause, accounting for about half of all cases, followed by intraabdominal and urinary tract infections. Staphylococcus aureus and Streptococcus pneumoniae are the most common gram-positive isolates, whereas Escherichia coli, klebsiella species, and Pseudomonas aeruginosa predominate among gram-negative isolates. An epidemiologic study of sepsis showed that during the period from 1979 to 2000, gram-positive infections overtook gram-negative infections. However, in a more recent study involving 14,000 ICU patients in 75 countries, gram-negative bacteria were isolated in 62% of patients with severe sepsis who had positive cultures, gram-positive bacteria in 47%, and fungi in 19%.\n• What are the risk factors for severe sepsis?\nRisk factors for severe sepsis are related both to a patient’s predisposition for infection and to the likelihood of acute organ dysfunction if infection develops. There are many well-known risk factors for the infections that most commonly precipitate severe sepsis and septic shock, including chronic diseases (e.g., the acquired immunodeficiency syndrome, chronic obstructive pulmonary disease, and many cancers) and the use of immunosuppressive agents. Among patients with such infections, however, the risk factors for organ dysfunction are less well studied but probably include the causative organism and the patient’s genetic composition, underlying health status, and preexisting organ function, along with the timeliness of therapeutic intervention. Age, sex, and race or ethnic group all influence the incidence of severe sepsis, which is higher in infants and elderly persons than in other age groups, higher in males than in females, and higher in blacks than in whites.\nMorning Report Questions\nQ: What are common clinical manifestations of severe sepsis?\nA: The signs of both infection and organ dysfunction may be subtle, and thus the most recent international consensus guidelines provide a long list of warning signs of incipient sepsis. Acute organ dysfunction most commonly affects the respiratory and cardiovascular systems. Respiratory compromise is classically manifested as the acute respiratory distress syndrome (ARDS), which is defined as hypoxemia with bilateral infiltrates of noncardiac origin. Cardiovascular compromise is manifested primarily as hypotension or an elevated serum lactate level. After adequate volume expansion, hypotension frequently persists, requiring the use of vasopressors, and myocardial dysfunction may occur. The brain and kidneys are also often affected. Central nervous system dysfunction is typically manifested as obtundation or delirium. Imaging studies generally show no focal lesions, and findings on electroencephalography are usually consistent with nonfocal encephalopathy. Critical illness polyneuropathy and myopathy are also common, especially in patients with a prolonged ICU stay. Acute kidney injury is manifested as decreasing urine output and an increasing serum creatinine level and frequently requires treatment with renal-replacement therapy. Paralytic ileus, elevated aminotransferase levels, altered glycemic control, thrombocytopenia and disseminated intravascular coagulation, adrenal dysfunction, and the euthyroid sick syndrome are all common in patients with severe sepsis.\nQ: What are the principles of treatment of severe sepsis?\nA: Intravenous antibiotic therapy should be started as early as possible and should cover all likely pathogens. It has not been determined whether combination antimicrobial therapy produces better outcomes than adequate single-agent antibiotic therapy in patients with severe sepsis. Current guidelines recommend combination antimicrobial therapy only for neutropenic sepsis and sepsis caused by pseudomonas species. Empirical antifungal therapy should be used only in patients at high risk for invasive candidiasis. After the first 6 hours, attention focuses on monitoring and support of organ function, avoidance of complications, and de-escalation of care when possible. De-escalation of initial broad-spectrum therapy may prevent the emergence of resistant organisms, minimize the risk of drug toxicity, and reduce costs, and evidence from observational studies indicates that such an approach is safe. The only immunomodulatory therapy that is currently advocated is a short course of hydrocortisone (200 to 300 mg per day for up to 7 days or until vasopressor support is no longer required) for patients with refractory septic shock. This recommendation is supported by a recent meta-analysis, but the two largest studies had conflicting results, and other clinical trials are ongoing.']	['<urn:uuid:15590c4d-d339-4544-b4c2-4937feffe929>', '<urn:uuid:fb5cdd28-c79f-491f-866d-73a142810e47>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	11	66	1380
79	ways dangerous bacteria spread between people living close together prevention methods stop infection	These infections can spread when an infected person coughs or sneezes and others breathe it in, or through close contact like kissing. The risk is higher for people living in close quarters such as dormitories or boarding schools. To prevent spread, several measures are important: washing hands frequently, covering coughs and sneezes, avoiding contact with sick people, and maintaining good hygiene. Vaccination is the most effective prevention method. Both pneumococcal disease and meningitis can be prevented through various vaccines including pneumococcal vaccine, meningococcal vaccine, and Haemophilus influenzae type B vaccine. Without proper prevention and treatment, these infections can lead to serious complications including permanent disability or death.	['Pneumococcal disease is a contagious disease. Symptoms including fever and headaches. It can affect people of all ages but can be prevented with vaccination. Pneumococcal disease is treated with antibiotics.\nWhat is pneumococcal disease?\nPneumococcal disease is a bacterial infection caused by Streptococcus pneumoniae, sometimes known as pneumococcus. Most people carry pneumococcus in their nose and throat, where the bacteria do not cause any symptoms. However, sometimes the bacteria grow and spread to other parts of the body and that’s when you become sick.\nThe bacteria can cause a number of different types of very serious disease, which can affect the lungs, ears, sinuses and brain. Pneumococcal disease is serious, and can lead to:\n- swelling and infection of the brain (meningitis)\n- infection of the blood (septicaemia)\nPneumococcal disease symptoms depend on which part of the body is infected. Symptoms can include:\n- sore ear(s), hearing loss and fever (middle ear infection)\n- sore face, blocked nose, headaches and a yellow-green mucus (sinus infection)\n- high fever, headache, nausea, vomiting, stiff neck and coma (brain and spinal cord infection)\n- fever, cough, chest pain and problems breathing (lung infection)\n- fever, muscle aches and pains, and headaches (bloodstream infection)\n- pain, swelling and stiffness in the affected joint (joint infection)\n- fever, pain in the bone and stiffness (bone infection).\nWho is at risk\nPneumococcal disease can affect people at any age. The following people have higher risk of infection:\n- young children\n- people over 65\n- Aboriginal and Torres Strait Islander people\n- anyone with a weakened immune system due to illness or injury\n- people with a malfunctioning spleen, or who have had their spleen removed\n- people with chronic diseases such as diabetes, lung disease, kidney disease, heart disease, liver disease or cancer\n- premature babies\n- people who have Down syndrome\n- people who smoke\n- people who drink harmful amounts of alcohol\n- people who have had specific operations including cochlear implants and intracranial shunts\n- people who have a condition where the fluid leaks out from around their brain and spinal cord\n- people who have had pneumococcal disease before.\nHow it spreads\nPneumococcal disease spreads:\n- when an infected person coughs or sneezes, and you breathe it in\n- by kissing.\n- If you have pneumococcal disease, you can help stop it spreading by:\n- washing your hands often\n- covering your coughs and sneezes.\nPneumococcal disease can be prevented with vaccination.\nFind out more about getting vaccinated against pneumococcal disease.\nYour doctor can diagnosis pneumococcal disease by:\n- checking your symptoms\n- examining you\nTests to confirm the diagnosis depend on which body part is infected and include:\n- chest X-ray\n- phlegm test\n- blood test\n- urine test\n- spinal fluid test.\nIf you have pneumococcal disease your doctor may be required to notify your state or territory health department.\nPneumococcal disease is treated with antibiotics.\nYou can relieve the symptoms by:\n- drinking fluids, particularly water\n- taking paracetamol to reduce pain and fever.\nSome people with a severe illness may need to go to hospital.', 'What is meningitis and meninges?\nBrain and spinal cord are protected by a layer known as meninges. An inflammation or a swelling in meninges is termed as meningitis. Meningitis is a life-threatening infection and if left untreated, it can cause brain swelling, permanent disability and even death.\nWhat are the causes and types of meningitis?\nMeninges protect the brain and spinal cord from harmful agents and acts as a shock absorber. However, sometimes these protective layers get infected by some agents such as bacteria, viruses, etc and results in infection.\nWhen such agents enter the membrane, they become isolated from the body’s immune system and starts spreading fast. By the time body fights back, the infection spreads and becomes worse. The blood vessels becomes leaky as the body fights the infection and the infection can even spread in the brain and spinal cord, causing life threatening consequences.\nThere are various agents responsible for causing meningitis. Meningitis is classified into different types on the basis of different causative agents. They are as follows-\nWhat is bacterial meningitis?\nThere are certain bacteria that act as the main cause of meningitis. These bacteria can enter the bloodstream and reach the brain and spinal cord, thus infecting the meninges. This results in bacterial meningitis.\nBacterial meningitis can also occur when the bacteria directly invades the meninges. Many different strains are responsible of causing bacterial meningitis. Some of them are:\n- Streptococcus pneumoniae (pneumococcus)\n- Listeria monocytogenes\n- Group B Streptococcus\n- Neisseria meningitides\n- Haemophilus influenzae\nBacterial meningitis is a very serious form of meningitis and can even cause death in some cases if the condition worsens.\nWhat is viral meningitis?\nViral meningitis is the most common type of meningitis but it is not as severe as bacterial meningitis. It usually takes a few weeks for a person to become well without any treatment but consulting a healthcare professional is important as some cases can be or become severe.\nThere are different types of viruses that can cause viral meningitis. Some of them are:\n- Mumps virus\n- Herpes viruses(Herpes simplex virus and varicella- zoster virus)\n- Measles virus\n- Influenza virus\nWhat is fungal meningitis?\nFungal meningitis is a rare type of meningitis and is caused by the spreading of fungus in the meninges. The fungus can enter the layers through blood. People suffering from HIV infection or cancer are at a higher risk of developing fungal meningitis. People in Africa suffer mostly from fungal meningitis.\nDifferent types of fungus responsible for causing fungal meningitis are as follows:\nMeningitis suffering by fungus can even spread to different organs such as lungs, etc.\nWhat are the risk factors for meningitis?\nAny person can suffer from meningitis. However, there are certain specific factors that can cause meningitis.\nSome common risk factors are as follows:\nPeople with certain immune deficiency are more prone to suffer from meningitis. Some of the immunodeficiency diseases are as follows HIV AIDS, autoimmune disease, etc.\nMeningitis is a communicable disease and can even spread when people live in close quarters such as dormitories, boarding schools, etc.\nPregnant women are at a higher risk of getting meningitis. It can even spread to her unborn child. Pregnancy increases the risk of listeriosis in women.\nPeople of all age are at a risk of developing meningitis. However, children below the age of 5 and young adults are at higher risk of suffering from the infection.\nWhat are the symptoms of meningitis?\nMeningitis shows its symptoms in a person soon after the infection develops. The most common symptoms of meningitis are fever and headache followed by stiff neck.\nSome more symptoms associated with it are:\n- Extreme sensitivity to bright light (photophobia)\nSymptoms of meningitis depend upon the cause of the infection.\nSymptoms of viral meningitis\nSymptoms of viral meningitis include:\n- Decreased appetite\n- Fever and pain\n- Stiff neck, etc.\nSymptoms of Bacterial meningitis\nSymptoms of bacterial meningitis include:\n- Altered mental status\nA person should immediately consult a doctor if he or she experiences any of the above symptoms. Delay in treatment of meningitis can lead to severe consequences.\nHow is meningitis diagnosed?\nAs soon as the symptoms are noticed, the person should consult a doctor for accurate diagnosis. Diagnosis usually starts with a physical examination in which your doctor will check for the signs of infection near head, ear, throat and skin and near the spinal cord.\nAfter physical examination, the doctor will advise you for certain diagnostic tests which may further help him to understand the nature of meningitis and develop an appropriate treatment plan.\nDiagnostic tests and methods for meningitis\nCommon diagnostic tests for meningitis are as follows:\nBlood samples are collected from patients and are tested for the presence of microorganisms, especially bacteria.\nGram stain may also be performed to understand the physiology of bacteria under the microscope.\nMagnetic resonance (MR) and Computerized tomography (CT) scans are used to observe the swelling and inflammation in the head.\nSpinal tap (Lumbar puncture)\nFor proper diagnosis, a sample of cerebrospinal fluid (CSF) is taken with the help of spinal tap. People with meningitis have low sugar level in CSF. This test also helps the doctor to identify the exact bacteria that has caused meningitis in that patient.\nWhat are the complications of meningitis?\nGenerally, people recover fully from meningitis. However, in some cases, a case can become very severe and it can lead to various complications such as:\n- Partial or full loss of hearing\n- Epilepsy (recurrent seizures)\n- Problem with memory and concentration\n- Partial or full loss of vision\n- Loss of limbs due to spreading of infection in the body\n- Bone and joint problems like arthritis\n- Kidney problems, etc.\nWhat are the treatment options for meningitis?\nTreatment for meningitis depends upon its cause.\nTreatment of viral meningitis\nViral meningitis cannot be cured by antibiotics and can improve on their own in some weeks. The most common treatment for viral meningitis is:\n- Complete bed rest\n- Intake of fluids\n- Over-the-counter medicines for fever and body aches.\nSome doctors can even prescribe corticosteroids that can help in reducing brain swelling and anticonvulsant medicine to control seizures.\nTreatment of bacterial meningitis\nBacterial meningitis is the most severe form of meningitis and should be immediately treated after diagnosis.\nMedications such as intravenous antibiotics, corticosteroids, etc are prescribed which can help in reducing and controlling the symptoms of the infection such as swelling and seizures.\nHow is meningitis prevented?\nMeningitis is a serious infection and can lead to severe consequences. It is therefore important to follow certain basic things to maintain a healthy lifestyle such as:\n- Taking proper rest\n- Avoiding alcohol and smoking\n- Avoiding contact with sick people\n- Taking medications at proper time interval\nApart from the above points, the most important preventive measure for meningitis is vaccination. There are some vaccines that can protect a child from developing meningitis such as:\n- Meningococcal vaccine\n- Pneumococcal conjugate vaccine\n- Haemophilus influenzae type B vaccine']	['<urn:uuid:12eec351-893f-4160-a8e0-6f2ce71e418e>', '<urn:uuid:74bf2a1f-e339-48da-8b1f-6f986ad702bf>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T01:37:09.743768	13	107	1691
80	best goose decoys practical features movement tricks	Top goose decoys combine practicality with movement features for maximum effectiveness. Premium options like Hard Core Rugged Series include Trumotion bases for realistic bobbing in water and removable heads for portability. Big Foot decoys use feeding postures to attract geese, while Avian-X offers varied postures and sturdy steel bases. For enhanced realism, hunters can add movement using jerk cords by tying 3-5 floating decoys 5-7 feet apart, and can also incorporate crow decoys as confidence decoys. Additionally, using white trash bags as flags can effectively draw attention to the spread.	"['Quality gear can make a huge difference when it comes to hunting. And that’s definitely the case when it comes to geese, who do their best to elude and escape even the most diligent and skilled of hunters. Successful goose hunting means using quality gear, and using it well. We’re here to look at one key part of the puzzle: goose decoys.\nSo, what qualities should a good goose decoy have? Here are the three main ones:\nDurability – Hunting isn’t always a fair weather sport, and by the end of the seasons both hunters and their equipment will have endured their fair share of tough conditions. Quality goose decoys will withstand the elements – and keep working well – all winter.\nRealism – With this one, we’re referring to how realistic a goose decoy appears. Geese are wily creatures, and decoys need to be able to fool them. This means serving as a clever disguise that doesn’t stick out. Also, it’s not enough to look realistic – it also needs to be durable enough to maintain its disguise despite wear and tear.\nPracticality – Hunting doesn’t take place in a design studio. It happens out there in the marshes, rivers, and fields. Goose decoys should reflect this reality, and be usable, portable, and practical for a variety of terrains, vehicles, and situations.\nWith all that in mind: what are the best goose decoys out there today? After searching around and considering all the options, here are our top choices.\nGet Great Guns and Ammo Deals!\n1. Big Foot Full Body Goose Decoys – Feeding Geese\nThe Big Foot decoys are incredibly realistic – photos of them might fool you into thinking you’re looking at a picture of real geese! Big Foot is a great company bringing many years of experience to the table, and their decoys reflect this.\nTheir full body decoys are molded to include realistic texture and color that doesn’t wear off with time, as it’s not based in paint. So, Big Foot meets the durability test. Beyond that, the feeding posture is a clever trick designed to fool geese into thinking there’s food to be found, an excellent way of luring them down.\nThe one drawback of Big Foot’s feeding geese decoys is their lack of sturdiness against tough winds. Despite that, though, these feeding geese are fantastic decoys proven by experience.\n2. Hard Core Rugged Series Full Body Canada Goose Decoys\nHard Core’s decoy series come with a slightly larger price tag, but reflect it in quality and a great set of features that make them practical and effective. For one, the series includes decoys representing a variety of postures and positions, essential for fooling geese (especially wise, end-of-season flocks) unconvinced by groups of identical decoys.\nThey come with Trumotion bases, excellent for creating realistic bobbing motions when used in water, and have removable heads to provide portability. Plus, armored paint keeps the decoys free from chips and cracks.\nThough they’re a bit more expensive, the Hard Core Rugged Series combines all the essential elements of quality decoys, and makes for an excellent foundation for a set.\n3. Avian-X AXP Goose Decoys Fusion Honker Decoys\nAvian-X reigns among goose decoy manufacturers for the sturdiness and realistic appearance of their decoys. Looking at them shows incredibly detailed painting and carving – including a realistic black for the heads that doesn’t shimmer, glare, or betray itself in any light or from any angle.\nIn addition, Avian-X’s uses a tough, adhesive paint that doesn’t wear, making their decoys practical for use season after season. Another great feature of the Fusion Honkers is the sturdy construction and base.\nThe paint is tough, as are the bodies as a whole – and so are the heads, which are mounted in the factory. The Fusion Honkers can handle rough rides in the bag of a truck without suffering dents, chipped paint, or other damage that can decrease their utility.\nPlus: they’re mounted on a base of solid steel, providing them resistance to winds other decoys lack. Seeing as they also offer models in a variety of postures, Avian-X offers a complete package with all the essential qualities needed for great, effective decoys.\n4. DOA Rogue Series Goose Decoys\nDOA – Dead on Arrival – is the tongue and cheek name of this excellent series of decoys. This company has made a name for itself by placing its focus on carving and producing the most realistic decoys possible. It shows. Placed next to a real goose, you’d be hard pressed to tell the difference between your DOA and his real-life counterpart.\nComing with excellent carving, non-reflective paint, and a hex-shaped base that allows them to stay upright in tough winds and on a variety of uneven terrains, DOA has succeeded in producing one of the most excellent decoys on the market.\nThe price reflects this – the DOAs are slightly more expensive than some of their competitors, but given their incredible quality, we think the price is well worth it.\nHonorable Mention: Glacier Glove Decoy Gloves\nDecoys can lose their utility if they’re used without one key accessory: decoy gloves. There are many options out there, but Glacier Gloves produces a fine pair of decoy gloves perfect for keeping your hands warm – and covert – while moving, replacing, and handling your decoys.\nThis is especially key on cold mornings in winter, when cold hands can mean losing the dexterity required to take a great shot. The Glacier Gloves are waterproof neoprene, come up to the elbow, and are lined with fleece that retains warmth beautifully.\nAnd as an added bonus, they come in a variety of sizes, an essential for maintaining a smooth, camouflaged appearance out there on water. Our favorite part? They come at a great price considerably below many of the other decoy gloves out there. The Glacier Gloves are an excellent way to round off a decoy set, and make sure it gets the job done.', 'Hunting VideosBowhunt or Die\nA Dozen Goose Hunting Tips\nWould you like to improve your goose-hunting success and make each trip more enjoyable? These 12 tips could help.\nFind the Best Concealment\nWhen selecting a goose-hunting location in a harvested crop field, look for areas that have elevated stubble or more prevalent stubble. Hiding is the most important factor in waterfowl hunting. If you can locate areas that will better conceal the hunters, your chances for killing some geese greatly improve.\nMud Your Ground Blind\nLooking natural is always crucial. The best way to fully conceal your layout blind or pit is to “mud” the material before any natural vegetation is applied. Simply mix up a bucket of dirt and water and rub the slurry across the ground blind material. Be sure to let it dry before going afield. The mud will eliminate all unnatural shine or sharpness.\nElongate Goose Decoy Stakes For Even Better Concealment\nGoose hunters also can use any type of goose decoy—full body, shell or other styles—to help conceal layout blinds where natural cover is scarce and shadows are prevalent. Raise the goose decoys slightly by lengthening the stakes 6 to 8 inches, then place them around the perimeter of your ground blind.\nProper Goose Decoy Placement\nPlace your most realistic goose decoys (full bodies, shells and floaters) on the downwind side of your goose decoy spread near the “landing zone” where geese will see them as they approach. Place the least realistic goose decoys (silhouettes, windsocks and rags) farthest from the view of approaching birds.\nAvoid Edges and Cover\nIt’s best not to set goose decoys near fence lines, brush piles, high vegetation or ditches that offer natural cover; geese shy away from any cover that can conceal a predator. Place your goose decoys in the middle of the field or water where the geese can see all around them.\nWatch the Weather\nPay attention to weather conditions and bird activity when you set up your goose decoy spread. When it is warm or calm, geese tend to spread out more across fields and water, and often will break into smaller “family groups.” As the weather gets colder or when the wind picks up, geese tend to “huddle” into more tightly packed flocks. Set your spread accordingly.\nAdapt to Conditions\nAlso remember, when the weather gets cold and snowy, geese naturally tend to lay down in the field. Utilizing more shell goose decoys helps you better replicate these cold-weather flocks. If full-bodied goose decoys are used, simply remove the bases or legs so the decoys lay flat on the ground.\nAdd Some Crows\nGoose hunters often place a few crow decoys off to one side of their field spreads. These “confidence” goose decoys help lessen the wariness of geese by making the goose decoy spread appear more lifelike.\nUse Jerk Cords\nOn calm days, rigging several jerk cords is an inexpensive way to add movement to your spread that will help fool the wariest birds. Tie three to five floating goose decoys 5 to 7 feet apart along a cord. Anchor one end and use the other end to “pull” the goose decoys into motion.\nTrash Bag Flag\nRealistic goose flags and kites are great for drawing the attention of snow geese to your goose decoy spread, but a 13-gallon white trash bag tied to a long pole works just as well.\nDon\'t stop goose calling when geese are close. Feeding and landing geese make a lot of noise; a silent goose flock is either asleep or alarmed.\nLook for the ""X""\nGeese will always have a specific area where they tend to land. Each day you hunt, be sure to remember the exact spot in the field where the birds pitched in. The next day you will be much more successful if you are set up on the “X.”']"	['<urn:uuid:b62d544e-fc5f-43bd-a479-3aa22645277f>', '<urn:uuid:d481bdef-c7b2-4f8f-a231-0e0dd8c92fc9>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:37:09.743768	7	90	1643
81	fungus skin problems hospital stay costs	Fungal skin infections cause problems like pimple-like bumps and oozing patches, requiring treatment with medications like nystatin cream. In hospital settings, patients with fungal infections typically face longer hospital stays and higher costs, as demonstrated by data showing that patients with Candida auris infections had intensive care unit stays of 12 days (compared to 6 days for other infections) and incurred median costs of $10,121 (compared to $8,608 for control groups).	['None of us like to think about it, but fungus is everywhere, and the potential for fungal infections is lurking around us constantly.\nWhen you think of fungal infections, you probably think of things like athlete’s foot or jock itch, both of which are common fungal infections that are uncomfortable and inconvenient.\nFungal infections can be more than uncomfortable, however – they can be deadly. What may start as a simple yeast infection of the skin, known as candidiasis, can become a severe invasive infection within the bloodstream.\nYeast infections are the cause of more than 3.6 million U.S. healthcare visits each year, resulting in an estimated 3 billion dollars in direct medical costs, according to the Centers for Disease Control and Prevention (CDC).\nSome species of fungus are becoming increasingly resistant to antifungal treatment because patients do not complete their recommended course of treatment, allowing the fungus to adapt and change to improve its immunity to the medication.\nHowever, if caught early, most fungal infections of the skin can be treated with topical medications like nystatin cream and other similar drugs.\nIf you find yourself wondering what is nystatin cream used for, you may be surprised by the applications it has to a very common problem affecting infants and adults alike.\nNystatin cream is a topical medication that belongs to a class of drugs called antifungals. Antifungal medications are used to kill a fungus that may be infecting the skin and to prevent the growth of the fungus.\nNystatin is sold under several brand names, including Mycostatin, Nystex, and Pediaderm AF.\nIt can be used on its own, but it is often combined with corticosteroid creams that are used for inflammation treatments.\nWhen sold as nystatin topical cream, it is for use as an antifungal medication only.\nCombination medications, such as nystatin and triamcinolone, are sold under brand names like Mycolog-II, Mytrex, and Quenalog.\nNystatin cream is an antifungal medication that is used to treat certain fungal infections and conditions, including Candida (Monilia), or yeast infections.\nIt can also be used to treat diaper rash in infants.\nFungal infections, called mycosis, are caused by an infestation of fungus on the skin. Fungi can be found everywhere in our environment, including on household surfaces, in dirt, on plants, and on the skin.\nNystatin cream treats a type of fungal infection called cutaneous candidiasis, which is caused by yeast.\nYeast infections aren’t contagious, but they can be uncomfortable. They are most commonly found in warm areas of the body that are creased and are regularly moist, such as the armpits or groin.\nPeople who are obese and have folds of skin that are difficult to clean may experience yeast infections within the folds.\nPeople with diabetes and those taking antibiotics are also considered to be at a higher risk of yeast infection. Signs of a yeast infection on the skin include:\n- Pimple-like bumps\n- Oozing patches\nYeast infections can also occur in the nail beds, in the mouth (called thrush), or in the vagina.\nHowever, nystatin cream is only used for the treatment of yeast infections on the skin.\nIn some cases, fungal infections can become severe and become more serious.\nYou should seek medical attention if you notice any of the following symptoms in addition to those listed above:\n- Flu-like symptoms\n- Body aches and pains\n- Night sweats\n- Respiratory issues\n- Shortness of breath\n- Symptoms of meningitis\n- Severe headache\n- Stiff neck\n- Light sensitivity\nBecause we are always surrounded by fungus, anyone can get a fungal infection at any time.\nHowever, fungal infections are most common in people with weakened immune systems or certain other characteristics. People considered at increased risk of a fungal infection include:\n- People who are currently hospitalized\n- Patients living with HIV or AIDS\n- Transplant recipients\n- People taking immunosuppressant medications\n- People with compromised immune systems\n- Patients undergoing treatment for cancer\n- People suffering from obesity\nIn addition to skin infections, fungi can also cause other types of illnesses, including asthma or allergies, lung infections, infections of the nail bed, bloodstream infections, meningitis, and more.\nHowever, most fungal infections are minor in their early stages and can be treated effectively if caught early.\nAntifungal medications like nystatin cream work by directly attacking and killing fungal cells and by preventing fungal cells from growing.\nIn order to kill the fungal cells and stop the growth of new cells, medications like nystatin cream specifically attempt to destroy cellular structures and functions that are critical to fungal cells but not to human cells.\nThis way, fungal medications do not cause additional damage to your skin or other parts of the body.\nFungal medications commonly target the cell membrane and cell wall of the fungus, both of which are responsible for the protection of the cell.\nThe cell membrane and cell wall hold the fungal cell together, so when one or both is destroyed, the fungal cell dies and cannot reproduce.\nNystatin cream belongs to a specific subclass of antifungal medications called polyenes, which work by attacking the cell wall of the fungus to make it more porous.\nOver time, the cells of the fungus burst and die.\nBecause nystatin cream is produced by a variety of manufacturers in both brand-name and generic forms and in a variety of strengths, it is relatively inexpensive.\nA 15-gram tube of nystatin cream costs anywhere from 7 to 8 dollars, while a 30-gram tube would cost approximately 13 dollars.\nExtremely large quantities of the medication at a very high strength can be expensive, but the medication is generally considered highly affordable.\nIt is covered by many insurance plans. Patients who have difficulty affording their prescriptions of nystatin cream may receive additional savings by using a pharmacy discount card like Pharmacists.org, which offers savings on all brand name and generic FDA-approved medications at more than 60,000 participating pharmacies.\nPharmacists.org is not insurance, and no personal information or medical history is needed besides your name and contact information.\nParticipants in the Pharmacists.org pharmacy discount program pay a negotiated rate that Pharmacists.org arranges which each drug manufacturer in order to offer the maximum possible discount without complicated paperwork, copays, or coverage limits.\nNystatin cream is generally well tolerated by most patients, so there are many benefits associated with its use:\n- Effective results for treating skin yeast infections\n- Simple to apply\n- Inexpensive and accessible for most patients\n- Available in generic form\n- Relatively few side effects\n- It can be used safely on children and infants for the treatment of diaper rash.\nAlthough there are many benefits associated with nystatin, as listed above, taking the medication also poses some risks. Risks associated with nystatin include:\n- The medication should not be applied anywhere besides the skin. Patients should avoid getting the medication in the eyes or mouth.\n- Nystatin cream can cause an allergic reaction. Patients should make their doctors aware of any known allergies.\n- Patients should avoid wearing clothing that is tight, made of synthetic materials, or doesn’t allow for proper air circulation. Patients should wear loose-fitting clothing with natural fibers until the infection is healed.\n- The medication should be taken for the full length of the prescription. Symptoms may improve prior to the end of the prescription, but patients should continue to use the cream as directed. Failure to do so could result in a return of the infection.\n- When used on children for diaper rash, children should not wear tight-fitting diapers or plastic pants and make sure to keep it out of the reach of children.\n- If used for longer than directed, nystatin cream may begin to cause thinning of the skin and stretch marks on areas with thin skin, such as the face, armpits, and groin. Nystatin cream should be used only as directed.\nWhat Dose of Nystatin Cream Should I Use?\nThe dosage of nystatin cream recommended for use will vary depending on the patient and the severity of the infection.\nThe amount that is prescribed to you will also vary depending on the strength of nystatin cream that is prescribed by your doctor. In general, nystatin cream should be applied in a quantity that covers the affected area two to four times per day, or as directed by your doctor.\nNo airtight bandages should be used to cover the treated area, but a sterile gauze dressing or bandage is ok.\nThe average length of treatment is between two to eight weeks and is determined by the severity and type of infection.\nIt is important that patients do not skip doses of their medication and wash their hands before and after use in order to prevent further fungal spread and increased resistance of the fungus to medication.\nApproximately seven percent of bloodstream infections caused by yeast are resistant to antifungals, and one in four patients hospitalized with a bloodstream yeast infection dies of the infection.\nTherefore, it is absolutely critical that patients apply their medication as directed, do not skip any doses, and continue their treatment until directed to stop by their doctor.\nVery few side effects are associated with nystatin cream, and the medication is well tolerated by most people.\nSkin irritation is the most common possible side effect associated with nystatin cream, and it usually does not require medical attention.\nPatients should report adverse effects like skin irritation to their doctor or healthcare professional if it continues, gets worse, or becomes bothersome. Some people may experience mild itching.\nAllergic reactions to nystatin cream are also possible; signs of allergic reactions include hives, difficulty breathing, and swelling of the face, lips, tongue, or throat.\nPatients who experience severe burning, itching, rash, pain, or other serious side effects on the application site should discontinue the use of nystatin cream immediately and contact their healthcare professional.\nCan Women Who Are Pregnant or Breastfeeding Use Nystatin Cream?\nNystatin cream is currently classified by the Food and Drug Administration as a Category C medication, meaning that it is not known whether the medication will harm an unborn baby.\nBecause the effects of nystatin cream on unborn babies are unknown, it is important that women who are pregnant or planning to become pregnant speak with their doctors before applying nystatin cream.\nIt is also unknown whether nystatin cream passes through breast milk and could affect a nursing infant. Women considering using nystatin cream should talk to their doctors before breastfeeding and using the cream.\nDo Any Other Drugs Interact With Nystatin Cream?\nNystatin cream carries a low risk of interaction with other drugs taken orally or injected. However, it is important that you give your doctor a comprehensive list of all medications that you are currently taking, including prescription drugs, over the counter medications, vitamins, supplements, and herbal products.\nYou should not start a new medication without checking with your doctor first.\nHow Do I Know If Nystatin Cream Is Right for Me?\nNystatin cream is generally well-tolerated, but it is not right for everyone, including those patients who are allergic to the medication or its ingredients.\nNystatin cream is capable of treating fungal skin infections caused by candida (yeast), including diaper rash.\nYou should get medical advice from your healthcare provider about any prescription drugs, over-the-counter medications, supplements, or herbs that you are taking prior to using nystatin cream.\nNo studies have been conducted regarding the impacts of nystatin cream on pregnant or nursing mothers and their children, so patients should speak to their doctors if they are pregnant, planning to become pregnant, or nursing prior to using nystatin cream.\nNystatin cream must be applied multiple times per day consistently, and patients should not skip any doses or end their treatment early.\nDoing so may cause the fungal infection to get worse, so patients should not use nystatin cream if they cannot commit to applying it appropriately.\nReferences, Studies and Sources:\nKate Byrd, PharmD, is a highly accomplished Medical Writer with a strong background in pharmacy and an unwavering commitment to producing accurate, informative content. After earning her Doctor of Pharmacy degree, Kate embarked on her career as a Medical Writer, where she has since gained valuable experience in developing evidence-based content that translates complex medical information into easy-to-understand articles. We are thrilled to announce that Kate is now bringing her expertise and dedication to the medical writing team at Pharmacists.org. Her passion for empowering readers with reliable and accessible health information aligns perfectly with our mission, making her a valuable addition to our team.', 'To save content items to your account,\nplease confirm that you agree to abide by our usage policies.\nIf this is the first time you use this feature, you will be asked to authorise Cambridge Core to connect with your account.\nFind out more about saving content to .\nTo save content items to your Kindle, first ensure email@example.com\nis added to your Approved Personal Document E-mail List under your Personal Document Settings\non the Manage Your Content and Devices page of your Amazon account. Then enter the ‘name’ part\nof your Kindle email address below.\nFind out more about saving to your Kindle.\nNote you can select to save to either the @free.kindle.com or @kindle.com variations.\n‘@free.kindle.com’ emails are free but can only be saved to your device when it is connected to wi-fi.\n‘@kindle.com’ emails can be delivered even when you are not connected to wi-fi, but note that service fees apply.\nThe burden of healthcare-associated infections (HAIs) is higher in low- and middle-income countries, but HAIs are often missed because surveillance is not conducted. Here, we describe the identification of and response to a cluster of Burkholderia cepacia complex (BCC) bloodstream infections (BSIs) associated with high mortality in a surgical ICU (SICU) that joined an HAI surveillance network.\nA 780-bed, tertiary-level, public teaching hospital in northern India.\nAfter detecting a cluster of BCC in the SICU, cases were identified by reviewing laboratory registers and automated identification and susceptibility testing outputs. Sociodemographic details, clinical records, and potential exposure histories were collected, and a self-appraisal of infection prevention and control (IPC) practices using assessment tools from the World Health Organization and the US Centers for Disease Control and Prevention was conducted. Training and feedback were provided to hospital staff. Environmental samples were collected from high-touch surfaces, intravenous medications, saline, and mouthwash.\nBetween October 2017 and October 2018, 183 BCC BSI cases were identified. Case records were available for 121 case patients. Of these 121 cases, 91 (75%) were male, the median age was 35 years, and 57 (47%) died. IPC scores were low in the areas of technical guidelines, human resources, and monitoring and evaluation. Of the 30 environmental samples, 4 grew BCC. A single source of the outbreak was not identified.\nImplementing standardized HAI surveillance in a low-resource setting detected an ongoing Burkholderia cepacia outbreak. The outbreak investigation and use of a multimodal approach reduced incident cases and informed changes in IPC practices.\nBackground: Antimicrobial decision making in the ICU is challenging. Injudicious use of antimicrobials contributes to the development of resistant pathogens and drug-related adverse events. However, inadequate antimicrobial therapy is associated with mortality in critically ill patients. Antimicrobial stewardship programs are increasingly being implemented to improve prescribing. Methods: This prospective study was conducted over 11 months, during which the pharmacist used a standardized survey form to collect data on antibiotic use. Evaluation of antimicrobial use and stewardship practices in a 12-bed polytrauma ICU and a 20-bed neurosurgery ICU of the 248-bed AIIMS Trauma Center in Delhi, India. Antimicrobial consumption was measured using WHO-recommended defined daily dose (DDD) of given antimicrobials and days of therapy (DOT). Results: Antibiotics were ranked by frequency of use over the 11-month period based on empirical therapy and culture-based therapy. The 11-month DDD and DOT averages when empiric antibiotics were used were 532 of 1,000 patient days and 484 per 1,000 patient days, respectively (Figure 1). When cultures were available, DDD was 486 per 1,000 patient days and DOT was 442 per 1,000 patient days (Figure). Conclusions: The quantity and frequency of antibiotics used in the ICUs allowed the AMSP to identify areas to optimize antibiotic use such as educational initiatives, early specimen collection, and audit and feedback opportunities.\nBackground: Healthcare-associated infections (HAIs) are a major global threat to patient safety. Systematic surveillance is crucial for understanding HAI rates and antimicrobial resistance trends and to guide infection prevention and control (IPC) activities based on local epidemiology. In India, no standardized national HAI surveillance system was in place before 2017. Methods: Public and private hospitals from across 21 states in India were recruited to participate in an HAI surveillance network. Baseline assessments followed by trainings ensured that basic microbiology and IPC implementation capacity existed at all sites. Standardized surveillance protocols for central-line–associated bloodstream infections (CLABSIs) and catheter-associated urinary tract infections (CAUTIs) were modified from the NHSN for the Indian context. IPC nurses were trained to implement surveillance protocols. Data were reported through a locally developed web portal. Standardized external data quality checks were performed to assure data quality. Results: Between May 2017 and April 2019, 109 ICUs from 37 hospitals (29 public and 8 private) enrolled in the network, of which 33 were teaching hospitals with >500 beds. The network recorded 679,109 patient days, 212,081 central-line days, and 387,092 urinary catheter days. Overall, 4,301 bloodstream infection (BSI) events and 1,402 urinary tract infection (UTI) events were reported. The network CLABSI rate was 9.4 per 1,000 central-line days and the CAUTI rate was 3.4 per 1,000 catheter days. The central-line utilization ratio was 0.31 and the urinary catheter utilization ratio was 0.57. Moreover, 3,542 (73%) of 4,742 pathogens reported from BSIs and 868 (53%) of 1,644 pathogens reported from UTIs were gram negative. Also, 1,680 (26.3%) of all 6,386 pathogens reported were Enterobacteriaceae. Of 1,486 Enterobacteriaceae with complete antibiotic susceptibility testing data reported, 832 (57%) were carbapenem resistant. Of 951 Enterobacteriaceae subjected to colistin broth microdilution testing, 62 (7%) were colistin resistant. The surveillance platform identified 2 separate hospital-level HAI outbreaks; one caused by colistin-resistant K. pneumoniae and another due to Burkholderia cepacia. Phased expansion of surveillance to additional hospitals continues. Conclusions: HAI surveillance was successfully implemented across a national network of diverse hospitals using modified NHSN protocols. Surveillance data are being used to understand HAI burden and trends at the facility and national levels, to inform public policy, and to direct efforts to implement effective hospital IPC activities. This network approach to HAI surveillance may provide lessons to other countries or contexts with limited surveillance capacity.\nBackground: The multidrug-resistant fungus Candida auris is emerging as a major cause of healthcare-associated infection globally. Understanding the epidemiology of these infections in vulnerable groups such as cancer patients is important for hospital infection control and their effective management. In this report we present diagnostic, clinical, antifungal resistance and outcome data of 11 cases of C. auris infection from an oncology center in India. Methods:C. auris strains were identified by Sanger-based DNA sequencing of the internal transcriber spacer (ITS) gene. Antifungal susceptibility testing (AFST) was performed using the broth dilution method. Identification and AFST were checked by the WHO Collaborating Center for Reference & Research on Fungi of Medical Importance. Patients had both empirical as well as directed therapy with antifungal agents based on AFST results and clinical assessment. Results: Between November 2018 and March 2019, 11 cases of C. auris (8 from patients with solid-organ tumors and 3 from hematological malignancy) were detected. Two distinct genetic clusters were identified by ITS gene sequencing; one of these clusters showed 100% homology with a previously unknown C. auris isolate (GenBank accession no. MK881076) and the other cluster had a 100% identity score with isolates from Japan and South Korea (GenBank accession nos. MH071441, KY657027, and EU884189). All 11 strains were resistant to fluconazole. With voriconazole, 1 isolate was susceptible, 3 were resistant, and 7 showed dose-dependent susceptibility. Two isolates were resistant to amphotericin B. Resistance to caspofungin or anidulafungin was noted in 1 of 11 isolates (9%); most showed intermediate susceptibility (63% to caspofungin). Among all of the patients, 72% were from the intensive care unit (ICU) or the high-dependency unit. The 30-day all-cause mortality was 5 of 11 (45%) in the C. auris group and 4 of 11 (36%) the control group (ie, infections with other Candida spp during same period). Duration of ICU stay in the C. auris group was 12 days and in the control group it was 6 days. The median cost (in terms of hospital bill at the time of discharge or death) for management of Candida auris infection and the primary medical condition was US$10,121 for the C. auris groups and US$8,608 for the control group. Most cases (10 of 11) were detected in wards without isolation rooms, and 8 of the 11 C. auris cases (73%) were detected in patients in the intensive care unit. Conclusions: Morbidity, mortality, ICU stay, and healthcare costs are significant in C. auris infection.']	['<urn:uuid:3353fa4b-7b43-4584-b5ae-3b83c549733b>', '<urn:uuid:fb96a79a-b388-4b7e-a8cc-0fd888b23eac>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	6	71	3497
82	What changes in the sky during sunset?	During sunset, as the sun lowers toward the horizon, its light must travel through more of our atmosphere than when it is higher up in the sky. More atmosphere means more air molecules which scatter violet and blue wavelengths away from your line of sight, while red and orange wavelengths persist. This explains why sunset skies feature more vibrant red-orange hues than blue hues.	['Oliver Scott of Wombarra Our atmosphere interacts with sunlight to produce blue hued skies on Earth. Blue light scatters more readily than other colours; thus creating its distinct blue color.\nSunlight contains all of the colors of the rainbow, which we can see when passing it through a prism. But when the sun is low on the horizon at sunset or sunrise, its light has to travel farther through our atmosphere before reaching you and thus becomes even more blue-scattered before reaching you.\nLight traveling through the atmosphere scatters in all directions, producing a pale and blue glow as its path passes over particles in the atmosphere (including air molecules). This phenomenon, which was first identified by 19th-century British physicist Lord Rayleigh, is known as Rayleigh scattering.\nOn a clear day, when looking up at the sky you will likely witness scattered blue light that is lighter than direct sunlight coming directly from the Sun due to more atmospheric molecules blocking its path through lower elevations than at higher altitudes.\nAs you travel higher into the atmosphere, blue light becomes even paler as more atmosphere has to pass through before reaching your eyes. Additionally, many scattered beams reach only some parts of their original path before reaching all your senses.\nLight of all wavelengths scatters in the atmosphere, but air molecules tend to scatter shorter-wavelength light such as blue and violet more readily due to having closer wavelengths with vibration frequencies of electrons in molecules – when these electrons vibrate they radiate energy and give the sky its signature blue hue.\nRed light wavelengths are longer, making them less vulnerable to scattering; this explains why astronauts see Earth as red rather than blue from space, and why sunrise and sunset make more visible red hues than usual.\nRefraction and total internal reflection cause multicolored light to bend into various wavelengths, while scattering leads to its blue hue. It varies based on how much of each type of particle there are in the atmosphere – this depends on where you are.\nAs sunlight travels through our atmosphere, it becomes scattered by gas molecules. Blue light tends to be scattered more than red or orange light due to its shorter wavelength; when struck by air molecules that are smaller than itself, these vibrate, producing Rayleigh scattering – named for the scientist who first discovered this form of electromagnetic radiation.\nWhen blue light strikes water molecules in our oceans and lakes, it takes on a deeper blue hue due to the water molecules’ ability to absorb certain wavelengths of light – not because of what lies above but because water molecules excel at absorbing short blue and violet visible light waves while not being as effective at absorbing longer red or orange visible light waves; hence why open sea appears dark blue.\nAs sunlight travels through our atmosphere, its hue may shift toward red near the horizon due to blue light having to travel further through it before reaching you and being scattered away in other directions. As one ascends higher into our atmosphere, less blue light gets scattered; thus giving our skies above a distinctive dark or bluish violet appearance.\nAs you ascend into the stratosphere, your atmosphere thins further as its temperature increases with altitude. This layer extends approximately 30 miles above Earth’s surface and features dry air with few clouds, perfect for airplane flights as it rarely experiences turbulent airflow. Airplanes generally fly within this layer without encountering much turbulence as a result.\nAt even higher altitudes lies the exosphere – our atmosphere’s outermost layer, composed of extremely thin and sparse molecules. The exosphere contains most of our planet’s oxygen supply as well as creating a clear starry night sky due to sodium’s presence here. You might even spot shooting stars with distinct hues!\nOur eyes are most sensitive to blue light, so when combined with all of its other hues we perceive the sky as blue. Otherwise, any remaining sunlight may become mixed with different colors to appear white – but if our sun were instead emitting violet or red wavelengths instead, our view may change drastically and reveal more purple or red shades instead.\nAt nighttime, the sky becomes blue due to sunlight scattering through the atmosphere. At higher elevations of atmosphere, more air molecules come into contact with it, increasing its likelihood of scattering shorter wavelengths such as blue light. When this happens, longer wavelengths remain that cause it to mix together to appear white in appearance.\nIf you were standing on another planet like Mars, the sky might appear different than on Earth. Because the atmosphere on Mars is much thinner and less likely to scatter blue light, its sky typically displays as darker shades of blue or even bluish violet hues.\nOcean and lake waters appear blue due to how light interacts with them: long wavelengths of red and orange light absorb into the sea or lake water, leaving only blue light visible for you to see.\nAt sunrise and sunset, the Sun appears lower in the sky than during daytime. This requires it to travel through a greater portion of atmosphere before reaching your eyes – more blue light is scattered by this process, while shorter wavelengths (red and violet) pass straight through without scattering into space. As a result, sunrise and sunset seem to alter its hue as they pass from horizon to horizon.\nSunsets are one of the most spectacular natural events you can witness, offering an ideal opportunity to step back from daily stressors and experience nature’s beauty. Watching an idyllic sunset is great for mind, body, and soul; not to mention great for eyesight! But have you ever wondered why the sky becomes blue at sunset? That is because our atmosphere scatters sunlight differently during these evening moments.\nAs the sun lowers toward the horizon, its light must travel through more of our atmosphere than when it is higher up in the sky. More atmosphere means more air molecules which scatter violet and blue wavelengths away from your line of sight while red and orange wavelengths have difficulty being dispersed – this explains why sunset skies tend to feature more vibrant red-orange hues than blue hues.\nAt sunrise and sunset, the sky appears more yellow due to the Sun passing closer to Earth’s surface – meaning a thinner atmosphere layer which scatters shorter wavelengths more readily – while red and orange wavelengths have more of a chance of making their way through.\nAs you ascend in altitude, the amount of blue light that penetrates the atmosphere decreases due to fewer gas molecules being available to scatter it; this causes skies at high elevations to appear paler or even whiter due to less of its blue wavelengths reaching your eyes.\nAnother frequently-asked question about the sky concerns why it darkens at night. One reason could be due to how the Moon lacks an atmosphere; therefore there are no gases to diffuse its light, leading to almost 100% direct illumination with greater luminosity than would exist here on Earth.']	['<urn:uuid:e1db9875-edb0-4c5c-8f49-0af3a87b03ec>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	7	64	1177
83	What are some digital platforms available for teachers to collect and evaluate student assignments with inline comments and feedback?	There are two main tools for this purpose: Dropbox, which allows file sharing and feedback bundling, and Blackboard's in-line grading feature, which enables opening word documents, adding comments/feedback, and linking the submission to a grade within Blackboard.	['What tools can I use to measure understanding?\nWhat are Assessment Mechanics?\nAssessment Mechanics involve the various frameworks (tools/software) that facilitate our assessment activities and how they are implemented. If your assessments are given online, whether a quiz, discussion, or multimedia presentation, there are tools available to help you distribute, collect, and grade student work.\nHow Can I Use Assessment Mechanics in My Course?\nQuizzing and Collecting Answers\nGoogle Forms – Use Google Apps to create a Google Form; Collect answers to your questions (multiple choice, essay-answer, matching, etc.) in a Google Spreadsheet, exportable as an Excel file; automatically collect the usernames of the students submitting answers.\nBlackboard Quiz – Blackboard Quizzes and Tests can be self-gracustomizableisable down to the type of question and when it becomes available, if it is available without a password, etc. Automatically loads into the Blackboard Gradebook.\nAdaptive Release – With Blackboard’s Adaptive Release framework, you can unlock materials or assessments based on individual student performance. This is one way to personalize the student’s learning experience.\nAssessing Essays/Complex Assignments, and Providing Feedback\nDropbox – Dropbox is a filesharing tool that allows you to share files and folders among students (and them to share files and folders among themselves) in which assignments can be both turned in and bundled with feedback.\nIn-Line Grading Feature of Blackboard – Blackboard’s in-line grading feature allows you to open up word documents, mark them up with comments/feedback and link that submission/feedback bundle to a grade, all within Blackboard.\nWordPress Sites for Presentations – Essays and complex multi-media presentations can easily be uploaded onto the course website or blog. The comment feature allows for faculty feedback in the open (as long as it is general and not grade-specific), which can allow for a revision cycle to take place if the student is asked to revise their presentation.\nStudents can assess each other for you. Probably the most natural medium for peer assessment is discussion, where students can present to each other in a low-stakes environment, positing new ideas and receiving a great deal more feedback than the teacher, by herself, can provide.\nBlackboard Discussion Boards – Recently revamped to encourage more discussion, Blackboard Discussion Boards have the benefit/downside of being private. The Discussion Boards in Blackboard still aren’t as easy to navigate as most website comments fields, but their improvement is promising.\nWordPress and Blog Comments Fields – One of the best options for student discussion is free and open-source: the comments sections beneath a page or post on a WordPress site. Discussions can branch within WordPress while continuing to evolve in a more-or less linear fashion, making the discussion (in my opinion) very manageable and encouraging for both students and teachers.\nAuto-graded quizzes or peer review – If you allow students to engage in an iterative assessment process, their final outcome can be improved. Providing a way for self-assessment (non-graded) to happen is one way to encourage practice towards greater mastery.\nConsiderations for Online Courses\nThere are many tools now available, and more developing every day, for assessing your students’ understanding. First, you should ask yourself what evidence of understanding you wish to assess. If it is intelligence of term definitions and formulas, rote memory of certain facts, exhibition that the student can solve basic mathematic problems, etc., then maybe self-grading quizzes available through Blackboard can best serve that purpose.\nIf you’re looking for evidence of more comprehensive understanding, however, then there are a number of tools available. The technology you choose depends on your outcome goals.\nHistory of Alaskan Ungulates\nGreg Finstad’s “History of Alaskan Ungulates” asks students to develop research projects that are later incorporated into the course as learning materials for subsequent semesters.\nĆukušić, M., Garača, Z., Jadrić, M. (2014, March). Online self-assessment and students’ success in higher education institutions. Computers & Education, 72. 100-109.\nKhawaja, M., Prusty, G.B., Ford, R.A.J. (2013). Can more become less? Effects of an intensive assessment environment on student’s learning performance (PDF). European Journal of Engineering Education, 38 (6). 631-651.\nKihlstrom, J. F. (2014). How students learn and how we can help them. University of California Berkeley.\nPennebaker, J.W., Gosling, S.D., & Ferrell, J.D. (2013). Daily online testing in large classes: Boosting college performance while reducing achievement gaps. PLoS One, 8(11): e79774.\nWatch: Effective Assessment and Measurement\nUAF Instructional Designers\nThis page has been authored collectively by the experts on the UAF Instructional Design Team. Let us know if you have suggestions or email@example.com']	['<urn:uuid:ec3cc8c2-fbdf-424b-8907-516640a01caf>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	19	37	742
84	bad smell car exhaust what chemicals cause it	Car exhaust smells are caused by soot (tiny carbon particles), lead (from vehicles using unleaded petrol), carbon monoxide, oxides of nitrogen and hydrocarbons, which are produced due to the incomplete burning of non-renewable fuels.	['* Pollutants enter the surroundings from numerous sources, although burning of fossil fuels adds the most to air pollution. * Exhaust smells from cars contains soot (tiny carbon particles), lead (from vehicles using unleaded petrol), carbon monoxide, oxides of nitrogen and hydrocarbons due to the incomplete burning of non-renewable fuels. * Burning of non-renewable fuels in the combustable engines of vehicles and electrical power channels also emits large amount of co2 into the atmosphere. * Human being activities including open burning cause extreme smog and haze. * Industrial plants and industrial facilities also pump large amount of carbon monoxide, oxides of nitrogen and sulphur dioxide into the atmosphere. * The two oxides of nitrogen and sulphur dioxide can match water vapour in the ambiance to form nitric acid and sulphuric chemical p respectively. * Then, they will fall returning to the Earth as acid rainwater.\n* Rain typically has a pH benefit or your five. 6, due to the presence of dissolved carbon, which forms carbonic acid. * The pH of acid rainwater is, nevertheless , less then 5. 0.\nMajor primary contaminants produced by human activity include: 2. Sulphur oxidesВ (SO4) - specifically sulfur dioxide, a chemical compound with all the formula SO2. SO2В is produced by volcanoes and in various commercial processes. Seeing that coal and petroleum generally contain sulfur compounds, all their combustion creates sulfur dioxide. Further oxidation process of SO2, usually in the presence of your catalyst just like NO2, forms H2SO4, and therefore acid rain. This is one of many causes to get concern within the environmental effect of the use of these fuels as power sources. * Nitrogen oxidesВ (NO3) - especiallyВ nitrogen dioxideВ are provided from high temperature combustion, and are generally produced normally duringВ thunderstormsВ byВ electrical launch. Can be seen while the brown haze dome above orВ plumeВ downwind of metropolitan areas. Nitrogen dioxide is the chemical compound together with the formula NO2. It is one of many several nitrogen oxides. This reddish-brown toxic gas contains a characteristic sharpened, biting smell. NO2В is probably the most prominent surroundings pollutants. 2. Carbon monoxideВ (CO)- is a colourless, odourless, non-irritating but incredibly poisonous gas. It is a merchandise byВ incomplete combustionВ of fuel just like natural gas, coal or wooden. Vehicular exhaust is a key source of carbon monoxide. * Co2 dioxideВ (CO2) - a colourless, odourless, non-toxicВ greenhouse gasВ also linked withВ ocean acidification, emitted by sources including combustion, bare cement production, andВ respiration. It is in any other case recycled in the atmosphere in theВ carbon pattern. * Volatile organic compoundsВ - VOCs are an important outdoor air pollutant. In this field they are often split up into the separate categories of methane (CH4) and non-methane (NMVOCs). Methane is an extremely efficient greenhouse gas which in turn contributes to increased global warming. Different hydrocarbon VOCs are also significant greenhouse fumes via their job in creating ozone and prolonging the life of methane in the atmosphere, although the effect varies according to local quality of air. Within the NMVOCs, the perfumed compounds benzene, toluene and xylene will be suspected carcinogens and may cause leukemia through prolonged coverage. 1, 3-butadiene is another hazardous compound which is often connected with industrial uses. * Atmospheric particulate matterВ - Particulates, otherwise referred to as particulate matter (PM) or good particles, are tiny debris of sturdy or liquid suspended within a gas. In comparison, aerosol refers to particles as well as the gas together. Sources of particulate matter could be manmade or natural. Some particulates arise naturally, originating from volcanoes, dust particles storms, forest and grassland fires, living vegetation, and sea apply. Human activities, such as the burning up of non-renewable fuels in cars, power crops and numerous industrial techniques also generate significant amounts of aerosols. Averaged above the globe, anthropogenic...']	['<urn:uuid:dc447dc9-abe4-488d-ad61-4da9811d04e8>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	8	34	634
85	How do energy drinks affect tooth health and cavity formation?	Energy drinks are highly acidic with an average pH of 3.05, well below the 5.5 threshold where tooth enamel begins dissolving. This acidity can lead to dental erosion, where enamel's mineral content permanently dissolves through de-mineralization. While saliva normally neutralizes mouth acid and restores minerals to enamel, frequent consumption of these drinks can overwhelm this natural protection, increasing risk of tooth decay and cavities, especially if the beverages are sipped over extended periods.	"[""Posts for tag: nutrition\nAlthough a variety of foods provide energy-producing carbohydrates, sugar is among the most popular. It’s believed we universally crave sugar because of the quick energy boost after eating it, or that it also causes a release in our brains of serotonin endorphins, chemicals which relax us and make us feel good.\nBut there is a downside to refined sugars like table sugar or high-fructose corn syrup: too much in our diets contributes to conditions like cardiovascular disease, diabetes, and dental disease. On the latter, sugar is a primary food source for oral bacteria; the more sugar available in the mouth the higher the levels of bacteria that lead to tooth decay and gum disease.\nModerating your intake of refined sugars and other carbohydrates can be hard to do, given that many processed foods contain various forms of refined sugar. A diet rich in fresh fruits and vegetables helps control sugar intake as well as contribute to overall health. Many people also turn to a variety of sugar substitutes: one study found roughly 85% of Americans use some form of it in place of sugar. They’re also being added to many processed foods: unless you’re checking ingredients labels, you may be consuming them unknowingly.\nSugar substitutes are generally either artificial, manufactured products like saccharin or aspartame or extractions from natural substances like stevia or sorbitol. The good news concerning your teeth and gums is that all the major sugar substitutes don’t encourage bacterial growth. Still, while they’re generally safe for consumption, each has varying properties and may have side-effects for certain people. For example, people with phenylketonuria, a rare genetic condition, can’t process aspartame properly and should avoid it.\nOne alcohol-based sweetener in particular is of interest in oral care. A number of studies indicate xylitol may actually inhibit bacterial growth and thus reduce the risk of tooth decay. You can find xylitol in a variety of gum and mint products.\nWhen considering what sugar substitutes to use, be sure you’re up to date on their potential health effects for certain individuals, as well as check the ingredients labels of processed foods for added sweeteners. As your dentist, we’ll also be glad to advise you on strategies to reduce sugar in your diet and promote better dental health.\nIf you would like more information on your best options for sweeteners, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Artificial Sweeteners.”\nYour teeth’s hard, enamel coating protects them from environmental dangers or disease. But although it’s made of the hardest substance in the human body, enamel isn’t invincible — prolonged exposure to acid can cause dental erosion, a condition in which the enamel’s mineral content permanently dissolves, a process known as de-mineralization.\nDe-mineralization occurs anytime our mouth environment becomes too acidic due to eating or drinking items with high acid content. Saliva normally neutralizes mouth acid in thirty minutes to an hour after we eat, as well as restores mineral content to the enamel (re-mineralization). Danger arises, though, if the saliva’s buffering action is overwhelmed by chronic acidity, caused mainly by constant snacking or sipping on acidic foods and beverages throughout the day — in this situation, saliva can’t complete the process of buffering and re-mineralization.\nAs a result, the enamel may permanently lose its mineral content and strength over time. This permanent dental erosion leads to serious consequences: the teeth become more susceptible to decay; the dentin becomes exposed, which causes pain and sensitivity to pressure and temperature changes; and changes in the teeth’s size and color can negatively alter your appearance.\nIt’s important to take action then before dental erosion occurs. Along with daily oral hygiene, restrict your consumption of acidic foods and beverages to meal times and cut back on between-meal snacks. Rather than a sports drink after exercising, drink nature’s hydrator — water. You should also alter your brushing habits slightly — rather than brush right after you eat, wait thirty minutes to an hour. This gives saliva time to restore the mouth to its normal pH and re-mineralize the enamel. Brushing right after can remove even more of the minerals in softened enamel.\nIf significant erosion has occurred, there are a number of treatment options we can undertake to preserve remaining tooth structure and enhance your appearance. In moderate cases, we can reshape and cover damaged teeth using dental materials like composite resins or porcelain to fill decayed areas or cover teeth with veneers or crowns.\nThe key of course, is to identify dental erosion through clinical examination as soon as possible to minimize damage. Your enamel plays a critical role in protecting your teeth from disease — so take the right steps to protect your enamel.\nIf you would like more information on protecting your enamel, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Dental Erosion.”\nGood nutrition is vital for maintaining health and preventing disease, especially for your mouth. A diet rich in whole foods — fresh fruits and vegetables, protein and dairy products — and low in sugar will not only promote strong teeth and gums, but lessen your chances of developing tooth decay or periodontal (gum) disease.\nDiet is also a prominent factor in reducing the risk for another serious mouth disease — oral cancer. While oral cancer makes up only 3% of total cancer cases reported annually, the five-year survival rate is a sobering 50%, much lower than for other types of common cancers. While genetics plays a role in your susceptibility to oral cancer, lifestyle choices and practices present the greater risk factors for the disease.\nOf these lifestyle factors, refraining from tobacco products, moderating your alcohol consumption and avoiding risky sexual behavior are of primary importance in reducing your cancer risk. With that said, you should also take into account the foods that are part of your daily diet — both what you should and shouldn’t eat. As an example of the latter, some foods contain a class of chemicals known as nitrosamines that are carcinogenic (cancer-causing). One such chemical, nitrite, is used as a preservative in meats like bacon or ham, and may also be found in beer, and seafood products.\nOn the positive side, your diet should be rich in foods that supply antioxidants, substances that protect the body’s cells from damaging, unstable molecules known as free radicals. The best sources for antioxidants (more so than dietary supplements) are plant foods rich in fiber and vitamins C and E. Eating more of these may also reduce your intake of nitrates, animal fat and saturated fat.\nAdopting a moderate, nutritious diet, along with exercise, can have a huge positive impact on your general health and quality of life. Along with other lifestyle changes, better dietary choices can also help ensure a healthy mouth and reduce your risk of oral cancer.\nIf you would like more information on the role of nutrition in reducing your risk of oral cancer, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Diet and Prevention of Oral Cancer.”\nEating is one of the pleasures — and necessities — of life, but people who suffer from temporomandibular joint disorders (TMD) may find eating no pleasure at all — and they may not be eating the right nutritional balance of foods.\nTMD is a collection of conditions that affect the jaw joints, connecting muscles and other related facial structures. If you've been diagnosed with TMD, you're probably not only acquainted with severe pain, but also difficulty opening your jaw as widely as normal. This can make it difficult to chew certain foods.\nThere are a number of effective treatments for TMD, including thermal therapy (hot or cold packs), joint exercise, medication or surgery (as a last resort). But these treatments often take time to make a noticeable difference. In the meantime, you may still need to change what and how you eat to ensure you're getting the nutrients your body needs.\nThe overall strategy should be to soften and reduce the chewing size of your food. With fruits and vegetables, you'll want to peel and discard any hard or chewy skins, and then chop the fruit flesh into smaller pieces. Steam or cook vegetables like greens, broccoli or cauliflower until they're soft and then chop them into smaller portions. You might also consider pureeing your fruit (and some vegetables) to make smoothies with ice, milk or yogurt, or vegetable-based soups.\nTreat meat, poultry or seafood in much the same way, especially biting sizes. Besides cooking meats to tenderness, include moisteners like broths, gravies or brazing liquids to further make them easier to chew.\nDairy foods are an important source of nutrition: eat milk-based products like yogurt or cheese as much as you can handle. If you have problems with these or also nut butters, then consider meal replacement beverages like instant breakfast or whey protein beverages.\nAnd don't forget whole grains. Although some can be hard to chew, you can prepare them in hot cereal form (like oatmeal) to tenderize them. You can also prepare thin bread toast and cut into smaller pieces.\nHopefully, your treatment will bring your TMD symptoms under manageable control. Until then (and after, if need be) adjust your diet to eat the foods that keep you healthy.\nMarch is national nutrition month—a good time to look at the connection between diet and oral health. You probably know that sugar is a major culprit in dental problems. This is because bacteria feed on sugars and create acid that can lead to tooth decay and gum disease. Avoiding sugary foods and drinks as much as possible is a good rule of thumb, but there are some food choices that actually benefit your oral health. Here are nutrition tips that will help keep your smile healthy for life:\nSay cheese. Dairy products such as milk, cheese and yogurt contain calcium and phosphorus to build teeth and strengthen the supporting bone. And cheese neutralizes acid in the mouth to help fight cavities and gum disease.\nChoose lean proteins. Lean meats, poultry, fish, milk and eggs help strengthen teeth. They are rich in protein and phosphorous, which is essential for building strong bones and teeth.\nEat a rainbow. Fruits and vegetables provide many key nutrients, including vitamins necessary for healing, bone strength, and healthy gums. Besides being nutritious, fruits and veggies scrub your teeth while you chew and stimulate the production of saliva, which is necessary for neutralizing acid and rebuilding enamel.\nNibble on nuts. Nuts contain protein, fiber and healthy fats. They also contain essential vitamins and minerals to keep teeth strong and gums healthy. Further, chewing nuts stimulates saliva production, lowering the risk of tooth decay.\nGo for the grains. Studies have shown that eating too many refined carbohydrates such as white bread and sweet bakery items can lead to chronic inflammation, which is a factor in gum disease, heart disease, stroke and other conditions. In contrast, eating complex carbohydrates such as whole grains may reduce inflammation in the body.\nWhat you put in your body can play a big role in preventing tooth decay and gum disease, so choose foods that provide the right building blocks for optimal dental and overall health."", ""Posts for tag: tooth decay\nFinding out you have a cavity isn't the best of news. But finding out it's a root cavity is even worse: if not treated, the decay can spread more rapidly than a cavity occurring in the tooth's crown surfaces.\nOur teeth are basically composed of two parts: the crown, the visible tooth above the gum line, and the roots, the hidden portion beneath the gums. The root in turn fits into a bony socket within the jaw to help hold the tooth in place (along with attached gum ligaments).\nA tooth crown is covered by an ultra-hard layer of enamel, which ordinarily protects it from harmful bacteria. But when acid produced by bacteria comes into prolonged contact with enamel, it can soften and erode its mineral content and lead to a cavity.\nIn contrast to enamel, the roots have a thin layer of material called cementum. Although it offers some protection, it's not at the same performance level as enamel. But roots are also normally covered by the gums, which rounds out their protection.\nBut what happens when the gums shrink back or recede? This often occurs with gum disease and is more prevalent in older people (and why root cavities are also more common among seniors). The exposed area of the roots with only cementum standing in the way of bacteria and acid becomes more susceptible to cavity formation.\nRoot cavities can be treated in much the same way as those that occur in the crown. We first remove any decayed tooth structure with a drill and then place a filling. But there's also a scenario in which the cavity is below the gum line: In that case, we may need to gain access to the cavity surgically through the gums.\nIf you have exposed root areas, we can also treat these with fluoride to strengthen the area against cavity formation. And, as always, prevention is the best treatment: maintain a daily schedule of brushing and flossing and regular dental cleanings to remove bacterial plaque.\nBecause decay can spread within a tooth, dealing with a root cavity should be done as promptly as possible. But if we diagnose and initiate treatment early, your chances of a good outcome are high.\nIf you would like more information on treating root cavities and other forms of tooth decay, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Root Cavities.”\nTooth enamel, to play on a phrase from Shakespeare, is made of “sterner stuff.” The strongest substance in the body, enamel can take years of biting and chewing and keep on going.\nIt does have one nemesis, though—mouth acid, which can soften and erode enamel’s mineral content. This is less of a concern if you have healthy saliva flow, because saliva neutralizes acid in thirty minutes to an hour after an acid attack and can also help re-mineralize the enamel. Daily brushing and flossing also help curb mouth acid by reducing the bacteria that produces it.\nBut as effective as saliva is at neutralizing mouth acidity, it can be overwhelmed by outside acid derived through certain foods and beverages. In the past couple of decades, at least two of these acid sources have grown in prominence: energy drinks and, believe it or not, sports drinks.\nJust how acidic are they? The pH scale runs from 1 to 14, with acidity on the low end and alkalinity on the higher (7 is neutral). Tooth enamel begins dissolving below 5.5. Laboratory tests have pegged the average pH of energy drinks at 3.05 and sports drinks, 2.91.\nBecause of their acidity, frequent energy or sports drink consumption will bring mouth pH into the danger zone for tooth enamel. It’s even more likely if these beverages are sipped over an extended period, which can prevent saliva from getting ahead of any newly introduced acid.\nKeeping your distance from these beverages is probably the safest bet. But if you do imbibe occasionally, follow these common sense tips:\n- Avoid sipping the beverage over long periods—and try to limit drinking them to meal times;\n- After drinking a beverage, wash your mouth out with water and wait an hour to brush to give your saliva time to neutralize any acid.\n- Practice consistent, daily brushing and flossing.\nAbove all, keep a healthy respect for acidic foods and beverages like energy and sports drinks and don’t overuse them. Your tooth enamel will appreciate it.\nIf you would like more information on the effect of sports and energy drinks on dental health, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Think Before You Drink Sports and Energy Beverages.”\nTooth decay doesn't occur out of thin air, but is the end result of bacteria feeding on sugar, multiplying and producing acid. High acidity erodes tooth enamel and creates an environment for cavity development.\nModern dentistry can effectively treat cavities and often save the tooth from further damage. But you don't have to wait: You can reduce your chances of cavities by managing risk factors that contribute to decay.\nHere are 4 top risk factors for tooth decay and what you can do about them.\nPoor saliva flow. Saliva neutralizes acid and helps restore minerals to enamel after acid contact. But your enamel may not have full protection against acid if you have diminished saliva flow, often due to certain medications. You can help increase your saliva by consulting with your doctor about drug alternatives, drinking more water or using a saliva boosting product. Smoking can also inhibit saliva, so consider quitting if you smoke.\nEating habits. High sugar content in your diet can increase bacterial growth and acid production. Reducing your overall sugar consumption, therefore, can reduce your risk of decay. Continuous snacking can also increase your decay risk, preventing saliva from bringing your mouth back to its normal neutral pH. Instead, limit your snack periods to just a few times a day, or reserve all your eating for mealtimes.\nDental plaque. Daily eating creates a filmy buildup on the teeth called dental plaque. If not removed, plaque can then harden into a calcified form called calculus, an ideal haven for bacteria. You can help curtail this accumulation by thoroughly brushing and flossing daily, followed by dental cleanings at least every six months. These combined hygiene practices can drastically reduce your cavity risk.\nYour genetics. Researchers have identified up to 50 specific genes that can influence the risk for cavities. As a result, individuals with similar dietary and hygiene practices can have vastly different experiences with tooth decay. Besides continuing good lifestyle habits, the best way to manage a genetic disposition for dental disease is not to neglect ongoing professional dental care.\nIf you would like more information on managing your tooth decay risk factors, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “What Everyone Should Know About Tooth Decay.”\nWe’ve waged war for decades against tooth decay through oral hygiene and the increasing use of fluoride, nature’s “super weapon” against this disease. And yet, tooth decay remains a significant health problem.\nOne major reason is refined sugar found in many processed foods. In the 1970s researchers raised concerns about the fat content of many processed foods, so manufacturers began removing fat from their products — along with much of the flavor. To compensate, they added sugar. Today, three-quarters of approximately 600,000 food products contain sugar.\nThis has increased average individual consumption to 90 pounds of sugar annually. The World Health Organization says we should consume no more than 20 pounds annually, or about 6 teaspoons a day. A single can of soda contains 4 teaspoons, two-thirds of the daily allowance.\nHigh sugar consumption is an obvious threat to dental health: decay-causing bacteria thrive on it. But the trend has also been linked to serious health problems like diabetes and heart disease.\nHopefully, changes in public policy will one day modify the addition of sugar in processed foods. In the meantime, you can take action for yourself and your family to create a more healthy relationship with this popular carbohydrate.\nShop wisely. Learn to read and understand food labels: steer clear of those containing sugar or large numbers of ingredients. Become acquainted with sugar’s many other “names” like corn syrup or evaporated cane juice. And maximize your shopping on a store’s outer perimeters where you’ll find fresh fruits, vegetables and dairy products, rather than the middle aisles with “boxed” processed items.\nAvoid sugar-added drinks. Limit consumption of sodas, sports drinks, sweet teas or even juice to avoid added sugar. Make water or sugar-free beverages your go-to drinks. It’s much better to eat sugar naturally found in fresh fruits and vegetables, where fiber helps slow it’s absorption in the body, than to drink it.\nExercise. Depending on your condition, physical exertion is good for your overall health. It’s especially beneficial for your body’s ability to metabolize sugar. So with your doctor’s advice, exert your body every day.\nIt’s important to engender a proper relationship with sugar — a little can go a long way. Putting sugar in its rightful place can help you avoid tooth decay and increase your chances of greater overall health.\nIf you would like more information on sugar’s impact on dental and general health, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “The Bitter Truth About Sugar.”\nTooth decay is a destructive oral disease, which along with periodontal (gum) disease is most responsible for tooth loss. And as you age, your disease risk goes up.\nOne form of decay older people often experience is root cavities. Unlike those occurring in the visible crown, root cavities often occur below the gum line and are especially destructive to tooth structure.\nThat's because, unlike the crown protected by ultra-hard enamel, the roots are covered by a thin, mineralized material called cementum. Although cementum offers some protection, it can't compare with the decay-resistant capacity of enamel.\nThe roots also depend on gum coverage for protection. But unfortunately, the gums can shrink back or recede, usually due to gum disease or over-aggressive brushing, and expose some of the root surface. With only the cementum to protect them, the roots can become highly susceptible to decay. If a cavity forms here, it can rapidly advance into the tooth's interior, the pulp, weakening the tooth and increasing its risk of loss.\nTo stop the decay, we must treat root cavities much like we do with crown cavities: by removing any decayed structure and then filling the cavity. But root cavities are often more difficult to access depending on how far below the gum line they extend. We may need to perform minor gum surgery to expose the cavity to treat it.\nBut as with any form of tooth decay, the best strategy is to prevent root cavities in the first place. Your first line of defense is a daily hygiene habit of brushing and flossing to remove dental plaque, the main cause for tooth decay. You should also visit your dentist at least twice a year (or more, if recommended) for more thorough cleanings and checkups. Your dentist can also recommend or prescribe preventive rinses, or apply fluoride to at-risk tooth surfaces to strengthen them.\nYou should also be on the lookout for any signs of gum disease. If you see swollen, reddened or bleeding gums, see your dentist as soon as possible. Stopping possible gum recession will further reduce your risk of root cavities.\nIf you would like more information on the prevention and treatment of tooth decay, please contact us or schedule an appointment for a consultation. You can also learn more about this topic by reading the Dear Doctor magazine article “Root Cavities: Tooth Decay Near the Gum Line Affects Many Older Adults.”""]"	['<urn:uuid:8a0aa93f-90db-4fdc-b730-296c22f85e2f>', '<urn:uuid:57a2e2f8-88c4-4798-9759-c2d46d473394>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	10	73	3936
86	What are the specific emission reduction targets that the European Union aims to achieve through its Green Deal initiative?	The European Union's Green Deal seeks to make Europe carbon-neutral by 2050 and reduce emissions by at least 55 per cent by 2030.	['The reason energy security is a key geostrategic aspiration for all world powers is illustrated by the current situation in Germany where the ‘energy trilemma’ has restricted Europe’s most powerful country from reacting as robustly as it may have liked to Russian threats, including in Ukraine.\nAs a European foreign policy expert wrote in the Financial Times recently, a gas price and supply crisis has made Europe, and especially Germany, “vulnerable to political blackmail” from Moscow. Caught between ambitious climate goals and Russian gas dependence, Berlin has been leaden-footed in response to President Vladimir Putin’s aggressive posture, much to the chagrin of Washington which is leading the transAtlantic response to Russia’s troop build-up on the Ukrainian border.\nGermany’s energy trilemma ~ a term used to describe a country’s ability to provide energy in terms of environmental sustainability, social impact, and security ~ is evident, and due largely to the premium placed on the first two criteria at the cost of the third. Europe’s economic powerhouse is now paying the price and restricting the ability of the continent to collectively push back against Russia.\nThe European Union’s “Green Deal” seeks to make Europe carbon-neutral by 2050 and reduce emissions by at least 55 per cent by 2030. Experts say that while these are laudable aims given the dangers of climate change, and are supported by European public opinion, a sharp spike in demand from a pandemic slump amidst shrinking European gas production and a failure to replenish gas storage facilities after last year’s winter have combined to produce months of energy market volatility.\nAdd to this the massive demand from China, and it is no surprise gas prices have soared. Russia, which provides over 40 per cent of EU gas imports, officially denies restricting supplies to Europe but it is an open secret that this screw is tightened by the Kremlin when required tactically. It is key leverage for President Putin, and he knows it.\nIn fact, he’s been taunting the EU for bringing energy volatility upon itself. If only German regulators approved the Nord Stream 2 pipeline, Russian gas would again flow abundantly to Europe, Mr Putin has suggested. The International Energy Agency, however, points out that Russia “could increase gas deliveries by at least one-third” if it so chose. Earlier this month, gas prices in Europe jumped again after West-Russia talks on the crisis in Ukraine ended in a stalemate.\nIf Russia does indeed go to war with Ukraine, its impact on the energy woes of Germany and by extension Europe will be devastating. Germany, which is committed to phasing out nuclear power by December 2022 and coal by 2030, is set to scramble for energy in the months and years ahead. The country’s environmental protection rules which allow even tiny interest groups to sue against new wind turbines or power lines only complicates matters further. The bottom line is that given an increasingly aggressive Russia, Germany’s leaders will have to find a way to balance climate and security concerns.']	['<urn:uuid:241cf237-5469-4e7c-a284-4819994e3b06>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:37:09.743768	19	23	501
87	how many people got sick from salmonella outbreak linked jalapeno serrano peppers 2008	In 2008, a Salmonella outbreak linked to Jalapeno and Serrano peppers occurred across 43 states and caused 1,442 illnesses.	['Here is the latest DFI newsletter on produce safety and on-farm risk management extraordinaire. WASH HOUSE NEWS shares lessons learned and the latest information from the world of produce safety. This issue includes a FOCUS on Peppers and Produce Safety.\nDFI will be launching a new seasonal newsletter in Fall 2021 that will include all of our usual wash house news, as well as other topics on sustainable agriculture and our northern Nevada food system – stay tuned!\nFOCUS: PEPPERS AND PRODUCE SAFETY\nThis topic is part of an ongoing series to provide concise, crop specific guidance for preventing food borne illness – with DFI’s own practical tips thrown in. Peppers are an essential summer time crop here at DFI, as well as a higher risk type of produce. They have a high water content and can harbor pathogens if they are damaged or affected by heat after harvest. Taking precautions to prevent contamination is important.\nFood borne illnesses associated with fresh peppers have primarily been linked to hot varieties that have been imported to the U.S. from other countries according to the FDA’s microbiological sample research summary report on peppers. One of the larger Salmonella outbreaks linked to hot peppers occurred in 2008 across 43 states, caused 1,442 illnesses, and upon investigation was linked specifically to Jalapeno and Serrano peppers where contamination was likely due to irrigation water on the farm site of origin. Sweet pepper varieties such as bell peppers have been linked to very few outbreaks and resulted in only voluntary recalls.\nAll peppers should be handled with Good Agricultural Practices (GAP’s) from pre to post harvest. Here at DFI, we perform our pre-harvest risk assessment before every harvest to identify hazards; remove any contaminated peppers, leaves and soil; harvest with clean tools into clean containers; and place peppers directly into the cooler without being washed. Any time we can avoid the introduction of water into our wash and pack processes, we do. Water can be a vector that spreads contamination. Key tips for keeping peppers safe are listed below – read more about pepper handling from New Mexico State University.\n- Harvest peppers in the morning and keep them cool during post-harvest handling (do not allow them to sit for more than an hour in direct sunlight).\n- Carefully sanitize surfaces and water during any washing.\n- Remove any damaged or overripe peppers before storage.\n- Cover (while allowing for air flow) and place in cool storage – this reduces food safety risks as well as preventing water loss.\nWe are looking forward to a healthy harvest of peppers this year! Our starts were just transplanted out into the field and we project we’ll send 700 pounds of King Arthur F1, Gourmet F1 and jalapeños to market this year. To maintain pepper crop safety and quality, we recommend developing a comprehensive food safety plan for the farm, designating someone to oversee its implementation, training workers, and keeping records. Need assistance with produce safety planning or best practices for specific crops? Contact DFI for a free call or visit.\nNEWS AND SCIENCE\nProduce safety news from DFI: Row Cover and Food Safety Risks\nIn the high desert, row cover plays an important role in protecting crops from frosty spring nights, foraging wildlife, and insect damage. However, lately we have been considering the potential for food safety risks that could be introduced by our extensive use of row cover at DFI. Row cover is handled by the farm crew daily (moved over the crop during cold nights and back to the side during warm days); it comes into direct contact with crops; it can be contaminated by wildlife visiting or flying over the farm; and it is reused as many seasons as possible (as it is an expensive investment). We know these practices could present contamination risks.\nHere are some practices we are trying out to manage these risks related to row cover:\n- No walking on row cover: If row cover must be removed prior to harvesting or during a warm day, one side of the bed is designated for the row cover to rest on (for the duration of that crop’s life in the field). Harvest and other crop management then takes place on the opposite side. This minimizes the risk of contamination from dirty boots. Bed widths may need to be reduced to make this practice feasible.\n- Use hoops under row cover as often as possible: Hoops minimize the amount of contact that row cover has with the crops beneath it.\n- Remove excessively contaminated cover: Row cover with visual contamination from wildlife should be evaluated and replaced if necessary. Long pieces of row cover are not easy to clean, but we will continue thinking on that one.\n- When not in use, store row cover in a protected area: Clean, covered storage will protect row cover from further risks of contamination from wind, water or wildlife borne pathogens. (We find labeling row cover also saves a lot of time when you want to use it again!)\n- Avoid dragging row cover to the field or within the field: Using equipment or recruiting help to move heavy rolls of row cover will prevent dragging that can lead to contamination from high travel areas of the farm.\nWant to weigh in or contribute further ideas for reducing food safety risks from row cover? Please send a note to email@example.com!\nThe Desert Farming Initiative (DFI) began in 2013, and is a collaboration between the University of Nevada, Reno (UNR), College of Agriculture, Biotechnology, and Natural Resources, Nevada Agricultural Experiment Station (NAES), and Cooperative Extension. DFI is a diversified organic farm at UNR comprised of hoop houses, a propagation greenhouse and 3 acres of field crops. Currently DFI grows up to 90 varieties of certified organic fruit and vegetables each year for local wholesale and retail markets, as well as food security programs in Reno/Sparks and Tahoe region. DFI supports hands-on student learning, advancement of desert crop production, farm extension, and the sustainability of the regional food system.']	['<urn:uuid:a0f6fddd-bbe5-48bd-af79-c599fdfbd9c6>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	13	19	1011
88	What were the historical origins of the very first guitar pickup, and how did they prove it actually worked?	The first guitar pickup was invented by George Beauchamp in the early 1930s for the Rickenbacker 'Frying Pan' instrument. To acquire the patent, Adolph Rickenbacker had to send Hawaiian guitarist Sol Hoopii to Washington to demonstrate the invention to U.S. Patent Office examiners, proving that it actually worked.	['- Rig Rundowns\n- Premier Blogs\nThe Fluence team at Fishman (left to right): Larry Fishman; VP of OEM Sales Rob Ketch; Fluence project manager Charlie Russell; Electrical Engineer John Eck; R&D head Ching-Yu Lin.\nI highly recommend having a conversation with Ritchie Fliegler. Fliegler, a guitar industry vet boasting lengthy tenures at both Fender and Marshall, is psyched about his new collaboration with Larry Fishman. That project can—correction, will—change the way you think about electric guitar pickups.\nMy initial “pre-interview” with Fliegler lasted almost two hours. We discussed everything from aerospace technology to Lionel trains, from Hendrix’s association with Marshall to why no one considers the New York Philharmonic to be a cover band. And somehow it all seemed relevant. Within days I was on my way to Massachusetts to meet with the team at Fishman.\nWe’re accustomed to hearing about “the next new thing” in pickups every time NAMM season comes around. What the Fishman team has concocted is not a new sound. Quite the opposite: It’s a way to consistently and accurately recreate the sounds of the world’s best pickups.\nPickups: A Black Art?\nPickup winding can be something of a black art. No matter the manufacturer or the person doing the winding, one pickup can sound different from the next. Even with consistent production methods, there are such wild card factors as inconsistency in the raw materials used by the manufacturer.\nGuitars, too, are also full of uncontrollable variables. For one thing, they’re made from trees, and trees vary. So do wood-drying conditions, shaping techniques, manufacturing and assembly methods, and the design of the metal and plastic parts we attach to that wood.\nBut for all of us electric guitar huggers, sound eventually comes down to that critical point where a vibrating string excites a pickup magnet. The invisible events that occur in that instant are responsible for all the things we love about pickups—and many of the things that frustrate us.\nThe sound coming from a vintage ’54 Stratocaster pickup or a ’57 PAF humbucker can be magical. But according to Larry Fishman, that “magic” is precisely why his company sidestepped electric guitar pickups for its first 34 years: “Too much voodoo,” he says. Fishman felt he could match the performance of existing electric guitar pickups, but not bring anything new to the party—until now.\nAbout 18 months ago Fishman and his team started thinking about magnetic pickups in a bold unique new way. The result is a new pickup line: Fluence.\nA New Path to Old Sounds\nUnderstanding what’s unique about Fluence requires an understanding of how traditional pickups work—and don’t work. Like beloved family members, pickups have their faults. But as anyone associated with Fluence will tell you, this is not a story focused on bashing traditional pickups. Rather, it’s a love story about preserving the best qualities of great pickups without their associated problems, and at a reasonable cost.\nLayers of printed coils awaiting magnets. The small rings are “vias” that electrically connect the layers.\nA bit of pickup history: In the early 1930s, George Beauchamp applied for a patent on an odd looking guitar-like instrument that included a “pickup.” (The patent uses the variations “pickup,” “pick–up” and “pick up” interchangeably.) His invention was the now-famous Rickenbacker “Frying Pan,” which hosted the first guitar pickup. (To acquire the patent, Adolph Rickenbacker had to send Hawaiian guitarist Sol Hoopii to Washington to demonstrate Beauchamp’s invention, proving to U.S. Patent Office examiners that it worked.)\nWhile there have been thousands of pickup variations and refinements over the last 80 years, most of today’s magnetic guitar pickups aren’t all that different from Beauchamp’s invention. In a conventional pickup, a continuous length of copper wire is wound thousands of times around a bobbin or coil former, surrounding the magnet or magnets. (The wire doesn’t short itself out because the copper strand is coated with a thin layer of insulating material.)\nFluence, however, is based on the notion that coils can be applied rather than wound. Like traces on a circuit board, concentric spirals of “coil” can be printed. Picture, for example, a racetrack- shaped printed circuit board the size of a Stratocaster pickup, with an opening in its center reserved for magnets. One board can hold one spiral, and because it’s printed, each copy is perfectly consistent. The next step involves stacking multiple layers of printed coils and interconnecting them until “pickup” ability is reached. It’s a technique used in the aerospace and telecommunications industries, though it’s never been applied to guitars.']	['<urn:uuid:ed52b4a7-5ebe-4027-a01d-74d6259fb750>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	19	48	756
89	What helps reduce pain after an arm nerve injury?	Physiotherapy plays an important role in pain relief after nerve injuries. When started early, it helps in recovery, prevents complications like contractures and autonomic dysfunction, and may reduce cortical reorganization in the deafferented motor area by maintaining afferent input, which can help relieve phantom limb pain.	['| Article Access Statistics|\n| Viewed||747 |\n| Printed||18 |\n| Emailed||0 |\n| PDF Downloaded||44 |\n| Comments ||[Add] |\nClick on image for details.\n|NI FEATURE: THE EDITORIAL DEBATE II-- PROS AND CONS\n|Year : 2019 | Volume\n| Issue : 7 | Page : 25-26\nNeuropathic pain: Searching for the magic bullet\nLukas Rasulic1, Vikram Singh2, MS Gopalakrishnan3, K V L N Rao2\n1 Department of Neurosurgery, University of Belgrade, Serbia\n2 Department of Neurosurgery, National Institute of Mental Health and Neurosciences, Bangalore, Karnataka, India\n3 Department of Neurosurgery, Jawaharlal Institute of Post-graduate Medical Education and Research, Pondicherry, India\n|Date of Web Publication||23-Jan-2019|\nDr. M S Gopalakrishnan\nDepartment of Neurosurgery, Jawaharlal Institute of Post-graduate Medical Education and Research, Pondicherry\nSource of Support: None, Conflict of Interest: None\n|How to cite this article:|\nRasulic L, Singh V, Gopalakrishnan M S, Rao K V. Neuropathic pain: Searching for the magic bullet. Neurol India 2019;67, Suppl S1:25-6\nSocolovsky et al.;, have provided a comprehensive review detailing the pathophysiological mechanisms and management strategies for neuropathic pain.\nBrachial plexus and peripheral nerve injuries are most prevalent in the young working population, forming a significant cause of disability and decreasing the quality of life. The early initiation of physiotherapy following injury plays a large role in recovery, relief from pain and prevention of complications such as formation of contractures and autonomic dysfunction. Furthermore, physiotherapy might have a role in reducing the extent of cortical reorganization in the deafferented motor area by maintaining some degree of afferent input, thus relieving phantom limb pain. The authors could have stressed on the importance of physiotherapy as a first line measure in pain management.\nThe surgical management of brachial plexus injuries has also seen considerable technological advancements over the last decade. While the selection of treatment strategy is based on the nature and severity of injury, a direct nerve repair is preferred wherever possible. Autografts play an important role when the nerve ends are separated by greater than 30 mm (because decellularized allografts remain unproven in such cases). Smaller nerve gaps may be managed with tubulisation techniques or allografts. Nerve transfers have now become the standard for proximal injuries owing to their capacity to reduce the regeneration time, thereby improving the speed of end-organ re-innervation and outcomes., The use of epineurial repair and fibrin glue for achieving better coaptation has also led to better results. In the cases resistant to other treatment measures, the dorsal route entry zone (DREZ) lesioning has provided a good alternative. Accelerated peripheral nerve repair using stem cells is also under research. A detailed discussion on the surgical options and techniques would have brought these issues to the fore.\nWith the relative plateauing of surgical results, research on central mechanisms and neuromodulation has gained momentum. The authors provide a good overview of the experience with spinal cord and cortical stimulation. Recent research suggests that a major component of pain can be attributed to the ingrowth of sensory representation into the deafferented motor area as a result of plasticity. Resting state functional magnetic resonance imaging fMRI studies have shown widespread disturbances in neural networks following brachial plexus injuries. These changes involve not just the sensorimotor network, but higher networks such as the default mode network as well. With increasing evidence and elucidation of these mechanisms, the role of neuromodulation in pain relief can only be expected to increase.\nTo conclude, the cause of pain following nerve injuries is multifactorial. The authors have, therefore, rightfully stressed on the role of multidisciplinary management. We congratulate them for providing a holistic overview of a frequently neglected, but increasingly common health issue.\n| » References|| |\nLovaglio AC, Socolovsky M, Di Masi G, Bonilla G. Treatment of neuropathic pain after peripheral nerve and brachial plexus traumatic injury. Neurol India 2019;67:S32-7.\nMidha R. Epidemiology of brachial plexus injuries in a multitrauma population. Neurosurgery. 1997;40:1182-9.\nBaruah S, Deepika A, Shukla D, Devi BI, Preethish-Kumar V, Sathyaprabha TN. Demonstration of autonomic dysfunction in traumatic brachial plexus injury using quantitative sudomotor axon reflex test: Preliminary results. Neurol India 2017;65:1317-21.\n] [Full text]\nLotze M, Grodd W, Birbaumer N, Erb M, Huse E, Flor H. Does use of a myoelectric prosthesis prevent cortical reorganization and phantom limb pain? Nat Neurosci 1999;2:501-2.\nKhuong HT, Midha R. Advances in nerve repair. Curr Neurol Neurosci Rep 2013;13:322.\nMoiyadi AV, Devi BI, Nair KP. Brachial plexus injuries: Outcome following neurotization with intercostal nerve. J Neurosurg 2007;107:308-13.\nSinha S, Khani M, Mansoori N, Midha R. Adult brachial plexus injuries: Surgical strategies and approaches. Neurol India 2016;64:289-96.\n] [Full text]\nBaruah S, Devi BI, Bhat DI, Shukla D. Drezotomy in the management of post brachial plexus injury neuropathic pain: Preliminary results. Indian J Neurotrauma 2014;11;27-9.\nGrochmal J, Midha R. Recent advances in stem cell-mediated peripheral nerve repair. Cells Tissues Organs 2014;200:13-22.\nMohanty CB, Bhat D, Indira Devi B. Role of central plasticity in the outcome of peripheral nerve regeneration. Neurosurgery 2015;77:418-23.\nBhat DI, Indira Devi B, Bharti K, Panda R. Cortical plasticity after brachial plexus injury and repair: A resting-state functional MRI study. Neurosurg Focus 2017;42:E14.']	['<urn:uuid:411f1cca-0c11-452e-897b-ab9dacf30398>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	9	46	847
90	main products from biomass conversion methods	High temperature pyrolysis primarily produces gas and biochar plus heat, while low temperature pyrolysis yields oil, gas, and biochar plus heat. In contrast, gasification produces gas and biochar plus heat, but also generates harmful dioxins and furans due to oxygen presence. The USDA's system has tested these processes on various biomass sources including switchgrass, corn stover, and other agricultural waste materials.	['|OSGOOD-JACOBS, LOGAN - Swarthmore College|\n|CARLSON, PEREGRINE - Swarthmore College|\n|MACKEN, NELSON - Swarthmore College|\nSubmitted to: Journal of Energy Resources Technology\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: 7/12/2012\nPublication Date: 10/19/2012\nCitation: Boateng, A.A., Mullen, C.A., Osgood-Jacobs, L., Carlson, P., Macken, N. 2012. Mass balance, energy and exergy analysis of bio-oil production by fast pyrolysis. Journal of Energy Resources Technology. 134/042001-1-9.\nInterpretive Summary: The U.S. Departments of Agriculture and Energy are committed to achieving the country’s energy security through the development of domestic renewable energy and advanced biofuels which will at the same time create opportunities for the farm and rural communities. Of the various biomass conversion technologies being studied, fast pyrolysis has received the farmers’ attention due to its small footprint and potential ease of deployment on-farm. At ARS, a bench scale unit has been developed to test this process on many sources of biomass, including dedicated energy crops (e.g. switchgrass), crop residues (e.g. corn stover, straw) and other waste materials. For the pilot scale system, small size, the high viscosity of bio-oil and the variability of feedstock make it difficult to achieve a balance of the material flows in and out. Therefore, a mathematical mass balance model was developed which can take data generated from test runs and determine how much of the carbon, hydrogen, oxygen and nitrogen in the biomass is ultimately found in each of the products from this process. It can also help researchers determine the amount of each material that is not collected because it is stuck in the system or escaped the system prior to collection. These losses will be minimized in scale up to a production system, and this model allows for prediction of product yields on a larger scale. Another model was then developed which takes the outcome of the mass balance model and data from testing on the products and determines the exergy efficiency of the system. Exergy is an advanced theory which is related to energy, but measures the “real world” work which can be done by a material. The exergy results for the system can be incorporated into a larger context where the process is part of a system where biomass is grown, converted to fuel and then consumed as fuel and be used to determine the sustainability of the entire system. This information will be useful to those developing and commercializing biofuel products via pyrolysis and those studying sustainable biofuels development.\nTechnical Abstract: Mass, energy and exergy balances are analyzed for bio-oil production in a bench scale fast pyrolysis system developed by the USDA’s Agricultural Research Service (ARS) for the processing of commodity crops to fuel intermediates. Because mass balance closure is difficult to achieve due, in part, to the system’s small size and complexity a linear programming optimization model is developed to improve closure of elemental balances without losing the overall representation of the pyrolysis products. The model results provide an opportunity to analyze true energy and exergy balances for the system. While energy comparisons are based on heating values, exergy flows are computed using statistical relationships and other standard techniques. Comparisons were made for a variety of biomass feedstocks including energy crops and various byproducts of agriculture and Bioenergy industry. The mass model allows for proper accounting of sources of mass loss and suggestions for improved system performance. Energy recovery and exergetic efficiency are compared for a variety of pyrolysis product utilization scenarios including use of biochar and non-condensable gases as heat sources. Exergetic efficiencies show high potential for energy utilization when all the pyrolysis product streams can be recycled to recuperate their internal energy. The exergy analysis can be beneficial to developing exergetic life cycle assessments (ELCA) for the fast pyrolysis process as sustainable technology for advanced biofuels production.', 'The gasification process is conduct similar to pyrolysis (high process temperatures close to 1000 °C, as in the case of high-temperature pyrolysis), thus often both processes are confused with each other. Gasification consists of a series of complex sequential chemical reactions and thermal decomposition reactions.\nThe main difference between the pyrolysis and gasification processes is the addition of a gasifying agent to the gasified waste. It may be oxygen or air, but also water vapor, carbon dioxide or simply exhaust gas. Gasification takes place in the presence of O2, which results in the formation of dioxins and furans harmful to health. The dioxins and furans formed with the participation of oxygen very strongly disturb the hormonal balance of the body, leading to the occurrence of many dangerous diseases.\nSome of the reactions occurring during the gasification process are exothermic (it does not require the supply of heat from the outside) and some of it is endothermic (requires the supply of heat from the outside). The gasification process is therefore more difficult to control, and its course depends on the gasifier used. Pyrolysis is an endothermic process. Gasification can also be seen as a continuation of the pyrolysis process, i.e. both processes may be successive stages, which increases the yield of energy gas. The gas generated in the gasification process is used directly for the production of electricity – burned in gas engines, or subjected to refining to the synthesis gas used in the synthesis of chemicals, including synthetic fuels. The calorific value of gas obtained as a result of gasification depends on the type of oxidizing agent and ranges from 5 MJ/m3 (for air and water vapor) to 10 MJ/m3 (for pure oxygen). For pyrolysis the calorific value of the obtained gas is from about 15 MJ/m3 even to over 30 MJ/m3 (depending on the substrate).\nThe main difference between the pyrolysis and gasification processes is the presence of an oxidizing agent in the process in the case of gasification and the calorific value of the gas obtained – higher calorific value in the case of pyrolysis.\nRys. Fig. Comparison of pyrolysis, gasification and incineration processes\nThe resulting pyrolysis and gasification processes:\n- High temperature pyrolysis (for example EL PIRO):\nIn this process, the main products are gas and biochar, heat is also generated\n- Low temperature pyrolysis\nIn this process, the main products are oil, gas and biochar, heat is also generated\nIn this process, the main products are gas and biochar, heat is also generated, and due to the access of oxygen in the process – dioxins and furans\nIntrocuction to biorafineries and biofuels Assignment 8: comparison of gasification, pyrolysis and combustion, Aalto University School of Chemical Technology 2013\nComparative assessment of municipal sewage sludge incineration, gasification and pyrolysis for a sustainable sludge to energy management in Greece, M.C. Samolada, A.A. Zabaniotou, Waste Management, 2014\nLife cycle assessment of pyrolysis, gasification and inceneration waste-to-energy technologies: Theoretical analysis and case study of commercial plants, J. Dong, Y. Tang, A. Nzihou, Y. Chi, E. Weiss-Hortala, M. Ni, Science of the Total Environment, 2018']	['<urn:uuid:51439f12-dbd8-443f-97b1-e637ddac2d92>', '<urn:uuid:e52020b5-af4c-4293-9ca7-7b48095de205>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	6	61	1141
91	What caused Rome's fall and how was its legacy preserved?	Rome fell due to several military factors, including the loss of around 50,000 soldiers (half its fighting strength) during Honorius's reign and the inability to maintain the Northern frontier, which led to increased barbarian migrations. The Visigoths under Alaric ultimately sacked Rome in 410 AD, dealing both a physical and symbolic blow to Roman power. However, Rome's legacy endured through the occupation of Roman-founded cities, elements of its political system and philosophy in modern Western culture, and its role in spreading Christianity as a westernized religion.	['After Arcadius reduced the Visigoths’ stipend, angry calls for revenge went up among the barbarians. To reaffirm the Goths’ favored position within the Empire, Alaric felt the need to lead his people in a new war against the same legions they had fought alongside at Aquileia. Throughout their ceded homeland of Moesia, northwest of Constantinople, Goth farmer-warriors cast aside their plows and took up their spears once more.\nFrom the trackless forests north of the Danube, other fierce, bearded barbarians heeded Alaric’s call. They rolled their ponderous war wagons across the broad and icy Danube. Thus reinforced, the Visigoths swept unopposed through Macedonia and Thrace until they approached the outskirts of Constantinople. Teenage Emperor Arcadius possessed neither the courage nor the troops to meet the Goths in open battle. Even so, Alaric soon realized that the city’s lofty walls were beyond his means to conquer. Instead, he turned west toward Thessaly, where Flavius Stilicho, now the supreme commander of the western Roman armies, waited to confront him. Alaric knew that Stilicho was not a man to be trifled with. The son of a Vandal cavalry officer and a Roman mother, Stilicho had worked his way up to become the most powerful man in the Empire. He had with him not only the crack troops of the West Roman army, but the Eastern Roman field army as well.\nBehind a hastily built stockade at Thessalonika, Alaric awaited an assault by Stilicho’s larger army. The attack never came. Seeds of distrust had sprouted between the two Imperial courts. The self-serving court advisers who held Aracadius’s ear were worried that Stilicho, who already had complete control over Honorius, would extend his influence over the Eastern Empire as well. Arcadius ordered Stilicho to send the eastern army to Constantinople and return to Italy immediately. A baffled Alaric watched as Stilicho’s army suddenly broke camp and departed the field. The Visigoths marched unopposed down the historically fateful path to Thermopylae. Attica’s villages went up in flames, their men succumbing to Goth spears and their women becoming the conquerors’ spoils of war. Alaric spared Athens in return for most of its treasures. After a triumphant entrance, he relaxed with a warm bath and attended a banquet in his honor. From Attica, the Goths moved down the Peloponnese peninsula, where Corinth, Argos and Sparta yielded their treasures as well.\nStilicho Strikes Back\nIn the spring of AD 397, the sails of a massive armada billowed above the azure waves of the Ionian Sea. Stilicho was returning with a powerful force to put an end to Alaric’s outrages. With his greater army Stilicho drove Alaric and his Goths up the arid slopes of Mount Pholoe, where the beleaguered barbarians hunkered down for a last stand. With hunger gnawing at their ribs and thirst parching their throats, defeat seemed inevitable.\nWith victory all but won, Stilicho took time to enjoy the theaters and dancers of Greece. Unfortunately, so did many of his troops, who made themselves a nuisance in the countryside. Alaric saw his chance and broke through the depleted encirclement, transporting his troops, captives, and loot to the Epirus coast of northwestern Greece.\nThe ease of Alaric’s escape stirred rumors that Stilicho had purposely let Alaric go so that he could later use him against the Eastern Empire. At the same time, the Eastern Empire was hoping to use Alaric against Stilicho. Even though he had wreaked havoc over much of the Eastern Empire’s domain, Alaric was awarded the post of Master of Soldiers and the Prefecture of Illyricum. The fox was now guarding the hen house.\nIllyricum was of no small importance to the Empire. The Prefecture covered all the Greek and Balkan provinces outside of Thrace. Both the eastern and western courts desired jurisdiction over Illyricum. Alaric had attained his original goals, and his people were pleased. In the time-honored custom of the barbarians, they lifted him high on a shield and enthusiastically proclaimed him their king.\nFor the next four years there was peace in Illyricum, but Alaric’s Visigoths were not the only barbarians stirring up trouble within the Empire. An independent contingent of Goth mercenaries at Constantinople seized control of the city. They were quickly overthrown, and 7,000 of them were massacred inside the city. The resultant anti-German sentiment at Constantinople turned against Alaric and his Goths, who were consequently stripped of their titles and lands.\nIt was time for the Visigoths to load up their wagons, mount their horses, and make the Empire take notice of them again. The west looked enticingly vulnerable at the moment. Vandals and Alans led by an eastern Goth warlord named Radagaisus, were advancing from Pannonia, and Stilicho was even now moving against them. With Stilicho squaring off against Radagaisus, Alaric’s Goths tramped into unprotected Italy in November AD 401. The invasion sent shock waves of terror through the Roman Empire. Most of the towns of Venetia fell to Alaric. Honorius quivered in fear in Milan, the seat of emperors since the end of the 3rd century AD, praying for Stilicho’s imminent return. In the midst of winter, Stilicho smashed Radagaisus’s invasion. Radagaisus escaped, but otherwise the haul of captives was so great that the price of slaves plunged on the open market. Stilicho then led his battle-hardened veterans through the snow and ice of the Alps. With them marched 12,000 of the defeated Vandals and Alans, who had been drafted into the Roman army. Stilicho was taking no chances against Alaric. Far away, along the forts of the Rhine and the Caledonian border, his messengers raced to summon all available aid against the Visigoths.\nBy the time Stilicho returned in February 402, Alaric had pillaged northern Italy for three months. At the Imperial palace of Milan, the palace attendants cried that Alaric’s cavalry was already riding through the suburbs. Terrified, Honorius fled Milan. Nearly captured by the Goth horsemen, he found shelter in the walled Ligurian town of Asta on the River Tanarus. As the footfalls of Stilicho’s army drew near, Alaric conferred with his long-haired chieftains. He looked at faces scared from battle, at old warriors leaning on their spears. One of the eldest had given Alaric his first bow and quiver. The grizzled veteran advised Alaric to escape from the spreading Roman net while he still had time. Unabashed, Alaric retorted that “witless age” had deprived the old man of his senses. “This land shall be mine whether I hold it in fee as a conqueror or in death as conquered,”1 he declared. With those defiant words, he decided to pursue the emperor to Asta.\nThe Easter Surprise Attack\nAt Milan, the lookouts on the walls could not tell if the dust cloud raised by Stilicho’s approaching army heralded friend or foe. When Stilicho’s helmet, glittering like a star, was recognized, the population broke out in relieved jubilation. Stilicho did not tarry in the city for long before pressing on after Alaric. He found the Visigoths encamped near Pollentia, having put Asta under siege. The Roman forces enjoyed a 2-to-1 superiority over Alaric’s estimated 20,000 warriors. Despite such odds, however, Stilicho was too prudent to waste his men in an assault on the Goth wagon barricades. Instead, he enclosed the Goths within a line of fortifications and waited until April 6, Easter Sunday, when Alaric and his men, who were Christians of the Aryan faith, devoutly celebrated the festival. Riding along the ranks of his soldiers, Stilicho shouted: “Friends of Rome, the time has come for you to exact vengeance for outraged Italy! Protect Father Tiber with your shields!”2\nAfter receiving the treasures and hostages, Alaric kept his word and lifted the siege, but he did not leave Italy itself. Instead, he settled into Etruria for the rest of the winter. Alaric had done well—not only were the Goths sitting on a pile of booty, but their ranks were swelled with the newly freed barbarian slaves. More reinforcements were on the way as well. Athaulf, Alaric’s brother-in-law, led a body of Goths and Huns across the Danube and cut his way through leagues of Imperial territory. Near Pisae, however, 300 Huns in Honorius’s service sprang upon Athaulf’s columns. Firing from their swift ponies, the compound bows of the Huns took a deadly toll. Before Athaulf’s men could gather their greater numbers and drive off the raiders, 1,100 Goths had been killed or wounded.\nBack in Italy, Alaric was ready to settle for being named Master of Soldiers of the impoverished frontier province of Noricum. This was a lesser rank than he had held in Illyricum, but Alaric desperately needed to provide food and shelter for the multitudes of Goths under his care. Meanwhile, Honorius faced another near-mutiny in Ravenna. Olympius, the newest scapegoat for the Empire’s woes, was dismissed and forced to flee Italy. The troops, however, did not quiet down until several other high-ranking commanders were exiled and subsequently killed by their guards.\nHonorius’ handling of the latest crisis must have raised his self-confidence for he refused Alaric’s demand for Master of Soldiers. When it was suggested that Alaric might soften his already reasonable territorial demands if given Stilicho’s old title, Honorius scoffed that “such an honor should never be held by Alaric, or any of his relatives.”7\nTo protect Rome from Alaric’s anticipated reprisal, Honorius released the flower of the legions from Ravenna. Some 6,000 Roman soldiers were heading toward Rome. Apparently, their commander, Valens, thought it was beneath his dignity to sneak around Alaric’s detachments. Alaric prepared an ambush and completely overwhelmed Valens’s puny army. Only a hundred legionaries ever reached Rome.\nThe Sacking of Rome\nAt the end of 409, Alaric, now reinforced by Athaulf, marched on Rome with 40,000 Goth, Vandal, Alan, and Hun warriors. This time Alaric occupied the huge Port of Ostia, with its massive wave-breaking moles, deep, capacious basins, and numerous outbuildings. It was here that the great grain shipments from the Province of Africa were stored. Their food supply assured, the Goths welcomed the shelter from the violent rainstorms that marked the Italian winter.\nProclaiming that his enemy was not Rome but Honorius, Alaric threatened to cut off Rome’s grain supplies unless the Senate elected a new emperor. Honorius having been of no help to them, the Senate obliged by placing the crown and purple on the city prefect, a Greek named Priscus Attalus. The fickle mob greeted the Senate’s choice with jubilation, not just in Rome but in Milan as well. Attalus boasted that he “would leave Honorius not even the name of Emperor nor yet a sound body, but would mutilate him and exile him to an island.”8 Confident of his new ally, Alaric marched immediately to besiege Ravenna.\nWith Attalus in Rome and Alaric’s army outside his gates, Honorius was deeply worried. He was about to flee to Constantinople when six legions, some 4,000 men, arrived from the eastern capital. With the reinforcements manning the parapets and towers, Honorius felt confident enough to remain holed up in Ravenna. He had another ally as well. Count Heraclian of Africa closed his ports. No more ships laden with grain sailed into Ostia. Whatever grain was left in the magazines the Goths used for themselves. Famine again afflicted the Romans.\nAlaric broke off his investment of Ravenna and reduced most of the cities of Aemilia, which had refused Attalus’ rule. In this he received no help from Attalus, who seemed capable only of conducting fruitless negotiations with Honorius and Heraclian. Alaric soon had enough and summoned Attalus to Ariminum to publicly strip him of the purple. Alaric decided to have another try at negotiating with Honorius, whom he met in July 410, a few miles from Ravenna.\nAlso in Ravenna was Sarus, Alaric’s old enemy. He did not wish to see peace between Alaric and the emperor. Yelling and waving their weapons, Sarus and his Goths tore through Alaric’s camp before hightailing it back to the safety of Ravenna’s bastions. Convinced that Honorius and Sarus were working together, Alaric angrily broke off talks and marched on Rome for the third time. This time he was in no mood for mercy.\nOnce more the barbarians were at the gates, blockading Rome and starving its hapless population. The Goth slaves and servants inside the city, asked themselves why they should suffer for their Roman masters. At midnight on the night of August 24, 410, a group of them stole to the Salarian Gate and opened it to their erstwhile kinsmen.\nThe citizens of Rome awoke to the sound of the Goth trumpets—the enemy was inside the city. The barbarians stormed through the streets, sacking the city for three nightmarish days. The palace of the historian Sallust was burned to the ground, and the aristocratic houses along the Aventine went up in flames. The worst offenders were the Huns who served in Alaric’s army. Rich furniture was thrown out of windows, silk hangings were torn from the walls, jeweled flourishes were pried out of statues. Wealthy Romans were repeatedly pummeled and kicked until they revealed hidden treasures. At last the conquerors filed out of the wasted city, laden with booty and followed by throngs of captives. Among the latter was the stunningly beautiful sister of Honorius, Galla Placidia, who remained in comfortable captivity with her childhood friend Alaric.\nThe Legacy of Alaric\nAlaric did not survive the sack of Rome for long. He led his people south, planning to sail for Africa. At Cosenza, however, the Visigoth ruler was struck by a violent fever that killed him within a matter of days. Mourned by his people, Alaric was only 40 years old when he died. His bereaved followers dammed the Busento River, and slaves buried Alaric’s body alongside a trove of treasure in the dry stream bed. To insure that no one knew the exact burial place, all the slaves were killed. The dam was then broken and the waters of the Busento washed over Alaric’s grave. Alaric died, but his people lived on. Athaulf married Galla Placidia and became the new king of the Visigoths. Under his leadership and those of succeeding kings, the Visigoths eventually settled in Spain. Their rule lasted until the Islamic invasion of the early 8th century.\nHonorius continued to preside over his fragmented Empire until his death in AD 423. Although history has judged him a weak ruler, he managed to stay alive and remain in power in a political atmosphere that was more reminiscent of gang warfare than organized government. During his reign, however, Roman military losses were enormous. At least 50,000 soldiers of the field army, half its fighting strength, were killed. It was the beginning of many years of woe. In 455, the Vandals laid waste to Rome again. Twenty-two years later, a barbarian king named Odoacer disposed of the last of the Roman emperors, Romulus Augustulus, bringing to an end over eight centuries of Roman domination of the western world.\n“Barbarians at the Gate: the 410 Sack of Rome” was first published in Military Heritage Magazine, October 2005 and republished on January 20, 2017, online at Warfare History Network. The above version includes additional editing by the author, L.H. Dyck and images sourced from the net for educational purposes only.\nBill Thayer, Lacus Curtius, http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Claudian, Boak and Sinnegin in Nardo Don. Editor. The End of Ancient Rome. San Diago: Greenhaven Press. Inc. 2001, Bunson Matthew. A Dictionary of the Roman Empire. New York: Oxford University. 1991, Bury.J.B. The Invasion of Europe by the Barbarians. New York: The Norton Library. 1967, Cary M. and Scullard H.H. A History of Rome. London: MacMillan Education.1988, Gibbon Edward. The History of the Decline and Fall of the Roman Empire. Vol.III. London: Methuen & Co. 1909, Grant Michael. The Fall of the Roman Empire. New York: Colllier Books.1990, Heather P.J. Goths and Romans 332-489. Oxford: Clarendon Press. 1991, Jones A.H.M. The Later Roman Empire. Oxford : B. Blackwell, 1973, MacDowall Simon and Embelton Gerry. Late Roman Infantryman 236-565 AD. Osprey Publishing: Oxford. 1999, MacDowall Simon and Hook Christa. Late Roman Cavalryman 236-565 AD. Osprey Publishing: Oxford. 1999, MacDowall Simon and McBride Angus. Germanic Warrior 236 AD-568 AD. Osprey Publishing. Oxford. 1996, Malcolm Todd. Everyday Life of the Barbarians. New York: Dorset Press. 1972, Mannix Daniel P. The Way of the Gladiator. New York: ibooks. 2001, Norwich John Julius. Byzantium The Early Centuries. London: Penguin Books.1990, Orosius Paulus. The Seven Books of History against the Pagans. Translated by Deferrari Roy. J. Washington: The Catholic University of America Press. 1961, Twine Kevin, The City in Decline Rome in Late Antiquity. Middle States Geographer Vol. 25, 1992, Montclair. Montclair State College, Wolfram Herwig. History of the Goths. Berkeley: University of California Press, c1988.', 'What do a bloody battle outside the gates of an impregnable imperial city, the destruction of the city that once led an Empire and the abdication of a sixteen-year-old Emperor all have in common? These events aligned to produce the downfall of the Western Roman Empire, in tandem with centuries of political disputes and chaos. Many scholars have been unable to identify the exact moment the Roman Empire ended. Nonetheless, the fall of the Western Empire in the late fourth and fifth century catalyzed the beginning of the end of Rome as an entity of global power. Despite the survival of Constantinople, the fall of Rome and the disintegration of Roman rule in the West signified the end of the Roman Empire as a political structure and its sovereign influence.\nOpposition to the mighty Roman Army began to impact the Western Empire’s capacity to succeed in military conquest. Since the third century, emperors had been recruited based on their ability as military generals and were often found directly leading campaigns, contrasting the politicized role of the Emperor in the principate. Arguably, Rome’s biggest struggle was its potential to maintain the Northern frontier, leading to the rise of barbarian nomads and an increased influx of migrants into the Empire. One particular group of barbarians, the Goths, came very close to capturing the newly established capital of the Empire, Constantinople. Occurring in the Battle of Adrianople in 378, the conflict also led to the death of the Emperor Valens and emphasised the strength of barbarian armies and their qualities as violent and brutish war lords. This signified a change in military tactics as emperors no longer led armies into battle and instead, as described by Stephen Mitchell, allowed ‘their courts became sedentary’.\nAt the beginning of the fifth century, the Goths branched out into factions and the Visigoths, led by Alaric, turned their attention to the Western half of the Empire. Alaric and his Visigothic army continually threatened to invade Italy and demanded to be offered monetary compensation for his tribe to establish resources. These attempts throughout the fifth century AD put the whole Empire on edge. When no money was sent, Alaric besieged Rome in 408, leading to peaceful negotiations being set up to counteract his blockade. However, leading generals had sworn to never strike a deal with Alaric and negotiations disintegrated rapidly. The anxiety regarding capture proved justified on the 24 August, 410 when the city of Rome fell. Jerome – a fifth century historian- explicitly described how ‘the City that had once [conquered] the entire world was captured’. Not only was this a physical blow to the strength of the Roman Empire and its military strength but was also a symbolic one and the wealth of the city alongside its reputation had been demolished.\nEventually, the West remained political unstable and, as chaotic and unsuitable leadership dominated the later fifth century, the child emperor Romulus Augustulus abdicated in 476AD. Not only was this illustrative of the potential disarray caused by the appointment of child emperors across the fifth century – Theodosius II was raised to the role of co-Emperor a year after his birth- but also led to a political crisis. The Western capital had relocated to Ravenna because of the continued attacks happening in Italy. As Odoacer established himself as King of Italy, the Roman influence in the West was at a minimal level. The Emperor Zeno decided to permit Theoderic and the Ostrogoth to establish a new dynasty in Italy. Theodoric usurped Odoacer and maintained Roman values but marked his succession as a separate entity from the Empire.\nAttempts to re-conquer the Western Half of the Empire made by the sixth-century Emperor Justinian, although successful, did not last. Despite this, the Eastern half of the Empire continued to thrive into the Middle Ages and Constantinople remained the imperial capital of a declining Empire. Commonly referred to as the Byzantine Empire, any traces of Roman rule ceased to exist following the Fall of Constantinople in 1451. Despite its destruction, the Roman Empire has had huge influence on modern times. Not only do we now occupy cities founded by Romans, elements of the Empire’s political system and philosophy are ingrained in modern Western culture and Rome encouraged the spread of Christianity as a westernized religion. The empire may have been brought to its knees by the might of the barbarian hordes, but the legacy of imperial Rome has withstood the test of time.']	['<urn:uuid:62985dc5-27d6-4460-b3c2-13da537b22ba>', '<urn:uuid:9f81d2e0-62ff-4431-8726-3a937da470ad>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	10	86	3530
92	compare temperature conditions operation process granulated refining flux aluminum melt vs high emissivity coating steel reheating	Granulated Refining Flux operates at relatively lower temperatures, with a low melting point of 480°C and is used when aluminum liquid temperature is 700-750°C, with a refining time of 15-20 minutes. In contrast, high emissivity coating in steel reheating furnaces operates at much higher temperatures, handling steel billets that are heated from charge temperatures of 62-80°C to discharge temperatures of around 1,226-1,245°C. The coating material itself must withstand extreme conditions, demonstrating thermal shock resistance up to 1,600°C for at least 12 cycles. This significant temperature difference reflects the distinct metallurgical processes and requirements of aluminum refining versus steel reheating.	['Granulated Refining Flux improves the quality of aluminum alloy ingots, and fully removes alkali metals, oxidized slag in the aluminum melt. Granulated Refining Flux plays a vital role in removing impurities from aluminum melt.\nGranulated Refining Flux product advantages\n1. The emissions of hydrogen chloride and dust are lower than the environmental protection standards set by the EPA for harmful air pollutants.\n2. Due to its low melting point (480°C), it decomposes extremely quickly after being added to the alloy melt.\n3. Granular melting products are smoke-free and dust-free when they are put into use, which improves the production environment of employees and protects their health.\n4. It is beneficial to increase the extrusion speed and extend the life of the die during the production process of the profile. The data used in industrial production proves that the extrusion speed of the profile can be increased by about 20%.\n5. When rolling high magnesium alloy, eliminate edge cracks.\n6. Smokeless, tasteless, dust-free, without any harmful compounds, reducing corrosion to plant equipment and reducing hidden costs.\n7. Keep the furnace, launder, and online degassing equipment free of slag accumulation.\n8. Significantly reduce the load of the flue gas dust removal equipment and reduce the operating cost of environmental protection equipment.\n9. No sodium, and effectively remove the electrolyte in the electrolytic aluminum water, the removal efficiency of sodium, calcium, lithium and other alkali metals can reach more than 70%. Eliminate the sodium brittleness during the processing, increase the total throughput of the filter plate, and reduce the filtration cost.\nGranulated Refining Flux use process\nAdd the granular refining flux into the powder spraying refining tank as required, close the lid, turn on the refining tank switch when the aluminum liquid temperature is 700-750℃, and pass in nitrogen or argon (required nitrogen pressure is 1.2-1.8Kg /cm2). When the refining agent is sprayed from the refining iron pipe, the refining iron pipe can be inserted into the molten pool and moved slowly and evenly. The high-purity nitrogen or argon gas will spray the granular refining agent into the aluminum melt regularly and quantitatively. Under the combined action of bubbles and granular molten flux, the oxidized inclusions and hydrogen in the aluminum liquid are removed, and the refining time is 15-20 minutes each time. When the molten flux in the tank is completely sprayed out and the time to fill with nitrogen reaches 15-20 minutes, draw out the refining tube and turn off the nitrogen or argon.\nPowder spraying and refining of continuous aluminum melting furnace\n1. Generally, continuous aluminum melting furnace refining every hour or every 2 hours, calculate the amount of particle flux that needs to be added according to the amount of molten aluminum in the interval, and spray it into the bottom of the melt regularly and quantitatively. The requirements for powder spraying refining parameters are the same as above.\n2. The dosage is 0.6-3Kg/ton AL. If it is only used for refining, the dosage can be 0.6-1.2Kg/ton aluminum, and 1.5-3Kg/ton aluminum for alkali metal removal (to be determined according to the original content of alkali metal sodium, calcium and lithium).', 'HIGH EMISSIVITY COATING FOR ENERGY SAVING IN INDUSTRIAL FURNACES: CASE STUDY OF STEEL REHEATING FURNACE\nDate : 09 Oct 2017\nBOOK : SOUTH EAST ASIA IRON AND STEEL INSTITUTE quarterly, 2015 vol. 44 no.1\nMore than 90% of energy liberated in steel reheating furnaces is transferred by radiation. Considering the scale and the importance of the steel reheating furnaces, an improvement of radiation heat transfer can lead to remarkable increase in the production yields or decrease in the fuel gas consumption.\nThe high emissivity coating was developed to maximize thermal efficiency of reheating furnaces by improving radiation absorptivity and emissivity of inner walls that affected on reduction of radiation in band of gas. While increase amount of additional radiation passes directly to billet inside the furnace.\nThe high emissivity coating material was successfully applied in steel reheating furnace that presented the appreciate reduction of fuel gas consumption. The application of high emissivity coating resulted on decrease of fuel gas consumption up to 6.5%.\nKeywords: High emissivity coating, Steel reheating furnace, Hot rolling mill, Energy conservation and Fuel saving\nEnergy problem is one of the most important issues seeming endless and continually affects all human beings directly. The price of crude oil and other energy sources in the global market has been increased from time to time . In 2008, increasing demand for energy and rising energy costs throughout the world has inevitably increased the need for energy users to save or conserve energy, particularly among industrial entities. In many instances, industries that use fired heaters or furnaces have attempted to maximize furnaces efficiency to thereby reduce fuel consumption.\nFired heaters or furnaces are versatile heat transfer equipment in several industries such as refineries, petrochemicals, ceramic production and steelmaking . The iron and steel industry is one of the largest energy-consuming manufacturing industries. Energy is consumed in steel production during the several stages in production process; raw materials preparation, iron making, steel making, finishing and heat treatment. Finishing process includes the operation of reheating the slabs, blooms, and billets and transforming them through hot and cold rolling steps into final products. Reheating furnaces are used to heat steel shapes to desired temperatures and suitable for plastic deformation in the rolling processes .\nReheating is a continuous process where the stock is charged at the furnace entrance, heated, and discharged. Energy is transferred to the items during their traverse through the furnace by means of convection and radiation from the hot burner gases and the furnace walls .\nDue to high temperature of the combustion gases and the furnace walls, radiation is the predominant mode of heat transfer in the reheating furnace. Considering the scale of the reheating furnace in steelmaking process, even a small rise in thermal efficiency can be transferred into an important increase in the production yields or an important decrease in the required fuel input.\nThis work aimed to study the influence of application of high emissivity coating in reheating furnace. The results on thermal efficiency and fuel gas consumption in hot rolling mill process were obtained.\n2.High emissivity coating\nMore than 90% of energy liberated in steel reheating furnace is transferred by thermal radiation. The application of high emissivity coatings in reheating furnaces is believed to improve the heat transfer from furnace to steel billets to effectively increase radiant heat transfer and save energy without compromising process reliability and safe operation. Higher emissivity corresponds to an increase in thermal efficiency. An increase in thermal efficiency attributed to high emissivity coatings in high temperature fired heater or furnace applications results in an increase in the furnace performance or output and/or a decrease in fuel consumption and overall energy demand. The benefits and advantages of high emissivity coatings have led to various research and development efforts over the years to improve the performance of high emissivity coatings .\nHigh emissivity coating material developed to maximize furnace thermal efficiency to reduce fuel consumption in various industrial furnaces. High emissivity coating material contains materials capable of absorbing and re-radiating thermal energy and is designed to provide high coating surface strength, higher abrasion resistance, good thermal shock resistance, thermal expansion characteristics similar to their intended substrates, and adequate bond strength with the substrate .\nIn general, High emissivity coating maximizes and stabilizes the emissivity over varying process temperatures, thus promoting rapid and efficient heat transfer, uniform heating and extended refractory life.\nMost of high emissivity coatings contain materials capable of absorbing and re-radiating thermal energy . Nevertheless, it still needs to be improved on coating adhesion properties for long service life over operating condition with several thermal shock cycles. There are some chemicals or substances that serve as binders or binding agents to promote a bonding between a coating composition and a substrate on which the coating composition is applied. Therefore, the formulation of water based coating for good adhesion, good thermal strength, high emissivity and less toxicity coating formulation was developed. The coating material was supply by Texplore Co., Ltd. Table 1 presents the general properties of high emissivity coating.\nTable 1 General performance properties\n|Emissivity||Pyrometry||0.80 - 0.95|\n|Surface strength||ASTM C1624-05 (2010)||> 20 N|\n|Adhesion pressure||ASTM D3359-09||> 1000 psi|\n|Thermal shock resistivity||Alternate heat/cold (Ambient-1,600 °C)||12 Cycles|\n3.2 Coating application\nTo ensure the thermal efficiency of high emissivity coating, the appropriate application procedures needs to be developed for each coating material.\nSurface preparation is the essential first stage treatment of a substrate before the application of high emissivity coating. The service life of high emissivity coating is significantly influenced by its ability to adhere properly to the substrate materials. When repairing an existing brick or refractory materials, make sure that removal all deteriorated refractory and dust. Dust should be caused of delamination of coating layer.\nThe spray coating of high emissivity coating is performed using spray gun.\nDry out and furnace startup\nCoating material needs several hours at ambient condition for drying before startup of furnace.\n3.3 Implementation in steel reheating furnace\nThe coating material was applied in steel reheating furnace in hot rolling mill process with production capacity of 400,000 Ton annually. The billet size of 150x150 mm was used as reference size for energy saving evaluation.\n4.Results and Discussion\nTable 2 and 3 presents the comparison of operating and energy parameters in conditions of before and after high emissivity coating application, it was found that the reheating still maintained production capacity with slightly increase of production rate from 134 to 139 Ton/hr while the fuel gas consumption was decrease from 4,761 to 4,626 M3/hr and specific energy consumption was decrease from 1,185 to 1,109 MJ/Ton of billet or saving of 6.5 % energy consumption.\nThe effect of high emissivity coating material on furnace wall on the furnace thermal efficiency should be explained on the basis of the radiation. Firstly, the application of high emissivity coating on a furnace wall implies that the amount of radiation energy that is reflected by the wall decreases. Secondly, the amount of radiation energy that is absorbed by the wall increases. As the furnace walls are insulated and the heat loss to the environment through the wall is small, more of this energy is re-radiated back in the furnace. Unlike the reflected energy that preserves its spectral character, the re-radiated energy is redistributed over the entire wavelength spectrum. Furthermore, only small part of the absorbed radiation is re-radiated within the range of absorption bands of the gas. Thus, in the event of a coated furnace, an additional amount of radiation passes directly to the targeting loads or billets with less amount of radiation being absorbed by the gas medium  that resulted on decrease of exhaust gas temperature from 704 to 692 °C.\nTable 2 Comparison of operating parameters in conditions of before and after high emissivity coating application\n|Parameter||Before coating||After coating|\n|Production rate (Ton/hr)||134||139|\n|Charge temperature (°C)||80||62|\n|Discharge temperature (°C)||1,245||1,226|\n|Temperature difference (°C)||1,165||1,164|\n|Cooling water flow rate (M3/hr)||490||494|\n|Inlet temperature (°C)||30||30|\n|Outlet temperature (°C)||38||38|\nTable 3 Comparison of energy parameters in conditions of before and after high emissivity coating application\n|Parameter||Before coating||After coating|\n|Fuel gas consumption (M3/hr)||4,761||4,626|\n|Heating value (BTU/ft3)||894.3||894.3|\n|Exhaust temperature (°C)||704||692|\n|Specific energy consumption (MJ/Ton)||1,186||1,109|\n|Energy saving (%)||-||6.5|\nTable 4 is presented the energy distribution in reheating furnace in condition of before and after high emissivity coating application and the effectiveness of using high emissivity coating materials that could be confirmed by the increasing of billet heating from 43.6 to 49.6 % and decreasing of loss from exhaust gas from 40.2 to 34.0 %.\nTable 4 Energy utilization in conditions of before and after high emissivity coating application\n|Segment||Before coating||After coating|\n|Billet heating||43.6 %||49.6 %|\n|Exhaust gas loss||40.2 %||34.0 %|\n|Wall loss||5.6 %||4.5 %|\n|Cooling loss||9.2 %||8.2 %|\n|Scale||0.3 %||0.4 %|\n|Other||1.1 %||3.3 %|\nHigh emissivity coating was successfully implementation in steel reheating furnace that resulted on reduction of energy consumption, improving furnace thermal efficiency and increasing production yields by maximizing thermal radiation capability. The utilization of high emissivity coating material is steel reheating furnace is not only solution for competitive advantage in steelmaking industry. Furthermore, it is the way for being eco-friendly of iron and steel industry to sustain business growth with care of environment.\n International Energy Agency. 2007. Tracking Industrial Energy Efficiency and CO2 Emissions, Paris, France.\n Heynderickx, G.J .and Nozawa, M. 2004. High-emissivity coatings on reactor tubes and furnace walls in steam cracking furnaces. Chemical Engineering Science, 59: 5657‐5662.\n Martenson, A. Energy Efficiency Improvement by Measurement and Control: A case study of reheating furnaces in the steel industry. In Proceedings from the 14th National Industrial Energy Technology Conference, Houston, Texas, USA, April 22-23, 1992.\n Dan Z et al. 2008 High emissivity coatings for high temperature application: Progress and prospect. Journal of University of Science and Technology Beijing, Mineral, Metallurgy, Material, 15: 627‐632.\nCarburization inspection robots for radiant tubes inside ethylene cracking furnaces\nDate : 01 Aug 2017\nBOOK : HP-Hydrocarbonprocessing']	['<urn:uuid:1e796abd-2440-478a-bc98-b3539a9f00ea>', '<urn:uuid:2846a276-8f80-4c82-bc23-2ee3728208c8>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T01:37:09.743768	16	99	2161
93	I'm curious about California's last volcanic eruption. What happened?	The Lassen Peak eruption in 1914-17 produced steam blasts for a year, then a larger explosion with an eruption column 30,000 feet high. Ash drifted 275 miles eastward to Elko, Nevada, and hazardous conditions lasted for several years afterwards.	['Not Only San Andreas — California’s Next Big Hazard Could Be A VOLCANO, USGS Warns\nThe U.S. Geological Survey (USGS) has warned that California’s next big hazard could come in the form of a volcanic eruption—not a massive earthquake, landslide or wildfire, which the state is often known for.\nIn a report released Monday, the USGS said that at least 10 volcanic eruptions have taken place in the past 1,000 years and that “future volcanic eruptions are inevitable.”\nEstimating the risk based on the past 5,000 years of volcanic activity, the report found there is a 16 percent chance of a small to moderate-sized eruption over the next 30 years. By comparison, there is a 22 percent chance of a magnitude 6.7 or larger earthquake at the San Andreas Fault in the San Francisco Bay Area in the next 25 years.\n“The potential for damaging earthquakes, landslides, floods, tsunamis, and wildfires is widely recognized in California,” the report said. “The same cannot be said for volcanic eruptions, despite the fact that they occur in the state about as frequently as the largest earthquakes on the San Andreas Fault.”\nAccording to the USGS, there are eight young volcanic areas that could pose a risk to California, ranging from the border of Oregon to Mexico. The report assesses these volcanoes for their risk, looking at general state of unrest, the type of volcano, the tendency toward violent eruptions and the frequency of eruptions. The report also looked at their location in relation to land use, such as population.\nThe report said that seven of the volcanoes assessed have magma beneath them, meaning they are considered “active.” They are the Medicine Lake volcano, Mount Shasta, the Lassen Volcanic Center, the Clear Lake volcanic field and the Long Valley volcanic region. The latter, in the Sierra Nevada Range, is one of the biggest calderas in the world, measuring 20 miles by 10 miles. It formed during a “supereruption” around 760,000 years ago.\nExplaining what happens when a volcano erupts, the USGS said a series of events unfold: “Steam blasts, noxious gas emissions, and landslides may occur as magma (subsurface molten rock) ascends into the volcanic edifice,” the report said.\n“Typically, the intensity of an eruption accelerates to a climactic phase and then gradually subsides. The 1914-17 eruption of Lassen Peak in Shasta County, for example, produced a yearlong series of minor steam blasts before a larger explosion sent an eruption column 30,000 feet high and unleashed destructive pyroclastic flows and lahars.”\nThe report went on: “Although considered a ‘small’ eruption by volcanologists, windborne ash drifted 275 miles eastward and fell as far away as Elko, Nevada. The most energetic part of the eruption was over in a matter of days, but recurring steam blasts and lahars created hazardous conditions for several years afterwards.”\nThe report said that, unlike other natural disasters, the associated hazards of volcanoes can sometimes last for decades, making understanding the risks they pose all the more important.\nThe potentially hazardous volcanoes in California are being monitored closely for any changes that indicate an eruption could be on the way. “Understanding the hazards and identifying what and who is in harm’s way is the first step in mitigating volcanic risk and building community resilience to volcanic hazards,” it said. “Characterizing exposure—the who and what is in harm’s way—is the first step in mitigating volcanic risk.”\nThe report found that a volcanic eruption will be more than just a local problem, with any event likely to affect the natural resources and infrastructure on a wide scale. It said the next steps will be to assess specific volcanoes and sites for vulnerabilities so mitigation strategies can be fully developed.\n“This will require the collective efforts of scientists, land managers, civil authorities, lifeline operators, and communities,” the report concludes. “Only together can California successfully manage volcanic risk and continue to enjoy the beauty and benefits of a geologically dynamic state.”']	['<urn:uuid:9c8e0522-4724-43d2-b2ea-5f0a1c6ffe5e>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	9	39	656
94	I'm looking at Microsoft Power BI for my business analysis - can you tell me what makes it good for looking at sales growth and customer metrics?	Power BI is a free business intelligence tool from Microsoft that offers several key advantages. It allows you to import data files and create interactive dashboards showing various metrics like sales growth and profit margins. You can visualize data in multiple ways, such as bar charts or interactive maps, and easily switch between visualization types. The tool supports AI-powered natural language queries for data exploration and offers features like automatic data refresh, PowerPoint export capabilities, and the ability to share dashboards with customized data access permissions. For analyzing sales, you can track metrics like total sales, profits, year-over-year comparisons, and profit margins across different products and locations.	['Effective dashboards and analytics are often missing from many LMS platforms. That’s because it’s very challenging to implement a training analytics dashboard—and every end-user wants something just a bit different from their peers.\nThis problem actually extends far beyond LMS platforms. That’s good news because the result is a powerful tool that can empower L&D teams to collect training analytics for free—and easily create clear, attractive data visualizations.\nAll disciplines, from marketing to sales to logistics, are itching for better ways to visualize their metrics and KPIs. This has sparked creation of a class of business intelligence, or BI software. These tools empower users to load in their own data and create their own dashboards and visualizations.\nPopular tools include GoodData, Looker, Tableau, and Domo. However, one tool stands out from all of them: Power BI. This free BI tool from Microsoft allows you to quickly import data files and create interactive dashboards and reports.\nAt OttoLearn, we use Power BI as part of the analytics suite with our Agile Microlearning platform. This allows our enterprise clients to build their own custom dashboards. Any L&D team can use Power BI in a similar manner.\nLet’s explore the steps in building a custom interactive dashboard. We’ll use fictional OttoLearn data.\nFigure 1: The default OttoLearn advanced analytics dashboard powered by Power BI, showing an overview of learner mastery\nAlt text: The default OttoLearn dashboard powered by Power BI includes charts showing learners’ activities and progress over time. Individual data are displayed, along with averages\nThe first step to using Power BI is to load in your data files. With OttoLearn, we export a single XLS file that contains all the data. Some platforms may generate or multiple data files, which Power BI also supports.\nOnce your data files are uploaded, you must establish relationships between the different tables. For instance, if you have a table of your users and a table with their scores (in our example, mastery levels by module), you simply link the common data columns so that Power BI knows how to connect your data.\nFigure 2: An example of linking related data across tables\nAlt text: A list of users is linked to a module that measures progress\nOnce your data points are connected, you’re ready to design your dashboard. Power BI displays all of your data. Simply drag data onto the page, then select a type of visualization. Presto!\nFigure 3: Power BI creates many types of data visualization from your data\nAlt text: Clear graphical and drop-down menus allow Power BI users to select a data visualization type and drag the desired data fields onto the visualization page\nFor example, you can create a bar chart that shows how many employees have reached the “mastery” level of a particular skill—and display the percentages by region or location.\nInsert video file “OttoLearn PowerBI Add Mastery by Location Tile”\nFigure 4: This short video shows the steps in creating a bar chart visualization of learner mastery by location\nAlt text: A bar chart shows what percentage of learners in each location has mastered a selected skill\nIt’s easy to change a visualization to a different format, or emphasize different data. Just click on it and select a different visualization type. Let’s change from a bar chart to an interactive map showing learner mastery by location.\nInsert video file “OttoLearn PowerBI Convert from Bar to Map”\nFigure 5: This short video shows the steps in converting a bar chart to an interactive map\nAlt text: With a few steps, the bar chart becomes a map; the circles over each location are sized according to the number of employees in each location have achieved mastery, and hovering over the location with the mouse pops up a box with the relevant data\nPower BI offers more features with its AI-powered natural language queries. Explore your data by typing in a question. Power BI dynamically finds and visualizes the answer. Simply drag that visualization onto a page.\nInsert video file “OttoLearn PowerBI NLP Queryp”\nFigures 6 and 7: NLP query support makes it easy to find the data you’re looking for\nAlt text: A screen capture shows the field where a user can type a query. A short video shows that typing a question dynamically creates a visualization with the data that answer the question\nIn addition to the clear dashboard with your learning analytics and the ability to easily create data visualizations, Power BI offers many more features:\n- Add filters that automatically carry over and apply to your other pages. For instance, you may want the user to be able to filter the analytics by department, brand, country, job function, etc.\n- Share dashboards with others in your organization; set row level data permissions to limit who can access what data\n- Set an automatic refresh interval for your data (using the free Personal Gateway)\n- Download your analytics as a ready-to-present PowerPoint\nUsing a tool such as Power BI to obtain and analyze free training analytics enables you to break free of the pre-packaged reporting from your LMS platform, and start designing your own visualizations. You can give each learner access to the information they need to turn training data into actionable insights.\nPower BI offers guidance, sample data, and other resources to help you become a power user:\n- Download Power BI for free\n- Sample dashboards & reports\n- Importing your data\n- Creating and managing relationships\n- Visualization options\n- Using AI-powered Q&A to create visualizations\nRegister for the Free Tools for eLearning Spotlight, an online event taking place January 30, to discover more ways to enhance your eLearning without draining the budget.', 'When you want to add more insights to customer dashboards in Power BI, you need to be able to select products.\nYou need to know what products are going well this year versus last year and the 2 years before that.\nLooking At Product Performance\nDrag the Product Name, Total Sales, Sales LY, and Sales 2Yrs Ago into the canvas. Then, turn it into a clustered bar chart visualization. Change the sort order of the table.\nYou can see a significant growth in Product 11. It wasn’t much two years ago, but then it increased last year and now it’s the highest selling product. If you select a product, the visualization also changes. It’s a great insight.\nCalculating Total Profits\nThe next thing is to calculate your total profits this year versus last year.\nAll you need to do is copy and paste the other measure, call it Profits LY, and put the Total Profits inside CALCULATE.\nNext, create a table, place the Date column in it and get rid of the hierarchy. Also, place Total Profits and Profits LY in the table. Turn the table into a clustered column chart.\nThis could potentially identify if there’s a sharp increase in profits over a time period versus another. You can also reduce the time frame and change the interaction to get better insights in Power BI.\nLooking At Individual Transactions\nIf you want to see what individual transactions contribute to your profits versus the ones that detracted to it, you need to create a table. To make this easier, utilize the measures and tables that you already have.\nGet your City, Product Name, and Order Date and put them in the table. Throw in the measures Total Sales and Total Profits.\nCalculating Profit Margins\nCreate another measure and call it Profit Margins. Use the function DIVIDE to divide the total profits by the total sales. After that, change the format to percentage.\nDrag that measure into the table and you’ll then have your table of information.\nIf you select a product, you can look at its sales along with all the different sales areas and cities that it was sold.\nFor this next step, you have to work out another logic around the profit margins. Place the Date into a table along with the Profit Margins and then turn them into a visual. These are your profit margins through time.\nHowever, this needs to be more insightful because you’ll need data and answers about the profit margins that you are selling. You need to know why some are high, above average or below average.\nTo extract those insights in Power BI, you only need to calculate your total profit margins without using any time frame. You begin by creating another measure.\nCall the measure Lifetime Profit Margin. Calculate the profit margin and then use the ALL function to disregard any day that might be in it. It will only look at everything through time. Next, turn the format into a percentage.\nIf you drag this down into the visualization, you’ll see there’s a line. This represents the average of the profit margins.\nChange the visual into an area chart.\nYou can now see your sales in the visual. Some are below the line, some are above.\nThe date August 16 has 16% while the total average is 38%. If you want to look at the individual transaction you could bring in your product or salesperson dimension. This is a great way to find insights in Power BI quickly.\nThere’s even more you could do to improve this dashboard. You could break down and evaluate all the dates that were under the average margin and only narrow in on those.\nAdding Key Metrics To The Dashboard\nTo finish things off, occupy the space on the side with key metrics and by using the calculations that you have already created.\nCreate and set up a few cards in the canvas, and then copy and paste it. Set the cards value to Total Sales, Total Profits, Sales LY, Profit Margins, and Profits LY.\nThere are a few things you can do here. Try to become familiar with the visualization and how you would incorporate them in your models.\nAnother thing you could do is create a measure for Sales Growth. Divide your total sales by your sales last year and then subtract it by 1. Don’t forget to set the format to percentage.\nNext, change the Profits LY card to Sales Growth. You can see there’s a significant sales growth.\nIf you change things here, all of these metrics change. That’s why these cards are great because they drill into specific information based on the context of the calculation.\nBy adding product insights in your customer dashboards, you’re able to further drill down to specific pieces of information that may be valuable in understanding your organization’s performance.\nThe dynamic capabilities of this added technique allows you to arrive at different results that reflect the selections you’re making.\nIf you select a different subset of customers, you’ll also see how the product’s financial performance change with it.\nAll the best,']	['<urn:uuid:1204402d-ebe7-4cc6-82e3-16fdd72b9036>', '<urn:uuid:7d5dec60-0784-41df-beba-f3474fbbfe94>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T01:37:09.743768	27	107	1804
95	What can scientists learn about ancient life and environments by studying the teeth found in archaeological remains?	By analyzing teeth remains, scientists can reconstruct former environmental conditions, determine the geographical extension of animal species, and establish their place in evolutionary history. Through examining tooth surfaces, they can understand the types of food ancient beings ate - including the proportion of fruit, grass, leaves, and meat in their diet - which helps reconstruct how our ancestors lived. For example, studies of surface abrasions on Neanderthal teeth have revealed that they had a more varied diet than previously thought, influenced by their living environment. The fossil remains, spanning almost 200 million years of mammal evolution, document how teeth evolved in different species, with chewing processes and food properties leaving distinct traces on tooth surfaces.	['In the department of Paleoanthropology and Messel Research of the Frankfurt based Senckenberg Research Institute and National History Museum, very fine textures of tooth surfaces are scientifically examined by aid of high-resolution virtual tooth models, with the main emphasis being on the molar teeth of human beings and other primates.\nTo analyse the aspect of occlusion (contact between the teeth of the upper and lower jaw) as well as the function of mammal teeth, an interdisciplinary working group of paleontologists, paleoanthropologists, dentists and dental technicians has examined and evaluated respective findings based on their digital replica generated with the help of an AICON scanning system. The combination of different research approaches as well as the exchange of different, yet complimenting specialist knowledge helps to promote the general understanding of the natural shape and functional evolution of tooth crowns.\nObjective and measuring object\nTeeth as an insight into prehistoric times: Archaeological teeth findings are of particular research interest not only for the area of dental medicine, but also very much so for the field of paleontology (science of prehistoric living creatures and organisms): The only way to identify many of the extinct animal species is through the remains of their dentition. Scientists use the teeth remains for the reconstruction of former environmental conditions, the geographical extension and phylogenetic analysis of animal species as well as their chronological classification in the evolutionary history.\nAlready very early paleontological examinations of the fossils have shown indications that not only the overall organism but also the teeth are being subjected to evolutionary changes with regard to their size, shape and quantity. Among other aspects, the teeth surfaces are affected by the physical properties of the ingested food particles which have a major effect on the individual tooth profile.\nBy analysing the chewing surfaces of the teeth, paleontologists are able to capture and interpret their respective functional changes. In doing so, it is possible to explain adaptations to the different kinds of available foodstuffs and indirectly to derive clues with regard to the climate conditions of past times.\nThanks to the knowledge about the abrasion process of teeth, a functional teeth profile comparison can help draw conclusions about our ancestors’ way of life. The information gathered gives insight, for instance, into the proportion of fruit, types of grass and leaves and meat in the diet of human beings and other living creatures, allowing the reconstruction of the conditions of how our ancestors used to live.\nThe fossil remains, dating back over almost 200 million years of mammal evolution, provide documentation of the continuous evolutionary changes of the teeth of different types of beings. The chewing process and specific properties of the nutrition intake, together with characteristic behavioral patterns, leave behind clear and distinct traces on the surfaces of the teeth. Applying this kind of knowledge, recent studies of surface abrasions of the so-called contact facets of teeth crowns have shown evidence that Neanderthals had a much more varied diet than previously assumed, whereby the food supply was significantly characterised by their living environment.\nIn the 3D laboratory of the Senckenberg Institute, the chewing surface, i.e. the contact surfaces of upper and lower jaw, are thoroughly examined. Using a special software (Occlusal Fingerprint Analyzer), newly developed with the help of research funding kindly sponsored by the German Research Foundation (DFG-FOR771), the action of the masticatory movements can be simulated through triangulated surface models and quantitatively evaluated.\nMeasuring system and setup\nDifferent approaches for the 3D scanning of the tooth surfaces are possible: Mechanic measurements, for example, capture objects on a point by point basis with the aid of tactile probes. It is a very time consuming procedure as the objects firstly have to be prepared with reference markers. Due to the point-based acquisition approach, in some cases the measuring times can take several hours, or even days. Besides, tactile measuring methods are only to a certain degree appropriate for delicate fossil objects which could be damaged during the probing process.\nFor studies in the fields of paleoanthropology and paleontology, the opticalnmeasuring approach with the AICON SmartScan is the best suited scanning method. It operates contact-free, it digitises the surface area within just a matter of seconds, captures large data volumes at a very high level of accuracy and is easily adjusted to any object size.\nUsing the AICON SmartScan, fast and precise digitisation of the archaeological findings is quickly accomplished within just a few measurements. The flexible sensor configuration, one of the key characteristic features of the SmartScan series, enables the execution of scanning processes at triangulation angles of 30°, 20° und 10°, allowing for even very difficult accessible object areas to be captured with high-detail accuracy.\nTo capture very reflecting surfaces like tooth surfaces coated by transparent dental enamel at the required accuracy to detail, there are — subject to the condition of the original — two options to prepare the object:\nIn most cases, it is sufficient to spray the surfaces to be measured (or the entire object) with an anti-reflection substance prior to the digitisation (the spray can be easily removed with water without leaving any residue). In case of very sensitive objects, the Senckenberg Research Institute steams the surfaces with ammonium chloride, a powder heated in a flask and thereafter evenly sprayed onto the object with the aid of a small pump operated nozzle.\nAlternatively, on occasion scans are also carried out on highly precise dental casts which have been reproduced as negative molds with the help of high-definition dental materials and then cast, for instance, in dental stone (Everest® Rock, KaVo). These castings made of special non-reflective plaster are perfect replicas which are ideally suited for the white-light scanner AICON SmartScan.\nThe system calibration is followed by the 3D measurement of the tooth surface within a single measuring sequence of around one second. After calculation of the 3D data in the computer, the exact digital image of the object is available, ready to be used for in-depth studies as well as for documentation and archiving purposes.\nUsing the AICON SmartScan nables the Senckenberg Research Industry and National History Museum to capture even their extremely delicate paleontological findings contact-free as well as at high speed and precision. By performing research studies using digital replica, the original finding is not only preserved, the 3D scan data also allows for even more detailed studies of the tooth surfaces than it would be possible using their originals.\nMoreover, the use of a corresponding digital data archive allows research studies and evaluations of the same finding to be conducted anywhere in the world at the same time — simply by accessing the worldwide web!\nInformation material and pictures have been kindly provided by the Senckenberg Research Institute and Natural History Museum. Upon request, we are pleased to provide a bibliography of related articles.']	['<urn:uuid:b37bc56c-9e24-4920-aff0-456a4aecbb58>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:37:09.743768	17	115	1136
96	smart home technology benefits risks security	Smart home technology offers several benefits for energy savings through devices like WeMO remote control switches, PIR motion sensors, and smart thermostats like Nest that learn user preferences and adjust energy expenditure. However, these connected devices also pose significant security risks. Without proper security measures, malicious users could remotely control home systems, potentially turning off electricity or locking residents out. To ensure security, smart home devices should implement strong encryption, automatic updates, two-factor verification, and avoid default passwords. The system should maintain three crucial security components: confidentiality (limited access), integrity (accurate information), and availability (reliable access for authorized users).	['Gadgets to save energy at home\nThe household energy is becoming more and more expensive: heating, electronic devices, light bulbs left on by mistake ... So, why not learn some tricks with these gadgets that optimize energy consumption while saving on your bills?\nWe tell you how to efficiently manage energy by taking advantage of the new technologies.\nRemote control switch\nWeMO is a control system by Belkin company that allows us to turn on or off our electronic devices quickly and easily from our smartphone. We simply install the application and enable each Wemo Switch that we want to use. Each of these Wemo Switch connects with plugs and can manage its activity from the mobile phone.\nPIR motion sensor\nAnother handy gadget is the on and off detector by presence or PIR (Passive Infra Red). These artifacts include infrared motion detector and can be easily installed in lamps or bulbs or placed in a strategic location in the room on another sensor. The motion sensor is undoubtedly the best choice to ensure the lights won’t be turned on when no one is inside.\nFull motion sensor\nIf PIR technology was already a breakthrough, this motion detector system is much more complete. The Energy Egg doesn’t just turn off the light when the sensor realizes that no one is in the room, but also turns off the other devices. How can this happen? First, devices are installed in sockets on each appliance, such as lamp and television. If we left the room without turning off any device and after a time previously set by us, the Energy Egg will disconnect these devices.\nSolar energy charger\nThese chargers use solar energy to charge the batteries of all portable devices: mobile phones, mp3 players, tablets, laptops, e-books ... So they actually have a dual function: they are respectful towards the environment and allow us to save money as we don’t need an electrical outlet to plug the devices. They are also a guarantee for those who fear running out of battery when they don’t have an outlet near.\nElectricity monitors are not a gadget itself, but its functionality justifies its inclusion in this list. These meters evaluate real-time consumption of household appliances: shows which appliances or devices generate more consumption and tells us how we can reduce spending depending on the use we make of each, saving between 10% and 15%.\nDid you know that the standby function of electrical appliances consumes up to 14% when activated? This unnecessary expenditure can be completely eliminated with standby protectors. A gadegt like TrickleStar ensures that the residual consumption of these devices is zero thanks to its sensor, which checks if the device is not in use and cuts the flow of electricity even if the standby mode is on.\nSomething similar happens to the standby function with mobile phone chargers. When we plug them to fill in the battery we don’t usually notice when the charging cycle is completed and leave it several minutes or even hours connected to electricity. Fortunately, lithium batteries do not suffer overloads like the old nickel-cadmium ones, but the consumption of electricity, to a lesser extent, still occurs. Now, thanks to the gadget Leech Plug the charger cable will automatically drop from the plug when the mobile reaches 100% charge level.\nA gadget that surely many have devised will finally become a reality after several years of development. We are talking about charging insoles, which allow users to charge batteries while walking. An insole that can be adapted to almost any type of footwear transforms energy generated by movement into charge compatible with portable devices such as mobile phones, tablets and e-books. Certainly the healthiest way to save money and protect the environment.\nThe thermostat is one of the first gadgets that boosted domestic savings. Technology has moved on and now the range of possibilities is much larger and offers many improvements. For example, Nest smart thermostats learns the preferences of the inhabitants of the house and detect if there are people or not to adjust energy expenditure. Its high price is offset in a few months of use because the savings will be evident from day one.\nAn efficient way to combine water consumption with energy consumption at home? Yes, it is possible with hydroelectric lamps. These devices use hydraulic energy to generate light through a glass molding which connects the water pipes of a house. This way, whenever the water moves in these pipes, turbines are activated and produce electric current.\nOne ideal complement for the cold winter days is the radiator booster. Sometimes, we notice that the heating does not warm up the room enough and we are tempted to turn up the heat with the consequent cost in the bill. However, with this simple gadget you can optimize its performance. We simply place it on top of the radiator, plug it in, and its bottom will begin to absorb the heat and then disperse thanks to a fan that will irradiate hot air to every corner of the room.', 'The world is increasingly interconnected through the internet. More and more “smart” devices and appliances are now online 24 hours a day. But what are the real implications of the development of IoT and home automation? Are users really safe? What is the level of protection for their data? And what are the biggest risks to privacy?\nDuring Codemotion Milan 2018, Guy Rombaut, CTO at OneFit, spoke to us in depth about these issues.\nThere are now many low-cost devices always connected and already installed in the workplace and home. Exposing these devices to the internet without precautions and without sensitising the users to their implications is a big risk. In addition to the possible violations of individual privacy, there is also a more relevant aspect concerning security.\nIn the absence of the necessary countermeasures, malicious users could take possession of our “smart” car, or turn off the electricity of a home remotely, or lock us out of the house. This is the potential downside of the domotics and IoT devices.\nThe well-known Ransomware WannaCry is a striking example of how cybersecurity is a topic too often ignored today. Although there is a lot of work being done in schools to make students aware of cybersecurity, very often the most basic security rules are not applied as much by individual users as by companies. So can a secure smart system be implemented? Obviously, yes. Just follow some basic rules that should be applied by all manufacturers on the market:\nAlways employ staff specialised in web security during development. Working with developers already accustomed to the use of normal security protocols will improve the security of your project and your service. This is because having a good knowledge of the rules of cybersecurity will mean you implement them during the early stages of development. Present the project in production, already prepared for the implementation of the right security protocols.\nPrioritize security and security features of the device before sending it to production. This will protect not only the end user but also the company itself from possible future security problems. Another important factor in ensuring the security of an IoT system is to ensure constant updates, for the entire lifecycle of the device. Moreover, it’s very important to ensure that the device updates automatically without requiring user intervention. Eliminating the need for manual intervention for the update allows you to not only improve the user experience but also to prevent the user from postponing the update.\nSecurity obviously goes through cryptographic algorithms. Use data encryption algorithms during communications to and from the device and to store passwords; it’s key to ensuring that user data is kept and stored in a stable and secure manner. For example, you can create web apps for your IoT device which always use the HTTPS protocol, so as to protect its users with an SSL or TLS certificate.\nAlthough it is possible to implement a vast array of protocols, one of the main security risks remains the user themselves. That’s why you have to oblige the user to set a strong password, to change it at least once a year and to activate the two-factor verification. The development team will also have to implement and use smart methods to recover login credentials.\nOnce you get to the stable version of the software, it is good to perform, as well as during the whole life cycle of the device, various penetration tests to ensure that the implementing security protocols work well.\nIn addition, a company that makes IoT devices should never use a “default” password in the factory settings. This is because users often do not change the password if not forced. Leaving a standard password on the device is like leaving the door open at home day and night. Another fatal mistake that a development team can make is to implement secret and undocumented remote access methods such as backdoors. This hidden remote access does nothing but make the attack platform wider and, if discovered by a hacker, can lead to intrusions against which you can not defend.\nAt the end of his talk, Guy Rombaut showed us how easy it is to get the password of a device that has not been designed with the right security protocols. He looked for a simple target like a public webcam and, after finding the public IP, he used logic and an open source tool, nmap, to understand the username and password to access the webcam control panel.\nNmap can be used to discover hosts and services on a computer network, thus building a “map” of the network. To accomplish its goal, Nmap sends specially crafted packets to the target host(s) and then analyses the responses. For example, Rombaut scans the network in this way:\nnmap -T4 -A -v 192.168.0.0/24 -p 21, 75-100,443, 554, 22222, 3322,3333, 5000, 5550, 8080, 8081, 100002\nNow imagine that this system is not a simple public webcam but for your car or the door of your home. Can you comprehend how great the risks of an IoT ecosystem without security standards are?\nSo a truly safe IoT system should always have three characteristics:confidentiality, integrity and availability, also known as the CIA triad, is a model designed to guide policies for information security within an organisation. The model is also sometimes referred to as the AIC triad (availability, integrity and confidentiality) to avoid confusion with the Central Intelligence Agency. The elements of the triad are considered the three most crucial components of security.\nIn this context, confidentiality is a set of rules that limits access to information, integrity is the assurance that the information is trustworthy and accurate, and availability is a guarantee of reliable access to the information by authorised people.\nIn addition, our devices should always be able to be controlled by something that can be directly associated with us, such as another device or biometric values (fingerprints or face ID).\nIn conclusion, Guy Rombaut hopes that all developers and users will start to become more aware of the risks of the IoT world and put pressure on companies to implement ever better security systems.']	['<urn:uuid:6716aa6e-a4df-461f-80f8-22418193e0c7>', '<urn:uuid:a5e90dd6-352d-44ad-98ab-56f1729b20ee>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:37:09.743768	6	99	1868
97	ritual mask purposes cultural significance practical usage functions different societies	Masks serve dual purposes across societies. In cultural and spiritual contexts, they are used in traditional ceremonies and rituals, allowing performers to create powerful connections to the sacred world and transform their identities. For example, mask traditions span from Mongolia to South America, and from West Africa to the Himalayas, where they're used in religious ceremonies and cultural festivals. On the practical side, masks serve defensive and utilitarian functions, particularly in conflict and sports. They are used to protect wearers during battle, like samurai face guards, and in modern sports like ice hockey. Additionally, masks are used to intimidate opponents both in combat scenarios and competitive sports, with many athletes customizing their masks to demonstrate team affiliation or past injuries.	"[""Chris Rainier and Insight Editions are pleased to present the publication and exhibition tour of MASK.\nFor over thirty years, Chris has been in search of the meaning of the mask. What began as a thorough visual documentation of the traditional mask rituals of New Guinea developed into a voyage of discovery that took him around the globe photographing traditional mask traditions. From the steppes of Mongolia to the jungles of South America, from the deserts of West Africa to the continent of India and the high Himalayan mountain monasteries of Tibet and Bhutan, from Day of the Dead Festivals in Mexico to the lands of the First Nation tribes of North America and from Sri Lanka to modern Europe, Chris has documented hundreds of different rituals of the tradition of the mask, both on still film and video.\n© Chris Rainier, Tlingit Raven Mask, Alaska\nThe MASK book (12X12 size) explores the origins of the mask with over 130 images as well as text by both Chris Rainier and Pico Iyer, the highly respected author of Video Nights in Kathmandu and the TED book The Art of Stillness. In addition, the work is complemented by deeply researched anthropological captions that shed light on why mankind has always worn masks - authored by Dr. Robert Welsch.\nThese photographs, distilled from a lifetime of work, reveal the precision of Chris Rainier's vision, the breadth of his imagination and the reach of his spirit as expansive and all embracing as the many worlds through which he has traveled.\n© Chris Rainier, Hamatsa Raven Mask, Alert Bay Canada\n© Chris Rainier, Butterfly Mask, Bwa region, Burkina Faso, West Africa\nEver since the dawn of human civilization, mankind has been in search of the sacred and a profound connection to the spiritual world. For countless traditional cultures around the planet, the dancing of the mask allows the performers and their audience to create a powerful relationship to a meaningful sense of the sacred. While masks have been hung in the great museums of the world, they are all too often separated from their context and meaning. Chris' mission has been to bring the masks back to life. His powerful images bring out their deep spiritual meaning and each of the masks is shown in the world in which it is worshipped and understood. They are profoundly alive and intensely powerful. Chris' images truly reveal how the masks still dance.\nAll about Chris Rainier:\nChris Rainier is a National Geographic Society EXPLORER and documentary photographer/filmmaker - who is highly respected for his documentation of endangered cultures and traditional languages around the globe. In 2002 he was awarded the Lowell Thomas Award by the Explorers Club for his efforts on cultural preservation, and in 2014 he was elected a Fellow of the Royal Geographic Society of London/UK -specializing in cultural preservation\nHe is the Director of The Cultural Sanctuaries Foundation\n- a global program focused on preserving Biodiversity and Traditional Cultural Knowledge.\n© Chris Rainier, Buddhist Deer Mask, Mustang region, Nepal\nDuring his continued tenure with the National Geographic Society he has been the co-founder and co-director of both the Enduring Voices Language Project and Director of the All Roads Photography Program, designed to support indigenous groups with modern technology desiring to document their traditional culture and create sustainable solutions to preserve the planet in the 21st Century. In addition as a NG Fellow he was an Editor for NG Traveler focused on documentation of traditional culture.\n© Chris Rainier, Shaman performing, Northern Mongolia\n© Chris Rainier, Sirige Mask, Dogon region, Mali, West Africa\nRainier has completed photographic projects for the United Nations, UNESCO, Amnesty International, Conservation International, the Smithsonian Institution, Time Magazine, the New York Times, LIFE Magazine, and the National Geographic Society. Rainier has photographed global culture, conflict, famine, and war in such places as: Somalia, Sarajevo/Bosnia, Sudan, Ethiopia, Rwanda, Cambodia, and Iraq for TIME Magazine, - and for NPR Radio.\n© Chris Rainier, Black Buffalo Mask, Ohkay Owingeh Pueblo, New Mexico\n© Chris Rainier, Naga Raksha Mask, Southern Sri Lanka\nIn the early 1980's Rainier was Ansel Adams last photographic assistant- during his tenure with the noted photographer- he worked with Mr. Adams to amplify the use of Art Photography as a social tool - helping to preserve threatened wilderness areas and National Parks. Rainier went on to collaborate with UNESCO and IUCN on a Global Project using photography to preserve endangered wilderness areas around the world.\n© Chris Rainier, Theyyam Performance, Kerala, India\n© Chris Rainier, Kathakali Pefromance, Kerala, India\nRainier's photography and books have been widely shown and collected by museums around the world, including the Australian Museum in Sydney, the Bibliothèque Nationale in Paris, the International Center of Photography in New York, the George Eastman House International Museum in Rochester, New York, The National Geographic Society, and the United Nations.\n© Chris Rainier, Buddhist performance Masks, Thimpu, Bhutan\n© Chris Rainier, Egungun Mask Yoruba region, Benin, West Africa\n© Chris Rainier, Samurai Mask, Tokyo, Japan\n© Chris Rainier, Mud Men performance, Highlands, New Guinea\n© Chris Rainier, Plank Mask, Bwa region, Burkina Faso, West Africa\n© Chris Rainier, Buddhist Mask, Paro, Bhutan\n© Chris Rainier, Masked Malangan Tatanua Dancers, New Ireland, New Guinea"", 'Masks and masking\nby volunteer Rebecca Vicary-Smith\nMasks assume a variety of roles and they are present in every society in one form or another. Masks are worn primarily as part of a costume or they can be used as a method of defence. The primary purpose of the masking tradition is to hide the wearer’s identity. This often provides an anonymity that allows the wearer to transform and change their identity, even assume a completely new persona.\nThis process is for the benefit of one’s self and others, with masks playing a distinct role in performance and ritual. The changes are not a separate phenomenon but also, to an extent, reflect humanity’s desire and ability to transform and adapt. It can also be thought of as a method of covering one’s physical identity in order to display a separate part of themselves. This may be through the channelling of spirits or in modern fiction such as superheroes, both of which require the concealing of an outer identity to allow a part of their personality to show through. Or, in the case of villains, hiding one’s true purpose.\nThe idea of transforming oneself into another self is one that has gripped humanity in desire and fear for a very long time. When someone’s ‘true’ identity is concealed we are naturally fearful of the new identity being portrayed through costume. This applies even more so to the ritual use of masks where a supernatural being is present through the mask.\nComing of age ceremonies are just one of the many rituals that involve masks. Masks resume their role of transforming an individual when used in rituals as they often allow the wearer the ability to change into a representation of the spirits being honoured during these rituals. This can be seen in the mukyeem mask of the Kuba (Figure 1). This mask is used to represent mythological characters, specifically the mythical ancestor Woot.\nThis process seems foreign and strange for Europeans to understand, and therefore has come under fire as a primitive practice. However, we forget that this is a process that results in the wearer undergoing a change to connect with a higher power. This is something that almost every society has experienced and which includes the ancient Romans and Greeks, classical civilisations considered to be the foundation of Western society. This notion of masks and costumes as foreign or relating to ignorance and the superstitious has led to a serious decline in their presence and their use in rituals within Oceania from the period of European contact.\nChief mourner’s costume\nHere (Figure 2) we have a rare example of a Polynesian costume that has survived 200 years of European contact. This particular costume was worn on funerary occasions. The face of the wearer is deliberately obscured and their body is exaggerated with the intention of causing fear and respect within the community for the recently deceased chief. Visitors can see this costume in the World Cultures gallery.\nThe wearing and making of masks, especially in prehistoric times, is an area dominated by men with very few women being involved in these activities. Men were the traditional hunters and the masks were used in rituals and festivals served to celebrate or protect the hunt. Therefore the idea of a male-controlled environment has continued to this day. This can be seen in modern theatre with men assuming women’s roles in plays, both historically and in present day.\nFemale roles that require these masks are therefore portrayed from the eyes of men with 3 main roles being emphasised: the maiden, the mother and the crone. These can be seen in many cultures, especially in coming of age ceremonies for men and women, where it may be expected that women would be portrayed in a separate light. Even in women’s societies, such as those found in Sierra Leone, Liberia and Guinea, the masks are made by men. The main difference is that they are owned and used by women which is rarely seen elsewhere.\nMasks also play an indispensable role in conflict and sport. These masks are for defensive and offensive purposes. In most modern sports we can often find at least one member wearing a defensive mask, for example in ice hockey. Masks are also used to not only conceal one’s identity but to also promote one’s performance, take Mexican wrestling, for example.\nLike the face guards on samurai armour, sport masks are designed to look intimidating, with many players customising their masks to demonstrate their previous injuries in the sport or to show their team affiliation. Many groups wear masks in battle or during execution to intimidate their enemy or those receiving punishment.\nIn the collection here at the RAMM we have a janus-mask worn by a ritual specialist called an nganga. It originates from the Democratic Republic of Congo in Central Africa and was worn as an entire costume for maximum impact (Figure 3). This Ndungu costume is worn by the nganga to cross boundaries. He is a terrifying figure with the means of moving in both the real and ancestral worlds. This example was acquired by ivory and rubber trader Richard E. Dennett.\nMasks worn in conflict are used both to intimidate their opponents but also to protect the wearer throughout the conflict. These can be seen everywhere, whether the opponent be human in a battle, such as the metallic helmet of a knight, or the elements in the case of astronauts.']"	['<urn:uuid:50f7aa83-5fc2-46de-a66a-9e5fb557ec71>', '<urn:uuid:991c830b-aab9-42b3-bb5a-54b219798991>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:37:09.743768	10	120	1786
98	What percentage of bone marrow cells indicate multiple myeloma?	For people who have multiple myeloma, bone marrow plasma cell levels usually make up 10% or more of their cells, while healthy individuals have less than 5% plasma cells.	"[""Understanding Multiple Myeloma | Confirming Your Diagnosis | Staging | Subtypes | Resources\nMultiple myeloma is a bone marrow cancer. It can affect your bones and kidneys, as well as your levels of healthy blood cells. It is a fairly rare cancer, with just under 35,000 new cases diagnosed each year. Multiple myeloma happens when cancerous plasma cells (white blood cells that make antibodies) build up in your marrow and crowd out healthy blood cells. The word “multiple” means that cancer cells are found in more than one area of the body. Smoldering myeloma and monoclonal gammopathy of undetermined significance (MGUS) are conditions that can happen before multiple myeloma, but these conditions don’t usually need treatment.\nThe first step in creating a treatment plan is checking your diagnosis of multiple myeloma or another plasma cell disorder.\nYour physicians will use blood tests and a bone marrow biopsy to learn more about your myeloma, find out the stage of your cancer, get other information that can help predict what will happen with your disease and find out which organs in your body are affected.\nStaging means finding out how far your multiple myeloma has spread. Knowing the stage of your cancer helps your physicians predict which treatments are most likely to control your disease or put it into remission.\nPhysicians use the Revised International Staging System (R-ISS) to stage multiple myeloma. It measures several different things in your body — albumin levels, Beta-2 microglobulin (B2M), lactate dehydrogenase (LDH) and genetic changes — to place you in one of three stages:\nTo find out how far your multiple myeloma has spread, your Physician will use blood tests and a bone marrow biopsy.\nMost patients with myeloma have a protein that can be measured in their blood. This protein is called a monoclonal protein, or sometimes an M-spike or M-protein. The levels of this protein help your doctor understand the state of your disease.\nBone marrow biopsy and aspiration is another type of test that is often used. It is the best way to check the percentage of plasma cells in the bone marrow. For people who don’t have multiple myeloma, blood plasma cells make up less than 5% of their cells. For people who have multiple myeloma, bone marrow plasma cell levels usually make up 10% or more of their cells.\nIf your disease is advanced, your physician may do imaging to see if there is any bone damage or large tumors. This imaging may be a whole-body low-dose CT scan, PET/CT scan, bone marrow MRI or skeletal survey.\nThere are more treatment options for multiple myeloma than ever before. No matter what type of treatment you need, multiple myeloma specialists at Fred Hutch will work closely with you, your family and each other to get you back to health.\nMost patients who get treatment for multiple myeloma have this standard subtype, which means that the buildup of cancerous plasma cells is causing symptoms. This is also called having symptomatic multiple myeloma.\nSmoldering myeloma, also known as asymptomatic myeloma, often happens before active multiple myeloma. Patients with smoldering myeloma don’t usually get treatment unless they are considered high-risk. Most people with smoldering myeloma will eventually develop active multiple myeloma.\nMGUS stands for monoclonal gammopathy of undetermined significance. Usually, MGUS is diagnosed when a physician finds abnormal plasma cells during a routine blood test. It’s rare that patients have symptoms from MGUS, and most cases don’t turn into cancer. However, regular monitoring is important in case it turns into smoldering myeloma, which could later turn into active multiple myeloma.\nThere are many resources online for learning about your disease. Health educators at the Fred Hutch Patient and Family Resource Center have compiled a list of trusted sources to help you get started.\nWhether you are newly diagnosed, going through treatment or know someone with cancer, our staff are available to tailor personalized resources and answer questions about support options in the community.\nAmerican Cancer Society (ACS): Overview of Multiple Myeloma\nIf you have multiple myeloma or are a caregiver for someone who does, knowing what to expect can be helpful. Here you can find out all about multiple myeloma in adults, including risk factors, symptoms and how they are found and treated.\nAmerican Society of Clinical Oncology (ASCO): Guide to Multiple Myeloma\nThis is ASCO's guide to multiple myeloma. Here you can learn more about multiple myeloma cancer, treatment, the latest research and clinical trials.\nASCO Answers: Multiple Myeloma\nASCO Answers is a collection of oncologist-approved patient education materials developed by ASCO for people with cancer and their caregivers. Here you can find illustrations and information on multiple myeloma.\nLeukemia & Lymphoma Society (LLS): Overview of Myeloma\nIf you have multiple myeloma, LLS is a good place to start to better understand your diagnosis, treatment and support options.\nNational Cancer Institute (NCI): Plasma Cell Neoplasms (Including Multiple Myeloma)-Patient Version\nThe NCI is the federal government's principal agency for cancer research and training. Here you can find more information about multiple myeloma cancer treatment, research and coping with cancer.\nNational Comprehensive Cancer Network (NCCN) Guidelines for Patients: Multiple Myeloma\nThis step-by-step guide to the latest advances in cancer care features questions to ask your physician, patient-friendly illustrations and glossaries of terms and acronyms.\nCancerCare: Multiple Myeloma General Information and Support\nCancerCare provides free, professional support services for people affected by multiple myeloma, as well as myeloma cancer treatment information and additional resources, including financial and co-pay assistance.\nInternational Myeloma Foundation (IMF)\nThe oldest and largest foundation focusing specifically on multiple myeloma. IMF funds research, provides education, lends support and promotes advocacy initiatives that allow collective voices to be heard.\nMultiple Myeloma Research Foundation (MMRF)\nA nonprofit organization that works to accelerate the development of new multiple myeloma treatments.\nA division of The CrowdCare Foundation, a patient-driven, 501(c)3 non-profit organization that empowers patients with rare diseases at each step of their disease – from diagnosis, through education, care and on to a cure.\nNorthwest Multiple Myeloma Fighters\nA support organization for multiple myeloma patients and family in the Greater Puget Sound area.""]"	['<urn:uuid:795c7327-228b-4187-b708-5c0664443095>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:37:09.743768	9	29	1017
99	What are the main improvements in protecting kids' data under the new California privacy law that was recently approved?	The California Privacy Rights Act (CPRA) strengthens protection for children's data in two main ways. First, it prohibits businesses from selling or sharing personal information of consumers under 16 years old without proper authorization - either from the child themselves (if aged 13-16) or from a parent/guardian (if under 13). Second, it increases enforcement by tripling the maximum penalties for privacy violations involving consumers under 16 years of age.	['On November 5th, California voters passed Proposition 24, a ballot initiative that expands consumer rights and regulatory enforcement authority under the California Consumer Privacy Act of 2018 (CCPA). The ballot initiative, titled the California Privacy Rights Act of 2020 (CPRA), imposes new compliance obligations on affected businesses less than a year after the CCPA went into effect. Even though the CPRA is not effective until Jan. 1, 2023 (and will not be enforceable until six months after that), businesses should immediately begin revising their privacy policies and procedures given the complexities of the new law and the fact that the CPRA applies to personal information collected by a business on or after Jan. 1, 2022.\nAmong other things, the CPRA does the following:\n- Establishes the California Privacy Protection Agency: The California Privacy Protection Agency is a new agency dedicated to enforcing the state’s consumer data privacy laws. Previously, the state’s attorney general’s office was tasked with implementing and enforcing the CCPA, but the attorney general’s office stated that it only had the resources to pursue a couple of cases each year. The California Privacy Protection Agency is a stand-alone agency dedicated to protecting privacy and will investigate and adjudicate potential violations, assess penalties for violations and assume rulemaking authority.\n- Defines a new category of “sensitive personal information:” “Sensitive personal information” includes a consumer’s social security, driver’s license, state identification and passport numbers, a consumer’s account log-in, financial account, debit card or credit card number in combination with any required security or access code, password or credentials allowing access to an account, a consumer’s precise geolocation, a consumer’s racial or ethnic origin, religious or philosophical beliefs, or union membership, the contents of a consumer’s mail, email and text messages (unless the business is the intended recipient of the communication), a consumer’s genetic data, the processing of biometric information for the purpose of uniquely identifying a consumer and personal information collected and analyzed concerning a consumer’s sex life or sexual orientation.\n- Limits businesses’ use of sensitive personal information: The CPRA requires businesses to disclose whether the business collects sensitive personal information, the types of sensitive personal information collected, the purpose for which the sensitive personal information would be collected and the length of time that the business intends to retain the sensitive personal information. Further, the CPRA allows consumers to opt-out of having their sensitive personal information used or disclosed for advertising or marketing.\n- Requires businesses to correct a consumer’s inaccurate personal information upon the consumer’s request: Under the CPRA, a business must correct any inaccurate personal information maintained by the business upon the consumer’s request.\n- Prohibits businesses from sharing or selling a consumer’s personal information to third parties if requested by a consumer: Under the CPRA, a consumer may, at any time, direct a business that sells or shares personal information about the consumer to third parties not to sell or share the consumer’s personal information.\n- Imposes additional protections for children’s data: The CPRA prohibits businesses from selling or sharing the personal information of consumers if the business has actual knowledge that the consumer is less than 16 years of age, unless the child (in the case of children 13-16 years old) or the child’s parent or guardian (in the case of children who are less than 13 years of age) affirmatively authorize the sale or sharing of the personal information. In addition, the CPRA triples maximum penalties for violations concerning consumers under the age of 16.\n- Limits the length of time in which businesses may retain consumers’ personal information and sensitive personal information: The CPRA prohibits businesses from retaining a consumer’s personal information or sensitive personal information for longer than reasonably necessary for the disclosed purpose for which the information was collected. This provision likely will lead to specific rulemaking or litigation on what is “reasonably necessary.”\n- Requires businesses to adopt reasonable security measures with respect to personal information: The CPRA expressly requires businesses to implement reasonable security procedures and practices appropriate to the nature of the personal information to protect the personal information from unauthorized or illegal access, destruction, use, modification or disclosure. This section, due to the “reasonable security practices and procedures” term, will also be subject to significant disagreements and disputes.\nThe CPRA just passed, but businesses have a short window of time to implement new policies and procedures that are compliant with this new law. If you would like assistance in crafting these policies or procedures or in interpreting the CPRA, please feel free to contact Mark Rasch (firstname.lastname@example.org /301.547.6925), Andrew Wilber (email@example.com / 216.736.7298) or any member of KJK’s Cybersecurity, Data Breach, and Privacy Team.\nKJK publications are intended for general information purposes only and should not be construed as legal advice on any specific facts or circumstances. All articles published by KJK state the personal views of the authors. This publication may not be quoted or referred without our prior written consent. To request reprint permission for any of our publications, please use the “Contact Us” form located on this website. The mailing of our publications is not intended to create, and receipt of them does not constitute, an attorney-client relationship. The views set forth therein are the personal views of the author and do not necessarily reflect those of KJK.']	['<urn:uuid:5bcbba1c-dd84-488a-9c58-ce925ec2b957>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	19	69	885
100	home brewing what is minimum oxygen level needed for good yeast performance	Five ppm (parts per million) dissolved oxygen in wort is considered a bare minimum for proper yeast growth. Commercial breweries typically aim for 8 to 10 ppm dissolved oxygen in wort prior to fermentation. Eight ppm of oxygen can be achieved using plain air, while higher levels usually require pure oxygen.	['Oxygen in beer is undesirable except at one point (and only one point) in the brewing process. That lone point is when the post-boil wort has been chilled down to fermentation temperature, but before the yeast has been pitched into it.\nOxygen dissolves into wort as a function of temperature and specific gravity. As such, the colder and less concentrated the wort, the more oxygen will be able to enter into solution. All the bubbling and splashing that occurs during the boil drives most of the oxygen out of solution because the wort is too hot while boiling. Therefore, oxygen must be replenished after the wort is cool and able to retain the oxygen in solution again.\nOxygen is essential for yeast growth and reproduction. Yeast must grow and reproduce first, before actually fermenting the wort to make beer. Yeast needs oxygen to synthesize the material for expanding cell walls; namely sterols and fatty acids. Overlooking proper wort aeration can lead to problems such as long lag times before the start of fermentation, stuck or incomplete fermentation, or excessive ester (fruit flavor) production, any of which would produce less than desired results. Now that we understand the “when” and “why” of wort aeration, let’s examine how homebrewers can supply oxygen to chilled wort to meet the needs of their chosen favorite fungi.\nCommercial breweries typically shoot for 8 to 10 ppm dissolved oxygen in wort prior to fermentation. Five ppm dissolved oxygen in wort is considered a bare minimum for proper yeast growth. Eight ppm of oxygen in chilled wort can be achieved using plain old air (which is 21% oxygen). Oxygen saturation above eight ppm in wort usually requires the use of pure oxygen. Again, as the specific gravity of wort increases, its ability to absorb oxygen decreases, thus making wort oxygenation of big beers even more critical to their successful production. Because it is difficult and expensive for homebrewers to measure dissolved oxygen in wort, experimentation will be needed in order to determine if the oxygenation method you perform is sufficient. If your fermentation temperature and yeast pitching rate are good, but fermentation is sluggish or incomplete, you should look at stepping up your oxygenation technique for future batches until you achieve success. There are three approaches homebrewers typically use to oxygenate wort; agitation, splashing and injection of either air or pure oxygen.\nAfter the wort has been chilled and transferred to the primary fermenter, there are a number of methods that can be employed to agitate the wort to introduce oxygen. If your wort is in a glass carboy, you can cover the mouth of the carboy with a loose fitting cap, get a firm grip and rock the carboy forth and back to slosh the wort around inside. Care must be taken to support the carboy on a cushioned surface and to maintain a secure hold on the carboy at all times while agitating.\nIf your wort will be fermented in a bucket or other vessel with a wide open top, a stainless steel whisk borrowed from the kitchen can be sanitized and used to whip the wort until it has at least a couple inches (several centimeters) of foam on top. If you don’t think your arm will last long enough to manually whisk the wort, you can sanitize a (new) paint stirrer, attach it to an electric drill and agitate the wort accordingly. A word of caution if using an electric drill and paint stirrer: be sure the drill is connected to a ground-fault protected receptacle, take care not to damage the fermenting vessel (or yourself), and avoid splashing the wort out\nof the fermenter.\nRegardless if your wort is in either a carboy or bucket, if you need to add cold water to make up the volume of wort to the desired level, splashing or spraying the cold water into the wort can also increase the amount of oxygen that will go into solution. Agitation is the simplest and least expensive method for aerating wort, but involves a little more time and elbow grease than splashing or injection.\nAnother opportunity to add oxygen to your batch of homebrew is when the chilled wort is being transferred from the kettle to the fermenter. If the transfer is taking place through some type of hose or tubing, attaching a sanitized bent pen clip or other device (such as a Fermentap Siphon Spray) to the end of the tubing will cause the wort to spray and splash as it enters the fermenter. If you have two sanitized vessels, you can transfer the wort between the two (at least five or six times) with as much splashing as practical. Pouring the wort through a sanitized wire mesh strainer can also increase splashing during the wort transfer process. As with agitation, splashing should result in at least a couple inches of foam on the wort surface. Be sure to brace the container receiving the wort so it cannot move and cause spilled wort. Also, be certain that containers and strainers are thoroughly sanitized before use. Splashing is a relatively simple and inexpensive approach to oxygenation, but if accomplished by pouring wort between vessels it may lend itself to contamination from airborne microbes.\nPerhaps the most effective approach to oxygenate wort is to directly inject air or pure oxygen into the wort. This can be done after the wort has been chilled in the kettle and conveyed to the fermenter, or during the delivery of wort from a wort chiller to a fermenter. While this technique is very effective, it usually involves more equipment and expense than both the agitation and splashing methods\nThe most common method of oxygen injection used by homebrewers is to infuse air or oxygen into the wort after it has been chilled and transferred to the primary fermenter. This technique uses either pressurized air or oxygen and some type of diffuser to bubble the gas into the wort to get oxygen into solution.\nTo accomplish oxygenation using air, an aquarium pump (or other air compressor) can be used to pressurize the air and send it through tubing and some type of filter to remove dust and/or microorganisms. A HEPA (High Efficiency Particulate Air) filter with a .023 micron sized filter, or a filter improvised from cotton balls wetted with alcohol is recommended to keep the air as contaminant-free as possible as it enters the wort. Many homebrew suppliers now carry air pumps, filters and diffusers for this method of wort oxygenation.\nSimply running air from the end of a section of tubing into the wort is not a very efficient means of gas transfer as the bubbles will be few in number and large in size. To get the most efficient transfer of oxygen into solution, some sort of diffusing apparatus is necessary at the end of the tube immersed in the wort. Diffusers range from very inexpensive aquarium-type air diffusion stones to sintered stainless steel diffusers.\nAquarium stones are inexpensive, but less efficient than a 0.5 micron stainless steel diffuser, which is preferred from both an efficiency and longevity standpoint. The small 0.5 micron holes in a stainless steel diffuser create a profusion of very tiny bubbles that create a larger gas to liquid surface area to dissolve oxygen into the wort with greater efficiency. A stainless steel diffuser can be sanitized by boiling it in water for 15 minutes prior to use. Artificial stones designed for use in aquariums can be sanitized by a quick soak in vodka, but after a few uses become fragile and tend to disintegrate quite easily (as has been my personal experience).\nWhen using pure oxygen instead of air to oxygenate wort, filtration is not necessary as it is unlikely for microorganisms to survive in an atmosphere as reactive as pure oxygen. Otherwise, the technique for using oxygen is the same as that used for air, except for the amount of time the gas is bubbled through the wort. Since air is only 21% oxygen, it takes more bubbles of air to dissolve the desired amount of oxygen into solution. An apparatus diffusing air into chilled (< 70 °F/21 °C) wort typically needs to run for a minimum of 15 minutes to achieve adequate oxygenation, where the same set-up using pure oxygen would require only a minute or two at the most to achieve the same result.\nThe Oxynater™ is a ready-made apparatus for diffusing oxygen into a vessel of wort and includes a cylinder of oxygen, gas regulator, tubing and a stainless steel diffuser. Commercial or industrial grade oxygen is all that is necessary for use in wort oxygenation. Aviation or medical grade oxygen is more expensive, difficult to obtain and is not necessary for the purposes of brewing. A word of caution when using pure oxygen: there are few things as flammable as pure gaseous oxygen, so be sure there are no sparks\nor flames in the vicinity of where the gas will be used.\nA more sophisticated method of oxygenating wort is to inject air or pure oxygen into the wort as it passes from the kettle or chiller into the fermentation vessel. This is the approach commercial breweries commonly use to properly oxygenate their wort.\nThere are two basic techniques that can be used to inject air or oxygen into wort as it streams into the fermenter; by means of a simple venturi, or by injecting compressed air or oxygen into the flowing wort. The venturi approach can be accomplished by allowing the wort to flow through a section of tubing or pipe that has small (0.02 inches or 0.5mm) holes drilled through it to allow air to be drawn into the tube as the liquid passes through it. A commercial example of such a device is the Wort Wizard™ that uses a venturi in flowing water to draw wort from the kettle into a carboy and draw air into the wort at the same time (http://www.wortwizard.\ncom/). A homemade venturi tube can be constructed as described at: http://www.kettlemoraine.com/mikesbeerpage/gadgets.php. One drawback of some venturi systems, however, is that they lack a filter for the air being drawn into the wort, which creates an opportunity for microbial contamination.\nAn alternative to the venturi tube technique is to use pressurized air or pure oxygen which is directly injected into the wort stream. This can be accomplished by placing a tee in the wort transfer line and injecting the air or oxygen through a diffuser inside the tee as the wort streams past. Such a project is described on page 61 in this issue. The injection process lasts for as long as it takes to run the wort between vessels, resulting in all of the wort being exposed to the injected gas.\nSimple regulators that attach to disposable oxygen cylinders that are equipped with a 1⁄4-inch barbed fitting are available from many homebrew suppliers. Disposable oxygen cylinders are available from most hardware stores. The gas regulators simply have a knob that is turned to adjust the pressure of the oxygen leaving the cylinder. This allows the user to release only enough gas to generate the desired degree of bubbling from a diffuser into the wort. Infusing air or oxygen into wort may generate a significant amount of foam that may overflow from a fermenter if left unmonitored. Anti-foaming agents made of food grade silicone are available from homebrew suppliers to control foaming during oxygenation without impacting head retention or flavor of the finished beer when they are used as recommended.\nOxygenation of cooled, post-boil wort is essential for proper fermentation by brewing yeast. To be successful, homebrewers must pay heed to this important step in the brewing process by selecting and implementing a successful wort oxygenation technique that fits their equipment and budget. Oxygen is critical for the growth of brewing yeast . . . and happy yeast makes for happy homebrewers! For more information about aeration, read “Keys to Aeration” in the “Advanced Brewing” column from December 2005.\nJon Stika is an avid homebrewer from East Dickinson, North Dakota and a member of the Heart River Homebrewer’s club. He writes “Techniques” for every issue of BYO.']	['<urn:uuid:40f868f5-6b57-4f22-851e-60903e17624e>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:37:09.743768	12	51	2031
